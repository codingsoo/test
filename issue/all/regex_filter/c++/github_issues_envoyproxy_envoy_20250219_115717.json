[
  {
    "number": 37908,
    "title": "A question about Incremental xDS in high concurrency scenarios",
    "created_at": "2025-01-07T10:10:01Z",
    "closed_at": "2025-02-13T20:01:30Z",
    "labels": [
      "question",
      "stale",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37908",
    "body": "Hello, I would like to know how Envoy handles concurrent scenarios when using the incremental XDS of Envoy. When the control plane sends a request and Envoy has not yet completed processing and responded with an ACK, the control plane issues a new configuration update? Is it maintaining an internal queue for sequential execution? How much does its performance improve compared to full updates when the control surface is frequently updated?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37908/comments",
    "author": "bearslyricattack",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2025-01-07T17:19:42Z",
        "body": "The control plane messages (which are a stream of responses to Envoy's stream of requests) are processed sequentially. There's no explicit queue within Envoy -- the response stream will accumulate in the grpc client, which will exert back pressure to the control plane in the event that multiple incremental updates are issued quickly enough.\r\n\r\nI can't speak to the performance of the update within Envoy. I know that configurations with a large number of listeners, clusters and/or clusters with high cardinality can take significant time just to transmit from the control plane to Envoy. In my experience the network delay dominated the latency between the control plane initiating an update and completing it within Envoy."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-02-06T20:01:22Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-02-13T20:01:30Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 37478,
    "title": "Fine-tuning of happy eyeballs dns resolution",
    "created_at": "2024-12-03T09:22:31Z",
    "closed_at": "2024-12-18T14:33:53Z",
    "labels": [
      "question",
      "area/load balancing",
      "area/dns"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37478",
    "body": "*Title*: *Fine-tuning of happy eyeballs dns resolution*\r\n\r\n*Description*:\r\n\r\nWith that configuration and clusterType \"LOGICAL_DNS\"\r\n\r\n```\r\n        caresDnsResolverConfig, err := anypb.New(&cares.CaresDnsResolverConfig{\r\n\t\tDnsResolverOptions: &configCoreV3.DnsResolverOptions{\r\n\t\t\tUseTcpForDnsLookups: true,\r\n\t\t},\r\n\t})\r\n\tif err != nil {\r\n\t\tlog.Println(\"error converting to Any: %v\\n\", err)\r\n\t}\r\n\tdnsResolverConfig := &configCoreV3.TypedExtensionConfig{\r\n\t\tName:        \"envoy.network.dns_resolver.cares\",\r\n\t\tTypedConfig: caresDnsResolverConfig,\r\n\t}\r\n\r\n\treturn &clusterV3.Cluster{\r\n\t\tName:                   clusterName,\r\n\t\tConnectTimeout:         durationpb.New(time.Duration(connectTimeout) * time.Second),\r\n\t\tClusterDiscoveryType:   &clusterV3.Cluster_Type{Type: clusterType},\r\n\t\tLoadAssignment:         BuildEndpoint(clusterName, host, port),\r\n\t\tDnsLookupFamily:        clusterV3.Cluster_ALL,\r\n\t\tCircuitBreakers:        circuitBreakers,\r\n\t\tTypedDnsResolverConfig: dnsResolverConfig,\r\n\t}\r\n}\r\n```\r\n\r\nWe are having issues to access a system that has both valid IPv6 and IPv4 addresses like www.google.com. We are trying to access it from a proxy that relies on Envoy and when source IP is V4 we are getting \r\n`upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: immediate connect error: Network is unreachable|remote address:[2404:6800:4004:818::2004]:443`\r\nand it it did not pick the IPv4 address. We have tried adding \"UseTcpForDnsLookups\" with no success. Could you help on that, how can we fine-tune happy eyeballs to pick IPv4 when source is IPv4 and to pick IPv6 when source is IPv6 in order to work for such dual-stack target systems? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37478/comments",
    "author": "VladislavAtanasov95",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2024-12-03T17:15:06Z",
        "body": "cc @RyanTheOptimist"
      },
      {
        "user": "RyanTheOptimist",
        "created_at": "2024-12-05T02:43:01Z",
        "body": "Hm. I'm surprised. I would have expected that with `DnsLookupFamily` set to `ALL` that DNS would have returned both IPv4 and IPv6 addresses. Then the Happy Eyeballs code would make connection attempts for each address in the list until the a connection succeeded. Is that not what you're seeing? Do you have a log? "
      },
      {
        "user": "VladislavAtanasov95",
        "created_at": "2024-12-05T10:58:25Z",
        "body": "We sometimes receive\r\n\r\n< HTTP/1.1 503 Service Unavailable\r\n< content-length: 231\r\n< content-type: text/plain\r\n< date: Thu, 21 Nov 2024 13:43:13 GMT\r\n< upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: immediate connect error: Network is unreachable|remote address:[2404:6800:4004:818::2004]:443\r\n\r\nWhen the client service only has IPv4 and we are trying to reach google.com through our envoy-based proxy, sometimes it picks ipv6, sometime ipv4 of the target url."
      },
      {
        "user": "VladislavAtanasov95",
        "created_at": "2024-12-05T13:16:53Z",
        "body": "The Cluster type is actually \"STRICT_DNS\", not \"LOGICAL_DNS\"(my mistake) when we sporadically receive the error. I ran an example with static envoy locally \r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 2000\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager \r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          codec_type: auto\r\n          stat_prefix: http\r\n          route_config:\r\n            name: search_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/abv\"\r\n                route:\r\n                  cluster: abv\r\n                  host_rewrite_literal: www.abv.bg\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: google\r\n                  host_rewrite_literal: www.google.com\r\n          http_filters:\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n  clusters:\r\n  - name: google\r\n    connect_timeout: 10s\r\n    type: strict_dns\r\n    dns_lookup_family: ALL\r\n    load_assignment:\r\n      cluster_name: google\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.google.com\r\n                port_value: 443\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n        sni: www.google.com\r\n  - name: abv\r\n    connect_timeout: 10s\r\n    type: logical_dns\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: google\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.abv.bg\r\n                port_value: 443\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n        sni: www.abv.bg\r\nadmin:\r\n  access_log_path: \"/dev/stdout\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 15000\r\n```\r\n\r\nWhen I change it from STRICT_DNS to LOGICAL_DNS, requests to google are working stable every time, but when it is STRICT_DNS sometimes I receive \r\n`upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: delayed connect error: 61`\r\n Does that make sense?\r\n"
      },
      {
        "user": "akhilsingh-git",
        "created_at": "2024-12-18T14:08:19Z",
        "body": "When Envoy is configured with dns_lookup_family: ALL and receives both IPv4 and IPv6 addresses for a host, it uses a “happy eyeballs” approach to try establishing a connection. This approach attempts to connect to IPv6 first, and if that doesn’t succeed within a certain timeframe, it will try IPv4. However, depending on network conditions, default settings might lead to occasional failures if IPv6 is not truly reachable or if there’s an immediate IPv6 route issue.\r\n\r\nKey Points to Consider:\r\n\t1.\tHappy Eyeballs Mechanism:\r\nBy default, Envoy implements a Happy Eyeballs algorithm (RFC 6555-like behavior) when dns_lookup_family: ALL is set. Envoy will attempt a connection to an IPv6 address first and then, after a delay (happy_eyeballs_connection_delay), attempt IPv4 if IPv6 hasn’t connected. If IPv6 is immediately unreachable, you might see intermittent failures before Envoy attempts IPv4.\r\n\t2.\tStrict DNS vs. Logical DNS:\r\n\t•\tSTRICT_DNS: Envoy continuously re-resolves DNS at runtime and uses all returned IPs (both v4 and v6) as load-balancing endpoints. This can lead to Envoy attempting IPv6 endpoints that aren’t actually reachable, causing occasional errors.\r\n\t•\tLOGICAL_DNS: Envoy resolves DNS once at startup and treats the resulting IP addresses as a logical group, typically sticking to a single address family more consistently. This often appears more stable for dual-stack hosts because Envoy is less aggressive in rotating through all endpoints, but it may not provide the same dynamic behavior as STRICT_DNS.\r\n\t3.\tFine-Tuning Happy Eyeballs Behavior:\r\nEnvoy’s cluster configuration allows you to adjust the Happy Eyeballs timing. By default, the IPv4 connection attempt is delayed by a certain amount of time if IPv6 is available. If IPv6 repeatedly fails immediately, reducing this delay can help Envoy fall back to IPv4 faster, preventing those intermittent “network unreachable” errors.\r\nIn the cluster configuration for your STRICT_DNS cluster, try setting:\r\n\r\nhappy_eyeballs_connection_delay: 50ms\r\n\r\nor another suitably small value. The default is 300ms, so lowering it can speed up IPv4 fallback.\r\nExample:\r\n\r\nclusters:\r\n- name: google\r\n  connect_timeout: 10s\r\n  type: STRICT_DNS\r\n  dns_lookup_family: ALL\r\n  happy_eyeballs_connection_delay: 50ms\r\n  load_assignment:\r\n    cluster_name: google\r\n    endpoints:\r\n      - lb_endpoints:\r\n          - endpoint:\r\n              address:\r\n                socket_address:\r\n                  address: www.google.com\r\n                  port_value: 443\r\n  transport_socket:\r\n    name: envoy.transport_sockets.tls\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n      sni: www.google.com\r\n\r\nWith this change, if IPv6 is unreachable, Envoy should fall back to IPv4 more quickly, reducing the frequency of those “network unreachable” errors.\r\n\r\n\t4.\tIf You Don’t Need IPv6:\r\nIf IPv6 connectivity is not required or not guaranteed from your environment, consider simplifying by using:\r\n\r\ndns_lookup_family: V4_ONLY\r\n\r\nThis will ensure Envoy only attempts IPv4 addresses, eliminating IPv6-related connect errors altogether.\r\n\r\n\t5.\tEnvironment Considerations:\r\nIf your source network is IPv4-only, and the destination returns both IPv6 and IPv4 addresses, Envoy will try IPv6 first (due to Happy Eyeballs). Ensuring proper IPv6 routing, or just disabling it if not needed, is often the simplest solution. If dual-stack support is truly required, fine-tuning happy_eyeballs_connection_delay is your best bet.\r\n\r\nSummary:\r\n\t•\tUse dns_lookup_family: ALL for dual-stack, but tune happy_eyeballs_connection_delay to shorten the fallback time to IPv4.\r\n\t•\tConsider switching from STRICT_DNS to LOGICAL_DNS if that yields more stable behavior in your environment.\r\n\t•\tIf IPv6 isn’t actually needed, use V4_ONLY to avoid complexity.\r\n\t•\tAdjusting the cluster configuration, particularly happy_eyeballs_connection_delay, should help Envoy more reliably fall back to IPv4 when IPv6 fails, reducing the intermittent “network unreachable” errors."
      },
      {
        "user": "VladislavAtanasov95",
        "created_at": "2024-12-18T14:33:53Z",
        "body": "Thank you for the detailed explanation, appreciate it, closing"
      }
    ]
  },
  {
    "number": 37476,
    "title": "Build Envoy with Golang Filter from Contribution Package",
    "created_at": "2024-12-03T06:49:52Z",
    "closed_at": "2024-12-04T17:09:37Z",
    "labels": [
      "question",
      "area/build"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37476",
    "body": "I'm trying to build Envoy with the Golang filter included in the binary and I'd like to avoid including any other additional extension filter.\r\n\r\nI attempted using the following command:\r\n\r\n`./ci/run_envoy_docker.sh 'BAZEL_BUILD_EXTRA_OPTIONS=\"--//contrib/golang/filters/http/source:enabled=true\" ./ci/do_ci.sh release.server_only'`\r\n\r\nHowever, this approach wasn't successful.\r\n\r\nMy questions are:\r\n\r\n- Is there a recommended way to build Envoy with just the Golang filter from the contribution package using the Bazel build system?\r\n- Is there a way to achieve this using a Docker build mechanism? If so, could you please provide some guidance or examples?\r\n\r\nAny insights or suggestions would be greatly appreciated.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37476/comments",
    "author": "aabfalterer",
    "comments": [
      {
        "user": "aabfalterer",
        "created_at": "2024-12-04T09:26:24Z",
        "body": "I would like to kindly ask the maintainers of the golang filter extension, @doujiang24, @wangfakang, @StarryVae, @spacewander, and @antJack, for any input"
      },
      {
        "user": "phlax",
        "created_at": "2024-12-04T10:21:27Z",
        "body": "@aabfalterer assuming you want to build the normal build target i think you would need to edit `source/extensions/extensions_build_config.bzl` \r\n\r\notherwise - you can build the contrib target and disable any you dont want to include"
      },
      {
        "user": "aabfalterer",
        "created_at": "2024-12-04T17:08:47Z",
        "body": "@phlax Many thanks for the hint. I was able to enable the golang filter with the following change (based on v1.32.0)\r\n\r\n```\r\ndiff --git a/source/extensions/extensions_build_config.bzl b/source/extensions/extensions_build_config.bzl\r\nindex 104ca2e63d..f0ef8e1c23 100644\r\n--- a/source/extensions/extensions_build_config.bzl\r\n+++ b/source/extensions/extensions_build_config.bzl\r\n@@ -542,7 +542,7 @@ EXTENSIONS = {\r\n # need to directly reference Envoy extensions.\r\n EXTENSION_CONFIG_VISIBILITY = [\"//:extension_config\", \"//:contrib_library\", \"//:mobile_library\"]\r\n EXTENSION_PACKAGE_VISIBILITY = [\"//:extension_library\", \"//:contrib_library\", \"//:mobile_library\"]\r\n-CONTRIB_EXTENSION_PACKAGE_VISIBILITY = [\"//:contrib_library\"]\r\n+CONTRIB_EXTENSION_PACKAGE_VISIBILITY = [\"//visibility:public\"]\r\n MOBILE_PACKAGE_VISIBILITY = [\"//:mobile_library\"]\r\n\r\n # Set this variable to true to disable alwayslink for envoy_cc_library.\r\n````\r\n\r\n````\r\ndiff --git a/source/extensions/extensions_build_config.bzl b/source/extensions/extensions_build_config.bzl\r\nindex 7d3cdbad71..104ca2e63d 100644\r\n--- a/source/extensions/extensions_build_config.bzl\r\n+++ b/source/extensions/extensions_build_config.bzl\r\n@@ -187,6 +187,7 @@ EXTENSIONS = {\r\n     \"envoy.filters.http.wasm\":                          \"//source/extensions/filters/http/wasm:config\",\r\n     \"envoy.filters.http.stateful_session\":              \"//source/extensions/filters/http/stateful_session:config\",\r\n     \"envoy.filters.http.header_mutation\":               \"//source/extensions/filters/http/header_mutation:config\",\r\n+    \"envoy.filters.http.golang\":                        \"//contrib/golang/filters/http/source:config\",\r\n\r\n     #\r\n     # Listener filters\r\n````"
      }
    ]
  },
  {
    "number": 37442,
    "title": "Method to increase incorming request size ",
    "created_at": "2024-12-02T05:09:47Z",
    "closed_at": "2025-01-15T20:01:14Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37442",
    "body": "\r\nMethod to increase incorming HTTP request size \r\n\r\n\r\n*Description*:\r\n\r\nI am getting the below error when the incoming request size is greater than 300 MB. The istio proxy sidecar container throws the below error and doesn't allow the request. How to increase the threshold value here.\r\n\r\nERROR:\r\n\r\n503 UC upstream_reset_before_response_started{connection_termination}\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37442/comments",
    "author": "csakthivel3773",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2024-12-02T19:28:58Z",
        "body": "That error indicates that envoy was proxying the request to an upstream, and the upstream sent back an error. Envoy doesn't limit body size, unless a buffering (or similar) filter is configured."
      },
      {
        "user": "csakthivel3773",
        "created_at": "2024-12-03T07:39:08Z",
        "body": "No filter is configured to limit request body size. But the issue was observed when request size was high and issue reproduced exactly everytime after 2 seconds (from istio proxy sidecar logs). The upstream server is a nodejs application which has its default keepalivetimeout as 5 seconds. No other timeouts have been configured from envoy/istio side. Any way to find and fix this issue ?"
      },
      {
        "user": "wbpcode",
        "created_at": "2024-12-09T16:00:41Z",
        "body": "The log actually is pretty clear. May be you need to check the server why close the connection."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-01-08T16:01:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-01-15T20:01:14Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 37401,
    "title": "Envoy as Redis proxy",
    "created_at": "2024-11-28T07:32:09Z",
    "closed_at": "2025-01-09T20:01:20Z",
    "labels": [
      "question",
      "stale",
      "area/redis"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37401",
    "body": "Hello Team,\r\nWe have been recently think of adopting envoy as a Redis proxy, yet according to this issue, it looks like there is a performance problems with it.\r\n\r\nIs Envoy a good candidate for using as Redis proxy with very high traffic applications? If so, what should be the considerations that should  be taken?\r\n\r\nThanks\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37401/comments",
    "author": "h4ckroot",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2024-12-02T19:23:32Z",
        "body": "Questions like this may receive more response from either `envoy-users` on slack, or the envoy-users mailing list."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2024-12-03T15:02:52Z",
        "body": "> yet according to this issue, it looks like there is a performance problems with it.\r\n\r\nWhich issue are you referring to here?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-01-02T16:01:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-01-09T20:01:19Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 37340,
    "title": "Is it possible to customize HTTP requests on low-priority endpoints",
    "created_at": "2024-11-25T04:12:40Z",
    "closed_at": "2025-01-04T00:03:24Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37340",
    "body": "I've a cluster, which has two priorities: 0 and 1. If priority 0 is unhealthy, the request would be routed to the priority 1.\r\n\r\nMy question is if it's possible to customize HTTP requests on low-priority endpoints, such as adding a header or rewrite path while routing to the low-priority endpoints?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37340/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "RyanTheOptimist",
        "created_at": "2024-11-26T16:09:29Z",
        "body": "@alyssawilk @zuercher @yanavlasov "
      },
      {
        "user": "zuercher",
        "created_at": "2024-11-27T18:03:40Z",
        "body": "The choice of endpoint happens per request at the point where the request is forwarded. Nothing in the filter chain has any knowledge of the cluster priorities or cluster health. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-27T20:01:22Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-01-04T00:03:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 37266,
    "title": "Build results in Protobuf errors",
    "created_at": "2024-11-20T17:45:14Z",
    "closed_at": "2025-01-30T20:21:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37266",
    "body": "*Title*: Building envoy results in protobuf compile errors\r\n\r\n*Description*:\r\nWhen attempting to build envoy, the build fails with the below errors related to protobuf.\r\n`bazel build --subcommands --cpu=ppc --sandbox_debug --action_env=CC=clang-14 --action_env=CXX=clang++-14 --linkopt=-fuse-ld=lld-14 --verbose_failures --define=boringssl=fips //source/exe:envoy-static`\r\n```\r\nIn file included from external/com_google_protobuf/src/google/protobuf/util/field_mask_util.cc:8:\r\nIn file included from bazel-out/ppc-opt-exec-ST-d57f47055a04/bin/external/com_google_protobuf/src/google/protobuf/util/_virtual_includes/field_mask_util/google/protobuf/util/field_mask_util.\r\nh:17:\r\nbazel-out/ppc-opt-exec-ST-d57f47055a04/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/field_mask_proto/google/protobuf/field_mask.pb.h:166:3: e\r\nrror: unknown type name 'PROTOBUF_ATTRIBUTE_REINITIALIZES'\r\n  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;\r\n  ^\r\nbazel-out/ppc-opt-exec-ST-d57f47055a04/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/field_mask_proto/google/protobuf/field_mask.pb.h:173:29:\r\nerror: only virtual member functions can be marked 'final'\r\n  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }\r\n                            ^~~~~~\r\nbazel-out/ppc-opt-exec-ST-d57f47055a04/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/field_mask_proto/google/protobuf/field_mask.pb.h:178:38:\r\nerror: only virtual member functions can be marked 'final'\r\n  void SetCachedSize(int size) const final;\r\n                                     ^~~~~\r\nbazel-out/ppc-opt-exec-ST-d57f47055a04/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/field_mask_proto/google/protobuf/field_mask.pb.h:92:9: \r\nerror: use of undeclared identifier 'GetOwningArena'\r\n    if (GetOwningArena() == from.GetOwningArena()\r\n        ^\r\nbazel-out/ppc-opt-exec-ST-d57f47055a04/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/field_mask_proto/google/protobuf/field_mask.pb.h:92:34: \r\nerror: no member named 'GetOwningArena' in 'google::protobuf::FieldMask'\r\n    if (GetOwningArena() == from.GetOwningArena()\r\n                            ~~~~ ^\r\nbazel-out/ppc-opt-exec-ST-d57f47055a04/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/field_mask_proto/google/protobuf/field_mask.pb.h:139:9: \r\nerror: use of undeclared identifier 'GetOwningArena'\r\n    if (GetOwningArena() == other->GetOwningArena()) {\r\n        ^\r\nbazel-out/ppc-opt-exec-ST-d57f47055a04/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/field_mask_proto/google/protobuf/field_mask.pb.h:139:36: \r\nerror: no member named 'GetOwningArena' in 'google::protobuf::FieldMask'\r\n    if (GetOwningArena() == other->GetOwningArena()) {\r\n                            ~~~~~  ^\r\nbazel-out/ppc-opt-exec-ST-d57f47055a04/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/field_mask_proto/google/protobuf/field_mask.pb.h:148:17: \r\nerror: use of undeclared identifier 'GetOwningArena'\r\n    ABSL_DCHECK(GetOwningArena() == other->GetOwningArena());\r\n                ^\r\nbazel-out/ppc-opt-exec-ST-d57f47055a04/bin/external/com_google_protobuf/src/google/protobuf/_virtual_includes/field_mask_proto/google/protobuf/field_mask.pb.h:148:44: \r\nerror: no member named 'GetOwningArena' in 'google::protobuf::FieldMask'\r\n    ABSL_DCHECK(GetOwningArena() == other->GetOwningArena());\r\n                                    ~~~~~  ^\r\n```\r\n\r\nAll of the errors appear to be in a file (field_mask.pb.h) that is generated by the protobuf build process:\r\n```\r\n// Generated by the protocol buffer compiler.  DO NOT EDIT!\r\n// source: google/protobuf/field_mask.proto\r\n```\r\n\r\nUsing bazel query, I've found the dependency path to the failing protobuf target:\r\n```\r\n//source/exe:envoy-static\r\n//source/exe:envoy_main_entry_lib\r\n//source/server:options_lib\r\n@envoy_api//envoy/admin/v3:pkg_cc_proto\r\n@com_google_protobuf//:protobuf\r\n@com_google_protobuf//src/google/protobuf:protobuf_layering_check_legacy\r\n@com_google_protobuf//src/google/protobuf/util:field_mask_util\r\n```\r\n\r\nEnvoy version (from VERSION.txt): 1.32.0-dev\r\n\r\nAny information would be helpful.\r\nThank you\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37266/comments",
    "author": "Jenkins-J",
    "comments": [
      {
        "user": "soulxu",
        "created_at": "2024-11-21T23:35:13Z",
        "body": "cc @phlax "
      },
      {
        "user": "Jenkins-J",
        "created_at": "2024-12-09T20:08:27Z",
        "body": "To ensure the Protobuf repository was not the cause of the issue, I cloned the Protobuf repository separately and was able to build the target successfully. (I built Protobuf version 26.1, the same version that Envoy was building when the failures above occurred)."
      },
      {
        "user": "phlax",
        "created_at": "2024-12-09T20:13:41Z",
        "body": "apologies i missed this - not sure exactly what the issue is\r\n\r\nif protobuf can build independently - _might_ be worth checking the `bazel/protobuf.patch` to see if anything there is at issue"
      },
      {
        "user": "phlax",
        "created_at": "2024-12-09T20:15:20Z",
        "body": "also - i guess you might want to check what flags are being set when you build in envoy/independently"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-01-09T00:03:43Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "Jenkins-J",
        "created_at": "2025-01-09T17:16:28Z",
        "body": "While debugging, I noticed that in the `external` directory of the bazel cache there are two directories, `com_google_protobuf` and `protobuf~`. They appear to contain the same files, but the files differ from each other in slight ways (for example `PROTOBUF_ATTRIBUTE_REINITIALIZES` is defined in a file under `protobuf~` but is not defined in the same file under `com_google_protobuf`).\r\n\r\nAre both of these directories expected to exist in the bazel cache?"
      },
      {
        "user": "Jenkins-J",
        "created_at": "2025-01-14T20:21:49Z",
        "body": "@phlax I found in my bazel cache that there are two different protoc binaries created:\n\n```\n./execroot/_main/bazel-out/ppc-opt-exec-ST-d57f47055a04/bin/external/protobuf~/protoc\n./external/com_google_protobuf_protoc_linux_ppcle_64/bin/protoc\n```\n\nWhen running the binaries with the `--version` flag I get two different versions, but the binary under the `com_google_protobuf_protoc_linux_ppcle_64` directory is the correct version (26.1).\n\nI believe that somehow the other protoc binary is being used to compile protobuf files during the build.\nI know how the binary under the `com_google_protobuf_protoc_linux_ppcle_64` directory is created (generated by the `_compiled_protoc_deps function`), but I do not know where the other binary is coming from.\n\nDo you happen to know how the other binary is getting created or how to ensure that the proper protoc binary is used to compile protobuf files during the build?"
      },
      {
        "user": "Jenkins-J",
        "created_at": "2025-01-30T20:21:12Z",
        "body": "Issue resolved. I was using Bazel modules in addition to the WORKSPACE file to load dependencies. Once I saw that the `common --noenable_bzlmod` option is supposed to be set in the .bazelrc, I disabled bazel modules and the conflict no longer occurs. Using bzlmod in addition to loading dependencies via the WORKSPACE file must have caused multiple versions of the protobuf dependency to be loaded. "
      }
    ]
  },
  {
    "number": 37242,
    "title": "Reason for not having a DownstreamHttp11ConnectSocket",
    "created_at": "2024-11-19T10:35:50Z",
    "closed_at": "2024-12-27T20:01:20Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37242",
    "body": "*Title*: *Reason for not having a DownstreamHttp11ConnectSocket*\r\n\r\n*Description*:\r\n> Is the reason why there is an UpstreamHttp11ConnectSocket but no DownstreamHttp11ConnectSocket because there are no other filters aside from the http filter which requires the information parsed out of the HTTP CONNECT method? If I were to implement a DownstreamHttp11ConnectSocket that would parse out such information and set them under a struct in the Connection object, would this feature be useful for Envoy?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37242/comments",
    "author": "FH-30",
    "comments": [
      {
        "user": "soulxu",
        "created_at": "2024-11-20T10:25:20Z",
        "body": "cc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2024-11-20T14:29:23Z",
        "body": "The reason the one exists and not the other is that someone wrote it and there was a maintainer sponsor.\r\nAll Envoy features are driven by user demand and the existing documented-somewhere extension policy :-)"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-20T16:01:27Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-27T20:01:20Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 37173,
    "title": "Error with Dynamic Configuration for DNS Filter in Listener Filters",
    "created_at": "2024-11-15T16:12:24Z",
    "closed_at": "2024-12-23T00:04:05Z",
    "labels": [
      "question",
      "stale",
      "area/udp",
      "area/ecds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37173",
    "body": "Hello Envoy team,\r\n\r\nI’m encountering the following error when trying to define a DNS filter under listener_filters using config_discovery:\r\n\r\n```\r\n[2024-11-15 19:00:28.143][119127102][warning][config] [./source/extensions/config_subscription/grpc/xds_mux/subscription_state.h:116] Config for type.googleapis.com/envoy.config.listener.v3.Listener rejected: Error adding/updating listener(s) dnstestPVZ2Wb: UDP listener filter: dnstestPVZ2Wb-lisfilPSRw97 is configured with unsupported dynamic configuration\r\n```\r\n\r\nIt appears that the DNS filter in the listener_filters section is being rejected when configured with dynamic discovery.\r\n\r\nIs this a known limitation of Envoy, or is there a workaround to support DNS filter configuration dynamically under listener_filters?\r\n\r\nThank you in advance for your help!\r\n\r\nEnvoy version: 1.32.1\r\nads, delta-grpc",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37173/comments",
    "author": "sefaphlvn",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2024-11-15T16:59:25Z",
        "body": "ECDS is not supported by UDP listeners filters at this point."
      },
      {
        "user": "sefaphlvn",
        "created_at": "2024-11-15T17:21:06Z",
        "body": "Thank you for the response.\r\n\r\nCould you please let me know if there are any plans to support ECDS for UDP listener filters in the future? If so, is there a tentative timeline or roadmap for this feature?\r\n\r\nHaving ECDS support for UDP listener filters would be very useful for our use case, and it would help us plan our implementation accordingly.\r\n\r\nThank you again for your assistance!"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-15T20:01:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-23T00:04:05Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 37124,
    "title": "Golang custom http filter failed with Unable to parse JSON as proto (INVALID_ARGUMENT: could not find @type 'type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config')",
    "created_at": "2024-11-13T15:47:18Z",
    "closed_at": "2024-11-14T11:53:54Z",
    "labels": [
      "question",
      "area/go"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37124",
    "body": "*Title*: *Testing the go custom http filter as provided in the envoy examples and looks like go filter  is not enabled ????*\r\n\r\nI am testing on the arm64 env.\r\n\r\n*Description*:\r\n>when starting the envoy with below configuration looks like the go http filter is not enabled by default or am I missing something\r\n\r\nFrom logs I can see only the below filters but go is not present not sure why\r\n```\r\nenvoy.filters.http: envoy.bandwidth_limit, envoy.buffer, envoy.cors, envoy.csrf, envoy.ext_authz, envoy.ext_proc, envoy.fault, envoy.filters.http.adaptive_concurrency, envoy.filters.http.admission_control, envoy.filters.http.alternate_protocols_cache, envoy.filters.http.aws_lambda, envoy.filters.http.aws_request_signing, envoy.filters.http.bandwidth_limit, envoy.filters.http.basic_auth, envoy.filters.http.buffer, envoy.filters.http.cache, envoy.filters.http.cdn_loop, envoy.filters.http.composite, envoy.filters.http.compressor, envoy.filters.http.connect_grpc_bridge, envoy.filters.http.cors, envoy.filters.http.credential_injector, envoy.filters.http.csrf, envoy.filters.http.custom_response, envoy.filters.http.decompressor, envoy.filters.http.dynamic_forward_proxy, envoy.filters.http.ext_authz, envoy.filters.http.ext_proc, envoy.filters.http.fault, envoy.filters.http.file_system_buffer, envoy.filters.http.gcp_authn, envoy.filters.http.geoip, envoy.filters.http.grpc_field_extraction, envoy.filters.http.grpc_http1_bridge, envoy.filters.http.grpc_http1_reverse_bridge, envoy.filters.http.grpc_json_transcoder, envoy.filters.http.grpc_stats, envoy.filters.http.grpc_web, envoy.filters.http.header_mutation, envoy.filters.http.header_to_metadata, envoy.filters.http.health_check, envoy.filters.http.ip_tagging, envoy.filters.http.json_to_metadata, envoy.filters.http.jwt_authn, envoy.filters.http.local_ratelimit, envoy.filters.http.lua, envoy.filters.http.match_delegate, envoy.filters.http.oauth2, envoy.filters.http.on_demand, envoy.filters.http.original_src, envoy.filters.http.rate_limit_quota, envoy.filters.http.ratelimit, envoy.filters.http.rbac, envoy.filters.http.router, envoy.filters.http.set_filter_state, envoy.filters.http.set_metadata, envoy.filters.http.stateful_session, envoy.filters.http.tap, envoy.filters.http.thrift_to_metadata, envoy.filters.http.wasm, envoy.geoip, envoy.grpc_http1_bridge, envoy.grpc_json_transcoder, envoy.grpc_web, envoy.health_check, envoy.ip_tagging, envoy.local_rate_limit, envoy.lua, envoy.rate_limit, envoy.router\r\n```\r\n```\r\nenvoy  version: 7b8baff1758f0a584dcc3cb657b5032000bcb3d7/1.31.0/Distribution/RELEASE/BoringSSL\r\n```\r\n```\r\n envoy -c ~/Downloads/golang.yaml --mode validate  --enable-fine-grain-logging -l trace\r\n```\r\nThe server is throwing the following error.\r\n\r\n```\r\n\r\n[2024-11-13 21:07:49.577][3560563][critical][main] [source/server/config_validation/server.cc:71] error initializing configuration '/Users/satya.kotipalli/Downloads/golang.yaml': Unable to parse JSON as proto (INVALID_ARGUMENT: could not find @type 'type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config'): {\"static_resources\":{\"listeners\":[{\"filter_chains\":[{\"filters\":[{\"typed_config\":{\"route_config\":{\"name\":\"route_0\",\"virtual_hosts\":[{\"name\":\"service_0\",\"domains\":[\"*\"],\"routes\":[{\"match\":{\"prefix\":\"/\"},\"route\":{\"cluster\":\"cluster_0\"}}]}]},\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"http_filters\":[{\"name\":\"envoy.filters.http.golang\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\",\"plugin_name\":\"my_plugin\",\"library_path\":\"lib/my_plugin.so\",\"library_id\":\"my-plugin-id\"}},{\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.header_to_metadata.v3.Config\"},\"name\":\"envoy.filters.http.header_to_metadata\"},{\"typed_config\":{\"library_id\":\"my-other-plugin-id\",\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\",\"library_path\":\"lib/my_other_plugin.so\",\"plugin_name\":\"my_other_plugin\"},\"name\":\"envoy.filters.http.golang\"},{\"name\":\"envoy.filters.http.router\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"}}],\"codec_type\":\"AUTO\",\"stat_prefix\":\"ingress_http\"},\"name\":\"envoy.filters.network.http_connection_manager\"}]}],\"address\":{\"socket_address\":{\"address\":\"0.0.0.0\",\"port_value\":8080,\"protocol\":\"TCP\"}},\"name\":\"listener_0\"}],\"clusters\":[{\"type\":\"LOGICAL_DNS\",\"name\":\"cluster_0\",\"dns_lookup_family\":\"V4_ONLY\",\"lb_policy\":\"ROUND_ROBIN\",\"load_assignment\":{\"endpoints\":[{\"lb_endpoints\":[{\"endpoint\":{\"address\":{\"socket_address\":{\"port_value\":80,\"address\":\"service.local\"}}}}]}\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 8080\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          codec_type: AUTO\r\n          stat_prefix: ingress_http\r\n          http_filters:\r\n          - name: envoy.filters.http.golang\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\r\n              library_id: my-plugin-id\r\n              library_path: \"lib/my_plugin.so\"\r\n              plugin_name: my_plugin\r\n          - name: envoy.filters.http.header_to_metadata\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.header_to_metadata.v3.Config\r\n          - name: envoy.filters.http.golang\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\r\n              library_id: my-other-plugin-id\r\n              library_path: \"lib/my_other_plugin.so\"\r\n              plugin_name: my_other_plugin\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n          route_config:\r\n            name: route_0\r\n            virtual_hosts:\r\n            - name: service_0\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: cluster_0\r\n\r\n  clusters:\r\n  - name: cluster_0\r\n    type: LOGICAL_DNS\r\n    lb_policy: ROUND_ROBIN\r\n    dns_lookup_family: V4_ONLY\r\n    load_assignment:\r\n      cluster_name: service_cluster\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: service.local\r\n                port_value: 80\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37124/comments",
    "author": "satya256",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2024-11-13T20:45:29Z",
        "body": "This filter requires build with contrib extensions. "
      },
      {
        "user": "yanavlasov",
        "created_at": "2024-11-13T20:46:42Z",
        "body": "Adding filter maintainers: @doujiang24 @wangfakang @StarryVae @spacewander @antJack"
      },
      {
        "user": "spacewander",
        "created_at": "2024-11-14T02:26:50Z",
        "body": "> This filter requires build with contrib extensions.\r\n\r\nYes. That's the reason."
      },
      {
        "user": "satya256",
        "created_at": "2024-11-14T03:34:44Z",
        "body": "Thanks for the update.\r\n\r\nCan you please provide the documentation on how to build the envoy with contrib extensions?"
      },
      {
        "user": "satya256",
        "created_at": "2024-11-14T11:53:54Z",
        "body": "For now instead of building from source, I have used the contrib image."
      }
    ]
  },
  {
    "number": 37123,
    "title": "There is Any Feature for Log Extraction for error.log",
    "created_at": "2024-11-13T08:28:37Z",
    "closed_at": "2024-12-21T04:01:19Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37123",
    "body": "Helo friend,\r\n\r\nI want to ask about the extraction for error.log in envoy. Is there any possibility to extract the error.log from the envoy?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37123/comments",
    "author": "masabed",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2024-11-13T20:47:25Z",
        "body": "What is error.log?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-14T00:04:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-21T04:01:19Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 37095,
    "title": "UPSTREAM_HOST_NAME for DFP cluster returns the port with the hostname while other clusters return the hostname only",
    "created_at": "2024-11-11T14:55:18Z",
    "closed_at": "2024-11-15T03:23:28Z",
    "labels": [
      "question",
      "area/http_dynamic_forward_proxy"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37095",
    "body": "*Title*: *UPSTREAM_HOST_NAME for DFP cluster returns the port with the hostname, while other clusters return hostname only*\r\n\r\n*Description*:\r\nWe're using Dynamic Forward Proxy cluster in our setup in addition to Strict DNS clusters and we need to have the upstream host name (DNS name) in our access log.\r\nWe had tried to use UPSTREAM_HOST_NAME and we saw inconsistency in its returned value:\r\n- For Strict DNS cluster: We're getting the DNS name only as expected.\r\n- For Dynamic Forward Proxy cluster: We're getting the DNS name with the port value.\r\n\r\nIs this the expected behavior? Do we have a way to get DNS name in all clusters without the port?\r\ncc. @wbpcode",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37095/comments",
    "author": "IssaAbuKalbein",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2024-11-11T14:58:46Z",
        "body": "This is by design. DFP cluster returns host name with port because it needs to create different connection pools for requests that go to different destination ports."
      },
      {
        "user": "IssaAbuKalbein",
        "created_at": "2024-11-11T15:12:18Z",
        "body": "Thanks @yanavlasov for the quick response.\r\nDo we have a way to get DNS name only without the port in all clusters? or at least to have the hostname with the port in all for consistency in the logs?"
      },
      {
        "user": "yanavlasov",
        "created_at": "2024-11-12T19:47:18Z",
        "body": "I do not think there is a way right now. You can create and enhancement request to add UPSTREAM_HOST_NAME_WITHOUT_PORT variable."
      },
      {
        "user": "IssaAbuKalbein",
        "created_at": "2024-11-12T22:17:13Z",
        "body": "Thanks @yanavlasov \r\nI've created this PR for adding UPSTREAM_HOST_NAME_WITHOUT_PORT variable: #37114 \r\nAppreciate if you could take a look."
      }
    ]
  },
  {
    "number": 36963,
    "title": "Is it possible to use dynamic metadata to select directly endpoint of a cluster",
    "created_at": "2024-11-04T11:58:05Z",
    "closed_at": "2025-01-12T12:01:30Z",
    "labels": [
      "question",
      "stale",
      "area/load balancing"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36963",
    "body": "I know that we can select endpoints by metadata. For example,\r\n\r\n```\r\n  load_assignment:\r\n    cluster_name: cluster_gateway_https\r\n    endpoints:\r\n    - lb_endpoints:\r\n      - endpoint:\r\n          address:\r\n            socket_address: *gateway_224_https_addr\r\n        metadata:\r\n          filter_metadata:\r\n            envoy.lb:\r\n              canary: \"0\"\r\n      - endpoint:\r\n          address:\r\n            socket_address: *gateway_225_https_addr\r\n        metadata:\r\n          filter_metadata:\r\n            envoy.lb:\r\n              canary: \"1\"\r\n```\r\nNow I want to select endpoints by dynamic metadata which comes from JWT.\r\n```\r\n         - name: envoy.filters.http.jwt_authn\r\n           typed_config:\r\n             \"@type\": type.googleapis.com/envoy.extensions.filters.http.jwt_authn.v3.JwtAuthentication\r\n             providers:\r\n               sso_jwt_provider:\r\n                 local_jwks:\r\n                   filename: /conf/envoy/jwks.json\r\n                 from_headers:\r\n                 - name: YL-Authorization\r\n                   value_prefix: \"Bearer \"\r\n                 payload_in_metadata: jwt_payload\r\n```\r\nAs my understanding, the config of JWT writes its payload into the dynamic metadata in the `envoy.filters.http.jwt_authn`. Then I want to use the kvs in the `envoy.filters.http.jwt_authn` to select some endpoints in a cluster. It seems that I need to transmit the dynamic metadata into the metadata `envoy.lb`.but I don't know how.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36963/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2024-11-05T15:52:08Z",
        "body": "cc @wbpcode @zuercher "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-05T16:01:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "YvesZHI",
        "created_at": "2024-12-06T00:56:10Z",
        "body": " Still need help"
      },
      {
        "user": "wbpcode",
        "created_at": "2024-12-06T02:14:05Z",
        "body": "> It seems that I need to transmit the dynamic metadata into the metadata envoy.lb.but I don't know how.\r\n\r\nA simple lua script may helps for this requirement if you don't want to maintain an envoy fork."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-01-05T08:01:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2025-01-12T12:01:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 36958,
    "title": "would envoy close the downstream connection when removing one upstram backend server?",
    "created_at": "2024-11-04T02:32:22Z",
    "closed_at": "2024-11-11T07:13:28Z",
    "labels": [
      "question",
      "area/connection"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36958",
    "body": "*Description*:\r\n\r\n   | <--downstream-> | <----------upstream-------------> |\r\n\r\nChange config from:\r\n ................................./---------**weight 50**--------->  1.1.1.1 \r\nclient ----  -----  envoy \r\n ................................\\\\---------weight 50----------> 1.1.1.2\r\n                         to:\r\n................................../---------**weight 0**--------->  1.1.1.1\r\nclient -----------  envoy \r\n ................................ \\\\---------weight 100----------> 1.1.1.2\r\n\r\nAs shows,  at the beginning,  client has a long mysql connection to 1.1.1.1 via envoy. After change  weight of server 1.1.1.1,1 to 0, will envoy disconnect the link between client and envoy?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36958/comments",
    "author": "bigjordon",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2024-11-04T16:55:17Z",
        "body": "Can you provide some more concrete details?e.g.  what config is being used?"
      },
      {
        "user": "bigjordon",
        "created_at": "2024-11-06T08:10:40Z",
        "body": "I used a product and I don't know its configuration, but I know it uses Envoy. Is it possible to disconnect the link in front under certain configurations?"
      },
      {
        "user": "bigjordon",
        "created_at": "2024-11-11T07:13:51Z",
        "body": "it may depents"
      }
    ]
  },
  {
    "number": 36873,
    "title": "Which kind of status should be returned after sendLocalReply",
    "created_at": "2024-10-28T15:29:23Z",
    "closed_at": "2024-12-06T20:01:17Z",
    "labels": [
      "question",
      "stale",
      "area/http_filter"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36873",
    "body": "I'm developing my custom HTTP filter to do some validation for HTTP requests.\r\n```\r\nHttp::FilterHeadersStatus decodeHeaders(Http::HeaderMap& headers, bool) {\r\n    if (header_validation_failure()) {\r\n        callback->sendLocalReply();\r\n        return Http::FilterHeadersStatus::???;  // which status should be returned here??? \r\n    }\r\n    Http::FilterHeadersStatus::Continue;\r\n}\r\n\r\nHttp::FilterDataStatus decodeData(Buffer::Instance&, bool) {\r\n    if (data_validation_failure()) {\r\n        callback->sendLocalReply();\r\n        return Http::FilterDataStatus::???;  // which status should be returned here??? \r\n    }\r\n    Http::FilterDataStatus::Continue;\r\n}\r\n```\r\nAs you see, if the validations fail in `decodeHeader` or in `decodeData`, I want to send a local reply to client immediately, and the current request shouldn't be held by Envoy or be passed to remaining filter anymore. But I don't know which kind of status should be returned.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36873/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "nezdolik",
        "created_at": "2024-10-30T12:53:43Z",
        "body": "you can return `StopIteration` for both decodeHeaders/Data after sending local reply."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-29T16:01:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-06T20:01:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 36801,
    "title": "How to make the filter continue after StopAllIterationAndBuffer and addDecodedData",
    "created_at": "2024-10-24T08:49:40Z",
    "closed_at": "2024-12-04T16:01:33Z",
    "labels": [
      "question",
      "stale",
      "area/http_filter"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36801",
    "body": "I'm developing two custom HTTP filters. The first filter will send a HTTP request in its `decodeHeader()` to a server, return `FilterHeadersStatus::StopAllIterationAndBuffer`, wait for the response and set the body of current request. Then it passes request to the second filter. The second filter will `StopIteration` in `decodeHeader` and `continue` in `decodeData`.\r\n\r\nIn a word, what I'm trying to do is setting a body for the request in the first filter and doing some process in the second filter.\r\n\r\n```\r\n// first filter\r\ndecodeHeader()\r\n    sendReqAsync(); // send a http request to a server\r\n    return FilterHeadersStatus::StopAllIterationAndBuffer; // stop the whole process\r\n\r\nextractResAsync()\r\n    callback->addDecodedData(valueFromRes, true); // set the body\r\n    callback->continueDecoding(); // continue the process\r\n\r\n// second filter\r\ndecodeHeader()\r\n    return FilterHeadersStatus::StopIteration; // wait for the process in the decodeData\r\n\r\ndecodeData()\r\n    // do something with the body set by the first filter\r\n   return FilterDataStatus::continue; // continue the whole process\r\n```\r\n\r\nHowever, I find that the process above will stop in the second filter: It seems that the `decodeData` is not triggered and the whole process stops because of `return FilterHeadersStatus::StopIteration;` in the `decodeHeader`.\r\n\r\nDid I wrongly use the mechanism of Envoy?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36801/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "nezdolik",
        "created_at": "2024-10-28T15:59:50Z",
        "body": "It would be easier to troubleshoot if you paste complete code of the filter"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-27T16:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-04T16:01:33Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 36728,
    "title": "Why Are Requests Passing the Dynamic Listener Not Collected?",
    "created_at": "2024-10-21T13:19:04Z",
    "closed_at": "2024-10-23T03:32:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36728",
    "body": "*Title*: *Why Are Requests Passing the Dynamic Listener Not Collected?*\r\n\r\n*Description*:\r\nI pushed Cluster and Listener to Envoy(Latest version **1.32**) through istio and accessed backend services through Envoy many times. But I requested the /stats and /cluster interfaces and only got rq_xx=0\r\n\r\nHere is the result from /clusters, the result of 'rq_total' and 'rq_total' is correct!\r\n```\r\noutbound|8088||nginx-perf-8088.some-namespace.svc.cluster.local::20.0.1.148:80::rq_success::495\r\noutbound|8088||nginx-perf-8088.some-namespace.svc.cluster.local::20.0.1.148:80::rq_timeout::0\r\noutbound|8088||nginx-perf-8088.some-namespace.svc.cluster.local::20.0.1.148:80::rq_total::495\r\n```\r\nHere is the result from /stats, it doesn't have any request data about my cluster like /clusters.\r\n```\r\ncluster.xds-grpc.upstream_rq_200: 0\r\ncluster.xds-grpc.upstream_rq_2xx: 0\r\ncluster.xds-grpc.upstream_rq_total: 0\r\n```\r\n\r\nHere is some configuration exported from envoy /config_dump?include_eds\r\n\r\n[optional *Relevant Links*:]\r\n```yaml\r\n{\r\n \"configs\": [\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v3.ClustersConfigDump\",\r\n   \"dynamic_active_clusters\": [\r\n    {\r\n     \"cluster\": {\r\n      \"@type\": \"type.googleapis.com/envoy.config.cluster.v3.Cluster\",\r\n      \"name\": \"outbound|8088||nginx-perf-8088.some-namespace.svc.cluster.local\",\r\n      \"type\": \"EDS\",\r\n      \"eds_cluster_config\": {\r\n       \"eds_config\": {\r\n        \"ads\": {},\r\n        \"initial_fetch_timeout\": \"0s\",\r\n        \"resource_api_version\": \"V3\"\r\n       },\r\n       \"service_name\": \"outbound|8088||nginx-perf-8088.some-namespace.svc.cluster.local\"\r\n      },\r\n      \"connect_timeout\": \"10s\",\r\n      \"lb_policy\": \"LEAST_REQUEST\",\r\n      \"circuit_breakers\": {\r\n       \"thresholds\": [\r\n        {\r\n         \"max_connections\": 4294967295,\r\n         \"max_pending_requests\": 4294967295,\r\n         \"max_requests\": 4294967295,\r\n         \"max_retries\": 4294967295,\r\n         \"track_remaining\": true\r\n        }\r\n       ]\r\n      },\r\n      \"upstream_bind_config\": {\r\n       \"socket_options\": [\r\n        {\r\n         \"description\": \"ens-qwxnmb\",\r\n         \"level\": \"1\",\r\n         \"name\": \"25\",\r\n         \"buf_value\": \"ZW5zLXF3eG5tYg==\"\r\n        }\r\n       ]\r\n      },\r\n      \"metadata\": {\r\n       \"filter_metadata\": {\r\n        \"istio\": {\r\n         \"services\": [\r\n          {\r\n           \"namespace\": \"some-namespace\",\r\n           \"name\": \"nginx-perf-8088\",\r\n           \"host\": \"nginx-perf-8088.some-namespace.svc.cluster.local\"\r\n          }\r\n         ],\r\n         \"default_original_port\": 8088\r\n        }\r\n       }\r\n      },\r\n      \"common_lb_config\": {\r\n       \"locality_weighted_lb_config\": {}\r\n      },\r\n      \"filters\": [\r\n       {\r\n        \"name\": \"istio.metadata_exchange\",\r\n        \"typed_config\": {\r\n         \"@type\": \"type.googleapis.com/envoy.tcp.metadataexchange.config.MetadataExchange\",\r\n         \"protocol\": \"istio-peer-exchange\"\r\n        }\r\n       }\r\n      ]\r\n     }\r\n    }\r\n   ]\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v3.EndpointsConfigDump\",\r\n   \"dynamic_endpoint_configs\": [\r\n    {\r\n     \"endpoint_config\": {\r\n      \"@type\": \"type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignment\",\r\n      \"cluster_name\": \"outbound|8088||nginx-perf-8088.some-namespace.svc.cluster.local\",\r\n      \"endpoints\": [\r\n       {\r\n        \"locality\": {\r\n         \"region\": \"az0.dc0\"\r\n        },\r\n        \"lb_endpoints\": [\r\n         {\r\n          \"endpoint\": {\r\n           \"address\": {\r\n            \"socket_address\": {\r\n             \"address\": \"20.0.1.80\",\r\n             \"port_value\": 80\r\n            }\r\n           },\r\n           \"health_check_config\": {}\r\n          },\r\n          \"health_status\": \"HEALTHY\",\r\n          \"metadata\": {\r\n           \"filter_metadata\": {\r\n            \"istio\": {\r\n             \"workload\": \"cce01-nginx-perf-test-db5d7c6cd-lhvrv;some-namespace;;;Kubernetes\"\r\n            }\r\n           }\r\n          },\r\n          \"load_balancing_weight\": 1\r\n         },\r\n         {\r\n          \"endpoint\": {\r\n           \"address\": {\r\n            \"socket_address\": {\r\n             \"address\": \"20.0.1.148\",\r\n             \"port_value\": 80\r\n            }\r\n           },\r\n           \"health_check_config\": {}\r\n          },\r\n          \"health_status\": \"HEALTHY\",\r\n          \"metadata\": {\r\n           \"filter_metadata\": {\r\n            \"istio\": {\r\n             \"workload\": \"cce01-nginx-perf-test-db5d7c6cd-m87sh;some-namespace;;;Kubernetes\"\r\n            }\r\n           }\r\n          },\r\n          \"load_balancing_weight\": 1\r\n         }\r\n        ],\r\n        \"load_balancing_weight\": 2\r\n       }\r\n      ],\r\n      \"policy\": {\r\n       \"overprovisioning_factor\": 140\r\n      }\r\n     }\r\n    },\r\n   ]\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v3.ListenersConfigDump\",\r\n   \"dynamic_listeners\": [\r\n    {\r\n     \"name\": \"30.0.7.3_11111_ens-qwxnmb\",\r\n     \"active_state\": {\r\n      \"listener\": {\r\n       \"@type\": \"type.googleapis.com/envoy.config.listener.v3.Listener\",\r\n       \"name\": \"30.0.7.3_11111_ens-qwxnmb\",\r\n       \"address\": {\r\n        \"socket_address\": {\r\n         \"address\": \"30.0.7.3\",\r\n         \"port_value\": 11111\r\n        }\r\n       },\r\n       \"filter_chains\": [\r\n        {\r\n         \"filters\": [\r\n          {\r\n           \"name\": \"envoy.filters.network.tcp_proxy\",\r\n           \"typed_config\": {\r\n            \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\",\r\n            \"stat_prefix\": \"outbound|8088||nginx-perf-8088.some-namespace.svc.cluster.local\",\r\n            \"cluster\": \"outbound|8088||nginx-perf-8088.some-namespace.svc.cluster.local\"\r\n           }\r\n          }\r\n         ]\r\n        }\r\n       ],\r\n       \"traffic_direction\": \"OUTBOUND\",\r\n       \"stat_prefix\": \"ens-qwxnmb\"\r\n      }\r\n     }\r\n    }\r\n   ]\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v3.ScopedRoutesConfigDump\"\r\n  }\r\n ]\r\n}\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36728/comments",
    "author": "SpongeBobIsTheKingInTheNorth",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2024-10-21T15:16:13Z",
        "body": "I think Istio disables a bunch of metrics by default but I'm not 100% sure. I would talk to them."
      },
      {
        "user": "SpongeBobIsTheKingInTheNorth",
        "created_at": "2024-10-23T03:32:24Z",
        "body": "I find that the configuration of the Envoy is incorrect. As a result, the `/stats` interface does not collect statistics. This problem can be solved by modifying `stats_config.stats_matcher`.\r\nThanks all!"
      }
    ]
  },
  {
    "number": 36713,
    "title": "When I run multiple envoy containers in docker and restart hot one of them, the envoy can't find the shm file, \"envoy_shared_memory_60 check user permissions. Error: No such file or directory\"",
    "created_at": "2024-10-21T03:16:13Z",
    "closed_at": "2024-11-27T20:01:27Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36713",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *When I run multiple envoy containers in docker and restart hot one of them, the envoy can't find the shm file.*\r\n\r\n*Description*:\r\n>When I run multiple envoy containers in docker and restart hot one of them, the envoy can't find the shm file.\r\n The logs :[source/server/hot_restart_impl.cc:44] panic: cannot open shared memory region /envoy_shared_memory_60 check user permissions. Error: No such file or directory.\r\nI have used the \"--base-id\" for every envoy container, and when I check the path, \"/var/lib/docker/containers/7dc2aa931d5aba0c25ff107ad4f9ce7f10ce11f79a87b334b4c3b3061ba6f775/shm\" which is the target container's shm path, the envoy_shared_memory_60 exits.\r\nThanks for help.\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36713/comments",
    "author": "AMagnolia",
    "comments": [
      {
        "user": "AMagnolia",
        "created_at": "2024-10-21T03:18:42Z",
        "body": "@phlax @danzh2010 could you please give me some help?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-20T16:01:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-27T20:01:27Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 36459,
    "title": "Support for future Redis versions Vs. ValKey",
    "created_at": "2024-10-07T06:07:06Z",
    "closed_at": "2024-12-15T00:04:20Z",
    "labels": [
      "question",
      "stale",
      "area/redis"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36459",
    "body": "Title: Is there any discussion around whether the Envoy Redis filter plans to remain (in the future) compatible with Redis or the Open source \"equivalent\" ValKey project?\r\n\r\nDescription:\r\nIs there any discussion around whether the Envoy Redis filter plans to remain (in the future) compatible with Redis or the Open source \"equivalent\" ValKey project?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36459/comments",
    "author": "AnirKulk",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2024-10-07T13:16:25Z",
        "body": "pinging @msukalski @henryyyang @weisisea for commentary"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-06T16:01:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "AnirKulk",
        "created_at": "2024-11-07T10:04:35Z",
        "body": "@msukalski  / @HenryYYang  / @weisisea can you please comment?"
      },
      {
        "user": "weisisea",
        "created_at": "2024-11-07T18:33:03Z",
        "body": "Thanks for the question!\r\n\r\nValkey is currently compatible with existing Redis clients including Envoy Redis filter. As far as I know, in the Valkey open source community, there is no plan to diverge the client interface for the time being. Using `CLUSTER SLOTS` command to discover cluster topology will continue to be supported by Valkey.\r\n\r\nHowever, it's possible in the future that Redis introduces commands that Valkey doesn't support or vice versa. If that happens, we may need to enhance the Redis filter to be able to detect whether it's talking to Redis or Valkey and then process commands accordingly. If there is a more drastic divergence in the client interface (e.g., using different commands to discover cluster topology and the responses have different formats), we may need to create a new filter for Valkey."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-07T20:01:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-15T00:04:20Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 36379,
    "title": "x-envoy-upstream-service-time reporting only for 25% of requests",
    "created_at": "2024-09-30T06:52:00Z",
    "closed_at": "2024-11-07T00:03:45Z",
    "labels": [
      "question",
      "stale",
      "area/router"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36379",
    "body": "I am using an envoy proxy sidecar as part of istio mesh, and reporting to the database every request and this header returns only on 25% of requests",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36379/comments",
    "author": "aryehlev",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2024-09-30T18:17:40Z",
        "body": "Do you see Istio metrics for the remaining 75% of requests? Do the requests flow through the same path? There are difference in how Envoy is configured for inbound/outbound/gateway use cases."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-30T20:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-07T00:03:44Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 36349,
    "title": "liburing compilation on Rocky Linux 8 which supports 4.18 kernel ",
    "created_at": "2024-09-26T14:06:36Z",
    "closed_at": "2024-11-13T12:01:49Z",
    "labels": [
      "question",
      "area/build",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36349",
    "body": "When we are compiling envoy 1.30.x version using below command on rocky linux 8 which supports 4.18 kenel version, we are not getting liburing compilation error and we are able to compile envoy successfully.\r\nbazel --bazelrc=/dev/null build --sandbox_debug --verbose_failures --copt=\"-DENVOY_IGNORE_GLIBCXX_USE_CXX11_ABI_ERROR=1 -DLLVM_USE_SPLIT_DWARF\" --define wasm=disabled --define tcmalloc=disabled --define signal_trace=disabled --config=sizeopt --compilation_mode=fastbuild --copt=-Wno-error=maybe-uninitialized  //source/exe:envoy-static.stripped\r\n\r\nWhen we are compiling envoy 1.31.x version using above command on rocky linux 8 which supports 4.18 kenel version, we are getting liburing compilation error and we are not able to compile envoy successfully. We are getting liburing compilation error. I am having below queries\r\n1. 1.30.x was not using liburing library?\r\n2. Is there any changes done in 1.31.x for using liburing library?\r\n3.  As per my understanding kernel version greater than 5.1 use liburing library so how the operating system which are having kernel version lesser than 5.0?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36349/comments",
    "author": "Gaurav0411GitHub",
    "comments": [
      {
        "user": "Gaurav0411GitHub",
        "created_at": "2024-09-26T14:08:18Z",
        "body": "@phlax , Thanks in advance for your reply"
      },
      {
        "user": "phlax",
        "created_at": "2024-09-26T14:18:39Z",
        "body": "hi @Gaurav0411GitHub my response is that the supported way to build is using the envoy build image - ie `ci/run_envoy_docker.sh`\r\n\r\nit _should_ be possible to build in other environments - and where necessary we will take patches to facilitate that - but we provide a build image for precisely this reason - so that others can build with reasonable confidence on any platform that supports docker (with some qualifications about kernel versions)"
      },
      {
        "user": "phlax",
        "created_at": "2024-09-26T14:22:33Z",
        "body": "> 1.30.x was not using liburing library?\r\n\r\nit was\r\n\r\n> Is there any changes done in 1.31.x for using liburing library?\r\n\r\nyou would have to check the diffs between the branches\r\n\r\n> As per my understanding kernel version greater than 5.1 use liburing library so how the operating system which are having kernel version lesser than 5.0?\r\n\r\nnot sure on specifics wrt liburing kernel versions - i _do_ remember that it required a fairly recent kernel - iirc we could not add it until we updated our vms to ubuntu jammy\r\n"
      },
      {
        "user": "Gaurav0411GitHub",
        "created_at": "2024-09-27T07:28:19Z",
        "body": "I want to use rhel 8.10 (kernel-devel-4.18.0-553.16.1.el8_10.x86_64 ,kernel-headers-4.18.0-553.16.1.el8_10.x86_64), Is envoy 1.31.x version is supported on this above mentioned version?\r\n"
      },
      {
        "user": "phlax",
        "created_at": "2024-09-27T08:13:12Z",
        "body": "io_uring requires a 5.1 kernel"
      },
      {
        "user": "Gaurav0411GitHub",
        "created_at": "2024-09-28T06:12:09Z",
        "body": "Thanks for your reply, \r\nI understood io_uring requires 5.1 kernel.\r\nI am using the below command to compile on Rocky Linux 8. \r\nbazel --bazelrc=/dev/null build --sandbox_debug --verbose_failures --copt=\"-DENVOY_IGNORE_GLIBCXX_USE_CXX11_ABI_ERROR=1 -DLLVM_USE_SPLIT_DWARF\" --define wasm=disabled --define tcmalloc=disabled --define signal_trace=disabled --config=sizeopt --compilation_mode=fastbuild --copt=-Wno-error=maybe-uninitialized  //source/exe:envoy-static.stripped \r\nIs there any way to avoid io_uring in envoy compilation?\r\nor we will not be able to compile envoy 1.31.x without io_uring?\r\n\r\n"
      },
      {
        "user": "Gaurav0411GitHub",
        "created_at": "2024-10-07T08:41:20Z",
        "body": "@phlax @timperrett @pjjw @mkbehr Thanks is advance for the answer of above question"
      },
      {
        "user": "phlax",
        "created_at": "2024-10-07T09:06:30Z",
        "body": ">  Is there any way to avoid io_uring in envoy compilation?\r\n\r\nnot atm i think - the current select to determine inclusion only checks the platform is linux - a flag could be added to disable this"
      },
      {
        "user": "Gaurav0411GitHub",
        "created_at": "2024-10-07T09:19:57Z",
        "body": "Can we expect this kind of flag to disable somewhere in 1.32.x?"
      },
      {
        "user": "phlax",
        "created_at": "2024-10-07T09:21:47Z",
        "body": "it doesnt exist atm - someone would need to add it"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-06T12:01:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-13T12:01:48Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 36338,
    "title": "Send response header to gRPC access logs server, but don't send it to user",
    "created_at": "2024-09-25T18:02:50Z",
    "closed_at": "2024-11-03T16:01:07Z",
    "labels": [
      "question",
      "stale",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36338",
    "body": "I use gRPC server to collect access logs. My service responses with header I need to log. So my config looks like this:\r\n```\r\n- name: envoy.access_loggers.http_grpc\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.access_loggers.grpc.v3.HttpGrpcAccessLogConfig\r\n      additional_response_headers_to_log:\r\n        - x-header-to-log\r\n      additional_request_headers_to_log:\r\n        - Range\r\n      common_config:\r\n        log_name: grpc\r\n        grpc_service:\r\n          google_grpc:\r\n            target_uri: \"[grpc.server.ip.address]:5001\"\r\n            stat_prefix: streams_total\r\n```\r\nBut I need not to send this header to user. I tried to use `response_headers_to_remove: - x-header-to-log` inside `route_config` section to achieve it, but in this case header doesn't present in access log message. How can I fix it?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36338/comments",
    "author": "asidorochev",
    "comments": [
      {
        "user": "asidorochev",
        "created_at": "2024-09-27T10:56:42Z",
        "body": "I've found possible solution using Lua script inside `http_filters` section:\r\n```\r\nfunction envoy_on_response(response_handle)\r\n  local value = response_handle:headers():get(\"x-header-to-log\")\r\n  if value ~= nil then\r\n    response_handle:streamInfo():dynamicMetadata():set(\"context\", \"x-header-to-log\", value)\r\n    response_handle:headers():remove(\"x-header-to-log\")\r\n  end\r\nend\r\n```\r\nI set the header value to dynamic metadata and then I remove the header from the response. Then I get that value from `AccessLogEntry.CommonProperties.Metadata.FilterMetadata`. I'm not sure if this is the best solution, but at least it works."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-27T12:01:31Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-03T16:01:07Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 36121,
    "title": "Didn't find a registered implementation for 'envoy.filters.http.router' with type URL: ''",
    "created_at": "2024-09-13T14:10:13Z",
    "closed_at": "2024-09-13T14:25:21Z",
    "labels": [
      "question",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36121",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *Didn't find a registered implementation for 'envoy.filters.http.router' with type URL: ''*\r\n\r\n*Description*:\r\n>I am very new to this software. I set up an `envoy.yaml` with 12 listeners and 12 clusters. I formatted the `yaml` correctly and I get the following error \r\n```\r\nDidn't find a registered implementation for 'envoy.filters.http.router' with type URL: ''\r\n```\r\nThis is the beginning of one of my listeners:\r\n\r\n```yaml\r\n  listeners:\r\n  # Listener for port 1\r\n  - name: listener_1\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 50001\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          codec_type: AUTO\r\n          stat_prefix: grpc_service_1\r\n          route_config:\r\n            name: grpc_service_1_route\r\n            virtual_hosts:\r\n            - name: grpc_service_1_host\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: grpc_service_1_cluster\r\n                  timeout: 0s\r\n          http_filters:\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\nI installed envoy 1.31 via debian repository for `bullseye` What is wrong with my setup? Thanks for your patience!\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36121/comments",
    "author": "bytersproblem",
    "comments": [
      {
        "user": "bytersproblem",
        "created_at": "2024-09-13T14:11:58Z",
        "body": "I didn't say that I'm setting up anysync-dockercompose which uses gRPC as a trasfer protocol to be able to foward gRPC calls to nginx and serve them on default https port."
      },
      {
        "user": "phlax",
        "created_at": "2024-09-13T14:21:00Z",
        "body": "posted config looks correct to me - are you certain its this defined listener that is erroring?"
      },
      {
        "user": "bytersproblem",
        "created_at": "2024-09-13T14:25:07Z",
        "body": "You are correct. This is indeed a long config. The last lype `\"@type:\"` was missing on the fifth listener. Now it starts with `starting main dispatch loop`. Will proceed from here. I wll close the issue."
      }
    ]
  },
  {
    "number": 36089,
    "title": "gRPC Async client metadata usage",
    "created_at": "2024-09-12T10:10:11Z",
    "closed_at": "2024-10-22T16:01:20Z",
    "labels": [
      "question",
      "stale",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36089",
    "body": "*Title*: *need help passing metadata to async gRPC request*\r\n\r\n*Description*:\r\nI have a use case where an async Rate-Limit unary gRPC request is created, \r\nTo issue the request, we use: \r\n`async_client_->send(METHOD, REQUEST, HANDLER, SPAN, Envoy::Http::AsyncClient::RequestOptions().setTimeout(TIMEOUT));`\r\n\r\nWe saw `Envoy::Http::AsyncClient::RequestOptions()` has a `setMetadata` method, but were not able to use it..\r\n\r\nIn case needed, here is our cluster config:\r\n\r\n```  {\r\n   \"name\": \"rl_cluster\",\r\n   \"type\": \"STRICT_DNS\",\r\n   \"connect_timeout\": \"10s\",\r\n   \"circuit_breakers\": {\r\n    \"thresholds\": [ { \"max_connections\": 100000, \"max_pending_requests\": 100000, \"max_requests\": 100000 } ]\r\n   },\r\n   \"dns_refresh_rate\": \"5s\",\r\n   \"transport_socket\": {\r\n    \"name\": \"envoy.transport_sockets.tls\",\r\n    \"typed_config\": {\r\n     \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\",\r\n     \"common_tls_context\": {\r\n      \"tls_certificate_sds_secret_configs\": [ {\r\n        \"name\": \"tls_sds\",\r\n        \"sds_config\": {\r\n         \"resource_api_version\": \"V3\",\r\n         \"path_config_source\": {  \"path\": \"...\" }\r\n        }\r\n       }\r\n      ],\r\n      \"validation_context_sds_secret_config\": {\r\n       \"name\": \"validation_context_sds\",\r\n       \"sds_config\": {\r\n        \"resource_api_version\": \"V3\",\r\n        \"path_config_source\": { \"path\": \"...\" }\r\n       }\r\n      }\r\n     }\r\n    }\r\n   },\r\n   \"upstream_connection_options\": {\r\n    \"tcp_keepalive\": { \"keepalive_time\": 15 }\r\n   },\r\n   \"load_assignment\": {\r\n    \"cluster_name\": \"rl_cluster\",\r\n    \"endpoints\": [ {\r\n      \"lb_endpoints\": [ {\r\n        \"endpoint\": {\r\n         \"address\": {\r\n          \"socket_address\": { \"address\": \"rate-limiter.svc\", \"port_value\": 8081 }\r\n         }\r\n        }\r\n       }\r\n      ]\r\n     }\r\n    ]\r\n   },\r\n   \"typed_extension_protocol_options\": {\r\n    \"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\": {\r\n     \"@type\": \"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\r\n     \"explicit_http_config\": { \"http2_protocol_options\": {} }\r\n    }\r\n   }\r\n  }",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36089/comments",
    "author": "valentinegu",
    "comments": [
      {
        "user": "ravenblackx",
        "created_at": "2024-09-12T14:22:01Z",
        "body": "Could you elaborate on how you *tried* to use `setMetadata`, that didn't work, and in what way it failed to work?\r\n\r\n@wbpcode to take it from there."
      },
      {
        "user": "valentinegu",
        "created_at": "2024-09-15T10:44:20Z",
        "body": "Amongst other things i tired using following code to create a Metadata Struct.\r\n```envoy::config::core::v3::Metadata meta;\r\nauto& filter_metadata = (*meta.mutable_filter_metadata());\r\n\r\nEnvoy::ProtobufWkt::Struct data;\r\nEnvoy::ProtobufWkt::Value value;\r\nvalue.set_string_value(\"SOME_VALUE\");\r\n(*data.mutable_fields())[\"SOME_KEY\"] = value;\r\n\r\nfilter_metadata[\"some.path\"].MergeFrom(data);\r\n```\r\nthan passed it to `SetMetadata()` and finally tried to look for ways to reuse it.."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-15T12:01:47Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-22T16:01:20Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 36076,
    "title": "Outbound rate limiting in requests per second per domain + queue management?",
    "created_at": "2024-09-11T18:44:52Z",
    "closed_at": "2024-10-23T20:01:22Z",
    "labels": [
      "question",
      "stale",
      "area/http_filter"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36076",
    "body": "Hello,\r\n\r\nI'm trying to use Envoy proxy in forward proxy mode for HTTP and HTTPS requests.\r\nI'm looking for a way to set a rate limit in terms of requests per seconds per domain (without knowing the visited domains in advance).\r\nEnvoy proxy can restrict requests with a local or a global rate limiter which is great!\r\nHowever, I'd like to process all requests instead of rejecting requests exceeding the rate limit for a given domain (which Envoy does not seem to provide). I also would like to preserve the requests order.\r\n\r\nSadly, to my knowledge Envoy does not support queuing of requests. \r\nHow could I do this ? Any input would be appreciated.  \r\n\r\nBest regards",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36076/comments",
    "author": "bern548456",
    "comments": [
      {
        "user": "ravenblackx",
        "created_at": "2024-09-12T14:50:47Z",
        "body": "I suspect the answer for this might be you'd need to make a custom filter for it - my company has a queue filter like what you need internally but it uses our own system for metrics so is unfortunately not suitable for simply pushing into `contrib`. The fact that we have that custom filter leads me to suspect that there isn't a readily available pre-existing way to do it, though the filter is quite old so it's possible another option has been added since that filter was created.\r\n\r\n@alyssawilk to confirm/deny that adding a new filter is probably the only way to achieve that."
      },
      {
        "user": "bern548456",
        "created_at": "2024-09-14T09:20:44Z",
        "body": "Thanks for your quick response and feedback. I will definitely consider creating a custom filter, perhaps extending the existing local rate limit filter, to get off to a good start.\r\n\r\nI'm still hesitating between using a separte component (e.g.: redis) to store queues and rate limiting info (to avoid local memory exhaustion), or doing everything locally (for ease of use and better performances). But since there would only be a unique proxy instance (even if it's multi-threaded), I'd be inclined to go for the local in-memory option (and not the redis one) - ok, that would require synchronization and thread-safe mechanisms.\r\n\r\nOne of the difficulties is that the number of visited domains and requests sent per domain can quickly use too much memory. This is why I would consider using a global rate limiter (for all incoming requests and domains) that would help restrict the growth of each domain's queue if necessary. And yes, each domain would still have its own rate limiter. Perhaps this (the global rate limiter) could be adjusted at runtime, as a backpressure mechanism. I don't want to process all requests as quickly as possible but rather find a compromise.\r\n\r\nAll in all, I'm well aware this could be complex to implement which is why I'm still thinking about it ;)."
      },
      {
        "user": "alyssawilk",
        "created_at": "2024-09-16T14:09:14Z",
        "body": "I agree with Raven I don't think you can do this in Envoy today.  Arguably if you use the dynamic forward proxy module, you might be able to do something close to this taking advantage of various connection and stream limits, but I don't think there's per-host controls anywhere else.  I think it would be generically useful so fine to add to existing rate limit filters if you can overcome the design challenges you thoughtfully call out.\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-16T16:01:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-23T20:01:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 36013,
    "title": "How to return real error message for a \"two hop\" proxy?",
    "created_at": "2024-09-06T19:12:04Z",
    "closed_at": "2024-10-14T00:04:03Z",
    "labels": [
      "question",
      "area/tls",
      "stale",
      "area/configuration",
      "area/internal_listener"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36013",
    "body": "*Title*: *How to return real error message for a \"two hop\" proxy?*\r\n\r\n*Description*:\r\nI have an envoy instance which is an \"HTTP2 CONNECT over TLS\". But for some reason I implemented it as a two hop (with the help of `internal_listener`). \r\n\r\nBut I found that when the second hop fails (e.g. TLS handshake fails), my client only gets `\"upstream connect error or disconnect/reset before headers. reset reason: connection termination\"` which is a somewhat ambiguous reason.\r\n\r\nHere is a streamlined configuration (envoy.yaml) to reproduce the problem:\r\n```yaml\r\n\r\nbootstrap_extensions:\r\n- name: envoy.bootstrap.internal_listener\r\n  typed_config:\r\n    \"@type\": type.googleapis.com/envoy.extensions.bootstrap.internal_listener.v3.InternalListener\r\n\r\n\r\nstatic_resources:\r\n\r\n  listeners:\r\n  - name: ingress0\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 10001\r\n    socket_options:\r\n      \r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress0_entry\r\n          access_log:\r\n          - name: envoy.access_loggers.stdout\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\r\n              log_format:\r\n                text_format: \"[%START_TIME%] ingress(0): \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\" %RESPONSE_CODE%(%RESPONSE_CODE_DETAILS%) %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%DOWNSTREAM_DIRECT_REMOTE_ADDRESS%\\\" \\\"%REQ(USER-AGENT)%\\\" \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\"\\n\"\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: ingress0_wrap_in_h2_tls_upstream\r\n\r\n          http_filters:\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n              suppress_envoy_headers: true\r\n\r\n  - name: ingress0_wrap_in_h2_tls\r\n    internal_listener: {}\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.tcp_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n          stat_prefix: ingress0_wrap_in_h2_tls\r\n          cluster: \"ingress0_upstream\"\r\n          tunneling_config:\r\n            hostname: \"just.a.example.com\"\r\n\r\n  clusters:\r\n  - name: ingress0_wrap_in_h2_tls_upstream\r\n    load_assignment:\r\n      cluster_name: ingress0_wrap_in_h2_tls_upstream\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              envoy_internal_address:\r\n                server_listener_name: ingress0_wrap_in_h2_tls\r\n    transport_socket:\r\n      name: envoy.transport_sockets.internal_upstream\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.internal_upstream.v3.InternalUpstreamTransport\r\n        transport_socket:\r\n          name: envoy.transport_sockets.raw_buffer\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.raw_buffer.v3.RawBuffer\r\n\r\n\r\n  - name: ingress0_upstream\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    load_assignment:\r\n      cluster_name: ingress0_upstream\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.github.com # For demonstration purposes, pick an upstream that would cause the TLS handshake to fail.\r\n                port_value: 80\r\n    typed_extension_protocol_options:\r\n      envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n        \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n        explicit_http_config:\r\n          http2_protocol_options: {}\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n        common_tls_context:\r\n```\r\n\r\nLaunch envoy with\r\n```sh\r\nenvoy-static -l debug -c envoy.yaml\r\n```\r\n\r\nAnd then, send a request with `curl`\r\n```sh\r\ncurl 127.0.0.1:10001\r\n```\r\nThe HTTP response is\r\n```txt\r\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\r\n```\r\n\r\nAnd the real reason for the error can only be found in the envoy log.\r\n```txt\r\n[2024-09-06 18:36:48.802][1315388][debug][connection] [source/common/tls/ssl_socket.cc:280] [Tags: \"ConnectionId\":\"3\"] remote address:20.205.243.166:80,TLS_error:|268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER:TLS_error_end\r\n[2024-09-06 18:36:48.802][1315388][debug][connection] [source/common/network/connection_impl.cc:281] [Tags: \"ConnectionId\":\"3\"] closing socket: 0\r\n[2024-09-06 18:36:48.802][1315388][debug][connection] [source/common/tls/ssl_socket.cc:280] [Tags: \"ConnectionId\":\"3\"] remote address:20.205.243.166:80,TLS_error:|268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER:TLS_error_end:TLS_error_end\r\n[2024-09-06 18:36:48.802][1315388][debug][client] [source/common/http/codec_client.cc:107] [Tags: \"ConnectionId\":\"3\"] disconnect. resetting 0 pending requests\r\n[2024-09-06 18:36:48.802][1315388][debug][pool] [source/common/conn_pool/conn_pool_base.cc:495] [Tags: \"ConnectionId\":\"3\"] client disconnected, failure reason: TLS_error:|268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER:TLS_error_end:TLS_error_end\r\n```\r\nShowing that a TLS error occurred on the second hop.\r\n\r\nI'd like to know if there is any way (either by tweaking the envoy config file, or by writing some code for envoy) to return the real error message in the HTTP response?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36013/comments",
    "author": "imlk0",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2024-09-06T20:13:50Z",
        "body": "cc @kyessenov @adisuissa"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-07T00:03:55Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-14T00:04:03Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 36006,
    "title": "Help needed to get started extending envoy L7 protocol with MQTT and also its WASM capabilities",
    "created_at": "2024-09-06T16:55:04Z",
    "closed_at": "2024-10-07T06:11:59Z",
    "labels": [
      "enhancement",
      "question",
      "stale",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/36006",
    "body": "*Title*: *Help needed to get started extending envoy WASM capabilities*\r\n\r\n*Description*: I am planning to extend envoy to proxy MQTT at layer 7 (similar to how HTTP is proxied) and not just at layer 4. Once I have that in place, I want to implement MQTT-Request and -Response filter capabilities for WASM so that I can chain WASM filters for MQTT Requests and responses.\r\n\r\nCan someone help me and share references / issues / or any other pointers that help me get started?\r\n\r\nFor example, I found this #35420 and wonder if that discussion provides the latest state of the whole WASM integration?\r\n\r\nAlso, which existing integration is the best to look at / use as basis to implement MQTT requests and responses so that I have onMqttRequest(...)  abd onMqttResponse(...) handler functions?\r\n\r\nAny help would gladly be appreciated.\r\n\r\nI am also happy to contribute the source code back to the code base if accepted.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/36006/comments",
    "author": "algermissen",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2024-09-06T20:15:49Z",
        "body": "cc @mpwarres @lizan\r\n\r\n@algermissen might be worth asking on Envoy slack"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-07T00:03:53Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      }
    ]
  },
  {
    "number": 35871,
    "title": "No response code details in upstream_log access logs for each attempt to upstream",
    "created_at": "2024-08-27T18:04:43Z",
    "closed_at": "2024-09-04T13:33:36Z",
    "labels": [
      "question",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/35871",
    "body": "*Title*: *No response code details in upstream_log access logs for each attempt to upstream*\r\n\r\n*Description*:\r\n>We have configured upstream_log in envoy.filters.http.router filter. We wanted to log each failed attempt to upstream to find out what is the reason for failure. \r\nBut we see that when upstream cluster responds with 5xx error, the upstream_log prints empty values for some of the fields like: \r\nresponse_code_details: null\r\nretry_count: 0\r\n\r\nIn 2 upstream request attempts it gives above values in both log entries.\r\n\r\nEnvoy prints 5xx response_code though in upstream log. \r\nAnd the envoy.filters.network.http_connection_manager filter level access logs prints below values:\r\nresponse_code_details: via_upstream\r\nretry_count: 2\r\n\r\nWe would like to have these 2 fields in upstream logs as well As it provided useful info while debugging any issue with upstream cluster.\r\n\r\nSo, Is this an expected behavior or a bug in upstream_log implementation?\r\n\r\n(see our access log config below)\r\n\r\n*Repro steps*:\r\n> Tested with envoy version 1.30.4 with below config for upstream_log.\r\n\r\n\r\n*Admin and Stats Output*:\r\n\r\n*Config*:\r\n   \r\n```name: envoy.filters.http.router\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n          dynamic_stats: true\r\n          start_child_span: true\r\n          suppress_envoy_headers: false\r\n          upstream_log:\r\n          - name: envoy.access_loggers.file\r\n            filter:\r\n              or_filter:\r\n                filters:\r\n                  - status_code_filter:\r\n                      comparison:\r\n                        op: GE\r\n                        value:\r\n                          default_value: 500\r\n                          runtime_key: access_log.access_error.status\r\n                  - status_code_filter:\r\n                      comparison:\r\n                        op: EQ\r\n                        value:\r\n                          default_value: 0\r\n                          runtime_key: access_log.access_error.status\r\n                  - traceable_filter: {}\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n              path: \"/dev/stdout\"\r\n              log_format:\r\n                formatters:\r\n                - name: envoy.formatter.req_without_query\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.formatter.req_without_query.v3.ReqWithoutQuery\r\n                json_format:\r\n                  type: \"upstream_log\"\r\n                  response_code: \"%RESPONSE_CODE%\"\r\n                  response_code_details: \"%RESPONSE_CODE_DETAILS%\"\r\n                  connection_termination_details: \"%CONNECTION_TERMINATION_DETAILS%\"\r\n                  response_flags: \"%RESPONSE_FLAGS%\"\r\n                  upstream_transport_failure_reason: \"%UPSTREAM_TRANSPORT_FAILURE_REASON%\"\r\n                  upstream_remote_address: \"%UPSTREAM_REMOTE_ADDRESS%\"\r\n                  x_request_id: \"%REQ(X-REQUEST-ID)%\"\r\n                  upstream_cluster: \"%UPSTREAM_CLUSTER%\"\r\n                  retry_count: \"%UPSTREAM_REQUEST_ATTEMPT_COUNT%\"```\r\n \r\n*Logs*:\r\n>Include the access logs and the Envoy logs.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/35871/comments",
    "author": "VishalDamgude",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2024-08-27T20:10:32Z",
        "body": "`UPSTREAM_REQUEST_ATTEMPT_COUNT` is only valid on the downstream request and can be logged there. \r\n\r\nFor `RESPONSE_CODE_DETAILS`, as you can see from the docs, they're referring to the details about the response code sent back to the downstream client, so that is also only applicable on downstream access logs.\r\n\r\n> HTTP response code details provides additional information about the response code, such as who set it (the upstream or envoy) and why.\r\n\r\nSo you probably need to configure both downstream and upstream logs, and you can link them by request id or `%STREAM_ID%`."
      },
      {
        "user": "VishalDamgude",
        "created_at": "2024-09-04T13:33:37Z",
        "body": "Thanks @ggreenway for clarifying."
      }
    ]
  },
  {
    "number": 35773,
    "title": "Upstream health checks threading model",
    "created_at": "2024-08-21T16:19:39Z",
    "closed_at": "2024-10-25T20:01:27Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/35773",
    "body": "I tried looking it up in docs, but didn't find (please help referring if it exist): Are Envoy upstream health checks performed per-worker, or are they running globally from the main thread, and results are posted to workers?\r\nI'd assume the latter, but wanted to make sure",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/35773/comments",
    "author": "ohadvano",
    "comments": [
      {
        "user": "botengyao",
        "created_at": "2024-08-22T13:17:53Z",
        "body": "Yes, health checks are performed on the main thread, and then posted to the thread local configs."
      },
      {
        "user": "ohadvano",
        "created_at": "2024-08-22T13:48:10Z",
        "body": "Thanks @botengyao, is this documented somewhere? If not, I can add it"
      },
      {
        "user": "botengyao",
        "created_at": "2024-09-18T15:09:34Z",
        "body": "@ohadvano no sure, but feel free to add any docs. Thanks!"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-18T16:01:31Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-25T20:01:26Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 35679,
    "title": "Why the HTTP filter will be destructed even when encodeData returns StopIterationAndBuffer?",
    "created_at": "2024-08-12T23:53:32Z",
    "closed_at": "2024-09-23T04:01:22Z",
    "labels": [
      "question",
      "stale",
      "area/http_filter"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/35679",
    "body": "Why the HTTP filter will be destructed even when encodeData returns StopIterationAndBuffer?\r\nIt might be a bug since a filter should not be destroyed when it's still needed for future processing. \r\nCould anyone familiar with that provide me with some clues? Thank you so much!\r\n\r\n\r\nMy code is:\r\n\r\n```cpp\r\nFilterDataStatus AppnetFilter::encodeData(Buffer::Instance &data, bool end_of_stream) {\r\n    ENVOY_LOG(info, \"[Appnet Filter] encodeData not done in one time, req_appnet_blocked_={}\", this->req_appnet_blocked_);\r\n    return FilterDataStatus::StopIterationAndBuffer;\r\n}\r\n```\r\n\r\nOutput\r\n```\r\n[2024-08-12 17:46:48.585][612617][info][filter] [appnet_filter/appnet_filter.cc:163] [Appnet Filter] encodeData not done in one time, req_appnet_blocked_=false\r\n[2024-08-12 17:46:48.587][612617][info][filter] [appnet_filter/appnet_filter.cc:90] [Appnet Filter] ~AppnetFilter\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/35679/comments",
    "author": "jokerwyt",
    "comments": [
      {
        "user": "jokerwyt",
        "created_at": "2024-08-13T00:21:23Z",
        "body": "Let me provide a little bit more background:\r\nMy filter is a DecoderEncoder filter. I will do some processing in `decodeData` and let the request pass. My backend server receives and responds normally. But my GRPC client reports an error with the message \"EOF\".\r\n\r\nHere is the description of what my filter does:\r\nI am going to send an async HTTP request to an external cache server in `encodeData()`. And I have to wait for the response back. Therefore, I need to stop the encoder filter processing and resume it in the external HTTP response callback (which is implemented as a method of my filter). I expect the filter will survive until I pass the response to the client, But it destructs immediately after I return `StopIterationAndBuffer`. \r\n\r\nWhat's more, I found that my client will always receive a response successfully when I return `StopIterationAndBuffer`, `StopIterationAndWatermark` and `Continue` in `encodeData()`, and will report error when I return `StopIterationNoBuffer`.\r\n\r\n I always return `StopIteration` in `encodeHeader()`. \r\n\r\nI wonder:\r\n1.  if I am correctly doing things.\r\n2. if it's the expected behavior of that return value. If it's, please explain the underlying mechanism a little bit more. \r\n3. Thank you guys!"
      },
      {
        "user": "adisuissa",
        "created_at": "2024-08-13T03:12:47Z",
        "body": "cc @yanavlasov"
      },
      {
        "user": "jokerwyt",
        "created_at": "2024-08-16T21:10:25Z",
        "body": "@adisuissa  @yanavlasov Could you please take a look? I think this is an important issue for those who want to develop things based on HTTP filters. Thanks for your effort!"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-09-16T00:03:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-09-23T04:01:22Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 35622,
    "title": "Hashing in xDS",
    "created_at": "2024-08-07T17:32:19Z",
    "closed_at": "2024-10-26T08:01:15Z",
    "labels": [
      "question",
      "stale",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/35622",
    "body": "Will appreciate help figuring out if I understand the hashing purpose of xDS correctly.\r\nIIUC, the idea of doing this hashing, is to optimize in cases where the remote xDS server may send the same configuration as already exists in Envoy. In that case, by hashing, we're able to guarantee that the config is the same, and skip the work of modifying the underlying resource.\r\nAre there other reasons to use hashing for xDS config consumption purposes?\r\n\r\ncc @htuch, @adisuissa ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/35622/comments",
    "author": "ohadvano",
    "comments": [
      {
        "user": "adisuissa",
        "created_at": "2024-08-12T17:35:22Z",
        "body": "For an xDS resource I think it is also being used to avoid re-ingesting that resource if it hasn't changed.\r\nSpecifically this is useful for SotW."
      },
      {
        "user": "ohadvano",
        "created_at": "2024-08-12T17:43:38Z",
        "body": "If I can trust the xDS server I am communicating with, will never send config object unless there was indeed an update, does it make sense to optimize the flow by not doing the hashing operation? We observed that the hashing may take high percentage of the overall config update latency, and I'm thinking that we could have an option flag to optimize this by not hashing (if specified by xDS config, by default - do hash)"
      },
      {
        "user": "ohadvano",
        "created_at": "2024-08-19T17:09:27Z",
        "body": "If the suggestion above makes sense as an optional consumer optimization, can we please mark this issue as \"Help wanted\"? I'll try to follow up on this"
      },
      {
        "user": "kyessenov",
        "created_at": "2024-08-19T17:27:33Z",
        "body": "I think the idea makes sense, but making it work would require plumbing through the consumption sites of xDS. Some of this hashing/diffing happens later, e.g. during structural comparison of filter chains. My suggestion is to start small, and focus on the specific config that is causing too much CPU in hashing."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-09-18T20:01:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "ohadvano",
        "created_at": "2024-09-18T20:08:35Z",
        "body": "Not stale"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-19T04:01:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-26T08:01:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 35317,
    "title": "Question about managing state across multiple phases calls in ext-proc for a HTTP request lifecycle",
    "created_at": "2024-07-22T07:52:13Z",
    "closed_at": "2024-07-23T05:59:05Z",
    "labels": [
      "question",
      "area/ext_proc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/35317",
    "body": "Hi, It seems like ext-proc will call the process over gRPC with multiple stages as soon as it can. In case I need to maintain some state per HTTP request on the external server, I'm curious if there's anything we could leverage in ext-proc.\r\n\r\nIn the WASM filter, each httpContext corresponds to a single HTTP request lifecycle (request/response), meaning the application does not need to manage its own state. I'm wondering if there's a similar mechanism in ext-proc.\r\n\r\nFor example, how can we check whether an incoming `onResponseHeaders` is part of the same HTTP request stream as a previous `onRequestHeaders` call without maintaining the state ourselves?\r\n\r\nSome custom solutions I could come up with:\r\n\r\n1. Whenever I receive `ProcessingRequest_RequestHeaders`, generate a unique `contextId` or leverage the envoy `requestId` attribute.\r\n2. Keep the state based on the `contextId` in our own logic.\r\n3. Whenever I receive `ProcessingRequest_ResponseHeaders`, use the `contextId` to retrieve the proper state.\r\n\r\nIt would be great if we have a way to abstract away the above custom logic for maintaining the state, or it might be better to keep the logic externally from envoy, appreciate any insight!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/35317/comments",
    "author": "xr",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2024-07-22T20:30:21Z",
        "body": "Adding @tyxia @yanjunxiang-google for comments."
      },
      {
        "user": "yanjunxiang-google",
        "created_at": "2024-07-22T22:25:50Z",
        "body": "HI, @xr \r\nOne way I can think of is to have the ext_proc server to track the gRPC stream open/close events, as  Envoy starts a new gRPC stream to communicate with ext_proc server for each HTTP stream. "
      },
      {
        "user": "tyxia",
        "created_at": "2024-07-22T23:01:07Z",
        "body": "@xr  No additional work is required from ext_proc filter side as such an affinity model has already been maintained in ext_proc's gRPC callout.\r\n\r\nThis can be purely your external server logic: On the first event (e.g., request header), server has a unique ID per stream and maintain it throughout HTTP lifecycle (request/response).\r\nWe actually have an internal product that is doing something similar. And it has been working well.  "
      },
      {
        "user": "xr",
        "created_at": "2024-07-23T05:59:05Z",
        "body": "@yanjunxiang-google @tyxia thanks for the info! good to hear that you are doing sth similar, if each http stream is corresponding with one grpcStream then I could also store the http state inside grpc stream."
      }
    ]
  },
  {
    "number": 35296,
    "title": " 503 UC upstream_reset_before_response_started{connection_termination} and Invalid HTTP header field errors",
    "created_at": "2024-07-21T08:10:02Z",
    "closed_at": "2024-08-29T00:03:42Z",
    "labels": [
      "question",
      "stale",
      "area/websockets"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/35296",
    "body": "I'm occasionally seeing 503 UC upstream_reset_before_response_started{connection_termination} errors in my service mesh ingress gateway. Around the same time, I'm also seeing Invalid HTTP header field errors in an upstream service (API Gateway). I believe this occurs when trying to establish a WebSocket connection. All pods are up and running.\r\nInterestingly, after refreshing the browser several times, the connection often works fine again. The issue seems to disappear after applying an EnvoyFilter, or changing the port names in the service definition from http2 to http.However, I'm troubled by the intermittent nature of the problem. If this was strictly an HTTP/2 issue, I would expect it to occur consistently.\r\n\r\nThe EnvoyFilter that seems to resolve the issue: \r\n\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: sef-http2-websockets\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      app: api-gateway\r\n  configPatches:\r\n    - applyTo: NETWORK_FILTER\r\n      match:\r\n        context: SIDECAR_INBOUND\r\n        proxy:\r\n          proxyVersion: '^1\\.*.*'\r\n        listener:\r\n          filterChain:\r\n            filter:\r\n              name: envoy.filters.network.http_connection_manager\r\n      patch:\r\n        operation: MERGE\r\n        value:\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n            http2_protocol_options:\r\n              allow_connect: true\r\n```\r\n\r\nHere's the service configurations. \r\nBefore changing the ports:\r\napi-gateway      ClusterIP     x.x.x.x                    http2-proxy:8080►0 http-pm-tls:8999►0 http2-forward:8888►0\r\nAfter changing the ports\r\napi-gateway      ClusterIP     x.x.x.x                    http-proxy:8080►0 http-pm-tls:8999►0 http-forward:8888►0\r\n\r\nLogs in my upstream envoy:\r\nenvoy http2\\t[Tags: \\\"ConnectionId\\\":\\\"29367\\\"] invalid http2: **Invalid HTTP header field was received: frame type: 1, stream: 3, name: [:protocol], value: [websocket]**\r\nhttp2\\t[Tags: \\\"ConnectionId\\\":\\\"29367\\\"] invalid frame: Invalid HTTP header field was received on stream 3\r\nhttp\\t[Tags: \\\"ConnectionId\\\":\\\"29367\\\"] dispatch error: The user callback function failed\r\n\r\nAround the same time in ingress gateway - down stream service:\r\n\"GET [priv100]/log/viewer/[/priv100] **HTTP/1.1\\\" 503 UC upstream_reset_before_response_started{connection_termination}** - \\\"-\\\" 0 95 1002 - \\\"[priv8]x.x.x.x[/priv8]\\\" \\\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\\\" \\\"17bfbac0-0424-494a-9be6-bf51a32cb346\\\" \\\"xyz.com\\\" \\\"[priv8]x.x.x.x[/priv8]:8080\\\" outbound|8080||api-gateway.deploy.svc.cluster.local [priv8]x.x.x.x[/priv8]:49186 [priv8]x.x.x.x[/priv8]:8443 [priv8]x.x.x.x[/priv8]:53300 xyz.com\r\n\"GET [priv100]/log/viewer/[/priv100] **HTTP/2\\\" **503 UC upstream_reset_before_response_started{connection_termination**}** - \\\"-\\\" 0 95 1420 - \\\"[priv8]x.x.x.x[/priv8]\\\" \\\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\\\" \\\"d1088bff-d57b-4d4b-98d5-b287bc79916b\\\" \\\"xyz.com\\\" \\\"[priv8]x.x.x.x[/priv8]:8080\\\" outbound|8080||api-gateway.deploy.svc.cluster.local [priv8]x.x.x.x[/priv8]:49186 [priv8]x.x.x.x[/priv8]:8443 [priv8]x.x.x.x[/priv8]:2009 xyz.com\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/35296/comments",
    "author": "sahityacodes",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2024-07-22T23:35:20Z",
        "body": "You may get better results asking on the Istio forums. Here not many may be familiar with Istio configuration or how to make it work with WebSockets."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-22T00:03:27Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-29T00:03:41Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 35260,
    "title": "Why not add POST REST API method in admin module to support the dynamic configuration resources (such as listeners) update ?",
    "created_at": "2024-07-18T13:50:37Z",
    "closed_at": "2024-08-26T13:52:10Z",
    "labels": [
      "question",
      "stale",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/35260",
    "body": "\r\n*Title*: *Proposal to add new admin REST API to support the dynamic resource update via POST methods*\r\n\r\n*Description*:\r\n>We know that the envoy admin rest api have some http get methods to get the configuration dumps of resources such as the listeners, So why not add some post method to support the dynamic resources update via admin REST API ?\r\nSo that it's very easy to update the LDS configurations via admin module.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/35260/comments",
    "author": "wufanqqfsc",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2024-07-19T14:26:39Z",
        "body": "Hi @wufanqqfsc ,\r\n\r\nI think this would create a large amount of complexity e.g. what happens in the cases where you're using a management server or file system xds which contradicts the POST provided? How would you monitor this fleetwide to know what configs your proxys are running? What happens if the proxy restarts (either from crash, hot restart -- how do we make that persist?\r\n\r\nI'd suggest looking at file system xds if you aren't using a management server for your envoys and seeing if you can use that for your usecase. "
      },
      {
        "user": "wufanqqfsc",
        "created_at": "2024-07-20T13:08:20Z",
        "body": "@KBaichoo  thanks for your comments here . We are now using the file system xds, \r\nbut as we know that we can't know the file based xds update results. This is not friendly for the file based xds.\r\n\r\nAnd in our case we were not using a xds server because our envoy were running in some embedded system for some memory limitation. And also there is no mature XDS server which was implemented via c++. So using xds server means we should totally re-write a c++ version xDS server, so the cost is a little big...\r\n\r\nSo the best way may be we add some Post method for the Envoy admin and then hander the resource update via http message and then response the xds update result to the client. \r\n\r\nThis is just our idea right now ,and so i ask this question for more suggestions from the envoy community. \r\n\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-19T16:01:31Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      }
    ]
  },
  {
    "number": 34878,
    "title": "Callback for request cancelled from client",
    "created_at": "2024-06-25T03:13:39Z",
    "closed_at": "2024-08-02T08:01:27Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/34878",
    "body": "If the request is cancelled from client, is there callback functions we can use to do some cleanup work?\r\n\r\nStreamFilter only provide decode/encode header/data/trailer, I don't see any function for request cancel.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/34878/comments",
    "author": "zhiyong-gayang",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2024-06-25T19:39:26Z",
        "body": "There are callback events for resets which should work for this."
      },
      {
        "user": "zhiyong-gayang",
        "created_at": "2024-06-26T01:16:57Z",
        "body": "@mattklein123 Can you share an example how to use reset events in network/http filters?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-26T04:01:26Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-02T08:01:27Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 34686,
    "title": "Envoy is not terminating the TLS connection at listeners",
    "created_at": "2024-06-11T16:06:15Z",
    "closed_at": "2024-07-20T16:01:22Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/34686",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *Envoy is not terminating the TLS connection at listeners*\r\n\r\n*Description*:\r\n>envoy is not terminating the TLS connection at listeners instead it forwards to the services. Due to this, my service is failing as it only accepts the plain TCP connection. The request failed with TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER\r\n\r\nI have envoy running as a sidecar in both my_service_1 and my_service_2 instances as below, my_service_1(tcp on port 1062) --> my_service_1_envoy_sidecar(tls on port 2062) --> my_service_2_envoy_sidecar(tls on port 3062) --> my_service_2(tcp port 4062)\r\n\r\nbelow config used for both services:\r\n\r\n```\r\nstatic_resources:\r\n  listeners:\r\n    - name: incoming_request_to_local_my_service\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: ENVOY_LOCAL_PORT #(2062 for service_1 and 3062 for service_2)\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.tcp_proxy\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n                stat_prefix: tcp_proxy\r\n                cluster: local_my_service\r\n                access_log:\r\n                  - name: envoy.access_loggers.stdout\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\r\n          transport_socket:\r\n            name: envoy.transport_sockets.tls\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n              common_tls_context:\r\n                tls_certificates:\r\n                  - certificate_chain: { \"filename\": \"/config/cert/server-cert.pem\" }\r\n                    private_key: { \"filename\": \"/config/cert/server-key.pem\" }\r\n    - name: outgoing_request_to_remote_my_service\r\n      address:\r\n        socket_address:\r\n          address: LOCAL_MY_SERVICE_IP_ADDR #(current host ip address)\r\n          port_value: OTHER_MY_SERVICE_ENVOY_PORT #(3062 for service_1 and 2062 for service_2)\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.tcp_proxy\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n                stat_prefix: tcp_proxy\r\n                cluster: remote_my_service\r\n                access_log:\r\n                  - name: envoy.access_loggers.stdout\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\r\n  clusters:\r\n    - name: local_my_service\r\n      type: static\r\n      connect_timeout: 5s\r\n      dns_lookup_family: V4_ONLY\r\n      load_assignment:\r\n        cluster_name: local_my_service\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: LOCAL_SERVICE_IP_ADDR #(current host ip address)\r\n                      port_value: MY_SERVICE_PORT #(1062 for service_1 and 4062 for service_2)\r\n    - name: remote_my_service\r\n      type: static\r\n      connect_timeout: 5s\r\n      dns_lookup_family: V4_ONLY\r\n      load_assignment:\r\n        cluster_name: remote_my_service\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: REMOTE_SERVICE_IP_ADDR #(remote my_service host ip address)\r\n                      port_value: REMOTE_ENVOY_PORT #(3062 for service_1 and 2062 for service_2)\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n          common_tls_context:\r\n            validation_context:\r\n              trusted_ca:\r\n                filename: /config/cert/ca-cert.pem\r\nadmin:\r\n  access_log_path: /var/log/admin_access.log\r\n  address:\r\n    socket_address:\r\n      address: 127.0.0.1\r\n      port_value: 9908\r\n\r\n```\r\n\r\n\r\nThe request failed with errors:\r\n\r\n```\r\n[2024-06-11 11:05:59.520][994][debug][main] [external/envoy/source/server/server.cc:209] flushing stats\r\n[2024-06-11 11:06:04.521][994][debug][main] [external/envoy/source/server/server.cc:209] flushing stats\r\n[2024-06-11 11:06:04.781][1012][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:249] [C0] new tcp proxy session\r\n[2024-06-11 11:06:04.781][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:349] [C0] readDisable: disable=true disable_count=0 state=0 buffer_length=0\r\n[2024-06-11 11:06:04.782][1012][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:397] [C0] Creating connection to cluster local_my_service\r\n[2024-06-11 11:06:04.785][1012][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:98] creating a new connection\r\n[2024-06-11 11:06:04.786][1012][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:383] [C1] connecting\r\n[2024-06-11 11:06:04.786][1012][debug][connection] [external/envoy/source/common/network/connection_impl.cc:860] [C1] connecting to 200.0.0.2:4062\r\n[2024-06-11 11:06:04.786][1012][debug][connection] [external/envoy/source/common/network/connection_impl.cc:876] [C1] connection in progress\r\n[2024-06-11 11:06:04.787][1012][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:125] queueing request due to no available connections\r\n[2024-06-11 11:06:04.787][1012][debug][conn_handler] [external/envoy/source/server/active_tcp_listener.cc:328] [C0] new connection\r\n[2024-06-11 11:06:04.787][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C0] socket event: 2\r\n[2024-06-11 11:06:04.787][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C0] write ready\r\n[2024-06-11 11:06:04.814][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_handshaker.cc:237] [C0] ssl error occurred while read: WANT_READ\r\n[2024-06-11 11:06:04.814][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C1] socket event: 2\r\n[2024-06-11 11:06:04.814][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C1] write ready\r\n[2024-06-11 11:06:04.814][1012][debug][connection] [external/envoy/source/common/network/connection_impl.cc:665] [C1] connected\r\n[2024-06-11 11:06:04.814][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:410] [C1] raising connection event 2\r\n[2024-06-11 11:06:04.815][1012][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:303] [C1] assigning connection\r\n[2024-06-11 11:06:04.815][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:349] [C0] readDisable: disable=false disable_count=1 state=0 buffer_length=0\r\n[2024-06-11 11:06:04.815][1012][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:660] [C0] TCP:onUpstreamEvent(), requestedServerName: \r\n[2024-06-11 11:06:04.816][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C0] socket event: 2\r\n[2024-06-11 11:06:04.816][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C0] write ready\r\n[2024-06-11 11:06:04.816][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_handshaker.cc:237] [C0] ssl error occurred while read: WANT_READ\r\n[2024-06-11 11:06:04.831][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C0] socket event: 3\r\n[2024-06-11 11:06:04.831][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C0] write ready\r\n[2024-06-11 11:06:04.838][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:410] [C0] raising connection event 2\r\n[2024-06-11 11:06:04.839][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:585] [C0] read ready. dispatch_buffered_data=false\r\n[2024-06-11 11:06:04.839][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:92] [C0] ssl read returns: -1\r\n[2024-06-11 11:06:04.839][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:132] [C0] ssl error occurred while read: WANT_READ\r\n[2024-06-11 11:06:04.839][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:168] [C0] ssl read 0 bytes\r\n[2024-06-11 11:06:04.845][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C0] socket event: 3\r\n[2024-06-11 11:06:04.845][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C0] write ready\r\n[2024-06-11 11:06:04.845][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:585] [C0] read ready. dispatch_buffered_data=false\r\n[2024-06-11 11:06:04.846][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:92] [C0] ssl read returns: 1161\r\n[2024-06-11 11:06:04.846][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:92] [C0] ssl read returns: -1\r\n[2024-06-11 11:06:04.846][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:132] [C0] ssl error occurred while read: WANT_READ\r\n[2024-06-11 11:06:04.846][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:168] [C0] ssl read 1161 bytes\r\n[2024-06-11 11:06:04.846][1012][trace][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:570] [C0] downstream connection received 1161 bytes, end_stream=false\r\n[2024-06-11 11:06:04.846][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:470] [C1] writing 1161 bytes, end_stream false\r\n[2024-06-11 11:06:04.847][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C1] socket event: 2\r\n[2024-06-11 11:06:04.847][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C1] write ready\r\n[2024-06-11 11:06:04.847][1012][trace][connection] [external/envoy/source/common/network/raw_buffer_socket.cc:67] [C1] write returns: 1161\r\n[2024-06-11 11:06:04.878][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C1] socket event: 3\r\n[2024-06-11 11:06:04.878][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C1] write ready\r\n[2024-06-11 11:06:04.878][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:585] [C1] read ready. dispatch_buffered_data=false\r\n[2024-06-11 11:06:04.879][1012][trace][connection] [external/envoy/source/common/network/raw_buffer_socket.cc:24] [C1] read returns: 402\r\n[2024-06-11 11:06:04.879][1012][trace][connection] [external/envoy/source/common/network/raw_buffer_socket.cc:38] [C1] read error: Resource temporarily unavailable\r\n[2024-06-11 11:06:04.879][1012][trace][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:616] [C0] upstream connection received 402 bytes, end_stream=false\r\n[2024-06-11 11:06:04.879][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:470] [C0] writing 402 bytes, end_stream false\r\n[2024-06-11 11:06:04.880][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C0] socket event: 2\r\n[2024-06-11 11:06:04.880][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C0] write ready\r\n[2024-06-11 11:06:04.880][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:254] [C0] ssl write returns: 402\r\n[2024-06-11 11:06:05.304][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C1] socket event: 3\r\n[2024-06-11 11:06:05.304][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C1] write ready\r\n[2024-06-11 11:06:05.305][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:585] [C1] read ready. dispatch_buffered_data=false\r\n[2024-06-11 11:06:05.305][1012][trace][connection] [external/envoy/source/common/network/raw_buffer_socket.cc:24] [C1] read returns: 603\r\n[2024-06-11 11:06:05.305][1012][trace][connection] [external/envoy/source/common/network/raw_buffer_socket.cc:38] [C1] read error: Resource temporarily unavailable\r\n[2024-06-11 11:06:05.305][1012][trace][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:616] [C0] upstream connection received 603 bytes, end_stream=false\r\n[2024-06-11 11:06:05.305][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:470] [C0] writing 603 bytes, end_stream false\r\n[2024-06-11 11:06:05.305][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C0] socket event: 2\r\n[2024-06-11 11:06:05.305][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C0] write ready\r\n[2024-06-11 11:06:05.305][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:254] [C0] ssl write returns: 603\r\n[2024-06-11 11:06:06.333][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C1] socket event: 3\r\n[2024-06-11 11:06:06.333][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C1] write ready\r\n[2024-06-11 11:06:06.333][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:585] [C1] read ready. dispatch_buffered_data=false\r\n[2024-06-11 11:06:06.333][1012][trace][connection] [external/envoy/source/common/network/raw_buffer_socket.cc:24] [C1] read returns: 766\r\n[2024-06-11 11:06:06.333][1012][trace][connection] [external/envoy/source/common/network/raw_buffer_socket.cc:38] [C1] read error: Resource temporarily unavailable\r\n[2024-06-11 11:06:06.333][1012][trace][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:616] [C0] upstream connection received 766 bytes, end_stream=false\r\n[2024-06-11 11:06:06.333][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:470] [C0] writing 766 bytes, end_stream false\r\n[2024-06-11 11:06:06.334][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C0] socket event: 2\r\n[2024-06-11 11:06:06.334][1012][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C0] write ready\r\n[2024-06-11 11:06:06.334][1012][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:254] [C0] ssl write returns: 766\r\n[2024-06-11 11:06:06.470][1022][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:249] [C2] new tcp proxy session\r\n[2024-06-11 11:06:06.470][1022][trace][connection] [external/envoy/source/common/network/connection_impl.cc:349] [C2] readDisable: disable=true disable_count=0 state=0 buffer_length=0\r\n[2024-06-11 11:06:06.470][1022][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:397] [C2] Creating connection to cluster local_my_service\r\n[2024-06-11 11:06:06.470][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:98] creating a new connection\r\n[2024-06-11 11:06:06.470][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:383] [C3] connecting\r\n[2024-06-11 11:06:06.470][1022][debug][connection] [external/envoy/source/common/network/connection_impl.cc:860] [C3] connecting to 200.0.0.2:4062\r\n[2024-06-11 11:06:06.470][1022][debug][connection] [external/envoy/source/common/network/connection_impl.cc:876] [C3] connection in progress\r\n[2024-06-11 11:06:06.470][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:125] queueing request due to no available connections\r\n[2024-06-11 11:06:06.471][1022][debug][conn_handler] [external/envoy/source/server/active_tcp_listener.cc:328] [C2] new connection\r\n[2024-06-11 11:06:06.471][1022][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C2] socket event: 2\r\n[2024-06-11 11:06:06.471][1022][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C2] write ready\r\n[2024-06-11 11:06:06.471][1022][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_handshaker.cc:237] [C2] ssl error occurred while read: SSL\r\n[2024-06-11 11:06:06.472][1022][debug][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:219] [C2] TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER\r\n[2024-06-11 11:06:06.472][1022][debug][connection] [external/envoy/source/common/network/connection_impl.cc:243] [C2] closing socket: 0\r\n[2024-06-11 11:06:06.473][1022][debug][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:219] [C2] TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER\r\n[2024-06-11 11:06:06.473][1022][trace][connection] [external/envoy/source/common/network/connection_impl.cc:410] [C2] raising connection event 0\r\n[2024-06-11 11:06:06.473][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:223] canceling pending request\r\n[2024-06-11 11:06:06.473][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:231] canceling pending connection\r\n[2024-06-11 11:06:06.473][1022][debug][connection] [external/envoy/source/common/network/connection_impl.cc:133] [C3] closing data_to_write=0 type=1\r\n[2024-06-11 11:06:06.473][1022][debug][connection] [external/envoy/source/common/network/connection_impl.cc:243] [C3] closing socket: 1\r\n[2024-06-11 11:06:06.474][1022][trace][connection] [external/envoy/source/common/network/connection_impl.cc:410] [C3] raising connection event 1\r\n[2024-06-11 11:06:06.474][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:140] [C3] client disconnected\r\n[2024-06-11 11:06:06.474][1022][trace][main] [external/envoy/source/common/event/dispatcher_impl.cc:252] item added to deferred deletion list (size=1)\r\n[2024-06-11 11:06:06.474][1022][debug][conn_handler] [external/envoy/source/server/active_tcp_listener.cc:76] [C2] adding to cleanup list\r\n[2024-06-11 11:06:06.474][1022][trace][main] [external/envoy/source/common/event/dispatcher_impl.cc:252] item added to deferred deletion list (size=2)\r\n[2024-06-11 11:06:06.474][1022][trace][main] [external/envoy/source/common/event/dispatcher_impl.cc:252] item added to deferred deletion list (size=3)\r\n[2024-06-11 11:06:06.475][1022][trace][main] [external/envoy/source/common/event/dispatcher_impl.cc:115] clearing deferred deletion list (size=3)\r\n[2024-06-11 11:06:06.475][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:255] [C3] connection destroyed\r\n[2024-06-11 11:06:06.985][1022][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:249] [C4] new tcp proxy session\r\n[2024-06-11 11:06:06.985][1022][trace][connection] [external/envoy/source/common/network/connection_impl.cc:349] [C4] readDisable: disable=true disable_count=0 state=0 buffer_length=0\r\n[2024-06-11 11:06:06.985][1022][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:397] [C4] Creating connection to cluster local_my_service\r\n[2024-06-11 11:06:06.986][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:98] creating a new connection\r\n[2024-06-11 11:06:06.986][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:383] [C5] connecting\r\n[2024-06-11 11:06:06.986][1022][debug][connection] [external/envoy/source/common/network/connection_impl.cc:860] [C5] connecting to 200.0.0.2:4062\r\n[2024-06-11 11:06:06.986][1022][debug][connection] [external/envoy/source/common/network/connection_impl.cc:876] [C5] connection in progress\r\n[2024-06-11 11:06:06.986][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:125] queueing request due to no available connections\r\n[2024-06-11 11:06:06.986][1022][debug][conn_handler] [external/envoy/source/server/active_tcp_listener.cc:328] [C4] new connection\r\n[2024-06-11 11:06:06.986][1022][trace][connection] [external/envoy/source/common/network/connection_impl.cc:547] [C4] socket event: 2\r\n[2024-06-11 11:06:06.986][1022][trace][connection] [external/envoy/source/common/network/connection_impl.cc:656] [C4] write ready\r\n[2024-06-11 11:06:06.986][1022][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_handshaker.cc:237] [C4] ssl error occurred while read: SSL\r\n[2024-06-11 11:06:06.986][1022][debug][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:219] [C4] TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER\r\n[2024-06-11 11:06:06.987][1022][debug][connection] [external/envoy/source/common/network/connection_impl.cc:243] [C4] closing socket: 0\r\n[2024-06-11 11:06:06.987][1022][debug][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:219] [C4] TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER\r\n[2024-06-11 11:06:06.987][1022][trace][connection] [external/envoy/source/common/network/connection_impl.cc:410] [C4] raising connection event 0\r\n[2024-06-11 11:06:06.987][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:223] canceling pending request\r\n[2024-06-11 11:06:06.987][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:231] canceling pending connection\r\n[2024-06-11 11:06:06.987][1022][debug][connection] [external/envoy/source/common/network/connection_impl.cc:133] [C5] closing data_to_write=0 type=1\r\n[2024-06-11 11:06:06.987][1022][debug][connection] [external/envoy/source/common/network/connection_impl.cc:243] [C5] closing socket: 1\r\n[2024-06-11 11:06:06.987][1022][trace][connection] [external/envoy/source/common/network/connection_impl.cc:410] [C5] raising connection event 1\r\n[2024-06-11 11:06:06.987][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:140] [C5] client disconnected\r\n[2024-06-11 11:06:06.987][1022][trace][main] [external/envoy/source/common/event/dispatcher_impl.cc:252] item added to deferred deletion list (size=1)\r\n[2024-06-11 11:06:06.987][1022][debug][conn_handler] [external/envoy/source/server/active_tcp_listener.cc:76] [C4] adding to cleanup list\r\n[2024-06-11 11:06:06.987][1022][trace][main] [external/envoy/source/common/event/dispatcher_impl.cc:252] item added to deferred deletion list (size=2)\r\n[2024-06-11 11:06:06.987][1022][trace][main] [external/envoy/source/common/event/dispatcher_impl.cc:252] item added to deferred deletion list (size=3)\r\n[2024-06-11 11:06:06.987][1022][trace][main] [external/envoy/source/common/event/dispatcher_impl.cc:115] clearing deferred deletion list (size=3)\r\n[2024-06-11 11:06:06.987][1022][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:255] [C5] connection destroyed\r\n[2024-06-11 11:06:09.525][994][debug][main] [external/envoy/source/server/server.cc:209] flushing stats\r\n[2024-06-11 11:06:14.531][994][debug][main] [external/envoy/source/server/server.cc:209] flushing stats\r\n[2024-06-11 11:06:19.533][994][debug][main] [external/envoy/source/server/server.cc:209] flushing stats\r\n\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/34686/comments",
    "author": "SubhashC37",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-13T16:01:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-20T16:01:22Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 34667,
    "title": "Why can't Envoy log the Connection in header",
    "created_at": "2024-06-11T10:11:19Z",
    "closed_at": "2024-07-20T16:01:20Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/34667",
    "body": "My clients send HTTP requests to my Envoy containing the `Connection: keep-alive` in header.\r\n\r\nI'm trying to log the header so I configure the log as below:\r\n```\r\n[{name: envoy.access_loggers.stdout, typed_config: {\"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog, log_format: {text_format_source: {inline_string: \"%REQ(CONNECTION)% ... \r\n```\r\nAs you see, I tried to use `%REQ(CONNECTION)%` to log the `Connection` in header of request. However, it seems that I always get an empty output `-`. I'm sure that the client has set `Connection: keep-alive` in its request header.\r\n\r\nWhy can't Envoy log the Connection in header?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/34667/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2024-06-13T12:26:23Z",
        "body": "Envoy clears hop by hop headers per the HTTP spec and it's almost certainly doing so before logging. cc @yanavlasov "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-13T16:01:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-20T16:01:19Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 34399,
    "title": "May I know about the performance of the gRPC-JSON Transcoder ?",
    "created_at": "2024-05-29T00:19:07Z",
    "closed_at": "2024-07-08T04:01:16Z",
    "labels": [
      "question",
      "area/perf",
      "stale",
      "area/grpc-transcoding"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/34399",
    "body": "*Title*: *May I know about the performance performance of the grpcson trancoder?*\r\n\r\n*Description*:\r\n>\r\nA client -> (rest) -> B server\r\nand\r\nA Client -> (rest) -> envoy -> (transcoder) -> (grpc) -> B Server\r\n\r\nWhen I did the performance test, it was confirmed that the performance improved when the response size was small. (About 1.5 times the TPS?)\r\n\r\nHowever, since envoy is unable to decompress the gRPC response, we have identified the problem of increasing the response size in the end.\r\n\r\nHave you ever done a performance test on this?\r\n\r\nAnd how much performance penalty will there be if Transcoder is used?\r\n\r\nex)\r\n\r\nA Client -> (grpc) > B Client\r\nand\r\nA Client -> (rest) -> envoy -> (transcoder) -> (grpc) -> B Server\r\n\r\n(The above configuration is a simple example, and I think only communication should be changed while being configured the same considering network hops.)",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/34399/comments",
    "author": "eottabom",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-01T00:03:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-08T04:01:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 34226,
    "title": "Error when setting up Envoy as a forward proxy.",
    "created_at": "2024-05-17T09:21:14Z",
    "closed_at": "2024-08-10T12:01:21Z",
    "labels": [
      "question",
      "stale",
      "area/configuration",
      "area/forward proxy"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/34226",
    "body": "*Error when setting up Envoy as a forward proxy.*\r\n\r\nHi there, i want to use envoy as a proxy to handle all out going traffic. This is my set up:\r\n\r\nEnvoy config:\r\n```yaml\r\nadmin:\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 127.0.0.1\r\n      port_value: 9901\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 10000\r\n    listener_filters:\r\n    - name: envoy.filters.listener.tls_inspector\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          access_log:\r\n          - name: envoy.access_loggers.file\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n              path: /dev/stdout\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: dynamic_forward_proxy_cluster\r\n          http_filters:\r\n          - name: envoy.filters.http.dynamic_forward_proxy\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.dynamic_forward_proxy.v3.FilterConfig\r\n              dns_cache_config:\r\n                name: dynamic_forward_proxy_cache_config\r\n                dns_lookup_family: V4_ONLY\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n  clusters:\r\n  - name: dynamic_forward_proxy_cluster\r\n    lb_policy: CLUSTER_PROVIDED\r\n    connect_timeout: 5s\r\n    http2_protocol_options: {}\r\n    cluster_type:\r\n      name: envoy.clusters.dynamic_forward_proxy\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.clusters.dynamic_forward_proxy.v3.ClusterConfig\r\n        dns_cache_config:\r\n          name: dynamic_forward_proxy_cache_config\r\n          dns_lookup_family: V4_ONLY\r\n        allow_insecure_cluster_options: true\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n        common_tls_context:\r\n          tls_params:\r\n            tls_minimum_protocol_version: TLSv1_2\r\n            tls_maximum_protocol_version: TLSv1_3\r\n          alpn_protocols:\r\n            - h2\r\n            - http/1.1\r\n          validation_context:\r\n            trusted_ca: {filename: /etc/ssl/certs/ca-certificates.crt}\r\n```\r\nAnd my `iptables` run:\r\n```bash\r\niptables -t nat -N PROXY_INIT_OUTPUT\r\niptables -t nat -A PROXY_INIT_OUTPUT -o lo -j RETURN\r\niptables -t nat -A PROXY_INIT_OUTPUT -p tcp -j REDIRECT --to-port 10000\r\niptables -t nat -A OUTPUT -j PROXY_INIT_OUTPUT\r\n```\r\nWhen I try something like `curl google.com` it returns this: \r\n\r\n```\r\n[2024-05-17 08:16:56.274][17][trace][connection] [source/common/network/connection_impl.cc:474] [Tags: \"ConnectionId\":\"0\"] raising connection event 2\r\n[2024-05-17 08:16:56.274][17][trace][connection] [source/common/network/connection_impl.cc:619] [Tags: \"ConnectionId\":\"0\"] socket event: 3\r\n[2024-05-17 08:16:56.274][17][trace][connection] [source/common/network/connection_impl.cc:742] [Tags: \"ConnectionId\":\"0\"] write ready\r\n[2024-05-17 08:16:56.274][17][trace][connection] [source/common/network/connection_impl.cc:659] [Tags: \"ConnectionId\":\"0\"] read ready. dispatch_buffered_data=0\r\n[2024-05-17 08:16:56.274][17][trace][connection] [source/common/network/raw_buffer_socket.cc:25] [Tags: \"ConnectionId\":\"0\"] read returns: 73\r\n[2024-05-17 08:16:56.274][17][trace][connection] [source/common/network/raw_buffer_socket.cc:39] [Tags: \"ConnectionId\":\"0\"] read error: Resource temporarily unavailable, code: 0\r\n[2024-05-17 08:16:56.274][17][debug][connection] [./source/common/network/connection_impl.h:98] [Tags: \"ConnectionId\":\"0\"] current connecting state: false\r\n[2024-05-17 08:16:56.275][1][debug][upstream] [source/extensions/clusters/dynamic_forward_proxy/cluster.cc:300] Adding host info for google.com:443\r\n[2024-05-17 08:16:56.275][1][debug][upstream] [source/extensions/clusters/dynamic_forward_proxy/cluster.cc:279] adding new dfproxy cluster host 'google.com:443'\r\n[2024-05-17 08:16:56.275][1][debug][upstream] [source/common/upstream/upstream_impl.cc:458] transport socket match, socket default selected for host with address 142.251.175.138:443\r\n[2024-05-17 08:16:56.275][10][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][11][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][17][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][13][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][11][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][17][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][13][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][10][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][15][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][22][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][15][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][25][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][1][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][1][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][25][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][18][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][28][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][18][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][28][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][32][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][32][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][24][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][22][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][24][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][29][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1542] membership update for TLS cluster dynamic_forward_proxy_cluster added 1 removed 0\r\n[2024-05-17 08:16:56.275][29][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1548] re-creating local LB for TLS cluster dynamic_forward_proxy_cluster\r\n[2024-05-17 08:16:56.275][17][debug][connection] [./source/common/network/connection_impl.h:98] [Tags: \"ConnectionId\":\"2\"] current connecting state: true\r\n[2024-05-17 08:16:56.275][17][debug][connection] [source/common/network/connection_impl.cc:1021] [Tags: \"ConnectionId\":\"2\"] connecting to 142.251.175.138:443\r\n[2024-05-17 08:16:56.275][17][debug][connection] [source/common/network/connection_impl.cc:1040] [Tags: \"ConnectionId\":\"2\"] connection in progress\r\n[2024-05-17 08:16:56.276][17][trace][connection] [source/common/network/connection_impl.cc:619] [Tags: \"ConnectionId\":\"2\"] socket event: 2\r\n[2024-05-17 08:16:56.276][17][trace][connection] [source/common/network/connection_impl.cc:742] [Tags: \"ConnectionId\":\"2\"] write ready\r\n[2024-05-17 08:16:56.276][17][debug][connection] [source/common/network/connection_impl.cc:751] [Tags: \"ConnectionId\":\"2\"] connected\r\n[2024-05-17 08:16:56.276][17][trace][connection] [source/common/tls/ssl_handshaker.cc:93] [Tags: \"ConnectionId\":\"2\"] ssl error occurred while read: WANT_READ\r\n[2024-05-17 08:16:56.276][15][trace][connection] [source/common/network/connection_impl.cc:474] [Tags: \"ConnectionId\":\"3\"] raising connection event 2\r\n[2024-05-17 08:16:56.276][15][trace][connection] [source/common/network/connection_impl.cc:619] [Tags: \"ConnectionId\":\"3\"] socket event: 3\r\n[2024-05-17 08:16:56.276][15][trace][connection] [source/common/network/connection_impl.cc:742] [Tags: \"ConnectionId\":\"3\"] write ready\r\n[2024-05-17 08:16:56.276][15][trace][connection] [source/common/network/connection_impl.cc:659] [Tags: \"ConnectionId\":\"3\"] read ready. dispatch_buffered_data=0\r\n[2024-05-17 08:16:56.276][15][trace][connection] [source/common/network/raw_buffer_socket.cc:25] [Tags: \"ConnectionId\":\"3\"] read returns: 247\r\n[2024-05-17 08:16:56.276][15][trace][connection] [source/common/network/raw_buffer_socket.cc:39] [Tags: \"ConnectionId\":\"3\"] read error: Resource temporarily unavailable, code: 0\r\n[2024-05-17 08:16:56.276][15][trace][connection] [source/common/network/connection_impl.cc:534] [Tags: \"ConnectionId\":\"3\"] writing 145 bytes, end_stream false\r\n[2024-05-17 08:16:56.276][15][trace][connection] [source/common/network/connection_impl.cc:534] [Tags: \"ConnectionId\":\"3\"] writing 11 bytes, end_stream false\r\n[2024-05-17T08:16:56.276Z] \"- - HTTP/1.1\" 400 DPE 0 11 0 - \"-\" \"-\" \"-\" \"-\" \"-\"\r\n```\r\n\r\nI'm just a newcomer with 1 week of reading documents. Can any one point out why I got this errors and how to fix it. I have already tried to google but nothing can help.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/34226/comments",
    "author": "ahcognmm",
    "comments": [
      {
        "user": "ravenblackx",
        "created_at": "2024-05-17T14:51:47Z",
        "body": "@phlax might be able to help, or to ping someone who is."
      },
      {
        "user": "phlax",
        "created_at": "2024-05-17T14:55:58Z",
        "body": "the problem/solution is not immediately obvious to me - but i have limited dfp knowledge\r\n\r\ncc @alyssawilk @mattklein123 as codeowners\r\n\r\n@wbpcode might also have some idea"
      },
      {
        "user": "moderation",
        "created_at": "2024-05-17T20:22:59Z",
        "body": "Not sure about your iptables stuff but here is a simplified config that I've been using for ages with all the custom access log and tracing stuff elided. I update my `git`, `rust` `apt` configs etc to leverage localhost:9904 as a proxy and it works well. You can export `HTTPS_PROXY` to point to this for adhoc. I never worked out how to have this work with HTTP/3\r\n\r\n```yaml\r\nadmin:\r\n  address:\r\n    socket_address:\r\n      address: 127.0.0.1\r\n      port_value: 9903\r\nstatic_resources:\r\n  clusters:\r\n  - cluster_type:\r\n      name: envoy.clusters.dynamic_forward_proxy\r\n      typed_config:\r\n        '@type': type.googleapis.com/envoy.extensions.clusters.dynamic_forward_proxy.v3.ClusterConfig\r\n        allow_coalesced_connections: true\r\n        dns_cache_config:\r\n          dns_lookup_family: ALL\r\n          name: dynamic_forward_proxy_cache_config\r\n    connect_timeout: 2s\r\n    dns_lookup_family: ALL\r\n    lb_policy: CLUSTER_PROVIDED\r\n    name: dynamic_forward_proxy_cluster\r\n  listeners:\r\n  - additional_addresses:\r\n    - address:\r\n        socket_address:\r\n          address: ::1\r\n          port_value: 9904\r\n    address:\r\n      socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 9904\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          '@type': type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          codec_type: AUTO\r\n          http2_protocol_options:\r\n            allow_connect: true\r\n          http_filters:\r\n          - name: envoy.filters.http.dynamic_forward_proxy\r\n            typed_config:\r\n              '@type': type.googleapis.com/envoy.extensions.filters.http.dynamic_forward_proxy.v3.FilterConfig\r\n              dns_cache_config:\r\n                dns_lookup_family: ALL\r\n                name: dynamic_forward_proxy_cache_config\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              '@type': type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - domains:\r\n              - '*'\r\n              name: local_service\r\n              routes:\r\n              - match:\r\n                  prefix: /\r\n                route:\r\n                  cluster: dynamic_forward_proxy_cluster\r\n              - match:\r\n                  connect_matcher: {}\r\n                route:\r\n                  cluster: dynamic_forward_proxy_cluster\r\n                  upgrade_configs:\r\n                  - connect_config: {}\r\n                    upgrade_type: CONNECT\r\n          stat_prefix: dynamic_forward_proxy_upgrade\r\n    name: dynamic_forward_proxy_upgrade\r\n    traffic_direction: OUTBOUND\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-06-17T00:03:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "ahcognmm",
        "created_at": "2024-06-18T03:43:07Z",
        "body": "> Not sure about your iptables stuff but here is a simplified config that I've been using for ages with all the custom access log and tracing stuff elided. I update my `git`, `rust` `apt` configs etc to leverage localhost:9904 as a proxy and it works well. You can export `HTTPS_PROXY` to point to this for adhoc. I never worked out how to have this work with HTTP/3\r\n> \r\n\r\nSorry for late reply. But it doesn't work for me. I want config envoy as a transparent proxy, which handle all routed traffic via `iptables` . I dont want manually config like `curl -x localhost:9904 google.com` , i want `curl google.com` still going through proxy."
      },
      {
        "user": "ahcognmm",
        "created_at": "2024-07-04T08:27:15Z",
        "body": "@wbpcode do you have any ideas?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-03T12:01:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-10T12:01:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 34210,
    "title": "Didn't find a registered implementation for 'ip-matcher' with type URL: 'xds.type.matcher.v3.IPMatcher'",
    "created_at": "2024-05-16T17:19:25Z",
    "closed_at": "2024-06-27T04:01:17Z",
    "labels": [
      "question",
      "stale",
      "area/matching"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/34210",
    "body": "I tried to use an `IPMatche` cutomMatch within the RBAC filter for the IP allow/deny list, but got the following warning:\r\n\r\n```\r\nDidn't find a registered implementation for 'ip-matcher' with type URL: 'xds.type.matcher.v3.IPMatcher'\r\n```\r\n\r\n```\r\n[2024-05-16 16:53:17.258][1][warning][config] [source/extensions/config_subscription/grpc/grpc_subscription_impl.cc:138] gRPC config for type.googleapis.com/envoy.config.route.v3.RouteConfiguration rejected: Didn't find a registered implementation for 'ip-matcher' with type URL: 'xds.type.matcher.v3.IPMatcher'\r\n```\r\n\r\nThe RouteConfiguration used:\r\n\r\n```\r\n  virtualHosts:\r\n  - domains:\r\n    - www.example.com\r\n    name: envoy-gateway/gateway-1/http/www_example_com\r\n    routes:\r\n    - match:\r\n        pathSeparatedPrefix: /bar\r\n      name: httproute/default/httproute-2/rule/0/match/0/www_example_com\r\n      route:\r\n        cluster: httproute/default/httproute-2/rule/0\r\n        upgradeConfigs:\r\n        - upgradeType: websocket\r\n      typedPerFilterConfig:\r\n        envoy.filters.http.rbac:\r\n          '@type': type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBACPerRoute\r\n          rbac:\r\n            matcher:\r\n              matcherList:\r\n                matchers:\r\n                - onMatch:\r\n                    action:\r\n                      name: action\r\n                      typedConfig:\r\n                        '@type': type.googleapis.com/envoy.config.rbac.v3.Action\r\n                        name: ALLOW\r\n                  predicate:\r\n                    singlePredicate:\r\n                      customMatch:\r\n                        name: ip-matcher\r\n                        typedConfig:\r\n                          '@type': type.googleapis.com/xds.type.matcher.v3.IPMatcher\r\n                          rangeMatchers:\r\n                          - onMatch:\r\n                              action:\r\n                                name: allow\r\n                                typedConfig:\r\n                                  '@type': type.googleapis.com/envoy.config.rbac.v3.Action\r\n                                  name: ALLOW\r\n                            ranges:\r\n                            - addressPrefix: 10.0.1.0/24\r\n                              prefixLen: 24\r\n                            - addressPrefix: 10.0.2.0/24\r\n                              prefixLen: 24\r\n                      input:\r\n                        name: source-ip\r\n                        typedConfig:\r\n                          '@type': type.googleapis.com/envoy.extensions.matching.common_inputs.network.v3.SourceIPInput\r\n              onNoMatch:\r\n                action:\r\n                  name: default\r\n                  typedConfig:\r\n                    '@type': type.googleapis.com/envoy.config.rbac.v3.Action\r\n                    action: DENY\r\n                    name: DENY\r\n```\r\n\r\nDo I need to turn on some options to enable the 'xds.type.matcher.v3.IPMatcher' ?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/34210/comments",
    "author": "zhaohuabing",
    "comments": [
      {
        "user": "zhaohuabing",
        "created_at": "2024-05-16T18:30:24Z",
        "body": "I figured out: `IPMatcher` can only be used within a `matcherTree`. Is there any way I can match an IP range within a `matcherList`?"
      },
      {
        "user": "ravenblackx",
        "created_at": "2024-05-16T18:44:20Z",
        "body": "Maybe `extensions.matching.input_matchers.ip.v3.Ip` would be usable here?\r\n\r\n@aguinet ? (Also question about whether we should be consolidating IPMatchers into a common proto.)"
      },
      {
        "user": "aguinet",
        "created_at": "2024-05-20T09:23:15Z",
        "body": "I've just seen this now. I see this ticket has been closed: what's the conclusion?"
      },
      {
        "user": "ravenblackx",
        "created_at": "2024-05-20T14:22:04Z",
        "body": "I guess the closing was because `extensions.matching.input_matchers.ip.v3.Ip` solved for the original problem.\r\n\r\nMy bonus question about whether xds `IPMatcher` and envoy `matchers.ip.v3.Ip` should be consolidated remains open but isn't bothering anyone right now."
      },
      {
        "user": "zhaohuabing",
        "created_at": "2024-05-21T02:15:51Z",
        "body": "@ravenblackx reopened this one to track it."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-06-20T04:01:08Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-06-27T04:01:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 33767,
    "title": "[Questions] Why envoy access log not support size limit & rotation ?  And is there any good practice for this access log & envoy binary debug log ? ",
    "created_at": "2024-04-24T13:16:44Z",
    "closed_at": "2024-05-06T01:54:38Z",
    "labels": [
      "question",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/33767",
    "body": "Seems there is no such log file size limit & rotation configurations supported by Envoy ？ \r\n\r\nIs there any good practice for these log file collection ?  \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/33767/comments",
    "author": "wufanqqfsc",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2024-04-26T12:12:34Z",
        "body": "`logrotate` is good choice for this requirement. You can set the `postrotate` in the logrotate config to \r\n\r\n```\r\nkill -USR1 `pgrep envoy`\r\n```\r\n\r\nThen the logrotate will send a signal to envoy to force envoy to reopen the log file after logrotate moved the old one."
      },
      {
        "user": "wufanqqfsc",
        "created_at": "2024-05-06T01:54:38Z",
        "body": "> logrotate\r\n\r\nThanks very much @wbpcode  ,this logrotate seems a good choice. "
      }
    ]
  },
  {
    "number": 33279,
    "title": "max_requests behavior in cluster circuit_breaker",
    "created_at": "2024-04-02T21:28:29Z",
    "closed_at": "2024-05-10T12:01:15Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/33279",
    "body": "*Title*: *behavior of max_requests in cluster circuit_breaker*\r\n\r\n*Description*:\r\n> When max_requests limit is reached, the http request is immediately failed instead of queuing up till the max_pending_requests limit. This behavior is different from the max_connections limit wherein, on hitting the limit, the request is queued. This results in inconsistent behavior.\r\n\r\nWe noticed this while switching to http2 for upstream connections. Currently, with http1, we have following circuit breakers in place:\r\n\r\ncircuit_breakers:\r\n  thresholds:\r\n     max_connections: 128\r\n\r\nFor http2, we have changed the circuit breaker to use max_requests as follows:\r\n\r\ncircuit_breakers:\r\n  thresholds:\r\n     max_requests: 128\r\nhttp2_protocol_options: {}\r\n\r\nIn http1 case, the downstream clients could send more than 128 concurrent requests and all requests will succeed with no overflow. In http2 case, only 128 requests succeed, rest all fail with overflow. Logs show \"max streams overflow\".\r\n\r\nThe only way I can make the circuit_breaker work is with following config. I can now send more than 128 concurrent requests and all succeed.\r\ncircuit_breakers:\r\n  thresholds:\r\n     max_requests: 128\r\n     max_connections: 1\r\nhttp2_protocol_options:\r\n    max_concurrent_streams: 32\r\n\r\nmax_concurrent_streams is set to 32 because concurrency=4. However, this results in higher latency if all 128 concurrent requests come on one downstream http2 connection.\r\n\r\nWhy does envoy not queue up the request on hitting max_requests limit?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/33279/comments",
    "author": "sharma-mona",
    "comments": [
      {
        "user": "soulxu",
        "created_at": "2024-04-03T01:57:41Z",
        "body": "cc @wbpcode "
      },
      {
        "user": "soulxu",
        "created_at": "2024-04-03T01:57:59Z",
        "body": "@sharma-mona have you tried `max_pending_requests`? "
      },
      {
        "user": "sharma-mona",
        "created_at": "2024-04-03T06:23:00Z",
        "body": "@soulxu  Since max_pending_requests is not set, it defaults to 1024. Also, I have tried explicitly setting it to 2048  like so but does not help.\r\n\r\n```\r\ncircuit_breakers:\r\n   thresholds:\r\n       max_requests: 128\r\n       max_pending_requests: 2048\r\nhttp2_protocol_options: {}\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-05-03T08:01:19Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-05-10T12:01:14Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 33171,
    "title": "Http to TCP",
    "created_at": "2024-03-28T01:24:15Z",
    "closed_at": "2024-05-05T20:01:20Z",
    "labels": [
      "question",
      "stale",
      "area/tcp_proxy"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/33171",
    "body": "Hello, I've set up an envoy proxy to receive HTTP requests and forward them over TCP. The communication is successful, but the behavior is not okay. After Envoy forwards the data to the TCP server, Envoy sends a .FIN to the server, which responds with an ACK. The expectation is that the connection would be maintained so that the TCP server can return a response. I have changed various connection timeout parameters, but none have been successful.\r\n\r\n```\r\nadmin:\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 127.0.0.1\r\n      port_value: 9902\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 10000\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: ingress_http\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains:\r\n                        - \"*\"\r\n                      routes:\r\n                        - match:\r\n                            prefix: \"/\"\r\n                            headers:\r\n                              - name: \":method\"\r\n                                string_match:\r\n                                  exact: \"POST\"\r\n                          route:\r\n                            cluster: service\r\n                            upgrade_configs:\r\n                              - upgrade_type: CONNECT\r\n                                connect_config:\r\n                                  allow_post: true\r\n                http_filters:\r\n                  - name: envoy.filters.http.router\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n                http2_protocol_options:\r\n                  allow_connect: true\r\n  clusters:\r\n    - name: service\r\n      connect_timeout: 30s\r\n      type: LOGICAL_DNS\r\n      dns_lookup_family: V4_ONLY\r\n      lb_policy: ROUND_ROBIN\r\n      load_assignment:\r\n        cluster_name: service\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: 192.168.0.31\r\n                      port_value: 13370\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/33171/comments",
    "author": "evertondmorais",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2024-03-28T14:31:14Z",
        "body": "This is by design. Envoy indicates that it has no more data to send by half closing its write side of the TCP connection. This does not prevent it from receiving the response from the peer."
      },
      {
        "user": "evertondmorais",
        "created_at": "2024-03-28T16:13:25Z",
        "body": "When receiving an ACK from the server and Envoy sending a .FIN, the HTTP call is also terminated, leaving no chance for the TCP server to respond."
      },
      {
        "user": "evertondmorais",
        "created_at": "2024-03-28T16:15:11Z",
        "body": "It's not possible to modify the behavior of the TCP server. The Envoy must wait until the TCP server returns a payload with data to consider the end of the call."
      },
      {
        "user": "yanavlasov",
        "created_at": "2024-03-29T14:38:37Z",
        "body": "Sounds like the behavior of your TCP server is incorrect. Unfortunately I do not think it is feasible to make Envoy work the way your server expects."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-28T16:01:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-05-05T20:01:19Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 32951,
    "title": "Running extp roc per route or vhost",
    "created_at": "2024-03-18T06:24:52Z",
    "closed_at": "2024-04-30T04:01:56Z",
    "labels": [
      "question",
      "stale",
      "area/ext_proc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32951",
    "body": "We would like to run an ext proc for selected routes. At the beginning we would like to enable a few selected routes only (allowlist), later in the project we want to open most routes to the ext proc after testing, but still keep some unaffected (denylist). The latter I can achieve by doing the following:\r\n\r\n```\r\nhttp_filters:\r\n- name: envoy.filters.http.ext_proc\r\n  typed_config:\r\n    \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.ext_proc.v3.ExternalProcessor\"\r\n    failure_mode_allow: true\r\n    async_mode: false\r\n    message_timeout:\r\n      seconds: 5\r\n      nanos: 0\r\n    processing_mode:\r\n      request_header_mode: \"SEND\"\r\n      response_header_mode: \"SKIP\"\r\n      request_body_mode: \"NONE\"\r\n      response_body_mode: \"NONE\"\r\n      request_trailer_mode: \"SKIP\"\r\n      response_trailer_mode: \"SKIP\"\r\n    grpc_service:\r\n      google_grpc:\r\n        target_uri: \"<our service>:9090\"\r\n        stat_prefix: \"<prefix>\"\r\n```\r\n\r\nand then disabling every single vhost/route like this:\r\n\r\n```\r\nroutes:\r\n- name: root\r\n   match: { prefix: \"/\" }\r\n   route: { cluster: echo_service }\r\n  typed_per_filter_config:\r\n    envoy.filters.http.ext_proc:\r\n      \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.ext_proc.v3.ExtProcPerRoute\"\r\n      disabled: true\r\n```\r\n\r\nAny attempt to allow a few selected routes/vhosts have failed so far. \r\nI have tried applying `ExtProcPerRoute` with `disabled: true` to all VIRTUAL_HOSTs and then re-enable some routes with `disabled: false`, but \"false\" is not allowed.\r\nAlso using \"overrides\" either empty (`{}`) or with the same grpc service does not re-enable anything.\r\n\r\nIs there a simple, working approach to have ext proc on _some_ routes?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32951/comments",
    "author": "tfrokt",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2024-03-19T04:42:17Z",
        "body": "@tyxia "
      },
      {
        "user": "zengyuxing007",
        "created_at": "2024-03-24T01:03:09Z",
        "body": "@tyxia I'd be happy to look at this if you don't mind."
      },
      {
        "user": "tyxia",
        "created_at": "2024-03-24T03:08:24Z",
        "body": "@zengyuxing007  Sounds great! I have not got chance to look at this due to other internal priorities\r\n\r\nThank you!\r\n\r\n"
      },
      {
        "user": "zengyuxing007",
        "created_at": "2024-03-24T03:11:40Z",
        "body": "/assign @zengyuxing007 "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-23T04:01:05Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-30T04:01:56Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 32940,
    "title": "Why do envoy returns different errors during certificate validation when certificate is found revoked upstream vs downstream?",
    "created_at": "2024-03-16T12:22:05Z",
    "closed_at": "2024-04-26T12:01:14Z",
    "labels": [
      "question",
      "area/tls",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32940",
    "body": "*Title*: *Why do envoy returns different errors during certificate validation when certificate is found revoked upstream vs downstream?*\r\n\r\n*Description*:\r\nAssume a scenario where there are two services running with envoy side car one being a client and other being a server. Envoy configuration have crl check enabled and have respective crl information. \r\n- When the **client** certificate is revoked and the tls connection is established via envoy, the tls connection is terminated and we get following error - `Response: 503 : upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: TLS error: 268436500:SSL routines:OPENSSL_internal:SSLV3_ALERT_CERTIFICATE_REVOKED`\r\n- When the **server** certificate is revoked and the tls connection is established via envoy, the tls connection is terminated and we get following error - `Response: 503 : upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: TLS error: 268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED`\r\n\r\nWhy is there this difference of error propagation. I see that both the errors SSLV3_ALERT_CERTIFICATE_REVOKED, CERTIFICATE_VERIFY_FAILED are logged in envoy sidecar but eventually only one gets propagated till the end. \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32940/comments",
    "author": "aghatt",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2024-03-18T21:47:20Z",
        "body": "@ggreenway "
      },
      {
        "user": "ggreenway",
        "created_at": "2024-03-19T22:13:34Z",
        "body": "The specific error messages come from BoringSSL, the TLS library used by Envoy; they're not directly created by Envoy."
      },
      {
        "user": "aghatt",
        "created_at": "2024-03-20T06:33:03Z",
        "body": "@ggreenway  Thats right the errors come from the boringssl but I can see both the errors in envoy logs but only one getting propagated till the end as response to the client.\r\n`[2024-03-16 11:46:02.091][18][debug][connection] [external/envoy/source/extensions/transport_sockets/tls/cert_validator/default_validator.cc:323] verify cert failed: X509_verify_cert: certificate verification error at depth 0: certificate revoked\r\n[2024-03-16 11:46:02.091][18][debug][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:373] [C26] Async cert validation completed\r\n[2024-03-16 11:46:02.091][18][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_handshaker.cc:92] [C26] ssl error occurred while read: SSL\r\n[2024-03-16 11:46:02.091][18][debug][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:247] [C26] remote address:127.0.0.1:38266,TLS error: 268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED\r\n[2024-03-16 11:46:02.091][18][debug][connection] [external/envoy/source/common/network/connection_impl.cc:250] [C26] closing socket: 0\r\n[2024-03-16 11:46:02.091][19][trace][connection] [external/envoy/source/common/network/connection_impl.cc:575] [C25] socket event: 3\r\n[2024-03-16 11:46:02.091][19][trace][connection] [external/envoy/source/common/network/connection_impl.cc:686] [C25] write ready\r\n[2024-03-16 11:46:02.091][18][debug][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:247] [C26] remote address:127.0.0.1:38266,TLS error: 268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED\r\n[2024-03-16 11:46:02.091][19][trace][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_handshaker.cc:92] [C25] ssl error occurred while read: SSL\r\n[2024-03-16 11:46:02.091][19][debug][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:247] [C25] remote address:127.0.0.1:8084,TLS error: 268436500:SSL routines:OPENSSL_internal:SSLV3_ALERT_CERTIFICATE_REVOKED\r\n[2024-03-16 11:46:02.091][19][debug][connection] [external/envoy/source/common/network/connection_impl.cc:250] [C25] closing socket: 0\r\n[2024-03-16 11:46:02.091][19][debug][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:247] [C25] remote address:127.0.0.1:8084,TLS error: 268436500:SSL routines:OPENSSL_internal:SSLV3_ALERT_CERTIFICATE_REVOKED`\r\n\r\nLogs are same in case of both client & server but there is the difference in the way the errors are propogated. If the certificate is revoked we should get SSLV3_ALERT_CERTIFICATE_REVOKED instead of CERTIFICATE_VERIFY_FAILED when server cert is found revoked."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-19T08:01:23Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-26T12:01:14Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 32903,
    "title": "When is `ConnectionManagerImpl` destroyed",
    "created_at": "2024-03-14T11:19:03Z",
    "closed_at": "2024-03-15T00:59:00Z",
    "labels": [
      "question",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32903",
    "body": "I'm learning about metrics of Envoy and I just found `downstream_cx_destroy`. I don't understand what it means.\r\n\r\nIt seems that it will be increased when the destructor of `ConnectionManagerImpl` is called.\r\n\r\nThe class `ConnectionManagerImpl` is to manage some http connections as its name implies. But I don't know when its desctructor will be called.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32903/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2024-03-14T18:33:25Z",
        "body": "The HTTP ConnectionManagerImpl is network filter for http1 and http2 traffic.\r\nYou can imagine this filter  created after a connection is accepted by Envoy and destroyed after the downstream connection is closed.\r\n\r\nConnectionManager manages multiple http requests that are either pipelined or multiplexed in the same connection"
      }
    ]
  },
  {
    "number": 32806,
    "title": "golang-http-filter：an unknown error",
    "created_at": "2024-03-10T15:09:44Z",
    "closed_at": "2024-03-11T22:31:53Z",
    "labels": [
      "question",
      "area/golang"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32806",
    "body": "Hello, this error occurs when I use the following shell command to compile the go plugin library, I use the code from the Main branch and the envoy-release-v1.29 branch.\r\n\r\nShell command: \r\ndocker-compose -f docker-compose-go.yaml run --rm go_plugin_compile\r\n\r\nErrors:\r\n[root@MiWiFi-RA81-srv golang-http]# docker-compose -f docker-compose-go.yaml run --rm go_plugin_compile\r\n[+] Building 0.0s (0/0)                                                                                                  \r\n[+] Building 0.0s (0/0)                                                                                                  \r\n  #   github.com/envoyproxy/envoy/contrib/golang/common/go/api_impl\r\nIn file included from vendor/github.com/envoyproxy/envoy/contrib/golang/common/go/api_impl/capi_impl.go:30:\r\nvendor/github.com/envoyproxy/envoy/contrib/golang/common/go/api_impl/api.h:1:1: error: expected identifier or '(' before '.' token\r\n    1 | ../api/api.h\r\n      | ^\r\ncgo-gcc-prolog: In function '_cgo_fdb86200d1f7_Cfunc_envoyGoFilterLog':\r\ncgo-gcc-prolog:46:3: error: unknown type name 'uint32_t'\r\ncgo-gcc-prolog:53:2: warning: implicit declaration of function 'envoyGoFilterLog' [-Wimplicit-function-declaration]\r\ncgo-gcc-prolog: In function '_cgo_fdb86200d1f7_Cfunc_envoyGoFilterLogLevel':\r\ncgo-gcc-prolog:62:3: error: unknown type name 'uint32_t'\r\ncgo-gcc-prolog:68:11: warning: implicit declaration of function 'envoyGoFilterLogLevel' [-Wimplicit-function-declaration]",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32806/comments",
    "author": "Hectorsong",
    "comments": [
      {
        "user": "doujiang24",
        "created_at": "2024-03-11T00:53:59Z",
        "body": "> In file included from vendor/github.com/envoyproxy/envoy/contrib/golang/common/go/api_impl/capi_impl.go:30:\r\n\r\nThe `vendor` path looks unexpected, do you have vendor in your local side?\r\nPlease remove it and have another try."
      },
      {
        "user": "Hectorsong",
        "created_at": "2024-03-11T15:34:00Z",
        "body": "It seems that the relative path in vendor was invalid, such as \r\n(1)\"../../../../../../common/go/api/api.h\" in file \"./vendor/github.com/envoyproxy/envoy/contrib/golang/filters/http/source/go/pkg/http/api.h\",  \r\n(2) \"../api/api.h\" in file \"./vendor/github.com/envoyproxy/envoy/contrib/golang/common/go/api_impl/api.h\"\r\n\r\nThen, I copied the file \"./vendor/github.com/envoyproxy/envoy/contrib/golang/common/go/api/api.h\" to the above two paths, and the problem was solved. \r\n\r\nThanks a lot!"
      }
    ]
  },
  {
    "number": 32788,
    "title": "Redirecting message to dynamic port",
    "created_at": "2024-03-08T09:17:33Z",
    "closed_at": "2024-04-14T17:57:32Z",
    "labels": [
      "question",
      "stale",
      "area/cluster"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32788",
    "body": "I'm trying to use envoy to redirect any message recieved to the port specified wether on a gRPC field of the message or a meta field in the header, but I can't manage to do so.\r\n\r\nUsing:\r\n```\r\n{\r\n    \"name\": \"envoy.filters.http.lua\",\r\n     \"typed_config\": {\r\n          \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\",\r\n          \"inline_code\": \"function envoy_on_request(request_handle)\\n   local target_port = request_handle:headers():get(\\\"x-target-port\\\")\\n  \\n  if target_port then\\n    request_handle:headers():replace(\\\"host\\\", \\\"localhost:\\\" .. target_port)\\n  end\\nend\"\r\n          }\r\n},\r\n```\r\n\r\nI manage to change the header port, but it's not enough. I know i should change the cluster, but I don't find any way to assign the cluster port to a variable. Is there any way to do it?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32788/comments",
    "author": "vicbentuupc",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2024-03-08T15:41:46Z",
        "body": "IIUC, your ask is to dynamically route to a cluster on a certain port besides the specified addresses you have listed for the endpoints?\r\n\r\nIf there's a fixed set of ports, you could have multiple clusters and select the cluster based on that.\r\n\r\nOtherwise I don't know of a way to do what you're trying to achieve as the endpoints of a cluster contains the ports."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-07T16:01:03Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-14T17:57:31Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 32736,
    "title": "Domains with underscores return 400 Bad Request",
    "created_at": "2024-03-06T10:39:33Z",
    "closed_at": "2024-05-09T16:01:49Z",
    "labels": [
      "question",
      "stale",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32736",
    "body": "**Title:** Domains with underscores return 400 Bad Request\r\n\r\n**Description:**\r\nWhen configuring Envoy with a domain name that contains underscores, Envoy returns a `400 Bad Request` error for incoming requests. According to RFC 1035, underscores in domain names are not recommended, but they are commonly used in internal domain names and should be supported or handled gracefully.\r\n\r\n**Reproduction Steps:**\r\n1. Configure an Envoy listener with a route that specifies a domain name containing an underscore.\r\n2. Send a request to the Envoy instance targeting the domain with the underscore.\r\n3. Observe a `400 Bad Request` response.\r\n\r\n**Config Snippet:**\r\n```yaml\r\nstatic_resources:\r\n  listeners:\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: ingress_http  \r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: gm_tool\r\n                      domains: [\"gm_tool.test.local\"]\r\n                      routes:\r\n                        - match: { prefix: \"/\" }\r\n                          route: { cluster: gm_tool_cluster }\r\n                http_filters:\r\n                  - name: envoy.filters.http.router\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n  clusters:\r\n\r\n    - name: gm_tool_cluster\r\n      type: STRICT_DNS\r\n      connect_timeout: 1s\r\n      lb_policy: ROUND_ROBIN\r\n      load_assignment:\r\n        cluster_name: gm_tool_cluster\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: gm_tool.test.local\r\n                      port_value: 80\r\n```\r\n**Logs:**\r\n`[2024-03-05T20:52:45.395Z] \"- - HTTP/1.1\" 400 DPE 0 11 0 - \"-\" \"-\" \"-\" \"-\" \"-\" 10.15.38.183`\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32736/comments",
    "author": "mikezsin",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2024-03-06T15:49:16Z",
        "body": "cc @yanavlasov"
      },
      {
        "user": "KBaichoo",
        "created_at": "2024-03-06T15:49:42Z",
        "body": "@mikezsin what version of Envoy are you using?"
      },
      {
        "user": "mikezsin",
        "created_at": "2024-03-07T06:16:46Z",
        "body": "@KBaichoo  envoy-contrib-1.29.1-linux-x86_64\r\nenvoy  version: 4fda4d79d06e1bd59e591be3f348223495083648/1.29.1/Clean/RELEASE/BoringSSL"
      },
      {
        "user": "mikezsin",
        "created_at": "2024-04-02T09:22:53Z",
        "body": "Hi, any updates? "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-05-02T12:01:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-05-09T16:01:47Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "mikezsin",
        "created_at": "2024-05-13T07:48:30Z",
        "body": "help wanted"
      }
    ]
  },
  {
    "number": 32683,
    "title": "\"Stream XXX not found when consuming YYYY bytes\" in Envoy logs after upgrade to 1.29.1",
    "created_at": "2024-03-04T11:26:36Z",
    "closed_at": "2024-04-10T20:01:31Z",
    "labels": [
      "question",
      "stale",
      "area/http",
      "quiche"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32683",
    "body": "*Description*:\r\n\r\nHi. We've upgraded our Envoy 1.24.12 -> 1.29.1. After upgrade we've discovered such logs that appeared:\r\n`[2024-02-24 20:57:20.728][209699][error][quic] [external/com_github_google_quiche/quiche/http2/adapter/oghttp2_session.cc:515] Stream 371 not found when consuming 0 bytes`\r\n\r\nThis obviously connected with change of the codec used by Envoy\r\nWe're not sure if it is a normal behaviour or some sort of bug. Could you please help us to understand what does this error mean?\r\n\r\nAs we can see, this upgrade doesn't affect our services in bad way",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32683/comments",
    "author": "usovamaria",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2024-03-04T14:31:19Z",
        "body": "cc @birenroy "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-03T16:01:22Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-10T20:01:31Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2024-06-21T04:56:44Z",
        "body": "@alyssawilk @yanavlasov This seems to be real with issue with oghttp2. We ran in to similar issue in prod. Can we please reopen this issue and possibly change the default back if it is a real issue?"
      },
      {
        "user": "birenroy",
        "created_at": "2024-06-21T22:52:24Z",
        "body": "> @alyssawilk @yanavlasov This seems to be real with issue with oghttp2. We ran in to similar issue in prod. Can we please reopen this issue and possibly change the default back if it is a real issue?\r\n\r\nI've made an upstream QUICHE change to reduce the log spam. The log message is not indicative of a problem with HTTP/2 connection handling."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2024-06-24T04:29:18Z",
        "body": "ack. Then some thing else might be going on. Will try to come up with a reproducible case"
      }
    ]
  },
  {
    "number": 32673,
    "title": "Golang HTTP filter—handle httpheader and httpbody in one function",
    "created_at": "2024-03-03T12:49:31Z",
    "closed_at": "2024-04-10T20:01:30Z",
    "labels": [
      "question",
      "stale",
      "area/golang"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32673",
    "body": "Hello, It seems that there are DecodeHeaders function to parse HttpHeader and DecodeData function to parse HttpBody in the Golang HTTP filter examples. \r\nSuppose I need to parse and validate the value from the http request body and put it in the http request header, and I just want to parse httpheader and httpbody in one function, what should I do?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32673/comments",
    "author": "Hectorsong",
    "comments": [
      {
        "user": "doujiang24",
        "created_at": "2024-03-04T01:35:52Z",
        "body": "You can buffer the header into the filter instance, filter manager will create a new filter instance per request.\r\nAnd it's safe to read/write the header, before returning api.Continue back to Envoy, that means Envoy will pass the header to the next C++ filter.\r\n\r\nHere is an example:\r\n```\r\ntype filter struct {\r\n\tapi.PassThroughStreamFilter\r\n\tcallbacks api.FilterCallbackHandler\r\n\theader    api.RequestHeaderMap\r\n}\r\n\r\nfunc handler(header api.RequestHeaderMap, body api.BufferInstance) api.StatusType {\r\n\treturn api.Continue\r\n}\r\n\r\nfunc (f *filter) DecodeHeaders(header api.RequestHeaderMap, endStream bool) api.StatusType {\r\n\tif endStream {\r\n\t\treturn handler(header, nil)\r\n\t}\r\n\r\n\tf.header = header\r\n\treturn api.StopAndBuffer\r\n}\r\n\r\nfunc (f *filter) DecodeData(buf api.BufferInstance, endStream bool) api.StatusType {\r\n\t// Now, the buf means the whole body data.\r\n\treturn handler(f.header, buf)\r\n}\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-03T16:01:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-10T20:01:30Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 32670,
    "title": "Envoy doesn't recognize http2 connection needs to be terminated",
    "created_at": "2024-03-02T22:12:36Z",
    "closed_at": "2024-03-07T21:40:24Z",
    "labels": [
      "question",
      "area/connection"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32670",
    "body": "*Title*: Envoy doesn't recognize http2 connection needs to be terminated\r\n\r\n*Description*:\r\nI'm using envoy sidecar to route GRPC traffic between k8s with fairly straightforward config. When I kill upstream service I expect all the persistent http2 connections to it to be terminated and requests start failing instantly with UNAVAILABLE. However instead I'm seeing request timing out with either `http2.remote_reset` or `upstream_per_try_timeout` reaching the per-try timeout. What's even more surprising, even setting up tcp_keepalive does not help envoy understand the connection needs to be terminated. \r\n\r\nAm I missing some obvious setting or it's a bug?\r\n\r\n*Logs*:\r\n\r\nHere's example trace logs I get in this situation.\r\n\r\n```\r\n\"Mar 1, 2024 @ 08:21:50.203\",\"[2024-03-01 16:21:50.203][144][trace][http2] [external/envoy/source/common/http/http2/codec_impl.cc:1213] [C17] recv frame type=0\"\r\n\"Mar 1, 2024 @ 08:21:50.203\",\"[2024-03-01 16:21:50.203][144][trace][connection] [external/envoy/source/common/network/raw_buffer_socket.cc:38] [C17] read error: Resource temporarily unavailable\"\r\n\"Mar 1, 2024 @ 08:21:50.203\",\"[2024-03-01 16:21:50.203][144][trace][http2] [external/envoy/source/common/http/http2/codec_impl.cc:1213] [C17] recv frame type=8\"\r\n\"Mar 1, 2024 @ 08:21:50.203\",\"[2024-03-01 16:21:50.203][144][trace][http2] [external/envoy/source/common/http/http2/codec_impl.cc:1187] [C17] about to recv frame type=1, flags=5, stream_id=31\"\r\n\"Mar 1, 2024 @ 08:21:50.203\",\"[2024-03-01 16:21:50.203][144][trace][http2] [external/envoy/source/common/http/http2/codec_impl.cc:2233] [C17] track inbound frame type=1 flags=4 length=2 padding_length=0\"\r\n\"Mar 1, 2024 @ 08:21:50.203\",\"[2024-03-01 16:21:50.203][144][trace][http2] [external/envoy/source/common/http/http2/codec_impl.cc:2233] [C17] track inbound frame type=6 flags=0 length=8 padding_length=0\"\r\n\"Mar 1, 2024 @ 08:21:50.188\",\"[2024-03-01 16:21:50.188][144][trace][connection] [external/envoy/source/common/network/connection_impl.cc:563] [C17] socket event: 2\"\r\n\"Mar 1, 2024 @ 08:21:50.188\",\"[2024-03-01 16:21:50.188][144][trace][connection] [external/envoy/source/common/network/connection_impl.cc:674] [C17] write ready\"\r\n\"Mar 1, 2024 @ 08:21:50.188\",\"[2024-03-01 16:21:50.188][144][trace][connection] [external/envoy/source/common/network/raw_buffer_socket.cc:67] [C17] write returns: 182\"\r\n\"Mar 1, 2024 @ 08:21:50.187\",\"[2024-03-01 16:21:50.187][144][debug][pool] [external/envoy/source/common/conn_pool/conn_pool_base.cc:181] [C17] creating stream\"\r\n\"Mar 1, 2024 @ 08:21:50.187\",\"[2024-03-01 16:21:50.187][144][debug][pool] [external/envoy/source/common/conn_pool/conn_pool_base.cc:264] [C17] using existing fully connected connection\"\r\n\"Mar 1, 2024 @ 08:21:50.187\",\"[2024-03-01 16:21:50.187][144][trace][connection] [external/envoy/source/common/network/connection_impl.cc:478] [C17] writing 24 bytes, end_stream false\"\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32670/comments",
    "author": "Pchelolo",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2024-03-04T14:43:25Z",
        "body": "When I kill upstream service I expect all the persistent http2 connections to it to be terminated and requests start failing instantly with UNAVAILABLE.\r\n> How are you killing the upstreams e.g. what signal\r\n>\r\n> My first thoughts is that the upstream is being terminated abrupted s.t. the connection teardown gracefully. This leads to Envoy thinking the connection exists and using it for streams which timeout, etc. Eventually Envoy figures out that the connection is terminated.\r\n>\r\n> What keepalive configuration do you use?\r\n"
      },
      {
        "user": "Pchelolo",
        "created_at": "2024-03-04T15:34:41Z",
        "body": "> My first thoughts is that the upstream is being terminated abrupted s.t. the connection teardown gracefully. This leads to Envoy thinking the connection exists and using it for streams which timeout, etc. Eventually Envoy figures out that the connection is terminated.\r\n\r\nIn this case I'm using golang GRPC and call both GracefulShutdown on the server and the Shutdown. However, my thinking is that it shouldn't matter - in case of networking issue or node crashing there will be no graceful termination, and downstream will keep timing out calling non-existent dead pod. This will have huge effect on p99 latency.\r\n\r\n> What keepalive configuration do you use?\r\n\r\nI've added tcp_keepalive with keepalive_probes: 3, keepalive_time: 10, keepalive_interval: 10, but that did not make any difference. \r\n"
      },
      {
        "user": "KBaichoo",
        "created_at": "2024-03-05T16:08:59Z",
        "body": "golang GRPC and call both GracefulShutdown on the server and the Shutdown. However, my thinking is that it shouldn't matter\r\n> In this case, the resets are probably due to racing if the GRPC server sends goaway frames and the proxy had in-flight request and the GRPC server as resetting existing streams / new streams after the goaway.\r\n\r\ndownstream will keep timing out calling non-existent dead pod. This will have huge effect on p99 latency.\r\n> You can remediate this using health checks, outlier detection, hedging, etc. \r\n\r\n"
      },
      {
        "user": "Pchelolo",
        "created_at": "2024-03-05T16:25:22Z",
        "body": "> In this case, the resets are probably due to racing if the GRPC server sends goaway frames and the proxy had in-flight request and the GRPC server as resetting existing streams / new streams after the goaway.\r\n\r\nThe resets persist long past the server completely disappearing and definitely not sending anything anymore.\r\n\r\n> You can remediate this using health checks, outlier detection, hedging, etc.\r\n\r\nOutlier detection doesn't work here since reset doesn't result in any grpc status and thus is not mapped to http 5xx.\r\nHedging only works in response to a request timeout - so it would not have any effect here - we still will need to wait for a request to non-existent upstream to timeout first.\r\nI will try using active health checks - that will probably do it. \r\n\r\nMy main point though is that this should be easier - it seems like a very common situation and it's not clear why envoy keeps a connection to a completely non-existent upstream. Shouldn't TCP connection being terminated just terminate the http/2 connection as well?\r\n"
      },
      {
        "user": "KBaichoo",
        "created_at": "2024-03-06T21:24:05Z",
        "body": "My main point though is that this should be easier - it seems like a very common situation and it's not clear why envoy keeps a connection to a completely non-existent upstream. Shouldn't TCP connection being terminated just terminate the http/2 connection as well?\r\n\r\n> If you're not using health information (passive or active) or changing EDS to tell Envoy to not speak to a given backend, it will continue to load balance to those backends. TCP doesn't always terminate cleanly with a FIN / ACK, and Envoy might just attempt to open another connection if a backend closed a connection"
      },
      {
        "user": "Pchelolo",
        "created_at": "2024-03-07T21:40:13Z",
        "body": "I see, thank you for your responses. I'll try to implement active health checks"
      }
    ]
  },
  {
    "number": 32651,
    "title": "Can Envoyproxy support the VIP(virtual IP) or IP failover directly?",
    "created_at": "2024-03-01T06:09:52Z",
    "closed_at": "2024-03-08T03:27:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32651",
    "body": "*Title*: *Can Envoyproxy support the VIP(virtual IP) or IP fail-over directly?*\r\n\r\n*Description*:\r\n>Here is our use case: we are trying to use the Envoyproxy as the reverse proxy for our services, and one of the Envoyproxy instance will work as the entry/endpoint to allow the customer to visit/access.\r\n\r\n>But I  we will face the IP failover issue(out of service) once this Envoyproxy instance crashed.\r\nThe typical solution can be make use of keepalived to support the VIP and IP fail-over detection.\r\n\r\n>For some reason it's not convenient for us to introduce new tool such as keepalived, can somehow the envoyproxy directly support such kind of use-case/scenario?\r\n\r\nThanks in advance.  \r\n\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32651/comments",
    "author": "minjunjiang",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2024-03-01T14:56:53Z",
        "body": "How VIPs work is generally a function of the VIP itself and how it performs health checking. This is outside the scope of Envoy. If you are asking if Envoy has support for BGP and can health check other Envoy nodes and decide to publish routes, no it doesn't currently support that."
      },
      {
        "user": "minjunjiang",
        "created_at": "2024-03-03T11:11:03Z",
        "body": "> How VIPs work is generally a function of the VIP itself and how it performs health checking. This is outside the scope of Envoy. If you are asking if Envoy has support for BGP and can health check other Envoy nodes and decide to publish routes, no it doesn't currently support that.\r\n\r\nThank you so much for your kindly reply！"
      }
    ]
  },
  {
    "number": 32579,
    "title": "Is there some way to figure out why Envoy can't send a request to upstream?",
    "created_at": "2024-02-26T11:26:12Z",
    "closed_at": "2024-04-04T20:01:11Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32579",
    "body": "I deployed an Envoy as the gateway of my backend services. There are two kinds of protocols HTTP1.1 and gRPC on one port. Here is the config of the Envoy:\r\n```\r\n  - name: listener_portal_https\r\n    address:\r\n    listener_filters:\r\n    - name: envoy.filters.listener.tls_inspector\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector\r\n    - name: envoy.filters.listener.http_inspector\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.listener.http_inspector.v3.HttpInspector\r\n    filter_chains:\r\n    - filter_chain_match:        # HTTP1.1\r\n        transport_protocol: tls\r\n        server_names:\r\n        - \"www.example.com\"\r\n      filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          ...\r\n    - filter_chain_match:        # gRPC\r\n        transport_protocol: raw_buffer\r\n        application_protocols:\r\n        - \"h2c\"\r\n      filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          ...\r\n```\r\n\r\nAs you see, I set two `filter_chain_match`s on one same port. One is for HTTP1.1 and the oher is for gRPC.\r\n\r\nHowever, I just found that the gRPC requests had **a low probability** to fail to be sent to the upstream. When this occured, the gRPC requests can not be sent to the upstream for about several minutes.\r\n\r\n**I've thought it was because of poor network. However, when this occured, I tried to send some HTTP1.1 requests and they worked without any error... And rebooting Envoy can solve this immediately.**\r\n\r\nFor example, any gRPC requests can't be sent to the upstream from 09:00 to 09:10, but at the same time, any HTTP1.1 request can be sent to the upstream. And if I reboot Envoy, this issue will be gone immediately.\r\n\r\n\r\n```\r\ngRPC-client ---> Envoy(request blocked here!) ---> upstream\r\n```\r\nAs the timeout of gRPC is set to 0, I can see that the client keeps waiting for the response. After cancelling the request, the Envoy showed me the log, whose `%PROTOCOL%` is `HTTP/2`, `%RESPONSE_CODE%` is `0` and `%DURATION%` is a very large number.\r\n\r\nIn a word, it seems that the gRPC request kind of blocks in Envoy.\r\n\r\nI want to know why. So I set debug-level log:\r\n\r\n> {\"log\":\"[2024-02-26 15:06:14.637][15][debug][router] [source/common/router/router.cc:732] [Tags: \\\"ConnectionId\\\":\\\"39751\\\",\\\"StreamId\\\":\\\"3374533070535275650\\\"] router decoding headers:\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638036637Z\"}\r\n> {\"log\":\"':scheme', 'http'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.63804108Z\"}\r\n> {\"log\":\"':path', '/com.example.api/testApi'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638044647Z\"}\r\n> {\"log\":\"':method', 'POST'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638047823Z\"}\r\n> {\"log\":\"':authority', 'api.example.com:3455'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638050754Z\"}\r\n> {\"log\":\"'t2m-7uz4re', 'z1x0c9v8b7n6m5l4k3j2h'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638053909Z\"}\r\n> {\"log\":\"'grpc-accept-encoding', 'identity,deflate,gzip'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.63805677Z\"}\r\n> {\"log\":\"'accept-encoding', 'identity'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638060343Z\"}\r\n> {\"log\":\"'user-agent', 'grpc-node-js/1.8.10'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638063616Z\"}\r\n> {\"log\":\"'content-type', 'application/grpc'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638066541Z\"}\r\n> {\"log\":\"'te', 'trailers'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638069575Z\"}\r\n> {\"log\":\"'x-forwarded-for', '111.222.111.111'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638072629Z\"}\r\n> {\"log\":\"'x-forwarded-proto', 'http'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638076176Z\"}\r\n> {\"log\":\"'x-envoy-external-address', '111.222.111.111'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638079767Z\"}\r\n> {\"log\":\"'x-request-id', '9fa34df2-e5bc-9217-b9cx-56232f571193'\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638082773Z\"}\r\n> {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638086053Z\"}\r\n> {\"log\":\"[2024-02-26 15:06:14.637][15][debug][pool] [source/common/conn_pool/conn_pool_base.cc:265] [Tags: \\\"ConnectionId\\\":\\\"39752\\\"] using existing fully connected connection\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638088973Z\"}\r\n> {\"log\":\"[2024-02-26 15:06:14.637][15][debug][pool] [source/common/conn_pool/conn_pool_base.cc:182] [Tags: \\\"ConnectionId\\\":\\\"39752\\\"] creating stream\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638093546Z\"}\r\n> {\"log\":\"[2024-02-26 15:06:14.637][15][debug][router] [source/common/router/upstream_request.cc:563] [Tags: \\\"ConnectionId\\\":\\\"39751\\\",\\\"StreamId\\\":\\\"3374533070535275650\\\"] pool ready\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638097661Z\"}\r\n> {\"log\":\"[2024-02-26 15:06:14.637][15][debug][http] [source/common/http/conn_manager_impl.cc:1177] [Tags: \\\"ConnectionId\\\":\\\"39751\\\",\\\"StreamId\\\":\\\"3374533070535275650\\\"] request end stream\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638101248Z\"}\r\n> {\"log\":\"[2024-02-26 15:06:14.637][15][debug][client] [source/common/http/codec_client.cc:141] [Tags: \\\"ConnectionId\\\":\\\"39752\\\"] encode complete\\n\",\"stream\":\"stderr\",\"time\":\"2024-02-26T07:06:14.638104657Z\"}\r\n\r\nIt seems that there isn't any error log...\r\n\r\nThis issue has occured 3 times since last month. I want to know if there is some other way to figure this issue out.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32579/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2024-03-28T16:02:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-04T20:01:10Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 32297,
    "title": "Integration testing two upstream connections to the same cluster",
    "created_at": "2024-02-09T14:00:51Z",
    "closed_at": "2024-02-10T09:55:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32297",
    "body": "I'm trying to figure out if it's possible to test the following scenario in an integration test:\r\n\r\nThe source code is creating a TCP client per worker to a specific cluster.\r\nAssuming the integration test concurrency is 1 (as the default for the integration tests), during the tests, there'll be two TCP clients created.\r\nTo add the required cluster, I'd have to call ``setUpstreamCount(2);`` and therefore ``fake_upstreams_`` will have two slots.\r\nSince there are two TCP clients created, there'll be two client connections to that cluster, and two server connections.\r\nI would like to be able to capture the bytes sent on both of those connections.\r\nIt seems like the problem is that calling ``fake_upstreams_[1]->waitForRawConnection(fake_tcp_connection_)`` would wait for the first connection among those two, and I'm not sure how I could wait for the second one.\r\nWill appreciate help figuring this out\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32297/comments",
    "author": "ohadvano",
    "comments": [
      {
        "user": "botengyao",
        "created_at": "2024-02-09T15:14:47Z",
        "body": "you can use a new `upstream_connection_2_` object to get the ptr of the second upstream, e.g.,\r\n\r\n```\r\n  Envoy::FakeRawConnectionPtr upstream_connection_1_;\r\n  ASSERT_TRUE(fake_upstreams_[0]->waitForRawConnection(upstream_connection_1_));\r\n\r\n  Envoy::FakeRawConnectionPtr upstream_connection_2_;\r\n  ASSERT_TRUE(fake_upstreams_[1]->waitForRawConnection(upstream_connection_2_));\r\n```\r\n\r\nis this what you are looking for?"
      },
      {
        "user": "ohadvano",
        "created_at": "2024-02-09T15:19:39Z",
        "body": "Thanks. Clarifying what I meant: ``fake_upstreams_[0]`` would be mapped to ``cluster_0`` (the default integration cluster) and in that upstream the regular proxy traffic (for example, ``tcp_proxy``) would be transported.\r\n\r\nSo there are two clusters: ``cluster_0`` and ``other_cluster`` where ``cluster_0`` has 1 connection and ``other_cluster`` has 2 connections. I'd like to capture both of the ``other_cluster`` connections."
      },
      {
        "user": "ohadvano",
        "created_at": "2024-02-10T09:55:10Z",
        "body": "Apparently, this works: (not sure why I missed it first time)\r\n\r\n```\r\nfake_upstreams_[1]->waitForRawConnection(upstream_connection_1_)\r\nfake_upstreams_[1]->waitForRawConnection(upstream_connection_2_)\r\n```"
      }
    ]
  },
  {
    "number": 32242,
    "title": "Dyanmic cluster hostname based on regex path",
    "created_at": "2024-02-07T04:57:29Z",
    "closed_at": "2024-03-16T00:02:39Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32242",
    "body": "*Description*:\r\nI'm not sure if this is possible or not and I can't seem to find any documentation around it. I'm trying to use envoy's regex route match and somehow append the captured matches to a cluster hostname. Say I have 3 containers containerA, containerB and containerC names of the containers are dynamically created so I can't statically define the hostname of each container. How can I direct the request?\r\n\r\n```\r\n- match:\r\n    safe_regex:\r\n      google_re2: {}\r\n      regex: '/(.*)/hello'\r\n  route:\r\n    timeout: 240s\r\n    prefix_rewrite: \"/\"\r\n    cluster: authrz_web\r\n    auto_host_rewrite: true\r\n- name: container\r\n  dns_lookup_family: V4_ONLY\r\n  lb_policy: ROUND_ROBIN\r\n  connect_timeout: 60s\r\n  type: LOGICAL_DNS\r\n  load_assignment:\r\n    cluster_name: container\r\n    endpoints:\r\n      - lb_endpoints:\r\n          - endpoint:\r\n              hostname: container+(append captured regex)\r\n              address:\r\n                socket_address:\r\n                  address: container+(append captured regex)\r\n                  port_value: 80\r\n\r\n```\r\nA request comes in as  **/A/hello** I want to be able to capture **A** and include that to the hostname of the cluster and properly resolve the request to the container named containerA. Similarly, if a request comes in as **/Z/hello** I want to be able to resolve it to containerZ.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32242/comments",
    "author": "songwillow",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2024-03-08T20:01:07Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-03-16T00:02:38Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 32181,
    "title": "Transparent authorization in Envoy with Keycloak",
    "created_at": "2024-02-03T08:27:37Z",
    "closed_at": "2024-03-22T08:02:00Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32181",
    "body": "Hello Envoy team\r\n\r\nI'm planning to deploy Envoy for service-a, which doesn't have any built-in authorization. It will send requests without authorization through Envoy. Envoy will be configured with the necessary data (KEYCLOAK_URL, KEYCLOAK_CLIENT_ID, and KEYCLOAK_CLIENT_SECRET) for authentication with the Keycloak server. After authentication through Envoy, I need the process to be seamless, without requiring additional user input, and the authorization to be transparent. The requests from service-a should include an Authorization Bearer <token>, and they should be proxied to service-b, which requires verification of the JWT token from service-a. Could you provide an example configuration to achieve this, if it's possible?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32181/comments",
    "author": "dia-smr",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2024-02-05T20:08:15Z",
        "body": "not sure who could help with this one.   @taoxuy @lizan maybe?"
      },
      {
        "user": "dia-smr",
        "created_at": "2024-02-14T07:09:19Z",
        "body": "At the moment, I've solved this task using envoy.filters.http.lua. If there's a more interesting solution, I'd appreciate it if you could share it.\r\n\r\n```\r\nhttp_filters:\r\n  - name: envoy.filters.http.lua\r\n    typed_config:\r\n      '@type': type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n      default_source_code:\r\n        inline_string: |\r\n          function envoy_on_request(request_handle)\r\n              local keycloak_host = os.getenv(\"KEYCLOAK_HOST\")\r\n              local keycloak_path = os.getenv(\"KEYCLOAK_PATH\")\r\n              local client_id = os.getenv(\"KEYCLOAK_CLIENT_ID\")\r\n              local client_secret = os.getenv(\"KEYCLOAK_CLIENT_SECRET\")\r\n\r\n              -- Perform a request to Keycloak to obtain a token\r\n              local headers, body = request_handle:httpCall(\r\n                  -- The address of the keycloak cluster from the envoy configuration\r\n                  \"cluster_keycloak\",\r\n                  {\r\n                      [\":method\"] = \"POST\",\r\n                      [\":path\"] = keycloak_path,\r\n                      [\":authority\"] = keycloak_host:gsub(\"^https?://\", \"\"),\r\n                      [\"content-type\"] = \"application/x-www-form-urlencoded\",\r\n                      [\"content-length\"] = string.len(\"grant_type=client_credentials&client_id=\" .. client_id .. \"&client_secret=\" .. client_secret),\r\n                  },\r\n                  \"grant_type=client_credentials&client_id=\" .. client_id .. \"&client_secret=\" .. client_secret,\r\n                  5000\r\n              )\r\n\r\n              -- Logging the Keycloak response body\r\n              print(\"Keycloak Response Body: \" .. body)\r\n\r\n              -- Checking the response status\r\n              if headers and headers[\":status\"] == \"200\" then\r\n                  -- Converting the response body from JSON to Lua table\r\n                  local json = require(\"dkjson\")\r\n                  local response_table = json.decode(body)\r\n\r\n                  -- Checking for the presence of an access_token in the response body\r\n                  local access_token = response_table and response_table[\"access_token\"]\r\n\r\n                  if access_token then\r\n                      -- If access_token is found, add the Authorization header to the request\r\n                      request_handle:headers():add(\"Authorization\", \"Bearer \" .. access_token)\r\n                  else\r\n                      -- If access_token is absent, output an error to the log\r\n                      print(\"No access_token found in Keycloak response\")\r\n                  end\r\n              else\r\n                  -- If the request fails, output an error to the log\r\n                  print(\"Failed to get access token from Keycloak\")\r\n              end\r\n          end\r\n  - name: envoy.filters.http.router\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-03-15T08:01:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-03-22T08:02:00Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 32088,
    "title": "Logical DNS and health checks / outlier detection",
    "created_at": "2024-01-29T13:17:53Z",
    "closed_at": "2024-03-14T16:03:23Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32088",
    "body": "How does health checks / outlier detection works with a Logical DNS cluster?\r\nAssuming I have a logical DNS cluster that has a single host ``host.com`` that in time will return 5 different IPs due to DNS round robin, and there's health check, outlier detection set for that cluster. Will I have 5 different health checks (for each of the IPs)? Assuming that yes - what if just one of the IPs fail health check - is the hostname itself considers unhealthy, or only that IP (meaning that new connections to that cluster can still go to one of the other 4 valid IPs)?\r\nThanks",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32088/comments",
    "author": "ohadvano",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2024-02-06T14:19:03Z",
        "body": "cc @pradeepcrao "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-03-07T16:01:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-03-14T16:03:23Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 32004,
    "title": "FQDN Filter",
    "created_at": "2024-01-24T12:01:04Z",
    "closed_at": "2024-04-15T12:02:35Z",
    "labels": [
      "question",
      "stale",
      "area/tcp_proxy"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32004",
    "body": "*FQDN Filter*:\r\n\r\n*Description*:\r\nHello everyone,\r\nThe following question: Is it possible to implement a listener with an FQDN? I want Envoy to only forward traffic when Test.test.io is called. Currently, Envoy Proxy forwards all traffic that comes to port 443.\r\n\r\n```\r\n    -   connect_timeout: 5s\r\n        load_assignment:\r\n            cluster_name: ingress_https\r\n            endpoints:\r\n            -   lb_endpoints:\r\n                -   endpoint:\r\n                        address:\r\n                            socket_address:\r\n                                address: bla.bla.bla.io\r\n                                port_value: 443\r\n                                \r\n         name: ingress_https\r\n        per_connection_buffer_limit_bytes: 32768\r\n        type: strict_dns\r\n```\r\n\r\n```\r\n    -   address:\r\n            socket_address:\r\n                address: 0.0.0.0\r\n                port_value: 443\r\n        filter_chains:\r\n        -   filters:\r\n            -   name: envoy.filters.network.tcp_proxy\r\n                typed_config:\r\n                    '@type': type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n                    access_log:\r\n                    -   name: envoy.access_loggers.file\r\n                        typed_config:\r\n                            '@type': type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                            path: /var/log/envoy/ingress_https_access.log\r\n                    cluster: ingress_https\r\n                    stat_prefix: ingress_https\r\n        name: listener_ingress_https\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32004/comments",
    "author": "eliassteiner",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2024-01-25T18:11:19Z",
        "body": "I'm not sure I understand your question.\r\n\r\nYour config has the listener accept connections on port 443 for any IPv4 assigned to the host (that's what 0.0.0.0 implies). A specific IP can be configured (e.g. 10.0.0.1), which would cause Envoy to only listen on that IP address (not, for example, 127.0.0.1). But most hosts only have 1 IP and localhost, and your DNS name presumably points at the IP address.\r\n"
      },
      {
        "user": "jewertow",
        "created_at": "2024-01-25T21:02:59Z",
        "body": "@eliassteiner you need tls_inspector in you listener:\r\n```\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 443\r\n    listener_filters:\r\n    - name: \"envoy.filters.listener.tls_inspector\"\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector\r\n    filter_chains:\r\n    - filter_chain_match:\r\n        server_names: [\"test.test.io\"]\r\n      filters:\r\n      - name: envoy.filters.network.tcp_proxy\r\n        typed_config:\r\n          '@type': type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n          ...\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-25T00:03:45Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "eliassteiner",
        "created_at": "2024-02-25T06:36:18Z",
        "body": "perfect thank you. but do this need a ssl certificate? i will check this option thank you"
      },
      {
        "user": "jewertow",
        "created_at": "2024-03-09T09:59:08Z",
        "body": "> but do this need a ssl certificate?\r\n\r\nNo"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-08T12:01:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-15T12:02:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 31988,
    "title": "Buffer Per Route not being applied",
    "created_at": "2024-01-23T18:06:15Z",
    "closed_at": "2024-03-01T00:03:00Z",
    "labels": [
      "question",
      "stale",
      "area/buffer"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/31988",
    "body": "[*Title*: *Buffer Per Route not being applied*\r\n\r\n*Description*:\r\n>Hey! First of all, I am new to Envoyproxy, so the issue that I am describing here, may have been already mentioned, although I did not find anyghin related to the subject. So my sincere apologies if I am mistaken.\r\n\r\n**What I am trying to achieve:**\r\nI am trying to use envoy filter in order to have a global buffer, so that requests bigger than `10 MB` are blocked. Moreover, I want that for some particular routes/endpoints (i.e. `/shipments`), the max request size is smaller than the global filter (i.e. `5 MB`).\r\n\r\n**What I did:**\r\nThe following `yaml` describes the implementation of a global filter of `10 MB` using `envoy.extensions.filters.http.buffer.v3.Buffer`, as well as the implementation of a more specific filter of `5 MB` to the route `/shipments` using `envoy.extensions.filters.http.buffer.v3.Buffer`.\r\n\r\n**What is not working:**\r\nAlthough the global filter is working, the **buffer per route is not.** I already tried reading the documentation more carefully, but without success. \r\n\r\nI'm not sure the `yaml` is 100% correct, neither if this is the best way to implement this as I don't fully grasp Envoy's capabilities.\r\nAny suggestion is welcomed\r\n\r\nThanks\r\n\r\n\r\n\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: global-buffer\r\n  namespace: istio-system\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n    - applyTo: HTTP_FILTER\r\n      match:\r\n        context: GATEWAY\r\n        listener:\r\n          filterChain:\r\n            filter:\r\n              name: \"envoy.filters.network.http_connection_manager\"\r\n              subFilter:\r\n                name: \"envoy.filters.http.router\"\r\n      patch:\r\n        operation: INSERT_BEFORE\r\n        value:\r\n          name: envoy.filters.http.buffer\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.extensions.filters.http.buffer.v3.Buffer\r\n            max_request_bytes: 104857606 # 10 MB.\r\n\r\n---\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: buffer-per-route-filter\r\n  namespace: istio-system\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n    - applyTo: HTTP_ROUTE\r\n      match:\r\n        context: GATEWAY\r\n        routeConfiguration:\r\n          vhost:\r\n            name: \"\"\r\n            route:\r\n              action: ANY\r\n      patch:\r\n        operation: INSERT_BEFORE\r\n        value:\r\n          name: envoy.filters.buffer.BufferPerRoute\r\n          match:\r\n            prefix: \"/shipments\"\r\n          typed_per_filter_config:\r\n            envoy.filters.http.buffer:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.buffer.v3.BufferPerRoute\r\n              buffer:\r\n                max_request_bytes: 5242880 # 5 MB.\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/31988/comments",
    "author": "MaggGomes",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2024-01-23T20:17:35Z",
        "body": "Asking for help with the istio project is probably a better starting point. I suspect that something about your istio config is preventing the per-filter config from being installed correctly. If you can provide the underlying Envoy config, we'd be happy to look at that. "
      },
      {
        "user": "MaggGomes",
        "created_at": "2024-01-23T21:35:54Z",
        "body": "> Asking for help with the istio project is probably a better starting point. I suspect that something about your istio config is preventing the per-filter config from being installed correctly. If you can provide the underlying Envoy config, we'd be happy to look at that.\r\n\r\nCould you please elaborate in what you mean here by Envoy config? Or did you mean Istio Config? \r\nAlso, from what you can see, are these `EnvoyFilter` correctly set up, or any of them has any misconfiguration?\r\n\r\nThanks"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-23T00:02:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-03-01T00:03:00Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 31925,
    "title": "Redis filter with Socks capabilities",
    "created_at": "2024-01-22T07:34:25Z",
    "closed_at": "2024-02-29T04:01:14Z",
    "labels": [
      "question",
      "stale",
      "area/redis"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/31925",
    "body": "Hello Envoy team,\r\n\r\ncan the Redis filter be extended with SOCKS capabilities and be used as a a plugin? I see that at the moment that for \"MOVED\" and \"ASK\" responses from a Redis cluster a DNS config is required to resolve not known hosts. It would be great if that can happen through SOCKS proxy. Can you give an opinion on the topic?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/31925/comments",
    "author": "DimitarHKostov",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2024-01-22T23:52:39Z",
        "body": "cc @msukalski @henryyyang"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-22T00:02:45Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-29T04:01:14Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 31839,
    "title": "Limit number of sessions based on business logic",
    "created_at": "2024-01-16T14:52:51Z",
    "closed_at": "2024-02-22T20:01:17Z",
    "labels": [
      "question",
      "stale",
      "area/sessions"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/31839",
    "body": "*Title*: *Limit number of sessions based on business logic*\r\n\r\n*Description*:\r\nWhat is the best way to limit number of sessions (across multiple services) based on some business logic, using envoy?\r\nGiven that a session gets started with start_session api, session gets ended with end session api, and we want to centralize session limit business logic in a separate service.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/31839/comments",
    "author": "chaitanyashete-n",
    "comments": [
      {
        "user": "ravenblackx",
        "created_at": "2024-01-16T16:35:26Z",
        "body": "@cpakulski"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-15T20:01:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-22T20:01:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 31729,
    "title": "TCP Challenge Ack and envoy SO_REUSEADDR ",
    "created_at": "2024-01-09T19:09:56Z",
    "closed_at": "2024-02-16T16:01:09Z",
    "labels": [
      "bug",
      "question",
      "stale",
      "area/listener"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/31729",
    "body": "*Title*:  TCP Challenge Ack and envoy SO_REUSEADDR \r\n\r\n*Description*:\r\nPlato is using envoy in L2 mode behind a AWS NLB (no preserve source ip & proxy protocol, no cross zone) and we have been seeing our average request time for hitting envoy (returning 404) spike to over a second every now and then.\r\n\r\nTracing this down I have found that the amazon linux 2 (kernel 5.10) that envoy is running on is sending a challenge ack causing the client to wait a second and then restart the tcp connection (same port) and every thing works fine their after. With the way the NLB from AWS is configured it is doing NAT and using the same ip for hitting the envoy host every time but the strange thing is that wireshark (i.e. tcpdump) traces dont show that the port was reused recently but still the linux kernel is triggering a tcp challenge ack. \r\n\r\nThe one thing that has made this issue almost disappear is disabling `SO_REUSEADDR` by disabling `enable_reuse_port` in envoy listener.\r\n\r\nNote: envoy is running in k8s with host networking enabled on a 2 core ec2 node to it self.\r\n \r\nHas any one seen this before? \r\nIs there any other envoy settings that might cause the linux kernel to trigger this bad behavior? \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/31729/comments",
    "author": "mchandler-plato",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2024-01-10T09:34:43Z",
        "body": "cc @mattklein123 @alyssawilk "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-09T12:01:23Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-16T16:01:08Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 31627,
    "title": "DC causes coredump in my custom http filter",
    "created_at": "2024-01-04T02:52:36Z",
    "closed_at": "2024-02-11T16:01:10Z",
    "labels": [
      "question",
      "stale",
      "area/http_filter"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/31627",
    "body": "I developed a custom http filter, a request with a custom header can make the filter send a http request to another server and do something with the response.  Many requests with the same custom header can use the same response.\r\n```\r\nHttp::FilterHeadersStatus decodeHeaders(Http::RequestHeaderMap& headers) {\r\n    custom_header = extractCustomHeader(headers);\r\n    statusResponse = responseList.getStatus(custom_header);\r\n    if (statusResponse == DONE) {  // the response has been received and stored, use it immediately\r\n        doJob(responseList[custom_header], headers)\r\n        return Http::FilterHeadersStatus::Continue;\r\n    } elseif (statusResponse == NIL) {  // the response hasn't been found in the responseList, so send a request to the server to get one\r\n        asyncSendRequestToServer(custom_header);\r\n    } elseif (statusResponse == ONFLIGHT) {  // the request to the server has been sent but the response hasn't been received\r\n        filterList.insert(this);\r\n        return Http::FilterHeadersStatus::StopAllIterationAndBuffer;\r\n    }\r\n    ...\r\n}\r\n\r\nvoid onSuccess() {  // get response from the server successfully\r\n    for_each (auto filter : filterList) {\r\n        filter->getCallBacks()->dispatcher().post(\r\n            [filter, headers]() {filter->doJob(response, headers);}\r\n        );\r\n    }\r\n}\r\n```\r\n\r\nAs you see, if some request with the custom header has sent the request to the server but the response hasn't been received, all of requests with the same custom header will be stored in a list `filterList`. After the response received, all of filters in the list can execute `doJob`.\r\n\r\nIt seems that everyting works fine. However, I just found if the client disconnects, which means the flags of Envoy would be `DC`, my http filter could get segmentation fault. I guess a `DC` would make its filter (one element in the `filterList`) be released. That's why I get segmentation fault. It seems that I need some mechanism of Envoy, which can tell me which filter has been released. Is there such a kind of mechanism?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/31627/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "adisuissa",
        "created_at": "2024-01-05T14:39:35Z",
        "body": "It would be good to run this with ASAN and see what exactly causes the seg-fault.\r\nGenerally speaking, the filter's lifetime is the request's lifetime. After the request is no longer needed, the filter will be removed.\r\n\r\nI suggest looking at the ext-proc filter is it seems to be doing something similar to what you are describing, if I understand correctly."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-04T16:01:06Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-11T16:01:09Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 31597,
    "title": "How does retry policy of Envoy work in a micro-service backend",
    "created_at": "2024-01-03T01:55:47Z",
    "closed_at": "2024-02-10T04:01:14Z",
    "labels": [
      "question",
      "stale",
      "area/retry"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/31597",
    "body": "I have such a micro-service backend as below:\r\n```\r\napp --> envoy1 --> server1 --> envoy2 --> server2\r\n```\r\nI configure retry_policy in the envoy1 and I can set retry times too in the server1, which is developed with Springboot. As all we know, modern frameworks, such as Springboot, support retry mechanism.\r\n\r\nIf I enable retry_policy of the envoy1 and use retry mechanism of the server1, it doesn't seem that this is good because it's duplicated.\r\n\r\nIn a word, I don't know when we use the retry_policy of Envoy and when we use the retry mechanism of endpoints (server1, server2 etc).\r\n\r\nFurthermore, in a chain of request, if each node enables retry mechanism, I think it's dangerous because it may cause an explosion of retry even though Envoy supports circuit breaker to avoid too many retry requests...",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/31597/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "adisuissa",
        "created_at": "2024-01-03T18:29:59Z",
        "body": "Sorry, I'm not sure I full understand the question.\r\nConfiguration of a distributed system with multi-components is challenging, if elements such as retry mechanism are used. I guess in the case described above, the retry can be set on any of the steps between the app and server2 and its up to the owner of the system to decide where to configure retries."
      },
      {
        "user": "YvesZHI",
        "created_at": "2024-01-04T01:33:17Z",
        "body": "@adisuissa  Here is an example:\r\n```\r\napp --> envoy1 ---> server1 ---> envoy2 ---> server2`\r\n```\r\nThe `server1` is a web service, which coded like this:\r\n```\r\n@RestController\r\nclass UserController {\r\n    @PostMapping(\"/userinfo\")\r\n    User setUserInfo(@RequestBody User user) {\r\n        return sendReqToServer2(user, retry = 3);  // this will send a http request to the envoy2\r\n    }\r\n}\r\n```\r\nAs you see, when I send a http request to the `server1` , it will generate a http request and send it to the `server2` through the `envoy2`. And there is a parameter `retry = 3`, which means that if the http status code of the response coming from the `envoy2` is not 200, the `server1` will retry maximum 3 times.\r\n\r\nNow if I configure `retry_policy` in the `envoy1` (num_retries = 3), it means that this gonna make maximum 3 * 3 retries. And if I configure `retry_policy` in the `envoy2` (num_retries = 3), it means that this gonna make maximum 3 * 3 * 3 retries. You see, the retry is exploding.\r\n\r\nMy backend is typical micro-service, so I'm asking if there is some solution about retry_policy from the official Envoy.\r\n\r\nIt doesn't seem that setting retry in the `server1` is not a good idea? Because I think the max_retries/retry_budget of the circuit breaker in Envoy couldn't limit this kind of retry, am I right?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-03T04:01:00Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-10T04:01:13Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 31415,
    "title": "How to log cookie values in access.log in istio envoy",
    "created_at": "2023-12-17T09:39:39Z",
    "closed_at": "2024-02-02T08:01:22Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/31415",
    "body": "#22346 , #6695\r\n\r\nHi, Can someone pls share the example for the same. I have set the filter and logs format but in logs, value is still null\r\n\r\nFilter yaml\r\n\r\nspec:\r\n  configPatches:\r\n  - applyTo: HTTP_FILTER\r\n    match:\r\n      context: ANY\r\n      listener:\r\n        filterChain:\r\n          filter:\r\n            name: envoy.filters.network.http_connection_manager\r\n            subFilter:\r\n              name: envoy.filters.http.router\r\n    patch:\r\n      operation: INSERT_BEFORE\r\n      value:\r\n        name: envoy.filters.http.header_to_metadata\r\n        typed_config:\r\n          '@type': type.googleapis.com/envoy.extensions.filters.http.header_to_metadata.v3.Config\r\n          request_rules:\r\n          - cookie: test1_id\r\n            on_header_present:\r\n              key: test1_id\r\n              metadata_namespace: userInfo\r\n              type: STRING\r\n          - cookie: test2_id\r\n            on_header_present:\r\n              key: test2_id\r\n              metadata_namespace: userInfo\r\n              type: STRING\r\n\r\nLogs format:\r\n\r\n\"test1_id\": \"%DYNAMIC_METADATA(userInfo:test1_id)%\",\r\n\"test2_id\": \"%DYNAMIC_METADATA(userInfo:test2_id)%\"",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/31415/comments",
    "author": "ankitmahajan507",
    "comments": [
      {
        "user": "StarryVae",
        "created_at": "2023-12-27T02:57:12Z",
        "body": "it seems that the config you provide has no problem, can you also provide your test request?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-01-26T08:01:11Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-02T08:01:22Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 31274,
    "title": "How to track the order of req response in Network Filters",
    "created_at": "2023-12-11T09:00:31Z",
    "closed_at": "2024-02-10T00:05:36Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/31274",
    "body": "This is intended for authors of database filters. I am trying to generate audit logs from databases fitlers (like- pg, mysql etc). I want to track the req/response status. If I get a response from upstream server, how can I be certain what was the corresponding request for the response? Do they always come in order?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/31274/comments",
    "author": "shiponcs",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-12-27T02:22:52Z",
        "body": "which proxy or filter you mean? Then we can ping it's owner."
      },
      {
        "user": "shiponcs",
        "created_at": "2024-01-03T10:15:57Z",
        "body": "> which proxy or filter you mean? Then we can ping it's owner.\r\n\r\nPostgres/MySQL proxy filter"
      },
      {
        "user": "wbpcode",
        "created_at": "2024-01-03T10:48:43Z",
        "body": "cc @cpakulski "
      },
      {
        "user": "cpakulski",
        "created_at": "2024-01-03T23:24:27Z",
        "body": "As I explained on the slack, you have to rely on the protocol spec (postgres, mysql). So, for example, if the postgres filter has sent 1 message to the server, the next reply must be response to that request. If postgres protocol allows to send to the server several requests, most likely the server is going to send the same number of responses, so you can correlate. Basically this is sort of FIFO."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-03T00:02:53Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-10T00:05:36Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 31176,
    "title": "W3C trace with XRay Extension",
    "created_at": "2023-12-05T04:58:03Z",
    "closed_at": "2024-01-11T20:01:22Z",
    "labels": [
      "question",
      "area/tracing",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/31176",
    "body": "Hello Team,\r\n\r\nWe are using Envoy Image 1.26.x with Otlp w3c tracer to send traces to another envoy with X-ray tracer. We are hoping that traces would connect for the request being generated from w3c envoy to X-ray but we don't see any parent trace id.\r\n\r\nTo provide more detail on the flow:\r\n\r\n- Envoy1 is the upstream, it's the virtual gateway that will send a health check request to Envoy2. This is done in Otel and we \r\ncan see that the last trace is indicating an egress to the health check endpoint of Envoy2. \r\n- Envoy2 is the downstream, we can see in the trace that there is an incoming request hitting the healthcheck endpoint. \r\nHowever, the parent span coming from the VG is not recorded in the span.\r\n\r\nWe have tried confirming with new AWS Q chat bot if this is supported and the answer was \"when a trace is generated using an OpenTelementry SDK instrumented with the AWS X-Ray SDK, the W3C Trace Context will connect as the parent span of the X-Ray segments\" so we think the trace should connect ideally.\r\n\r\nThinking if this could be something related to older X-ray sdk version incorporated in envoy code? \r\n\r\nRegards,\r\nDeepti Gupta",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/31176/comments",
    "author": "DeeptiGuptaP",
    "comments": [
      {
        "user": "soulxu",
        "created_at": "2023-12-05T07:36:37Z",
        "body": "cc @wbpcode if he can help on this"
      },
      {
        "user": "soulxu",
        "created_at": "2023-12-05T07:38:00Z",
        "body": "cc @abaptiste @suniltheta @mattklein123 "
      },
      {
        "user": "suniltheta",
        "created_at": "2023-12-05T12:29:08Z",
        "body": "The XRay extension in Envoy is not yet updated to support W3C trace format.\n@soulxu can you please add help wanted & aws tag?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-01-04T16:01:26Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-01-11T20:01:22Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 31159,
    "title": " Envoy filter is not supporting if https request",
    "created_at": "2023-12-04T11:01:29Z",
    "closed_at": "2023-12-06T06:11:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/31159",
    "body": "i have below ext_authz where every http request flow will go the the ext_authz OPA \r\n```\r\n- applyTo: HTTP_FILTER\r\n  match:\r\n    context: SIDECAR_OUTBOUND\r\n    listener:\r\n      filterChain:\r\n        filter:\r\n          name: \"envoy.filters.network.http_connection_manager\"\r\n        subFilter:\r\n          name: \"envoy.filters.http.router\"\r\n  patch:\r\n    operation: INSERT_BEFORE\r\n    value:\r\n      name: envoy.ext_authz\r\n      typed_config:\r\n        '@type': type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz\r\n        transport_api_version: V3\r\n        status_on_error:\r\n          code: ServiceUnavailable\r\n        grpc_service:\r\n          google_grpc:\r\n            target_uri: 127.0.0.1:9191\r\n          stat_prefix: \"ext_authz\"\r\n```\r\nbut when request is http then it's working fine as expected. but when i request https then it's not reaching in this filter block.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/31159/comments",
    "author": "amankumarcs",
    "comments": [
      {
        "user": "soulxu",
        "created_at": "2023-12-04T12:49:47Z",
        "body": "Emm.. it seems a question for the istio also. cc @kyessenov \r\n\r\n@amankumarcs would you be able fix the format? it will be easy for people to look at it. thanks!"
      },
      {
        "user": "amankumarcs",
        "created_at": "2023-12-04T17:57:02Z",
        "body": "> Emm.. it seems a question for the istio also. cc @kyessenov\r\n> \r\n> @amankumarcs would you be able fix the format? it will be easy for people to look at it. thanks!\r\n\r\nupdated"
      },
      {
        "user": "soulxu",
        "created_at": "2023-12-06T06:11:30Z",
        "body": "@amankumarcs I think it is hard to answer from the Envoy side, the Istio pushed a lot of config to Envoy, that EnvoyFilter only patch a few parts of the whole config which pushed by Istio. You better to ask this question in the Istio community. Thanks!"
      }
    ]
  },
  {
    "number": 31044,
    "title": "Question - how to regenerate protobufs after removal of generate_go_prototobuf.py?",
    "created_at": "2023-11-24T10:44:34Z",
    "closed_at": "2024-01-04T20:01:39Z",
    "labels": [
      "question",
      "area/build",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/31044",
    "body": "*Title*: *How to regenerate protobufs after removal of generate_go_prototobuf.py?*\r\n\r\n*Description*:\r\ntools/api/generate_go_prototobuf.py used to be a simple way to regenerate protobufs for envoy, but now it's been removed in favor of ci/do_ci.sh... but it doesn't seem to be equivalent?\r\n\r\nI build envoy with 'bazel build envoy' and try to execute this script (ci/do_ci.sh api.go), and it exits with message \"No go proto targets found'. \r\n\r\nCould someone provide a set of commands for regenerating all the protobufs? \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/31044/comments",
    "author": "VivekSubr",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-28T20:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-01-04T20:01:38Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 30984,
    "title": "Start logging on Retry",
    "created_at": "2023-11-20T20:21:34Z",
    "closed_at": "2024-01-03T16:01:28Z",
    "labels": [
      "question",
      "stale",
      "area/retry"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30984",
    "body": "I need help with this question.\r\n\r\nI want to be able to configure logging such that request and response headers and body are logged if the first try call to upstream is not successful.  \r\n\r\nI looked at the options under Access Logging and Tap I don't want to log all requests and response and I don't want to invoke tap from Admin.\r\n\r\nIf it is possible to invoke a web hook or gRPC when an error is returned by upstream and send the request and response to that gRPC/Webhook.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30984/comments",
    "author": "himanshuz2",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2023-11-22T14:48:42Z",
        "body": "cc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2023-11-27T15:13:58Z",
        "body": "I don't think that hook exists but it's not a bad one to add"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-27T16:01:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-01-03T16:01:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 30954,
    "title": "LB extensibility",
    "created_at": "2023-11-17T16:41:21Z",
    "closed_at": "2023-12-27T12:01:22Z",
    "labels": [
      "question",
      "stale",
      "area/load balancing",
      "area/extension"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30954",
    "body": "LB extensibility\r\n\r\nDo you have any example of implementing LB extensibility?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30954/comments",
    "author": "Artemu25",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2023-11-20T04:08:36Z",
        "body": "@pianiststickman \r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-20T08:01:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-27T12:01:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 30918,
    "title": "Is it proper to make a high keep alive timeout for upstream hosts",
    "created_at": "2023-11-16T10:42:03Z",
    "closed_at": "2023-12-23T20:01:28Z",
    "labels": [
      "question",
      "stale",
      "area/connection"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30918",
    "body": "I used Envoy as the gateway of my backend servers.\r\n\r\nThe servers are developed with SpringBoot with Tomcat.\r\n\r\nTomcat allows us to change its keep alive timeout, which is a very small value (20 seconds) for now.\r\n\r\nA small keep alive timeout is reasonable as it avoid wasting resources on idle connections with clients.\r\n\r\nBut since I deployed an Envoy as the gateway, I'm thinking if I should increase significantly the value of keep alive timeout of Tomcat (1 hour maybe?).\r\n\r\nAs my understanding, Envoy used some mechanism of connection pool to create/maintain the connections between upstream hosts and itself. Different requests coming from clients should be able to share/reuse the connections between upstream host and Envoy. Am I right? If so, it seems that we should keep the connections between upstream hosts and Envoy alive for a long time...",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30918/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2023-11-16T14:33:27Z",
        "body": ">As my understanding, Envoy used some mechanism of connection pool to create/maintain the connections between upstream hosts and itself. Different requests coming from clients should be able to share/reuse the connections between upstream host and Envoy. Am I right? \r\n\r\nYes\r\n\r\n> If so, it seems that we should keep the connections between upstream hosts and Envoy alive for a long time...\r\n\r\nIt really depends on specific of your deployment. In general, you want some amount of connection reuse, so should configure keepalives to ensure connections aren't randomly terminated when you want them open."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-16T16:01:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-23T20:01:27Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 30779,
    "title": "Configuring flow control to prevent outofmemory",
    "created_at": "2023-11-08T12:51:49Z",
    "closed_at": "2023-12-23T20:01:24Z",
    "labels": [
      "question",
      "stale",
      "area/buffer"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30779",
    "body": "Need a clarification with flow control.\r\n\r\nWe are using envoy proxy and we have some upstream which are actually slow, sometimes taking upto 22 seconds:\r\n\r\n[2023-10-23T08:37:01.659Z] \"POST /test.sl.EventDB.v1.EventDB/Create HTTP/2\" 200 - 529 7 4006 4006 \"-\" \"grpc-go/1.56.2\" \"77cab6b6-0638-430e-b59a-4c3b3029b3b5\" \"localhost:8887\" \"172.16.95.212:8080\"\r\n[2023-10-23T08:37:02.273Z] \"POST /test.sl.EventDB.v1.EventDB/Create HTTP/2\" 200 - 685 7 3392 3392 \"-\" \"grpc-go/1.56.2\" \"f5042724-1468-41a8-b6d5-75a5af0370b7\" \"localhost:8887\" \"172.16.95.212:8080\"\r\n[2023-10-23T08:36:42.770Z] \"POST /test.sl.EventDB.v1.EventDB/Create HTTP/2\" 200 - 498 7 **22895 22895** \"-\" \"grpc-go/1.56.2\" \"5ae72910-3b33-4957-969d-731710bc3206\" \"localhost:8887\" \"172.16.95.212:8080\"\r\n\r\n\r\nenvoy counter increasing with time:\r\n\r\ndownstream_cx_rx_bytes_buffered: 78252\r\ndownstream_cx_rx_bytes_buffered: 21600\r\ndownstream_cx_rx_bytes_buffered: 21600\r\ndownstream_cx_rx_bytes_buffered: 21600\r\ndownstream_cx_rx_bytes_buffered: 71942\r\ndownstream_cx_rx_bytes_buffered: 170274\r\ndownstream_cx_rx_bytes_buffered: 1134517\r\ndownstream_cx_rx_bytes_buffered: 682432\r\ndownstream_cx_rx_bytes_buffered: 3349588\r\ndownstream_cx_rx_bytes_buffered: 9399489\r\ndownstream_cx_rx_bytes_buffered: 15503287\r\ndownstream_cx_rx_bytes_buffered: 12460896\r\ndownstream_cx_rx_bytes_buffered: 10434389\r\ndownstream_cx_rx_bytes_buffered: 11567799\r\n\r\nIn such a case envoy is buffering, resulting in out of memory.\r\n\r\nIn order to protect envoy we wanted to bring in flow control. The doubt is if we configure initial_stream_window_size and per_connection_buffer_limit_bytes to be less than the size of request, will envoy reject such requests?\r\n\r\nFor example: if I configure initial_stream_window_size as 1 MB and my request is 2 MB will envoy reject such requests?\r\n\r\nIrrespective of the upstream slowness we want to protect **envoy serving the incoming requests without buffering.**\r\n Please give your valuable suggestions",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30779/comments",
    "author": "rookrunner",
    "comments": [
      {
        "user": "rookrunner",
        "created_at": "2023-11-08T12:52:28Z",
        "body": "@alyssawilk If you can help please"
      },
      {
        "user": "botengyao",
        "created_at": "2023-11-11T03:37:49Z",
        "body": "> For example: if I configure initial_stream_window_size as 1 MB and my request is 2 MB will envoy reject such requests?\r\n\r\nI don't think Envoy will reject the request. Instead, it will allow the request to be sent in chunks up to the window size with pressure/flow control.\r\n\r\nConfiguring `per_connection_buffer_limit_bytes` in listener may take more effect, like it will apply pressure to the client peer at L3/4 level. When the buffer limit is reached, Envoy calls `readDisable` on the connection to disable further reading of data from the connection, and the read will resume when the buffer is drained to low water mark. You can try it with your tests. Thanks!"
      },
      {
        "user": "htuch",
        "created_at": "2023-11-14T05:37:32Z",
        "body": "@KBaichoo "
      },
      {
        "user": "KBaichoo",
        "created_at": "2023-11-16T18:53:48Z",
        "body": "The doubt is if we configure initial_stream_window_size and per_connection_buffer_limit_bytes to be less than the size of request, will envoy reject such requests?\r\n> unless you have a buffering filter or something similar within the filter chain, setting these < request size should still be ok as there will be backpressure from either HTTP2 flow control or TCP."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-16T20:01:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-23T20:01:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 30745,
    "title": "Intermittently X-Masked-Path header is not working",
    "created_at": "2023-11-06T09:20:25Z",
    "closed_at": "2024-01-04T20:01:29Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30745",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *We are using X-Masked-Path for masking query params, but this seems not working Intermittently*\r\n\r\n*Description*:\r\n>\r\n\r\n- Below is our match block which seems to work for 99% of the total request but it fails in some requests\r\n`              - match:\r\n                  prefix: \"/test/pii\"\r\n                request_headers_to_add:\r\n                  append_action: \"OVERWRITE_IF_EXISTS_OR_ADD\"\r\n                  header:\r\n                    key: \"X-Masked-Path\"\r\n                    value: \"/test/pii\"`\r\n\r\n- Here for almost all request it is masking the query param and in logs we are only getting `/test/pii` but there are some request where we see entire query parameters being logged as well.\r\n- We have observed that all such request are having response code `0DC`\r\n\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30745/comments",
    "author": "aalokjha-gits",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-28T20:01:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-01-04T20:01:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 30743,
    "title": "Does active health check support happy eyeballs?",
    "created_at": "2023-11-06T07:40:58Z",
    "closed_at": "2023-12-27T20:01:15Z",
    "labels": [
      "question",
      "stale",
      "area/health_checking"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30743",
    "body": "*Description*:\r\n> I want to configure my upstream cluster to support IPv6 and IPv4 dual stack. Expectation is to prefer IPv6 address for establishing the connection and if the attempt fails, fall back to IPv4 address for establishing connection. With `LOGICAL_DNS` with happy eyeballs, I'm able to achieve this.\r\n\r\n> The same upstream cluster is also configured with http active health checking. And since it shares the same dns policy and endpoints configuration with the cluster, I expect health check to follow the same happy eyeballs scheme. But from what I observe, health check always attempts to connect to the first IP address in the list and never fall back to the remaining ones even if the first is unreachable.\r\n\r\n\r\nFor http health check, Envoy only tries IPv6(first one in the list) and considers health check fail if it is not reachable without trying the remaining IPv4 address in the list.\r\n```\r\n[2023-11-05 19:54:19.667][2581853][debug][dns] [source/extensions/network/dns_resolver/getaddrinfo/getaddrinfo.cc:137] getaddrinfo resolution complete for host <hostname>: [[2402:740:0:40e::20:80]:0, 100.65.9.208:0]\r\n\r\n[2023-11-05 19:54:20.644][2581847][debug][connection] [./source/common/network/connection_impl.h:98] [C3] current connecting state: true\r\n[2023-11-05 19:54:20.644][2581847][debug][client] [source/common/http/codec_client.cc:57] [C3] connecting\r\n[2023-11-05 19:54:20.644][2581847][debug][connection] [source/common/network/connection_impl.cc:941] [C3] connecting to [2402:740:0:40e::20:80]:8443\r\n[2023-11-05 19:54:20.644][2581847][debug][connection] [source/common/network/connection_impl.cc:960] [C3] connection in progress\r\n[2023-11-05 19:54:20.644][2581847][trace][connection] [source/common/network/connection_impl.cc:483] [C3] writing 87 bytes, end_stream false\r\n[2023-11-05 19:54:20.644][2581847][debug][client] [source/common/http/codec_client.cc:139] [C3] encode complete\r\n[2023-11-05 19:54:22.645][2581847][debug][hc] [source/extensions/health_checkers/http/health_checker_impl.cc:418] [C3] connection/stream timeout health_flags=/failed_active_hc/active_hc_timeout\r\n[2023-11-05 19:54:22.645][2581847][debug][connection] [source/common/network/connection_impl.cc:139] [C3] closing data_to_write=87 type=3\r\n[2023-11-05 19:54:22.645][2581847][debug][connection] [source/common/network/connection_impl.cc:250] [C3] closing socket: 1\r\n[2023-11-05 19:54:22.645][2581847][trace][connection] [source/common/network/connection_impl.cc:423] [C3] raising connection event 1\r\n[2023-11-05 19:54:22.645][2581847][debug][client] [source/common/http/codec_client.cc:107] [C3] disconnect. resetting 1 pending requests\r\n[2023-11-05 19:54:22.645][2581847][debug][client] [source/common/http/codec_client.cc:156] [C3] request reset\r\n[2023-11-05 19:54:22.645][2581847][trace][main] [source/common/event/dispatcher_impl.cc:250] item added to deferred deletion list (size=1)\r\n[2023-11-05 19:54:22.645][2581847][debug][hc] [source/extensions/health_checkers/http/health_checker_impl.cc:283] [C3] connection/stream error health_flags=/failed_active_hc/active_hc_timeout\r\n[2023-11-05 19:54:22.645][2581847][trace][main] [source/common/event/dispatcher_impl.cc:250] item added to deferred deletion list (size=2)\r\n[2023-11-05 19:54:22.645][2581847][trace][main] [source/common/event/dispatcher_impl.cc:125] clearing deferred deletion list (size=2)\r\n```\r\n\r\n\r\nFor non health check traffic, connection can be established successfully with IPv4 address in case IPv6 is unreachable.\r\n```\r\n[2023-11-01 04:24:40.415][3848626][debug][pool] [source/common/http/conn_pool_base.cc:78] queueing stream due to no available connections (ready=0 busy=0 connecting=0)\r\n[2023-11-01 04:24:40.415][3848626][debug][pool] [source/common/conn_pool/conn_pool_base.cc:291] trying to create new connection\r\n[2023-11-01 04:24:40.415][3848626][trace][pool] [source/common/conn_pool/conn_pool_base.cc:292] ConnPoolImplBase 0x6cf7ebaa400, ready_clients_.size(): 0, busy_clients_.size(): 0, connecting_clients_.size(): 0, connecting_stream_capacity_: 0, num_active_streams_: 0, pending_streams_.size(): 1 per upstream preconnect ratio: 1\r\n[2023-11-01 04:24:40.415][3848626][debug][pool] [source/common/conn_pool/conn_pool_base.cc:145] creating a new connection (connecting=0)\r\n[2023-11-01 04:24:40.415][3848626][debug][multi_connection] [source/common/network/multi_connection_base_impl.cc:14] [C12] connections=2\r\n[2023-11-01 04:24:40.415][3848626][debug][happy_eyeballs] [source/common/network/happy_eyeballs_connection_impl.cc:33] C[12] address=[2402:740:0:40e::20:80]:8443\r\n[2023-11-01 04:24:40.415][3848626][trace][http2] [source/common/http/http2/codec_impl.cc:1786] Codec does not have Metadata frame support.\r\n[2023-11-01 04:24:40.415][3848626][debug][http2] [source/common/http/http2/codec_impl.cc:1586] [C12] updating connection-level initial window size to 1048576\r\n[2023-11-01 04:24:40.415][3848626][debug][connection] [./source/common/network/connection_impl.h:98] [C13] current connecting state: true\r\n[2023-11-01 04:24:40.415][3848626][debug][client] [source/common/http/codec_client.cc:57] [C12] connecting\r\n[2023-11-01 04:24:40.415][3848626][debug][connection] [source/common/network/connection_impl.cc:941] [C13] connecting to [2402:740:0:40e::20:80]:8443\r\n[2023-11-01 04:24:40.415][3848626][debug][connection] [source/common/network/connection_impl.cc:960] [C13] connection in progress\r\n[2023-11-01 04:24:40.415][3848626][trace][multi_connection] [source/common/network/multi_connection_base_impl.cc:436] Scheduling next attempt.\r\n[2023-11-01 04:24:40.415][3848626][trace][pool] [source/common/conn_pool/conn_pool_base.cc:131] not creating a new connection, shouldCreateNewConnection returned false.\r\n[2023-11-01 04:24:40.415][3848626][trace][http] [source/common/http/filter_manager.cc:539] [C11][S16024757736835540634] decode headers called: filter=envoy.filters.http.upstream_codec status=4\r\n[2023-11-01 04:24:40.415][3848626][trace][http] [source/common/http/filter_manager.cc:539] [C11][S16024757736835540634] decode headers called: filter=envoy.filters.http.router status=1\r\n[2023-11-01 04:24:40.415][3848626][trace][http2] [source/common/http/http2/codec_impl.cc:1030] [C11] about to recv frame type=0, flags=1, stream_id=1\r\n[2023-11-01 04:24:40.415][3848626][trace][http2] [source/common/http/http2/codec_impl.cc:1056] [C11] recv frame type=0\r\n[2023-11-01 04:24:40.415][3848626][trace][http2] [source/common/http/http2/codec_impl.cc:2098] [C11] track inbound frame type=0 flags=1 length=89 padding_length=0\r\n[2023-11-01 04:24:40.415][3848626][debug][http] [source/common/http/conn_manager_impl.cc:1022] [C11][S16024757736835540634] request end stream\r\n[2023-11-01 04:24:40.415][3848626][trace][http] [source/common/http/filter_manager.cc:671] [C11][S16024757736835540634] decode data called: filter=envoy.filters.http.router status=3\r\n[2023-11-01 04:24:40.415][3848626][trace][http2] [source/common/http/http2/codec_impl.cc:958] [C11] dispatched 755 bytes\r\n[2023-11-01 04:24:40.715][3848626][trace][multi_connection] [source/common/network/multi_connection_base_impl.cc:426] Trying another connection.\r\n[2023-11-01 04:24:40.716][3848626][debug][happy_eyeballs] [source/common/network/happy_eyeballs_connection_impl.cc:33] C[12] address=100.65.9.208:8443\r\n[2023-11-01 04:24:40.716][3848626][debug][connection] [source/common/network/connection_impl.cc:941] [C14] connecting to 100.65.9.208:8443\r\n[2023-11-01 04:24:40.716][3848626][debug][connection] [source/common/network/connection_impl.cc:960] [C14] connection in progress\r\n[2023-11-01 04:24:40.716][3848626][trace][connection] [source/common/network/connection_impl.cc:568] [C14] socket event: 2\r\n[2023-11-01 04:24:40.716][3848626][trace][connection] [source/common/network/connection_impl.cc:679] [C14] write ready\r\n[2023-11-01 04:24:40.716][3848626][debug][connection] [source/common/network/connection_impl.cc:688] [C14] connected\r\n```\r\n\r\n\r\nDoes health check also follow happy eyeballs mechanism if it is configured for the cluster? Any additional configuration needed for health check in order to have this kind of behavior?\r\n\r\nI'm using Envoy version v1.26.4. Any help is appreciated.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30743/comments",
    "author": "rjiarui",
    "comments": [
      {
        "user": "botengyao",
        "created_at": "2023-11-11T02:05:44Z",
        "body": "I don't think right now active hc supports happy eyeball and only the first address is used, but it can be added eventually. @RyanTheOptimist for more details."
      },
      {
        "user": "rjiarui",
        "created_at": "2023-11-14T17:22:45Z",
        "body": "@botengyao @RyanTheOptimist Thanks! If this is not supported yet, are there already plans to support this in the future?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-20T16:01:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-27T20:01:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 30720,
    "title": "Question: Any existing mechanism for L7 Filter determining L4 session reusing",
    "created_at": "2023-11-03T20:00:21Z",
    "closed_at": "2024-01-04T20:01:25Z",
    "labels": [
      "question",
      "area/tls",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30720",
    "body": "*Title*: *Question: Any existing mechanism for L7 Filter determining L4 session reusing*\r\n\r\n*Description*:\r\nDoes anyone know how to determine if the current request being processed by an Envoy http filter is reusing the session or not?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30720/comments",
    "author": "EltonzHu",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2023-11-28T18:35:47Z",
        "body": "I assume you mean reusing TLS session. This property is not presently exposed to HTTP filters."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-28T20:01:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-01-04T20:01:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 30712,
    "title": "503 UH - no healthy host for HTTP connection pool",
    "created_at": "2023-11-03T06:36:33Z",
    "closed_at": "2024-01-04T20:01:23Z",
    "labels": [
      "question",
      "stale",
      "area/cluster"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30712",
    "body": "could you please clarify what categories of retry the case 'no healthy host for HTTP connection pool' belongs to: reset or connect-failure?\r\ni'd like to set retry policy for this.\r\n\r\nupdate:\r\ni suspect that we encounter a case when request is coming at moment when configuration is updating and request ending with the 503 UH. This happens rarely\r\n\r\nupdate:\r\nWe started receiving rare 503 responses in out night tests. I noticed that it happens right after CDS update and i decided to try reproduce. i set up load test: from one hand i make endless update cluster that causes a reset connection (i see this in console using netstat) and from another hand there are a dozen of threads sending requests. may be this is not legal) but this works. i catch 503 and also i catch another case when envoy is killed with last messages:\r\n```\r\n[2023-11-02T19:34:40.074] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:104] Caught Segmentation fault, suspect faulting address 0x0\r\n[2023-11-02T19:34:40.074] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:91] Backtrace (use tools/stack_decode.py to get line numbers):\r\n[2023-11-02T19:34:40.074] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:92] Envoy version: cfa32deca25ac57c2bbecdad72807a9b13493fc1/1.26.4/Clean/RELEASE/BoringSSL\r\n[2023-11-02T19:34:40.075] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:96] #0: __restore_rt [0x7f296dc37420]\r\n[2023-11-02T19:34:40.075] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #1: [0x556a5b3633c1]\r\n[2023-11-02T19:34:40.075] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #2: [0x556a5b3623ba]\r\n[2023-11-02T19:34:40.075] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #3: [0x556a5a56466b]\r\n[2023-11-02T19:34:40.075] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #4: [0x556a5a56ad23]\r\n[2023-11-02T19:34:40.075] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #5: [0x556a5a56a01c]\r\n[2023-11-02T19:34:40.168] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #6: [0x556a5a56a9d9]\r\n[2023-11-02T19:34:40.168] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #7: [0x556a5a56919a]\r\n[2023-11-02T19:34:40.168] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #8: [0x556a5b367b0e]\r\n[2023-11-02T19:34:40.168] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #9: [0x556a5bae55c1]\r\n[2023-11-02T19:34:40.168] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #10: [0x556a5bae698d]\r\n[2023-11-02T19:34:40.168] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #11: [0x556a5beb23a0]\r\n[2023-11-02T19:34:40.168] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #12: [0x556a5beb0ce1]\r\n[2023-11-02T19:34:40.168] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #13: [0x556a5b3fffc1]\r\n[2023-11-02T19:34:40.168] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #14: [0x556a59d16f54]\r\n[2023-11-02T19:34:40.168] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #15: [0x556a59d176ee]\r\n[2023-11-02T19:34:40.169] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:98] #16: [0x556a59d1511c]\r\n[2023-11-02T19:34:40.169] [critical] [thread=11] [backtrace] [./source/server/backtrace.h:96] #17: __libc_start_main [0x7f296da55083]\r\n'envoy' process exited with code = 139\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30712/comments",
    "author": "nichd147",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2023-11-28T19:03:22Z",
        "body": "Please report Envoy crashes to the envoy-security@googlegroups.com mailing list.\r\n\r\nNo healthy upstreams condition may not be retriable. Envoy does not retry cases where a cluster was just updated via CDS and is waiting for the endpoints to be resolved."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-28T20:01:07Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-01-04T20:01:23Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 30606,
    "title": "Question: Resetting stream after sending local reply",
    "created_at": "2023-10-30T01:35:06Z",
    "closed_at": "2023-12-07T20:01:23Z",
    "labels": [
      "question",
      "stale",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30606",
    "body": "I'd like to know if there's a way to reset a stream right after local reply was sent from a custom http filter. Calling `StreamDecoderFilterCallbacks::resetStream()` after `StreamDecoderFilterCallbacks::sendLocalReply()` leads to an error because the request is already completed. The goal is in some cases to send local reply to a downstream and then forcefully reset stream. \r\n\r\nServer deals ok if a client closes a connection after recieving the entire response but if it doesn't close I need it to be immediately closed and with `LocalClose` type. Please give me a hint if it could be done via HttpFilter's API, or maybe this case is somehow configurable using timeouts.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30606/comments",
    "author": "emdgit",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2023-10-31T04:23:21Z",
        "body": "cc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2023-10-31T17:11:17Z",
        "body": "I'm not quite sure what you want to be doing.  The stream is closed after you sendLocalReply so there's nothing to reset.  If you want to not send a local reply downstream you can use onLocalReply and return ContinueAndResetStream to not actually send the local reply?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-30T20:01:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-12-07T20:01:22Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 30511,
    "title": "In-tree custom network filter fails to initialize causing segmentation fault",
    "created_at": "2023-10-25T20:34:08Z",
    "closed_at": "2023-10-26T18:33:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30511",
    "body": "*Title*: *In-tree custom network filter fails to initialize causing segmentation fault*\r\n\r\n*Description*:\r\n>I have written a simple C++ filter within the Envoy source tree, under `source/extensions/filters/network/<my_custom_filter>`. The corresponding configuration proto is defined under `api/envoy/extensions/filters/network/<my_custom_filter/v3`. I have updated the necessary build files / yamls etc. and have listed them below. The filter itself builds and so does the Envoy static binary.\r\n\r\nHowever, when I add the custom filter in before the Tcp Proxy filter in a filter chain, and bring Envoy up, then on the first attempt to establish a connection to the listener which use this filter in its filter chain, Envoy crashes. I have pasted the backtrace outline, as seen in the log below (timestamps have been replaced with `...` for readability).\r\n\r\n```\r\n... [31171][debug][filter] [source/common/tcp_proxy/tcp_proxy.cc:246] [Tags: \"ConnectionId\":\"0\"] new tcp proxy session\r\n... [31171][debug][filter] [source/extensions/filters/network/address_mapper/address_mapper.cc:42] [Tags: \"ConnectionId\":\"0\"] address_mapper: filter activated\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:104] Caught Segmentation fault, suspect faulting address 0x0\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:91] Backtrace (use tools/stack_decode.py to get line numbers):\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:92] Envoy version: 706fe7871ab5fe631406db1e0fe5af1c4d0eb1b8/1.28.0-dev/Modified/DEBUG/BoringSSL\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #0: Envoy::SignalAction::sigHandler() [0x55c54ae8ba85]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #1: __restore_rt [0x7f0c8e6673c0]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #2: Envoy::Network::FilterManagerImpl::initializeReadFilters() [0x55c54a9be159]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #3: Envoy::Network::ConnectionImpl::initializeReadFilters() [0x55c54a8f27d2]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #4: Envoy::Network::ServerConnectionImpl::initializeReadFilters() [0x55c54a90032a]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #5: Envoy::Server::Configuration::FilterChainUtility::buildFilterChain() [0x55c54a304dc7]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #6: Envoy::Server::ListenerImpl::createNetworkFilterChain() [0x55c54a1cbbdb]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #7: Envoy::Server::ActiveStreamListenerBase::newConnection() [0x55c54a258f3e]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #8: Envoy::Server::ActiveTcpSocket::newConnection() [0x55c54a263810]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #9: Envoy::Server::ActiveTcpSocket::continueFilterChain() [0x55c54a26343f]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #10: Envoy::Server::ActiveTcpSocket::startFilterChain() [0x55c548f9afe0]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #11: Envoy::Server::ActiveStreamListenerBase::onSocketAccepted() [0x55c548f9b1ec]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #12: Envoy::Server::ActiveTcpListener::onAcceptWorker() [0x55c54a255ba2]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #13: Envoy::Server::ActiveTcpListener::onAccept() [0x55c54a2558a7]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #14: Envoy::Network::TcpListenerImpl::onSocketEvent() [0x55c54a9152dc]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #15: Envoy::Network::TcpListenerImpl::TcpListenerImpl()::{lambda()#1}::operator()() [0x55c54a915820]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #16: std::_Function_handler<>::_M_invoke() [0x55c54a9166ad]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #17: std::function<>::operator()() [0x55c548fb0068]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #18: Envoy::Event::DispatcherImpl::createFileEvent()::{lambda()#1}::operator()() [0x55c54a8ca08d]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #19: std::_Function_handler<>::_M_invoke() [0x55c54a8d0586]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #20: std::function<>::operator()() [0x55c548fb0068]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #21: Envoy::Event::FileEventImpl::mergeInjectedEventsAndRunCb() [0x55c54a8dd70b]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #22: Envoy::Event::FileEventImpl::assignEvents()::{lambda()#1}::operator()() [0x55c54a8dbce7]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #23: Envoy::Event::FileEventImpl::assignEvents()::{lambda()#1}::_FUN() [0x55c54a8dbd9a]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #24: event_persist_closure [0x55c54b600579]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #25: event_process_active_single_queue [0x55c54b60085e]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #26: event_process_active [0x55c54b600df3]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #27: event_base_loop [0x55c54b6016e2]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #28: Envoy::Event::LibeventScheduler::run() [0x55c54ae87e78]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #29: Envoy::Event::DispatcherImpl::run() [0x55c54a8cd014]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #30: Envoy::Server::WorkerImpl::threadRoutine() [0x55c5496c1642]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #31: Envoy::Server::WorkerImpl::start()::{lambda()#1}::operator()() [0x55c5496c09da]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #32: std::_Function_handler<>::_M_invoke() [0x55c5496c2380]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #33: std::function<>::operator()() [0x55c545fdbdb6]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #34: Envoy::Thread::ThreadImplPosix::ThreadImplPosix()::{lambda()#1}::operator()() [0x55c54b61b0e2]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #35: Envoy::Thread::ThreadImplPosix::ThreadImplPosix()::{lambda()#1}::_FUN() [0x55c54b61b10a]\r\n... [31171][critical][backtrace] [./source/server/backtrace.h:96] #36: start_thread [0x7f0c8e65b609]\r\n```\r\n\r\nHow can I debug this issue? I can post my filter code if needed.\r\n\r\nList of files modified to include the new filter:\r\n1. `api/BUILD`\r\n2. `api/versioning/BUILD`\r\n3. `source/extensions/extensions/build_config.bzl`\r\n4. `source/extensions/extensions_metadata.yaml`\r\n5. `source/extensions/filters/network/well known_names.h`\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30511/comments",
    "author": "amukherj",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2023-10-26T12:34:53Z",
        "body": "I'd suggest GDB?  Offhand I'd suspect something in your init function but GDB should help you verify."
      },
      {
        "user": "amukherj",
        "created_at": "2023-10-26T18:33:35Z",
        "body": "@alyssawilk thank you and that proved adequate. There is a problem in my filter code."
      }
    ]
  },
  {
    "number": 30374,
    "title": "Can someone help me compile a Windows version?",
    "created_at": "2023-10-22T13:12:44Z",
    "closed_at": "2023-11-29T08:01:36Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30374",
    "body": "Please help me compile a Windows version, thank you",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30374/comments",
    "author": "fanhai",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2023-10-22T22:06:44Z",
        "body": "CC @grantbarry29 "
      },
      {
        "user": "dimo414",
        "created_at": "2023-10-23T01:26:51Z",
        "body": "For the time being I believe the best option is to copy the executable out of the Windows Docker image. Hopefully this will change soon.\r\n\r\nRelated: #5668, #22579, #24711, #28588"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-22T04:01:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-29T08:01:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "grantbarry29",
        "created_at": "2023-12-12T21:16:27Z",
        "body": "I build using windows 2019 server VM from Azure. I've been unable to build on windows 11. Install msys2, bazel (I used chocolatey), and windows 2017 c++ using Visual Studio installer on the VM. I believe you need to disable MSYS2's automatic path conversion for the build to work. You should be able to then clone the envoy repo and build from msys2"
      }
    ]
  },
  {
    "number": 30070,
    "title": "Throws 'grpc-status', '12' when using domain name with service name it works",
    "created_at": "2023-10-10T17:26:56Z",
    "closed_at": "2023-11-24T00:02:57Z",
    "labels": [
      "question",
      "stale",
      "area/opentelemetry"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30070",
    "body": "*Title*: *Throws 'grpc-status', '12' when using domain name*\r\n\r\n*Description*:\r\n> I have front running outside Kubernetes cluster, I am configuring opentelemetry tracing to it. Opentelemetry collector is running inside the k8s cluster. When I give domain name log shows 'grpc-status', '12' but with the same config when I run the envoy inside k8s cluster and give local service name and port it is sending spans to opentelemetry collector. \r\n\r\nenvoy version: envoyproxy/envoy:v1.27.0    \r\n\r\n*Config*:\r\n> cluster config:\r\n  - \"@type\": type.googleapis.com/envoy.config.cluster.v3.Cluster\r\n    name: opentelemetry_collector\r\n    type: LOGICAL_DNS\r\n    lb_policy: ROUND_ROBIN\r\n    typed_extension_protocol_options:\r\n      envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n        \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n        explicit_http_config:\r\n          http2_protocol_options: {}\r\n    load_assignment:\r\n      cluster_name: opentelemetry_collector\r\n      endpoints:\r\n        - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: ${OTEL_ADDRESS}\r\n                    port_value: ${OTEL_PORT}\r\n\r\nlds config:\r\n  - \"@type\": type.googleapis.com/envoy.config.listener.v3.Listener\r\n    name: auth_proxy_listener_http\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: ${HTTP_PORT}\r\n    filter_chains:\r\n      - filters:\r\n          - name: envoy.filters.network.http_connection_manager\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n              codec_type: auto\r\n              stat_prefix: ingress_http\r\n              tracing:\r\n                provider:\r\n                  name: envoy.tracers.opentelemetry\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.config.trace.v3.OpenTelemetryConfig\r\n                    grpc_service:\r\n                      envoy_grpc:\r\n                        cluster_name: opentelemetry_collector\r\n                      timeout: 1s\r\n                    service_name: front-envoy\r\n              route_config:\r\n                name: local_route\r\n                virtual_hosts:\r\n                  - name: upstream\r\n                    domains:\r\n                      - \"*\"\r\n                    routes:\r\n                    -----\r\n                    \r\n ingress:-\r\n \r\n apiVersion: projectcontour.io/v1\r\nkind: HTTPProxy\r\nmetadata:\r\n  name: otel-test\r\n  namespace: observability\r\nspec:\r\n  routes:\r\n    - conditions:\r\n        - prefix: /\r\n      services:\r\n        - name: otel-test-opentelemetry-collector\r\n          port: 4317\r\n          protocol: h2c\r\n  virtualhost:\r\n    fqdn: otel-test.com\r\n    tls:\r\n      secretName: ingress-default-cert\r\n\r\n\r\n\r\n*Logs*:\r\n>opentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][67][debug][connection] [source/common/network/connection_impl.cc:948] [C5] connecting to 10.222.17.180:80\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][77][debug][pool] [source/common/conn_pool/conn_pool_base.cc:291] trying to create new connection\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][77][debug][pool] [source/common/conn_pool/conn_pool_base.cc:145] creating a new connection (connecting=0)\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][67][debug][connection] [source/common/network/connection_impl.cc:967] [C5] connection in progress\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][68][debug][connection] [./source/common/network/connection_impl.h:98] [C1] current connecting state: true\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.425][73][debug][router] [source/common/router/router.cc:690] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"14483004215633223044\"] router decoding headers:\r\nopentelemetry-front-envoy-1  | ':method', 'POST'\r\nopentelemetry-front-envoy-1  | ':path', '/opentelemetry.proto.collector.trace.v1.TraceService/Export'\r\nopentelemetry-front-envoy-1  | ':authority', 'opentelemetry_collector'\r\nopentelemetry-front-envoy-1  | ':scheme', 'http'\r\nopentelemetry-front-envoy-1  | 'te', 'trailers'\r\nopentelemetry-front-envoy-1  | 'content-type', 'application/grpc'\r\nopentelemetry-front-envoy-1  | 'x-envoy-internal', 'true'\r\nopentelemetry-front-envoy-1  | 'x-forwarded-for', '172.31.0.2'\r\nopentelemetry-front-envoy-1  |\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][79][debug][client] [source/common/http/codec_client.cc:57] [Tags: \"ConnectionId\":\"2\"] connecting\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][68][debug][client] [source/common/http/codec_client.cc:57] [Tags: \"ConnectionId\":\"1\"] connecting\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][68][debug][connection] [source/common/network/connection_impl.cc:948] [C1] connecting to 10.222.17.180:80\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][79][debug][connection] [source/common/network/connection_impl.cc:948] [C2] connecting to 10.222.17.180:80\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][68][debug][connection] [source/common/network/connection_impl.cc:967] [C1] connection in progress\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][73][debug][pool] [source/common/http/conn_pool_base.cc:78] queueing stream due to no available connections (ready=0 busy=0 connecting=0)\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][73][debug][pool] [source/common/conn_pool/conn_pool_base.cc:291] trying to create new connection\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][73][debug][pool] [source/common/conn_pool/conn_pool_base.cc:145] creating a new connection (connecting=0)\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][77][debug][http2] [source/common/http/http2/codec_impl.cc:1605] [Tags: \"ConnectionId\":\"6\"] updating connection-level initial window size to 268435456\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][77][debug][connection] [./source/common/network/connection_impl.h:98] [C6] current connecting state: true\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][77][debug][client] [source/common/http/codec_client.cc:57] [Tags: \"ConnectionId\":\"6\"] connecting\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][77][debug][connection] [source/common/network/connection_impl.cc:948] [C6] connecting to 10.222.17.180:80\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][73][debug][http2] [source/common/http/http2/codec_impl.cc:1605] [Tags: \"ConnectionId\":\"7\"] updating connection-level initial window size to 268435456\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][73][debug][connection] [./source/common/network/connection_impl.h:98] [C7] current connecting state: true\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][73][debug][client] [source/common/http/codec_client.cc:57] [Tags: \"ConnectionId\":\"7\"] connecting\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][73][debug][connection] [source/common/network/connection_impl.cc:948] [C7] connecting to 10.222.17.180:80\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][77][debug][connection] [source/common/network/connection_impl.cc:967] [C6] connection in progress\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][79][debug][connection] [source/common/network/connection_impl.cc:967] [C2] connection in progress\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.426][73][debug][connection] [source/common/network/connection_impl.cc:967] [C7] connection in progress\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.433][74][debug][http2] [source/common/http/http2/codec_impl.cc:1605] [Tags: \"ConnectionId\":\"8\"] updating connection-level initial window size to 268435456\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.433][74][debug][connection] [./source/common/network/connection_impl.h:98] [C8] current connecting state: true\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.433][74][debug][client] [source/common/http/codec_client.cc:57] [Tags: \"ConnectionId\":\"8\"] connecting\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.433][74][debug][connection] [source/common/network/connection_impl.cc:948] [C8] connecting to 10.222.17.180:80\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.433][74][debug][connection] [source/common/network/connection_impl.cc:967] [C8] connection in progress\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.483][60][debug][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:155] dns resolution without records for manhattan-api.wms-uat.devops.ohlogistics.com\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.483][60][debug][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:155] dns resolution without records for shopify-api.wms-uat.devops.ohlogistics.com\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.484][60][debug][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:278] dns resolution for shopify-api.wms-uat.devops.ohlogistics.com completed with status 0\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.484][60][debug][upstream] [source/extensions/clusters/logical_dns/logical_dns_cluster.cc:112] async DNS resolution complete for shopify-api.wms-uat.devops.ohlogistics.com\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.484][60][debug][upstream] [source/extensions/clusters/logical_dns/logical_dns_cluster.cc:163] DNS refresh rate reset for shopify-api.wms-uat.devops.ohlogistics.com, refresh rate 5000 ms\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.537][60][debug][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:155] dns resolution without records for woocommerce-api.wms-core-pg.npaz.ohl.com\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.538][60][debug][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:278] dns resolution for woocommerce-api.wms-core-pg.npaz.ohl.com completed with status 0\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.538][60][debug][upstream] [source/extensions/clusters/logical_dns/logical_dns_cluster.cc:112] async DNS resolution complete for woocommerce-api.wms-core-pg.npaz.ohl.com\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.538][60][debug][upstream] [source/extensions/clusters/logical_dns/logical_dns_cluster.cc:163] DNS refresh rate reset for woocommerce-api.wms-core-pg.npaz.ohl.com, refresh rate 5000 ms\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.702][60][debug][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:155] dns resolution without records for otel-test.wms-infra-blue-eus.npaz.ohl.com\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.704][60][debug][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:278] dns resolution for otel-test.wms-infra-blue-eus.npaz.ohl.com completed with status 0\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.704][60][debug][upstream] [source/extensions/clusters/logical_dns/logical_dns_cluster.cc:112] async DNS resolution complete for otel-test.wms-infra-blue-eus.npaz.ohl.com\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.704][60][debug][upstream] [source/extensions/clusters/logical_dns/logical_dns_cluster.cc:163] DNS refresh rate reset for otel-test.wms-infra-blue-eus.npaz.ohl.com, refresh rate 5000 ms\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.803][60][debug][connection] [source/common/network/connection_impl.cc:695] [C0] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.803][60][debug][client] [source/common/http/codec_client.cc:88] [Tags: \"ConnectionId\":\"0\"] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.804][60][debug][pool] [source/common/conn_pool/conn_pool_base.cc:328] [Tags: \"ConnectionId\":\"0\"] attaching to next stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.804][60][debug][pool] [source/common/conn_pool/conn_pool_base.cc:182] [Tags: \"ConnectionId\":\"0\"] creating stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.804][60][debug][router] [source/common/router/upstream_request.cc:562] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"5115567610999886236\"] pool ready\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.822][60][debug][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:278] dns resolution for manhattan-api.wms-uat.devops.ohlogistics.com completed with status 0\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.823][60][debug][upstream] [source/extensions/clusters/logical_dns/logical_dns_cluster.cc:112] async DNS resolution complete for manhattan-api.wms-uat.devops.ohlogistics.com\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.823][60][debug][upstream] [source/extensions/clusters/logical_dns/logical_dns_cluster.cc:163] DNS refresh rate reset for manhattan-api.wms-uat.devops.ohlogistics.com, refresh rate 5000 ms\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.846][71][debug][connection] [source/common/network/connection_impl.cc:695] [C3] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.846][79][debug][connection] [source/common/network/connection_impl.cc:695] [C2] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.846][79][debug][client] [source/common/http/codec_client.cc:88] [Tags: \"ConnectionId\":\"2\"] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.846][71][debug][client] [source/common/http/codec_client.cc:88] [Tags: \"ConnectionId\":\"3\"] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.846][79][debug][pool] [source/common/conn_pool/conn_pool_base.cc:328] [Tags: \"ConnectionId\":\"2\"] attaching to next stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][79][debug][pool] [source/common/conn_pool/conn_pool_base.cc:182] [Tags: \"ConnectionId\":\"2\"] creating stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.846][71][debug][pool] [source/common/conn_pool/conn_pool_base.cc:328] [Tags: \"ConnectionId\":\"3\"] attaching to next stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][71][debug][pool] [source/common/conn_pool/conn_pool_base.cc:182] [Tags: \"ConnectionId\":\"3\"] creating stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][71][debug][router] [source/common/router/upstream_request.cc:562] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"10482387932740156988\"] pool ready\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][79][debug][router] [source/common/router/upstream_request.cc:562] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"9221043107495459294\"] pool ready\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][67][debug][connection] [source/common/network/connection_impl.cc:695] [C5] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][67][debug][client] [source/common/http/codec_client.cc:88] [Tags: \"ConnectionId\":\"5\"] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][73][debug][connection] [source/common/network/connection_impl.cc:695] [C7] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][73][debug][client] [source/common/http/codec_client.cc:88] [Tags: \"ConnectionId\":\"7\"] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][67][debug][pool] [source/common/conn_pool/conn_pool_base.cc:328] [Tags: \"ConnectionId\":\"5\"] attaching to next stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][67][debug][pool] [source/common/conn_pool/conn_pool_base.cc:182] [Tags: \"ConnectionId\":\"5\"] creating stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][67][debug][router] [source/common/router/upstream_request.cc:562] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"13908334028172398003\"] pool ready\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][73][debug][pool] [source/common/conn_pool/conn_pool_base.cc:328] [Tags: \"ConnectionId\":\"7\"] attaching to next stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][73][debug][pool] [source/common/conn_pool/conn_pool_base.cc:182] [Tags: \"ConnectionId\":\"7\"] creating stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.847][73][debug][router] [source/common/router/upstream_request.cc:562] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"14483004215633223044\"] pool ready\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][77][debug][connection] [source/common/network/connection_impl.cc:695] [C6] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][68][debug][connection] [source/common/network/connection_impl.cc:695] [C1] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][77][debug][client] [source/common/http/codec_client.cc:88] [Tags: \"ConnectionId\":\"6\"] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][68][debug][client] [source/common/http/codec_client.cc:88] [Tags: \"ConnectionId\":\"1\"] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][81][debug][connection] [source/common/network/connection_impl.cc:695] [C4] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][81][debug][client] [source/common/http/codec_client.cc:88] [Tags: \"ConnectionId\":\"4\"] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][77][debug][pool] [source/common/conn_pool/conn_pool_base.cc:328] [Tags: \"ConnectionId\":\"6\"] attaching to next stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][68][debug][pool] [source/common/conn_pool/conn_pool_base.cc:328] [Tags: \"ConnectionId\":\"1\"] attaching to next stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][68][debug][pool] [source/common/conn_pool/conn_pool_base.cc:182] [Tags: \"ConnectionId\":\"1\"] creating stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][81][debug][pool] [source/common/conn_pool/conn_pool_base.cc:328] [Tags: \"ConnectionId\":\"4\"] attaching to next stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][81][debug][pool] [source/common/conn_pool/conn_pool_base.cc:182] [Tags: \"ConnectionId\":\"4\"] creating stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][77][debug][pool] [source/common/conn_pool/conn_pool_base.cc:182] [Tags: \"ConnectionId\":\"6\"] creating stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][77][debug][router] [source/common/router/upstream_request.cc:562] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"10154506552088034706\"] pool ready\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][68][debug][router] [source/common/router/upstream_request.cc:562] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"223119805621023921\"] pool ready\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.851][81][debug][router] [source/common/router/upstream_request.cc:562] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"10544147708367589109\"] pool ready\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.870][74][debug][connection] [source/common/network/connection_impl.cc:695] [C8] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.870][74][debug][client] [source/common/http/codec_client.cc:88] [Tags: \"ConnectionId\":\"8\"] connected\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.870][74][debug][pool] [source/common/conn_pool/conn_pool_base.cc:328] [Tags: \"ConnectionId\":\"8\"] attaching to next stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.870][74][debug][pool] [source/common/conn_pool/conn_pool_base.cc:182] [Tags: \"ConnectionId\":\"8\"] creating stream\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:24.870][74][debug][router] [source/common/router/upstream_request.cc:562] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"1575837063188313264\"] pool ready\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:25.237][60][debug][client] [source/common/http/codec_client.cc:128] [Tags: \"ConnectionId\":\"0\"] response complete\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:25.237][60][debug][client] [source/common/http/codec_client.cc:136] [Tags: \"ConnectionId\":\"0\"] waiting for encode to complete\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:25.237][60][debug][router] [source/common/router/router.cc:1437] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"5115567610999886236\"] upstream headers complete: end_stream=true\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:25.237][60][debug][router] [source/common/router/upstream_request.cc:469] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"5115567610999886236\"] resetting pool request\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:25.237][60][debug][client] [source/common/http/codec_client.cc:158] [Tags: \"ConnectionId\":\"0\"] request reset\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:25.237][60][debug][pool] [source/common/conn_pool/conn_pool_base.cc:215] [Tags: \"ConnectionId\":\"0\"] destroying stream: 0 remaining\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:25.237][60][debug][http] [source/common/http/async_client_impl.cc:123] async http request response headers (end_stream=true):\r\nopentelemetry-front-envoy-1  | ':status', '200'\r\nopentelemetry-front-envoy-1  | 'content-type', 'application/grpc'\r\nopentelemetry-front-envoy-1  | 'grpc-status', '12'\r\nopentelemetry-front-envoy-1  | 'date', 'Tue, 10 Oct 2023 13:10:26 GMT'\r\nopentelemetry-front-envoy-1  | 'server', 'envoy'\r\nopentelemetry-front-envoy-1  |\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:25.237][60][debug][http2] [source/common/http/http2/codec_impl.cc:1350] [Tags: \"ConnectionId\":\"0\"] stream 1 closed: 0\r\nopentelemetry-front-envoy-1  | [2023-10-10 13:10:25.237][60][debug][http2] [source/common/http/http2/codec_impl.cc:1414] [Tags: \"ConnectionId\":\"0\"] Recouping 0 bytes of flow control window for stream 1.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30070/comments",
    "author": "Prasoon6667",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2023-10-17T17:32:25Z",
        "body": "Is there something different in authority header maybe?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-16T20:01:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-24T00:02:56Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 30037,
    "title": "how about tcp proxy update when listener update",
    "created_at": "2023-10-09T11:22:36Z",
    "closed_at": "2023-11-20T08:03:59Z",
    "labels": [
      "question",
      "stale",
      "area/listener"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/30037",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *how about tcp proxy update in lds update*\r\n\r\n*Description*:\r\nWe use istio 1.16.4 and enable sidecar for our service.\r\nWe have change accesslog config and cause listener update. We find that in the accesslog of envoy there is many responce code is 0, most of them is about cassandra/mysql/redis/kafka.\r\nWe also find that listener for database or kafka is update and old listener is draning.  But in envoy doc，it shows that tcp proxy is not  supported draning(listener file chain config in envoy of kafak/database is tcp_proxy). \r\n\r\nSo we want to know how about tcp proxy update when listener update?\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/30037/comments",
    "author": "id-id-id",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2023-10-09T18:15:16Z",
        "body": "Listener update always causes draining but it's only done gracefullly for protocols that support it (e.g. HTTP2 GOAWAY frame). A non-graceful drain will abruptly terminate the connection eventually, which is why you see the response code 0."
      },
      {
        "user": "id-id-id",
        "created_at": "2023-10-10T06:45:26Z",
        "body": "I have use tcp dump to watch the change in listener update. I noticed a strange phenomenon.The following is the result of packet capture.\r\nNo             Time                       Source           Destination    Protocol           Lengh        Info\r\n69483\t19:15:08.085752\tservice pod ip\tkafka pod ip\tTCP\t\t\t66\t\t49710 → 9092 [FIN, ACK] Seq=17753 Ack=14973 Win=2190 Len=0 TSval=3046944231 TSecr=3833115671\r\n69999\t19:15:08.129126\tkafka pod ip\tservice pod ip\tTCP\t\t\t66\t\t9092 → 49710 [ACK] Seq=14973 Ack=17754 Win=32768 Len=0 TSval=3833116135 TSecr=3046944231\r\n70389\t19:15:08.173839\tkafka pod ip\tservice pod ip\tTCP\t\t\t74\t\t9092 → 49710 [PSH, ACK] Seq=14973 Ack=17754 Win=32768 Len=8 TSval=3833116180 TSecr=3046944231 [TCP segment of a reassembled PDU]\r\n70391\t19:15:08.173840\tkafka pod ip\tservice pod ip\tTCP\t\t\t80\t\t9092 → 49710 [PSH, ACK] Seq=14981 Ack=17754 Win=32768 Len=14 TSval=3833116180 TSecr=3046944231 [TCP segment of a reassembled PDU]\r\n70390\t19:15:08.173856\tservice pod ip\tkafka pod ip\tTCP\t\t\t54\t\t49710 → 9092 [RST] Seq=17754 Win=0 Len=0\r\n70392\t19:15:08.173862\tservice pod ip\tkafka pod ip\tTCP\t\t\t54\t\t49710 → 9092 [RST] Seq=17754 Win=0 Len=0\r\n70393\t19:15:08.173874\tkafka pod ip\tservice pod ip\tKafka\t\t142\t\tKafka Fetch v11 Response\r\n70394\t19:15:08.173877\tservice pod ip\tkafka pod ip\tTCP\t\t\t54\t\t49710 → 9092 [RST] Seq=17754 Win=0 Len=0\r\n70395\t19:15:08.174307\tkafka pod ip\tservice pod ip\tTCP\t\t\t66\t\t9092 → 49710 [FIN, ACK] Seq=15071 Ack=17754 Win=32768 Len=0 TSval=3833116180 TSecr=3046944231\r\n70396\t19:15:08.174314\tservice pod ip\tkafka pod ip\tTCP\t\t\t54\t\t49710 → 9092 [RST] Seq=17754 Win=0 Len=0\r\n\r\nWhen my service send FIN to kafka and receiver ACK from kafka. Then kafka send data to my servcie, but my service return rst to kafka.  It seems that my servcie has forcibly closed the connection, so the port is not listened。However, this process does not conform to tcp's process of closing the connection. \r\n\r\nIs this the design of envoy to update listener which network filter is tcp_proxy?"
      },
      {
        "user": "id-id-id",
        "created_at": "2023-10-14T06:46:55Z",
        "body": "Recently, I have read code of envoy about draning. There is some questions.\r\n\r\n1, I find that when draning listener, it will stop to accepting new connections. For TCP listener, how what happen to old connections？ They can be used normally?\r\n2, After stopListener, it will wait drain-time before remove listener. While waiting, both the client and the server continue to use the existing tcp connection for data interaction.These connections are broken until the function named removeListener is started(The time when the tcp connection started to close was the same as the time when the log showed that the removeListener started). What is the whole process of TCP listener draning? (StopListener will callback another function named maybeCloseSocketsForListener which seem to close socket for listener will be drained. But according to tcpdump result my watch, the connection is used continue until listener be removed)\r\n\r\n@kyessenov "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-13T08:01:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-20T08:03:58Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 29974,
    "title": "Why config is complex and not unmemorable",
    "created_at": "2023-10-05T09:21:23Z",
    "closed_at": "2023-11-12T08:01:09Z",
    "labels": [
      "question",
      "stale",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29974",
    "body": "Hi I intererested envoy it is good but config is yaml and it is wanna proto path in config. It is not memorable, I can't remember never. Why envoy using that syntax? Why not like nginx? Is envoy not for basic jobs doing with config? Is it desined for diffrent using without config file?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29974/comments",
    "author": "efeArdaYildirim",
    "comments": [
      {
        "user": "pxpnetworks",
        "created_at": "2023-10-05T11:55:24Z",
        "body": "Well, I guess Envoy config is not supposed to be read by humans :)"
      },
      {
        "user": "zuercher",
        "created_at": "2023-10-06T00:57:22Z",
        "body": "Envoy configs are defined using protobuf objects. The bootstrap config is loaded from yaml format into a protobuf-generated object. \r\n\r\nEnvoy is extremely extensible and that extensibility requires configuration. Since extension configurations are defined in protobuf as well, everything is uniform and readily machine-generated. I don't see us defining an alternate format."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-05T04:01:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-12T08:01:09Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 29929,
    "title": "When is it planned to move the docker build container from ubuntu 20.04 to 22.04?",
    "created_at": "2023-10-03T17:39:23Z",
    "closed_at": "2023-11-10T00:02:40Z",
    "labels": [
      "enhancement",
      "question",
      "stale",
      "area/docker",
      "area/toolchains"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29929",
    "body": null,
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29929/comments",
    "author": "sambercovici",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2023-10-03T17:47:55Z",
        "body": "good question. i think out-of-the-box this would change which glibc versions we support\r\n\r\nif that is correct then we need to think in terms of supported distros before making a decision\r\n\r\ncc @lizan "
      },
      {
        "user": "phlax",
        "created_at": "2023-10-03T17:49:01Z",
        "body": "@sambercovici was there a specific reason for asking ?"
      },
      {
        "user": "sambercovici",
        "created_at": "2023-10-03T17:59:10Z",
        "body": "I am looking on compiling for arm64 and especially looking for gcc support for SIMD like SVE and SVE2 \r\nUbuntu 20.04 gcc does not support it. Ubuntu 22.04 does."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-02T20:01:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-10T00:02:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 29836,
    "title": "Filesystem config update failure: Unable to parse JSON as proto",
    "created_at": "2023-09-27T13:09:12Z",
    "closed_at": "2023-11-05T04:01:21Z",
    "labels": [
      "question",
      "stale",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29836",
    "body": "*Title*: *One line description*\r\nFilesystem config update failure: Unable to parse JSON as proto. This is happening after upgrading from v1.12 to v1.14\r\n\r\n*Description*:\r\nFilesystem config update failure: Unable to parse JSON as proto (INVALID_ARGUMENT:(http_filters[2].typed_config): invalid value Invalid type URL, unknown type: envoy.extensions.filters.http.health_check.v3.HealthCheck for type Any)\r\n\r\nThis is the config:\r\n```\r\nhttp_filters:\r\n233         - name: envoy.filters.http.local_ratelimit\r\n234           typed_config:\r\n235             \"@type\": type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit\r\n236             stat_prefix: http_local_rate_limiter\r\n237         - name: envoy.filters.http.header_to_metadata\r\n238           typed_config:\r\n239             \"@type\": type.googleapis.com/envoy.extensions.filters.http.header_to_metadata.v3.Config\r\n240         - name: envoy.filters.http.health_check\r\n241           typed_config:\r\n242             \"@type\": type.googleapis.com/envoy.extensions.filters.http.health_check.v3.HealthCheck\r\n243             pass_through_mode: false\r\n244             headers:\r\n245               - name: \":path\"\r\n246                 exact_match: \"/http_health_check\"\r\n247         - name: envoy.filters.http.router\r\n```\r\nTried editing a few things, looks like the typed_config key value is causing problems.\r\nAny help would be appreciated. Thanks!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29836/comments",
    "author": "martha889",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2023-09-28T20:16:39Z",
        "body": "hi @martha889 what version of Envoy are you testing with ?\r\n\r\ncan you check that you are following docs matching the version - if so we can folllow up"
      },
      {
        "user": "phlax",
        "created_at": "2023-09-28T20:17:52Z",
        "body": "apologies - i just read ...\r\n\r\n> v1.12 to v1.14\r\n\r\nunfortunately we no longer support either of those versions - current latest is 1.27.0 - we can proabably help if you run into issues upgrading"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-10-29T04:01:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-05T04:01:20Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 29814,
    "title": "`RESPONSE_CODE` is always zero when added as a response header",
    "created_at": "2023-09-26T17:33:40Z",
    "closed_at": "2023-09-28T14:49:07Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29814",
    "body": "*Title*: `RESPONSE_CODE` is always zero when added as a response header\r\n\r\n*Description*:\r\nI am trying to add `RESPONSE_CODE` to the header of calls going through envoy, but have not been successful. This may sound like an odd request because the call already returns status, but we have a few microservices that are responsible for communicating with third-parties and proxying the response, and we want to be 100% sure the issue is not inside the microservice.\r\n\r\n*Repro steps*:\r\nI have crafted what I think is the simplest possible example where envoy is doing a direct response, and the response code header is still 0. I have tons of other examples but this is the smallest.\r\n```\r\nstatic_resources:\r\n  listeners:\r\n    - name: reverse_proxy\r\n      address:\r\n        socket_address:\r\n          protocol: TCP\r\n          address: 0.0.0.0\r\n          port_value: 10005\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: ingress_http\r\n                route_config:\r\n                  name: local_route\r\n                  response_headers_to_add:\r\n                    - header:\r\n                        key: \"response-code\"\r\n                        value: \"%RESPONSE_CODE%\"\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match:\r\n                            prefix: \"/\"\r\n                          direct_response:\r\n                            status: 200\r\n                            body:\r\n                              inline_string: \"{true}\"\r\n                http_filters:\r\n                  - name: envoy.filters.http.router\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n\r\nAt this point I am thinking its just an edge-case problem, or I am missing something small but critical.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29814/comments",
    "author": "inssein",
    "comments": [
      {
        "user": "inssein",
        "created_at": "2023-09-26T21:52:32Z",
        "body": "I ended up getting it working via lua, but would be nice to keep it simpler :)\r\n\r\n```\r\nhttp_filters:\r\n  - name: lua_response_code\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n      default_source_code:\r\n        inline_string: |\r\n          function envoy_on_response(response_handle)\r\n            response_handle:headers():add(\"response-code\", response_handle:headers():get(\":status\"))\r\n          end\r\n```"
      },
      {
        "user": "StarryVae",
        "created_at": "2023-09-27T08:21:32Z",
        "body": "i think `RESPONSE_CODE` in `response_headers_to_add` is only supported after this patch #29028 , maybe you can update your Envoy version and try again."
      },
      {
        "user": "alyssawilk",
        "created_at": "2023-09-27T12:26:39Z",
        "body": "If this works with modern versions of Envoy and you think it merits backports, let us know!"
      },
      {
        "user": "inssein",
        "created_at": "2023-09-27T16:09:31Z",
        "body": "I just tried the latest dev build and it works! Not a huge rush for us and looks like this will make it out on the next release (2023/10/16)."
      }
    ]
  },
  {
    "number": 29697,
    "title": "Decoding buffer is not linked to the current request (buffer accounting)",
    "created_at": "2023-09-19T13:35:12Z",
    "closed_at": "2023-09-22T13:07:24Z",
    "labels": [
      "question",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29697",
    "body": "The request decoding buffer does not use the account of the request. This means that the buffer accounting feature does not work properly. Currently the memory that is used by the request decoding buffer is not tracked.\r\n\r\nThe buffer is returned by `StreamDecoderFilterCallbacks::decodingBuffer()`, and the account is returned by `StreamDecoderFilterCallbacks::account()`. `Envoy::Buffer::Instance::bindAccount()` can be used to link a buffer with an account.\r\n\r\nIt's also interesting that the accounting feature currently only exists for decoding (requests), not for encoding (responses). I think it should also be implemented for responses: `StreamEncoderFilterCallbacks::encodingBuffer()` should use the account that is returned by `StreamEncoderFilterCallbacks::account()` (new method).\r\n\r\nSee also #15791.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29697/comments",
    "author": "mkauf",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2023-09-19T16:20:27Z",
        "body": "cc @KBaichoo "
      },
      {
        "user": "KBaichoo",
        "created_at": "2023-09-19T20:36:38Z",
        "body": "The buffer is returned by StreamDecoderFilterCallbacks::decodingBuffer(), and the account is returned by StreamDecoderFilterCallbacks::account(). Envoy::Buffer::Instance::bindAccount() can be used to link a buffer with an account.\r\n> The tagging of the memory occurs during the codec phase e.g. attributing the bytes in the system to a particular stream. \r\n> So for example if a downstream request comes and is parsed as some HTTPX stream we will tag those bytes as being charged to that account. Similarly for response from the upstream.  The Slices that move around know the paarticular account, etc. that they're charged to.\r\n\r\nHTH,\r\nKevin "
      },
      {
        "user": "mkauf",
        "created_at": "2023-09-20T16:20:37Z",
        "body": "@KBaichoo : Thank you for the explanation. I have overlooked that the slices in a buffer can have an account, too.\r\n\r\nUnfortunately, it seems that this does not work as intended. The slices of the default decoding buffer do not have an account. I have found this with \"debugging-by-printf\" and the test scenario of #29581."
      },
      {
        "user": "KBaichoo",
        "created_at": "2023-09-22T13:07:24Z",
        "body": "Let's consolidate discussion in that one issue then since it's coupled."
      }
    ]
  },
  {
    "number": 29576,
    "title": "lua filter call static cluster 504 error",
    "created_at": "2023-09-12T09:33:58Z",
    "closed_at": "2023-10-28T12:01:23Z",
    "labels": [
      "bug",
      "question",
      "stale",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29576",
    "body": "*Description*:\r\n\r\ncall the static cluster by lua filter respond 504 status, but curl this API directly is 200 OK\r\n\r\n*Repro steps*:\r\n\r\n- envoy.yaml set a static cluster\r\n- call the static cluster in lua filter by HttpCall\r\n\r\n\r\n*Logs*:\r\n>Include the access logs and the Envoy logs.\r\n\r\n[2023-09-12 09:07:33.608][104][debug][pool] [source/common/http/conn_pool_base.cc:76] queueing stream due to no available connections (ready=0 busy=0 connecting=0)\r\n[2023-09-12 09:07:33.608][104][debug][pool] [source/common/conn_pool/conn_pool_base.cc:291] trying to create new connection\r\n[2023-09-12 09:07:33.608][104][debug][pool] [source/common/conn_pool/conn_pool_base.cc:145] creating a new connection (connecting=0)\r\n[2023-09-12 09:07:33.608][104][debug][connection] [./source/common/network/connection_impl.h:97] [C126] current connecting state: true\r\n[2023-09-12 09:07:33.608][104][debug][client] [source/common/http/codec_client.cc:57] [C126] connecting\r\n[2023-09-12 09:07:33.608][104][debug][connection] [source/common/network/connection_impl.cc:940] [C126] connecting to 10.255.71.16:443\r\n[2023-09-12 09:10:23.072][50][debug][connection] [source/common/network/connection_impl.cc:720] [C1174] write flush complete\r\n[2023-09-12 09:10:23.073][50][debug][connection] [source/common/network/connection_impl.cc:590] [C1174] remote early close\r\n[2023-09-12 09:10:23.073][50][debug][connection] [source/common/network/connection_impl.cc:250] [C1174] closing socket: 0\r\n[2023-09-12 09:10:23.073][50][debug][conn_handler] [source/extensions/listener_managers/listener_manager/active_stream_listener_base.cc:121] [C1174] adding to cleanup list\r\n[2023-09-12 09:10:23.075][103][debug][router] [source/common/router/router.cc:1017] [C0][S9625662234908647271] upstream timeout\r\n[2023-09-12 09:10:23.075][103][debug][router] [source/common/router/upstream_request.cc:545] [C0][S9625662234908647271] resetting pool request\r\n[2023-09-12 09:10:23.075][103][debug][client] [source/common/http/codec_client.cc:156] [C761] request reset\r\n[2023-09-12 09:10:23.075][103][debug][connection] [source/common/network/connection_impl.cc:138] [C761] closing data_to_write=0 type=1\r\n[2023-09-12 09:10:23.075][103][debug][connection] [source/common/network/connection_impl.cc:250] [C761] closing socket: 1\r\n[2023-09-12 09:10:23.075][103][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:321] [C761] SSL shutdown: rc=0\r\n[2023-09-12 09:10:23.075][103][debug][client] [source/common/http/codec_client.cc:106] [C761] disconnect. resetting 0 pending requests\r\n[2023-09-12 09:10:23.075][103][debug][pool] [source/common/conn_pool/conn_pool_base.cc:484] [C761] client disconnected, failure reason:\r\n[2023-09-12 09:10:23.076][103][debug][pool] [source/common/conn_pool/conn_pool_base.cc:453] invoking idle callbacks - is_draining_for_deletion_=false\r\n[2023-09-12 09:10:23.076][103][debug][http] [source/common/http/async_client_impl.cc:122] async http request response headers (end_stream=false):\r\n':status', '504'\r\n'content-length', '24'\r\n'content-type', 'text/plain'\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29576/comments",
    "author": "qicz",
    "comments": [
      {
        "user": "adisuissa",
        "created_at": "2023-09-12T18:59:50Z",
        "body": "Seems that the request times out.\r\nCan you check if the request arrives at its destination, and if a response is sent?\r\n"
      },
      {
        "user": "qicz",
        "created_at": "2023-09-13T02:48:09Z",
        "body": "> Seems that the request times out. Can you check if the request arrives at its destination, and if a response is sent?\r\n\r\nthe destination does not receive anything. but by using curl the destination responds OK "
      },
      {
        "user": "Belrestro",
        "created_at": "2023-09-21T05:16:52Z",
        "body": "Have the same problem, response is 404, but destination does not receive anything."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-10-21T08:01:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-10-28T12:01:22Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 29537,
    "title": "Throwing incompatible issue with dynamic_ot using Jaeger-client-cpp v0.6.0 plugin",
    "created_at": "2023-09-10T08:01:48Z",
    "closed_at": "2023-10-10T17:35:06Z",
    "labels": [
      "question",
      "area/tracing"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29537",
    "body": "*Title*: *Throwing incompatible issue with dynamic_ot using Jaeger-client-cpp v0.6.0 plugin*\r\n\r\n*Description*:\r\nWhen I use jaeger-client-cpp plugin with envoy for tracing it shows error:-\r\n\r\n`Filesystem config update rejected: Error adding/updating listener(s) auth_proxy_listener_https: opentracing: failed to load dynamic library: opentracing: versions of opentracing libraries are incompatible\r\nproxy-front-envoy-1  | auth_proxy_listener_http: opentracing: failed to load dynamic library: opentracing: versions of opentracing libraries are incompatible\r\nproxy-front-envoy-1  |\r\nproxy-front-envoy-1  | [2023-09-10 07:01:32.870][43][debug][config] [source/common/config/filesystem_subscription_impl.cc:61] Failed configuration:`\r\n\r\nDoes envoy removed support for Jaeger-client-cpp? \r\n\r\nIs it because Jaeger-client-cpp in v0.6.0 uses OpenTracing 1.6.0\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29537/comments",
    "author": "Prasoon6667",
    "comments": [
      {
        "user": "adisuissa",
        "created_at": "2023-09-11T13:09:46Z",
        "body": "cc @wbpcode @Shikugawa @basvanbeek as opentracing tracer extension owners.\r\n\r\nFWIW there is currently a plan to deprecate opentracing as the upstream project is frozen (#27401)."
      },
      {
        "user": "wbpcode",
        "created_at": "2023-09-13T03:03:58Z",
        "body": "Yeah, because the `OPENTRACING_ABI_VERSION` is updated to `3` in the  OpenTracing 1.6.0."
      }
    ]
  },
  {
    "number": 29409,
    "title": "Why does `retry policy` send all of requests to the unhealthy endpoint",
    "created_at": "2023-09-04T11:49:15Z",
    "closed_at": "2023-09-07T03:45:40Z",
    "labels": [
      "question",
      "area/retry"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29409",
    "body": "I deployed two grpc servers as a cluster and I configured `retry_policy` as below:\r\n```\r\n...\r\nvirtual_hosts:\r\n- name: test\r\n  domains: [\"*\"]\r\n  routes:\r\n  - match:\r\n      prefix: \"/com.api.test\"\r\n    route:\r\n      cluster: cluster_test\r\n      retry_policy:\r\n        retry_on: \"5xx,unavailable.connection-failure\"\r\n        num_retries: 1\r\n        per_try_timeout: 5s\r\n...\r\nclusters:\r\n- name: cluster_test\r\n... \r\n  load_assignment:\r\n    endpoints:\r\n    - lb_endpoints:\r\n      - endpoint: {address: {socket_address: {address: 192.168.10.1, port_value: 10000}}}\r\n      - endpoint: {address: {socket_address: {address: 192.168.10.2, port_value: 10000}}} ### timeout endpoint\r\n```\r\nNow, I intentionally make the endpoint `192.168.10.2:10000` timeout, which means that any request routed to this endpoint will get the grpc timeout error: `14 UNAVAILABLE`.\r\n\r\nI was expecting that some requets could be routed to `192.168.10.1:10000` and others are routed to the timeout endpoint. And for those requests of `192.168.10.2:10000`, they should spend more than 10 seconds to get their responses because they need to do retry on the other endpoint.\r\n\r\nBut to my surprise, it seems that all of requests are routed to the timeout endpoint because they all spent more than 10 seconds. \r\n\r\nWhy does Envoy route all of requests to the unhealthy endpoint? Or did I misunderstand something?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29409/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-09-05T12:15:27Z",
        "body": "Could your check your debug log first to see which endpoint is selected first? And you can paste related log here. I think the community cannot help with current context that you provided."
      },
      {
        "user": "YvesZHI",
        "created_at": "2023-09-06T06:55:05Z",
        "body": "@wbpcode   I just enabled debug-level log and I executed `tail -f *.log | grep -vi dns | grep -v transport | grep -v flushing` in the directory of the logfile.\r\n\r\nHere is the output of Envoy log:\r\n```\r\n{\"log\":\"[2023-09-06 14:40:53.122][38][debug][http] [source/common/http/conn_manager_impl.cc:299] [C0] new stream\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123588959Z\"}\r\n{\"log\":\"[2023-09-06 14:40:53.122][38][debug][http] [source/common/http/conn_manager_impl.cc:904] [C0][S13909738579083801758] request headers complete (end_stream=false):\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123637868Z\"}\r\n{\"log\":\"':scheme', 'http'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123646029Z\"}\r\n{\"log\":\"':path', '/common.protobuf.CommonServices/ping'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123651588Z\"}\r\n{\"log\":\"':method', 'POST'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123658164Z\"}\r\n{\"log\":\"':authority', '192.168.117.52:10000'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123663836Z\"}\r\n{\"log\":\"'grpc-accept-encoding', 'identity,deflate,gzip'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123669846Z\"}\r\n{\"log\":\"'accept-encoding', 'identity'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123675497Z\"}\r\n{\"log\":\"'user-agent', 'grpc-node-js/1.8.10'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.12372128Z\"}\r\n{\"log\":\"'content-type', 'application/grpc'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123745213Z\"}\r\n{\"log\":\"'te', 'trailers'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.12376724Z\"}\r\n{\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123789286Z\"}\r\n{\"log\":\"[2023-09-06 14:40:53.122][38][debug][connection] [./source/common/network/connection_impl.h:89] [C0] current connecting state: false\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123819508Z\"}\r\n{\"log\":\"[2023-09-06 14:40:53.122][38][debug][router] [source/common/router/router.cc:470] [C0][S13909738579083801758] cluster 'cluster_ac_dbproxy_http' match for URL '/common.protobuf.CommonServices/ping'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123839613Z\"}\r\n{\"log\":\"[2023-09-06 14:40:53.122][38][debug][router] [source/common/router/router.cc:673] [C0][S13909738579083801758] router decoding headers:\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123897752Z\"}\r\n{\"log\":\"':scheme', 'https'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123929497Z\"}\r\n{\"log\":\"':path', '/common.protobuf.CommonServices/ping'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123951799Z\"}\r\n{\"log\":\"':method', 'POST'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.123973919Z\"}\r\n{\"log\":\"':authority', '192.168.117.52:10000'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124000594Z\"}\r\n{\"log\":\"'grpc-accept-encoding', 'identity,deflate,gzip'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124054754Z\"}\r\n{\"log\":\"'accept-encoding', 'identity'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124064911Z\"}\r\n{\"log\":\"'user-agent', 'grpc-node-js/1.8.10'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124070453Z\"}\r\n{\"log\":\"'content-type', 'application/grpc'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124082757Z\"}\r\n{\"log\":\"'te', 'trailers'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124151448Z\"}\r\n{\"log\":\"'x-forwarded-proto', 'https'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124174273Z\"}\r\n{\"log\":\"'x-request-id', '27644626-b36d-40ca-914b-299820023cd1'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124196132Z\"}\r\n{\"log\":\"'x-envoy-expected-rq-timeout-ms', '5000'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124217794Z\"}\r\n{\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124239917Z\"}\r\n{\"log\":\"[2023-09-06 14:40:53.123][38][debug][pool] [source/common/conn_pool/conn_pool_base.cc:252] [C1] using existing connection\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124261907Z\"}\r\n{\"log\":\"[2023-09-06 14:40:53.123][38][debug][pool] [source/common/conn_pool/conn_pool_base.cc:177] [C1] creating stream\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124280793Z\"}\r\n{\"log\":\"[2023-09-06 14:40:53.123][38][debug][router] [source/common/router/upstream_request.cc:420] [C0][S13909738579083801758] pool ready\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.124306653Z\"}\r\n{\"log\":\"[2023-09-06 14:40:53.123][38][debug][http] [source/common/http/filter_manager.cc:849] [C0][S13909738579083801758] request end stream\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:53.12432856Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.122][38][debug][router] [source/common/router/upstream_request.cc:381] [C0][S13909738579083801758] upstream per try timeout\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.123079186Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.122][38][debug][router] [source/common/router/upstream_request.cc:343] [C0][S13909738579083801758] resetting pool request\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.123194627Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.122][38][debug][client] [source/common/http/codec_client.cc:139] [C1] request reset\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.12320535Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.122][38][debug][pool] [source/common/conn_pool/conn_pool_base.cc:210] [C1] destroying stream: 0 remaining\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.123213892Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.122][38][debug][http2] [source/common/http/http2/codec_impl.cc:1300] [C1] sent reset code=0\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.123222342Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.122][38][debug][http2] [source/common/http/http2/codec_impl.cc:1417] [C1] stream 7 closed: 0\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.123265199Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.128][38][debug][router] [source/common/router/router.cc:1749] [C0][S13909738579083801758] performing retry\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.128406083Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.128][38][debug][pool] [source/common/conn_pool/conn_pool_base.cc:252] [C2] using existing connection\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.128463609Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.128][38][debug][pool] [source/common/conn_pool/conn_pool_base.cc:177] [C2] creating stream\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.128521912Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.128][38][debug][router] [source/common/router/upstream_request.cc:420] [C0][S13909738579083801758] pool ready\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.128536355Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.136][38][debug][router] [source/common/router/router.cc:1331] [C0][S13909738579083801758] upstream headers complete: end_stream=false\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.136372317Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.136][38][debug][http] [source/common/http/conn_manager_impl.cc:1516] [C0][S13909738579083801758] encoding headers via codec (end_stream=false):\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.136407721Z\"}\r\n{\"log\":\"':status', '200'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.136415879Z\"}\r\n{\"log\":\"'content-type', 'application/grpc'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.136428036Z\"}\r\n{\"log\":\"'grpc-encoding', 'identity'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.136473178Z\"}\r\n{\"log\":\"'grpc-accept-encoding', 'gzip'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.136520892Z\"}\r\n{\"log\":\"'x-envoy-upstream-service-time', '5012'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.136576984Z\"}\r\n{\"log\":\"'date', 'Wed, 06 Sep 2023 06:40:57 GMT'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.136589409Z\"}\r\n{\"log\":\"'server', 'envoy'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.136605667Z\"}\r\n{\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.136666016Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.137][38][debug][client] [source/common/http/codec_client.cc:126] [C2] response complete\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.13768149Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.137][38][debug][pool] [source/common/conn_pool/conn_pool_base.cc:210] [C2] destroying stream: 0 remaining\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.13771039Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.137][38][debug][http] [source/common/http/conn_manager_impl.cc:1533] [C0][S13909738579083801758] encoding trailers via codec:\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.137718333Z\"}\r\n{\"log\":\"'grpc-status', '0'\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.137724759Z\"}\r\n{\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.137743909Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.137][38][debug][http2] [source/common/http/http2/codec_impl.cc:1417] [C0] stream 7 closed: 0\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.137789571Z\"}\r\n{\"log\":\"[2023-09-06 14:40:58.137][38][debug][http2] [source/common/http/http2/codec_impl.cc:1417] [C2] stream 7 closed: 0\\n\",\"stream\":\"stderr\",\"time\":\"2023-09-06T06:40:58.137823826Z\"}\r\n{\"log\":\"[2023-09-06T06:40:53.122Z] \\\"POST /common.protobuf.CommonServices/ping HTTP/2\\\" 200 - 5 11 5015 5012 \\\"-\\\" \\\"grpc-node-js/1.8.10\\\" \\\"27644626-b36d-40ca-914b-299820023cd1\\\" \\\"192.168.117.52:10000\\\" \\\"192.168.10.1:10000\\\"\\n\",\"stream\":\"stdout\",\"time\":\"2023-09-06T06:40:58.592469404Z\"}\r\n```\r\n\r\n`192.168.117.52:10000` is the listener of Envoy.\r\n\r\nSo I used postman to simulate a client to send requests one by one. Each request would generate the same log as above. It seems that all of requests would trigger `upstream per try timeout` and then be routed to `192.168.10.1:10000`."
      },
      {
        "user": "wbpcode",
        "created_at": "2023-09-07T01:14:38Z",
        "body": "It's a little weird. From your description, seems that the `192.168.10.2` always be selected first. Could you replace you lb plicy to random and try again?"
      },
      {
        "user": "YvesZHI",
        "created_at": "2023-09-07T02:49:19Z",
        "body": "@wbpcode The RANDOM lb_policy works as expected. There are three kinds of cases:\r\n1. Some requests are directly routed to `192.168.10.1`;\r\n2. Some requests are directly routed to `192.168.10.2` retry on `192.168.10.2`, so timeout errors occur;\r\n3. Some requests are directly routed to `192.168.10.2` retry on `192.168.10.1`, so these requests spend about 5 seconds.\r\n\r\nIf I add `envoy.retry_host_predicates.previous_hosts` to the retry_policy, the case 2 will disappear.\r\n\r\nI think I might have figured out the issue.\r\n\r\n`lb_policy` would decide the host selection while retrying/reattempting. So under the ROUND_ROBIN lb_policy, when a request is routed to `192.168.10.2`, it would definitely be routed to `192.168.10.1` to retry. So the next request must be routed to `192.168.10.2` as the last request is routed  to  `192.168.10.1`..."
      },
      {
        "user": "wbpcode",
        "created_at": "2023-09-07T03:35:07Z",
        "body": "> lb_policy would decide the host selection while retrying/reattempting. So under the ROUND_ROBIN lb_policy, when a request is routed to 192.168.10.2, it would definitely be routed to 192.168.10.1 to retry. So the next request must be routed to 192.168.10.2 as the last request is routed to 192.168.10.1...\r\n\r\nYeah, this is also my guession."
      }
    ]
  },
  {
    "number": 29370,
    "title": "Envoy returns 404 for socket address with svc.cluster.local suffix",
    "created_at": "2023-08-31T23:01:29Z",
    "closed_at": "2023-10-12T16:01:37Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29370",
    "body": "*Title*: *Envoy returns 404 for socket address with svc.cluster.local suffix*\r\n\r\n*Description*:\r\nWe had a working solution for envoy. \r\nBut than it was required to change socket_address.address for each cluster so that it has 'svc.cluster.local' suffix.\r\nAfter this change all the responses become 404.\r\n\r\n*Repro steps*:\r\nThe configuration for the cluster:\r\n```\r\n{\r\n     \"version_info\": \"1693358303846006511\",\r\n     \"cluster\": {\r\n      \"@type\": \"type.googleapis.com/envoy.config.cluster.v3.Cluster\",\r\n      \"name\": \"mytest-server\",\r\n      \"type\": \"STRICT_DNS\",\r\n      \"connect_timeout\": \"120s\",\r\n      \"lb_policy\": \"LEAST_REQUEST\",\r\n      \"circuit_breakers\": {\r\n       \"thresholds\": [\r\n        {\r\n         \"max_connections\": 1048576,\r\n         \"max_pending_requests\": 1048576,\r\n         \"max_requests\": 1048576,\r\n         \"max_retries\": 1048576\r\n        },\r\n        {\r\n         \"priority\": \"HIGH\",\r\n         \"max_connections\": 1048576,\r\n         \"max_pending_requests\": 1048576,\r\n         \"max_requests\": 1048576,\r\n         \"max_retries\": 1048576\r\n        }\r\n       ]\r\n      },\r\n      \"lb_subset_config\": {\r\n       \"fallback_policy\": \"DEFAULT_SUBSET\",\r\n       \"default_subset\": {\r\n        \"version\": \"v1\"\r\n       },\r\n       \"subset_selectors\": [\r\n        {\r\n         \"keys\": [\r\n          \"version\"\r\n         ]\r\n        }\r\n       ]\r\n      },\r\n      \"load_assignment\": {\r\n       \"cluster_name\": \"mytest-server\",\r\n       \"endpoints\": [\r\n        {\r\n         \"lb_endpoints\": [\r\n          {\r\n           \"endpoint\": {\r\n            \"address\": {\r\n             \"socket_address\": {\r\n              \"address\": \"mytest-server.my-testproject-namespace.svc.cluster.local\",\r\n              \"port_value\": 8080\r\n             }\r\n            }\r\n           },\r\n           \"metadata\": {\r\n            \"filter_metadata\": {\r\n             \"envoy.lb\": {\r\n              \"version\": \"v1\"\r\n             }\r\n            }\r\n           }\r\n          }\r\n         ]\r\n        }\r\n       ]\r\n      },\r\n      \"typed_extension_protocol_options\": {\r\n       \"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\": {\r\n        \"@type\": \"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\r\n        \"explicit_http_config\": {\r\n         \"http_protocol_options\": {\r\n          \"allow_chunked_length\": true\r\n         }\r\n        }\r\n       }\r\n      }\r\n     },\r\n     \"last_updated\": \"2023-08-30T01:18:24.261Z\"\r\n    },\r\n```\r\n\r\n**Note**: \r\nIn the logs of envoy there are no errors.\r\nI see\r\n```\r\n[debug][12][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:275] dns resolution for mytest-server.my-testproject-namespace.svc.cluster.local completed with status 0\r\n[debug][12][upstream] [source/common/upstream/upstream_impl.cc:457] transport socket match, socket default selected for host with address 172.30.6.134:8080\r\n[debug][12][upstream] [source/extensions/clusters/strict_dns/strict_dns_cluster.cc:177] DNS refresh rate reset for mytest-server.my-testproject-namespace.svc.cluster.local, refresh rate 5000 ms\r\n```\r\nIf I send the request directly to mytest-server.my-testproject-namespace.svc.cluster.local , it works.\r\n\r\n\r\n*Logs*:\r\n```\r\n[2023-08-31 22:44:27.235][debug][20][http] [source/common/http/conn_manager_impl.cc:1039] [C813][S2675663578643635050] request headers complete (end_stream=false):\r\n':authority', 'envoy-gateway-my-testproject-namespace.test.com'\r\n':path', '/api/v2/test'\r\n':method', 'POST'\r\n'x-request-id', 'a4d1f54cefc44dfbd05cf225c99f5088'\r\n'x-real-ip', '10.10.10.26'\r\n'x-forwarded-for', '10.10.10.26'\r\n'x-forwarded-host', 'envoy-gateway-my-testproject-namespace.test.com'\r\n'x-forwarded-port', '80'\r\n'x-forwarded-proto', 'http'\r\n'x-forwarded-scheme', 'http'\r\n'x-scheme', 'http'\r\n'content-length', '308'\r\n'authorization', 'Bearer ****'\r\n'content-type', 'application/json'\r\n'user-agent', 'PostmanRuntime/7.28.4'\r\n'accept', '*/*'\r\n'postman-token', '1e67cc83-408d-485b-a2c3-a04bf7ba94b7'\r\n'accept-encoding', 'gzip, deflate, br'\r\n\r\n[2023-08-31 22:44:27.235][debug][20][connection] [./source/common/network/connection_impl.h:98] [C813] current connecting state: false\r\n[2023-08-31 22:44:27.235][debug][20][router] [source/common/router/router.cc:478] [C0][S16948381921116986791] cluster 'test-authz' match for URL '/envoy.service.auth.v3.Authorization/Check'\r\n[2023-08-31 22:44:27.235][debug][20][router] [source/common/router/router.cc:686] [C0][S16948381921116986791] router decoding headers:\r\n':method', 'POST'\r\n':path', '/envoy.service.auth.v3.Authorization/Check'\r\n':authority', 'test-authz'\r\n':scheme', 'http'\r\n'te', 'trailers'\r\n'grpc-timeout', '15000m'\r\n'content-type', 'application/grpc'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '10.10.10.20'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2023-08-31 22:44:27.235][debug][20][pool] [source/common/conn_pool/conn_pool_base.cc:265] [C814] using existing fully connected connection\r\n[2023-08-31 22:44:27.235][debug][20][pool] [source/common/conn_pool/conn_pool_base.cc:182] [C814] creating stream\r\n[2023-08-31 22:44:27.235][debug][20][router] [source/common/router/upstream_request.cc:650] [C0][S16948381921116986791] pool ready\r\n[2023-08-31 22:44:27.236][debug][20][client] [source/common/http/codec_client.cc:139] [C814] encode complete\r\n[2023-08-31 22:44:27.236][debug][20][http] [source/common/http/filter_manager.cc:485] [C813][S2675663578643635050] request data too large watermark exceeded\r\n[2023-08-31 22:44:27.236][debug][20][http] [source/common/http/conn_manager_impl.cc:1720] [C813][S2675663578643635050] Read-disabling downstream stream due to filter callbacks.\r\n[2023-08-31 22:44:27.236][debug][20][http] [source/common/http/conn_manager_impl.cc:1022] [C813][S2675663578643635050] request end stream\r\n[2023-08-31 22:44:27.236][debug][20][connection] [source/common/network/connection_impl.cc:523] [C813] onBelowReadBufferLowWatermark\r\n[2023-08-31 22:44:27.263][debug][20][router] [source/common/router/router.cc:1442] [C0][S16948381921116986791] upstream headers complete: end_stream=false\r\n[2023-08-31 22:44:27.263][debug][20][http] [source/common/http/async_client_impl.cc:123] async http request response headers (end_stream=false):\r\n':status', '200'\r\n'content-type', 'application/grpc'\r\n'x-envoy-upstream-service-time', '27'\r\n\r\n[2023-08-31 22:44:27.263][debug][20][client] [source/common/http/codec_client.cc:126] [C814] response complete\r\n[2023-08-31 22:44:27.263][debug][20][pool] [source/common/conn_pool/conn_pool_base.cc:215] [C814] destroying stream: 0 remaining\r\n[2023-08-31 22:44:27.263][debug][20][http] [source/common/http/async_client_impl.cc:150] async http request response trailers:\r\n'grpc-status', '0'\r\n'grpc-message', ''\r\n\r\n[2023-08-31 22:44:27.263][debug][20][ext_authz] [source/extensions/filters/http/ext_authz/ext_authz.cc:253] [C813][S2675663578643635050] ext_authz is clearing route cache\r\n[2023-08-31 22:44:27.263][debug][20][router] [source/common/router/router.cc:478] [C813][S2675663578643635050] cluster 'mytest-server' match for URL '/api/v2/test'\r\n[2023-08-31 22:44:27.263][debug][20][router] [source/common/router/router.cc:686] [C813][S2675663578643635050] router decoding headers:\r\n':authority', 'envoy-gateway-my-testproject-namespace.test.com'\r\n':path', '/api/v2/test'\r\n':method', 'POST'\r\n':scheme', 'http'\r\n'x-request-id', 'a4d1f54cefc44dfbd05cf225c99f5088'\r\n'x-real-ip', '10.10.10.26'\r\n'x-forwarded-for', '10.10.10.26'\r\n'x-forwarded-host', 'envoy-gateway-my-testproject-namespace.test.com'\r\n'x-forwarded-port', '80'\r\n'x-forwarded-proto', 'http'\r\n'x-forwarded-scheme', 'http'\r\n'x-scheme', 'http'\r\n'content-length', '308'\r\n'content-type', 'application/json'\r\n'user-agent', 'PostmanRuntime/7.28.4'\r\n'accept', '*/*'\r\n'postman-token', '1e67cc83-408d-485b-a2c3-a04bf7ba94b7'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'x-envoy-internal', 'true'\r\n'authorization', 'Bearer ****'\r\n'x-envoy-expected-rq-timeout-ms', '120000'\r\n'x-envoy-original-path', '/api/v2/test'\r\n\r\n[2023-08-31 22:44:27.263][debug][20][pool] [source/common/http/conn_pool_base.cc:78] queueing stream due to no available connections (ready=0 busy=0 connecting=0)\r\n[2023-08-31 22:44:27.263][debug][20][pool] [source/common/conn_pool/conn_pool_base.cc:291] trying to create new connection\r\n[2023-08-31 22:44:27.263][debug][20][pool] [source/common/conn_pool/conn_pool_base.cc:145] creating a new connection (connecting=0)\r\n[2023-08-31 22:44:27.263][debug][20][connection] [./source/common/network/connection_impl.h:98] [C836] current connecting state: true\r\n[2023-08-31 22:44:27.263][debug][20][client] [source/common/http/codec_client.cc:57] [C836] connecting\r\n[2023-08-31 22:44:27.263][debug][20][connection] [source/common/network/connection_impl.cc:941] [C836] connecting to 172.30.6.134:8080\r\n[2023-08-31 22:44:27.264][debug][20][connection] [source/common/network/connection_impl.cc:960] [C836] connection in progress\r\n[2023-08-31 22:44:27.264][debug][20][http] [source/common/http/conn_manager_impl.cc:1710] [C813][S2675663578643635050] Read-enabling downstream stream due to filter callbacks.\r\n[2023-08-31 22:44:27.264][debug][20][http] [source/common/http/filter_manager.cc:485] [C813][S2675663578643635050] request data too large watermark exceeded\r\n[2023-08-31 22:44:27.264][debug][20][http] [source/common/http/conn_manager_impl.cc:1720] [C813][S2675663578643635050] Read-disabling downstream stream due to filter callbacks.\r\n[2023-08-31 22:44:27.264][debug][20][http2] [source/common/http/http2/codec_impl.cc:1331] [C814] stream 3 closed: 0\r\n[2023-08-31 22:44:27.264][debug][20][http2] [source/common/http/http2/codec_impl.cc:1395] [C814] Recouping 0 bytes of flow control window for stream 3.\r\n[2023-08-31 22:44:27.265][debug][20][connection] [source/common/network/connection_impl.cc:688] [C836] connected\r\n[2023-08-31 22:44:27.265][debug][20][client] [source/common/http/codec_client.cc:88] [C836] connected\r\n[2023-08-31 22:44:27.265][debug][20][pool] [source/common/conn_pool/conn_pool_base.cc:328] [C836] attaching to next stream\r\n[2023-08-31 22:44:27.265][debug][20][pool] [source/common/conn_pool/conn_pool_base.cc:182] [C836] creating stream\r\n[2023-08-31 22:44:27.265][debug][20][router] [source/common/router/upstream_request.cc:650] [C813][S2675663578643635050] pool ready\r\n[2023-08-31 22:44:27.265][debug][20][http] [source/common/http/conn_manager_impl.cc:1710] [C813][S2675663578643635050] Read-enabling downstream stream due to filter callbacks.\r\n[2023-08-31 22:44:27.265][debug][20][client] [source/common/http/codec_client.cc:139] [C836] encode complete\r\n[2023-08-31 22:44:27.266][debug][20][client] [source/common/http/codec_client.cc:126] [C836] response complete\r\n[2023-08-31 22:44:27.266][debug][20][router] [source/common/router/router.cc:1442] [C813][S2675663578643635050] upstream headers complete: end_stream=true\r\n[2023-08-31 22:44:27.266][debug][20][http] [source/common/http/conn_manager_impl.cc:1680] [C813][S2675663578643635050] encoding headers via codec (end_stream=true):\r\n':status', '404'\r\n'date', 'Thu, 31 Aug 2023 22:44:27 GMT'\r\n'content-length', '0'\r\n'x-envoy-upstream-service-time', '1'\r\n'x-content-type-options', 'nosniff'\r\n'x-xss-protection', '0'\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29370/comments",
    "author": "anvo1115",
    "comments": [
      {
        "user": "anvo1115",
        "created_at": "2023-08-31T23:03:04Z",
        "body": "Probably my socket_address.address is too long?"
      },
      {
        "user": "wbpcode",
        "created_at": "2023-09-05T11:40:24Z",
        "body": "I think the 404 is returned by the upstream  from the response header `'x-envoy-upstream-service-time', '1'`.\r\n\r\nIt would be better to check the log of upstream server first to see why it return a 404."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-10-05T12:01:57Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-10-12T16:01:36Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 29315,
    "title": "Is it necessary for go extension route config to store the configuration of different plugins",
    "created_at": "2023-08-29T09:59:35Z",
    "closed_at": "2023-10-07T12:01:41Z",
    "labels": [
      "question",
      "stale",
      "area/golang"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29315",
    "body": "*Title*: *Is it necessary for go extension route config to store the configuration of different plugins?*\r\n\r\n*Description*:\r\n```\r\nmessage ConfigsPerRoute {\r\n  // Configuration of the Go plugin at the per-router or per-virtualhost level,\r\n  // keyed on the :ref:`plugin_name <envoy_v3_api_field_extensions.filters.http.golang.v3alpha.Config.plugin_name>`\r\n  // of the Go plugin.\r\n  //\r\n  map<string, RouterPlugin> plugins_config = 1;\r\n}\r\n```\r\nSince now getting per filter config by the http filter config name is supported, is it necessary for go extension route config to  store the configuration of different go filters? For example, if we have two go filters `go-basic-auth` and `go-rate-limit`, for `http_filters` and `typed_per_filter_config`, the config name can be the same, so the go filter can get its own route config.\r\n\r\n```\r\n          http_filters:\r\n          - name: go-basic-auth\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\r\n              ......\r\n          - name: go-rate-limit\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\r\n              ......\r\n\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n              - name: host-one\r\n                domains:\r\n                  - \"*\"\r\n                routes:\r\n                  - match:\r\n                      prefix: \"/\"\r\n                    route:\r\n                      cluster: httpbin\r\n                    typed_per_filter_config:\r\n                      go-basic-auth:\r\n                        \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.ConfigsPerRoute\r\n                        config:\r\n                        ......\r\n                      go-rate-limit:\r\n                        \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.ConfigsPerRoute\r\n                        config:\r\n                        ......\r\n```\r\n\r\nIn this way, the go filter will just be like a c++ filter, every go filter has its own route config, which maybe simple for control plane.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29315/comments",
    "author": "StarryVae",
    "comments": [
      {
        "user": "StarryVae",
        "created_at": "2023-08-29T10:00:00Z",
        "body": "cc @doujiang24 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T00:50:34Z",
        "body": "@StarryVae Sorry, I may not understand you correctly. For now, every go filter has its own per route config is already supported. Now, as you said `ConfigsPerRoute` store per route configs for each golang filters, indexed by their plugin name, and each golang filter could get this own per route config by the plugin name.\r\n\r\nCould you please describe the changes that you sugguested? I haven't see what do we need to change. maybe a little design doc? Thanks."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-30T02:09:43Z",
        "body": "Ah, what i mean is that golang filter route config may not need store per route configs for each golang filters, it could only be its own route config just like c++ filter:\r\n\r\n```\r\nmessage ConfigPerRoute {\r\n  oneof override {\r\n    option (validate.required) = true;\r\n\r\n    // [#not-implemented-hide:]\r\n    // Disable the filter for this particular vhost or route.\r\n    // If disabled is specified in multiple per-filter-configs, the most specific one will be used.\r\n    bool disabled = 1 [(validate.rules).bool = {const: true}];\r\n\r\n    // The config field is used for setting per-route and per-virtualhost plugin config.\r\n    google.protobuf.Any config = 2;\r\n  }\r\n}\r\n```"
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T03:59:07Z",
        "body": "Oh, I see, your suggestion is not about lack of capability, but about implementation of capability.\r\n\r\nWe have also considered the way that you suggested, which is also a attractive solution, it's simper.\r\nHowever, we finally chose the current implementation way, since we do think it is more controllable for the golang plugin ecosystem, for example, the plugin name does not need to care about the conflicting about the existing name."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-30T06:26:41Z",
        "body": "`for example, the plugin name does not need to care about the conflicting about the existing name.`\r\n\r\ndoes the plugin name refer to http filter name `go-basic-auth` above? if so, there may be conflicts on route config, but it can be avoided by control plane. Anyway, the design now is also fine, just a littile bit complex. 😄 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T11:00:02Z",
        "body": "nope, each golang filter/plugin has its own plugin name, which is defined in `Golang.Config` proto:\r\n```\r\n  string plugin_name = 3 [(validate.rules).string = {min_len: 1}];\r\n```\r\nalso golang source code register golang plugin with that name, see: `RegisterHttpFilterConfigFactoryAndParser`.\r\nit's another name compare to the http filter name, so that we do not need to care about the confliction.\r\n\r\nyep, they both should works, we choose to introduce the new plugin name, since we think it might be more controllable, as said above."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-30T11:27:15Z",
        "body": "emm,,, if the plugin name refers to `plugin_name` in `Golang.Config`, then i am confused about what confliction it can solve? 😕 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T13:59:17Z",
        "body": "after discussed in wechat, summary a bit here:\r\nin the current implementation, introduce a new `plugin_name`, so that the per route config won't rely on the http filter name.\r\neven two golang plugins/filters in the http filter chain, has the same http filter name, the two plugins still have their own per route plugin config, since the keys in the `ConfigPerRoute. plugins_config ` are different.\r\n\r\non the other hand, rely on the http filter name, is also a workable solution, and it is simpler."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-31T06:08:09Z",
        "body": "yes, but how can we ensure the keys in the `ConfigPerRoute. plugins_config`  are different? 😕 "
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-31T07:13:34Z",
        "body": "as we discussed in wechat, the `plugin name` should be better suited for resolving potential routing configuration conflicts, thanks."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-09-30T08:01:31Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-10-07T12:01:40Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 29260,
    "title": "should go extension support header operation in body phase?",
    "created_at": "2023-08-25T09:40:15Z",
    "closed_at": "2023-08-28T06:52:15Z",
    "labels": [
      "enhancement",
      "question",
      "area/go"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29260",
    "body": "*Title*: *should go extension support header operation in body phase?*\r\n\r\n*Description*:\r\ninspired by #28587 , should go extension also support header operation in body phase? since some filters may need to change headers according to some content from body.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29260/comments",
    "author": "StarryVae",
    "comments": [
      {
        "user": "StarryVae",
        "created_at": "2023-08-25T09:40:37Z",
        "body": "cc @doujiang24 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-26T01:18:28Z",
        "body": "yep, I do think the golang extension already support it.\r\nGo could return `StopAndBufferWatermark` in `DecodeHeaders`, then Go will receive data in `DecodeData` and could still write headers before invoke `Continue`.\r\nGolang extension is designed to be able do the same thing(almostly) align to C++."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-28T06:51:46Z",
        "body": "oh, i see, we can store the headers in the filter object just like what C++ do 🤣 ."
      }
    ]
  },
  {
    "number": 29167,
    "title": "Stat to indicate a connection reuse?",
    "created_at": "2023-08-21T13:50:39Z",
    "closed_at": "2023-09-28T20:01:32Z",
    "labels": [
      "question",
      "stale",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29167",
    "body": "Is there a stat that indicates that a new HTTP stream is reusing an existing upstream connection rather than creating a new one?\r\nWill appreciate guidance if this info can be inferred from existing stats, otherwise please mark as help-wanted.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29167/comments",
    "author": "ohadvano",
    "comments": [
      {
        "user": "RyanTheOptimist",
        "created_at": "2023-08-22T18:35:41Z",
        "body": "cc: @jmarantz "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-09-21T20:01:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-09-28T20:01:31Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 29043,
    "title": "Questions about xDS API flow",
    "created_at": "2023-08-16T06:42:22Z",
    "closed_at": "2023-09-29T12:01:24Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29043",
    "body": "There are two common xDS API flows:\r\n\r\n1. Fetch from Listener and Cluster as root during startup, followed by fetching dependent RouteConfiguration and ClusterLoadAssignment configurations.\r\n2. On-demand vhds and cds updates using the on demand filter with :authority header. The flow involves vhds triggering cds, and cds is driven by routes defined in vhds.\r\n\r\nQuestion 1:\r\nFor 1. If there are 1000 listeners, will the Envoy instance fetch all 1000 listeners, potentially resulting in Envoy listening on 1000 ports?\r\n\r\nQuestion 2:\r\nFor 2. Why can't we trigger RDS based on vhds, creating the flow: vhds (VirtualHost) -> RDS (RouteConfiguration) -> CDS (Cluster) -> EDS (ClusterLoadAssignment)? Instead of incorporating all route data into vhds, segregating the routing information into a separate layer can offer improved maintenance and easier updates.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29043/comments",
    "author": "whutwhu",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2023-08-16T23:56:50Z",
        "body": "On 1 yes, Envoy will fetch all listeners and listen on all their ports.\r\n\r\n@adisuissa to comment on the 2nd question."
      },
      {
        "user": "adisuissa",
        "created_at": "2023-08-17T13:19:48Z",
        "body": "RE question 2:\r\nBrowsing the comments in #12640, I don't think there's some barrier other than having someone actively implementing this feature.\r\nFWIW, this may cause multiple round-trips to the server, that may significantly delay the request processing."
      },
      {
        "user": "whutwhu",
        "created_at": "2023-08-19T18:39:59Z",
        "body": "@yanavlasov @adisuissa thank you for the information!"
      },
      {
        "user": "whutwhu",
        "created_at": "2023-08-23T05:19:01Z",
        "body": "For 2. I'm curious whether it's possible to utilize LDS in a setup where RDS is initiated by LDS, deviating from the conventional flow that starts with VHDS. This would result in the sequence: LDS -> RDS -> CDS -> EDS.\r\n\r\nIn a scenario involving 1000 RouteConfiguration, each one named after a service, the difficulty arises in applying LDS. This involves creating a Listener entry with a fictional port, enabling the establishment of 1000 filter chains. Each filter chain would be named according to its respective service name, thereby aligning with the names of corresponding RDS instances. Is this approach viable, which is related with Matching Filter Chains in Listener?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-09-22T08:01:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-09-29T12:01:23Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 29029,
    "title": "SSH proxy support for SSH inspection",
    "created_at": "2023-08-15T17:27:51Z",
    "closed_at": "2023-09-23T04:01:36Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29029",
    "body": "Hi,\r\nI'm considering using Envoy proxy for security operations over web/SSH traffic.\r\nRegarding SSH, I wanted to ask what is the procedure to support SSH proxy via Envoy?\r\n\r\nI would like to perform SSH inspection, and wanted to know if you have any recommendations on how it can be done with Envoy.\r\nIn addition, Is it possible for it to be done via runtime plugins (in Go for example) to avoid re-compiling Envoy?\r\n\r\nThanks!,\r\nBen",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29029/comments",
    "author": "BenAgai",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2023-08-15T20:02:23Z",
        "body": "Envoy does not currently support termination of SSH protocol. At best you can implement a TCP proxy without inspection.\r\n\r\nYes, it is possible to add a network filter that can terminate SSH and allow deep inspection. Envoy does support HTTP filters written in Go, but not Network filters, yet. You can write a network filter in C++ or languages that WASM supports."
      },
      {
        "user": "BenAgai",
        "created_at": "2023-08-16T08:52:02Z",
        "body": "Hi @yanavlasov, thank you for the quick response.\r\n\r\nThis means that for any extension, other than HTTP (for example DNS filtering, SSH etc), we can't use Go and have to use C++?\r\nIn addition, is there any plan to add\r\n1. Support to network filters using Go\r\n2. Support SSH proxy (and SSH inspection)\r\n "
      },
      {
        "user": "yanavlasov",
        "created_at": "2023-08-17T00:18:27Z",
        "body": "@doujiang24  @wangfakang  @StarryVae  For Go support of Network filers \r\n\r\nThere are no plans to support SSH proxy that I'm aware of"
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-17T01:52:59Z",
        "body": "i think Go extension has alreay supports network filters.cc @doujiang24 "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-09-16T04:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-09-23T04:01:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 28991,
    "title": "Default Behaviour on upstream",
    "created_at": "2023-08-12T15:33:17Z",
    "closed_at": "2023-09-20T16:01:38Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28991",
    "body": "*Title*: *Default Behaviour on upstream*\r\n\r\n*Description*:\r\n\r\n- We are using istio to our application and envoy proxy is running as sidecars\r\n- we want request external api call to the outside world from our applications(calling https request) , below are my questions\r\n\r\n1. Do envoy intercept this traffic ?\r\n2. If intercept, will the envoy creates a new socket connection for the https request\r\n3. Is it perform any iptables nat forward operation\r\n\r\nRest of the question , will ask in the thread based on the response\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28991/comments",
    "author": "balu-ce",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2023-08-14T15:45:32Z",
        "body": "This looks like an Istio related question. Please ask in Istio forums."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-09-13T16:01:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-09-20T16:01:38Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 28908,
    "title": "Configuring HTTP health check filters with aggregate clusters?",
    "created_at": "2023-08-08T23:19:52Z",
    "closed_at": "2023-11-18T00:02:46Z",
    "labels": [
      "question",
      "stale",
      "area/health_checking",
      "area/aggregate_cluster"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28908",
    "body": "*Title*: *Configuring HTTP health check filters with aggregate clusters?*\r\n\r\n*Description*:\r\nI have implemented a route that sends requests to an aggregate cluster. I assume that, with active cluster health checking configured for all of the clusters referenced by the aggregate cluster, if one host in one of those clusters is healthy then Envoy will route requests to that host. The issue I'm having is being able to expose that health status through the `envoy.filters.http.health_check` HTTP filter.\r\n\r\nGiven aggregate cluster `A` that references clusters `a1`, `a2`, etc. I tried configuring the filter like the following but it returns a 503 when one of the clusters `ak` is healthy but the others are not:\r\n\r\n```\r\nname: envoy.filters.http.health_check\r\ntyped_config:\r\n  \"@type\": type.googleapis.com/envoy.extensions.filters.http.health_check.v3.HealthCheck\r\n  pass_through_mode: false\r\n  cluster_min_healthy_percentages:\r\n    A:\r\n      value: 1\r\n  headers: [...]\r\n```\r\n\r\nI also tried this, but as expected this returns 503 if one of the clusters is unhealthy:\r\n\r\n```\r\nname: envoy.filters.http.health_check\r\ntyped_config:\r\n  \"@type\": type.googleapis.com/envoy.extensions.filters.http.health_check.v3.HealthCheck\r\n  pass_through_mode: false\r\n  cluster_min_healthy_percentages:\r\n    a1:\r\n      value: 1\r\n    a2:\r\n      value: 1\r\n    ...\r\n  headers: [...]\r\n```\r\n\r\nAny help to resolve this would be appreciated!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28908/comments",
    "author": "v1b1",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2023-08-10T18:01:59Z",
        "body": "cc @adisuissa as health check code owner\r\ncc @yxue as agreegate cluster code owner"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-09-09T20:01:11Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "v1b1",
        "created_at": "2023-09-11T14:13:59Z",
        "body": "Any thoughts or opinions on this? The aggregate health-check would be exposed to other downstream proxies so I'd like to be able to utilise the built-in behaviour instead of having to expose a route to an HTTP server that would itself query individually exposed health endpoints representing the sets of clusters to provide the aggregated status."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-10-11T16:01:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "v1b1",
        "created_at": "2023-10-11T17:46:31Z",
        "body": "@adisuissa @yxue I would appreciate any feedback on this!"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-11T00:02:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-11-18T00:02:45Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 28611,
    "title": "Dangling upstream connections after HTTP stream and downstream connections were closed",
    "created_at": "2023-07-25T14:20:25Z",
    "closed_at": "2023-08-01T16:06:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28611",
    "body": "*Description*:\r\nI am tunneling raw TCP traffic in HTTP/2 stream. The upstream connection and HTTP/2 stream work as expected. At some point the HTTP stream is closed, as well as the downstream connection, however it seems like the upstream connection persists, and only closed after 1 hour, I assume due to idle timeout. Logs indicate that in ``tcp_proxy.cc`` both ``onUpstreamEvent`` as well as ``onDownstreamEvent``, and that ``has upstream = false`` is printed and IIUC it means that the downstream connection is no longer attached to an upstream connection. \r\nI'm trying to understand whether this is an expected behavior, a bug or a misconfiguration. Will appreciate help with this one.\r\n\r\n*Config*:\r\ntcp_proxy config:\r\n```\r\n- name: tcp_proxy\r\n  typed_config:\r\n    \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n    cluster: upstream\r\n    tunneling_config:\r\n      hostname: [masked]\r\n      use_post: true\r\n      post_path: [masked]\r\n```\r\ncluster config:\r\n```\r\n  - name: upstream\r\n    lb_policy: CLUSTER_PROVIDED\r\n    cluster_type:\r\n      name: envoy.clusters.dynamic_forward_proxy\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.clusters.dynamic_forward_proxy.v3.ClusterConfig\r\n        dns_cache_config:\r\n          name: dynamic_forward_proxy_cache_config\r\n          dns_lookup_family: V4_ONLY\r\n          typed_dns_resolver_config:\r\n            name: envoy.network.dns_resolver.cares\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.network.dns_resolver.cares.v3.CaresDnsResolverConfig\r\n              use_resolvers_as_fallback: true\r\n              dns_resolver_options:\r\n                use_tcp_for_dns_lookups: true\r\n                no_default_search_domain: true\r\n        allow_insecure_cluster_options: true\r\n    connection_pool_per_downstream_connection: true\r\n    typed_extension_protocol_options:\r\n      envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n        \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n        explicit_http_config:\r\n          http2_protocol_options:\r\n            max_concurrent_streams: 100\r\n            initial_stream_window_size: 65536\r\n            initial_connection_window_size: 1048576\r\n```\r\n\r\n*Logs*:\r\n```\r\ntime\t                       SourceLine\t                          Level\t  ThreadId\t Message\r\n2023-07-25 08:26:40.9290705\t   tcp_proxy.cc:234\t                      debug\t  82\t     [C4695] new tcp proxy session\r\n2023-07-25 08:26:40.9290737\t   connection_impl.cc:362                 trace\t  82\t     [C4695] readDisable: disable=true disable_count=0 state=0 buffer_length=0\t\r\n2023-07-25 08:26:40.9291342\t   tcp_proxy.cc:707\t                      trace\t  82\t\t [C4695] onNewConnection\t\r\n2023-07-25 08:26:40.9291375\t   tcp_proxy.cc:413\t                      debug\t  82\t\t [C4695] Creating connection to cluster [masked]\r\n2023-07-25 08:26:40.9292207\t   conn_pool_base.cc:79\t                  debug\t  82\t\t queueing stream due to no available connections (ready=0 busy=0 connecting=0)\t\r\n2023-07-25 08:26:40.9292240\t   conn_pool_base.cc:299\t              debug\t  82\t\t trying to create new connection\t\r\n2023-07-25 08:26:40.9292283\t   conn_pool_base.cc:300\t              trace\t  82\t\t ConnPoolImplBase 0x44e7def05a0, ready_clients_.size(): 0, busy_clients_.size(): 0, connecting_clients_.size(): 0, connecting_stream_capacity_: 0, num_active_streams_: 0, pending_streams_.size(): 1 per upstream preconnect ratio: 1\t\r\n2023-07-25 08:26:40.9292317\t   conn_pool_base.cc:145\t              debug\t  82\t\t creating a new connection (connecting=0)\t\r\n2023-07-25 08:26:40.9292977\t   codec_impl.cc:1786                     trace\t  82\t\t Codec does not have Metadata frame support.\t\r\n2023-07-25 08:26:40.9293025\t   codec_impl.cc:1586                     debug\t  82\t\t [C4696] updating connection-level initial window size to 1048576\r\n2023-07-25 08:26:40.9293059\t   connection_impl.h:98\t                  debug\t  82\t\t [C4696] current connecting state: true\t\r\n2023-07-25 08:26:40.9293088\t   codec_client.cc:57\t                  debug\t  82\t\t [C4696] connecting\t\r\n2023-07-25 08:26:40.9293150\t   connection_impl.cc:941\t              debug\t  82\t\t [C4696] connecting to [masked]\r\n2023-07-25 08:26:40.9294005\t   connection_impl.cc:960\t              debug\t  82\t\t [C4696] connection in progress\r\n2023-07-25 08:26:40.9294047\t   conn_pool_base.cc:131\t              trace\t  82\t\t not creating a new connection, shouldCreateNewConnection returned false.\r\n2023-07-25 08:26:40.9294142\t   connection_impl.cc:423\t              trace\t  82\t\t [C4695] raising connection event 2\r\n2023-07-25 08:26:40.9294172\t   tcp_proxy.cc:740\t                      trace\t  82\t\t [C4695] on downstream event 2, has upstream = false\r\n2023-07-25 08:26:40.9294202\t   active_tcp_listener.cc:155             debug\t  82\t\t [C4695] new connection from [masked]\r\n2023-07-25 08:26:40.9294354\t   connection_impl.cc:568\t              trace\t  82\t\t [C4695] socket event: 2\t\r\n2023-07-25 08:26:40.9294396\t   connection_impl.cc:679\t              trace\t  82\t\t [C4695] write ready\t\r\n2023-07-25 08:26:40.9351912\t   connection_impl.cc:568\t              trace\t  82\t\t [C4696] socket event: 2\t\r\n2023-07-25 08:26:40.9352130\t   connection_impl.cc:679\t              trace\t  82\t\t [C4696] write ready\t\r\n2023-07-25 08:26:40.9352247\t   connection_impl.cc:688\t              debug\t  82\t\t [C4696] connected\t\r\n2023-07-25 08:26:40.9353567\t   ssl_handshaker.cc:93\t                  trace\t  82\t\t [C4696] ssl error occurred while read: WANT_READ\t\r\n2023-07-25 08:26:40.9417688\t   connection_impl.cc:568\t              trace\t  82\t\t [C4696] socket event: 3\t\r\n2023-07-25 08:26:40.9417942\t   connection_impl.cc:679\t              trace\t  82\t\t [C4696] write ready\t\r\n2023-07-25 08:26:40.9424065\t   ssl_socket.cc:360\t                  debug\t  82\t\t [C4696] Async cert validation completed\t\r\n2023-07-25 08:26:40.9434191\t   ssl_handshaker.cc:93\t                  trace\t  82\t\t [C4696] ssl error occurred while read: WANT_READ\t\r\n2023-07-25 08:26:40.9434258\t   connection_impl.cc:608\t              trace\t  82\t\t [C4696] read ready. dispatch_buffered_data=0\t\r\n2023-07-25 08:26:40.9434283\t   ssl_handshaker.cc:93\t                  trace\t  82\t\t [C4696] ssl error occurred while read: WANT_READ\t\r\n2023-07-25 08:26:40.9488736\t   connection_impl.cc:568\t              trace\t  82\t\t [C4696] socket event: 3\t\r\n2023-07-25 08:26:40.9488838\t   connection_impl.cc:679\t              trace\t  82\t\t [C4696] write ready\t\r\n2023-07-25 08:26:40.9489089\t   connection_impl.cc:423\t              trace\t  82\t\t [C4696] raising connection event 2\t\r\n2023-07-25 08:26:40.9489226\t   codec_client.cc:88\t                  debug\t  82\t\t [C4696] connected\t\r\n2023-07-25 08:26:40.9489278\t   conn_pool_base.cc:338\t              debug\t  82\t\t [C4696] attaching to next stream\t\r\n2023-07-25 08:26:40.9489335\t   conn_pool_base.cc:182\t              debug\t  82\t\t [C4696] creating stream\r\n2023-07-25 08:26:40.9489905\t   codec_impl.cc:1306\t                  trace\t  82\t\t [C4696] send data: bytes=24\t\r\n2023-07-25 08:26:40.9489955\t   connection_impl.cc:483\t              trace\t  82\t\t [C4696] writing 24 bytes, end_stream false\t\r\n2023-07-25 08:26:40.9490004\t   codec_impl.cc:1284                     trace\t  82\t\t [C4696] about to send frame type=4, flags=0\t\r\n2023-07-25 08:26:40.9490052\t   codec_impl.cc:1306                     trace\t  82\t\t [C4696] send data: bytes=39\t\r\n2023-07-25 08:26:40.9490099\t   connection_impl.cc:483                 trace\t  82\t\t [C4696] writing 39 bytes, end_stream false\t\r\n2023-07-25 08:26:40.9490150\t   codec_impl.cc:1177\t                  trace\t  82\t\t [C4696] sent frame type=4, stream_id=0, length=30\t\r\n2023-07-25 08:26:40.9490199\t   codec_impl.cc:1284\t                  trace\t  82\t\t [C4696] about to send frame type=8, flags=0\t\r\n2023-07-25 08:26:40.9490247\t   codec_impl.cc:1306\t                  trace\t  82\t\t [C4696] send data: bytes=13\t\r\n2023-07-25 08:26:40.9490296\t   connection_impl.cc:483                 trace\t  82\t\t [C4696] writing 13 bytes, end_stream false\t\r\n2023-07-25 08:26:40.9490348\t   codec_impl.cc:1177\t                  trace\t  82\t\t [C4696] sent frame type=8, stream_id=0, length=4\t\r\n2023-07-25 08:26:40.9490412\t   codec_impl.cc:1284\t                  trace\t  82\t\t [C4696] about to send frame type=1, flags=4\t\r\n2023-07-25 08:26:40.9490461\t   codec_impl.cc:1306\t                  trace\t  82\t\t [C4696] send data: bytes=106\t\r\n2023-07-25 08:26:40.9490508\t   connection_impl.cc:483                 trace\t  82\t\t [C4696] writing 106 bytes, end_stream false\t\r\n2023-07-25 08:26:40.9490560\t   codec_impl.cc:1177\t                  trace\t  82\t\t [C4696] sent frame type=1, stream_id=1, length=9\r\n2023-07-25 08:26:40.9490796\t   connection_impl.cc:679\t              trace\t  82\t\t [C4696] write ready\t\r\n2023-07-25 08:26:40.9490858\t   ssl_socket.cc:269\t                  trace\t  82\t\t [C4696] ssl write returns: 182\t\r\n2023-07-25 08:26:40.9490906\t   connection_impl.cc:608                 trace\t  82\t\t [C4696] read ready. dispatch_buffered_data=0\t\r\n2023-07-25 08:26:40.9490956\t   ssl_socket.cc:87\t                      trace\t  82\t\t [C4696] ssl read returns: 46\t\r\n2023-07-25 08:26:40.9491005\t   ssl_socket.cc:87\t                      trace\t  82\t\t [C4696] ssl read returns: -1\t\r\n2023-07-25 08:26:40.9491053\t   ssl_socket.cc:127\t                  trace\t  82\t\t [C4696] ssl error occurred while read: WANT_READ\t\r\n2023-07-25 08:26:40.9491101\t   ssl_socket.cc:163\t                  trace\t  82\t\t [C4696] ssl read 46 bytes\t\r\n2023-07-25 08:26:40.9491149\t   codec_impl.cc:923\t                  trace\t  82\t\t [C4696] dispatching 46 bytes\t\r\n2023-07-25 08:26:40.9491198\t   codec_impl.cc:1030\t                  trace\t  82\t\t [C4696] about to recv frame type=4, flags=0, stream_id=0\t\r\n2023-07-25 08:26:40.9491250\t   codec_impl.cc:2003\t                  trace\t  82\t\t [C4696] track inbound frame type=4 flags=0 length=24 padding_length=0\t\r\n2023-07-25 08:26:40.9491314\t   codec_impl.cc:1056\t                  trace\t  82\t\t [C4696] recv frame type=4\t\r\n2023-07-25 08:26:40.9491363\t   codec_impl.cc:1030\t                  trace\t  82\t\t [C4696] about to recv frame type=8, flags=0, stream_id=0\t\r\n2023-07-25 08:26:40.9491413\t   codec_impl.cc:2003\t                  trace\t  82\t\t [C4696] track inbound frame type=8 flags=0 length=4 padding_length=0\t\r\n2023-07-25 08:26:40.9491458\t   codec_impl.cc:1056\t                  trace\t  82\t\t [C4696] recv frame type=8\t\r\n2023-07-25 08:26:40.9491546\t   codec_impl.cc:958\t                  trace\t  82\t\t [C4696] dispatched 46 bytes\t\r\n2023-07-25 08:26:40.9491592\t   codec_impl.cc:1284\t                  trace\t  82\t\t [C4696] about to send frame type=4, flags=1\t\r\n2023-07-25 08:26:40.9491642\t   codec_impl.cc:1306\t                  trace\t  82\t\t [C4696] send data: bytes=9\t\r\n2023-07-25 08:26:40.9491687\t   connection_impl.cc:483\t              trace\t  82\t\t [C4696] writing 9 bytes, end_stream false\t\r\n2023-07-25 08:26:40.9491735\t   codec_impl.cc:1177                     trace\t  82\t\t [C4696] sent frame type=4, stream_id=0, length=0\t\r\n2023-07-25 08:26:40.9491781\t   connection_impl.cc:568                 trace\t  82\t\t [C4696] socket event: 2\t\r\n2023-07-25 08:26:40.9491824\t   connection_impl.cc:679                 trace\t  82\t\t [C4696] write ready\t\r\n2023-07-25 08:26:40.9491863\t   ssl_socket.cc:269\t                  trace\t  82\t\t [C4696] ssl write returns: 9\t\r\n2023-07-25 08:26:40.9541314\t   connection_impl.cc:568\t              trace\t  82\t\t [C4696] socket event: 3\t\r\n2023-07-25 08:26:40.9541463\t   connection_impl.cc:679\t              trace\t  82\t\t [C4696] write ready\t\r\n2023-07-25 08:26:40.9541500\t   connection_impl.cc:608\t              trace\t  82\t\t [C4696] read ready. dispatch_buffered_data=0\t\r\n2023-07-25 08:26:40.9541532\t   ssl_socket.cc:87\t                      trace\t  82\t\t [C4696] ssl read returns: 223\t\r\n2023-07-25 08:26:40.9541563\t   ssl_socket.cc:87\t                      trace\t  82\t\t [C4696] ssl read returns: -1\t\r\n2023-07-25 08:26:40.9541592\t   ssl_socket.cc:127\t                  trace\t  82\t\t [C4696] ssl error occurred while read: WANT_READ\t\r\n2023-07-25 08:26:40.9541653\t   ssl_socket.cc:163\t                  trace\t  82\t\t [C4696] ssl read 223 bytes\t\r\n2023-07-25 08:26:40.9541683\t   codec_impl.cc:923\t                  trace\t  82\t\t [C4696] dispatching 223 bytes\t\r\n2023-07-25 08:26:40.9541810\t   codec_impl.cc:1030\t                  trace\t  82\t\t [C4696] about to recv frame type=4, flags=1, stream_id=0\t\r\n2023-07-25 08:26:40.9541844\t   codec_impl.cc:2003\t                  trace\t  82\t\t [C4696] track inbound frame type=4 flags=1 length=0 padding_length=0\t\r\n2023-07-25 08:26:40.9541876\t   codec_impl.cc:1056\t                  trace\t  82\t\t [C4696] recv frame type=4\t\r\n2023-07-25 08:26:40.9541906\t   codec_impl.cc:1030\t                  trace\t  82\t\t [C4696] about to recv frame type=1, flags=4, stream_id=1\t\r\n2023-07-25 08:26:40.9541943\t   codec_impl.cc:2003\t                  trace\t  82\t\t [C4696] track inbound frame type=1 flags=4 length=205 padding_length=0\t\r\n2023-07-25 08:26:40.9541974\t   codec_impl.cc:1056\t                  trace\t  82\t\t [C4696] recv frame type=1\r\n2023-07-25 08:26:40.9542092\t   connection_impl.cc:362                 trace\t  82\t\t [C4695] readDisable: disable=false disable_count=1 state=0 buffer_length=0\t\r\n2023-07-25 08:26:40.9542121\t   tcp_proxy.cc:826\t                      debug\t  82\t\t [C4695] TCP:onUpstreamEvent(), requestedServerName:\r\n2023-07-25 08:26:40.9544251\t   codec_impl.cc:958\t                  trace\t  82\t\t [C4696] dispatched 223 bytes\t\r\n2023-07-25 08:26:40.9544304\t   connection_impl.cc:568                 trace\t  82\t\t [C4695] socket event: 3\t\r\n2023-07-25 08:26:40.9544335\t   connection_impl.cc:679                 trace\t  82\t\t [C4695] write ready\t\r\n2023-07-25 08:26:40.9544364\t   connection_impl.cc:608                 trace\t  82\t\t [C4695] read ready. dispatch_buffered_data=0\t\r\n2023-07-25 08:26:40.9544432\t   raw_buffer_socket.cc:24                trace\t  82\t\t [C4695] read returns: 26\t\r\n2023-07-25 08:26:40.9544463\t   raw_buffer_socket.cc:38                trace\t  82\t\t [C4695] read error: Resource temporarily unavailable\r\n2023-07-25 08:26:40.9544815\t   tcp_proxy.cc:692\t                      trace\t  82\t\t [C4695] downstream connection received 1865 bytes, end_stream=false\t\r\n2023-07-25 08:26:40.9544848\t   connection_impl.cc:483\t              trace\t  82\t\t [C4696] writing 1874 bytes, end_stream false\t\r\n2023-07-25 08:26:40.9544879\t   codec_impl.cc:1177\t                  trace\t  82\t\t [C4696] sent frame type=0, stream_id=1, length=1865\t\r\n2023-07-25 08:26:40.9544917\t   connection_impl.cc:568\t              trace\t  82\t\t [C4696] socket event: 2\t\r\n2023-07-25 08:26:40.9544945\t   connection_impl.cc:679\t              trace\t  82\t\t [C4696] write ready\t\r\n2023-07-25 08:26:40.9545318\t   ssl_socket.cc:269\t                  trace\t  82\t\t [C4696] ssl write returns: 1874\t\r\n2023-07-25 08:26:40.9946489\t   connection_impl.cc:568\t              trace\t  82\t\t [C4696] socket event: 3\t\r\n2023-07-25 08:26:40.9946625\t   connection_impl.cc:679\t              trace\t  82\t\t [C4696] write ready\t\r\n2023-07-25 08:26:40.9946661\t   connection_impl.cc:608\t              trace\t  82\t\t [C4696] read ready. dispatch_buffered_data=0\t\r\n2023-07-25 08:26:40.9946693\t   ssl_socket.cc:87\t                      trace\t  82\t\t [C4696] ssl read returns: 111\t\r\n2023-07-25 08:26:40.9946723\t   ssl_socket.cc:87\t                      trace\t  82\t\t [C4696] ssl read returns: -1\t\r\n2023-07-25 08:26:40.9946841\t   ssl_socket.cc:127\t                  trace\t  82\t\t [C4696] ssl error occurred while read: WANT_READ\t\r\n2023-07-25 08:26:40.9946874\t   ssl_socket.cc:163\t                  trace\t  82\t\t [C4696] ssl read 111 bytes\t\r\n2023-07-25 08:26:40.9946903\t   codec_impl.cc:923\t                  trace\t  82\t\t [C4696] dispatching 111 bytes\t\r\n2023-07-25 08:26:40.9946932\t   codec_impl.cc:1030\t                  trace\t  82\t\t [C4696] about to recv frame type=8, flags=0, stream_id=1\t\r\n2023-07-25 08:26:40.9946963\t   codec_impl.cc:2003\t                  trace\t  82\t\t [C4696] track inbound frame type=8 flags=0 length=4 padding_length=0\t\r\n2023-07-25 08:26:40.9946993\t   codec_impl.cc:1056\t                  trace\t  82\t\t [C4696] recv frame type=8\t\r\n2023-07-25 08:26:40.9947022\t   codec_impl.cc:1030\t                  trace\t  82\t\t [C4696] about to recv frame type=0, flags=0, stream_id=1\t\r\n2023-07-25 08:26:40.9947058\t   codec_impl.cc:1056\t                  trace\t  82\t\t [C4696] recv frame type=0\t\r\n2023-07-25 08:26:40.9947088\t   codec_impl.cc:2003\t                  trace\t  82\t\t [C4696] track inbound frame type=0 flags=0 length=59 padding_length=0\t\r\n2023-07-25 08:26:40.9947117\t   tcp_proxy.cc:767\t                      trace\t  82\t\t [C4695] upstream connection received 59 bytes, end_stream=false\t\r\n2023-07-25 08:26:40.9947147\t   connection_impl.cc:483\t              trace\t  82\t\t [C4695] writing 52 bytes, end_stream false\t\r\n2023-07-25 08:26:40.9947177\t   codec_impl.cc:1030                     trace\t  82\t\t [C4696] about to recv frame type=8, flags=0, stream_id=0\t\r\n2023-07-25 08:26:40.9947209\t   codec_impl.cc:2003                     trace\t  82\t\t [C4696] track inbound frame type=8 flags=0 length=4 padding_length=0\t\r\n2023-07-25 08:26:40.9947238\t   codec_impl.cc:1056                     trace\t  82\t\t [C4696] recv frame type=8\t\r\n2023-07-25 08:26:40.9947267\t   codec_impl.cc:1030                     trace\t  82\t\t [C4696] about to recv frame type=6, flags=0, stream_id=0\t\r\n2023-07-25 08:26:40.9947296\t   codec_impl.cc:2003                     trace\t  82\t\t [C4696] track inbound frame type=6 flags=0 length=8 padding_length=0\t\r\n2023-07-25 08:26:40.9947352\t   codec_impl.cc:1056                     trace\t  82\t\t [C4696] recv frame type=6\t\r\n2023-07-25 08:26:40.9947381\t   codec_impl.cc:958\t                  trace\t  82\t\t [C4696] dispatched 111 bytes\t\r\n2023-07-25 08:26:40.9947415\t   codec_impl.cc:1284                     trace\t  82\t\t [C4696] about to send frame type=6, flags=1\t\r\n2023-07-25 08:26:40.9947444\t   codec_impl.cc:1306                     trace\t  82\t\t [C4696] send data: bytes=17\t\r\n2023-07-25 08:26:40.9947474\t   connection_impl.cc:483\t              trace\t  82\t\t [C4696] writing 17 bytes, end_stream false\t\r\n2023-07-25 08:26:40.9947502\t   codec_impl.cc:1177\t                  trace\t  82\t\t [C4696] sent frame type=6, stream_id=0, length=8\t\r\n2023-07-25 08:26:40.9947531\t   connection_impl.cc:568\t              trace\t  82\t\t [C4695] socket event: 2\t\r\n2023-07-25 08:26:40.9947559\t   connection_impl.cc:679\t              trace\t  82\t\t [C4695] write ready\t\r\n2023-07-25 08:26:40.9947588\t   raw_buffer_socket.cc:67\t              trace\t  82\t\t [C4695] write returns: 52\t\r\n2023-07-25 08:26:40.9947617\t   connection_impl.cc:568\t              trace\t  82\t\t [C4696] socket event: 2\t\r\n2023-07-25 08:26:40.9947645\t   connection_impl.cc:679\t              trace\t  82\t\t [C4696] write ready\t\r\n2023-07-25 08:26:40.9947673\t   ssl_socket.cc:269\t                  trace\t  82\t\t [C4696] ssl write returns: 17\t\r\n2023-07-25 08:26:40.9952988\t   connection_impl.cc:568                 trace\t  82\t\t [C4696] socket event: 3\t\r\n2023-07-25 08:26:40.9953062\t   connection_impl.cc:679                 trace\t  82\t\t [C4696] write ready\t\r\n2023-07-25 08:26:40.9953096\t   connection_impl.cc:608                 trace\t  82\t\t [C4696] read ready. dispatch_buffered_data=0\t\r\n2023-07-25 08:26:40.9953127\t   ssl_socket.cc:87\t                      trace\t  82\t\t [C4696] ssl read returns: 16\t\r\n2023-07-25 08:26:40.9953278\t   ssl_socket.cc:87\t                      trace\t  82\t\t [C4696] ssl read returns: -1\t\r\n2023-07-25 08:26:40.9953327\t   ssl_socket.cc:127\t                  trace\t  82\t\t [C4696] ssl error occurred while read: WANT_READ\t\r\n2023-07-25 08:26:40.9953377\t   ssl_socket.cc:163\t                  trace\t  82\t\t [C4696] ssl read 16 bytes\t\r\n2023-07-25 08:26:40.9953422\t   codec_impl.cc:923\t                  trace\t  82\t\t [C4696] dispatching 16 bytes\t\r\n2023-07-25 08:26:40.9953468\t   codec_impl.cc:1030\t                  trace\t  82\t\t [C4696] about to recv frame type=0, flags=0, stream_id=1\t\r\n2023-07-25 08:26:40.9953516\t   codec_impl.cc:1056\t                  trace\t  82\t\t [C4696] recv frame type=0\t\r\n2023-07-25 08:26:40.9953617\t   codec_impl.cc:2003\t                  trace\t  82\t\t [C4696] track inbound frame type=0 flags=0 length=7 padding_length=0\t\r\n2023-07-25 08:26:40.9953649\t   tcp_proxy.cc:767\t                      trace\t  82\t\t [C4695] upstream connection received 7 bytes, end_stream=false\r\n2023-07-25 08:26:40.9953752\t   codec_impl.cc:958\t                  trace\t  82\t\t [C4696] dispatched 16 bytes\t\r\n2023-07-25 08:26:40.9953785\t   connection_impl.cc:568\t              trace\t  82\t\t [C4696] socket event: 3\t\r\n2023-07-25 08:26:40.9953814\t   connection_impl.cc:679\t              trace\t  82\t\t [C4696] write ready\t\r\n2023-07-25 08:26:40.9953843\t   connection_impl.cc:608\t              trace\t  82\t\t [C4696] read ready. dispatch_buffered_data=0\t\r\n2023-07-25 08:26:40.9953872\t   ssl_socket.cc:87\t                      trace\t  82\t\t [C4696] ssl read returns: 52\t\r\n2023-07-25 08:26:40.9953900\t   ssl_socket.cc:87\t                      trace\t  82\t\t [C4696] ssl read returns: -1\t\r\n2023-07-25 08:26:40.9953965\t   ssl_socket.cc:127\t                  trace\t  82\t\t [C4696] ssl error occurred while read: WANT_READ\t\r\n2023-07-25 08:26:40.9953994\t   ssl_socket.cc:163\t                  trace\t  82\t\t [C4696] ssl read 52 bytes\t\r\n2023-07-25 08:26:40.9954023\t   codec_impl.cc:923\t                  trace\t  82\t\t [C4696] dispatching 52 bytes\t\r\n2023-07-25 08:26:40.9954052\t   codec_impl.cc:1030\t                  trace\t  82\t\t [C4696] about to recv frame type=1, flags=5, stream_id=1\t\r\n2023-07-25 08:26:40.9954104\t   codec_impl.cc:2003\t                  trace\t  82\t\t [C4696] track inbound frame type=1 flags=5 length=30 padding_length=0\t\r\n2023-07-25 08:26:40.9954134\t   codec_impl.cc:1056\t                  trace\t  82\t\t [C4696] recv frame type=1\t\r\n2023-07-25 08:26:40.9954163\t   codec_client.cc:126\t                  debug\t  82\t\t [C4696] response complete\t\r\n2023-07-25 08:26:40.9954192\t   codec_client.cc:134\t                  debug\t  82\t\t [C4696] waiting for encode to complete\t\r\n2023-07-25 08:26:40.9954221\t   codec_impl.cc:1030\t                  trace\t  82\t\t [C4696] about to recv frame type=3, flags=0, stream_id=1\t\r\n2023-07-25 08:26:40.9954250\t   codec_impl.cc:2003\t                  trace\t  82\t\t [C4696] track inbound frame type=3 flags=0 length=4 padding_length=0\t\r\n2023-07-25 08:26:40.9954279\t   codec_impl.cc:1056\t                  trace\t  82\t\t [C4696] recv frame type=3\t\r\n2023-07-25 08:26:40.9954307\t   codec_impl.cc:1160\t                  trace\t  82\t\t [C4696] remote reset: 0\t\r\n2023-07-25 08:26:40.9954335\t   codec_impl.cc:1331\t                  debug\t  82\t\t [C4696] stream 1 closed: 0\t\r\n2023-07-25 08:26:40.9954363\t   codec_client.cc:156\t                  debug\t  82\t\t [C4696] request reset\t\r\n2023-07-25 08:26:40.9954391\t   dispatcher_impl.cc:250\t              trace\t  82\t\t item added to deferred deletion list (size=1)\t\r\n2023-07-25 08:26:40.9954422\t   conn_pool_base.cc:223\t              debug\t  82\t\t [C4696] destroying stream: 0 remaining\t\t\r\n2023-07-25 08:26:40.9954801\t   connection_impl.cc:139\t              debug\t  82\t\t [C4695] closing data_to_write=0 type=0\t\r\n2023-07-25 08:26:40.9954830\t   connection_impl.cc:250\t              debug\t  82\t\t [C4695] closing socket: 1\t\r\n2023-07-25 08:26:40.9954872\t   connection_impl.cc:423\t              trace\t  82\t\t [C4695] raising connection event 1\t\r\n2023-07-25 08:26:40.9954900\t   tcp_proxy.cc:740\t                      trace\t  82\t\t [C4695] on downstream event 1, has upstream = false\t\r\n2023-07-25 08:26:40.9954929\t   active_stream_listener_base.cc:111     trace\t  82\t\t [C4695] connection on event 1\t\r\n2023-07-25 08:26:40.9954959\t   active_stream_listener_base.cc:121     debug\t  82\t\t [C4695] adding to cleanup list\t\r\n2023-07-25 08:26:40.9955049\t   dispatcher_impl.cc:250\t              trace\t  82\t\t item added to deferred deletion list (size=2)\t\r\n2023-07-25 08:26:40.9955080\t   dispatcher_impl.cc:250\t              trace\t  82\t\t item added to deferred deletion list (size=3)\t\r\n2023-07-25 08:26:40.9955108\t   dispatcher_impl.cc:250\t              trace\t  82\t\t item added to deferred deletion list (size=4)\t\r\n2023-07-25 08:26:40.9955137\t   codec_impl.cc:1395\t                  debug\t  82\t\t [C4696] Recouping 0 bytes of flow control window for stream 1.\t\r\n2023-07-25 08:26:40.9955168\t   codec_impl.cc:958\t                  trace\t  82\t\t [C4696] dispatched 52 bytes\t\r\n2023-07-25 08:26:40.9955198\t   dispatcher_impl.cc:125\t              trace\t  82\t\t clearing deferred deletion list (size=4)\r\n2023-07-25 09:26:40.9940766\t   connection_impl.cc:139\t              debug\t  82\t\t [C4696] closing data_to_write=0 type=1\r\n2023-07-25 09:26:40.9940937\t   connection_impl.cc:250\t              debug\t  82\t\t [C4696] closing socket: 1\r\n2023-07-25 09:26:40.9940974\t   ssl_socket.cc:321\t                  debug\t  82\t\t [C4696] SSL shutdown: rc=0\r\n2023-07-25 09:26:40.9942070\t   connection_impl.cc:423\t              trace\t  82\t\t [C4696] raising connection event 1\r\n2023-07-25 09:26:40.9942189\t   codec_client.cc:107\t                  debug\t  82\t\t [C4696] disconnect. resetting 0 pending requests\r\n2023-07-25 09:26:40.9942224\t   conn_pool_base.cc:494\t              debug\t  82\t\t [C4696] client disconnected, failure reason: \r\n2023-07-25 09:26:40.9942255\t   dispatcher_impl.cc:250\t              trace\t  82\t\t item added to deferred deletion list (size=1)\r\n2023-07-25 09:26:40.9942285\t   conn_pool_base.cc:464\t              debug\t  82\t\t invoking idle callbacks - is_draining_for_deletion_=false\r\n2023-07-25 09:26:40.9942315\t   cluster_manager_impl.cc:1747\t          trace\t  82\t\t Erasing idle pool for host [masked]\r\n2023-07-25 09:26:40.9942347\t   dispatcher_impl.cc:250\t              trace\t  82\t\t item added to deferred deletion list (size=2)\r\n2023-07-25 09:26:40.9942376\t   cluster_manager_impl.cc:1754\t          trace\t  82\t\t Pool container empty for host [masked], erasing host entry\r\n2023-07-25 09:26:40.9942410\t   dispatcher_impl.cc:125\t              trace\t  82\t\t clearing deferred deletion list (size=2)\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28611/comments",
    "author": "ohadvano",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2023-07-26T23:07:01Z",
        "body": "IIRC the TCP proxy filter uses an async drain mechanism but I can't recall why. cc @ggreenway "
      },
      {
        "user": "ohadvano",
        "created_at": "2023-07-31T12:52:33Z",
        "body": "Closing as this behavior is by design"
      },
      {
        "user": "ggreenway",
        "created_at": "2023-07-31T18:19:42Z",
        "body": "TCP proxy can have dangling upstream connections, but normally they should be closed very soon after the downstream is closed. They should only be held open long enough to drain any data that needs to be sent to the upstream, to handle a sequence such as downstream sends large blob of data + FIN, upstream had previously sent FIN. This results in immediate closure of downstream connection, so envoy holds the upstream connection long enough to sent the received blob of data, then it should close it immediately.\r\n\r\nSo it seems like there is a bug in this somewhere. Re-opening."
      },
      {
        "user": "ggreenway",
        "created_at": "2023-07-31T18:23:47Z",
        "body": "Can you check the tcp proxy stats `upstream_flush_total` and/or `upstream_flush_active` to see how they relate to the dangling connections you're seeing?"
      },
      {
        "user": "ohadvano",
        "created_at": "2023-08-01T06:20:51Z",
        "body": "@ggreenway I think a combination of configurations made this by design. I'm using TCP tunneling here, so the upstream connections are meant for HTTP streams. In that case IIUC Envoy would try to re-use connections and keep them open until idle timeout, to potentially multiplex the next HTTP/2 stream over the same connection. Since I'm using ``connection_pool_per_downstream_connection: true``, it cause each downstream connection to create a new upstream TCP connection and therefore never multiplexing streams, so connections were dangling for 1 hour after HTTP stream ended.\r\n\r\nThe problem was solved (connections closed right after stream ended) after adding:\r\n```\r\ncommon_http_protocol_options:\r\n  max_requests_per_connection: 1\r\n```"
      },
      {
        "user": "ggreenway",
        "created_at": "2023-08-01T16:06:08Z",
        "body": "Yeah, that's an interesting set of interactions for that configuration. Ok, I'll re-close since this seems to be resolved for you. Thanks for following up!"
      }
    ]
  },
  {
    "number": 28386,
    "title": "Connection draining on SDS update",
    "created_at": "2023-07-13T15:57:15Z",
    "closed_at": "2023-07-17T10:09:08Z",
    "labels": [
      "question",
      "api"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28386",
    "body": "In case I'm using LDS, where some of the filter chains in the listener config have SDS config (downstream transport TLS that uses SDS to fetch secrets). After a while, the certificate is refreshed from the SDS. Does it cause connections that currently use the filter chain to drain? Will only new connections get the new certificate?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28386/comments",
    "author": "ohadvano",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2023-07-14T17:04:59Z",
        "body": "cc @adisuissa "
      },
      {
        "user": "soulxu",
        "created_at": "2023-07-17T09:51:52Z",
        "body": "there is no draining, only the new connection will use the new certificate"
      },
      {
        "user": "ohadvano",
        "created_at": "2023-07-17T10:09:08Z",
        "body": "Thanks"
      }
    ]
  },
  {
    "number": 28301,
    "title": "Doesn't Round Robin policy distribute requests to upstream endpoints one by one",
    "created_at": "2023-07-10T02:29:38Z",
    "closed_at": "2023-07-11T01:10:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28301",
    "body": "```\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 9999\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          access_log:\r\n          - name: envoy.access_loggers.stdout\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\r\n          http_filters:\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: example_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: cluster_example\r\n                  retry_policy:\r\n                    retry_on: \"5xx,unavailable,connection-failure\"\r\n                    num_retries: 1\r\n                  timeout: 60s\r\n  clusters:\r\n  - name: cluster_example\r\n    connect_timeout: 30s\r\n    type: STATIC\r\n    lb_policy: ROUND_ROBIN\r\n    dns_lookup_family: V4_ONLY\r\n    http2_protocol_options: {}\r\n    load_assignment:\r\n      cluster_name: cluster_example\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 172.1.0.2\r\n                port_value: 9999\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 172.1.0.1\r\n                port_value: 9999\r\n```\r\nHere is my config of Envoy. I use it to handle my grpc clients and distribute requests of clients to the two endpoints.\r\n\r\nI used one client to do testing. It seems that everything works fine. The two servers can process requests and the client can get the responses.\r\n\r\nBut I just found that the two servers did NOT process the requests one after another. I mean I've thought if the nth request had been sent to the server `172.1.0.1`, the (n+1)th request would be sent to the server `172.1.0.2` definitely. But it doesn't seem that Envoy work like this. I can see that sometimes one server would keep processing requests while the other server is idle.\r\n\r\nI don't know why. As my understanding, Round Robin means processing requests one after another. But obviously the case above is not working like this.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28301/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2023-07-10T13:30:34Z",
        "body": "There's a bit more complexity here.  If the RR algorithm sorts your upstreams A B C, on a given thread _and_ if you have a single threaded Envoy, the first request to hit the router (not the same as the first request to the server) may be sent to A, the second to B, and the third to C. But there's generally multiple worker threads and jitter to contend with.  \r\n\r\nSo with multiple threads if there were no jitter, the first request to worker thread (1) router filter would to to A, _and_ the first request to worker thread (2) router filter would go to A, so we give each thread a random offset, so with a 64 core machine we don't send 64 request to A, then B, then C.  The net result is with decent load you're going to have equal spread of requests, but if you had 2 threads and 2 upstreams I believe (offhand, not checking) jitter would still land both threads pointed at the same upstream and the first 2 requests would land on the first upstream."
      }
    ]
  },
  {
    "number": 28293,
    "title": "Is dispatcher thread-local?",
    "created_at": "2023-07-08T03:26:32Z",
    "closed_at": "2023-08-18T04:01:27Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28293",
    "body": "That is, could I call `dispatcher.post()` from another thread? I need to put a task into the worker thread from another thread.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28293/comments",
    "author": "kingluo",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2023-07-08T20:51:39Z",
        "body": "Yes, that's a normal way how thread local storage works, it posts from main thread to worker threads with an updated value."
      },
      {
        "user": "kingluo",
        "created_at": "2023-07-09T04:14:14Z",
        "body": "> Yes, that's a normal way how thread local storage works, it posts from main thread to worker threads with an updated value.\r\n\r\nThanks for your reply. And I just confirmed this fact in my coding. BTW, you mean the main thread posts xds update to the worker thread, right?\r\n\r\nAnother side question is whether the post is done by a locked queue and eventfd in Linux, and the worker thread will handle the post callbacks in batch, right?"
      },
      {
        "user": "kyessenov",
        "created_at": "2023-07-11T21:31:24Z",
        "body": "I think it's edge triggered. It's using libevent for event management, but I don't recall the details."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-08-11T00:02:43Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-08-18T04:01:26Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 28264,
    "title": "Does envoy coalesce metrics before sending to stats agents?",
    "created_at": "2023-07-06T14:20:32Z",
    "closed_at": "2023-08-13T00:02:44Z",
    "labels": [
      "question",
      "stale",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28264",
    "body": "*Description*:\r\nWe are investigating some high usage of NET I/O  on datadog agents, right now we have `953GB IN` and only `331MB OUT` from datadog container.\r\nUsing tcpdump `sudo tcpdump -c 10000 -A -s 0 \"udp port 8125\" > output.txt` the capture is completed in less than 1 seconds and after sorting the file we found that there are duplicates lines.\r\nThe flush interval is the default (5 seconds) so we don't expect duplicated lines in a less than 1 second capture.\r\nThe ratio we saw is 4x-5x reduction if we remove duplicated lines.\r\n\r\nDoes envoy not coalesce metrics of all workers before flush?\r\n\r\n\r\n**_Envoy configuration:_**\r\n```\r\nstats_sinks:\r\n- name: envoy.stat_sinks.dog_statsd\r\n  typed_config:\r\n    \"@type\": type.googleapis.com/envoy.config.metrics.v3.DogStatsdSink\r\n    max_bytes_per_datagram: 8192\r\n    prefix: some_prefix\r\n    address:\r\n      socket_address:\r\n        address: $DATADOG_ADDRESS\r\n        port_value: 8125\r\n```\r\n\r\n**_Docker stats:_**\r\n```\r\nCONTAINER ID   NAME                   CPU %     MEM USAGE / LIMIT     MEM %     NET I/O           BLOCK I/O         PIDS\r\n29ca39aa5661   envoy                130.21%    505.3MiB / 15.17GiB    3.25%     2.18TB / 3.27TB   0B / 1.16GB       38\r\n1ad47abd707d   datadog_agent         35.45%    223.1MiB / 15.17GiB    1.44%     953GB / 331MB     379MB / 53.2kB    51\r\n\r\n```\r\n\r\n**_Captured metrics(duplicated):_**\r\n```\r\n....\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\nenvoyapp.cluster.internal.upstream_rq_time:3|ms|#tag1:tagvalue1,criticality:low,environment:prod,envoy.cluster_name:cluster01\r\n...\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28264/comments",
    "author": "andresmargalef",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2023-07-06T17:20:37Z",
        "body": "My understanding is that worker stats are coalesced, but there could be multiple \"scopes\" with the same stat name (or rewrites causing a collision). I'm not familiar with dogstatsd sink, however.\r\n\r\ncc @jmarantz "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-08-05T20:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-08-13T00:02:44Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 28207,
    "title": "Local reply modification body substitution",
    "created_at": "2023-06-30T20:15:47Z",
    "closed_at": "2023-07-07T14:36:00Z",
    "labels": [
      "question",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28207",
    "body": "*Title*: *Local reply modification body substitution*\r\n\r\n*Description*:\r\nAs it currently stands, one can use local reply format modification to do something such as the following:\r\n```\r\nmappers:\r\n- filter:\r\n    status_code_filter:\r\n      comparison:\r\n        op: EQ\r\n        value:\r\n          default_value: 400\r\n          runtime_key: key_b\r\n  status_code: 401\r\n  body:\r\n    inline_string: \"not allowed\"\r\nbody_format:\r\n  text_format: \"%LOCAL_REPLY_BODY% %CEL(request.id)%\"\r\n```\r\nWhile this works for simple use cases, I would like to be able to treat `%LOCAL_REPLY_BODY%` as a template, meaning that if `body` looks like the following:\r\n```\r\n  body:\r\n    inline_string: \"not allowed! request id: %CEL(request.id)%\"\r\n```\r\n`%CEL(request.id)%` should be replaced with the request ID, however no substitution can currently happen on `%LOCAL_REPLY%` from within `body_format`.\r\nTo circumvent this, I would need to have something along the lines of:\r\n```\r\nmappers:\r\n- filter:\r\n    status_code_filter:\r\n      comparison:\r\n        op: EQ\r\n        value:\r\n          default_value: 400\r\n          runtime_key: key_b\r\n  status_code: 401\r\n  body:\r\n    inline_string: \"not allowed! request id:\"\r\nbody_format:\r\n  text_format: \"%LOCAL_REPLY_BODY% %CEL(request.id)%\"\r\n```\r\nHowever, this is also not a very viable solution since the response body needs to be split between `body` and `body_format`, with the separator being where the variable is to be inserted, and will also not work for multiple attributes. I have also tried putting the entire contents of my local reply in `body_format`, however Envoy will fail to load the configuration depending on what the content of the local reply is. Is there any currently supported way to go about this?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28207/comments",
    "author": "pchaseh",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2023-07-05T18:53:31Z",
        "body": "`body_format` is meant as the template I believe. can you elaborate on:\r\n\r\n>  I have also tried putting the entire contents of my local reply in body_format, however Envoy will fail to load the configuration depending on what the content of the local reply is\r\n\r\nLocal reply body is meant as a short fixed string, because it's populated by the internals of Envoy for the standard codec errors."
      },
      {
        "user": "pchaseh",
        "created_at": "2023-07-07T14:36:00Z",
        "body": "> `body_format` is meant as the template I believe. can you elaborate on:\r\n> \r\n> > I have also tried putting the entire contents of my local reply in body_format, however Envoy will fail to load the configuration depending on what the content of the local reply is\r\n> \r\n> Local reply body is meant as a short fixed string, because it's populated by the internals of Envoy for the standard codec errors.\r\n\r\nI mentioned that putting the entire contents of my reply in `body_format` doesn't work and assumed that this field may be strictly limited in the characters I can use without having to escape them, however I found out that Envoy was only rejecting my configuration because I was using the `%` symbol for some inline CSS. After I escaped the occurrences with another `%`, Envoy loaded the entire template fine. This approach works fine, so I will close out the issue."
      }
    ]
  },
  {
    "number": 28010,
    "title": "envoy tcp proxy to redis proxy",
    "created_at": "2023-06-16T07:27:32Z",
    "closed_at": "2023-08-02T16:01:26Z",
    "labels": [
      "question",
      "stale",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28010",
    "body": "need configuration details for envoy proxy\r\nwhere from one node using envoy tcp proxy send requests to another node which is having envoy redis proxy which inturn sends to redis server\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28010/comments",
    "author": "shankar4322",
    "comments": [
      {
        "user": "shankar4322",
        "created_at": "2023-06-26T09:03:13Z",
        "body": "any updates here?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-07-26T12:02:00Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-08-02T16:01:25Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 27921,
    "title": "transport_socket_connect_timeout not working in Envoy proxy config",
    "created_at": "2023-06-12T08:50:28Z",
    "closed_at": "2023-07-19T16:02:08Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27921",
    "body": "              transport_socket:\r\n                name: envoy.transport_sockets.tls\r\n                typed_config:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n                  common_tls_context:\r\n                    alpn_protocols: [\"h2,http/1.1\"]\r\n                    tls_certificates:\r\n                      - certificate_chain:\r\n                          filename: \"/etc/certs/server.crt\"\r\n                        private_key:\r\n                          filename: \"/etc/certs/server.key\"\r\n              transport_socket_connect_timeout: 30s\r\n\r\n\r\nError:\r\nCaused by: io.grpc.StatusRuntimeException: UNAVAILABLE: Connection closed while performing TLS negotiation\r\nChannel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]\r\n        at io.grpc.Status.asRuntimeException(Status.java:535)\r\n        at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:542)\r\n        at io.grpc.internal.DelayedClientCall$DelayedListener$3.run(DelayedClientCall.java:468)\r\n        at io.grpc.internal.DelayedClientCall$DelayedListener.delayOrExecute(DelayedClientCall.java:432)\r\n        at io.grpc.internal.DelayedClientCall$DelayedListener.onClose(DelayedClientCall.java:465)\r\n        at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:562)\r\n        at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:70)\r\n        at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:743)\r\n        at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:722)\r\n        at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\n      at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27921/comments",
    "author": "renuraj18",
    "comments": [
      {
        "user": "renuraj18",
        "created_at": "2023-06-12T09:00:02Z",
        "body": "Envoy error:\r\n[2023-06-12 08:58:48.864][1][critical][main] [source/server/server.cc:101] error initializing configuration '/etc/envoy/envoy.yaml': Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(static_resources.listeners[0].filter_chains[0]) transport_socket_connect_timeout: Cannot find field.) has unknown fields\r\n[2023-06-12 08:58:48.864][1][info][main] [source/server/server.cc:704] exiting\r\n[2023-06-12 08:58:48.864][1][debug][main] [source/common/access_log/access_log_manager_impl.cc:19] destroyed access loggers\r\n[2023-06-12 08:58:48.864][1][debug][init] [source/common/init/watcher_impl.cc:27] init manager Server destroyed\r\nProtobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(static_resources.listeners[0].filter_chains[0]) transport_socket_connect_timeout: Cannot find field.) has unknown fields "
      },
      {
        "user": "KBaichoo",
        "created_at": "2023-06-12T13:18:39Z",
        "body": "ISTM the YAML is not indented correctly for the field: `          transport_socket_connect_timeout: 30s`\r\ne.g. it's not under the transport socket config as it should be. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-07-12T16:01:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-07-19T16:02:07Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 27789,
    "title": "External authentication must HTTPS if Envoy using certificate?",
    "created_at": "2023-06-04T15:48:52Z",
    "closed_at": "2023-06-05T18:53:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27789",
    "body": "Hi,\r\n\r\nI have envoy configured and running with Lets Encrypt certificate.\r\n\r\nI have also .net core http service for external authentication running on the same machine.\r\n\r\nIf I send a request to one of the routes, I got response 404 with no certificate in the response.\r\nWithout external authentication everything works as expected.\r\n\r\nSame service works as expected in local docker-compose without certificate and all setup is plain HTTP.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27789/comments",
    "author": "hbarilan",
    "comments": [
      {
        "user": "ravenblackx",
        "created_at": "2023-06-05T15:06:35Z",
        "body": "I'm not sure what the question is here. Could you elaborate, and include some relevant configuration files?"
      },
      {
        "user": "hbarilan",
        "created_at": "2023-06-05T18:53:18Z",
        "body": "I'm closing the question since it was a misunderstanding of the docs from my side.\r\nThanks."
      }
    ]
  },
  {
    "number": 27549,
    "title": "Why is max_connections not a constraint for Envoy health check?",
    "created_at": "2023-05-22T19:54:51Z",
    "closed_at": "2023-08-01T08:01:41Z",
    "labels": [
      "question",
      "stale",
      "area/connection"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27549",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *Why is max_connections not a constraint for Envoy health check?*\r\n\r\n*Description*:\r\n>We have 2 VMs (virtual machine) connected by Envoy to Envoy gRPC connection. On the client VM, the Envoy serves as forward proxy, and on the server VM, the Envoy on it serves as a reverse proxy. Now this Envoy mesh got 1 tcp connection. When I am adding the active health check, it got 2 connections. We can see from here that the health check is creating the 2nd connection. And all the other services use the 1st connection between Envoy proxy.\r\n\r\nHowever, when I try to set the circuit_breakers max_connections from 2 to 1 on SE Envoy, the connections now are still always 2. Now what does max_connections here really mean? Why it can not constrain the real number of tcp connection?\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27549/comments",
    "author": "johnf3",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-05-24T04:11:42Z",
        "body": "> Why is max_connections not a constraint for Envoy health check?\r\n\r\nIt's hard to say why. I think it is designed that way. The health checker will manage it's connection by itself and all other connections are managed by the connection pool.\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-06-23T08:01:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-07-25T04:01:44Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-08-01T08:01:40Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 27548,
    "title": "load balancer behavior from envoy after changing server application to headless",
    "created_at": "2023-05-22T18:40:50Z",
    "closed_at": "2023-06-30T20:01:32Z",
    "labels": [
      "question",
      "stale",
      "area/load balancing"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27548",
    "body": "*Title*: load balancer behavior after changing service to headless\r\n\r\n*Description*:\r\nWe are looking for help/advise/pointer on the particular problem below.\r\n\r\nWe want to use client-side load balancing from envoy, and we change our server application service to headless. \r\n\r\nWe now observe some behavior change that client grpc requests of the same connection do not go to the same pod; in other words, the requests could be routed to any of the application pod (depending on which LB strategy we pick). We want to know if there is a way make envoy always stay a particular pod for the same client connection once it's established?\r\n\r\nGiven that the server application might be stateful and perform based on client state kept in its memory. \r\n\r\nHere is an example of our envoy configuration\r\n```\r\n    - name: server-app\r\n      type: STRICT_DNS\r\n      dns_lookup_family: V4_ONLY\r\n      lb_policy: ROUND_ROBIN\r\n      http2_protocol_options: {}\r\n      load_assignment:\r\n        cluster_name: server-app\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: server-app-headless\r\n                      port_value: <port>\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27548/comments",
    "author": "mcfongtw",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-05-23T02:58:29Z",
        "body": "This is like a quesion about the session sticky. Could the ring hash lb solve your problem?"
      },
      {
        "user": "mcfongtw",
        "created_at": "2023-05-24T15:11:31Z",
        "body": "We will look into the ring hash lb for this. Thanks!"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-06-23T16:01:43Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-06-30T20:01:31Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 27492,
    "title": "Issues with route timeout configuration",
    "created_at": "2023-05-18T17:39:28Z",
    "closed_at": "2023-05-31T19:10:01Z",
    "labels": [
      "question",
      "investigate",
      "area/router"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27492",
    "body": "Hello\r\n\r\nI am trying to set my route timeout to use 60s value:\r\n\r\n          routes:\r\n          - match:\r\n              prefix: \"/\"\r\n            route:\r\n              cluster: load_balancer_3\r\n              timeout: 60s\r\n              idle_timeout: 60s\r\n              \r\nEnvoy version: v1.26.1 \r\n\r\nAlthough, when I am receiving its response... I am getting 15s default value\r\n\r\nWe are completely uploaded and fine\r\nConnection state changed (MAX_CONCURRENT_STREAMS updated)!\r\n< HTTP/2 504\r\n< date: Mon, 08 Mai 2023 20:15:11 GMT\r\n< content-type: text/plain\r\n< content-length: 24\r\n< server: envoy\r\n<\r\nConnection #0 to host left intact\r\nupstream request timeout\r\nreal 0m15,655s\r\nuser 0m0,025s\r\nsys 0m0,010s\r\nI need to envoy wait 60s for the request\r\n\r\nCould you help me ?\r\nThank you",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27492/comments",
    "author": "thaleslimao",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-05-22T01:47:30Z",
        "body": "Hi, could you provide a complete example config to make the community could reproduce this problem easily? thanks."
      },
      {
        "user": "thaleslimao",
        "created_at": "2023-05-22T14:21:00Z",
        "body": "Hi, follow my config\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address: \r\n        address: 0.0.0.0\r\n        port_value: 80    \r\n      filter_chains:\r\n      - filters:      \r\n        - name: envoy.filters.network.http_connection_manager\r\n            typed_config:          \r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n            preserve_external_request_id: true          \r\n            access_log:\r\n            - name: envoy.access_loggers.file            \r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                path: \"/dev/stdout\"              \r\n                json_format: {\r\n                    \"start_time\": \"%START_TIME%\",                  \r\n                    \"req_method\": \"%REQ(:METHOD)%\",\r\n                    \"req_path\": \"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",                  \r\n                    \"protocol\": \"%PROTOCOL%\",\r\n                    \"response_code\": \"%RESPONSE_CODE%\",\r\n                    \"response_flags\": \"%RESPONSE_FLAGS%\",\r\n                    \"bytes_received\": \"%BYTES_RECEIVED%\",\r\n                    \"bytes_sent\": \"%BYTES_SENT%\",\r\n                    \"duration\": \"%DURATION%\",\r\n                    \"resp_time\": \"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\r\n                    \"req_forwarded_for\": \"%REQ(X-FORWARDED-FOR)%\",\r\n                    \"req_user_agent\": \"%REQ(USER-AGENT)%\",\r\n                    \"req_id\": \"%REQ(X-REQUEST-ID)%\",\r\n                    \"req_authority\": \"%REQ(:AUTHORITY)%\",\r\n                    \"upstream_host\": \"%UPSTREAM_HOST%\"\r\n                  }\r\n          codec_type: auto\r\n          http_filters:\r\n          - name: envoy.filters.http.router\r\n          skip_xff_append: false\r\n          stat_prefix: http\r\n          use_remote_address: true\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: health_check\r\n              domains:\r\n              - \"10.182.*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/health\"\r\n                direct_response:\r\n                  status: 200\r\n            - name: load_balancer_2\r\n              domains:\r\n              - \"a*\"\r\n              - \"b*\"\r\n              - \"c*\"\r\n              - \"d*\"\r\n              - \"e*\"\r\n              - \"f*\"\r\n              - \"g*\"\r\n              - \"h*\"\r\n              - \"i*\"\r\n              - \"j*\"\r\n              - \"k*\"\r\n              - \"l*\"\r\n              - \"m*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: load_balancer_2\r\n                  timeout: 0s\r\n            - name: load_balancer_3\r\n              domains:\r\n              - \"n*\"\r\n              - \"o*\"\r\n              - \"p*\"\r\n              - \"q*\"\r\n              - \"r*\"\r\n              - \"s*\"\r\n              - \"t*\"\r\n              - \"u*\"\r\n              - \"v*\"\r\n              - \"w*\"\r\n              - \"x*\"\r\n              - \"y*\"\r\n              - \"z*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: load_balancer_3\r\n                  timeout: 0s\r\n  clusters:\r\n  - name: load_balancer_2\r\n    connect_timeout: 5s\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: load_balancer_2\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: url_lb2_interno.elb.amazonaws.com\r\n                port_value: 80\r\n  - name: load_balancer_3\r\n    connect_timeout: 5s\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: load_balancer_3\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: url_lb3_interno.elb.amazonaws.com\r\n                port_value: 80\r\n```"
      },
      {
        "user": "wbpcode",
        "created_at": "2023-06-01T12:57:00Z",
        "body": "Sorry for the delayed response. I recently missed lots of github messages.\r\n\r\nI had did a try in my local env but everything just work well. And you also close this issue, so i guess it's fine. 😄 "
      }
    ]
  },
  {
    "number": 27452,
    "title": "What are the cases where connections drain due to xDS updates?",
    "created_at": "2023-05-17T12:20:34Z",
    "closed_at": "2023-07-17T11:38:23Z",
    "labels": [
      "question",
      "area/listener"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27452",
    "body": "I am trying to figure out all the possible cases where connections would drain due to some xDS update, and for some I couldn't properly understand, so will appreciate the community help with this. Some use cases:\r\n\r\n1. Using LDS, where some of the filter chains in the listener config have SDS config (downstream transport TLS that uses SDS to fetch secrets). After a while, the certificate is refreshed from the SDS. Does it cause connections that currently use the filter chain to drain? Will only new connections get the new certificate?\r\n2. Using LDS, and after a while there's a listener update which is identical to the previous listener config, only a single filter chain is removed from the filter chains of that listener - will all the listener connections drain, or just those of the specific filter chain.\r\n3. Similar to second question - only here there's an update to the filter chain config, instead of its removal - for example adding/removing a filter to/from the chain. In this case - will all the listener connections drain? Only those of the specific filter chain? Or otherwise, no drain but only new connections get the updated filter chain config?\r\n4. Using LDS with filter chain that has HCM with HTTP filters that are configured by ECDS - in this case, does connections that use the filter chain drain if any of the filters in the HCM chain get config updated?\r\n5. Same concept question for only RDS update within some specific filter chain.\r\n6. Using LDS where some filter chain has downstream TLS context with ``tls_certificate_sds_secret_configs``, after a while the list of SDS certificates is updated with a new SDS certificate or removed SDS certificate. Does this cause the connection of the filter chain to drain?\r\n\r\nThanks for helpers, sorry for the blast of questions.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27452/comments",
    "author": "ohadvano",
    "comments": [
      {
        "user": "ohadvano",
        "created_at": "2023-06-13T05:31:26Z",
        "body": "cc @adisuissa, @htuch, @ggreenway maybe you could help with some of these as with your expertise in xDS"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-07-13T08:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "ohadvano",
        "created_at": "2023-07-13T09:43:35Z",
        "body": "Still relevant"
      },
      {
        "user": "AmitKatyal-Sophos",
        "created_at": "2024-11-06T14:36:08Z",
        "body": "@ohadvano Could you please share your findings ?"
      }
    ]
  },
  {
    "number": 27441,
    "title": "Envoy TLS config causes SSL WRONG_VERSION_NUMBER error",
    "created_at": "2023-05-17T06:59:25Z",
    "closed_at": "2023-06-24T00:03:14Z",
    "labels": [
      "question",
      "area/tls",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27441",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: Envoy TLS config causes SSL WRONG_VERSION_NUMBER error\r\n\r\n*Description*:\r\nI've set up an Envoy config which connects to Trino on localhost port 11610. It adds TLS configs and redirects to a host. When I call the API, I see the envoy logs resolve the host to the correct IP address, but I get this error:\r\n\r\n```\r\n[source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C29] handshake expecting read\r\n[2023-05-17 04:14:24.357][911][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:198] [C29] handshake error: 1\r\n[2023-05-17 04:14:24.357][911][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C29] TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER\r\n[2023-05-17 04:14:24.357][911][debug][connection] [source/common/network/connection_impl.cc:208] [C29] closing socket: 0\r\nHowever, when I call the endpoint directly using curl (and provide all the TLS certs) it works as expected.\r\n```\r\n\r\nI'm using Envoy version 1.15, and it's supposed to support TLSv1.2 and 1.3. The server I'm trying to connect to also supports those TLS versions. Here is my Envoy config for TLS:\r\n\r\n```\r\ntls_context:\r\n          common_tls_context:\r\n            validation_context:\r\n              trusted_ca:\r\n                filename: /etc/cacerts.pem\r\n            tls_certificates:\r\n              - certificate_chain:\r\n                  filename: \"/etc/client.pem\"\r\n                private_key:\r\n                  filename: \"/etc/client-key.pem\"\r\n        hosts:\r\n        - socket_address:\r\n            address: trino-gateway.path.com\r\n            port_value: 443\r\n```\r\n\r\nI want to see where the TLS configs are being added in the call being made. However, that isn't printing out in the logs.\r\n\r\nI'm outputting the highest level of logging - trace but I don't see the certificates getting added to the request. Is there a way to get more details on it?\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27441/comments",
    "author": "ngalagali3",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2023-05-17T21:10:30Z",
        "body": "That version of envoy is very old and not supported anymore. Does it still reproduce on a newer version?\r\n\r\nAs you noted, the log message is indicating that the server presented a TLS protocol version that envoy doesn't support with this configuration. I'd recommend capturing the handshake with wireshark and figuring out what version is actually being sent, to help narrow down the issue."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-06-17T00:02:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-06-24T00:03:13Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 27368,
    "title": "envoyfilter ：how different configPatches share data",
    "created_at": "2023-05-12T07:17:46Z",
    "closed_at": "2023-06-23T00:03:04Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27368",
    "body": "my envoyfilter\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  creationTimestamp: \"2023-05-12T06:27:35Z\"\r\n  generation: 3\r\n  labels:\r\n    app_id: 93ed9e765a1f499abc22344c355ecc11\r\n  name: newtest\r\n  namespace: zhangqh\r\n  resourceVersion: \"10124649\"\r\n  uid: 70a38753-e0b8-4fb1-aa5d-3953bb6bd763\r\nspec:\r\n  configPatches:\r\n  - applyTo: HTTP_FILTER\r\n    match:\r\n      context: SIDECAR_INBOUND\r\n      listener:\r\n        filterChain:\r\n          filter:\r\n            name: envoy.filters.network.http_connection_manager\r\n    patch:\r\n      operation: INSERT_BEFORE\r\n      value:\r\n        name: envoy.filters.http.lua\r\n        typed_config:\r\n          '@type': type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n          inlineCode: |\r\n            function envoy_on_request(request_handle)\r\n              request_handle:streamInfo():dynamicMetadata():set(\"envoy.filters.http.lua\", \"name\", \"abababc\")\r\n            end\r\n  - applyTo: HTTP_FILTER\r\n    match:\r\n      context: SIDECAR_OUTBOUND\r\n      listener:\r\n        filterChain:\r\n          filter:\r\n            name: envoy.filters.network.http_connection_manager\r\n    patch:\r\n      operation: INSERT_BEFORE\r\n      value:\r\n        name: envoy.filters.http.lua\r\n        typed_config:\r\n          '@type': type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n          inlineCode: |\r\n            function envoy_on_request(request_handle)\r\n              request_handle:headers():add('name', \"zqh\")\r\n            end\r\n  workloadSelector:\r\n    labels:\r\n      service_id: 051a4ad827419a099a5059d8532d9991\r\n```\r\nI want to store data in the first configPatches and get it in the second configPatches",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27368/comments",
    "author": "ZhangSetSail",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2023-05-16T20:34:22Z",
        "body": "This looks Istio-specific; I suggest asking there."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-06-16T00:03:00Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-06-23T00:03:03Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 27362,
    "title": "Envoy listener should enhance listening to IPv6 addresses",
    "created_at": "2023-05-12T01:43:37Z",
    "closed_at": "2023-05-16T20:30:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27362",
    "body": "When I test envoy's support for IPv4/IPv6 dual stack, I need to add '' when listening to all IPv6 addresses\r\n\r\n```\r\n- name: listener_1\r\n          address:\r\n            socket_address: { address: '::' , port_value: 10000 }\r\n          filter_chains:\r\n            - filters:\r\n                - name: envoy.filters.network.http_connection_manager\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                    stat_prefix: ingress_http\r\n                    codec_type: AUTO\r\n                    route_config:\r\n                      name: local_route\r\n                      virtual_hosts:\r\n                      - name: local_service\r\n                        domains: [\"*\"]\r\n                        routes:\r\n                        - match: { prefix: \"/\" }\r\n                          route: { cluster: some_service }\r\n                    http_filters:\r\n                    - name: envoy.filters.http.router\r\n                      typed_config:\r\n                        \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27362/comments",
    "author": "opencmit2",
    "comments": [
      {
        "user": "soulxu",
        "created_at": "2023-05-12T04:59:48Z",
        "body": "that is probably a limit of yaml format."
      },
      {
        "user": "ggreenway",
        "created_at": "2023-05-16T20:30:59Z",
        "body": "I don't think there's anything further to be done on this issue; closing. If this is incorrect, let me know and I can reopen."
      }
    ]
  },
  {
    "number": 27205,
    "title": "custom Log Format",
    "created_at": "2023-05-05T08:45:25Z",
    "closed_at": "2023-05-08T14:09:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27205",
    "body": "I cant use '=' while defining my custom log format in istio\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27205/comments",
    "author": "Ayaz-Tanzeem",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2023-05-05T13:40:54Z",
        "body": "Please direct Istio related issues to Istio forums."
      }
    ]
  },
  {
    "number": 27006,
    "title": "set listener to read_policy: REPLICA, but still seeing traffic going to master node",
    "created_at": "2023-04-26T22:54:08Z",
    "closed_at": "2023-06-03T20:01:28Z",
    "labels": [
      "question",
      "stale",
      "area/redis"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27006",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *set listener to read_policy: REPLICA, but still seeing traffic going to master node*\r\n\r\n*Description*:\r\nhi, I set the read_policy: REPLICA, but when I check the commands count to redis using query: avg(rate(redis_commands_total{cmd=\"get\", \"192.168.0.0\"}[1m])), I still see a lot of commands sending to master node. May I know what could be the reason? Thanks.\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27006/comments",
    "author": "cq-liu",
    "comments": [
      {
        "user": "adisuissa",
        "created_at": "2023-04-27T13:31:09Z",
        "body": "cc @weisisea @mattklein123 as REDIS code-owners\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-27T16:01:52Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-06-03T20:01:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "dceravigupta",
        "created_at": "2023-06-20T20:46:03Z",
        "body": "@cq-liu did you find any update on this issue? In my case, I'm seeing the other way around where Envoy is sending GET commands to replica nodes even though I have configure read_policy as MASTER.\r\n\r\nI'm using envoy version 1.25.6"
      }
    ]
  },
  {
    "number": 26973,
    "title": "failed build: defining a type within '__builtin_offsetof' is a Clang extension",
    "created_at": "2023-04-26T08:34:57Z",
    "closed_at": "2023-06-03T20:01:23Z",
    "labels": [
      "question",
      "area/build",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26973",
    "body": "```\r\n[root@sbc-stage-a0 envoy]# bazel build -c opt --config=clang --verbose_failures envoy\r\nINFO: Analyzed target //:envoy (814 packages loaded, 38567 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/upb/BUILD:102:11: Compiling upb/arena.c [for tool] failed: (Exit 1): clang-16 failed: error executing command (from target @upb//:upb) \r\n  (cd /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/sandbox/processwrapper-sandbox/269/execroot/envoy && \\\r\n  exec env - \\\r\n    PATH=/root/.local/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/clang-16 -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/aarch64-opt-exec-2B5CBBC6/bin/external/upb/_objs/upb/arena.d '-frandom-seed=bazel-out/aarch64-opt-exec-2B5CBBC6/bin/external/upb/_objs/upb/arena.o' '-DBAZEL_CURRENT_REPOSITORY=\"upb\"' -iquote external/upb -iquote bazel-out/aarch64-opt-exec-2B5CBBC6/bin/external/upb -g0 '-std=c99' -pedantic '-Werror=pedantic' -Wall -Wstrict-prototypes -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/upb/upb/arena.c -o bazel-out/aarch64-opt-exec-2B5CBBC6/bin/external/upb/_objs/upb/arena.o)\r\n# Configuration: d85be2fa4cfcda06404dffd7fcf6834fd782850638c3fc09087e28ac549bf9da\r\n# Execution platform: @local_config_platform//:host\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox and retain the sandbox build root for debugging\r\nexternal/upb/upb/arena.c:177:25: error: defining a type within '__builtin_offsetof' is a Clang extension [-Werror,-Wgnu-offsetof-extensions]\r\n  n = UPB_ALIGN_DOWN(n, UPB_ALIGN_OF(upb_Arena));\r\n                        ^~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/upb/upb/port_def.inc:95:38: note: expanded from macro 'UPB_ALIGN_OF'\r\n#define UPB_ALIGN_OF(type) offsetof (struct { char c; type member; }, member)\r\n                                     ^~~~~~\r\n/usr/lib64/clang/16/include/stddef.h:111:43: note: expanded from macro 'offsetof'\r\n#define offsetof(t, d) __builtin_offsetof(t, d)\r\n                                          ^\r\nexternal/upb/upb/port_def.inc:93:48: note: expanded from macro 'UPB_ALIGN_DOWN'\r\n#define UPB_ALIGN_DOWN(size, align) ((size) / (align) * (align))\r\n                                               ^~~~~\r\nexternal/upb/upb/arena.c:177:25: error: defining a type within '__builtin_offsetof' is a Clang extension [-Werror,-Wgnu-offsetof-extensions]\r\n  n = UPB_ALIGN_DOWN(n, UPB_ALIGN_OF(upb_Arena));\r\n                        ^~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/upb/upb/port_def.inc:95:38: note: expanded from macro 'UPB_ALIGN_OF'\r\n#define UPB_ALIGN_OF(type) offsetof (struct { char c; type member; }, member)\r\n                                     ^~~~~~\r\n/usr/lib64/clang/16/include/stddef.h:111:43: note: expanded from macro 'offsetof'\r\n#define offsetof(t, d) __builtin_offsetof(t, d)\r\n                                          ^\r\nexternal/upb/upb/port_def.inc:93:58: note: expanded from macro 'UPB_ALIGN_DOWN'\r\n#define UPB_ALIGN_DOWN(size, align) ((size) / (align) * (align))\r\n                                                         ^~~~~\r\n2 errors generated.\r\nTarget //source/exe:envoy-static failed to build\r\nINFO: Elapsed time: 316.308s, Critical Path: 51.63s\r\nINFO: 341 processes: 74 internal, 267 processwrapper-sandbox.\r\nFAILED: Build did NOT complete successfully\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26973/comments",
    "author": "sergey-safarov",
    "comments": [
      {
        "user": "sergey-safarov",
        "created_at": "2023-04-26T08:38:29Z",
        "body": "One more\r\n\r\n```\r\n[root@sbc-stage-a0 envoy]# bazel build -c opt --config=clang --verbose_failures --cxxopt=\"-Wno-gnu-offsetof-extensions\" envoy\r\nINFO: Build option --cxxopt has changed, discarding analysis cache.\r\nINFO: Analyzed target //:envoy (1 packages loaded, 39165 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/upb/BUILD:189:11: Compiling upb/mini_table.c [for tool] failed: (Exit 1): clang-16 failed: error executing command (from target @upb//:mini_table) \r\n  (cd /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/sandbox/processwrapper-sandbox/471/execroot/envoy && \\\r\n  exec env - \\\r\n    PATH=/root/.local/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/clang-16 -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/aarch64-opt-exec-2B5CBBC6/bin/external/upb/_objs/mini_table/mini_table.d '-frandom-seed=bazel-out/aarch64-opt-exec-2B5CBBC6/bin/external/upb/_objs/mini_table/mini_table.o' '-DBAZEL_CURRENT_REPOSITORY=\"upb\"' -iquote external/upb -iquote bazel-out/aarch64-opt-exec-2B5CBBC6/bin/external/upb -g0 '-std=c99' -pedantic '-Werror=pedantic' -Wall -Wstrict-prototypes -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/upb/upb/mini_table.c -o bazel-out/aarch64-opt-exec-2B5CBBC6/bin/external/upb/_objs/mini_table/mini_table.o)\r\n# Configuration: d85be2fa4cfcda06404dffd7fcf6834fd782850638c3fc09087e28ac549bf9da\r\n# Execution platform: @local_config_platform//:host\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox and retain the sandbox build root for debugging\r\nexternal/upb/upb/mini_table.c:637:14: error: defining a type within '__builtin_offsetof' is a Clang extension [-Werror,-Wgnu-offsetof-extensions]\r\n  UPB_ASSERT(UPB_ALIGN_OF(upb_StringView) ==\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/upb/upb/port_def.inc:95:38: note: expanded from macro 'UPB_ALIGN_OF'\r\n#define UPB_ALIGN_OF(type) offsetof (struct { char c; type member; }, member)\r\n                                     ^~~~~~\r\n/usr/lib64/clang/16/include/stddef.h:111:43: note: expanded from macro 'offsetof'\r\n#define offsetof(t, d) __builtin_offsetof(t, d)\r\n                                          ^\r\nexternal/upb/upb/port_def.inc:146:49: note: expanded from macro 'UPB_ASSERT'\r\n#define UPB_ASSERT(expr) do {} while (false && (expr))\r\n                                                ^~~~\r\n1 error generated.\r\nTarget //source/exe:envoy-static failed to build\r\nINFO: Elapsed time: 26.791s, Critical Path: 12.00s\r\nINFO: 221 processes: 20 internal, 201 processwrapper-sandbox.\r\nFAILED: Build did NOT complete successfully\r\n```"
      },
      {
        "user": "phlax",
        "created_at": "2023-04-26T08:39:43Z",
        "body": "@sergey-safarov clang-16 is not currently supported or tested - clang-14 is, could you try with that please"
      },
      {
        "user": "adisuissa",
        "created_at": "2023-04-27T13:19:17Z",
        "body": "Related: #26972\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-27T16:01:46Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-06-03T20:01:22Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 26893,
    "title": "An thrift protocol request question",
    "created_at": "2023-04-24T10:15:25Z",
    "closed_at": "2023-06-01T12:01:24Z",
    "labels": [
      "question",
      "stale",
      "area/thrift"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26893",
    "body": "The realization of the work for envoy.filters.network.thrift_proxy:\r\nWhen a Thrift protocol request goes to the Envoy, the Envoy decodes the request through the ThriftFilter, converts it into a new HTTP request, and sends the HTTP request to the back-end service. When the response from the back-end service arrives, the Envoy converts the HTTP response into the Thrift serialization format and sends it back to the client.\r\nAm I reading this correctly?\r\nWaiting for your reply, thank you!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26893/comments",
    "author": "jiayoukun",
    "comments": [
      {
        "user": "adisuissa",
        "created_at": "2023-04-24T21:00:12Z",
        "body": "cc @JuniorHsu as code-owner."
      },
      {
        "user": "jiayoukun",
        "created_at": "2023-04-25T06:22:04Z",
        "body": "@JuniorHsu \r\nWaiting for your answer, thank you！"
      },
      {
        "user": "JuniorHsu",
        "created_at": "2023-04-25T06:34:37Z",
        "body": "There's no HTTP protocol involved. It's `thrift_proxy` so envoy basically *proxy* the thrift request to the upstream and the thrift response back to the downstream. We can do metrics/load balancing/routing/filter from features of thrift like thrift header or method. The connection model is like HTTP1 at the moment, i.e., ping-pong model. "
      },
      {
        "user": "jiayoukun",
        "created_at": "2023-04-25T06:50:04Z",
        "body": "> There's no HTTP protocol involved. It's `thrift_proxy` so envoy basically _proxy_ the thrift request to the upstream and the thrift response back to the downstream. We can do metrics/load balancing/routing/filter from features of thrift like thrift header or method. The connection model is like HTTP1 at the moment, i.e., ping-pong model.\r\n\r\nThank you very much for your reply!"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-25T08:01:36Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-06-01T12:01:23Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 26885,
    "title": "invalid value Invalid type URL, unknown type",
    "created_at": "2023-04-24T03:52:37Z",
    "closed_at": "2023-04-25T00:54:17Z",
    "labels": [
      "question",
      "area/golang"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26885",
    "body": "*Title*: *Configuration file running error*\r\n\r\n*Description*:\r\nI plan to use Golang to develop an HTTP filter plugin for Envoy, but after building it locally, there was an error running the golang example in Repo\r\n\r\nCompilation and operation process\r\n>\r\n>- Building the environment\r\n>   - Apple M1\r\n>   - macos 13.3.1 (22E261)\r\n>   - envoy (v1.26.0)\r\n>\r\n>- Build Command\r\n>\r\n>   ```sh\r\n>    bazel build -c dbg --spawn_strategy=local //source/exe:envoy-static \r\n>    ```\r\n>\r\n>   ```sh\r\n>    go build -o simple.so -buildmode=c-shared .\r\n>    ```\r\n>\r\n>- Execute Command\r\n>\r\n>   ```sh\r\n>    ../../bazel-bin/source/exe/envoy-static -c ./envoy.yaml\r\n>    ```\r\n>\r\nError message output by the terminal\r\n>\r\n>- Command execution error\r\n>\r\n>  ```shell\r\n>  [2023-04-24 11:42:08.658][161426][info][main] [source/server/server.cc:408]   envoy.transport_sockets.downstream: envoy.transport_sockets.alts, envoy.transport_sockets.quic, envoy.transport_sockets.raw_buffer, envoy.transport_sockets.starttls, envoy.transport_sockets.tap, envoy.transport_sockets.tcp_stats, envoy.transport_sockets.tls, raw_buffer, starttls, tls\r\n>  [2023-04-24 11:42:08.658][161426][info][main] [source/server/server.cc:408]   envoy.path.rewrite: envoy.path.rewrite.uri_template.uri_template_rewriter\r\n>  [2023-04-24 11:42:08.658][161426][info][main] [source/server/server.cc:408]   envoy.matching.network.input: envoy.matching.inputs.application_protocol, envoy.matching.inputs.destination_ip, envoy.matching.inputs.destination_port, envoy.matching.inputs.direct_source_ip, envoy.matching.inputs.dns_san, envoy.matching.inputs.filter_state, envoy.matching.inputs.server_name, envoy.matching.inputs.source_ip, envoy.matching.inputs.source_port, envoy.matching.inputs.source_type, envoy.matching.inputs.subject, envoy.matching.inputs.transport_protocol, envoy.matching.inputs.uri_san\r\n>  [2023-04-24 11:42:08.682][161426][critical][main] [source/server/server.cc:131] error initializing configuration './envoy.yaml': Unable to parse JSON as proto (INVALID_ARGUMENT:(http_filters[0].typed_config): invalid value Invalid type URL, unknown type: envoy.extensions.filters.http.golang.v3alpha.Config for type Any): {\"static_resources\":{\"clusters\":[{\"type\":\"STRICT_DNS\",\"name\":\"helloworld_service_cluster\",\"load_assignment\":{\"cluster_name\":\"helloworld_service_cluster\",\"endpoints\":[{\"lb_endpoints\":[{\"endpoint\":{\"address\":{\"socket_address\":{\"port_value\":8080,\"address\":\"helloworld_service\"}}}}]}]},\"lb_policy\":\"ROUND_ROBIN\"}],\"listeners\":[{\"filter_chains\":[{\"filters\":[{\"name\":\"envoy.filters.network.http_connection_manager\",\"typed_config\":{\"http_filters\":[{\"name\":\"envoy.filters.http.golang\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\",\"library_id\":\"simple\",\"library_path\":\"/Users/mu/Downloads/attains/be/envoy/examples/golang/simple/simple.so\",\"plugin_name\":\"simple\",\"plugin_config\":{\"value\":{\"prefix_localreply_body\":\"Configured local reply from go\"},\"@type\":\"type.googleapis.com/xds.type.v3.TypedStruct\"}}},{\"name\":\"envoy.filters.http.router\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"}}],\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"route_config\":{\"virtual_hosts\":[{\"routes\":[{\"match\":{\"prefix\":\"/\"},\"route\":{\"cluster\":\"helloworld_service_cluster\"}}],\"domains\":[\"*\"],\"name\":\"local_service\"}],\"name\":\"local_route\"},\"stat_prefix\":\"ingress_http\"}}]}],\"address\":{\"socket_address\":{\"port_value\":10000,\"address\":\"0.0.0.0\"}},\"name\":\"listener_0\"}]}}\r\n>  [2023-04-24 11:42:08.684][161426][info][main] [source/server/server.cc:980] exiting\r\n>  Unable to parse JSON as proto (INVALID_ARGUMENT:(http_filters[0].typed_config): invalid value Invalid type URL, unknown type: envoy.extensions.filters.http.golang.v3alpha.Config for type Any): {\"static_resources\":{\"clusters\":[{\"type\":\"STRICT_DNS\",\"name\":\"helloworld_service_cluster\",\"load_assignment\":{\"cluster_name\":\"helloworld_service_cluster\",\"endpoints\":[{\"lb_endpoints\":[{\"endpoint\":{\"address\":{\"socket_address\":{\"port_value\":8080,\"address\":\"helloworld_service\"}}}}]}]},\"lb_policy\":\"ROUND_ROBIN\"}],\"listeners\":[{\"filter_chains\":[{\"filters\":[{\"name\":\"envoy.filters.network.http_connection_manager\",\"typed_config\":{\"http_filters\":[{\"name\":\"envoy.filters.http.golang\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\",\"library_id\":\"simple\",\"library_path\":\"/Users/mu/Downloads/attains/be/envoy/examples/golang/simple/simple.so\",\"plugin_name\":\"simple\",\"plugin_config\":{\"value\":{\"prefix_localreply_body\":\"Configured local reply from go\"},\"@type\":\"type.googleapis.com/xds.type.v3.TypedStruct\"}}},{\"name\":\"envoy.filters.http.router\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"}}],\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"route_config\":{\"virtual_hosts\":[{\"routes\":[{\"match\":{\"prefix\":\"/\"},\"route\":{\"cluster\":\"helloworld_service_cluster\"}}],\"domains\":[\"*\"],\"name\":\"local_service\"}],\"name\":\"local_route\"},\"stat_prefix\":\"ingress_http\"}}]}],\"address\":{\"socket_address\":{\"port_value\":10000,\"address\":\"0.0.0.0\"}},\"name\":\"listener_0\"}]}}\r\n>  ```\r\n>\r\n>- Error in verifying configuration command\r\n>\r\n>  ```shell\r\n>  ➜  golang git:(v1.26.0) ✗ ../../bazel-bin/source/exe/envoy-static --mode validate -c ./envoy.yaml\r\n>  [2023-04-24 11:42:37.440][161852][critical][main] [source/server/config_validation/server.cc:66] error initializing configuration './envoy.yaml': Unable to parse JSON as proto (INVALID_ARGUMENT:(http_filters[0].typed_config): invalid value Invalid type URL, unknown type: envoy.extensions.filters.http.golang.v3alpha.Config for type Any): {\"static_resources\":{\"clusters\":[{\"load_assignment\":{\"cluster_name\":\"helloworld_service_cluster\",\"endpoints\":[{\"lb_endpoints\":[{\"endpoint\":{\"address\":{\"socket_address\":{\"port_value\":8080,\"address\":\"helloworld_service\"}}}}]}]},\"lb_policy\":\"ROUND_ROBIN\",\"name\":\"helloworld_service_cluster\",\"type\":\"STRICT_DNS\"}],\"listeners\":[{\"filter_chains\":[{\"filters\":[{\"name\":\"envoy.filters.network.http_connection_manager\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"stat_prefix\":\"ingress_http\",\"http_filters\":[{\"typed_config\":{\"library_id\":\"simple\",\"plugin_config\":{\"value\":{\"prefix_localreply_body\":\"Configured local reply from go\"},\"@type\":\"type.googleapis.com/xds.type.v3.TypedStruct\"},\"library_path\":\"/Users/mu/Downloads/attains/be/envoy/examples/golang/simple/simple.so\",\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\",\"plugin_name\":\"simple\"},\"name\":\"envoy.filters.http.golang\"},{\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"},\"name\":\"envoy.filters.http.router\"}],\"route_config\":{\"name\":\"local_route\",\"virtual_hosts\":[{\"domains\":[\"*\"],\"name\":\"local_service\",\"routes\":[{\"match\":{\"prefix\":\"/\"},\"route\":{\"cluster\":\"helloworld_service_cluster\"}}]}]}}}]}],\"address\":{\"socket_address\":{\"address\":\"0.0.0.0\",\"port_value\":10000}},\"name\":\"listener_0\"}]}}\r\n>  ```\r\n>\r\nconfiguration file\r\n>\r\n>```yaml\r\n># envoy demo with golang extension enabled\r\n>static_resources:\r\n>  listeners:\r\n>  - name: listener_0\r\n>    address:\r\n>      socket_address:\r\n>        address: 0.0.0.0\r\n>        port_value: 10000\r\n>    filter_chains:\r\n>    - filters:\r\n>      - name: envoy.filters.network.http_connection_manager\r\n>        typed_config:\r\n>          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n>          stat_prefix: ingress_http\r\n>          http_filters:\r\n>          - name: envoy.filters.http.golang\r\n>            typed_config:\r\n>              \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\r\n>              library_id: simple\r\n>              library_path: \"/Users/mu/Downloads/attains/be/envoy/examples/golang/simple/simple.so\"\r\n>              plugin_name: simple\r\n>              plugin_config:\r\n>                \"@type\": type.googleapis.com/xds.type.v3.TypedStruct\r\n>                value:\r\n>                  prefix_localreply_body: \"Configured local reply from go\"\r\n>          - name: envoy.filters.http.router\r\n>            typed_config:\r\n>              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n>          route_config:\r\n>            name: local_route\r\n>            virtual_hosts:\r\n>            - name: local_service\r\n>              domains: [\"*\"]\r\n>              routes:\r\n>              - match:\r\n>                  prefix: \"/\"\r\n>                route:\r\n>                  cluster: helloworld_service_cluster\r\n>  clusters:\r\n>  - name: helloworld_service_cluster\r\n>    type: STRICT_DNS\r\n>    lb_policy: ROUND_ROBIN\r\n>    load_assignment:\r\n>      cluster_name: helloworld_service_cluster\r\n>      endpoints:\r\n>      - lb_endpoints:\r\n>        - endpoint:\r\n>            address:\r\n>              socket_address:\r\n>                address: helloworld_service\r\n>                port_value: 8080\r\n>```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26885/comments",
    "author": "lyonmu",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-04-24T07:00:32Z",
        "body": "golang filter is only provided by the binary with contrib extensions. You may need to build envoy binary with target `//contrib/exe:envoy-static` ."
      },
      {
        "user": "lyonmu",
        "created_at": "2023-04-24T07:49:59Z",
        "body": "Hi @wbpcode \r\nI attempted to use the following command for the build operation, but a new error occurred.\r\n>\r\n>\r\n>```shell\r\n>➜  envoy git:(v1.26.0) ✗ bazel build -c dbg --spawn_strategy=local //contrib/exe:envoy-static                                                     \r\n>ERROR: Target //contrib/exe:envoy-static is incompatible and cannot be built, but was explicitly requested.\r\n>Dependency chain:\r\n>    //contrib/exe:envoy-static (e0803d)\r\n>    //contrib/vcl/source:config_envoy_extension (e0803d)\r\n>    //contrib/vcl/source:config (e0803d)\r\n>    //contrib/vcl/source:vcl_interface_lib (e0803d)\r\n>    //contrib/vcl/source:vpp_vcl (e0803d)\r\n>    //contrib/vcl/source:external/libsvm.a (e0803d)\r\n>    //contrib/vcl/source:build (e0803d)   <-- target platform (@envoy//bazel:macos_arm64) didn't satisfy constraint @platforms//os:linux\r\n>INFO: Elapsed time: 0.128s\r\n>INFO: 0 processes.\r\n>FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n>```"
      },
      {
        "user": "wbpcode",
        "created_at": "2023-04-24T08:27:46Z",
        "body": "Lots of contrib extensions have constraints to the os/platform. You may need comment these extenstions in the `contrib_build_config.bzl`."
      },
      {
        "user": "lyonmu",
        "created_at": "2023-04-24T09:18:24Z",
        "body": "Thank you, I have successfully built the envoy locally."
      }
    ]
  },
  {
    "number": 26801,
    "title": "Description Failed to convert the accesslog proto file",
    "created_at": "2023-04-18T08:42:17Z",
    "closed_at": "2023-05-26T00:02:57Z",
    "labels": [
      "question",
      "area/build",
      "stale",
      "api"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26801",
    "body": "I need to convert the method of accesslog proto file into go method for use, but the following error was reported after I tried to convert. Have you ever encountered it? How to solve it?\r\nWaiting for your reply, thank you!\r\n\r\nI'm executing under an envoy/api directory.\r\n$ git submodule update --init --recursive\r\n$ protoc --go_out=. --go-grpc_out=. envoy/config/accesslog/v3/accesslog.proto\r\nudpa/annotations/status.proto: File not found.\r\nudpa/annotations/versioning.proto: File not found.\r\nvalidate/validate.proto: File not found.\r\n\r\n\r\n\r\nI have installed protoc.\r\n$ protoc --version\r\nlibprotoc 22.3\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26801/comments",
    "author": "jiayoukun",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2023-04-18T12:13:25Z",
        "body": "The API protos need to be built under Bazel, there's no support for direct invocation via protoc. @kyessenov I think you might have some experience building Go protos IIRC?"
      },
      {
        "user": "kyessenov",
        "created_at": "2023-04-18T16:41:24Z",
        "body": "Yeah, you need to include all the dependencies protos with `-I`. Here you are missing protoc-gen-validate and cncf/xds protos."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-18T20:01:37Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-26T00:02:57Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 26794,
    "title": "grpc web max payload size",
    "created_at": "2023-04-17T20:22:20Z",
    "closed_at": "2023-04-19T18:42:32Z",
    "labels": [
      "question",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26794",
    "body": "Is there a way to increase the maximum payload size of grpc web requests in envoy? It seems that the default max is 4mb. I tried adding `max_request_size ` to the `grpc_web` http filter under `typed_config`, but it didn't seem to work (gives the error `Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:max_request_size: Cannot find field.) has unknown fields`). Would really appreciate if anyone can help on this, Thanks!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26794/comments",
    "author": "Bairou2001",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2023-04-18T12:16:12Z",
        "body": "@fengli79 @lizan "
      },
      {
        "user": "Bairou2001",
        "created_at": "2023-04-19T18:42:32Z",
        "body": "Closed. sorry about the confusion, it was unrelated to envoy. Envoy's http filter doesn't have a limit on the request size."
      }
    ]
  },
  {
    "number": 26719,
    "title": "how envoy close the connection actively  when upstream clusters changed？",
    "created_at": "2023-04-13T07:12:07Z",
    "closed_at": "2023-05-24T12:01:25Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26719",
    "body": "I would like to ask a question. \r\nEnvoy uses TcpProxy to proxy redis. The upstream redis cluster changes, and the control plane sends changes. Envoy updates the upstream cluster. \r\nThe new connection of the redis client can point to the new cluster, but the existing long connection always exists. \r\nDoes envoy have it? How to transfer the old connection to the new cluster or disconnect the old connection?\r\n\r\nThe configuration file is as follows：\r\n### envoy.yaml\r\n```\r\nnode:\r\n  id: id_1\r\n  cluster: test\r\n\r\ndynamic_resources:\r\n  cds_config:\r\n    path_config_source:\r\n      path: examples/dynamic-config-fs/tcp-proxy-configs/cds.yaml\r\n  lds_config:\r\n    path_config_source:\r\n      path: examples/dynamic-config-fs/tcp-proxy-configs/lds.yaml\r\n\r\nadmin:\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 19000\r\n```\r\n\r\n### lds.yaml\r\n```\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.config.listener.v3.Listener\r\n  name: listener_2222\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 2222\r\n  filter_chains:\r\n    - filters:\r\n        - name: envoy.filters.network.tcp_proxy\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n            stat_prefix: destination\r\n            cluster: cluster_2222\r\n            access_log:\r\n              - name: envoy.access_loggers.stdout\r\n                typed_config:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\r\n```\r\n\r\n### cds.yaml\r\n```\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.config.cluster.v3.Cluster\r\n  name: cluster_2222\r\n  connect_timeout: 30s\r\n  type: LOGICAL_DNS\r\n  dns_lookup_family: V4_ONLY\r\n  load_assignment:\r\n    cluster_name: cluster_2222\r\n    endpoints:\r\n      - lb_endpoints:\r\n          - endpoint:\r\n              address:\r\n                socket_address:\r\n                  address: 127.0.0.1\r\n                  port_value: 7777\r\n```\r\n\r\nThe startup command is as follows：\r\n`./linux/amd64/build_envoy_debug/envoy -c examples/dynamic-config-fs/envoy.yaml --concurrency 1 --log-level trace\r\n`\r\n\r\nStart the redis server:\r\n```\r\nredis-server --port 7777   #redis1\r\nredis-server --port 8888   #redis2\r\n```\r\n\r\nStart the redis client:\r\n`redis-cli -p 2222\r\n`\r\n\r\nAs you can see, envoy establishes a connection with redis1.\r\nAt this time, change the port_value in cds.yaml from 7777 to 8888.\r\nWhen creating a new connection to envoy, envoy establishes a connection with redis2, but the connection with redis1 will always exist.\r\nI want to disconnect the connection between envoy and redis1 now, is there any way?\r\n\r\nLooking forward to your answer.\r\nThanks.\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26719/comments",
    "author": "mloves0824",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2023-04-14T14:44:16Z",
        "body": "Envoy will wait for connections to drain so I would recommend using an idle timeout or max connection length timeout of some kind. Note also that there is a drain timeout and IIRC after some period of time the connections will be forced closed."
      },
      {
        "user": "mloves0824",
        "created_at": "2023-04-17T07:02:09Z",
        "body": "> Envoy will wait for connections to drain so I would recommend using an idle timeout or max connection length timeout of some kind. Note also that there is a drain timeout and IIRC after some period of time the connections will be forced closed.\r\n\r\nThanks. \r\nAre there other side effects of actively disconnecting by drain and timeout?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-17T08:01:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-24T12:01:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 26692,
    "title": "What is a proper way to make a HTTP or grpc request to some server after receiving a HTTP request from client",
    "created_at": "2023-04-12T07:56:45Z",
    "closed_at": "2023-04-14T01:43:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26692",
    "body": "I used Envoy as the gateway of my backend. Each request of clients contains a token, which represents a user. So at the backend side, we need to extract the token and validate it. If it's an illegal token, reply an error to the client directly.\r\n\r\nI'm thinking if I can do this in Envoy. Let's say that we put the token of client into the header of request. So what Envoy should do is to extract the token from the header of request and send it to a server, which can validate the token, and then Envoy should wait for the response. If the response says the token is valid, Envoy should route the request to upstream clusters. Otherwise, Envoy should reply the client directly with some error info.\r\n\r\nFor this case, I think the simplest way is to use `extensions.filters.http.lua.V3.Lua`. Meaning that I can do this with a piece of Lua code inside of the config of Envoy. However, I don't know if this is a proper way. As my understanding, Lua is simply a kind of script and there are about 80 thousands clients in my web server for now. I'm worrying about the performance of Lua inside of Envoy...\r\n\r\nSo is there some better solution to do so? Should I develop a custom http filter and compile my own Envoy?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26692/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2023-04-12T17:24:20Z",
        "body": "I don't think you need to write any custom filter for this. You could do this with one of: lua, ext-authz, ext-proc. I would take a look at those."
      },
      {
        "user": "MinusRF",
        "created_at": "2023-04-13T02:44:42Z",
        "body": "Shouldn't that be handled by upstream server itself, rather than envoy? If you want to respond to client in-certain way upon getting illegal token that you can do using response flag filter. One more way could be Lua, I am using Lua for processing  headers and body of request at a scale similar to yours, didn't see any performance issue. "
      },
      {
        "user": "YvesZHI",
        "created_at": "2023-04-13T08:20:33Z",
        "body": "@MinusRF  Thanks. Since Envoy is responsible for RBAC, I'm thinking it might be good to validate tokens of clients in Envoy. So I would try Lua or ext-authz."
      }
    ]
  },
  {
    "number": 26684,
    "title": "gRPC-JSON transcoder ignoring CORS",
    "created_at": "2023-04-11T15:49:25Z",
    "closed_at": "2023-05-19T20:01:25Z",
    "labels": [
      "question",
      "stale",
      "area/cors",
      "area/grpc-transcoding"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26684",
    "body": "I am currently having an issue with the transcoder ignoring the preflight CORS and returning a 405 when OPTIONS is being sent.\r\n\r\nHere is my current config:\r\n```\r\nadmin:\r\n  access_log_path: /dev/stdout\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\r\nstatic_resources:\r\n  listeners:\r\n    - name: grpc-listener\r\n      address:\r\n        socket_address: { address: 0.0.0.0, port_value: 10000 }\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: grpc_json\r\n                codec_type: AUTO\r\n                access_log:\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                    path: /dev/stdout\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains: [\"*\"]\r\n                      typed_per_filter_config:\r\n                        envoy.filters.http.cors:\r\n                          \"@type\": type.googleapis.com/envoy.extensions.filters.http.cors.v3.CorsPolicy\r\n                          allow_origin_string_match:\r\n                          - safe_regex:\r\n                              regex: \\*\r\n                          allow_methods: \"GET,POST,PUT,DELETE,OPTIONS\"\r\n                      routes:\r\n                        - match: { prefix: \"/\"}\r\n                          route: { cluster: Greeter, timeout: { seconds: 60 } }\r\n                        - match: { prefix: \"/\", grpc: {} }\r\n                          route: { cluster: Illustration, timeout: { seconds: 60 } }\r\n                        - match: { prefix: \"/\", grpc: {} }\r\n                          route: { cluster: Products, timeout: { seconds: 60 } }\r\n                        - match: { prefix: \"/\", grpc: {} }\r\n                          route: { cluster: Validate, timeout: { seconds: 60 } }\r\n                http_filters:\r\n                - name: envoy.filters.http.cors\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.filters.http.cors.v3.Cors\r\n                - name: envoy.filters.http.grpc_json_transcoder\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.filters.http.grpc_json_transcoder.v3.GrpcJsonTranscoder\r\n                    proto_descriptor: \"/greet.pb\"\r\n                    services: [\"greet.v1.Greeter\"]\r\n                    match_incoming_request_route: false\r\n                    ignore_unknown_query_parameters: true\r\n                    auto_mapping: false\r\n                    convert_grpc_status: true\r\n                    print_options:\r\n                      add_whitespace: true\r\n                      always_print_primitive_fields: true\r\n                      always_print_enums_as_ints: false\r\n                      preserve_proto_field_names: true\r\n                - name: envoy.filters.http.grpc_json_transcoder\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.filters.http.grpc_json_transcoder.v3.GrpcJsonTranscoder\r\n                    proto_descriptor: \"/illustration.pb\"\r\n                    services: [\"illustration.v1.Illustration\"]\r\n                    match_incoming_request_route: false\r\n                    ignore_unknown_query_parameters: true\r\n                    auto_mapping: false\r\n                    convert_grpc_status: true\r\n                    print_options:\r\n                      add_whitespace: true\r\n                      always_print_primitive_fields: true\r\n                      always_print_enums_as_ints: false\r\n                      preserve_proto_field_names: true\r\n                - name: envoy.filters.http.grpc_json_transcoder\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.filters.http.grpc_json_transcoder.v3.GrpcJsonTranscoder\r\n                    proto_descriptor: \"/products.pb\"\r\n                    services: [\"products.v1.Products\"]\r\n                    match_incoming_request_route: false\r\n                    ignore_unknown_query_parameters: true\r\n                    auto_mapping: false\r\n                    convert_grpc_status: true\r\n                    print_options:\r\n                      add_whitespace: true\r\n                      always_print_primitive_fields: true\r\n                      always_print_enums_as_ints: false\r\n                      preserve_proto_field_names: true\r\n                - name: envoy.filters.http.grpc_json_transcoder\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.filters.http.grpc_json_transcoder.v3.GrpcJsonTranscoder\r\n                    proto_descriptor: \"/validate.pb\"\r\n                    services: [\"validate.v1.Validate\"]\r\n                    match_incoming_request_route: false\r\n                    ignore_unknown_query_parameters: true\r\n                    auto_mapping: false\r\n                    convert_grpc_status: true\r\n                    print_options:\r\n                      add_whitespace: true\r\n                      always_print_primitive_fields: true\r\n                      always_print_enums_as_ints: false\r\n                      preserve_proto_field_names: true\r\n                - name: envoy.filters.http.router\r\n                  typed_config: \r\n                    \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n  clusters:\r\n    - name: Greeter\r\n      connect_timeout: 1.25s\r\n      type: logical_dns\r\n      lb_policy: round_robin\r\n      dns_lookup_family: V4_ONLY\r\n      http2_protocol_options: {}\r\n      load_assignment:\r\n        cluster_name: Greeter\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: host.docker.internal\r\n                      port_value: 5202\r\n    - name: Illustration\r\n      connect_timeout: 1.25s\r\n      type: logical_dns\r\n      lb_policy: round_robin\r\n      dns_lookup_family: V4_ONLY\r\n      http2_protocol_options: {}\r\n      load_assignment:\r\n        cluster_name: Illustration\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: host.docker.internal\r\n                      port_value: 5184\r\n    - name: Products\r\n      connect_timeout: 1.25s\r\n      type: logical_dns\r\n      lb_policy: round_robin\r\n      dns_lookup_family: V4_ONLY\r\n      http2_protocol_options: {}\r\n      load_assignment:\r\n        cluster_name: Products\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: host.docker.internal\r\n                      port_value: 5202\r\n    - name: Validate\r\n      connect_timeout: 1.25s\r\n      type: logical_dns\r\n      lb_policy: round_robin\r\n      dns_lookup_family: V4_ONLY\r\n      http2_protocol_options: {}\r\n      load_assignment:\r\n        cluster_name: Validate\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: host.docker.internal\r\n                      port_value: 5202\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26684/comments",
    "author": "RickCurrey",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2023-04-12T16:16:59Z",
        "body": "cc @qiwzhang @lizan "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-12T20:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-19T20:01:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "mnkg561",
        "created_at": "2023-08-19T00:55:05Z",
        "body": "Are you able to figure out why CORS is failing when using with GrpcJsonTranscoder?"
      }
    ]
  },
  {
    "number": 26391,
    "title": "For load balancing, what's the best way to handle upstream gRPC server shutdown?",
    "created_at": "2023-03-28T06:03:49Z",
    "closed_at": "2023-05-06T00:02:31Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26391",
    "body": "Here is my envoy config:\r\n\r\n```\r\n  clusters:\r\n    - name: cluster_0\r\n      connect_timeout: 30s\r\n      type: STATIC\r\n      lb_policy: round_robin\r\n      http2_protocol_options: {}\r\n      health_checks:\r\n        - timeout: 1s\r\n          interval: 5s\r\n          unhealthy_interval: 5s\r\n          no_traffic_interval: 5s\r\n          unhealthy_threshold: 1\r\n          healthy_threshold: 1\r\n          grpc_health_check: {}\r\n      common_lb_config:\r\n        healthy_panic_threshold: \r\n          value: 0\r\n      load_assignment:\r\n        cluster_name: cluster_0\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      (gRPC server A)\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      (gRPC server B)\r\n```\r\n\r\nWhile rolling restart the upstream gRPC servers, I want to make sure that downstream clients won't see any errors. What's the best way to achieve this?\r\n\r\nMy current solution is to implement the following logic in the gRPC server to handle shutdown signal:\r\n\r\n- Make the health check service return `UNHEALTHY`.\r\n- Sleep 10 seconds so that Envoy know about the `UNHEALTHY` status. (Skipping this may cause downstream client receiving gRPC errors.)\r\n- Shutdown the gRPC server gracefully for pending requests.\r\n\r\nThe solution works but it's quite ugly.\r\n\r\nAn alternative solution is to update the list of endpoints through an xDS protocol. But this seems complicated.\r\n\r\nAny other solutions?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26391/comments",
    "author": "zhouxiaobo1990",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2023-04-28T20:01:50Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-06T00:02:31Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 26303,
    "title": "Allow regex based descriptor values in Local Rate Limiter",
    "created_at": "2023-03-23T13:59:32Z",
    "closed_at": "2023-05-04T08:01:24Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26303",
    "body": "Hello all,\r\n\r\nI'm trying to setup specific rate limits if the incoming requests contain an Authorization header, but I'm having some difficulties getting that to work. It works well if I set a specific value like you see in the config, but we don't want that. \r\n\r\nCan I add in a regex in the descriptor value? I tried not providing a descriptor value but that didn't work either.\r\n\r\nHere is what I have: so far:\r\n\r\n    patch:\r\n      operation: MERGE\r\n      value:\r\n        route:\r\n          rate_limits:\r\n          - actions:\r\n            - request_headers:\r\n                descriptor_key: authorization\r\n                header_name: Authorization\r\n        typed_per_filter_config:\r\n          envoy.filters.http.local_ratelimit:\r\n            '@type': type.googleapis.com/udpa.type.v1.TypedStruct\r\n            type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit\r\n            value:\r\n              descriptors:\r\n              - entries:\r\n                - key: authorization\r\n                  value: test\r\n                token_bucket:\r\n                  fill_interval:\r\n                    seconds: 60\r\n                  max_tokens: 100\r\n                  tokens_per_fill:\r\n                    value: 100\r\n              filter_enabled:\r\n                default_value:\r\n                  numerator: 100\r\n                runtime_key: rate limit reached\r\n              filter_enforced:\r\n                default_value:\r\n                  numerator: 100\r\n                runtime_key: local_rate_limit_enforced\r\n                \r\nAny suggestions as to how to get this working?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26303/comments",
    "author": "91pavan",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2023-03-24T12:09:50Z",
        "body": "cc @kyessenov @zhxie "
      },
      {
        "user": "zhxie",
        "created_at": "2023-03-27T06:17:38Z",
        "body": "It might be ok to add a string matcher field in the rate limit descriptor entry to support prefix, suffix, regex and so on, which will request some code changes."
      },
      {
        "user": "therealaditigupta",
        "created_at": "2023-03-27T15:23:35Z",
        "body": "As a follow up question, is there any way to check if the Authorization header just *exists* in the local rate limit config rather than checking against any specific value?"
      },
      {
        "user": "zhxie",
        "created_at": "2023-03-28T01:50:51Z",
        "body": "I am afraid that we do not have any ability to do logical comparison against rate limit descriptors now."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-04-27T04:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-04T08:01:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 26199,
    "title": "Unable to do integration test in linux64 system due to  access denied to: /config_dump",
    "created_at": "2023-03-21T06:32:01Z",
    "closed_at": "2023-03-24T08:44:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26199",
    "body": "\r\n\r\n[Title] Unable to do integration test in linux64 system due to  access denied to: /config_dump\r\n\r\n[Description]\r\nHi envoy,\r\nI'm currently trying to write an integration test for my customize filter, While in my setup() function, when I called initialize, I always got Error message like:\r\n\r\n`[2023-03-21 06:15:10.537][21][debug][http] [source/common/http/conn_manager_impl.cc:939] [C1][S11182445455535709229] request headers complete (end_stream=true):\r\n':authority', 'host'\r\n':path', '/config_dump'\r\n':method', 'GET'\r\n\r\n[2023-03-21 06:15:10.537][21][debug][http] [source/common/http/conn_manager_impl.cc:923] [C1][S11182445455535709229] request end stream\r\n[2023-03-21 06:15:10.537][21][debug][connection] [./source/common/network/connection_impl.h:91] [C1] current connecting state: false\r\n[2023-03-21 06:15:10.537][21][debug][admin] [source/server/admin/admin_filter.cc:85] [C1][S11182445455535709229] request complete: path: /config_dump\r\n[2023-03-21 06:15:10.538][21][error][admin] [source/server/admin/admin.cc:359] admin interface, access denied to: /config_dump\r\n[2023-03-21 06:15:10.538][21][trace][http] [source/common/http/filter_manager.cc:1064] [C1][S11182445455535709229] encode headers called: filter= status=0\r\n[2023-03-21 06:15:10.538][21][debug][http] [source/common/http/conn_manager_impl.cc:1564] [C1][S11182445455535709229] encoding headers via codec (end_stream=false):\r\n':status', '405'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Tue, 21 Mar 2023 06:15:10 GMT'\r\n'server', 'envoy'`\r\n\r\nThe backtrace is:\r\n`test/integration/base_integration_test.cc:721: Failure\r\nExpected equality of these values:\r\n  \"200\"\r\n    Which is: 0x558ccf8a8141\r\n  response->headers().getStatusValue()\r\n    Which is: \"405\"\r\nStack trace:\r\n  0x558ccc2dfdbf: Envoy::BaseIntegrationTest::checkForMissingTagExtractionRules()\r\n  0x558ccc2d4fff: Envoy::BaseIntegrationTest::initialize()\r\n  0x558ccc26d241: Envoy::HttpIntegrationTest::initialize()\r\n  0x558ccc0ffeef: Envoy::Extensions::HttpFilters::JwtAuthn::(anonymous namespace)::LocalJwksIntegrationTest_WithGoodToken_Test::TestBody()\r\n  0x558ccf397fdd: testing::internal::HandleSehExceptionsInMethodIfSupported<>()\r\n  0x558ccf39314d: testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n  0x558ccf379d06: testing::Test::Run()\r\n  0x558ccf37a729: testing::TestInfo::Run()\r\n... Google Test internal frames ...\r\n\r\nMy system is ubuntu16.04 \r\nbazel version 6.1.0\r\n\r\nThe config used in prependFilter is\r\n\r\n`const std::string DefaultConfig = R\"EOF(\r\nname: envoy.filters.http.authn\r\ntyped_config:\r\n  \"@type\": type.googleapis.com/envoy.extensions.filters.http.authn.v3.Authn\r\n)EOF\";`\r\n\r\nCode:\r\n`    config_helper_.prependFilter(DefaultConfig);\r\n    initialize();`\r\n\r\nCan you give me some help on this? How can I avoid this problem and run my test.\r\n\r\nThanks,\r\nMartin",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26199/comments",
    "author": "iamjustadd",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2023-03-21T16:34:10Z",
        "body": "are you aiming your config_dump request at the main listener port or the admin port?  It may be helpful to look at the existing integration tests to see how this works."
      },
      {
        "user": "iamjustadd",
        "created_at": "2023-03-22T02:04:05Z",
        "body": "Hi wilk,\r\nI'm not aiming the config_dump request to anywhere.\r\nIt is called by Envoy::BaseIntegrationTest::initialize().\r\nAnd yes, I'm imitating to write these code like other integration test: just extend the HttpIntegrationTest, implement the setup function , which add a prepend filter and the call initialize which is point to HttpIntegrationTest::initialize();"
      },
      {
        "user": "iamjustadd",
        "created_at": "2023-03-24T08:44:18Z",
        "body": "Close this issue, it's an user error."
      }
    ]
  },
  {
    "number": 26172,
    "title": "Cannot check peer: missing selected ALPN property.",
    "created_at": "2023-03-20T10:36:33Z",
    "closed_at": "2023-04-27T16:01:30Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26172",
    "body": "I'm trying to use the Envoy as the gateway of my backend servers.\r\n\r\nI've a grpc streaming server so I need to configure the Envoy with grpc.\r\n\r\nHere is the proto:\r\n```\r\nsyntax = \"proto3\";\r\n\r\noption java_multiple_files = true;\r\noption java_package = \"io.grpc.examples.helloworld\";\r\noption java_outer_classname = \"HelloWorldProto\";\r\noption objc_class_prefix = \"HLW\";\r\n\r\npackage helloworld;\r\n\r\nservice Greeter {\r\n  rpc SayHelloStream(stream MessageReq) returns (stream MessageRes) {}\r\n}\r\n\r\nmessage MessageReq {\r\n  string text = 1;\r\n}\r\n\r\nmessage MessageRes {\r\n  int32 count = 1;\r\n}\r\n```\r\n\r\nHere is my server:\r\n```\r\n#include <iostream>\r\n#include <memory>\r\n#include <string>\r\n#include <thread>\r\n#include <grpcpp/grpcpp.h>\r\n#include \"helloworld.grpc.pb.h\"\r\n\r\nusing grpc::Server;\r\nusing grpc::ServerBuilder;\r\nusing grpc::ServerContext;\r\nusing grpc::Status;\r\nusing helloworld::Greeter;\r\nusing helloworld::MessageReq;\r\nusing helloworld::MessageRes;\r\n\r\nclass BiStreamServiceImpl final : public Greeter::Service {\r\n public:\r\n  Status SayHelloStream(ServerContext* context, grpc::ServerReaderWriter<MessageRes, MessageReq>* stream) override {\r\n    MessageReq request;\r\n    while (stream->Read(&request)) {\r\n      std::cout << \"received\" << std::endl;\r\n      MessageRes response;\r\n      response.set_count(request.text().length());\r\n      stream->Write(response);\r\n    }\r\n    return Status::OK;\r\n  }\r\n};\r\n\r\nvoid RunServer() {\r\n  std::string server_address(\"0.0.0.0:50051\");\r\n  BiStreamServiceImpl service;\r\n\r\n  ServerBuilder builder;\r\n  builder.AddListeningPort(server_address, grpc::InsecureServerCredentials());\r\n  builder.RegisterService(&service);\r\n\r\n  std::unique_ptr<Server> server(builder.BuildAndStart());\r\n  std::cout << \"Server listening on \" << server_address << std::endl;\r\n  server->Wait();\r\n}\r\n\r\nint main(int argc, char** argv) {\r\n  std::thread server_thread(RunServer);\r\n  server_thread.join();\r\n  return 0;\r\n}\r\n```\r\nHere is my client:\r\n```\r\n#include <iostream>\r\n#include <thread>\r\n#include <memory>\r\n#include <string>\r\n#include <grpcpp/grpcpp.h>\r\n#include \"helloworld.grpc.pb.h\"\r\n\r\nusing grpc::Channel;\r\nusing grpc::ClientContext;\r\nusing grpc::ClientReaderWriter;\r\nusing grpc::Status;\r\nusing helloworld::Greeter;\r\nusing helloworld::MessageReq;\r\nusing helloworld::MessageRes;\r\n\r\n\r\nclass MyClient {\r\n public:\r\n  MyClient(std::shared_ptr<Channel> channel) : stub_(Greeter::NewStub(channel)) { std::cout << \"init channel\" << std::endl; }\r\n\r\n  void doJob() {\r\n    MessageRes response;\r\n    ClientContext context;\r\n    std::shared_ptr<ClientReaderWriter<MessageReq, MessageRes>> stream(stub_->SayHelloStream(&context));\r\n    std::thread writer([&stream]() {\r\n      while (true) {\r\n        std::cout << \"write msg\" << std::endl;\r\n        MessageReq request;\r\n        request.set_text(\"abc\");\r\n        stream->Write(request);\r\n        std::this_thread::sleep_for(std::chrono::seconds(5));\r\n      }\r\n    });\r\n\r\n    while (stream->Read(&response)) {\r\n      std::cout << \"Server responded with count: \" << response.count() << std::endl;\r\n    }\r\n\r\n    Status status = stream->Finish();\r\n    if (!status.ok()) {\r\n       std::cerr << status.error_code() << \". StreamData rpc failed: \" << status.error_message() << std::endl;\r\n    }\r\n    writer.join();\r\n  }\r\n\r\n private:\r\n  std::unique_ptr<Greeter::Stub> stub_;\r\n};\r\n\r\nint main(int argc, char** argv) {\r\n  const char *addr = nullptr;\r\n  if (argc == 2) {\r\n    addr = argv[1];\r\n  } else {\r\n    addr = \"example.com:1444\";\r\n  }\r\n//  MyClient client(grpc::CreateChannel(addr, grpc::InsecureChannelCredentials()));\r\n  MyClient client(grpc::CreateChannel(addr, grpc::SslCredentials(grpc::SslCredentialsOptions())));\r\n\r\n  client.doJob();\r\n  return 0;\r\n}\r\n```\r\n\r\nHere is the config of my Envoy:\r\n```\r\n  - name: listener_official_common_http\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 1444 }\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress_https\r\n          common_http_protocol_options:\r\n            idle_timeout: 3600s\r\n          request_timeout: 200s\r\n          http_filters:\r\n          - name: envoy.filters.http.compressor\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.compressor.v3.Compressor\r\n              compressor_library:\r\n                name: text_optimized\r\n                typed_config:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.compression.gzip.compressor.v3.Gzip\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"example.com:*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                  grpc: {}\r\n                route:\r\n                  cluster: cluster_robot_grpc_testing\r\n                  timeout: 0s\r\n                  idle_timeout: 0s\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n          common_tls_context: {tls_certificates: [{certificate_chain: {filename: \"/test.crt\"}, private_key: {filename: \"/test.key\"}}]}\r\n      transport_socket_connect_timeout: 30s\r\n...\r\n  - name: cluster_robot_grpc_testing\r\n    connect_timeout: 30s\r\n    type: LOGICAL_DNS\r\n    typed_extension_protocol_options:\r\n      envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n        \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n        upstream_http_protocol_options:\r\n          auto_sni: true\r\n        common_http_protocol_options:\r\n          idle_timeout: 3600s\r\n        explicit_http_config:\r\n          http2_protocol_options: {}\r\n    load_assignment:\r\n      cluster_name: cluster_robot_grpc_testing\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address: {address: 172.16.0.3, port_value: 50051}\r\n```\r\n\r\nIf I comment the TLS config (`transport_socket`) and create an insecure channel `MyClient client(grpc::CreateChannel(addr, grpc::InsecureChannelCredentials()));`, it would work without any error. Then I enable `transport_socket` and try to create a secure channel `MyClient client(grpc::CreateChannel(addr, grpc::SslCredentials(grpc::SslCredentialsOptions())));` between client and Envoy. When I execute the client, I always get the error:\r\n```\r\n14. StreamData rpc failed: failed to connect to all addresses; last error: UNKNOWN: ipv4:172.16.0.3:1444: Cannot check peer: missing selected ALPN property.\r\n```\r\n\r\nThe client, the server and the Envoy are all deployed on CentOS7 and the versions of `openssl` are `OpenSSL 1.1.1k  25 Mar 2021`.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26172/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2023-03-20T13:11:04Z",
        "body": "cc @ggreenway @RyanTheOptimist "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-04-20T12:01:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-04-27T16:01:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 26006,
    "title": "how to pass paths from api to routes envoy",
    "created_at": "2023-03-09T07:31:33Z",
    "closed_at": "2023-04-16T00:02:58Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26006",
    "body": "Hi, I am automating the envoy upload, but I am encountering a problem, because from a .yaml API specification, I can't automate the routes, \r\nfor example in the yaml, if I have a path that is \r\n/hello\r\n/hello/world2\r\n/bye\r\n/bye/{id}/bye\r\n\r\nI am not finding a plugin or any way to make it autogenerate an envoy. \r\nIs the only way is to upload it manually?\r\n\r\nthanks!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26006/comments",
    "author": "Naski86",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2023-04-08T20:01:41Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-04-16T00:02:56Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 25938,
    "title": "Safe Regex not working for External Authorization Filter..",
    "created_at": "2023-03-06T17:39:16Z",
    "closed_at": "2023-03-07T05:35:45Z",
    "labels": [
      "question",
      "area/matching"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25938",
    "body": "I do not want to apply external authorization filter for routes starting with /css, /img, /assets. While it is working fine if I put 3 entries using prefix but its not working with safe_regex.\r\n```\r\nstatic_resources:\r\n\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 10000\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          access_log:\r\n          - name: envoy.access_loggers.stdout\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              typed_per_filter_config:\r\n                envoy.filters.http.ext_authz:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthzPerRoute\r\n                  check_settings:\r\n                    context_extensions:\r\n                      virtual_host: local_service\r\n              routes:\r\n              - match:\r\n                  safe_regex:\r\n                    google_re2: {}\r\n                    regex: \"^/(css|img|assets)/\"\r\n                route:\r\n                  host_rewrite_literal: www.envoyproxy.io\r\n                  cluster: service_envoyproxy_io\r\n                typed_per_filter_config:\r\n                  envoy.filters.http.ext_authz:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthzPerRoute\r\n                    disabled: true\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  host_rewrite_literal: www.envoyproxy.io\r\n                  cluster: service_envoyproxy_io    \r\n          http_filters:\r\n          - name: envoy.filters.http.ext_authz\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz\r\n              grpc_service:\r\n                envoy_grpc:\r\n                  cluster_name: ext_authz-grpc-service\r\n                timeout: 0.250s\r\n              transport_api_version: V3\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n\r\n  clusters:\r\n  - name: service_envoyproxy_io\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    load_assignment:\r\n      cluster_name: service_envoyproxy_io\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.envoyproxy.io\r\n                port_value: 443\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n        sni: www.envoyproxy.io\r\n\r\n  - name: ext_authz-grpc-service\r\n    type: STRICT_DNS\r\n    lb_policy: ROUND_ROBIN\r\n    typed_extension_protocol_options:\r\n      envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n        \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n        explicit_http_config:\r\n          http2_protocol_options: {}\r\n    load_assignment:\r\n      cluster_name: ext_authz-grpc-service\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 0.0.0.0\r\n                port_value: 7058\r\n```    ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25938/comments",
    "author": "rakesh-eltropy",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-03-07T03:09:57Z",
        "body": "cc @rakesh-eltropy full match is used by the safe_regex matching here. So, may be `\"^/(css|img|assets)/.*\"` should be used here?"
      },
      {
        "user": "rakesh-eltropy",
        "created_at": "2023-03-07T05:35:45Z",
        "body": "Thanks @wbpcode. I was not aware that full match is used by safe_regex."
      }
    ]
  },
  {
    "number": 25920,
    "title": "Aggregate Cluster: do cluster members share healthcheck connectionpool and data_plane connectionpool?",
    "created_at": "2023-03-04T02:37:15Z",
    "closed_at": "2023-03-06T16:54:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25920",
    "body": "\r\n*Title*: *Aggregate Cluster: do cluster members share healthcheck connectionpool and data_plane connectionpool?*\r\n\r\n*Description*:\r\nHi folks, having a question about `Aggregate Cluster`:  it contains 2 clusters: cluster_1 and cluster_2, and we hope to migrate traffic from cluster_1 to cluster_2 (we can't directly use xDS to do the migration). They both have active healthcheck. These 2 clusters will point to the same endpoints, but the 2 clusters have different properties.  Our production machine is kind of at capacity.  Is it possible that these 2 clusters share the same healthcheck connection_pool? or even data_plane connection_pool ?  Thanks!! \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25920/comments",
    "author": "xianggao001",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2023-03-06T15:34:46Z",
        "body": "IIRC for aggregate cluster the 2 underlying clusters are independent so they will have independent health checking and connection pools."
      },
      {
        "user": "xianggao001",
        "created_at": "2023-03-06T16:04:16Z",
        "body": "@mattklein123 Thank you Matt! A related question: is there a way to share health checking and connection pools between 2 clusters?  I searched around in Issues and Community Slack and it seems the answer is NO. Thanks! "
      },
      {
        "user": "mattklein123",
        "created_at": "2023-03-06T16:41:58Z",
        "body": "No, not currently AFAIK."
      }
    ]
  },
  {
    "number": 25784,
    "title": "Develop debugging envoy using vscode Container",
    "created_at": "2023-02-26T04:06:44Z",
    "closed_at": "2023-03-02T03:56:06Z",
    "labels": [
      "question",
      "help wanted",
      "area/dev"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25784",
    "body": "The generate_debug_config.py script is stuck and cannot proceed\r\n\r\n\r\n\r\nvscode@ddy-desktop:/workspaces/envoy$ tools/vscode/generate_debug_config.py //source/exe:envoy-static --args \"-c  envoyproxy_io_proxy.yaml\"  --debugger lldb\r\nStarting local Bazel server and connecting to it...\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25784/comments",
    "author": "Webster-Yang",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2023-03-02T03:53:51Z",
        "body": "@lizan might know more"
      }
    ]
  },
  {
    "number": 25774,
    "title": "https_passthrough with jwt_authn in the middle",
    "created_at": "2023-02-24T22:36:35Z",
    "closed_at": "2023-04-08T04:01:39Z",
    "labels": [
      "question",
      "stale",
      "area/jwt_authn"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25774",
    "body": "I'm trying to set up a proxy with a jwt auth in the middle of a client/server communicating using https.\r\n\r\nSo i tried something like : \r\n\r\n```\r\nstatic_resources:\r\n\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 9901\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.tcp_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n          cluster: qm\r\n          stat_prefix: https_passthrough\r\n\r\n          http_filters:\r\n          - name: envoy.filters.http.jwt_authn\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.jwt_authn.v3.JwtAuthentication\r\n              providers:\r\n                jwt_auth:\r\n                  local_jwks:\r\n                    inline_string: '{\"keys\":[{\"kty\":\"oct\",\"alg\":\"HS256\",\"k\":\"cGFydGVj\"}]}'\r\n                  from_headers:\r\n                  - name: Authorization\r\n                    value_prefix: \"Bearer \"\r\n              rules:\r\n              - match:\r\n                  prefix:  \"/\"\r\n                requires:\r\n                  provider_name:  jwt_auth    \r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  host_rewrite_literal: xxxxx.com\r\n                  cluster: qm-cluster\r\n\r\n\r\n  clusters:\r\n  - name: qm\r\n    type: STRICT_DNS\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: qm\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: xxxxx.com\r\n                port_value: 443\r\n```\r\n \r\nBut i get this error : Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:http_filters: Cannot find field.) has unknown fields\r\n\r\nI manage to get jwt_auth work with http so i feel like the config for jwt_auth and router are OK.\r\n\r\nI don't really know what's wrong with this config so i get to ask myself : \r\n**Is this even possible to combine https_passthrough and jwt_auth ?**",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25774/comments",
    "author": "rbahegne",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2023-03-02T03:45:07Z",
        "body": "The TCP proxy network filter does not support the use of http filters (therefore is has no http_filters field, which causes the error you're getting). The jwt_authn http filter only works with the HttpConnectionManager. Using the HTTP connection manager will require setting up TLS since it requires access to unencrypted HTTP requests."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-04-01T04:01:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-04-08T04:01:38Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 25754,
    "title": "Websocket client disconnects when related upstream is down",
    "created_at": "2023-02-23T21:21:37Z",
    "closed_at": "2023-04-05T16:01:30Z",
    "labels": [
      "question",
      "stale",
      "area/websockets",
      "area/failover"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25754",
    "body": "*Description*:\r\nWe run kubernetes websocket server pods.\r\n \r\nBetween clients and the services we want envoy proxy to take care of connections lifecycle.\r\n \r\nMore precisely, the idea is to \"preserve\" a websocket connection in such way that the connection is taken over by another healthy upstream (once original upstream is gone).\r\n \r\nWe are not in control of the clients.\r\n \r\nSo far, we tried different configuration options in order to reroute/fail over/keep alive/prevent disconnecting websocket connection, but we keep failing.\r\n \r\nOnce pod is `Terminating`/`Terminated` the downstream connection got broken too - the client gets disconnected.\r\n \r\nLet me share what we tried to do:\r\n - First, we do gracefully close websocket connections on SIGTERM signal\r\n - With Lua script, we cannot intercept any request/response since the connection is upgraded from the start and the communication is binary from that point on\r\n - Health checks, circuit breaker and weighted clusters don't help mitigate this issue\r\n \r\nWe strongly believe this preservation of websocket connection is possible to manage with envoy.\r\nCould you please give us a hand how best we can approach this problem.\r\n\r\nHere is the current`envoy.yaml` example:\r\n```yaml\r\nadmin:\r\n  access_log_path: /dev/stdout\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 14000\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 7070\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: filter_wsprogram\r\n          codec_type: auto\r\n          upgrade_configs:\r\n          - upgrade_type: websocket\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: app\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  idle_timeout: 0s\r\n                  priority: HIGH\r\n                  weighted_clusters:\r\n                    clusters:\r\n                      - name: wsprogram_cluster\r\n                        weight: 90\r\n                      - name: wsprogram_backup_cluster\r\n                        weight: 10\r\n                    total_weight: 100\r\n          http_filters:\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n  clusters:\r\n  - name: wsprogram_cluster\r\n    type: STRICT_DNS\r\n    lb_policy: ROUND_ROBIN\r\n    dns_lookup_family: V4_ONLY\r\n    #connect_timeout: 1s\r\n    dns_refresh_rate: 0.01s\r\n    load_assignment:\r\n      cluster_name: wsprogram_cluster\r\n      endpoints:\r\n        - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: wsprogram-headless-zero\r\n                    port_value: 80\r\n    circuit_breakers:\r\n      thresholds:\r\n        - priority: DEFAULT\r\n          max_connections: 1000\r\n          max_pending_requests: 1000\r\n          max_requests: 10000\r\n          max_retries: 3\r\n          track_remaining: true\r\n    \r\n    health_checks:\r\n      - timeout: 1s\r\n        interval: 1s\r\n        unhealthy_threshold: 1\r\n        healthy_threshold: 2\r\n        tcp_health_check:\r\n          send:\r\n            text: \"00\"\r\n          receive:\r\n            text: \"00\"\r\n  - name: wsprogram_backup_cluster\r\n    type: STRICT_DNS\r\n    lb_policy: ROUND_ROBIN\r\n    dns_lookup_family: V4_ONLY\r\n    #connect_timeout: 1s\r\n    dns_refresh_rate: 0.01s\r\n    load_assignment:\r\n      cluster_name: wsprogram_backup_cluster\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: wsprogram-headless-one\r\n                port_value: 80\r\n    circuit_breakers:\r\n      thresholds:\r\n        - priority: DEFAULT\r\n          max_connections: 1000\r\n          max_pending_requests: 1000\r\n          max_requests: 10000\r\n          max_retries: 3\r\n          track_remaining: true\r\n\r\n    health_checks:\r\n    - timeout: 1s\r\n      interval: 1s\r\n      unhealthy_threshold: 1\r\n      healthy_threshold: 2\r\n      tcp_health_check:\r\n        send:\r\n          text: \"00\"\r\n        receive:\r\n          text: \"00\"\r\n```\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25754/comments",
    "author": "zargor",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2023-02-24T10:25:20Z",
        "body": "@kyessenov any ideas about this ?"
      },
      {
        "user": "jajaislanina",
        "created_at": "2023-02-27T09:19:36Z",
        "body": "Following as we have similar requirements and constraints."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-03-29T12:01:52Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-04-05T16:01:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 25737,
    "title": "Resolve diskspace issues with ubuntu images used in the azure pipelines",
    "created_at": "2023-02-23T10:04:25Z",
    "closed_at": "2023-04-05T09:02:20Z",
    "labels": [
      "bug",
      "enhancement",
      "question",
      "stale",
      "investigate",
      "area/ci",
      "area/release"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25737",
    "body": "We have an issue that keeps on cycling where CI runs out of diskspace at various points\r\n\r\nMostly this is due to the currently provisioned ubuntu image taking up huge amounts of space with dotnet and lots of other arbitrary build environments we would never use, and if we did we would use specific versions anyway\r\n\r\nAs the branches work a little differently in presubmit/postsubmit/releases this can break things unexpectedly - its an ongoing issue that just keeps coming back\r\n\r\nThe main hack we have used to resolve is uninstalling and in some cases just `rm -rf` large parts of the filesystem - but that takes quite a long time so is really undesirable if its not necessary\r\n\r\n**If** it was possible to customize the base image and just use something with a very minimal set of requirements for our CI this problem would just go away and save everyone a lot of time and stress",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25737/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2023-02-23T10:04:35Z",
        "body": "cc @lizan @mattklein123 "
      },
      {
        "user": "phlax",
        "created_at": "2023-02-27T09:12:20Z",
        "body": "apart from all the useless bloat in the image, the stuff it has installed is installed badly\r\n\r\nfor example, it has a version of skopeo that is not in any repo anywhere - including microsoft's custom debs repo - they have just compiled it and installed it randomly and you have no way of knowing where it came from or how it got there"
      },
      {
        "user": "phlax",
        "created_at": "2023-03-06T07:46:35Z",
        "body": "looking at this further it seems like the problem we have is that we are using the ms hosted agent for some jobs - so in that case we definitely cannot customize - and fwiw ms only guarantees 10G of disk space\r\n\r\nim still not entirely clear if we can customize our self-hosted agents - it seems like we install all of the ms bloat, but maybe that is required in some way.\r\n\r\ni think it should be possible to mark our self-hosted agents as having certain capabilities to force them to be used in some circumstances - so if we can customize that, it seems like we could do something there\r\n\r\neither way - for now - i have raised a Pr to switch the x64/release job to use the `x64-large` ami/pool as this just resolves. Similar can be done for the other jobs that have disk pressure."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-04-05T08:01:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "phlax",
        "created_at": "2023-04-05T09:02:20Z",
        "body": "this is being addressed"
      }
    ]
  },
  {
    "number": 25729,
    "title": "Issue with read error: Resource temporarily unavailable",
    "created_at": "2023-02-23T01:20:29Z",
    "closed_at": "2023-02-25T14:37:00Z",
    "labels": [
      "question",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25729",
    "body": "*Title*: Envoy unable to connect to application\r\n\r\n*Description*:\r\nI am using EKS `1.24` and Istio `1.14.3`. However I am getting error 503 in HTTP calls. It seems like Envoy container cannot connect to my application. I can connect to the application using port forwarding.\r\n\r\nBelow is the trace log in Envoy container\r\n```\r\n2023-02-23T01:12:15.853235Z\ttrace\tenvoy upstream\tStale original dst hosts cleanup triggered.\r\n2023-02-23T01:12:17.849896Z\ttrace\tenvoy misc\tenableTimer called on 0x56380bd5c280 for 3600000ms, min is 3600000ms\r\n2023-02-23T01:12:17.849936Z\tdebug\tenvoy conn_handler\t[C110] new connection from 10.22.8.184:57296\r\n2023-02-23T01:12:17.849953Z\ttrace\tenvoy connection\t[C110] socket event: 3\r\n2023-02-23T01:12:17.849956Z\ttrace\tenvoy connection\t[C110] write ready\r\n2023-02-23T01:12:17.849959Z\ttrace\tenvoy connection\t[C110] read ready. dispatch_buffered_data=0\r\n2023-02-23T01:12:17.849971Z\ttrace\tenvoy connection\t[C110] read returns: 118\r\n2023-02-23T01:12:17.849982Z\ttrace\tenvoy connection\t[C110] read error: Resource temporarily unavailable\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25729/comments",
    "author": "khtee",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2023-02-23T12:11:37Z",
        "body": "@KHTee this seems at first glance like more of an issue with your k8s/CNI setup - i would try to ensure that you can connect between the containers as you expect to\r\n\r\nif you can connect as expected between them i would try to debug your app to see what is happening there"
      },
      {
        "user": "khtee",
        "created_at": "2023-02-25T14:37:00Z",
        "body": "The root cause is the communication between nodes are blocked by security group. Closing this thread. "
      }
    ]
  },
  {
    "number": 25501,
    "title": "When a request being retried, does it counted as active requests as well?",
    "created_at": "2023-02-12T15:28:27Z",
    "closed_at": "2023-02-20T06:58:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25501",
    "body": "*Title*: *Understanding active requests and active retries*\r\n\r\n*Description*:\r\n>I checked we defined both requests(active_requests) and retries(active_retries) in ResourceManager class separately, when we're retrying a request, are we incrementing the active_requests or not? I couldn't find a reference in the code as to how we're calling the callback when we're retrying a request and how it's relating to the connection pool.\r\nas I was looking at implementing retry budget I was confused about these two terminologies. Please help me understand these two gauges. Thanks.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25501/comments",
    "author": "rafatbiin",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-02-14T04:09:43Z",
        "body": "I did a quick check to the source. Seems both will be incremented.\r\n\r\nThe retry state in the router will handled the `retries` and the conn pool will handle the `requests` (just like there is a normal request)."
      }
    ]
  },
  {
    "number": 25455,
    "title": "How to know whether you are facing an internal error vs destination issue",
    "created_at": "2023-02-09T14:09:24Z",
    "closed_at": "2023-03-19T16:01:31Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25455",
    "body": "Hello envoy team!\r\n\r\nI'm playing around with a more complex envoy configuration.\r\nI'm running a filter chain consisting of a WASM filter, an external processor filter changing the body of the request and an external auth filter which adds some headers.\r\nAfter these filters, my request is routed to a final (external) destination.\r\n\r\nI'm trying to figure out if, as a client, I can tell in case of a failure if there was an internal issue in my envoy (any of the filters failed) or if there were problems at the destination (service unavailable / connection issues).\r\n\r\nIdeally I'd have a header set on the response which will let me know that my filters worked fine, is there anything I can use to achieve this?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25455/comments",
    "author": "dragosc28",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-02-10T02:02:48Z",
        "body": "There is no a explicit flag to tell you everything. But response flag and response code detaill would always be helpful to determine what is wrong.\r\n\r\nA global `response_headers_to_add` could help to set these flag or detail to your response header."
      },
      {
        "user": "dragosc28",
        "created_at": "2023-02-10T09:30:31Z",
        "body": "@wbpcode thanks for your answer, it's very helpful.\r\n\r\nI think setting specific response flags, like UH/UF, to my response headers would work.\r\nCan you provide a small example on how this can be achieved?"
      },
      {
        "user": "dragosc28",
        "created_at": "2023-02-10T09:42:58Z",
        "body": "Think I got it right\r\n\r\n```         route_config:\r\n            name: local_route\r\n            response_headers_to_add:\r\n              - header:\r\n                  key: \"response-flags\"\r\n                  value: \"%RESPONSE_FLAGS%\""
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-03-12T12:01:56Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-03-19T16:01:30Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 25450,
    "title": "Currently Envoy Support HTTP2 RFC 9113 or not",
    "created_at": "2023-02-09T04:06:24Z",
    "closed_at": "2023-03-18T12:01:38Z",
    "labels": [
      "question",
      "stale",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25450",
    "body": "Currently Envoy Support HTTP2 RFC 7450, it is mentioned on envoy site. When envoy could be able to support HTTP2 RFC 9113 or it is already supported?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25450/comments",
    "author": "Gaurav0411GitHub",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2023-03-11T12:01:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-03-18T12:01:37Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 25400,
    "title": "Develop debugging envoy using vscode Container",
    "created_at": "2023-02-07T10:33:15Z",
    "closed_at": "2023-02-09T02:25:37Z",
    "labels": [
      "question",
      "area/dev"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25400",
    "body": "windows 10\r\nwsl2\r\nvscode\r\n\r\n\r\nvscode@docker-desktop:/workspaces/envoy$ cat  tools/vscode/refresh_compdb.sh\r\n#!/usr/bin/env bash\r\n\r\n[[ -z \"${SKIP_PROTO_FORMAT}\" ]] && tools/proto_format/proto_format.sh fix\r\n\r\nbazel_or_isk=bazelisk\r\ncommand -v bazelisk &> /dev/null || bazel_or_isk=bazel\r\n\r\n[[ -z \"${EXCLUDE_CONTRIB}\" ]] || opts=\"--exclude_contrib\"\r\n\r\n# Setting TEST_TMPDIR here so the compdb headers won't be overwritten by another bazel run\r\nTEST_TMPDIR=${BUILD_DIR:-/tmp}/envoy-compdb tools/gen_compilation_database.py --vscode --bazel=$bazel_or_isk ${opts}\r\n\r\n# Kill clangd to reload the compilation database\r\npkill clangd || :\r\n\r\n\r\n\r\nerror:\r\nvscode@docker-desktop:/workspaces/envoy$ ./tools/vscode/refresh_compdb.sh\r\n/usr/bin/env: ‘bash\\r’: No such file or directory",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25400/comments",
    "author": "Webster-Yang",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-02-08T13:57:14Z",
        "body": "Is there a `bash` command in your container?"
      },
      {
        "user": "Webster-Yang",
        "created_at": "2023-02-09T02:25:52Z",
        "body": "dos2unix"
      }
    ]
  },
  {
    "number": 25358,
    "title": "How does the fallback policy work with healch_checks in Envoy",
    "created_at": "2023-02-04T13:46:09Z",
    "closed_at": "2023-02-08T11:00:58Z",
    "labels": [
      "question",
      "area/load balancing"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25358",
    "body": "I'm using Envoy as the gateway of my backend servers.\r\n\r\nI just read the docs of Envoy and I knew that we could control the route rules with the help of `header_to_metadata`. I also knew that Envoy provided a mechanism of `health_checks`, which allows us to check if an endpoint is healthy or not. But when I put the two stuffs together, I'm confused...\r\n\r\nI knew that if a request failed on matching the metadata, it would be routed to the endpoint by default only if I configured a fallback endpoint, but if the metadata has been matched to some endpoint which it is being unhealthy, what whould happen? The fallback policy would be applied or an error would be generated immediately?\r\n\r\nHere is an example:\r\n\r\n    - name: envoy.filters.http.header_to_metadata\r\n                typed_config:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.filters.http.header_to_metadata.v3.Config\r\n                  request_rules:\r\n                  - header: x-canary\r\n                    on_header_present:\r\n                      metadata_namespace: envoy.lb\r\n                      key: canary\r\n                      type: STRING\r\n                    on_header_missing:\r\n                      metadata_namespace: envoy.lb\r\n                      key: canary\r\n                      value: \"0\"\r\n                      type: STRING\r\n                    remove: false\r\n      clusters:\r\n      - name:\r\n        connect_timeout:\r\n        type: STATIC\r\n        lb_subset_config:\r\n          fallback_policy: DEFAULT_SUBSET\r\n          default_subset:\r\n            canary: \"0\"\r\n          subset_selectors:\r\n          - keys:\r\n            - canary\r\n        load_assignment:\r\n          cluster_name:\r\n          endpoints:\r\n          - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address: <canary_address>\r\n              metadata:\r\n                filter_metadata:\r\n                  envoy.lb:\r\n                    canary: \"0\"\r\n            - endpoint:\r\n                address:\r\n                  socket_address: <prod_address>\r\n              metadata:\r\n                filter_metadata:\r\n                  envoy.lb:\r\n                    canary: \"1\"\r\n\r\nThe config above allows us to control the route rules by `x-canary` in the headers of requests.\r\n\r\nNow, I want to configure the `healch_checks`, which can decide if an endpoint is healthy or not. If an endpoint is unhealthy, Envoy would route requests to other endpoints. But what if the unhealthy endpoint is just the matched endpoint?\r\n\r\nLet's say a request should be routed to the endpoint \"prod address\" because of its `x-canary:1`, but at the same time, the endpoint \"prod address\" is unhealthy. In this case, the Envoy would reply an error to the client or the Envoy would route the request to the endpoint \"canary address\" regardless of the metadata?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25358/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "botengyao",
        "created_at": "2023-02-04T19:37:38Z",
        "body": "I think if there is no healthy host in the metadata matched subset, it will fall back to the default subset if there is a DEFAULT_SUBSET fallback_policy."
      }
    ]
  },
  {
    "number": 25221,
    "title": "Okta Oauth flow failed",
    "created_at": "2023-01-27T23:07:21Z",
    "closed_at": "2023-02-02T18:27:31Z",
    "labels": [
      "question",
      "area/oauth"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25221",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\nI'm trying to implement Oauth using Okta and got the following error: \"OAuth flow failed.\" (401). I checked the token endpoint and it's the good one. I think the error comes from the token_secret. However, it's not clear what to use for that with okta because right no I'm using the \"client secret\" provided by okta but it doesn't work.\r\n\r\nAnyone manage to make envoy works with okta that way?\r\n\r\nThank you\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25221/comments",
    "author": "Sindvero",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2023-01-30T20:35:57Z",
        "body": "cc @derekargueta @snowp "
      },
      {
        "user": "Sindvero",
        "created_at": "2023-02-02T18:27:31Z",
        "body": "I was able to fix it by switching to https.\r\n\r\nThanks"
      }
    ]
  },
  {
    "number": 25095,
    "title": "API to edit internal data structure",
    "created_at": "2023-01-23T00:22:42Z",
    "closed_at": "2023-03-08T16:01:37Z",
    "labels": [
      "question",
      "stale",
      "area/quic"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25095",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\nTitle: API to read/write internal state of Envoy Proxy and filters\r\nDescription:\r\nI'm currently doing my master thesis and I'm searching for some way to edit the state of Envoy Proxy and filters. \r\nThe idea is to migrate a QUIC connection from one Envoy proxy to a new one, in order to simulate a client that moves in an edge computing environment. Regardless QUIC related issues (implement server migration), is there any way to edit internal data structure of envoy related to a QUIC connection? Some API?\r\nThank you for your help!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25095/comments",
    "author": "lorenzogio97",
    "comments": [
      {
        "user": "RyanTheOptimist",
        "created_at": "2023-01-24T19:51:21Z",
        "body": "I don't believe there is any public API for accessing the QUIC connection internals. You could hack around the code in source/common/quic, or else look into the QUICHE code, but that's about it."
      },
      {
        "user": "lorenzogio97",
        "created_at": "2023-01-30T08:36:38Z",
        "body": "Thank you for the answer. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-03-01T12:01:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-03-08T16:01:36Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 24727,
    "title": "many ports in one click",
    "created_at": "2023-01-02T23:09:22Z",
    "closed_at": "2023-02-09T20:01:42Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24727",
    "body": "how to forward more ports?\r\n25565-25665 - without entering thousands of lines of code?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24727/comments",
    "author": "lootoos",
    "comments": [
      {
        "user": "soulxu",
        "created_at": "2023-01-03T06:38:32Z",
        "body": "probably you only can do that by using iptables to redirect those ports' connection to the same listening address which the envoy listener is listening  "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-02-02T16:01:37Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-02-09T20:01:41Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 24715,
    "title": "Envoy proxy random load balancing option is not random",
    "created_at": "2022-12-30T23:04:31Z",
    "closed_at": "2023-02-10T04:01:27Z",
    "labels": [
      "bug",
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24715",
    "body": "Latest version - Windows Envoy 1.24.1\r\n\r\nWe are using the below Envoy load balancing YAML file. It is working but it is NOT randomly distributing requests across nodes 1, 2, and 3. In our use case, we have high load coming from a single source IP address (customer server). We want to distribute the traffic across the nodes like so:\r\n\r\nRequest 1 -> Node 1 Request 2 -> Node 2 Request 3 -> Node 3 Request 4 -> Node 1\r\n\r\nHowever, this is what is happening:\r\n\r\nRequest 1 -> Node 1 Request 2 -> Node 1 Request 3 -> Node 1 Request 4 -> Node 1\r\n\r\nAfter a LONG time (minutes), it will eventually start sending some traffic to Node 2, but then only to node 2. Again, note that all requests are coming from one client IP.\r\n\r\nWe are using Random below but also round robin does the same thing. So our question is: how to fix this? This feels like a huge bug as it is basically not actually load balancing. Any guidance would be appreciated. We are using the latest version of Envoy.\r\n\r\n```\r\nstatic_resources:\r\n    listeners:\r\n      - name: listener_http\r\n        address:\r\n          socket_address:\r\n            address: 0.0.0.0\r\n            port_value: 80\r\n        filter_chains:\r\n          - filters:\r\n              - name: envoy.filters.network.tcp_proxy\r\n                typed_config:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n                  stat_prefix: destination\r\n                  cluster: http_cluster\r\n      - name: listener_https\r\n        address:\r\n          socket_address:\r\n            address: 0.0.0.0\r\n            port_value: 443\r\n        filter_chains:\r\n          - filters:\r\n              - name: envoy.filters.network.tcp_proxy\r\n                typed_config:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n                  stat_prefix: destination2\r\n                  cluster: https_cluster\r\n    clusters:\r\n      - name: http_cluster\r\n        connect_timeout: 30s\r\n        type: strict_dns\r\n        dns_lookup_family: V4_ONLY\r\n        lb_policy: random\r\n        load_assignment:\r\n          cluster_name: http_cluster\r\n          endpoints:\r\n            - lb_endpoints:\r\n                - endpoint:\r\n                    address:\r\n                      socket_address:\r\n                        address: node1.mydomain.com\r\n                        port_value: 80\r\n                - endpoint:\r\n                    address:\r\n                      socket_address:\r\n                        address: node2.mydomain.com\r\n                        port_value: 80\r\n                - endpoint:\r\n                    address:\r\n                      socket_address:\r\n                        address: node3.mydomain.com\r\n                        port_value: 80\r\n      - name: https_cluster\r\n        connect_timeout: 30s\r\n        type: strict_dns\r\n        dns_lookup_family: V4_ONLY\r\n        lb_policy: random\r\n        load_assignment:\r\n          cluster_name: https_cluster\r\n          endpoints:\r\n            - lb_endpoints:\r\n                - endpoint:\r\n                    address:\r\n                      socket_address:\r\n                        address: node1.mydomain.com\r\n                        port_value: 443\r\n                - endpoint:\r\n                    address:\r\n                      socket_address:\r\n                        address: node2.mydomain.com\r\n                        port_value: 443\r\n                - endpoint:\r\n                    address:\r\n                      socket_address:\r\n                        address: node3.mydomain.com\r\n                        port_value: 443\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24715/comments",
    "author": "chewbacca9000",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2022-12-31T14:59:12Z",
        "body": "Are you using a persistent connection to call into Envoy? It might be that you're performing L4 load balancing of connections when you really want L7 load balancing of individual requests"
      },
      {
        "user": "zhangbo1882",
        "created_at": "2023-01-04T01:57:56Z",
        "body": "I think you should use http connection manager instead of tcpproxy if client sends http request to envoy.\r\n\r\nfilter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-02-03T04:01:17Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-02-10T04:01:26Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 24663,
    "title": "Is there a away to validate the whole filesystem-based dynamic configuration ?",
    "created_at": "2022-12-22T10:05:41Z",
    "closed_at": "2023-01-29T00:03:21Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24663",
    "body": "It's seems `--mode validate` only check the main `envoy.yaml` config. It can't check filesystem-based dynamic resource, even yaml format error. Is any way to do the whole check ?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24663/comments",
    "author": "acynothia",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-21T20:02:01Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-29T00:03:20Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "ericychoi",
        "created_at": "2023-04-28T16:13:25Z",
        "body": "plus one on this, what's the best way to go about validating your lds/cds config files?\r\n"
      }
    ]
  },
  {
    "number": 24616,
    "title": "Query - Original src listener filter per route",
    "created_at": "2022-12-18T12:02:40Z",
    "closed_at": "2023-01-10T04:37:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24616",
    "body": "*Title*: *Original src listener filter per route*\r\n\r\n*Description*:\r\nIs it possible to configure a listener-filter, say for ex, original_source filter, for selected routes only, on Envoy.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24616/comments",
    "author": "kumarjp",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-12-19T06:45:14Z",
        "body": "Listener filter is executed before network filter. It's say, when the listener filter is running, there is no any route info.\r\n\r\nSo, answer is \"it's impossible\"."
      },
      {
        "user": "kumarjp",
        "created_at": "2022-12-19T07:16:40Z",
        "body": "Thanks for the response, .. There is a http filter - envoy.filters.http.original_src, which serves the same purpose but per HTTP connection manager.. Probably, this is the closest i can get.  So, wanted to know if there's a similar one for Routes"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-12-19T07:30:45Z",
        "body": "> Thanks for the response, .. There is a http filter - envoy.filters.http.original_src, which serves the same purpose but per HTTP connection manager.. Probably, this is the closest i can get. So, wanted to know if there's a similar one for Routes\r\n\r\nPer route http filter chain is not supported for now. See #20867.\r\n\r\nBut it's possible indeed to make the `envoy.filters.http.original_src` route-level configurable."
      },
      {
        "user": "kumarjp",
        "created_at": "2022-12-19T08:48:29Z",
        "body": "Thanks.\r\n\r\n> But it's possible indeed to make the envoy.filters.http.original_src route-level configurable.\r\n\r\nIs it supported already or is it something to be implemented?  ( I see compressor per route was implemented recently)"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-12-19T09:30:40Z",
        "body": "> Thanks.\r\n> \r\n> > But it's possible indeed to make the envoy.filters.http.original_src route-level configurable.\r\n> \r\n> Is it supported already or is it something to be implemented? ( I see compressor per route was implemented recently)\r\n\r\nSorry, it's not implemented for now."
      },
      {
        "user": "kumarjp",
        "created_at": "2022-12-19T09:53:56Z",
        "body": "Thanks."
      }
    ]
  },
  {
    "number": 24609,
    "title": "FAILED: Build did NOT complete successfully",
    "created_at": "2022-12-17T02:14:46Z",
    "closed_at": "2023-01-29T00:03:16Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24609",
    "body": "\r\nroot@tv3--c9d4bf90:/tmp# gcc --version\r\ngcc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\nroot@tv3--c9d4bf90:/tmp# clang --version\r\nUbuntu clang version 13.0.1-2ubuntu2.1\r\nTarget: x86_64-pc-linux-gnu\r\nThread model: posix\r\nInstalledDir: /usr/bin\r\n\r\nroot@tv3--c9d4bf90:/tmp# lld --version\r\nlld is a generic driver.\r\nInvoke ld.lld (Unix), ld64.lld (macOS), lld-link (Windows), wasm-ld (WebAssembly) instead\r\nroot@tv3--c9d4bf90:/tmp# \r\n\r\nroot@tv3--c9d4bf90:/tmp# uname -a\r\nLinux tv3--c9d4bf90 5.15.0-52-generic #58-Ubuntu SMP Thu Oct 13 08:03:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nroot@tv3--c9d4bf90:/tmp# bazel version\r\nWARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).\r\nBuild label: 6.0.0-pre.20220421.3\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue May 3 16:41:11 2022 (1651596071)\r\nBuild timestamp: 1651596071\r\nBuild timestamp as int: 1651596071\r\nroot@tv3--c9d4bf90:/tmp# \r\n\r\nenvoy tag:v1.23.1\r\n\r\nbazel build \\\r\n--jobs=12 \\\r\n--test_filter=\"skip-all-test\" \\\r\n--copt=\"-DENVOY_IGNORE_GLIBCXX_USE_CXX11_ABI_ERROR=1\" \\\r\n--cxxopt=\"-DENVOY_IGNORE_GLIBCXX_USE_CXX11_ABI_ERROR=1\" \\\r\n--define=ENVOY_IGNORE_GLIBCXX_USE_CXX11_ABI_ERROR=1 \\\r\n--cxxopt='-std=c++17' \\\r\n--//bazel:http3=False \\\r\n--subcommands \\\r\n--verbose_failures \\\r\n--sandbox_debug \\\r\n--cxxopt=\"-Wno-type-limits\" \\\r\n--copt=\"-Wno-array-bounds\" \\\r\n--copt=\"-Wno-vla-parameter\" \\\r\n--copt=\"-Werror=range-loop-construct\" \\\r\n-c dbg //source/exe:envoy-static\r\n\r\n\r\n\r\n\r\n\r\nERROR: /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/external/grpc_httpjson_transcoding/src/BUILD:134:11: Compiling src/path_matcher_node.cc failed: (Exit 1): linux-sandbox failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/1344/execroot/envoy && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/bin:/usr/goPath:/usr/local/go/bin:/usr/local/go \\\r\n    PWD=/proc/self/cwd \\\r\n    TMPDIR=/tmp \\\r\n  /root/.cache/bazel/_bazel_root/install/105cdae8e95eaae9b55d5e684bad8944/linux-sandbox -t 15 -w /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/1344/execroot/envoy -w /tmp -w /dev/shm -D -- /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g '-std=c++0x' -MD -MF bazel-out/k8-dbg/bin/external/grpc_httpjson_transcoding/src/_objs/path_matcher/path_matcher_node.pic.d '-frandom-seed=bazel-out/k8-dbg/bin/external/grpc_httpjson_transcoding/src/_objs/path_matcher/path_matcher_node.pic.o' -fPIC -iquote external/grpc_httpjson_transcoding -iquote bazel-out/k8-dbg/bin/external/grpc_httpjson_transcoding -iquote external/com_google_absl -iquote bazel-out/k8-dbg/bin/external/com_google_absl -isystem external/grpc_httpjson_transcoding/src/include -isystem bazel-out/k8-dbg/bin/external/grpc_httpjson_transcoding/src/include '-DENVOY_IGNORE_GLIBCXX_USE_CXX11_ABI_ERROR=1' -Wno-array-bounds -Wno-vla-parameter '-Werror=range-loop-construct' '-DENVOY_IGNORE_GLIBCXX_USE_CXX11_ABI_ERROR=1' '-std=c++17' -Wno-type-limits -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/grpc_httpjson_transcoding/src/path_matcher_node.cc -o bazel-out/k8-dbg/bin/external/grpc_httpjson_transcoding/src/_objs/path_matcher/path_matcher_node.pic.o)\r\n1671242958.596223429: src/main/tools/linux-sandbox.cc:152: calling pipe(2)...\r\n1671242958.596293717: src/main/tools/linux-sandbox.cc:171: calling clone(2)...\r\n1671242958.596604608: src/main/tools/linux-sandbox.cc:180: linux-sandbox-pid1 has PID 316976\r\n1671242958.596686427: src/main/tools/linux-sandbox-pid1.cc:641: Pid1Main started\r\n1671242958.596773531: src/main/tools/linux-sandbox.cc:197: done manipulating pipes\r\n1671242958.596978361: src/main/tools/linux-sandbox-pid1.cc:260: working dir: /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/1344/execroot/envoy\r\n1671242958.597004636: src/main/tools/linux-sandbox-pid1.cc:292: writable: /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/1344/execroot/envoy\r\n1671242958.597014122: src/main/tools/linux-sandbox-pid1.cc:292: writable: /tmp\r\n1671242958.597020964: src/main/tools/linux-sandbox-pid1.cc:292: writable: /dev/shm\r\n1671242958.597103508: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /\r\n1671242958.597113210: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /dev\r\n1671242958.597118767: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /dev/pts\r\n1671242958.597124165: src/main/tools/linux-sandbox-pid1.cc:362: remount rw: /dev/shm\r\n1671242958.597129016: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /dev/hugepages\r\n1671242958.597134068: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /dev/mqueue\r\n1671242958.597139210: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /run\r\n1671242958.597144208: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /run/lock\r\n1671242958.597149722: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /run/credentials/systemd-sysusers.service\r\n1671242958.597155378: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /run/snapd/ns\r\n1671242958.597161096: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /run/user/1001\r\n1671242958.597169387: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys\r\n1671242958.597174522: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/kernel/security\r\n1671242958.597210603: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/fs/cgroup\r\n1671242958.597217419: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/fs/pstore\r\n1671242958.597222997: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/fs/bpf\r\n1671242958.597228459: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/kernel/debug\r\n1671242958.597234250: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/kernel/tracing\r\n1671242958.597239979: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/fs/fuse/connections\r\n1671242958.597247020: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/kernel/config\r\n1671242958.597253046: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /proc\r\n1671242958.597258291: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /proc/sys/fs/binfmt_misc\r\n1671242958.597267811: src/main/tools/linux-sandbox-pid1.cc:382: remount(nullptr, /proc/sys/fs/binfmt_misc, nullptr, 2101281, nullptr) failure (Operation not permitted) ignored\r\n1671242958.597273535: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /proc/sys/fs/binfmt_misc\r\n1671242958.597279276: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /snap/core20/1623\r\n1671242958.597285158: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /snap/lxd/23541\r\n1671242958.597302510: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /boot/efi\r\n1671242958.597310018: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /snap/snapd/17883\r\n1671242958.597316612: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /snap/core20/1738\r\n1671242958.597322138: src/main/tools/linux-sandbox-pid1.cc:362: remount rw: /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/1344/execroot/envoy\r\n1671242958.597328030: src/main/tools/linux-sandbox-pid1.cc:362: remount rw: /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/1344/execroot/envoy\r\n1671242958.597332900: src/main/tools/linux-sandbox-pid1.cc:362: remount rw: /tmp\r\n1671242958.597347430: src/main/tools/linux-sandbox-pid1.cc:362: remount rw: /dev/shm\r\n1671242958.597424207: src/main/tools/linux-sandbox-pid1.cc:451: calling fork...\r\n1671242958.597614601: src/main/tools/linux-sandbox-pid1.cc:481: child started with PID 2\r\nexternal/grpc_httpjson_transcoding/src/path_matcher_node.cc: In member function 'void google::grpc::transcoding::PathMatcherNode::LookupPath(std::vector<std::__cxx11::basic_string<char> >::const_iterator, std::vector<std::__cxx11::basic_string<char> >::const_iterator, google::grpc::transcoding::HttpMethod, google::grpc::transcoding::PathMatcherLookupResult*) const':\r\nexternal/grpc_httpjson_transcoding/src/path_matcher_node.cc:171:27: error: loop variable 'child_key' of type 'const string&' {aka 'const std::__cxx11::basic_string<char>&'} binds to a temporary constructed from type 'const char* const' [-Werror=range-loop-construct]\r\n  171 |   for (const std::string& child_key :\r\n      |                           ^~~~~~~~~\r\nexternal/grpc_httpjson_transcoding/src/path_matcher_node.cc:171:27: note: use non-reference type 'const string' {aka 'const std::__cxx11::basic_string<char>'} to make the copy explicit or 'const char* const&' to prevent copying\r\ncc1plus: some warnings being treated as errors\r\n1671242959.827154482: src/main/tools/linux-sandbox-pid1.cc:498: wait returned pid=2, status=0x100\r\n1671242959.827292135: src/main/tools/linux-sandbox-pid1.cc:516: child exited normally with code 1\r\n1671242959.828168830: src/main/tools/linux-sandbox.cc:233: child exited normally with code 1\r\nTarget //source/exe:envoy-static failed to build\r\nINFO: Elapsed time: 594.290s, Critical Path: 43.39s\r\nINFO: 553 processes: 15 internal, 538 linux-sandbox.\r\nFAILED: Build did NOT complete successfully",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24609/comments",
    "author": "Webster-Yang",
    "comments": [
      {
        "user": "Webster-Yang",
        "created_at": "2022-12-17T02:59:13Z",
        "body": "root@tv3--c9d4bf90:/tmp# gcc --version\r\ngcc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\nroot@tv3--c9d4bf90:/tmp# clang --version\r\nUbuntu clang version 13.0.1-2ubuntu2.1\r\nTarget: x86_64-pc-linux-gnu\r\nThread model: posix\r\nInstalledDir: /usr/bin\r\n\r\nroot@tv3--c9d4bf90:/tmp# lld --version\r\nlld is a generic driver.\r\nInvoke ld.lld (Unix), ld64.lld (macOS), lld-link (Windows), wasm-ld (WebAssembly) instead\r\nroot@tv3--c9d4bf90:/tmp# \r\n\r\nroot@tv3--c9d4bf90:/tmp# uname -a\r\nLinux tv3--c9d4bf90 5.15.0-52-generic #58-Ubuntu SMP Thu Oct 13 08:03:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nroot@tv3--c9d4bf90:/tmp# bazel version\r\nWARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).\r\nBuild label: 6.0.0-pre.20220421.3\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue May 3 16:41:11 2022 (1651596071)\r\nBuild timestamp: 1651596071\r\nBuild timestamp as int: 1651596071\r\nroot@tv3--c9d4bf90:/tmp# \r\n\r\nenvoy tag:v1.23.1\r\n\r\n\r\n\r\nbazel build \\\r\n--jobs=12 \\\r\n--test_filter=\"skip-all-test\" \\\r\n--cxxopt='-std=c++17' \\\r\n--//bazel:http3=False \\\r\n--subcommands \\\r\n--verbose_failures \\\r\n--sandbox_debug \\\r\n--cxxopt=\"-Wno-type-limits\" \\\r\n--copt=\"-Wno-array-bounds\" \\\r\n--copt=\"-Wno-vla-parameter\" \\\r\n--copt=\"-Werror=range-loop-construct\" \\\r\n-c dbg //source/exe:envoy-static\r\n\r\n\r\n\r\n_formats.pic.o)\r\n# Configuration: 4345402fc0fbbd8cf71551c0e75130756d238ec2ed8b3cedeb33cf5c5e50037e\r\n# Execution platform: @local_config_platform//:host\r\nERROR: /usr/local/istio/envoy/v1.23.1/source/extensions/common/matcher/BUILD:24:19: Compiling source/extensions/common/matcher/trie_matcher.cc failed: (Exit 1): linux-sandbox failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/3897/execroot/envoy && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/bin:/usr/goPath:/usr/local/go/bin:/usr/local/go \\\r\n    PWD=/proc/self/cwd \\\r\n    TMPDIR=/tmp \\\r\n  /root/.cache/bazel/_bazel_root/install/105cdae8e95eaae9b55d5e684bad8944/linux-sandbox -t 15 -w /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/3897/execroot/envoy -w /tmp -w /dev/shm -D -- /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g '-std=c++0x' -MD -MF bazel-out/k8-dbg/bin/source/extensions/common/matcher/_objs/trie_matcher_lib/trie_matcher.pic.d '-frandom-seed=bazel-out/k8-dbg/bin/source/extensions/common/matcher/_objs/trie_matcher_lib/trie_matcher.pic.o' -fPIC -DFMT_HEADER_ONLY -DSPDLOG_FMT_EXTERNAL -DNGHTTP2_STATICLIB -iquote . -iquote bazel-out/k8-dbg/bin -iquote external/com_google_absl -iquote bazel-out/k8-dbg/bin/external/com_google_absl -iquote external/com_github_fmtlib_fmt -iquote bazel-out/k8-dbg/bin/external/com_github_fmtlib_fmt -iquote external/envoy_api -iquote bazel-out/k8-dbg/bin/external/envoy_api -iquote external/com_google_googleapis -iquote bazel-out/k8-dbg/bin/external/com_google_googleapis -iquote external/com_google_protobuf -iquote bazel-out/k8-dbg/bin/external/com_google_protobuf -iquote external/com_envoyproxy_protoc_gen_validate -iquote bazel-out/k8-dbg/bin/external/com_envoyproxy_protoc_gen_validate -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-dbg/bin/external/com_googlesource_code_re2 -iquote external/com_github_cncf_udpa -iquote bazel-out/k8-dbg/bin/external/com_github_cncf_udpa -iquote external/opencensus_proto -iquote bazel-out/k8-dbg/bin/external/opencensus_proto -iquote external/com_github_gabime_spdlog -iquote bazel-out/k8-dbg/bin/external/com_github_gabime_spdlog -iquote external/com_github_cyan4973_xxhash -iquote bazel-out/k8-dbg/bin/external/com_github_cyan4973_xxhash -iquote external/com_github_jbeder_yaml_cpp -iquote bazel-out/k8-dbg/bin/external/com_github_jbeder_yaml_cpp -iquote external/boringssl -iquote bazel-out/k8-dbg/bin/external/boringssl -iquote external/com_googlesource_googleurl -iquote bazel-out/k8-dbg/bin/external/com_googlesource_googleurl -iquote external/envoy -iquote bazel-out/k8-dbg/bin/external/envoy -iquote external/com_github_circonus_labs_libcircllhist -iquote bazel-out/k8-dbg/bin/external/com_github_circonus_labs_libcircllhist -Ibazel-out/k8-dbg/bin/external/com_google_protobuf/_virtual_includes/any_proto -Ibazel-out/k8-dbg/bin/external/com_google_protobuf/_virtual_includes/descriptor_proto -Ibazel-out/k8-dbg/bin/external/com_google_protobuf/_virtual_includes/duration_proto -Ibazel-out/k8-dbg/bin/external/com_google_protobuf/_virtual_includes/empty_proto -Ibazel-out/k8-dbg/bin/external/com_google_protobuf/_virtual_includes/struct_proto -Ibazel-out/k8-dbg/bin/external/com_google_protobuf/_virtual_includes/timestamp_proto -Ibazel-out/k8-dbg/bin/external/com_google_protobuf/_virtual_includes/wrappers_proto -Ibazel-out/k8-dbg/bin/source/common/common/_virtual_includes/logger_impl_lib_standard -Ibazel-out/k8-dbg/bin/source/common/common/_virtual_includes/thread_impl_lib_posix -Ibazel-out/k8-dbg/bin/source/common/api/_virtual_includes/os_sys_calls_lib -isystem external/com_github_fmtlib_fmt/include -isystem bazel-out/k8-dbg/bin/external/com_github_fmtlib_fmt/include -isystem external/com_google_protobuf/src -isystem bazel-out/k8-dbg/bin/external/com_google_protobuf/src -isystem bazel-out/k8-dbg/bin/external/envoy/bazel/foreign_cc/zlib/include -isystem external/com_github_gabime_spdlog/include -isystem bazel-out/k8-dbg/bin/external/com_github_gabime_spdlog/include -isystem external/com_github_jbeder_yaml_cpp/include -isystem bazel-out/k8-dbg/bin/external/com_github_jbeder_yaml_cpp/include -isystem external/boringssl/src/include -isystem bazel-out/k8-dbg/bin/external/boringssl/src/include -isystem bazel-out/k8-dbg/bin/external/envoy/bazel/foreign_cc/event/include -isystem external/envoy/bazel/external/http_parser -isystem bazel-out/k8-dbg/bin/external/envoy/bazel/external/http_parser -isystem bazel-out/k8-dbg/bin/external/envoy/bazel/foreign_cc/nghttp2/include -isystem external/com_github_circonus_labs_libcircllhist/src -isystem bazel-out/k8-dbg/bin/external/com_github_circonus_labs_libcircllhist/src -Wno-array-bounds -Wno-vla-parameter '-Werror=range-loop-construct' '-std=c++17' -Wno-type-limits -Wall -Wextra -Werror -Wnon-virtual-dtor -Woverloaded-virtual -Wold-style-cast -Wformat -Wformat-security -Wvla -Wno-deprecated-declarations -Wreturn-type -ggdb3 -Wno-maybe-uninitialized -DTCMALLOC -DENVOY_HANDLE_SIGNALS -DENVOY_OBJECT_TRACE_ON_DUMP -DENVOY_HOT_RESTART -DENVOY_ADMIN_HTML -DENVOY_GOOGLE_GRPC -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c source/extensions/common/matcher/trie_matcher.cc -o bazel-out/k8-dbg/bin/source/extensions/common/matcher/_objs/trie_matcher_lib/trie_matcher.pic.o)\r\n1671245761.298263389: src/main/tools/linux-sandbox.cc:152: calling pipe(2)...\r\n1671245761.298415532: src/main/tools/linux-sandbox.cc:171: calling clone(2)...\r\n1671245761.298774623: src/main/tools/linux-sandbox.cc:180: linux-sandbox-pid1 has PID 347890\r\n1671245761.298839898: src/main/tools/linux-sandbox-pid1.cc:641: Pid1Main started\r\n1671245761.298938425: src/main/tools/linux-sandbox.cc:197: done manipulating pipes\r\n1671245761.299127301: src/main/tools/linux-sandbox-pid1.cc:260: working dir: /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/3897/execroot/envoy\r\n1671245761.299146653: src/main/tools/linux-sandbox-pid1.cc:292: writable: /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/3897/execroot/envoy\r\n1671245761.299154508: src/main/tools/linux-sandbox-pid1.cc:292: writable: /tmp\r\n1671245761.299161584: src/main/tools/linux-sandbox-pid1.cc:292: writable: /dev/shm\r\n1671245761.299242238: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /\r\n1671245761.299253411: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /dev\r\n1671245761.299259345: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /dev/pts\r\n1671245761.299265421: src/main/tools/linux-sandbox-pid1.cc:362: remount rw: /dev/shm\r\n1671245761.299270500: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /dev/hugepages\r\n1671245761.299275999: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /dev/mqueue\r\n1671245761.299281503: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /run\r\n1671245761.299286863: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /run/lock\r\n1671245761.299292204: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /run/credentials/systemd-sysusers.service\r\n1671245761.299297532: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /run/snapd/ns\r\n1671245761.299303849: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /run/user/1001\r\n1671245761.299309727: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys\r\n1671245761.299314447: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/kernel/security\r\n1671245761.299349654: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/fs/cgroup\r\n1671245761.299356621: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/fs/pstore\r\n1671245761.299362191: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/fs/bpf\r\n1671245761.299367656: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/kernel/debug\r\n1671245761.299373658: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/kernel/tracing\r\n1671245761.299379104: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/fs/fuse/connections\r\n1671245761.299385728: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /sys/kernel/config\r\n1671245761.299391336: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /proc\r\n1671245761.299396496: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /proc/sys/fs/binfmt_misc\r\n1671245761.299404974: src/main/tools/linux-sandbox-pid1.cc:382: remount(nullptr, /proc/sys/fs/binfmt_misc, nullptr, 2101281, nullptr) failure (Operation not permitted) ignored\r\n1671245761.299411082: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /proc/sys/fs/binfmt_misc\r\n1671245761.299416773: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /snap/core20/1623\r\n1671245761.299422210: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /snap/lxd/23541\r\n1671245761.299438910: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /boot/efi\r\n1671245761.299444141: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /snap/snapd/17883\r\n1671245761.299448927: src/main/tools/linux-sandbox-pid1.cc:362: remount ro: /snap/core20/1738\r\n1671245761.299454288: src/main/tools/linux-sandbox-pid1.cc:362: remount rw: /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/3897/execroot/envoy\r\n1671245761.299460418: src/main/tools/linux-sandbox-pid1.cc:362: remount rw: /root/.cache/bazel/_bazel_root/1763248ac6ccfb59ce28d32232ec949a/sandbox/linux-sandbox/3897/execroot/envoy\r\n1671245761.299465296: src/main/tools/linux-sandbox-pid1.cc:362: remount rw: /tmp\r\n1671245761.299478604: src/main/tools/linux-sandbox-pid1.cc:362: remount rw: /dev/shm\r\n1671245761.299538921: src/main/tools/linux-sandbox-pid1.cc:451: calling fork...\r\n1671245761.299706355: src/main/tools/linux-sandbox-pid1.cc:481: child started with PID 2\r\nIn file included from source/extensions/common/matcher/trie_matcher.cc:1:\r\n./source/extensions/common/matcher/trie_matcher.h: In instantiation of 'typename Envoy::Matcher::MatchTree<DataType>::MatchResult Envoy::Extensions::Common::Matcher::TrieMatcher<DataType>::match(const DataType&) [with DataType = Envoy::Network::MatchingData; typename Envoy::Matcher::MatchTree<DataType>::MatchResult = Envoy::Matcher::MatchTree<Envoy::Network::MatchingData>::MatchResult]':\r\n./source/extensions/common/matcher/trie_matcher.h:67:45:   required from here\r\n./source/extensions/common/matcher/trie_matcher.h:85:21: error: loop variable 'node' creates a copy from type 'const Envoy::Extensions::Common::Matcher::TrieNode<Envoy::Network::MatchingData>' [-Werror=range-loop-construct]\r\n   85 |     for (const auto node : values) {\r\n      |                     ^~~~\r\n./source/extensions/common/matcher/trie_matcher.h:85:21: note: use reference type to prevent copying\r\n   85 |     for (const auto node : values) {\r\n      |                     ^~~~\r\n      |                     &\r\n./source/extensions/common/matcher/trie_matcher.h: In instantiation of 'typename Envoy::Matcher::MatchTree<DataType>::MatchResult Envoy::Extensions::Common::Matcher::TrieMatcher<DataType>::match(const DataType&) [with DataType = Envoy::Network::UdpMatchingData; typename Envoy::Matcher::MatchTree<DataType>::MatchResult = Envoy::Matcher::MatchTree<Envoy::Network::UdpMatchingData>::MatchResult]':\r\n./source/extensions/common/matcher/trie_matcher.h:67:45:   required from here\r\n./source/extensions/common/matcher/trie_matcher.h:85:21: error: loop variable 'node' creates a copy from type 'const Envoy::Extensions::Common::Matcher::TrieNode<Envoy::Network::UdpMatchingData>' [-Werror=range-loop-construct]\r\n./source/extensions/common/matcher/trie_matcher.h:85:21: note: use reference type to prevent copying\r\n   85 |     for (const auto node : values) {\r\n      |                     ^~~~\r\n      |                     &\r\ncc1plus: all warnings being treated as errors\r\n1671245792.859067426: src/main/tools/linux-sandbox-pid1.cc:498: wait returned pid=2, status=0x100\r\n1671245792.859157630: src/main/tools/linux-sandbox-pid1.cc:516: child exited normally with code 1\r\n1671245792.859636664: src/main/tools/linux-sandbox.cc:233: child exited normally with code 1\r\nTarget //source/exe:envoy-static failed to build\r\nINFO: Elapsed time: 2411.806s, Critical Path: 97.58s\r\nINFO: 2572 processes: 23 internal, 2549 linux-sandbox.\r\nFAILED: Build did NOT complete successfully\r\nenvoy_buld4.sh: 18: [[: not found"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-21T20:01:57Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-29T00:03:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 24608,
    "title": "build fail",
    "created_at": "2022-12-17T01:39:01Z",
    "closed_at": "2023-01-29T00:03:13Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24608",
    "body": "How to solve it？\r\n\r\n\r\nuname -a\r\nLinux tv3--c9d4bf90 5.15.0-52-generic #58-Ubuntu SMP Thu Oct 13 08:03:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n\r\ng++ --version\r\ng++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PA\r\n\r\n\r\nbazel version\r\nBuild label: 6.0.0-pre.20220421.3\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue May 3 16:41:11 2022 (1651596071)\r\nBuild timestamp: 1651596071\r\nBuild timestamp as int: 1651596071\r\n\r\n\r\n\r\nERROR:\r\nexternal/org_brotli/c/dec/decode.c:2036:41: error: argument 2 of type 'const uint8_t *' {aka 'const unsigned char *'} declared as a pointer [-Werror=vla-parameter]\r\n 2036 |     size_t encoded_size, const uint8_t* encoded_buffer, size_t* decoded_size,\r\n      |                          ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\r\nIn file included from external/org_brotli/c/dec/decode.c:7:\r\nbazel-out/k8-dbg/bin/external/org_brotli/_virtual_includes/brotli_inc/brotli/decode.h:204:19: note: previously declared as a variable length array 'const uint8_t[*decoded_size]' {aka 'const unsigned char[*decoded_size]'}\r\n  204 |     const uint8_t encoded_buffer[BROTLI_ARRAY_PARAM(encoded_size)],\r\n      |     ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/org_brotli/c/dec/decode.c:2037:14: error: argument 4 of type 'uint8_t *' {aka 'unsigned char *'} declared as a pointer [-Werror=vla-parameter]\r\n 2037 |     uint8_t* decoded_buffer) {\r\n      |     ~~~~~~~~~^~~~~~~~~~~~~~\r\nIn file included from external/org_brotli/c/dec/decode.c:7:\r\nbazel-out/k8-dbg/bin/external/org_brotli/_virtual_includes/brotli_inc/brotli/decode.h:206:13: note: previously declared as a variable length array 'uint8_t[encoded_size]' {aka 'unsigned char[encoded_size]'}\r\n  206 |     uint8_t decoded_buffer[BROTLI_ARRAY_PARAM(*decoded_size)]);\r\n\r\nbazel build\r\n--jobs=12\r\n--test_filter=\"skip-all-test\"\r\n--copt=\"-DENVOY_IGNORE_GLIBCXX_USE_CXX11_ABI_ERROR=1\"\r\n--cxxopt=\"-DENVOY_IGNORE_GLIBCXX_USE_CXX11_ABI_ERROR=1\"\r\n--define=ENVOY_IGNORE_GLIBCXX_USE_CXX11_ABI_ERROR=1\r\n--cxxopt='-std=c++17'\r\n--//bazel:http3=False\r\n--subcommands\r\n--verbose_failures\r\n--sandbox_debug\r\n--cxxopt=\"-Wno-type-limits\"\r\n--copt=\"-Wno-array-bounds\"\r\n--copt=\"-Wno-vla-parameter\"\r\n--copt=\"-Werror=range-loop-construct\"\r\n-c dbg //source/exe:envoy-static\r\n\r\n1671110591.210527617: src/main/tools/linux-sandbox-pid1.cc:481: child started with PID 2\r\nexternal/grpc_httpjson_transcoding/src/path_matcher_node.cc: In member function 'void google::grpc::transcoding::PathMatcherNode::LookupPath(std::vector<std::__cxx11::basic_string >::const_iterator, std::vector<std::__cxx11::basic_string >::const_iterator, google::grpc::transcoding::HttpMethod, google::grpc::transcoding::PathMatcherLookupResult*) const':\r\nexternal/grpc_httpjson_transcoding/src/path_matcher_node.cc:171:27: error: loop variable 'child_key' of type 'const string&' {aka 'const std::__cxx11::basic_string&'} binds to a temporary constructed from type 'const char* const' [-Werror=range-loop-construct]\r\n171 | for (const std::string& child_key :\r\n| ^~~~~~~~~\r\nexternal/grpc_httpjson_transcoding/src/path_matcher_node.cc:171:27: note: use non-reference type 'const string' {aka 'const std::__cxx11::basic_string'} to make the copy explicit or 'const char* const&' to prevent copying\r\ncc1plus: some warnings being treated as errors\r\n1671110592.270675362: src/main/tools/linux-sandbox-pid1.cc:498: wait returned pid=2, status=0x100\r\n1671110592.270779674: src/main/tools/linux-sandbox-pid1.cc:516: child exited normally with code 1\r\n1671110592.271164805: src/main/tools/linux-sandbox.cc:233: child exited normally with code 1\r\nTarget //source/exe:envoy-static failed to build\r\nINFO: Elapsed time: 2558.791s, Critical Path: 54.19s\r\nINFO: 3010 processes: 24 internal, 2986 linux-sandbox.\r\nFAILED: Build did NOT complete successfully\r\nenvoy_buld3.sh: 21: [[: not found\r\nnext build 1\r\nbuild done",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24608/comments",
    "author": "Webster-Yang",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-21T20:01:55Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-29T00:03:12Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 24416,
    "title": "How to configure scoped_routes with scoped_rds ?",
    "created_at": "2022-12-07T09:38:51Z",
    "closed_at": "2023-01-13T16:01:48Z",
    "labels": [
      "question",
      "stale",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24416",
    "body": "*Description*:\r\n>I am trying configure scoped_rds with filesystem rds. But I got an error `Unexpected RDS resource length: 2`. How can I configure scoped_routes with multi-routes. configure as followed.\r\n\r\nstart.sh\r\n```\r\n#!bin/bash\r\n\r\ndocker run --rm \\\r\n    --name envoy \\\r\n    -v $(pwd)/envoy.yaml:/etc/envoy/envoy.yaml \\\r\n    -v $(pwd)/xds:/etc/envoy/xds \\\r\n    -p 10002:10002 \\\r\n    -p 9081:9081 \\\r\n    envoyproxy/envoy:v1.24.0\r\n```\r\n\r\nenvoy.yaml\r\n```yaml\r\nnode:\r\n  cluster: cluster-1\r\n  id: envoy-instance-1\r\n\r\ndynamic_resources:\r\n  lds_config:\r\n    resource_api_version: V3\r\n    path_config_source:\r\n      path: \"/etc/envoy/xds/lds/lds.yaml\"\r\n      watched_directory:\r\n        path: \"/etc/envoy/xds/lds\"\r\n\r\nadmin:\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9081\r\n```\r\n\r\nxds/lds/lds.yaml\r\n```\r\nresources:\r\n  - \"@type\": type.googleapis.com/envoy.config.listener.v3.Listener\r\n    name: \"listener_0\"\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 10002\r\n    filter_chains:\r\n      - filters:\r\n          - name: envoy.filters.network.http_connection_manager\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n              stat_prefix: ingress_http_scoped\r\n              http_filters:\r\n                - name: envoy.filters.http.router\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n              scoped_routes:\r\n                name: rds_scoped\r\n                scope_key_builder:\r\n                  fragments:\r\n                    - header_value_extractor:\r\n                        name: \"X-Route-Selector\"\r\n                        element_separator: \";\"\r\n                        index: 0\r\n                scoped_rds:\r\n                  scoped_rds_config_source:\r\n                    resource_api_version: V3\r\n                    path_config_source:\r\n                      path: /etc/envoy/xds/rds/rds_scoped.yaml\r\n                      watched_directory:\r\n                        path: /etc/envoy/xds/rds\r\n                rds_config_source:\r\n                  resource_api_version: V3\r\n                  path_config_source:\r\n                    path: /etc/envoy/xds/rds/rds_scoped_route.yaml\r\n                    watched_directory:\r\n                      path: /etc/envoy/xds/rds\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24416/comments",
    "author": "acynothia",
    "comments": [
      {
        "user": "acynothia",
        "created_at": "2022-12-07T09:40:39Z",
        "body": "rds_scoped.yaml\r\n```\r\nresources:\r\n  - \"@type\": type.googleapis.com/envoy.config.route.v3.ScopedRouteConfiguration\r\n    name: route_scope_1\r\n    route_configuration_name: rds_scoped_config_1\r\n    key:\r\n      fragments:\r\n        - string_key: \"web1\"\r\n\r\n  - \"@type\": type.googleapis.com/envoy.config.route.v3.ScopedRouteConfiguration\r\n    name: route_scope_2\r\n    route_configuration_name: rds_scoped_config_1\r\n    key:\r\n      fragments:\r\n        - string_key: \"web2\"\r\n```\r\n\r\nrds_scoped_route.yaml\r\n```\r\nresources:\r\n  - \"@type\": type.googleapis.com/envoy.config.route.v3.RouteConfiguration\r\n    name: rds_scoped_config_1\r\n    virtual_hosts:\r\n      - name: direct_response_service\r\n        domains: [\"*\"]\r\n        routes:\r\n          - match:\r\n              prefix: \"/\"\r\n            direct_response:\r\n              status: 200\r\n              body:\r\n                inline_string: \"scoped yay 1\\n\"\r\n\r\n  - \"@type\": type.googleapis.com/envoy.config.route.v3.RouteConfiguration\r\n    name: rds_scoped_config_2\r\n    virtual_hosts:\r\n      - name: direct_response_service\r\n        domains: [\"*\"]\r\n        routes:\r\n          - match:\r\n              prefix: \"/\"\r\n            direct_response:\r\n              status: 200\r\n              body:\r\n                inline_string: \"scoped yay 2\\n\"\r\n```"
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-12-07T14:03:33Z",
        "body": "cc @envoyproxy/api-shepherds "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-06T16:01:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-13T16:01:47Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "andresmargalef",
        "created_at": "2023-02-07T17:19:13Z",
        "body": "Can be reopened this issue? it keeps failing in version 1.25.0 of envoy"
      }
    ]
  },
  {
    "number": 24346,
    "title": "how to set cluster_name for local ratelimit stat metrics?",
    "created_at": "2022-12-05T12:44:20Z",
    "closed_at": "2023-01-12T04:01:31Z",
    "labels": [
      "question",
      "stale",
      "area/ratelimit"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24346",
    "body": "Title:how to set cluster_name for local ratelimit stat metrics?\r\n\r\nDescription:\r\n\r\nwhen using global ratelimit and local ratelimit, the stat metrics are deffirent.\r\nglobal ratelimit:\r\nenvoy_cluster_ratelimit_ok{app=\"reviews\", cluster_name=\"inbound|9080||\", hskp_io_parent_workload_name=\"reviews-v1\", instance=\"10.244.30.37:15020\", job=\"kubernetes-pods\", namespace=\"ratelimit\", pod=\"reviews-v1-844c9b6c77-82b52\", pod_template_hash=\"844c9b6c77\", security_istio_io_tlsMode=\"istio\", service_istio_io_canonical_name=\"reviews\", service_istio_io_canonical_revision=\"v1\", version=\"v1\"}\r\nenvoy_cluster_ratelimit_ok{app=\"ratings\", cluster_name=\"outbound|9080||productpage.ratelimit.svc.cluster.local\", instance=\"10.244.30.16:15020\", job=\"kubernetes-pods\", namespace=\"ratelimit\", pod=\"ratings-v1-557bcd6d57-67z2m\", pod_template_hash=\"557bcd6d57\", security_istio_io_tlsMode=\"istio\", service_istio_io_canonical_name=\"ratings\", service_istio_io_canonical_revision=\"v1\", version=\"v1\"}\r\n\r\ncluster_name=\"inbound|9080||\"\r\ncluster_name=\"outbound|9080||productpage.ratelimit.svc.cluster.local\"\r\n**there have cluster_name labels ,which can separate inbound and outbound requests.**\r\n\r\nloca ratelimit:\r\nenvoy_ratelimit_http_local_rate_limit_ok{app=\"ratings\",instance=\"10.244.30.46:15020\", job=\"kubernetes-pods\", namespace=\"ratelimit\", pod=\"ratings-v1-557bcd6d57-xqxvm\", pod_template_hash=\"557bcd6d57\", security_istio_io_tlsMode=\"istio\", service_istio_io_canonical_name=\"ratings\", service_istio_io_canonical_revision=\"v1\", version=\"v1\"}\r\n**there not find cluster_name, then cannot separate inbound and outbound requests.**\r\n\r\nhow to set cluster_name for local ratelimit stat metrics?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24346/comments",
    "author": "HailiangChang",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2022-12-05T14:04:20Z",
        "body": "cc @kyessenov "
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-12-05T14:04:42Z",
        "body": "cc @wbpcode "
      },
      {
        "user": "kyessenov",
        "created_at": "2022-12-05T17:49:33Z",
        "body": "Yes, that sounds correct: the global rate limit emits in the cluster scope while the local rate limit emits in the route scope. That is inconsistent. The route name would probably be enough to figure out which data path is involved but it seems be stripped in istio."
      },
      {
        "user": "HailiangChang",
        "created_at": "2022-12-06T01:04:13Z",
        "body": "@kyessenov Thank you. that mean istio-proxy handle and not display the cluster or route information? Is there any person related to istio who can help to confirm?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-05T04:01:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-12T04:01:30Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 24282,
    "title": "http local rate limit  can not get expected QPS ？",
    "created_at": "2022-12-01T02:20:02Z",
    "closed_at": "2022-12-21T09:57:34Z",
    "labels": [
      "question",
      "area/ratelimit"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24282",
    "body": "\r\n*Description*:\r\n>\r\nwe have local rate limit config as follow:\r\n\r\n```\r\n  token_bucket:\r\n    max_tokens: 1000\r\n    tokens_per_fill: 1000\r\n    fill_interval: 1s\r\n```\r\n\r\nwe send 5k grpc request  in one second to this service,  the success response is only 600-700 a second.\r\nthat mean client send 5k request and only get 600-700 request be handled correctly which is not expected (the expectation is almost 1000 ).\r\nwhy this happened？\r\nis there something we have ignored？\r\n\r\n\r\nenvoy version: 1.19.3\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24282/comments",
    "author": "song0071000",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2022-12-01T18:01:50Z",
        "body": "There was an issue with the local rate limit that underfilled the bucket, which results in lower than expected limit. Can you try a more recent version of Envoy?"
      },
      {
        "user": "song0071000",
        "created_at": "2022-12-02T06:46:58Z",
        "body": "> There was an issue with the local rate limit that underfilled the bucket, which results in lower than expected limit. Can you try a more recent version of Envoy?\r\n\r\nThx, i will retest with recent version envoy."
      }
    ]
  },
  {
    "number": 24281,
    "title": "Envoy filter for outbound https traffic",
    "created_at": "2022-12-01T01:37:04Z",
    "closed_at": "2022-12-01T18:00:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24281",
    "body": "I am trying to intercept all outbound http/s traffic from a pod and add a custom header to the request. After reading some documentation I came to the understanding that an envoy filter on SIDECAR_OUTBOUND with some custom lua code would do the trick. So this is the configuration that I did:\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: outbound-filter\r\nspec:\r\n  configPatches:\r\n    - applyTo: HTTP_FILTER\r\n      match:\r\n        context: SIDECAR_OUTBOUND\r\n        listener:\r\n          filterChain:\r\n            filter:\r\n              name: envoy.http_connection_manager\r\n              subFilter:\r\n                name: envoy.router\r\n      patch:\r\n        operation: INSERT_BEFORE\r\n        value:\r\n          name: envoy.lua\r\n          typed_config:\r\n            '@type': type.googleapis.com/envoy.config.filter.http.lua.v2.Lua\r\n            inlineCode: |\r\n              function envoy_on_request(request_handle)\r\n                  request_handle:logWarn(\"Hello World\")\r\n                  request_handle:headers():add(\"origin\", os.getenv(\"ISTIO_META_WORKLOAD_NAME\"))\r\n              end\r\n```\r\nAnd it works perfectly fine for http requests. However, it seems that the filter does not run at all when the request is over https.\r\nAs such I tried to use the Egress gateway for it. This is what I did\r\n1. Deploy an egress gateway in my k8s cluster\r\n2. Change the outbound mesh traffic to registry only\r\n3. Create a service entry for a particular host\r\n4. Create a gateway kubernetes object which routes the requests from the egress gateway to a particular URL and listens at a particular port\r\n5. Create a virtualService which serves the purpose of making sure that the routing of all http requests that are going to a particular URL are redirected to the egress gateway service and then the egress gateway can send it to its destination which is the final URL\r\n\r\nAll of this is good, but this can only be done for one particular remote service outside the mesh and for every service I want to add , I would have to add its url in the preexisting serviceentry and basically go over the full loop again.\r\n\r\nIs there a way I can do that for all requests going out of my mesh ?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24281/comments",
    "author": "solarkaka",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2022-12-01T18:00:22Z",
        "body": "This question is more appropriate for the Istio organization. But to answer, yes, I think having a dedicated egress gateway for all traffic is probably the right solution to inspect all outgoing traffic. Otherwise, Envoy probably cannot parse encrypted HTTP traffic."
      }
    ]
  },
  {
    "number": 24276,
    "title": "Migrating from v2 -> v3 config (currently running envoy v1.16.0, want to upgrade to latest)",
    "created_at": "2022-11-30T18:25:34Z",
    "closed_at": "2023-01-07T04:01:15Z",
    "labels": [
      "question",
      "stale",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24276",
    "body": "Currently running v1.16.0 of envoy and looking to upgrade it, but having a hard time finding any documentation that really explains the differences and upgrade path from v2 -> v3.  Maybe I'm blind, but that's me.  Our current config is:\r\n\r\n```\r\n#/etc/envoy/envoy.yaml\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8095\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 8094\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              config:\r\n                stat_prefix: ingress_http\r\n                use_remote_address: true\r\n                stream_idle_timeout: 0s\r\n                access_log:\r\n                  - name: envoy.file_access_log\r\n                    config:\r\n                      path: /dev/stdout\r\n                      format: >\r\n                        [%START_TIME%] \"%REQ(:METHOD)%\r\n                        %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\"\r\n                        %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED%\r\n                        %BYTES_SENT% %DURATION%\r\n                        %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\r\n                        \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\"\r\n                        \"%REQ(X-TRACE)%\" \"%REQ(X-CUSTOMER-ID)%\"\r\n                        \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\r\n                  - name: envoy.http_grpc_access_log\r\n                    config:\r\n                      common_config:\r\n                        log_name: apirate\r\n                        grpc_service:\r\n                          envoy_grpc:\r\n                            cluster_name: ratelimit\r\n                      additional_request_headers_to_log:\r\n                        - x-customer-id\r\n                        - x-trace\r\n                        - x-forwarded-for\r\n                        - user-agent\r\n                route_config:\r\n                  name: local_route\r\n                  request_headers_to_add:\r\n                    - header:\r\n                        key: x-forwarded-proto\r\n                        value: https\r\n                      append: false\r\n                    - header:\r\n                        key: connection\r\n                        value: close\r\n                      append: false\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains:\r\n                        - '*'\r\n                      routes:\r\n                        - match:\r\n                            prefix: /rest/transport/\r\n                            headers:\r\n                              - name: X-HAPROXY-CLUSTER\r\n                                exact_match: kube\r\n                          route:\r\n                            cluster: backend-kube\r\n                            timeout: 0s\r\n                            retry_policy:\r\n                              retry_on: connect-failure\r\n                              num_retries: 5\r\n                              retry_host_predicate:\r\n                                - name: envoy.retry_host_predicates.previous_hosts\r\n                              host_selection_retry_max_attempts: 5\r\n                        - match:\r\n                            prefix: /\r\n                            headers:\r\n                              - name: X-HAPROXY-CLUSTER\r\n                                exact_match: kube\r\n                          route:\r\n                            cluster: backend-kube\r\n                            timeout: 0s\r\n                            retry_policy:\r\n                              retry_on: connect-failure\r\n                              num_retries: 5\r\n                              retry_host_predicate:\r\n                                - name: envoy.retry_host_predicates.previous_hosts\r\n                              host_selection_retry_max_attempts: 5\r\n                            rate_limits:\r\n                              actions:\r\n                                - generic_key:\r\n                                    descriptor_value: apirate\r\n                                - request_headers:\r\n                                    header_name: ':authority'\r\n                                    descriptor_key: authority\r\n                                - request_headers:\r\n                                    header_name: ':method'\r\n                                    descriptor_key: method\r\n                                - request_headers:\r\n                                    header_name: ':path'\r\n                                    descriptor_key: path\r\n                                - request_headers:\r\n                                    header_name: user-agent\r\n                                    descriptor_key: userAgent\r\n                                - request_headers:\r\n                                    header_name: x-customer-id\r\n                                    descriptor_key: customerId\r\n                                - request_headers:\r\n                                    header_name: x-trace\r\n                                    descriptor_key: xTrace\r\n                                - request_headers:\r\n                                    header_name: x-forwarded-for\r\n                                    descriptor_key: xForwardedFor\r\n                http_filters:\r\n                  - name: envoy.rate_limit\r\n                    config:\r\n                      domain: apirate\r\n                      failure_mode_deny: false\r\n                      timeout: 0.25s\r\n                      rate_limit_service:\r\n                        grpc_service:\r\n                          timeout: 0.05s\r\n                          envoy_grpc:\r\n                            cluster_name: ratelimit\r\n                  - name: envoy.router\r\n  clusters:\r\n    - name: backend-kube\r\n      connect_timeout: 0.25s\r\n      type: STRICT_DNS\r\n      lb_policy: ROUND_ROBIN\r\n      max_requests_per_connection: 1\r\n      http_protocol_options: {}\r\n      dns_refresh_rate: 5s\r\n      load_assignment:\r\n        cluster_name: backend-kube\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: 127.0.0.1\r\n                      port_value: 81\r\n      health_checks:\r\n        http_health_check:\r\n          path: /healthz\r\n        timeout: 1s\r\n        interval: 1s\r\n        interval_jitter_percent: 25\r\n        unhealthy_threshold: 3\r\n        healthy_threshold: 1\r\n        reuse_connection: true\r\n        always_log_health_check_failures: true\r\n      circuit_breakers:\r\n        thresholds:\r\n          - priority: DEFAULT\r\n            max_connections: 65536\r\n            max_pending_requests: 65536\r\n            max_requests: 65536\r\n            max_retries: 1024\r\n          - priority: HIGH\r\n            max_connections: 65536\r\n            max_pending_requests: 65536\r\n            max_requests: 65536\r\n            max_retries: 1024\r\n    - name: ratelimit\r\n      connect_timeout: 0.25s\r\n      type: STRICT_DNS\r\n      lb_policy: ROUND_ROBIN\r\n      http2_protocol_options: {}\r\n      load_assignment:\r\n        cluster_name: ratelimit\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: 192.168.0.1\r\n                      port_value: 8081\r\n```\r\n\r\nI'd like to upgrade that to v3 format, but get errors like `INVALID_ARGUMENT:(static_resources.listeners[0].filter_chains[0].filters[0]) config: Cannot find field.) has unknown fields`, which though I'm aware of config being depricated, I know type_config is available.. but not 100% sure how all this works.  Any help (with examples) would be appreciated.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24276/comments",
    "author": "sharkannon",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2022-11-30T23:17:47Z",
        "body": "Yeah, the biggest thing is the usage of the \"typed_config\" consistently:\r\n\r\n```\r\n - name: envoy.http_connection_manager\r\n   typed_config:\r\n     \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n     stat_prefix: ingress_http\r\n```\r\n\r\nThere were some field renames that can be found by looking at the protobuf annotations, so you probably want to gradually convert config and enable strict validation (disallow unknown fields in static and dynamic configs)."
      },
      {
        "user": "sharkannon",
        "created_at": "2022-11-30T23:53:50Z",
        "body": "I'm not really impressed with the logging messages about the deprications, but I *think* I got it.  I basically had to fix deprication warnings in 1.16, then upgrade toi 1.17, then fix those.. all the way up to 1.21.. then in 1.22 there was a couple of those type_config issues that came up...\r\n\r\n\r\n```\r\nadmin:\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8095\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 8094\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: ingress_http\r\n                use_remote_address: true\r\n                stream_idle_timeout: 0s\r\n                access_log:\r\n                  - name: envoy.access_loggers\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\r\n                      log_format:\r\n                        text_format_source: \r\n                          inline_string: >\r\n                            [%START_TIME%] \"%REQ(:METHOD)%\r\n                            %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\"\r\n                            %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED%\r\n                            %BYTES_SENT% %DURATION%\r\n                            %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\r\n                            \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\"\r\n                            \"%REQ(X-TRACE)%\" \"%REQ(X-CUSTOMER-ID)%\"\r\n                            \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\r\n                  - name: envoy.access_loggers\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.access_loggers.grpc.v3.HttpGrpcAccessLogConfig\r\n                      common_config:\r\n                        log_name: apirate\r\n                        transport_api_version: V3\r\n                        grpc_service:\r\n                          envoy_grpc:\r\n                            cluster_name: ratelimit\r\n                      additional_request_headers_to_log:\r\n                        - x-customer-id\r\n                        - x-trace\r\n                        - x-forwarded-for\r\n                        - user-agent\r\n                route_config:\r\n                  name: local_route\r\n                  request_headers_to_add:\r\n                    - header:\r\n                        key: x-forwarded-proto\r\n                        value: https\r\n                      append: false\r\n                    - header:\r\n                        key: connection\r\n                        value: close\r\n                      append: false\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains:\r\n                        - '*'\r\n                      routes:\r\n                        - match:\r\n                            prefix: /rest/transport/\r\n                            headers:\r\n                              - name: X-HAPROXY-CLUSTER\r\n                                string_match: \r\n                                  exact: kube\r\n                          route:\r\n                            cluster: backend-kube\r\n                            timeout: 0s\r\n                            retry_policy:\r\n                              retry_on: connect-failure\r\n                              num_retries: 5\r\n                              retry_host_predicate:\r\n                                - name: envoy.retry_host_predicates\r\n                                  typed_config:\r\n                                    \"@type\": type.googleapis.com/envoy.extensions.retry.host.previous_hosts.v3.PreviousHostsPredicate\r\n                              host_selection_retry_max_attempts: 5\r\n                        - match:\r\n                            prefix: /\r\n                            headers:\r\n                              - name: X-HAPROXY-CLUSTER\r\n                                string_match: \r\n                                  exact: kube\r\n                          route:\r\n                            cluster: backend-kube\r\n                            timeout: 0s\r\n                            retry_policy:\r\n                              retry_on: connect-failure\r\n                              num_retries: 5\r\n                              retry_host_predicate:\r\n                                - name: envoy.retry_host_predicates\r\n                                  typed_config:\r\n                                    \"@type\": type.googleapis.com/envoy.extensions.retry.host.previous_hosts.v3.PreviousHostsPredicate\r\n                              host_selection_retry_max_attempts: 5\r\n                            rate_limits:\r\n                              actions:\r\n                                - generic_key:\r\n                                    descriptor_value: apirate\r\n                                - request_headers:\r\n                                    header_name: ':authority'\r\n                                    descriptor_key: authority\r\n                                - request_headers:\r\n                                    header_name: ':method'\r\n                                    descriptor_key: method\r\n                                - request_headers:\r\n                                    header_name: ':path'\r\n                                    descriptor_key: path\r\n                                - request_headers:\r\n                                    header_name: user-agent\r\n                                    descriptor_key: userAgent\r\n                                - request_headers:\r\n                                    header_name: x-customer-id\r\n                                    descriptor_key: customerId\r\n                                - request_headers:\r\n                                    header_name: x-trace\r\n                                    descriptor_key: xTrace\r\n                                - request_headers:\r\n                                    header_name: x-forwarded-for\r\n                                    descriptor_key: xForwardedFor\r\n                http_filters:\r\n                  - name: envoy.filters.http.ratelimit\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimit\r\n                      domain: apirate\r\n                      failure_mode_deny: false\r\n                      timeout: 0.25s\r\n                      rate_limit_service:\r\n                        transport_api_version: V3\r\n                        grpc_service:\r\n                          timeout: 0.05s\r\n                          envoy_grpc:\r\n                            cluster_name: ratelimit\r\n                  - name: envoy.filters.http.router\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n  clusters:\r\n    - name: backend-kube\r\n      connect_timeout: 0.25s\r\n      type: STRICT_DNS\r\n      lb_policy: ROUND_ROBIN\r\n      typed_extension_protocol_options:\r\n        envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n          \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n          common_http_protocol_options:\r\n            max_requests_per_connection: 1\r\n          use_downstream_protocol_config: {}\r\n      dns_refresh_rate: 5s\r\n      load_assignment:\r\n        cluster_name: backend-kube\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: 127.0.0.1\r\n                      port_value: 81\r\n      health_checks:\r\n        http_health_check:\r\n          path: /healthz\r\n        timeout: 1s\r\n        interval: 1s\r\n        interval_jitter_percent: 25\r\n        unhealthy_threshold: 3\r\n        healthy_threshold: 1\r\n        reuse_connection: true\r\n        always_log_health_check_failures: true\r\n      circuit_breakers:\r\n        thresholds:\r\n          - priority: DEFAULT\r\n            max_connections: 65536\r\n            max_pending_requests: 65536\r\n            max_requests: 65536\r\n            max_retries: 1024\r\n          - priority: HIGH\r\n            max_connections: 65536\r\n            max_pending_requests: 65536\r\n            max_requests: 65536\r\n            max_retries: 1024\r\n\r\n    - name: ratelimit\r\n      connect_timeout: 0.25s\r\n      type: STRICT_DNS\r\n      lb_policy: ROUND_ROBIN\r\n      load_assignment:\r\n        cluster_name: ratelimit\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: 192.168.0.1\r\n                      port_value: 8081\r\n```\r\n\r\nThat starts up, is it the same as my original, I'm not 100% sure.. but it at least starts up, I'll deploy the changes to dev tomorrow to see if it works.  Any suggestions/recommendations/fixes would be appreciated."
      },
      {
        "user": "kyessenov",
        "created_at": "2022-11-30T23:57:10Z",
        "body": "I don't see anything wrong but it's always worth testing before deploying. One piece of good news for you is that v3 is forever, there'll be no v4 migration."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-31T00:02:58Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-07T04:01:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 24194,
    "title": "`proxywasm.GetProperty([]string{\"request\", \"path\"})` got an empty result and and error in the response processing phase",
    "created_at": "2022-11-24T08:13:46Z",
    "closed_at": "2023-01-03T12:02:01Z",
    "labels": [
      "question",
      "stale",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24194",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n\r\nzh_CN: 我想获取到上游服务接收到的 request path (被重写后的 path), 但是我用 `proxywasm.GetProperty([]string{\"request\", \"path\"}) ` 只得到了一个 空的结果和一个 error.\r\n\r\nen: I want to get the request path (rewritten path) received by the upstream service, but I use `proxywasm.GetProperty([]string{\"request\", \"path\"}) ` and I only get a empty result and an error.\r\n\r\nthe error:\r\n```\r\nerror status returned by host: not found\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24194/comments",
    "author": "dspo",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-11-24T08:45:09Z",
        "body": "which version of Envoy are you using ?"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-11-27T11:10:22Z",
        "body": "The header name of request path is \":path\", not the \"path\". There is a colon."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-27T12:01:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-03T12:02:01Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 24135,
    "title": "Measure/Detect client side timeout",
    "created_at": "2022-11-22T01:11:17Z",
    "closed_at": "2022-12-31T12:01:35Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24135",
    "body": "\r\n\r\n*Title*: Measure/Detect downstream timeout (client side timeout)\r\n\r\n*Description*: \r\nHow can we detect if the caller/client of EnvoyProxy has timed out on a request? Is there a way to detect this from EnvoyProxy?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24135/comments",
    "author": "arjunsingriuber",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-11-24T03:17:47Z",
        "body": "If the caller timeout, the caller should cancel the request, right? Then envoy will presume that the caller is timeout.\r\n\r\nAnd if the caller is envoy, an `expect-rq-timeout` header would be also helpful."
      },
      {
        "user": "arjunsingriuber",
        "created_at": "2022-11-24T03:33:07Z",
        "body": "Is there a metric emitted by Envoy Proxy when the caller is timed out and it cancels the request? "
      },
      {
        "user": "wbpcode",
        "created_at": "2022-11-24T04:51:35Z",
        "body": "There are no direct stats for it. But `downstream_cx_destroy_remote_active_rq` could be used as a reference."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-24T08:01:33Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-31T12:01:34Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 24113,
    "title": "Question regarding tcp proxy src ip",
    "created_at": "2022-11-21T19:48:29Z",
    "closed_at": "2023-01-04T20:01:35Z",
    "labels": [
      "question",
      "stale",
      "area/tcp_proxy"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24113",
    "body": "Hi,\r\nI have a use case where I want to use envoy as a proxy to forward TCP & TLS traffic to a service (running in my k8s cluster).\r\nfor the following traffic flow: `client -> envoy -> service`\r\nI need the src ip to remain the same (based on the client IP) for TCP & TLS traffic that is meant for my service.\r\nis it possible? how do I configure such thing for TCP proxy? does envoy handles responses on the way back properly (if the src ip is the client's ip and not the host running envoy)",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24113/comments",
    "author": "or-adar",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-11-24T03:14:08Z",
        "body": "As far as I know, seems it's impossible? 🤔 \r\n\r\ncc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-11-28T15:16:05Z",
        "body": "yeah if you're terminating TCP generally the best you can do is something like set x-forwarded-for header or the proxy proto header to communicate the original IP."
      },
      {
        "user": "or-adar",
        "created_at": "2022-11-28T17:54:40Z",
        "body": "Great, thanks!\r\nI was actually trying to understand if envoy could be a suitable replacement to stunnel.\r\nGood to know!"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-28T20:01:31Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-04T20:01:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23934,
    "title": "A large http request will lose its body",
    "created_at": "2022-11-10T16:01:51Z",
    "closed_at": "2022-11-14T03:24:40Z",
    "labels": [
      "question",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23934",
    "body": "I'm using Envoy V1.22 as the gateway of my backend web servers. The web servers provided http POST requests, one of them is about uploading files to server.\r\n\r\nTo my surprise, if I upload a tiny file, such as a 3KB file, it works as expected but if I upload a larger file, such as a 20KB file, it would fail.\r\n\r\nSo I'm trying to add a Lua script to figure out what happened.\r\n\r\nHere is my config file of Envoy:\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_common_https\r\n    address:\r\n      socket_address: *common_ingress_https_addr\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress_https\r\n          access_log: *common_https_access_log\r\n          http_filters:\r\n          - name: envoy.filters.http.lua\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n              inline_code:\r\n                function envoy_on_request(request_handle)\r\n                  local index = 0\r\n                  local header = request_handle:headers()\r\n                  request_handle:logInfo(\"header:\")\r\n                  request_handle:logInfo(tostring(header:get(\"content-length\")))\r\n                  request_handle:logInfo(\"body:\")\r\n                  local body = request_handle:body()\r\n                  if body == nil then\r\n                    request_handle:logInfo(\"nil? wtf man!\")\r\n                  else\r\n                    request_handle:logInfo(body:length())\r\n                  end\r\n                end\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: file_service\r\n              domains: [\"fileserver.example.com:*\", \"fileserver2.example.com:*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: cluster_filesvr_https\r\n                  timeout: 1200s\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n          common_tls_context: *ingress_https_crt\r\n      transport_socket_connect_timeout: 30s\r\n  clusters:\r\n  - name: cluster_filesvr_https\r\n    connect_timeout: 30s\r\n    type: LOGICAL_DNS\r\n    typed_extension_protocol_options:\r\n      envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n        \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n        upstream_http_protocol_options:\r\n          auto_sni: true\r\n        common_http_protocol_options:\r\n          idle_timeout: 3600s\r\n        explicit_http_config:\r\n          http_protocol_options: {}\r\n#    dns_lookup_family: V4_ONLY\r\n    load_assignment:\r\n      cluster_name: cluster_filesvr_https\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address: *filesvr_egress_https_addr\r\n```\r\n\r\nNow, if I upload a tiny file, the Lua script will print the log as below:\r\n```\r\nheader:\r\n123\r\nbody:\r\n123\r\n```\r\nWhich looks fine.\r\n\r\nBut a larger file will print:\r\n```\r\nheader:\r\n34333\r\nbody:    # it would block here for a long time\r\nnil? wtf man!\r\n```\r\n\r\nI've tried to add some other configs about timeout and buffer size, but nothing changed.\r\n**It seems that the Envoy started to receive the request packet and suddenly all of received data was abandoned so that the variable `body` in Lua script became `nil`.**\r\nCould someone help me on this issue?\r\nBTW, I deployed my Envoy with Docker container and I just tried to use Nginx and it works fine even with larger file...",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23934/comments",
    "author": "YvesZHI",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-11-11T13:58:21Z",
        "body": "cc @dio "
      },
      {
        "user": "YvesZHI",
        "created_at": "2022-11-11T15:56:00Z",
        "body": "Please notice that\r\n1. The phenomenon / bug still happens without any Lua script;\r\n2. It seems that the phenomenon / bug would be gone if I deploy the Envoy with binary, instead of Docker."
      }
    ]
  },
  {
    "number": 23887,
    "title": "uhv: determine if slash merging and percent decoding should be independent of path normalization",
    "created_at": "2022-11-08T13:19:41Z",
    "closed_at": "2022-12-16T16:01:38Z",
    "labels": [
      "enhancement",
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23887",
    "body": "*Title*: uhv: determine if slash merging and percent decoding should be independent of path normalization\r\n\r\n*Description*:\r\nUHV path normalization performs slash merging and percent decoding, these operations used to be separate and could be toggled independently outside of UHV. This is a breaking change so we need to determine if we should separate these options from path normalization so that users can have a piecemeal configuration for now path normalization is performed.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23887/comments",
    "author": "ameily",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-11-09T10:36:35Z",
        "body": "cc @yanavlasov @mattklein123 "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-09T12:01:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-16T16:01:37Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23886,
    "title": "uhv: config option to control path fragment handling",
    "created_at": "2022-11-08T13:12:33Z",
    "closed_at": "2022-12-16T16:01:36Z",
    "labels": [
      "enhancement",
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23886",
    "body": "*Title*: uhv: config option to control path fragment handling\r\n\r\n*Description*:\r\nUHV currently accepts and validates the fragment component of the request `:path`. There needs to be a new config option that controls this behavior (accept, reject, strip).\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23886/comments",
    "author": "ameily",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-11-09T10:36:55Z",
        "body": "cc @yanavlasov @mattklein123 "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-09T12:01:36Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-16T16:01:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23885,
    "title": "uhv: config option to not reject invalid percent-encoded octets",
    "created_at": "2022-11-08T13:07:23Z",
    "closed_at": "2022-12-16T16:01:34Z",
    "labels": [
      "enhancement",
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23885",
    "body": "*Title*: uhv: config option to not reject invalid percent-encoded octets\r\n\r\n*Description*:\r\nUHV currently rejects all requests with a `:path` that contains an invalid percent-encoded octet. This is a breaking change so we need a new configuration option that controls this behavior.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23885/comments",
    "author": "ameily",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-11-09T10:37:52Z",
        "body": "cc @yanavlasov @mattklein123 "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-09T12:01:33Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-16T16:01:33Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23839,
    "title": "How would one return CORS headers along with a 404 via Envoy?",
    "created_at": "2022-11-04T10:58:57Z",
    "closed_at": "2022-11-08T09:52:49Z",
    "labels": [
      "question",
      "area/cors"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23839",
    "body": "*Title*: *How would one return CORS headers along with a 404 via Envoy?*\r\n\r\n*Description*:\r\nWe are using Envoy as our API Gateway. Every filter chain only has one virtual_host under its route_config and when the traffic gets there, we are using routes to figure out where to send traffic next.\r\n\r\nIf a route has not been defined under the configuration - a 404 will automatically be returned by Envoy (since there’s nowhere to route the traffic to).\r\nWhen a 404 happens - we always receive a CORS error, since there are no CORS headers included with the request.\r\nHere’s an example of our configuration:\r\n```yaml\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_http\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 80 }\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          codec_type: AUTO\r\n          use_remote_address: true\r\n          access_log: \r\n          - name: redirect_access_log\r\n            typed_config: \r\n              \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n              path: /var/log/envoy-proxy/lb/redirect/redirect.log\r\n          route_config:\r\n            name: http_redirect\r\n            response_headers_to_remove: ['x-envoy-upstream-service-time']\r\n            virtual_hosts:\r\n            - name: http_redirect\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match: { prefix: \"/\" }\r\n                redirect: { https_redirect: true }\r\n          http_filters:\r\n          - name: envoy.filters.http.cors\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.cors.v3.Cors\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n\r\n### dynamic block https listener ###\r\n  - name: listener_https_public\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 443 }\r\n    listener_filters:\r\n      - name: \"envoy.filters.listener.tls_inspector\"\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector\r\n    filter_chains:   \r\n    - filter_chain_match:\r\n        server_names: [\"somenvironment-api.somedomain\"]\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n          common_tls_context:\r\n            tls_certificates:\r\n            - certificate_chain: { filename: \"/etc/ssl/somedomain.crt\" } \r\n              private_key: { filename: \"/etc/ssl/somedomain.key\" }\r\n            tls_params:\r\n              tls_minimum_protocol_version: \"TLSv1_2\"\r\n      filters: \r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: somenvironment-api.somedomain\r\n          upgrade_configs:\r\n          - upgrade_type: websocket\r\n          codec_type: AUTO\r\n          use_remote_address: true\r\n          http_filters:\r\n          - name: envoy.filters.http.cors\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.cors.v3.Cors\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n          access_log: \r\n            - name: somenvironment-api.somedomain\r\n              typed_config: \r\n                \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                path: /var/log/envoy-proxy/lb/access/somenvironment-api.somedomain.log\r\n          route_config:\r\n            name: somenvironment-api.somedomain\r\n            response_headers_to_remove: ['x-envoy-upstream-service-time']\r\n            virtual_hosts:\r\n            - name: somenvironment-api.somedomain\r\n              domains: [\"somenvironment-api.somedomain\"]\r\n              cors:\r\n                allow_origin_string_match:\r\n                - safe_regex:\r\n                    google_re2: {}\r\n                    regex: \\*\r\n              routes:\r\n              - match:\r\n                  safe_regex:\r\n                    google_re2: {}\r\n                    regex: \\/API\\/route\r\n                route:\r\n                  cluster:  \"someenvironment-api\" \r\n                  cors:\r\n                    allow_methods: POST\r\n                    allow_headers: \"*\"\r\n                  timeout: 0s\r\n              - match:\r\n                  safe_regex:\r\n                    google_re2: {}\r\n                    regex: \\/OtherAPI\\/route\\/path\r\n                route:\r\n                  cluster:  \"someotherenvironment-api\" \r\n                  cors:\r\n                    allow_methods: GET\r\n                    allow_headers: \"*\"\r\n                  timeout: 0s\r\n              - match:\r\n                  prefix: \"/\"\r\n                direct_response:\r\n                  status: 404\r\n\r\n  - name: someenvironment-api\r\n    connect_timeout: 7s \r\n    circuit_breakers: \r\n      thresholds:\r\n      - priority: 0\r\n        max_connections: 25000\r\n        max_pending_requests: 10000\r\n        max_requests: 25000\r\n        max_retries: 3\r\n    type: STATIC\r\n    lb_policy: ROUND_ROBIN \r\n    load_assignment:\r\n      cluster_name: someenvironment-api\r\n      endpoints: \r\n      - lb_endpoints: \r\n        - endpoint: \r\n            address: \r\n              socket_address:\r\n                address: 10.0.0.175\r\n                port_value: 27022\r\n  - name: someotherenvironment-api\r\n    connect_timeout: 7s \r\n    circuit_breakers: \r\n      thresholds:\r\n      - priority: 0\r\n        max_connections: 25000\r\n        max_pending_requests: 10000\r\n        max_requests: 25000\r\n        max_retries: 3\r\n    type: STATIC\r\n    lb_policy: ROUND_ROBIN \r\n    load_assignment:\r\n      cluster_name: someotherenvironment-api\r\n      endpoints: \r\n      - lb_endpoints: \r\n        - endpoint: \r\n            address: \r\n              socket_address:\r\n                address: 10.0.0.123\r\n                port_value: 18632\r\n```\r\n\r\nWe have tried adding a default direct_response route to the end of the chain as follows :\r\n```yaml\r\n  - match:\r\n      prefix: “/“\r\n    direct_response:\r\n      status: 404\r\n```\r\nUnfortunately, though, that didn’t help.\r\nIs there a way to have Envoy return CORS headers along with a 404?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23839/comments",
    "author": "truehhart",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-11-07T09:19:08Z",
        "body": "cc @wbpcode @daixiang0 "
      },
      {
        "user": "wbpcode",
        "created_at": "2022-11-07T10:31:44Z",
        "body": "@truehhart So you want add cors headers to the 404 response? right?\r\n\r\nAs far as I know, cors filter is only enabled for the requests that will match routes that have a `route` action.\r\n\r\nIf your cors header value is fixed, then maybe you can add `response_headers_to_add` to the last `direct_response` route to achieve this aim."
      },
      {
        "user": "truehhart",
        "created_at": "2022-11-08T09:52:49Z",
        "body": "Hey there,\r\n\r\nWe basically wanted to make sure that when we are deploying new applications under the same virtual host - we would receive a 404 from a front-end application as opposed to the CORS error if a route wasn’t defined for it in Envoy.\r\nWhat was happening is that since envoy returned a blank 404 - there was no header information and browsers would treat it as a CORS failure.\r\n\r\n@wbpcode I do appreciate your advice on the direct response with response_headers_to_add, though it hadn’t worked for me previously. Nonetheless, I’ve dug deeper and found what was the cause of the problem, maybe this would help someone in the future.\r\n\r\nWhat was happening is that we needed 2 different direct responses with CORS headers - one for a preflight request and another one for a 404.\r\n\r\nHere’s what we went for:\r\n\r\n```yaml\r\n              - match:\r\n                  prefix: \"/\"\r\n                  headers:\r\n                    - name: \":method\"\r\n                      exact_match: \"OPTIONS\"\r\n                direct_response:\r\n                  status: 200\r\n                response_headers_to_add:\r\n                  - header: {key: \"access-control-allow-origin\", value: \"*\"}\r\n                    append_action: \"ADD_IF_ABSENT\"\r\n                  - header: {key: \"access-control-allow-methods\", value: \"*\"}\r\n                    append_action: \"ADD_IF_ABSENT\"\r\n                  - header: {key: \"access-control-allow-headers\", value: \"*\"}\r\n                    append_action: \"ADD_IF_ABSENT\"\r\n                  - header: {key: \"access-control-allow-credentials\", value: \"false\"}\r\n                    append_action: \"ADD_IF_ABSENT\"\r\n              - match:\r\n                  prefix: \"/\"\r\n                direct_response:\r\n                  status: 404\r\n                response_headers_to_add:\r\n                  - header: {key: \"access-control-allow-origin\", value: \"*\"}\r\n                    append_action: \"ADD_IF_ABSENT\"\r\n                  - header: {key: \"access-control-allow-methods\", value: \"*\"}\r\n                    append_action: \"ADD_IF_ABSENT\"\r\n                  - header: {key: \"access-control-allow-headers\", value: \"*\"}\r\n                    append_action: \"ADD_IF_ABSENT\"\r\n                  - header: {key: \"access-control-allow-credentials\", value: \"false\"}\r\n                    append_action: “ADD_IF_ABSENT\"\r\n```\r\n\r\nWhenever Envoy receives a request to a route that is not defined - it reaches these two routes at the end of the list. First, it would reply with a 200 to preflight with “allow all but credentials” CORS statement, and then - with a similar 404.\r\n\r\nAgain, very thankful for your insights guys, take care and keep working on an amazing product!"
      }
    ]
  },
  {
    "number": 23762,
    "title": "Set gRPC ext_authz request headers based on incoming request header",
    "created_at": "2022-10-31T20:24:48Z",
    "closed_at": "2022-12-09T00:03:19Z",
    "labels": [
      "question",
      "stale",
      "area/ext_authz"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23762",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: _Set gRPC ext_authz request headers based on incoming request header_\r\n\r\n*Description*:\r\nI have a list of headers that I want to set before sending it to gRPC/HTTP ext_authz filter. I want to set different values to those headers based on the request’s :path. Is there any way to achieve this in Envoy? I know we can add static headers using things like `initial_metadata` but those can not be dependent on the request information. Ideally, I want to have a knob like initial_metadata_based_on_request like the following. Is similar functionality supported in Envoy or is there any workarounds to achieve the same goal? \r\n```\r\nhttp_filters:\r\n  - name: envoy.filters.http.ext_authz\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz\r\n      grpc_service:\r\n        envoy_grpc:\r\n          cluster_name: ext-authz\r\n        initial_metadata_based_on_request: [\r\n\t  - match:\r\n\t      prefix: \"path1\"\r\n\t    initial_metadata:\r\n\t       {key: \"header-based-on-bath\", value: \"value1\" }\r\n      \t  - match:\r\n\t      prefix: \"path2\"\r\n\t    initial_metadata:\r\n\t        {key: \"header-based-on-bath\", value: \"value2\" }\r\n        ]\r\n```\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23762/comments",
    "author": "qixuan-li1",
    "comments": [
      {
        "user": "qixuan-li1",
        "created_at": "2022-11-01T08:13:59Z",
        "body": "I actually figured out a workaround that uses `ExtAuthzPerRoute` + `typed_per_filter_config` and pass custom headers as `context_extensions`. Howerver, our routes match against headers set by the `ext_authz` server, and it seems in such case,  Envoy can't find the destination route and is unable to add those headers. "
      },
      {
        "user": "KBaichoo",
        "created_at": "2022-11-01T19:00:36Z",
        "body": "cc @esmet @pradeepcrao as extension owners whom have context "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-01T20:01:29Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-09T00:03:18Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23693,
    "title": "build error(v1.24.0)",
    "created_at": "2022-10-26T10:24:00Z",
    "closed_at": "2022-11-02T17:58:17Z",
    "labels": [
      "question",
      "contrib"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23693",
    "body": "I found a syntax error build to v1.24.0 envoy\r\n\r\nThe wrong path in \\contrib\\sip_proxy\\filters\\network\\source\\metadata.cc:29\r\n\r\ncode:  // Eg: Route: <sip:test@pcsf-cfed.cncs.svc.cluster.local;role=anch;lr;transport=udp; \\\r\n\r\nPlease check if the symbol “\\” is correct ？\r\n\r\nPlease timely solve, thank you\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23693/comments",
    "author": "jiayoukun",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2022-10-31T15:28:10Z",
        "body": "cc @dorisd0102 @durd07 as extension owner"
      },
      {
        "user": "KBaichoo",
        "created_at": "2022-10-31T15:29:35Z",
        "body": "Is this causing a build error? If so can you also include the error in the description? Thank you!"
      },
      {
        "user": "durd07",
        "created_at": "2022-11-01T12:05:56Z",
        "body": "@jiayoukun \r\nCould you please share the build error description and your build env?\r\nThis is just one symbol in comments. I don't think it would block the building."
      },
      {
        "user": "jiayoukun",
        "created_at": "2022-11-02T08:23:37Z",
        "body": "@durd07 \r\nerror description : \r\nexternal/envoy/contrib/sip_proxy/filters/network/source/metadata.cc:29:3: error: multi-line comment [-Werror=comment]\r\n   29 |   // Eg: Route: <sip:test@pcsf-cfed.cncs.svc.cluster.local;role=anch;lr;transport=udp; \\\r\n        |   ^\r\n-------------------------------------------------------------\r\nI think the annotation symbol is used incorrectly. Please determine whether the symbol “\\” is used correctly.This comment symbol does not exist in the previous line of code.\r\nI actually build istio-proxy because sidecar relies on envoy, so it pulls the latest envoy version. This problem did not occur in older envoy versions 1.23.2, but only in 1.24.0 and 1.25.0-dev.\r\nbuild env: x86_64  ubuntu22.04"
      }
    ]
  },
  {
    "number": 23680,
    "title": "External Processing Filter",
    "created_at": "2022-10-26T06:15:45Z",
    "closed_at": "2022-12-07T20:01:30Z",
    "labels": [
      "question",
      "stale",
      "area/ext_proc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23680",
    "body": "*External Processing Filter for grpc connection*\r\n\r\n>When is the \"External Processing Filter\" going to be covered by a Threat model and ready to use in scenarios where downstream/upstream are not trusted? This filter solves limitations of other filters such as the header/body buffering with Lua and would be really helpful if we can use it.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23680/comments",
    "author": "DimitarHKostov",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2022-10-31T14:15:39Z",
        "body": "cc @gbrail @snowp @pradeepcrao "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-11-30T16:01:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-07T20:01:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23650,
    "title": "Lua filter modify outbound host when envoy acts as sidecar in istio",
    "created_at": "2022-10-25T10:11:45Z",
    "closed_at": "2022-11-07T08:32:36Z",
    "labels": [
      "question",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23650",
    "body": "Hi, I'm new with envoy and istio. \r\nIs it possible to modify outbound request's host, and real request routing to other upstream cluster or host?\r\n\r\n*Description*:\r\nRun with the sample project - bookinfo, and apply custom envoy lua filter with istio.\r\nDesired scenario is that, when productpage calls upstream reviews service, the envoy sidecar in productpage can modify the outbound request's host. The real request is calling \"www.google.com\" domain.\r\n\r\nI wrote an envoyfilter template. In lua script, host is replaced with \"www.google.com:80\" in envoy_on_request. But it is not working, thought productpage still call the real reviews service. I think the desired response should be like 404 or 500, if outbound calls \"www.google.com\" instead.\r\n \r\nIs it possible to fulfill that?\r\n\r\nEnvoyFilter config file is below:\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: test-lua-stable\r\n  namespace: default\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      app: productpage\r\n  configPatches:\r\n    - applyTo: HTTP_FILTER\r\n      match:\r\n        context: SIDECAR_OUTBOUND\r\n        listener:\r\n          filterChain:\r\n            filter:\r\n              name: \"envoy.filters.network.http_connection_manager\"\r\n              subFilter:\r\n                name: \"envoy.filters.http.router\"\r\n      patch:\r\n        operation: INSERT_BEFORE\r\n        value: # lua filter specification\r\n          name: envoy.filters.http.lua\r\n          typed_config:\r\n            \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\"\r\n            inlineCode: |\r\n              function envoy_on_request(request_handle)\r\n                request_handle:logWarn(\"outbound request...\")\r\n                local host = request_handle:headers():get(\"host\")\r\n                if host == \"reviews:9080\" then\r\n                  -- here to modify outbound request's host, but not work\r\n                  request_handle:headers():replace(\"host\", \"www.google.com:80\")\r\n                  request_handle:headers():replace(\":authority\", \"www.google.com:80\")\r\n                end\r\n              end\r\n              \r\n              function envoy_on_response(response_handle)\r\n                response_handle:logWarn(\"outbound response...\")\r\n              end\r\n```\r\n\r\nAny help is appreciated\r\nThank you very much.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23650/comments",
    "author": "AtticusLv",
    "comments": [
      {
        "user": "StarryVae",
        "created_at": "2022-10-26T08:57:31Z",
        "body": "`if host == \"reviews:9080\" then`\r\n\r\nWhere is the value of the `host `obtained? Do you miss some code? OR i think you should get the value of `host` from headers first. "
      },
      {
        "user": "AtticusLv",
        "created_at": "2022-10-27T03:37:42Z",
        "body": "> `if host == \"reviews:9080\" then`\r\n> \r\n> Where is the value of the `host `obtained? Do you miss some code? OR i think you should get the value of `host` from headers first.\r\n\r\nsorry, miss the code before the line. I have modified that.\r\n```\r\nlocal host = request_handle:headers():get(\"host\")\r\n```"
      },
      {
        "user": "KBaichoo",
        "created_at": "2022-10-31T13:49:44Z",
        "body": "cc @mattklein123 @wbpcode "
      },
      {
        "user": "wbpcode",
        "created_at": "2022-10-31T14:20:52Z",
        "body": "sidecar will re-research route table after headers are updated by default. May be there is a default route that will catch all request and route it to original target?\r\nI seem to remember that istio will create something like that."
      },
      {
        "user": "AtticusLv",
        "created_at": "2022-11-01T02:22:35Z",
        "body": "> sidecar will re-research route table after headers are updated by default. May be there is a default route that will catch all request and route it to original target? I seem to remember that istio will create something like that.\r\n\r\nwhere can i find that 're-route' thing? is it the \"PassthroughCluster\"?"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-11-01T02:46:57Z",
        "body": "> > sidecar will re-research route table after headers are updated by default. May be there is a default route that will catch all request and route it to original target? I seem to remember that istio will create something like that.\r\n> \r\n> where can i find that 're-route' thing? is it the \"PassthroughCluster\"?\r\n\r\nIt's implementation detail of lua filter. You can only check it in the code of lua filter.\r\n\r\n> is it the \"PassthroughCluster\"\r\n\r\nIIRC, yeah."
      },
      {
        "user": "AtticusLv",
        "created_at": "2022-11-03T02:22:03Z",
        "body": "Updates!\r\n\r\nI dive into envoy filters and listeners source logic, and find outbound of envoy is listening with only one port.\r\n\r\nIn BookInfo sample, outbound of 'productpage' listens to port '9080'. when modified cluster's port is same with that(9080), replacing host and cluster works in LUA filter.\r\n\r\nBut, with other replaced cluster port, such as 'port=80' in my codes, it doesn't work at all.\r\n\r\nSo the question is,  is there any way to modify outbound listening port?\r\n\r\nAny answer is appreciated."
      },
      {
        "user": "wbpcode",
        "created_at": "2022-11-03T02:37:26Z",
        "body": "Sorry, but I didn't get what's your mean by `modified cluster's port`. It would be better to provide complete example config. "
      },
      {
        "user": "AtticusLv",
        "created_at": "2022-11-03T09:35:27Z",
        "body": "> Sorry, but I didn't get what's your mean by `modified cluster's port`. It would be better to provide complete example config.\r\n\r\nsorry for my fuzzy description. Here's an example istio EnvoyFilter config I use to replace request host, and route the request to other cluster.\r\n\r\n```\r\n apiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: test-lua-filter\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      app: productpage\r\n  configPatches:\r\n    # The first patch adds the lua filter to the listener/http connection manager\r\n    - applyTo: HTTP_FILTER\r\n      match:\r\n        context: SIDECAR_INBOUND\r\n        listener:\r\n          portNumber: 9080\r\n          filterChain:\r\n            filter:\r\n              name: \"envoy.filters.network.http_connection_manager\"\r\n              subFilter:\r\n                name: \"envoy.filters.http.router\"\r\n      patch:\r\n        operation: INSERT_BEFORE\r\n        value: # lua filter specification\r\n          name: envoy.filters.http.lua\r\n          typed_config:\r\n            \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\"\r\n            inlineCode: |\r\n              function envoy_on_request(request_handle)\r\n                request_handle:logWarn(\"inbound request...\")\r\n                local host = request_handle:headers():get(\"HOST\")\r\n                request_handle:logWarn(\"inbound host: \"..host)\r\n              end\r\n              \r\n              function envoy_on_response(response_handle)\r\n                response_handle:logWarn(\"inbound response...\")\r\n              end\r\n    - applyTo: HTTP_FILTER\r\n      match:\r\n        context: SIDECAR_OUTBOUND\r\n        listener:\r\n          portNumber: 9080\r\n          filterChain:\r\n            filter:\r\n              name: \"envoy.filters.network.http_connection_manager\"\r\n              subFilter:\r\n                name: \"envoy.filters.http.router\"\r\n      patch:\r\n        operation: INSERT_BEFORE\r\n        value: # lua filter specification\r\n          name: envoy.filters.http.lua\r\n          typed_config:\r\n            \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\"\r\n            inlineCode: |\r\n              function envoy_on_request(request_handle)\r\n                request_handle:logWarn(\"outbound request...\")\r\n                local host = request_handle:headers():get(\"HOST\")\r\n                local uri = request_handle:headers():get(\":path\")\r\n                request_handle:logWarn(\"host=\"..host..\", uri=\" .. uri)\r\n                if host == \"reviews:9080\" then\r\n                  request_handle:headers():replace(\"HOST\", \"service-else.stable.svc.cluster.local:9080\")\r\n                  request_handle:headers():replace(\":authority\", \"service-else.stable.svc.cluster.local:9080\")\r\n                  request_handle:headers():replace(\":path\", \"/get\")\r\n                  request_handle:logWarn(\"replaced host=\"..request_handle:headers():get(\"HOST\")..\", uri=\" .. request_handle:headers():get(\":path\"))\r\n                end\r\n              end\r\n              \r\n              function envoy_on_response(response_handle)\r\n\r\n              end\r\n```\r\n\r\nLook at the lines \r\n```\r\n        context: SIDECAR_OUTBOUND\r\n        listener:\r\n          portNumber: 9080\r\n```\r\nThis is what I mean ```cluster port```. Not sure the request life logic I dived into is right.\r\n\r\nFirst outbound listens 9080 port, and passes the request to some virtual filters, then lua filter.\r\n\r\nin the lua filter, replace host with a new one, which is \"service-else.stable.svc.cluster.local:9080\". If 'service-else' has the same port '9080' with the sample reviews, request will be re-route to 'service-else'.\r\n\r\nBut, if service-else provide other port, such as 80, even host is replaced with a new one, request is still routed to 'reviews'.\r\n\r\nCheck envoy log, above scenario goes with PassthroughCluster, and matches with 'reviews' service.\r\n\r\nwanna wonder what really happens in my case."
      },
      {
        "user": "wbpcode",
        "created_at": "2022-11-03T09:47:07Z",
        "body": "@AtticusLv When you request is entered envoy, one listener will be selected according to request's original dst (ip and port, especially, the port) to handle this request.\r\nThis selected listener has a route table which contains all reachable services that have **same `port` as the listener**.\r\n\r\nWhen you update you host with lua filter, envoy will re-research the route table for the request. If your new host has a different port, then the request will miss all route entries and downgrade to Passthrough."
      },
      {
        "user": "AtticusLv",
        "created_at": "2022-11-04T03:05:53Z",
        "body": "\r\n> @AtticusLv When you request is entered envoy, one listener will be selected according to request's original dst (ip and port, especially, the port) to handle this request. This selected listener has a route table which contains all reachable services that have **same `port` as the listener**.\r\n> \r\n> When you update you host with lua filter, envoy will re-research the route table for the request. If your new host has a different port, then the request will miss all route entries and downgrade to Passthrough.\r\n\r\nGet it! That makes sense my sample failed to re-route to other service. Thanks for your patient help!\r\n\r\nBTW, due to outbound listener mechanism, same port is used to route the request, how can we re-route the request to a differenct port in outbound filters?"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-11-04T03:26:50Z",
        "body": "> BTW, due to outbound listener mechanism, same port is used to route the request, how can we re-route the request to a differenct port in outbound filters?\r\n\r\nMay be `ignore_port_in_host_matching` or `strip_any_host_port` could help. "
      },
      {
        "user": "AtticusLv",
        "created_at": "2022-11-07T02:47:43Z",
        "body": "> May be ignore_port_in_host_matching or strip_any_host_port could help.\r\n\r\ndo you know how to config those parameters in istio?"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-11-07T07:49:51Z",
        "body": "hmmm，I am not sure, maybe only `EnvoyFilter` could be used to set these configrations by writing a complex patch. 🤔 "
      },
      {
        "user": "AtticusLv",
        "created_at": "2022-11-07T08:32:36Z",
        "body": "> hmmm，I am not sure, maybe only `EnvoyFilter` could be used to set these configrations by writing a complex patch. 🤔\r\n\r\nOK, I'll run to Istio to ask the question relate with configs.\r\n\r\nThank you very much!"
      }
    ]
  },
  {
    "number": 23535,
    "title": "How to generate a token JWT internally in envoyproxy",
    "created_at": "2022-10-18T09:31:37Z",
    "closed_at": "2023-01-04T12:01:30Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23535",
    "body": "Hello.\r\n\r\nI'm trying to make a custom plugin to generate and add in header a JWT token, however, I managed through filters to put a harcode token, but I don't know how I should do to generate one, if with lua through libraries or how, as there is hardly any documentation on generation and signing of JWT token inside envoy.\r\n\r\nthanks\r\n               ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23535/comments",
    "author": "Naski86",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-10-21T01:27:16Z",
        "body": "I think it's more like a quesion about jwt-lib. You can find lots of jwt lib by the google and integrate it with envoy."
      },
      {
        "user": "Naski86",
        "created_at": "2022-11-25T11:24:42Z",
        "body": "but the libs can´t execute inside of the envoy, always call oustisde...."
      },
      {
        "user": "wbpcode",
        "created_at": "2022-11-28T01:42:12Z",
        "body": "sorry, I am not sure what is your mean by `call outside`, couldn't we compile and link the lib with envoy to make it part of envoy binary?\r\nMay be some wrapper code is necessary to make the lib easy to be called. 🤔 "
      },
      {
        "user": "Naski86",
        "created_at": "2022-11-28T07:29:47Z",
        "body": "> sorry, I am not sure what is your mean by `call outside`, couldn't we compile and link the lib with envoy to make it part of envoy binary? May be some wrapper code is necessary to make the lib easy to be called. 🤔\r\n\r\nyes, I mean, the examples that there are for generation, are always external.\r\nI mean, is there an example of a library that generates it, or should I make one in lua/wasm to generate a Jwt token from envoy?\r\nI just want to get an idea if it is possible or not.\r\n\r\nmaybe now I explain better what I want to test"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-11-28T08:53:30Z",
        "body": "I think there is no exist example for it for now. But it's possible. And lua/wasm is a choice to do that."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-28T12:01:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-01-04T12:01:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23532,
    "title": "Need Suggestion on Redis Envoy Java library",
    "created_at": "2022-10-18T05:19:05Z",
    "closed_at": "2022-11-27T08:01:15Z",
    "labels": [
      "question",
      "stale",
      "area/redis"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23532",
    "body": "We are working on POC for Redis envoy to replace our caching solution. We use Micronaut Frameworks but we lettuce is not able to connect t envoy and facing issue. Need to understand how everyone is using Redis envoy with Java or which is best fit Java library .",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23532/comments",
    "author": "darjisanket",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-10-21T00:51:44Z",
        "body": "Sorry, but most developers here are cpp player. 🤣 "
      },
      {
        "user": "wbpcode",
        "created_at": "2022-10-21T00:52:05Z",
        "body": "cc @mattklein123 for redis code owner"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-11-20T04:04:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-11-27T08:01:14Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23487,
    "title": "Question about release: 1.22.3 2aca584: envoy/VERSION.txt => 1.22.3-dev",
    "created_at": "2022-10-13T19:53:54Z",
    "closed_at": "2022-10-14T12:22:56Z",
    "labels": [
      "question",
      "area/release"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23487",
    "body": "We use git submodule like this: git submodule status\r\n2aca584b3bca81622d3b009612f0c7be93eeea34 envoy (v1.22.3)\r\n\r\nBut envoy/VERSION.txt contains:\r\n1.22.3-dev\r\n\r\nThus, the RELEASE full string looks like this:\r\n\"version\": \"b87295fae172bc56584b65fdb05a05c31acd2ffa/1.22.3-dev/Clean/RELEASE/BoringSSL\",\r\n\r\n\"xxx-dev\" can be an issue for us if running in the PRODUCTION even though it's official release.  Anyway to overwrite this version txt?\r\n\r\nWe do have c++ filters internal to us, requests us to a full envoy build.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23487/comments",
    "author": "newwuhan5",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-10-14T08:49:42Z",
        "body": "this was a mistake - release 1.22.5 should have the correct versioning"
      },
      {
        "user": "newwuhan5",
        "created_at": "2022-10-14T12:23:39Z",
        "body": "We will upgrade to use release 1.22.25 then.  Thank you @phlax  for your quick response!"
      }
    ]
  },
  {
    "number": 23417,
    "title": "path rewrite based on endpoint metadata",
    "created_at": "2022-10-09T08:03:24Z",
    "closed_at": "2022-11-17T12:01:44Z",
    "labels": [
      "question",
      "stale",
      "area/router"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23417",
    "body": "*Title*: *path rewrite based on endpoint metadata*\r\n\r\n*Description*:\r\n> I have an upstream cluster with endpoints with different path prefixes. Does envoy support path rewrite based on endpoint metadata?\r\n\r\n> As far as I know, rewrite can be done in route action. Is there any other configuration or extension that supports rewrite based on endpoint metadata?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23417/comments",
    "author": "masquee",
    "comments": [
      {
        "user": "RyanTheOptimist",
        "created_at": "2022-10-10T23:12:43Z",
        "body": "cc: @zuercher\r\n"
      },
      {
        "user": "zuercher",
        "created_at": "2022-10-11T04:29:10Z",
        "body": "I don't believe there's anything available to support this at the moment.\r\n\r\nThis isn't really possible in an HTTP filter, because HTTP filters are applied before an upstream host is selected. Thus the filter has no mechanism by which it can access the endpoint's metadata. Further, any filter can cause Envoy to recompute the selected route (e.g. if the filter has modified headers). This could potentially cause a different route to be selected, with a different target cluster and endpoints. That seems like it would be a problem for your use case.\r\n\r\nOff the top of my head, I expect you'd want an upstream HTTP filter that could modify the request once forwarding began. At present I believe there's only support for upstream network filters: you'd have to handle the HTTP part yourself and that's a pretty big effort."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-11-10T08:01:33Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-11-17T12:01:43Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23414,
    "title": "How to redirect tcp pocket to envoy",
    "created_at": "2022-10-09T03:01:30Z",
    "closed_at": "2022-10-29T09:34:38Z",
    "labels": [
      "question",
      "area/docker"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23414",
    "body": "I deployed envoy **in a container**, and then I added the iptables rules(using **DNAT**) to the host. However, I found that envoy could not find the original address of the tcp packet correctly.\r\n\r\nMy purpose is to make envoy(PROXY_IP=10.32.0.8) proxy all traffic to the client(CLIENT_IP).\r\n\r\nThe rule I add to host is:\r\n```shell\r\n# PROXY_IP=10.32.0.8\r\niptables -t nat -N REDIRECT\r\niptables -t nat -A REDIRECT -p tcp -j DNAT --to-destination $PROXY_IP:15006\r\n\r\niptables -t nat -N INBOUND_PREROUTING\r\niptables -t nat -A INBOUND_PREROUTING -p tcp -s $PROXY_IP -j RETURN\r\niptables -t nat -A INBOUND_PREROUTING -p tcp -j REDIRECT\r\n\r\niptables -t nat -N INBOUND_OUTPUT\r\niptables -t nat -A INBOUND_OUTPUT -p tcp -s $PROXY_IP -j RETURN\r\niptables -t nat -A INBOUND_OUTPUT -p tcp -j REDIRECT\r\n\r\niptables -t nat -A OUTPUT -p tcp -d $CLIENT_IP -j INBOUND_OUTPUT\r\niptables -t nat -A PREROUTING -p tcp -d $CLIENT_IP -j INBOUND_PREROUTING\r\n```\r\n\r\nThen, I send a HTTP request to client.\r\n\r\nThe log of envoy is:\r\n```\r\n2022-10-09T02:51:16.800302Z     debug   envoy filter    original_dst: new connection accepted\r\n2022-10-09T02:51:16.800332Z     trace   envoy filter    original_dst: set destination to 10.32.0.8:15006\r\n```\r\nThis means envoy didn't get correct original dst(what it got is the dst after redirect)\r\n\r\nMy question is **how should I set the redirect rules so that envoy can get the correct original dst**.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23414/comments",
    "author": "He-Jingkai",
    "comments": [
      {
        "user": "RyanTheOptimist",
        "created_at": "2022-10-10T23:14:07Z",
        "body": "cc: @phlax do you have any insight here?"
      },
      {
        "user": "He-Jingkai",
        "created_at": "2022-10-11T02:26:38Z",
        "body": "I read the source code of envoy, this is because envoy uses the `getsockopt(…, SO_ORIGINAL_DST, …)` syscall when obtaining the original dst. This syscall uses the records in the kernel to obtain the original dst before being modified by nat.\r\nThis requires that the location where nat is used to forward network packets and the location where envoy runs must be the same linux kernel or container (that is, linux namespace).Otherwise, it will fail."
      },
      {
        "user": "He-Jingkai",
        "created_at": "2022-10-11T02:31:43Z",
        "body": "I solved this problem using policy routing, by modifying the routing table. This means that network packets are routed to the container where envoy is located, and then nat is used in the container where envoy is located.\r\nI wonder if there is a better solution"
      },
      {
        "user": "phlax",
        "created_at": "2022-10-11T11:33:23Z",
        "body": "> do you have any insight here?\r\n\r\nno immediate insight im afraid - half following the problem/solution"
      }
    ]
  },
  {
    "number": 23303,
    "title": "how to decode grpc in envoy wasm",
    "created_at": "2022-09-29T04:37:21Z",
    "closed_at": "2022-11-06T09:01:42Z",
    "labels": [
      "question",
      "stale",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23303",
    "body": "hello,i am using envoy wasm to get grpc stream ,but grpc is using protobuf ,not plain text ,can i decode it using envoy plugin",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23303/comments",
    "author": "INT2ECALL",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2022-09-29T14:17:52Z",
        "body": "@PiotrSikora for any suggestions\r\n\r\nYou may also try asking in envoy-dev@googlegroups.com or envoy-dev slack channel."
      },
      {
        "user": "INT2ECALL",
        "created_at": "2022-09-30T01:35:49Z",
        "body": "@yanavlasov @timperrett @pjjw @mkbehr help"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-09-30T03:50:16Z",
        "body": "complete proto is necessary. You can read the code of `grpc_json_transcoder` filter which do the translation between grpc and json and could be a ref."
      },
      {
        "user": "INT2ECALL",
        "created_at": "2022-09-30T03:59:54Z",
        "body": "> complete proto is necessary. You can read the code of `grpc_json_transcoder` filter which do the translation between grpc and json and could be a ref.\r\n\r\nthank for reply,but i want to decode raw grpc ,i do not have any proto file,this like protoc decode_raw < text.bin\r\n\r\ndecode data like this:\r\n\r\n1:hello world\r\n2.test\r\n\r\nis there any conveninent way to do this?"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-09-30T04:35:02Z",
        "body": "Then maybe you can write a wasm extension that implement similar feature of `protoc`. Or you can just log all the data to offline files and decode it with `protoc`."
      },
      {
        "user": "INT2ECALL",
        "created_at": "2022-09-30T04:44:37Z",
        "body": "> Then maybe you can write a wasm extension that implement similar feature of `protoc`. Or you can just log all the data to offline files and decode it with `protoc`.\r\n\r\nthank "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-30T08:01:41Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-11-06T09:01:42Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23051,
    "title": "Question regarding filtering metrics",
    "created_at": "2022-09-09T17:12:33Z",
    "closed_at": "2022-10-20T16:03:59Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23051",
    "body": "*Title*: *Question regarding filtering metrics*\r\n\r\n*Description*:\r\n>I'm using the following image `840364872350.dkr.ecr.eu-west-1.amazonaws.com/aws-appmesh-envoy:v1.22.2.1-prod` and after creation of container in AWS ECS, I'd like to test some metrics filters by updating configuration inside the running container. I see there is an `inclusion` / `exclusion` list available from `stats_matcher`.\r\n>\r\n>How to achieve that? What commands or what API calls may I use to update that?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23051/comments",
    "author": "v-Kris",
    "comments": [
      {
        "user": "13567436138",
        "created_at": "2022-09-11T13:05:40Z",
        "body": "it sealms you can config in startup args"
      },
      {
        "user": "v-Kris",
        "created_at": "2022-09-12T08:52:47Z",
        "body": "yes, I can do that, but what after startup and how to update those settings when the envoy instance is running?"
      },
      {
        "user": "snowp",
        "created_at": "2022-09-12T13:47:13Z",
        "body": "Currently this can only be configured in bootstrap, which means that you cannot change this without re-starting the process.  The way this is implemented makes it pretty hard to change this at runtime, as this filter is evaluated when stat handles are created, not when stats are recorded"
      },
      {
        "user": "v-Kris",
        "created_at": "2022-09-13T12:41:58Z",
        "body": "Thank you for your answer @snowp \r\n\r\nI'm checking some filters at startup and I noticed that when I create any filter (`inclusion` or `exclusion` list) I don't see any metrics at AWS CloudWatch. For example, even if I create only the following `exclusion` list:\r\n\r\n```\r\nstats_config:\r\n stats_matcher:\r\n  exclusion_list:\r\n    patterns:\r\n      - safe_regex:\r\n          google_re2: {}\r\n          regex: \".*membership_total$\"\r\n```\r\nit seems that all metrics are filtered out and nothing is going to AWS CloudWatch.\r\n\r\nAm I missing something in that config above?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-13T16:03:49Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-20T16:03:58Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23034,
    "title": "Question",
    "created_at": "2022-09-08T11:40:14Z",
    "closed_at": "2022-10-13T09:19:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23034",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\nAuto-Generate envoy route-config from swaggerfile\r\n\r\nI'm currently working on a project to automatically generate route-config for a \"service-envoy\". I have several ways in mind but can't believe that noone has done that before me. Is there a way to do this the easy way?\r\n\r\n We specifically want to use \"pure envoy\" and not any frameworks because we have some specific requirements to the config-generation. \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23034/comments",
    "author": "Lehp",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2022-09-12T13:38:14Z",
        "body": "It's possible that something like this exists, but I can't think of anything at the top of my mind. There are a ton of different implementations that end up generating a route config based on various inputs, most commonly integrating with Kubernetes APIs. I'd guess that some companies probably do this or something similar internally but haven't open sourced it"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-12T16:05:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "Lehp",
        "created_at": "2022-10-13T09:19:49Z",
        "body": "@snowp  I've done it now. A simple node-application that takes a swagger 2.0 json and generates a simple envoy with route matchers out of it. If anyone wants it dm me\r\n\r\n"
      }
    ]
  },
  {
    "number": 23018,
    "title": "how to debug envoy external dependency.set break point in dependency",
    "created_at": "2022-09-07T13:24:44Z",
    "closed_at": "2022-10-19T20:01:15Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23018",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\nhow to debug envoy external dependency.set break point in dependency\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23018/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2022-09-12T13:57:36Z",
        "body": "This probably comes down to the tooling you're using, in gdb for example you would be able to set a breakpoint based on a symbol name which should achieve this"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-12T16:05:22Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-19T20:01:14Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 23016,
    "title": "ECDS config source from path - discovery response format for resource ",
    "created_at": "2022-09-07T12:30:04Z",
    "closed_at": "2022-10-15T16:01:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23016",
    "body": "Hi folks,\r\nI'm also trying to implement ECDS but the config should come from a file.\r\nI'm struggling to make it work... please help me with how to define the contents of the yaml file.\r\n\r\nthis config:\r\n```\r\nversion_info: \"100\"\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\nfails with:\r\n`Filesystem config update rejected: Unable to unpack as envoy.config.core.v3.TypedExtensionConfig: [type.googleapis.com/envoy.extensions.filters.http.router.v3.Router] `\r\n\r\n```\r\n          http_filters:\r\n          - name: router\r\n            config_discovery:\r\n              type_urls: [\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"]\r\n              config_source:\r\n                path: /usr/local/bin/test-ecds-v1.yml\r\n              default_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23016/comments",
    "author": "pxpnetworks",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2022-09-07T16:18:05Z",
        "body": "The issue is that you need to wrap `Router` message into `TypedExtensionConfig` message. That means something like this:\r\n```yaml\r\nversion_info: \"100\"\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.config.core.v3.TypedExtensionConfig\r\n  name: router\r\n  typed_config:\r\n    \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```"
      },
      {
        "user": "pxpnetworks",
        "created_at": "2022-09-07T17:17:37Z",
        "body": "Thank you @kyessenov , it is accepted now however i tried to add a second http filter (rbac) before router and still get errors loading both rbac and router.\r\nCan you help me with this once again? Thanks!\r\n\r\ncode:\r\n```\r\n- name: envoy.filters.http.rbac\r\n  typed_config:\r\n    \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBAC\"\r\n    shadow_rules:\r\n      action: LOG\r\n      policies:\r\n        \"log\":\r\n          permissions: {any: true}\r\n          principals: {any: true}\r\n```\r\n```\r\nversion_info: \"100\"\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.config.core.v3.TypedExtensionConfig\r\n  name: router\r\n  typed_config:\r\n    \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n```\r\nhttp_filters:\r\n- name: router\r\n  config_discovery:\r\n    type_urls:\r\n    - \"type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBAC\"\r\n    - \"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"\r\n    config_source:\r\n      path: /usr/local/bin/test-ecds-v1.yml\r\n    default_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n\r\nBR,\r\nStoyan"
      },
      {
        "user": "pxpnetworks",
        "created_at": "2022-09-08T08:27:09Z",
        "body": "Figured out I need a separate config_source file for each HTTP filter in the chain, hope that is how it is supposed to work.\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-08T12:01:41Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-15T16:01:53Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22967,
    "title": "FCDS draining logic",
    "created_at": "2022-09-02T05:28:15Z",
    "closed_at": "2022-10-10T00:04:42Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22967",
    "body": "*Title*: *Filter-Chain Discovery Service: configure draining granularity*\r\n\r\n*Description*:\r\nHello All, I am trying to implement FCDS, where I will be having a bunch of access policies inside a filter chain. Much like a listener is having a bunch of filter chains.\r\n\r\nRequirement 1:\r\nWhen a filter-chain update is received through FCDS, for the connection under HTTP connection manager filter, I dont want them to drain at all. Can I turn off the draining somehow? Is there a setting? \r\n\r\nRequirement 2: \r\nI want more granular draining, meaning within a filter chain I will check which policy rule config is updated (much like doing filter chain only update in LDS) and just drain the connections which did hit that modified policy.\r\n\r\nNeed implementation suggestion.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22967/comments",
    "author": "rakeshdatta",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-09-02T15:51:37Z",
        "body": "cc @kyessenov "
      },
      {
        "user": "kyessenov",
        "created_at": "2022-09-02T18:01:32Z",
        "body": "For requirement 1: I think ECDS for RBAC would avoid draining and just apply to new requests.\r\nFor requirement 2: filter chain \"owns\" all its connections. So if you want something more granular (e.g. something decided by an individual filter) you have to look inside the filter chain. One idea would be to allow ECDS apply retro-actively in some cases. That sounds useful for long-living TCP connections, for example, if only a subset of them need to be denied. Another idea is not to reload xDS filter config at all, and manage policies internally within the filter via a separate subscription model."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-02T20:01:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-10T00:04:41Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22933,
    "title": "http2 server push issue",
    "created_at": "2022-09-01T02:13:46Z",
    "closed_at": "2022-10-10T08:04:22Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22933",
    "body": "As far as I know, the downstream connection and upstream connection are decoupled completely. \r\nThere are no map relationship between a downstream and upstream connection.\r\nHowever, if the upstream server want to push something to downstream, how can envoy handle this case ?\r\nIn my understanding, if we need server push, we must use tcp listener instead of http, is it ? \r\n\r\nFor my case, I want to create a  envoy cluster for our kubernetes API servers.  \r\nkubernetes API server works in http2 and the go-client library will use the list-watch mechanism to get object information from kubernetes API server. If go-client sends list request with http2 to envoy, I think envoy can choose an upstream API server to forward the request. But for watch case that means whenever there is some change for k8s object, API server will push back the data to client proactively. How can envoy know which client should go?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22933/comments",
    "author": "zhangbo1882",
    "comments": [
      {
        "user": "daixiang0",
        "created_at": "2022-09-01T06:58:16Z",
        "body": "As I know, Enovy works as data plane, it use xDS protocol to sync those info from control plane, like Istio."
      },
      {
        "user": "zhangbo1882",
        "created_at": "2022-09-02T01:39:16Z",
        "body": "@daixiang0 , I do not get your point. I do not mean the control plane such as xDS. Here I just use envoy as a L7 load balancer to distribute the traffic to the k8s api server. \r\n\r\nclient-go---->envoy------>k8s api server"
      },
      {
        "user": "qiming-007",
        "created_at": "2022-09-03T02:57:29Z",
        "body": "@zhangbo1882 Interesting problem. I think we can consider the server push as another server response. Normally the HTTP request and HTTP response is 1:1 mapping, but after introducing server push, the mapping can be 1:N. Since Envoy can work well in 1:1 request/response mapping, should be no problem when it comes to 1:N. Correct me if I'm wrong."
      },
      {
        "user": "zhangbo1882",
        "created_at": "2022-09-03T05:33:06Z",
        "body": "From my testing, in http2,  the downstream connection and upstream connection is 1:1 map so that the server push can work. In my original understanding, the http2 map will be N:1. In that case, envoy can not know which client it should send message to."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-03T08:01:33Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-10T08:04:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22894,
    "title": "How to specify the python version for bazel build",
    "created_at": "2022-08-30T08:11:54Z",
    "closed_at": "2022-10-09T08:01:34Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22894",
    "body": "Currently envoy bazel build can only support python3.10, so I upgrade my local python version to 3.10.\r\nBut sometimes I will build some older version envoy which only support python3.6.\r\nHow can I specify the python version for bazel build?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22894/comments",
    "author": "zhangbo1882",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-08-30T15:00:42Z",
        "body": "cc @phlax "
      },
      {
        "user": "phlax",
        "created_at": "2022-08-30T15:04:47Z",
        "body": "> Currently envoy bazel build can only support python3.10, so I upgrade my local python version to 3.10.\r\n\r\nwe use the bazel toolchains from `rules_python` so you shouldnt need to install any particular version in your host"
      },
      {
        "user": "zhangbo1882",
        "created_at": "2022-08-31T01:19:40Z",
        "body": "From my understanding, if you build envoy with\r\n ./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.debug.server_only'. It should be ok. \r\nBut if you build envoy with\r\nbazel build   //source/exe:envoy-static --config=docker-clang \r\nIt will meet the python version issue."
      },
      {
        "user": "phlax",
        "created_at": "2022-08-31T11:44:15Z",
        "body": "> But if you build envoy with bazel build\r\n\r\ni generally dont build with `run_envoy_docker` and in fact moving to toolchains was in part to not depend on that environment - running `bazel build/run/etc` is what no longer relies on the host python \r\n\r\ni dont use `--config=docker-clang` but im wondering if its possible you need to update or rebuild a container image somewhere"
      },
      {
        "user": "zhangbo1882",
        "created_at": "2022-09-01T01:54:02Z",
        "body": "I install pyenv so that I can switch to any python version now.\r\n@phlax, which build command will you recommend me to use? "
      },
      {
        "user": "phlax",
        "created_at": "2022-09-02T05:06:13Z",
        "body": "as said - you shouldnt need pyenv at all\r\n\r\nthere are a couple of commands that require host python installed - but any commands that go through bazel should download and use their own python version"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-02T08:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-09T08:01:33Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22812,
    "title": "Endpoint-level path rewrite when these endpoints in the same cluster",
    "created_at": "2022-08-23T08:19:10Z",
    "closed_at": "2022-10-23T08:54:50Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22812",
    "body": "Does endpoint-level path rewrite supported when these endpoints are in the same cluster?  And **these endpoints have different path prefix.**\r\n\r\n*Description*:\r\n> I have some endpoints in the same cluster, but they have different path prefix. `RouteAction` config seems only support cluster-level rewrite config.\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22812/comments",
    "author": "masquee",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-23T00:03:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-23T08:01:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22768,
    "title": "[Help wanted]  the impact of cgroup blkio rate on envoy(pod)",
    "created_at": "2022-08-19T06:05:52Z",
    "closed_at": "2022-10-30T12:01:48Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22768",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\nI use k8s pod to deploy envoy, how to limit pod blkio through Cgroup to get the impact of blkio rate limit on envoy.\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22768/comments",
    "author": "tanjunchen",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-23T04:33:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-23T12:01:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-30T12:01:47Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22680,
    "title": "OcAgent trace exporter: Export() failed: UNIMPLEMENTED: unknown service opencensus.proto.agent.trace.v1.TraceService     ",
    "created_at": "2022-08-12T16:49:14Z",
    "closed_at": "2022-09-21T20:01:27Z",
    "labels": [
      "question",
      "area/tracing",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22680",
    "body": "*Description*:\r\nI try to trace with openCensus  but when i enable ocagent_exporter_enabled , i have this error :\r\nOcAgent trace exporter: Export() failed: UNIMPLEMENTED: unknown service opencensus.proto.agent.trace.v1.TraceService     \r\n\r\nI think i forgot something but i don't know how \r\n\r\n- filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          tracing: \r\n            provider :\r\n              name :  envoy.tracers.opencensus\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.config.trace.v3.OpenCensusConfig\r\n                trace_config:\r\n                    probability_sampler :\r\n                      samplingProbability : 1\r\n                stdout_exporter_enabled : true\r\n                stackdriver_exporter_enabled: false\r\n                zipkin_exporter_enabled: false\r\n                ocagent_exporter_enabled: true\r\n                ocagent_grpc_service :\r\n                    google_grpc :\r\n                      target_uri : otel.xxxx.intra:80\r\n                      stat_prefix: userprefix\r\n                incoming_trace_context: [\"TRACE_CONTEXT\"]\r\n                outgoing_trace_context: [\"TRACE_CONTEXT\"]\r\n\r\n\r\nThx for the help\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22680/comments",
    "author": "jean13360",
    "comments": [
      {
        "user": "jean13360",
        "created_at": "2022-08-12T16:49:35Z",
        "body": "I use envoy with version 1.23.0"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-08-15T02:11:48Z",
        "body": "> OcAgent trace exporter: Export() failed: UNIMPLEMENTED: unknown service opencensus.proto.agent.trace.v1.TraceService\r\n\r\nWhere is the log from? The envoy tracer or `Oc` server?"
      },
      {
        "user": "jean13360",
        "created_at": "2022-08-15T10:22:42Z",
        "body": "> > OcAgent trace exporter: Export() failed: UNIMPLEMENTED: unknown service opencensus.proto.agent.trace.v1.TraceService\r\n> \r\n> Where is the log from? The envoy tracer or `Oc` server?\r\n\r\nthe envoy tracer 😥😥\r\n"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-08-15T12:24:03Z",
        "body": "From the source code of caagent, this is a error return by the server, because the server is not implemented the related grpc service.\r\nSo, could you check the server's address and version first?\r\n\r\n```\r\nvoid Handler::ExportRpcRequest(\r\n    const ::opencensus::proto::agent::trace::v1::ExportTraceServiceRequest\r\n        &request) {\r\n  // TODO: Re-work this to have a single long-running streaming RPC.\r\n  grpc::ClientContext context;\r\n  context.set_deadline(absl::ToChronoTime(absl::Now() + opts_.rpc_deadline));\r\n\r\n  auto stream = opts_.trace_service_stub->Export(&context);\r\n  if (stream == nullptr) {\r\n    std::cerr << \"OcAgent trace exporter: Export() got a NULL stream.\\n\";\r\n    return;\r\n  }\r\n\r\n  if (!stream->Write(request)) {\r\n    std::cerr << \"OcAgent trace exporter: Export() stream broken.\\n\";\r\n  }\r\n\r\n  stream->WritesDone();\r\n  grpc::Status status = stream->Finish();\r\n  if (!status.ok()) {\r\n    std::cerr << \"OcAgent trace exporter: Export() failed: \"                      <----- here here here\r\n              << opencensus::common::ToString(status) << \"\\n\";\r\n  }\r\n}\r\n\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-14T16:01:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-21T20:01:27Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22668,
    "title": "[Question] Some questions regarding the mechanism of service discovery in Envoy proxy",
    "created_at": "2022-08-12T03:33:06Z",
    "closed_at": "2022-09-25T20:01:17Z",
    "labels": [
      "question",
      "stale",
      "investigate",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22668",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: Some questions regarding the mechanism of service discovery in Envoy proxy\r\n\r\n\r\n## Description:\r\n>Describe the issue.\r\n\r\nHey Envoy community, this is Jason from the Shopify Caching platform team. We have some questions regarding the mechanism of Envoy Service Discovery.\r\n\r\n\r\nHere are my questions:\r\n\r\n1. How does Envoy and xDS connect with each other? Do they communicate with each other through long connections? Does Envoy send gRPC requests to xDS periodically? If so, what's the frequency? How could we check if the connection between Envoy and xDS is still going through?\r\n2. Is there a mechanism in Envoy that checks if it can talk with a xDS (like a health check)? Based on what we observed during the incident, do you have some suggestions for us to debug the issue?\r\n\r\nI attached one problematic Envoy debug logs and xDS logs during the incident to help understand the issue. Any reply will be really appreciated. Thank you!\r\n\r\n\r\n## (optional) Relevant Links:\r\n>Any extra documentation required to understand the issue.\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22668/comments",
    "author": "drinkbeer",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-08-15T01:51:45Z",
        "body": "> How does Envoy and xDS connect with each other?\r\n\r\nBy a long connection.\r\n\r\n> Does Envoy send gRPC requests to xDS periodically?\r\n\r\nNo. Envoy will send a stream request at the start of connection.\r\n\r\n> Is there a mechanism in Envoy that checks if it can talk with a xDS?\r\n\r\nAs far as I know, there is no special check for it. But the tcp keepalive can be used to ensure the connection is healthy."
      },
      {
        "user": "wbpcode",
        "created_at": "2022-08-15T01:59:32Z",
        "body": "I have encountered similar problem before. **But I am not sure if it's same issue with yours**.\r\n\r\nMy issue is caused by the the lost of connection contract. I configured a 300s keepalive time for the xds connection to ensure the connection keep active.\r\n\r\n```yaml\r\n     clusters:\r\n      - name: xds_grpc\r\n        upstream_connection_options:\r\n          tcp_keepalive:\r\n            keepalive_time: 300\r\n```"
      },
      {
        "user": "nickamorim",
        "created_at": "2022-08-19T14:41:23Z",
        "body": "@wbpcode how did you come up with the 300s keepalive time? It seems significantly shorter than the Linux default (2h)."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-18T16:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-25T20:01:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22605,
    "title": "Question:  Is envoy capable  to forward udp traffic as tcp traffic?",
    "created_at": "2022-08-08T11:20:07Z",
    "closed_at": "2022-10-15T04:30:26Z",
    "labels": [
      "question",
      "stale",
      "area/udp"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22605",
    "body": "*Description*:\r\n\r\nThe question is envoy capable  to forward udp traffic as tcp traffic?\r\n\r\nMy specific use case is that I'd need to implement DNSOverTLs using envoy. This means, for example,  that the local 127.0.01:53  traffic needs to be reverse proxied to 8.8.8.8:853.\r\n\r\nWhereas I've got an Envoy config that forwards DNS traffic from localhost:53 to some external DNS server like 8.8.8.8:53 (i.e UDP to UDP forwarding) I could not configure Envoy to forward DNS traffic from localhost:53 to an external DnsOverTLS(DoT) server like 8.8.8.8:853 in order to achieve DoT. Since DoT implies that a TCP based TLS handshake happens first on 8.8.8.8:853 before falling back to UDP on port 53 (on the DoT server side) then the issue should be narrowed to convincing Envoy to forward DNS/UDP traffic from local port 53 as TCP traffic to the remote 8.8.8.8:853(i.e. UDP to TCP forwarding ). The first question: is this really possible?\r\n\r\n```\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          protocol: UDP\r\n          address: 127.0.0.53\r\n          port_value: 53\r\n      udp_listener_config:\r\n        downstream_socket_config:\r\n          max_rx_datagram_size: 9000\r\n      filter_chains:\r\n        - filters:\r\n          - name: envoy.filters.network.tcp_proxy\r\n            typed_config: \r\n              '@type': \"type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\"\r\n              stat_prefix: tcp_proxy\r\n              cluster: allbackend_cluster\r\n  clusters:\r\n    - name: allbackend_cluster\r\n      connect_timeout: 1s\r\n      type: STATIC\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n          common_tls_context:\r\n            validation_context:\r\n              trusted_ca:\r\n                filename: /etc/ssl/certs/ca-certificates.crt\r\n      lb_policy: round_robin\r\n      load_assignment:\r\n        cluster_name: allbackend_cluster\r\n        endpoints:\r\n          - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: 8.8.8.8\r\n                    port_value: 853\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22605/comments",
    "author": "livius-ungureanu",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2022-08-08T18:23:21Z",
        "body": "cc @yanjunxiang-google  (as a dns_filter owner)"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-07T20:01:44Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "yanjunxiang-google",
        "created_at": "2022-09-07T21:23:54Z",
        "body": "Sending udp traffic upstream as tcp traffic does not sound like a proxy behavior, but a gateway behavior.  I am not aware of Envoy can support this.  CC @yanavlasov "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-08T00:05:46Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-10-15T04:30:26Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22598,
    "title": "No logs in Envoy Access Logs",
    "created_at": "2022-08-08T01:11:35Z",
    "closed_at": "2022-09-14T20:01:52Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22598",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *There are NO logs in the envoy access logs. How to enable access logging*\r\n\r\n*Description*:\r\n>No logs in Envoy access logs with the following configuration. Please suggest if it has to be enabled somewhere or if any config is missing.\r\n\r\n```    \r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n      path: /Users/path/admin_access.log\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22598/comments",
    "author": "mukundv18",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-08-08T06:18:35Z",
        "body": "@mukundv18 which version of Envoy are you using?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-07T20:01:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-14T20:01:51Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22592,
    "title": "question regarding websocket proxying",
    "created_at": "2022-08-06T06:38:38Z",
    "closed_at": "2022-09-14T20:01:50Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22592",
    "body": "my backend support only websocket with binary data, \r\n\r\ncan envoy accept websocket connection accept text mode from downstream and convert to binary when talking to upstream ?\r\n\r\nsomething like \r\n\r\n<Client Ws: Text> ---<Envoy WS:Binary> ----<Backend>\r\n\r\nThank you\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22592/comments",
    "author": "SunChero",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2022-08-08T17:25:45Z",
        "body": "Envoy supports websockets. Can you clarify what you mean by text and binary? Envoy can terminate HTTP websocket and then open a TCP stream to a backend."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-07T20:01:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-14T20:01:49Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22495,
    "title": "How to use cpu usage as a way of resource monitors？",
    "created_at": "2022-08-02T07:52:02Z",
    "closed_at": "2022-09-08T20:01:29Z",
    "labels": [
      "question",
      "stale",
      "area/overload_manager"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22495",
    "body": "Envoy's resource monitors only supports memory.:\r\n\r\n`extensions.resource_monitors.fixed_heap.v3.FixedHeapConfig`\r\n\r\nSo why doesn't envoy support CPU as a resource monitor? Now I want to use cpu usage as resource monitor, how to do it?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22495/comments",
    "author": "sjtuzbk",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2022-08-02T13:50:35Z",
        "body": "cc @KBaichoo "
      },
      {
        "user": "KBaichoo",
        "created_at": "2022-08-02T14:27:33Z",
        "body": "why doesn't envoy support CPU as a resource monitor?\r\n> No one has gotten around to doing so yet.\r\n\r\nNow I want to use cpu usage as resource monitor\r\n> I'm curious of the problem you're trying to solve with a CPU resource\r\n> monitor, can you elaborate?\r\n\r\nhow to do it?\r\n> Would be creating a resource monitor extension for CPU. You could get this\r\n> information from a few places: /proc, getrusage(), netlink. Whether we want\r\n> this per thread, or for the process is another element to dig into; e.g.\r\n> it's possible that a single worker gets saturated but the others are idle. Whether to do cpu usage per client e.g. to prevent one client from dominating."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-01T16:01:50Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-08T20:01:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22372,
    "title": "How to get SSL socket file descriptor from eBPF uprobes on SSL_{read,write}",
    "created_at": "2022-07-22T21:13:46Z",
    "closed_at": "2022-09-02T04:31:01Z",
    "labels": [
      "question",
      "area/tls",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22372",
    "body": "*Title*: *How to get SSL socket file descriptor from eBPF uprobes on SSL_{read,write}*\r\n\r\n*Description*:\r\n\r\nPixie uses eBPF to capture network traffic. When tracing Envoy, Pixie uses eBPF uprobes on SSL_{read,write} functions to trace the clear-text network traffic. Envoy uses SSL_{read,write} in a way that does not set file descriptor of the socket connection in the SSL struct. Where is the file descriptor get set and how to access them?\r\n\r\n[optional *Relevant Links*:]\r\npx.dev is the Pixie project website.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22372/comments",
    "author": "yzhao1012",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2022-07-26T22:25:23Z",
        "body": "Take this with a grain of salt, since I've not puzzled all this out before, but as I understand the code:\r\n\r\nEnvoy is passing a boringssl `SSL*` to SSL_read/write and that struct is actually a `struct ssl_st*`. That `SSL*` contains `rbio` and `wbio` of type `bssl::UniquePtr<BIO>` which are used for reading and writing.\r\n\r\nEnvoy constructs the `BIO` struct (a `bio_st`) with a `ptr` initialized to an `Envoy::Newtork::IoHandle`, and `method` configured to use Envoy-provided functions for reading and writing (see `source/extensions/transport_sockets/tls/io_handle_bio.cc`).\r\n\r\nNormally, the `IoHandle` is an `Envoy::Network::IoHandleSocketHandeImpl` (`source/common/network/io_socket_handle_impl.h`) which holds the socket's file descriptor and provides methods to operate on it.\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-26T00:03:19Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-02T04:31:00Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "yzhao1012",
        "created_at": "2022-09-16T22:18:14Z",
        "body": "> Take this with a grain of salt, since I've not puzzled all this out before, but as I understand the code:\r\n> \r\n> Envoy is passing a boringssl `SSL*` to SSL_read/write and that struct is actually a `struct ssl_st*`. That `SSL*` contains `rbio` and `wbio` of type `bssl::UniquePtr<BIO>` which are used for reading and writing.\r\n> \r\n> Envoy constructs the `BIO` struct (a `bio_st`) with a `ptr` initialized to an `Envoy::Newtork::IoHandle`, and `method` configured to use Envoy-provided functions for reading and writing (see `source/extensions/transport_sockets/tls/io_handle_bio.cc`).\r\n> \r\n> Normally, the `IoHandle` is an `Envoy::Network::IoHandleSocketHandeImpl` (`source/common/network/io_socket_handle_impl.h`) which holds the socket's file descriptor and provides methods to operate on it.\r\n\r\nThis has been what we found as well. Thanks! "
      }
    ]
  },
  {
    "number": 22247,
    "title": "Error while using TLS",
    "created_at": "2022-07-17T21:31:21Z",
    "closed_at": "2022-08-25T00:03:08Z",
    "labels": [
      "question",
      "area/tls",
      "stale",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22247",
    "body": "I have a problem connecting NodeJS (as client) to Go (as server) when using TLS with envoy. \r\nError: Failed to dial target host \"localhost:50000\" x509: certificate relies on legacy Common Name field, use SANs instead\r\nEnvoy config:\r\n```\r\n\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address: { address: 127.0.0.1, port_value: 50000 }\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                codec_type: auto\r\n                stat_prefix: ingress_http\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains: [ \"*\" ]\r\n                      routes:\r\n                        - name: items_route\r\n                          match: { prefix: \"/items\" }\r\n                          route:\r\n                            cluster: items_service\r\n                            max_stream_duration:\r\n                              grpc_timeout_header_max: 0s\r\n                        - name: auth_route\r\n                          match: { prefix: \"/auth\" }\r\n                          route:\r\n                            cluster: auth_service\r\n                            max_stream_duration:\r\n                              grpc_timeout_header_max: 0s\r\n                      cors:\r\n                        allow_origin_string_match:\r\n                          - prefix: \"*\"\r\n                        allow_methods: GET, PUT, DELETE, POST, OPTIONS\r\n                        allow_headers: authorization,keep-alive,user-agent,cache-control,content-type,content-transfer-encoding,custom-header-1,x-accept-content-transfer-encoding,x-accept-response-streaming,x-user-agent,x-grpc-web,grpc-timeout\r\n                        max_age: \"1728000\"\r\n                        expose_headers: custom-header-1,grpc-status,grpc-message\r\n                http_filters:\r\n                  - name: envoy.filters.http.grpc_web\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.grpc_web.v3.GrpcWeb\r\n                  - name: envoy.filters.http.cors\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.cors.v3.Cors\r\n                  - name: envoy.filters.http.router\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n          transport_socket:\r\n            name: envoy.transport_sockets.tls\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n              common_tls_context:\r\n                validation_context:\r\n                  trusted_ca:\r\n                    filename: ca.crt\r\n                tls_certificates:\r\n                  - certificate_chain:\r\n                      filename: server.crt\r\n                    private_key:\r\n                      filename: server.key\r\n\r\n  clusters:\r\n    - name: auth_service\r\n      connect_timeout: 0.25s\r\n      type: logical_dns\r\n      http2_protocol_options: { }\r\n      lb_policy: round_robin\r\n      load_assignment:\r\n        cluster_name: cluster_0\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: 127.0.0.1\r\n                      port_value: 50051\r\n    - name: items_service\r\n      connect_timeout: 0.25s\r\n      type: logical_dns\r\n      http2_protocol_options: { }\r\n      lb_policy: round_robin\r\n      load_assignment:\r\n        cluster_name: cluster_0\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: 127.0.0.1\r\n                      port_value: 50052\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22247/comments",
    "author": "KevinBdev",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-07-18T07:59:46Z",
        "body": "cc @ggreenway "
      },
      {
        "user": "ggreenway",
        "created_at": "2022-07-18T16:33:31Z",
        "body": "The error message `certificate relies on legacy Common Name field, use SANs instead` is describing what is in your TLS server certificate. Envoy serves this unmodified, so the fix is to get a new certificate that uses SANs instead of CommonName."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-17T20:01:41Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-25T00:03:08Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22195,
    "title": "how to config QuicDownstreamTransport ,got connection refused",
    "created_at": "2022-07-14T05:27:54Z",
    "closed_at": "2022-08-24T12:01:47Z",
    "labels": [
      "question",
      "stale",
      "area/quic"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22195",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\nset env\r\n\r\n```\r\nkubectl set env -n istio-system deploy istiod PILOT_ENABLE_QUIC_LISTENERS=true\r\nkubectl rollout restart deploy -n istio-system istiod\r\n```\r\n\r\n\r\n\r\n```\r\n          - name: http3\r\n            port: 443\r\n            targetPort: 8443\r\n            protocol: UDP\r\n```\r\n\r\n\r\n\r\ncreate secret\r\n\r\nhttpbin.cnf\r\n\r\n```\r\n[req]\r\ndefault_bits       = 2048\r\nprompt             = no\r\ndistinguished_name = req_distinguished_name\r\nreq_extensions     = san_reqext\r\n\r\n[ req_distinguished_name ]\r\ncountryName         = IN\r\nstateOrProvinceName = KA\r\norganizationName    = QuicCorp\r\n\r\n[ san_reqext ]\r\nsubjectAltName      = @alt_names\r\n\r\n[alt_names]\r\nDNS.0   = httpbin.quic-corp.com\r\n```\r\n\r\n```\r\nopenssl req -x509 -sha256 -nodes -days 365 -newkey rsa:4096 -subj \"/C=IN/ST=KA/O=QuicCorp\" -keyout quiccorp-ca.key -out quiccorp-ca.crt\r\n\r\nopenssl req -out httpbin.csr -newkey rsa:2048 -nodes -keyout httpbin.key -config httpbin.cnf\r\n\r\nopenssl x509 -req -days 365 -CA quiccorp-ca.crt -CAkey quiccorp-ca.key -set_serial 0 -in httpbin.csr -out httpbin.crt -extfile httpbin.cnf -extensions san_reqext\r\n\r\n```\r\n\r\n```\r\nkubectl -n istio-system create secret tls httpbin-cred --key=httpbin.key --cert=httpbin.crt\r\n```\r\n\r\n\r\n\r\nef-filter_chains-quic_downstream.yaml\r\n\r\nkubectl apply -f ef-filter_chains-quic_downstream.yaml -n istio-system\r\n\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: transport-socket\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n  - applyTo: LISTENER\r\n    match:\r\n      context: GATEWAY\r\n    patch:\r\n      operation: ADD\r\n      value:\r\n        name: proxy\r\n        address:\r\n          socket_address:\r\n            protocol: UDP\r\n            address: 0.0.0.0\r\n            port_value: 8443\r\n        filter_chains:\r\n        - filters:\r\n          - name: \"envoy.filters.network.http_connection_manager\"\r\n            typed_config:\r\n              \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\"\r\n              stat_prefix: ingress_proxy\r\n              codec_type: \"HTTP3\"\r\n              route_config:\r\n                name: route_a\r\n                virtual_hosts:\r\n                - name: envoy_cyz\r\n                  domains:\r\n                  - \"*\"\r\n                  routes:\r\n                  - name: testroute\r\n                    match: \r\n                      prefix: /\r\n                    direct_response:\r\n                      status: 200\r\n                      body: \r\n                        inline_string: \"prefix\"\r\n                    response_headers_to_add:\r\n                    - header:\r\n                        key: \"alt-svc\"\r\n                        value: \"h3=:443; ma=86400\"\r\n                      append: true\r\n              http_filters:\r\n              - name: \"envoy.filters.http.router\"\r\n                typed_config:\r\n                  \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"\r\n              http3_protocol_options:\r\n                quic_protocol_options:\r\n                  max_concurrent_streams: 100\r\n                  initial_stream_window_size: 65536 \r\n                  initial_connection_window_size: 65536 \r\n                  num_timeouts_to_trigger_port_migration: 1\r\n                  connection_keepalive:\r\n                    max_interval: 60s\r\n                    initial_interval: 30s\r\n                override_stream_error_on_invalid_http_message: true\r\n                allow_extended_connect: true\r\n          transport_socket: \r\n            name: \"envoy.transport_sockets.quic\"\r\n            typed_config:\r\n              \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.quic.v3.QuicDownstreamTransport\"\r\n              downstream_tls_context:\r\n                common_tls_context:\r\n                  alpn_protocols:\r\n                  - h3\r\n                  tls_certificate_sds_secret_configs:\r\n                  - name: \"kubernetes://httpbin-cred\"\r\n                    sds_config:\r\n                      ads: { }\r\n                      resource_api_version: \"V3\"\r\n                require_client_certificate: \r\n        traffic_direction: \"OUTBOUND\"\r\n        udp_listener_config:\r\n          downstream_socket_config: { }\r\n          quic_options: { }\r\n        reuse_port: true\r\n```\r\n```\r\n[root@node01 transport-socket]# telnet 192.168.229.128 30508\r\nTrying 192.168.229.128...\r\ntelnet: connect to address 192.168.229.128: Connection refused\r\n[root@node01 transport-socket]# kubectl get svc -n istio-system\r\nNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP       PORT(S)                                                                      AGE\r\ngrafana                ClusterIP   10.68.126.229   <none>            3000/TCP                                                                     23d\r\nistio-egressgateway    ClusterIP   10.68.67.157    <none>            80/TCP,443/TCP                                                               21d\r\nistio-ingressgateway   NodePort    10.68.181.46    192.168.229.189   15021:32661/TCP,80:30563/TCP,443:30508/UDP,31400:30639/TCP,15443:31702/TCP   21d\r\nistiod                 ClusterIP   10.68.180.246   <none>            15010/TCP,15012/TCP,443/TCP,15014/TCP                                        21d\r\njaeger-collector       ClusterIP   10.68.177.123   <none>            14268/TCP,14250/TCP                                                          23d\r\nkiali                  ClusterIP   10.68.50.247    <none>            20001/TCP,9090/TCP                                                           23d\r\nprometheus             ClusterIP   10.68.89.176    <none>            9090/TCP                                                                     30d\r\ntracing                NodePort    10.68.82.93     <none>            80:30628/TCP                                                                 23d\r\nzipkin                 ClusterIP   10.68.223.115   <none>            9411/TCP                                                                     23d\r\n```\r\n\r\ningressgateway has no logs\r\n\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22195/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-07-18T08:18:53Z",
        "body": "cc @RyanTheOptimist "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-17T12:01:41Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-24T12:01:47Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22193,
    "title": "Unable to unpack as envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext",
    "created_at": "2022-07-14T04:52:59Z",
    "closed_at": "2022-08-24T12:01:46Z",
    "labels": [
      "question",
      "stale",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22193",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: transport-socket\r\n  namespace: istio-system \r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n  - applyTo: NETWORK_FILTER\r\n    match:\r\n      listener:\r\n        #name: 0.0.0.0_8080  \r\n        portNumber: 8080\r\n        filterChain:\r\n          filter:\r\n            name: \"envoy.filters.network.http_connection_manager\"\r\n    patch:\r\n      operation: MERGE\r\n      value:\r\n        name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\"\r\n          codec_type: AUTO\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: http.8080\r\n            virtual_hosts:\r\n            - name: “*.8080”\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  caseSensitive: true\r\n                  headers:\r\n                  - name: :method\r\n                    safeRegexMatch:\r\n                      googleRe2: {}\r\n                      regex: G.*T\r\n                  prefix: /\r\n                route:\r\n                  cluster: my-productpage\r\n  - applyTo: CLUSTER\r\n    patch:\r\n      operation: ADD     \r\n      value:\r\n        name: my-productpage\r\n        type: STRICT_DNS\r\n        connect_timeout: 10s\r\n        lb_policy: ROUND_ROBIN\r\n        load_assignment:\r\n          cluster_name: my-productpage\r\n          endpoints:\r\n          - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                   socket_address:\r\n                    address: productpage.istio.svc.cluster.local\r\n                    port_value: 9080\r\n        transport_socket:\r\n          name: envoy.transport_sockets.tap\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tap.v3.Tap\r\n            common_config:\r\n              admin_config:\r\n                config_id: test_config_id\r\n              static_config:\r\n                match_config:\r\n                  any_match: true\r\n                output_config:\r\n                  sinks:\r\n                  - file_per_tap:\r\n                      path_prefix: /var/log/tap\r\n                    #format: JSON_BODY_AS_BYTES             \r\n            transport_socket:\r\n                name: envoy.transport_sockets.tls\r\n                typed_config:\r\n                  '@type': type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n                  common_tls_context:\r\n                    alpn_protocols:\r\n                    - h2\r\n                    - http/1.1\r\n                    combined_validation_context:\r\n                      default_validation_context:\r\n                        match_subject_alt_names:\r\n                        - prefix: spiffe://cluster.local/\r\n                      validation_context_sds_secret_config:\r\n                        name: ROOTCA\r\n                        sds_config:\r\n                          api_config_source:\r\n                            api_type: GRPC\r\n                            grpc_services:\r\n                            - envoy_grpc:\r\n                                cluster_name: sds-grpc\r\n                            set_node_on_first_message_only: true\r\n                            transport_api_version: V3\r\n                          initial_fetch_timeout: 0s\r\n                          resource_api_version: V3\r\n                    tls_certificate_sds_secret_configs:\r\n                    - name: default\r\n                      sds_config:\r\n                        api_config_source:\r\n                          api_type: GRPC\r\n                          grpc_services:\r\n                          - envoy_grpc:\r\n                              cluster_name: sds-grpc\r\n                          set_node_on_first_message_only: true\r\n                          transport_api_version: V3\r\n                        initial_fetch_timeout: 0s\r\n                        resource_api_version: V3\r\n                    tls_params:\r\n                      cipher_suites:\r\n                      - ECDHE-ECDSA-AES256-GCM-SHA384\r\n                      - ECDHE-RSA-AES256-GCM-SHA384\r\n                      - ECDHE-ECDSA-AES128-GCM-SHA256\r\n                      - ECDHE-RSA-AES128-GCM-SHA256\r\n                      - AES256-GCM-SHA384\r\n                      - AES128-GCM-SHA256\r\n                      tls_minimum_protocol_version: TLSv1_2\r\n                  require_client_certificate: true\r\n            \r\n```\r\n\r\n2022-07-14T04:51:27.195560Z     warn    ads     ADS:CDS: ACK ERROR istio-ingressgateway-7845746bf7-xpg7k.istio-system-119 Internal:Error adding/updating cluster(s) my-productpage: Unable to unpack as envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext: [type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext] {\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22193/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "soulxu",
        "created_at": "2022-07-15T07:32:48Z",
        "body": "```\r\ntyped_config:\r\n                  '@type': type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n```\r\n\r\nYou write this as `DownstreamTlsContext`, have you try to use `UpstreamTlsContext`?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-17T12:01:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-24T12:01:45Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22192,
    "title": "connection_failure,QUIC_NO_ERROR_with_details:_Closed_by_application",
    "created_at": "2022-07-14T04:45:40Z",
    "closed_at": "2022-08-24T12:01:44Z",
    "labels": [
      "question",
      "stale",
      "area/quic"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22192",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: transport-socket\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n  - applyTo: NETWORK_FILTER\r\n    match:\r\n      listener:\r\n        #name: 0.0.0.0_8080  \r\n        portNumber: 8080\r\n        filterChain:\r\n          filter:\r\n            name: \"envoy.filters.network.http_connection_manager\"\r\n    patch:\r\n      operation: MERGE\r\n      value:\r\n            name: envoy.filters.network.http_connection_manager\r\n            typed_config:\r\n              \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\"\r\n              codec_type: AUTO\r\n              stat_prefix: ingress_http\r\n              route_config:\r\n                name: http.8080\r\n                virtual_hosts:\r\n                - name: “*.8080”\r\n                  domains:\r\n                  - \"*\"\r\n                  routes:\r\n                  - match:\r\n                      caseSensitive: true\r\n                      headers:\r\n                      - name: :method\r\n                        safeRegexMatch:\r\n                          googleRe2: {}\r\n                          regex: G.*T\r\n                      prefix: /\r\n                    route:\r\n                      host_rewrite_literal: firebase.google.cn\r\n                      cluster: service_google\r\n  - applyTo: CLUSTER\r\n    patch:\r\n        operation: ADD\r\n        value:\r\n            name: service_google\r\n            connect_timeout: 30s\r\n            type: LOGICAL_DNS\r\n            dns_lookup_family: V4_ONLY\r\n            lb_policy: ROUND_ROBIN\r\n            load_assignment:\r\n              cluster_name: service_google\r\n              endpoints:\r\n              - lb_endpoints:\r\n                - endpoint:\r\n                    address:\r\n                      socket_address:\r\n                        address: firebase.google.cn\r\n                        port_value: 443\r\n            typed_extension_protocol_options:\r\n              envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n                \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n                explicit_http_config:\r\n                  http3_protocol_options: {}\r\n                common_http_protocol_options:\r\n                  idle_timeout: 1s\r\n            transport_socket:\r\n              name: envoy.transport_sockets.quic\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.quic.v3.QuicUpstreamTransport\r\n                upstream_tls_context:\r\n                  sni: firebase.google.cn\r\n```\r\n\r\n[2022-07-14T04:43:39.602Z] \"GET / HTTP/1.1\" 503 UF upstream_reset_before_response_started{connection_failure,QUIC_NO_ERROR_with_details:_Closed_by_application} - \"-\" 0 168 1002 - \"172.20.0.0\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36\" \"c4f3727f-4f57-9c9d-8ebb-23bef0827159\" \"firebase.google.cn\" \"120.253.253.226:443\" service_google - 172.20.2.67:8080 172.20.0.0:28115 - -\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22192/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-07-18T08:19:37Z",
        "body": "cc @RyanTheOptimist "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-17T12:01:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-24T12:01:43Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22166,
    "title": "how to get route_metadata in wasm?",
    "created_at": "2022-07-13T09:02:46Z",
    "closed_at": "2022-08-25T16:01:31Z",
    "labels": [
      "question",
      "stale",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22166",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\npackage main\r\n\r\nimport (\r\n        \"github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm\"\r\n        \"github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types\"\r\n)\r\n\r\nfunc main() {\r\n        proxywasm.SetVMContext(&vmContext{})\r\n}\r\n\r\ntype vmContext struct {\r\n        // Embed the default VM context here,\r\n        // so that we don't need to reimplement all the methods.\r\n        types.DefaultVMContext\r\n}\r\n\r\n// Override types.DefaultVMContext.\r\nfunc (*vmContext) NewPluginContext(contextID uint32) types.PluginContext {\r\n        return &pluginContext{}\r\n}\r\n\r\ntype pluginContext struct {\r\n        // Embed the default plugin context here,\r\n        // so that we don't need to reimplement all the methods.\r\n        types.DefaultPluginContext\r\n}\r\n\r\n// Override types.DefaultPluginContext.\r\nfunc (*pluginContext) NewHttpContext(contextID uint32) types.HttpContext {\r\n        return &httpHeaders{contextID: contextID}\r\n}\r\n\r\ntype httpHeaders struct {\r\n        // Embed the default http context here,\r\n        // so that we don't need to reimplement all the methods.\r\n        types.DefaultHttpContext\r\n        contextID uint32\r\n}\r\n\r\n// Override types.DefaultHttpContext.\r\nfunc (ctx *httpHeaders) OnHttpRequestHeaders(numHeaders int, endOfStream bool) types.Action {\r\n        path :=[]string{\"plugin_vm_id\"}\r\n        data,err := proxywasm.GetProperty(path) \r\n        if err != nil {\r\n                proxywasm.LogCritical(\"failed to get property\")\r\n        }\r\n\r\n        proxywasm.LogCriticalf(\"property(%s): %s\", \"plugin_vm_id\",string(data))\r\n        \r\n        path=[]string{\"route_metadata\",\"envoy.lb\",\"canary\"}\r\n        data,err = proxywasm.GetProperty(path)\r\n        \r\n        if err != nil {\r\n                proxywasm.LogCritical(\"failed to get property\")\r\n        }\r\n\r\n        proxywasm.LogCriticalf(\"property(%s): %s\", \"route_metadata\",string(data))\r\n        return types.ActionContinue\r\n}\r\n\r\n\r\n\r\n```\r\n\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: metadata\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      app: productpage\r\n  priority: 10\r\n  configPatches:\r\n  - applyTo: NETWORK_FILTER\r\n    match:\r\n      context: SIDECAR_INBOUND\r\n      listener:\r\n        filterChain:\r\n          destinationPort: 9080\r\n          filter:\r\n            name: \"envoy.filters.network.http_connection_manager\"\r\n    patch:\r\n      operation: REPLACE\r\n      value:\r\n              name: envoy.filters.network.http_connection_manager\r\n              typedConfig:\r\n                '@type': type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: \"inbound_0.0.0.0_9080\"\r\n                route_config:\r\n                  name: test\r\n                  virtual_hosts:\r\n                  - name: test\r\n                    domains:\r\n                    - \"*\"\r\n                    routes:\r\n                    - name: testroute\r\n                      match: \r\n                        prefix: /\r\n                      metadata:\r\n                        filter_metadata:\r\n                          \"envoy.lb\": \r\n                            canary: true\r\n                      route:\r\n                        cluster: inbound|9080||\r\n                        timeout: \"0s\"\r\n                        max_stream_duration:\r\n                          max_stream_duration: \"0s\"\r\n                http_filters:\r\n                - name: \"envoy.filters.http.router\"\r\n                  typed_config:\r\n                    \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"\r\n```\r\n\r\n```\r\n2022-07-13T09:00:06.471846Z     critical        envoy wasm      wasm log my_plugin attribute_id attribute_vm_id: property(plugin_vm_id): attribute_vm_id\r\n2022-07-13T09:00:06.471868Z     critical        envoy wasm      wasm log my_plugin attribute_id attribute_vm_id: failed to get property\r\n2022-07-13T09:00:06.471871Z     critical        envoy wasm      wasm log my_plugin attribute_id attribute_vm_id: property(route_metadata): \r\n```\r\n\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22166/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2022-07-13T16:18:56Z",
        "body": "It should be available as `{\"route_metadata\"}` attribute in a serialized proto form. I'm not familiar with Go SDK though."
      },
      {
        "user": "13567436138",
        "created_at": "2022-07-14T00:49:32Z",
        "body": "how to unserialized proto"
      },
      {
        "user": "13567436138",
        "created_at": "2022-07-14T02:29:53Z",
        "body": "```\r\npackage main\r\n\r\nimport (\r\n        \"github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm\"\r\n        \"github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types\"\r\n        \"github.com/golang/protobuf/proto\"\r\n)\r\n\r\nfunc main() {\r\n        proxywasm.SetVMContext(&vmContext{})\r\n}\r\n\r\ntype vmContext struct {\r\n        // Embed the default VM context here,\r\n        // so that we don't need to reimplement all the methods.\r\n        types.DefaultVMContext\r\n}\r\n\r\n// Override types.DefaultVMContext.\r\nfunc (*vmContext) NewPluginContext(contextID uint32) types.PluginContext {\r\n        return &pluginContext{}\r\n}\r\n\r\ntype pluginContext struct {\r\n        // Embed the default plugin context here,\r\n        // so that we don't need to reimplement all the methods.\r\n        types.DefaultPluginContext\r\n}\r\n\r\n// Override types.DefaultPluginContext.\r\nfunc (*pluginContext) NewHttpContext(contextID uint32) types.HttpContext {\r\n        return &httpHeaders{contextID: contextID}\r\n}\r\n\r\ntype httpHeaders struct {\r\n        // Embed the default http context here,\r\n        // so that we don't need to reimplement all the methods.\r\n        types.DefaultHttpContext\r\n        contextID uint32\r\n}\r\n\r\ntype Metadata struct{\r\n        filter_metadata []map[string]Struct\r\n}\r\n\r\n// Override types.DefaultHttpContext.\r\nfunc (ctx *httpHeaders) OnHttpRequestHeaders(numHeaders int, endOfStream bool) types.Action {\r\n        path :=[]string{\"plugin_vm_id\"}\r\n        data,err := proxywasm.GetProperty(path) \r\n        if err != nil {\r\n                proxywasm.LogCritical(\"failed to get property\")\r\n        }\r\n\r\n        proxywasm.LogCriticalf(\"property(%s): %s\", \"plugin_vm_id\",string(data))\r\n        \r\n        path=[]string{\"route_metadata\"}\r\n        data,err = proxywasm.GetProperty(path)\r\n        \r\n        if err != nil {\r\n                proxywasm.LogCritical(\"failed to get property\")\r\n        }\r\n        var data2 Metadata=Metadata{};\r\n        err = proto.Unmarshal(data, &data2)\r\n        proxywasm.LogCriticalf(\"property(%s): %s\", \"route_metadata\",string(data2.(type)))\r\n        return types.ActionContinue\r\n}\r\n```\r\n\r\ngot\r\n```\r\n../../../../go/pkg/mod/google.golang.org/protobuf@v1.26.0/internal/impl/pointer_unsafe.go:141:16: unsafe.Sizeof(unsafe.Pointer(nil)) - unsafe.Sizeof((MessageState literal)) (constant -4 of type uintptr) overflows uintptr\r\n```"
      },
      {
        "user": "Neil-Boyle",
        "created_at": "2022-07-19T08:22:55Z",
        "body": "what you need:\r\n\r\nvalue, err := proxywasm.GetProperty([]string{\"route_metadata\", \"filter_metadata\", \"envoy.lb\", \"canary\"})\r\n\r\nref: github.com/json-iterator/tinygo\r\n\r\nto unmarshal json value if necessary.\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-18T12:01:47Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-25T16:01:30Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22162,
    "title": "transaction mode redis",
    "created_at": "2022-07-13T08:29:30Z",
    "closed_at": "2022-09-26T16:02:16Z",
    "labels": [
      "question",
      "stale",
      "area/redis"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22162",
    "body": "why does the redis transaction mode is not supported by envoy?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22162/comments",
    "author": "Jacobien1",
    "comments": [
      {
        "user": "adisuissa",
        "created_at": "2022-07-18T01:47:34Z",
        "body": "cc  @weisisea @mattklein123"
      },
      {
        "user": "mattklein123",
        "created_at": "2022-07-18T14:21:21Z",
        "body": "Because in general there is no good way to perform a transaction across multiple sharded redis hosts using a stateless proxy like Envoy."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-17T16:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-19T12:01:37Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-26T16:02:16Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22158,
    "title": "why this config is no healthy upstreams",
    "created_at": "2022-07-13T03:41:30Z",
    "closed_at": "2022-08-20T04:11:10Z",
    "labels": [
      "question",
      "stale",
      "area/dns"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22158",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: cluster\r\n  namespace: istio-system\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n  - applyTo: NETWORK_FILTER\r\n    match:\r\n      listener:\r\n        #name: 0.0.0.0_8080  \r\n        portNumber: 8080\r\n        filterChain:\r\n          filter:\r\n            name: \"envoy.filters.network.http_connection_manager\"\r\n    patch:\r\n      operation: MERGE\r\n      value:\r\n        name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\"\r\n          codec_type: AUTO\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: http.8080\r\n            virtual_hosts:\r\n            - name: “*.8080”\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  caseSensitive: true\r\n                  headers:\r\n                  - name: :method\r\n                    safeRegexMatch:\r\n                      googleRe2: {}\r\n                      regex: G.*T\r\n                  prefix: /\r\n                route:\r\n                  cluster: productpage_cluster\r\n  - applyTo: CLUSTER\r\n    patch:\r\n      operation: ADD\r\n      value:\r\n          name: productpage_cluster\r\n          type: STRICT_DNS\r\n          connect_timeout: 10s\r\n          lb_policy: ROUND_ROBIN\r\n          dns_refresh_rate: 5000ms\r\n          dns_failure_refresh_rate:\r\n            base_interval: 5000ms\r\n            max_interval: 50000ms\r\n          respect_dns_ttl: true\r\n          dns_lookup_family: AUTO\r\n          #dns_resolvers:已废弃\r\n          use_tcp_for_dns_lookups: true\r\n          dns_resolution_config:\r\n            resolvers:\r\n            - socket_address:\r\n                address: 10.68.0.2\r\n                port_value: 53\r\n            dns_resolver_options:\r\n              use_tcp_for_dns_lookups: true\r\n              no_default_search_domain: false\r\n          load_assignment:\r\n            cluster_name: productpage_cluster\r\n            endpoints:\r\n            - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                     socket_address:\r\n                      address: productpage.istio.svc.cluster.local\r\n                      port_value: 9080\r\n```\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22158/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-07-13T05:29:43Z",
        "body": "You can try run a `ping` command to ensure the dns resolver is working as your expect."
      },
      {
        "user": "wbpcode",
        "created_at": "2022-07-13T05:30:51Z",
        "body": "cc @yanavlasov"
      },
      {
        "user": "13567436138",
        "created_at": "2022-07-14T03:12:30Z",
        "body": "```\r\n[root@node01 ~]# telnet  10.68.0.2 53\r\nTrying 10.68.0.2...\r\nConnected to 10.68.0.2.\r\nEscape character is '^]'.\r\nsdf\r\nsfd\r\nsdfConnection closed by foreign host.\r\n[root@node01 ~]# sdf\r\nbash: sdf: command not found...\r\n[root@node01 ~]# ping  10.68.0.2\r\nPING 10.68.0.2 (10.68.0.2) 56(84) bytes of data.\r\n64 bytes from 10.68.0.2: icmp_seq=1 ttl=64 time=0.041 ms\r\n64 bytes from 10.68.0.2: icmp_seq=2 ttl=64 time=0.024 ms\r\n64 bytes from 10.68.0.2: icmp_seq=3 ttl=64 time=0.023 ms\r\n^C\r\n--- 10.68.0.2 ping statistics ---\r\n3 packets transmitted, 3 received, 0% packet loss, time 2075ms\r\nrtt min/avg/max/mdev = 0.023/0.029/0.041/0.009 ms\r\n```"
      },
      {
        "user": "13567436138",
        "created_at": "2022-07-14T03:15:08Z",
        "body": "```\r\n[root@node01 ~]# kubectl exec -n istio -it busybox-88847f6-l9t8r /bin/sh\r\nkubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.\r\n/ # ping 10.68.0.2\r\nPING 10.68.0.2 (10.68.0.2): 56 data bytes\r\n64 bytes from 10.68.0.2: seq=0 ttl=64 time=0.066 ms\r\n64 bytes from 10.68.0.2: seq=1 ttl=64 time=0.059 ms\r\n64 bytes from 10.68.0.2: seq=2 ttl=64 time=0.060 ms\r\n64 bytes from 10.68.0.2: seq=3 ttl=64 time=0.058 ms\r\n64 bytes from 10.68.0.2: seq=4 ttl=64 time=0.057 ms\r\n64 bytes from 10.68.0.2: seq=5 ttl=64 time=0.060 ms\r\n64 bytes from 10.68.0.2: seq=6 ttl=64 time=0.058 ms\r\n^X64 bytes from 10.68.0.2: seq=7 ttl=64 time=0.059 ms\r\n^C\r\n--- 10.68.0.2 ping statistics ---\r\n8 packets transmitted, 8 packets received, 0% packet loss\r\nround-trip min/avg/max = 0.057/0.059/0.066 ms\r\n/ # telnet 10.68.0.2 53\r\nConnected to 10.68.0.2\r\nsfds\r\nConnection closed by foreign host\r\n/ # \r\n```"
      },
      {
        "user": "13567436138",
        "created_at": "2022-07-14T03:17:16Z",
        "body": "/ # nslookup productpage.istio.svc.cluster.local 10.68.0.2\r\nServer:         10.68.0.2\r\nAddress:        10.68.0.2:53\r\n\r\n\r\n*** Can't find productpage.istio.svc.cluster.local: No answer"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-13T04:05:46Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-20T04:11:09Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22152,
    "title": "Is it possible to use variable environment in envoy configfile ?",
    "created_at": "2022-07-12T20:04:10Z",
    "closed_at": "2022-08-20T04:11:08Z",
    "labels": [
      "question",
      "stale",
      "area/cors"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22152",
    "body": "I want to use environment variable in cors to specify the source adresse.\r\nBut I don't know if it is possible.\r\n\r\nRegards,",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22152/comments",
    "author": "fredericpougnault",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-07-13T05:49:18Z",
        "body": "What do you mean by `source adresse`?\r\n\r\nAs I know, It's not supported to create cors config by the env for now. In most of cases, the config of envoy is created by a control plane and then send to the envoy. Could you achieve your target by the control plane?"
      },
      {
        "user": "fredericpougnault",
        "created_at": "2022-07-13T07:26:15Z",
        "body": "By source address I mean allow_origin_string_match.\r\nI'll generate the config by control plane."
      },
      {
        "user": "fredrischter",
        "created_at": "2022-07-13T23:32:18Z",
        "body": "Envoy itself don't do it. You need to replace env variables in the config file using something like envsubst. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-13T00:03:17Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-20T04:11:06Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 22119,
    "title": "How envoy handle old and new quic connections when reload？",
    "created_at": "2022-07-12T02:38:17Z",
    "closed_at": "2022-08-20T20:01:36Z",
    "labels": [
      "question",
      "stale",
      "area/quic"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22119",
    "body": "how envoy handle old and new quic connections when reload.\r\nwhen reload, if envoy can keep quic connections open to complete old connections.\r\nand if new connections can not still be handled by old workers.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22119/comments",
    "author": "yfming",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-07-13T06:57:18Z",
        "body": "cc @alyssawilk @danzh2010 "
      },
      {
        "user": "danzh2010",
        "created_at": "2022-07-13T16:01:15Z",
        "body": "What kind of reload? If the config update requires tearing down the old listener, currently QUIC still not support both old and new listeners listening on the same port. The old one will abruptly tear down its connections.\r\nIf the config update doesn't requires tearing down the old listener, quic support in-place filter chain update."
      },
      {
        "user": "yfming",
        "created_at": "2022-07-14T06:25:07Z",
        "body": "restart."
      },
      {
        "user": "yfming",
        "created_at": "2022-07-14T06:26:57Z",
        "body": "how envoy handle old and new quic connections when restart.\r\nwhen restart, if envoy can keep quic connections open to complete old connections.\r\nand if new connections can not still be handled by old workers."
      },
      {
        "user": "danzh2010",
        "created_at": "2022-07-14T14:23:48Z",
        "body": "Envoy won't wait for the old connections to complete nor handle new connection in old listeners. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-13T16:01:17Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-20T20:01:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21999,
    "title": "how to send connect method connection?",
    "created_at": "2022-07-03T06:13:34Z",
    "closed_at": "2022-08-11T08:01:42Z",
    "labels": [
      "question",
      "stale",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21999",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\nhow to send connect method connection?\r\n*Description*:\r\n>Describe the issue.\r\n[root@node01 httpmatch]# curl -X CONNECT 192.168.229.128:30639 -I\r\nHTTP/1.1 400 Bad Request\r\ncontent-length: 11\r\ncontent-type: text/plain\r\ndate: Sun, 03 Jul 2022 06:13:15 GMT\r\nserver: envoy\r\nconnection: close\r\n\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: match\r\n  namespace: istio-system\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n  - applyTo: LISTENER\r\n    patch:\r\n      operation: ADD\r\n      value:\r\n        name: listener_0\r\n        address:\r\n          socket_address:\r\n            protocol: TCP\r\n            address: 0.0.0.0\r\n            port_value: 31400\r\n        filter_chains:\r\n        - filters:\r\n          - name: envoy.filters.network.http_connection_manager\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n              stat_prefix: ingress_http\r\n              route_config:\r\n                name: local_route\r\n                virtual_hosts:\r\n                - name: local_service\r\n                  domains:\r\n                  - \"*\"\r\n                  routes:\r\n                  - match:\r\n                      connect_matcher:\r\n                        {}\r\n                    route:\r\n                      cluster: service_google\r\n                      upgrade_configs:\r\n                      - upgrade_type: CONNECT\r\n                        connect_config:\r\n                          {}\r\n              http_filters:\r\n              - name: envoy.filters.http.router\r\n                typed_config:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n              http_protocol_options: {}\r\n              upgrade_configs:\r\n              - upgrade_type: CONNECT\r\n  - applyTo: CLUSTER\r\n    patch:\r\n      operation: ADD\r\n      value: \r\n        name: service_google\r\n        connect_timeout: 0.25s\r\n        type: LOGICAL_DNS\r\n        dns_lookup_family: V4_ONLY\r\n        lb_policy: ROUND_ROBIN\r\n        load_assignment:\r\n          cluster_name: service_google\r\n          endpoints:\r\n          - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: www.baidu.com\r\n                    port_value: 443\r\n```\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21999/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-07-05T07:34:08Z",
        "body": "Typically, when the client requests to HTTPS server by the proxy, the client may send a connect request first. If you want send it by yourself, may be you can try to use the telnet and send the raw content by yourself."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-04T08:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-11T08:01:41Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21952,
    "title": "Is it possible to configure Envoy to not start listening until all upstreams is healthy?",
    "created_at": "2022-06-29T20:38:11Z",
    "closed_at": "2022-08-11T08:01:30Z",
    "labels": [
      "question",
      "stale",
      "area/listener"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21952",
    "body": "*Title*: *Is it possible to configure Envoy to not start listening until all upstreams is healthy?*\r\n\r\n*Description*:\r\nWe configured a static upstream cluster with active health check enabled, and we want Envoy to not start listening util the upstream is healthy.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21952/comments",
    "author": "wlhee",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-07-05T01:35:11Z",
        "body": "Sorry, it seems that's not supported for now."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-04T04:14:29Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-11T08:01:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21906,
    "title": "retry request using envoy filter",
    "created_at": "2022-06-27T13:50:39Z",
    "closed_at": "2022-08-04T20:01:38Z",
    "labels": [
      "question",
      "stale",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21906",
    "body": "my use case is to retry sending request from envoy_on_response method when response code >= 5XX received .\r\nPlease help\r\n\r\nThanks\r\nAsis",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21906/comments",
    "author": "Asisranjan",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-06-28T13:01:51Z",
        "body": "Could you just use the `httpcall` in the `envoy_on_response`?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-28T16:01:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-04T20:01:38Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21888,
    "title": "Dynamic metadata match not working",
    "created_at": "2022-06-25T02:51:19Z",
    "closed_at": "2022-08-04T16:01:36Z",
    "labels": [
      "question",
      "no stalebot",
      "area/router"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21888",
    "body": "*Title*: *Dynamic metadata match not working*\r\n\r\n*Description*:\r\nI'm using the lua filter to analize some headers and set some dynamic metadata. That metadata is used in `config.route.v3.RouteMatch.dynamic_metadata` for route matching.\r\nUsing the following configuration with `curl localhost:8080 -i` then the result is:\r\n```\r\nHTTP/1.1 404 Not Found\r\ndate: Sat, 25 Jun 2022 02:42:15 GMT\r\nserver: envoy\r\ncontent-length: 0\r\n```\r\nbut if I uncomment these lines in lua method\r\n```\r\n--request_handle:headers():add(\"my-header\", \"my-header-value\")\r\n--request_handle:headers():remove(\"my-header\")\r\n```\r\nthen the result for the same curl is\r\n\r\n```\r\nHTTP/1.1 200 OK\r\ncontent-length: 9\r\ncontent-type: text/plain\r\ndate: Sat, 25 Jun 2022 02:45:59 GMT\r\nserver: envoy\r\n\r\nIt works!\r\n```\r\n\r\n*Config*:\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8080\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          http_filters:\r\n          - name: envoy.filters.http.lua\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n              inline_code: |\r\n                function envoy_on_request(request_handle)\r\n                  -- UNCOMMENT NEXT 2 LINES TO START WORKING\r\n\r\n                  --request_handle:headers():add(\"my-header\", \"my-header-value\")\r\n                  --request_handle:headers():remove(\"my-header\")\r\n\r\n                  request_handle:streamInfo():dynamicMetadata():set(\"envoy.filters.http.router\", \"somepath\", \"somevalue\")\r\n                end\r\n          - name: envoy.filters.http.router\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                  dynamic_metadata:\r\n                  - filter: \"envoy.filters.http.router\"\r\n                    path:\r\n                    - key: somepath\r\n                    value:\r\n                      string_match:\r\n                        prefix: somevalue\r\n                direct_response:\r\n                  body:\r\n                    inline_string: 'It works!'\r\n                  status: 200\r\n```\r\n\r\n*Repro steps*:\r\n1. Download envoy 1.21.1\r\n2. Run envoy with config file from above\r\n3. run `curl localhost:8080 -i` and expect status code 404\r\n4. stop envoy\r\n5. Uncomment the lines mentioned in the description\r\n6. repeat step 2) and 3) but now expect status code 200\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21888/comments",
    "author": "andresmargalef",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-06-28T10:32:42Z",
        "body": "The requests cannot match any route entry at first because the `dynamic_metadata` match. Although you update the dynamic metadata by the `Lua` code, the initial match result (null) will not be changed. So `404 Not Found` will be return by the Envoy.\r\n\r\nBut if you uncomment the headers updating code, the Lua filter will clear the initial match result and force Envoy to make a re-match.  So `200 OK` will be return by the Envoy."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-28T12:02:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-08-04T16:01:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "tobeyOguney",
        "created_at": "2024-01-08T16:03:19Z",
        "body": "@wbpcode this is also an issue with a go plugin implementation.\r\n\r\nMy understanding is that this is a bug.\r\n\r\nCan this issue be re-opened?"
      },
      {
        "user": "wbpcode",
        "created_at": "2024-01-09T02:51:06Z",
        "body": "@tobeyOguney This is not a bug. It is by design. The go plugin need to support route cache clearing if you want to match the dynamic metadata after the go plugin updated the dynamic metadata."
      },
      {
        "user": "tobeyOguney",
        "created_at": "2024-01-09T05:40:07Z",
        "body": "Thanks for clarifying that. @wbpcode \r\n\r\nDid further digging in the [documentation](envoyproxy.io/docs/envoy/latest/intro/life_of_a_request#http-filter-chain-processing) and found where the route caching mechanism is referenced.\r\n\r\nIssue can remain closed."
      }
    ]
  },
  {
    "number": 21685,
    "title": "Per upstream host stats",
    "created_at": "2022-06-13T22:20:43Z",
    "closed_at": "2023-10-25T21:13:25Z",
    "labels": [
      "question",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21685",
    "body": "*Title*: Is there any way to expose per-host upstream stats\r\n\r\n*Description*:\r\nIn our deployment, we care a lot about long tail stats, including P99 latency, bad/slow hosts causing frequent timeouts, etc. Currently it seems we only have aggregate upstream stats. I am wondering is there a way to expose per-host upstream stats, which could help us quickly pinpoint the bad/slow hosts. \r\n\r\nMore specifically, 1. would this feature require code changes? 2. would this feature be generally useful or beneficial for the community too? If yes, we can contribute back to the community. 3. what is the general guideline for adding this change, e.g., should we follow the way how tagged stats is implemented today? \r\n\r\nAppreciate any feedback or comments! \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21685/comments",
    "author": "vandyvilla",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-06-14T03:26:18Z",
        "body": "cc @jmarantz "
      },
      {
        "user": "jmarantz",
        "created_at": "2022-06-14T04:55:47Z",
        "body": "The host stats are kept by default and are visible in admin /clusters. AFAIK there is no way to stream those out to the stats sinks; you can only see snapshots on admin."
      },
      {
        "user": "jmarantz",
        "created_at": "2022-06-14T04:57:55Z",
        "body": "I suspect the main reason they were originally designed to be kept separate from stats is that there can be an extremely large number of backend hosts in some configurations so the cardinality would stress several aspects of the stats system (memory, cpu overhead). They way they are held in the host system is much lower overhead than other stats.\r\n"
      },
      {
        "user": "vandyvilla",
        "created_at": "2022-06-14T16:35:04Z",
        "body": "Thanks Joshua. Our upstream is Redis Cluster with no more than 200 master nodes. Would that cause a cardinality problem? If not, is it possible to export those host stats from admin to stats sinks, any pointers on how to achieve this? \r\n\r\nThanks! "
      },
      {
        "user": "jmarantz",
        "created_at": "2022-06-14T17:29:02Z",
        "body": "It would not be a cardinality problem for you. But you would definitely need that to be under API, runtime, or flag-flip control; it would blow out memory for large existing deployments if enabled by default.\r\n\r\nif you can find a memory neutral way to conditionalize the stats block in each host to point to real stats that would be ideal. Then you could run Envoy in \"export_host_stats\" mode but for your use-case it would not be too expensive.\r\n\r\nI don't have specific code pointers handy but I suspect source/common/upstream is the place to start looking for how these host-stats get populated. I think @antoniovicente refactored that code to make it even leaner, I'd guess 2 years ago, and he may have a more specific pointer."
      },
      {
        "user": "jmarantz",
        "created_at": "2022-06-15T02:03:24Z",
        "body": "I looked at this again and I think I have a better idea. I think it might not be too hard to have the stats sinks simply iterate through the cluster/hosts hierarchy to enumerate those stats to stats sinks and the /admin endpoint. But I would still only do this if a flag was set.\r\n\r\nFor the admin /stats endpoint you could enable this by query-param, e.g. `/stats?hosts` though I'm not sure if that's any more convenient than just visiting /clusters. I guess what you mainly want are the stats sinks and it might not be too hard to have a runtime option that toggles inclusion of the host stats."
      },
      {
        "user": "jmarantz",
        "created_at": "2022-06-15T02:19:01Z",
        "body": "Here are some code pointers if you want to try this:\r\n * envoy/stats/primitive_stats.h - definition of the very low-cost stats held in each host\r\n * envoy/upstream/host_description.h - ALL_HOST_STATS defines the stats held in each host\r\n * Upstream::HostDescription::stats().counters() returns a vector of name/value pairs for each counter. Note that the name is not elaborated -- it's just the leaf name of the stat, so it would have to be combined in the sink with the names of the cluster and host to disambiguate.\r\n * Upstream::HostDescription::stats().gauges() -- same thing but for gauges\r\n * source/server/server.cc -- contains the code to generate a MetricSnapshot, which is provided to sinks. You'd have to add new elements to MetricSnapshotImpl to hold the name/value pairs, organized by host. Sinks would each have to be modified to read this data.\r\n * source/extensions/stat_sinks/common/statsd/statsd.cc, function UdpStatsdSink::flush would be an example of one of the sink functions that would have to be modified to include the host-owned gauges and counters in its iteration.\r\n\r\nThis is a pretty medium-to-large sized project but I think it's not too risky or complicated. The main issue is that you'd want to, at least eventually, make sure that all known sinks behave the same. And the technical challenge is you'd probably want to avoid bloating memory while populating it. For example, you probably want to share the string storage when populating the host metrics into the MetricSnapshotImpl, and have a lazy way of elaborating the cluster/host-name as the sink is generating its output.\r\n\r\nI've probably given as much detail as you can digest but why don't you take a look at that and see if you want to go further with this."
      },
      {
        "user": "mattklein123",
        "created_at": "2022-06-15T16:14:06Z",
        "body": "Per @jmarantz the reason this is not done by default is similar to the reason we don't output per-route stats by default. Stats are expensive and can easily blow out your backend TSDB without careful control. \r\n\r\nI think it's fine to have optional output of per host stats, either globally or per cluster, but this cannot be enabled by default."
      },
      {
        "user": "vandyvilla",
        "created_at": "2022-06-22T17:07:36Z",
        "body": "Thanks @jmarantz and @mattklein123 ! I will take a look at the pointers and see if we want to proceed. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-22T20:01:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-30T00:02:37Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21672,
    "title": "how to share data between inbound wasm and outbound wasm？",
    "created_at": "2022-06-13T06:38:52Z",
    "closed_at": "2022-06-18T05:15:25Z",
    "labels": [
      "question",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21672",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\nhow to share data between inbound wasm and outbound wasm？\r\n\r\n*Description*:\r\n>Describe the issue.\r\nI recieved a request header in inbound wasm and I want to set the same request header in outbound wasm？\r\nhow to do this，any code will be thankful、\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21672/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-06-14T04:00:31Z",
        "body": "cc @PiotrSikora"
      }
    ]
  },
  {
    "number": 21636,
    "title": "[Question] get namespace and pod info in accesslog",
    "created_at": "2022-06-09T07:25:18Z",
    "closed_at": "2022-07-20T04:15:11Z",
    "labels": [
      "question",
      "stale",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21636",
    "body": "can i append the information of pod and namespace in access log",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21636/comments",
    "author": "shuxnhs",
    "comments": [
      {
        "user": "pxpnetworks",
        "created_at": "2022-06-09T09:15:01Z",
        "body": "I think it is possible using the Access Log command operator %ENVIRONMENT(X):Z%\r\n\r\nExample:\r\n\r\nnode_name: %ENVIRONMENT(NODE_NAME)%\r\n\r\nand on the k8s deployment define the Env Vars like this:\r\n```\r\nenv:\r\n- name: NODE_NAME\r\nvalueFrom:\r\n\tfieldRef:\r\n\tfieldPath: spec.nodeName\r\n- name: POD_NAME\r\nvalueFrom:\r\n\tfieldRef:\r\n\tfieldPath: metadata.name\r\n- name: POD_NAMESPACE\r\nvalueFrom:\r\n\tfieldRef:\r\n\tfieldPath: metadata.namespace\r\n```"
      },
      {
        "user": "shuxnhs",
        "created_at": "2022-06-09T11:51:00Z",
        "body": "> ine the Env Vars lik\r\n\r\nthanks for your answers but i got ` Not supported field in StreamInfo: ENVIRONMENT(XXX)` on the access log"
      },
      {
        "user": "daixiang0",
        "created_at": "2022-06-10T00:58:48Z",
        "body": "This feature is added by 1.22, could you ensure the version you use is newer than 1.22?"
      },
      {
        "user": "shuxnhs",
        "created_at": "2022-06-10T06:38:40Z",
        "body": "oh，my envoy version is 1.18.3."
      },
      {
        "user": "daixiang0",
        "created_at": "2022-06-13T00:19:45Z",
        "body": "That is reasonable to me, please try a newer version."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-13T04:13:43Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-20T04:15:11Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21625,
    "title": "Edge Proxy Latency SLI",
    "created_at": "2022-06-08T16:28:18Z",
    "closed_at": "2022-07-16T20:01:28Z",
    "labels": [
      "question",
      "stale",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21625",
    "body": "*Title*: *Edge Proxy Latency SLI*\r\n\r\n*Description*:\r\nI'm hoping to define p50, p95, etc latency SLIs for an Envoy edge proxy, where latency is defined as the processing time within Envoy. In my head, this would be `downstream_time - sum(upstream_time...)` reported as a histogram (across all upstreams). \r\n\r\nGranted, no such statistic exists, but is there a reasonable way to approximate it with the current instrumentation? As is, the lack of a latency histogram aggregating all upstream requests seems to suggest no. \r\n\r\nAlternatively, is this something that a WASM filter could achieve with reasonable fidelity? We already have other WASM filters (the latency of which is in part what we're looking to make sure we capture).",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21625/comments",
    "author": "jwilner",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-09T16:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-16T20:01:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21564,
    "title": "Envoy proxy (Error) MOVED on an Elasticache Cluster mode on",
    "created_at": "2022-06-02T23:05:17Z",
    "closed_at": "2022-07-12T04:26:09Z",
    "labels": [
      "question",
      "stale",
      "area/redis"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21564",
    "body": "We have an Elasticache that we're using and now we're trying to use envoy to have some pool of connections to the cluster but, there is an issue when using Redis-CLI or Redis-py which is the famous error: (Error) MOVED. I've tried muñltiple changes but nothing seems to be working. sometimes it works well but other times it shows that error. any help with the issue, please.\r\n\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 0.0.0.0\r\n      port_value: 1936\r\nstatic_resources:\r\n  listeners:\r\n  - name: redis_listener\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 1999\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.redis_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.redis_proxy.v3.RedisProxy\r\n          stat_prefix: redis_proxy\r\n          settings:\r\n            op_timeout: 4s\r\n            enable_redirection: true\r\n          prefix_routes:\r\n            catch_all_route:\r\n              cluster: redis_cluster\r\n  clusters:\r\n    name: redis_cluster\r\n    type: STRICT_DNS\r\n    connect_timeout: 10s\r\n    dns_lookup_family: V4_ONLY\r\n    connect_timeout: 4s \r\n    load_assignment:\r\n      cluster_name: redis_cluster\r\n      endpoints:\r\n        lb_endpoints:\r\n          endpoint:\r\n            address:\r\n              socket_address:\r\n                address: my-redis\r\n                port_value: 6379 \r\n    upstream_connection_options:\r\n      tcp_keepalive:\r\n        keepalive_interval: 5\r\n        keepalive_probes: 15\r\n        keepalive_time: 30\r\n    upstream_config:\r\n      name: envoy.upstreams.tcp.generic\r\n      typed_config: \r\n        \"@type\": type.googleapis.com/envoy.extensions.upstreams.tcp.generic.v3.GenericConnectionPoolProto\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21564/comments",
    "author": "Jacobien1",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2022-06-04T14:08:01Z",
        "body": "cc @weisisea "
      },
      {
        "user": "ramaraochavali",
        "created_at": "2022-06-05T04:02:58Z",
        "body": "For redis cluster, you need to use lb_policy as `CLUSTER_PROVIDED` and cluster_type as `envoy.clusters.redis`. \r\n\r\nSample config\r\n\r\n```\r\n        lb_policy: CLUSTER_PROVIDED\r\n        cluster_type:\r\n          name: envoy.clusters.redis\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/google.protobuf.Struct\r\n            value:\r\n              cluster_refresh_rate: 360s\r\n              cluster_refresh_timeout: 4s\r\n              \r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-05T04:14:16Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-12T04:26:08Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21455,
    "title": "SSL Error connecting to a redis cluster 'Elasticache'",
    "created_at": "2022-05-26T10:33:49Z",
    "closed_at": "2022-07-03T16:01:45Z",
    "labels": [
      "question",
      "stale",
      "area/redis"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21455",
    "body": "I'm trying to connect to my Redis cluster 'Elasticache' through the envoy. when I do it through the TCP filter, it works fine, but the connection pool doesn't work. on the other hand, I've tested the Redis filter and it works perfectly on my local with the connection pool, but when I try to implement it on AWS it doesn't work 'I'm using Redis-py as a client ' and it gives me the following error:\r\n```\r\n    sslsock = context.wrap_socket(sock, server_hostname=self.host)\r\n  File \"/usr/lib/python3.7/ssl.py\", line 423, in wrap_socket\r\n    session=session\r\n  File \"/usr/lib/python3.7/ssl.py\", line 870, in _create\r\n    self.do_handshake()\r\n  File \"/usr/lib/python3.7/ssl.py\", line 1139, in do_handshake\r\n    self._sslobj.do_handshake()\r\nssl.SSLError: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1091)\r\n```\r\nconfig file is :\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 0.0.0.0\r\n      port_value: 1936\r\nstatic_resources:\r\n  listeners:\r\n  - name: redis_listener\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 1999\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.redis_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.redis_proxy.v3.RedisProxy\r\n          stat_prefix: redis_proxy\r\n          settings:\r\n            op_timeout: 4s\r\n          prefix_routes:\r\n            catch_all_route:\r\n              cluster: redis_cluster\r\n  clusters:\r\n    name: redis_cluster\r\n    type: STRICT_DNS\r\n    connect_timeout: 10s\r\n    dns_lookup_family: V4_ONLY\r\n    connect_timeout: 4s \r\n    load_assignment:\r\n      cluster_name: redis_cluster\r\n      endpoints:\r\n        lb_endpoints:\r\n          endpoint:\r\n            address:\r\n              socket_address:\r\n                address: my-redis\r\n                port_value: 6379 \r\n    upstream_connection_options:\r\n      tcp_keepalive:\r\n        keepalive_interval: 5\r\n        keepalive_probes: 15\r\n        keepalive_time: 30\r\n    upstream_config:\r\n      name: envoy.upstreams.tcp.generic\r\n      typed_config: \r\n        \"@type\": type.googleapis.com/envoy.extensions.upstreams.tcp.generic.v3.GenericConnectionPoolProto\r\n```\r\n\r\nThank you in advance.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21455/comments",
    "author": "Jacobien1",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2022-05-27T04:55:56Z",
        "body": "This listener is not configured for TLS, so if the client is expecting that, it will produce the message you describe."
      },
      {
        "user": "htuch",
        "created_at": "2022-05-27T04:57:21Z",
        "body": "@msukalski @HenryYYang @weisisea are CODEOWNERS and can help with any followups here."
      },
      {
        "user": "Jacobien1",
        "created_at": "2022-05-27T08:16:50Z",
        "body": "> envoy.filters.network.redis_proxy\r\n\r\nRedis works with TCP traffic, and as I saw online this listener is used for Redis databases, and I've tried it on my local Redis database and it worked but it shows this error for Elasticache 'Redis on AWS'"
      },
      {
        "user": "Jacobien1",
        "created_at": "2022-05-27T15:57:49Z",
        "body": "any solution please???"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-26T16:01:16Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-03T16:01:44Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21454,
    "title": "Envoy %GRPC_STATUS_NUMBER% got String instead of Number",
    "created_at": "2022-05-26T09:13:31Z",
    "closed_at": "2022-07-03T12:01:13Z",
    "labels": [
      "question",
      "stale",
      "area/access_log",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21454",
    "body": "Hi, I have use Envoy with custom logging with `%GRPC_STATUS_NUMBER%` but I got String instead of Number\r\n\r\n*Config*\r\n```json\r\n\"status_code\": \"%GRPC_STATUS_NUMBER%\",\r\n```\r\n\r\n*Result*\r\n```json\r\n\"status_code\": \"OK\",\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21454/comments",
    "author": "mrnonz",
    "comments": [
      {
        "user": "daixiang0",
        "created_at": "2022-05-27T00:12:45Z",
        "body": "Could you share which version of Envoy you use? "
      },
      {
        "user": "mrnonz",
        "created_at": "2022-05-27T02:50:55Z",
        "body": "@daixiang0 I use this version\r\n\r\n`envoy  version: 4ce93dc3ace00ae9108b179d0afaceac13f4602a/1.17.4/Modified/RELEASE/BoringSSL`\r\n\r\nI think this my misunderstand; I think if this command operator not set will be error instead of return the main string. I will try to upgrade envoy to >= `1.22.0` since command operator ``%GRPC_STATUS_NUMBER%`` added.\r\n\r\nThank you"
      },
      {
        "user": "daixiang0",
        "created_at": "2022-05-27T04:03:17Z",
        "body": "Sure, you are welcome, hope it helps you."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-26T08:01:22Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-03T12:01:12Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21389,
    "title": "Envoy Proxy as Redis Proxy, how to authenticate?",
    "created_at": "2022-05-20T01:32:10Z",
    "closed_at": "2022-06-29T04:16:33Z",
    "labels": [
      "question",
      "stale",
      "area/redis"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21389",
    "body": "*Title*: *The password passed via Redis Client has to be passed as Bearer token to external authorisation service OPA. Is it possible? If yes, how?*\r\n\r\n*Description*:\r\nWe are using Envoy Proxy as Proxy for Redis. All the communication to Redis is via Envoy proxy. \r\nWhen using 'redis-cli', we send the password with '-a' option. Is it possible send this as Bearer token to OPA (used for external authorisation) to be validated against an AD?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21389/comments",
    "author": "mukundv18",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2022-05-23T02:49:13Z",
        "body": "@yanavlasov you've been looking at this filter recently, any thoughts? @weisisea as CODEOWNER."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-22T04:10:55Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-29T04:16:32Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21350,
    "title": "Prepared Clusters",
    "created_at": "2022-05-18T06:15:43Z",
    "closed_at": "2022-06-24T20:01:20Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21350",
    "body": "Hi all,\r\n\r\nDo unused clusters cause a problem? \r\nI want to keep a prepared cluster for APM(skywalking). I want to use cluster if APM is enabled.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21350/comments",
    "author": "sefaphlvn",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2022-05-18T15:09:57Z",
        "body": "In general having extra clusters should be fine, there might be minimal memory overhead being introduced but if its completely unused this should be minimal. Even if you enable active health checking it should be operating in a low overhead mode until used"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-17T16:01:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-24T20:01:19Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21349,
    "title": "Docker build fail for branch release/v1.17",
    "created_at": "2022-05-18T02:10:24Z",
    "closed_at": "2022-06-25T16:01:39Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21349",
    "body": "*docker build fail for branch release/v1.17*\r\n\r\n*Description*:\r\nimage used: envoyproxy/envoy-build-ubuntu:11efa5680d987fff33fde4af3cc5ece105015d04\r\n\r\n➜  envoy git:(release/v1.17) ✗\r\n\r\n```console\r\n$ ./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.debug'\r\nNo remote cache is set, skipping setup remote cache.\r\nENVOY_SRCDIR=/source\r\nENVOY_BUILD_TARGET=//source/exe:envoy-static\r\nENVOY_BUILD_ARCH=aarch64\r\n$TEST_TMPDIR defined: output root default is '/build/tmp' and max_idle_secs default is '15'.\r\nStarting local Bazel server and connecting to it...\r\n$TEST_TMPDIR defined: output root default is '/build/tmp' and max_idle_secs default is '15'.\r\nHEAD is now at 30e5df3b config: update configs for v3 readiness. (#137)\r\n$TEST_TMPDIR defined: output root default is '/build/tmp' and max_idle_secs default is '15'.\r\nbuilding using 5 CPUs\r\nbuilding for aarch64\r\nclang toolchain with libc++ configured\r\nTesting //test/... @com_googlesource_quiche//:ci_tests\r\n$TEST_TMPDIR defined: output root default is '/build/tmp' and max_idle_secs default is '15'.\r\nDEBUG: /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/external/bazel_toolchains/rules/rbe_repo/version_check.bzl:68:14:\r\nCurrent running Bazel is ahead of bazel-toolchains repo. Please update your pin to bazel-toolchains repo in your WORKSPACE file.\r\nDEBUG: /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/external/bazel_toolchains/rules/rbe_repo/version_check.bzl:68:14:\r\nCurrent running Bazel is ahead of bazel-toolchains repo. Please update your pin to bazel-toolchains repo in your WORKSPACE file.\r\nDEBUG: /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/external/bazel_toolchains/rules/rbe_repo/version_check.bzl:68:14:\r\nCurrent running Bazel is ahead of bazel-toolchains repo. Please update your pin to bazel-toolchains repo in your WORKSPACE file.\r\nDEBUG: /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/external/bazel_toolchains/rules/rbe_repo/version_check.bzl:68:14:\r\nCurrent running Bazel is ahead of bazel-toolchains repo. Please update your pin to bazel-toolchains repo in your WORKSPACE file.\r\nDEBUG: /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/external/bazel_toolchains/rules/rbe_repo/version_check.bzl:68:14:\r\nCurrent running Bazel is ahead of bazel-toolchains repo. Please update your pin to bazel-toolchains repo in your WORKSPACE file.\r\nWARNING: /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/external/remote_java_tools_linux/BUILD:733:11: in hdrs attribute of cc_library rule @remote_java_tools_linux//:combiners: Artifact 'external/remote_java_tools_linux/java_tools/src/tools/singlejar/zip_headers.h' is duplicated (through '@remote_java_tools_linux//:transient_bytes' and '@remote_java_tools_linux//:zip_headers'). Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nINFO: Analyzed 1610 targets (1297 packages loaded, 47928 targets configured).\r\nINFO: Found 809 targets and 801 test targets...\r\nINFO: From CcCmakeMakeRule external/envoy/bazel/foreign_cc/zlib/include [for host]:\r\n\r\nERROR: /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/external/envoy/bazel/foreign_cc/BUILD:346:21: TreeArtifact external/envoy/bazel/foreign_cc/zlib/include was not created\r\nERROR: /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/external/envoy/bazel/foreign_cc/BUILD:346:21: TreeArtifact external/envoy/bazel/foreign_cc/copy_zlib/zlib was not created\r\nERROR: /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/external/envoy/bazel/foreign_cc/BUILD:346:21: not all outputs were created or valid\r\nINFO: Elapsed time: 34.058s, Critical Path: 10.16s\r\nINFO: 57 processes: 56 internal, 1 processwrapper-sandbox.\r\nFAILED: Build did NOT complete successfully\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21349/comments",
    "author": "LaurenLiu123",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2022-05-18T15:08:40Z",
        "body": "@keith \r\n\r\nIs there possibly a mismatch between the docker image you're using and the version used by 1.17?"
      },
      {
        "user": "phlax",
        "created_at": "2022-05-18T15:28:03Z",
        "body": "`11efa5680d987fff33fde4af3cc5ece105015d04` is the correct build image for 1.17 i think"
      },
      {
        "user": "LaurenLiu123",
        "created_at": "2022-05-19T07:44:55Z",
        "body": "I have this issue on macbook m1. I change to a x86 machine seems fine."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-18T16:01:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-25T16:01:38Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21234,
    "title": "Whether the client can control the recent access by itself when using the locality feature",
    "created_at": "2022-05-11T09:47:49Z",
    "closed_at": "2022-06-18T00:02:43Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21234",
    "body": "We want to use locality, whether the client can control the nearest access by itself? For example, the client is deployed in region-a, and the server has region-a, region-b, and region-c. We enable nearby access by default, so that the client of region-a can access the server of region-a. But sometimes, the client of region-a wants to access the server of region-b.\r\n\r\nAt present, our design is that the http request is sent to the istio gateway, and then the istio gateway forwards it to the corresponding service. At this point, it can be said that the client is the gateway. Can the client control which region to access nearby?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21234/comments",
    "author": "WangChao0326",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-05-11T16:19:26Z",
        "body": "This question is somewhat out of scope for Envoy, it's more about how you use Envoy in your deployment. Do you have a specific Envoy question that would help you with your goal?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-10T20:01:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-18T00:02:42Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21232,
    "title": "retriable_statuses: Cannot find field ",
    "created_at": "2022-05-11T07:10:46Z",
    "closed_at": "2022-05-11T16:51:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21232",
    "body": "*Title*: *retriable_statuses: Cannot find field*\r\n\r\n*Description*:\r\nTrying to implement http_health_check with retriable_statuses. \r\nWhen adding the retriable_status the following error is encountered any envoy fails to start\r\nProtobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(static_resources.clusters[1].health_checks.http_health_check) retriable_statuses: Cannot find field.) has unknown fields\r\nNo issues when omitting retriable_status property.\r\nPer documentation retriable_statuses is under http_health_check as expected_statuses.\r\nTested this on envoy 1.19.1 and 1.20.3.\r\n\r\n\r\n*Config*:\r\n```\r\nclusters:\r\n- name: service_1\r\n    connect_timeout: 60s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: service_1\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: service_1.local\r\n                port_value: 80\r\n    health_checks:\r\n      timeout: 3s\r\n      interval: 2s\r\n      healthy_threshold: 1\r\n      unhealthy_threshold: 3\r\n      http_health_check:\r\n        path: \"/status\"\r\n        expected_statuses: { start: 200, end: 499 }\r\n        retriable_statuses: { start: 200, end: 499 }\r\n      always_log_health_check_failures: true\r\n      event_log_path: \"/dev/stdout\"\r\n    outlier_detection:\r\n      consecutive_5xx: 3\r\n      interval: 10s\r\n      base_ejection_time: 30s\r\n      max_ejection_percent: 10\r\n```\r\n\r\n*Logs*:\r\nProtobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(static_resources.clusters[1].health_checks.http_health_check) retriable_statuses: Cannot find field.) has unknown fields\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21232/comments",
    "author": "aruandre",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-05-11T16:18:29Z",
        "body": "I would imagine this was added in a version newer than what you are using."
      },
      {
        "user": "aruandre",
        "created_at": "2022-05-11T16:51:10Z",
        "body": "@mattklein123 thanks, found it now in the release notes of v1.21.0"
      }
    ]
  },
  {
    "number": 21215,
    "title": "Why does dubbo filter set inbound|20880|| in inbound so that it will not report dubbo router: unknown cluster inbound|20880||",
    "created_at": "2022-05-10T04:32:21Z",
    "closed_at": "2022-06-17T20:01:22Z",
    "labels": [
      "question",
      "stale",
      "area/dubbo"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21215",
    "body": "*Description*:\r\nask  @wbpcode  Why does dubbo filter set inbound|20880|| in inbound so that it will not report dubbo router: unknown cluster inbound|20880||\r\nE.g:\r\nspec:\r\n  configPatches:\r\n  - applyTo: NETWORK_FILTER\r\n    match:\r\n      listener:\r\n        filterChain:\r\n          filter:\r\n            name: envoy.filters.network.tcp_proxy\r\n        name: virtualInbound\r\n    patch:\r\n      operation: REPLACE\r\n      value:\r\n        name: envoy.filters.network.dubbo_proxy\r\n        typed_config:\r\n          '@type': type.googleapis.com/udpa.type.v1.TypedStruct\r\n          type_url: type.googleapis.com/envoy.extensions.filters.network.dubbo_proxy.v3.DubboProxy\r\n          value:\r\n            dubboFilters:\r\n            - name: envoy.filters.dubbo.router\r\n            routeConfig:\r\n            - interface: '*'\r\n              name: inbound|20880||\r\n              routes:\r\n              - match:\r\n                  method:\r\n                    name:\r\n                      safeRegex:\r\n                        googleRe2: {}\r\n                        regex: .*\r\n                route:\r\n                  cluster: inbound|20880||\r\n            statPrefix: inbound|20880||\r\n  workloadSelector:\r\n    labels:\r\n      app: dubbo-sample-provider\r\n\r\n`\r\n  Upstream::ThreadLocalCluster* cluster = cluster_manager_.get(route_entry_->clusterName());\r\n  if (!cluster) {\r\n    ENVOY_STREAM_LOG(debug, \"dubbo router: unknown cluster '{}'\", *callbacks_,\r\n                     route_entry_->clusterName());\r\n    callbacks_->sendLocalReply(\r\n        AppException(ResponseStatus::ServerError, fmt::format(\"dubbo router: unknown cluster '{}'\",\r\n                                                              route_entry_->clusterName())),\r\n        false);\r\n    return FilterStatus::StopIteration;\r\n  }\r\n\r\n`\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21215/comments",
    "author": "yingjianjian",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-05-10T06:04:48Z",
        "body": "Sorry, can you describe your question with more detailed info? I don't get your problem clearly.\r\n\r\nAs you can see in the code snippet you post, when the dubbo router cannot get the cluster by the name, the error `unknown cluster` will be returned. So may be the xds server just only create a ` inbound|20880||` for the indound proxy.\r\n\r\nIt seems that this issue is not directly related with Envoy."
      },
      {
        "user": "yingjianjian",
        "created_at": "2022-05-11T09:43:23Z",
        "body": "> * inbound|20880||\r\n\r\nSorry just saw the reply. My question is this. At present, the inbound cluster configuration in Dubbo's case is inbound|20880|, not, for example, inbound|20880|http-80111|***** why can it be configured as inbound|20880||"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-05-11T12:19:33Z",
        "body": "> > * inbound|20880||\r\n> \r\n> Sorry just saw the reply. My question is this. At present, the inbound cluster configuration in Dubbo's case is inbound|20880|, not, for example, inbound|20880|http-80111|***** why can it be configured as inbound|20880||\r\n\r\nThis depends on the implementation of xds server. From the inspect of Envoy, the name of cluster has no special meaning.\r\n\r\n> why can it be configured as `inbound|20880||`\r\n\r\nBecause xds server create it."
      },
      {
        "user": "yingjianjian",
        "created_at": "2022-05-11T13:02:27Z",
        "body": "> > > * inbound|20880||\r\n> > \r\n> > \r\n> > Sorry just saw the reply. My question is this. At present, the inbound cluster configuration in Dubbo's case is inbound|20880|, not, for example, inbound|20880|http-80111|***** why can it be configured as inbound|20880||\r\n> \r\n> This depends on the implementation of xds server. From the inspect of Envoy, the name of cluster has no special meaning.\r\n> \r\n> > why can it be configured as `inbound|20880||`\r\n> \r\n> Because xds server create it.\r\n\r\nThank you, the code implementation of this part is in that function, I will study it.\r\nBecause the current version I use here cannot see the inbound|20880| cluster through 127.0.0.1:15000/clusters, I need to see which version is the last supported version\r\nThank you."
      },
      {
        "user": "yingjianjian",
        "created_at": "2022-05-11T13:27:01Z",
        "body": "> > > > * 入站|20880||\r\n> > > \r\n> > > \r\n> > > 不好意思刚看到回复。我的问题是这个。目前 Dubbo 的情况下 inbound 集群配置是 inbound|20880|，而不是比如 inbound|20880|http-80111|***** 为什么可以配置成 inbound|20880||\r\n> > \r\n> > \r\n> > 这取决于 xds 服务器的实现。从 Envoy 的检查来看，集群的名称并没有什么特别的含义。\r\n> > > 为什么可以配置为`inbound|20880||`\r\n> > \r\n> > \r\n> > 因为 xds 服务器创建它。\r\n> \r\n> Thank you, the code implementation of this part is in that function, I will study it\r\n\r\n\r\n\r\n> > > > * inbound|20880||\r\n> > > \r\n> > > \r\n> > > Sorry just saw the reply. My question is this. At present, the inbound cluster configuration in Dubbo's case is inbound|20880|, not, for example, inbound|20880|http-80111|***** why can it be configured as inbound|20880||\r\n> > \r\n> > \r\n> > This depends on the implementation of xds server. From the inspect of Envoy, the name of cluster has no special meaning.\r\n> > > why can it be configured as `inbound|20880||`\r\n> > \r\n> > \r\n> > Because xds server create it.\r\n> \r\n> Thank you, the code implementation of this part is in that function, I will study it. Because the current version I use here cannot see the inbound|20880| cluster through 127.0.0.1:15000/clusters, I need to see which version is the last supported version Thank you.\r\n\r\n@wbpcode  thank I understand that XDS is implemented in the expansion component"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-10T16:01:45Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-17T20:01:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 21121,
    "title": "How envoy queues requests?",
    "created_at": "2022-05-03T06:06:46Z",
    "closed_at": "2024-06-26T13:38:23Z",
    "labels": [
      "question",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21121",
    "body": "In my case, there are multiple grpc endpoints, each of which can only process one request at a time. It may take several seconds to several minutes to process one request.\r\n\r\nWhat I need:\r\n\r\nEnvoy takes a bunch of requests, assigns one to each endpoint, and enqueues the rest. Once an endpoint finishes, envoy assigns the next in queue to this endpoint.\r\n\r\nI have looked into \"Circuit Breakers\", but it just fails the requests beyond max_requests. \r\n\r\n```\r\n    circuit_breakers:\r\n      thresholds:\r\n        - max_connections: 5\r\n          max_pending_requests: 20\r\n          max_requests: 5\r\n```\r\nUsing the config above, I send 10 requests, only first 5 are successful.\r\n\r\nI have also checked \"connection pool\" and tested max_concurrent_streams. It seems not relevant.\r\n\r\nI am new to envoy. Thanks if anyone could give a hint.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21121/comments",
    "author": "exhau",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-05-03T14:26:57Z",
        "body": "cc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-05-04T16:16:41Z",
        "body": "so the circuit breakers will hard fail if you go over the request limit.\r\nI _believe_ if you go over the connection limit it'll queue.\r\nso I think if you configure your connect limit at 1, and max concurrent streams at 1 you'd get the behavior you wanted.  Please give it a shot and let me know if it doesn't work."
      },
      {
        "user": "exhau",
        "created_at": "2022-05-04T17:10:48Z",
        "body": "thanks very much @alyssawilk. Your config works.\r\n\r\nmax_connections: 1\r\nmax_concurrent_streams: 1\r\n\r\nBut it seems, envoy assigns requests when they come, not assign when one endpoint becomes available. "
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-05-04T20:46:47Z",
        "body": "Yeah, the queuing is done in the connection pool, not at the cluster level, which is sub-optimal for your use case.  I think your use case was one Envoy wasn't really designed for, but I think we'd welcome changes if you're game for queuing at a higher level."
      },
      {
        "user": "exhau",
        "created_at": "2022-05-05T13:51:37Z",
        "body": "got it. thanks!"
      },
      {
        "user": "exhau",
        "created_at": "2024-06-23T12:02:36Z",
        "body": "``` \r\n    http2_protocol_options: \r\n      max_concurrent_streams: 1\r\n    circuit_breakers:\r\n      thresholds:\r\n        - max_connections: 1\r\n```\r\n \r\nIf \"one\" client sends a lot of requests simultaneously, this config works as expected. Each backend processes one request at a time. \r\n \r\nBut when a second client starts sending requests, the backend may process two requests at the same time.\r\n \r\nI wonder if it is by designed. Is there a way to achieve processing request one by one for each endpoint, when multiple clients are sending multiple request?\r\n\r\nThanks~"
      },
      {
        "user": "alyssawilk",
        "created_at": "2024-06-24T13:07:06Z",
        "body": "Do you perhaps have multiple worker threads?  These limits apply to each worker thread so my suspicion is Envoy is working as intended but you need to limit worker threads if you want to rate limit so much"
      },
      {
        "user": "exhau",
        "created_at": "2024-06-26T13:38:23Z",
        "body": "--concurrency 1\r\nsolved it. thank you again!"
      }
    ]
  },
  {
    "number": 21089,
    "title": "ext_authz with WebSocket support?",
    "created_at": "2022-04-29T11:12:49Z",
    "closed_at": "2022-06-05T16:01:17Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21089",
    "body": "Is it possible to use an `ext_authz` filter while trying to establish connections through WebSockets?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21089/comments",
    "author": "catalin-me",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2022-04-29T14:02:57Z",
        "body": "cc @esmet @gsagula "
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-04-29T14:03:23Z",
        "body": "AFIK yes, but once for the connection not per websocket frame.  "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-29T16:01:13Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-05T16:01:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20939,
    "title": "access_log:  upstream tls details",
    "created_at": "2022-04-22T08:41:42Z",
    "closed_at": "2022-04-26T23:13:30Z",
    "labels": [
      "question",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20939",
    "body": "*Title*: *adding upstream TLS session details in access log*\r\n\r\n*Description*:\r\nI am using \"dynamic forward proxy\", want to log upstream TLS details (tls version, ciphers, session id etc). I could only find the formatter for downstream connections, not for upstream connections. Before making changes to add those, wanted to check  if its already available in someway/I am overlooking.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20939/comments",
    "author": "surki",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-04-22T09:49:32Z",
        "body": "Yeah, there is no specific upstream connnections info in current formatter."
      },
      {
        "user": "surki",
        "created_at": "2022-04-22T10:44:37Z",
        "body": "@wbpcode thanks, will make the changes"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-04-22T11:02:51Z",
        "body": "I am not sure if it is easy to get upstream connections info when we try to flush logs. 🤔 "
      }
    ]
  },
  {
    "number": 20870,
    "title": "Warm-up/ Data Migration Functionality with envoy proxy",
    "created_at": "2022-04-18T11:14:53Z",
    "closed_at": "2022-05-25T16:01:52Z",
    "labels": [
      "question",
      "stale",
      "area/redis"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20870",
    "body": "Need the suggestions and recommendations for doing warm-up with envoy proxy on open source Redis. We have explored some open-source tools for warm-up but the performance is very slow, looking for more fast and reliable approaches. \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20870/comments",
    "author": "ayasha05",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-18T16:01:37Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-25T16:01:52Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20861,
    "title": "what's the difference between cache filter and buffer filter?",
    "created_at": "2022-04-17T06:03:58Z",
    "closed_at": "2022-05-25T04:08:03Z",
    "labels": [
      "question",
      "stale",
      "area/http_filter"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20861",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\nwhat's the difference between cache filter and buffer filter?\r\n*Description*:\r\n>Describe the issue.\r\nwhat's the difference between cache filter and buffer filter?\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20861/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-04-18T02:20:03Z",
        "body": "Envoy http filters will handle HTTP requests in a streamlined manner by default. \r\n```\r\nGet header -> Process header -> Send header -> Get body -> Process body -> Send body.\r\n```\r\n\r\nThe buffer filter will stop the filter iteration. And continue the iteration after the whole http request is buffered.\r\nThe cache filter will cache the http responses to reduce accesses to the backend services and get higher throughput and lower latency."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-18T04:07:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-25T04:08:03Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20842,
    "title": "Large tail latency when using sync HttpCall function in lua filter",
    "created_at": "2022-04-15T07:51:40Z",
    "closed_at": "2022-06-12T12:01:36Z",
    "labels": [
      "question",
      "area/perf",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20842",
    "body": "*Description*:\r\nWe are using lua filter **sync** HttpCall function in lua filter to call a thirdParty service, the script is just like this\r\n```\r\n    local headers, body= handle:httpCall(\r\n        \"cluster_name\",\r\n        {\r\n            [\":method\"] = \"GET\",\r\n            [\":path\"] = \"service_path\",\r\n            [\":authority\"] = \"authority\",\r\n        },\r\n        \"\",\r\n        5000\r\n    )\r\n```\r\nWe found that with a higher QPS like 500, there will be a large tail latency like P99=20-40ms when envoy concurrency > 1, after breaking down the latency, we found that the bottleneck is the latency from the third party service respond to envoy.\r\n\r\nWhen we change the envoy' concurrency to 1 or  change the http call mode to async, there won't be such problems. So is there any issue for lua filter to deal with the response at sync mode?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20842/comments",
    "author": "siyigao1212",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2022-04-15T16:47:34Z",
        "body": "cc @wbpcode "
      },
      {
        "user": "wbpcode",
        "created_at": "2022-04-16T10:40:21Z",
        "body": "The async request does not care about the third-party response, so it will be quickly of course. I think the sync or async does not mattter. The performance of third-party service is the key point. \r\n\r\nCan you provide some more stats about your scene? For example:\r\n- qps of the third-party service? will it be different when set envoy concurrency > 1?\r\n- envoy metrics data of your target cluster (third-party service)? We can get the P99/P95 value from the `upstream_rq_time`. We can compare these values under different configurations.\r\n- A simple benmark result of third-party service will also be helpful for figuring out this problem."
      },
      {
        "user": "siyigao1212",
        "created_at": "2022-04-18T03:45:48Z",
        "body": "Thanks @wbpcode , very happy to know there is no problem with sync mode for lua filter.\r\n\r\nAfter checking the third party service, we found that it's some problem with the tcp_nodelay settings. Thanks for your response anyway."
      },
      {
        "user": "jtway",
        "created_at": "2022-05-05T14:00:17Z",
        "body": "@siyigao1212 any chance you're able to share more on the tcp_nodelay settings that were problematic? We've seen similar issues (without LUA) in select environments. Unsure if they are the same."
      },
      {
        "user": "siyigao1212",
        "created_at": "2022-05-06T07:47:09Z",
        "body": "\r\nThe solution we used to solve the tail latency is to set tcp_nodelay as true, which means disable the Nagle's Algorithm. Nagle's Algorithm is to reduce the number of small packets of tcp socket and then improve network utilization, but at our scenario, it could cause the large tail latency problem. So if the tail latency is very important, it's better to disable Nagle Algorithm."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-05T08:02:00Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-12T12:01:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20708,
    "title": "Custom handshaker extension: can it be used to implement dynamic tls termination",
    "created_at": "2022-04-07T06:57:05Z",
    "closed_at": "2022-04-23T02:04:06Z",
    "labels": [
      "question",
      "help wanted",
      "area/tls"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20708",
    "body": "Description: I want to implement client side forward proxy, for which i may need to generate certificates for each domain that user has requested at runtime. Need help if Custom handshaker extension is not what I am looking for. If yes, what are your suggestions to make my requirement possible with envoy",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20708/comments",
    "author": "vorishirne",
    "comments": [
      {
        "user": "vorishirne",
        "created_at": "2022-04-21T13:11:27Z",
        "body": "@htuch pls help if that would help or pls refer the right person to ask this."
      },
      {
        "user": "htuch",
        "created_at": "2022-04-22T04:54:08Z",
        "body": "@ggreenway "
      },
      {
        "user": "ggreenway",
        "created_at": "2022-04-22T16:27:22Z",
        "body": "I think this is asking for the same thing as #18928. @vorishirne does that sound correct?"
      },
      {
        "user": "vorishirne",
        "created_at": "2022-04-22T17:26:00Z",
        "body": "Oh thats an exact match.He is having that as part of a listener filter. \r\nIs that also possible via overriding this entire function\r\n```\r\nconst std::string& ConnectionInfoImplBase::subjectLocalCertificate() const {\r\n  if (!cached_subject_local_certificate_.empty()) {\r\n    return cached_subject_local_certificate_;\r\n  }\r\n  X509* cert = SSL_get_certificate(ssl());\r\n  if (!cert) {\r\n    ASSERT(cached_subject_local_certificate_.empty());\r\n    return cached_subject_local_certificate_;\r\n  }\r\n  cached_subject_local_certificate_ = Utility::getSubjectFromCertificate(*cert);\r\n  return cached_subject_local_certificate_;\r\n}\r\n```"
      },
      {
        "user": "lambdai",
        "created_at": "2022-04-22T22:40:09Z",
        "body": "> cached_subject_local_certificate_ = Utility::getSubjectFromCertificate(*cert);\r\n\r\nInteresting. This subjectLocalCertificate is supposed to be called after the handshake is done. To drive the handshake, you need a certificate. \r\n\r\nI am not sure how you'd like to moving forward with a new subjectLocalCertificate() impl"
      },
      {
        "user": "vorishirne",
        "created_at": "2022-04-23T01:49:36Z",
        "body": "Any function that would ask for the certificate to send to the client. I want to tweak in that. "
      },
      {
        "user": "vorishirne",
        "created_at": "2022-04-23T02:04:06Z",
        "body": "But now I think the dedicated change from above mentioned issue are already the suitable ones. Will try them only"
      }
    ]
  },
  {
    "number": 20685,
    "title": "Implement other protocol as a HTTP codec",
    "created_at": "2022-04-06T08:19:18Z",
    "closed_at": "2022-05-20T08:01:29Z",
    "labels": [
      "question",
      "stale",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20685",
    "body": "Now, in source/extensions/filters/network/http_connection_manager/config.cc,\r\nwe createCodec according to codec_type_, so if we want to extend a codec, we have to add codes in tree.\r\n\r\nHow about createCodec using a factory like many other extensions, e.g. network filters extensions, in this way, anyone can extend a new codec in a separate code rep，and static link to envoy using REGISTER_FACTORY when building the separate code rep.\r\n\r\nWhat's more, if someone want to implement a new RPC protocol, it can be implemented as a new HTTP codec, and it has some advantages:\r\n\r\n- The new protocol can reuse all of the HTTP filter stacks\r\n- Istio can config envoy for the new protocol using the existing VirtualService and DestinationRule etc.\r\n\r\nIs this a proper way to extend Istio & Envoy to support other RPC protocols?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20685/comments",
    "author": "smwyzi",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2022-04-11T21:53:59Z",
        "body": "In this case, codec type is an HTTP major version (or the AUTO special case). For what protocol are you trying to add a new codec type?"
      },
      {
        "user": "smwyzi",
        "created_at": "2022-04-12T12:09:39Z",
        "body": "> In this case, codec type is an HTTP major version (or the AUTO special case). For what protocol are you trying to add a new codec type?\r\n\r\n@zuercher \r\nSome private RPC protocol, what's more, many network filters, e.g. dubbo, thrift ... , can try to become an http codec, and then all reuse http filter stacks."
      },
      {
        "user": "zuercher",
        "created_at": "2022-04-12T16:26:57Z",
        "body": "In my opinion, it's a mistake to try to make non-HTTP protocols fit into the HTTP connection manager. Even if all these protocols can be adapted to the HTTP's shape, it will lead to all manner of complicated exceptions to HTTP processing.\r\n"
      },
      {
        "user": "smwyzi",
        "created_at": "2022-04-13T02:24:45Z",
        "body": "> it will lead to all manner of complicated exceptions to HTTP processing\r\n\r\n@zuercher yes, i agree with you.\r\n\r\nBut in the other hand, each protocol is implemented as a separate network filter, has the following defects:\r\n\r\n- Each network filter need to implement some commen filters(like HTTP filters), e.g. RateLimit, Router, wasm, fault etc.\r\n- Istio can't config other network filters, VirtualService and DestinationRule can only config HTTP connection manager\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-13T08:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-20T08:01:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20618,
    "title": "Envoy only registers the admin port on server (re)boot",
    "created_at": "2022-03-31T17:39:32Z",
    "closed_at": "2022-04-02T03:08:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20618",
    "body": "*Title*: Envoy setup as a systemd service doesn't seem to register the listeners, just registers the admin port.\r\n\r\n*Description*:\r\nWe seem to have an issue with Envoy not registering the listeners. We have envoy setup as a systemd service, and when the server starts/reboots, only the admin port is registered. The upstream is a Redis cluster.\r\n\r\n- `envoy.service`\r\n\r\n```\r\n[Unit]\r\nDescription=Envoy Proxy\r\nWants=network-online.target\r\nRequires=network-online.target\r\n\r\n[Service]\r\nUser=root\r\nGroup=root\r\nType=simple\r\nExecStart=/usr/bin/envoy -c /etc/envoy/config.yaml\r\nRestart=on-failure\r\nRestartSec=30s\r\n\r\n[Install]\r\nWantedBy=multi-user.target\r\n```\r\n\r\n- `config.yaml`\r\n```\r\nadmin:\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 3000\r\nstatic_resources:\r\n  listeners:\r\n  - name: redis_listener\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 127.0.0.1\r\n        port_value: 6380\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.redis_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.redis_proxy.v3.RedisProxy\r\n          stat_prefix: egress_redis\r\n          settings:\r\n            op_timeout: 3s\r\n            read_policy: PREFER_MASTER\r\n          prefix_routes:\r\n            catch_all_route:\r\n              cluster: redis_cluster\r\n  clusters:\r\n  - name: redis_cluster\r\n    connect_timeout: 2s\r\n    lb_policy: CLUSTER_PROVIDED\r\n    cluster_type:\r\n      name: envoy.clusters.redis\r\n      typed_config:\r\n        '@type': type.googleapis.com/google.protobuf.Struct\r\n        value:\r\n          cluster_refresh_rate: 3600s\r\n          cluster_refresh_timeout: 3s\r\n          host_degraded_refresh_threshold: 1\r\n    outlier_detection:\r\n      base_ejection_time: 30s\r\n      consecutive_5xx: 1\r\n      interval: 5s\r\n    upstream_connection_options:\r\n      tcp_keepalive:\r\n        keepalive_interval: 5\r\n        keepalive_probes: 1\r\n        keepalive_time: 30\r\n    load_assignment:\r\n      cluster_name: redis_cluster\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: app.example.com\r\n                port_value: 6379\r\n    typed_extension_protocol_options:\r\n      envoy.filters.network.redis_proxy:\r\n        '@type': type.googleapis.com/google.protobuf.Struct\r\n        value:\r\n          auth_password:\r\n            inline_string: password\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n```\r\n\r\nScanning the log, it seems fine overall.\r\n\r\n* Startup log\r\n```\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:385] initializing epoch 0 (base id=0, hot restart version=11.104)\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:387] statically linked extensions:\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.compression.decompressor: envoy.compression.brotli.decompressor, envoy.compression.gzip.decompressor\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.resource_monitors: envoy.resource_monitors.fixed_heap, envoy.resource_monitors.injected_resource\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.internal_redirect_predicates: envoy.internal_redirect_predicates.allow_listed_routes, envoy.internal_redirect_predicates.previous_routes, envoy.internal_redirect_predicates.safe_cross_scheme\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.http.cache: envoy.extensions.http.cache.simple\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.guarddog_actions: envoy.watchdog.abort_action, envoy.watchdog.profile_action\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.quic.proof_source: envoy.quic.proof_source.filter_chain\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.stats_sinks: envoy.dog_statsd, envoy.graphite_statsd, envoy.metrics_service, envoy.stat_sinks.dog_statsd, envoy.stat_sinks.graphite_statsd, envoy.stat_sinks.hystrix, envoy.stat_sinks.metrics_service, envoy.stat_sinks.statsd, envoy.stat_sinks.wasm, envoy.statsd\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.http.original_ip_detection: envoy.http.original_ip_detection.custom_header, envoy.http.original_ip_detection.xff\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.dubbo_proxy.serializers: dubbo.hessian2\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.upstreams: envoy.filters.connection_pools.tcp.generic\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.dubbo_proxy.route_matchers: default\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.dubbo_proxy.filters: envoy.filters.dubbo.router\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.wasm.runtime: envoy.wasm.runtime.null, envoy.wasm.runtime.v8\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.access_logger.extension_filters: envoy.access_loggers.extension_filters.cel\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.thrift_proxy.filters: envoy.filters.thrift.header_to_metadata, envoy.filters.thrift.rate_limit, envoy.filters.thrift.router\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.retry_priorities: envoy.retry_priorities.previous_priorities\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.tls.cert_validator: envoy.tls.cert_validator.default, envoy.tls.cert_validator.spiffe\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.common.key_value: envoy.key_value.file_based\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.matching.common_inputs: envoy.matching.common_inputs.environment_variable\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.matching.input_matchers: envoy.matching.matchers.consistent_hashing, envoy.matching.matchers.ip\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.filters.http: envoy.bandwidth_limit, envoy.buffer, envoy.cors, envoy.csrf, envoy.ext_authz, envoy.ext_proc, envoy.fault, envoy.filters.http.adaptive_concurrency, envoy.filters.http.admission_control, envoy.filters.http.alternate_protocols_cache, envoy.filters.http.aws_lambda, envoy.filters.http.aws_request_signing, envoy.filters.http.bandwidth_limit, envoy.filters.http.buffer, envoy.filters.http.cache, envoy.filters.http.cdn_loop, envoy.filters.http.composite, envoy.filters.http.compressor, envoy.filters.http.cors, envoy.filters.http.csrf, envoy.filters.http.decompressor, envoy.filters.http.dynamic_forward_proxy, envoy.filters.http.dynamo, envoy.filters.http.ext_authz, envoy.filters.http.ext_proc, envoy.filters.http.fault, envoy.filters.http.grpc_http1_bridge, envoy.filters.http.grpc_http1_reverse_bridge, envoy.filters.http.grpc_json_transcoder, envoy.filters.http.grpc_stats, envoy.filters.http.grpc_web, envoy.filters.http.header_to_metadata, envoy.filters.http.health_check, envoy.filters.http.ip_tagging, envoy.filters.http.jwt_authn, envoy.filters.http.local_ratelimit, envoy.filters.http.lua, envoy.filters.http.oauth2, envoy.filters.http.on_demand, envoy.filters.http.original_src, envoy.filters.http.ratelimit, envoy.filters.http.rbac, envoy.filters.http.router, envoy.filters.http.set_metadata, envoy.filters.http.stateful_session, envoy.filters.http.tap, envoy.filters.http.wasm, envoy.grpc_http1_bridge, envoy.grpc_json_transcoder, envoy.grpc_web, envoy.health_check, envoy.http_dynamo_filter, envoy.ip_tagging, envoy.local_rate_limit, envoy.lua, envoy.rate_limit, envoy.router, match-wrapper\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.bootstrap: envoy.bootstrap.wasm, envoy.extensions.network.socket_interface.default_socket_interface\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.dubbo_proxy.protocols: dubbo\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.compression.compressor: envoy.compression.brotli.compressor, envoy.compression.gzip.compressor\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.rbac.matchers: envoy.rbac.matchers.upstream_ip_port\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.request_id: envoy.request_id.uuid\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.filters.network: envoy.client_ssl_auth, envoy.echo, envoy.ext_authz, envoy.filters.network.client_ssl_auth, envoy.filters.network.connection_limit, envoy.filters.network.direct_response, envoy.filters.network.dubbo_proxy, envoy.filters.network.echo, envoy.filters.network.ext_authz, envoy.filters.network.http_connection_manager, envoy.filters.network.local_ratelimit, envoy.filters.network.mongo_proxy, envoy.filters.network.ratelimit, envoy.filters.network.rbac, envoy.filters.network.redis_proxy, envoy.filters.network.sni_cluster, envoy.filters.network.sni_dynamic_forward_proxy, envoy.filters.network.tcp_proxy, envoy.filters.network.thrift_proxy, envoy.filters.network.wasm, envoy.filters.network.zookeeper_proxy, envoy.http_connection_manager, envoy.mongo_proxy, envoy.ratelimit, envoy.redis_proxy, envoy.tcp_proxy\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.transport_sockets.downstream: envoy.transport_sockets.alts, envoy.transport_sockets.quic, envoy.transport_sockets.raw_buffer, envoy.transport_sockets.starttls, envoy.transport_sockets.tap, envoy.transport_sockets.tcp_stats, envoy.transport_sockets.tls, raw_buffer, starttls, tls\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.tracers: envoy.dynamic.ot, envoy.lightstep, envoy.tracers.datadog, envoy.tracers.dynamic_ot, envoy.tracers.lightstep, envoy.tracers.opencensus, envoy.tracers.skywalking, envoy.tracers.xray, envoy.tracers.zipkin, envoy.zipkin\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.transport_sockets.upstream: envoy.transport_sockets.alts, envoy.transport_sockets.quic, envoy.transport_sockets.raw_buffer, envoy.transport_sockets.starttls, envoy.transport_sockets.tap, envoy.transport_sockets.tcp_stats, envoy.transport_sockets.tls, envoy.transport_sockets.upstream_proxy_protocol, raw_buffer, starttls, tls\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.retry_host_predicates: envoy.retry_host_predicates.omit_canary_hosts, envoy.retry_host_predicates.omit_host_metadata, envoy.retry_host_predicates.previous_hosts\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.rate_limit_descriptors: envoy.rate_limit_descriptors.expr\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.access_loggers: envoy.access_loggers.file, envoy.access_loggers.http_grpc, envoy.access_loggers.open_telemetry, envoy.access_loggers.stderr, envoy.access_loggers.stdout, envoy.access_loggers.tcp_grpc, envoy.access_loggers.wasm, envoy.file_access_log, envoy.http_grpc_access_log, envoy.open_telemetry_access_log, envoy.stderr_access_log, envoy.stdout_access_log, envoy.tcp_grpc_access_log, envoy.wasm_access_log\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.http.stateful_header_formatters: preserve_case\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.filters.listener: envoy.filters.listener.http_inspector, envoy.filters.listener.original_dst, envoy.filters.listener.original_src, envoy.filters.listener.proxy_protocol, envoy.filters.listener.tls_inspector, envoy.listener.http_inspector, envoy.listener.original_dst, envoy.listener.original_src, envoy.listener.proxy_protocol, envoy.listener.tls_inspector\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.http.stateful_session: envoy.http.stateful_session.cookie\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.formatter: envoy.formatter.metadata, envoy.formatter.req_without_query\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.filters.udp_listener: envoy.filters.udp.dns_filter, envoy.filters.udp_listener.udp_proxy\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.resolvers: envoy.ip\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.quic.server.crypto_stream: envoy.quic.crypto_stream.server.quiche\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.upstream_options: envoy.extensions.upstreams.http.v3.HttpProtocolOptions, envoy.upstreams.http.http_protocol_options\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.network.dns_resolver: envoy.network.dns_resolver.cares\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.clusters: envoy.cluster.eds, envoy.cluster.logical_dns, envoy.cluster.original_dst, envoy.cluster.static, envoy.cluster.strict_dns, envoy.clusters.aggregate, envoy.clusters.dynamic_forward_proxy, envoy.clusters.redis\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.matching.action: composite-action, skip\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.thrift_proxy.transports: auto, framed, header, unframed\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.matching.http.input: request-headers, request-trailers, response-headers, response-trailers\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.health_checkers: envoy.health_checkers.redis\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.thrift_proxy.protocols: auto, binary, binary/non-strict, compact, twitter\r\n[2022-03-31 17:13:06.595][696][info][main] [source/server/server.cc:389]   envoy.grpc_credentials: envoy.grpc_credentials.aws_iam, envoy.grpc_credentials.default, envoy.grpc_credentials.file_based_metadata\r\n[2022-03-31 17:13:06.600][696][info][main] [source/server/server.cc:437] HTTP header map info:\r\n[2022-03-31 17:13:06.601][696][info][main] [source/server/server.cc:440]   request header map: 656 bytes: :authority,:method,:path,:protocol,:scheme,accept,accept-encoding,access-control-request-headers,access-control-request-method,authentication,authorization,cache-control,cdn-loop,connection,content-encoding,content-length,content-type,expect,grpc-accept-encoding,grpc-timeout,if-match,if-modified-since,if-none-match,if-range,if-unmodified-since,keep-alive,origin,pragma,proxy-connection,proxy-status,referer,te,transfer-encoding,upgrade,user-agent,via,x-client-trace-id,x-envoy-attempt-count,x-envoy-decorator-operation,x-envoy-downstream-service-cluster,x-envoy-downstream-service-node,x-envoy-expected-rq-timeout-ms,x-envoy-external-address,x-envoy-force-trace,x-envoy-hedge-on-per-try-timeout,x-envoy-internal,x-envoy-ip-tags,x-envoy-max-retries,x-envoy-original-path,x-envoy-original-url,x-envoy-retriable-header-names,x-envoy-retriable-status-codes,x-envoy-retry-grpc-on,x-envoy-retry-on,x-envoy-upstream-alt-stat-name,x-envoy-upstream-rq-per-try-timeout-ms,x-envoy-upstream-rq-timeout-alt-response,x-envoy-upstream-rq-timeout-ms,x-envoy-upstream-stream-duration-ms,x-forwarded-client-cert,x-forwarded-for,x-forwarded-host,x-forwarded-proto,x-ot-span-context,x-request-id\r\n[2022-03-31 17:13:06.601][696][info][main] [source/server/server.cc:440]   request trailer map: 128 bytes:\r\n[2022-03-31 17:13:06.601][696][info][main] [source/server/server.cc:440]   response header map: 432 bytes: :status,access-control-allow-credentials,access-control-allow-headers,access-control-allow-methods,access-control-allow-origin,access-control-expose-headers,access-control-max-age,age,cache-control,connection,content-encoding,content-length,content-type,date,etag,expires,grpc-message,grpc-status,keep-alive,last-modified,location,proxy-connection,proxy-status,server,transfer-encoding,upgrade,vary,via,x-envoy-attempt-count,x-envoy-decorator-operation,x-envoy-degraded,x-envoy-immediate-health-check-fail,x-envoy-ratelimited,x-envoy-upstream-canary,x-envoy-upstream-healthchecked-cluster,x-envoy-upstream-service-time,x-request-id\r\n[2022-03-31 17:13:06.601][696][info][main] [source/server/server.cc:440]   response trailer map: 152 bytes: grpc-message,grpc-status\r\n[2022-03-31 17:13:06.603][696][info][main] [source/server/server.cc:780] runtime: {}\r\n[2022-03-31 17:13:06.603][696][info][admin] [source/server/admin/admin.cc:134] admin address: 0.0.0.0:3000\r\n[2022-03-31 17:13:06.603][696][info][config] [source/server/configuration_impl.cc:127] loading tracing configuration\r\n[2022-03-31 17:13:06.603][696][info][config] [source/server/configuration_impl.cc:87] loading 0 static secret(s)\r\n[2022-03-31 17:13:06.603][696][info][config] [source/server/configuration_impl.cc:93] loading 1 cluster(s)\r\n[2022-03-31 17:13:06.607][696][info][config] [source/server/configuration_impl.cc:97] loading 1 listener(s)\r\n[2022-03-31 17:13:06.609][696][info][config] [source/server/configuration_impl.cc:109] loading stats configuration\r\n[2022-03-31 17:13:06.610][696][info][main] [source/server/server.cc:876] starting main dispatch loop\r\n```\r\n\r\n```\r\n$ sudo lsof -i -P -n | grep envoy\r\nenvoy      696            root   20u  IPv4  18878      0t0  TCP *:3000 (LISTEN)\r\n```\r\n\r\nAlso, if I just restart the envoy service, it works fine.\r\n```\r\n$ sudo systemctl restart envoy.service\r\n$ sudo lsof -i -P -n | grep envoy\r\nenvoy     6003            root   20u  IPv4  97088      0t0  TCP *:3000 (LISTEN)\r\nenvoy     6003            root   22u  IPv4  97101      0t0  TCP 127.0.0.1:6380 (LISTEN)\r\nenvoy     6003            root   23u  IPv4  97102      0t0  TCP 127.0.0.1:6380 (LISTEN)\r\n\r\n```\r\n\r\nI tried a different port as well, but that didn't do much. I've also noticed that if I change the `cluster_refresh_rate` to the default value of 5 seconds instead of the 3600 seconds, then it works fine. But keeping it a higher value made sense because it should rarely change. Are we missing anything here? Any help would be appreciated. Thank you.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20618/comments",
    "author": "sulavvr",
    "comments": [
      {
        "user": "zhangbo1882",
        "created_at": "2022-04-01T04:52:23Z",
        "body": "I think you can set log level to get more debug information to see what happens.\r\n\r\nExecStart=/usr/bin/envoy -c /etc/envoy/config.yaml --log-level trace"
      },
      {
        "user": "sulavvr",
        "created_at": "2022-04-02T02:48:32Z",
        "body": "Ahh yes, thanks @zhangbo1882! I probably could've done just that. :facepalm:  Seems like it can't connect to Redis just yet.\r\n\r\n```\r\n[2022-04-02 02:32:01.184][707][debug][connection] [source/common/network/connection_impl.cc:907] [C0] connecting to 127.0.0.1:6379\r\n[2022-04-02 02:32:01.185][707][debug][connection] [source/common/network/connection_impl.cc:927] [C0] connection in progress\r\n[2022-04-02 02:32:01.185][707][trace][connection] [source/common/network/connection_impl.cc:478] [C0] writing 29 bytes, end_stream false\r\n[2022-04-02 02:32:01.185][707][trace][connection] [source/common/network/connection_impl.cc:478] [C0] writing 28 bytes, end_stream false\r\n[2022-04-02 02:32:01.185][707][trace][connection] [source/common/network/connection_impl.cc:563] [C0] socket event: 3\r\n[2022-04-02 02:32:01.185][707][trace][connection] [source/common/network/connection_impl.cc:672] [C0] write ready\r\n[2022-04-02 02:32:01.185][707][debug][connection] [source/common/network/connection_impl.cc:693] [C0] delayed connect error: 111\r\n[2022-04-02 02:32:01.185][707][debug][connection] [source/common/network/connection_impl.cc:250] [C0] closing socket: 0\r\n[2022-04-02 02:32:01.185][707][trace][connection] [source/common/network/connection_impl.cc:418] [C0] raising connection event 0\r\n[2022-04-02 02:32:01.185][707][trace][main] [source/common/event/dispatcher_impl.cc:228] item added to deferred deletion list (size=1)\r\n[2022-04-02 02:32:01.185][707][trace][main] [source/common/event/dispatcher_impl.cc:112] clearing deferred deletion list (size=1)\r\n[2022-04-02 02:32:06.179][707][debug][main] [source/server/server.cc:246] flushing stats\r\n[2022-04-02 02:32:06.179][707][debug][main] [source/server/server.cc:256] Envoy is not fully initialized, skipping histogram merge and flushing stats\r\n[2022-04-02 02:32:11.182][707][debug][main] [source/server/server.cc:246] flushing stats\r\n[2022-04-02 02:32:11.182][707][debug][main] [source/server/server.cc:256] Envoy is not fully initialized, skipping histogram merge and flushing stats\r\n[2022-04-02 02:32:16.275][707][debug][main] [source/server/server.cc:246] flushing stats\r\n[2022-04-02 02:32:16.277][707][debug][main] [source/server/server.cc:256] Envoy is not fully initialized, skipping histogram merge and flushing stats\r\n[2022-04-02 02:32:21.346][707][debug][main] [source/server/server.cc:246] flushing stats\r\n[2022-04-02 02:32:21.350][707][debug][main] [source/server/server.cc:256] Envoy is not fully initialized, skipping histogram merge and flushing stats\r\n[2022-04-02 02:32:26.321][707][debug][main] [source/server/server.cc:246] flushing stats\r\n[2022-04-02 02:32:26.325][707][debug][main] [source/server/server.cc:256] Envoy is not fully initialized, skipping histogram merge and flushing stats\r\n```"
      }
    ]
  },
  {
    "number": 20607,
    "title": "generate compile_commands.json fail",
    "created_at": "2022-03-31T06:02:12Z",
    "closed_at": "2022-03-31T11:29:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20607",
    "body": "[Wed Mar 30 22:59:31][#120# ] (main)$./tools/gen_compilation_database.py \r\nERROR: /mnt/cache/_bazel_stack/5a617312af51e0dbefb3398d3212136c/external/base_pip3_sphinx/BUILD.bazel:22:11: no such package '@base_pip3_importlib_metadata//': The repository '@base_pip3_importlib_metadata' could not be resolved: Repository '@base_pip3_importlib_metadata' is not defined and referenced by '@base_pip3_sphinx//:pkg'\r\nERROR: Analysis of target '//tools/docs:sphinx_runner' failed; build aborted: \r\nINFO: Elapsed time: 1.008s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (3 packages loaded, 0 targets configured)\r\n    Fetching @emscripten_bin_linux; fetching\r\n    Fetching @base_pip3_pyparsing; fetching\r\n    Fetching @base_pip3_yarl; fetching\r\n    Fetching @base_pip3_python_gnupg; fetching\r\n    Fetching @base_pip3_multidict; fetching\r\n    Fetching @rust_linux_x86_64; fetching\r\n    Fetching @base_pip3_idna; fetching\r\n    Fetching @base_pip3_cffi; fetching\r\nTraceback (most recent call last):\r\n  File \"./tools/gen_compilation_database.py\", line 124, in <module>\r\n    fix_compilation_database(args, generate_compilation_database(args))\r\n  File \"./tools/gen_compilation_database.py\", line 20, in generate_compilation_database\r\n    subprocess.check_call([\"bazel\", \"build\"] + bazel_options + [\r\n  File \"/usr/lib/python3.8/subprocess.py\", line 364, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['bazel', 'build', '--config=compdb', '--remote_download_outputs=all', '--aspects=@bazel_compdb//:aspects.bzl%compilation_database_aspect', '--output_groups=compdb_files,header_files', '//source/...', '//test/...', '//tools/...', '//contrib/...']' returned non-zero exit status 1.\r\n\r\n\r\n[Wed Mar 30 22:59:46][#121# ] (main)$pip3 install importlib_metadata\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.8/dist-packages (4.10.1)\r\nRequirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib_metadata) (1.0.0)\r\n\r\nBut I have already installed the pkg.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20607/comments",
    "author": "zhangbo1882",
    "comments": [
      {
        "user": "zhxie",
        "created_at": "2022-03-31T06:10:10Z",
        "body": "You may require Python 3.10 to refresh compilation database."
      },
      {
        "user": "wbpcode",
        "created_at": "2022-03-31T08:06:23Z",
        "body": "cc @phlax "
      },
      {
        "user": "phlax",
        "created_at": "2022-03-31T09:10:16Z",
        "body": "> You may require Python 3.10 to refresh compilation database.\r\n\r\nyes, this is the issue - python3.10 _doesnt_ require `importlib-metadata` (and its deps) so it is not listed/pinned in the requirements file"
      },
      {
        "user": "zhangbo1882",
        "created_at": "2022-03-31T11:29:05Z",
        "body": "Thanks @zhxie.  It works now after I upgrade the python to 3.10. "
      }
    ]
  },
  {
    "number": 20541,
    "title": "how many overload action typed_config are threre？",
    "created_at": "2022-03-27T09:06:01Z",
    "closed_at": "2022-05-04T08:01:34Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20541",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\nhow many overload anction typed_config are threre？\r\n*Description*:\r\n>Describe the issue.\r\nI want to config overload action，the typed_config in the doc I discoveried one is   type.googleapis.com/envoy.config.overload.v3.ScaleTimersOverloadActionConfig。 what are others？ the doc don't say。\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20541/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "daixiang0",
        "created_at": "2022-03-28T01:36:30Z",
        "body": "I think now only support it."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-27T04:15:44Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-04T08:01:33Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20524,
    "title": "is enabled runtime_key random or it has some rules",
    "created_at": "2022-03-25T09:44:34Z",
    "closed_at": "2022-05-03T16:06:47Z",
    "labels": [
      "question",
      "stale",
      "area/configuration",
      "area/runtime"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20524",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\nis enabled runtime_key random or it has some rules*Description*:\r\n>Describe the issue.\r\nsometime I have to config enabled field,It has a sub field runtime_key,I don't know how to config it for some config don't have examples.is it random or it has some rules?\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20524/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-03-27T08:18:56Z",
        "body": "searching the docs and grepping the code it seems there are quite a few different uses of `runtime_key`\r\n\r\nmy reading is that the key names are significant, and as documented\r\n\r\n(eg try `$ git grep shadow_enabled` )"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-26T12:06:48Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-03T16:06:47Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20485,
    "title": "router check tool: eliminate testonly dependencies",
    "created_at": "2022-03-23T13:32:17Z",
    "closed_at": "2022-07-03T00:03:17Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20485",
    "body": "*Title*: eliminate testonly dependencies in the router check tool.\r\n\r\n*Description*:\r\nAt the present, the tool's library and binary targets are marked as testonly (as they're envoy_cc_test_library and envoy_cc_test_binary) and have testonly dependencies. We'd like to use the tool in validation workflows in production, but unable to comply with our internal policies while this tool is testonly.\r\n\r\nAs this tool could be used in various scenarios not limited to test environments or manual launches, we could get rid of test dependencies in the tool and build it as a regular binary. Test dependencies to eliminate are mocks and utility classes and functions.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20485/comments",
    "author": "seventhscream",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-03-23T15:00:26Z",
        "body": "FWIW I don't think this is a good use of time. You are just going to end up reimplementing a bunch of the test dependencies as \"production code.\" Certainly this is doable, but it seems like busy work to me. \r\n\r\nNote that this same issue applies to the schema validator tool which we are now shipping in the tools image. I would actually love it if the route check tool were also shipped in the tools image if you would like to work on that!"
      },
      {
        "user": "seventhscream",
        "created_at": "2022-03-23T18:31:03Z",
        "body": "Unfortunately, the docker image wouldn't make things easier for us.\r\nAlso, packaging the tool in the docker image would technically help us to work around the issue, but actually wouldn't remove the test-only dependencies.\r\nI understand the complexity of the task, but if we end up with no other way to use this tool in prod, would you mind if we make an attempt to solve this issue?\r\nAnother thing to consider, maybe some of the test-only dependencies could also be converted to non-test-only instead of reimplementing them? I mean file_system_for_test, parts of utility_lib and so on."
      },
      {
        "user": "mattklein123",
        "created_at": "2022-03-23T18:45:49Z",
        "body": "If you want to put together a design doc of what would be required feel free, but I will be honest that I'm skeptical that this churn is a good use of time."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-22T20:01:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-29T20:01:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "jmarantz",
        "created_at": "2022-05-26T17:12:44Z",
        "body": "Matt -- I haven't looked to deeply yet but do we need to use mocks for the dependencies or is the needed behavior available in the validation server implementation?"
      },
      {
        "user": "mattklein123",
        "created_at": "2022-05-26T18:35:29Z",
        "body": "@jmarantz I have no idea what would be required. As I said above, I have no objection, but someone will need to do the analysis and the work."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-26T00:02:50Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-03T00:03:16Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20482,
    "title": "how to use bootstrap watchdog,I can not start the envoy sidecar with error",
    "created_at": "2022-03-23T08:39:28Z",
    "closed_at": "2022-04-29T16:01:48Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20482",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\nhow to use bootstrap watchdog,I can not start the envoy sidecar with error.\r\nis my action config right?\r\n*Description*:\r\n>Describe the issue.\r\nI have the following config:\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: ef-bootstrap\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      app: productpage\r\n  configPatches:\r\n  - applyTo: BOOTSTRAP\r\n    patch:\r\n      operation: MERGE\r\n      value:\r\n          watchdogs:\r\n            main_thread_watchdog:\r\n              actions:\r\n              - event: KILL\r\n              - event: MULTIKILL\r\n              - event: MEGAMISS\r\n              - event: MISS\r\n              miss_timeout: 30s\r\n              megamiss_timeout: 20s\r\n              kill_timeout: 10s\r\n              max_kill_timeout_jitter: 20s\r\n              multikill_timeout: 20s\r\n              multikill_threshold: \r\n                value: 70\r\n            worker_watchdog:\r\n              actions:\r\n              - event: KILL\r\n              - event: MULTIKILL\r\n              - event: MEGAMISS\r\n              - event: MISS\r\n              miss_timeout: 30s\r\n              megamiss_timeout: 20s\r\n              kill_timeout: 10s\r\n              max_kill_timeout_jitter: 20s\r\n              multikill_timeout: 20s\r\n              multikill_threshold: \r\n                value: 70\r\n```\r\n\r\nwith the fllowing error:\r\ncritical        envoy main      error initializing configuration 'etc/istio/proxy/envoy-rev0.json': Provided name for static registration lookup was empty.\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20482/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-03-23T09:03:50Z",
        "body": "this seems to be more of a question for istio  - no ?"
      },
      {
        "user": "13567436138",
        "created_at": "2022-03-23T09:40:28Z",
        "body": "it is a envoy question,because my major problem is how to config watchdogs?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-22T12:01:41Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-29T16:01:48Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20387,
    "title": "Cannot connect to TLSV1.2 endpoint with Envoy proxy",
    "created_at": "2022-03-17T04:23:23Z",
    "closed_at": "2022-04-24T00:02:47Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20387",
    "body": "*Description*:\r\nWe are building a service that runs with Istio. We are seeing an issue where this service cannot complete the TLSv1.2 handshake with another service. This is what the handshake looks like:\r\n\r\nNo.\tTime\tSource\tDestination\tProtocol\tLength\tInfo\r\n625\t8.220923\t192.168.70.10\txxx.xx.xxx.xx\tTCP\t66\t59450 → 443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 WS=256 SACK_PERM=1\r\n660\t8.432916\txxx.xx.xxx.xx\t192.168.70.10\tTCP\t66\t443 → 59450 [SYN, ACK] Seq=0 Ack=1 Win=64240 Len=0 MSS=1412 SACK_PERM=1 WS=128\r\n661\t8.433073\t192.168.70.10\txxx.xx.xxx.xx\tTCP\t54\t59450 → 443 [ACK] Seq=1 Ack=1 Win=131072 Len=0\r\n662\t8.436082\t192.168.70.10\txxx.xx.xxx.xx\tTLSv1.2\t384\tClient Hello\r\n667\t8.643338\txxx.xx.xxx.xx\t192.168.70.10\tTCP\t60\t443 → 59450 [ACK] Seq=1 Ack=331 Win=64128 Len=0\r\n668\t8.645912\txxx.xx.xxx.xx\t192.168.70.10\tTLSv1.2\t1466\tServer Hello\r\n669\t8.646088\txxx.xx.xxx.xx\t192.168.70.10\tTLSv1.2\t2878\tCertificate [TCP segment of a reassembled PDU]\r\n670\t8.646088\txxx.xx.xxx.xx\t192.168.70.10\tTLSv1.2\t160\tServer Key Exchange, Server Hello Done\r\n671\t8.646177\t192.168.70.10\txxx.xx.xxx.xx\tTCP\t54\t59450 → 443 [ACK] Seq=331 Ack=4237 Win=131072 Len=0\r\n672\t8.646344\t192.168.70.10\txxx.xx.xxx.xx\tTCP\t54\t59450 → 443 [ACK] Seq=331 Ack=4343 Win=131072 Len=0\r\n673\t8.655383\t192.168.70.10\txxx.xx.xxx.xx\tTCP\t54\t59450 → 443 [FIN, ACK] Seq=331 Ack=4343 Win=131072 Len=0\r\n674\t8.873433\txxx.xx.xxx.xx\t192.168.70.10\tTCP\t60\t443 → 59450 [FIN, ACK] Seq=4343 Ack=332 Win=64128 Len=0\r\n675\t8.873611\t192.168.70.10\txxx.xx.xxx.xx\tTCP\t54\t59450 → 443 [ACK] Seq=332 Ack=4344 Win=131072 Len=0\r\n\r\nI am not overly familiar with TLS, but I can't see any errors here. It looks like the client abruptly resets after receiving the Server Hello Done message. The service will continue to attempt this connection multiple times before failing. We have run this same workload in a standalone Docker container with the same OS image and it connects successfully.\r\n\r\nWe can't be sure that Envoy is causing the issue here, but it's the main difference in configuration between the two systems. Are there any reasons why this would happen? Are we potentially missing a configuration, etc.?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20387/comments",
    "author": "ryan-motive",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2022-03-17T16:50:34Z",
        "body": "Is Envoy the TLS client or the TLS server? Can you post the Envoy configuration? Does the client indicate any more specific error?"
      },
      {
        "user": "ryan-motive",
        "created_at": "2022-03-17T16:55:49Z",
        "body": "In this case Envoy is the TLS client. We've found some documentation for the service/SDK we're using that it may be related to certificate checks failing. Not clear to me why Envoy would make a difference, though.\r\n\r\nWe've tried quite a few configurations, including trying to disable egress altogether. In any case the presence of Envoy seems to be the only common factor. Personally it really seems like Envoy is a red herring, but I wondered if there were any known issues with Envoy and TLS that might impact this."
      },
      {
        "user": "ggreenway",
        "created_at": "2022-03-17T18:01:17Z",
        "body": "There is one that could be related. See #18510 and evaluate if that applies to the TLS cert from the server or not."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-16T20:01:06Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-24T00:02:47Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20363,
    "title": "How does the proxy handles concurrency",
    "created_at": "2022-03-16T05:02:09Z",
    "closed_at": "2022-04-22T08:01:32Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20363",
    "body": "How is Envoy handling http header propagation with concurrent connections via the inbound and the outbound iptables ?\r\n\r\nIn depth:\r\nAs far as I understand the documentation, envoy has an inbound and outbound interceptors from the application container\r\nIs it blindly propagating the inbound headers to the outbound headers?\r\n\r\nis envoy dependent on a client-application to have tracing installed ?   (zipkin/opentelemetry etc.. ) ? \r\nor is it seamlessly managing to add tracing via the inbound/outbound interceptors? ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20363/comments",
    "author": "Ben-Mark",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-15T08:01:26Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-22T08:01:32Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20058,
    "title": "envoy oath2 refresh access token",
    "created_at": "2022-02-21T00:38:49Z",
    "closed_at": "2022-04-01T20:01:26Z",
    "labels": [
      "question",
      "area/docs",
      "stale",
      "area/oauth"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20058",
    "body": "This is more of a question than issue but I believe it could lead to few issues \r\n1. I do not see any documentation or example to get new access token using refresh_token. This will be much helpful. \r\n2. Is there a way to get new access token using refresh token using backend call?  Usecase I am looking for user activity happens from front end , call comes to backend (envoy) with refresh token (coming from cookie) then envoy makes call to authorisation endpoint to get new access token using refres_token. \r\n\r\nApologies for putting question here as the envoy-user group is not allowing me to post new conversation. I also believe documentation is very thin on Oauth2 filter. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20058/comments",
    "author": "saurabh256",
    "comments": [
      {
        "user": "adisuissa",
        "created_at": "2022-02-23T19:20:09Z",
        "body": "cc @rgs1 @derekargueta @snowp as oauth2 filter owners"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-25T20:01:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-01T20:01:26Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "thomasvargiu",
        "created_at": "2022-07-18T08:50:59Z",
        "body": "Hi! No news on this issue?"
      }
    ]
  },
  {
    "number": 19865,
    "title": "Build envoy failed with boringssl=fips",
    "created_at": "2022-02-08T01:57:02Z",
    "closed_at": "2022-03-17T16:06:23Z",
    "labels": [
      "question",
      "area/build",
      "area/tls",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19865",
    "body": "*Build envoy failed with boringssl=fips*\r\n\r\nI'm trying to build Envoy(release/v1.18) with fips enabled on Ubuntu 16.04 machine and I'm passing in the '--define boringssl=fips' in the build option below:\r\n\r\n`ENVOY_DOCKER_BUILD_DIR=\"<output-dir>\" ./ci/run_envoy_docker.sh 'BAZEL_BUILD_EXTRA_OPTIONS=--define=boringssl=fips ./ci/do_ci.sh bazel.release.server_only'`\r\n\r\nI get the following error. Any suggestions? Thank you!\r\n```\r\nERROR: /source/source/exe/BUILD:23:16 Executing genrule @boringssl_fips//:build failed (Exit 1): bash failed: error executing command\r\n  (cd /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/sandbox/processwrapper-sandbox/3384/execroot/envoy && \\\r\n  exec env - \\\r\n    BAZEL_COMPILER=clang \\\r\n    BAZEL_CXXOPTS='-stdlib=libc++' \\\r\n    BAZEL_LINKLIBS=-l%:libc++.a:-l%:libc++abi.a \\\r\n    BAZEL_LINKOPTS=-lm:-pthread \\\r\n    CC=clang \\\r\n    CXX=clang++ \\\r\n    CXXFLAGS='-stdlib=libc++' \\\r\n    LDFLAGS='-stdlib=libc++' \\\r\n    LLVM_CONFIG=/opt/llvm/bin/llvm-config \\\r\n    PATH=/opt/llvm/bin:/opt/llvm/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; #!/bin/bash\r\n```\r\n  \r\n  \r\n  \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19865/comments",
    "author": "rjiarui",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2022-02-08T04:38:45Z",
        "body": "I don't think there is sufficient information to understand what is happening here, CC @PiotrSikora @lizan "
      },
      {
        "user": "PiotrSikora",
        "created_at": "2022-02-08T05:53:34Z",
        "body": "I just sucessfully built `release/v1.18` branch using the provided command, so the reason is either specific to Ubuntu 16.04 (which is long EOL'ed) or maybe you're running out of disk space."
      },
      {
        "user": "madpsy",
        "created_at": "2022-02-08T12:17:11Z",
        "body": "I'm having similar issues, is this how you're meant to enable FIPS for a docker based build?\r\n\r\n`envoy git:(release/v1.18) ./ci/run_envoy_docker.sh 'BAZEL_BUILD_EXTRA_OPTIONS=--define=boringssl=fips' './ci/do_ci.sh bazel.release'`\r\n\r\n```Testing //test/... @com_googlesource_quiche//:ci_tests with options: --config=libc++ --verbose_failures --show_task_finish --experimental_generate_json_trace_profile --test_output=errors --noshow_progress --noshow_loading_progress --repository_cache=/build/repository_cache --experimental_repository_cache_hardlinks --define=boringssl=fips --nocache_test_results --test_env=ENVOY_MEMORY_TEST_EXACT=true\r\n$TEST_TMPDIR defined: output root default is '/build/tmp' and max_idle_secs default is '15'.\r\nINFO: Analyzed 1687 targets (1401 packages loaded, 97065 targets configured).\r\nINFO: Found 828 targets and 859 test targets...\r\nERROR: /source/test/test_common/BUILD:211:17: undeclared inclusion(s) in rule '//test/test_common:global_lib':\r\nthis rule is missing dependency declarations for the following files included by 'test/test_common/global.cc':\r\n  '/opt/llvm/lib/clang/11.0.1/include/stddef.h'\r\n  '/opt/llvm/lib/clang/11.0.1/include/__stddef_max_align_t.h'\r\n  '/opt/llvm/lib/clang/11.0.1/include/stdint.h'\r\n  '/opt/llvm/lib/clang/11.0.1/include/stdarg.h'\r\n  '/opt/llvm/lib/clang/11.0.1/include/limits.h'\r\n  '/opt/llvm/lib/clang/11.0.1/include/emmintrin.h'\r\n  '/opt/llvm/lib/clang/11.0.1/include/xmmintrin.h'\r\n  '/opt/llvm/lib/clang/11.0.1/include/mmintrin.h'\r\n  '/opt/llvm/lib/clang/11.0.1/include/mm_malloc.h'\r\nINFO: Elapsed time: 102.292s, Critical Path: 14.36s\r\nINFO: 399 processes: 398 internal, 1 processwrapper-sandbox.```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-10T16:05:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-17T16:06:22Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19825,
    "title": "Error attempting to build on Windows with provided docker image",
    "created_at": "2022-02-04T18:58:02Z",
    "closed_at": "2022-03-16T12:01:33Z",
    "labels": [
      "question",
      "area/build",
      "stale",
      "area/windows"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19825",
    "body": "I'm attempting to follow the instructions on the ci/README.md page, \"Developer use of CI docker images\" to build Envoy on Windows to try and diagnose an issue we're having.\r\n\r\nI'm running the shell script as described.\r\n```\r\n./ci/run_envoy_docker.sh './ci/windows_ci_steps.sh'\r\n```\r\nI'm running this from within a \"git bash\" shell, so MSYS shell. I'm not running this under WSL for example.\r\n\r\nIf I run the script as my normal windows user I get:\r\n\r\n```\r\nERROR: SymlinkDirectories(c:\\build\\tmp/_bazel_ContainerAdministrator/install/123df92719b107a86cd72886e258fa44, c:\\bui\r\nld\\tmp\\install): CreateJunction: ERROR: src/main/native/windows/file.cc(401): DeviceIoControl(\\\\?\\c:\\build\\tmp\\instal\r\nl): Access is denied.\r\n\r\nFATAL: failed to create installation symlink 'c:\\build\\tmp\\install': (error: 5): Access is denied.\r\n```\r\n\r\nHowever, if I try running the same thing from a command prompt running as administrator, I get the following instead:\r\n```\r\nERROR: SymlinkDirectories(c:\\build\\tmp/_bazel_ContainerAdministrator/install/123df92719b107a86cd72886e258fa44, c:\\bui\r\nld\\tmp\\install): CreateJunction:\r\nFATAL: failed to create installation symlink 'c:\\build\\tmp\\install': success\r\n```\r\n\r\nActually if I try deleting the complete tmp directory and starting again, I get the same \"Access Denied\" error as above. Interestingly the directory appears to be created anyway.\r\n\r\nI'm unclear what I need to do to make this work. The instructions for building envoy on Windows \"manually\" talk about making sure symlinks are enabled, or you're running as administrator and so on, but I was naively assuming that the shell script and docker image would be set up to do anything that needed to be done?\r\n\r\nDoes \"developer mode\" have to be turned on my machine in order for this to work, even though the build is being run inside a docker container?\r\n\r\nAnyway, any pointers anyone could give me on how to build on Windows using the docker image would be very welcome.\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19825/comments",
    "author": "tomqwpl",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2022-02-06T18:47:27Z",
        "body": "@envoyproxy/windows-dev @phlax "
      },
      {
        "user": "daixiang0",
        "created_at": "2022-02-07T05:00:38Z",
        "body": "Could you try opening cmd using admin permission?"
      },
      {
        "user": "tomqwpl",
        "created_at": "2022-02-07T10:33:45Z",
        "body": "@daixiang0 Apologies if my original problem description wasn't clear. It also doesn't work if I run the script from within a command shell running as admin."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-09T12:01:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-16T12:01:32Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19803,
    "title": "route mach on # not working",
    "created_at": "2022-02-03T17:45:46Z",
    "closed_at": "2022-02-09T14:59:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19803",
    "body": "using a config like this: \r\n\r\n```\r\nadmin:\r\n  address:\r\n    socket_address: {address: 0.0.0.0, port_value: 9901}\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener1\r\n    address:\r\n      socket_address: {address: 0.0.0.0, port_value: 10000}\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: demo\r\n          codec_type: AUTO\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match: {prefix: \"/something/#/status\"}\r\n                direct_response:\r\n                  status: 200\r\n                  body:\r\n                    inline_string: \"matched #\\n\"\r\n              - match: {prefix: \"/\"}\r\n                route: {cluster: demo, timeout: 60s}                \r\n          http_filters:\r\n          - name: envoy.filters.http.router\r\n```\r\n\r\n`curl localhost:10000/something/#/status` doesn't work \r\nTested with Prefix and Exact matching ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19803/comments",
    "author": "asayah",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2022-02-04T17:20:59Z",
        "body": "I wonder if we strip off the URL fragment before matching - @yanavlasov do you know?"
      },
      {
        "user": "lizan",
        "created_at": "2022-02-04T22:50:24Z",
        "body": "I don't think curl even send out URL fragment:\r\n\r\n```\r\ncurl -v 'localhost:10000/something/#/status'\r\n*   Trying 127.0.0.1:10000...\r\n* Connected to localhost (127.0.0.1) port 10000 (#0)\r\n> GET /something/ HTTP/1.1\r\n> Host: localhost:10000\r\n> User-Agent: curl/7.74.0\r\n> Accept: */*\r\n>\r\n* Mark bundle as not supporting multiuse\r\n* HTTP 1.0, assume close after body\r\n< HTTP/1.0 404 File not found\r\n```"
      },
      {
        "user": "yanavlasov",
        "created_at": "2022-02-09T14:59:31Z",
        "body": "By default Envoy rejects requests with fragment in the URI path. You can preserve fragment in path by setting both `envoy.reloadable_features.http_reject_path_with_fragment` and `envoy.reloadable_features.http_strip_fragment_from_path_unsafe_if_disabled` to false, but these flags will eventually go away.\r\n"
      }
    ]
  },
  {
    "number": 19798,
    "title": "upstream timeout is not working, if value is greater than 30 secs",
    "created_at": "2022-02-03T14:00:56Z",
    "closed_at": "2022-03-20T00:02:46Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19798",
    "body": "*Title*: *timeout_ms is not working, if value is greater than 30 secs*\r\n\r\n*Description*:\r\nI am using Ambassador for external authentication and rerouting, which in turn uses envoy proxy.\r\nI have deployed a service in the cluster which sleeps for 31 secs and returns 200 HTTP status code.\r\n\r\nAll my HTTP calls are failing with 502 bad gateway. Looking at logs, it seems to be timing out after 30 secs:\r\n\"POST /test-endpoint/run HTTP/1.1\" 0 DC 16 0 30000 - , ,\" \"insomnia/2021.7.2\" \"7819d615-9bdb-4f6b-a198-97f2ad89f023\" \"\" \"*:12000\r\n\r\nBelow is the envoy route:\r\n\r\n{\r\n\"match\": {\r\n\"case_sensitive\": true,\r\n\"prefix\": \"/test-endpoint/\",\r\n\"runtime_fraction\": {\r\n\"default_value\": {\r\n\"denominator\": \"HUNDRED\",\r\n\"numerator\": 100\r\n},\r\n\"runtime_key\": \"routing.traffic_shift.cluster_test_service_12000_test\"\r\n}\r\n},\r\n\"route\": {\r\n\"cluster\": \"cluster_test_service_12000_test\",\r\n\"cors\": {\r\n\"allow_credentials\": true,\r\n\"allow_headers\": \"\",\r\n\"allow_methods\": \"GET, HEAD, PUT, POST, DELETE, OPTIONS\",\r\n\"allow_origin_string_match\": [\r\n{\r\n\"exact\": \"\"\r\n}\r\n],\r\n\"filter_enabled\": {\r\n\"default_value\": {\r\n\"denominator\": \"HUNDRED\",\r\n\"numerator\": 100\r\n},\r\n\"runtime_key\": \"routing.cors_enabled.**\"\r\n},\r\n\"max_age\": \"86400\"\r\n},\r\n\"prefix_rewrite\": \"/\",\r\n\"priority\": null,\r\n\"timeout\": \"600.000s\"\r\n}\r\n}\r\n\r\nKubernetes environment : GKE\r\nKubernetes Version: 1.19.5\r\nAmbassador: 1.12.4\r\nEnvoy: 1.15.4\r\n\r\n[optional *Relevant Links*:]\r\n>I am looking for recommendation, on what I might be doing wrong, in my set up or do I need to configure some other timeout. Please, note I have also tried added x-envoy-upstream-rq-timeout-ms header.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19798/comments",
    "author": "charuhans",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2022-02-04T17:23:07Z",
        "body": "I don't think Envoy would ever return 502 Bad Gateway - do you have another layer that might be enforcing this timeout?"
      },
      {
        "user": "charuhans",
        "created_at": "2022-02-07T14:03:01Z",
        "body": "I do not have any other layer. Interesting thing for me in the log, upstream timeout is 30 sec:\r\n\"POST /test-endpoint/run HTTP/1.1\" 0 DC 16 0 **30000** - , ,\" \"insomnia/2021.7.2\" \"7819d615-9bdb-4f6b-a198-97f2ad89f023\" \"\" \"*:12000\r\n\r\nIf I put any value, less than 30 secs in the header: x-envoy-upstream-rq-timeout-ms.\r\nI see the correct value in upstream tiemout."
      },
      {
        "user": "lambdai",
        "created_at": "2022-02-10T23:22:38Z",
        "body": "The error code “DC\" in is downstream_termination, any possible that the client insomnia disconnected actively after 30 seconds?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-13T00:02:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-20T00:02:45Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19719,
    "title": "bazel build the http filter with envoy v1.16.0 failed by \"execv: Argument list too long\"",
    "created_at": "2022-01-27T15:08:23Z",
    "closed_at": "2022-01-28T14:59:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19719",
    "body": "I am trying to build the http filter with envoy v1.16.0 with following commands:\r\nbazel build //http-filter-example:envoy\r\n\r\nthe result of building is following with gnu7 or gnu9:\r\n\r\ngnu7\r\nwith bazel \"fatal error: cannot execute '/usr/lib/gcc/x86_64-linux-gnu/7/cc1plus': execv: Argument list too long\"\r\n\r\ngnu9\r\nwith bazel \"fatal error: cannot execute '/usr/lib/gcc/x86_64-linux-gnu/9/cc1plus': execv: Argument list too long\"\r\n\r\nThen I try to build the http filter with envoy v1.16.0 with following commands:\r\n(bazel build //http-filter-example:envoy -c dbg) or (bazel build //http-filter-example:envoy -c opt)\r\nIt will build successfully, however the binary file size of envoy is too big.\r\n\r\nthe default value of my environment by \"ulimit -s \" is 8192. Then I set “ulimit -s 65536” or  “ulimit -s 655360” , can't solve this problem yet. How can I solve this problem?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19719/comments",
    "author": "xajhljq",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-01-27T15:57:15Z",
        "body": "I'm pretty sure this has been fixed somehow in newer releases but I'm not completely sure. cc @yanavlasov @wrowe "
      },
      {
        "user": "yanavlasov",
        "created_at": "2022-01-28T14:59:45Z",
        "body": "Fixed in PR #16711"
      }
    ]
  },
  {
    "number": 19687,
    "title": "In the /stats/prometheus statistics list is there a method that will allow me to monitor http to grpc transcoded request per endpoint? ",
    "created_at": "2022-01-25T20:28:21Z",
    "closed_at": "2022-04-10T00:02:52Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19687",
    "body": "I'm trying to monitor at the envoy level all the http calls per endpoint that are grpc transcoded, but so far I was not able to locate a proper method to do this in the /stats/prometheus list.\r\n\r\nThis is what I'm using for grpc service, but I want something similar for envoy, if possible \r\n`sum (increase(grpc_server_processing_duration_seconds_count{namespace=~\\\"$Namespace\\\",job=~\\\"$Service\\\"}[1m])) by (method)`\r\n\r\nAny idea ?\r\n\r\nThanks\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19687/comments",
    "author": "ghevge",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-01-26T18:24:40Z",
        "body": "cc @qiwzhang "
      },
      {
        "user": "ghevge",
        "created_at": "2022-02-01T12:59:00Z",
        "body": "@qiwzhang any idea? Thanks"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-03T16:06:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "ghevge",
        "created_at": "2022-03-03T16:22:26Z",
        "body": "I'm still interested in this functionality. Thanks!"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-02T20:01:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-10T00:02:52Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19664,
    "title": "Connection reset generated by Envoy",
    "created_at": "2022-01-24T12:28:41Z",
    "closed_at": "2022-03-09T20:01:16Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19664",
    "body": "I am using **EnvoyProxy version : 1.15.2**. My usecase is a reverse proxy solution meant for multi-tenant system. \r\n\r\nAs part of my connection manager configuration : \r\n\r\n```\r\n- name: envoy.http_connection_manager\r\n   config:\r\n     stat_prefix: ingress_http\r\n     codec_type: AUTO\r\n     common_http_protocol_options:\r\n       max_connection_duration: 120s\r\n```\r\n\r\nIntermittently, my customers are seeing \"Connection reset by Peer\" exception in their code. I took TCPDump for one such customer, and saw following behavior : \r\n\r\n```\r\n19:15:19.286535 IP (tos 0x0, ttl 255, id 57091, offset 0, flags [DF], proto TCP (6), length 60)\r\n    10.18.191.67.44332 > 10.80.65.146.8080: Flags [S], cksum 0x1566 (incorrect -> 0xfb6a), seq 849832810, win 62727, options [mss 8961,sackOK,TS val 3662798824 ecr 0,nop,wscale 7], length 0\r\n\r\n19:15:19.288045 IP (tos 0x0, ttl 252, id 0, offset 0, flags [DF], proto TCP (6), length 60)\r\n    10.80.65.146.8080 > 10.18.191.67.44332: Flags [S.], cksum 0x8538 (correct), seq 2913470994, ack 849832811, win 26847, options [mss 1460,sackOK,TS val 297489952 ecr 3662798824,nop,wscale 8], length 0\r\n\r\n19:15:19.288053 IP (tos 0x0, ttl 255, id 57092, offset 0, flags [DF], proto TCP (6), length 52)\r\n    10.18.191.67.44332 > 10.80.65.146.8080: Flags [.], cksum 0x155e (incorrect -> 0x1af8), ack 1, win 491, options [nop,nop,TS val 3662798826 ecr 297489952], length 0\r\n\r\n19:15:19.288211 IP (tos 0x0, ttl 255, id 57093, offset 0, flags [DF], proto TCP (6), length 672)\r\n    10.18.191.67.44332 > 10.80.65.146.8080: Flags [P.], cksum 0x17ca (incorrect -> 0x93a8), seq 1:621, ack 1, win 491, options [nop,nop,TS val 3662798826 ecr 297489952], length 620: HTTP, length: 620\r\n\r\n19:15:19.289166 IP (tos 0x0, ttl 252, id 16385, offset 0, flags [DF], proto TCP (6), length 52)\r\n    10.80.65.146.8080 > 10.18.191.67.44332: Flags [.], cksum 0x1a09 (correct), ack 621, win 110, options [nop,nop,TS val 297489952 ecr 3662798826], length 0\r\n\r\n19:15:19.296689 IP (tos 0x0, ttl 252, id 16386, offset 0, flags [DF], proto TCP (6), length 505)\r\n    10.80.65.146.8080 > 10.18.191.67.44332: Flags [P.], cksum 0xdb48 (correct), seq 1:454, ack 621, win 110, options [nop,nop,TS val 297489953 ecr 3662798826], length 453: HTTP, length: 453\r\n\r\n19:15:19.296699 IP (tos 0x0, ttl 255, id 57094, offset 0, flags [DF], proto TCP (6), length 52)\r\n    10.18.191.67.44332 > 10.80.65.146.8080: Flags [.], cksum 0x155e (incorrect -> 0x16c1), ack 454, win 488, options [nop,nop,TS val 3662798834 ecr 297489953], length 0\r\n\r\n19:15:32.166437 IP (tos 0x0, ttl 255, id 57095, offset 0, flags [DF], proto TCP (6), length 723)\r\n    10.18.191.67.44332 > 10.80.65.146.8080: Flags [P.], cksum 0x17fd (incorrect -> 0x2472), seq 621:1292, ack 454, win 488, options [nop,nop,TS val 3662811704 ecr 297489953], length 671: HTTP, length: 671\r\n\r\n19:15:32.191067 IP (tos 0x0, ttl 252, id 16387, offset 0, flags [DF], proto TCP (6), length 779)\r\n    10.80.65.146.8080 > 10.18.191.67.44332: Flags [P.], cksum 0xe552 (correct), seq 454:1181, ack 1292, win 115, options [nop,nop,TS val 297491242 ecr 3662811704], length 727: HTTP, length: 727\r\n\r\n19:15:32.191079 IP (tos 0x0, ttl 255, id 57096, offset 0, flags [DF], proto TCP (6), length 52)\r\n    10.18.191.67.44332 > 10.80.65.146.8080: Flags [.], cksum 0x155e (incorrect -> 0xd9e7), ack 1181, win 483, options [nop,nop,TS val 3662811729 ecr 297491242], length 0\r\n\r\n19:16:17.470597 IP (tos 0x0, ttl 255, id 57097, offset 0, flags [DF], proto TCP (6), length 723)\r\n    10.18.191.67.44332 > 10.80.65.146.8080: Flags [P.], cksum 0x17fd (incorrect -> 0x3f3d), seq 1292:1963, ack 1181, win 483, options [nop,nop,TS val 3662857008 ecr 297491242], length 671: HTTP, length: 671\r\n\r\n19:16:17.491355 IP (tos 0x0, ttl 252, id 16388, offset 0, flags [DF], proto TCP (6), length 785)\r\n    10.80.65.146.8080 > 10.18.191.67.44332: Flags [P.], cksum 0xcb4d (correct), seq 1181:1914, ack 1963, win 121, options [nop,nop,TS val 297495772 ecr 3662857008], length 733: HTTP, length: 733\r\n\r\n19:16:17.491367 IP (tos 0x0, ttl 255, id 57098, offset 0, flags [DF], proto TCP (6), length 52)\r\n    10.18.191.67.44332 > 10.80.65.146.8080: Flags [.], cksum 0x155e (incorrect -> 0x11ca), ack 1914, win 478, options [nop,nop,TS val 3662857029 ecr 297495772], length 0\r\n\r\n19:16:28.241150 IP (tos 0x0, ttl 255, id 57099, offset 0, flags [DF], proto TCP (6), length 723)\r\n    10.18.191.67.44332 > 10.80.65.146.8080: Flags [P.], cksum 0x17fd (incorrect -> 0x828f), seq 1963:2634, ack 1914, win 478, options [nop,nop,TS val 3662867779 ecr 297495772], length 671: HTTP, length: 671\r\n\r\n19:16:28.259600 IP (tos 0x0, ttl 252, id 16389, offset 0, flags [DF], proto TCP (6), length 785)\r\n    10.80.65.146.8080 > 10.18.191.67.44332: Flags [P.], cksum 0x917e (correct), seq 1914:2647, ack 2634, win 126, options [nop,nop,TS val 297496849 ecr 3662867779], length 733: HTTP, length: 733\r\n\r\n19:16:28.259617 IP (tos 0x0, ttl 255, id 57100, offset 0, flags [DF], proto TCP (6), length 52)\r\n    10.18.191.67.44332 > 10.80.65.146.8080: Flags [.], cksum 0x155e (incorrect -> 0xde0d), ack 2647, win 473, options [nop,nop,TS val 3662867797 ecr 297496849], length 0\r\n\r\n19:17:25.287512 IP (tos 0x0, ttl 255, id 57101, offset 0, flags [DF], proto TCP (6), length 723)\r\n    10.18.191.67.44332 > 10.80.65.146.8080: Flags [P.], cksum 0x17fd (incorrect -> 0x83c7), seq 2634:3305, ack 2647, win 473, options [nop,nop,TS val 3662924825 ecr 297496849], length 671: HTTP, length: 671\r\n\r\n19:17:25.289906 IP (tos 0x0, ttl 252, id 16390, offset 0, flags [DF], proto TCP (6), length 52)\r\n    10.80.65.146.8080 > 10.18.191.67.44332: Flags [R.], cksum 0xe7b4 (correct), seq 2647, ack 3305, win 131, options [nop,nop,TS val 297502552 ecr 3662924825], length 0\r\n```\r\n\r\nIn this, because Envoy didnt generate ```[F.]``` flag, client never knew connection was triggered for close and received reset exception. Any ideas, why it can happen this way. Is there any property I am not making use of which can help in this scenario.\r\n\r\nThis is not happening in all the cases, but only for few of them.  Any leads in this would be very helpful.\r\n\r\n**Note** : configurations like delayed_close_timeout, drain_timeout, max_stream_duration are not set in configuration and defaults are used for them.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19664/comments",
    "author": "prateek0394",
    "comments": [
      {
        "user": "prateek0394",
        "created_at": "2022-01-31T14:15:53Z",
        "body": "Hi guys,\r\n\r\nCan someone help me in this?\r\n\r\nThanks."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-02T16:06:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-09T20:01:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "dvob",
        "created_at": "2022-11-23T22:38:42Z",
        "body": "@prateek0394 we have similar problems in our environment. To avoid such errors you should set the max connection time of the HTTP client (your customer) lower then the max connection time on Envoy. This way the client will terminate the connection and you don't have the case where Envoy is closing the connection and at the same time the client tries to send data.\r\nIn many cases it probably also makes more sense to use `idle_timeout`, because then you close the connection only if it was not used for that period.\r\n\r\nFrom what I understand in your tcpdump the following thing happens:\r\n```\r\n19:15:19 connection start\r\n19:17:19 (+120s max_connection_duration) \r\n  -> connection termination starts\r\n  -> header `Connection: close` is set\r\n19:17:24 (+5s drain_timeout) \r\n  -> no new data accepted, no FIN sent yet\r\n  -> requests are answered with connection reset\r\n19:17:25 (+1s delayed_close_timeout) envoy sends FIN\r\n```\r\n\r\nIf a request is sent after the `drain_timeout` and before the `delayed_close_timeout` elapses Envoy answers your request with a connection reset."
      }
    ]
  },
  {
    "number": 19606,
    "title": "Questions on Caching based on query param and POST requests",
    "created_at": "2022-01-19T10:06:50Z",
    "closed_at": "2022-02-27T12:01:24Z",
    "labels": [
      "question",
      "stale",
      "area/cache"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19606",
    "body": "Title: working with query parameters and POST requests\r\n\r\nDescription:\r\n\r\nQuestions :\r\n\r\n1. can Envoy be configured to take query parameters into account when it looks for cached document? That is, we need it to recognise the “/v2/ui/net/1/reports/discovery?limit=10\" and \"/v2/ui/net/1/reports/discovery?offset10&limit=10\" as different requests ? \r\n\r\n2. can Envoy be configured to cache idempotent POST requests while taking into account request body? Some caching proxies compute hash of the POST request, using its url, some headers and the body, and then use this hash as the key to find cached documents ? ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19606/comments",
    "author": "prajith-nair",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2022-01-20T21:07:09Z",
        "body": "cc @toddmgreer @jmarantz"
      },
      {
        "user": "toddmgreer",
        "created_at": "2022-01-21T07:24:24Z",
        "body": "1. Envoy's CacheFilter takes query parameters into account. “/v2/ui/net/1/reports/discovery?limit=10\" and \"/v2/ui/net/1/reports/discovery?offset10&limit=10\" are different URLs and produce different cache keys. (envoy.extensions.filters.http.cache.v3.CacheConfig.KeyCreatorParams defines config settings to override this, but those settings aren't yet implemented.)\r\n2. Envoy does not cache POST requests. RFC7231§4 requires that \"a cache MUST write through requests with methods that are unsafe (Section 4.2.1 of [RFC7231]) to the origin server\". While this does not entirely preclude caching POST messages, it does mean that a detailed design would be critically important. I'd be happy to help review such a design."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-20T08:01:37Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-27T12:01:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19599,
    "title": "How to let ext_authz see destination hosts?",
    "created_at": "2022-01-19T00:51:29Z",
    "closed_at": "2022-02-27T04:01:22Z",
    "labels": [
      "question",
      "stale",
      "area/ext_authz"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19599",
    "body": "*Title*: How can an ExtAuthz service see which destination host the request is supposed to be sent to?\r\n\r\n*Description*:\r\nI'm building an API gateway that will be handling requests for a reasonably large number (10-100) of backend services. I'm using ExtAuthz to authenticate and authorize requests before forwarding, but now I'd like my authorization logic to be able to react to which backend service the request is supposed to be routed to.\r\n\r\nIn all the examples I've found, the ExtAuthz filter is placed early in the chain, so it can't see how any subsequent routing functionality is planning to handle the request.  I've seen the workaround in #3876, which makes sense but would still require me to duplicate (at least!) those 8-10 lines per backend. I realize I can do this dynamically via xDS but I'm trying to get the job done with static configuration first.\r\n\r\nIs there some way to split the routing logic into two phases, where the desired destination is identified first, and the actual dispatch is done later?\r\n\r\nMaybe I'm better off with a multi-stage topology, where the first stage uses matchers etc. to identify the desired backend and tag the stage 1 outbound requests, and a subsequent authz service could read the tags... But I'm not sure how I'd go about that. 😅  ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19599/comments",
    "author": "acruise",
    "comments": [
      {
        "user": "acruise",
        "created_at": "2022-01-20T00:57:32Z",
        "body": "For now I guess I can copypasta a new `route_config/virtual_hosts` entry, as well as a matching cluster entry, per backend service"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-20T04:01:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-27T04:01:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19567,
    "title": "bazel build the http filter with envoy v1.16.0 failed by \"execv: Argument list too long\"",
    "created_at": "2022-01-17T09:40:21Z",
    "closed_at": "2022-03-05T16:01:15Z",
    "labels": [
      "question",
      "area/build",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19567",
    "body": "I am trying to build the http filter with envoy v1.16.0 with following commands:\r\nbazel build //http-filter-example:envoy\r\n\r\nthe result of building is following with gnu7 or gnu9:\r\n\r\ngnu7\r\nwith bazel \"fatal error: cannot execute '/usr/lib/gcc/x86_64-linux-gnu/7/cc1plus': execv: Argument list too long\"\r\n\r\ngnu9\r\nwith bazel \"fatal error: cannot execute '/usr/lib/gcc/x86_64-linux-gnu/9/cc1plus': execv: Argument list too long\"\r\n\r\nThen I try to build the http filter with envoy v1.16.0 with following commands:\r\n(bazel build //http-filter-example:envoy -c dbg) or (bazel build //http-filter-example:envoy -c opt)\r\nIt will build successfully, however the binary file size of envoy is too big.\r\n\r\nHow can I solve this problem in a faster way?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19567/comments",
    "author": "xajhljq",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2022-01-20T21:18:06Z",
        "body": "What's the `ulimit` of your environment?"
      },
      {
        "user": "xajhljq",
        "created_at": "2022-01-27T14:50:47Z",
        "body": "> What's the `ulimit` of your environment?\r\n\r\nthe default value of \"ulimit -s \" is 8192. Then I set “ulimit -s 65536”, can't solve this problem yet. How can I solve this problem in another way?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-26T16:01:11Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-05T16:01:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19559,
    "title": "How can I use gRPC libraries in an Envoy filter?",
    "created_at": "2022-01-15T06:48:49Z",
    "closed_at": "2022-01-21T05:49:05Z",
    "labels": [
      "question",
      "area/build"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19559",
    "body": "I am attempting to write a filter that uses the gRPC C++ libraries. \r\n\r\nThe BUILD file specifies gRPC libraries as dependencies:\r\n\r\n```\r\nload(\r\n    \"@envoy//bazel:envoy_build_system.bzl\",\r\n    \"envoy_cc_library\",\r\n    \"envoy_package\",\r\n)\r\n\r\nenvoy_package()\r\n\r\nenvoy_cc_library(\r\n    name = \"my_filter_lib\",\r\n    srcs = [\"my_filter_lib.cc\"],\r\n    hdrs = [\"my_filter_lib.h\"],\r\n    repository = \"@envoy\",\r\n    deps = [\r\n        \"//envoy_extensions/source:well_known_names\",\r\n        \"//envoy_extensions/protos/squareup/envoy:sq_extensions_cc_proto\",\r\n        \"@envoy//envoy/http:filter_interface\",\r\n        \"@envoy//source/common/http:headers_lib\",\r\n        \"@envoy//source/common/grpc:codec_lib\",\r\n        \"@envoy//source/common/grpc:common_lib\",\r\n        \"@envoy//source/common/common:minimal_logger_lib\",\r\n        \"@envoy//source/extensions/filters/http/common:pass_through_filter_lib\",\r\n        \"@com_github_grpc_grpc//:grpc++\",\r\n        \"@com_github_grpc_grpc//:grpc++_reflection\",\r\n        \"@com_github_grpc_grpc//:grpcpp_admin\",\r\n    ],\r\n)\r\n\r\nenvoy_cc_library(\r\n    name = \"config\",\r\n    srcs = [\"config.cc\"],\r\n    hdrs = [\"config.h\"],\r\n    repository = \"@envoy\",\r\n    deps = [\r\n        \":json_grpc_shim_lib\",\r\n        \"@envoy//envoy/http:filter_interface\",\r\n        \"@envoy//envoy/registry\",\r\n        \"@envoy//envoy/server:filter_config_interface\",\r\n        \"@envoy//source/extensions/filters/http/common:factory_base_lib\",\r\n        \"//envoy_extensions/protos/squareup/envoy:sq_extensions_cc_proto\",\r\n    ],\r\n)\r\n```\r\n\r\nWhen I compile the filter in isolation, it compiles fine. \r\nWhen I try to compile it as part of Envoy, I get the following multiple definition errors:\r\n\r\n```\r\n/usr/bin/ld.gold: bazel-out/k8-fastbuild/bin/external/com_github_grpc_grpc/src/proto/grpc/testing/xds/v3/_objs/_percent_proto_only/percent.pb.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-fastbuild/bin/external/envoy_api/envoy/type/v3/_objs/pkg/percent.pb.pic.o: multiple definition of 'envoy::type::v3::FractionalPercent::~FractionalPercent()'\r\n/usr/bin/ld.gold: bazel-out/k8-fastbuild/bin/external/com_github_grpc_grpc/src/proto/grpc/testing/xds/v3/_objs/_percent_proto_only/percent.pb.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-fastbuild/bin/external/envoy_api/envoy/type/v3/_objs/pkg/percent.pb.pic.o: multiple definition of 'envoy::type::v3::FractionalPercent::~FractionalPercent()'\r\n/usr/bin/ld.gold: bazel-out/k8-fastbuild/bin/external/com_github_grpc_grpc/src/proto/grpc/testing/xds/v3/_objs/_percent_proto_only/percent.pb.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-fastbuild/bin/external/envoy_api/envoy/type/v3/_objs/pkg/percent.pb.pic.o: multiple definition of 'envoy::type::v3::FractionalPercent::~FractionalPercent()'\r\n/usr/bin/ld.gold: bazel-out/k8-fastbuild/bin/external/com_github_grpc_grpc/src/proto/grpc/testing/xds/v3/_objs/_percent_proto_only/percent.pb.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-fastbuild/bin/external/envoy_api/envoy/type/v3/_objs/pkg/percent.pb.pic.o: multiple definition of 'envoy::type::v3::FractionalPercent::ArenaDtor(void*)'\r\n/usr/bin/ld.gold: bazel-out/k8-fastbuild/bin/external/com_github_grpc_grpc/src/proto/grpc/testing/xds/v3/_objs/_percent_proto_only/percent.pb.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-fastbuild/bin/external/envoy_api/envoy/type/v3/_objs/pkg/percent.pb.pic.o: multiple definition of 'envoy::type::v3::FractionalPercent::SetCachedSize(int) const'\r\n/usr/bin/ld.gold: bazel-out/k8-fastbuild/bin/external/com_github_grpc_grpc/src/proto/grpc/testing/xds/v3/_objs/_percent_proto_only/percent.pb.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-fastbuild/bin/external/envoy_api/envoy/type/v3/_objs/pkg/percent.pb.pic.o: multiple definition of 'envoy::type::v3::FractionalPercent::Clear()'\r\n/usr/bin/ld.gold: bazel-out/k8-fastbuild/bin/external/com_github_grpc_grpc/src/proto/grpc/testing/xds/v3/_objs/_percent_proto_only/percent.pb.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-fastbuild/bin/external/envoy_api/envoy/type/v3/_objs/pkg/percent.pb.pic.o: multiple definition of 'envoy::type::v3::FractionalPercent::_InternalParse(char const*, google::protobuf::internal::ParseContext*)'\r\n```\r\n\r\n**How can I include these dependencies in a way that does not cause definition collisions?**",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19559/comments",
    "author": "hq6",
    "comments": [
      {
        "user": "daixiang0",
        "created_at": "2022-01-19T02:22:13Z",
        "body": "I think you can check global rate limit filter as it calls outside server via gRPC."
      },
      {
        "user": "lizan",
        "created_at": "2022-01-20T21:14:34Z",
        "body": "It might be an issue of how your proto libraries are built, what's the BUILD looks like for `\"//envoy_extensions/protos/squareup/envoy:sq_extensions_cc_proto\"`?"
      },
      {
        "user": "hq6",
        "created_at": "2022-01-21T05:48:37Z",
        "body": "```\r\nload(\"@envoy_api//bazel:api_build_system.bzl\", \"api_cc_py_proto_library\")\r\n\r\nlicenses([\"notice\"])  # Apache 2\r\n\r\napi_cc_py_proto_library(\r\n    name = \"auth_policy_proto\",\r\n    srcs = [\"auth_policy.proto\"],\r\n)\r\n\r\napi_cc_py_proto_library(\r\n    name = \"sq_extensions\",\r\n    srcs = [\"sq_extensions.proto\", \"log_stream.proto\"],\r\n    visibility = [\r\n        \"//visibility:public\",\r\n    ],\r\n)\r\n```\r\n"
      },
      {
        "user": "hq6",
        "created_at": "2022-01-21T05:49:05Z",
        "body": "Looks like removing the following line fixed it:\r\n```\r\n        \"@com_github_grpc_grpc//:grpcpp_admin\",\r\n```"
      }
    ]
  },
  {
    "number": 19556,
    "title": "Working with Lua and EnvoyFilter in Istio environment",
    "created_at": "2022-01-14T17:20:14Z",
    "closed_at": "2022-03-02T08:06:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19556",
    "body": "Description:\r\nI'm trying to add the client certificate URL encoded as an header to my service using EnvoyFilter.\r\n\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: test-filter\r\n  namespace: istio-system\r\nspec:\r\n  configPatches:\r\n  - applyTo: HTTP_FILTER\r\n    match:\r\n      context: GATEWAY\r\n      listener:\r\n        filterChain:\r\n          filter:\r\n            name: \"envoy.http_connection_manager\"\r\n            subFilter:\r\n              name: \"envoy.router\"\r\n    patch:\r\n      operation: INSERT_BEFORE\r\n      value: # lua filter specification\r\n        name: envoy.filters.http.lua\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n          inlineCode: |\r\n            function envoy_on_request(handle)\r\n              local headers = handle:headers()\r\n              local streamInfo = handle:streamInfo()\r\n              dsc = streamInfo:downstreamSslConnection()\r\n              if dsc ~= nil then\r\n                if dsc:peerCertificatePresented() then\r\n                  headers:add(\"x-peer-given\", \"yes\")\r\n                else\r\n                  headers:add(\"x-peer-given\", \"no\") #this is added\r\n                end\r\n                headers:add(\"x-ssl-client-certificate\", dsc:urlEncodedPemEncodedPeerCertificateChain()) #this is added and empty\r\n              end\r\n            end\r\n```\r\nAnd when I try to address to my service I get that a peer certificate wasn't presented.\r\nThe client request works in nginx which I pass the certificate using the following configuration:\r\nproxy_set_header X-SSL-Client-Certificate $ssl_client_escaped_cert;\r\nAm I missing something with Envoy? Is there a better way?\r\n\r\n[optional Relevant Links:]\r\nI used the Lua filter docs.\r\nNginx SSL embedded variables\r\nIstio gateway:\r\n\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: Gateway\r\nmetadata:\r\n  name: istio-gateway\r\n  namespace: istio-system\r\nspec:\r\n  selector:\r\n    istio: ingressgateway\r\n  servers:\r\n  - hosts:\r\n    - example.com\r\n    port:\r\n      name: https\r\n      number: 443\r\n      protocol: HTTPS\r\n    tls:\r\n      credentialName: gateway-certificate #cert manager generated cert\r\n      mode: SIMPLE\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19556/comments",
    "author": "anas-hassari-jemmic",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2022-01-20T21:12:08Z",
        "body": "It seems Lua filter doesn't see client certificate, any idea @dio?"
      },
      {
        "user": "anas-hassari-jemmic",
        "created_at": "2022-01-24T07:51:06Z",
        "body": "> It seems Lua filter doesn't see client certificate, any idea @dio?\r\n\r\nIs it a bug in envoy or is something not done right on my side?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-23T08:01:11Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-02T08:06:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "SachinHg",
        "created_at": "2023-12-20T08:04:45Z",
        "body": "@anas-hassari-jemmic  were you able to get this working?"
      },
      {
        "user": "anas-hassari-jemmic",
        "created_at": "2023-12-20T08:20:06Z",
        "body": "nope."
      },
      {
        "user": "SachinHg",
        "created_at": "2023-12-20T08:31:10Z",
        "body": "@anas-hassari-jemmic  any workaround you used or any other approach? I am hitting the exact same issue in my usecase too."
      },
      {
        "user": "anas-hassari-jemmic",
        "created_at": "2023-12-20T08:49:58Z",
        "body": "We've put a WAF before the istio gateway, for this exact purpose, the WAF will basically extract the client certificate from the incoming request, put it in headers and forward it to the upstream, it's a filthy workaround because we had to add logic in the application to handle this exact use case. If you don't have control over the application then I guess it's a gg."
      }
    ]
  },
  {
    "number": 19555,
    "title": "Transport Socket TCP stats  - emitted counters stay at 0",
    "created_at": "2022-01-14T15:01:04Z",
    "closed_at": "2022-01-21T06:40:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19555",
    "body": "Hello,\r\nTransport Socket TCP stats  - emitted counters stay at 0, please find below info about the setup.\r\n\r\nEnvoy: version: a9d72603c68da3a10a1c0d021d01c7877e6f2a30/1.21.0/Clean/RELEASE/BoringSSL\r\nOS: CentOS Linux 7\r\n\r\nCluster config:\r\n..\r\n```\r\ntransport_socket:\r\n    name: envoy.transport_sockets.upstream\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tcp_stats.v3.Config\r\n      update_period: 5s\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n```\r\n          \r\n\r\nstats emitted:\r\nenvoy_cluster_tcp_stats_cx_rx_data_segments{envoy_cluster_name=\"abc\"} 0\r\nTYPE envoy_cluster_tcp_stats_cx_rx_segments counter\r\nenvoy_cluster_tcp_stats_cx_rx_segments{envoy_cluster_name=\"abc\"} 0\r\nTYPE envoy_cluster_tcp_stats_cx_tx_data_segments counter\r\n\r\nThanks!\r\nBR,\r\nStoyan\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19555/comments",
    "author": "pxpnetworks",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2022-01-20T17:13:06Z",
        "body": "Do all of the `tcp_stats` stats stay at zero, or just these? What version of the linux kernel are you using?"
      },
      {
        "user": "pxpnetworks",
        "created_at": "2022-01-21T06:40:14Z",
        "body": "@ggreenway , thanks for the hint - turns out on this particular server the kernel was old version (3.10).\r\nTested this transport socket on another server with kernel version >5 and now i see Envoy stats for all _tcp_stats_ metrics !"
      }
    ]
  },
  {
    "number": 19552,
    "title": "Deployed Web App Not Secure with Valid Certificate",
    "created_at": "2022-01-14T12:59:05Z",
    "closed_at": "2022-01-19T23:15:04Z",
    "labels": [
      "question",
      "area/tls"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19552",
    "body": "I have a web app written in the following tech stack:\r\n\r\n- Web Server: Nginx\r\n- UI: Flutter/Dart\r\n- Backend: Go\r\n- Communication Protocol: gRPC/gRPC-Web\r\n- HTTP1.1 to HTTP2 Proxy: Envoy Proxy (Dockerized)\r\n- SSL Certificate Provider: LetsEncrypt Certbot\r\n- Environment: Ubuntu 20.10\r\n\r\nWhen I access the site, initially I see the lock on chrome showing the site is secure, however when the page actually loads, I see `Not Secure`, though the certificate is valid according to chrome and I can't figure out why.\r\n\r\nAs mentioned above, I am using a certificate generated by LetsEncrypt's Certbot which I have used a bunch of times in the past, though with REST instead of gRPC (Nginx web server, node backend), and have never had this issue, so I'm assuming it's an issue with the way I have configured Envoy or my backend service with TLS.\r\n\r\nHere is my `Dockerfile`:\r\n\r\n    FROM envoyproxy/envoy:v1.20-latest\r\n    COPY ./envoy.yaml /etc/envoy/envoy.yaml\r\n    ADD ./ssl_certs/privkey.pem /etc/envoy/privkey.pem\r\n    ADD ./ssl_certs/fullchain.pem /etc/envoy/fullchain.pem\r\n    CMD /usr/local/bin/envoy -c /etc/envoy/envoy.yaml -l debug\r\n\r\nMy `envoy.yaml`:\r\n\r\n    static_resources:\r\n      listeners:\r\n        - name: listener_0\r\n          address:\r\n            socket_address: { address: 0.0.0.0, port_value: 9000 }\r\n          filter_chains:\r\n          - filters:\r\n            - name: envoy.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                codec_type: auto\r\n                stat_prefix: ingress_http\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                  - name: local_service\r\n                    domains: [\"*\"]\r\n                    routes:\r\n                    - match: { prefix: \"/\" }\r\n                      route:\r\n                        cluster: greeter_service\r\n                        max_grpc_timeout: 0s\r\n                    cors:\r\n                      allow_origin_string_match:\r\n                      - prefix: \"*\"\r\n                      allow_methods: GET, PUT, DELETE, POST, OPTIONS\r\n                      allow_headers: keep-alive,user-agent,cache-control,content-type,content-transfer-encoding,custom-header-1,x-accept-content\r\n    -transfer-encoding,x-accept-response-streaming,x-user-agent,x-grpc-web,grpc-timeout\r\n                      max_age: \"1728000\"\r\n                      expose_headers: custom-header-1,grpc-status,grpc-message\r\n                http_filters:\r\n                - name: envoy.filters.http.cors\r\n                - name: envoy.filters.http.grpc_web\r\n                - name: envoy.filters.http.router\r\n            transport_socket:\r\n              name: envoy.transport_sockets.tls\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n                common_tls_context:\r\n                  tls_certificates:\r\n                  - certificate_chain: {filename: \"/etc/envoy/fullchain.pem\"}\r\n                    private_key: {filename: \"/etc/envoy/privkey.pem\"}\r\n      clusters:\r\n        - name: greeter_service\r\n          connect_timeout: 0.25s\r\n          type: logical_dns\r\n          lb_policy: round_robin\r\n          http2_protocol_options: {}\r\n          load_assignment:\r\n            cluster_name: cluster_0\r\n            endpoints:\r\n              - lb_endpoints:\r\n                  - endpoint:\r\n                      address:\r\n                        socket_address:\r\n                          address: 127.0.0.1\r\n                          port_value: 9001\r\n          transport_socket:\r\n            name: envoy.transport_sockets.tls\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n              common_tls_context:\r\n                tls_certificates:\r\n                - certificate_chain: {filename: \"/etc/envoy/fullchain.pem\"}\r\n                  private_key: {filename: \"/etc/envoy/privkey.pem\"}\r\n\r\n\r\n\r\nAnd my Go controller:\r\n\r\n    import (\r\n            \"fmt\"\r\n            \"log\"\r\n            \"net\"\r\n            \"crypto/tls\"\r\n            \"google.golang.org/grpc\"\r\n            \"google.golang.org/grpc/credentials\"\r\n            protos \"example.com/protos\"\r\n            \"example.com/repositories\"\r\n            \"example.com/services\"\r\n    )\r\n    const (\r\n            port = 9001\r\n    )\r\n    var (\r\n            grpcServer *grpc.Server\r\n    )\r\n    func loadTLSCredentials() (credentials.TransportCredentials, error) {\r\n            // Load TLS certificate and key\r\n            serverCert, err := tls.LoadX509KeyPair(\"/etc/letsencrypt/live/example.com/fullchain.pem\", \"/etc/letsencrypt/live/example.com/privkey.pem\")\r\n            if err != nil {\r\n                    return nil, err\r\n            }\r\n            // Create TLS credentials\r\n            config := &tls.Config {\r\n                    Certificates: []tls.Certificate{serverCert},\r\n                    ClientAuth:   tls.NoClientCert,\r\n            }\r\n            return credentials.NewTLS(config), nil\r\n    }\r\n    // StartServer starts gRPC server\r\n    func StartServer() {\r\n            log.Println(\"Starting server\")\r\n            tlsCredentials, err := loadTLSCredentials()\r\n            if err != nil {\r\n                    log.Fatalf(\"Unable to load TLS Credentials: %v\\n \", err);\r\n            }\r\n            listener, err := net.Listen(\"tcp4\", fmt.Sprintf(\":%v\", port))\r\n            if err != nil {\r\n                    log.Fatalf(\"Unable to listen to port %v\\n%v\\n\", port, err)\r\n            }\r\n            repositories.ConnectToMongoDB()\r\n            grpcServer = grpc.NewServer(\r\n                    grpc.Creds(tlsCredentials),\r\n                    // grpc.UnaryInterceptor(interceptor.Unary()),\r\n                    // grpc.StreamInterceptor(interceptor.Stream()),\r\n            )\r\n            registerServices()\r\n            if err = grpcServer.Serve(listener); err != nil {\r\n                    log.Fatalf(\"Failed to serve gRPC\\n%v\\n\", err)\r\n            }\r\n    }\r\n\r\n\r\nAll of my services are using the same `fullchain.pem` and `privkey.pem`, I had copied it into `ssl_certs` manually which is why I can access it in 2 places.\r\n\r\nWhat am I doing wrong? At first, I was unable to call my backend service when Nginx was set to listen to HTTPS only and I had not yet configured Envoy or my backend service with HTTPS, which is what I had expected since the webapp itself had SSL enabled. Once I added SSL to Envoy and my backend service, as shown above, the API calls were successful and I see the correct responses, however the site is marked `Not secure`, even with a valid certificate. I am at a loss. All of the API calls being made are also `https` calls, and not `http`.\r\n\r\nThis is my first time posting an issue, I apologize if this is not the correct way or place for this. Please let me know if that's the case and I can update/move it accordingly.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19552/comments",
    "author": "shash222",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2022-01-14T14:51:09Z",
        "body": " @lizan @phlax @ggreenway"
      },
      {
        "user": "shash222",
        "created_at": "2022-01-19T21:21:34Z",
        "body": "I was hoping to follow up on this, I haven't been able to find a solution yet. Any ideas why this is happening?"
      },
      {
        "user": "ggreenway",
        "created_at": "2022-01-19T22:04:43Z",
        "body": "Trying to make sure I understand correctly: everything works EXCEPT chrome says it is \"not secure\"?"
      },
      {
        "user": "shash222",
        "created_at": "2022-01-19T22:09:22Z",
        "body": "@ggreenway Correct. After attaching the certbot-generated SSL/TLS certificate, I am able to load the web app on `https`, and am able to make gRPC calls as well over https, and I get the expected responses. The only issue is that chrome says \"Not secure\""
      },
      {
        "user": "ggreenway",
        "created_at": "2022-01-19T22:48:29Z",
        "body": "I don't know what criteria chrome uses to determine whether a site is \"secure\" or not. I wonder if it's related to Certificate Transparency?"
      },
      {
        "user": "shash222",
        "created_at": "2022-01-19T23:15:04Z",
        "body": "So I tried loading the application on Chrome, Chrome Incognito, Firefox, and Edge and got different behaviors for each one:\r\n\r\nChrome:\r\nAs mentioned above, page loads, API calls are successful, but Chrome says \"Not Secure\"\r\n\r\nChrome Incognito:\r\nSame as Chrome\r\n\r\nFirefox:\r\nApplication loads and site is secure, but gRPC calls failed\r\n`OPTIONS` call failed with `CORS Failed`\r\n`POST` call failed\r\n\r\nMicrosoft Edge\r\nApplication loads and site is secure, but gRPC calls failed\r\nBoth `OPTIONS` and `POST` call failed with `ERR_CERT_COMMON_NAME_INVALID`\r\n\r\n\r\n\r\nI checked the warning on the Firefox page, and it looked like that had failed the API calls because I was calling the API on the server IP instead of the domain. This was an issue because the certificate was only valid for the domain.\r\n\r\nI updated the client to use the domain instead of the server IP, and it looks like the browsers are now happy with the change. Hopefully this helps anyone else that's facing the same issue.\r\n\r\n\r\nTLDR\r\nThe issue was not a server-side issue, it looks like my configuration was correct for both envoy and my go server. The issue lied with the URL I was calling through gRPC. I was calling my server's IP address instead of my domain, which is the only address that my certificate was configured for, but updating that fixed the issue."
      }
    ]
  },
  {
    "number": 19549,
    "title": "Need more guidance on success_rate_stdev_factor of Outlier_Detection",
    "created_at": "2022-01-14T07:39:49Z",
    "closed_at": "2022-02-20T20:01:21Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19549",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: Need more guidance on success_rate_stdev_factor of Outlier_Detection\r\n\r\n*Description*:\r\n>I'm currently testing Envoy's Outlier_Detection.\r\nI'm conducting a test related to the success rate, but it is difficult to understand the meaning of success_rate_stdev_factor only with the description of the website.\r\nFrom what I've searched, I can't find any examples of using success_rate_stdev_factor.\r\nThere are too many difficulties to understand this expression --> 'mean - (stdev * success_rate_stdev_factor)'\r\nAdditional explanation is needed on how to use this setting.\r\nPlease help me\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19549/comments",
    "author": "Lee-Jong-Woo",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2022-01-14T12:52:27Z",
        "body": "cc @junr03 "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-13T16:01:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-20T20:01:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19477,
    "title": "Is it possible to get a handle to the current upstream gRPC channel inside a C++ filter?",
    "created_at": "2022-01-11T08:02:55Z",
    "closed_at": "2022-01-11T18:38:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19477",
    "body": "I'm using Envoy as a sidecar and trying to build an Envoy filter that uses gRPC reflection.\r\n\r\nAs far as I know (please correct if wrong), Envoy already has a pool of channels for speaking gRPC to upstream when Envoy is configured to proxy gRPC.\r\nIs there a way to retrieve a pointer to one of the channels from inside a filter?\r\nAlternately, is there a way to retrieve the configured gPRC ingress port?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19477/comments",
    "author": "hq6",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2022-01-11T15:16:18Z",
        "body": "Unfortunately there's no way for a downstream filter to gt access to the upstream connection, and upstream filters are not implemented yet.\r\nThere should be address info once the upstream connection is established - check out stream info for upstream address etc."
      },
      {
        "user": "hq6",
        "created_at": "2022-01-11T18:38:37Z",
        "body": "Thanks for explaining! :)"
      }
    ]
  },
  {
    "number": 19348,
    "title": "Will we support connection transfer when hot restart?",
    "created_at": "2021-12-23T16:16:30Z",
    "closed_at": "2022-01-06T07:49:40Z",
    "labels": [
      "question",
      "area/hot_restart"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19348",
    "body": "*Title*: Will we support connection transfer when host restart?\r\n\r\n*Description*:\r\n\r\nMosn has supported connection  transfer when hot restart? will envoy support this?\r\n\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19348/comments",
    "author": "cwdsuzhou",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2022-01-04T21:25:28Z",
        "body": "We don't have any plans to support this.\r\n\r\nTo implement, it is neccessary to transfer all of the state of connection handling from the old process to the new process. For all cases except a very simple tcp_proxy configuration, this is extremely complicated. In the case of http and TLS, it is probably not possible because the libraries we use for those protocols don't support this. \r\n\r\nSo given that it could only be done for an extemely small subset of common envoy configurations, it has not so far seemed worthwhile."
      },
      {
        "user": "cwdsuzhou",
        "created_at": "2022-01-06T07:49:40Z",
        "body": "> We don't have any plans to support this.\r\n> \r\n> To implement, it is neccessary to transfer all of the state of connection handling from the old process to the new process. For all cases except a very simple tcp_proxy configuration, this is extremely complicated. In the case of http and TLS, it is probably not possible because the libraries we use for those protocols don't support this.\r\n> \r\n> So given that it could only be done for an extemely small subset of common envoy configurations, it has not so far seemed worthwhile.\r\n\r\n@ggreenway thanks for your reply!"
      }
    ]
  },
  {
    "number": 19337,
    "title": "Getting reset reason: protocol error when working with nodejs",
    "created_at": "2021-12-22T07:10:09Z",
    "closed_at": "2022-01-29T20:01:37Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19337",
    "body": "Hi!\r\nI used to work with envoy as a proxy and load balancer for nginx servers and everything worked fine.\r\nWhen I try use it for nodejs project I get strange error `upstream connect error or disconnect/reset before headers. reset reason: protocol error` (http code 502).\r\nThats my config:\r\n```\r\nadmin:\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 9900 }\r\n\r\nstatic_resources:\r\n  listeners:\r\n    - name: node-test\r\n      address:\r\n        socket_address: { address: 0.0.0.0, port_value: 8082 }\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: http\r\n                codec_type: AUTO\r\n                always_set_request_id_in_response: true\r\n                use_remote_address: false\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains: [ \"test-1.mydomain.com\", \"test-2.mydomain.com\" ]\r\n                      include_request_attempt_count: true\r\n                      include_attempt_count_in_response: true\r\n                      retry_policy:\r\n                        retry_on: \"5xx\"\r\n                        num_retries: 5\r\n                        host_selection_retry_max_attempts: 1\r\n                      routes:\r\n                        - match:\r\n                            safe_regex:\r\n                              google_re2: { }\r\n                              regex: \"\\/(img|uploaded-images|uploaded-files|storage|js|css|sitemap).*\"\r\n                          route:\r\n                            timeout: 60s\r\n                            cluster: static\r\n                        - match: { prefix: \"/\" }\r\n                          route:\r\n                            timeout: 60s\r\n                            cluster: backend-nodejs\r\n                http_filters:\r\n                  - name: envoy.filters.http.router\r\n  clusters:\r\n    - name: backend-nodejs\r\n      connect_timeout: 15s\r\n      type: STATIC\r\n      lb_policy: LEAST_REQUEST\r\n      circuit_breakers:\r\n        thresholds:\r\n          - priority: \"HIGH\"\r\n            max_connections: 1\r\n            max_requests: 1\r\n            max_pending_requests: 1\r\n      max_requests_per_connection: 15\r\n      http_protocol_options: {}\r\n      health_checks:\r\n        timeout: 10s\r\n        interval: 60s\r\n        unhealthy_threshold: 1\r\n        healthy_threshold: 1\r\n        http_health_check:\r\n          expected_statuses: { start: 200, end: 400 }\r\n          request_headers_to_add: [\r\n              header: {\r\n                \"key\": \"User-Agent\",\r\n                \"value\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\r\n              }\r\n          ]\r\n          path: \"/\"\r\n      load_assignment:\r\n        cluster_name: backend_nodejs_cluster\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  hostname: \"request-nodejs-1\"\r\n                  address:\r\n                    socket_address:\r\n                      address: --changed server ip--\r\n                      port_value: 801\r\n              - endpoint:\r\n                  hostname: \"request-nodejs-2\"\r\n                  address:\r\n                    socket_address:\r\n                      address: --changed server ip--\r\n                      port_value: 802\r\n              - endpoint:\r\n                  hostname: \"request-nodejs-3\"\r\n                  address:\r\n                    socket_address:\r\n                      address: --changed server ip--\r\n                      port_value: 803\r\n              - endpoint:\r\n                  hostname: \"request-nodejs-4\"\r\n                  address:\r\n                    socket_address:\r\n                      address: --changed server ip--\r\n                      port_value: 804\r\n              - endpoint:\r\n                  hostname: \"request-nodejs-5\"\r\n                  address:\r\n                    socket_address:\r\n                      address: --changed server ip--\r\n                      port_value: 805\r\n    - name: static\r\n      connect_timeout: 30s\r\n      type: STATIC\r\n      lb_policy: LEAST_REQUEST\r\n      circuit_breakers:\r\n        thresholds:\r\n          - priority: \"DEFAULT\"\r\n            max_requests: 10\r\n            max_pending_requests: 5\r\n            retry_budget:\r\n              budget_percent:\r\n                value: 25.0\r\n              min_retry_concurrency: 10\r\n      load_assignment:\r\n        cluster_name: backend_cluster\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  hostname: \"static-1\"\r\n                  address:\r\n                    socket_address:\r\n                      address: 10.110.32.14\r\n                      port_value: 80\r\n```\r\nFor static it works great. But for `backend_nodejs_cluster` I get this error.\r\nIf I request server-ip:801/802/etc I get correct response. On server side I use docker with node:latest image. So it ​many instances on one docker image working on different port",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19337/comments",
    "author": "rseasta",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-22T20:01:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-29T20:01:37Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19272,
    "title": "extract jwt token to header with jwtProvider",
    "created_at": "2021-12-13T20:22:32Z",
    "closed_at": "2022-01-20T20:01:43Z",
    "labels": [
      "question",
      "stale",
      "area/configuration",
      "area/jwt_authn"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19272",
    "body": "**I want to extract jwt token that was received from the request header and put the properties into header**\r\n**I tied this yaml file but I got an error**\r\n**i have a problem with defining http_filters using the available extension in envoy such as jwtHeader and jwtProvider,...**\r\n\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - name: main\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 1337\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          codec_type: AUTO\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  host_rewrite_literal: 0.0.0.0\r\n                  cluster: web_service\r\n          http_filters:\r\n          - name: config.filter.http.jwt_authn.v2alpha.JwtHeader // I guess this file might change\r\n            from_params:\r\n            - jwt_token\r\n          - name: envoy.filters.http.lua\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n              inline_code: |\r\n                function envoy_on_request(request_handle)\r\n                  local meta = request_handle:streamInfo():dynamicMetadata()\r\n                  for key, value in pairs(meta) do\r\n                    request_handle:logInfo(\"extract dynamicMetadata key: \"..key)\r\n                    request_handle:logInfo(\"extract dynamicMetadata value: \"..value.jwt_payload.usr)\r\n                  end\r\n                end\r\n          - name: envoy.filters.http.router\r\n  clusters:\r\n  - name: web_service\r\n    connect_timeout: 5s\r\n    type: STRICT_DNS  # static\r\n    load_assignment:\r\n      cluster_name: web_service\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 0.0.0.0\r\n                port_value: 3000\r\n```\r\n\r\n\r\n**I really appreciate someone helping me**\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19272/comments",
    "author": "0xbeny",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2021-12-14T14:58:24Z",
        "body": "cc @dio "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-13T16:06:10Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-20T20:01:43Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19218,
    "title": "Add a string to a command operator - Envoy Access logging",
    "created_at": "2021-12-08T11:16:02Z",
    "closed_at": "2022-01-23T04:01:23Z",
    "labels": [
      "question",
      "stale",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19218",
    "body": "Hello,\r\n\r\nI am trying to add a string to a command operator but as far as I can understand this command operator does not return a string. Is there any way how to convert it to string?\r\n\r\n**I am trying to do the changes in substitution_formatter.cc for the DOWNSTREAM_LOCAL_ADDRESS command operator.**\r\nThe idea was to convert the command operator to a string and then concatenate with another string (see below).\r\n\r\nI tried to use asSring() but apparently it did not work (How can I check if asString is supported for this command operator).\r\n\r\n  } else if (field_name == \"DOWNSTREAM_LOCAL_ADDRESS\") {\r\n    field_extractor_ =\r\n        StreamInfoAddressFieldExtractor::withPort([](const StreamInfo::StreamInfo& stream_info) {\r\n          std::string localaddress;\r\n          localaddress = stream_info.downstreamAddressProvider().localAddress()**->asString();**\r\n          std::string changed_localaddress = \"[text I want to add]\" + localaddress + \"[/text I want to add]\";\r\n          return changed_localaddress;\r\n        }); ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19218/comments",
    "author": "Edimo24",
    "comments": [
      {
        "user": "Edimo24",
        "created_at": "2021-12-09T13:53:27Z",
        "body": "@ggreenway "
      },
      {
        "user": "ggreenway",
        "created_at": "2021-12-13T20:53:39Z",
        "body": "I don't understand the question. Can you elaborate/rephrase?"
      },
      {
        "user": "Edimo24",
        "created_at": "2021-12-14T09:10:50Z",
        "body": "@ggreenway I tried the above mentioned scenario but it does not work since I am returning a string where I should not return one. To keep the long story short I need any ideas how to add tags in some of command operators. I want to add tags at several command operators but I am struggling a lot how to do it. (tried different changes in Envoy code) \r\n\r\nWhen accessing Access logs I need to see something like  [tag]%DOWNSTREAM_LOCAL_ADDRESS%[/tag],[tag]%DOWNSTREAM_REMOTE_ADDRESS%[/tag]. Do you have any idea how to do it?"
      },
      {
        "user": "ggreenway",
        "created_at": "2021-12-14T18:25:26Z",
        "body": "Why not just put `[tag]` directly into the format string? Why does the replacement formatting need to change?"
      },
      {
        "user": "Edimo24",
        "created_at": "2021-12-14T19:35:19Z",
        "body": "@ggreenway  How to do it? Because I only find the command operators under envoy/source/common/formatter/substitution_formatter.cc\r\nAnd the way how these command operators are generated are as below: (line 838)\r\n\r\n  **} else if (field_name == \"DOWNSTREAM_LOCAL_ADDRESS\") {\r\n    field_extractor_ =\r\n        StreamInfoAddressFieldExtractor::withPort([](const StreamInfo::StreamInfo& stream_info) {\r\n          return stream_info.downstreamAddressProvider().localAddress();**\r\n\r\nDo you have any suggestion where to add the [tag] directly?"
      },
      {
        "user": "Edimo24",
        "created_at": "2021-12-15T09:04:13Z",
        "body": "@ggreenway I would appreciate any suggestion! I also realized that in the default format string there are not all the command operators so I am not sure if I can add the tag directly there... (for instance the DOWNSTREAM_LOCAL_ADDRESS is not there)"
      },
      {
        "user": "Edimo24",
        "created_at": "2021-12-16T22:15:57Z",
        "body": "@ggreenway any suggestion?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-16T00:02:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-23T04:01:23Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19183,
    "title": "What is the Envoy retry behavior when we don't set retry_policy for upstream?",
    "created_at": "2021-12-03T09:54:45Z",
    "closed_at": "2022-01-20T20:01:37Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19183",
    "body": "Hi, team, \r\nCurrently, when sending the request to upstream, one of the upstream nodes returns some 503 responses and route_config as below which isn't set any retry_policy. For this use case, I want to ask 2 questions.\r\n\r\n1. will the request be retried?\r\n2. If we add the `retry_policy`, will the request be sent to another upstream node or does it just re-send the request to the same upstream? \r\n\r\n```\r\n{\r\n     \"route_config\": {\r\n      \"@type\": \"type.googleapis.com/envoy.api.v2.RouteConfiguration\",\r\n      \"name\": \"xxx\",\r\n      \"virtual_hosts\": [\r\n       {\r\n        \"name\": \"xxx\",\r\n        \"domains\": [\r\n         \"*\"\r\n        ],\r\n        \"routes\": [\r\n         {\r\n          \"match\": {\r\n           \"prefix\": \"/\"\r\n          },\r\n          \"route\": {\r\n           \"cluster\": \"xxx\"\r\n          }\r\n         }\r\n        ]\r\n       }\r\n      ]\r\n     }\r\n    }\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n``",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19183/comments",
    "author": "Andy-Gong",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2021-12-14T14:25:03Z",
        "body": "If there's no retry policy, Envoy will not retry.  If there is, the host for the retry will be selected based on the load balancing configured (least loaded, round robin etc) "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-13T16:05:57Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-20T20:01:36Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19142,
    "title": "TLS certificate validation: how to asynchronously fetch CRL using CRL DP present in cert",
    "created_at": "2021-11-30T20:34:31Z",
    "closed_at": "2022-02-19T08:01:22Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19142",
    "body": "*Title*: *TLS certificate validation: how to asynchronously fetch CRL using CRL DP present in cert*\r\n\r\n*Description*:\r\n> I've a custom cert validator. In our environment, leaf cert doesn't have CRL support because of management challenges with large volume of leaf cert. Instead, next level cert has CRL support. So, I want to asynchronously fetch that CRL and do validation. Apparently envoy http async client is available in upstream context. \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19142/comments",
    "author": "vikaskumar11",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-31T20:01:41Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-08T00:02:07Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "vikaskumar11",
        "created_at": "2022-01-11T18:04:28Z",
        "body": "@ggreenway Can you please add label \"help wanted\". "
      },
      {
        "user": "ggreenway",
        "created_at": "2022-01-11T19:09:16Z",
        "body": "cc @htuch. This is related to the discussion of libcurl in the community meeting this morning. Is the recommended approach for the extension to create a cluster for each CRL host it needs to connect to, and then use AsyncHttpClient on that cluster? I've never looked at how to do this; can extensions dynamically create clusters that aren't in the config?"
      },
      {
        "user": "htuch",
        "created_at": "2022-01-12T05:46:36Z",
        "body": "I think this is a place we could make use of the dynamic forward cluster, but I'm not aware of any place in Envoy that does this beside the data forwarding plane. If someone can establish this pattern, I think it will be useful in many places."
      },
      {
        "user": "ggreenway",
        "created_at": "2022-01-12T17:10:30Z",
        "body": "It may make more sense to make a simpler API, that doesn't do any connection pooling, that just does an http request to a given url asynchronously. It could make a cluster under the hood if needed as an implementation detail."
      },
      {
        "user": "htuch",
        "created_at": "2022-01-13T02:57:36Z",
        "body": "Yeah, we could have an implicit \"Envoy internal service dynamic forward proxy cluster\" for this."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-12T04:02:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-19T08:01:22Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19100,
    "title": "How does RBAC filter restrict access to a specific cluster according to the source IP of the client?",
    "created_at": "2021-11-25T06:59:09Z",
    "closed_at": "2022-01-06T00:02:02Z",
    "labels": [
      "question",
      "stale",
      "area/rbac"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19100",
    "body": "*Title*: How does RBAC filter restrict access to a specific cluster according to the source IP of the client?\r\n\r\n*Description*:I want to use RBAC filter to intercept the client's access to an upstream service according to the client's source IP; But I don't know how to configure RBAC permissions; Ask for help。\r\n\r\nThis is my configuration：\r\n\r\n````\r\n   - name: envoy.filters.network.rbac\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.network.rbac.v3.RBAC\r\n        stat_prefix: tcp.\r\n        rules:\r\n          action: DENY\r\n          policies:\r\n            \"foo\":\r\n              permissions:\r\n                    ???\r\n              principals:\r\n                  - source_ip:\r\n                      address_prefix: 192.168.0.1\r\n                      prefix_len: 32\r\n````\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19100/comments",
    "author": "Panxu1219",
    "comments": [
      {
        "user": "Panxu1219",
        "created_at": "2021-11-25T07:55:53Z",
        "body": "I tried to implement it with the following configuration, but there were errors.\r\n````\r\n    - name: envoy.filters.http.rbac\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.network.rbac.v3.RBAC\r\n        stat_prefix: tcp.\r\n        rules:\r\n          action: DENY\r\n          policies:\r\n            \"foo\":\r\n              permissions:\r\n                  - and_rules:\r\n                      rules:\r\n                         - header: { name: \":authority\", prefix_match: \"oa\" }\r\n                         - url_path:\r\n                             path: { prefix: \"/\" }\r\n                         - or_rules:\r\n                             rules:\r\n                               - destination_port: 80\r\n                               - destination_port: 443\r\n\r\n              principals:\r\n                  - source_ip:\r\n                      address_prefix: 192.168.0.1\r\n                      prefix_len: 32    \r\n\r\n````\r\n\r\n\r\nError : \r\n````\r\nFilesystem config update rejected: Error adding/updating listener(s) listener_0: Found header(name: \":authority\"\r\n) rule,not supported by RBAC network filter\r\n\r\n````\r\n\r\nWhat is the correct configuration？"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-29T20:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-06T00:02:02Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19065,
    "title": "Using the default now-deprecated value AUTO for enum 'envoy.config.core.v3.ConfigSource.resource_api_version'",
    "created_at": "2021-11-19T19:11:05Z",
    "closed_at": "2021-12-27T00:03:29Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19065",
    "body": "Hi,\r\n\r\nI want to fix the warning messages that I see during the envoy bootstrap:\r\n```\r\nUsing the default now-deprecated value AUTO for enum 'envoy.config.core.v3.ConfigSource.resource_api_version'\r\n\r\n```\r\nBut in my case SDS defined in the envoy confguration is watching only on filesystem,eg:\r\n\r\n```\r\nvalidation_context_sds_secret_config:\r\n    name: validation__context_sip_sds\r\n    sds_config:\r\n        path: /etc/envoy/validation_sip_context_sds_secret.yaml\r\n\r\n```\r\n\r\nWhy shall we  define the xDS version?\r\n\r\n```\r\nconfig.core.v3.ApiVersion\r\n\r\n```\r\nIf yes, what is the suggested value? V3?\r\n\r\n```\r\nresource_api_version: V3\r\n```\r\n\r\nThanks a lot!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19065/comments",
    "author": "eoregua",
    "comments": [
      {
        "user": "davinci26",
        "created_at": "2021-11-19T19:45:00Z",
        "body": "See answer in #19061"
      },
      {
        "user": "eoregua",
        "created_at": "2021-11-19T20:34:20Z",
        "body": "\r\nI tried with the setting:\r\n\r\nvalidation_context_sds_secret_config:\r\n    name: validation_context\r\n    sds_config:\r\n        resource_api_version: V3\r\n        path: /path/to/secret.yaml\r\n\r\nbut no way, the warning is still present.\r\n\r\nI also checked some good examples in envoy docs but I didn't find any other info to remove such warning.\r\n\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-20T00:02:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-27T00:03:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18973,
    "title": "question: how to improve the ipv6 support in envoy",
    "created_at": "2021-11-11T09:10:04Z",
    "closed_at": "2021-12-23T04:01:26Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18973",
    "body": "I'm trying to answer a very generic question about how was the ipv6 support in Envoy. \r\n\r\nFor answer this question, ​I checked the unit-test and integration test, there is infrastructure in those tests for testing both ipv4 and ipv6. I think that means the major functionality was being tested with ipv6. I also searched the issue, there aren't too much ipv6 related issues. \r\n\r\nSo can I say envoy already support ipv6 well? Do we need anything to improve ipv6 support further? (I guess through test, more integration and functional tests..etc) \r\n\r\nAppreciate any feedback or comment!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18973/comments",
    "author": "soulxu",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2021-11-15T17:26:38Z",
        "body": "As far as I know, Envoy fully supports IPv6. "
      },
      {
        "user": "soulxu",
        "created_at": "2021-11-16T00:25:53Z",
        "body": "> As far as I know, Envoy fully supports IPv6.\r\n\r\nthanks for the feedback!"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-16T04:01:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-23T04:01:26Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18912,
    "title": "Is there a way to get both the grpc headers and the JSON body when convert_grpc_status=True ?",
    "created_at": "2021-11-05T12:03:13Z",
    "closed_at": "2021-12-22T16:01:20Z",
    "labels": [
      "question",
      "stale",
      "area/grpc-transcoding"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18912",
    "body": "Hi,\r\n\r\nI've set recently the convert_grpc_status=True, but by doing this I might be causing problems on clients side, as before the status was returned in headers.\r\n\r\nIs there a way to instruct envoy to return both the JSON and the grpc headers at the same time?\r\n\r\nThanks\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18912/comments",
    "author": "ghevge",
    "comments": [
      {
        "user": "ghevge",
        "created_at": "2021-11-10T13:51:59Z",
        "body": "@qiwzhang do you know if this is possible? "
      },
      {
        "user": "rojkov",
        "created_at": "2021-11-15T13:51:08Z",
        "body": "/cc @lizan "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-15T16:01:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-22T16:01:20Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18891,
    "title": "Can xds control plane be dynamically discovered after bootstrap?",
    "created_at": "2021-11-04T02:42:39Z",
    "closed_at": "2021-11-19T06:45:50Z",
    "labels": [
      "question",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18891",
    "body": "*Title*: *Can xds control plane be dynamically discovered after initial bootstrap?*\r\n\r\n*Description*:\r\nAll the Envoy config today define xds control plane cluster as static resource. Usually it is accessed by resolving a DNS name. Is it possible after the initial bootstrapping process, the xds control plane just push down its own endpoints to proxies? That is to dynamically override xds control plane's static cluster configuration. \r\n\r\nThanks.  ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18891/comments",
    "author": "manlinl",
    "comments": [
      {
        "user": "rojkov",
        "created_at": "2021-11-15T13:59:29Z",
        "body": "/cc @markdroth @htuch "
      },
      {
        "user": "htuch",
        "created_at": "2021-11-16T05:50:54Z",
        "body": "I have deja vu on this topic of dynamic clusters overriding static ones. I think we don't allow it due to complexity and confusion potential, @mattklein123 or @snowp  might remember better (it's come up a few times)."
      },
      {
        "user": "mattklein123",
        "created_at": "2021-11-16T15:28:02Z",
        "body": "> I have deja vu on this topic of dynamic clusters overriding static ones. I think we don't allow it due to complexity and confusion potential, @mattklein123 or @snowp might remember better (it's come up a few times).\r\n\r\nYeah agreed this has been discussed before, and I also agree that the complexity seems dubiously worth it. Is there some reason DNS is not a workable solution for you?"
      },
      {
        "user": "manlinl",
        "created_at": "2021-11-19T06:45:50Z",
        "body": "Thanks for the insight. DNS works for us. I was curious if current setup allows making XDS control plane part of the mesh itself.  "
      }
    ]
  },
  {
    "number": 18867,
    "title": "Run On-Proxy",
    "created_at": "2021-11-02T17:13:05Z",
    "closed_at": "2021-12-19T04:01:11Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18867",
    "body": "I desire to run our applications on-proxy - meaning, like how blockchain apps get run \"on-chain\", I'd like to integrate our game engine directly onto envoy itself and use the envoy APIs to send/recv.\r\n\r\nWhy? The feature set and performance of envoy is amazing. Along with the statistics. We can see gRPC attempting to replicate envoy, and do just this, with their new, native xDS implementation.\r\n\r\nIn particular, we have a rust app. It does two things very well - real time streaming over web-sockets and real time streaming over http2. \r\n\r\nSince we're using rust, this means we can receive calls into our rust app from C/C++ through a binding with no overhead. This means that we can technically call and receive data from the envoy apis directly.\r\n\r\nSo, the process for accomplishing this should be quite simple. Rather than pointing downstream to a remote server, one points to a local process and shares memory instead of an additional hop on the network. Or, one simply injects the application code in a filter to receive http1.1/http2/websocket requests and accept/respond accordingly.\r\n\r\nMy question, therefore, is where would one modify envoy or attach such that this could be possible?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18867/comments",
    "author": "lacasaprivata2",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-11-03T15:25:20Z",
        "body": "> My question, therefore, is where would one modify envoy or attach such that this could be possible?\r\n\r\nI think you could probably pretty easily embed your application code in a terminal filter, and then use AsyncClient, etc. to make additional network calls if you want. The application code even be global and instantiated once, and then the filter would just receive requests."
      },
      {
        "user": "lacasaprivata2",
        "created_at": "2021-11-11T21:53:50Z",
        "body": "Will the TerminalFilter, though, operate with websockets + asynchronous workers?\r\n\r\nSay that I receive a request and require to do 1second of async i/o work (fetch a database, cache, third party library http call, etc)\r\n\r\nDoes the filter expect synchronous, immediate behavior? Or is this deemed okay?\r\n\r\nTrying to cover all scenarios..... I proposed hooking into \"upstream cluster\" and instead making that a compiled snippet of code to run because I thought that would likely cover the most scenarios"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-12T00:02:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-19T04:01:11Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18859,
    "title": "How to get service meta data in wasm filter",
    "created_at": "2021-11-02T09:15:30Z",
    "closed_at": "2021-12-09T20:01:27Z",
    "labels": [
      "question",
      "stale",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18859",
    "body": "I need to get  service specific information(meta data?)  and do corresponding job in wasm filter, is it possible? \r\n\r\nMaking a grcp/http call in wasm can do that but it is not a efficient way.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18859/comments",
    "author": "rtttech",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-02T16:06:50Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-09T20:01:27Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18826,
    "title": "A simpler way to run both on HTTP and HTTPS?",
    "created_at": "2021-10-29T16:34:23Z",
    "closed_at": "2021-12-06T00:02:14Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18826",
    "body": "Can you please suggest a way to listen to apply filter chains both on port 80 and 443 with repeating configs? Currently I think the only way it to write the same listener two times (with a different port).",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18826/comments",
    "author": "dclipca",
    "comments": [
      {
        "user": "dclipca",
        "created_at": "2021-10-29T16:34:55Z",
        "body": "What I want:\r\n```\r\nstatic_resources:\r\n  listeners:\r\n    - name: http_listener  \r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 80 // 80 and 443 instead of just 80\r\n```"
      },
      {
        "user": "snowp",
        "created_at": "2021-10-29T17:58:04Z",
        "body": "I don't think there's a simpler way than to specify both, Envoy needs explicit configuration for how to handle transport security per listener"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-28T20:01:27Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-06T00:02:14Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18716,
    "title": "Tracing using http filters for header propagation ovewritten",
    "created_at": "2021-10-21T18:37:18Z",
    "closed_at": "2022-03-12T00:03:11Z",
    "labels": [
      "question",
      "area/tracing",
      "stale",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18716",
    "body": "*Title*: *Tracing header propagation done via http filters are overwritten*\r\n\r\n*Description*:\r\nTracing typically involves propagation of headers, x-b3 Zipkin headers in my case, done at the application layer to get a full trace. I'm trying to do the propagation via Envoy filters by adding some logic in a sidecar that injects those headers for outgoing requests. What I'm noticing is that despite the tracing headers getting set, the final outgoing request to some other service has those readers overwritten, so the correlation between the incoming and outgoing is lost. Is there a way around this or another method to do custom propagation? The filters are included below (we're using Istio):\r\n\r\n*inboundFilter.yaml*\r\n```\r\n{{ if .Values.tracing.enabled }}\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: tracing-inbound\r\n  namespace: istio-system\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      { app: \"myApp\" }\r\n  configPatches:\r\n  - applyTo: HTTP_FILTER\r\n    match:\r\n      context: SIDECAR_INBOUND\r\n      listener:\r\n        filterChain:\r\n          filter:\r\n            name: envoy.filters.network.http_connection_manager\r\n            subFilter:\r\n              name: envoy.filters.http.router\r\n    patch:\r\n      operation: INSERT_BEFORE\r\n      value:\r\n        name: envoy.lua\r\n        typed_config:\r\n          '@type': type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n          inlineCode: |-\r\n            function envoy_on_request(handle)\r\n                -- cache the tracing headers based on correlation header\r\n                -- the original span is translated to the parentSpan for this request, so extract accordingly\r\n                local headers, body = handle:httpCall(\"outbound|6060||network.local\", {\r\n                    [\":method\"] = \"GET\",\r\n                    [\":path\"] = \"/incoming\",\r\n                    [\":authority\"] = \"\",\r\n                    [\"x-iot-correlation\"] = handle:headers():get(\"x-iot-correlation\"),\r\n                    [\"x-b3-traceid\"] = handle:headers():get(\"x-b3-traceid\"),\r\n                    [\"x-request-id\"] = handle:headers():get(\"x-request-id\")\r\n                }, \"\", 5000)\r\n            end\r\n\r\n            function envoy_on_response(handle)\r\n            end\r\n{ {end }}\r\n```\r\n\r\n*outboundFilter.yaml*:\r\n```\r\n{{ if .Values.tracing.enabled }}\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: tracing-outbound\r\n  namespace: istio-system\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      { app: \"myApp\" }\r\n  configPatches:\r\n  - applyTo: HTTP_FILTER\r\n    match:\r\n      context: SIDECAR_OUTBOUND\r\n      listener:\r\n        filterChain:\r\n          filter:\r\n            name: envoy.filters.network.http_connection_manager\r\n            subFilter:\r\n              name: envoy.filters.http.router\r\n    patch:\r\n      operation: INSERT_BEFORE\r\n      value:\r\n        name: envoy.lua\r\n        # insertPosition:\r\n        #   index: LAST\r\n        typed_config:\r\n          '@type': type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n          inlineCode: |-\r\n            function envoy_on_request(handle)\r\n                -- fetch trace headers based on correlation value, these are set in the response headers accordingly\r\n                local headers, body = handle:httpCall(\"outbound|6060||network.local\", {\r\n                    [\":method\"] = \"GET\",\r\n                    [\":path\"] = \"/outgoing\",\r\n                    [\":authority\"] = \"\",\r\n                    [\"x-iot-correlation\"] = handle:headers():get(\"x-iot-correlation\")\r\n                }, \"\", 5000)\r\n                for key, value in pairs(headers) do\r\n                  handle:headers():replace(key, value)\r\n                end\r\n            end\r\n\r\n            function envoy_on_response(handle)\r\n            end\r\n{{ end }}\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18716/comments",
    "author": "arhamahmed",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2021-10-22T04:31:51Z",
        "body": "This is the `x-iot-correlation` header? I'm not sure why that would be overwriten, it might help to see the full Envoy config dump from Istio. CC @dio for Lua."
      },
      {
        "user": "arhamahmed",
        "created_at": "2021-10-22T17:07:52Z",
        "body": "No, that header can be ignored, the issue is with the b3 headers. To add some context, services have always used `x-iot-correlation` to track correlation between services - I'm now hooking that up with tracing where:\r\n- if service A calls service B\r\n- the sidecar for service B caches the b3 tracing headers by the correlation header value from service A's request\r\n- if sidecar B makes a call to service C and has a similar correlation header, the b3 headers from service A's request are injected into the outgoing request to service C by Envoy filters\r\n\r\nAll of the above works except when service B calls service C, the b3 headers I wrote are gone and presumably replaced by Envoy due to there being some sort of final HTTP filter in the chain that doesn't acknowledge that the b3 headers were already set.\r\n\r\nAs for the Envoy configuration logs, please let me know if this is sufficient:\r\n\r\n```\r\n2021-10-22T16:57:48.417791Z\tinfo\tFLAG: --concurrency=\"2\"\r\n2021-10-22T16:57:48.417820Z\tinfo\tFLAG: --domain=\"applejack.svc.cluster.local\"\r\n2021-10-22T16:57:48.417825Z\tinfo\tFLAG: --help=\"false\"\r\n2021-10-22T16:57:48.417829Z\tinfo\tFLAG: --log_as_json=\"false\"\r\n2021-10-22T16:57:48.417832Z\tinfo\tFLAG: --log_caller=\"\"\r\n2021-10-22T16:57:48.417836Z\tinfo\tFLAG: --log_output_level=\"default:info\"\r\n2021-10-22T16:57:48.417845Z\tinfo\tFLAG: --log_rotate=\"\"\r\n2021-10-22T16:57:48.417849Z\tinfo\tFLAG: --log_rotate_max_age=\"30\"\r\n2021-10-22T16:57:48.417852Z\tinfo\tFLAG: --log_rotate_max_backups=\"1000\"\r\n2021-10-22T16:57:48.417856Z\tinfo\tFLAG: --log_rotate_max_size=\"104857600\"\r\n2021-10-22T16:57:48.417859Z\tinfo\tFLAG: --log_stacktrace_level=\"default:none\"\r\n2021-10-22T16:57:48.417867Z\tinfo\tFLAG: --log_target=\"[stdout]\"\r\n2021-10-22T16:57:48.417871Z\tinfo\tFLAG: --meshConfig=\"./etc/istio/config/mesh\"\r\n2021-10-22T16:57:48.417876Z\tinfo\tFLAG: --outlierLogPath=\"\"\r\n2021-10-22T16:57:48.417879Z\tinfo\tFLAG: --proxyComponentLogLevel=\"misc:error\"\r\n2021-10-22T16:57:48.417885Z\tinfo\tFLAG: --proxyLogLevel=\"warning\"\r\n2021-10-22T16:57:48.417889Z\tinfo\tFLAG: --serviceCluster=\"cluster-management.applejack\"\r\n2021-10-22T16:57:48.417893Z\tinfo\tFLAG: --stsPort=\"0\"\r\n2021-10-22T16:57:48.417896Z\tinfo\tFLAG: --templateFile=\"\"\r\n2021-10-22T16:57:48.417900Z\tinfo\tFLAG: --tokenManagerPlugin=\"GoogleTokenExchange\"\r\n2021-10-22T16:57:48.417905Z\tinfo\tVersion 1.9.7-501c07f3ca45400cd42fa276c4c14a66518956a8-Clean\r\n2021-10-22T16:57:48.418126Z\tinfo\tApply proxy config from env {\"tracing\":{\"zipkin\":{\"address\":\"zipkin.istio-system:9411\"},\"sampling\":100},\"proxyMetadata\":{\"ISTIO_META_DNS_CAPTURE\":\"true\"},\"holdApplicationUntilProxyStarts\":true}\r\n\r\n2021-10-22T16:57:48.420055Z\tinfo\tEffective config: binaryPath: /usr/local/bin/envoy\r\nconcurrency: 2\r\nconfigPath: ./etc/istio/proxy\r\ncontrolPlaneAuthPolicy: MUTUAL_TLS\r\ndiscoveryAddress: istiod.istio-system.svc:15012\r\ndrainDuration: 45s\r\nholdApplicationUntilProxyStarts: true\r\nparentShutdownDuration: 60s\r\nproxyAdminPort: 15000\r\nproxyMetadata:\r\n  ISTIO_META_DNS_CAPTURE: \"true\"\r\nserviceCluster: cluster-management.applejack\r\nstatNameLength: 189\r\nstatusPort: 15020\r\nterminationDrainDuration: 5s\r\ntracing:\r\n  sampling: 100\r\n  zipkin:\r\n    address: zipkin.istio-system:9411\r\n\r\n2021-10-22T16:57:48.420093Z\tinfo\tProxy role\tips=[10.240.13.240 fe80::88ac:b6ff:fe9b:3feb] type=sidecar id=cluster-management-12000-b7844b86f-w2cn7.applejack domain=applejack.svc.cluster.local\r\n2021-10-22T16:57:48.420101Z\tinfo\tJWT policy is third-party-jwt\r\n2021-10-22T16:57:48.420109Z\tinfo\tPilot SAN: [istiod.istio-system.svc]\r\n2021-10-22T16:57:48.420113Z\tinfo\tCA Endpoint istiod.istio-system.svc:15012, provider Citadel\r\n2021-10-22T16:57:48.420167Z\tinfo\tUsing CA istiod.istio-system.svc:15012 cert with certs: var/run/secrets/istio/root-cert.pem\r\n2021-10-22T16:57:48.420284Z\tinfo\tcitadelclient\tCitadel client using custom root cert: istiod.istio-system.svc:15012\r\n2021-10-22T16:57:48.449958Z\tinfo\tads\tAll caches have been synced up in 34.167149ms, marking server ready\r\n2021-10-22T16:57:48.450238Z\tinfo\tsds\tSDS server for workload certificates started, listening on \"./etc/istio/proxy/SDS\"\r\n2021-10-22T16:57:48.450328Z\tinfo\tsds\tStart SDS grpc server\r\n2021-10-22T16:57:48.450623Z\tinfo\txdsproxy\tInitializing with upstream address \"istiod.istio-system.svc:15012\" and cluster \"Kubernetes\"\r\n2021-10-22T16:57:48.450914Z\tinfo\tStarting proxy agent\r\n2021-10-22T16:57:48.452527Z\tinfo\tdns\tStarting local tcp DNS server at localhost:15053\r\n2021-10-22T16:57:48.452588Z\tinfo\tdns\tStarting local udp DNS server at localhost:15053\r\n2021-10-22T16:57:48.452584Z\tinfo\tOpening status port 15020\r\n2021-10-22T16:57:48.452661Z\tinfo\tReceived new config, creating new Envoy epoch 0\r\n2021-10-22T16:57:48.452693Z\tinfo\tEpoch 0 starting\r\n2021-10-22T16:57:48.787101Z\tinfo\tcache\tRoot cert has changed, start rotating root cert\r\n2021-10-22T16:57:48.787136Z\tinfo\tads\tXDS: Incremental Pushing:0 ConnectedEndpoints:0 Version:\r\n2021-10-22T16:57:48.787150Z\tinfo\tcache\tgenerated new workload certificate\tlatency=336.847257ms ttl=23h59m59.21285754s\r\n2021-10-22T16:57:49.469605Z\twarn\tHTTP request unsuccessful with status: 400 Bad Request\r\n2021-10-22T16:57:49.476301Z\tinfo\tEnvoy command: [-c etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster cluster-management.applejack --service-node sidecar~10.240.13.240~cluster-management-12000-b7844b86f-w2cn7.applejack~applejack.svc.cluster.local --local-address-ip-version v4 --bootstrap-version 3 --log-format %Y-%m-%dT%T.%fZ\t%l\tenvoy %n\t%v -l warning --component-log-level misc:error --concurrency 2]\r\n\r\n```"
      },
      {
        "user": "dio",
        "created_at": "2021-10-25T11:37:27Z",
        "body": "OK, so in the envoy `http_filters` you set the B3 headers, and seems like you have Zipkin tracing activated in that outbound sidecar proxy.\r\n\r\nProbably you can do: `istioctl proxy-config listeners <POD> -o json` and see how the actual `http_filters` being stacked."
      },
      {
        "user": "arhamahmed",
        "created_at": "2021-10-25T16:25:35Z",
        "body": "Correct. Running that I see that my filter is second last in the chain with `envoy.filters.http.router` succeeding it. I don't think it's possible to change the chain order to put a custom filter after the `envoy.filters.http.router` (got into a crashloop when testing it out, and I think that filter must be at the end by design), so is there any option to configure it not to set those headers if they exist?"
      },
      {
        "user": "dio",
        "created_at": "2021-10-26T08:52:38Z",
        "body": "@arhamahmed mind pasting the content of `http_filters` above here? Thanks!"
      },
      {
        "user": "arhamahmed",
        "created_at": "2021-10-26T17:23:01Z",
        "body": "Sure, the output from the command was incredibly large and with entries on an ip/port basis so I chose one that included the custom filter arbitrarily:\r\n\r\n```\r\n\"httpFilters\": [\r\n                                {\r\n                                    \"name\": \"istio.metadata_exchange\",\r\n                                    \"typedConfig\": {\r\n                                        \"@type\": \"type.googleapis.com/udpa.type.v1.TypedStruct\",\r\n                                        \"typeUrl\": \"type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm\",\r\n                                        \"value\": {\r\n                                            \"config\": {\r\n                                                \"configuration\": {\r\n                                                    \"@type\": \"type.googleapis.com/google.protobuf.StringValue\",\r\n                                                    \"value\": \"{}\\n\"\r\n                                                },\r\n                                                \"vm_config\": {\r\n                                                    \"code\": {\r\n                                                        \"local\": {\r\n                                                            \"inline_string\": \"envoy.wasm.metadata_exchange\"\r\n                                                        }\r\n                                                    },\r\n                                                    \"runtime\": \"envoy.wasm.runtime.null\"\r\n                                                }\r\n                                            }\r\n                                        }\r\n                                    }\r\n                                },\r\n                                {\r\n                                    \"name\": \"istio.alpn\",\r\n                                    \"typedConfig\": {\r\n                                        \"@type\": \"type.googleapis.com/istio.envoy.config.filter.http.alpn.v2alpha1.FilterConfig\",\r\n                                        \"alpnOverride\": [\r\n                                            {\r\n                                                \"alpnOverride\": [\r\n                                                    \"istio-http/1.0\",\r\n                                                    \"istio\",\r\n                                                    \"http/1.0\"\r\n                                                ]\r\n                                            },\r\n                                            {\r\n                                                \"upstreamProtocol\": \"HTTP11\",\r\n                                                \"alpnOverride\": [\r\n                                                    \"istio-http/1.1\",\r\n                                                    \"istio\",\r\n                                                    \"http/1.1\"\r\n                                                ]\r\n                                            },\r\n                                            {\r\n                                                \"upstreamProtocol\": \"HTTP2\",\r\n                                                \"alpnOverride\": [\r\n                                                    \"istio-h2\",\r\n                                                    \"istio\",\r\n                                                    \"h2\"\r\n                                                ]\r\n                                            }\r\n                                        ]\r\n                                    }\r\n                                },\r\n                                {\r\n                                    \"name\": \"envoy.filters.http.cors\",\r\n                                    \"typedConfig\": {\r\n                                        \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.cors.v3.Cors\"\r\n                                    }\r\n                                },\r\n                                {\r\n                                    \"name\": \"envoy.filters.http.fault\",\r\n                                    \"typedConfig\": {\r\n                                        \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.fault.v3.HTTPFault\"\r\n                                    }\r\n                                },\r\n                                {\r\n                                    \"name\": \"istio.stats\",\r\n                                    \"typedConfig\": {\r\n                                        \"@type\": \"type.googleapis.com/udpa.type.v1.TypedStruct\",\r\n                                        \"typeUrl\": \"type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm\",\r\n                                        \"value\": {\r\n                                            \"config\": {\r\n                                                \"configuration\": {\r\n                                                    \"@type\": \"type.googleapis.com/google.protobuf.StringValue\",\r\n                                                    \"value\": \"{\\n  \\\"debug\\\": \\\"false\\\",\\n  \\\"stat_prefix\\\": \\\"istio\\\",\\n  \\\"metrics\\\": [\\n    {\\n      \\\"dimensions\\\": {\\n        \\\"source_cluster\\\": \\\"node.metadata['CLUSTER_ID']\\\",\\n        \\\"destination_cluster\\\": \\\"upstream_peer.cluster_id\\\"\\n      }\\n    }\\n  ]\\n}\\n\"\r\n                                                },\r\n                                                \"root_id\": \"stats_outbound\",\r\n                                                \"vm_config\": {\r\n                                                    \"code\": {\r\n                                                        \"local\": {\r\n                                                            \"inline_string\": \"envoy.wasm.stats\"\r\n                                                        }\r\n                                                    },\r\n                                                    \"runtime\": \"envoy.wasm.runtime.null\",\r\n                                                    \"vm_id\": \"stats_outbound\"\r\n                                                }\r\n                                            }\r\n                                        }\r\n                                    }\r\n                                },\r\n                                {\r\n                                    \"name\": \"envoy.lua\",\r\n                                    \"typedConfig\": {\r\n                                        \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\",\r\n                                        \"inlineCode\": \"function envoy_on_request(handle)\\n    handle:logInfo(\\\"start headers\\\")\\n    for key, value in pairs(handle:headers()) do\\n      handle:logInfo(\\\"key:\\\" .. key .. \\\" \\u003c--\\u003e value:\\\" .. value)\\n    end\\n    handle:logInfo(\\\"end headers\\\")\\n\\n    handle:logInfo(\\\"calling outgoing endpoint for OSR\\\")\\n    -- fetch the spanId correlated with a cv if one exists from the network-watcher sidecar\\n    local headers, body = handle:httpCall(\\\"outbound|6060||network.local\\\", {\\n        [\\\":method\\\"] = \\\"GET\\\",\\n        [\\\":path\\\"] = \\\"/outgoing\\\",\\n        [\\\":authority\\\"] = \\\"\\\",\\n        [\\\"x-iot-correlation\\\"] = handle:headers():get(\\\"x-iot-correlation\\\")\\n    }, \\\"\\\", 5000)\\n    handle:logInfo(\\\"got outgoing body=\\\"..body)\\n    for key, value in pairs(headers) do\\n      handle:logInfo(\\\"got outgoing watcher resp header:\\\" .. key .. \\\" \\u003c--\\u003e value:\\\" .. value)\\n      handle:logInfo(\\\"replacing that resp header to outbound header\\\")\\n      handle:headers():replace(key, value)\\n    end\\n\\n    handle:logInfo(\\\"done outbound req processing\\\")\\nend\\n\\nfunction envoy_on_response(handle)\\n  --todo\\nend\"\r\n                                    }\r\n                                },\r\n                                {\r\n                                    \"name\": \"envoy.filters.http.router\",\r\n                                    \"typedConfig\": {\r\n                                        \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"\r\n                                    }\r\n                                }\r\n                            ]\r\n```"
      },
      {
        "user": "arhamahmed",
        "created_at": "2021-11-01T16:25:33Z",
        "body": "@dio was the config helpful, or is there anything else I can provide?"
      },
      {
        "user": "78188869",
        "created_at": "2021-11-03T08:05:03Z",
        "body": "How did you propagate the value of header \"x-iot-correlation\" itself? "
      },
      {
        "user": "arhamahmed",
        "created_at": "2021-11-03T23:30:44Z",
        "body": "That header was set at the application layer, @78188869 ."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-04T00:02:01Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "arhamahmed",
        "created_at": "2021-12-04T00:07:03Z",
        "body": "@dio hey any updates?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-03T08:01:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "arhamahmed",
        "created_at": "2022-01-03T18:29:31Z",
        "body": "Bump, since issue was not resolved"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-02T20:01:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "arhamahmed",
        "created_at": "2022-02-02T21:02:13Z",
        "body": "Bump, since issue was not resolved"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-05T00:02:31Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-12T00:03:10Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18709,
    "title": "fallback connections with hash lb after health check OK",
    "created_at": "2021-10-21T13:10:51Z",
    "closed_at": "2021-12-29T20:01:17Z",
    "labels": [
      "question",
      "stale",
      "area/load balancing",
      "area/health_checking"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18709",
    "body": "Hello! \r\nI am using maglev hash lb. So any connection from client must go to only one selected endpoint server (SRV1 for example). When active health check become failed for SRV1 - all connections are dropped with 'close_connections_on_host_health_failure' option and switched to another endpoint (lets say SRV5). But if previous endpoint (SRV1) become available again - there is no way to drop current tcp sessions to SRV5. Only new requests are going to SRV1. \r\nIs there any way to force envoy to fallback to SRV1 server? \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18709/comments",
    "author": "p-o-nch",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2021-10-22T04:26:23Z",
        "body": "@snowp @zuercher any thoughts on this? I don't know of any obvious way."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-21T08:01:23Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "p-o-nch",
        "created_at": "2021-11-22T08:14:40Z",
        "body": "so.. no workaround for this possible? "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-22T16:01:17Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-12-29T20:01:16Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18673,
    "title": "what is Attribute Context   service function ，the doc is too simple",
    "created_at": "2021-10-19T05:12:55Z",
    "closed_at": "2021-11-26T16:01:13Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18673",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\nwhat is Attribute Context service function ，the doc is too simple\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18673/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2021-10-20T11:28:28Z",
        "body": "@13567436138 can you provide some more information\r\n\r\nspecifically links to the docs that you are struggling with, and perhaps suggestions as to how they can be improved\r\n\r\nif you are able it would also be helpful to raise a PR with any suggested changes"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-19T12:01:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-26T16:01:13Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18662,
    "title": "what's the difference between listener level accesslog and httpConnectionmanager lever accesslog？",
    "created_at": "2021-10-18T07:31:04Z",
    "closed_at": "2021-11-25T00:02:12Z",
    "labels": [
      "question",
      "stale",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18662",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\nwhat's the difference between listener level accesslog and httpConnectionmanager lever accesslog？\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18662/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-17T20:01:49Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-25T00:02:11Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18660,
    "title": "can anyone  give me example of how to config OpenTelemetry (gRPC) Access Log",
    "created_at": "2021-10-18T04:25:27Z",
    "closed_at": "2021-11-25T00:02:10Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18660",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\ncan anyone  give me example of how to config OpenTelemetry (gRPC) Access Log\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18660/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "davinci26",
        "created_at": "2021-10-18T16:44:15Z",
        "body": "```\r\n        access_log:\r\n        - name: envoy.access_loggers.open_telemetry\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.extensions.access_loggers.open_telemetry.v3.OpenTelemetryAccessLogConfig\r\n            common_config:\r\n                 log_name: name\r\n                 grpc_service:\r\n                      envoy_grpc:\r\n                          cluster_name: grpc_cluster\r\n\r\n```\r\n\r\nSomething like the above should work."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-17T20:01:48Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-25T00:02:10Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18654,
    "title": "is there a example implemention of  External Processing grpc service？",
    "created_at": "2021-10-17T06:02:56Z",
    "closed_at": "2021-11-25T00:02:08Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18654",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\nis there a example implemention of  External Processing grpc service？\r\n\r\nI want to use  envoy.filters.http.ext_proc filter，but I do not have a external grpc service？\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18654/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-17T20:01:47Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-25T00:02:08Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18624,
    "title": "circuit-breakers  custom fallback",
    "created_at": "2021-10-14T08:55:44Z",
    "closed_at": "2021-11-25T00:02:02Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18624",
    "body": "how to custom fallback for  circuit-breakers?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18624/comments",
    "author": "Ithrael",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-17T20:01:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-25T00:02:01Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18575,
    "title": "what is filter_chain_match direct_source_prefix_ranges ip,use pod cidr and svc cidr,client ip all failed",
    "created_at": "2021-10-12T07:58:36Z",
    "closed_at": "2021-11-27T12:01:28Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18575",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: listener\r\n  namespace: istio \r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      app: productpage\r\n  configPatches:\r\n  - applyTo: LISTENER\r\n    match:\r\n      context: SIDECAR_INBOUND\r\n    patch:\r\n      operation: ADD\r\n      value:\r\n        name: proxy\r\n        address:\r\n          socket_address:\r\n            protocol: TCP\r\n            address: 0.0.0.0\r\n            port_value: 8083\r\n        filter_chains:\r\n        - filter_chain_match:\r\n            destination_port: 8083\r\n            direct_source_prefix_ranges:\r\n            - address_prefix: 0.0.0.0\r\n              prefix_len: 0\r\n          filters:\r\n          - name: \"envoy.filters.network.http_connection_manager\"\r\n            typed_config:\r\n              \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\"\r\n              stat_prefix: ingress_proxy\r\n              route_config:\r\n                name: route_a\r\n                virtual_hosts:\r\n                - name: envoy_cyz\r\n                  domains:\r\n                  - \"*\"\r\n                  routes:\r\n                  - match:\r\n                      prefix: \"/\"\r\n                    route:\r\n                      cluster: cluster123\r\n              http_filters:\r\n              - name: \"envoy.filters.http.router\"\r\n                typed_config:\r\n                  \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"\r\n  - applyTo: CLUSTER\r\n    patch:\r\n      operation: ADD\r\n      value: \r\n        name: \"cluster123\"\r\n        type: STATIC\r\n        connect_timeout: 0.5s\r\n        lb_policy: ROUND_ROBIN\r\n        load_assignment:\r\n          cluster_name: cluster123\r\n          endpoints:\r\n          - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: 127.0.0.1\r\n                    port_value: 9080\r\n```\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18575/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "soulxu",
        "created_at": "2021-10-21T08:48:24Z",
        "body": "if none of condition matched in the filter_chain_match, it should be reject the connection."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-20T12:01:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-27T12:01:27Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18547,
    "title": "how to config cluster upstream_config ,the doc is empty?",
    "created_at": "2021-10-11T06:45:16Z",
    "closed_at": "2021-11-18T00:02:08Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18547",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\nupstream_config\r\n(config.core.v3.TypedExtensionConfig) Optional customization and configuration of upstream connection pool, and upstream type.\r\n\r\nCurrently this field only applies for HTTP traffic but is designed for eventual use for custom TCP upstreams.\r\n\r\nFor HTTP traffic, Envoy will generally take downstream HTTP and send it upstream as upstream HTTP, using the http connection pool and the codec from http2_protocol_options\r\n\r\nFor routes where CONNECT termination is configured, Envoy will take downstream CONNECT requests and forward the CONNECT payload upstream over raw TCP using the tcp connection pool.\r\n\r\nThe default pool used is the generic connection pool which creates the HTTP upstream for most HTTP requests, and the TCP upstream if CONNECT termination is configured.\r\n\r\nIf users desire custom connection pool or upstream behavior, for example terminating CONNECT only if a custom filter indicates it is appropriate, the custom factories can be registered and configured here.\r\n\r\nTip\r\n\r\nThis extension category has the following known extensions:\r\n\r\nenvoy.upstreams.http.generic\r\n\r\nenvoy.upstreams.http.http\r\n\r\nenvoy.upstreams.http.http_protocol_options\r\n\r\nenvoy.upstreams.http.tcp\r\n\r\nenvoy.upstreams.tcp.generic\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18547/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "davinci26",
        "created_at": "2021-10-11T23:35:51Z",
        "body": "I am not sure I follow the question exactly. But this could be configured with something like:\r\n\r\n```\r\n    upstream_config:\r\n      name: envoy.upstreams.http.generic\r\n      typed_config:\r\n        \"@type\":  type.googleapis.com/envoy.extensions.upstreams.http.generic.v3.GenericConnectionPoolProto\r\n```\r\n\r\nAnd similar for the rest of the extensions"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-11T00:01:59Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-18T00:02:07Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18546,
    "title": "when will cluster filters be used?",
    "created_at": "2021-10-11T06:20:18Z",
    "closed_at": "2021-11-25T00:01:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18546",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: cluster\r\n  namespace: istio-system \r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n  - applyTo: CLUSTER\r\n    match:\r\n      cluster:\r\n        name: outbound|9080||productpage.istio.svc.cluster.local\r\n    patch:\r\n      operation: MERGE\r\n      value:\r\n          lb_policy: LEAST_REQUEST\r\n          filters:\r\n          - name: istio.metadata_exchange\r\n            typedConfig:\r\n              '@type': type.googleapis.com/udpa.type.v1.TypedStruct\r\n              typeUrl: type.googleapis.com/envoy.tcp.metadataexchange.config.MetadataExchange\r\n              value:\r\n                protocol: istio-peer-exchange\r\n```\r\n\r\nwhen will cluster filters be used?\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18546/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-17T20:01:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-25T00:01:53Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18543,
    "title": "load_balancer_endpoints,leds_cluster_locality_config doc is missing?",
    "created_at": "2021-10-10T05:14:23Z",
    "closed_at": "2021-11-19T08:01:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18543",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\nA group of endpoints belonging to a Locality. One can have multiple LocalityLbEndpoints for a locality, but this is generally only done if the different groups need to have different load balancing weights or different priorities.\r\n\r\n{\r\n  \"locality\": \"{...}\",\r\n  \"lb_endpoints\": [],\r\n  \"load_balancer_endpoints\": \"{...}\",\r\n  \"leds_cluster_locality_config\": \"{...}\",\r\n  \"load_balancing_weight\": \"{...}\",\r\n  \"priority\": \"...\"\r\n}\r\nlocality\r\n(config.core.v3.Locality) Identifies location of where the upstream hosts run.\r\n\r\nlb_endpoints\r\n(repeated config.endpoint.v3.LbEndpoint) The group of endpoints belonging to the locality specified.\r\n\r\nload_balancing_weight\r\n(UInt32Value) Optional: Per priority/region/zone/sub_zone weight; at least 1. The load balancing weight for a locality is divided by the sum of the weights of all localities at the same priority level to produce the effective percentage of traffic for the locality. The sum of the weights of all localities at the same priority level must not exceed uint32_t maximal value (4294967295).\r\n\r\nLocality weights are only considered when locality weighted load balancing is configured. These weights are ignored otherwise. If no weights are specified when locality weighted load balancing is enabled, the locality is assigned no load.\r\n\r\npriority\r\n(uint32) Optional: the priority for this LocalityLbEndpoints. If unspecified this will default to the highest priority (0).\r\n\r\nUnder usual circumstances, Envoy will only select endpoints for the highest priority (0). In the event all endpoints for a particular priority are unavailable/unhealthy, Envoy will fail over to selecting endpoints for the next highest priority group.\r\n\r\nPriorities should range from 0 (highest) to N (lowest) without skipping.\r\n```\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18543/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-12T04:01:23Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-19T08:01:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18542,
    "title": "what typed_extension_config does load_balancing_policy have?no document?",
    "created_at": "2021-10-10T04:41:27Z",
    "closed_at": "2021-11-18T08:01:20Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18542",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\nconfig.cluster.v3.LoadBalancingPolicy.Policy¶\r\n[config.cluster.v3.LoadBalancingPolicy.Policy proto]\r\n\r\n{\r\n  \"typed_extension_config\": \"{...}\"\r\n}\r\ntyped_extension_config\r\n(config.core.v3.TypedExtensionConfig)\r\n```\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18542/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "davinci26",
        "created_at": "2021-10-12T00:20:10Z",
        "body": "None. This is an entry point for users to develop they own extension to handle LoadBalancingPolicy."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-11T04:01:22Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-18T08:01:19Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18540,
    "title": "how to delete gperftools for build envoy",
    "created_at": "2021-10-09T08:19:43Z",
    "closed_at": "2021-11-17T16:05:44Z",
    "labels": [
      "question",
      "area/build",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18540",
    "body": "*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the desired behavior, what scenario it enables and how it\r\nwould be used.\r\n\r\n I want build envoy for chinese cpu, but gperftools is not use for this, how to delete this\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18540/comments",
    "author": "NameHaibinZhang",
    "comments": [
      {
        "user": "rojkov",
        "created_at": "2021-10-11T09:34:22Z",
        "body": "You can disable tcmalloc with\r\n```\r\nbazel build -c opt //source/exe:envoy-static --define=tcmalloc=disabled\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-10T12:01:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-17T16:05:43Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18538,
    "title": "tap transport_socket not working?",
    "created_at": "2021-10-09T05:24:17Z",
    "closed_at": "2021-11-18T00:02:05Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18538",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: cluster\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      app: productpage\r\n  configPatches:\r\n  - applyTo: FILTER_CHAIN\r\n    match:\r\n      listener:\r\n        filterChain:\r\n          transportProtocol: tls\r\n          destinationPort: 9080\r\n    patch:\r\n      operation: MERGE\r\n      value:\r\n        transport_socket:\r\n          name: envoy.transport_sockets.tap\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tap.v3.Tap\r\n            common_config:\r\n              admin_config:\r\n                config_id: test_config_id\r\n              static_config:\r\n                match_config:\r\n                  any_match: true\r\n                output_config:\r\n                  sinks:\r\n                  - file_per_tap:\r\n                      path_prefix: /var/log/tap\r\n                    #format: JSON_BODY_AS_BYTES             \r\n            transport_socket:\r\n                name: envoy.transport_sockets.tls\r\n                typed_config:\r\n                  '@type': type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n                  common_tls_context:\r\n                    alpn_protocols:\r\n                    - h2\r\n                    - http/1.1\r\n                    combined_validation_context:\r\n                      default_validation_context:\r\n                        match_subject_alt_names:\r\n                        - prefix: spiffe://cluster.local/\r\n                      validation_context_sds_secret_config:\r\n                        name: ROOTCA\r\n                        sds_config:\r\n                          api_config_source:\r\n                            api_type: GRPC\r\n                            grpc_services:\r\n                            - envoy_grpc:\r\n                                cluster_name: sds-grpc\r\n                            set_node_on_first_message_only: true\r\n                            transport_api_version: V3\r\n                          initial_fetch_timeout: 0s\r\n                          resource_api_version: V3\r\n                    tls_certificate_sds_secret_configs:\r\n                    - name: default\r\n                      sds_config:\r\n                        api_config_source:\r\n                          api_type: GRPC\r\n                          grpc_services:\r\n                          - envoy_grpc:\r\n                              cluster_name: sds-grpc\r\n                          set_node_on_first_message_only: true\r\n                          transport_api_version: V3\r\n                        initial_fetch_timeout: 0s\r\n                        resource_api_version: V3\r\n                    tls_params:\r\n                      cipher_suites:\r\n                      - ECDHE-ECDSA-AES256-GCM-SHA384\r\n                      - ECDHE-RSA-AES256-GCM-SHA384\r\n                      - ECDHE-ECDSA-AES128-GCM-SHA256\r\n                      - ECDHE-RSA-AES128-GCM-SHA256\r\n                      - AES256-GCM-SHA384\r\n                      - AES128-GCM-SHA256\r\n                      tls_minimum_protocol_version: TLSv1_2\r\n                  require_client_certificate: true\r\n```\r\n\r\n```\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: productpage-v1\r\n  labels:\r\n    app: productpage\r\n    version: v1\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: productpage\r\n      version: v1\r\n  template:\r\n    metadata:\r\n      annotations:\r\n        sidecar.istio.io/userVolumeMount: '[{\"name\":\"log\", \"mountPath\":\"/var/log/tap\"}]'\r\n        sidecar.istio.io/userVolume: '[{\"name\": \"log\", \"emptyDir\":{}}]'\r\n      labels:\r\n        app: productpage\r\n        version: v1\r\n    spec:\r\n      serviceAccountName: bookinfo-productpage\r\n      containers:\r\n      - name: productpage\r\n        image: docker.io/istio/examples-bookinfo-productpage-v1:1.16.2\r\n        imagePullPolicy: IfNotPresent\r\n        ports:\r\n        - containerPort: 9080\r\n        volumeMounts:\r\n        - name: tmp\r\n          mountPath: /tmp\r\n        securityContext:\r\n          runAsUser: 1000\r\n      volumes:\r\n      - name: tmp\r\n        emptyDir: {}\r\n```\r\n\r\ncan not find any result in /var/log/tap directory??\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18538/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-11T00:01:57Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-18T00:02:04Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18515,
    "title": "Envoys which connection to same control plane have different memory usage. ",
    "created_at": "2021-10-08T07:56:59Z",
    "closed_at": "2021-11-18T00:02:01Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18515",
    "body": "There are 2 envoys and connect to same control plane with same config file,but they have different memory usage.\r\nHere are admin interface about /memeory.\r\n```\r\nNo.1 envoy\r\n{\r\n \"allocated\": \"11345864\",\r\n \"heap_size\": \"197132288\",\r\n \"pageheap_unmapped\": \"3751936\",\r\n \"pageheap_free\": \"24576\",\r\n \"total_thread_cache\": \"72408\",\r\n \"total_physical_bytes\": \"211913462\"\r\n}\r\n\r\nNo.2 envoy\r\n{\r\n \"allocated\": \"10958840\",\r\n \"heap_size\": \"18874368\",\r\n \"pageheap_unmapped\": \"0\",\r\n \"pageheap_free\": \"4112384\",\r\n \"total_thread_cache\": \"2725568\",\r\n \"total_physical_bytes\": \"20628054\"\r\n}\r\n```\r\nI also use top to see memroy usage:\r\n```\r\nNo.1 envoy\r\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\r\n4127970 root    20   0 3722012 209416   8300 S   0.0  0.0 274:39.53 envoy\r\nNo.2 envoy\r\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\r\n24117 root    20   0 3722000  44152  23292 S   0.0  0.0 228:41.91 envoy\r\n```\r\nThey have quite similar virtual memory,but the res is quite different.\r\nwhat's possible reason cause this problem?\r\nAny help will be appreciated!!!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18515/comments",
    "author": "pyrl247",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2021-10-08T16:11:03Z",
        "body": "The allocated and heap_size are close \r\n\"allocated\": \"11345864\" vs \"10958840\"\r\n\r\nThat's the heap envoy requested for traffic and configuration.\r\n\r\nThe divergence of total_physical_bytes and heap_size is held by heap allocator.\r\n\r\nThe allocator is either gperftools/tcmalloc or google/tcmalloc, depending on how you build your envoy.\r\n\r\nThe above 2 have corresponding tuning guides on their own page."
      },
      {
        "user": "pyrl247",
        "created_at": "2021-10-09T08:30:01Z",
        "body": "The heap_size are quite different,the No.1 envoy is 10 times bigger than No.2.\r\n\r\nI havn't used gperftools/tcmalloc so  can't dump the heap profile.\r\n\r\nThe virtualmemory I see through top are close ,but the res are quite different.\r\n\r\nAnd the memory are different at envoy first start.\r\n\r\n"
      },
      {
        "user": "lambdai",
        "created_at": "2021-10-11T22:29:58Z",
        "body": "You are right. The reserved heap_size has 10x difference.\r\n\r\nMy two cents: blindly try another memory dump via this handler. A `tryShrinkHeap` will be called and the allocator will do shrink at the first envoy since the allocated is 100MB away from heap_size.\r\nIf the triggered shrink erase the gap, you are good. \r\n\r\nOtherwise, you can try the following steps:\r\n1. Confirm the two envoys are build with the same tcmalloc, boostrap config, command line, and same host env( --concurrency = 0) spawn # of cpu cores worker thread.\r\n2. do not connect to control plane and see if there is difference. \r\nOnce you figured out the phase where the gap is introduce, try using  gperftool tcmalloc, profile the memory usage covering that phase.\r\nThen you can compare alloc_objects and alloc_spaces between the two envoy instance. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-11T00:01:56Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-18T00:02:00Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18404,
    "title": "Routing TCP traffic with original destination preserved through hops over multiple Envoys",
    "created_at": "2021-10-04T15:52:33Z",
    "closed_at": "2021-11-15T00:02:02Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18404",
    "body": "*Title*: *Routing TCP traffic with original destination preserved through hops over multiple Envoys*\r\n\r\n*Description*:\r\n>Is there a way to route original destination traffic over multiple Envoy hops? Here is the scheme I had in mind.\r\n\r\n    caller --> NAT rule ---> Envoy1 [original dst listener ---> Envoy2] ----> Envoy2 [original dst listener ---> original dst cluster] ---> original dst\r\n> The idea is that Envoy2 might terminate TLS or do some authentication and then forward the traffic to the original destination. Plus Envoy2 acts like a gateway / front-proxy for a set of services? I am trying to do this specifically for TCP traffic.\r\n\r\n\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18404/comments",
    "author": "amukherj1",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2021-10-05T06:39:27Z",
        "body": "You must find a carrier to deliver the \"original dst\" at each hop. Pure Tcp doesn't work.\r\n\r\nThe first hop is carried by NAT impl and the original can be obtained by socket opt SO_ORIGINAL_DST\r\n\r\nThere are various options at the 2nd hop, including mapping from TLS extensions, ProxyProtocol ,or HTTP CONNECT. \r\n\r\nAll the above 3 approaches can embed out of bound TCP data at handshake-ish phase so that you can some how recover the original_dst from"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-07T20:01:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-15T00:02:01Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18396,
    "title": "document for filter_metadata (repeated map<string, Struct>) is wrong ,not repeated?",
    "created_at": "2021-10-04T10:11:20Z",
    "closed_at": "2021-11-10T16:06:17Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18396",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\nconfig.core.v3.Metadata¶\r\n[config.core.v3.Metadata proto]\r\n\r\nMetadata provides additional inputs to filters based on matched listeners, filter chains, routes and endpoints. It is structured as a map, usually from filter name (in reverse DNS format) to metadata specific to the filter. Metadata key-values for a filter are merged as connection and request handling occurs, with later values for the same key overriding earlier values.\r\n\r\nAn example use of metadata is providing additional values to http_connection_manager in the envoy.http_connection_manager.access_log namespace.\r\n\r\nAnother example use of metadata is to per service config info in cluster metadata, which may get consumed by multiple filters.\r\n\r\nFor load balancing, Metadata provides a means to subset cluster endpoints. Endpoints have a Metadata object associated and routes contain a Metadata object to match against. There are some well defined metadata used today for this purpose:\r\n\r\n{\"envoy.lb\": {\"canary\": <bool> }} This indicates the canary status of an endpoint and is also used during header processing (x-envoy-upstream-canary) and for stats purposes.\r\n\r\n{\r\n  \"filter_metadata\": \"{...}\",\r\n  \"typed_filter_metadata\": \"{...}\"\r\n}\r\nfilter_metadata\r\n(repeated map<string, Struct>) Key is the reverse DNS filter name, e.g. com.acme.widget. The envoy.* namespace is reserved for Envoy’s built-in filters. If both filter_metadata and typed_filter_metadata fields are present in the metadata with same keys, only typed_filter_metadata field will be parsed.\r\n\r\ntyped_filter_metadata\r\n(repeated map<string, Any>) Key is the reverse DNS filter name, e.g. com.acme.widget. The envoy.* namespace is reserved for Envoy’s built-in filters. The value is encoded as google.protobuf.Any. If both filter_metadata and typed_filter_metadata fields are present in the metadata with same keys, only typed_filter_metadata field will be parsed.\r\n```\r\nfilter_metadata in document is (repeated map<string, Struct>)  but when I add - infront \r\n\r\n```\r\nResource: \"networking.istio.io/v1alpha3, Resource=envoyfilters\", GroupVersionKind: \"networking.istio.io/v1alpha3, Kind=EnvoyFilter\"\r\nName: \"httpconnectionmanager\", Namespace: \"istio-system\"\r\nfor: \"ef-route_config-virtual_hosts-routes-route-genral.yaml\": admission webhook \"validation.istio.io\" denied the request: configuration is invalid: Envoy filter: can't unmarshal Any nested proto type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager: json: cannot unmarshal array into Go value of type map[string]json.RawMessage\r\n```\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: httpconnectionmanager\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n  - applyTo: NETWORK_FILTER\r\n    match:\r\n      context: GATEWAY\r\n      listener:\r\n        portNumber: 8080\r\n        filterChain:\r\n          filter:\r\n            name: \"envoy.filters.network.http_connection_manager\"\r\n    patch:\r\n      operation: MERGE\r\n      value:\r\n              name: envoy.filters.network.http_connection_manager\r\n              typedConfig:\r\n                '@type': type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                route_config:\r\n                  name: test\r\n                  virtual_hosts:\r\n                  - name: test\r\n                    domains:\r\n                    - \"*\"\r\n                    routes:\r\n                    - name: testroute\r\n                      match: \r\n                        prefix: /product\r\n                      route:\r\n                        cluster_not_found_response_code: 404\r\n                        metadata_match:\r\n                          filter_metadata:\r\n                          - \"envoy.lb\": \r\n                              canary: true\r\n                        timeout: 10s\r\n                        idle_timeout: 5s\r\n                        priority: HIGH\r\n                        max_stream_duration: \r\n                          max_stream_duration: 10s\r\n                          grpc_timeout_header_max: 5s\r\n                          grpc_timeout_header_offset: 3s\r\n                        cluster: outbound|9080||productpage.istio.svc.cluster.local\r\n```\r\n\r\nif I delete -,it is ok.\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: httpconnectionmanager\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n  - applyTo: NETWORK_FILTER\r\n    match:\r\n      context: GATEWAY\r\n      listener:\r\n        portNumber: 8080\r\n        filterChain:\r\n          filter:\r\n            name: \"envoy.filters.network.http_connection_manager\"\r\n    patch:\r\n      operation: MERGE\r\n      value:\r\n              name: envoy.filters.network.http_connection_manager\r\n              typedConfig:\r\n                '@type': type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                route_config:\r\n                  name: test\r\n                  virtual_hosts:\r\n                  - name: test\r\n                    domains:\r\n                    - \"*\"\r\n                    routes:\r\n                    - name: testroute\r\n                      match: \r\n                        prefix: /product\r\n                      route:\r\n                        cluster_not_found_response_code: 404\r\n                        metadata_match:\r\n                          filter_metadata:\r\n                            \"envoy.lb\": \r\n                              canary: true\r\n                        timeout: 10s\r\n                        idle_timeout: 5s\r\n                        priority: HIGH\r\n                        max_stream_duration: \r\n                          max_stream_duration: 10s\r\n                          grpc_timeout_header_max: 5s\r\n                          grpc_timeout_header_offset: 3s\r\n                        cluster: outbound|9080||productpage.istio.svc.cluster.local\r\n```\r\n\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18396/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-03T16:06:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-10T16:06:16Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18395,
    "title": "what is predicates's @type config?I can not find them in the envoy config?",
    "created_at": "2021-10-04T09:42:23Z",
    "closed_at": "2021-11-10T16:06:15Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18395",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\npredicates\r\n(repeated config.core.v3.TypedExtensionConfig) Specifies a list of predicates that are queried when an upstream response is deemed to trigger an internal redirect by all other criteria. Any predicate in the list can reject the redirect, causing the response to be proxied to downstream.\r\n\r\nTip\r\n\r\nThis extension category has the following known extensions:\r\n\r\nenvoy.internal_redirect_predicates.allow_listed_routes\r\n\r\nenvoy.internal_redirect_predicates.previous_routes\r\n\r\nenvoy.internal_redirect_predicates.safe_cross_scheme\r\n```\r\nwhat is follow config's @type config?\r\nenvoy.internal_redirect_predicates.allow_listed_routes\r\n\r\nenvoy.internal_redirect_predicates.previous_routes\r\n\r\nenvoy.internal_redirect_predicates.safe_cross_scheme\r\n\r\n```\r\nResource: \"networking.istio.io/v1alpha3, Resource=envoyfilters\", GroupVersionKind: \"networking.istio.io/v1alpha3, Kind=EnvoyFilter\"\r\nName: \"httpconnectionmanager\", Namespace: \"istio-system\"\r\nfor: \"ef-route_config-virtual_hosts-routes-route-internal_redirect_policy.yaml\": admission webhook \"validation.istio.io\" denied the request: configuration is invalid: Envoy filter: can't unmarshal Any nested proto type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager: Any JSON doesn't have '@type'\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18395/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-03T16:06:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-10T16:06:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18391,
    "title": "per_try_idle_timeout is missing",
    "created_at": "2021-10-04T02:13:11Z",
    "closed_at": "2021-11-11T04:01:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18391",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n```\r\nconfig.route.v3.RetryPolicy¶\r\n[config.route.v3.RetryPolicy proto]\r\n\r\nHTTP retry architecture overview.\r\n\r\n{\r\n  \"retry_on\": \"...\",\r\n  \"num_retries\": \"{...}\",\r\n  \"per_try_timeout\": \"{...}\",\r\n  \"per_try_idle_timeout\": \"{...}\",\r\n  \"retry_priority\": \"{...}\",\r\n  \"retry_host_predicate\": [],\r\n  \"retry_options_predicates\": [],\r\n  \"host_selection_retry_max_attempts\": \"...\",\r\n  \"retriable_status_codes\": [],\r\n  \"retry_back_off\": \"{...}\",\r\n  \"rate_limited_retry_back_off\": \"{...}\",\r\n  \"retriable_headers\": [],\r\n  \"retriable_request_headers\": []\r\n}\r\nretr\r\n```\r\n\r\nwe have per_try_idle_timeout  in the document? bug apply got error?\r\n\r\n```\r\n[root@node01 httpconnectionmanager]# kubectl apply -f ef-route_config-virtual_hosts-routes-route-retry_policy-general.yaml -n istio-system\r\nWarning: Envoy filter: can't unmarshal Any nested proto type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager: unknown field \"per_try_idle_timeout\" in envoy.config.route.v3.RetryPolicy\r\n```\r\n\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: httpconnectionmanager\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n  - applyTo: NETWORK_FILTER\r\n    match:\r\n      context: GATEWAY\r\n      listener:\r\n        portNumber: 8080\r\n        filterChain:\r\n          filter:\r\n            name: \"envoy.filters.network.http_connection_manager\"\r\n    patch:\r\n      operation: MERGE\r\n      value:\r\n              name: envoy.filters.network.http_connection_manager\r\n              typedConfig:\r\n                '@type': type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                route_config:\r\n                  name: test\r\n                  virtual_hosts:\r\n                  - name: test\r\n                    domains:\r\n                    - \"test.test:8080\"\r\n                    - \"test.test:32688\"\r\n                    routes:\r\n                    - name: testroute\r\n                      match: \r\n                        prefix: /product\r\n                      route:\r\n                        retry_policy:\r\n                          retry_on: 5xx,gateway-error,reset,connect-failure\r\n                          num_retries: 3\r\n                          per_try_timeout: 10s\r\n                          per_try_idle_timeout: 10s\r\n                          retry_priority:\r\n                            name: envoy.retry_priorities.previous_priorities\r\n                            typed_config:\r\n                              \"@type\": type.googleapis.com/envoy.extensions.retry.priority.previous_priorities.v3.PreviousPrioritiesConfig\r\n                              update_frequency: 2\r\n                          retry_host_predicate:\r\n                          - name: envoy.retry_host_predicates.previous_hosts  \r\n                          host_selection_retry_max_attempts: 3\r\n                          retriable_status_codes: \r\n                          - 503 \r\n                          retry_back_off:\r\n                            base_interval: 10ms\r\n                            max_interval: 50ms\r\n                          rate_limited_retry_back_off:\r\n                            reset_headers:\r\n                            - name: Retry-After\r\n                              format: SECONDS\r\n                            - name: X-RateLimit-Reset\r\n                              format: UNIX_TIMESTAMP\r\n                            max_interval: \"300s\"\r\n                          retriable_headers:\r\n                          - name: test\r\n                            exact_match: test\r\n                          retriable_request_headers:\r\n                          - name: test\r\n                            exact_match: test\r\n                        weighted_clusters:\r\n                          clusters:\r\n                          - name: outbound|9080||productpage.istio.svc.cluster.local\r\n                            weight: 100\r\n                          total_weight: 100\r\n                          runtime_key_prefix: test\r\n```\r\n*Description*:\r\n>Describe the issue.\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18391/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "ramaraochavali",
        "created_at": "2021-10-04T13:57:24Z",
        "body": "This is an Istio issue not an Envoy issue. Which release of Istio are you testing with? The reason `per_try_idle_timeout` does not work it is newly added and Istio might not have the Envoy with that version."
      },
      {
        "user": "13567436138",
        "created_at": "2021-10-05T00:39:08Z",
        "body": "ok，I see"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-04T04:01:16Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-11T04:01:18Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18390,
    "title": "what  extension configuration does this config have?",
    "created_at": "2021-10-04T01:58:52Z",
    "closed_at": "2021-11-10T16:06:13Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18390",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n\r\n```\r\nconfig.core.v3.TypedExtensionConfig¶\r\n[config.core.v3.TypedExtensionConfig proto]\r\n\r\nMessage type for extension configuration. .\r\n\r\n{\r\n  \"name\": \"...\",\r\n  \"typed_config\": \"{...}\"\r\n}\r\nname\r\n(string, REQUIRED) The name of an extension. This is not used to select the extension, instead it serves the role of an opaque identifier.\r\n\r\ntyped_config\r\n(Any) The typed config for the extension. The type URL will be used to identify the extension. In the case that the type URL is udpa.type.v1.TypedStruct, the inner type URL of TypedStruct will be utilized. See the extension configuration overview for further details.\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18390/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-03T16:06:10Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-10T16:06:13Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18386,
    "title": "the envoy document has cluster_header,but when config show error?",
    "created_at": "2021-10-03T10:04:56Z",
    "closed_at": "2021-11-11T04:01:17Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18386",
    "body": "*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the desired behavior, what scenario it enables and how it\r\nwould be used.\r\n```\r\nconfig.route.v3.WeightedCluster.ClusterWeight¶\r\n[config.route.v3.WeightedCluster.ClusterWeight proto]\r\n\r\n{\r\n  \"name\": \"...\",\r\n  \"cluster_header\": \"...\",\r\n  \"weight\": \"{...}\",\r\n  \"metadata_match\": \"{...}\",\r\n  \"request_headers_to_add\": [],\r\n  \"request_headers_to_remove\": [],\r\n  \"response_headers_to_add\": [],\r\n  \"response_headers_to_remove\": [],\r\n  \"typed_per_filter_config\": \"{...}\",\r\n  \"host_rewrite_literal\": \"...\"\r\n}\r\nname\r\n(string) Only one of name and cluster_header may be specified. Name of the upstream cluster. The cluster must exist in the cluster manager configuration.\r\n\r\ncluster_header\r\n(string) Only one of name and cluster_header may be specified. Envoy will determine the cluster to route to by reading the value of the HTTP header named by cluster_header from the request headers. If the header is not found or the referenced cluster does not exist, Envoy will return a 404 response.\r\n```\r\n\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: httpconnectionmanager\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingressgateway\r\n  configPatches:\r\n  - applyTo: NETWORK_FILTER\r\n    match:\r\n      context: GATEWAY\r\n      listener:\r\n        portNumber: 8080\r\n        filterChain:\r\n          filter:\r\n            name: \"envoy.filters.network.http_connection_manager\"\r\n    patch:\r\n      operation: MERGE\r\n      value:\r\n              name: envoy.filters.network.http_connection_manager\r\n              typedConfig:\r\n                '@type': type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                route_config:\r\n                  name: test\r\n                  virtual_hosts:\r\n                  - name: test\r\n                    domains:\r\n                    - \"*\"\r\n                    routes:\r\n                    - name: testroute\r\n                      match: \r\n                        path: /productpage\r\n                        case_sensitive: false\r\n                      route:\r\n                        weighted_clusters:\r\n                          clusters:\r\n                          - cluster_header: upstream_cluster\r\n                            weight: 100\r\n                          total_weight: 100\r\n                          runtime_key_prefix: test\r\n```\r\n\r\n```\r\n[root@node01 httpconnectionmanager]# kubectl apply -f ef-route_config-virtual_hosts-routes-route-weighted_clusters-cluster_header.yaml -n istio-system\r\nWarning: Envoy filter: can't unmarshal Any nested proto type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager: unknown field \"cluster_header\" in envoy.config.route.v3.WeightedCluster.ClusterWeight\r\n```\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18386/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "ramaraochavali",
        "created_at": "2021-10-04T14:01:45Z",
        "body": "This an Istio issue not an Envoy issue. Weighted cluster's cluster_header is newly added and the Envoy with Istio version you are using might not have that field."
      },
      {
        "user": "13567436138",
        "created_at": "2021-10-05T00:40:02Z",
        "body": "ok，I see"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-04T04:01:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-11T04:01:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18385,
    "title": "MetadataMatcher what filter can we use ? can anyone give an example of this config?",
    "created_at": "2021-10-03T08:52:12Z",
    "closed_at": "2021-11-10T00:01:57Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18385",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\ntype.matcher.v3.MetadataMatcher¶\r\n[type.matcher.v3.MetadataMatcher proto]\r\n\r\n{\r\n  \"filter\": \"...\",\r\n  \"path\": [],\r\n  \"value\": \"{...}\",\r\n  \"invert\": \"...\"\r\n}\r\nfilter\r\n(string, REQUIRED) The filter name to retrieve the Struct from the Metadata.\r\n\r\npath\r\n(repeated type.matcher.v3.MetadataMatcher.PathSegment, REQUIRED) The path to retrieve the Value from the Struct.\r\n\r\nvalue\r\n(type.matcher.v3.ValueMatcher, REQUIRED) The MetadataMatcher is matched if the value retrieved by path is matched to this value.\r\n\r\ninvert\r\n(bool) If true, the match result will be inverted.\r\n```\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18385/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-02T20:01:37Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-10T00:01:56Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18383,
    "title": "what is the runtime_key? is it customed or fixed?runtime_key: \"bogus_key\"",
    "created_at": "2021-10-02T02:42:44Z",
    "closed_at": "2021-11-10T00:01:55Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18383",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\nredis_proxy fault config\r\n*Description*:\r\n>Describe the issue.\r\n```\r\nfaults:\r\n- fault_type: ERROR\r\n  fault_enabled:\r\n    default_value:\r\n      numerator: 10\r\n      denominator: HUNDRED\r\n    runtime_key: \"bogus_key\"\r\n    commands:\r\n    - GET\r\n  - fault_type: DELAY\r\n    fault_enabled:\r\n      default_value:\r\n        numerator: 10\r\n        denominator: HUNDRED\r\n      runtime_key: \"bogus_key\"\r\n    delay: 2s\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18383/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-02T20:01:36Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-10T00:01:55Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18369,
    "title": "params_match, Method parameter definition. The key is the parameter index, starting from 0. The value is the parameter matching type.  is parameter  type int?I wander?can anyone give an example? ",
    "created_at": "2021-10-01T07:58:12Z",
    "closed_at": "2021-11-12T04:01:19Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18369",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n\r\nextensions.filters.network.dubbo_proxy.v3.MethodMatch¶\r\n[extensions.filters.network.dubbo_proxy.v3.MethodMatch proto]\r\n\r\n{\r\n  \"name\": \"{...}\",\r\n  \"params_match\": \"{...}\"\r\n}\r\nname\r\n(type.matcher.v3.StringMatcher) The name of the method.\r\n\r\nparams_match\r\n(repeated map<uint32, extensions.filters.network.dubbo_proxy.v3.MethodMatch.ParameterMatchSpecifier>) Method parameter definition. The key is the parameter index, starting from 0. The value is the parameter matching type.\r\n\r\nextensions.filters.network.dubbo_proxy.v3.MethodMatch.ParameterMatchSpecifier¶\r\n[extensions.filters.network.dubbo_proxy.v3.MethodMatch.ParameterMatchSpecifier proto]\r\n\r\nThe parameter matching type.\r\n\r\n{\r\n  \"exact_match\": \"...\",\r\n  \"range_match\": \"{...}\"\r\n}\r\nexact_match\r\n(string) If specified, header match will be performed based on the value of the header.\r\n\r\nOnly one of exact_match, range_match may be set.\r\n\r\nrange_match\r\n(type.v3.Int64Range) If specified, header match will be performed based on range. The rule will match if the request header value is within this range. The entire request header value must represent an integer in base 10 notation: consisting of an optional plus or minus sign followed by a sequence of digits. The rule will not match if the header value does not represent an integer. Match will fail for empty values, floating point numbers or if only a subsequence of the header value is an integer.\r\n\r\nExamples:\r\n\r\nFor range [-10,0), route will match for header value -1, but not for 0, “somestring”, 10.9, “-1somestring”\r\n\r\nOnly one of exact_match, range_match may be set.\r\n\r\n\r\n\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18369/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-10-03T17:17:19Z",
        "body": "cc @wbpcode "
      },
      {
        "user": "wbpcode",
        "created_at": "2021-10-06T02:32:41Z",
        "body": "Hi. Again, sorry for the delayed response. I am in my long holiday and cannot pay attention to every news in time.\r\n\r\nThe parameter match is used to match the dubbo RPC args. **Since the type of args/params is determined by Dubbo RPC itself, it is very diverse.**(It can be any Java type. So it is hard to give specific examples.) But only a few types of args/params can be matched correctly. Such as string or number type args.\r\n\r\nUnless there is a clear demand, the parameter match is not recommend because it's a little complex and low performance."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-05T04:01:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-12T04:01:19Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "13567436138",
        "created_at": "2022-07-06T03:14:20Z",
        "body": "/reopen"
      }
    ]
  },
  {
    "number": 18366,
    "title": "what is dubbo_proxy route_config group version for?when I add group config,the call will timeout and server no log?",
    "created_at": "2021-10-01T05:51:25Z",
    "closed_at": "2021-11-12T04:01:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18366",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: envoyfilter-dubbo-proxy\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      app: dubbo-hello-provider\r\n  configPatches:\r\n  - applyTo: NETWORK_FILTER\r\n    match:\r\n      listener:\r\n        name: 10.68.45.215_20880\r\n        filterChain:\r\n          filter:\r\n            name: \"envoy.filters.network.tcp_proxy\"\r\n    patch:\r\n      operation: REPLACE\r\n      value:\r\n        name: envoy.filters.network.dubbo_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.dubbo_proxy.v3.DubboProxy\r\n          stat_prefix: outbound|20880||org.example.api.IGroupService\r\n          protocol_type: Dubbo\r\n          serialization_type: Hessian2\r\n          route_config:\r\n          - name: outbound|20880||org.example.api.IGroupService\r\n            interface: org.example.api.IGroupService\r\n            #group: group1\r\n            routes:\r\n            - match:\r\n                method:\r\n                  name:\r\n                    exact: dubboCallProiderService\r\n              route:\r\n                cluster: outbound|20880|v2|dubbo-hello-provider.istio.svc.cluster.local\r\n  - applyTo: NETWORK_FILTER\r\n    match:\r\n      listener:\r\n        name: virtualInbound\r\n        filterChain:\r\n          destinationPort: 20880\r\n          filter:\r\n            name: \"envoy.filters.network.tcp_proxy\"\r\n    patch:\r\n      operation: REPLACE\r\n      value:\r\n        name: envoy.filters.network.dubbo_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.dubbo_proxy.v3.DubboProxy\r\n          stat_prefix: inbound|20880||\r\n          protocol_type: Dubbo\r\n          serialization_type: Hessian2\r\n          route_config:\r\n          - name: inbound|20880||\r\n            interface: org.example.api.IGroupService\r\n            #group: group1\r\n            routes:\r\n            - match:\r\n                method:\r\n                  name:\r\n                    exact: dubboCallProiderService\r\n              route:\r\n                cluster: outbound|20880|v2|dubbo-hello-provider.istio.svc.cluster.local\r\n```\r\n\r\nwhen I add group config,the call will timeout and server no log?\r\ndoes it mean client setting or server setting?\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18366/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-10-03T17:16:56Z",
        "body": "cc @wbpcode "
      },
      {
        "user": "wbpcode",
        "created_at": "2021-10-06T02:11:52Z",
        "body": "Hi, sorry for the delayed response. I am in my long holiday and cannot pay attention to every news in time.\r\n\r\n`group` is used to match the dubbo interface group.\r\n\r\nCan you give some more detailed info about this question such as envoy version and how to reproduce it. Thanks."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-05T04:01:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-12T04:01:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "13567436138",
        "created_at": "2022-07-06T03:13:47Z",
        "body": "\\reopen"
      }
    ]
  },
  {
    "number": 18364,
    "title": "bug in request_mirror_policies when putting large object ",
    "created_at": "2021-10-01T03:56:02Z",
    "closed_at": "2021-11-10T00:01:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18364",
    "body": "I use the feature of mirror policy.\r\n```\r\n                          route:\r\n                            cluster: srvA\r\n                            request_mirror_policies:\r\n                              cluster: srvB\r\n                              runtime_fraction:\r\n                                { default_value: { numerator: 100 } \r\n```\r\nwhen I put a small object to envoy(KB), srvA and srvB would both process the request. \r\nbut when I put a larger object(about 10M), only srvA can receive the request.\r\ncan someone help?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18364/comments",
    "author": "huangp0600",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-02T20:01:33Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-10T00:01:52Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18357,
    "title": "Key-based Authentication filter",
    "created_at": "2021-09-30T17:08:42Z",
    "closed_at": "2021-11-10T00:01:51Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18357",
    "body": "# Key-based Authentication Filter\r\n\r\nA filter to authenticate that computes the SHA256 hash of request's authorization key with an appended salt to validate against pre-calculated hashes. The filter uses the following pseudo-code:\r\n```\r\n(\r\n  SHA256(<key><salt1>) == <hash1> ||\r\n  SHA256(<key><salt2>) == <hash2>\r\n) ? HTTP 200, Ok\r\n  : HTTP 401, Unauthorized\r\n```\r\nFor example, only requests with `-H \"Authorization: Bearer foo\"` or `-H \"Authorization: Bearer bar\"` would be allowed with this filter configuration:\r\n```\r\n\"RequestKeys\": {\r\n  \"Primary\": {\r\n    \"Hash\": \"d8b4e3b92dec140f77fe712a580468e5b340672970f82028144a5f56a793c1bd\",\r\n    \"Salt\": \"barbarba\"\r\n  },\r\n  \"Secondary\": {\r\n    \"Hash\": \"344ea5535fa4924d48497e084fb29daaf536c41264d712b585ad4e872f771524\",\r\n    \"Salt\": \"foofoofo\"\r\n  }\r\n}\r\n```\r\nWith this `config.proto`:\r\n```\r\nmessage KeyAuth {\r\n    // The HTTP header name to check for key.\r\n    // Defaults to \"Authorization\" if left empty\r\n    string header = 1;\r\n\r\n    // The HTTP header key prefix. The value format is \"<prefix><key>\"\r\n    // Defaults to \"Bearer \" if left empty, space matters!\r\n    string prefix = 2;\r\n\r\n    // Hash: 64 character hexadecimal SHA256 digest of Salted Key\r\n    // Salt: 8 character string appended to Key\r\n    HashSalt primary = 3;\r\n    HashSalt secondary = 4;\r\n}\r\n\r\nmessage HashSalt {\r\n    // 64 character hexadecimal SHA256 digest of Salted Key\r\n    string hash = 1 [(validate.rules).string = {len: 64 pattern: \"^[a-fA-F0-9]*$\"}];\r\n    // 8 character string appended to Key\r\n    string salt = 2 [(validate.rules).string = {len: 8 pattern: \"^[a-zA-Z0-9./]*$\"}];\r\n}\r\n```\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18357/comments",
    "author": "Mutilar",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-10-03T17:15:47Z",
        "body": "Is this an implementation of some standard? If so which one? Otherwise seems like a good for for Lua or Wasm?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-02T20:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-10T00:01:50Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18172,
    "title": "Question: Can envoy mirror tcp traffic when acting as a tcp proxy?",
    "created_at": "2021-09-18T12:01:51Z",
    "closed_at": "2021-09-22T04:49:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18172",
    "body": "I'm running envoy in tcp proxy mode. I know envoy can mirror traffic when acting in http mode, but not sure if that is possible in tcp proxy mode. I don't see any documentation which suggests  mirroring is supported in tcp mode, but I still wanted to ask.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18172/comments",
    "author": "samitpal",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2021-09-20T14:04:24Z",
        "body": "As far as I know that's not possible today."
      },
      {
        "user": "samitpal",
        "created_at": "2021-09-22T04:49:20Z",
        "body": "Ok, thanks! Closing the issue."
      }
    ]
  },
  {
    "number": 18171,
    "title": "gdb debug problem",
    "created_at": "2021-09-18T07:42:10Z",
    "closed_at": "2021-10-28T00:02:00Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18171",
    "body": "I build envoy with -c dbg option but can't watch variables when debuging with gdb, says:\r\n\r\n-var-create: unable to create variable object \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18171/comments",
    "author": "rtttech",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2021-09-20T14:05:27Z",
        "body": "Not a problem I've run across - have you tried asking on the envoy-dev slack channel to see if anyone else has run into it?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-10-20T16:01:29Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-10-28T00:01:59Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18139,
    "title": "How to measure envoy latency",
    "created_at": "2021-09-16T03:29:35Z",
    "closed_at": "2021-10-23T12:02:07Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18139",
    "body": "*Description*:\r\nWe want to accurately measure the latency overhead of envoy based on the access log, especially for the scenario that the downstream/upstream network is not stable, there are two options in my mind:\r\n1. DURATION - REQUEST_DURATION - UPSTREAM_SERVICE_TIME\r\n2. REQUEST_TX_DURATION - REQUEST_DURATION  + RESPONSE_TX_DURATION\r\n\r\nOne question is REQUEST_DURATION end by the listener receive the last byte from downstream or by the router receive the last byte? the difference is the time spent on the filter chains.\r\n\r\nIf the above two options are not correct, what is the best option to measure envoy latency, any suggestions? thanks.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18139/comments",
    "author": "zhiyong-gayang",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-10-16T12:01:29Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-10-23T12:02:06Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18111,
    "title": "impl zoneaware with ringhash/maglev",
    "created_at": "2021-09-14T09:45:57Z",
    "closed_at": "2021-10-27T20:01:31Z",
    "labels": [
      "enhancement",
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18111",
    "body": "*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the desired behavior, what scenario it enables and how it\r\nwould be used.\r\n\r\nnow,when i config the hash policy with zoneaware opend, the zoneaware doesn't work,how to set a session affinity with zoneaware routing?\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18111/comments",
    "author": "du2016",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-10-20T16:01:23Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-10-27T20:01:30Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 18038,
    "title": "assert failure: SOCKET_VALID(result.rc_). Details: socket(2) failed, got error: Too many open files",
    "created_at": "2021-09-09T08:29:35Z",
    "closed_at": "2021-11-03T12:01:18Z",
    "labels": [
      "question",
      "stale",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18038",
    "body": "*Title*: *assert failure: SOCKET_VALID(result.rc_). Details: socket(2) failed, got error: Too many open files*\r\n\r\n*Description*:\r\nWhen I use envoy release binaries directly on a host machine (not with docker), the proxy crashes under high load giving the above assert failure. I went through some of the similar issues and seems like the issue is with the file descriptor limit of the host machine. In envoy docker containers `ulimit` is set as `unlimited`. Also I saw an option to configure the proxy for `max_connections` either per listener or globally. So I have the following questions regarding this scenario.\r\n\r\n1. Are there any critical host OS specific configs like `ulimit` we need to do to have the proxy without crashing ? Or any security related configs in the host OS ?\r\n2. When configuring `max_connection`, how it relates with the `ulimit` ? Probably max_connection should be lesser than `ulimit` because of multiple TCP connections per each client request. Maybe having `max_connection` 50% of `ulimit` ? (based on `downstream -> proxy` and `proxy -> upstream`)\r\n\r\nAlso I debugged the issue with gdb and have the following logs.\r\n\r\n```\r\n[2021-09-05 15:58:55.379][300071][critical][assert] [external/envoy/source/common/network/socket_interface_impl.cc:51] assert failure: SOCKET_VALID(result.rc_). Details: socket(2) failed, got error: Too many open files\r\nThread 22 \"wrk:worker_4\" received signal SIGABRT, Aborted.\r\n[Switching to Thread 0x7ff0ed3e5640 (LWP 300071)]\r\n__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:49\r\n49\t../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n```     \r\n\r\nFull backtrace\r\n\r\n```\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:49\r\n#1  0x00007ffff7c73864 in __GI_abort () at abort.c:79\r\n#2  0x00005555588a4b94 in Envoy::Network::SocketInterfaceImpl::socket(Envoy::Network::Socket::Type, Envoy::Network::Address::Type, Envoy::Network::Address::IpVersion, bool) const ()\r\n#3  0x00005555588a4caa in Envoy::Network::SocketInterfaceImpl::socket(Envoy::Network::Socket::Type, std::__1::shared_ptr<Envoy::Network::Address::Instance const>) const ()\r\n#4  0x00005555586e8b8b in Envoy::Network::ClientSocketImpl::ClientSocketImpl(std::__1::shared_ptr<Envoy::Network::Address::Instance const> const&, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<Envoy::Network::Socket::Option const>, std::__1::allocator<std::__1::shared_ptr<Envoy::Network::Socket::Option const> > > > const&) ()\r\n#5  0x00005555586e5a91 in Envoy::Network::ClientConnectionImpl::ClientConnectionImpl(Envoy::Event::Dispatcher&, std::__1::shared_ptr<Envoy::Network::Address::Instance const> const&, std::__1::shared_ptr<Envoy::Network::Address::Instance const> const&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<Envoy::Network::Socket::Option const>, std::__1::allocator<std::__1::shared_ptr<Envoy::Network::Socket::Option const> > > > const&) ()\r\n#6  0x0000555558219e8b in Envoy::Event::DispatcherImpl::createClientConnection(std::__1::shared_ptr<Envoy::Network::Address::Instance const>, std::__1::shared_ptr<Envoy::Network::Address::Instance const>, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<Envoy::Network::Socket::Option const>, std::__1::allocator<std::__1::shared_ptr<Envoy::Network::Socket::Option const> > > > const&) ()\r\n#7  0x00005555583fe4b6 in Envoy::Upstream::HostImpl::createConnection(Envoy::Event::Dispatcher&, Envoy::Upstream::ClusterInfo const&, std::__1::shared_ptr<Envoy::Network::Address::Instance const> const&, Envoy::Network::TransportSocketFactory&, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<Envoy::Network::Socket::Option const>, std::__1::allocator<std::__1::shared_ptr<Envoy::Network::Socket::Option const> > > > const&, std::__1::shared_ptr<Envoy::Network::TransportSocketOptions const>) ()\r\n#8  0x00005555583fe1b8 in Envoy::Upstream::HostImpl::createConnection(Envoy::Event::Dispatcher&, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<Envoy::Network::Socket::Option const>, std::__1::allocator<std::__1::shared_ptr<Envoy::Network::Socket::Option const> > > > const&, std::__1::shared_ptr<Envoy::Network::TransportSocketOptions const>) const ()\r\n#9  0x00005555583fe6e5 in non-virtual thunk to Envoy::Upstream::HostImpl::createConnection(Envoy::Event::Dispatcher&, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<Envoy::Network::Socket::Option const>, std::__1::allocator<std::__1::shared_ptr<Envoy::Network::Socket::Option const> > > > const&, std::__1::shared_ptr<Envoy::Network::TransportSocketOptions const>) const ()\r\n#10 0x00005555583eaa40 in Envoy::Http::ActiveClient::ActiveClient(Envoy::Http::HttpConnPoolImplBase&, unsigned int, unsigned int) ()\r\n#11 0x00005555583ea855 in Envoy::Http::Http1::ActiveClient::ActiveClient(Envoy::Http::HttpConnPoolImplBase&) ()\r\n#12 0x00005555583ebc88 in std::__1::__function::__func<Envoy::Http::Http1::allocateConnPool(Envoy::Event::Dispatcher&, Envoy::Random::RandomGenerator&, std::__1::shared_ptr<Envoy::Upstream::Host const>, Envoy::Upstream::ResourcePriority, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<Envoy::Network::Socket::Option const>, std::__1::allocator<std::__1::shared_ptr<Envoy::Network::Socket::Option const> > > > const&, std::__1::shared_ptr<Envoy::Network::TransportSocketOptions const> const&, Envoy::Upstream::ClusterConnectivityState&)::$_0, std::__1::allocator<Envoy::Http::Http1::allocateConnPool(Envoy::Event::Dispatcher&, Envoy::Random::RandomGenerator&, std::__1::shared_ptr<Envoy::Upstream::Host const>, Envoy::Upstream::ResourcePriority, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<Envoy::Network::Socket::Option const>, std::__1::allocator<std::__1::shared_ptr<Envoy::Network::Socket::Option const> > > > const&, std::__1::shared_ptr<Envoy::Network::TransportSocketOptions const> const&, Envoy::Upstream::ClusterConnectivityState&)::$_0>, std::__1::unique_ptr<Envoy::ConnectionPool::ActiveClient, std::__1::default_delete<Envoy::ConnectionPool::ActiveClient> > (Envoy::Http::HttpConnPoolImplBase*)>::operator()(Envoy::Http::HttpConnPoolImplBase*&&) ()\r\n#13 0x00005555583ebfc3 in Envoy::Http::FixedHttpConnPoolImpl::instantiateActiveClient() ()\r\n#14 0x00005555583f88be in Envoy::ConnectionPool::ConnPoolImplBase::tryCreateNewConnection(float) ()\r\n#15 0x00005555583fa98d in Envoy::ConnectionPool::ConnPoolImplBase::newStream(Envoy::ConnectionPool::AttachContext&) ()\r\n#16 0x00005555583f5abf in non-virtual thunk to Envoy::Http::HttpConnPoolImplBase::newStream(Envoy::Http::ResponseDecoder&, Envoy::Http::ConnectionPool::Callbacks&) ()\r\n#17 0x00005555586186fb in Envoy::Extensions::Upstreams::Http::Http::HttpConnPool::newStream(Envoy::Router::GenericConnectionPoolCallbacks*) ()\r\n#18 0x000055555861e154 in Envoy::Router::Filter::decodeHeaders(Envoy::Http::RequestHeaderMap&, bool) ()\r\n#19 0x000055555852bc88 in Envoy::Http::FilterManager::decodeHeaders(Envoy::Http::ActiveStreamDecoderFilter*, Envoy::Http::RequestHeaderMap&, bool) ()\r\n#20 0x000055555852aba4 in Envoy::Http::ActiveStreamFilterBase::commonContinue() ()\r\n#21 0x00005555575c0ceb in proxy_wasm::WasmBase::doAfterVmCallActions() ()\r\n#22 0x00005555575c2dae in proxy_wasm::ContextBase::onRequestHeaders(unsigned int, bool) ()\r\n#23 0x00005555573727d4 in virtual thunk to Envoy::Extensions::Common::Wasm::Context::decodeHeaders(Envoy::Http::RequestHeaderMap&, bool) ()\r\n#24 0x000055555852bc88 in Envoy::Http::FilterManager::decodeHeaders(Envoy::Http::ActiveStreamDecoderFilter*, Envoy::Http::RequestHeaderMap&, bool) ()\r\n#25 0x000055555851e6f4 in Envoy::Http::ConnectionManagerImpl::ActiveStream::decodeHeaders(std::__1::unique_ptr<Envoy::Http::RequestHeaderMap, std::__1::default_delete<Envoy::Http::RequestHeaderMap> >&&, bool) ()\r\n#26 0x0000555558554f41 in Envoy::Http::Http1::ServerConnectionImpl::onMessageCompleteBase() ()\r\n#27 0x000055555855263d in Envoy::Http::Http1::ConnectionImpl::onMessageComplete() ()\r\n--Type <RET> for more, q to quit, c to continue without paging--c\r\n#28 0x000055555855a8cf in Envoy::Http::Http1::LegacyHttpParserImpl::Impl::Impl(http_parser_type, void*)::{lambda(http_parser*)#3}::__invoke(http_parser*) ()\r\n#29 0x00005555588af9ce in http_parser_execute ()\r\n#30 0x000055555855a30f in Envoy::Http::Http1::LegacyHttpParserImpl::execute(char const*, int) ()\r\n#31 0x00005555585509f4 in Envoy::Http::Http1::ConnectionImpl::dispatchSlice(char const*, unsigned long) ()\r\n#32 0x000055555855013f in Envoy::Http::Http1::ConnectionImpl::dispatch(Envoy::Buffer::Instance&) ()\r\n#33 0x0000555558550c35 in virtual thunk to Envoy::Http::Http1::ConnectionImpl::dispatch(Envoy::Buffer::Instance&) ()\r\n#34 0x000055555851a20c in Envoy::Http::ConnectionManagerImpl::onData(Envoy::Buffer::Instance&, bool) ()\r\n#35 0x00005555586ee4ef in Envoy::Network::FilterManagerImpl::onContinueReading(Envoy::Network::FilterManagerImpl::ActiveReadFilter*, Envoy::Network::ReadBufferSource&) ()\r\n#36 0x00005555586e4a79 in Envoy::Network::ConnectionImpl::onReadReady() ()\r\n#37 0x00005555586e2589 in Envoy::Network::ConnectionImpl::onFileEvent(unsigned int) ()\r\n#38 0x000055555821d291 in std::__1::__function::__func<Envoy::Event::DispatcherImpl::createFileEvent(int, std::__1::function<void (unsigned int)>, Envoy::Event::FileTriggerType, unsigned int)::$_5, std::__1::allocator<Envoy::Event::DispatcherImpl::createFileEvent(int, std::__1::function<void (unsigned int)>, Envoy::Event::FileTriggerType, unsigned int)::$_5>, void (unsigned int)>::operator()(unsigned int&&) ()\r\n#39 0x000055555821ea7c in Envoy::Event::FileEventImpl::assignEvents(unsigned int, event_base*)::$_1::__invoke(int, short, void*) ()\r\n#40 0x00005555588ce8ab in event_process_active_single_queue ()\r\n#41 0x00005555588cd20e in event_base_loop ()\r\n#42 0x0000555558214393 in Envoy::Server::WorkerImpl::threadRoutine(Envoy::Server::GuardDog&) ()\r\n#43 0x000055555902cd93 in Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, std::__1::optional<Envoy::Thread::Options> const&)::{lambda(void*)#1}::__invoke(void*) ()\r\n#44 0x00007ffff7e42450 in start_thread (arg=0x7ff0eb3e1640) at pthread_create.c:473\r\n#45 0x00007ffff7d64d53 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n``` \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18038/comments",
    "author": "NomadXD",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2021-09-13T23:19:01Z",
        "body": "/etc/security/limits.conf could be the hidden one "
      },
      {
        "user": "aminebelroulccie",
        "created_at": "2021-09-26T21:51:50Z",
        "body": "hello did you find the solution of this problem??"
      },
      {
        "user": "NomadXD",
        "created_at": "2021-09-27T06:55:55Z",
        "body": "@aminebelroulccie It's not actually an issue. You need to figure out the configuration to handle these scenarios. eg: setting up max connection limit."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-10-27T08:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-03T12:01:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "GitHubAmos",
        "created_at": "2022-11-25T03:34:05Z",
        "body": "> /etc/security/limits.conf could be the hidden one\r\n\r\n /etc/security/limits.conf  add as fllows :\r\n   root    soft     nofile  65535\r\n   root    hard    nofile  65535\r\nreboot host ,`ulimit -n` is `65535`,but envoy crashes under high load, actual connect number is 1000 - 2000.\r\nHow config valid?\r\n\r\n "
      },
      {
        "user": "pjjwpc",
        "created_at": "2023-08-02T03:06:54Z",
        "body": "有解决方案么，我这边也遇到了这样的情况，limit配置的很大了"
      }
    ]
  },
  {
    "number": 17903,
    "title": "Customize status code when jwt validation fails",
    "created_at": "2021-08-28T23:05:48Z",
    "closed_at": "2021-10-06T08:01:28Z",
    "labels": [
      "question",
      "stale",
      "area/jwt_authn"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17903",
    "body": "*Title*: Customize status code when jwt validation fails\r\n\r\n*Description*:\r\nI was wondering if there is a way to specify a custom status code to be returned when the jwt validation fails in `envoy.filters.http.jwt_authn`. Normally this returns a 401 status code but I would like to change it to a custom status code like 443.\r\n\r\nI would like to do this because I need a way to somehow, in my client, differentiate between the 401 that envoy could return and the 401 that the server behind envoy may return. I hope I made myself clear.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17903/comments",
    "author": "santi-alvarez",
    "comments": [
      {
        "user": "rojkov",
        "created_at": "2021-08-30T07:02:26Z",
        "body": "/cc  @qiwzhang"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-29T08:01:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-10-06T08:01:27Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17890,
    "title": "Support tunneling tcp over HTTP/2+Connect by retaining the original destination address",
    "created_at": "2021-08-27T07:10:29Z",
    "closed_at": "2021-10-03T20:01:08Z",
    "labels": [
      "enhancement",
      "question",
      "stale",
      "area/configuration",
      "area/tcp_proxy"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17890",
    "body": "Description:\r\n\r\nWe want to use CONNECT over H2 to tunnel TCP traffic from a source to one of N possible destinations. We want to intercept requests with iptables rules, and route them over the CONNECT tunnels retaining their original destination information.\r\nIs there a way we can capture the original destination IP and map the flow to a matching upstream? Do we need to write a filter to do this? Are there any config pointers for achieving this?\r\n\r\nclient <-----> E1 <----------------------------> E2 <-----> server\r\n         &emsp;&emsp;&emsp;&emsp;raw&emsp;&emsp;&emsp;Upstream matching based on&emsp;&emsp;&ensp;raw\r\n&emsp;&emsp;&emsp;&emsp;TCP&emsp;&emsp;&emsp;original destination address&emsp;&emsp;&emsp;TCP\r\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;(TCP tunneled over HTTP/2+connect)\r\n\r\n\r\n\r\ncc: @amukherj1         \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17890/comments",
    "author": "abhinav1998agarwal",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-26T16:01:22Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-10-03T20:01:07Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17866,
    "title": "How to implement HTTP protocol (non TLS), different domain names are configured under different FilterChain or Filter",
    "created_at": "2021-08-26T09:25:01Z",
    "closed_at": "2021-10-03T08:01:29Z",
    "labels": [
      "question",
      "stale",
      "area/http",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17866",
    "body": "*Title*: *How to implement HTTP protocol (non TLS), different domain names are configured under different FilterChain*\r\n\r\n*Description*:\r\n> I configure this now, but an error will be reported after running.\r\n**error message**\r\n[2021-08-26 17:18:40.319][1609823][critical][main] [source/server/server.cc:112] error initializing configuration '/etc/envoy/envoy.yaml': error adding listener '0.0.0.0:80': filter chain '' has the same matching rules defined as ''\r\n[2021-08-26 17:18:40.320][1609823][info][main] [source/server/server.cc:855] exiting\r\nerror adding listener '0.0.0.0:80': filter chain '' has the same matching rules defined as ''\r\n\r\n**configuration file:**\r\n```yaml\r\nstatic_resources:\r\n  listeners:\r\n  - name: 'listener_quic'\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n        - name: envoy.filters.network.http_connection_manager\r\n          typed_config:\r\n            '@type': >-\r\n              type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n            codec_type: HTTP3\r\n            stat_prefix: ingress_http\r\n            route_config:\r\n              name: xxx.abc.com\r\n              virtual_hosts:\r\n                - name: xxx.abc.com\r\n                  domains:\r\n                    - xxx.abc.com\r\n                    - 'xxx.abc.com:80'\r\n                  routes:\r\n                    - match:\r\n                        prefix: /\r\n                      route:\r\n                        cluster: www_abc_com\r\n            http_filters:\r\n              - name: envoy.filters.http.router\r\n            access_log:\r\n              - name: envoy.file.access_log\r\n                typed_config:\r\n                  '@type': >-\r\n                    type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                  path: /var/log/envoy/access.log\r\n                  log_format:\r\n                    json_format:\r\n                      time: '%START_TIME%'\r\n                      bytes_sent: '%BYTES_SENT%'\r\n                      http_x_forwarded_for: '%REQ(X-FORWARDED-FOR)%'\r\n                      protocol: '%PROTOCOL%'\r\n                      remote_addr: '%DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT%'\r\n                      request_host: '%REQ(:AUTHORITY)%'\r\n                      request_method: '%REQ(:METHOD)%'\r\n                      request_time: '%DURATION%'\r\n                      request_uri: '%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%'\r\n                      status: '%RESPONSE_CODE%'\r\n                      upstream_addr: '%UPSTREAM_HOST%'\r\n                      upstream_response_time: '%RESPONSE_TX_DURATION%'\r\n            upgrade_configs:\r\n              - upgrade_type: websocket\r\n    - filters:\r\n        - name: envoy.filters.network.http_connection_manager\r\n          typed_config:\r\n            '@type': >-\r\n              type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n            stat_prefix: ingress_http\r\n            route_config:\r\n              name: www.abc.com\r\n              virtual_hosts:\r\n                - name: www.abc.com\r\n                  domains:\r\n                    - www.abc.com\r\n                    - 'www.abc.com:80'\r\n                  routes:\r\n                    - match:\r\n                        prefix: /\r\n                      route:\r\n                        cluster: www_abc_com\r\n            http_filters:\r\n              - name: envoy.filters.http.router\r\n            access_log:\r\n              - name: envoy.file.access_log\r\n                typed_config:\r\n                  '@type': >-\r\n                    type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                  path: /var/log/envoy/access.log\r\n                  log_format:\r\n                    json_format:\r\n                      time: '%START_TIME%'\r\n                      bytes_sent: '%BYTES_SENT%'\r\n                      http_x_forwarded_for: '%REQ(X-FORWARDED-FOR)%'\r\n                      protocol: '%PROTOCOL%'\r\n                      remote_addr: '%DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT%'\r\n                      request_host: '%REQ(:AUTHORITY)%'\r\n                      request_method: '%REQ(:METHOD)%'\r\n                      request_time: '%DURATION%'\r\n                      request_uri: '%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%'\r\n                      status: '%RESPONSE_CODE%'\r\n                      upstream_addr: '%UPSTREAM_HOST%'\r\n                      upstream_response_time: '%RESPONSE_TX_DURATION%'\r\n            upgrade_configs:\r\n              - upgrade_type: websocket\r\n   \r\n  clusters:\r\n  - name: www_abc_com\r\n    connect_timeout: 5s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: blackbox\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 10.31.153.10\r\n                port_value: 80\r\n\r\nadmin:\r\n  access_log:\r\n  - name: envoy.file_access_log\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n      log_format:\r\n        json_format:\r\n          time: '%START_TIME%'\r\n          bytes_sent: '%BYTES_SENT%'\r\n          http_x_forwarded_for: '%REQ(X-FORWARDED-FOR)%'\r\n          protocol: '%PROTOCOL%'\r\n          remote_addr: '%DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT%'\r\n          request_host: '%REQ(:AUTHORITY)%'\r\n          request_method: '%REQ(:METHOD)%'\r\n          request_time: '%DURATION%'\r\n          request_uri: '%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%'\r\n          status: '%RESPONSE_CODE%'\r\n          upstream_addr: '%UPSTREAM_HOST%'\r\n          upstream_response_time: '%RESPONSE_TX_DURATION%'\r\n      path: \"/var/log/envoy/admin.access.log\"\r\n  address:\r\n    socket_address:\r\n      address: 127.0.0.1\r\n      port_value: 8911\r\nlayered_runtime:\r\n  layers:\r\n    - name: static_layer_0\r\n      static_layer:\r\n        envoy:\r\n          resource_limits:\r\n            listener:\r\n              example_listener_name:\r\n                connection_limit: 10000\r\n        overload:\r\n          global_downstream_max_connections: 50000\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17866/comments",
    "author": "virnet",
    "comments": [
      {
        "user": "virnet",
        "created_at": "2021-08-26T09:33:39Z",
        "body": "If it is placed in the same filterchan and different filters, an error will also be reported.\r\n> **error message**\r\n[2021-08-26 17:27:47.370][1610912][critical][main] [source/server/server.cc:112] error initializing configuration '/etc/envoy/envoy.yaml': Error: terminal filter named envoy.filters.network.http_connection_manager of type envoy.filters.network.http_connection_manager must be the last filter in a network filter chain.\r\n[2021-08-26 17:27:47.371][1610912][info][main] [source/server/server.cc:855] exiting\r\nError: terminal filter named envoy.filters.network.http_connection_manager of type envoy.filters.network.http_connection_manager must be the last filter in a network filter chain.\r\n\r\n```yaml\r\n             ###################### More configurations ######################\r\n    filter_chains:\r\n    - filters:\r\n        - name: envoy.filters.network.http_connection_manager\r\n          typed_config:\r\n            '@type': >-\r\n              type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n            codec_type: HTTP3\r\n            stat_prefix: ingress_http\r\n            route_config:\r\n              name: xxx.abc.com\r\n              virtual_hosts:\r\n                - name: xxx.abc.com\r\n                  domains:\r\n                    - xxx.abc.com\r\n                    - 'xxx.abc.com:80'\r\n                  routes:\r\n                    - match:\r\n                        prefix: /\r\n                      route:\r\n                        cluster: www_abc_com\r\n            ###################### More configurations ######################\r\n        - name: envoy.filters.network.http_connection_manager\r\n          typed_config:\r\n            '@type': >-\r\n              type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n            stat_prefix: ingress_http\r\n            route_config:\r\n              name: www.abc.com\r\n              virtual_hosts:\r\n                - name: www.abc.com\r\n                  domains:\r\n                    - www.abc.com\r\n                    - 'www.abc.com:80'\r\n                  routes:\r\n                    - match:\r\n                        prefix: /\r\n                      route:\r\n                        cluster: www_abc_com\r\n              ###################### More configurations ######################\r\n```"
      },
      {
        "user": "phlax",
        "created_at": "2021-08-26T16:06:43Z",
        "body": "hi @virnet i think you probably need to use separate listeners , if you want separate filter chains for each domain - not 100% sure, but i cant find any examples that do what you want just with filters/chains\r\n\r\nyou can loopback from a listener that routes on domain to separate them "
      },
      {
        "user": "virnet",
        "created_at": "2021-08-27T01:24:57Z",
        "body": "> hi @virnet i think you probably need to use separate listeners , if you want separate filter chains for each domain - not 100% sure, but i cant find any examples that do what you want just with filters/chains\r\n> \r\n> you can loopback from a listener that routes on domain to separate them\r\n\r\nThanks @phlax \r\nI have an idea to read out the hosts information in the header in advance through \"Listener.ListenerFilters\". Then configure the corresponding domain name in \"Listeners.FilterChains.FilterChainMatch.ServerNames\",\r\n\r\nThis is how I configure HTTPS, but HTTP doesn't seem to support this method.\r\n```yaml\r\n  - name: listener_https\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 443\r\n    listener_filters:\r\n    - name: \"envoy.filters.listener.tls_inspector\"\r\n    filter_chains:\r\n    - filter_chain_match:\r\n        server_names:\r\n          - www.aaa.com\r\n      filters:\r\n        - name: envoy.filters.network.http_connection_manager\r\n```"
      },
      {
        "user": "virnet",
        "created_at": "2021-08-27T01:29:06Z",
        "body": "> hi @virnet i think you probably need to use separate listeners , if you want separate filter chains for each domain - not 100% sure, but i cant find any examples that do what you want just with filters/chains\r\n> \r\n> you can loopback from a listener that routes on domain to separate them\r\n\r\nThank you for the plan. Let me see if there is a better way. If there is really no way, then we can only implement it in the way you say."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-26T04:01:29Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-10-03T08:01:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17819,
    "title": "Populate more fields of per connection stream info",
    "created_at": "2021-08-24T01:41:45Z",
    "closed_at": "2021-10-04T00:02:04Z",
    "labels": [
      "enhancement",
      "question",
      "stale",
      "area/connection",
      "area/metadata"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17819",
    "body": "Currently a tcp connection has a stream info object but very limited fields are filled, mainly to record no filter chain found.\r\n\r\nInstead of make the stream info ephemeral, we can use it to track connection creation, including\r\n1. Time spent on listener filter\r\n2. Time spent on transport socket\r\n3. Time spent on balance\r\netc\r\n\r\nTCP socket is \"downstream\" of a tcp connection,  the existing `last_downstream_rx_byte_received` or ` upstream_timing_` can be used and ideally no confusion is introduced.\r\n\r\nAlternatively, we can always create new dynamic filter metadata.\r\n\r\nThoughts? \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17819/comments",
    "author": "lambdai",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2021-08-26T16:45:14Z",
        "body": "cc @alyssawilk @mattklein123 "
      },
      {
        "user": "alyssawilk",
        "created_at": "2021-08-26T16:58:59Z",
        "body": "Yeah I think more time tracking events would be great.\r\ncc @junr03 @goaway as this may or may not feed into some of the event tracking work, especially if we do upstream tagging as well."
      },
      {
        "user": "lambdai",
        "created_at": "2021-08-27T22:23:44Z",
        "body": "Thank you!\r\nI double checked that tcp proxy doesn't create another stream info. \r\nInstead tcp_proxy reuse the stream info along with the downstream connection.\r\n\r\nSo it make more sense to add more fields. "
      },
      {
        "user": "lambdai",
        "created_at": "2021-08-27T22:30:12Z",
        "body": "For the next step:\r\nThe stream info interface is shared by \r\n* downstream and upstream\r\n* tcp and http\r\n\r\nShould we start to distinguish them, or use the same one but tag and extract the desired?\r\n\r\nI inline the latter at this moment, but I also want to learn your opinions "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-27T00:01:58Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-10-04T00:02:03Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17760,
    "title": "TLS termination of HTTPS requests after CONNECT",
    "created_at": "2021-08-18T16:41:14Z",
    "closed_at": "2021-09-01T13:09:24Z",
    "labels": [
      "question",
      "area/tls",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17760",
    "body": "I have a question regarding using Envoy as a forward proxy with HTTP CONNECT working with dynamic forward proxy to send traffic to any upstream sites. Is it possible to terminate the TLS connection at Envoy after the initial HTTP CONNECT request?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17760/comments",
    "author": "rohrit",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2021-08-26T16:46:59Z",
        "body": "cc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2021-08-26T17:01:38Z",
        "body": "not easily :-(\r\n\r\nso the \"proper\" way to do that today would be to have an HTTP handler which did connect termination, and sent the payload \"upstream\" directly back to another Envoy listener which would do TLS termination of the payload.\r\n\r\nIf it's incoming HTTP/1.1 you could probably one-off an extension like proxy proto which just stripped a fixed length connect header and passed the payload on through."
      },
      {
        "user": "rohrit",
        "created_at": "2021-08-27T04:00:53Z",
        "body": "Hey @alyssawilk , thanks for your reply!\r\n\r\nare there any plans in the roadmap to include support for TLS termination in a single listener rather than through the two-stage mechanism?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2021-08-31T15:40:15Z",
        "body": "@lambdai had plans which would at least avoid passage through loopback, but I don't think anyone has the cycles to simplify beyond this today."
      },
      {
        "user": "rohrit",
        "created_at": "2021-09-01T13:09:24Z",
        "body": "Thank you for the reply, @alyssawilk !"
      }
    ]
  },
  {
    "number": 17758,
    "title": "global FilterChain for all listeners",
    "created_at": "2021-08-18T15:31:06Z",
    "closed_at": "2021-08-26T22:38:54Z",
    "labels": [
      "question",
      "area/configuration",
      "area/http_filter"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17758",
    "body": "*Title*: Is it possible to specify a global FilterChain that is used by all listeners\r\n\r\n*Description*:\r\nWould like to be able to specify a FilterChain that is used by all listeners rather than repeatedly specify the same filter chain inside of each individual listener. The use case we currently have utilizes a wasm filter with its own config. When this gets specified for each listener -> cluster combination the config map is getting very large. We are attempting to simplify the custom config inside of the wasm filter to reduce size, but if it's possible would prefer to be able to specify the filter chain once and apply it to each listener.\r\n\r\nWe have considered using a custom resolver with a named port, but I'm unsure of whether or not it's possible to correctly match inbound connections on a range of ports to a specific cluster. In the scenario we are trying to solve, connections would need to resolve as such:\r\ninbound on port 1 -> cluster1\r\ninbound on port 2 -> cluster2\r\ninbound on port 3 -> cluster3\r\n...\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17758/comments",
    "author": "josephmcknightgunvalson",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2021-08-18T16:19:13Z",
        "body": "May be ECDS would be a better choice? With ECDS, in theory, we can make Filters in different Listeners share the same configuration. "
      },
      {
        "user": "josephmcknightgunvalson",
        "created_at": "2021-08-18T16:34:08Z",
        "body": "> May be ECDS would be a better choice? With ECDS, in theory, we can make Filters in different Listeners share the same configuration.\r\n\r\nI hadn't considered that, I'll take a closer look at that. Thanks!"
      },
      {
        "user": "phlax",
        "created_at": "2021-08-26T16:47:59Z",
        "body": "@jgunvalson are you ok to close this ticket, given suggested way forward ?"
      }
    ]
  },
  {
    "number": 17733,
    "title": "Envoy Filter - dynamicMetadata() shared between INBOUND on_request and OUTBOUND on_request",
    "created_at": "2021-08-16T19:25:28Z",
    "closed_at": "2021-09-23T16:01:09Z",
    "labels": [
      "question",
      "stale",
      "area/metadata"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17733",
    "body": "\r\n*Description*:\r\nThis is not an issue. Just a question\r\n\r\nI was wondering if is it possible to share dynamicMetadata() between inbound and outbound streams. \r\n\r\nso I capture Inbound request_headers,  use dynamicMetadata():set() and use dynamicMetadata():get() on outbound request.\r\n\r\nAny suggestions or workarounds will be appreciated\r\n\r\nThank you\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17733/comments",
    "author": "Arsen-Uulu",
    "comments": [
      {
        "user": "rojkov",
        "created_at": "2021-08-17T08:25:55Z",
        "body": "Could you briefly describe your use case?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-16T12:01:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-23T16:01:08Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17678,
    "title": "Route table check tool: How to set an environment variable?",
    "created_at": "2021-08-11T16:36:57Z",
    "closed_at": "2021-09-18T00:01:54Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17678",
    "body": "*Title*: How can I set an internal envoy %VARIABLE% to be present when the route check tool runs?\r\n\r\n*Description*:\r\nI have a test that asserts:\r\n\r\n```\r\n        \"request_header_matches\": [\r\n          {\r\n            \"name\": \"X-Custom-Header\"\r\n          },\r\n        ]\r\n```\r\n\r\nThis header is added like so:\r\n\r\n```\r\n\"request_headers_to_add\": [\r\n            {\r\n              \"append\": false,\r\n              \"header\": {\r\n                \"key\": \"X-Custom-Header\",\r\n                \"value\": \"%DOWNSTREAM_PEER_CERT%\"\r\n              }\r\n            }\r\n          ]\r\n```\r\n\r\nThe test fails with:\r\n\r\n```\r\nexpected: [has(X-Custom-Header):true], actual: [has(X-Custom-Header):false], test type: request_header_matches.present_match\r\n```\r\n\r\nThe problem seems to be that the \"%DOWNSTREAM_PEER_CERT%\" variable is not set when the test runs. If I change it to, say, \"%PROTOCOL%\", then the test passes.\r\n\r\nHow can I set \"%DOWNSTREAM_PEER_CERT%\" for the test?\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17678/comments",
    "author": "DanTulovsky",
    "comments": [
      {
        "user": "DanTulovsky",
        "created_at": "2021-08-11T16:48:02Z",
        "body": "As a workaround, it seems I can set the value to \"%DOWNSTREAM_PEER_CERT% \" (notice the extra space), which then works.  presumably because it creates the header with that space in it."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-11T00:01:43Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-18T00:01:54Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17669,
    "title": "Setting http headers within the request body callback - proxy wasm",
    "created_at": "2021-08-11T09:27:22Z",
    "closed_at": "2021-09-18T12:01:29Z",
    "labels": [
      "question",
      "stale",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17669",
    "body": "A common use case is adding a checksum header to sign the body.\r\nHowever, the header API is not accessible in the body callbacks.\r\n\r\nIs there any workaround available for this?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17669/comments",
    "author": "dragosc28",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2021-08-11T23:08:43Z",
        "body": "cc @PiotrSikora "
      },
      {
        "user": "PiotrSikora",
        "created_at": "2021-08-12T07:07:09Z",
        "body": "For adding checksum of the HTTP body, I'd recommend using HTTP trailers instead of HTTP headers, assuming that your client can support them.\r\n\r\nAs for modifying HTTP headers after processing HTTP body, this is currently not possible in Wasm. It's going to be added back in a foreseeable future for small HTTP bodies (since the whole request/response needs to be buffered to change headers), but generally speaking trailers should be a preferred solution."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-11T08:01:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-18T12:01:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17649,
    "title": "Is there a way to make envoy modify http request body and not reroute(just send requests to the origin dst)?",
    "created_at": "2021-08-10T03:23:31Z",
    "closed_at": "2021-09-17T00:01:54Z",
    "labels": [
      "question",
      "stale",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17649",
    "body": "Thanks in advance!!\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17649/comments",
    "author": "FLAGLORD",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-09T20:01:33Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-17T00:01:53Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17615,
    "title": "HTTPS calls from lua filter through envoy_on_request and request_handle:httpCall",
    "created_at": "2021-08-06T13:05:36Z",
    "closed_at": "2021-08-19T11:12:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17615",
    "body": "\r\n\r\n*Title*: *HTTPS calls from lua filter through envoy_on_request*\r\n\r\n*Description*:\r\n\r\nI am using the envoy as an internal router, I need to make an API call to an external app every time a route is resolved. I was able to use `envoy_on_request` and `request_handle:httpCall` to forward this to an envoy cluster that is pointing to the external app. Everything was working perfectly until the external app got rid of the HTTP endpoint. Now it supports only HTTPS and `request_handle:httpCall` is not making an HTTPS request. \r\n\r\nIs there a way I can configure it to use HTTPS instead of HTTP? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17615/comments",
    "author": "riyasyash",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-08-06T22:08:17Z",
        "body": "httpCall just uses the configuration for the target cluster. Configure TLS on the target cluster."
      },
      {
        "user": "riyasyash",
        "created_at": "2021-08-12T13:55:53Z",
        "body": "@mattklein123  Will I be able to configure the TLS for this cluster since it is for an external app?\r\n"
      },
      {
        "user": "mattbailey",
        "created_at": "2021-08-12T15:00:54Z",
        "body": "Gotta love tab complete :D"
      },
      {
        "user": "riyasyash",
        "created_at": "2021-08-12T15:35:02Z",
        "body": "Sorry, @mattbailey for the ping out of nowhere 😅  "
      },
      {
        "user": "riyasyash",
        "created_at": "2021-08-19T11:12:02Z",
        "body": "Finally solved the issue, for the noobs like me (and as a reference for future me) adding the solution here\r\n\r\nFor the HTTPS cluster we have to configure the `transport_socket` like this:\r\n\r\n```yaml\r\n      - name: external_api_endpoint\r\n        connect_timeout: 0.25s\r\n        type: LOGICAL_DNS\r\n        lb_policy: ROUND_ROBIN\r\n        load_assignment:\r\n          cluster_name: external_api_endpoint\r\n          endpoints:\r\n          - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: external_api_endpoint.app.io #replace with your external server host name\r\n                    port_value: 443 #ssl port\r\n        transport_socket:\r\n          name: envoy.transport_sockets.tls\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n```\r\nThis will enable the SSL traffic from the envoy cluster to the configured external endpoint\r\nAlso, in the documentation for `request_handle:httpCall` it is said to pass the `:authority` header, and the documentation passes the cluster name as the value for `:authority`. In my case this was not working and I had to pass the actual endpoint host as the authority value. Like this:\r\n```lua\r\nfunction envoy_on_request(request_handle)\r\n  local headers, body = request_handle:httpCall(\r\n         'external_api_endpoint',\r\n         {\r\n              [':method'] = 'POST',\r\n              [':path'] = '/api',\r\n              [':authority'] = 'external_api_endpoint.app.io',\r\n              ['set-cookie'] = { 'lang=lua;', 'type=binding; Path=/api' }\r\n        },\r\n      '{}',\r\n       5000\r\n  )\r\nend\r\n```\r\nthat was it, and the external calls are working like a charm from the per route filter 🎉 \r\n\r\nThanks to @mattklein123 for pointing me in the right direction and `mattbailey`(not tagging him) for entertaining a random ping 😂 "
      }
    ]
  },
  {
    "number": 17578,
    "title": "“No healthy host for TCP connection pool”",
    "created_at": "2021-08-03T18:48:06Z",
    "closed_at": "2021-09-13T04:01:34Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17578",
    "body": "Hello there,\r\n\r\nI am facing a weird issue with envoy. Envoy is unable to make communication with the upstream service. I have a service deployed on a VM and its trying to reach to the external service via consul's terminating gateway.\r\n\r\nI keep on seeing the logs as below.\r\n`\r\n[2021-08-03 17:37:58.470][28051][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:242] [C357] new tcp proxy session\r\n[2021-08-03 17:37:58.470][28051][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:389] [C357] Creating connection to cluster local_app\r\n[2021-08-03 17:37:58.470][28051][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:98] creating a new connection\r\n[2021-08-03 17:37:58.470][28051][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:383] [C358] connecting\r\n[2021-08-03 17:37:58.470][28051][debug][connection] [external/envoy/source/common/network/connection_impl.cc:769] [C358] connecting to 127.0.0.1:9393\r\n[2021-08-03 17:37:58.470][28051][debug][connection] [external/envoy/source/common/network/connection_impl.cc:785] [C358] connection in progress\r\n[2021-08-03 17:37:58.470][28051][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:125] queueing request due to no available connections\r\n[2021-08-03 17:37:58.470][28051][debug][conn_handler] [external/envoy/source/server/connection_handler_impl.cc:476] [C357] new connection\r\n[2021-08-03 17:37:58.470][28049][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:242] [C359] new tcp proxy session\r\n[2021-08-03 17:37:58.470][28049][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:389] [C359] Creating connection to cluster static-server-external.default.<domain_name>.internal.68e1f25d-ffe8-a9cc-f88f-fe66cab6d592.consul\r\n[2021-08-03 17:37:58.470][28051][debug][connection] [external/envoy/source/extensions/transport_sockets/tls/ssl_socket.cc:215] [C357] \r\n[2021-08-03 17:37:58.470][28049][debug][upstream] [external/envoy/source/common/upstream/cluster_manager_impl.cc:1417] no healthy host for TCP connection pool\r\n[2021-08-03 17:37:58.470][28051][debug][connection] [external/envoy/source/common/network/connection_impl.cc:203] [C357] closing socket: 0\r\n[2021-08-03 17:37:58.470][28049][debug][connection] [external/envoy/source/common/network/connection_impl.cc:107] [C359] closing data_to_write=0 type=1\r\n[2021-08-03 17:37:58.470][28049][debug][connection] [external/envoy/source/common/network/connection_impl.cc:203] [C359] closing socket: 1\r\n[2021-08-03 17:37:58.470][28051][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:223] canceling pending request\r\n[2021-08-03 17:37:58.470][28051][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:231] canceling pending connection\r\n[2021-08-03 17:37:58.470][28051][debug][connection] [external/envoy/source/common/network/connection_impl.cc:107] [C358] closing data_to_write=0 type=1\r\n[2021-08-03 17:37:58.470][28051][debug][connection] [external/envoy/source/common/network/connection_impl.cc:203] [C358] closing socket: 1\r\n[2021-08-03 17:37:58.470][28051][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:140] [C358] client disconnected\r\n[2021-08-03 17:37:58.470][28051][debug][conn_handler] [external/envoy/source/server/connection_handler_impl.cc:152] [C357] adding to cleanup list\r\n[2021-08-03 17:37:58.470][28051][debug][pool] [external/envoy/source/common/tcp/original_conn_pool.cc:255] [C358] connection destroyed\r\n[2021-08-03 17:37:58.734][28022][debug][main] [external/envoy/source/server/server.cc:190] flushing stats\r\n[2021-08-03 17:38:03.734][28022][debug][main] [external/envoy/source/server/server.cc:190] flushing stats\r\n`\r\n\r\nEnvoy → 1.16.2\r\n\r\nWhat is going wrong here ? ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17578/comments",
    "author": "ashwinkupatkar",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-06T00:01:57Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-13T04:01:34Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17567,
    "title": "envoy_cluster_upstream_cx_active metric not incrementing",
    "created_at": "2021-08-02T20:21:07Z",
    "closed_at": "2021-09-13T04:01:30Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17567",
    "body": "*Description*:\r\nI am trying to track the active connections for a service deployed in Kubernetes using Istio, and I am only seeing the `envoy_cluster_upstream_cx_active` metric reporting 1 connection per pod.  Is this the right metric to be tracking active connections to hosts?  If so, why aren't we seeing more connections reported?  Please let me know if you need additional information.  Thank you in advance!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17567/comments",
    "author": "jgillis01",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-06T00:01:52Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-13T04:01:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17540,
    "title": "Context management in request-response cycle",
    "created_at": "2021-07-29T18:42:11Z",
    "closed_at": "2021-08-09T17:21:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17540",
    "body": "*Title*: *Context management in request-response cycle*\r\n\r\n*Description*:\r\n\r\n> We are currently testing out Envoy's new ExternalProcessor filter type.  We are using this filter to mutate request headers and request bodies before they reach our downstream applications.  We are removing some of this data from the request and then hoping to re-access it on response.\r\n> \r\n> In development, our filter containers (we have tested with 3 containers running a Go application as our cluster) seems to be maintaining a single context object across the entire request-response lifecycle, which is not what we expected to have happen.  Though this makes our lives easier in some ways (we don't need an external cache to hold onto this data if it can be held in memory of a container), we are concerned that this is not expected behavior and could change in the future.  Since Go's Context library is not doing anything fancy behind the scenes, we have a sneaking suspicion that this peculiar behavior is Envoy-related.\r\n> \r\n> Is maintaining this context expected behavior for Envoy?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17540/comments",
    "author": "mdettelson",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-08-06T22:01:36Z",
        "body": "cc @gbrail "
      },
      {
        "user": "gbrail",
        "created_at": "2021-08-07T00:49:44Z",
        "body": "ext_proc starts a bidirectional gRPC stream for each HTTP request/response. (It's basically a long-running gRPC with data going back in forth in two ways.) \r\n\r\nIn go, you'd receive a single call to \"Process,\" and in there you read and write to the stream. Envoy doesn't know anything about Go contexts, but I assume that the Go gRPC code creates a single context and you'll use it as you interact with the stream.\r\n\r\nThis is on purpose, and it is indeed supposed to make things easier for implementers of external processors, since you can maintain state with the gRPC stream. (In Go you handle the whole stream from a single function, running in a single goroutine, so it's particularly easy.) In most cases you shouldn't need a separate state table or anything like that."
      },
      {
        "user": "mdettelson",
        "created_at": "2021-08-09T17:21:27Z",
        "body": "Awesome!  Thank you for clarifying that this behavior is expected :)"
      },
      {
        "user": "liu-cong",
        "created_at": "2024-09-26T19:17:20Z",
        "body": "Had the same question and found this super useful. Thanks!\r\n\r\nBTW is this documented anywhere? It will help future developers :)"
      }
    ]
  },
  {
    "number": 17534,
    "title": "oAuth Response with \"upstream connect error or disconnect/reset before headers.\"",
    "created_at": "2021-07-29T07:37:49Z",
    "closed_at": "2021-09-13T04:01:26Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17534",
    "body": "Hey community, \r\nI have an oAuth authentication and when I´m successfully logged in and the journey goes back to my API gateway with Envoy, \r\n\r\nI get the following error message:\r\n`response upstream connect error or disconnect / reset before headers. reset reason: connection termination`\r\n\r\nDoes this require something specific to be configured that Envoy accepts and forwards the response?\r\n\r\nMy current configuration:\r\n\r\n```\r\nadmin:\r\n  access_log_path: \"/tmp/admin_access.log\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 10000\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: ingress_http\r\n                access_log:\r\n                  - name: envoy.access_loggers.stdout\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\r\n                http_filters:\r\n                  - name: envoy.filters.http.router\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match:\r\n                            prefix: \"/api/greeting\"\r\n                          route:\r\n                            prefix_rewrite: \"/v1.0/invoke/servicea/method/api/greeting\"\r\n                            cluster: dapr\r\n                        - match:\r\n                            prefix: \"/\"\r\n                          route:\r\n                            prefix_rewrite: \"/v1.0/invoke/mywebapp/method/\"\r\n                            cluster: dapr\r\n          transport_socket:\r\n            name: envoy.transport_sockets.tls\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n              common_tls_context:\r\n                tls_certificates:\r\n                  - certificate_chain:\r\n                      filename: \"/etc/ssl/certs/https.crt\"\r\n                    private_key:\r\n                      filename: \"/etc/ssl/certs/key.pem\"\r\n\r\n  clusters:\r\n    - name: dapr\r\n      connect_timeout: 0.25s\r\n      type: LOGICAL_DNS\r\n      dns_lookup_family: V4_ONLY\r\n      load_assignment:\r\n        cluster_name: dapr\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: apigateway-dapr\r\n                      port_value: 3500\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17534/comments",
    "author": "GregorBiswanger",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-06T00:01:48Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-13T04:01:25Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17527,
    "title": "Example for oAuth with Microsoft Identity",
    "created_at": "2021-07-28T17:14:08Z",
    "closed_at": "2021-09-13T04:01:24Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17527",
    "body": "Hello Community,\r\nI've been trying an oAuth configuration with Microsoft Identity for hours. \r\nDoes anyone have an example or already experience with it?\r\n\r\nI also wonder how you can also store anonymous routes on Envoy. For a start page, for example.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17527/comments",
    "author": "GregorBiswanger",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-06T00:01:47Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-13T04:01:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17516,
    "title": "self define response message when use local ratelimit",
    "created_at": "2021-07-28T03:29:13Z",
    "closed_at": "2021-08-12T06:18:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17516",
    "body": "*Title*: *One line description*\r\nself define response message when use local ratelimit\r\n*Description*:\r\n>Describe the desired behavior, what scenario it enables and how it\r\nwould be used.\r\n\r\nsome times when we migrate a service to envoy , we want consistent with business data format , so we should define rasponse message such as {code: 12345,message: \"ratelimit\"} and with a 200 http code ,not only return a reponse code(429),\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17516/comments",
    "author": "du2016",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-08-06T21:57:39Z",
        "body": "See the local reply mapper feature which should handle this."
      }
    ]
  },
  {
    "number": 17476,
    "title": "Is there a command to view the configuration of eds?",
    "created_at": "2021-07-24T02:30:22Z",
    "closed_at": "2021-07-29T07:06:53Z",
    "labels": [
      "question",
      "area/xds",
      "area/admin"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17476",
    "body": "The configuration of endpoints cannot be found using `127.0.0.1:15000/config_dump`. Is there a command to see the configuration issued by eds?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17476/comments",
    "author": "zhangzerui20",
    "comments": [
      {
        "user": "ramaraochavali",
        "created_at": "2021-07-25T09:59:51Z",
        "body": "config_dump?include_eds will give eds details"
      },
      {
        "user": "htuch",
        "created_at": "2021-07-25T18:10:28Z",
        "body": "Also some of this information is available on the `/clusters` admin endpoint."
      },
      {
        "user": "zhangzerui20",
        "created_at": "2021-07-29T07:06:53Z",
        "body": " I got what I want through `config_dump?include_eds`"
      }
    ]
  },
  {
    "number": 17447,
    "title": "[HELP] compile_commands.json cannot work on standard library",
    "created_at": "2021-07-22T04:29:22Z",
    "closed_at": "2021-08-29T08:01:16Z",
    "labels": [
      "question",
      "area/build",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17447",
    "body": "After successfully building envoy on macos, I run tools/vscode/refresh_compdb.sh to generate  compile_commands.json, which generated commands like\r\n{\r\n    \"command\": \"external/local_config_cc/wrapped_clang -D_FORTIFY_SOURCE=1 -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -O0 -DDEBUG DEBUG_PREFIX_MAP_PWD=. -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/MacOSX.platform/Developer/Library/Frameworks -mmacosx-version-min=11.1 -std=c++17 -no-canonical-prefixes -Wno-builtin-macro-redefined -D__DATE__=\\\"redacted\\\" -D__TIMESTAMP__=\\\"redacted\\\" -D__TIME__=\\\"redacted\\\" -target x86_64-apple-macosx -D\\\"CARES_STATICLIB\\\" -D\\\"FMT_HEADER_ONLY\\\" -D\\\"SPDLOG_FMT_EXTERNAL\\\" -isystem external/com_github_fmtlib_fmt/include -isystem bazel-out/darwin-fastbuild/bin/external/com_github_fmtlib_fmt/include -isystem external/com_google_protobuf/src -isystem bazel-out/darwin-fastbuild/bin/external/com_google_protobuf/src -isystem bazel-out/darwin-fastbuild/bin/external/envoy/bazel/foreign_cc/zlib/include -isystem external/com_github_gabime_spdlog/include -isystem bazel-out/darwin-fastbuild/bin/external/com_github_gabime_spdlog/include -isystem bazel-out/darwin-fastbuild/bin/external/envoy/bazel/foreign_cc/event/include -isystem external/com_github_jbeder_yaml_cpp/include -isystem bazel-out/darwin-fastbuild/bin/external/com_github_jbeder_yaml_cpp/include -isystem external/boringssl/src/include -isystem bazel-out/darwin-fastbuild/bin/external/boringssl/src/include -isystem bazel-out/darwin-fastbuild/bin/external/envoy/bazel/foreign_cc/ares/include -isystem external/com_github_circonus_labs_libcircllhist/src -isystem bazel-out/darwin-fastbuild/bin/external/com_github_circonus_labs_libcircllhist/src -isystem bazel-out/darwin-fastbuild/bin/external/envoy/bazel/foreign_cc/gperftools_build/include -I bazel-out/darwin-fastbuild/bin/external/com_google_protobuf/_virtual_includes/any_proto -I bazel-out/darwin-fastbuild/bin/external/com_google_protobuf/_virtual_includes/descriptor_proto -I bazel-out/darwin-fastbuild/bin/external/com_google_protobuf/_virtual_includes/duration_proto -I bazel-out/darwin-fastbuild/bin/external/com_google_protobuf/_virtual_includes/empty_proto -I bazel-out/darwin-fastbuild/bin/external/com_google_protobuf/_virtual_includes/struct_proto -I bazel-out/darwin-fastbuild/bin/external/com_google_protobuf/_virtual_includes/timestamp_proto -I bazel-out/darwin-fastbuild/bin/external/com_google_protobuf/_virtual_includes/wrappers_proto -I bazel-out/darwin-fastbuild/bin/source/common/common/_virtual_includes/logger_impl_lib_standard -I bazel-out/darwin-fastbuild/bin/source/common/common/_virtual_includes/thread_impl_lib_posix -I bazel-out/darwin-fastbuild/bin/source/common/event/_virtual_includes/dispatcher_lib -I bazel-out/darwin-fastbuild/bin/source/common/event/_virtual_includes/signal_impl_lib_posix -I bazel-out/darwin-fastbuild/bin/source/common/filesystem/_virtual_includes/watcher_lib -I bazel-out/darwin-fastbuild/bin/source/common/api/_virtual_includes/os_sys_calls_lib -I bazel-out/darwin-fastbuild/bin/source/common/filesystem/_virtual_includes/filesystem_impl_lib_posix -iquote . -iquote bazel-out/darwin-fastbuild/bin -iquote external/com_google_absl -iquote bazel-out/darwin-fastbuild/bin/external/com_google_absl -iquote external/com_github_fmtlib_fmt -iquote bazel-out/darwin-fastbuild/bin/external/com_github_fmtlib_fmt -iquote external/envoy_api -iquote bazel-out/darwin-fastbuild/bin/external/envoy_api -iquote external/com_google_googleapis -iquote bazel-out/darwin-fastbuild/bin/external/com_google_googleapis -iquote external/com_google_protobuf -iquote bazel-out/darwin-fastbuild/bin/external/com_google_protobuf -iquote external/com_envoyproxy_protoc_gen_validate -iquote bazel-out/darwin-fastbuild/bin/external/com_envoyproxy_protoc_gen_validate -iquote external/com_googlesource_code_re2 -iquote bazel-out/darwin-fastbuild/bin/external/com_googlesource_code_re2 -iquote external/com_github_cncf_udpa -iquote bazel-out/darwin-fastbuild/bin/external/com_github_cncf_udpa -iquote external/opencensus_proto -iquote bazel-out/darwin-fastbuild/bin/external/opencensus_proto -iquote external/com_github_gabime_spdlog -iquote bazel-out/darwin-fastbuild/bin/external/com_github_gabime_spdlog -iquote external/com_github_cyan4973_xxhash -iquote bazel-out/darwin-fastbuild/bin/external/com_github_cyan4973_xxhash -iquote external/com_github_jbeder_yaml_cpp -iquote bazel-out/darwin-fastbuild/bin/external/com_github_jbeder_yaml_cpp -iquote external/com_googlesource_googleurl -iquote bazel-out/darwin-fastbuild/bin/external/com_googlesource_googleurl -iquote external/boringssl -iquote bazel-out/darwin-fastbuild/bin/external/boringssl -iquote external/com_github_circonus_labs_libcircllhist -iquote bazel-out/darwin-fastbuild/bin/external/com_github_circonus_labs_libcircllhist -iquote external/envoy -iquote bazel-out/darwin-fastbuild/bin/external/envoy -Wall -Wextra -Werror -Wnon-virtual-dtor -Woverloaded-virtual -Wold-style-cast -Wformat -Wformat-security -Wvla -Wno-deprecated-declarations -Wreturn-type -DGPERFTOOLS_TCMALLOC -DENVOY_HANDLE_SIGNALS -DENVOY_OBJECT_TRACE_ON_DUMP -D__APPLE_USE_RFC_3542 -DENVOY_ENABLE_QUIC -DENVOY_GOOGLE_GRPC -I /Applications -I /Library -I /Users/bytedance/Library -I /Applications/Xcode.app/Contents/Developer -I /Applications/Xcode12.app/Contents/Developer -x c++ -c tools/bootstrap2pb.cc\",\r\n    \"directory\": \"tmp/_bazel_bytedance/b63b4b814b7aa085068de555ba49a36f/execroot/envoy\",\r\n    \"file\": \"tools/bootstrap2pb.cc\"\r\n  },\r\n\r\n\r\nhowever, vscode cannot find function defined in standard library, has error like \r\n{\r\n\t\"resource\": \"envoy/source/server/active_raw_udp_listener_config.cc\",\r\n\t\"owner\": \"_generated_diagnostic_collection_name_#0\",\r\n\t\"code\": \"pp_file_not_found\",\r\n\t\"severity\": 8,\r\n\t\"message\": \"In included file: 'cstdint' file not found\",\r\n\t\"source\": \"clang\",\r\n\t\"startLineNumber\": 1,\r\n\t\"startColumn\": 10,\r\n\t\"endLineNumber\": 1,\r\n\t\"endColumn\": 58,\r\n\t\"relatedInformation\": [\r\n\t\t{\r\n\t\t\t\"startLineNumber\": 3,\r\n\t\t\t\"startColumn\": 10,\r\n\t\t\t\"endLineNumber\": 3,\r\n\t\t\t\"endColumn\": 19,\r\n\t\t\t\"message\": \"Error occurred here\",\r\n\t\t\t\"resource\": \"envoy/network/connection_handler.h\"\r\n\t\t}\r\n\t]\r\n}\r\n\r\nhow to solve this problem?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17447/comments",
    "author": "tianxinheihei",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2021-07-23T01:01:07Z",
        "body": "@lizan do you know how to make this work on Mac? CC @ggreenway "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-22T04:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-29T08:01:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17417,
    "title": "TCP proxy, multiple listeners, consistent hashing",
    "created_at": "2021-07-20T04:45:33Z",
    "closed_at": "2021-08-27T12:01:39Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17417",
    "body": "I am trying to set up envoy as a load balancer for my client server application. The proxy is reverse proxy load balancer. The server listens on multiple ports (say 6618 (handshake), 6619, 6620,6621 (data channel)). I need to maintain session persistence between client and server ie. once a client connects to a server it should be routed to the same server. Hence I am using the lb_policy as MAGLEV in the config yaml.\r\n\r\nThe setup is as follows:\r\nServer 1                                                        Client 1\r\n             <-------->   Envoy Proxy <-------> \r\nServer 2                                                        Client 2\r\n\r\nHere Client1 is connected to server1 for handshake, but the next transaction between client1 and server1 is routed to server2. Hence the transaction is failing.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17417/comments",
    "author": "Nair-Sunil",
    "comments": [
      {
        "user": "Nair-Sunil",
        "created_at": "2021-07-20T04:54:11Z",
        "body": "`static_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 6618\r\n      filter_chains:\r\n        - filters:\r\n          - name: envoy.filters.network.tcp_proxy\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n              stat_prefix: gfc_tcp\r\n              cluster: cluster_0\r\n    - name: listener_1\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 6619\r\n      filter_chains:\r\n        - filters:\r\n          - name: envoy.filters.network.tcp_proxy\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n              stat_prefix: gfc_tcp\r\n              cluster: cluster_0\r\n    - name: listener_2\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 6620\r\n      filter_chains:\r\n        - filters:\r\n          - name: envoy.filters.network.tcp_proxy\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n              stat_prefix: gfc_tcp\r\n              cluster: cluster_0\r\n              access_log:\r\n              - name: envoy.access_loggers.file\r\n                typed_config:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                  path: \"/var/log/envoy/access.log\"\r\n    - name: listener_3\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 6621\r\n      filter_chains:\r\n        - filters:\r\n          - name: envoy.filters.network.tcp_proxy\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n              stat_prefix: gfc_tcp\r\n              cluster: cluster_0\r\n\r\n  clusters:\r\n  - name: cluster_0\r\n    connect_timeout: 1s\r\n    type: STATIC\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: MAGLEV\r\n    load_assignment:\r\n      cluster_name: core_10\r\n      endpoints:\r\n        - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: 10.254.1.10\r\n                    port_value: 6618\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: 10.254.1.11\r\n                    port_value: 6618\r\n        - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: 10.254.1.10\r\n                    port_value: 6619\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: 10.254.1.11\r\n                    port_value: 6619\r\n        - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: 10.254.1.10\r\n                    port_value: 6620\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: 10.254.1.11\r\n                    port_value: 6620\r\n        - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: 10.254.1.10\r\n                    port_value: 6621\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: 10.254.1.11\r\n                    port_value: 6621`"
      },
      {
        "user": "snowp",
        "created_at": "2021-07-20T14:10:39Z",
        "body": "I believe you need to configure a hash_policy on your TCP proxy to tell the load balancer what to hash on"
      },
      {
        "user": "Nair-Sunil",
        "created_at": "2021-07-21T04:09:24Z",
        "body": "Thanks a lot for the prompt response. I am trying to add the hash_policy to the filters section and it is giving me the following error\r\n\r\n> Proto constraint validation failed (TcpProxyValidationError.HashPolicy[0]: embedded message failed validation | caused by field: \"policy_specifier\", reason: is required): stat_prefix: \"gfc_tcp\"\r\ncluster: \"cluster_0\"\r\nhash_policy {\r\n}\r\n\r\nIt could be great if you can provide a example on how to specify the hash_policy."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-20T08:01:27Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-27T12:01:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17392,
    "title": "[wasm] Can I transfrom http request into tcp on HTTP Filter",
    "created_at": "2021-07-18T07:28:27Z",
    "closed_at": "2021-09-08T20:01:30Z",
    "labels": [
      "question",
      "stale",
      "area/wasm",
      "area/tcp_proxy",
      "area/http_filter"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17392",
    "body": "*Title* : *Can I transfrom HTTP request into tcp on HTTP Filter*\r\n\r\n*Description* :\r\nWe have an old RPC framework running on production, with a self-designed RPC protocol via TCP. And I've made a transformer to turn gRPC request onto this private RPC request, so that these old services can run in Kubernetes, work on L7, and be controlled by Istio-proxy.\r\nMy question is, can I do this transform by WASM on HTTP Filter? If I apply a custom wasm on HTTP Filter, how can I turn gPRC requests to TCP bytes(non HTTP format).\r\n\r\nI found *ReplaceDownstreamData* in APIs, but it seems only available in TcpContext, not HttpContext.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17392/comments",
    "author": "orangetangerine",
    "comments": [
      {
        "user": "orangetangerine",
        "created_at": "2021-07-19T09:28:20Z",
        "body": "Like:\r\nenvoyfilter reveive:\r\n```\r\nPOST / HTTP/1.1\r\nContent-Type: application/grpc\r\nbody...\r\n```\r\n\r\nand send to backend server:\r\n```\r\n0f 0b            0f 0a   00 01  00 ff           xx  xx xx xx\r\n(magic header) (length) (path)  (body length)  (http body)\r\n```"
      },
      {
        "user": "htuch",
        "created_at": "2021-07-20T01:50:26Z",
        "body": "So, you want to do gRPC -> some custom TCP protocol. I think if this was a terminal HTTP filter and able to perform a cluster pick itself via cluster manager from the TCP connection pool this would be possible in a C++ filter, but that's pretty complicated and not sure if this can be done in Wasm.\r\n\r\nMaybe you could convert to an HTTP CONNECT internally and then loopback to the Envoy to continue the request. @alyssawilk @PiotrSikora thoughts on how to do this simply in either C++ or Wasm?"
      },
      {
        "user": "orangetangerine",
        "created_at": "2021-07-21T04:01:24Z",
        "body": "> So, you want to do gRPC -> some custom TCP protocol. \r\n\r\nYes! \r\nI already made an external filter as sidecar in Golang to transform gRPC to custom protocol.\r\nSo there are two sidecars: istio-proxy and my filter. \r\nIf protocol transformation can be done in Wasm, it could get better performance.\r\n\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-01T16:01:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-08T20:01:30Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17390,
    "title": "Does envoy provide a mechanism to observe configuration changes?",
    "created_at": "2021-07-17T16:11:46Z",
    "closed_at": "2021-07-24T12:36:10Z",
    "labels": [
      "question",
      "area/xds",
      "area/dns"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17390",
    "body": "I'm doing something similar to `Istio`, using `Iptable` to intercept HTTP requests and envoy to handle the traffic. I use `xDS` to connect envoy to our own service registry. When a new service is registered, new `cDS` and rds will be generated and sent to `envoy`.\r\n\r\nJust doing this still does not work properly, because it also needs to generate DNS records. I found that `DNS Filter` can do this, but it is currently under development. Therefore, I am going to develop a module to watch the `envoy` cluster configuration and generate a DNS record for each cluster.\r\n\r\nSo, my question is whether `envoy` provides a watch mechanism so that I can perceive that the cluster configuration of `envoy` has changed. If not, I can only use polling to check whether changes have occurred.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17390/comments",
    "author": "zhangzerui20",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2021-07-20T01:54:22Z",
        "body": "Probably need more detail. Who is the watch mechanism used by? Some extensions inside of Envoy, some external process?\r\n\r\nHow do you plan on regenerating the DNS filter config?\r\n\r\nEnvoy itself will watch for xDS changes either via gRPC or filesytem (inotify)."
      }
    ]
  },
  {
    "number": 17361,
    "title": "TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER",
    "created_at": "2021-07-15T15:50:54Z",
    "closed_at": "2021-08-27T20:01:17Z",
    "labels": [
      "question",
      "area/tls",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17361",
    "body": "*Title*: *TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER*\r\n\r\n*Description*:\r\nStarting situation is having two apps talking to each other via pure http. Both of them have an Envoy sidecar in front of them to do mTLS on top of the http traffic. App A makes a request to Envoy Sidecar A's port 13001. The Listener then routes this request to App B's Service by using the Cluster defined in A's Envoy. The Cluster should use the defined tls_context to do mTLS. B's Service is connected to B's Envoy Sidecar which also uses a tls_context to terminate the mTLS.\r\nUnfortunately A's Envoy respons with a `TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER` error an B's Envoy states the error: ` Sending local reply with details http1.codec_error`\r\n\r\nThe apps http request should be fine as a normal curl (to the sidecar Envoy with port 13001) inside the workload container returns the same error.\r\n\r\nI use SPIRE (0.12.2) to create certificates and Envoy (1.16.0).\r\n\r\n*Logs*:\r\n\r\nApp A (does HTTP request):\r\n```\r\n[2021-07-15 15:44:23.943][20][debug][conn_handler] [source/server/connection_handler_impl.cc:459] [C38] new connection\r\n[2021-07-15 15:44:23.944][20][debug][http] [source/common/http/conn_manager_impl.cc:225] [C38] new stream\r\n[2021-07-15 15:44:23.945][20][debug][http] [source/common/http/conn_manager_impl.cc:837] [C38][S4697022230825120234] request headers complete (end_stream=false):\r\n':authority', 'localhost:13001'\r\n':path', '/send/msg/backend'\r\n':method', 'POST'\r\n'content-type', 'application/json'\r\n'content-length', '18'\r\n'connection', 'close'\r\n\r\n[2021-07-15 15:44:23.945][20][debug][router] [source/common/router/router.cc:429] [C38][S4697022230825120234] cluster 'workload-b' match for URL '/send/msg/backend'\r\n[2021-07-15 15:44:23.945][20][debug][router] [source/common/router/router.cc:586] [C38][S4697022230825120234] router decoding headers:\r\n':authority', 'localhost:13001'\r\n':path', '/send/msg/backend'\r\n':method', 'POST'\r\n':scheme', 'https'\r\n'content-type', 'application/json'\r\n'content-length', '18'\r\n'x-forwarded-proto', 'http'\r\n'x-request-id', '5617816c-4c63-46b1-b802-e78efd848ca9'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2021-07-15 15:44:23.945][20][debug][pool] [source/common/http/conn_pool_base.cc:71] queueing stream due to no available connections\r\n[2021-07-15 15:44:23.945][20][debug][pool] [source/common/conn_pool/conn_pool_base.cc:104] creating a new connection\r\n[2021-07-15 15:44:23.945][20][debug][client] [source/common/http/codec_client.cc:39] [C39] connecting\r\n[2021-07-15 15:44:23.945][20][debug][connection] [source/common/network/connection_impl.cc:755] [C39] connecting to 10.43.33.39:4002\r\n[2021-07-15 15:44:23.946][20][debug][connection] [source/common/network/connection_impl.cc:771] [C39] connection in progress\r\n[2021-07-15 15:44:23.946][20][debug][http] [source/common/http/filter_manager.cc:721] [C38][S4697022230825120234] request end stream\r\n[2021-07-15 15:44:23.946][20][debug][connection] [source/common/network/connection_impl.cc:611] [C39] connected\r\n[2021-07-15 15:44:23.949][20][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:215] [C39] TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER\r\n[2021-07-15 15:44:23.949][20][debug][connection] [source/common/network/connection_impl.cc:202] [C39] closing socket: 0\r\n[2021-07-15 15:44:23.949][20][debug][client] [source/common/http/codec_client.cc:96] [C39] disconnect. resetting 0 pending requests\r\n[2021-07-15 15:44:23.949][20][debug][pool] [source/common/conn_pool/conn_pool_base.cc:314] [C39] client disconnected, failure reason: TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER\r\n[2021-07-15 15:44:23.949][20][debug][router] [source/common/router/router.cc:1031] [C38][S4697022230825120234] upstream reset: reset reason: connection failure, transport failure reason: TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER\r\n[2021-07-15 15:44:23.949][20][debug][http] [source/common/http/filter_manager.cc:805] [C38][S4697022230825120234] Sending local reply with details upstream_reset_before_response_started{connection failure,TLS error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER}\r\n[2021-07-15 15:44:23.949][20][debug][http] [source/common/http/conn_manager_impl.cc:1380] [C38][S4697022230825120234] closing connection due to connection close header\r\n[2021-07-15 15:44:23.949][20][debug][http] [source/common/http/conn_manager_impl.cc:1435] [C38][S4697022230825120234] encoding headers via codec (end_stream=false):\r\n':status', '503'\r\n'content-length', '190'\r\n'content-type', 'text/plain'\r\n'date', 'Thu, 15 Jul 2021 15:44:23 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2021-07-15 15:44:23.949][20][debug][connection] [source/common/network/connection_impl.cc:106] [C38] closing data_to_write=344 type=2\r\n[2021-07-15 15:44:23.949][20][debug][connection] [source/common/network/connection_impl_base.cc:40] [C38] setting delayed close timer with timeout 1000 ms\r\n[2021-07-15 15:44:23.950][20][debug][connection] [source/common/network/connection_impl.cc:641] [C38] write flush complete\r\n[2021-07-15 15:44:23.951][20][debug][connection] [source/common/network/connection_impl.cc:522] [C38] remote early close\r\n[2021-07-15 15:44:23.951][20][debug][connection] [source/common/network/connection_impl.cc:202] [C38] closing socket: 0\r\n[2021-07-15 15:44:23.951][20][debug][conn_handler] [source/server/connection_handler_impl.cc:152] [C38] adding to cleanup list\r\n```\r\nApp B (gets HTTP request):\r\n```\r\n[2021-07-15 15:44:23.947][16][debug][conn_handler] [source/server/connection_handler_impl.cc:459] [C20] new connection\r\n[2021-07-15 15:44:23.947][16][debug][http] [source/common/http/conn_manager_impl.cc:225] [C20] new stream\r\n[2021-07-15 15:44:23.947][16][debug][http] [source/common/http/filter_manager.cc:805] [C20][S1259123553487185854] Sending local reply with details http1.codec_error\r\n[2021-07-15 15:44:23.948][16][debug][http] [source/common/http/conn_manager_impl.cc:1380] [C20][S1259123553487185854] closing connection due to connection close header\r\n[2021-07-15 15:44:23.948][16][debug][http] [source/common/http/conn_manager_impl.cc:1435] [C20][S1259123553487185854] encoding headers via codec (end_stream=false):\r\n':status', '400'\r\n'content-length', '11'\r\n'content-type', 'text/plain'\r\n'date', 'Thu, 15 Jul 2021 15:44:23 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2021-07-15 15:44:23.948][16][debug][http] [source/common/http/conn_manager_impl.cc:176] [C20][S1259123553487185854] doEndStream() resetting stream\r\n[2021-07-15 15:44:23.948][16][debug][http] [source/common/http/conn_manager_impl.cc:1484] [C20][S1259123553487185854] stream reset\r\n[2021-07-15 15:44:23.948][16][debug][connection] [source/common/network/connection_impl.cc:106] [C20] closing data_to_write=156 type=2\r\n[2021-07-15 15:44:23.948][16][debug][connection] [source/common/network/connection_impl_base.cc:40] [C20] setting delayed close timer with timeout 1000 ms\r\n[2021-07-15 15:44:23.948][16][debug][http] [source/common/http/conn_manager_impl.cc:240] [C20] dispatch error: http/1.1 protocol error: HPE_INVALID_METHOD\r\n[2021-07-15 15:44:23.948][16][debug][connection] [source/common/network/connection_impl.cc:106] [C20] closing data_to_write=156 type=2\r\n[2021-07-15 15:44:23.949][16][debug][connection] [source/common/network/connection_impl.cc:641] [C20] write flush complete\r\n[2021-07-15 15:44:23.949][16][debug][connection] [source/common/network/connection_impl.cc:641] [C20] write flush complete\r\n[2021-07-15 15:44:24.949][16][debug][connection] [source/common/network/connection_impl_base.cc:54] [C20] triggered delayed close\r\n[2021-07-15 15:44:24.949][16][debug][connection] [source/common/network/connection_impl.cc:202] [C20] closing socket: 1\r\n[2021-07-15 15:44:24.949][16][debug][conn_handler] [source/server/connection_handler_impl.cc:152] [C20] adding to cleanup list\r\n```\r\n\r\n-------\r\n\r\n*Reproduce*:\r\n\r\nApp A Deployment:\r\n```\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: workload-a\r\n  labels:\r\n    app: workload-a\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      app: workload-a\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: workload-a\r\n    spec:\r\n      containers:\r\n      - name: envoy\r\n        image: envoyproxy/envoy:v1.16.0\r\n        imagePullPolicy: Always\r\n        args: [\"-l\", \"debug\", \"--local-address-ip-version\", \"v4\", \"-c\", \"/run/envoy/envoy.yaml\"]\r\n        ports:\r\n        - containerPort: 4001\r\n        volumeMounts:\r\n        - name: envoy-config\r\n          mountPath: \"/run/envoy\"\r\n          readOnly: true\r\n        - name: spire-agent-socket\r\n          mountPath: /run/spire/sockets\r\n          readOnly: true\r\n        - name: envoy-dyn-config\r\n          mountPath: /envoy\r\n      - name: workload-a\r\n        image: dufner/dummyworkload\r\n        imagePullPolicy: Always\r\n        ports:\r\n        - containerPort: 3000\r\n[...]\r\n```\r\n\r\nApp A Envoy:\r\n```\r\nnode:\r\n  cluster: \"demo-cluster-spire\"\r\n  id: \"workload-a\"\r\nstatic_resources:\r\n  clusters:\r\n  - name: spire_agent\r\n    connect_timeout: 0.25s\r\n    http2_protocol_options: {}\r\n    hosts:\r\n      - pipe:\r\n          path: /run/spire/sockets/agent.sock\r\n  - name: this-workload #myself\r\n    connect_timeout: 1s\r\n    type: strict_dns\r\n    hosts: [{ socket_address: { address: 127.0.0.1, port_value: 3000 }}] \r\n  - name: workload-b\r\n    type: strict_dns\r\n    connect_timeout: 1s\r\n    hosts: [{ socket_address: { address: svc-workload-b, port_value: 4002 }}]\r\n    tls_context:\r\n      common_tls_context:\r\n        tls_certificate_sds_secret_configs:\r\n          - name: \"spiffe://example.org/ns/default/sa/default/workload-a\"\r\n            sds_config:\r\n              api_config_source:\r\n                api_type: GRPC\r\n                grpc_services:\r\n                  envoy_grpc:\r\n                    cluster_name: spire_agent\r\n        combined_validation_context:\r\n          default_validation_context:\r\n            match_subject_alt_names:\r\n              prefix: \"spiffe://\"\r\n          validation_context_sds_secret_config:\r\n            name: \"spiffe://example.org\"\r\n            sds_config:\r\n              api_config_source:\r\n                api_type: GRPC\r\n                grpc_services:\r\n                  envoy_grpc:\r\n                    cluster_name: spire_agent\r\n        tls_params:\r\n          ecdh_curves:\r\n            - X25519:P-256:P-521:P-384\r\n\r\n\r\ndynamic_resources:\r\n  lds_config:\r\n     path: /envoy/envoy-dyn-listeners.yaml\r\n\r\nadmin:\r\n  access_log_path: /tmp/admin_access0.log\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 19000\r\n```\r\n\r\n\r\nApp A Envoy-Listener:\r\n```\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.config.listener.v3.Listener\r\n  name: outbound_broadcast\r\n  address:\r\n    socket_address:\r\n      address: 127.0.0.1\r\n      port_value: 13001\r\n  filter_chains:\r\n  - filters:\r\n    - name: envoy.http_connection_manager\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n        common_http_protocol_options:\r\n          idle_timeout: 1s\r\n        codec_type: auto\r\n        access_log:\r\n        - name: envoy.file_access_log\r\n          config:\r\n            path: \"/tmp/outbound-proxy.log\"\r\n        stat_prefix: ingress_http\r\n        route_config:\r\n          name: service_route\r\n          virtual_hosts:\r\n          - name: outbound_broadcast\r\n            domains: [\"*\"]\r\n            routes:\r\n            - match:\r\n                prefix: \"/\"\r\n              route:\r\n                cluster: workload-b\r\n        http_filters:           \r\n        - name: envoy.router\r\n```\r\n\r\n-----------------\r\n\r\nApp B Deployment:\r\n```\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: workload-b\r\n  labels:\r\n    app: workload-b\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      app: workload-b\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: workload-b\r\n    spec:\r\n      containers:\r\n      - name: envoy\r\n        image: envoyproxy/envoy:v1.16.0\r\n        imagePullPolicy: Always\r\n        args: [\"-l\", \"debug\", \"--local-address-ip-version\", \"v4\", \"-c\", \"/run/envoy/envoy.yaml\"]\r\n        ports:\r\n        - containerPort: 4002\r\n        volumeMounts:\r\n        - name: envoy-config\r\n          mountPath: \"/run/envoy\"\r\n          readOnly: true\r\n        - name: spire-agent-socket\r\n          mountPath: /run/spire/sockets\r\n          readOnly: true\r\n        - name: envoy-dyn-config\r\n          mountPath: /envoy\r\n      - name: workload-b\r\n        image: dufner/dummyworkload\r\n        imagePullPolicy: Always\r\n        ports:\r\n        - containerPort: 3000\r\n      - name: opa\r\n        image: dufner/opa:0.29.4-envoy-2\r\n        imagePullPolicy: IfNotPresent\r\n        ports:\r\n          - name: opa-envoy\r\n            containerPort: 8182\r\n            protocol: TCP\r\n          - name: opa-api-port\r\n            containerPort: 8181\r\n            protocol: TCP\r\n        args:\r\n          - \"run\"\r\n          - \"--server\"\r\n          - \"--config-file=/run/opa/opa-config.yaml\"\r\n          - \"/run/opa/opa-policy.rego\"\r\n        volumeMounts:\r\n          - name: workload-b-opa-policy\r\n            mountPath: /run/opa\r\n            readOnly: true\r\n      volumes:\r\n      - name: envoy-config\r\n        configMap:\r\n          name: workload-b-envoy\r\n      - name: workload-b-opa-policy\r\n        configMap:\r\n          name: workload-b-opa-policy-config\r\n      - name: spire-agent-socket\r\n        hostPath:\r\n          path: /run/spire/sockets\r\n          type: Directory\r\n      - name: envoy-dyn-config\r\n        hostPath:\r\n          path: /home/ubuntu/own-workload/workload-b/config\r\n          type: Directory\r\n```\r\n\r\n\r\nApp B Service:\r\n```\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: svc-workload-b\r\nspec:\r\n  ports:\r\n   - port: 4002\r\n     protocol: TCP\r\n     targetPort: 4002\r\n  selector:\r\n      app: workload-b\r\n  type: NodePort\r\n```\r\n\r\nApp B Envoy:\r\n```\r\nnode:\r\n  cluster: \"demo-cluster-spire\"\r\n  id: \"workload-b\"\r\nstatic_resources:\r\n  clusters:\r\n  - name: spire_agent\r\n    connect_timeout: 0.25s\r\n    http2_protocol_options: {}\r\n    hosts:\r\n      - pipe:\r\n          path: /run/spire/sockets/agent.sock\r\n  - name: this-workload #myself\r\n    connect_timeout: 1s\r\n    type: strict_dns\r\n    hosts: [{ socket_address: { address: 127.0.0.1, port_value: 3000 }}] \r\ndynamic_resources:\r\n  lds_config:\r\n     path: /envoy/envoy-dyn-listeners.yaml\r\n\r\nadmin:\r\n  access_log_path: /tmp/admin_access0.log\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 19000\r\n\r\n```\r\n\r\n\r\nApp B Envoy-Listener:\r\n\r\n```\r\n- \"@type\": type.googleapis.com/envoy.config.listener.v3.Listener\r\n  name: inbound_traffic\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 4002\r\n  filter_chains:\r\n  - filters:\r\n    - name: envoy.http_connection_manager\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n        common_http_protocol_options:\r\n          idle_timeout: 1s\r\n        codec_type: auto\r\n        access_log:\r\n        - name: envoy.file_access_log\r\n          config:\r\n            path: \"/tmp/outbound-proxy.log\"\r\n        stat_prefix: ingress_http\r\n        route_config:\r\n          name: service_route\r\n          virtual_hosts:\r\n          - name: inbound_traffic\r\n            domains: [\"*\"]\r\n            routes:\r\n            - match:\r\n                prefix: \"/\"\r\n              route:\r\n                cluster: this-workload #forward request to myself\r\n        http_filters:\r\n        - name: envoy.ext_authz\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.config.filter.http.ext_authz.v2.ExtAuthz\r\n            with_request_body:\r\n              max_request_bytes: 8192\r\n              allow_partial_message: true\r\n            failure_mode_allow: false\r\n            grpc_service:\r\n              google_grpc:\r\n                target_uri: 127.0.0.1:8182\r\n                stat_prefix: ext_authz\r\n              timeout: 0.5s              \r\n        - name: envoy.router\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n          common_tls_context:\r\n            tls_certificate_sds_secret_configs:\r\n            - name: \"spiffe://example.org/ns/default/sa/default/workload-b\"\r\n              sds_config:\r\n                api_config_source:\r\n                  api_type: GRPC\r\n                  grpc_services:\r\n                    envoy_grpc:\r\n                      cluster_name: spire_agent\r\n            validation_context_sds_secret_config:\r\n              name: \"spiffe://example.org\"\r\n              sds_config:\r\n                api_config_source:\r\n                  api_type: GRPC\r\n                  grpc_services:\r\n                    envoy_grpc:\r\n                      cluster_name: spire_agent\r\n            tls_params:\r\n              ecdh_curves:\r\n                - X25519:P-256:P-521:P-384\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17361/comments",
    "author": "ddomnik",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2021-07-21T12:42:52Z",
        "body": "At a glance the config plumbing seems right. Do you have a PCAP or something to show what is going on? I'd probably look at Wireshark.\r\n\r\nCC @ggreenway @lizan "
      },
      {
        "user": "ggreenway",
        "created_at": "2021-07-21T15:39:50Z",
        "body": "The error messages indicate that A is doing TLS and B is doing plaintext.\r\n\r\nFrom your config for B, it looks like `transport_socket` is indented too deep, and is attached to the http_connection_manager, not to the filter chain.\r\n\r\nTry running with `--reject-unknown-dynamic-fields` to have this type of thing be an error."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-20T20:01:13Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-27T20:01:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17353,
    "title": "Relationship between grpc service definition timeout and cluster definition connect_timeout",
    "created_at": "2021-07-14T22:12:20Z",
    "closed_at": "2021-07-16T15:36:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17353",
    "body": "In the configuration shown below there is timeout: 1s of grpc_service which is defined to call cluster: ext_authz which also has connect_timeout: 5s. Is there a relationship between those two timeouts? The grpc_service timeout is defined as \"the timeout for a specific request\", vs connect_timeout as \"The timeout for new network connections to hosts in the cluster\", so in case the grpc_service timeout is 'started' first, should it be larger or at least, equal to that of the cluster connect_timeout? \r\n\r\nIs there a best practice advice I can follow? For now, I have set both to the same value of 5s for testing, to be set via an environment variable to 3s in prod.\r\n\r\nthanks in advance,\r\nswav\r\n\r\n\"http_filters\": [\r\n             {\r\n              \"name\": \"envoy.filters.http.ext_authz\",\r\n              \"typed_config\": {\r\n               \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz\",\r\n               \"grpc_service\": {\r\n                \"envoy_grpc\": {\r\n                 \"cluster_name\": \"ext_authz\"\r\n                },\r\n                **\"timeout\": \"1s\"**\r\n               },\r\n               \"transport_api_version\": \"V3\"\r\n              }\r\n             },\r\n...\r\n\"dynamic_active_clusters\": [\r\n{\r\n     \"version_info\": \"1626296116754330624\",\r\n     \"cluster\": {\r\n      \"@type\": \"type.googleapis.com/envoy.config.cluster.v3.Cluster\",\r\n      \"name\": \"ext_authz\",\r\n      \"type\": \"LOGICAL_DNS\",\r\n      **\"connect_timeout\": \"5s\",**\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17353/comments",
    "author": "swav",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-07-16T13:56:02Z",
        "body": "The connect timeout is how long to wait for the connection to be established, while the timeout is how long to wait for the response for a given request. It probably makes sense to have timeout > connect_timeout"
      },
      {
        "user": "swav",
        "created_at": "2021-07-16T15:36:01Z",
        "body": "@snowp  Thank you very much for clarifying this. :)"
      }
    ]
  },
  {
    "number": 17278,
    "title": "[Feature Request] Support dynamic target for batch workload",
    "created_at": "2021-07-09T06:13:46Z",
    "closed_at": "2021-08-27T20:01:12Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17278",
    "body": "*Title*: *Support dynamic target for batch workload*\r\n\r\n*Description*:\r\n\r\nWhen enable Spark on Kubernetes, we'd like to export Spark UI to end user out of cluster by ingress (e.g. Contour). Currently, the target service is pre-defined in config file; so it has to create one ingress service for each spark job. To make configuration simple, we'd like to support dynamic target service for batch workload. For example, if the URL is \"/xxx/spark-ui/`<spark-svc-id>`\", the target service is \"`<spark-svc-id>`\"; the \"`<spark-svc-id>`\" is unique for each spark job.\r\n\r\nI'm a beginner of Envoy, but I'd like to help on this feature; if any comments, please let me know :)",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17278/comments",
    "author": "k82cn",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-07-09T21:28:42Z",
        "body": "Sorry I'm not sure what Envoy feature you are asking for. Can you clarify?"
      },
      {
        "user": "k82cn",
        "created_at": "2021-07-10T03:21:45Z",
        "body": "hm... I'd like to build gateway by Envoy, if the URL is \"/my-platform/spark-ui/\\<spark-svc-id\\>\", I'd like to forward the request to \"\\<spark-svc-id\\>\" instance (e.g. Service in k8s). \"\\<spark-svc-id\\>\" ard dynamically generated when submit a spark job. AFAIK, Envoy support static target instance, e.g. \"my-first-spark-svc\"; in this case, I'd like to support dynamic target."
      },
      {
        "user": "k82cn",
        "created_at": "2021-07-12T03:51:14Z",
        "body": "@mattklein123 do you think that's reasonable for envoy for that scenario ?"
      },
      {
        "user": "mattklein123",
        "created_at": "2021-07-12T14:58:18Z",
        "body": "Envoy can definitely do this, but how you do it depends on your environment. For example, one solution would be:\r\n1) Use RDS to keep a route table up to date with all of the service IDs\r\n2) Create a cluster per service ID using CDS and potentially just static clusters (EDS might not buy very much here)\r\n\r\nThis would work, but might not scale that well if you have lots of services and a high rate of change.\r\n\r\nThere are probably other out of box options using something like the subset LB but that's not something I'm super familiar with. cc @rgs1 and @zuercher for thoughts.\r\n"
      },
      {
        "user": "zuercher",
        "created_at": "2021-07-12T16:52:57Z",
        "body": "The subset load balancing version of this is:\r\n1. Use the header_to_metadata filter to extract the service ID from the :path pseudo-header into the \"envoy.lb\" namespace with a key or your choose (e.g. \"service-id\" for this example)\r\n2. Create a \"spark\" cluster that contains all the hosts across service ids. Each endpoint must have a service id in its endpoint metadata with the same namespace and key. The cluster is configured to use the subset load balancer and build subsets based on the service id key.\r\n3. Configure a route that matches on any service-specific path (e.g. prefix of \"/my-platform/spark-ui/\"), specifies metadata_match with the namespace/key, and forwards to the cluster.\r\n\r\nAt that point, requests to the service specific path will be forwarded to the subset of hosts in the cluster with the matching service id.\r\n"
      },
      {
        "user": "rgs1",
        "created_at": "2021-07-12T17:55:33Z",
        "body": "Stephan's recipe is what you want. \r\n\r\nI am also happy to elaborate a bit more, if you are interested, on how we balance traffic across Spark History servers using hash policy based routing to ensure each job is loaded into memory only once. But that's outside the scope of your initial use case for now. "
      },
      {
        "user": "k82cn",
        "created_at": "2021-07-13T02:31:04Z",
        "body": "re History servers : I'm also interesting in that point, we're working on the e2e solution for spark on k8s :)\r\nre subset load balancing: Great! I'm going to have a try, thanks very much :)"
      },
      {
        "user": "jezhang2014",
        "created_at": "2021-07-19T07:47:08Z",
        "body": "@zuercher I've a quick question, about second step \"Create a \"spark\" cluster that contains all the hosts across service ids. Each endpoint must have a service id in its endpoint metadata with the same namespace and key. The cluster is configured to use the subset load balancer and build subsets based on the service id key.\". After i apply istio envoyfilter, I can see that the cluster contains new load_assignment for two new endpoints with filter_matadata, but it seems  subset_lb.cc:SubsetLoadBalancer::processSubsets did not create new subsets_, should I specify the target host at this place or I should use EDS or eds_config rather than set the endpoints in load_assignment?      \r\n\r\nHere is my envoyfilter by istio\r\n```yaml\r\npatch:\r\n        operation: MERGE\r\n        value:\r\n          load_assignment:\r\n            cluster_name: spark-service\r\n            endpoints:\r\n              - lb_endpoints:\r\n                - endpoint:\r\n                    address:\r\n                      socket_address:\r\n                        protocol: TCP \r\n                        address: 10.244.92.137\r\n                        port_value: 8123\r\n                  metadata:  #not readed at creating subsets_\r\n                    filter_metadata:\r\n                      envoy.lb:\r\n                        service-id: aaa\r\n                - endpoint:\r\n                    address:\r\n                      socket_address:\r\n                        protocol: TCP\r\n                        address: 10.244.92.152\r\n                        port_value: 8123\r\n                  metadata:\r\n                    filter_metadata:\r\n                      envoy.lb:\r\n                        service-id: unknown\r\n          lb_subset_config:\r\n            fallback_policy: DEFAULT_SUBSET\r\n            default_subset:\r\n              service-id: unknown\r\n            subset_selectors:\r\n              - keys:\r\n                - service-id \r\n                fallback_policy: NO_FALLBACK\r\n```"
      },
      {
        "user": "zuercher",
        "created_at": "2021-07-21T16:12:01Z",
        "body": "It shouldn't matter whether the endpoint are in-line in the cluster or delivered via EDS.\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-20T20:01:05Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-27T20:01:12Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17147,
    "title": "OutlierEjection base ejection time is not reducing once max ejection time is reached",
    "created_at": "2021-06-25T09:29:12Z",
    "closed_at": "2021-08-12T20:01:15Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17147",
    "body": "*Title*: *Ejected hosts baseEjectionTime is not reducing once maxEjectionTime is reached *\r\n\r\n*Description*: We are using passive health checks based on ConsecutiveLocalOriginFailure count. Below are the configurations used - \r\n`\tInterval       = 5\r\n\tbaseEjectionTime               = 30\r\n\tmaxEjectionPercent             = 50\r\n\tsplitExternalLocalOriginErrors = true\r\n        consecutiveLocalOriginFailure  = 5`\r\nI have brought down one host in cluster, and host got ejected. After multiple ejections, baseEjectionTime has reached to 5mins (validated it from envoy metrics - envoy_cluster_outlier_detection_ejections_active).\r\n\r\nMy understanding is baseEjectionTime should start decreasing gradually once host starts serving success requests. After multiple success runs from this host too, I don't see baseEjectionTime going down.\r\n\r\nCan someone explain what's the algorithm behind baseEjectionTime reduction when host is healthy again ? And how host is checked if it is healthy ? I haven't configured any active health check on this cluster. So my assumption is if host is serving requests during ejection sweep interval, baseEjectiontime will start decreasing.  \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17147/comments",
    "author": "hinawatts",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-05T16:05:36Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-12T20:01:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17144,
    "title": "Catch Segmentation Fault using UpstreamStartTls in network filter",
    "created_at": "2021-06-25T07:45:01Z",
    "closed_at": "2021-08-05T16:05:35Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17144",
    "body": "Hello, all!\r\nI'm trying to use UpstreamStartTls in my network filter and I catch a segfault ( when switch to tls ) :\r\n\r\n[2021-06-24 12:48:50.560][12412][critical][backtrace] [./source/server/backtrace.h:104] Caught Segmentation fault, suspect faulting address 0x0\r\n[2021-06-24 12:48:50.560][12412][critical][backtrace] [./source/server/backtrace.h:91] Backtrace (use tools/stack_decode.py to get line numbers):\r\n[2021-06-24 12:48:50.560][12412][critical][backtrace] [./source/server/backtrace.h:92] Envoy version: 1aa65ac88e9888127fe795e3ecb4c1bbc36534bf/1.19.0-dev/Modified/DEBUG/BoringSSL\r\n[2021-06-24 12:48:50.627][12412][critical][backtrace] [./source/server/backtrace.h:96] #0: Envoy::SignalAction::sigHandler() [0x5630ddd4fba9]\r\n[2021-06-24 12:48:50.627][12412][critical][backtrace] [./source/server/backtrace.h:96] #1: __restore_rt [0x7f6e517fd980]\r\n[2021-06-24 12:48:50.675][12412][critical][backtrace] [./source/server/backtrace.h:96] #2: Envoy::ConnectionPool::ActiveClient::onEvent() [0x5630dcf945b8]\r\n[2021-06-24 12:48:50.723][12412][critical][backtrace] [./source/server/backtrace.h:96] #3: Envoy::Tcp::ActiveTcpClient::onEvent() [0x5630dcf6eee6]\r\n[2021-06-24 12:48:50.770][12412][critical][backtrace] [./source/server/backtrace.h:96] #4: Envoy::Network::ConnectionImplBase::raiseConnectionEvent() [0x5630dda01d97]\r\n[2021-06-24 12:48:50.817][12412][critical][backtrace] [./source/server/backtrace.h:96] #5: Envoy::Network::ConnectionImpl::raiseEvent() [0x5630dd9eb8ad]\r\n[2021-06-24 12:48:50.864][12412][critical][backtrace] [./source/server/backtrace.h:96] #6: Envoy::Extensions::TransportSockets::Tls::SslSocket::onSuccess() [0x5630dda1ba80]\r\n[2021-06-24 12:48:50.911][12412][critical][backtrace] [./source/server/backtrace.h:96] #7: Envoy::Extensions::TransportSockets::Tls::SslHandshakerImpl::doHandshake() [0x5630ddae7dc1]\r\n[2021-06-24 12:48:50.958][12412][critical][backtrace] [./source/server/backtrace.h:96] #8: Envoy::Extensions::TransportSockets::Tls::SslSocket::doHandshake() [0x5630dda1a445]\r\n[2021-06-24 12:48:51.005][12412][critical][backtrace] [./source/server/backtrace.h:96] #9: Envoy::Extensions::TransportSockets::Tls::SslSocket::doWrite() [0x5630dda1bfbf]\r\n[2021-06-24 12:48:51.054][12412][critical][backtrace] [./source/server/backtrace.h:96] #10: Envoy::Extensions::TransportSockets::StartTls::StartTlsSocket::doWrite() [0x5630da571266]\r\n[2021-06-24 12:48:51.103][12412][critical][backtrace] [./source/server/backtrace.h:96] #11: Envoy::Network::ConnectionImpl::onWriteReady() [0x5630dd9f552e]\r\n[2021-06-24 12:48:51.157][12412][critical][backtrace] [./source/server/backtrace.h:96] #12: Envoy::Network::ConnectionImpl::onFileEvent() [0x5630dd9f3cd3]\r\n[2021-06-24 12:48:51.204][12412][critical][backtrace] [./source/server/backtrace.h:96] #13: Envoy::Network::ConnectionImpl::ConnectionImpl()::$_6::operator()() [0x5630dd9fd54e]\r\n[2021-06-24 12:48:51.253][12412][critical][backtrace] [./source/server/backtrace.h:96] #14: std::_Function_handler<>::_M_invoke() [0x5630dd9fd411]\r\n[2021-06-24 12:48:51.304][12412][critical][backtrace] [./source/server/backtrace.h:96] #15: std::function<>::operator()() [0x5630dc9f3604]\r\n[2021-06-24 12:48:51.350][12412][critical][backtrace] [./source/server/backtrace.h:96] #16: Envoy::Event::DispatcherImpl::createFileEvent()::$_5::operator()() [0x5630dc9eacbf]\r\n[2021-06-24 12:48:51.397][12412][critical][backtrace] [./source/server/backtrace.h:96] #17: std::_Function_handler<>::_M_invoke() [0x5630dc9eab01]\r\n[2021-06-24 12:48:51.397][12412][critical][backtrace] [./source/server/backtrace.h:96] #18: std::function<>::operator()() [0x5630dc9f3604]\r\n[2021-06-24 12:48:51.445][12412][critical][backtrace] [./source/server/backtrace.h:96] #19: Envoy::Event::FileEventImpl::mergeInjectedEventsAndRunCb() [0x5630dc9fbb63]\r\n[2021-06-24 12:48:51.491][12412][critical][backtrace] [./source/server/backtrace.h:96] #20: Envoy::Event::FileEventImpl::assignEvents()::$_1::operator()() [0x5630dc9fc0f4]\r\n[2021-06-24 12:48:51.553][12412][critical][backtrace] [./source/server/backtrace.h:96] #21: Envoy::Event::FileEventImpl::assignEvents()::$_1::__invoke() [0x5630dc9fbc89]\r\n[2021-06-24 12:48:51.604][12412][critical][backtrace] [./source/server/backtrace.h:96] #22: event_persist_closure [0x5630ddd2d76b]\r\n[2021-06-24 12:48:51.653][12412][critical][backtrace] [./source/server/backtrace.h:96] #23: event_process_active_single_queue [0x5630ddd2cde2]\r\n[2021-06-24 12:48:51.702][12412][critical][backtrace] [./source/server/backtrace.h:96] #24: event_process_active [0x5630ddd27728]\r\n[2021-06-24 12:48:51.761][12412][critical][backtrace] [./source/server/backtrace.h:96] #25: event_base_loop [0x5630ddd2662c]\r\n[2021-06-24 12:48:51.814][12412][critical][backtrace] [./source/server/backtrace.h:96] #26: Envoy::Event::LibeventScheduler::run() [0x5630ddce8cff]\r\n[2021-06-24 12:48:51.867][12412][critical][backtrace] [./source/server/backtrace.h:96] #27: Envoy::Event::DispatcherImpl::run() [0x5630dc9e5962]\r\n[2021-06-24 12:48:51.914][12412][critical][backtrace] [./source/server/backtrace.h:96] #28: Envoy::Server::WorkerImpl::threadRoutine() [0x5630dc9d02bb]\r\n[2021-06-24 12:48:51.960][12412][critical][backtrace] [./source/server/backtrace.h:96] #29: Envoy::Server::WorkerImpl::start()::$_5::operator()() [0x5630dc9d20e0]\r\n[2021-06-24 12:48:52.010][12412][critical][backtrace] [./source/server/backtrace.h:96] #30: std::_Function_handler<>::_M_invoke() [0x5630dc9d1efd]\r\n[2021-06-24 12:48:52.057][12412][critical][backtrace] [./source/server/backtrace.h:96] #31: std::function<>::operator()() [0x5630d8dbfdf5]\r\n[2021-06-24 12:48:52.106][12412][critical][backtrace] [./source/server/backtrace.h:96] #32: Envoy::Thread::ThreadImplPosix::ThreadImplPosix()::{lambda()#1}::operator()() [0x5630de19951d]\r\n[2021-06-24 12:48:52.157][12412][critical][backtrace] [./source/server/backtrace.h:96] #33: Envoy::Thread::ThreadImplPosix::ThreadImplPosix()::{lambda()#1}::__invoke() [0x5630de1994f5]\r\n[2021-06-24 12:48:52.157][12412][critical][backtrace] [./source/server/backtrace.h:96] #34: start_thread [0x7f6e517f26db]\r\n\r\n\r\nIt seems that the second Network::ConnectionEvent::Connected event is generated when handshake complete and it causes a segfault on source/common/conn_pool/conn_pool_base.cc:457 , before that the first event was generated when plaintext connection was established.\r\nIs the UpstreamStartTls functionality ready ( It's not mentioned in current.rst ) ? It's a bug or my mistake?\r\nThanks in advance.\r\n\r\nBase issue #15443",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17144/comments",
    "author": "ayyatsenko",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2021-06-28T16:19:45Z",
        "body": "What's your config? You need a network filter that support upstream STARTTLS to use with the transport socket.\r\n\r\ncc @bryce-anderson "
      },
      {
        "user": "ayyatsenko",
        "created_at": "2021-06-29T08:56:15Z",
        "body": "I used the following config:\r\n\r\n```yaml\r\nstats_config:\r\n  stats_matcher:\r\n    reject_all: true\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 127.0.0.1\r\n      port_value: 15001\r\nstatic_resources:\r\n  listeners:     \r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 127.0.0.1\r\n        port_value: 5432\r\n    filter_chains:\r\n      filters:\r\n      - name: envoy.filters.network.tcp_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n          stat_prefix: postgres\r\n          cluster: postgres_ssl\r\n  clusters:\r\n  - name: postgres_ssl\r\n    connect_timeout: 0.25s\r\n    type: STATIC\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: postgres\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 127.0.0.1\r\n                port_value: 15432\r\n    filters:\r\n    - name: envoy.filters.network.postgres_proxy\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.network.postgres_proxy.v3alpha.PostgresProxy\r\n        stat_prefix: postgres\r\n    transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.starttls.v3.UpstreamStartTlsConfig\r\n          cleartext_socket_config: {}\r\n          tls_socket_config:\r\n            common_tls_context:\r\n              tls_certificates:\r\n                certificate_chain:\r\n                  filename: client.crt\r\n                private_key:\r\n                  filename: client.key\r\n              validation_context:\r\n                trusted_ca:\r\n                  filename: ca.crt\r\n\r\n```\r\n\r\nPostgresProxy is a my modification of filter of the same name from default codebase. I registered it as the upstream network filter using macros :\r\n\r\n`REGISTER_FACTORY(PostgresConfigFactoryMy, Server::Configuration::NamedUpstreamNetworkFilterConfigFactory);`\r\n\r\nand I modifyed onData() and onWrite() calls to work with UpstreamStartTls transport socket."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-29T12:01:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-05T16:05:34Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17130,
    "title": "how to make routing choice  in wasm filter?",
    "created_at": "2021-06-24T13:58:05Z",
    "closed_at": "2021-08-01T00:02:05Z",
    "labels": [
      "question",
      "stale",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17130",
    "body": "I want to make routing choice according http body content in wasm filter, how can I achieve that? \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17130/comments",
    "author": "rtttech",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2021-06-24T18:04:09Z",
        "body": "cc @PiotrSikora @mathetake may know if there's docs pointers here"
      },
      {
        "user": "PiotrSikora",
        "created_at": "2021-06-24T18:32:26Z",
        "body": "I don't believe that you can currently make routing decisions based on the HTTP request body content."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-24T20:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-01T00:02:04Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17012,
    "title": "Question: How to embed XDS client in Envoy",
    "created_at": "2021-06-16T16:25:07Z",
    "closed_at": "2021-07-24T00:01:42Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17012",
    "body": "*Title*: *How to embed XDS client in Envoy*\r\n\r\n*Description*:\r\nWe are currently exploring possibilities of a file system based configuration reader which does the following:\r\n- Reads any updates to configuration from a  file/directory \r\n- Translates the configuration changes (whole or delta) to XDS\r\n- Speaks XDS to Envoy's XDS listener.\r\n\r\nHow can we achieve this within Envoy? One possibility is to launch a new thread for this which can setup a listener for file system. But this would be intrusive and change the threading model.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17012/comments",
    "author": "conqerAtapple",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-06-16T20:02:06Z",
        "body": "Couldn't you just use file based xDS and have some other process write the config out?"
      },
      {
        "user": "conqerAtapple",
        "created_at": "2021-06-16T20:23:55Z",
        "body": "> Couldn't you just use file based xDS and have some other process write the config out?\r\n\r\nAbsolutely makes sense to have another process write the xDS config.  We were exploring if the client/user could be saved from writing the error prone configurations and let the translator do that. But i think this idea might bring in other issues. \r\nThanks for the answer. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-17T00:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-24T00:01:42Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 17004,
    "title": "how to capture and handle upstream connection failure in wasm",
    "created_at": "2021-06-16T08:10:01Z",
    "closed_at": "2021-07-29T00:01:38Z",
    "labels": [
      "question",
      "stale",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17004",
    "body": "Is it possible to capture and handle upstream connection error in wasm?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17004/comments",
    "author": "rtttech",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2021-06-21T16:53:17Z",
        "body": "cc @PiotrSikora @mathetake @lizan "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-21T20:01:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-29T00:01:37Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16938,
    "title": "How to call a service get response from it and extract the token and set that token in header and call other service",
    "created_at": "2021-06-11T12:45:01Z",
    "closed_at": "2021-07-19T04:01:43Z",
    "labels": [
      "question",
      "stale",
      "area/http_filter"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16938",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16938/comments",
    "author": "rajneran7890",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-12T00:02:01Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-19T04:01:43Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16908,
    "title": "mongo access log error: invalid mongo op 2013",
    "created_at": "2021-06-09T19:05:59Z",
    "closed_at": "2021-07-17T20:01:36Z",
    "labels": [
      "question",
      "stale",
      "area/mongodb"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16908",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: mongo decoding error: invalid mongo op 2013\r\n\r\n*Description*:\r\nThere isn't record about 'insert', 'update' and 'delete' in mongo proxy's access log.\r\nThe main access log showed that 'mongo decoding error: invalid mongo op 2013'.\r\n\r\nShould I add something in config?\r\n\r\nI used docker image of envoy:v1.18.3 to test. I also tried MongoDB's 3.6 and 4.4.6 versions, but it still couldn't work.\r\n\r\nMy config:\r\n\r\n```\r\nadmin:\r\n  profile_path: /tmp/envoy.prof\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\nstatic_resources:\r\n  listeners:\r\n    - name: ng\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 81\r\n      filter_chains:\r\n        - filters:\r\n            name: envoy.filters.network.http_connection_manager\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n              stat_prefix: test_http\r\n              access_log:\r\n                - name: envoy.access_loggers.stdout\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\r\n              http_filters:\r\n                - name: envoy.filters.http.router\r\n              route_config:\r\n                name: local_route\r\n                virtual_hosts:\r\n                  - name: mysvc\r\n                    domains: [ \"*\" ]\r\n                    routes:\r\n                      - match:\r\n                          prefix: \"/\"\r\n                        route:\r\n                          cluster: service1\r\n                          host_rewrite_literal: test\r\n    - name: mgo\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 27018\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.mongo_proxy\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.mongo_proxy.v3.MongoProxy\r\n                stat_prefix: test_mongo\r\n                access_log: /tmp/mongo.log\r\n            - name: envoy.filters.network.tcp_proxy\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n                stat_prefix: test_tcp\r\n                cluster: service2\r\n  clusters:\r\n    - name: service1\r\n      type: LOGICAL_DNS\r\n      dns_lookup_family: V4_ONLY\r\n      connect_timeout: 1s\r\n      load_assignment:\r\n        cluster_name: service1\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: localhost\r\n                      port_value: 80\r\n    - name: service2\r\n      type: strict_dns\r\n      dns_lookup_family: V4_ONLY\r\n      connect_timeout: 1s\r\n      lb_policy: ROUND_ROBIN\r\n      load_assignment:\r\n        cluster_name: service2\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: localhost\r\n                      port_value: 27017\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16908/comments",
    "author": "Barber0",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-10T20:01:11Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-17T20:01:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "e7868a",
        "created_at": "2023-07-13T06:31:35Z",
        "body": "same issue in 1.26.2."
      }
    ]
  },
  {
    "number": 16905,
    "title": "Ext_proc filter performance question",
    "created_at": "2021-06-09T16:59:08Z",
    "closed_at": "2021-07-17T20:01:34Z",
    "labels": [
      "question",
      "stale",
      "area/ext_proc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16905",
    "body": "Hi, I'm trying to understand the performance expectations of the ext_proc filter.\r\nFrom our testing we have measured that adding the filter on, and sending all request parts (headers/body for req/res) and just returning the exact same content from our grpc server leads to an average overhead 3ms per request.\r\n\r\nIs this the expected overhead, or are there things we can do to optimize our configuration?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16905/comments",
    "author": "dragonbone81",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-06-09T17:38:17Z",
        "body": "@gbrail "
      },
      {
        "user": "gbrail",
        "created_at": "2021-06-09T18:00:04Z",
        "body": "Thanks and I'm glad that you're testing. We don't have a lot of test results yet and I'm glad to hear that you're trying it out with a real use case. I take it from your response that you're not happy with 3ms. Do you have a target?\r\n\r\nThe main thing that you can do now to reduce latency is to reduce the number of hops between the Envoy proxy and your processor. If you are sending request and response headers and request and response body, then that's four round trips. If you don't need all of those, then you can use the \"ProcessingMode\" to eliminate calls that you don't have.\r\n\r\nThe filter and protocol are also designed so that your processor can decide at runtime what calls it needs. For instance, if after using the headers, your processor can conclude that it doesn't need all four of those things, it can either change the processing mode, or it can simply close the gRPC stream.\r\n\r\n@ikepolinsky is also interested in this."
      },
      {
        "user": "gbrail",
        "created_at": "2021-06-09T18:05:45Z",
        "body": "Thinking about future optimizations...\r\n\r\nin your use case, do you always want the headers and the full buffered body for every request and response? And do you always wait for both before deciding how to handle the request and response? Because if so, perhaps more aggressive pipelining (send the body before getting back the headers response) would be appropriate as an option."
      },
      {
        "user": "gbrail",
        "created_at": "2021-06-09T18:11:24Z",
        "body": "Also, when you said: \"returning the exact same content from our grpc server\" did you mean that you're just returning the same request and response body that you just got? Because if you don't want to change the body (or the headers) you can just return an empty message and that will reduce the number of bytes on the wire."
      },
      {
        "user": "dragonbone81",
        "created_at": "2021-06-09T18:29:51Z",
        "body": "@gbrail Thanks for the quick response :)\r\nWe don't have an exact target yet, but we're trying to see if we can optimize any low hanging fruit. Ideally we would want something around the 1ms mark.\r\nCurrently we are looking at req/resp latencies of around 5ms, and we are adding envoy in which adds around 1ms overhead. And since we're currently seeing an overhead of 3ms with ext_proc its a large increase from the original 5ms.\r\n\r\nWe will definitely stop some requests after we read the headers, and that will optimize the performance for those, however we're also focused on the requests that will need all 4 grpc calls. I think also for some of our requests we will be able to tell ext_proc to not send the response body/headers so that would also optimize it.\r\n\r\n> Because if so, perhaps more aggressive pipelining (send the body before getting back the headers response) would be appropriate as an option.\r\n\r\nThis would actually be helpful I think as for most of our requests we need the headers and body in the same place (we use the buffered body so we can modify the headers in the body grpc call). So if there was a way to do only 2 calls (one for requestBody/Headers, and the other for resposeBody/Headers that might be nice (if thats what you are talking about.\r\nWe're still investigating the timings but it seems like the first call is the one that takes the majority of the time, I'm assuming because thats what is establishing the grpc stream.\r\n\r\n> Also, when you said: \"returning the exact same content from our grpc server\"\r\n\r\nYes we are returning the exact same response as comes in, but thats just to make sure the grpc server is adding as little latency as possible and not doing any extra logic, but we will need to change the content eventually so we're testing with that in mind.\r\n"
      },
      {
        "user": "gbrail",
        "created_at": "2021-06-09T18:35:41Z",
        "body": "Good feedback -- thanks. Since you see the biggest delay on the establishment of the stream, I wonder if there are some gRPC optimizations we can do, either in your configuration or in the code. I'm not deeply involved in that part of Envoy. Perhaps some others have an idea there. "
      },
      {
        "user": "moderation",
        "created_at": "2021-06-09T19:13:38Z",
        "body": "Not sure if it is possible with the `ext_proc` filter but in my experience the best way to reduce latency for these interactions is to use Unix Domain Sockets. With the global rate limiter it is possible to have envoyproxy/ratelimit and Redis set up on a socket and have Envoy use that. Effectively removes the network stack / path and in my testing enabled much higher throughput with reduced latency. There are resilience trade offs with the socket approach but if you want the highest performance I would definitely research."
      },
      {
        "user": "gbrail",
        "created_at": "2021-06-09T19:32:16Z",
        "body": "I think that there's also a gRPC stream optimization that's implemented in the ext_authz filter that's not implemented in ext_proc yet. That might be worth exploring.\r\n"
      },
      {
        "user": "dragonbone81",
        "created_at": "2021-06-10T16:26:05Z",
        "body": "@moderation we will explore this, thanks for the suggestion.\r\n\r\n@gbrail Do you have a link to this optimization (code or docs), just so we can take a look to see if we can get any ideas from this?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-10T20:01:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-17T20:01:34Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16900,
    "title": "Question: ext authz not be called when using gRPC client stream.",
    "created_at": "2021-06-09T12:43:05Z",
    "closed_at": "2021-07-17T20:01:32Z",
    "labels": [
      "question",
      "stale",
      "area/ext_authz",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16900",
    "body": "*Title*: *ext authz not be called when using gRPC client stream.*\r\n\r\n*Description*:\r\n>When using gRPC client stream, the ext authz filter not be called per client message. Only if client close send stream, the ext authz filter will be called.\r\nIs there any way to call ext authz filter when every message send by client in the same stream?\r\n\r\nThx a lot.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16900/comments",
    "author": "juchaosong",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-10T20:01:07Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-17T20:01:32Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16875,
    "title": "HTTP/JSON to gRPC/Flatbuffers transcoder",
    "created_at": "2021-06-08T09:10:53Z",
    "closed_at": "2021-06-14T13:10:59Z",
    "labels": [
      "question",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16875",
    "body": "I would like to know what is the best way to implement a transcoder for HTTP/JSON to gRPC/Flatbuffers.\r\n\r\nI am thinking on implementing my own filter that encodes and decodes but any recommendation and if possible point me to some documentation would be appreciated",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16875/comments",
    "author": "joseprupi",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2021-06-10T17:32:33Z",
        "body": "I think the existing grpc filters are probably a good place to start. The grpc_web and grpc_json_transcoder filters both convert protocols (although neither accepts json)."
      },
      {
        "user": "joseprupi",
        "created_at": "2021-06-14T13:11:31Z",
        "body": "Thanks, I will take a look to those"
      }
    ]
  },
  {
    "number": 16811,
    "title": "Question: is Envoy proxy somewhat breaking my gRPC communication?",
    "created_at": "2021-06-04T08:05:04Z",
    "closed_at": "2021-07-11T16:01:10Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16811",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *Question: is Envoy proxy somewhat breaking gRPC communication?*\r\n\r\n*Description*:\r\nHi everyone,\r\ndon't really know if here is the right place to ask, but I'll try anyway. \r\n\r\n## A bit of context\r\nWe've an Istio 1.8.3 installation on a 1.18.x Kubernetes clusters. There are two pods (let's say `ORIGIN` and `DEST`) where `ORIGIN` is making a **gRPC call** to `DEST`, through a Kubernetes service that is proxying the `DEST` pod.\r\n\r\nThe situation is this:\r\n\r\n* without Istio sidecars (i.e. Envoy proxies) all is going well. \r\n* With the sidecars the gRPC communication is not working, we receive an error like `Received unexpected EOS on empty DATA frame from server`.\r\n\r\nHere are the logs, one for successful communication (without sidecars) and one for bad communication (with sidecars)\r\n\r\n## Successful communications logs\r\n\r\n```\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:44.417 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] OUTBOUND SETTINGS: ack=false settings={ENABLE_PUSH=0, MAX_CONCURRENT_STREAMS=0, INITIAL_WINDOW_SIZE=1048576, MAX_HEADER_LIST_SIZE=8192}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:44.430 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] OUTBOUND WINDOW_UPDATE: streamId=0 windowSizeIncrement=983041\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:44.515 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] INBOUND SETTINGS: ack=false settings=[id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:44.516 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] OUTBOUND SETTINGS: ack=true\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:44.535 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] INBOUND SETTINGS: ack=true\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:44.542 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] OUTBOUND HEADERS: streamId=3 headers=GrpcHttp2OutboundHeaders[:authority: dest-service:9100, :path: /x.y.z/Create, :method: POST, :scheme: http, content-type: application/grpc, te: trailers, user-agent: grpc-java-netty/1.36.1, grpc-accept-encoding: gzip] streamDependency=0 weight=16 exclusive=false padding=0 endStream=false\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:44.551 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] OUTBOUND DATA: streamId=3 padding=0 endStream=true length=50 bytes=000000002d0a2439383937636436392d646636632d346362612d386631392d30613562316332613237663412057465737439\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:44.718 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] INBOUND WINDOW_UPDATE: streamId=0 windowSizeIncrement=9934465\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:45.500 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] INBOUND HEADERS: streamId=3 headers=GrpcHttp2ResponseHeaders[:status: 200, content-type: application/grpc+proto, grpc-encoding: gzip, date: Thu, 03 Jun 2021 16:18:45 GMT] padding=0 endStream=false\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:45.512 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] INBOUND DATA: streamId=3 padding=0 endStream=false length=89 bytes=01000000541f8b08000000000000000dc8bb1580201004c0dcd01a48f73de0be9473cad27f093ae15c6df98c71b6224c079429a8ca05299fbd18215177eb41ef...\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:45.512 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] OUTBOUND PING: ack=false bytes=1234\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:45.521 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] INBOUND HEADERS: streamId=3 headers=GrpcHttp2ResponseHeaders[grpc-status: 0, date: Thu, 03 Jun 2021 16:18:45 GMT] padding=0 endStream=true\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:45.530 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] INBOUND PING: ack=true bytes=1234\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD INFO  2021-06-03 16:18:45.542 [ioapp-compute-0] i.r.c.Server$ - [APPLICATION_LOGS_BLAH_BLAH]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:45.552 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] OUTBOUND GO_AWAY: lastStreamId=0 errorCode=0 length=0 bytes=\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n```\r\n\r\n## Bad Communication logs\r\n\r\n```\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:30.427 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] OUTBOUND SETTINGS: ack=false settings={ENABLE_PUSH=0, MAX_CONCURRENT_STREAMS=0, INITIAL_WINDOW_SIZE=1048576, MAX_HEADER_LIST_SIZE=8192}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:30.475 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] OUTBOUND WINDOW_UPDATE: streamId=0 windowSizeIncrement=983041\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:30.487 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] INBOUND SETTINGS: ack=false settings={HEADER_TABLE_SIZE=4096, MAX_CONCURRENT_STREAMS=2147483647, INITIAL_WINDOW_SIZE=268435456,=0}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:30.490 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] OUTBOUND SETTINGS: ack=true\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:30.511 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] INBOUND SETTINGS: ack=true\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:30.512 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] INBOUND WINDOW_UPDATE: streamId=0 windowSizeIncrement=268369921\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:30.521 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] OUTBOUND HEADERS: streamId=3 headers=GrpcHttp2OutboundHeaders[:authority: dest-service:9100, :path: /x.y.z/Create, :method: POST, :scheme: http, content-type: application/grpc, te: trailers, user-agent: grpc-java-netty/1.36.1, grpc-accept-encoding: gzip] streamDependency=0 weight=16 exclusive=false padding=0 endStream=false\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:30.533 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] OUTBOUND DATA: streamId=3 padding=0 endStream=true length=50 bytes=000000002d0a2439383937636436392d646636632d346362612d386631392d30613562316332613237663412057465737439\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:31.071 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] INBOUND HEADERS: streamId=3 headers=GrpcHttp2ResponseHeaders[:status: 200, grpc-encoding: gzip, date: Thu, 03 Jun 2021 15:13:31 GMT, content-type: application/grpc+proto, x-envoy-upstream-service-time: 529, server: envoy] padding=0 endStream=false\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:31.113 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] INBOUND DATA: streamId=3 padding=0 endStream=false length=89 bytes=01000000541f8b08000000000000000dc9b911c0300804c0dca16b50ca0c423ca21c71c6fd97606fbad710e6ee9a2099aca406a1b4d8a4484b632b71dc2375c9...\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:31.114 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] OUTBOUND PING: ack=false bytes=1234\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:31.125 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] INBOUND DATA: streamId=3 padding=0 endStream=true length=0 bytes=\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:31.129 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] INBOUND PING: ack=true bytes=1234\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:31.160 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] OUTBOUND GO_AWAY: lastStreamId=0 errorCode=0 length=0 bytes=\r\n[APPLICATION-LOGS_BLAH_BLAH]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD io.grpc.StatusRuntimeException: INTERNAL: Received unexpected EOS on empty DATA frame from server\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD\tat io.grpc.Status.asRuntimeException(Status.java:535)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD\tat io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:533)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD\tat io.grpc.internal.DelayedClientCall$DelayedListener$3.run(DelayedClientCall.java:464)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD\tat io.grpc.internal.DelayedClientCall$DelayedListener.delayOrExecute(DelayedClientCall.java:428)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD\tat io.grpc.internal.DelayedClientCall$DelayedListener.onClose(DelayedClientCall.java:461)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD\tat io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:553)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD\tat io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:68)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:739)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD\tat io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:718)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD\tat io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD\tat io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\t\r\n```\r\n\r\n## Differences in logs\r\nUnfortunately I'm no expert in debugging gGPC services. There's one thing that i clearly see: \r\n\r\n* in succesful calls (without envoys) we've got only one `INBOUND DATA` log line:\r\n\r\n```\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 16:18:45.512 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x0a2ce423, L:/10.172.22.165:60318 - R:dest-service/10.175.246.61:9100] INBOUND DATA: streamId=3 padding=0 endStream=false length=89 bytes=01000000541f8b08000000000000000dc8bb1580201004c0dcd01a48f73de0be9473cad27f093ae15c6df98c71b6224c079429a8ca05299fbd18215177eb41ef...\t\r\n```\r\n\r\n* in errored calls (with envoys) we've got TWO `INBOUND DATA` log lines:\r\n\r\n```\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:31.113 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] INBOUND DATA: streamId=3 padding=0 endStream=false length=89 bytes=01000000541f8b08000000000000000dc9b911c0300804c0dca16b50ca0c423ca21c71c6fd97606fbad710e6ee9a2099aca406a1b4d8a4484b632b71dc2375c9...\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nORIGIN-POD ORIGIN-POD DEBUG 2021-06-03 15:13:31.125 [grpc-nio-worker-ELG-1-1] i.g.n.NettyClientHandler - [id: 0x600c3d09, L:/10.172.22.162:33746 - R:dest-service/10.175.246.61:9100] INBOUND DATA: streamId=3 padding=0 endStream=true length=0 bytes=\t\r\n```\r\n\r\nand the last one has `length=0` bytes, so I suppose that this is giving the `Received unexpected EOS on empty DATA frame from server` error\r\n\r\nHope that this is enough to let someone of you understand what my problem is.\r\nThanks\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16811/comments",
    "author": "paoloyx",
    "comments": [
      {
        "user": "paoloyx",
        "created_at": "2021-06-04T08:29:24Z",
        "body": "Just to add an important info, here are the Envoy proxies logs for above communication\r\n\r\n## ORIGIN pod\r\n\r\n```\r\n[2021-05-28T15:32:52.440Z] \"POST /x.y.z/Create HTTP/2\" 200 - via_upstream - \"-\" 50 89 534 529 \"-\" \"grpc-java-netty/1.36.1\" 2b95ea42-d089-4ae5-9d45-b13dba9e983f\"  \"dest-service:9100\" 10.172.24.7:9000 outbound|9100||dest-service.default.svc.cluster.local 10.172.22.162:35838 10.175.246.61:9100 10.172.22.162:33746 - default\r\n```\r\n\r\n## DEST pod\r\n\r\n```\r\n[2021-05-28T15:32:52.442Z]  \"POST /x.y.z/Create HTTP/2\"  200  -  via_upstream  -  \"-\"  0  135  643  640  \"-\"  \"grpc-java-netty/1.36.1\"  \"96fa90ba-ab5e-4d03-818e-c762f5 │  │ 92aeb1\"  \"dest-service:9100\"  \"127.0.0.1:9000\"  inbound|9000||  127.0.0.1:45170  10.172.24.7:9000  10.172.22.162:35838  outbound_.9100_._.dest-service.default.svc.cluster.local  default\r\n```\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-04T16:01:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-11T16:01:10Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16758,
    "title": "Lua filters for TCP Proxy?",
    "created_at": "2021-06-01T23:30:26Z",
    "closed_at": "2021-07-09T20:01:25Z",
    "labels": [
      "question",
      "stale",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16758",
    "body": "Is there a way to use Lua filters with the TCP proxy like you can with the HTTPConnectionManager?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16758/comments",
    "author": "ajsalow",
    "comments": [
      {
        "user": "junr03",
        "created_at": "2021-06-02T14:42:16Z",
        "body": "Not currently. The Lua stack is only adapted for HTTP operation as an HTTP filter."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-02T16:01:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-09T20:01:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "amankumarcs",
        "created_at": "2023-11-08T16:03:01Z",
        "body": "Is there a way to use Lua filters with the TCP proxy like you can with the HTTP connection manager?"
      }
    ]
  },
  {
    "number": 16700,
    "title": "Filesystem config update rejected: Failed to load incomplete certificate ",
    "created_at": "2021-05-27T14:31:09Z",
    "closed_at": "2021-07-11T20:01:29Z",
    "labels": [
      "question",
      "area/tls",
      "stale",
      "area/sds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16700",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\nTitle: Filesystem config update rejected: Failed to load incomplete certificate \r\n\r\nDescription:\r\n\r\nHelm Chart installing envoy with dynamic configuration from Filesystem->problem to load certificates from k8s secrets when k8s secrets are installed in the same Helm Chart of envoy POD.\r\n\r\nThe secret could be created with empty certificates first and then updated later. If envoy starts and finds empty certificates it fails to become ready:\r\n\r\ne.g: `[2021-05-27 14:00:30.860][1][warning][config] [source/common/config/filesystem_subscription_impl.cc:43] Filesystem config update rejected: Failed to load incomplete certificate from , `\r\n\r\nIf the k8s secrets are created before to install the Helm Chart everything works fine.\r\n\r\nlds.yaml:\r\n\r\n```\r\nversion_info: '0'\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.config.listener.v3.Listener\r\n  name: listener1\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 6666\r\n  filter_chains:\r\n  - filters:\r\n    - name: envoy.filters.network.tcp_proxy\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n        cluster: remote-service\r\n        stat_prefix: tcp_remote_service\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n        require_client_certificate: true\r\n        common_tls_context:      \r\n          tls_certificate_sds_secret_configs:\r\n            name: tls_sds\r\n            sds_config:\r\n              path: /etc/envoy/tls_certificate_sds_secret.yaml\r\n          validation_context_sds_secret_config:\r\n            name: validation_context_sds\r\n            sds_config: \r\n              path: /etc/envoy/validation_context_sds_secret.yaml              \r\n```\r\n\r\ntls_certificate_sds_secret.yaml:\r\n\r\n```\r\nresources:\r\n  - \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\"\r\n    name: tls_sds\r\n    tls_certificate:\r\n      certificate_chain:\r\n        filename: /certs/sip_tls_certs/tls.crt\r\n      private_key:\r\n        filename: /certs/sip_tls_certs/tls.key\r\n```\r\nvalidation_context_sds_secret.yaml:\r\n\r\n```\r\nresources:\r\n  - \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\"\r\n    name: validation_context_sds\r\n    validation_context:\r\n      trusted_ca:\r\n        filename: /certs/sip_tls_ca/tls.crt  \r\n\r\n```\r\n\r\nThe certificates are mounted in envoy POD as secrets. If secrets are created before of the installlation of Helm Chart where is deployed the envoy POD we don't have any issue with certificates loading. If secrets are created inside the same Helm Chart but we are using static configuration for envoy we don't have any issue. If secrets are created inside the same Helm Chart and envoy is creating dynamic resources from Filesystem we trigger the issue reported above.\r\n\r\nWhy do we have such behaviour? I would like better to understand it. Thanks! \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16700/comments",
    "author": "eoregua",
    "comments": [
      {
        "user": "eoregua",
        "created_at": "2021-06-04T17:20:56Z",
        "body": "Any feedback?\r\n\r\nThanks!"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-04T20:01:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-11T20:01:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16654,
    "title": "Logging protobuf message with `ShortDebugString()`",
    "created_at": "2021-05-25T13:02:47Z",
    "closed_at": "2021-07-02T20:01:13Z",
    "labels": [
      "enhancement",
      "question",
      "stale",
      "area/envoy_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16654",
    "body": "There are many log prints in Envoy like `ENVOY_LOG(info, \"message: {}\", some_proto.DebugString())`, which makes the certain log outputs in multiple lines. For humans, multi-line logs are more readable, but they are not friendly to some log collection and storage systems. Usually these systems collects and stores logs line by line. For example, I want to store Envoy logs in elastic search for quick search, but when these logs are output in the form of multiple lines, it is difficult to find the complete logs.\r\n\r\nSo I was wondering if Envoy could output one log whose level is higher than `info` or `warn` in a single line. \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16654/comments",
    "author": "WeavingGao",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2021-05-26T13:15:41Z",
        "body": "cc @yanavlasov "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-25T16:01:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-02T20:01:13Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16648,
    "title": "How to support the host rewrite with regex style?",
    "created_at": "2021-05-25T03:00:44Z",
    "closed_at": "2021-07-02T12:01:28Z",
    "labels": [
      "question",
      "stale",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16648",
    "body": "Supposed that we make a request whose host is `*.test.app.com` to envoy，we want to rewrite `*.test.app.com` to `*.dev.app.com`. \r\nFor example, the original host `a.test.app.com` be rewritten to `a.dev.app.com`, which keep the first part of host unchanged. How can I use regex way similar with path to achieve my idea?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16648/comments",
    "author": "SpecialYang",
    "comments": [
      {
        "user": "SpecialYang",
        "created_at": "2021-05-26T08:17:08Z",
        "body": "cc @alyssawilk @mattklein123 @snowp"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-25T12:01:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-02T12:01:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16554,
    "title": "Envoy ignores \"common_http_protocol_options\" set on Cluster for ALS.",
    "created_at": "2021-05-19T00:28:20Z",
    "closed_at": "2021-07-01T00:01:42Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16554",
    "body": "ALS not respecting \"commmon_http_protocol_options\"\r\n\r\nSeems like the cluster options set for access log service are getting ignored by envoyproxy.\r\n\r\nI have set up a `max_stream_time` of 5 seconds to test, fired up Wireshark, and see no `RST_STREAM` frame being sent at all.\r\n\r\nrelevant config snippets:\r\n\r\ncluster definition:\r\n```\r\n  {\r\n       \"name\": \"envoy-log-service\",\r\n       \"type\": \"EDS\",\r\n       \"eds_cluster_config\": {\r\n        \"eds_config\": {\r\n         \"ads\": {}\r\n        }\r\n       },\r\n       \"connect_timeout\": \"0.250s\",\r\n       \"http2_protocol_options\": {},\r\n       \"common_http_protocol_options\": {\r\n        \"max_stream_duration\": \"5s\"\r\n       }\r\n  }\r\n```\r\n\r\n\r\nNot sure if this is a bug or a feature request.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16554/comments",
    "author": "ozkar99",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2021-05-20T18:43:46Z",
        "body": "It's a feature request to me. \r\nALS http request is not a proxied http request: e.g. this request doesn't have a downstream http request so it's not too surprising if some features are missing."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-23T20:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-01T00:01:41Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "lonfme",
        "created_at": "2024-05-17T14:49:59Z",
        "body": "The following configuration worked for me, setting `maxStreamDuration` to 300s and `maxConnectionDuration` to 310s.\r\n```json\r\n{\r\n  \"name\": \"custom_envoy_accesslog_service\",\r\n  \"type\": \"STRICT_DNS\",\r\n  \"connectTimeout\": \"1s\",\r\n  \"loadAssignment\": {\r\n    \"clusterName\": \"custom_envoy_accesslog_service\",\r\n    \"endpoints\": [\r\n      {\r\n        \"lbEndpoints\": [\r\n          {\r\n            \"endpoint\": {\r\n              \"address\": {\r\n                \"socketAddress\": {\r\n                  \"address\": \"envoy-als-headless.ops.svc.cluster.local\",\r\n                  \"portValue\": 9019\r\n                }\r\n              }\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    ]\r\n  },\r\n  \"typedExtensionProtocolOptions\": {\r\n    \"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\": {\r\n      \"@type\": \"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\r\n      \"commonHttpProtocolOptions\": {\r\n        \"maxConnectionDuration\": \"310s\",\r\n        \"maxStreamDuration\": \"300s\"\r\n      },\r\n      \"explicitHttpConfig\": {\r\n        \"http2ProtocolOptions\": {}\r\n      }\r\n    }\r\n  },\r\n  \"respectDnsTtl\": true,\r\n  \"dnsLookupFamily\": \"V4_ONLY\"\r\n}\r\n```\r\nI hope this helps."
      }
    ]
  },
  {
    "number": 16473,
    "title": "Questions: How to debug/test a lua plugin without deploying it to envoy?",
    "created_at": "2021-05-13T03:23:31Z",
    "closed_at": "2021-06-20T04:01:15Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16473",
    "body": "We're planning to develop a lua plugin for our istio service mesh's gateway, and we think it might be low effective to debug it by deploying it to a running envoy. so how  to debug/test a lua plugin without deploying it to envoy?\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16473/comments",
    "author": "alex-lx",
    "comments": [
      {
        "user": "antoniovicente",
        "created_at": "2021-05-13T23:59:25Z",
        "body": "cc @dio \r\n\r\nIt's possible that running against a real Envoy would be the most effective way to debug/test, but dio may know of alternate toolchains that would be useful during development of lua plugins."
      },
      {
        "user": "alex-lx",
        "created_at": "2021-05-14T02:05:50Z",
        "body": "My question may not so clear, but yes, what we are looking for is a dev tool. Thanks for clarify that. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-13T04:01:13Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-20T04:01:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16375,
    "title": "For http filters, when is body callbacks(for either request or response) are called?",
    "created_at": "2021-05-07T05:41:39Z",
    "closed_at": "2021-06-16T20:01:31Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16375",
    "body": "Hi,\r\n\r\nI have a question on how body callbacks are called for http filters, for either request or response. If the body is big and takes time to receive in envoy, how much body will be buffered, and when will the body callbacks(like onResponseBody) be invoked? How can I find the details on these behavior? \r\n\r\nThanks in advance.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16375/comments",
    "author": "dandlake",
    "comments": [
      {
        "user": "antoniovicente",
        "created_at": "2021-05-10T17:43:50Z",
        "body": "The filter decode body callbacks are called as chunks of the body are parsed by the codec and pushed through the filter pipeline.  There is a separate body callback call for each body chunk.  end_stream is set if the current chunk is the last body chunk associated with the request or response.  Most of the buffering happens in output buffers, after the bytes have been processed by the filter pipeline.\r\n\r\nDo you have some followup questions?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-09T20:01:13Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-16T20:01:31Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16308,
    "title": "SOCKS proxy support",
    "created_at": "2021-05-04T19:14:40Z",
    "closed_at": "2021-07-05T04:01:16Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16308",
    "body": "*Title*: *SOCKS proxy support*\r\n\r\n*Description*:\r\nAre there plans (or a desire) to support SOCKS proxy with envoy? I wasn't able to find anything on the topic\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16308/comments",
    "author": "hamdiallam",
    "comments": [
      {
        "user": "antoniovicente",
        "created_at": "2021-05-10T18:26:10Z",
        "body": "I'm not aware of existing efforts to add SOCKS proxy support.  The closest I have seen is support for tunneling over HTTP CONNECT."
      },
      {
        "user": "lambdai",
        "created_at": "2021-05-21T04:30:34Z",
        "body": "It should be trivial to support SOCKS4 CONNECT command via chained tcp proxy and original dst cluster"
      },
      {
        "user": "hamdiallam",
        "created_at": "2021-05-26T02:02:10Z",
        "body": "Can you elaborate on this a little more? I'm looking to implement support"
      },
      {
        "user": "hamdiallam",
        "created_at": "2021-05-26T06:07:24Z",
        "body": "Apologies for accidentally closing & re-opening"
      },
      {
        "user": "lambdai",
        "created_at": "2021-05-27T05:03:05Z",
        "body": "@hamdiallam From high level, you can create a SOCKS protocol network filter and insert it before TcpProxy network filter.\r\n\r\nThat SOCKS network filter consumes the SOCKS bytes and set the upstream endpoint address. \r\n\r\nMeanwhile, if you chain the tcp proxy with original_dst cluster, the SOCKS network can set the upstream endpoint transparently to tcp proxy.\r\n\r\nThe only issue is that TcpProxy currently create the upstream connection way to early(in onNewConnection). It is  before SOCKS filter consuming the SOCKS frame. You can either hack the tcp proxy.\r\nYou may want to hack the tcp proxy, or propose the new option in TcpProxy\r\n"
      },
      {
        "user": "hamdiallam",
        "created_at": "2021-05-29T01:27:06Z",
        "body": "Gotcha thanks, I'll give this a try.\r\n\r\nI'll keep this issue open to reference in any Draft PRs with an implementation"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-28T04:01:06Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-05T04:01:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "noob-vivek",
        "created_at": "2022-09-06T09:24:53Z",
        "body": "Would it be a bad idea to write socks protocol as new listener? I was wondering to write socks protocol parser as new listener and setup hostname as SNI and attach sni_dynamic_forward_proxy and tcp_proxy in filter chain."
      }
    ]
  },
  {
    "number": 16262,
    "title": "Clear /cluster counters",
    "created_at": "2021-05-01T10:10:39Z",
    "closed_at": "2021-06-10T12:51:59Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16262",
    "body": "*Title*: *Clear \"/cluster\" counters*\r\n\r\n*Description*:\r\n>Is there any way to clear /cluster(rq_error, rq_total ...etc) counters without restart.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16262/comments",
    "author": "sefaphlvn",
    "comments": [
      {
        "user": "antoniovicente",
        "created_at": "2021-05-11T05:40:51Z",
        "body": "cc @jmarantz \r\n\r\nI don't think so, but jmarantz would know for sure."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-10T08:01:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "jmarantz",
        "created_at": "2021-06-10T12:51:59Z",
        "body": "There is `/reset_counters`.\r\n\r\nThere is not a way (currently) to reset only a subset of the counters.\r\n"
      }
    ]
  },
  {
    "number": 16250,
    "title": "bazel build config_load_check_tool failure",
    "created_at": "2021-04-30T10:20:37Z",
    "closed_at": "2021-06-06T20:10:39Z",
    "labels": [
      "question",
      "area/build",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16250",
    "body": "Hi I am installing config_load_check_tool using bazel on ubuntu 16\r\n\r\nCommand:\r\nbazel build //test/tools/config_load_check:config_load_check_tool  --sandbox_debug\r\n\r\nError.\r\n\r\nTarget //test/tools/config_load_check:config_load_check_tool failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /root/.cache/bazel/_bazel_root/024c378c6e455b9118e04c1795b58121/external/envoy_api/envoy/annotations/BUILD:5:18 error executing shell command: '/bin/bash -c #!/usr/bin/env bash\r\nfunction cleanup_function() {\r\nlocal ecode=$?\r\nif [ $ecode -eq 0 ]; then\r\ncleanup_on_success\r\nelse\r\ncleanup_on_failure\r\nfi\r\n}\r\nset -e\r\nfunction cleanup_on_success() {\r\nprintf...' failed (Exit 127): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\n\r\nAm new to bazel so any help would be much appreciated , thanks",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16250/comments",
    "author": "mattygyk",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2021-04-30T13:33:33Z",
        "body": "You may want to ask on envoy-dev slack channel about this problem."
      },
      {
        "user": "mattygyk",
        "created_at": "2021-04-30T14:01:22Z",
        "body": "> You may want to ask on envoy-dev slack channel about this problem.\r\n\r\nThanks for the advice."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-05-30T16:15:17Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-06T20:10:38Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16233,
    "title": "Help setting up https for XDS cluster",
    "created_at": "2021-04-29T14:37:25Z",
    "closed_at": "2021-04-29T16:19:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16233",
    "body": "*Cannot make CDS over https work*: *upstream connect error or disconnect/reset before headers*\r\n\r\n*Description*:\r\nI am trying to setup Envoy as an edge proxy with control-plane data coming from a custom web service written in .Net. The web service is callable from a test client and logs the request as http2 over TLS so I know this is working correctly. However, when I try and point Envoy at this web service for CDS configuration, it cannot load it, I just see a lot of logs of:\r\n\r\n`opt/bin/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:101] StreamClusters gRPC config stream closed: 14, upstream connect error or disconnect/reset before headers. reset reason: connection termination`\r\n\r\nI am running envoy, the web service and a sample web application through Docker compose. I can access the web app directly via its exposed port and envoy is running due to a test stub that returns a message directly.\r\n\r\nIt is likely to be something simple but there are various parts of the documentation, which I might have not understood correctly so I can't break it down into any smaller tests. For example:\r\n\r\n- If I exec into the proxy container, I can curl the endpoint I have specified in the xds cluster and the only error I seem to get is that the certificate could not be validated, which is not necessarily surprising since it is an Alpine based image and might well not have the correct root certs installed.\r\n- But, the docs say that Envoy doesn't validate upstream certs by default, so is this a problem?\r\n- I understand that we should always use TLS over HTTP2 and that grpc should always use HTTP2 so that is the only configuration I have enabled on the control plane, and this is verified by calling it from the test client. Cert verifies OK in this scenario because I am calling it from my desktop machine.\r\n- The error message seems to be caused by any number of issues with ssl, but I am unsure how to debug this to determine what I have not done correctly.\r\n- There are various other details around what exactly envoy will handshake with depending on the presence of things like transport_socket, but it is unclear exactly what all of these mean, especially since none of the samples seem to have anything more than I have below.\r\n\r\n[optional *Relevant Links*:]\r\nRelevant section of envoy.yml\r\n\r\n```yaml\r\ndynamic_resources:\r\n  cds_config:\r\n    resource_api_version: V3\r\n    api_config_source:\r\n      api_type: GRPC\r\n      transport_api_version: V3\r\n      grpc_services:\r\n        - envoy_grpc:\r\n            cluster_name: xds_cluster\r\n\r\nstatic_resources:\r\n  clusters:\r\n  - name: xds_cluster\r\n    connect_timeout: 15s\r\n    per_connection_buffer_limit_bytes: 32768 # 32 KiB\r\n    type: STRICT_DNS\r\n    lb_policy: ROUND_ROBIN\r\n    typed_extension_protocol_options:\r\n      envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n        \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n        explicit_http_config:\r\n          http2_protocol_options: {}\r\n    load_assignment:\r\n      cluster_name: xds_cluster\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: containers_webservice_1\r\n                port_value: 443\r\n```\r\n\r\nI am happy to write some of this up for the docs once it works, if that is helpful (and I haven't managed to find something that already describes this more clearly!)",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16233/comments",
    "author": "lukos",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2021-04-29T15:57:41Z",
        "body": "You are likely to have better luck by asking this in the `envoy-users` slack channel.\r\nI think in this case you will also need to add tls config for the transport socket. Otherwise Envoy will try to connect to your server without TLS."
      },
      {
        "user": "lukos",
        "created_at": "2021-04-29T16:19:25Z",
        "body": "OK, I will ask there. Thanks."
      }
    ]
  },
  {
    "number": 16222,
    "title": "Getting errors from envoy proxy with istio 1.9.1 version on GRPC web service",
    "created_at": "2021-04-29T05:48:37Z",
    "closed_at": "2021-06-06T04:24:54Z",
    "labels": [
      "question",
      "stale",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16222",
    "body": "Hi Guys,\r\n\r\nI am using istio 1.9.1 and getting the below errors from my web service:\r\n\r\nistio-grpc-error logs:\r\n\r\n[2021-04-28T23:17:13.094Z] \"POST /int.expre.subscriptions.v1.SubscriptionService/GetSubscriptionsForAccount HTTP/2\" 415 UR upstream_reset_after_response_started{remote_reset} - \"-\" 60 0 5 4 \"172.20.49.218\" \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Mobile Safari/537.36\" \"62efd112-2004-955f-8a81-a9773cc7ef8d\" \"subscription.staging.expressplay.com\" \"100.102.102.179:9201\" outbound|80||subscription-service.cap-staging.svc.cluster.local 100.97.224.11:37818 100.97.224.11:8443 172.20.49.218:56345 subscription.staging.expressplay.com -\r\n\r\nService error logs:\r\n\r\n2021-04-28 23:14:57.403  WARN 1 --- [-worker-ELG-1-1] i.g.n.s.i.grpc.netty.NettyServerHandler  : Expected header TE: trailers, but null is received. This means some intermediate proxy may not support trailers\r\n2021-04-28 23:14:57.430  WARN 1 --- [-worker-ELG-1-1] i.g.n.s.i.grpc.netty.NettyServerHandler  : Exception in onDataRead()\r\njava.lang.NullPointerException: null\r\n\tat io.grpc.netty.shaded.io.grpc.netty.NettyServerHandler.onDataRead(NettyServerHandler.java:476) ~[grpc-netty-shaded-1.24.0.jar!/:1.24.0]\r\n\tat io.grpc.netty.shaded.io.grpc.netty.NettyServerHandler.access$800(NettyServerHandler.java:101) ~[grpc-netty-shaded-1.24.0.jar!/:1.24.0]\r\n\tat io.grpc.netty.shaded.io.grpc.netty.NettyServerHandler$FrameListener.onDataRead(NettyServerHandler.java:787) ~[grpc-netty-shaded-1.24.0.jar!/:1.24.0]\r\n\tat io.grpc.netty.shaded.io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onDataRead(DefaultHttp2ConnectionDecoder.java:292) ~[grpc-netty-shaded-1.24.0.jar!/:1.24.0]\r\n\tat io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onDataRead(Http2InboundFrameLogger.java:48) ~[grpc-netty-shaded-1.24.0.jar!/:1.24.0]\r\n\tat io.grpc.netty.shaded.io.netty.handler.codec.http2.DefaultHttp2FrameReader.readDataFrame(DefaultHttp2FrameReader.java:422) ~[grpc-netty-shaded-1.24.0.jar!/:1.24.0]\r\n\tat io.grpc.netty.shaded.io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:251) ~[grpc-netty-shaded-1.24.0.jar!/:1.24.0]\r\n\tat io.grpc.netty.shaded.io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:160) ~[grpc-netty-shaded-1.24.0.jar!/:1.24.0]\r\n\tat io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41) ~[grpc-netty-shaded-1.24.0.jar!/:1.24.0]\r\n\tat io.grpc.netty.shaded.io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:174) ~[grpc-netty-shaded-1.24.0.jar!/:1.24.0]\r\n\tat io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:378) ~[grpc-netty-shaded-1.24.0.jar!/:1.24.0]\r\n\tat io.grpc.netty.shaded.io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:438) ~[grpc-netty-shaded-\r\n\r\n\r\nOn  web page error:\r\n\r\n  grpc-message: Content-Type 'application/grpc-web-text' is not supported\r\n\r\n\r\n\r\n\r\nMy envoy.yml file:\r\n\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: subscription-service\r\nspec:\r\n  workloadLabels:\r\n    app: subscription-service\r\n  filters:\r\n  - listenerMatch:\r\n      listenerType: ANY\r\n      listenerProtocol: HTTP\r\n    insertPosition:\r\n      index: FIRST\r\n    filterType: HTTP\r\n    filterName: \"envoy.filters.http.grpc_web\"\r\n    filterConfig: {}\r\n```\r\n\r\n\r\n\r\ncan anybody help with this issue?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16222/comments",
    "author": "akhilgoel09",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2021-04-29T16:09:07Z",
        "body": "@fengli79 @lizan may be able to help with the grpc-web filter"
      },
      {
        "user": "akhilgoel09",
        "created_at": "2021-04-29T20:59:57Z",
        "body": "sure, it will be a great help if you can."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-05-30T00:13:55Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-06T04:24:54Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "lacasaprivata2",
        "created_at": "2021-07-07T06:52:41Z",
        "body": "+1"
      },
      {
        "user": "GNUGradyn",
        "created_at": "2022-08-17T21:30:04Z",
        "body": "+1"
      }
    ]
  },
  {
    "number": 16204,
    "title": "how to modify envoy timezone ,for ingressgateway",
    "created_at": "2021-04-28T06:48:50Z",
    "closed_at": "2021-05-12T06:56:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16204",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue.\r\n[root@master01 ~]# kubectl cp /usr/share/zoneinfo/Asia/Shanghai istio-ingressgateway-d46c4ff9b-6qh4k:/etc/localtime -n istio-system\r\ntar: localtime: Cannot open: Read-only file system\r\ntar: Exiting with failure status due to previous errors\r\ncommand terminated with exit code 2\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16204/comments",
    "author": "13567436138",
    "comments": [
      {
        "user": "13567436138",
        "created_at": "2021-04-28T06:53:20Z",
        "body": "su root,anybody know the password of root"
      },
      {
        "user": "hobbytp",
        "created_at": "2021-05-07T01:01:30Z",
        "body": "> su root,anybody know the password of root\r\n\r\nYou cannot su root in istio ingress-gateway which is using not-root now. currently changing timezone in istio/envoy is also not supported. so\r\n1. you could update the istio-ingress gateway to use root privilege (default is not root), and then try again.\r\n2. Or changing the timezone in your own image."
      }
    ]
  },
  {
    "number": 16084,
    "title": "Route configuration using variuos query parameters matchers with the same URL prefix",
    "created_at": "2021-04-20T17:23:15Z",
    "closed_at": "2021-06-12T00:01:21Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16084",
    "body": "*Title*: *Route configuration using various query parameters matchers with the same URL prefix*\r\n\r\n*Description*:\r\nWe are working on a configuration where the same prefix is used by two distinct clusters and the routing path is decided based on query parameters provided with the request.\r\n\r\nWe would like the routing to work as follows:\r\n* if no query parameters are passed, the request should return 404 Not Found;\r\n* if query parameter \"one\" is present and parameter \"two\" is not present the request should be forwarded to \"webapp-a\" cluster;\r\n* if query parameter \"two\" is present and parameter \"one\" is not present the request should be forwarded to \"webapp-b\" cluster;\r\n* if both \"one\" and \"two\" query parameters are passed, the request should return 404 Not Found;\r\n\r\nWe are using \"envoyproxy/envoy-alpine:v1.18.2\" Docker image, our Envoy configuration file looks like follows:\r\n\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8080\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          http_filters:\r\n          - name: envoy.filters.http.router\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/webapp\"\r\n                  query_parameters:\r\n                  - name: one\r\n                    present_match: true\r\n                  - name: two\r\n                    present_match: false\r\n                route:\r\n                  prefix_rewrite: /demo-apps/webapp-a\r\n                  cluster: webapp-a\r\n              - match:\r\n                  prefix: \"/webapp\"\r\n                  query_parameters:\r\n                  - name: one\r\n                    present_match: false\r\n                  - name: two\r\n                    present_match: true\r\n                route:\r\n                  prefix_rewrite: /demo-apps/webapp-b\r\n                  cluster: webapp-b\r\n\r\n  clusters:\r\n  - name: webapp-a\r\n    connect_timeout: 30s\r\n    type: STRICT_DNS\r\n    # Comment out the following line to test on v6 networks\r\n    dns_lookup_family: V4_ONLY\r\n    load_assignment:\r\n      cluster_name: webapp-a\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 10.1.1.1\r\n                port_value: 11111\r\n  - name: webapp-b\r\n    connect_timeout: 30s\r\n    type: STRICT_DNS\r\n    # Comment out the following line to test on v6 networks\r\n    dns_lookup_family: V4_ONLY\r\n    load_assignment:\r\n      cluster_name: webapp-b\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 10.2.2.2\r\n                port_value: 22222\r\n\r\nadmin:\r\n  access_log_path: /dev/null\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 10000\r\n```\r\n\r\nWe sent several test requests and the results did not work to our expectations. We would like to understand, is there an invalid configuration on our side or perhaps we misunderstood  the Envoy's documentation or perhaps it is a malfunction on the Envoy's side.\r\n\r\nSee below commands, outputs and excerpts from Envoy Debug logs:\r\n\r\n1. Sending request to `/webapp` without any query parameters works as expected and return **404 Not Found**:\r\n```\r\n/ # curl -v localhost:8080/webapp\r\n*   Trying ::1:8080...\r\n* connect to ::1 port 8080 failed: Connection refused\r\n*   Trying 127.0.0.1:8080...\r\n* Connected to localhost (127.0.0.1) port 8080 (#0)\r\n> GET /webapp HTTP/1.1\r\n> Host: localhost:8080\r\n> User-Agent: curl/7.76.1\r\n> Accept: */*\r\n>\r\n* Mark bundle as not supporting multiuse\r\n< HTTP/1.1 404 Not Found\r\n```\r\n\r\nLogs:\r\n```\r\n':authority', 'localhost:8080'\r\n':path', '/webapp'\r\n':method', 'GET'\r\n'user-agent', 'curl/7.76.1'\r\n'accept', '*/*'\r\n\r\n[2021-04-20 17:05:53.688][48][debug][http] [source/common/http/filter_manager.cc:779] [C0][S7150000844861950898] request end stream\r\n[2021-04-20 17:05:53.689][48][debug][router] [source/common/router/router.cc:379] [C0][S7150000844861950898] no cluster match for URL '/webapp'\r\n```\r\n\r\n2. Sending request `/webapp?one=some-value` doesn't work and returns **404 Not Found** while we expected it to return response from `webapp-a` cluster and **200 OK**:\r\n```\r\n/ # curl -v 'localhost:8080/webapp?one=some-value'\r\n*   Trying ::1:8080...\r\n* connect to ::1 port 8080 failed: Connection refused\r\n*   Trying 127.0.0.1:8080...\r\n* Connected to localhost (127.0.0.1) port 8080 (#0)\r\n> GET /webapp?one=some-value HTTP/1.1\r\n> Host: localhost:8080\r\n> User-Agent: curl/7.76.1\r\n> Accept: */*\r\n>\r\n* Mark bundle as not supporting multiuse\r\n< HTTP/1.1 404 Not Found\r\n```\r\n\r\nLogs:\r\n```\r\n[2021-04-20 17:07:21.081][43][debug][http] [source/common/http/conn_manager_impl.cc:882] [C1][S16647278409349704461] request headers complete (end_stream=true):\r\n':authority', 'localhost:8080'\r\n':path', '/webapp?one=some-value'\r\n':method', 'GET'\r\n'user-agent', 'curl/7.76.1'\r\n'accept', '*/*'\r\n\r\n[2021-04-20 17:07:21.081][43][debug][http] [source/common/http/filter_manager.cc:779] [C1][S16647278409349704461] request end stream\r\n[2021-04-20 17:07:21.081][43][debug][router] [source/common/router/router.cc:379] [C1][S16647278409349704461] no cluster match for URL '/webapp?one=some-value'\r\n```\r\n\r\n2. Sending request `/webapp?two=some-other-value` doesn't work and returns **404 Not Found** while we expected it to return response from `webapp-b` cluster and **200 OK**:\r\n```\r\n/ # curl -v 'localhost:8080/webapp?two=some-other-value'\r\n*   Trying ::1:8080...\r\n* connect to ::1 port 8080 failed: Connection refused\r\n*   Trying 127.0.0.1:8080...\r\n* Connected to localhost (127.0.0.1) port 8080 (#0)\r\n> GET /webapp?two=some-other-value HTTP/1.1\r\n> Host: localhost:8080\r\n> User-Agent: curl/7.76.1\r\n> Accept: */*\r\n>\r\n* Mark bundle as not supporting multiuse\r\n< HTTP/1.1 404 Not Found\r\n```\r\n\r\nLogs:\r\n```\r\n[2021-04-20 17:08:35.663][50][debug][http] [source/common/http/conn_manager_impl.cc:882] [C2][S4902632338841187132] request headers complete (end_stream=true):\r\n':authority', 'localhost:8080'\r\n':path', '/webapp?two=some-other-value'\r\n':method', 'GET'\r\n'user-agent', 'curl/7.76.1'\r\n'accept', '*/*'\r\n\r\n[2021-04-20 17:08:35.663][50][debug][http] [source/common/http/filter_manager.cc:779] [C2][S4902632338841187132] request end stream\r\n[2021-04-20 17:08:35.663][50][debug][router] [source/common/router/router.cc:379] [C2][S4902632338841187132] no cluster match for URL '/webapp?two=some-other-value'\r\n```\r\n\r\n3. Sending request `/webapp?one=first-value&two=second-value` doesn't work and returns **200 OK** while we expected it to fail with **404 Not Found**:\r\n```\r\n# curl -v 'localhost:8080/webapp?one=first-value&two=second-value'\r\n*   Trying ::1:8080...\r\n* connect to ::1 port 8080 failed: Connection refused\r\n*   Trying 127.0.0.1:8080...\r\n* Connected to localhost (127.0.0.1) port 8080 (#0)\r\n> GET /webapp?one=first-value&two=second-value HTTP/1.1\r\n> Host: localhost:8080\r\n> User-Agent: curl/7.76.1\r\n> Accept: */*\r\n>\r\n* Mark bundle as not supporting multiuse\r\n< HTTP/1.1 200 OK\r\n```\r\n\r\nLogs:\r\n```\r\n[2021-04-20 17:10:07.696][43][debug][http] [source/common/http/conn_manager_impl.cc:882] [C3][S8310434087266513830] request headers complete (end_stream=true):\r\n':authority', 'localhost:8080'\r\n':path', '/webapp?one=first-value&two=second-value'\r\n':method', 'GET'\r\n'user-agent', 'curl/7.76.1'\r\n'accept', '*/*'\r\n\r\n[2021-04-20 17:10:07.696][43][debug][http] [source/common/http/filter_manager.cc:779] [C3][S8310434087266513830] request end stream\r\n[2021-04-20 17:10:07.696][43][debug][router] [source/common/router/router.cc:445] [C3][S8310434087266513830] cluster 'webapp-a' match for URL '/webapp?one=first-value&two=second-value'\r\n```\r\n\r\nPlease advise.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16084/comments",
    "author": "aqua777",
    "comments": [
      {
        "user": "asraa",
        "created_at": "2021-04-23T00:45:48Z",
        "body": "Can you verify that matching with a config that requires no query params gives 200 first?"
      },
      {
        "user": "aqua777",
        "created_at": "2021-04-26T10:59:47Z",
        "body": "> Can you verify that matching with a config that requires no query params gives 200 first?\r\n\r\nHi @asraa , I am not entirely sure what do you mean ?\r\n\r\nThe configuration which I am trying to build DO REQUIRE query params for all requests. Without query params we expect Envoy to find no match and to respond with 404 Not Found, which it does:\r\n```\r\n# curl -v localhost:8080/webapp\r\n*   Trying 127.0.0.1:8080...\r\n* Connected to localhost (127.0.0.1) port 8080 (#0)\r\n> GET /webapp HTTP/1.1\r\n> Host: localhost:8080\r\n> User-Agent: curl/7.76.1\r\n> Accept: */*\r\n>\r\n* Mark bundle as not supporting multiuse\r\n< HTTP/1.1 404 Not Found\r\n```"
      },
      {
        "user": "aqua777",
        "created_at": "2021-05-05T17:51:34Z",
        "body": "Can anyone comment whether what I described above is the expected behavior or am I missing some configuration settings?\r\nI'll be happy to provide more details and/or answer any question if needed."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-04T20:25:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-12T00:01:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 16062,
    "title": "Stats tag regex not capturing substring ending in non-dot character",
    "created_at": "2021-04-19T11:34:15Z",
    "closed_at": "2021-05-27T16:13:32Z",
    "labels": [
      "question",
      "stale",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16062",
    "body": "*Description*:\r\nWe are trying to utilize the `stats_config` feature to extract/expose some metrics.\r\n\r\nMetric page outputs -\r\n```\r\ncluster.appname__someinfo__someotherinfo__anotherinfo.assignment_stale: 0\r\n.\r\n.\r\n```\r\n\r\nWe are trying to match the `appname` which is between `cluster.` and the first `_`. And the regex we tried is `^cluster\\\\.((.*?)_)`, this gives no output.\r\n\r\nWe get everything between two `.`'s when we use `^cluster\\\\.((.*?)\\\\.)`, but we need to match the substring.\r\n\r\nConfig -\r\n```\r\nstats_config:\r\n  use_all_default_tags: false\r\n  stats_tags:\r\n  - tag_name: \"app\"\r\n    regex: \"^cluster\\\\.((.*?)_)\"\r\n```\r\n\r\nIs it possible to do what we're trying?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16062/comments",
    "author": "VigneshSP94",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-05-20T16:05:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-05-27T16:13:31Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15991,
    "title": "Importing multiple Lua scripts through LDS/RDS API?",
    "created_at": "2021-04-15T16:18:22Z",
    "closed_at": "2021-09-04T12:01:25Z",
    "labels": [
      "question",
      "stale",
      "investigate",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15991",
    "body": "*Title*: Possible to import multiple Lua scripts through LDS/RDS API without requiring them to be available on Envoy's filesystem?\r\n\r\n*Description*:\r\nI configured my LDS to send a Lua V3 HTTP filter to Envoy v1.17.1 in the following format:\r\n\r\n```yaml\r\nhttp_filters:\r\n- name: envoy.filters.http.lua\r\n  typed_config:\r\n    '@type': type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n    inline_code: |\r\n      local lib = require \"mylib\"\r\n      ...\r\n    source_codes:\r\n      mylib.lua:\r\n        inline_string: ...\r\n      mylib_deps/otherlib.lua:\r\n        inline_string: ...\r\n```\r\n\r\nBut Envoy outputs the following:\r\n```\r\n[2021-04-15 16:11:52.430][1][warning][config] [source/common/config/http_subscription_impl.cc:123] REST config for /v3/discovery:listeners rejected: Error adding/updating listener(s) service_listener: script load error: [string \"local lib = require \"mylib\"...\"]:1: module 'mylib' not found:\r\n      no field package.preload['mylib']\r\n      no file './mylib.lua'\r\n      no file '/tmp/tmp.8O2loKmZrQ/luajit/share/luajit-2.1.0-beta3/mylib.lua'\r\n      no file '/usr/local/share/lua/5.1/mylib.lua'\r\n      no file '/usr/local/share/lua/5.1/mylib/init.lua'\r\n      no file '/tmp/tmp.8O2loKmZrQ/luajit/share/lua/5.1/mylib.lua'\r\n      no file '/tmp/tmp.8O2loKmZrQ/luajit/share/lua/5.1/mylib/init.lua'\r\n      no file './mylib.so'\r\n      no file '/usr/local/lib/lua/5.1/mylib.so'\r\n      no file '/tmp/tmp.8O2loKmZrQ/luajit/lib/lua/5.1/mylib.so'\r\n      no file '/usr/local/lib/lua/5.1/loadall.so'\r\n```\r\n\r\nHow can I reference `source_codes` files from my `GLOBAL` Lua script?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15991/comments",
    "author": "v1b1",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2021-04-15T18:24:19Z",
        "body": "@mattklein123 any thoughts as Lua owner?"
      },
      {
        "user": "mattklein123",
        "created_at": "2021-04-15T19:03:22Z",
        "body": "I honestly have no idea. Perhaps there is some load ordering issue here? cc @dio "
      },
      {
        "user": "v1b1",
        "created_at": "2021-04-29T18:11:36Z",
        "body": "Hi @dio any insight? I upgraded to v1.18.2 but no luck."
      },
      {
        "user": "v1b1",
        "created_at": "2021-05-26T18:10:10Z",
        "body": "@mattklein123 @dio what is the expected behaviour? Are all of the Lua scripts under `source_codes` supposed to be saved to a temporary directory?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-06-25T20:00:58Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "v1b1",
        "created_at": "2021-06-28T12:43:58Z",
        "body": "Hi @dio I'd appreciate any help on this. =)"
      },
      {
        "user": "v1b1",
        "created_at": "2021-06-28T18:31:22Z",
        "body": "From what I've been able to discern from the source, `per_lua_code_setups_map_` in `lua_filter.cc` iterates over the `proto_config` upon construction and stores the Lua scripts in-memory. As a result it's impossible for any Lua script to import/require another Lua file without that file already existing in Envoy's local filesystem.\r\n\r\n- Is it possible to save these Lua scripts to a temporary directory visible by LuaJIT upon filter construction?\r\n- Will there be any issues with such an approach, e.g. two copies of the same Lua script used by a route? (One loaded via LuaJIT using the require syntax as well as being configured as a `source_codes` map value)"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-28T20:01:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "v1b1",
        "created_at": "2021-07-28T21:30:41Z",
        "body": "I think I'd like to help with this, but not sure on an appropriate approach:\r\n\r\n1. Save all `source_codes` to the filesystem during configuration so that any module references are loaded into LuaJIT correctly\r\n2. Optionally configure each `source_codes` element with a property like `export_module_name: foobar` such that only those files will be saved to the filesystem so that `local lib = require \"foobar\"` will work as expected"
      },
      {
        "user": "htuch",
        "created_at": "2021-07-29T04:12:23Z",
        "body": "I'm not really an expert here on Lua, but it would be ideal to avoid saving xDS state to the filesystem. Envoy should generally be able to run in   a diskless environment. It would be preferable to make this work via appropriate interaction with the LuaJIT runtime."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-28T08:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-09-04T12:01:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15964,
    "title": "can envoy enable to auto add token to header after validate token to request?",
    "created_at": "2021-04-14T02:02:14Z",
    "closed_at": "2021-05-23T04:12:38Z",
    "labels": [
      "enhancement",
      "question",
      "stale",
      "area/jwt_authn"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15964",
    "body": "when I use *AuthorizationPolicy* to authenticate a jwt token, \r\nstep1, client to jwt server to generate token using private key and return token\r\nstep2, client requests with token and verify token using public key through  Api gateway\r\nstep3, after verfication, access to protected api to finish jwt check\r\n\r\nI wonder If envoy can add token to header automatically in step2, in that case ,client won't take token to header?\r\n\r\nI know **EnvoyFilter** with *Lua* extension can add header automatically before request to api, If envoy can do this by any configration in any policy ?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15964/comments",
    "author": "jinglina",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2021-04-14T15:51:18Z",
        "body": "Not sure I fully understand the question, but maybe @qiwzhang can help."
      },
      {
        "user": "jinglina",
        "created_at": "2021-04-16T01:13:02Z",
        "body": "Let's put it in that way:  After verifing authorization in **Request authentication** , envoy knows the jwksUri or jwks; To verify author\r\npolicy ,like **AuthorizationPolicy** which need token , can token to added to header automatically by envoy throgh some rule?"
      },
      {
        "user": "jinglina",
        "created_at": "2021-04-16T01:13:54Z",
        "body": "Or any further efforts to it ?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-05-16T04:09:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-05-23T04:12:38Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15869,
    "title": "Connecting with IP when the listener is configured with SNI",
    "created_at": "2021-04-07T13:10:16Z",
    "closed_at": "2021-04-20T01:59:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15869",
    "body": "We are PoC-ing envoy to use it as our load balancer, we are trying to utilize the SNI feature and it works perfectky fine.\r\n\r\nBut when we try to connect to IP of the listener that is configured with SNI, we get `no matching filter chain found` and the request fails.\r\n\r\nWe use curl with option `-k` to do this.\r\n\r\nBelow is the minimal configuration to do this\r\n\r\n```\r\n- \"@type\": type.googleapis.com/envoy.config.listener.v3.Listener\r\n  name: demo-https\r\n  address:\r\n    socket_address:\r\n      address: 100.x.x.x\r\n      port_value: 443\r\n  listener_filters:\r\n  - name: \"envoy.filters.listener.tls_inspector\"\r\n    typed_config: {}\r\n  filter_chains:\r\n          #- use_proxy_proto: true\r\n  - filter_chain_match:\r\n      server_names: [\"*.example.net\", \"example.net\"]\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n        cluster: demo-https-cluster\r\n        common_tls_context:\r\n          tls_certificates:\r\n          - certificate_chain: { filename: \"/etc/certs/asterisk.example.net.chain\" }\r\n            private_key: { filename: \"/etc/certs/asterisk.example.net.key\" }\r\n    filters:\r\n    - name: envoy.filters.network.http_connection_manager\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n        stat_prefix: http\r\n        cluster: demo-https-cluster\r\n        rds:\r\n          route_config_name: demo-rds\r\n          config_source:\r\n            path: \"/etc/envoy/rds/demo-rds.yaml\"\r\n        access_log:\r\n        - name: log\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n            path: \"/var/log/envoy.log\"\r\n            typed_json_format: *json_Format\r\n        http_filters:\r\n        - name: envoy.router\r\n          config: {}\r\n```\r\nMy questions are : \r\n\r\n1. Will connecting with IP work if SNI is enabled and specified?\r\n2. If it does not support, any suggestions to use multiple certificates like haproxy supports?\r\n\r\nThank you !",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15869/comments",
    "author": "VigneshSP94",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-04-08T17:11:29Z",
        "body": "I believe you'd either need to include the ip as one of the SNIs (assuming the client sets the IP as the SNI) or use a second filter chain that doesn't try to match on SNI."
      },
      {
        "user": "lambdai",
        "created_at": "2021-04-08T21:12:41Z",
        "body": "Or you can create another filter chain with empty `server_names` to expclity accept either no-SNI or `unknown-SNI`"
      },
      {
        "user": "VigneshSP94",
        "created_at": "2021-04-13T15:54:22Z",
        "body": "> Or you can create another filter chain with empty `server_names` to expclity accept either no-SNI or `unknown-SNI`\n\nThis worked, thanks for your humble help !"
      }
    ]
  },
  {
    "number": 15698,
    "title": "Getting error: NR filter_chain_not_found and 404 NR route_not_found - \"-\" on my istio ingressgateway and grpc ingressgateway",
    "created_at": "2021-03-26T00:11:06Z",
    "closed_at": "2021-05-06T04:01:22Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15698",
    "body": "Hi, \r\nI am getting the below error on new istio 1.9.1 on kubernetes, can you please help:\r\n\r\n[2021-03-25T23:59:51.372Z] \"- - -\" 0 NR filter_chain_not_found - \"-\" 0 0 0 - \"-\" \"-\" \"-\" \"-\" \"-\" - - 100.97.92.14:8443 172.20.49.60:57605 - -\r\n[2021-03-26T00:01:25.979Z] \"GET /.env HTTP/1.1\" 404 NR route_not_found - \"-\" 0 0 0 - \"100.117.94.128\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\" \"04cc6a38-2d53-9a7c-8f22-01703fd22eb3\" \"34.213.68.78\" \"-\" - - 100.97.92.14:8080 100.117.94.128:49723 - -\r\n[2021-03-26T00:01:26.592Z] \"POST / HTTP/1.1\" 404 NR route_not_found - \"-\" 0 0 0 - \"100.127.250.64\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\" \"44d05f01-267d-96ae-ad98-d7bd9c2a8043\" \"34.213.68.78\" \"-\" - - 100.97.92.14:8080 100.127.250.64:29826 - -\r\n2021-03-26T00:03:54.637718Z\twarning\tenvoy config\tIgnoring unwatched type URL type.googleapis.com/envoy.config.route.v3.RouteConfiguration\r\n[2021-03-26T00:04:40.932Z] \"GET / HTTP/1.1\" 404 NR route_not_found - \"-\" 0 0 0 - \"100.121.53.192\" \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:60.0) Gecko/20100101 Firefox/60.0\" \"07796957-b074-9739-8e13-a8b386830730\" \"52.26.176.236\" \"-\" - - 100.97.92.14:8080 100.121.53.192:33167 - -\r\n[2021-03-26T00:05:04.934Z] \"GET /robots.txt HTTP/1.1\" 404 NR route_not_found - \"-\" 0 0 0 - \"100.121.53.192\" \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:60.0) Gecko/20100101 Firefox/60.0\" \"b2021521-918d-9edc-9faa-eb0d46a8ced0\" \"52.26.176.236\" \"-\" - - 100.97.92.14:8080 100.121.53.192:4398 - -\r\n[2021-03-26T00:05:32.432Z] \"POST /Adminecb01a34/Login.php HTTP/1.1\" 404 NR route_not_found - \"-\" 0 0 0 - \"100.121.53.192\" \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:60.0) Gecko/20100101 Firefox/60.0\" \"694e5583-7f0a-925d-a437-14b429cb00d6\" \"52.26.176.236\" \"-\" - - 100.97.92.14:8080 100.121.53.192:41217 - -\r\n[2021-03-26T00:05:56.439Z] \"GET / HTTP/1.1\" 404 NR route_not_found - \"-\" 0 0 0 - \"100.121.53.192\" \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0\" \"e412927f-3fc7-95a6-a2bc-270b4e883af5\" \"52.26.176.236\" \"-\" - - 100.97.92.14:8080 100.121.53.192:60886 - -\r\n\r\n```\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: Gateway\r\nmetadata:\r\n  name: api-key-service\r\n  annotations:\r\n   type: grpc\r\n#   ingress.kubernetes.io/force-ssl-redirect: \"true\"\r\n#   kubernetes.io/ingress.class: \"istio-gateway\"\r\nspec:\r\n  selector:\r\n    istio: grpc-ingressgateway\r\n  servers:\r\n  - port:\r\n      number: 80\r\n      name: http3\r\n      protocol: gRPC\r\n    hosts:\r\n    - \"*.cap-staging.saturn.expressplay-npd.net\"\r\n    - \"*.staging.expressplay.com\"\r\n    tls:\r\n      httpsRedirect: true\r\n  - port:\r\n      number: 443\r\n      name: https3\r\n      protocol: HTTPS\r\n    tls:\r\n      mode: SIMPLE\r\n      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt\r\n      privateKey: /etc/istio/ingressgateway-certs/tls.key\r\n    hosts:\r\n    - \"*.cap-staging.xyz-npd.net\"\r\n    - \"*.staging.xyz.com\"\r\n---\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: VirtualService\r\nmetadata:\r\n  name: api-key-service\r\nspec:\r\n  hosts:\r\n  - \"api-key.cap-staging.saturn.expressplay-npd.net\"\r\n  - \"api-key.staging.expressplay.com\"\r\n#  - \"*\"\r\n  gateways:\r\n  - api-key-service\r\n  http:\r\n  - route:\r\n    - destination:\r\n        host: api-key-service\r\n        port:\r\n          number: 80\r\n    corsPolicy:\r\n      allowOrigin:\r\n        - \"*\"\r\n      allowMethods:\r\n        - POST\r\n        - GET\r\n        - OPTIONS\r\n        - PUT\r\n        - DELETE\r\n      allowHeaders:\r\n        - grpc-timeout\r\n        - content-type\r\n        - keep-alive\r\n        - user-agent\r\n        - cache-control\r\n        - content-type\r\n        - content-transfer-encoding\r\n        - custom-header-1\r\n        - x-accept-content-transfer-encoding\r\n        - x-accept-response-streaming\r\n        - x-user-agent\r\n        - x-grpc-web\r\n        - authorization\r\n      maxAge: 1728s\r\n      exposeHeaders:\r\n        - custom-header-1\r\n        - grpc-status\r\n        - grpc-message\r\n        - authorization\r\n      allowCredentials: true\r\n---\r\nenvoy filter:\r\n---\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: api-key-service\r\nspec:\r\n  workloadLabels:\r\n    app: api-key-service\r\n  filters:\r\n  - listenerMatch:\r\n      listenerType: ANY\r\n      listenerProtocol: HTTP\r\n    insertPosition:\r\n      index: FIRST\r\n    filterType: HTTP\r\n    filterName: \"envoy.grpc_web\"\r\n    filterConfig: {}\r\n\r\n---\r\nAny idea, will be helpful",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15698/comments",
    "author": "akhilgoel09",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2021-03-29T20:39:53Z",
        "body": "NR filter_chain_not found\r\nIt seems the request is hitting 100.97.92.14:8443 but I don't see the 8443 port defined in istio Gateway CR"
      },
      {
        "user": "lambdai",
        "created_at": "2021-03-29T20:45:23Z",
        "body": "404 NR route_not_found\r\nIt's either no Envoy http route found, or http route is found but the destination cluster is not ready.\r\nYou may want to check if the RDS/CDS/EDS is SYNCed. Also double check if the SYNCed route matches your http/grpc request."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-29T00:03:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-05-06T04:01:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15643,
    "title": "Universal dynamic configuration capability",
    "created_at": "2021-03-24T13:56:42Z",
    "closed_at": "2021-05-01T08:01:27Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15643",
    "body": "*Description*:\r\n>It seems reasonable to add an universal dynamic configuration capability to Envoy. In addition to not support dynamic configuration and complex value types, I found that RTDS is very suitable for this feature.\r\n>I have implemented a `DynamicRtdsSubscription class` which can accept RTDS resource subscriptions at runtime and call the binding callbacks when the resources update. Since the original RTDS value is a `Struct`, I can convert it to my self-defined type in the callbacks. It works for me and I'm wondering if this feature could be added to Envoy.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15643/comments",
    "author": "WeavingGao",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-03-24T14:11:39Z",
        "body": "Can you speak a bit to the specific use case you have in mind? Between RTDS and ECDS we have plenty of options for dynamic configuration, so your use case might already be solved by them. "
      },
      {
        "user": "WeavingGao",
        "created_at": "2021-03-25T01:53:22Z",
        "body": "I developed a common component that can be used in L4 or L7 Filter or any other component. This component relies on complex dynamic configurations. These configurations have nothing to do with cluster or listener, so we found that RTDS is relatively suitable. Any other suggestions?  "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-24T04:02:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-05-01T08:01:26Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15512,
    "title": "Question: What is the maximum number of routes",
    "created_at": "2021-03-16T08:24:07Z",
    "closed_at": "2021-04-23T00:03:25Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15512",
    "body": "We're using Envoy 15.3 with XDS cluster configuration. 12 vhosts and 1663 routes per vhost. It's for a migration project and that number should go down later. Before that we had a lot fewer routes. We noticed that the first `232` routes work but the rest didn't. I googled a lot but I couldn't find any indication what the limit with regards to configuration are. I tried reading a bit of the C++ code but couldn't find it there. But that's not a good indication of whether it is there or not :P \r\n\r\nSo my question is, what technical limits does Envoy have? All I could find are connection based limitations which I get is important for a proxy. But what are the configuration limitations or where could I find them?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15512/comments",
    "author": "obeleh",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-03-16T20:26:05Z",
        "body": "There is no explicit limit on the number of routes, other than memory usage and the fact that right now route picking is a linear FIFO scan."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-16T00:03:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-23T00:03:25Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15440,
    "title": "How to configure envoy short link just as the configurtion keepalive in nginx",
    "created_at": "2021-03-11T14:23:50Z",
    "closed_at": "2021-04-18T12:01:41Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15440",
    "body": "I use envoy as a forward proxy. How can I set up short link transmission, that is, when the data transmission is completed, the link will automatically end. Just like the keepalive attribute of nginx",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15440/comments",
    "author": "jtou100",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-11T12:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-18T12:01:40Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15378,
    "title": "Could Lua Filter use Mysql?",
    "created_at": "2021-03-09T03:46:00Z",
    "closed_at": "2021-04-17T08:01:22Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15378",
    "body": "I want to use Lua script get data from Mysql in Lua Filter. \r\nCould envoy support that? And how to?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15378/comments",
    "author": "ikingye",
    "comments": [
      {
        "user": "dio",
        "created_at": "2021-03-09T10:42:35Z",
        "body": "I think it is not supported for now. Lua is HTTP filter only for now."
      },
      {
        "user": "ikingye",
        "created_at": "2021-03-09T12:08:19Z",
        "body": "> I think it is not supported for now. Lua is HTTP filter only for now.\r\n\r\nCan I extend luasql in envoy?"
      },
      {
        "user": "ikingye",
        "created_at": "2021-03-10T06:51:43Z",
        "body": "How Can I import luasql.mysql in Envoy Lua Filter?\r\n\r\n```\r\nproxy_1  | script load error: [string \"...\"]:2: module 'luasql.mysql' not found:\r\nproxy_1  | \tno field package.preload['luasql.mysql']\r\nproxy_1  | \tno file './luasql/mysql.lua'\r\nproxy_1  | \tno file '/tmp/tmp.ymE0JeD4gu/luajit/share/luajit-2.1.0-beta3/luasql/mysql.lua'\r\nproxy_1  | \tno file '/usr/local/share/lua/5.1/luasql/mysql.lua'\r\nproxy_1  | \tno file '/usr/local/share/lua/5.1/luasql/mysql/init.lua'\r\nproxy_1  | \tno file '/tmp/tmp.ymE0JeD4gu/luajit/share/lua/5.1/luasql/mysql.lua'\r\nproxy_1  | \tno file '/tmp/tmp.ymE0JeD4gu/luajit/share/lua/5.1/luasql/mysql/init.lua'\r\nproxy_1  | \tno file './luasql/mysql.so'\r\nproxy_1  | \tno file '/usr/local/lib/lua/5.1/luasql/mysql.so'\r\nproxy_1  | \tno file '/tmp/tmp.ymE0JeD4gu/luajit/lib/lua/5.1/luasql/mysql.so'\r\nproxy_1  | \tno file '/usr/local/lib/lua/5.1/loadall.so'\r\nproxy_1  | \tno file './luasql.so'\r\nproxy_1  | \tno file '/usr/local/lib/lua/5.1/luasql.so'\r\nproxy_1  | \tno file '/tmp/tmp.ymE0JeD4gu/luajit/lib/lua/5.1/luasql.so'\r\nproxy_1  | \tno file '/usr/local/lib/lua/5.1/loadall.so'\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-10T04:02:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-17T08:01:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15343,
    "title": "[Question] Send a local response without closing the connection",
    "created_at": "2021-03-06T07:07:34Z",
    "closed_at": "2021-04-16T00:03:07Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15343",
    "body": "*Title*: *Send a local response without closing the connection*\r\n\r\n*Description*:\r\nI have written a custom http filter for web socket connections to rate limit based on the number of frames sent through a connection. The filter basically publishes metadata needed for rate limiting and analytics through a gRPC bidi stream. I have a requirement to send a local reply from the filter when the connection is rate limited.\r\n\r\neg: Message throttled out due to subscription level throttling limit reached.\r\n\r\nBut since this is a websocket connection, I don't want to close the connection. Just send local responses without closing the connection. Envoy ext_auth and ratelimit filters use `sendLocalReply()` but it closes the connection. Is my requirement possible with the current envoy implementation in filter level without modifying envoy source ? Thanks\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15343/comments",
    "author": "NomadXD",
    "comments": [
      {
        "user": "dio",
        "created_at": "2021-03-07T11:06:30Z",
        "body": "cc. @alyssawilk @snowp "
      },
      {
        "user": "alyssawilk",
        "created_at": "2021-03-08T13:51:53Z",
        "body": "I think I need a bit more information about what you're trying to do here.\r\n\r\nIf you're proxying websocket, you're forwarding headers and streaming body upstrema, then proxying headers and streaming websocket data from upstream.\r\n\r\nsendLocalReply sends headers and body and end stream downstream.  There's not much point keeping the connection open once you've sent end stream as the upstream can't send more data.  Also having sendLocalReply send headers isn't going to play well with upstream sending response headers.    Are you just wanting to send some websocket payload downstream?   That might be doable but there will be some complications."
      },
      {
        "user": "NomadXD",
        "created_at": "2021-03-08T21:23:54Z",
        "body": "@alyssawilk Yeah, keeping the connection open after sendLocalReply() doesn't make any sense that way. So how do I send some websocket payload downstream ? By calling encodeData() from a filter ? If it's doable with my current knowledge about envoy (implemented some http filters with grpc async client), would like to give it a try. Thanks "
      },
      {
        "user": "alyssawilk",
        "created_at": "2021-03-09T13:43:27Z",
        "body": "yeah, this is where you have to be a bit careful.  Sending data before upstream has sent response headers is going to cause problems.  If you make sure you delay until your filter has seen response headers, you should be able to addEncodedData/injectEncodedDataToFilterChain to add payload.  I'd suggest reading up on the differences between the two and integration testing to make sure you're happy with the results but I think they'll handle the work you're looking for."
      },
      {
        "user": "NomadXD",
        "created_at": "2021-03-09T17:33:08Z",
        "body": "@alyssawilk yeah probably my filter sees the response headers first because for websocket connections,  response headers are sent at the connection initialization time with 101 upgrade accepted. And I'm talking about a some point later in time that some websocket frames have already sent through envoy and when the throttle limit (5000 frames per minute) is reached, sending a websocket payload saying that the limit is reached without closing the connection. Thanks a lot for the help. Will go through the integration tests and see."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-08T20:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-16T00:03:06Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15287,
    "title": "Globally limiting maximum concurrent h/2 requests per endpoint",
    "created_at": "2021-03-03T18:39:48Z",
    "closed_at": "2021-04-10T08:01:36Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15287",
    "body": "*Title*: *Globally limiting maximum concurrent h/2 requests per endpoint*\r\n\r\n*Description*:\r\nHi,\r\n\r\nI’m trying to limit the concurrent streams forwarded to the upstream endpoints, such that each endpoint sees at most N requests at any given time. (Upstream is a cluster of python grpc servers.)\r\n\r\nSince `http2_protocol_option` supports manually specifying `max_concurrent_streams`, I tried combining the option with the circuit breaker's max_connections configuration.\r\n\r\nSo far, I've come up with the following configuration:\r\n```yaml\r\n  clusters:\r\n    - name: processor_cluster\r\n      connect_timeout: 0.25s\r\n      circuit_breakers:\r\n        thresholds:\r\n        - max_pending_requests: 16384\r\n          max_connections: 1\r\n      typed_extension_protocol_options:\r\n        envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n          \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n          explicit_http_config:\r\n            http2_protocol_options:  # Forces HTTP/2 on upstream connection\r\n              max_outbound_frames: 160000\r\n              max_outbound_control_frames: 16000\r\n              max_concurrent_streams: 4\r\n      eds_cluster_config:\r\n        eds_config:\r\n          path: /<some-path>/edsconf.json\r\n          resource_api_version: \"V3\"\r\n        service_name: localservices\r\n      lb_policy: ROUND_ROBIN\r\n      type: EDS\r\n```\r\nThis configuration works well when there are only one downstream connection. That is, `upstream_rq_active` is capped at `max_concurrent_streams` * `NUM_OF_ENDPOINTS`.   \r\nHowever, when there are more than one downstream connection, `upstream_rq_active` peaks at `downstream_cx_active` * `max_concurrent_streams` * `NUM_OF_ENDPOINTS`, which clearly means that there are more than `MAX_CONCURRENT_STREAMS` active per endpoint.\r\n\r\nOne thing I noticed at this point was that there are actually more than one connection established to the endpoint, as seen by `upstream_cx_active`. Thus, I also tried limiting connection pool creation by specifying `max_connection_pools` in the circuit breaker configuration, but the subsequent requests made by other clients fails with `no healthy upstream` error in this case. This is also not desirable as we cannot determine the total number of clients that will be active before deployment.\r\n\r\nQuestion:\r\n> Is it possible to limit the maximum concurrent requests for each endpoint across multiple downstream connection?\r\n\r\nThanks.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15287/comments",
    "author": "puilp0502",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2021-03-03T19:35:14Z",
        "body": "You may mislead by the max_concurrent_streams. It's not protecting the endpoint. It's protecting the connection to that endpoint. IIRC max_concurrent_streams is max_concurrent_streams **per upstream connection** and that endpoint can have multiple connections from this Envoy.\r\n\r\nCould you look into circuit  breaker?\r\nThere are `max_connections` and `max_requests`. yet no concrete setting for per endpoint but that probably the right direction"
      },
      {
        "user": "puilp0502",
        "created_at": "2021-03-04T03:48:13Z",
        "body": "@lambdai Thank you for the answer!\r\n\r\nAlthough not satisfactory, limiting the maximum connections via `max_connections` combined with `--max-concurrency 1` option seems to do the trick:\r\n```yaml\r\n  clusters:\r\n    - name: processor_cluster\r\n      connect_timeout: 0.25s\r\n      circuit_breakers:\r\n        thresholds:\r\n        - max_pending_requests: 16384\r\n          max_connections: 1  # This, combined with --concurrency 1 does the trick\r\n      typed_extension_protocol_options:\r\n        envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n          \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n          explicit_http_config:\r\n            http2_protocol_options:  # Forces HTTP/2 on upstream connection\r\n              max_outbound_frames: 160000\r\n              max_outbound_control_frames: 16000\r\n              max_concurrent_streams: 4\r\n      eds_cluster_config:\r\n        eds_config:\r\n          path: /etc/apx/eds.conf\r\n          resource_api_version: \"V3\"\r\n        service_name: localservices\r\n      lb_policy: ROUND_ROBIN\r\n      type: EDS\r\n```\r\n\r\n"
      },
      {
        "user": "lambdai",
        "created_at": "2021-03-04T06:11:32Z",
        "body": "max_connection = 1 means you can only create 1 connection even you have 10 endpoints in that cluster.\r\n\r\nIt's probably not what you need if the cluster has more than 1 endpoint. Good for test though."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-03T08:01:31Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-10T08:01:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15271,
    "title": "Does envoy provide cluster-specific configurations for filters?",
    "created_at": "2021-03-03T03:12:02Z",
    "closed_at": "2021-03-24T01:47:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15271",
    "body": "*Title*: *cluster-specific configurations for filters*\r\n\r\n*Description*:\r\n>There seems to be no way to configure a cluster-specific configuration for a certain network or http filter. \r\nIs there any plan to support it?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15271/comments",
    "author": "WeavingGao",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2021-03-03T14:12:41Z",
        "body": "Some of the filter have per route config but I don't know of any plans to add per cluster config.  I think it'd be a reasonable feature to add though."
      },
      {
        "user": "WeavingGao",
        "created_at": "2021-03-04T06:30:09Z",
        "body": "> Some of the filter have per route config but I don't know of any plans to add per cluster config. I think it'd be a reasonable feature to add though.\r\n\r\nGot it. I'll try to implement it. Thanks a lot."
      }
    ]
  },
  {
    "number": 15257,
    "title": "configurate the \"adaptive concurrency filter\" for a specific cluster",
    "created_at": "2021-03-02T09:48:16Z",
    "closed_at": "2021-04-10T08:01:35Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15257",
    "body": "I have 3 clusters and only one need to be limited. How to configurate the adaptive concurrency filter for that cluster?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15257/comments",
    "author": "WeavingGao",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2021-03-02T14:49:59Z",
        "body": "cc @tonya11en "
      },
      {
        "user": "tonya11en",
        "created_at": "2021-03-03T19:40:35Z",
        "body": "You should be able to add it to the filter chain config parameter as part of the cluster proto like any other filter. The docs don't recommend doing this unless the cluster is representing a server local to the host. Adaptive concurrency doesn't work if you're using it on the egress in a sidecar configuration, since it needs to have full control over the number of active requests allowed through to whatever you're protecting. "
      },
      {
        "user": "WeavingGao",
        "created_at": "2021-03-04T02:35:57Z",
        "body": "> Adaptive concurrency doesn't work if you're using it on the egress in a sidecar configuration, since it needs to have full control over the number of active requests allowed through to whatever you're protecting.\r\n\r\nIs it possible to make a per route config/controller for adaptive concurrency filter like what local rate limit filter does?\r\n"
      },
      {
        "user": "tonya11en",
        "created_at": "2021-03-04T03:35:24Z",
        "body": "> Is it possible to make a per route config/controller for adaptive concurrency filter like what local rate limit filter does?\r\n\r\nIt's possible to implement, but you'd need to keep the system stationary. What I mean by that is you couldn't just have a separate concurrency limit for all the different routes, because they all use the same underlying resources and the control loop wouldn't work. The local rate limit filter is using token buckets and there's no control loop per-se, so it's able to just have different token buckets for different routes.\r\n\r\nI'm assuming you want this to address the different resource costs of various request types? If so, the way I'd go about this is assigning weights to different route prefixes and factoring that in to the admission decisions during the filter's decode step. I'm happy to help review a patch or implementation plan if you want to give it a go."
      },
      {
        "user": "WeavingGao",
        "created_at": "2021-03-04T06:27:14Z",
        "body": "> > Is it possible to make a per route config/controller for adaptive concurrency filter like what local rate limit filter does?\r\n> \r\n> It's possible to implement, but you'd need to keep the system stationary. What I mean by that is you couldn't just have a separate concurrency limit for all the different routes, because they all use the same underlying resources and the control loop wouldn't work. The local rate limit filter is using token buckets and there's no control loop per-se, so it's able to just have different token buckets for different routes.\r\n> \r\n> I'm assuming you want this to address the different resource costs of various request types? If so, the way I'd go about this is assigning weights to different route prefixes and factoring that in to the admission decisions during the filter's decode step. I'm happy to help review a patch or implementation plan if you want to give it a go.\r\n\r\nThank you for your reply. It really helps."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-03T08:01:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-10T08:01:34Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15206,
    "title": "route_config.virtual_hosts.domains can't route the Fully-Qualified Domain Names",
    "created_at": "2021-02-26T03:04:57Z",
    "closed_at": "2021-04-16T16:02:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15206",
    "body": "*Title*: *route_config.virtual_hosts.domains can't route the Fully-Qualified Domain Names*\r\n\r\n*Description*:\r\n>Describe the the desired behavior, what scenario it enables and how it\r\nwould be used.\r\n\r\nthe domains is \r\n```yaml\r\n        route_config:\r\n          name: \"nginx\"\r\n          virtual_hosts:\r\n          - name: nginx\r\n            domains:\r\n            - \"www.baidu.com\"\r\n```\r\n\r\nreq is : \r\nthe host is `www.baidu.com.`\r\n\r\n```bash\r\ncurl --location --request GET 'xxx:xxx' \\\r\n--header 'Host: www.baidu.com.'\r\n```\r\n\r\ni get the access log : \r\n```bash\r\n[2021-02-26T03:03:37.987Z] \"GET / HTTP/1.1\" 404 NR 0 0 0 - \"-\" \"PostmanRuntime/7.26.8\" \"053a4ee8-00e0-435a-b6a0-bd2abbb9d1d4\" \"www.baidu.com.\" \"-\"\r\n```\r\n\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15206/comments",
    "author": "sunnoy",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-02-27T18:56:33Z",
        "body": "Envoy can definitely route on FQDN, so there must be more to this story. Can you provide more debug logs from Envoy?"
      },
      {
        "user": "sunnoy",
        "created_at": "2021-03-10T08:50:52Z",
        "body": "debug logs\r\n\r\n```bash\r\n[2021-03-10 08:49:49.481][1][debug][main] [source/server/server.cc:177] flushing stats\r\n[2021-03-10 08:49:51.335][12][debug][conn_handler] [source/server/connection_handler_impl.cc:372] [C2] new connection\r\n[2021-03-10 08:49:51.336][12][debug][http] [source/common/http/conn_manager_impl.cc:268] [C2] new stream\r\n[2021-03-10 08:49:51.336][12][debug][http] [source/common/http/conn_manager_impl.cc:781] [C2][S9562914068908578895] request headers complete (end_stream=true):\r\n':authority', 'www.baidu.com.'\r\n':path', '/'\r\n':method', 'GET'\r\n'user-agent', 'curl/7.64.1'\r\n'accept', '*/*'\r\n\r\n[2021-03-10 08:49:51.336][12][debug][http] [source/common/http/conn_manager_impl.cc:1333] [C2][S9562914068908578895] request end stream\r\n[2021-03-10 08:49:51.336][12][debug][router] [source/common/router/router.cc:415] [C2][S9562914068908578895] no cluster match for URL '/'\r\n[2021-03-10 08:49:51.336][12][debug][http] [source/common/http/conn_manager_impl.cc:1475] [C2][S9562914068908578895] Sending local reply with details route_not_found\r\n[2021-03-10 08:49:51.336][12][debug][http] [source/common/http/conn_manager_impl.cc:1706] [C2][S9562914068908578895] encoding headers via codec (end_stream=true):\r\n':status', '404'\r\n'date', 'Wed, 10 Mar 2021 08:49:51 GMT'\r\n'server', 'envoy'\r\n\r\n[2021-03-10 08:49:52.364][12][debug][connection] [source/common/network/connection_impl.cc:558] [C2] remote close\r\n[2021-03-10 08:49:52.364][12][debug][connection] [source/common/network/connection_impl.cc:200] [C2] closing socket: 0\r\n[2021-03-10 08:49:52.364][12][debug][conn_handler] [source/server/connection_handler_impl.cc:86] [C2] adding to cleanup list\r\n[2021-03-10 08:49:54.481][1][debug][main] [source/server/server.cc:177] flushing stats\r\n[2021-03-10T08:49:51.336Z] \"GET / HTTP/1.1\" 404 NR 0 0 0 - \"-\" \"curl/7.64.1\" \"1ee7f395-3eef-4b62-8957-9413762cfe31\" \"www.baidu.com.\" \"-\"\r\n[2021-03-10 08:49:59.483][1][debug][main] [source/server/server.cc:177] flushing stats\r\n```\r\nconfig\r\n\r\n```bash\r\n        route_config:\r\n          name: \"nginx\"\r\n          virtual_hosts:\r\n          - name: nginx\r\n            domains:\r\n            # - \"www.baidu.com*\"\r\n            - \"www.baidu.com\"\r\n            routes:\r\n            - match:\r\n                prefix: \"/\"\r\n              route:\r\n                cluster: \"nginx\"\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-09T12:01:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-16T16:02:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15192,
    "title": "How to expose envoy outside the localhost",
    "created_at": "2021-02-25T14:18:58Z",
    "closed_at": "2021-02-26T03:44:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15192",
    "body": "Configured envoy as edge gateway using standalone exe, I can route and access the the https endpoints using localhost:PortNumber. I need to expose my localhost outside on the network so that other users on my domain access the endpoints. What configuration should I use to expose envoy-gateway on my localhost to the domain network?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15192/comments",
    "author": "RameshKandukuriB",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2021-02-26T02:30:11Z",
        "body": "I doubt anyone can answer this question. It depends on you network configuration and not on Envoy. You may need to do nothing or a lot depending on your situation."
      }
    ]
  },
  {
    "number": 15095,
    "title": "[Question] Thread safe way to use grpc bidi stream with envoy grpc async client inside a http filter. ",
    "created_at": "2021-02-18T09:21:35Z",
    "closed_at": "2021-03-28T20:01:16Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15095",
    "body": "*Title*: *Thread safe way to use grpc bidi stream with envoy grpc async client inside a http filter.* \r\n\r\n*Description*:\r\n\r\nI have written a http filter for web socket connections that is capturing websocket frames and publishing metadata related to these frames through a **grpc bidi stream with envoy grpc client**. According to the filter logic, there may be a possible race condition that will occur on `Grpc::AsyncStream stream_{};`. So the below code segment explains about the possible race condition. \r\n\r\nFrom `decodeData(Buffer::Instance& data,bool)` in my http filter, I'm calling a method in my helper class which publish metadata through a grpc bidi stream. Helper class has a pointer to `Grpc::AsyncStream stream_{};`\r\n\r\n```c++\r\nFilterDataStatus HttpSampleDecoderFilter::decodeData(Buffer::Instance& data, bool) {\r\n    HelperClass::publishMetadataAsync(data);\r\n    return FilterDataStatus::Continue;\r\n}\r\n```\r\nMy HelperClass implements `Grpc::AsyncStreamCallbacks` and has methods `onMessage()` and `onRemoteClose`. Also the `publishMetadataAsync` method too.\r\n\r\n```c++\r\nvoid HelperClass::publishMetaDataAsync(Buffer::Instance& data){\r\n    stream_->sendMessage(params);\r\n}\r\n\r\nvoid HelperClass::onRemoteClose(Grpc::Status::GrpcStatus status, const std::string& message) {\r\n    stream_ = nullptr;\r\n    ENVOY_LOG(trace, \"onRemoteClose : {} {}\", status, message);\r\n};\r\n\r\n```\r\nSo my concern is that when `onRemoteClose()` gets called when the remote upstream gets disconnected, there maybe a possibility of calling `stream_->sendMessage(params);` at the same time.Since I assign `stream_ = nullptr` in `onRemoteClose`, calling a method on nullptr makes a segmentation fault. And I don't think a simple if condition to check `stream_!= nullptr` will work because I assume these two happen in 2 different threads. I'm not sure whether it's a good choice to use mutex to handle `stream_` in a thread safe way because envoy encourages non-blocking thread model.\r\n\r\nSo my questions are, \r\n1. Is my concern valid ?\r\n2. How to handle this kind of a scenario by following envoy best practices ? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15095/comments",
    "author": "NomadXD",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-02-18T13:05:39Z",
        "body": "The threading here depends on where you're allocating the gRPC client, if it's done as part of the filter then it should end up on the same thread as the filter, so the null check should be sufficient. If you are trying to put it on another thread (e.g. the main thread) then you'd probably be posting the sends to the relevant dispatcher, which means that you'd also be okay with just a null check since the send and the remote close callback is still running on the same thread. "
      },
      {
        "user": "NomadXD",
        "created_at": "2021-02-19T12:40:05Z",
        "body": "@snowp I'm creating a **gRPC async client for each of the filter objects** that get created. So I think just doing a nullptr check will do the job. But since you mentioned about dispatching to main thread, I'm a little bit concerned about my implementation. So basically , according to my implementation **a gRPC bidi stream will be created for each of the filter objects that gets created.** Since the lifetime of a filter object is equal to the lifetime of a web socket connection, 1000 web socket connections means 1000 bidi streams from envoy to the external grpc service. Will that be a problem in terms of envoy memory management ? \r\nAlso I read this from envoy docs,\r\n \r\n`Maximum concurrent streams allowed for peer on one HTTP/2 connection. Valid values range from 1 to 2147483647 (2^31 - 1) and defaults to 2147483647.` \r\nSo I assumed I'm safe with having high number of concurrent streams in terms of connection management. But not sure in terms of memory management. Thanks a lot for the help. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-21T16:09:29Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-28T20:01:16Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15071,
    "title": "round robin load balancing issue on TCP_Proxy with envoy.filters.network.sni_cluster ",
    "created_at": "2021-02-17T08:54:27Z",
    "closed_at": "2021-04-29T08:01:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15071",
    "body": "Currently I am using Istio to form a service mesh on 2 k8s clusters, say, clusterA and clusterB.  15443 port is used for cross cluster communication. i.e. in clusterA, we can access the service in clusterB through the mtls port 15443 on the istio ingressgateway of clusterB.   The problem is the traffic is not evenly distributed to the work load of the service in clusterB. \r\n\r\ne.g. \r\n\r\n kubectl logs test-deploy-6df899c68d-fm7h6  -c istio-proxy  --context dev-svc-cluster | grep \"GET\" | wc -l\r\n11659\r\n kubectl logs test-deploy-6df899c68d-sbswr  -c istio-proxy  --context dev-svc-cluster | grep \"GET\" | wc -l\r\n19837\r\n\r\nMay I know there is anything I can do to make  round robin load balancing work in my case?  Thanks.\r\n\r\n\r\nhere is the listener configuration for port 15443 of istio ingressgateway of clusterB:\r\n\r\n    {\r\n        \"name\": \"0.0.0.0_15443\",\r\n        \"address\": {\r\n            \"socketAddress\": {\r\n                \"address\": \"0.0.0.0\",\r\n                \"portValue\": 15443\r\n            }\r\n        },\r\n        \"filterChains\": [\r\n            {\r\n                \"filterChainMatch\": {\r\n                    \"serverNames\": [\r\n                        \"*.local\"\r\n                    ]\r\n                },\r\n                \"filters\": [\r\n                    {\r\n                        \"name\": \"envoy.filters.network.sni_cluster\"\r\n                    },\r\n                    {\r\n                        \"name\": \"envoy.filters.network.rbac\",\r\n                        \"typedConfig\": {\r\n                            \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.rbac.v3.RBAC\",\r\n                            \"rules\": {\r\n                                \"policies\": {\r\n                                    \"ns[istio-system]-policy[allow-ingress-gateway]-rule[0]\": {\r\n                                        \"permissions\": [\r\n                                            {\r\n                                                \"andRules\": {\r\n                                                    \"rules\": [\r\n                                                        {\r\n                                                            \"any\": true\r\n                                                        }\r\n                                                    ]\r\n                                                }\r\n                                            }\r\n                                        ],\r\n                                        \"principals\": [\r\n                                            {\r\n                                                \"andIds\": {\r\n                                                    \"ids\": [\r\n                                                        {\r\n                                                            \"any\": true\r\n                                                        }\r\n                                                    ]\r\n                                                }\r\n                                            }\r\n                                        ]\r\n                                    }\r\n                                }\r\n                            },\r\n                            \"statPrefix\": \"tcp.\"\r\n                        }\r\n                    },\r\n                    {\r\n                        \"name\": \"istio.stats\",\r\n                        \"typedConfig\": {\r\n                            \"@type\": \"type.googleapis.com/udpa.type.v1.TypedStruct\",\r\n                            \"typeUrl\": \"type.googleapis.com/envoy.extensions.filters.network.wasm.v3.Wasm\",\r\n                            \"value\": {\r\n                                \"config\": {\r\n                                    \"configuration\": {\r\n                                        \"@type\": \"type.googleapis.com/google.protobuf.StringValue\",\r\n                                        \"value\": \"{\\n  \\\"metrics\\\": [\\n    {\\n      \\\"dimensions\\\": {\\n        \\\"source_cluster\\\": \\\"node.metadata['CLUSTER_ID']\\\",\\n        \\\"destination_cluster\\\": \\\"upstream_peer.cluster_id\\\"\\n      }\\n    }\\n  ]\\n}\\n\"\r\n                                    },\r\n                                    \"root_id\": \"stats_outbound\",\r\n                                    \"vm_config\": {\r\n                                        \"code\": {\r\n                                            \"local\": {\r\n                                                \"inline_string\": \"envoy.wasm.stats\"\r\n                                            }\r\n                                        },\r\n                                        \"runtime\": \"envoy.wasm.runtime.null\",\r\n                                        \"vm_id\": \"tcp_stats_outbound\"\r\n                                    }\r\n                                }\r\n                            }\r\n                        }\r\n                    },\r\n                    {\r\n                        \"name\": \"envoy.filters.network.tcp_proxy\",\r\n                        \"typedConfig\": {\r\n                            \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\",\r\n                            \"statPrefix\": \"BlackHoleCluster\",\r\n                            \"cluster\": \"BlackHoleCluster\",\r\n                            \"accessLog\": [\r\n                                {\r\n                                    \"name\": \"envoy.access_loggers.file\",\r\n                                    \"typedConfig\": {\r\n                                        \"@type\": \"type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\",\r\n                                        \"path\": \"/dev/stdout\",\r\n                                        \"logFormat\": {\r\n                                            \"textFormat\": \"[%START_TIME%] \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\" %RESPONSE_CODE% %RESPONSE_FLAGS% \\\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\\\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%REQ(X-FORWARDED-FOR)%\\\" \\\"%REQ(USER-AGENT)%\\\" \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME%\\n\"\r\n                                        }\r\n                                    }\r\n                                }\r\n                            ]\r\n                        }\r\n                    }\r\n                ]\r\n            }\r\n        ],\r\n        \"listenerFilters\": [\r\n            {\r\n                \"name\": \"envoy.filters.listener.tls_inspector\",\r\n                \"typedConfig\": {\r\n                    \"@type\": \"type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector\"\r\n                }\r\n            }\r\n        ],\r\n        \"trafficDirection\": \"OUTBOUND\",\r\n        \"accessLog\": [\r\n            {\r\n                \"name\": \"envoy.access_loggers.file\",\r\n                \"filter\": {\r\n                    \"responseFlagFilter\": {\r\n                        \"flags\": [\r\n                            \"NR\"\r\n                        ]\r\n                    }\r\n                },\r\n                \"typedConfig\": {\r\n                    \"@type\": \"type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\",\r\n                    \"path\": \"/dev/stdout\",\r\n                    \"logFormat\": {\r\n                        \"textFormat\": \"[%START_TIME%] \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\" %RESPONSE_CODE% %RESPONSE_FLAGS% \\\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\\\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%REQ(X-FORWARDED-FOR)%\\\" \\\"%REQ(USER-AGENT)%\\\" \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME%\\n\"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    },\r\n\r\nhere is cluster config\r\n\r\n    {\r\n        \"name\": \"outbound_.8000_._.test-svc.default.svc.cluster.local\",\r\n        \"type\": \"EDS\",\r\n        \"edsClusterConfig\": {\r\n            \"edsConfig\": {\r\n                \"ads\": {},\r\n                \"resourceApiVersion\": \"V3\"\r\n            },\r\n            \"serviceName\": \"outbound_.8000_._.test-svc.default.svc.cluster.local\"\r\n        },\r\n        \"connectTimeout\": \"10s\",\r\n        \"circuitBreakers\": {\r\n            \"thresholds\": [\r\n                {\r\n                    \"maxConnections\": 4294967295,\r\n                    \"maxPendingRequests\": 4294967295,\r\n                    \"maxRequests\": 4294967295,\r\n                    \"maxRetries\": 4294967295\r\n                }\r\n            ]\r\n        },\r\n        \"metadata\": {\r\n            \"filterMetadata\": {\r\n                \"istio\": {\r\n                    \"default_original_port\": 8000,\r\n                    \"services\": [\r\n                        {\r\n                            \"host\": \"test-svc.default.svc.cluster.local\",\r\n                            \"name\": \" test-svc\",\r\n                            \"namespace\": \"default\"\r\n                        }\r\n                    ]\r\n                }\r\n            }\r\n        },\r\n        \"filters\": [\r\n            {\r\n                \"name\": \"istio.metadata_exchange\",\r\n                \"typedConfig\": {\r\n                    \"@type\": \"type.googleapis.com/udpa.type.v1.TypedStruct\",\r\n                    \"typeUrl\": \"type.googleapis.com/envoy.tcp.metadataexchange.config.MetadataExchange\",\r\n                    \"value\": {\r\n                        \"protocol\": \"istio-peer-exchange\"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    },\r\n\r\nhere is the end points configuration:\r\n\r\n    {\r\n        \"name\": \"outbound_.8000_._.test-svc.default.svc.cluster.local\",\r\n        \"addedViaApi\": true,\r\n        \"hostStatuses\": [\r\n            {\r\n                \"address\": {\r\n                    \"socketAddress\": {\r\n                        \"address\": \"192.168.241.194\",\r\n                        \"portValue\": 8000\r\n                    }\r\n                },\r\n                \"stats\": [\r\n                    {\r\n                        \"name\": \"cx_connect_fail\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"cx_total\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_error\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_success\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_timeout\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"rq_total\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"31\",\r\n                        \"name\": \"cx_active\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"31\",\r\n                        \"name\": \"rq_active\"\r\n                    }\r\n                ],\r\n                \"healthStatus\": {\r\n                    \"edsHealthStatus\": \"HEALTHY\"\r\n                },\r\n                \"weight\": 1,\r\n                \"locality\": {}\r\n            },\r\n            {\r\n                \"address\": {\r\n                    \"socketAddress\": {\r\n                        \"address\": \"192.168.249.65\",\r\n                        \"portValue\": 8000\r\n                    }\r\n                },\r\n                \"stats\": [\r\n                    {\r\n                        \"name\": \"cx_connect_fail\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"cx_total\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_error\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_success\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_timeout\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"rq_total\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"30\",\r\n                        \"name\": \"cx_active\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"30\",\r\n                        \"name\": \"rq_active\"\r\n                    }\r\n                ],\r\n                \"healthStatus\": {\r\n                    \"edsHealthStatus\": \"HEALTHY\"\r\n                },\r\n                \"weight\": 1,\r\n                \"locality\": {}\r\n            }\r\n        ],\r\n        \"circuitBreakers\": {\r\n            \"thresholds\": [\r\n                {\r\n                    \"maxConnections\": 4294967295,\r\n                    \"maxPendingRequests\": 4294967295,\r\n                     \"maxRequests\": 4294967295,\r\n                    \"maxRetries\": 4294967295\r\n                },\r\n                {\r\n                    \"priority\": \"HIGH\",\r\n                    \"maxConnections\": 1024,\r\n                    \"maxPendingRequests\": 1024,\r\n                    \"maxRequests\": 1024,\r\n                    \"maxRetries\": 3\r\n                }\r\n            ]\r\n        }\r\n    },\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15071/comments",
    "author": "debbyku",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2021-02-18T05:41:45Z",
        "body": "This config looks good.\r\ngrep GET at log file is vague. Is there any metric, graph or access log that can drill down to \"gateway - 192.168.241.194\" and \"gateway - 192.168.249.65\" ? "
      },
      {
        "user": "debbyku",
        "created_at": "2021-02-18T06:19:26Z",
        "body": "grep GET is to return the access log like this \r\n\r\n[2021-02-18T06:16:33.248Z] \"GET /rt0/v1/api/racingtouch/ping HTTP/1.1\" 200 - \"-\" 0 18 1 1 \"192.168.177.192\" \"Mozilla/5.0 (pc-x86_64-linux-gnu) Siege/4.0.7\" \"82bf70f0-7b2c-9b69-aae0-94e3e963c989\" \"istio-ingressgateway.istio-system\" \"127.0.0.1:8000\" inbound|8000|| 127.0.0.1:53962 192.168.53.7:8000 192.168.177.192:0 outbound_.8000_._.test-svc.default.svc.cluster.local default\r\n[2021-02-18T06:16:33.249Z] \"GET /rt0/v1/api/racingtouch/ping HTTP/1.1\" 200 - \"-\" 0 18 1 0 \"192.168.98.64\" \"Mozilla/5.0 (pc-x86_64-linux-gnu) Siege/4.0.7\" \"b28ff319-1dc2-994e-aca3-26f88881f40b\" \"istio-ingressgateway.istio-system\" \"127.0.0.1:8000\" inbound|8000|| 127.0.0.1:53922 192.168.53.7:8000 192.168.98.64:0 outbound_.8000_._.test-svc.default.svc.cluster.local default\r\n\r\nit is to count how many requests going to the pod\r\nsorry, the ip changed as I restarted the pod many times."
      },
      {
        "user": "debbyku",
        "created_at": "2021-02-18T06:27:25Z",
        "body": "The stats in the endpoints do not count correctly.  The rq_total should be much larger than it and I rarely find the access log for 15443 in the istio ingressgateway."
      },
      {
        "user": "lambdai",
        "created_at": "2021-02-18T07:46:58Z",
        "body": "> The stats in the endpoints do not count correctly. The rq_total should be much larger than it and I rarely find the access log for 15443 in the istio ingressgateway.\r\n\r\nThe request in istio-gateway is loadbalanced per `tcp connection` since you use sni_cluster with tcp_proxy filter at istio-ingressgateway. This config doesn't guarantee http request is balanced.\r\n\r\nAt an extreme case, if your siege client use only 1 tcp connection during the your load test, you will see only 1 endpoint handle all the http request. You are right at the beginning: SNI cluster + tcp_proxy doesn't well load balancing http request. \r\n\r\nYou can either switch to another http benchmark tool with max-request-per-connection to give istio-ingressgateway more chances to load balance."
      },
      {
        "user": "debbyku",
        "created_at": "2021-02-18T13:32:46Z",
        "body": "Hi @lambdai,  thanks for your advice.\r\nAs the request is from siege -> clusterA isto-ingressgateway -> clusterB istio-ingressgateway(15443) -> service, in clusterA istio-ingressgateway, we set the max-request-per-connection to 10 in order to max. the no. of connections to clusterB 15443, it seems the load balancing performance is much better.  For 4xxxx requests, the difference of number of requests to the service pods is reduced within 100.  \r\n\r\nMay I ask, if max-request-per-connection is set to 10, is there any adverse effect to the overall performance, i.e. it takes more time to create connections. etc?  Originally there is no setting for it.   Thanks."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-20T16:05:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "lambdai",
        "created_at": "2021-03-23T02:07:10Z",
        "body": "Sorry I missed this one.\r\nFor light weight request the major cost will be tls handshake. I usually use 5ms cpu time to estimate. YMMV"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-22T08:01:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-29T08:01:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15055,
    "title": "ResponseMapper message filter inclusion",
    "created_at": "2021-02-15T14:12:34Z",
    "closed_at": "2021-03-26T04:06:51Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15055",
    "body": "Hi all,\r\n\r\nI would like to know whether it is possible to modify responses based on status code AND status message. Specifically, it would be good to be able to override responses like \"503, TLS error: Secret is not supplied by SDS\" with something more meaningful. \r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15055/comments",
    "author": "Coconut105",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-19T00:07:11Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-26T04:06:50Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 15015,
    "title": "Can envoy hold HTTP response headers while processing response body? in case filter needs to modify response body/header",
    "created_at": "2021-02-10T19:50:20Z",
    "closed_at": "2021-02-12T19:05:40Z",
    "labels": [
      "question",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15015",
    "body": "Can envoy hold HTTP response headers while processing response body? \r\n\r\nIf I have a filter that needs to modify response body, content-length will change. Return code may change too. Can I make envoy hold response headers so in response body filter I can modify content-length or the whole response?\r\n\r\nWithout holding response header, I got:\r\n[debug][http] [source/common/http/filter_manager.cc:847] [C0][S17311476643790345888] Resetting stream due to . Prior headers have already been sent",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15015/comments",
    "author": "dandlake",
    "comments": [
      {
        "user": "antoniovicente",
        "created_at": "2021-02-10T21:06:38Z",
        "body": "cc @alyssawilk \r\n\r\nIt seems to me that you need to stop iteration in decode headers and force buffering in order to be able to do header modifications based on the contents of the body.  It may be helpful to see how the compression filters handle this case, but I expect that they end up removing content-length in order to preserve the capability to stream the modified response."
      },
      {
        "user": "dandlake",
        "created_at": "2021-02-11T02:12:34Z",
        "body": "Thanks, @antoniovicente. I see the code in decompressor filters to remove content-length and modify content-encoding. Without changing content-encoding, not sure if I can remove content-length without causing problem in the receiving browser.\r\n\r\nIn cases where I want to change the return code(like from 200 to 4xx or 5xx to drop the body), is there a mechanism to modify/generate the entire response headers when doing body processing?"
      },
      {
        "user": "antoniovicente",
        "created_at": "2021-02-11T02:43:17Z",
        "body": "In order to modify response headers based on the computation on the body, I think you need to force buffering and do something to prevent headers being forwarded until the computation on the buffered body is complete.  I don't know of an example of a filter that does that, I can try to dig around a bit later."
      },
      {
        "user": "alyssawilk",
        "created_at": "2021-02-11T19:04:06Z",
        "body": "Yeah, in general once you pass the headers on to a follow-up filter, they may be shipped upstream/downstream so modifying doesn't make sense.\r\nThe buffer filter is a useful example both of buffering, and something you may want to put in your filter chain if you want to not process headers until the entire body has been read."
      },
      {
        "user": "antoniovicente",
        "created_at": "2021-02-11T22:43:49Z",
        "body": "See documentation for StopAllIterationAndBuffer return value from decodeHeaders"
      },
      {
        "user": "dandlake",
        "created_at": "2021-02-12T19:05:40Z",
        "body": "Thank you @antoniovicente @alyssawilk! I got the ideas and will look into it. "
      }
    ]
  },
  {
    "number": 14908,
    "title": "Downstream remote disconnect response",
    "created_at": "2021-02-02T13:18:25Z",
    "closed_at": "2021-03-27T04:01:59Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14908",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *What are possible scenarios where we get downstream_remote_disconnect  response ?*\r\n\r\n*Description*:\r\n>We are using envoy proxy to route requests based on header value to respective upstreams. While doing performance testing, few of our requests fail with response code details -  downstream_remote_disconnect. Wanted to understand when can we experience this ?\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14908/comments",
    "author": "hinawatts",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2021-02-17T21:18:42Z",
        "body": "This is set when the stream is terminated due to a downstream FIN.   If you're encountering a situation where that detail is set and a wireshark trace shows downstream isn't sending the FIN we'd be happy to look into it, but by default we'd assume it's client-caused at which point there's not much we can do to diagnose what's going on :-)"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-20T00:06:55Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-27T04:01:59Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14871,
    "title": "understanding migrating from nginx to envoy",
    "created_at": "2021-01-30T02:54:10Z",
    "closed_at": "2021-03-10T20:07:41Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14871",
    "body": "Guys, I am new in envoy and I just knew it today. \r\n\r\nI wanted to migrate my nginx server to envoy.. But couldn't find any resources on using PHP (app like wordpress) and how \"location directive\" and \"web root\" from nginx works in envoy (opening files like css, images, etc). Or do i need an app to run these? Also, I don't want to use laravel to run php.\r\n\r\nIs envoy a fully proxy? I wanted to know if these Nginx conf is also available or equivalent in envoy:\r\n\r\n```\r\nroot /var/www/web_folder;\r\n\r\nlocation{\r\n  try_files $uri $uri/ /index.php?q=$uri&$args;\r\n}\r\n```\r\n\r\n```\r\nlocation ~ \\.php${\r\n  fastcgi_split_path_info ^(.+\\.php)(/.+)$;\r\n  try_files $fastcgi_script_name =404;\r\n  set $path_info $fastcgi_path_info;\r\n  fastcgi_param PATH_INFO $path_info;\r\n  \r\n  fastcgi_index index.php;\r\n  \r\n  fastcgi_pass unix:/var/run/php/php7.4-fpm.sock;\r\n  fastcgi_read_timeout 300;\r\n}\r\n```\r\n\r\nIf not available, what are the best tool to run php with wordpress on envoy?\r\n\r\nReally sorry about this question.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14871/comments",
    "author": "dogecoin2018",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2021-02-01T17:42:13Z",
        "body": "Envoy does not function as a general-purpose webserver, so there is no direct equivalent for either CGI or  a www root."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-03T20:03:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-10T20:07:40Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14698,
    "title": "Cannot set an httpOnly cookie from python GRPC server. Envoy configs not correct?",
    "created_at": "2021-01-14T07:05:45Z",
    "closed_at": "2021-03-02T00:07:03Z",
    "labels": [
      "question",
      "stale",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14698",
    "body": "*Title*: Cannot set an httpOnly cookie from python GRPC server. Envoy configs not correct?\r\n\r\n*Description*:\r\nI am trying to set an httpOnly Authorization cookie from my gRPC python server through an envoy proxy and back to my ReactJS front-end client running grpc-web. Currently, I can send a JWT back in a response, set it into local storage, and append it onto subsequent calls using a client side grpc-web-interceptor; however, I do know that using local storage for a JWT is not best practice. How can I make it so that running context.set_trailing_metadata('set-cookie', 'mycookie') from my python gRPC server actually set the cookie, because currently I do not see anything in the storage tag of development tools but I also simultaneously see the response cookie when checking the API call in the response tab.\r\n\r\nAlso, would it be possible to make envoy grab the authorization cookie on every request if it is there? I have read so many posts, but none of them match what I need to achieve.\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14698/comments",
    "author": "kurnal-volt",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2021-01-14T18:51:23Z",
        "body": "It sounds like the cookie is sent back to the client based on what you say, as it appears in the API call response client-side, if I'm understanding what you wrote correctly. In that case, it sounds like your client is not correctly handling returned cookies; what happens if you have the client and server speak directly, without an Envoy between them?"
      },
      {
        "user": "kurnal",
        "created_at": "2021-01-17T21:07:04Z",
        "body": "@htuch Thank you for the response! Envoy is used so the ReactJS client can communicate with the Python gRPC server. Without it we would get CORS cross-origin errors. Do you know of things I should check client side to make sure a cookie is set since it is in fact making it to the response (i can see it in developer tools).\r\n\r\nIs there something in envoy that could possible be blocking a cookie from being set?"
      },
      {
        "user": "htuch",
        "created_at": "2021-01-17T23:34:58Z",
        "body": "I don't see how this can be an Envoy issue if the `set-cookie` header is propagated intact to the client. Maybe this is some browser CORS/cookie/AJAX issue."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-02-22T20:04:10Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-02T00:07:02Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14539,
    "title": "Question RE: tcp health checks, zone aware routing, endpoint priority",
    "created_at": "2020-12-30T02:25:39Z",
    "closed_at": "2021-02-02T22:37:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14539",
    "body": "Hello!\r\n\r\n### Current state of things\r\n\r\nWe have the following:\r\n\r\n* A cluster has multiple endpoints in different localities\r\n* Each endpoint has a priority based on it's locality, in case zone-aware routing doesn't work for some reason\r\n* The cluster has a TCP health check, which does a CONNECT only\r\n\r\n### What we would like to happen\r\n\r\nIf an endpoint in one locality is unhealthy (based on TCP health checks), that endpoint should not be used, and traffic should effectively fail-over to other regions.\r\n\r\n### Example configuration\r\n\r\nThis is for an envoy proxy in the AWS region ap-southeast-2 (this json is from CDS):\r\n\r\n```json\r\n{\r\n  \"version_info\": \"1966194959\",\r\n  \"resources\": [\r\n    {\r\n      \"name\": \"edge-echo-ingress-micros-elb\",\r\n      \"@type\": \"type.googleapis.com/envoy.api.v2.Cluster\",\r\n      \"type\": \"STRICT_DNS\",\r\n      \"connect_timeout\": \"5s\",\r\n      \"per_connection_buffer_limit_bytes\": 16777216,\r\n      \"common_http_protocol_options\": {\r\n        \"idle_timeout\": \"55s\"\r\n      },\r\n      \"dns_lookup_family\": \"V4_ONLY\",\r\n      \"respect_dns_ttl\": true,\r\n      \"load_assignment\": {\r\n        \"cluster_name\": \"edge-echo-ingress-micros-elb\",\r\n        \"endpoints\": [\r\n          {\r\n            \"priority\": 1,\r\n            \"locality\": {\r\n              \"zone\": \"us-west-2\"\r\n            },\r\n            \"lb_endpoints\": [\r\n              {\r\n                \"endpoint\": {\r\n                  \"address\": {\r\n                    \"socket_address\": {\r\n                      \"address\": \"edge-echo.us-west-2.staging.atl-paas.net\",\r\n                      \"port_value\": 443\r\n                    }\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          },\r\n          {\r\n            \"priority\": 0,\r\n            \"locality\": {\r\n              \"zone\": \"ap-southeast-2\"\r\n            },\r\n            \"lb_endpoints\": [\r\n              {\r\n                \"endpoint\": {\r\n                  \"address\": {\r\n                    \"socket_address\": {\r\n                      \"address\": \"edge-echo.ap-southeast-2.staging.atl-paas.net\",\r\n                      \"port_value\": 443\r\n                    }\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          },\r\n          {\r\n            \"priority\": 4,\r\n            \"locality\": {\r\n              \"zone\": \"us-east-1\"\r\n            },\r\n            \"lb_endpoints\": [\r\n              {\r\n                \"endpoint\": {\r\n                  \"address\": {\r\n                    \"socket_address\": {\r\n                      \"address\": \"edge-echo.us-east-1.staging.atl-paas.net\",\r\n                      \"port_value\": 443\r\n                    }\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          },\r\n          {\r\n            \"priority\": 10,\r\n            \"locality\": {\r\n              \"zone\": \"zone-padding-3\"\r\n            },\r\n            \"lb_endpoints\": [\r\n              {\r\n                \"endpoint\": {\r\n                  \"address\": {\r\n                    \"socket_address\": {\r\n                      \"address\": \"edge-echo.us-east-1.staging.atl-paas.net\",\r\n                      \"port_value\": 443\r\n                    }\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          },\r\n          {\r\n            \"priority\": 10,\r\n            \"locality\": {\r\n              \"zone\": \"zone-padding-4\"\r\n            },\r\n            \"lb_endpoints\": [\r\n              {\r\n                \"endpoint\": {\r\n                  \"address\": {\r\n                    \"socket_address\": {\r\n                      \"address\": \"edge-echo.ap-southeast-2.staging.atl-paas.net\",\r\n                      \"port_value\": 443\r\n                    }\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          },\r\n          {\r\n            \"priority\": 10,\r\n            \"locality\": {\r\n              \"zone\": \"zone-padding-5\"\r\n            },\r\n            \"lb_endpoints\": [\r\n              {\r\n                \"endpoint\": {\r\n                  \"address\": {\r\n                    \"socket_address\": {\r\n                      \"address\": \"edge-echo.us-west-2.staging.atl-paas.net\",\r\n                      \"port_value\": 443\r\n                    }\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          },\r\n          {\r\n            \"priority\": 10,\r\n            \"locality\": {\r\n              \"zone\": \"zone-padding-6\"\r\n            },\r\n            \"lb_endpoints\": [\r\n              {\r\n                \"endpoint\": {\r\n                  \"address\": {\r\n                    \"socket_address\": {\r\n                      \"address\": \"edge-echo.us-west-2.staging.atl-paas.net\",\r\n                      \"port_value\": 443\r\n                    }\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          }\r\n        ]\r\n      },\r\n      \"http_protocol_options\": {\r\n        \"header_key_format\": {\r\n          \"proper_case_words\": {}\r\n        }\r\n      },\r\n      \"circuit_breakers\": {\r\n        \"thresholds\": [\r\n          {\r\n            \"priority\": \"DEFAULT\",\r\n            \"max_connections\": 1048576,\r\n            \"max_requests\": 1048576,\r\n            \"max_pending_requests\": 1048576\r\n          }\r\n        ]\r\n      },\r\n      \"common_lb_config\": {\r\n        \"healthy_panic_threshold\": {\r\n          \"value\": 20\r\n        }\r\n      },\r\n      \"healthchecks\": [\r\n        {\r\n          \"interval\": \"5s\",\r\n          \"timeout\": \"5s\",\r\n          \"unhealthy_threshold\": 12,\r\n          \"healthy_threshold\": 3,\r\n          \"reuse_connection\": false,\r\n          \"tcp_health_check\": {}\r\n        }\r\n      ],\r\n      \"transport_socket\": {\r\n        \"name\": \"envoy.transport_sockets.tls\",\r\n        \"typed_config\": {\r\n          \"@type\": \"type.googleapis.com/envoy.api.v2.auth.UpstreamTlsContext\"\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThe FQDNs in this will most likely not be resolvable via DNS. But in this scenario, `edge-echo.ap-southeast-2.staging.atl-paas.net` refers to a backend service that has been removed, whereas all other endpoints refer to backend services that are still operating.\r\n\r\n### What we expect to happen\r\n\r\nWe expect in this situation that the TCP health check would detect that this one backend service is down, and even though it's priority is `0` and it's locality matches with that of the proxy, that traffic should *not* go to this endpoint anymore.\r\n\r\nAre we doing something (or many things) wrong here?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14539/comments",
    "author": "cetanu",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-01-02T23:37:55Z",
        "body": "cc @snowp who can probably advise."
      },
      {
        "user": "snowp",
        "created_at": "2021-01-03T19:28:59Z",
        "body": "Zone aware routing will only apply to P0, so this strategy is never going to do zone aware routing. That said, it sounds like it should do the right thing in terms for failover. The common issue in these cases is panic routing, where enough endpoints are unhealthy that Envoy starts routing to all endpoints instead.\r\n\r\nAre you seeing endpoints marked as unhealthy but still receiving traffic? "
      },
      {
        "user": "cetanu",
        "created_at": "2021-01-04T02:49:10Z",
        "body": "It shouldn't be doing zone aware routing?\r\n\r\nI've got the following metrics showing up for this cluster and all others in my environment:\r\n\r\n```statsd\r\ncluster.edge-echo-ingress-micros-elb.zone.ap-southeast-2.ap-southeast-2.upstream_rq_200: 3\r\ncluster.edge-echo-ingress-micros-elb.zone.ap-southeast-2.ap-southeast-2.upstream_rq_503: 2\r\ncluster.edge-echo-ingress-micros-elb.zone.ap-southeast-2.ap-southeast-2.upstream_rq_completed: 5\r\n```\r\n\r\n---\r\n\r\nI hadn't checked the health of the endpoints, but upon checking I noticed a few things:\r\n\r\n* The IP addresses for the dead/unresolvable endpoint remain in the cluster (as checked via `curl envoy:9901/clusters`)\r\n* The IP addresses are all marked healthy\r\n* I can't perform a TCP connect to the IPs, from the same machine (I used python `socket.create_connection((ip, port))` to test)\r\n\r\n```statsd\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::cx_active::0\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::cx_connect_fail::2\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::cx_total::3\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::rq_active::0\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::rq_error::2\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::rq_success::2\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::rq_timeout::0\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::rq_total::2\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::hostname::edge-echo.ap-southeast-2.staging.atl-paas.net\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::health_flags::healthy\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::weight::1\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::region::\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::zone::ap-southeast-2\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::sub_zone::\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::canary::false\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::priority::0\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::success_rate::-1.0\r\nedge-echo-ingress-micros-elb::10.118.19.18:443::local_origin_success_rate::-1.0\r\n```\r\n\r\n---\r\n\r\nThe cluster does not appear to be in a panic mode:\r\n\r\n```statsd\r\ncluster.edge-echo-ingress-micros-elb.lb_healthy_panic: 0\r\ncluster.edge-echo-ingress-micros-elb.lb_subsets_fallback_panic: 0\r\n```"
      },
      {
        "user": "snowp",
        "created_at": "2021-01-04T15:19:29Z",
        "body": "Zone aware routing is only done to localities within P0, so while you technically might be doing zone aware when routing to P0, none of the logic around it will apply since P0 only has one locality.\r\n\r\nBased on your config I would have expected endpoints to be marked as unhealthy about 1 min after being removed (assuming the cluster is sending traffic, about 12 min if not due to the \"no traffic interval\" default if not). The endpoints would be removed once the DNS refresh happens, after the TTL expires of the result. \r\n\r\nAt this point it would be good to verify that health checks are working properly - maybe try enabling health check event logging with `always_log_health_check_failures` and see if you're seeing failures for the bad IPs? And verify that the health check attempt stat is being incremented"
      },
      {
        "user": "cetanu",
        "created_at": "2021-02-02T22:37:26Z",
        "body": "Well, you'll be glad to know that this was something completely silly - as you can see in the JSON representation above, the field `healthchecks` is incorrect, it's meant to be `health_checks` :)"
      }
    ]
  },
  {
    "number": 14440,
    "title": "question: how to fetch the remote IP address in WASM",
    "created_at": "2020-12-16T14:03:30Z",
    "closed_at": "2020-12-23T11:20:12Z",
    "labels": [
      "question",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14440",
    "body": "I do not find any doc about how to fetch the remote IP address in WASM.\r\n\r\nMany thx for your help.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14440/comments",
    "author": "membphis",
    "comments": [
      {
        "user": "alandiegosantos",
        "created_at": "2020-12-18T23:15:56Z",
        "body": "It is possible to get the upstream IP address by getting the property _upstream.address_.\r\nI am working with WASM filters written in Rust, so the code looks like: \r\n```\r\nuse log::error;\r\nuse proxy_wasm::traits::*;\r\nuse proxy_wasm::types::*;\r\nuse std::str;\r\n\r\n#[no_mangle]\r\npub fn _start() {\r\n    proxy_wasm::set_log_level(LogLevel::Info);\r\n    proxy_wasm::set_http_context(|_, _| -> Box<dyn HttpContext> { Box::new(HttpFilter) });\r\n}\r\n\r\nstruct HttpFilter;\r\n\r\nimpl Context for HttpFilter{}\r\n\r\nimpl HttpContext for HttpFilter {\r\n    fn on_http_response_headers(&mut self, _: usize) -> Action {\r\n        // Add a header on the response.\r\n        let prop = self.get_property([\"upstream\", \"address\"].to_vec()).unwrap();\r\n        let addr = match str::from_utf8(&prop) {\r\n            Ok(v) => v,\r\n            Err(_e) => \"\",\r\n        };\r\n        error!(\"upstream address {}\",addr);\r\n        Action::Continue\r\n    }\r\n}\r\n```\r\nPS: Do not use that as production code. It is only an example.\r\n\r\nI would like to create the docs about these properties, if possible."
      },
      {
        "user": "membphis",
        "created_at": "2020-12-23T11:20:12Z",
        "body": "ok, got it. many thx"
      }
    ]
  },
  {
    "number": 14372,
    "title": "Header propagation over tcp",
    "created_at": "2020-12-11T06:49:04Z",
    "closed_at": "2021-01-20T20:28:03Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14372",
    "body": "How to propagation Header over tcp in envoy.\r\nService A is WebServer.\r\nService B is NettyServer.\r\nHow to propagation Header from Service A to Service B.\r\nThanks a lot.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14372/comments",
    "author": "liujiyuan",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2020-12-14T17:46:27Z",
        "body": "Please provide more details."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-13T20:27:53Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-20T20:28:02Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14346,
    "title": "decodeData() isn't call when using a direct response",
    "created_at": "2020-12-09T18:08:27Z",
    "closed_at": "2020-12-10T00:21:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14346",
    "body": "We have the following routes config for an Envoy instance meant to act as /dev/null:\r\n\r\n```\r\n              \"route_config\": {\r\n                \"name\": \"routes\",\r\n                \"virtual_hosts\": [\r\n                  {\r\n                    \"domains\": [\r\n                      \"*\"\r\n                    ],\r\n                    \"name\": \"test\",\r\n                    \"routes\": [\r\n                      {\r\n                        \"direct_response\": {\r\n                          \"status\": 200\r\n                        },\r\n                        \"match\": {\r\n                          \"prefix\": \"/\"\r\n                        }\r\n                      }\r\n                    ]\r\n                  }\r\n                ]\r\n              },\r\n```\r\n\r\nAlong with the above, we also have a filter that implements decodeHeaders() and decodeData() to analyze the incoming requests. However, we only see decodeHeaders() being called:\r\n\r\n```\r\n[2020-12-08 00:36:34.568][34][debug][filter] [pinterest/filters/http/test/filter.cc:111] TestFilter::decodeHeaders\r\n[2020-12-08 00:36:34.568][34][debug][http] [external/envoy/source/common/http/filter_manager.cc:783] [C134124][S4840569554702352551] Sending local reply with details direct_response\r\n[2020-12-08 00:36:34.568][34][debug][filter] [pinterest/filters/http/test/filter.cc:229] TestFilter::encodeHeaders\r\n```\r\n\r\nPossible options to work around this:\r\n\r\na) Fix the way we handle direct responses to ensure decodeData() is called too (currently it apparently emits an early local response after decodeHeaders() has been called and then stops the filter chain) \r\n\r\nb) Add docs stating that if you want to do the above, you should setup another listener with a direct response and use that as the upstream for the first (or some other similar trick, which sounds too convoluted tbh)\r\n\r\nThoughts? \r\n\r\ncc: @alyssawilk @mattklein123 \r\n\r\nSomewhat related to #13737.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14346/comments",
    "author": "rgs1",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2020-12-09T18:27:29Z",
        "body": "Just one check before I dig into this, it looks like you didn't have a direct response body configured.  Have you tried adding that and seeing if it fixes your problem?  At first glance I wouldn't expect decodeData to be called for a headers only response."
      },
      {
        "user": "rgs1",
        "created_at": "2020-12-09T18:31:20Z",
        "body": "Ah! Let me try that. "
      },
      {
        "user": "mattklein123",
        "created_at": "2020-12-09T18:38:34Z",
        "body": "Do you mean encodeData()? I'm confused."
      },
      {
        "user": "rgs1",
        "created_at": "2020-12-09T18:53:23Z",
        "body": "> Do you mean encodeData()? I'm confused.\r\n\r\nyou == Alyssa or me?\r\n\r\nI meant decodeData(), per the above log excerpt. But now that you mention this, I actually not sure how adding a body response to DR would trigger decodeData() into running...."
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-12-09T18:55:17Z",
        "body": "yeah, I had encode data and decode swapped, sorry, sigh.   Let me actually take a look"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-12-09T18:55:39Z",
        "body": "Yeah I'm asking you. :) \r\n\r\nI'm confused about the issue you are reporting. Why would decodeData run here? I wouldn't expect encodeData to run either without a body."
      },
      {
        "user": "rgs1",
        "created_at": "2020-12-09T18:57:22Z",
        "body": "> Yeah I'm asking you. :)\r\n> \r\n> I'm confused about the issue you are reporting. Why would decodeData run here? I wouldn't expect encodeData to run either without a body.\r\n\r\nIf it's a POST request and I have a filter running that inspects the body... why would that not run just because it matches a route that is configured to have a direct response? That sounds counter intuitive no?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-12-09T18:58:10Z",
        "body": "Yeah, so to make things clear,\r\nwhen the router decodes the header, it immediately looks to see what the action is.  If there's a direct response configured, it immediately starts sending the response, and calls StopIteration on the request pipeline.   It's basically treated like a local reply, where we assume it's not worth processing the POST body.  If you folks have a need for the body to pass through the filter chain, that'd take some work and probably need to be a config option since halting processing makes sense for the common case."
      },
      {
        "user": "rgs1",
        "created_at": "2020-12-09T18:58:27Z",
        "body": "> > Yeah I'm asking you. :)\r\n> > I'm confused about the issue you are reporting. Why would decodeData run here? I wouldn't expect encodeData to run either without a body.\r\n> \r\n> If it's a POST request and I have a filter running that inspects the body... why would that not run just because it matches a route that is configured to have a direct response? That sounds counter intuitive no?\r\n\r\nOk, maybe not that counter intuitive given the body is going to /dev/null.... Unless you have a filter that's doing inspection stuff (think a WAF). "
      },
      {
        "user": "rgs1",
        "created_at": "2020-12-09T19:01:16Z",
        "body": "> Yeah, so to make things clear,\r\n> when the router decodes the header, it immediately looks to see what the action is. If there's a direct response configured, it immediately starts sending the response, and calls StopIteration on the request pipeline. It's basically treated like a local reply, where we assume it's not worth processing the POST body. If you folks have a need for the body to pass through the filter chain, that'd take some work and probably need to be a config option since halting processing makes sense for the common case.\r\n\r\nWe do have a use case... but I am starting to think it might be a bit too contrived and maybe we shouldn't be using DR for it 🤔 "
      },
      {
        "user": "mattklein123",
        "created_at": "2020-12-09T19:19:42Z",
        "body": "If you put the buffer filter in front of your filter, decodeData() will run. You may or may not be able to do that in your scenario."
      },
      {
        "user": "rgs1",
        "created_at": "2020-12-09T19:33:39Z",
        "body": "That's great, let me try that. Thanks!"
      },
      {
        "user": "rgs1",
        "created_at": "2020-12-09T20:20:15Z",
        "body": "Adding the buffer filter did not do make decodeData() run....:\r\n\r\n```\r\n             {\r\n              \"name\": \"envoy.filters.http.buffer\",\r\n              \"typed_config\": {\r\n               \"@type\": \"type.googleapis.com/envoy.config.filter.http.buffer.v2.Buffer\",\r\n               \"max_request_bytes\": 209715200\r\n              }\r\n             },\r\n             {\r\n              \"name\": \"pinterest.filters.http.test\",\r\n              \"typed_config\": {\r\n               \"@type\": \"type.googleapis.com/pinterest.http.test.TestFilter\",\r\n              }\r\n             },\r\n             {\r\n              \"name\": \"envoy.filters.http.router\"\r\n             }\r\n```"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-12-09T22:10:58Z",
        "body": "Oh hmm, you are right, sorry. The router will respond right away even if the data is buffered.\r\n\r\nYou could potentially buffer the response in your filter if the request is not complete?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-12-09T22:16:12Z",
        "body": "yeah, I think you'd have to subclass the buffer filter, and block the headers from going to the router until the buffer filter got end stream.  As soon as the router sees the headers it's all over :-)"
      },
      {
        "user": "rgs1",
        "created_at": "2020-12-10T00:21:24Z",
        "body": "Alright we went with forwarding to a local process that acts as /dev/null for HTTP. Thanks nonetheless!\r\n\r\nP.S.: an envoy.router filter replacement that acts as /dev/null could be fun/useful..."
      }
    ]
  },
  {
    "number": 14342,
    "title": "Async gRPC service under envoy. A lot of concurrent requests",
    "created_at": "2020-12-09T17:05:20Z",
    "closed_at": "2021-01-22T04:30:44Z",
    "labels": [
      "question",
      "stale",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14342",
    "body": "I have async gRPC service written on Python\r\n\r\nMy envoy.yaml:\r\n```\r\n    static_resources:\r\n      listeners:\r\n      - reuse_port: true\r\n        address:\r\n          socket_address:\r\n            address: 0.0.0.0\r\n            port_value: 50051\r\n        filter_chains:\r\n        - filters:\r\n          - name: envoy.http_connection_manager\r\n            config:\r\n              access_log:\r\n              - name: envoy.file_access_log\r\n                config:\r\n                  path: \"/dev/stdout\"\r\n              codec_type: AUTO\r\n              stat_prefix: ingress_https\r\n              route_config:\r\n                name: local_route\r\n                virtual_hosts:\r\n                - name: https\r\n                  domains:\r\n                  - \"*\"\r\n                  routes:\r\n                  - match:\r\n                      prefix: \"/\"\r\n                    route:\r\n                      cluster: my-service-grpc\r\n              http_filters:\r\n              - name: envoy.health_check\r\n                config:\r\n                  pass_through_mode: false\r\n                  headers:\r\n                  - name: \":path\"\r\n                    exact_match: \"/healthz\"\r\n                  - name: \"x-envoy-livenessprobe\"\r\n                    exact_match: \"healthz\"\r\n              - name: envoy.router\r\n                config: {}\r\n      clusters:\r\n      - name: my-service-grpc\r\n        connect_timeout: 0.5s\r\n        type: STRICT_DNS\r\n        dns_lookup_family: V4_ONLY\r\n        lb_policy: ROUND_ROBIN\r\n        http2_protocol_options:\r\n          max_concurrent_streams: 10\r\n        load_assignment:\r\n          cluster_name: my-service-grpc\r\n          endpoints:\r\n          - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: my-service-headless.utils.svc.cluster.local\r\n                    port_value: 9000\r\n        health_checks:\r\n          timeout: 1s\r\n          interval: 10s\r\n          unhealthy_threshold: 2\r\n          healthy_threshold: 2\r\n          grpc_health_check: {}\r\n        health_checks:\r\n          timeout: 1s\r\n          interval: 5s\r\n          unhealthy_threshold: 2\r\n          healthy_threshold: 2\r\n          grpc_health_check: {}\r\n        circuit_breakers:\r\n          thresholds:\r\n            - priority: default\r\n              max_pending_requests: 10000\r\n              max_requests: 10000\r\n              max_connections: 10000\r\n            - priority: high\r\n              max_pending_requests: 10000\r\n              max_requests: 10000\r\n              max_connections: 10000\r\n    admin:\r\n      access_log_path: \"/dev/stdout\"\r\n      address:\r\n        socket_address:\r\n          address: 127.0.0.1\r\n          port_value: 8090\r\n```\r\n\r\nAs I understand properly, envoy will send request to host if it sees any available open socket(because it's async gRPC it doens't use pending requests queue, because async socket could keep number connection, not only one as single-threaded sync gRPC server), so I tried to make 200rps and I got very high latency for N requests -> overflow for N+X requests -> timeout for N + X + Y requests\r\n\r\nMy question is how to prevent a lot of concurrent requests(concurrent tasks in my python event loop) and put all requests which are over restriction to some queue?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14342/comments",
    "author": "nikita-davydov",
    "comments": [
      {
        "user": "nikita-davydov",
        "created_at": "2020-12-09T18:22:00Z",
        "body": "Also, I have to say that I tried to use example configuration of adaptive concurrency and I got an exception that concurrency limit is reached"
      },
      {
        "user": "nikita-davydov",
        "created_at": "2020-12-16T01:03:23Z",
        "body": "Guys, can anybody help me please? :("
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-15T04:27:10Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-22T04:30:43Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14215,
    "title": "lb_subset_config: fallback policy support next selector mode.",
    "created_at": "2020-11-30T17:23:48Z",
    "closed_at": "2021-01-13T12:22:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14215",
    "body": "@mattklein123 plz help,thx.  The fallback policy could support next （`NEXT_SUBSET`）selector mode? In other words, fallback policy could supports cascading mode. forexample：\r\n`[stage, idc, zone] —fallback—> [stage, zone] —fallback—>  [stage] `\r\n\r\n```\r\nlb_subset_config:\r\n  fallback_policy: ANY_ENDPOINT\r\n  subset_selectors:\r\n  - keys:\r\n    - stage\r\n    - idc\r\n    - zone\r\n    fallback_policy: NEXT_SUBSET\r\n  - keys:\r\n    - stage\r\n    - zone\r\n    fallback_policy: NEXT_SUBSET\r\n  - keys:\r\n    - stage\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14215/comments",
    "author": "wangfakang",
    "comments": [
      {
        "user": "wangfakang",
        "created_at": "2020-12-07T08:30:27Z",
        "body": "Now it can be solved by `KEYS_SUBSET` fallback policy,  What better way? Such as `NEXT_SUBSET`.\r\n```\r\nlb_subset_config:\r\n      fallback_policy: DEFAULT_SUBSET\r\n      default_subset:\r\n        stage: prod\r\n      subset_selectors:\r\n      - keys:\r\n        - stage\r\n        - zone\r\n        - idc\r\n        fallback_policy: KEYS_SUBSET\r\n        fallback_keys_subset:\r\n         - stage\r\n         - zone\r\n      - keys:\r\n        - stage\r\n        - zone\r\n        fallback_policy: KEYS_SUBSET\r\n        fallback_keys_subset:\r\n         - stage\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-06T12:07:08Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-13T12:22:53Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14201,
    "title": "Why the response status use 503 instead of 502 when the connection reset by peer scenario?",
    "created_at": "2020-11-27T11:33:21Z",
    "closed_at": "2021-01-13T20:27:47Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14201",
    "body": "Why the response status is 503 instead of 502 when the connection reset by peer scenario?\r\n503 is means the server is currently unable to handle the request `due to a temporary overloading or maintenance of the server`. \r\n502 is means the server, while acting as a gateway or proxy, `received an invalid response` from the upstream server it accessed in attempting to fulfill the request.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14201/comments",
    "author": "wangfakang",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-12-07T05:55:20Z",
        "body": "Historical reasons. You can remap it using the local reply mapper if you want."
      },
      {
        "user": "wangfakang",
        "created_at": "2020-12-07T08:33:03Z",
        "body": "> Historical reasons. You can remap it using the local reply mapper if you want.\r\n\r\nok, thx. And will later versions solve this problem?"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-12-07T17:26:56Z",
        "body": "> ok, thx. And will later versions solve this problem?\r\n\r\nNo, I doubt we will change the default here as I don't think the churn is worth it. HTTP status codes are somewhat arbitrary across the internet to be honest."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-06T20:10:58Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-13T20:27:46Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14176,
    "title": "Envoy suddenly stop accepting tcp connection in ports 15001 and 15006",
    "created_at": "2020-11-25T02:57:19Z",
    "closed_at": "2021-01-22T08:27:30Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14176",
    "body": "Envoy suddenly stop accepting tcp connection in ports 15001 and 15006\r\n\r\n*Description*:\r\nWe have a k8s cluster in a production environment with istio installed. Some of pods enabled istio。 Now I have a weird problem that an instance of a service enabled istio was working normally, but it will suddenly restart continuously. The service configured with k8s liveness probe (i.e. ip:8080/healthy/heartbeat) and that probe caused the containter restart. The istio-proxy container looked like normal and the liveness of it worked well.\r\nI logged in the istio-proxy container and service container respectively, and found:\r\n1. There was no any issue for the service container, and \"curl -v localhost:8080/healthy/heartbeat\" responsed 200 always.\r\n2. The admin port 15000 of envoy worked well. The interfaces \"/server_info\", \"/logging\", \"/listeners\", \"/config_dump\" worked well.\r\n3. The config (listener, route, cluster and endpoint) of envoy was good.\r\n4. In istio-proxy container, I found the recv-Q of 15001(virtualOutbound) and 15006 (virtualInbound) were not zero.\r\n\r\n    istio-proxy@bcs-ctrip-default-7994bd967c-hhkm5:/$ netstat -nltp\r\nActive Internet connections (only servers)\r\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\r\ntcp        0      0 0.0.0.0:8009            0.0.0.0:*               LISTEN      -\r\ntcp        0      0 0.0.0.0:15020           0.0.0.0:*               LISTEN      1/pilot-agent\r\ntcp        0      0 127.0.0.1:8527          0.0.0.0:*               LISTEN      -\r\ntcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN      -\r\ntcp        0      0 0.0.0.0:15090           0.0.0.0:*               LISTEN      62/envoy\r\ntcp        0      0 0.0.0.0:1234            0.0.0.0:*               LISTEN      -\r\ntcp        0      0 127.0.0.1:15000         0.0.0.0:*               LISTEN      62/envoy\r\ntcp        2      0 0.0.0.0:15001           0.0.0.0:*               LISTEN      62/envoy\r\ntcp        129  0 0.0.0.0:15006           0.0.0.0:*               LISTEN      62/envoy\r\nI think the issue was envoy didn't accept tcp connection at that time, but I did't know the reason. Before the issue happened, the envoy worked well and it could forward the request to the backend container.\r\nI guess 80% probability it's envoy issue. I tried to kill the envoy process in istio-proxy container causing envoy restart but not container, and I saw the issue disapper. Do you know who meet the same issue?\r\nIs there anyone know what happended or give some clues?\r\n\r\n*Repro steps*:\r\nI cannot 100% reproduct the issue, however the service (two instances) could randomly meet the issue once a week. The service is a java backend using tomcat and deployed as a jar.\r\n\r\nVersion (include the output of istioctl version --remote and kubectl version --short and helm version if you used Helm)\r\nistioctl version --remote\r\nclient version: 1.4.7\r\ncontrol plane version: 1.4.7\r\nkubectl version --short\r\nClient Version: v1.16.3\r\nServer Version: v1.16.3\r\n\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14176/comments",
    "author": "haniceboy",
    "comments": [
      {
        "user": "haniceboy",
        "created_at": "2020-12-16T06:24:26Z",
        "body": "Does anyone know what happened here?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-15T08:24:29Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-22T08:27:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14163,
    "title": "why not support localTimeZone for START_TIME?",
    "created_at": "2020-11-24T13:47:05Z",
    "closed_at": "2020-11-24T17:32:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14163",
    "body": "why not support localTimeZone for START_TIME ?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14163/comments",
    "author": "wangfakang",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2020-11-24T17:32:05Z",
        "body": "This is by design. c.f. #3546"
      }
    ]
  },
  {
    "number": 14151,
    "title": "Error using certificate rotation support for certificates in static resources.",
    "created_at": "2020-11-23T18:36:23Z",
    "closed_at": "2020-12-31T20:08:44Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14151",
    "body": "**Description** \r\n\r\n**Envoy version: 1.15.2**\r\n\r\nI am trying to make envoy pickup the renewed certs automatically. My envoy config looks like following\r\n\r\nRelevant portions of envoy config\r\n```\r\ntransport_socket:\r\n   name: envoy.transport_sockets.tls\r\n   typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n      common_tls_context:\r\n      tls_certificate_sds_secret_configs:\r\n         sds_config:\r\n            path: /envoy/envoy-sds.yaml\r\n      validation_context: {}\r\n      alpn_protocols:\r\n      - h2\r\n      - http/1.1\r\n```\r\n\r\nContents of envoy-sds.yaml\r\n```\r\nresources:\r\n- \"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\"\r\n  tls_certificate:\r\n    certificate_chain:\r\n      filename: \"/secrets/tls.crt\"\r\n    private_key:\r\n      filename: \"/secrets/tls.key\"\r\n```\r\n\r\nThis works as expected but sometimes envoy fails to start with following error\r\n```\r\nunable to add filesystem watch for file /envoy/envoy-sds.yaml: No space left on device\r\nfailed to watch file \r\n```\r\n\r\nThis goes away restarting the erring pods. \r\nHas others encountered similar issue ? If so how to fix this ?\r\n\r\nAny pointers will be very helpful\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14151/comments",
    "author": "aejeet",
    "comments": [
      {
        "user": "mk46",
        "created_at": "2020-11-24T10:55:54Z",
        "body": "@aejeet  I think this issue is related to your host machine. It seems that is running out of inotify watches."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-24T20:06:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-31T20:08:43Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14075,
    "title": "Questions re OAuth2 plugin behaviour",
    "created_at": "2020-11-18T11:07:50Z",
    "closed_at": "2021-01-30T04:06:58Z",
    "labels": [
      "question",
      "stale",
      "area/oauth"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14075",
    "body": "In the docs for the OAuth2 plugin it contains the following bullet point:-\r\n\r\n> - Upon receiving an access token, the filter sets cookies so that subseqeuent requests can skip the full flow. These cookies are calculated using the hmac_secret to assist in encoding.\r\n\r\nI'm wondering what this means in practical terms, e.g. is the access token sent back to the client as part of the cookie content? Or, is it an 'opaque' cookie that is sent and when subsequent requests are received from the client the access token is retrieved from some internal cache and added to the upstream requests?\r\n\r\nThis actually leads to another question, and, again quoting from the docs:\r\n\r\n> When the authn server validates the client and returns an authorization token back to the OAuth filter, no matter what format that token is, if forward_bearer_token is set to true the filter will send over a cookie named BearerToken to the upstream. Additionally, the Authorization header will be populated with the same value.\r\n\r\nIf the access token is returned as part of the cookie content, the client will not then populate the `Authorization` header value, it will simply send the cookie in all subsequent requests. In this instance what is the plugins behaviour? Does it extract the value from the BearerToken cookie and insert it as the `Authorization` header, or does this header value remain unset?\r\n\r\nMy final questions relate to cookie expiry, is it a simple session cookie that expires when the browser is closed? Also, if there is any kind of session cache in the plugin, is there a built in expiration based on inactivity, and if so what is the inactive period?\r\n\r\nApologies for asking these questions rather than simply testing the scenarios for myself but I'm not currently able to test things out.\r\n\r\nThanks in advance!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14075/comments",
    "author": "andye2004",
    "comments": [
      {
        "user": "junr03",
        "created_at": "2020-11-19T01:35:35Z",
        "body": "@williamsfu99 perhaps you can address these questions?"
      },
      {
        "user": "williamsfu99",
        "created_at": "2020-11-19T02:40:39Z",
        "body": "@junr03 Thanks for the tag.\r\n\r\n@andye2004 \r\n> e.g. is the access token sent back to the client as part of the cookie content? Or, is it an 'opaque' cookie that is sent and when subsequent requests are received from the client the access token is retrieved from some internal cache and added to the upstream requests?\r\n\r\nAfter the OAuth flow completely finishes, the client receives a response with SetCookie headers for an HMAC value and an Expiry epoch. The HMAC value is calculated by concatenating the domain, expiration, and token together before hashing it using the hmac-sha256(hmac_secret, data) function. Consequently, when that same user visits the domain again, the filter will first inspect these Cookie values, independently perform the same hash function, and compare these two values to validate the session; if this succeeds, the filter verifies that the current epoch is not larger than the Expiry cookie. As you may realize, the filter is stateless and the ability to auto-skip authentication is powered entirely by the exact contents of these explicit cookies. No internal cache.\r\n\r\n> If the access token is returned as part of the cookie content, the client will not then populate the Authorization header value, it will simply send the cookie in all subsequent requests. In this instance what is the plugins behaviour? Does it extract the value from the BearerToken cookie and insert it as the Authorization header, or does this header value remain unset?\r\n\r\nThe first behavior - it will extract the value from the BearerToken cookie and insert it into the Authorization header. This is an unconventional pattern but its purpose is to help the upstream expect consistently populated Authorization headers (when the boolean `forward_bearer_token` is flipped to true).\r\n\r\n> My final questions relate to cookie expiry, is it a simple session cookie that expires when the browser is closed? Also, if there is any kind of session cache in the plugin, is there a built in expiration based on inactivity, and if so what is the inactive period?\r\n\r\nThe cookies do not expire when the browser is necessarily closed - they are your standard HTTP Cookie headers with the `secure` and `httpOnly` flags enabled. The expiration epoch is determined from the `expires_in` field within the JSON response body received from the token_endpoint. Unless you clear these cookies in your browser, they will continue to be sent up until expiration.\r\n\r\nHope this helps - I do recognize that the filter needs some work before it can be widely adopted. We had to prune the filter substantially to omit proprietary interactions and the outcome is an oversimplified authentication plugin. Some improvements we need:\r\n* The filter should perform relevant data fetches - such as username - using the extracted token and pass them upstream for you, instead of expecting the upstream to do so.\r\n* Token signing from the token server should be supported, so that clients cannot arbitrary send invalid bearer tokens."
      },
      {
        "user": "juanvasquezreyes",
        "created_at": "2020-11-23T14:35:46Z",
        "body": "> Hope this helps - I do recognize that the filter needs some work before it can be widely adopted. We had to prune the filter substantially to omit proprietary interactions and the outcome is an oversimplified authentication plugin. Some improvements we need:\r\n> \r\n> * The filter should perform relevant data fetches - such as username - using the extracted token and pass them upstream for you, instead of expecting the upstream to do so.\r\n> * Token signing from the token server should be supported, so that clients cannot arbitrary send invalid bearer tokens.\r\n\r\nAnother enhancement is to add authorization mechanism such as group validations or roles validation and based on that deny or approved the request and forward to the cluster"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-23T16:14:57Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "andye2004",
        "created_at": "2020-12-23T23:12:16Z",
        "body": "I meant to respond to this before now but other things got in the way and I forgot, apologies. I just wanted to thank everyone, especially @williamsfu99, for commenting and clearing up some things. I'm sure in the longer term this will prove to be a very valuable plugin, but, for the moment at least, we will look at alternative methods of implementing oauth/oidc at the proxy level."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-23T00:36:57Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-30T04:06:58Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14060,
    "title": "NACK RDS rejections for a cluster that seems available in CDS",
    "created_at": "2020-11-17T04:29:17Z",
    "closed_at": "2020-11-18T00:07:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14060",
    "body": "*Title*: Seeing NACK RDS rejections for a cluster that seems available in CDS\r\n\r\n*Description*:\r\nUsing `envoyproxy/envoy-alpine:v1.15.0`\r\nWe are seeing NACK rejections from RDS in certain circumstances when populating both CDS and RDS to reach a remote cluster, with error looking like `route: unknown weighted cluster '<server/server>'`, from the client cluster, uwing `validate_clusters` in RDS config. \r\nIt seems, from envoy logging, that envoy does properly recognize the added/updated cluster, however the following RDS update using that cluster as a weighted cluster gives us back an error.\r\n\r\nAdditionally, when retried with the exact (diff-compared) same config, the RDS config is accepted. We are wondering if this can be a timing issue of sorts.\r\n\r\n```\r\n[2020-11-16 22:40:40.247][1][info][upstream] [source/common/upstream/cds_api_impl.cc:80] cds: add/update cluster 'server/server'\r\n[2020-11-16 22:40:40.249][1][info][upstream] [source/common/upstream/cds_api_impl.cc:80] cds: add/update cluster 'client0/client0-local'\r\n[2020-11-16 22:40:40.255][1][info][upstream] [source/server/lds_api.cc:74] lds: add/update listener 'outbound_listener'\r\n[2020-11-16 22:40:40.256][1][warning][config] [source/common/config/grpc_subscription_impl.cc:100] gRPC config for type.googleapis.com/envoy.config.route.v3.RouteConfiguration rejected: route: unknown weighted cluster 'server/server'\r\n```\r\n\r\n*Config*:\r\nAdding only relevant CDS and RDS to avoid clutter. \r\nCDS\r\n```\r\nresources:{[type.googleapis.com/envoy.config.cluster.v3.Cluster]:{name:\\\"server/server\\\" type:EDS eds_cluster_config:{eds_config:{ads:{} resource_api_version:V3}} connect_timeout:{seconds:1} http2_protocol_options:{} transport_socket:{name:\\\"envoy.transport_sockets.tls\\\" typed_config:{[type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext]:{common_tls_context:{tls_params:{tls_minimum_protocol_version:TLSv1_2 tls_maximum_protocol_version:TLSv1_3} tls_certificate_sds_secret_configs:{name:\\\"service-cert:client0/client0\\\" sds_config:{ads:{} resource_api_version:V3}} validation_context_sds_secret_config:{name:\\\"root-cert-for-mtls-outbound:server/server\\\" sds_config:{ads:{} resource_api_version:V3}} alpn_protocols:\\\"osm\\\"} sni:\\\"server.server.svc.cluster.local\\\"}}} protocol_selection:USE_DOWNSTREAM_PROTOCOL}}\r\n```\r\nRDS\r\n```\r\nresources:{[type.googleapis.com/envoy.config.route.v3.RouteConfiguration]:{name:\\\"RDS_Outbound\\\" virtual_hosts:{name:\\\"outbound_virtualHost|server\\\" domains:\\\"server.server.svc.cluster\\\" domains:\\\"server.server.svc.cluster.local\\\" domains:\\\"server.server:80\\\" domains:\\\"server.server.svc:80\\\" domains:\\\"server.server.svc.cluster:80\\\" domains:\\\"server.server.svc.cluster.local:80\\\" domains:\\\"server.server\\\" domains:\\\"server.server.svc\\\" routes:{match:{safe_regex:{google_re2:{} regex:\\\".*\\\"} headers:{name:\\\":method\\\" safe_regex_match:{google_re2:{} regex:\\\".*\\\"}}} route:{weighted_clusters:{clusters:{name:\\\"server/server\\\" weight:{value:100}} total_weight:{value:100}}}}} validate_clusters:{value:true}}}\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14060/comments",
    "author": "eduser25",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2020-11-17T20:08:12Z",
        "body": "> Additionally, when retried with the exact (diff-compared) same config, the RDS config is accepted. We are wondering if this can be a timing issue of sorts.\r\n\r\nThe target cluster 'server/server' can be found by RDS only if the cluster is warmed up.\r\nIf you can enable debug log, you can probably see the validation passed after \"warming cluster server/server complete\" shows up."
      },
      {
        "user": "eduser25",
        "created_at": "2020-11-18T00:07:03Z",
        "body": "Confirmed that's the case. Thanks lambdai."
      }
    ]
  },
  {
    "number": 14049,
    "title": "upstream_cx_active skyrocketing ",
    "created_at": "2020-11-16T22:36:56Z",
    "closed_at": "2020-11-18T16:40:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14049",
    "body": "*Title*: *Envoy 1:12.2 within an AKS cluster randomly generates high number of _upstream_cx_active_  producing _upstream_cx_destroy_with_active_rq_ and _upstream_rq_pending_failure_eject_ with traffic mirroring*\r\n\r\n*Description*:\r\nI'm running Envoy 1:12.2 as a sidecar within an AKS cluster and I'm trying to mirror traffic to another pod within the same cluster. The mirrored pod is behind an headless k8s service and the envoy service discovery type for this service is set to strict_dns.\r\n\r\nRunning a load test against the Envoy proxy with a load of 1200-1500 RPS consistently produces the following behavior:\r\n- upstream_cx_active is usually within the range of 50-100\r\n- periodically, the _upstream_cx_active_ skyrockets to 800-1200 and then either I got high number of _upstream_cx_destroy_with_active_rq_ or _upstream_rq_pending_failure_eject_, both resulting in 5xx errors. Running a netstat while _upstream_cx_active_ peaks shows a lot of established connections between the envoy proxy and the mirrored service\r\n\r\nDuring the test, the envoy container is never overloaded, and constantly runs at 50% of CPU assigned to it. \r\n\r\nI'm pretty sure I'm missing something really basic, I'd really appreciate if you could lend an hand here and pointing me to root cause.\r\nPlease find below the envoy configuration:\r\n\r\n  static_resources:\r\n\tlisteners:\r\n\t- address:\r\n\t\tsocket_address:\r\n\t\t  address: 0.0.0.0\r\n\t\t  port_value: 80\r\n\t  filter_chains:\r\n\t  - filters:\r\n\t\t- name: envoy.http_connection_manager\r\n\t\t  config:\r\n\t\t\tcodec_type: auto\r\n\t\t\tstat_prefix: ingress_http\r\n\t\t\troute_config:\r\n\t\t\t  name: local_route\r\n\t\t\t  virtual_hosts:\r\n\t\t\t  - name: bookservice\r\n\t\t\t\tdomains:\r\n\t\t\t\t- \"*\"\r\n\t\t\t\troutes:\r\n\t\t\t\t- match:\r\n\t\t\t\t\tprefix: \"/\" \r\n\t\t\t\t  route:\r\n\t\t\t\t\tcluster: local_service\r\n\t\t\t\t\trequest_mirror_policy:\r\n\t\t\t\t\t  cluster: mirror_service\r\n\t\t\thttp_filters:\r\n\t\t\t- name: envoy.router\r\n\t\t\t  config: {}\r\n\tclusters:\r\n\t- name: local_service\r\n\t  connect_timeout: 2.25s\r\n\t  type: static\r\n\t  lb_policy: round_robin\r\n\t  hosts:\r\n\t  - socket_address:\r\n\t\t  address: 127.0.0.1\r\n\t\t  port_value: 8080\r\n\t  circuit_breakers:\r\n\t\tthresholds:\r\n\t\t  max_connections: 10000\r\n\t\t  max_requests: 10000\r\n\t\t  max_pending_requests: 10000\r\n\t- name: mirror_service\r\n\t  connect_timeout: 2.25s\r\n\t  type: strict_dns\r\n\t  lb_policy: round_robin\r\n\t  hosts:\r\n\t  - socket_address:\r\n\t\t  address: {{ .Values.loggingService.ip }}\r\n\t\t  port_value: 80\r\n\t  circuit_breakers:\r\n\t\tthresholds:\r\n\t\t  max_connections: 10000\r\n\t\t  max_requests: 10000\r\n\t\t  max_pending_requests: 10000\r\n  admin:\r\n\taccess_log_path: \"/dev/stdout\"\r\n\taddress:\r\n\t  socket_address:\r\n\t\taddress: 0.0.0.0\r\n\t\tport_value: 8081",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14049/comments",
    "author": "tydurd",
    "comments": [
      {
        "user": "junr03",
        "created_at": "2020-11-17T16:48:28Z",
        "body": "@tydurd do you mind formatting the config so that it is easy to read?"
      },
      {
        "user": "yanavlasov",
        "created_at": "2020-11-17T20:22:16Z",
        "body": "The symptom indicates that the upstream can not keep up with the load. Or a networking problem preventing requests from reaching upstream servers."
      },
      {
        "user": "lambdai",
        "created_at": "2020-11-17T22:24:43Z",
        "body": "your upstream cluster is localhost. It is highly possible that there is a spike of new tcp SYN to localhost upstream, and both the SYN queue and accept queue are full.\r\nThen envoy will resend SYN packet in 3 seconds while your envoy connect timeout is 2.25s. \r\n\r\nTry increase connect time out to 5 or 10 seconds to see if number of upstream_cx_destroy_with_active_rq is reduced"
      },
      {
        "user": "tydurd",
        "created_at": "2020-11-18T16:40:27Z",
        "body": "Thanks a lot for the quick heads-up guys. After a full day of testing I've found out that the cause is that, occasionally, the mirrored service (which writes to an NFS volume) can't keep up with the load and hence the spikes in upstream_cx_active.  "
      }
    ]
  },
  {
    "number": 14032,
    "title": "envoy as a layer 2 proxy with downstream from HAproxy",
    "created_at": "2020-11-16T04:19:06Z",
    "closed_at": "2020-12-25T00:14:14Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14032",
    "body": "I have a HAproxy to route http2 grpc to server. But because grpc-web needs envoy so i configured envoy as a small reverse proxy with docker in server . HAproxy will reverse https connection to envoy which envoy will tls terminate it. But it got errors following:\r\n```\r\nenvoy_1  | [2020-11-16 04:01:22.744][25][debug][conn_handler] [source/server/connection_handler_impl.cc:459] [C213] new connection\r\nenvoy_1  | [2020-11-16 04:01:22.747][25][debug][connection] [source/common/network/connection_impl.cc:579] [C213] remote close\r\nenvoy_1  | [2020-11-16 04:01:22.747][25][debug][connection] [source/common/network/connection_impl.cc:202] [C213] closing socket: 0\r\nenvoy_1  | [2020-11-16 04:01:22.747][25][debug][misc] [source/common/network/io_socket_error_impl.cc:30] Unknown error code 104 details Connection reset by peer\r\nenvoy_1  | [2020-11-16 04:01:22.747][25][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:288] [C213] SSL shutdown: rc=-1\r\nenvoy_1  | [2020-11-16 04:01:22.747][25][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:215] [C213]\r\nenvoy_1  | [2020-11-16 04:01:22.747][25][debug][conn_handler] [source/server/connection_handler_impl.cc:152] [C213] adding to cleanup list\r\n\r\n```\r\nMy envoy yml config:\r\n```yml\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 10003 }\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address: { socket_address: { address: 0.0.0.0, port_value: 80 } }\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match: { prefix: \"/\" }\r\n                route:\r\n                  cluster: some_service\r\n              cors:\r\n                allow_origin_string_match:\r\n                - prefix: \"*\"\r\n                allow_methods: GET, PUT, DELETE, POST, OPTIONS\r\n                allow_headers: keep-alive,user-agent,cache-control,content-type,content-transfer-encoding,custom-header-1,x-accept-content-transfer-encoding,x-accept-response-streaming,x-user-agent,x-grpc-web,grpc-timeout\r\n                max_age: \"1728000\"\r\n                expose_headers: custom-header-1,grpc-status,grpc-message\r\n          http_filters:\r\n          - name: envoy.filters.http.grpc_web\r\n          - name: envoy.filters.http.cors\r\n          - name: envoy.filters.http.router   \r\n          stream_error_on_invalid_http_message: true\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n          common_tls_context:\r\n            alpn_protocols: [ \"h2,http/1.1\" ]\r\n            tls_certificates:\r\n              - certificate_chain: { filename: \"/etc/crt.pem\" }\r\n                private_key: { filename: \"/etc/key.pem\" }\r\n            validation_context:\r\n              trusted_ca:\r\n                filename: /etc/ca-cert.pem\r\n  clusters:\r\n  - name: some_service\r\n    connect_timeout: 0.25s\r\n    type: logical_dns\r\n    http2_protocol_options: {}\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: some_service\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 0.0.0.0\r\n                port_value: 10000\r\n```\r\nany ideas what happened? ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14032/comments",
    "author": "bettafish15",
    "comments": [
      {
        "user": "junr03",
        "created_at": "2020-11-17T16:59:25Z",
        "body": "From logs all I can gathered is the 104 error with connection reset by peer. If you can repro in isolation and gather a stat dump we could probably see more detail?"
      },
      {
        "user": "yanavlasov",
        "created_at": "2020-11-17T20:30:07Z",
        "body": "Does HAProxy have any logs that would explain why it reset the connection?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-18T00:09:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-25T00:14:13Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14008,
    "title": "When will envoy return \"100-continue\" to the client and write it into access log?",
    "created_at": "2020-11-13T06:02:30Z",
    "closed_at": "2020-11-17T19:46:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14008",
    "body": "In our production environment, we recently noticed there are some access log record showing envoy return `RESPONSE_CODE:100` to the client. But from client side, we got confirmed they never send any `Expect: 100-continue` header. Also I have tried if I added the `-H \"Expect: 100-continue\"` in the `curl` command with `-v`, I can see the response including a HTTP 100 response code followed by a HTTP 200 response code (as below `curl` output). And in the access log, this request is recorded as `RESPONSE_CODE:200`. Then it is confusing what triggered envoy return and record status code 100?\r\n\r\n```\r\n... ...\r\n> POST /v1/foo/bar HTTP/1.1\r\n> Host: foo.bar.net\r\n> User-Agent: curl/7.58.0\r\n> Accept: */*\r\n> Expect: 100-continue\r\n> Authorization: Bearer <tooooo long>\r\n> Content-Type: application/json\r\n> Content-Length: 332\r\n>\r\n* TLSv1.3 (IN), TLS Unknown, Certificate Status (22):\r\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\r\n* TLSv1.3 (IN), TLS Unknown, Unknown (23):\r\n< HTTP/1.1 100 Continue\r\n* TLSv1.3 (OUT), TLS Unknown, Unknown (23):\r\n* We are completely uploaded and fine\r\n* TLSv1.3 (IN), TLS Unknown, Unknown (23):\r\n< HTTP/1.1 200 OK\r\n< server: foo-bar-server\r\n< date: Thu, 12 Nov 2020 01:01:54 GMT\r\n< content-type: application/json\r\n... ...\r\n```\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14008/comments",
    "author": "NonStatic2014",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-11-13T22:48:10Z",
        "body": "cc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-11-16T14:54:19Z",
        "body": "My intuition was that 100-followed-by-200 would result in a 200, and 100 followed by disconnect would result in the 100 being logged, and looks correct according to a quick regression test.   I wonder if you have an upstream sending unexpected 100s, and your client is disconnecting?\r\n\r\n"
      },
      {
        "user": "NonStatic2014",
        "created_at": "2020-11-16T17:46:36Z",
        "body": "We don't see any 100 from upstream (an istio in kubernetes cluster) logs.\r\n\r\n\"100 followed by disconnect\": do you mean envoy received all headers but not start receiving body and then client disconnected somehow? In this case shouldn't I expect response code 0 with a DC flag?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-11-16T17:59:12Z",
        "body": "if an upstream sends 100-continue, and Envoy forwards it on, then the client disconnects, the 100 goes to the access logs.\r\n\r\nI don't know how 100 could end up in your access logs if neither Envoy nor your upstream is sending 100s, and AFIK Envoy only sends 100s when prompted, so I suspect either your client is occasionally sending expect 100-continue or your upstream is occasionally sending 100s.  I suspect the disconnect is because it's unexpected, which is why the full response isn't proxied."
      }
    ]
  },
  {
    "number": 13992,
    "title": "[Question] prefix_rewrite based on selected host in the upstream cluster",
    "created_at": "2020-11-12T07:30:38Z",
    "closed_at": "2020-11-14T16:44:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13992",
    "body": "*Title*: prefix_rewrite based on selected upstream cluster\r\n\r\n*Description*:\r\nIs there anyway possible to rewrite the prefix based on the selected host in a cluster?\r\nI have a static cluster of 4 hosts - 1,2,3,4 and when I receive a request with prefix `/msg.host_x/`, I want the `x` to be replaced by 1,2,3 or 4 based on whichever host is selected in the cluster.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13992/comments",
    "author": "rkbalgi",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2020-11-14T09:36:06Z",
        "body": "quoting @antoniovicente on slack channel:\r\n\r\n> No, that's not possible.  One of the things that makes this kind of rewrite difficult is that the proxy would need to rewrite the request again if a different host is selected on retry.\r\n"
      },
      {
        "user": "rkbalgi",
        "created_at": "2020-11-14T16:44:10Z",
        "body": "Closing as this is not possible at the moment. Thanks guys."
      }
    ]
  },
  {
    "number": 13977,
    "title": "Can Envoy support IPVS?",
    "created_at": "2020-11-11T14:37:43Z",
    "closed_at": "2020-12-21T00:10:30Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13977",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: Can Envoy support IPVS networking mode? Or only support iptables?\r\n\r\n*Description*:\r\n> There are 3 kinds of container communication mode (userspace, iptables, ipvs), I am not sure whether Enovy can support ipvs or not? If not, why?\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13977/comments",
    "author": "malphi",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-14T00:08:26Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-21T00:10:30Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 13962,
    "title": "Problems with logrotate",
    "created_at": "2020-11-10T14:59:37Z",
    "closed_at": "2020-12-21T00:10:29Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13962",
    "body": "I am trying to configure log rotation for access logs. \r\nHere are relevant parts of the config:\r\n\r\nEnvoy yaml:\r\n```\r\naccess_log:\r\n            name: envoy.access_loggers.file\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n              path: \"/var/log/envoy/envoy.log\"\r\n              json_format:\r\n                  starttime: \"%START_TIME%\"\r\n                  rduration: \"%DURATION%\"\r\n                  responsecode: \"%RESPONSE_CODE%\"\r\n                  bytesr: \"%BYTES_RECEIVED%\"\r\n                  bytess: \"%BYTES_SENT%\"\r\n                  rcaller: \"%REQ(X-FORWARDED-FOR)%\"\r\n                  routename: \"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\"\r\n```\r\n\r\nlogrotate.d conf file:\r\n```\r\n/var/log/envoy/envoy.log\r\n{\r\n    missingok\r\n    daily\r\n    create 0644 root root\r\n    rotate 50\r\n    size=100M\r\n    dateext\r\n    dateformat -%d%m%Y\r\n    sharedscripts\r\n    postrotate\r\n\t/bin/kill -USR1 `pgrep envoy` 2> /dev/null || true\r\n    endscript\r\n}\r\n```\r\n\r\nRunning logrotate by hand:\r\n\r\n```reading config file /etc/logrotate.d/envoy\r\nAllocating hash table for state file, size 15360 B\r\n\r\nHandling 1 logs\r\n\r\nrotating pattern: /var/log/envoy/envoy.log\r\n 104857600 bytes (50 rotations)\r\nempty log files are rotated, old logs are removed\r\nconsidering log /var/log/envoy/envoy.log\r\n  log needs rotating\r\nrotating log /var/log/envoy/envoy.log, log->rotateCount is 50\r\nConverted ' -%d%m%Y' -> '-%d%m%Y'\r\ndateext suffix '-10112020'\r\nglob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'\r\nglob finding old rotated logs failed\r\nrenaming /var/log/envoy/envoy.log to /var/log/envoy/envoy.log-10112020\r\ncreating new /var/log/envoy/envoy.log mode = 0644 uid = 0 gid = 0\r\nrunning postrotate script\r\nrunning script with arg /var/log/envoy/envoy.log\r\n: \"\r\n\t/bin/kill -USR1 `pgrep envoy` 2> /dev/null || true\r\n\"`\r\n```\r\n\r\nYet, envoy.log is still in place and growing ... Any idea what I'm doing wrong ?\r\nThank you!\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13962/comments",
    "author": "dantodor",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-14T00:08:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-21T00:10:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "clavinjune",
        "created_at": "2021-01-22T09:58:57Z",
        "body": "Have you found the answer yet?"
      },
      {
        "user": "clavinjune",
        "created_at": "2021-01-27T08:59:32Z",
        "body": "Hi @dantodor I happened to meet the same issue as yours. in the end I implemented my own log rotator manually, and this seems work, just in case you need a reference\r\n\r\n```bash\r\n#!/bin/sh\r\n\r\n# create this because the logrotate doesn't work\r\n\r\nset -e\r\n\r\nlog_file=\"$1\"\r\n\r\nif [ $# -lt 1 ]; then\r\n\techo \"$0 <log_file_name>\"\r\n\texit 1\r\nfi\r\n\r\nenvoy_pid=$(ps aux | grep envoy | grep root | awk -F' ' '{print $2}')\r\nlog_fd=$(sudo ls -l \"/proc/$envoy_pid/fd\" | grep \"$log_file\" | awk -F' ' '{print $9}')\r\nnow=$(date +\"%Y-%m-%d\")\r\n\r\ncat $log_file >> \"$log_file-$now\"\r\n# gzip -9 \"$log_file-$now\"\r\n\r\nsudo -u root bash <<EOF\r\n> \"/proc/$envoy_pid/fd/$log_fd\"\r\nEOF\r\n```"
      },
      {
        "user": "kinghrothgar",
        "created_at": "2022-03-29T21:40:45Z",
        "body": "Leaving this for future people finding this via Googling. This is the logrotate config I ended up with that worked:\r\n```\r\n/var/log/envoy/envoy.log {\r\n    missingok\r\n    weekly\r\n    compress\r\n    notifempty\r\n    nocreate\r\n    rotate 5\r\n    size=100M\r\n    sharedscripts\r\n    copytruncate\r\n}\r\n```\r\n\r\nThis `copytruncate` is the main directive that I think is needed."
      }
    ]
  },
  {
    "number": 13928,
    "title": "Question: Can I front proxy from a wildcard subdomain to upstream cluster with matching wildcard subdomain host pattern?",
    "created_at": "2020-11-06T20:59:44Z",
    "closed_at": "2020-12-17T08:03:37Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13928",
    "body": "*Title*: *Question: Can envoy front proxy from a wildcard subdomain to upstream cluster with matching wildcard subdomain host pattern?*\r\n\r\n*Description*:\r\n\r\nI have a goal of serving a front-proxy for an existing web service that is traditionally hosted at a wildcard domain such as *.my-service.com, so that it can also be accessed from an alternate wildcarded hostname such as *.my-service.services.my-company.com. We have some options for solving this problem of hosting the service at more than one wildcard domain, and I am researching the feasibility of using envoy as one of those options. I have very little experience with envoy, and proxy networking in general, but I have a decent grasp of http, and have spent a few days reading envoy documents and trying things out.\r\n\r\nI've set up envoy to listen to *.my-service.services.my-company.com, and am trying to find a way to:\r\n- Rewrite the incoming host header from <something>.my-service.services.my-company.com to <something>.my-service.com.\r\n- Configure a cluster that can forward the incoming requests to <something>.my-service.com.\r\n\r\nIt is important that the value of the leftmost subdomain in the request coming into envoy be passed to the upstream cluster host as the leftmost subdomain.\r\n\r\nI have not yet looked into using EDS or CDS to resolve upstream endpoints or cluster configuration at runtime, but have looked at LOGICAL_DNS and STATIC_DNS cluster configurations, and achieved some success with them when using hardcoded subdomains (such as foo.my-service.com). There is no need to balance load across multiple upstream IP addresses or hosts, as *.my-service.com is a logical service which hides its clustering and load balancing details from external systems like envoy.\r\n\r\nI and some from my team have looked for examples of how to do this and are finding them to be elusive, if they exist. Looking for some guidance here as my current understanding of envoy's capabilities do not seem to be dovetailing very well with my research goals. If you had a problem similar to the one stated here, would you solve it with envoy, and if so, how?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13928/comments",
    "author": "danludwig",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-10T04:03:58Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-17T08:03:37Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "danludwig",
        "created_at": "2020-12-18T19:40:02Z",
        "body": "bump"
      }
    ]
  },
  {
    "number": 13852,
    "title": "How to avoid \"panic: listener accept failure: Too many open files\"",
    "created_at": "2020-11-01T05:49:07Z",
    "closed_at": "2020-12-26T00:12:30Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13852",
    "body": "I'm running Envoy 1.5 on Ubuntu 18.04 (2 cores, 8G) and I have increased FD to a very large number. However, Envoy always crashes when WebSocket connection reaches about 500.  This is such a small number.\r\n\r\nUpstream is a cluster of 2 servers. Really confused by this annoying issue. I have checked similar issues in history but I couldn't get through it.\r\n\r\nif I simply replace Envoy with Haproxy, it can support 2000 WebSocket connections without any additional settings. \r\n\r\nIn Nginx, I improved `worker_connections` to a bigger number. Is there any equivalent setting like `worker_connections`?\r\n\r\nAppreciate any hit on this issue.\r\n\r\n```\r\nulimit -a\r\ncore file size          (blocks, -c) 0\r\ndata seg size           (kbytes, -d) unlimited\r\nscheduling priority             (-e) 0\r\nfile size               (blocks, -f) unlimited\r\npending signals                 (-i) 31705\r\nmax locked memory       (kbytes, -l) 16384\r\nmax memory size         (kbytes, -m) unlimited\r\nopen files                      (-n) 400000\r\npipe size            (512 bytes, -p) 8\r\nPOSIX message queues     (bytes, -q) 819200\r\nreal-time priority              (-r) 0\r\nstack size              (kbytes, -s) 8192\r\ncpu time               (seconds, -t) unlimited\r\nmax user processes              (-u) 65535\r\nvirtual memory          (kbytes, -v) unlimited\r\nfile locks                      (-x) unlimited\r\n```\r\n\r\nRight before crash:\r\n```\r\ncat /proc/net/sockstat\r\nsockets: used 1141\r\nTCP: inuse 1000 orphan 0 tw 0 alloc 1001 mem 4\r\nUDP: inuse 2 mem 1\r\nUDPLITE: inuse 0\r\nRAW: inuse 0\r\nFRAG: inuse 0 memory 0\r\n```\r\n\r\n```\r\ncat /proc/sys/net/core/somaxconn\r\n10240\r\n```\r\n\r\n```\r\nNov  1 06:07:42 portal bash[1076]: [2020-11-01 06:07:42.001][1422][critical][assert] [external/envoy/source/common/network/listener_impl.cc:111] panic: listener accept failure: Too many open files\r\nNov  1 06:07:42 portal bash[1076]: [2020-11-01 06:07:42.001][1422][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] Caught Aborted, suspect faulting address 0x49b\r\nNov  1 06:07:42 portal bash[1076]: [2020-11-01 06:07:42.001][1422][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:91] Backtrace (use tools/stack_decode.py to get line numbers):\r\nNov  1 06:07:42 portal bash[1076]: [2020-11-01 06:07:42.001][1422][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:92] Envoy version: 670a4a60080f5889d9f7c022a009dcf25f1062ca/1.15.1/clean-getenvoy-a5345f6-envoy/RELEASE/BoringSSL\r\nNov  1 06:07:42 portal bash[1076]: [symbolize_elf.inc : 951] RAW: /proc/self/task/1179/maps: errno=24\r\nNov  1 06:07:42 portal bash[1076]: [2020-11-01 06:07:42.001][1422][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #0: [0x7f76d77b78a0]\r\nNov  1 06:07:42 portal bash[1076]: [2020-11-01 06:07:42.001][1422][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #1: [0x55dcd2da1c15]\r\nNov  1 06:07:42 portal bash[1076]: [2020-11-01 06:07:42.001][1422][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #2: [0x55dcd2d9fd6b]\r\nNov  1 06:07:42 portal bash[1076]: [2020-11-01 06:07:42.001][1422][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #3: [0x55dcd2d9e5ee]\r\nNov  1 06:07:42 portal bash[1076]: [2020-11-01 06:07:42.001][1422][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #4: [0x55dcd29212f4]\r\nNov  1 06:07:42 portal bash[1076]: [2020-11-01 06:07:42.002][1422][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #5: [0x55dcd2e50e43]\r\nNov  1 06:07:42 portal bash[1076]: [2020-11-01 06:07:42.002][1422][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #6: [0x7f76d77ac6db]\r\nNov  1 06:07:42 portal bash[1076]: /opt/envoy/envoy.sh: line 1:  1081 Aborted                 sudo envoy -c /opt/envoy/envoy.yaml\r\nNov  1 06:07:42 portal systemd[1]: envoy.service: Main process exited, code=exited, status=134/n/a\r\nNov  1 06:07:42 portal systemd[1]: envoy.service: Failed with result 'exit-code'.\r\nNov  1 06:07:52 portal systemd[1]: envoy.service: Service hold-off time over, scheduling restart.\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13852/comments",
    "author": "xudesheng",
    "comments": [
      {
        "user": "xudesheng",
        "created_at": "2020-11-01T19:27:02Z",
        "body": "After upgrade to 1.16, error is similar\r\n```\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][assert] [external/envoy/source/common/network/socket_interface_impl.cc:51] assert failure: SOCKET_VALID(result.rc_). Details: socket(2) failed, got error: Too many open files\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] Caught Aborted, suspect faulting address 0x102a\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:91] Backtrace (use tools/stack_decode.py to get line numbers):\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:92] Envoy version: 8fb3cb86082b17144a80402f5367ae65f06083bd/1.16.0/clean-getenvoy-a5345f6-envoy/RELEASE/BoringSSL\r\nNov  1 19:22:20 portal envoy[4138]: [symbolize_elf.inc : 965] RAW: /proc/self/task/4138/maps: errno=24\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #0: [0x7fd69db838a0]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #1: [0x55e2c841ef7a]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #2: [0x55e2c7fecc29]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #3: [0x55e2c7feadad]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #4: [0x55e2c7fdb6db]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #5: [0x55e2c816d383]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #6: [0x55e2c816d088]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #7: [0x55e2c816d5b5]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #8: [0x55e2c815e582]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #9: [0x55e2c815e458]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #10: [0x55e2c815d688]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #11: [0x55e2c8168cdd]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #12: [0x55e2c816a46b]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #13: [0x55e2c81679ef]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.467][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #14: [0x55e2c834356b]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #15: [0x55e2c834914f]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #16: [0x55e2c82a2df3]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #17: [0x55e2c82968cc]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #18: [0x55e2c82b4ffa]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #19: [0x55e2c82b552d]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #20: [0x55e2c82b353d]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #21: [0x55e2c843067d]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #22: [0x55e2c82b250a]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #23: [0x55e2c82b208f]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #24: [0x55e2c82bb162]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #25: [0x55e2c8402467]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #26: [0x55e2c82b1d21]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #27: [0x55e2c829291c]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #28: [0x55e2c7fee503]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #29: [0x55e2c7fea385]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #30: [0x55e2c7fe823b]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #31: [0x55e2c7fdda26]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #32: [0x55e2c842b16b]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #33: [0x55e2c8429ace]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #34: [0x55e2c7fceb43]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #35: [0x55e2c85f1bd3]\r\nNov  1 19:22:20 portal envoy[4138]: [2020-11-01 19:22:20.468][4170][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:98] #36: [0x7fd69db786db]\r\nNov  1 19:22:20 portal envoy[4138]: ActiveStream 0xf98be55a800, stream_id_: 15835697160804341381&filter_manager_:\r\nNov  1 19:22:20 portal envoy[4138]:   FilterManager 0xf98be55a878, state_.has_continue_headers_: 0, state_.decoding_headers_only_: 0, state_.encoding_headers_only_: 0\r\n```"
      },
      {
        "user": "xudesheng",
        "created_at": "2020-11-01T19:34:33Z",
        "body": "Some settings:\r\n```\r\ncat /proc/sys/fs/file-max\r\n999999\r\ncat /proc/sys/fs/nr_open\r\n999999\r\nulimit -n\r\n65535\r\n```\r\n\r\nwhen crash happens:\r\n```\r\ncat /proc/net/sockstat\r\nsockets: used 1144\r\nTCP: inuse 1000 orphan 0 tw 0 alloc 1001 mem 996\r\nUDP: inuse 2 mem 1\r\nUDPLITE: inuse 0\r\nRAW: inuse 0\r\nFRAG: inuse 0 memory 0\r\n```"
      },
      {
        "user": "xudesheng",
        "created_at": "2020-11-01T19:44:56Z",
        "body": "What bothered me most is: on this machine, I can easily connect thousands of Websockets clients with **Nginx** or **Haproxy**. So, I guess I have missed some settings in Envoy but not on the OS side."
      },
      {
        "user": "lambdai",
        "created_at": "2020-11-18T23:58:41Z",
        "body": "Chatted with @xudesheng offline and confirmed the limit is defined in  \r\n`/etc/security/limits.conf`"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-19T00:08:49Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-26T00:12:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 13795,
    "title": "Rate limiting before Scheme redirection",
    "created_at": "2020-10-28T10:56:59Z",
    "closed_at": "2021-01-15T04:26:59Z",
    "labels": [
      "question",
      "stale",
      "area/ratelimit"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13795",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *Rate limiting before Scheme redirection*\r\n\r\n*Description*: i have http to https redirection, i am using envoy as front proxy, i would like to rate limit before redirection on remote address, this doesn't seem to work for me, i have rate limit specified in virtual host, not sure if rate limit is applied before or after redirection\r\n>Describe the issue.\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13795/comments",
    "author": "p53",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-09T02:57:13Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-08T04:24:03Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-15T04:26:58Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 13790,
    "title": "Some requests were dropped by envoy?",
    "created_at": "2020-10-28T00:51:59Z",
    "closed_at": "2020-11-13T02:28:29Z",
    "labels": [
      "question",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13790",
    "body": "*Title*: *Some requests were dropped by envoy?*\r\n\r\n*Description*:\r\nIn our envoy, we log twice: \r\n\r\n1. at very beginning when request arrives, we log a unique id in the request header in the lua filter;\r\n```lua\r\nlocal some_id = request_handle:headers():get(\"some-id\")\r\nif some_id ~= nil then\r\n  request_handle:logInfo(\"some-id: \"..string.sub(some_id , 1, 36))\r\nelse\r\n  request_handle:logInfo(\"some-id: -\")\r\nend\r\n```\r\n2. the access log, including the same request header.\r\n\r\nAfter run a while, we see some of requests could only be found in the #1 log above. Is there any clue to debug this issue? Currently the issue only repro in the prod environment, which may not be ok to turn on tracing log. \r\n\r\nThanks!\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13790/comments",
    "author": "NonStatic2014",
    "comments": [
      {
        "user": "dio",
        "created_at": "2020-10-28T01:15:04Z",
        "body": "@NonStatic2014 probably the access logs are not flushed yet? "
      },
      {
        "user": "NonStatic2014",
        "created_at": "2020-11-13T02:28:29Z",
        "body": "After debugging into the network setup, it was due to the access log was dropped by our logging system."
      }
    ]
  },
  {
    "number": 13643,
    "title": "Getting upstream_reset_before_response_started{local reset}",
    "created_at": "2020-10-20T06:49:41Z",
    "closed_at": "2021-01-15T04:26:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13643",
    "body": "*Title*: While doing perf testing we start getting the response upstream_reset_before_response_started{local reset} intermittently\r\n\r\n*Description*: We are using envoy as reverse proxy, where a request from client hits envoy and based on some header we route it to upstream configured with 4 hosts. It works fine based on functionality point, however while doing perf we start getting upstream_reset_before_response_started{local reset} intermittently on few upstream hosts.\r\n>Describe the issue. Getting upstream_reset_before_response_started{local reset} in response .\r\n\r\nCan you please help me understand what could be the reason behind this ? CPU usage of envoy and upstream is within limits.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13643/comments",
    "author": "hinawatts",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2020-10-22T00:58:40Z",
        "body": "There are multiple reasons for local reset. For instance response timeout, invalid messaging from upstream, etc. You need to see if there are any other counters that go up with the upstream_reset_before_response_started."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-09T02:57:37Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-08T04:23:41Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-15T04:26:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 13553,
    "title": "Wasm: Extract response protocol in the onResponseHeaders function",
    "created_at": "2020-10-14T00:09:22Z",
    "closed_at": "2020-12-27T00:13:08Z",
    "labels": [
      "question",
      "stale",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13553",
    "body": "### Description\r\nWe cannot seem to find an approach to extract the HTTP response protocol in the onResponseHeaders function. It seems there is no corresponding attribute that has this information. It also appears that the stream object itself is not accessible through the proxy wasm API which is there in the Lua API.\r\n\r\n### Observations / Areas Explored\r\n- In the request flow, the onRequestHeaders function does have an attribute called request protocol which is accessible through the getValue function call. However, there does not exist a response protocol attribute that can be accessed in onResponseHeaders which has similar information.\r\n- Some attributes such as response code are not accessible in the onResponseHeaders function but available in others functions such as onResponseBody or onLog.\r\n\r\n### Questions\r\n1. Is there an approach to extract the response protocol in the onResponseHeaders function?\r\n2. If an option does not exist through the API, would it be possible using a foreign function interface instead?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13553/comments",
    "author": "mcieplak",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2020-10-14T00:31:33Z",
        "body": "@PiotrSikora can you help out with this one? Thanks."
      },
      {
        "user": "PiotrSikora",
        "created_at": "2020-10-14T08:16:01Z",
        "body": "@kyessenov is the expert of all properties exposed in Wasm."
      },
      {
        "user": "kyessenov",
        "created_at": "2020-10-14T17:52:28Z",
        "body": "Request and response protocol are usually the same, since it is negotiated in the codec for two parties to be able to understand each other. Is that not the case for you?"
      },
      {
        "user": "mcieplak",
        "created_at": "2020-10-22T18:41:25Z",
        "body": "@kyessenov I thought the request protocol is actually the protocol from the incoming client connecting to Envoy. Is this not the case? If the Wasm filter needs to be attached before the main envoy.router filter then I believe we do not have the upstream cluster information and it's corresponding protocol here in the onRequest flow.\r\n\r\nThis is the reason which I was hoping the response protocol would have this information in the onResponse flow as that is part of the connection between Envoy and the upstream. Are there any properties here that contain this information?"
      },
      {
        "user": "kyessenov",
        "created_at": "2020-11-04T20:51:02Z",
        "body": "@mcieplak It makes sense to record the upstream protocol. I'm not sure if it's correct to call it response protocol, since it would be strange to have request and response have mismatched protocols (e.g. http/1.1 and h2). Maybe upstream.protocol? "
      },
      {
        "user": "mcieplak",
        "created_at": "2020-11-19T17:37:05Z",
        "body": "@kyessenov I agree that response protocol sounds a little bit confusing as I meant the protocol in terms of the upstream. It makes sense to call it upstream protocol instead. Is this attribute already available or is this something that will need to be added?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-19T20:03:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-27T00:13:08Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 13403,
    "title": "Allow IPs with TCP Listener using RBAC (Not working)",
    "created_at": "2020-10-06T06:39:49Z",
    "closed_at": "2020-12-09T16:17:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13403",
    "body": "Hi :wave: \r\n\r\nI am trying to achieve the following:\r\n\r\n- Allow TCP traffic only from the following IP addresses. \r\n\r\nThis is my listener setup.\r\n\r\n```yaml\r\n    - name: listener_postgres\r\n      address:\r\n        socket_address:\r\n          protocol: TCP\r\n          address: 0.0.0.0\r\n          port_value: 54322\r\n      filter_chains:\r\n        filters:\r\n          - name: envoy.filters.network.rbac\r\n            config:\r\n              stat_prefix: rbac_postgres\r\n              rules:\r\n                action: ALLOW\r\n                policies:\r\n                  \"allow\":\r\n                    permissions:\r\n                      - any: true\r\n                    principals:\r\n                      - source_ip:\r\n                          address_prefix: XX.XX.XX.XX\r\n                          prefix_len: 32\r\n                      - source_ip:\r\n                          address_prefix: XX.XX.XX.XX\r\n                          prefix_len: 32\r\n                      - source_ip:\r\n                          address_prefix: XX.XX.XX.XX\r\n                          prefix_len: 32\r\n          - name: envoy.tcp_proxy\r\n            config:\r\n              stat_prefix: tcp_postgres\r\n              cluster: database_service\r\n``` \r\n\r\nBut for some reason nothing seems to work, I have also tried *remote_ip* and *direct_remote_ip* in place of *source_ip*.\r\n\r\nAm I doing something wrong?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13403/comments",
    "author": "mrbenosborne",
    "comments": [
      {
        "user": "mrbenosborne",
        "created_at": "2020-10-13T07:37:00Z",
        "body": "@mattklein123 Any update on this?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-09T02:58:08Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      }
    ]
  },
  {
    "number": 13376,
    "title": "Envoy service proxy processes HTTP 1.1 requests with increasing delay on a persistent connection",
    "created_at": "2020-10-02T15:40:03Z",
    "closed_at": "2020-12-16T08:03:41Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13376",
    "body": "*Title*: *Envoy service proxy processes HTTP 1.1 requests with increasing delay on a persistent connection*\r\n\r\n*Description*:\r\n> I have a custom HTTP application which supports sending multiple HTTP1.1-wrapped RPC calls (using protobufs) from a client to a server on a persistent TCP connection. These are on different hosts and I have put an Envoy sidecar on each for egress and ingress, handling TLS and mTLS auth.\r\n> \r\n> I can see that the caller service dispatches the requests to Envoy immediately but they show up in Envoy's log with an increasing delay (varying between several seconds to several minutes). The communication between the Envoys doesn't show increasing latency. I suspect this could be because we open a persistent TCP connection and then send successive HTTP requests to the egress Envoy, but the egress Envoy might be waiting for the response of each request before dispatching the next - in other words serializing the requests.\r\n> If yes, Is there a way to support such configurations, or should we switch to an HTTP2 client?\r\n> If no, what could be the problem?\r\n\r\nVersion of Envoy: 1.12.2\r\n\r\n[optional *Relevant Links*:]\r\n>Envoy log snippet (from the egress proxy.\r\n\r\n```[2020-10-01 22:52:25.081][2019][debug][http] [source/common/http/conn_manager_impl.cc:259] [C69950] new stream\r\n\r\n[2020-10-01 22:52:25.081][2019][debug][http] [source/common/http/conn_manager_impl.cc:708] [C69950][S2904751784239169371] request headers complete (end_stream=false):\r\n':authority', 'www'\r\n':path', '/foo/bar.RpcSvc/'\r\n':method', 'POST'\r\n'content-type', 'application/x-myrpc'\r\n'content-length', '446'\r\n'some-unique-id', '1097097667177086992'\r\n'x-foo-envoy-clus', 'remote1'\r\n\r\n[2020-10-01 22:52:25.081][2019][debug][lua] [source/extensions/filters/common/lua/lua.cc:37] coroutine finished\r\n\r\n[2020-10-01 22:52:25.081][2019][debug][router] [source/common/router/router.cc:549] [C69950][S2904751784239169371] router decoding headers:\r\n':authority', 'www'\r\n':path', '/foo/bar.RpcSvc/'\r\n':method', 'POST'\r\n':scheme', 'https'\r\n'content-type', 'application/x-myrpc'\r\n'content-length', '446'\r\n'some-unique-id', '1097097667177086992'\r\n'x-foo-envoy-clus', 'remote1'\r\n'x-forwarded-for', 'myip'\r\n'x-forwarded-proto', 'http'\r\n'x-envoy-internal', 'true'\r\n'x-request-id', '198df9bd-45d0-43f6-aff2-3f1d2bbe9efc'\r\n'x-hop-count', '1'\r\n\r\n[2020-10-01 22:52:25.081][2019][debug][router] [source/common/router/router.cc:1618] [C69950][S2904751784239169371] pool ready\r\n\r\n[2020-10-01 22:52:25.081][2019][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C69950][S2904751784239169371] request end stream\r\n\r\n[2020-10-01 22:52:25.084][2019][debug][router] [source/common/router/router.cc:1036] [C69950][S2904751784239169371] upstream headers complete: end_stream=false\r\n\r\n[2020-10-01 22:52:25.084][2019][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C69950][S2904751784239169371] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'reorder-request', '1097097667177086992'\r\n'content-length', '71'\r\n'x-envoy-upstream-service-time', '2'\r\n'date', 'Thu, 01 Oct 2020 22:52:24 GMT'\r\n'server', 'envoy'\r\n```\r\n\r\n\r\nThe corresponding tcpdump record indicating that the request hit the network as soon as it was sent from the caller and wasn't queued at the caller's end.\r\n\r\n\r\n```\r\n22:52:07.870293 IP (tos 0x0, ttl 64, id 24011, offset 0, flags [DF], proto TCP (6), length 700) localhost.58624 > localhost.8331: Flags [P.], cksum 0x00b1 (incorrect -> 0xfecd), seq 7978:8638, ack 369, win 128, length 660\r\nE...].@.@..n..........$.Eq.....<P.......POST /foo/bar.RpcSvc/ HTTP/1.1\r\nContent-Type: application/x-myrpc\r\nContent-Length: 446 \r\nHost: www\r\nSome-Unique-Id: 1097097667177086992\r\nX-foo-envoy-clus: remote1\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13376/comments",
    "author": "amukherj1",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-10-04T20:36:20Z",
        "body": "This version is very old. Please try with latest and report back."
      },
      {
        "user": "amukherj1",
        "created_at": "2020-10-04T20:42:35Z",
        "body": "> This version is very old. Please try with latest and report back.\r\n\r\nI will try to. Just for my understanding, does this seem like some known issue with older versions?"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-10-04T20:44:03Z",
        "body": "There have been many changes that might effect those over the last year, hard to say."
      },
      {
        "user": "lipingtababa",
        "created_at": "2020-10-07T09:53:38Z",
        "body": "Hi @amukherj1 We have almost the identical problem. The communications between our envoy ingress sidecar and the AWS  LB are okay, but we from time to time experience delays between the ingress sidecar and the service container that share an awsvpc ENI.\r\nThe tcpdump files show that the service sends out the IP packets in time, but the envoy/sidecar doesn't receive it until 1-10s later.\r\nAnd it happens when the traffic, CPU utilization and everything else looks fine.\r\nAnother interesting thing to note is that it happens to one request per time, never in bulk.\r\n\r\nWe are going to upgrade our version from 1.10.0 to 1.14.3 and will share our results here."
      },
      {
        "user": "amukherj1",
        "created_at": "2020-10-14T18:55:08Z",
        "body": "The behavior seems to be the same with Envoy 1.15. I will try to put the test code (Golang) and config in a repo and share it here soon."
      },
      {
        "user": "lambdai",
        "created_at": "2020-10-24T08:53:21Z",
        "body": "I remember elder envoy didn't support http pipelining in http1. \r\nWhen envoy is not done with the current response, the early next request bytes delivered to envoy. \r\nThese new bytes is not lost but the dispatcher lost the notification.\r\n\r\nSomehow the dispatcher may be notified by unrelated events or timer.\r\n\r\nYour minutes of delay fit this pattern. And you should be able to confirm if your requests form the http pipeline case with a tcpdump running at envoy server side."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-09T03:10:49Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-16T08:03:40Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 13330,
    "title": "Ability to route match based on one or more headers",
    "created_at": "2020-09-30T16:04:37Z",
    "closed_at": "2020-12-16T04:04:12Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13330",
    "body": "Hi,\r\n\r\nI know we can route based on a single header field, is it possible to route based on say either header-X or header-Y\r\n\r\n`{\r\n                          \"match\": {\r\n                            \"prefix\": \"/\"\r\n                          },\r\n                          \"route\": {\r\n                            \"cluster_header\": \"header-X\"\r\n                          }\r\n                        }`",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13330/comments",
    "author": "rbkumar88",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-10-04T20:35:09Z",
        "body": "No it's not currently possible to do this kind of compound matching, however we are starting to look at what a new route config with a more extensible matching system might look like that would support something like this. cc @snowp "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-09T03:10:57Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-16T04:04:11Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 13286,
    "title": "How in envoy with link command reverting app to one of prior version to rollback migrations?",
    "created_at": "2020-09-27T06:58:05Z",
    "closed_at": "2020-12-16T04:04:02Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13286",
    "body": "With envoy deployment for Laravel apps most usefull feature seems possibility with link command revert app to one of prior versions. But What if with any new version upload migrations were applied? Is it possible with with link command to rollback migrations by given steps back? Are there some scripts/pluging for it ?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13286/comments",
    "author": "PetroGromovo",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-09T03:11:07Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-16T04:04:01Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 13170,
    "title": "Lots of 503 Errors after adding retries",
    "created_at": "2020-09-18T14:37:55Z",
    "closed_at": "2020-12-16T04:03:49Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13170",
    "body": "Hi, we have been using Envoy for a while now and have always seen a small amount of 503s occurring with the **UF** flag. We could not figure out the root cause of these errors so we decided to mitigate them by adding route level retries on the \"connect-failure\" condition (Envoy will attempt a retry if a request is failed because of a connection failure to the upstream server (connect timeout, etc.). (Included in 5xx)).\r\n\r\nAfter one load test it seemed like the retries had done their job as we saw much less errors reported. However, after our latest test we saw an increase in overall 503s and some flags we had not seen before: \r\n\r\nUF, **URX**: The request was rejected because the upstream retry limit (HTTP) or maximum connect attempts (TCP) was reached.\r\n**LR**: Connection local reset in addition to 503 response code.\r\nUF, **UO**: Upstream overflow (circuit breaking) in addition to 503 response code.\r\n**UC**: Upstream connection termination in addition to 503 response code.\r\n\r\nThe UF, URX I understand as it means even after retries, the request did not succeed and we got those errors. The others however we did not expect and I'm not sure why they are popping up now. I can't seem to find much more information about these flags anywhere so I am not sure how they are related to the addition of retries.\r\n\r\nSo basically does anyone have ideas on why these errors might be occurring, after the addition of retries? \r\n\r\nPart of our Envoy config below:\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - name: xxx\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8080\r\n    traffic_direction: INBOUND\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          codec_type: auto\r\n          stat_prefix: xxx\r\n          server_name: \"xxx\"\r\n          server_header_transformation: OVERWRITE\r\n          access_log:\r\n            - name: envoy.access_loggers.file\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                path: \"xxx\"\r\n                format: \"[%START_TIME%] \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\"\r\n                  %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION%\r\n                  %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%REQ(USER-AGENT)%\\\"\r\n                  \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\"\\n\"\r\n          request_timeout: 300s\r\n          route_config:\r\n            name: xxx\r\n            virtual_hosts:\r\n            - name: xxx\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  timeout: 0s\r\n                  idle_timeout: 300s\r\n                  cluster: xxx\r\n                  retry_policy:\r\n                    retry_on: connect-failure\r\n                    num_retries: 3\r\n                decorator:\r\n                  operation: xxx\r\n          http_filters:\r\n          - name: envoy.filters.http.jwt_authn\r\n            typed_config: \r\n              \"@type\": type.googleapis.com/envoy.config.filter.http.jwt_authn.v2alpha.JwtAuthentication\r\n              providers:\r\n                xxx:\r\n                  issuer: \"xxx\"\r\n                  remote_jwks:\r\n                    http_uri:\r\n                      uri: \"xxx\"\r\n                      cluster: jwks_cluster\r\n                      timeout: 5s\r\n                    cache_duration:\r\n                      seconds: 2592000 # 30 days\r\n                  forward: true\r\n                  forward_payload_header: x-jwt-payload\r\n                  payload_in_metadata: \"jwt_payload\"\r\n                  from_headers:\r\n                    - name: authorization\r\n                      value_prefix: \"Bearer \"\r\n              rules:\r\n                - match:\r\n                    prefix: /\r\n                  requires:\r\n                    requires_any:\r\n                      requirements:\r\n                        - provider_name: xxx\r\n                        - allow_missing_or_failed: {}\r\n          - name: envoy.filters.http.ratelimit\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimit\r\n              stage: 0\r\n              domain: xxx\r\n              failure_mode_deny: false\r\n              timeout: 0.25s\r\n              rate_limit_service:\r\n                grpc_service:\r\n                  envoy_grpc:\r\n                    cluster_name: rate_limit_service\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n              suppress_envoy_headers: true\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.api.v2.auth.DownstreamTlsContext\r\n          common_tls_context:\r\n            alpn_protocols: [ \"h2,http/1.1\" ]\r\n            tls_certificates:\r\n              - certificate_chain: { filename: \"xxx\" }\r\n                private_key: { filename: \"xxx\" }\r\n            validation_context:\r\n              trusted_ca: { filename: \"xxx\" }\r\n  clusters:\r\n  - name: xxx\r\n    connect_timeout: 1s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: xxx\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: localhost.ssl\r\n                port_value: 8443\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.api.v2.auth.UpstreamTlsContext\r\n        common_tls_context:\r\n          validation_context:\r\n            trusted_ca: { filename: \"xxx\" }\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13170/comments",
    "author": "jaegchoi",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-09T03:11:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-16T04:03:48Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 13119,
    "title": "send request to another upstream after retries",
    "created_at": "2020-09-15T22:25:49Z",
    "closed_at": "2020-09-18T14:56:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13119",
    "body": "*Title*: *send request to another upstream after retries*\r\n\r\n*Description*:\r\nI have a listener at port 8080 and I have the following retry policy \r\n```\r\n                     retry_policy:\r\n                        retry_on: 5xx,retriable-4xx,connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,reset,gateway-error,retriable-status-codes\r\n                        num_retries: 2\r\n                        per_try_timeout: 6s\r\n                        retry_back_off:\r\n                          base_interval: \"2s\"\r\n                          max_interval: \"5s\"\r\n```\r\n\r\nis there a way that after all retries if still getting an error, send the error output of the request to another upstream cluster which has an API that concentrates all errors?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13119/comments",
    "author": "juanvasquezreyes",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-09-16T23:47:08Z",
        "body": "I don't think this is possible today. cc @snowp "
      },
      {
        "user": "snowp",
        "created_at": "2020-09-17T16:38:40Z",
        "body": "Definitely not possible today, and would require some work to implement. With upstream HTTP filters you could imagine adding a new filter that tracks all the responses in FilterState and dispatching that at the end, but a lot of pieces are still missing for that to be possible"
      },
      {
        "user": "juanvasquezreyes",
        "created_at": "2020-09-17T17:23:03Z",
        "body": "is there a way to send the response output to another listener or another cluster? "
      },
      {
        "user": "snowp",
        "created_at": "2020-09-17T18:04:38Z",
        "body": "If you don't care about individual retry attempts you could probably do this with a C++ or Lua filter that makes an async call out once it sees the response "
      },
      {
        "user": "juanvasquezreyes",
        "created_at": "2020-09-18T14:56:50Z",
        "body": "I was able to make it work with LUA by doing something like this, it works after all retries\r\n```\r\n                  - name: envoy.filters.http.lua\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.config.filter.http.lua.v2.Lua\r\n                      inline_code: |\r\n                        function envoy_on_response(response_handle)\r\n                          local status = response_handle:headers():get(\":status\")\r\n                          local cluster = \"api\"\r\n                          local requestBodyBuffer = response_handle:body()\r\n                          local requestBodyData = requestBodyBuffer:getBytes(0, requestBodyBuffer:length())\r\n                          local req_headers = {\r\n                            [\":method\"] = \"GET\",\r\n                            [\":path\"] = \"/\",\r\n                            [\":authority\"] = cluster\r\n                          }\r\n\r\n                          if ( status ~= \"200\" ) then                            \r\n                            response_handle:logDebug(cluster)\r\n\r\n                            local headers, body = response_handle:httpCall(\r\n                              cluster,\r\n                              {\r\n                                 [\":method\"] = \"GET\",\r\n                                 [\":path\"] = \"/test\",\r\n                                 [\":authority\"] = \"api\",\r\n                                 [\"Content-Type\"] = \"application/json\"\r\n                              },\r\n                              nil,\r\n                              15000)\r\n                            response_handle:logDebug(\"async call done\")\r\n                            response_handle:logDebug(body)                           \r\n                          end\r\n                        end\r\n```\r\n\r\nwhere api is my cluster and i'm evaluating that the first status is not 200, you can change to a POST and send the body data from the first request by passing this requestBodyData  into the http Post call"
      }
    ]
  },
  {
    "number": 13089,
    "title": "extraneous cors http_filter in cors example",
    "created_at": "2020-09-14T20:26:02Z",
    "closed_at": "2020-12-16T04:03:45Z",
    "labels": [
      "question",
      "stale",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13089",
    "body": "*Title*: *extraneous cors filter in cors example*\r\n\r\n*Description*:\r\n> In github.com/envoyproxy/envoy/examples/cors/frontend/front-envoy.yaml there is a http_filter for envoy.filters.http.cors.  I could be confused but I don't think that is necessary.  The cors setup is on the backend front-envoy (backend/front-envoy.yaml). I removed it and everything worked the same - as expected.  If I am wrong can someone point me in the right direction?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13089/comments",
    "author": "ronaldpetty",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-09-14T21:32:21Z",
        "body": "cc @dschaller @phlax "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-09T03:11:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-16T04:03:44Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 13073,
    "title": "How to add per_filter_config envoy.ext_authz programmatically in JAVA ",
    "created_at": "2020-09-13T10:33:27Z",
    "closed_at": "2021-01-15T04:26:06Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13073",
    "body": " \r\nTitle: How to add per_filter_config envoy.ext_authz programmatically in JAVA using envoy v2 api \r\n\r\n*Description*:\r\n>\r\nvirtual_hosts:\r\n                    - name: application_services\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match: { prefix: \"/api/service/xxxxx/\" }\r\n                          route:\r\n                            cluster: SAMPLE_CLUSTER\r\n                          per_filter_config:\r\n                            envoy.ext_authz:\r\n                              disabled: true\r\n\r\nI am using `/v2/discovery:routes` \r\n\r\n\r\nfinal Route application_services_route =\r\n        Route.newBuilder()\r\n            .setMatch(RouteMatch.newBuilder().setPrefix(\"/api/service/xxxxx/\").build())\r\n            .setRoute(RouteAction.newBuilder().setCluster(SAMPLE_CLUSTER).build())\r\n            .build();\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13073/comments",
    "author": "agupta-cb",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-09T03:11:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "agupta-cb",
        "created_at": "2020-12-09T03:33:29Z",
        "body": "It would be great if anyone can provide help related to this issue. Still waiting for its solution. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-08T04:23:10Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-15T04:26:05Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 12985,
    "title": " envoy_on_request: respond() cannot be called if headers have been continued",
    "created_at": "2020-09-04T08:34:27Z",
    "closed_at": "2020-09-18T15:33:15Z",
    "labels": [
      "question",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12985",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *respond() cannot be called if headers have been continued*\r\n\r\n*Description*:\r\n>Describe the issue.\r\nI am configuring EnvoyFilter in istio, what I wanna do is:\r\n1. get the method and body of the request\r\n2. copy them and paste them into httpCall\r\n3. DO SOMETHING\r\nBut when I apply this file, I always get ` script log: [string \"function envoy_on_request(request_handle)...\"]:36: respond() cannot be called if headers have been continued` error and have no idea how to resolve it.\r\n\r\nHere is my yaml file:\r\n```yaml\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: httpbin-lua\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      app: httpbin-callee\r\n  configPatches:\r\n    # The first patch adds the lua filter to the listener/http connection manager\r\n  - applyTo: HTTP_FILTER\r\n    match:\r\n      context: SIDECAR_INBOUND\r\n      listener:\r\n        portNumber: 80\r\n        filterChain:\r\n          filter:\r\n            name: \"envoy.http_connection_manager\"\r\n            subFilter:\r\n              name: \"envoy.router\"\r\n    patch:\r\n      operation: INSERT_BEFORE\r\n      value: # lua filter specification\r\n       name: envoy.lua\r\n       typed_config:\r\n         \"@type\": \"type.googleapis.com/envoy.config.filter.http.lua.v2.Lua\"\r\n         inlineCode: |\r\n           function envoy_on_request(request_handle)\r\n             -- Make an HTTP call to an upstream host with the following headers, body, and timeout.\r\n             local requestMethod = request_handle:headers():get(\":method\")\r\n             local requestPath = request_handle:headers():get(\":path\")\r\n             local requestAuthorization = request_handle:headers():get(\"authorization\")\r\n             local index = 0\r\n             for chunk in request_handle:bodyChunks() do\r\n               local len = chunk:length()\r\n               result = chunk:getBytes(index, len)\r\n               index = index + len\r\n\r\n               request_handle:logDebug(result)\r\n\r\n               -- Make an HTTP call.\r\n               headers, body = request_handle:httpCall(\r\n               \"auth-server\",\r\n               {\r\n                [\":method\"] = requestMethod,\r\n                [\":path\"] = requestPath,\r\n                [\":authority\"] = \"httpbin-auth.default.svc.cluster.local\",\r\n                [\"authorization\"] = requestAuthorization\r\n               },\r\n               result,\r\n               5000)\r\n            end\r\n             print(\"-result--\")\r\n             print(result)\r\n             print(\"---Get all headers---\")\r\n             originhead = request_handle:headers()\r\n             for key, value in pairs(originhead) do\r\n               print('\\t', key, value)\r\n             end\r\n             print(\"-x-header status---\")\r\n             print(headers[\":status\"])\r\n             if (headers[\":status\"] ~= \"200\")\r\n             then\r\n               request_handle:respond(\r\n                 {\r\n                  [\":status\"] = \"403\",\r\n                  [\"content-type\"] = \"application/json;charset=UTF-8\"\r\n                 },\r\n                 result)\r\n             end\r\n           end\r\n  # The second patch adds the cluster that is referenced by the lua code\r\n  # cds match is omitted as a new cluster is being added\r\n  - applyTo: CLUSTER\r\n    match:\r\n      context: SIDECAR_OUTBOUND\r\n    patch:\r\n      operation: ADD\r\n      value: # cluster specification\r\n        name: \"auth-server\"\r\n        type: STRICT_DNS\r\n        connect_timeout: 5s\r\n        lb_policy: ROUND_ROBIN\r\n        hosts:\r\n        - socket_address:\r\n            protocol: TCP\r\n            address: \"httpbin-auth.default.svc.cluster.local\"\r\n            port_value: 80\r\n```\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12985/comments",
    "author": "linxuyalun",
    "comments": [
      {
        "user": "linxuyalun",
        "created_at": "2020-09-04T08:57:43Z",
        "body": "I also try another method(handle:body()) to get the content of body, which configure file is:\r\n```yaml\r\ninlineCode: |\r\n           function envoy_on_request(request_handle)\r\n             -- Make an HTTP call to an upstream host with the following headers, body, and timeout.\r\n             local requestMethod = request_handle:headers():get(\":method\")\r\n             local requestPath = request_handle:headers():get(\":path\")\r\n             local requestAuthorization = request_handle:headers():get(\"authorization\")\r\n             local requestBody = request_handle:body()\r\n             local requestBodyString = tostring(requestBody)\r\n             local headers, body = request_handle:httpCall(\r\n              \"auth-server\",\r\n              {\r\n               [\":method\"] = requestMethod,\r\n               [\":path\"] = requestPath,\r\n               [\":authority\"] = \"httpbin-auth.default.svc.cluster.local\",\r\n               [\"authorization\"] = requestAuthorization\r\n              },\r\n             requestBodyString,\r\n             5000)\r\n             print(\"--Get all headers---\")\r\n             originhead = request_handle:headers()\r\n             for key, value in pairs(originhead) do\r\n               print('\\t', key, value)\r\n             end\r\n             print(headers[\":status\"])\r\n             if (headers[\":status\"] ~= \"200\")\r\n             then\r\n               request_handle:respond(\r\n                 {\r\n                  [\":status\"] = \"403\",\r\n                  [\"content-type\"] = \"application/json;charset=UTF-8\"\r\n                 },\r\n                 requestBodyString)\r\n             end\r\n           end\r\n```\r\nHowever, the body content is a userdata like `userdata: 0x41c8c448` and I can't get the real data."
      },
      {
        "user": "snowp",
        "created_at": "2020-09-04T16:39:15Z",
        "body": "@dio Mind taking a look?"
      },
      {
        "user": "linxuyalun",
        "created_at": "2020-09-07T06:53:23Z",
        "body": "Hi, I solved the problem, I use the buffer:getBytes() and finally figure it out.\r\n\r\nThe core part is:\r\n\r\n```yaml\r\n             local requestBodyBuffer = request_handle:body()\r\n             local requestBodyData = requestBodyBuffer:getBytes(0, requestBodyBuffer:length())\r\n```\r\n\r\nI can then get the raw body data and will not get a `respond() cannot be called if headers have been continued` error now.\r\n\r\nHere is my full configuration:\r\n\r\n```yaml\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: httpbin-lua\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      app: httpbin-callee\r\n  configPatches:\r\n    # The first patch adds the lua filter to the listener/http connection manager\r\n  - applyTo: HTTP_FILTER\r\n    match:\r\n      context: SIDECAR_INBOUND\r\n      listener:\r\n        portNumber: 80\r\n        filterChain:\r\n          filter:\r\n            name: \"envoy.http_connection_manager\"\r\n            subFilter:\r\n              name: \"envoy.router\"\r\n    patch:\r\n      operation: INSERT_BEFORE\r\n      value: # lua filter specification\r\n       name: envoy.lua\r\n       typed_config:\r\n         \"@type\": \"type.googleapis.com/envoy.config.filter.http.lua.v2.Lua\"\r\n         inlineCode: |\r\n           function envoy_on_request(request_handle)\r\n             -- Make an HTTP call to an upstream host with the following headers, body, and timeout.\r\n             local requestMethod = request_handle:headers():get(\":method\")\r\n             local requestPath = request_handle:headers():get(\":path\")\r\n             local requestAuthorization = request_handle:headers():get(\"authorization\")\r\n             local requestBodyBuffer = request_handle:body()\r\n             local requestBodyData = requestBodyBuffer:getBytes(0, requestBodyBuffer:length())\r\n             headers, body = request_handle:httpCall(\r\n             \"auth-server\",\r\n             {\r\n              [\":method\"] = requestMethod,\r\n              [\":path\"] = requestPath,\r\n              [\":authority\"] = \"httpbin-auth.default.svc.cluster.local\",\r\n              [\"authorization\"] = requestAuthorization\r\n             },\r\n             requestBodyData,\r\n             5000)\r\n             print(\"---Get all headers---\")\r\n             originhead = request_handle:headers()\r\n             for key, value in pairs(originhead) do\r\n               print('\\t', key, value)\r\n             end\r\n             print(\"--header status---\")\r\n             print(headers[\":status\"])\r\n             if (headers[\":status\"] ~= \"200\")\r\n             then\r\n               request_handle:respond(\r\n                 {\r\n                  [\":status\"] = \"403\",\r\n                  [\"content-type\"] = \"application/json;charset=UTF-8\"\r\n                 },\r\n                 requestBodyData)\r\n             end\r\n           end\r\n  # The second patch adds the cluster that is referenced by the lua code\r\n  # cds match is omitted as a new cluster is being added\r\n  - applyTo: CLUSTER\r\n    match:\r\n      context: SIDECAR_OUTBOUND\r\n    patch:\r\n      operation: ADD\r\n      value: # cluster specification\r\n        name: \"auth-server\"\r\n        type: STRICT_DNS\r\n        connect_timeout: 5s\r\n        lb_policy: ROUND_ROBIN\r\n        hosts:\r\n        - socket_address:\r\n            protocol: TCP\r\n            address: \"httpbin-auth.default.svc.cluster.local\"\r\n            port_value: 80\r\n\r\n```\r\n"
      },
      {
        "user": "dio",
        "created_at": "2020-09-18T15:33:02Z",
        "body": "Seems like this can be closed, however @linxuyalun, do you mind creating a PR for improving docs? Thank you!"
      }
    ]
  },
  {
    "number": 12916,
    "title": "How to split log files?",
    "created_at": "2020-09-01T08:37:59Z",
    "closed_at": "2021-01-15T04:25:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12916",
    "body": "All access log write to  access.log, how to split log files by day?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12916/comments",
    "author": "waluo",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-09-01T16:48:35Z",
        "body": "logrotate"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-04T19:16:22Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-08T04:22:59Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-15T04:25:52Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 12892,
    "title": "what's the stage mean in rate limit",
    "created_at": "2020-08-31T17:00:54Z",
    "closed_at": "2020-09-01T07:50:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12892",
    "body": "```\r\n(optional, integer) Refers to the stage set in the filter. The rate limit configuration only applies to filters with the same stage number. The default stage number is 0.\r\n```\r\n\r\ni have find this in the envoy doc,but what's the stage mean？what is the different of state 0 or state 10?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12892/comments",
    "author": "du2016",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-09-01T00:11:14Z",
        "body": "It's a mechanism to have multiple rate limits in the filter chain. For example:\r\npre-auth rate limit -> auth -> post-auth rate limit"
      },
      {
        "user": "du2016",
        "created_at": "2020-09-01T07:50:11Z",
        "body": "thanks "
      },
      {
        "user": "jdomag",
        "created_at": "2021-12-15T11:46:10Z",
        "body": "@mattklein123 @du2016  \r\nAny idea how to use it? I'm wondering if I can have 2 different rate limiting services and point few workloads to rateLimitService1 and few workloads to rateLimitService2  - not sure if it's possible though. "
      }
    ]
  },
  {
    "number": 12698,
    "title": "[question] Can accesslog record each upstream_host and upstream_response_code when retrypolicy on?",
    "created_at": "2020-08-18T03:33:57Z",
    "closed_at": "2020-08-24T09:10:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12698",
    "body": "Hello:\r\n   I want ask some question about envoy http accesslog.\r\n   Can accesslog record each upstream_host and it's response_code when retry?\r\n   I use the %UPSTREAM_HOST% , it only record the last reqeust upstream_host.\r\n   I want to the log show like this:\r\n      upstream_host:\"192.168.1.1:80\",\"192.168.1.2:80\",\"192.168.1.3:80\"\r\n      response_code:     503 ,                       503      ,          200",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12698/comments",
    "author": "pyrl247",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2020-08-18T17:25:57Z",
        "body": "Not exactly like that, but if you specify an `upstream_log` in the http connection manager, it will log each upstream attempt on a separate entry. The downstream and upstream logs can be linked by including a common field (downstream address:port, trace-id, etc)."
      },
      {
        "user": "pyrl247",
        "created_at": "2020-08-19T06:17:46Z",
        "body": "I see,Thank u~ But here r some other problem , I set upstream_log like this:\r\n`\r\n\r\n          http_filters:\r\n          - name: front-router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n              upstream_log:\r\n                name: envoy.file_upstream_log\r\n                filter:\r\n                  status_code_filter:\r\n                    comparison:\r\n                      op: GE\r\n                      value:\r\n                        default_value: 500\r\n                        runtime_key: \"...\"\r\n                typed_config:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                  path: /data/envoylogs/upstream.log\r\n                  json_format:\r\n                   upstream_status: '%RESPONSE_CODE%'\r\n\r\nit shows each upstream attempt , but It seems the %RESPONSE_CODE% always be 0 during upstream_log. It work fine when use access_log in HttpConnectionManager."
      },
      {
        "user": "ggreenway",
        "created_at": "2020-08-19T15:23:34Z",
        "body": "I'm not sure why that's always printing zero. I have a similar config, but without the `filter` and using `format` instead of `json_format`, and it's working as expected."
      },
      {
        "user": "pyrl247",
        "created_at": "2020-08-24T09:10:40Z",
        "body": "Thank u! I will try it your way."
      }
    ]
  },
  {
    "number": 12679,
    "title": "Bazel build with error “/usr/bin/ld.gold: error: cannot find libc++.a”",
    "created_at": "2020-08-17T07:39:17Z",
    "closed_at": "2020-08-21T07:20:58Z",
    "labels": [
      "question",
      "area/build"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12679",
    "body": "Hi, I am building envoy on mips and enconter an error when compiling net_zlib. The error is as follows:\r\n```\r\n/usr/bin/ld.gold: error: cannot find libc++.a\r\n/usr/bin/ld.gold: error: cannot find libc++abi.a\r\n```\r\nI find the bazel uses rules_foreign_cc and will start a sandbox to compile zlib. The PATH env is set to `/root/.cache/bazel/_bazel_root/99befe685ff48e352428f1d9cabb5dd7/sandbox/processwrapper-sandbox/3/execroot/io_istio_proxy:/bin:/usr/bin:/usr/local/bin`,which doesn't include any libs such as libc++.a and  libc++abi.a\r\n\r\nHow can I solve this problem?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12679/comments",
    "author": "packyzbq",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2020-08-18T11:05:20Z",
        "body": "Can you paste the command you ran to build and the full output log? If you're using gcc/gold you shouldn't use libc++."
      },
      {
        "user": "packyzbq",
        "created_at": "2020-08-21T07:22:31Z",
        "body": "I solve the problem after installing the libc++-dev libc++abi-dev and setting clang-7 as the compiler."
      }
    ]
  },
  {
    "number": 12678,
    "title": "Warmup of host for Java Applications",
    "created_at": "2020-08-17T06:43:41Z",
    "closed_at": "2020-08-19T02:49:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12678",
    "body": "For applications developed using Java, when a new host starts, it takes a period of warmup to achieve the best performance. This leads to a possible demand scenario: For the newly started host, its weight in the load balancing process should be reduced. However, all current Envoy load balancing strategies do not support host warmup.\r\nIs it possible to add a new type of load balancing? Or turn the load balancing strategy into a part of extensions, so that developers can expand and develop new load balancing strategies according to their needs?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12678/comments",
    "author": "wbpcode",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2020-08-17T13:00:25Z",
        "body": "Would this be satisfied by #11050 ?"
      },
      {
        "user": "wbpcode",
        "created_at": "2020-08-19T02:49:38Z",
        "body": "Thanks. That's what I want."
      },
      {
        "user": "ejc3",
        "created_at": "2020-09-12T17:46:49Z",
        "body": "While slow start would be ideal, what we've done in our Kubernetes setup is before the readiness check succeeds for the first time, we send a good amount of local load (warm up url's) to the host."
      }
    ]
  },
  {
    "number": 12675,
    "title": "Stats filtering inclusion list is not working for server level metrics",
    "created_at": "2020-08-17T05:16:20Z",
    "closed_at": "2020-08-19T04:34:35Z",
    "labels": [
      "question",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12675",
    "body": "We have the following configuration in envoy\r\n\r\n```yml\r\n    stats_config:\r\n      stats_matcher:\r\n        inclusion_list:\r\n          patterns:\r\n          - suffix: upstream_cx_total\r\n          - exact: envoy_server_state\r\n```\r\nIn this case, the metrics with suffix upstream_cx_total is emitted properly, but the metric envoy_server_state is never emitted. Please check if this a bug, we have a large config and metrics is slowing envoy down, hence wanted to include only the required metrics which is a combination of cluster and server level metrics.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12675/comments",
    "author": "shyamradhakrishnan",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2020-08-18T11:03:00Z",
        "body": "@shyamradhakrishnan I believe what you want is `exact: server.state`, the stats matcher is based on their canonical name, names in Prometheus exporter are normalized to Prometheus naming convention (with envoy prefix)."
      },
      {
        "user": "shyamradhakrishnan",
        "created_at": "2020-08-19T04:34:35Z",
        "body": "Thanks @lizan , that was the issue."
      }
    ]
  },
  {
    "number": 12665,
    "title": "Timeout sometimes gives 408 instead of 504",
    "created_at": "2020-08-15T06:56:18Z",
    "closed_at": "2020-09-01T08:15:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12665",
    "body": "Description:\r\n\r\n> While testing timeout in envoy v1.12.2 most of the request gives 504 which is as expected, but once in a while it throws 408 status code which is unexpected.\r\n\r\nRepro steps:\r\n\r\n> In the testing environment 15s delay was tried for a timeout with below routing config.\r\n```\r\n\"route\": {\r\n           \"cluster\": \"test\",\r\n           \"timeout\": \"15s\",\r\n           \"idle_timeout\": \"315s\"\r\n          }\r\n```\r\nAdmin and Stats Output:\r\n\r\n<details>\r\n<summary>Stats</summary>\r\ncluster.testt1.assignment_stale: 0\r\ncluster.testt1.assignment_timeout_received: 0\r\ncluster.testt1.bind_errors: 0\r\ncluster.testt1.circuit_breakers.default.cx_open: 0\r\ncluster.testt1.circuit_breakers.default.cx_pool_open: 0\r\ncluster.testt1.circuit_breakers.default.rq_open: 0\r\ncluster.testt1.circuit_breakers.default.rq_pending_open: 0\r\ncluster.testt1.circuit_breakers.default.rq_retry_open: 0\r\ncluster.testt1.circuit_breakers.high.cx_open: 0\r\ncluster.testt1.circuit_breakers.high.cx_pool_open: 0\r\ncluster.testt1.circuit_breakers.high.rq_open: 0\r\ncluster.testt1.circuit_breakers.high.rq_pending_open: 0\r\ncluster.testt1.circuit_breakers.high.rq_retry_open: 0\r\ncluster.testt1.client_ssl_socket_factory.downstream_context_secrets_not_ready: 0\r\ncluster.testt1.client_ssl_socket_factory.ssl_context_update_by_sds: 0\r\ncluster.testt1.client_ssl_socket_factory.upstream_context_secrets_not_ready: 0\r\ncluster.testt1.default.total_match_count: 240\r\ncluster.testt1.external.upstream_rq_200: 4\r\ncluster.testt1.external.upstream_rq_2xx: 4\r\ncluster.testt1.external.upstream_rq_408: 3\r\ncluster.testt1.external.upstream_rq_4xx: 3\r\ncluster.testt1.external.upstream_rq_504: 236\r\ncluster.testt1.external.upstream_rq_5xx: 236\r\ncluster.testt1.external.upstream_rq_completed: 243\r\ncluster.testt1.http1.metadata_not_supported_error: 0\r\ncluster.testt1.lb_healthy_panic: 0\r\ncluster.testt1.lb_local_cluster_not_ok: 0\r\ncluster.testt1.lb_recalculate_zone_structures: 0\r\ncluster.testt1.lb_subsets_active: 0\r\ncluster.testt1.lb_subsets_created: 0\r\ncluster.testt1.lb_subsets_fallback: 0\r\ncluster.testt1.lb_subsets_fallback_panic: 0\r\ncluster.testt1.lb_subsets_removed: 0\r\ncluster.testt1.lb_subsets_selected: 0\r\ncluster.testt1.lb_zone_cluster_too_small: 0\r\ncluster.testt1.lb_zone_no_capacity_left: 0\r\ncluster.testt1.lb_zone_number_differs: 0\r\ncluster.testt1.lb_zone_routing_all_directly: 0\r\ncluster.testt1.lb_zone_routing_cross_zone: 0\r\ncluster.testt1.lb_zone_routing_sampled: 0\r\ncluster.testt1.max_host_weight: 1\r\ncluster.testt1.membership_change: 1\r\ncluster.testt1.membership_degraded: 0\r\ncluster.testt1.membership_excluded: 0\r\ncluster.testt1.membership_healthy: 1\r\ncluster.testt1.membership_total: 1\r\ncluster.testt1.original_dst_host_invalid: 0\r\ncluster.testt1.retry_or_shadow_abandoned: 0\r\ncluster.testt1.ssl.ciphers.ECDHE-RSA-AES128-GCM-SHA256: 222\r\ncluster.testt1.ssl.connection_error: 0\r\ncluster.testt1.ssl.curves.X25519: 222\r\ncluster.testt1.ssl.fail_verify_cert_hash: 0\r\ncluster.testt1.ssl.fail_verify_error: 0\r\ncluster.testt1.ssl.fail_verify_no_cert: 0\r\ncluster.testt1.ssl.fail_verify_san: 0\r\ncluster.testt1.ssl.handshake: 222\r\ncluster.testt1.ssl.no_certificate: 0\r\ncluster.testt1.ssl.session_reused: 221\r\ncluster.testt1.ssl.sigalgs.unknown_ssl_algorithm: 222\r\ncluster.testt1.ssl.versions.TLSv1.2: 222\r\ncluster.testt1.update_attempt: 240\r\ncluster.testt1.update_empty: 0\r\ncluster.testt1.update_failure: 0\r\ncluster.testt1.update_no_rebuild: 239\r\ncluster.testt1.update_success: 240\r\ncluster.testt1.upstream_cx_active: 1\r\ncluster.testt1.upstream_cx_close_notify: 0\r\ncluster.testt1.upstream_cx_connect_attempts_exceeded: 0\r\ncluster.testt1.upstream_cx_connect_fail: 0\r\ncluster.testt1.upstream_cx_connect_timeout: 0\r\ncluster.testt1.upstream_cx_destroy: 221\r\ncluster.testt1.upstream_cx_destroy_local: 221\r\ncluster.testt1.upstream_cx_destroy_local_with_active_rq: 221\r\ncluster.testt1.upstream_cx_destroy_remote: 0\r\ncluster.testt1.upstream_cx_destroy_remote_with_active_rq: 0\r\ncluster.testt1.upstream_cx_destroy_with_active_rq: 221\r\ncluster.testt1.upstream_cx_http1_total: 222\r\ncluster.testt1.upstream_cx_http2_total: 0\r\ncluster.testt1.upstream_cx_idle_timeout: 0\r\ncluster.testt1.upstream_cx_max_requests: 0\r\ncluster.testt1.upstream_cx_none_healthy: 0\r\ncluster.testt1.upstream_cx_overflow: 0\r\ncluster.testt1.upstream_cx_pool_overflow: 0\r\ncluster.testt1.upstream_cx_protocol_error: 0\r\ncluster.testt1.upstream_cx_rx_bytes_buffered: 154\r\ncluster.testt1.upstream_cx_rx_bytes_total: 7516\r\ncluster.testt1.upstream_cx_total: 222\r\ncluster.testt1.upstream_cx_tx_bytes_buffered: 0\r\ncluster.testt1.upstream_cx_tx_bytes_total: 108854\r\ncluster.testt1.upstream_flow_control_backed_up_total: 0\r\ncluster.testt1.upstream_flow_control_drained_total: 0\r\ncluster.testt1.upstream_flow_control_paused_reading_total: 0\r\ncluster.testt1.upstream_flow_control_resumed_reading_total: 0\r\ncluster.testt1.upstream_internal_redirect_failed_total: 0\r\ncluster.testt1.upstream_internal_redirect_succeeded_total: 0\r\ncluster.testt1.upstream_rq_200: 4\r\ncluster.testt1.upstream_rq_2xx: 4\r\ncluster.testt1.upstream_rq_408: 3\r\ncluster.testt1.upstream_rq_4xx: 3\r\ncluster.testt1.upstream_rq_504: 236\r\ncluster.testt1.upstream_rq_5xx: 236\r\ncluster.testt1.upstream_rq_active: 0\r\ncluster.testt1.upstream_rq_cancelled: 0\r\ncluster.testt1.upstream_rq_completed: 243\r\ncluster.testt1.upstream_rq_maintenance_mode: 0\r\ncluster.testt1.upstream_rq_pending_active: 0\r\ncluster.testt1.upstream_rq_pending_failure_eject: 0\r\ncluster.testt1.upstream_rq_pending_overflow: 0\r\ncluster.testt1.upstream_rq_pending_total: 222\r\ncluster.testt1.upstream_rq_per_try_timeout: 0\r\ncluster.testt1.upstream_rq_retry: 0\r\ncluster.testt1.upstream_rq_retry_overflow: 0\r\ncluster.testt1.upstream_rq_retry_success: 0\r\ncluster.testt1.upstream_rq_rx_reset: 0\r\ncluster.testt1.upstream_rq_timeout: 221\r\ncluster.testt1.upstream_rq_total: 243\r\ncluster.testt1.upstream_rq_tx_reset: 0\r\ncluster.testt1.version: 0\r\ncluster.cont-ads.assignment_stale: 0\r\ncluster.cont-ads.assignment_timeout_received: 0\r\ncluster.cont-ads.bind_errors: 0\r\ncluster.cont-ads.circuit_breakers.default.cx_open: 0\r\ncluster.cont-ads.circuit_breakers.default.cx_pool_open: 0\r\ncluster.cont-ads.circuit_breakers.default.rq_open: 0\r\ncluster.cont-ads.circuit_breakers.default.rq_pending_open: 0\r\ncluster.cont-ads.circuit_breakers.default.rq_retry_open: 0\r\ncluster.cont-ads.circuit_breakers.high.cx_open: 0\r\ncluster.cont-ads.circuit_breakers.high.cx_pool_open: 0\r\ncluster.cont-ads.circuit_breakers.high.rq_open: 0\r\ncluster.cont-ads.circuit_breakers.high.rq_pending_open: 0\r\ncluster.cont-ads.circuit_breakers.high.rq_retry_open: 0\r\ncluster.cont-ads.client_ssl_socket_factory\r\n.downstream_context_secrets_not_ready: 0\r\ncluster.cont-ads.client_ssl_socket_factory.ssl_context_update_by_sds: 0\r\ncluster.cont-ads.client_ssl_socket_factory.upstream_context_secrets_not_ready: 0\r\ncluster.cont-ads.default.total_match_count: 269\r\ncluster.cont-ads.http2.header_overflow: 0\r\ncluster.cont-ads.http2.headers_cb_no_stream: 0\r\ncluster.cont-ads.http2.inbound_empty_frames_flood: 0\r\ncluster.cont-ads.http2.inbound_priority_frames_flood: 0\r\ncluster.cont-ads.http2.inbound_window_update_frames_flood: 0\r\ncluster.cont-ads.http2.outbound_control_flood: 0\r\ncluster.cont-ads.http2.outbound_flood: 0\r\ncluster.cont-ads.http2.rx_messaging_error: 0\r\ncluster.cont-ads.http2.rx_reset: 0\r\ncluster.cont-ads.http2.too_many_header_frames: 0\r\ncluster.cont-ads.http2.trailers: 0\r\ncluster.cont-ads.http2.tx_reset: 0\r\ncluster.cont-ads.internal.upstream_rq_200: 1\r\ncluster.cont-ads.internal.upstream_rq_2xx: 1\r\ncluster.cont-ads.internal.upstream_rq_503: 2\r\ncluster.cont-ads.internal.upstream_rq_5xx: 2\r\ncluster.cont-ads.internal.upstream_rq_completed: 3\r\ncluster.cont-ads.lb_healthy_panic: 1\r\ncluster.cont-ads.lb_local_cluster_not_ok: 0\r\ncluster.cont-ads.lb_recalculate_zone_structures: 0\r\ncluster.cont-ads.lb_subsets_active: 0\r\ncluster.cont-ads.lb_subsets_created: 0\r\ncluster.cont-ads.lb_subsets_fallback: 0\r\ncluster.cont-ads.lb_subsets_fallback_panic: 0\r\ncluster.cont-ads.lb_subsets_removed: 0\r\ncluster.cont-ads.lb_subsets_selected: 0\r\ncluster.cont-ads.lb_zone_cluster_too_small: 0\r\ncluster.cont-ads.lb_zone_no_capacity_left: 0\r\ncluster.cont-ads.lb_zone_number_differs: 0\r\ncluster.cont-ads.lb_zone_routing_all_directly: 0\r\ncluster.cont-ads.lb_zone_routing_cross_zone: 0\r\ncluster.cont-ads.lb_zone_routing_sampled: 0\r\ncluster.cont-ads.max_host_weight: 1\r\ncluster.cont-ads.membership_change: 1\r\ncluster.cont-ads.membership_degraded: 0\r\ncluster.cont-ads.membership_excluded: 0\r\ncluster.cont-ads.membership_healthy: 1\r\ncluster.cont-ads.membership_total: 1\r\ncluster.cont-ads.original_dst_host_invalid: 0\r\ncluster.cont-ads.retry_or_shadow_abandoned: 0\r\ncluster.cont-ads.ssl.ciphers.ECDHE-RSA-AES128-GCM-SHA256: 1\r\ncluster.cont-ads.ssl.connection_error: 0\r\ncluster.cont-ads.ssl.curves.X25519: 1\r\ncluster.cont-ads.ssl.fail_verify_cert_hash: 0\r\ncluster.cont-ads.ssl.fail_verify_error: 0\r\ncluster.cont-ads.ssl.fail_verify_no_cert: 0\r\ncluster.cont-ads.ssl.fail_verify_san: 0\r\ncluster.cont-ads.ssl.handshake: 1\r\ncluster.cont-ads.ssl.no_certificate: 0\r\ncluster.cont-ads.ssl.session_reused: 0\r\ncluster.cont-ads.ssl.sigalgs.unknown_ssl_algorithm: 1\r\ncluster.cont-ads.ssl.versions.TLSv1.2: 1\r\ncluster.cont-ads.update_attempt: 269\r\ncluster.cont-ads.update_empty: 0\r\ncluster.cont-ads.update_failure: 0\r\ncluster.cont-ads.update_no_rebuild: 268\r\ncluster.cont-ads.update_success: 269\r\ncluster.cont-ads.upstream_cx_active: 1\r\ncluster.cont-ads.upstream_cx_close_notify: 0\r\ncluster.cont-ads.upstream_cx_connect_attempts_exceeded: 0\r\ncluster.cont-ads.upstream_cx_connect_fail: 1\r\ncluster.cont-ads.upstream_cx_connect_timeout: 1\r\ncluster.cont-ads.upstream_cx_destroy: 1\r\ncluster.cont-ads.upstream_cx_destroy_local: 1\r\ncluster.cont-ads.upstream_cx_destroy_local_with_active_rq: 0\r\ncluster.cont-ads.upstream_cx_destroy_remote: 0\r\ncluster.cont-ads.upstream_cx_destroy_remote_with_active_rq: 0\r\ncluster.cont-ads.upstream_cx_destroy_with_active_rq: 0\r\ncluster.cont-ads.upstream_cx_http1_total: 0\r\ncluster.cont-ads.upstream_cx_http2_total: 2\r\ncluster.cont-ads.upstream_cx_idle_timeout: 0\r\ncluster.cont-ads.upstream_cx_max_requests: 0\r\ncluster.cont-ads.upstream_cx_none_healthy: 1\r\ncluster.cont-ads.upstream_cx_overflow: 0\r\ncluster.cont-ads.upstream_cx_pool_overflow: 0\r\ncluster.cont-ads.upstream_cx_protocol_error: 0\r\ncluster.cont-ads.upstream_cx_rx_bytes_buffered: 30\r\ncluster.cont-ads.upstream_cx_rx_bytes_total: 10578457\r\ncluster.cont-ads.upstream_cx_total: 2\r\ncluster.cont-ads.upstream_cx_tx_bytes_buffered: 0\r\ncluster.cont-ads.upstream_cx_tx_bytes_total: 1286626\r\ncluster.cont-ads.upstream_flow_control_backed_up_total: 0\r\ncluster.cont-ads.upstream_flow_control_drained_total: 0\r\ncluster.cont-ads.upstream_flow_control_paused_reading_total: 0\r\ncluster.cont-ads.upstream_flow_control_resumed_reading_total: 0\r\ncluster.cont-ads.upstream_internal_redirect_failed_total: 0\r\ncluster.cont-ads.upstream_internal_redirect_succeeded_total: 0\r\ncluster.cont-ads.upstream_rq_200: 1\r\ncluster.cont-ads.upstream_rq_2xx: 1\r\ncluster.cont-ads.upstream_rq_503: 2\r\ncluster.cont-ads.upstream_rq_5xx: 2\r\ncluster.cont-ads.upstream_rq_active: 1\r\ncluster.cont-ads.upstream_rq_cancelled: 0\r\ncluster.cont-ads.upstream_rq_completed: 3\r\ncluster.cont-ads.upstream_rq_maintenance_mode: 0\r\ncluster.cont-ads.upstream_rq_pending_active: 0\r\ncluster.cont-ads.upstream_rq_pending_failure_eject: 1\r\ncluster.cont-ads.upstream_rq_pending_overflow: 0\r\ncluster.cont-ads.upstream_rq_pending_total: 2\r\ncluster.cont-ads.upstream_rq_per_try_timeout: 0\r\ncluster.cont-ads.upstream_rq_retry: 0\r\ncluster.cont-ads.upstream_rq_retry_overflow: 0\r\ncluster.cont-ads.upstream_rq_retry_success: 0\r\ncluster.cont-ads.upstream_rq_rx_reset: 0\r\ncluster.cont-ads.upstream_rq_timeout: 0\r\ncluster.cont-ads.upstream_rq_total: 1\r\ncluster.cont-ads.upstream_rq_tx_reset: 0\r\ncluster.cont-ads.version: 0\r\ncluster_manager.active_clusters: 2\r\ncluster_manager.cds.init_fetch_timeout: 0\r\ncluster_manager.cds.update_attempt: 1440\r\ncluster_manager.cds.update_failure: 1\r\ncluster_manager.cds.update_rejected: 0\r\ncluster_manager.cds.update_success: 1438\r\ncluster_manager.cds.version: 310770956959226155\r\ncluster_manager.cluster_added: 2\r\ncluster_manager.cluster_modified: 0\r\ncluster_manager.cluster_removed: 0\r\ncluster_manager.cluster_updated: 0\r\ncluster_manager.cluster_updated_via_merge: 0\r\ncluster_manager.update_merge_cancelled: 0\r\ncluster_manager.update_out_of_merge_window: 0\r\ncluster_manager.warming_clusters: 0\r\ncontrol_plane.connected_state: 1\r\ncontrol_plane.pending_requests: 0\r\ncontrol_plane.rate_limit_enforced: 19\r\nfilesystem.flushed_by_timer: 721\r\nfilesystem.reopen_failed: 0\r\nfilesystem.write_buffered: 107386\r\nfilesystem.write_completed: 4504\r\nfilesystem.write_failed: 0\r\nfilesystem.write_total_buffered: 122\r\nhttp.admin.downstream_cx_active: 1\r\nhttp.admin.downstream_cx_delayed_close_timeout: 0\r\nhttp.admin.downstream_cx_destroy: 2651\r\nhttp.admin.downstream_cx_destroy_active_rq: 0\r\nhttp.admin.downstream_cx_destroy_local: 2640\r\nhttp.admin.downstream_cx_destroy_local_active_rq: 0\r\nhttp.admin.downstream_cx_destroy_remote: 11\r\nhttp.admin.downstream_cx_destroy_remote_active_rq: 0\r\nhttp.admin.downstream_cx_drain_close: 0\r\nhttp.admin.downstream_cx_http1_active: 1\r\nhttp.admin.downstream_cx_http1_total: 2652\r\nhttp.admin.downstream_cx_http2_active: 0\r\nhttp.admin.downstream_cx_http2_total: 0\r\nhttp.admin.downstream_cx_http3_active: 0\r\nhttp.admin.downstream_cx_http3_total: 0\r\nhttp.admin.downstream_cx_idle_timeout: 0\r\nhttp.admin.downstream_cx_max_duration_reached: 0\r\nhttp.admin.downstream_cx_overload_disable_keepalive: 0\r\nhttp.admin.downstream_cx_protocol_error: 0\r\nhttp.admin.downstream_cx_rx_bytes_buffered: 86\r\nhttp.admin.downstream_cx_rx_bytes_total: 312741\r\nhttp.admin.downstream_cx_ssl_active: 0\r\nhttp.admin.downstream_cx_ssl_total: 0\r\nhttp.admin.downstream_cx_total: 2652\r\nhttp.admin.downstream_cx_tx_bytes_buffered: 0\r\nhttp.admin.downstream_cx_tx_bytes_total: 664935\r\nhttp.admin.downstream_cx_upgrades_active: 0\r\nhttp.admin.downstream_cx_upgrades_total: 0\r\nhttp.admin.downstream_flow_control_paused_reading_total: 0\r\nhttp.admin.downstream_flow_control_resumed_reading_total: 0\r\nhttp.admin.downstream_rq_1xx: 0\r\nhttp.admin.downstream_rq_2xx: 2640\r\nhttp.admin.downstream_rq_3xx: 0\r\nhttp.admin.downstream_rq_4xx: 8\r\nhttp.admin.downstream_rq_5xx: 4\r\nhttp.admin.downstream_rq_active: 1\r\nhttp.admin.downstream_rq_completed: 2652\r\nhttp.admin.downstream_rq_http1_total: 2653\r\nhttp.admin.downstream_rq_http2_total: 0\r\nhttp.admin.downstream_rq_http3_total: 0\r\nhttp.admin.downstream_rq_idle_timeout: 0\r\nhttp.admin.downstream_rq_non_relative_path: 0\r\nhttp.admin.downstream_rq_overload_close: 0\r\nhttp.admin.downstream_rq_response_before_rq_complete: 0\r\nhttp.admin.downstream_rq_rx_reset: 0\r\nhttp.admin.downstream_rq_timeout: 0\r\nhttp.admin.downstream_rq_too_large: 0\r\nhttp.admin.downstream_rq_total: 2653\r\nhttp.admin.downstream_rq_tx_reset: 0\r\nhttp.admin.downstream_rq_ws_on_non_ws_route: 0\r\nhttp.admin.rs_too_large: 0\r\nhttp.async-client.no_cluster: 0\r\nhttp.async-client.no_route: 0\r\nhttp.async-client.rq_direct_response: 0\r\nhttp.async-client.rq_redirect: 0\r\nhttp.async-client.rq_reset_after_downstream_response_started: 0\r\nhttp.async-client.rq_retry_skipped_request_not_complete: 0\r\nhttp.async-client.rq_total: 3\r\nhttp.ingress_http.downstream_cx_active: 0\r\nhttp.ingress_http.downstream_cx_delayed_close_timeout: 0\r\nhttp.ingress_http.downstream_cx_destroy: 10\r\nhttp.ingress_http.downstream_cx_destroy_active_rq: 0\r\nhttp.ingress_http.downstream_cx_destroy_local: 0\r\nhttp.ingress_http.downstream_cx_destroy_local_active_rq: 0\r\nhttp.ingress_http.downstream_cx_destroy_remote: 10\r\nhttp.ingress_http.downstream_cx_destroy_remote_active_rq: 0\r\nhttp.ingress_http.downstream_cx_drain_close: 0\r\nhttp.ingress_http.downstream_cx_http1_active: 0\r\nhttp.ingress_http.downstream_cx_http1_total: 10\r\nhttp.ingress_http.downstream_cx_http2_active: 0\r\nhttp.ingress_http.downstream_cx_http2_total: 0\r\nhttp.ingress_http.downstream_cx_http3_active: 0\r\nhttp.ingress_http.downstream_cx_http3_total: 0\r\nhttp.ingress_http.downstream_cx_idle_timeout: 0\r\nhttp.ingress_http.downstream_cx_max_duration_reached: 0\r\nhttp.ingress_http.downstream_cx_overload_disable_keepalive: 0\r\nhttp.ingress_http.downstream_cx_protocol_error: 0\r\nhttp.ingress_http.downstream_cx_rx_bytes_buffered: 0\r\nhttp.ingress_http.downstream_cx_rx_bytes_total: 59282\r\nhttp.ingress_http.downstream_cx_ssl_active: 0\r\nhttp.ingress_http.downstream_cx_ssl_total: 10\r\nhttp.ingress_http.downstream_cx_total: 10\r\nhttp.ingress_http.downstream_cx_tx_bytes_buffered: 0\r\nhttp.ingress_http.downstream_cx_tx_bytes_total: 42234\r\nhttp.ingress_http.downstream_cx_upgrades_active: 0\r\nhttp.ingress_http.downstream_cx_upgrades_total: 0\r\nhttp.ingress_http.downstream_flow_control_paused_reading_total: 0\r\nhttp.ingress_http.downstream_flow_control_resumed_reading_total: 0\r\nhttp.ingress_http.downstream_rq_1xx: 0\r\nhttp.ingress_http.downstream_rq_2xx: 4\r\nhttp.ingress_http.downstream_rq_3xx: 0\r\nhttp.ingress_http.downstream_rq_4xx: 3\r\nhttp.ingress_http.downstream_rq_5xx: 236\r\nhttp.ingress_http.downstream_rq_active: 0\r\nhttp.ingress_http.downstream_rq_completed: 243\r\nhttp.ingress_http.downstream_rq_http1_total: 243\r\nhttp.ingress_http.downstream_rq_http2_total: 0\r\nhttp.ingress_http.downstream_rq_http3_total: 0\r\nhttp.ingress_http.downstream_rq_idle_timeout: 0\r\nhttp.ingress_http.downstream_rq_non_relative_path: 0\r\nhttp.ingress_http.downstream_rq_overload_close: 0\r\nhttp.ingress_http.downstream_rq_response_before_rq_complete: 0\r\nhttp.ingress_http.downstream_rq_rx_reset: 0\r\nhttp.ingress_http.downstream_rq_timeout: 0\r\nhttp.ingress_http.downstream_rq_too_large: 0\r\nhttp.ingress_http.downstream_rq_total: 243\r\nhttp.ingress_http.downstream_rq_tx_reset: 0\r\nhttp.ingress_http.downstream_rq_ws_on_non_ws_route: 0\r\nhttp.ingress_http.no_cluster: 0\r\nhttp.ingress_http.no_route: 0\r\nhttp.ingress_http.rds.ingress-http-route-config.config_reload: 1\r\nhttp.ingress_http.rds.ingress-http-route-config.init_fetch_timeout: 0\r\nhttp.ingress_http.rds.ingress-http-route-config.update_attempt: 1439\r\nhttp.ingress_http.rds.ingress-http-route-config.update_empty: 0\r\nhttp.ingress_http.rds.ingress-http-route-config.update_failure: 0\r\nhttp.ingress_http.rds.ingress-http-route-config.update_rejected: 0\r\nhttp.ingress_http.rds.ingress-http-route-config.update_success: 1438\r\nhttp.ingress_http.rds.ingress-http-route-config.version: 310770956959226155\r\nhttp.ingress_http.rds.route-t1-t1-vh1.config_reload: 1\r\nhttp.ingress_http.rds.route-t1-t1-vh1.init_fetch_timeout: 0\r\nhttp.ingress_http.rds.route-t1-t1-vh1.update_attempt: 1439\r\nhttp.ingress_http.rds.route-t1-t1-vh1.update_empty: 0\r\nhttp.ingress_http.rds.route-t1-t1-vh1.update_failure: 0\r\nhttp.ingress_http.rds.route-t1-t1-vh1.update_rejected: 0\r\nhttp.ingress_http.rds.route-t1-t1-vh1.update_success: 1438\r\nhttp.ingress_http.rds.route-t1-t1-vh1.version: 310770956959226155\r\nhttp.ingress_http.rq_direct_response: 0\r\nhttp.ingress_http.rq_redirect: 0\r\nhttp.ingress_http.rq_reset_after_downstream_response_started: 0\r\nhttp.ingress_http.rq_retry_skipped_request_not_complete: 0\r\nhttp.ingress_http.rq_total: 243\r\nhttp.ingress_http.rs_too_large: 0\r\nhttp.ingress_http.tracing.client_enabled: 0\r\nhttp.ingress_http.tracing.health_check: 0\r\nhttp.ingress_http.tracing.not_traceable: 0\r\nhttp.ingress_http.tracing.random_sampling: 0\r\nhttp.ingress_http.tracing.service_forced: 0\r\nhttp1.metadata_not_supported_error: 0\r\ninit_fetch_timeout: 0\r\nlistener.0.0.0.0_10000.downstream_cx_active: 0\r\nlistener.0.0.0.0_10000.downstream_cx_destroy: 0\r\nlistener.0.0.0.0_10000.downstream_cx_total: 0\r\nlistener.0.0.0.0_10000.downstream_pre_cx_active: 0\r\nlistener.0.0.0.0_10000.downstream_pre_cx_timeout: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_1xx: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_2xx: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_3xx: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_4xx: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_5xx: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_completed: 0\r\nlistener.0.0.0.0_10000.no_filter_chain_match: 0\r\nlistener.0.0.0.0_10000.worker_0.downstream_cx_active: 0\r\nlistener.0.0.0.0_10000.worker_0.downstream_cx_total: 0\r\nlistener.0.0.0.0_10000.worker_1.downstream_cx_active: 0\r\nlistener.0.0.0.0_10000.worker_1.downstream_cx_total: 0\r\nlistener.0.0.0.0_10443.downstream_cx_active: 0\r\nlistener.0.0.0.0_10443.downstream_cx_destroy: 10\r\nlistener.0.0.0.0_10443.downstream_cx_total: 10\r\nlistener.0.0.0.0_10443.downstream_pre_cx_active: 0\r\nlistener.0.0.0.0_10443.downstream_pre_cx_timeout: 0\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_1xx: 0\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_2xx: 4\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_3xx: 0\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_4xx: 3\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_5xx: 236\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_completed: 243\r\nlistener.0.0.0.0_10443.no_filter_chain_match: 100\r\nlistener.0.0.0.0_10443.server_ssl_socket_factory.downstream_context_secrets_not_ready: 0\r\nlistener.0.0.0.0_10443.server_ssl_socket_factory.ssl_context_update_by_sds: 2\r\nlistener.0.0.0.0_10443.server_ssl_socket_factory.upstream_context_secrets_not_ready: 0\r\nlistener.0.0.0.0_10443.ssl.ciphers.ECDHE-RSA-AES128-GCM-SHA256: 3\r\nlistener.0.0.0.0_10443.ssl.ciphers.TLS_AES_128_GCM_SHA256: 7\r\nlistener.0.0.0.0_10443.ssl.connection_error: 0\r\nlistener.0.0.0.0_10443.ssl.curves.X25519: 10\r\nlistener.0.0.0.0_10443.ssl.fail_verify_cert_hash: 0\r\nlistener.0.0.0.0_10443.ssl.fail_verify_error: 0\r\nlistener.0.0.0.0_10443.ssl.fail_verify_no_cert: 0\r\nlistener.0.0.0.0_10443.ssl.fail_verify_san: 0\r\nlistener.0.0.0.0_10443.ssl.handshake: 10\r\nlistener.0.0.0.0_10443.ssl.no_certificate: 10\r\nlistener.0.0.0.0_10443.ssl.session_reused: 7\r\nlistener.0.0.0.0_10443.ssl.versions.TLSv1.2: 3\r\nlistener.0.0.0.0_10443.ssl.versions.TLSv1.3: 7\r\nlistener.0.0.0.0_10443.worker_0.downstream_cx_active: 0\r\nlistener.0.0.0.0_10443.worker_0.downstream_cx_total: 2\r\nlistener.0.0.0.0_10443.worker_1.downstream_cx_active: 0\r\nlistener.0.0.0.0_10443.worker_1.downstream_cx_total: 8\r\nlistener.admin.downstream_cx_active: 1\r\nlistener.admin.downstream_cx_destroy: 2651\r\nlistener.admin.downstream_cx_total: 2652\r\nlistener.admin.downstream_pre_cx_active: 0\r\nlistener.admin.downstream_pre_cx_timeout: 0\r\nlistener.admin.http.admin.downstream_rq_1xx: 0\r\nlistener.admin.http.admin.downstream_rq_2xx: 2640\r\nlistener.admin.http.admin.downstream_rq_3xx: 0\r\nlistener.admin.http.admin.downstream_rq_4xx: 8\r\nlistener.admin.http.admin.downstream_rq_5xx: 4\r\nlistener.admin.http.admin.downstream_rq_completed: 2652\r\nlistener.admin.main_thread.downstream_cx_active: 1\r\nlistener.admin.main_thread.downstream_cx_total: 2652\r\nlistener.admin.no_filter_chain_match: 0\r\nlistener_manager.lds.init_fetch_timeout: 0\r\nlistener_manager.lds.update_attempt: 1440\r\nlistener_manager.lds.update_failure: 1\r\nlistener_manager.lds.update_rejected: 0\r\nlistener_manager.lds.update_success: 1438\r\nlistener_manager.lds.version: 310770956959226155\r\nlistener_manager.listener_added: 2\r\nlistener_manager.listener_create_failure: 0\r\nlistener_manager.listener_create_success: 4\r\nlistener_manager.listener_modified: 0\r\nlistener_manager.listener_removed: 0\r\nlistener_manager.listener_stopped: 0\r\nlistener_manager.total_listeners_active: 2\r\nlistener_manager.total_listeners_draining: 0\r\nlistener_manager.total_listeners_warming: 0\r\nruntime.admin_overrides_active: 0\r\nruntime.deprecated_feature_use: 0\r\nruntime.load_error: 0\r\nruntime.load_success: 1\r\nruntime.num_keys: 0\r\nruntime.num_layers: 2\r\nruntime.override_dir_exists: 0\r\nruntime.override_dir_not_exists: 1\r\nserver.concurrency: 2\r\nserver.days_until_first_cert_expiring: 0\r\nserver.debug_assertion_failures: 0\r\nserver.dynamic_unknown_fields: 0\r\nserver.hot_restart_epoch: 0\r\nserver.live: 1\r\nserver.main_thread.watchdog_mega_miss: 0\r\nserver.main_thread.watchdog_miss: 0\r\nserver.memory_allocated: 4368128\r\nserver.memory_heap_size: 6291456\r\nserver.parent_connections: 0\r\nserver.state: 0\r\nserver.static_unknown_fields: 0\r\nserver.stats_recent_lookups: 0\r\nserver.total_connections: 0\r\nserver.uptime: 7211\r\nserver.version: 9381141\r\nserver.watchdog_mega_miss: 0\r\nserver.watchdog_miss: 0\r\nserver.worker_0.watchdog_mega_miss: 0\r\nserver.worker_0.watchdog_miss: 0\r\nserver.worker_1.watchdog_mega_miss: 0\r\nserver.worker_1.watchdog_miss: 0\r\ntls_inspector.alpn_found: 90\r\ntls_inspector.alpn_not_found: 20\r\ntls_inspector.client_hello_too_large: 0\r\ntls_inspector.connection_closed: 2841\r\ntls_inspector.read_error: 0\r\ntls_inspector.sni_found: 109\r\ntls_inspector.sni_not_found: 1\r\ntls_inspector.tls_found: 110\r\ntls_inspector.tls_not_found: 0\r\nupdate_attempt: 2878\r\nupdate_failure: 0\r\nupdate_rejected: 0\r\nupdate_success: 2876\r\nversion: 310770956959226155\r\n</details>\r\n\r\nConfig:\r\n\r\n<details><summary>config</summary>\r\n<p>\r\n```json\r\n{\r\n \"configs\": [\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.BootstrapConfigDump\",\r\n   \"bootstrap\": {\r\n    \"node\": {\r\n     \"id\": \"ingressnode\",\r\n     \"cluster\": \"cluster1\",\r\n     \"build_version\": \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n    },\r\n    \"static_resources\": {\r\n     \"clusters\": [\r\n      {\r\n       \"name\": \"rtcontroller-ads\",\r\n       \"type\": \"STRICT_DNS\",\r\n       \"connect_timeout\": \"25s\",\r\n       \"tls_context\": {\r\n        \"common_tls_context\": {\r\n         \"tls_certificates\": [\r\n          {\r\n           \"certificate_chain\": {\r\n            \"filename\": \"/home/cert/tls.crt\"\r\n           },\r\n           \"private_key\": {\r\n            \"filename\": \"/home/cert/tls.key\"\r\n           }\r\n          }\r\n         ],\r\n         \"validation_context\": {\r\n          \"trusted_ca\": {\r\n           \"filename\": \"home/rootca/tls.crt\"\r\n          }\r\n         }\r\n        }\r\n       },\r\n       \"http2_protocol_options\": {},\r\n       \"upstream_connection_options\": {\r\n        \"tcp_keepalive\": {\r\n         \"keepalive_probes\": 3,\r\n         \"keepalive_time\": 30,\r\n         \"keepalive_interval\": 5\r\n        }\r\n       },\r\n       \"load_assignment\": {\r\n        \"cluster_name\": \"---ads\",\r\n        \"endpoints\": [\r\n         {\r\n          \"lb_endpoints\": [\r\n           {\r\n            \"endpoint\": {\r\n             \"address\": {\r\n              \"socket_address\": {\r\n               \"address\": \"...svc.cluster.local\",\r\n               \"port_value\": 443\r\n              }\r\n             }\r\n            }\r\n           }\r\n          ]\r\n         }\r\n        ]\r\n       },\r\n       \"respect_dns_ttl\": true\r\n      }\r\n     ]\r\n    },\r\n    \"dynamic_resources\": {\r\n     \"lds_config\": {\r\n      \"ads\": {}\r\n     },\r\n     \"cds_config\": {\r\n      \"ads\": {}\r\n     },\r\n     \"ads_config\": {\r\n      \"api_type\": \"GRPC\",\r\n      \"grpc_services\": [\r\n       {\r\n        \"envoy_grpc\": {\r\n         \"cluster_name\": \"..-ads\"\r\n        }\r\n       }\r\n      ],\r\n      \"rate_limit_settings\": {\r\n       \"max_tokens\": 10,\r\n       \"fill_rate\": 1\r\n      }\r\n     }\r\n    },\r\n    \"admin\": {\r\n     \"access_log_path\": \"/dev/stdout\",\r\n     \"address\": {\r\n      \"socket_address\": {\r\n       \"address\": \"0.0.0.0\",\r\n       \"port_value\": 9901\r\n      }\r\n     }\r\n    }\r\n   },\r\n   \"last_updated\": \"2020-08-14T09:13:04.710Z\"\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.ClustersConfigDump\",\r\n   \"version_info\": \"---\",\r\n   \"static_clusters\": [\r\n    {\r\n     \"cluster\": {\r\n      \"name\": \"rtcontroller-ads\",\r\n      \"type\": \"STRICT_DNS\",\r\n      \"connect_timeout\": \"25s\",\r\n      \"tls_context\": {\r\n       \"common_tls_context\": {\r\n        \"tls_certificates\": [\r\n         {\r\n          \"certificate_chain\": {\r\n           \"filename\": \"/home/cert/tls.crt\"\r\n          },\r\n          \"private_key\": {\r\n           \"filename\": \"/home/cert/tls.key\"\r\n          }\r\n         }\r\n        ],\r\n        \"validation_context\": {\r\n         \"trusted_ca\": {\r\n          \"filename\": \"home/rootca/tls.crt\"\r\n         }\r\n        }\r\n       }\r\n      },\r\n      \"http2_protocol_options\": {},\r\n      \"upstream_connection_options\": {\r\n       \"tcp_keepalive\": {\r\n        \"keepalive_probes\": 3,\r\n        \"keepalive_time\": 30,\r\n        \"keepalive_interval\": 5\r\n       }\r\n      },\r\n      \"load_assignment\": {\r\n       \"cluster_name\": \"..-ads\",\r\n       \"endpoints\": [\r\n        {\r\n         \"lb_endpoints\": [\r\n          {\r\n           \"endpoint\": {\r\n            \"address\": {\r\n             \"socket_address\": {\r\n              \"address\": \"..svc.cluster.local\",\r\n              \"port_value\": 443\r\n             }\r\n            }\r\n           }\r\n          }\r\n         ]\r\n        }\r\n       ]\r\n      },\r\n      \"respect_dns_ttl\": true\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:04.715Z\"\r\n    }\r\n   ],\r\n   \"dynamic_active_clusters\": [\r\n    {\r\n     \"version_info\": \"----\",\r\n     \"cluster\": {\r\n      \"name\": \"----\",\r\n      \"type\": \"STRICT_DNS\",\r\n      \"connect_timeout\": \"5s\",\r\n      \"circuit_breakers\": {\r\n       \"thresholds\": [\r\n        {\r\n         \"max_connections\": 8000\r\n        },\r\n        {\r\n         \"priority\": \"HIGH\",\r\n         \"max_connections\": 10000\r\n        }\r\n       ]\r\n      },\r\n      \"transport_socket\": {\r\n       \"name\": \"envoy.transport_sockets.tls\",\r\n       \"typed_config\": {\r\n        \"@type\": \"type.googleapis.com/envoy.api.v2.auth.UpstreamTlsContext\",\r\n        \"common_tls_context\": {\r\n         \"validation_context\": {\r\n          \"trusted_ca\": {\r\n           \"filename\": \"/home/rootca/tls.crt\"\r\n          }\r\n         }\r\n        },\r\n        \"sni\": \"..svc.cluster.local\"\r\n       }\r\n      },\r\n      \"load_assignment\": {\r\n       \"cluster_name\": \"----\",\r\n       \"endpoints\": [\r\n        {\r\n         \"lb_endpoints\": [\r\n          {\r\n           \"endpoint\": {\r\n            \"address\": {\r\n             \"socket_address\": {\r\n              \"address\": \"...svc.cluster.local\",\r\n              \"port_value\": 8443\r\n             }\r\n            }\r\n           }\r\n          }\r\n         ]\r\n        }\r\n       ]\r\n      },\r\n      \"respect_dns_ttl\": true\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:31.576Z\"\r\n    }\r\n   ]\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.ListenersConfigDump\",\r\n   \"version_info\": \"----\",\r\n   \"dynamic_active_listeners\": [\r\n    {\r\n     \"version_info\": \"----\",\r\n     \"listener\": {\r\n      \"name\": \"listener-ingress-http\",\r\n      \"address\": {\r\n       \"socket_address\": {\r\n        \"address\": \"0.0.0.0\",\r\n        \"port_value\": 10000\r\n       }\r\n      },\r\n      \"filter_chains\": [\r\n       {\r\n        \"filters\": [\r\n         {\r\n          \"name\": \"envoy.http_connection_manager\",\r\n          \"typed_config\": {\r\n           \"@type\": \"type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\",\r\n           \"stat_prefix\": \"ingress_http\",\r\n           \"http_filters\": [\r\n            {\r\n             \"name\": \"envoy.router\"\r\n            }\r\n           ],\r\n           \"add_user_agent\": true,\r\n           \"access_log\": [\r\n            {\r\n             \"name\": \"envoy.file_access_log\",\r\n             \"typed_config\": {\r\n              \"@type\": \"type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\",\r\n              \"path\": \"/dev/stdout\",\r\n              \"format\": \"%START_TIME% [INFO] [INGRESS]:[Request_Id: %REQ(X-REQUEST-ID)%], Protocol: %PROTOCOL%, Method : %REQ(:METHOD)%, Path: %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%, Response Code: %RESPONSE_CODE%, Host: %REQ(:AUTHORITY)%, Duration: %DURATION%, Upstream Service Time: %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%, Bytes Received: %BYTES_RECEIVED%, Bytes Sent: %BYTES_SENT%\\n\"\r\n             }\r\n            }\r\n           ],\r\n           \"rds\": {\r\n            \"config_source\": {\r\n             \"ads\": {}\r\n            },\r\n            \"route_config_name\": \"ingress-http-route-config\"\r\n           }\r\n          }\r\n         }\r\n        ]\r\n       }\r\n      ],\r\n      \"listener_filters\": [\r\n       {\r\n        \"name\": \"envoy.listener.original_dst\"\r\n       }\r\n      ]\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:31.578Z\"\r\n    },\r\n    {\r\n     \"version_info\": \"---\",\r\n     \"listener\": {\r\n      \"name\": \"listener-ingress-tls\",\r\n      \"address\": {\r\n       \"socket_address\": {\r\n        \"address\": \"0.0.0.0\",\r\n        \"port_value\": 10443\r\n       }\r\n      },\r\n      \"filter_chains\": [\r\n       {\r\n        \"filter_chain_match\": {\r\n         \"server_names\": [\r\n          \"t1-vh1.apim.sap.com\"\r\n         ]\r\n        },\r\n        \"filters\": [\r\n         {\r\n          \"name\": \"envoy.http_connection_manager\",\r\n          \"typed_config\": {\r\n           \"@type\": \"type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\",\r\n           \"stat_prefix\": \"ingress_http\",\r\n           \"http_filters\": [\r\n            {\r\n             \"name\": \"envoy.router\"\r\n            }\r\n           ],\r\n           \"add_user_agent\": true,\r\n           \"access_log\": [\r\n            {\r\n             \"name\": \"envoy.file_access_log\",\r\n             \"typed_config\": {\r\n              \"@type\": \"type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\",\r\n              \"path\": \"/dev/stdout\",\r\n              \"format\": \"%START_TIME% [INFO] [INGRESS]:[Request_Id: %REQ(X-REQUEST-ID)%], Protocol: %PROTOCOL%, Method : %REQ(:METHOD)%, Path: %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%, Response Code: %RESPONSE_CODE%, Host: %REQ(:AUTHORITY)%, Duration: %DURATION%, Upstream Service Time: %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%, Bytes Received: %BYTES_RECEIVED%, Bytes Sent: %BYTES_SENT%\\n\"\r\n             }\r\n            }\r\n           ],\r\n           \"rds\": {\r\n            \"config_source\": {\r\n             \"ads\": {}\r\n            },\r\n            \"route_config_name\": \"route----\"\r\n           }\r\n          }\r\n         }\r\n        ],\r\n        \"transport_socket\": {\r\n         \"name\": \"envoy.transport_sockets.tls\",\r\n         \"typed_config\": {\r\n          \"@type\": \"type.googleapis.com/envoy.api.v2.auth.DownstreamTlsContext\",\r\n          \"common_tls_context\": {\r\n           \"tls_certificate_sds_secret_configs\": [\r\n            {\r\n             \"name\": \"---\",\r\n             \"sds_config\": {\r\n              \"ads\": {}\r\n             }\r\n            }\r\n           ],\r\n           \"validation_context_sds_secret_config\": {\r\n            \"name\": \"---\",\r\n            \"sds_config\": {\r\n             \"ads\": {}\r\n            }\r\n           }\r\n          }\r\n         }\r\n        },\r\n        \"name\": \"filterchain-t1-t1-vh1\"\r\n       }\r\n      ],\r\n      \"listener_filters\": [\r\n       {\r\n        \"name\": \"envoy.listener.tls_inspector\"\r\n       }\r\n      ]\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:31.579Z\"\r\n    }\r\n   ]\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.ScopedRoutesConfigDump\"\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.RoutesConfigDump\",\r\n   \"dynamic_route_configs\": [\r\n    {\r\n     \"version_info\": \"---\",\r\n     \"route_config\": {\r\n      \"name\": \"route-1--\",\r\n      \"virtual_hosts\": [\r\n       {\r\n        \"name\": \"route-1\",\r\n        \"domains\": [\r\n         \"---\"\r\n        ],\r\n        \"routes\": [\r\n         {\r\n          \"match\": {\r\n           \"prefix\": \"/\"\r\n          },\r\n          \"route\": {\r\n           \"cluster\": \"test\",\r\n           \"timeout\": \"15s\",\r\n           \"idle_timeout\": \"315s\"\r\n          }\r\n         }\r\n        ]\r\n       }\r\n      ]\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:31.581Z\"\r\n    },\r\n    {\r\n     \"version_info\": \"---\",\r\n     \"route_config\": {\r\n      \"name\": \"ingress-http-route-config\"\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:31.581Z\"\r\n    }\r\n   ]\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.SecretsConfigDump\",\r\n   \"dynamic_active_secrets\": [\r\n    {\r\n     \"name\": \"---\",\r\n     \"version_info\": \"---\",\r\n     \"last_updated\": \"2020-08-14T09:15:54.416Z\",\r\n     \"secret\": {\r\n      \"name\": \"----\",\r\n      \"tls_certificate\": {\r\n       \"certificate_chain\": {\r\n        \"inline_bytes\": \"---\"\r\n       },\r\n       \"private_key\": {\r\n        \"inline_string\": \"[redacted]\"\r\n       }\r\n      }\r\n     }\r\n    },\r\n    {\r\n     \"name\": \"----\",\r\n     \"version_info\": \"----\",\r\n     \"last_updated\": \"2020-08-14T09:15:54.416Z\",\r\n     \"secret\": {\r\n      \"name\": \"----\",\r\n      \"validation_context\": {\r\n       \"trusted_ca\": {\r\n        \"inline_bytes\": \"-------\"\r\n       }\r\n      }\r\n     }\r\n    }\r\n   ]\r\n  }\r\n ]\r\n}\r\n```\r\n</p>\r\n</details>\r\n\r\nLogs:\r\n\r\n<details><summary>Log</summary>\r\n[2020-08-14 10:35:44.511][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 247 bytes\r\n[2020-08-14 10:35:44.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7322 bytes\r\n[2020-08-14 10:35:44.511][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 873 bytes, end_stream false\r\n[2020-08-14 10:35:44.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:35:44.511][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:44.511][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:44.511][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 873\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 30\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 30 bytes into 1 slices\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 30 bytes\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 30 bytes\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:35:44.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1953] new connection\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1953] socket event: 2\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1953] write ready\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1953] socket event: 3\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1953] write ready\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1953] read ready\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1953] read returns: 118\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1953] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1953] parsing 118 bytes\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1953] message begin\r\n[2020-08-14 10:35:44.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1953] new stream\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1953] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1953] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1953] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1953] headers complete\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1953] completed header: key=Connection value=close\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1953] message complete\r\n[2020-08-14 10:35:44.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1953][S5255068674068296761] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:44.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1953][S5255068674068296761] request end stream\r\n[2020-08-14 10:35:44.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1953][S5255068674068296761] request complete: path: /ready\r\n[2020-08-14 10:35:44.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1953][S5255068674068296761] closing connection due to connection close header\r\n[2020-08-14 10:35:44.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1953][S5255068674068296761] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:44 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1953] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1953][S5255068674068296761] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1953] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:44.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1953] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1953][S5255068674068296761] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1953] parsed 118 bytes\r\n[2020-08-14 10:35:44.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1953] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:44.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:44.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1953] socket event: 2\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1953] write ready\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1953] write returns: 243\r\n[2020-08-14 10:35:44.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1953] write flush complete\r\n[2020-08-14 10:35:44.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1953] closing socket: 1\r\n[2020-08-14 10:35:44.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1953] adding to cleanup list\r\n[2020-08-14 10:35:44.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.636][13][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:44.636][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:44.636][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:44.636][13][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.636][13][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.636][13][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:44.636][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:44.636][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:44.636][13][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.636][13][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:45.041][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:35:47.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1954] new connection\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1954] socket event: 2\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1954] write ready\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1954] socket event: 3\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1954] write ready\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1954] read ready\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1954] read returns: 118\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1954] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1954] parsing 118 bytes\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1954] message begin\r\n[2020-08-14 10:35:47.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1954] new stream\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1954] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1954] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1954] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1954] headers complete\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1954] completed header: key=Connection value=close\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1954] message complete\r\n[2020-08-14 10:35:47.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1954][S14082497283590410047] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:47.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1954][S14082497283590410047] request end stream\r\n[2020-08-14 10:35:47.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1954][S14082497283590410047] request complete: path: /ready\r\n[2020-08-14 10:35:47.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1954][S14082497283590410047] closing connection due to connection close header\r\n[2020-08-14 10:35:47.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1954][S14082497283590410047] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:47 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1954] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1954][S14082497283590410047] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1954] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:47.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:47.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1954] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1954][S14082497283590410047] decode headers called: filter=0x564e43373f80 status=1\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1954] parsed 118 bytes\r\n[2020-08-14 10:35:47.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1954] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:47.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:47.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1954] socket event: 2\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1954] write ready\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1954] write returns: 243\r\n[2020-08-14 10:35:47.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1954] write flush complete\r\n[2020-08-14 10:35:47.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1954] closing socket: 1\r\n[2020-08-14 10:35:47.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1954] adding to cleanup list\r\n[2020-08-14 10:35:47.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:47.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:49.523][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:49.523][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:49.523][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 4644\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 2674\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7318 bytes into 1 slices\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7318 bytes\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:49.524][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4635 end_stream=false)\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:35:49.52335533 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3953\"\r\n\r\n[2020-08-14 10:35:49.524][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 231 bytes\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:49.524][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=430 end_stream=false)\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:49.524][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:35:49.524][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:35:49.52335533 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3954\"\r\n\r\n[2020-08-14 10:35:49.524][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 191 bytes\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:49.524][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=339 end_stream=false)\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.RouteConfiguration at version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.RouteConfiguration: version_info: \"2020-08-14 10:35:49.52335533 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"route-t1-t1-vh1\"\r\nresource_names: \"ingress-http-route-config\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.RouteConfiguration\"\r\nresponse_nonce: \"3955\"\r\n\r\n[2020-08-14 10:35:49.524][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 246 bytes\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:49.524][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1878 end_stream=false)\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:35:49.524][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:35:49.524][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:35:49.52335533 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3956\"\r\n\r\n[2020-08-14 10:35:49.524][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7318 bytes\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 869 bytes, end_stream false\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 869\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 30\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 30 bytes into 1 slices\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 30 bytes\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 30 bytes\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:35:49.777][14][debug][router] [source/common/router/router.cc:724] [C1916][S6524082276721005270] upstream timeout\r\n[2020-08-14 10:35:49.777][14][debug][router] [source/common/router/router.cc:1564] [C1916][S6524082276721005270] resetting pool request\r\n[2020-08-14 10:35:49.777][14][debug][client] [source/common/http/codec_client.cc:111] [C1948] request reset\r\n[2020-08-14 10:35:49.777][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:49.777][14][debug][connection] [source/common/network/connection_impl.cc:104] [C1948] closing data_to_write=0 type=1\r\n[2020-08-14 10:35:49.777][14][debug][connection] [source/common/network/connection_impl.cc:193] [C1948] closing socket: 1\r\n[2020-08-14 10:35:49.777][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:298] [C1948] SSL shutdown: rc=0\r\n[2020-08-14 10:35:49.777][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C1948] \r\n[2020-08-14 10:35:49.777][14][debug][client] [source/common/http/codec_client.cc:88] [C1948] disconnect. resetting 0 pending requests\r\n[2020-08-14 10:35:49.777][14][debug][pool] [source/common/http/http1/conn_pool.cc:136] [C1948] client disconnected, failure reason: \r\n[2020-08-14 10:35:49.777][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=2)\r\n[2020-08-14 10:35:49.777][14][debug][http] [source/common/http/conn_manager_impl.cc:1354] [C1916][S6524082276721005270] Sending local reply with details upstream_response_timeout\r\n[2020-08-14 10:35:49.777][14][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1916][S6524082276721005270] encoding headers via codec (end_stream=false):\r\n':status', '504'\r\n'content-length', '24'\r\n'content-type', 'text/plain'\r\n'date', 'Fri, 14 Aug 2020 10:35:49 GMT'\r\n'server', 'envoy'\r\n\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1916] writing 130 bytes, end_stream false\r\n[2020-08-14 10:35:49.777][14][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1916][S6524082276721005270] encoding data via codec (size=24 end_stream=true)\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1916] writing 24 bytes, end_stream false\r\n[2020-08-14 10:35:49.777][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=3)\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1916] readDisable: enabled=false disable=false\r\n[2020-08-14 10:35:49.777][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=3)\r\n2020-08-14T10:35:34.778Z [INFO] [ACCESS_LOG_INGRESS]:[Request_Id: 5dde75fe-11c3-446e-8e21-478fb13c82cc], Protocol: HTTP/1.1, Method : GET, Path: /pthru?d=20000, Response Code: 504, Host: test.host, Duration: 14999, Upstream Service Time: -, Bytes Received: 0, Bytes Sent: 24\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 2\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1916] ssl write returns: 154\r\n[2020-08-14 10:35:50.042][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 3\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1916] read ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1916] ssl read returns: 244\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1916] ssl read returns: -1\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1916] ssl read 244 bytes into 1 slices\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1916] parsing 244 bytes\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1916] message begin\r\n[2020-08-14 10:35:50.204][14][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1916] new stream\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Host value=test.host\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=User-Agent value=PostmanRuntime/7.26.3\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Accept value=*/*\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Cache-Control value=no-cache\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Postman-Token value=89dd6ee9-2ec4-405b-8303-5743fc405d59\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Accept-Encoding value=gzip, deflate, br\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1916] headers complete\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Connection value=keep-alive\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1916] message complete\r\n[2020-08-14 10:35:50.204][14][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1916][S14993064456358697936] request headers complete (end_stream=true):\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n'user-agent', 'PostmanRuntime/7.26.3'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '89dd6ee9-2ec4-405b-8303-5743fc405d59'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'connection', 'keep-alive'\r\n\r\n[2020-08-14 10:35:50.204][14][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1916][S14993064456358697936] request end stream\r\n[2020-08-14 10:35:50.204][14][debug][router] [source/common/router/router.cc:434] [C1916][S14993064456358697936] cluster 'policyengine-t1' match for URL '/pthru?d=20000'\r\n[2020-08-14 10:35:50.204][14][debug][router] [source/common/router/router.cc:549] [C1916][S14993064456358697936] router decoding headers:\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n':scheme', 'https'\r\n'user-agent', 'PostmanRuntime/7.26.3'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '89dd6ee9-2ec4-405b-8303-5743fc405d59'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'x-forwarded-proto', 'https'\r\n'x-envoy-downstream-service-cluster', 'cluster1'\r\n'x-envoy-downstream-service-node', 'ingressnode'\r\n'x-request-id', '7c1115df-3b51-458c-ab2e-ce744b40c6b0'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2020-08-14 10:35:50.204][14][debug][pool] [source/common/http/http1/conn_pool.cc:95] creating a new connection\r\n[2020-08-14 10:35:50.204][14][debug][client] [source/common/http/codec_client.cc:31] [C1955] connecting\r\n[2020-08-14 10:35:50.204][14][debug][connection] [source/common/network/connection_impl.cc:711] [C1955] connecting to 100.69.188.67:8443\r\n[2020-08-14 10:35:50.204][14][debug][connection] [source/common/network/connection_impl.cc:720] [C1955] connection in progress\r\n[2020-08-14 10:35:50.204][14][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1916][S14993064456358697936] decode headers called: filter=0x564e4337e060 status=1\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1916] parsed 244 bytes\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1916] readDisable: enabled=true disable=true\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 2\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1955] socket event: 2\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:35:50.204][14][debug][connection] [source/common/network/connection_impl.cc:559] [C1955] connected\r\n[2020-08-14 10:35:50.204][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C1955] handshake expecting read\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1955] socket event: 3\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:35:50.204][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:176] [C1955] handshake complete\r\n[2020-08-14 10:35:50.204][14][debug][client] [source/common/http/codec_client.cc:69] [C1955] connected\r\n[2020-08-14 10:35:50.204][14][debug][pool] [source/common/http/http1/conn_pool.cc:249] [C1955] attaching to next request\r\n[2020-08-14 10:35:50.204][14][debug][router] [source/common/router/router.cc:1618] [C1916][S14993064456358697936] pool ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1955] writing 448 bytes, end_stream false\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1955] ssl write returns: 448\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1955] read ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1955] ssl read returns: -1\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1955] ssl read 0 bytes into 0 slices\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1955] socket event: 2\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:35:50.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1956] new connection\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1956] socket event: 2\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1956] write ready\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1956] socket event: 3\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1956] write ready\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1956] read ready\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1956] read returns: 118\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1956] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1956] parsing 118 bytes\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1956] message begin\r\n[2020-08-14 10:35:50.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1956] new stream\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1956] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1956] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1956] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1956] headers complete\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1956] completed header: key=Connection value=close\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1956] message complete\r\n[2020-08-14 10:35:50.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1956][S6915782699599929615] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:50.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1956][S6915782699599929615] request end stream\r\n[2020-08-14 10:35:50.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1956][S6915782699599929615] request complete: path: /ready\r\n[2020-08-14 10:35:50.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1956][S6915782699599929615] closing connection due to connection close header\r\n[2020-08-14 10:35:50.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1956][S6915782699599929615] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:50 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1956] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1956][S6915782699599929615] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1956] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:50.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:50.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1956] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1956][S6915782699599929615] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1956] parsed 118 bytes\r\n[2020-08-14 10:35:50.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1956] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:50.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:50.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1956] socket event: 2\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1956] write ready\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1956] write returns: 243\r\n[2020-08-14 10:35:50.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1956] write flush complete\r\n[2020-08-14 10:35:50.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1956] closing socket: 1\r\n[2020-08-14 10:35:50.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1956] adding to cleanup list\r\n[2020-08-14 10:35:50.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:50.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:53.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1957] new connection\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1957] socket event: 2\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1957] write ready\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1957] socket event: 3\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1957] write ready\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1957] read ready\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1957] read returns: 118\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1957] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1957] parsing 118 bytes\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1957] message begin\r\n[2020-08-14 10:35:53.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1957] new stream\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1957] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1957] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1957] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1957] headers complete\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1957] completed header: key=Connection value=close\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1957] message complete\r\n[2020-08-14 10:35:53.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1957][S1757894607638928359] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:53.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1957][S1757894607638928359] request end stream\r\n[2020-08-14 10:35:53.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1957][S1757894607638928359] request complete: path: /ready\r\n[2020-08-14 10:35:53.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1957][S1757894607638928359] closing connection due to connection close header\r\n[2020-08-14 10:35:53.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1957][S1757894607638928359] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:53 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1957] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1957][S1757894607638928359] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1957] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:53.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:53.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1957] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1957][S1757894607638928359] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1957] parsed 118 bytes\r\n[2020-08-14 10:35:53.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1957] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:53.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:53.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1957] socket event: 2\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1957] write ready\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1957] write returns: 243\r\n[2020-08-14 10:35:53.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1957] write flush complete\r\n[2020-08-14 10:35:53.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1957] closing socket: 1\r\n[2020-08-14 10:35:53.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1957] adding to cleanup list\r\n[2020-08-14 10:35:53.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:53.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.327][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:54.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:54.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:54.327][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.327][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.327][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:54.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:54.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:54.327][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.327][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 7322\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7322 bytes into 1 slices\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7322 bytes\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:54.514][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4636 end_stream=false)\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:35:54.513647013 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3957\"\r\n\r\n[2020-08-14 10:35:54.514][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 232 bytes\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:54.514][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1879 end_stream=false)\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:35:54.514][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:35:54.514][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:35:54.513647013 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3958\"\r\n\r\n[2020-08-14 10:35:54.514][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 193 bytes\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:54.514][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=431 end_stream=false)\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:54.514][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:35:54.514][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:35:54.513647013 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3959\"\r\n\r\n[2020-08-14 10:35:54.514][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:54.514][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=340 end_stream=false)\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.RouteConfiguration at version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.RouteConfiguration: version_info: \"2020-08-14 10:35:54.513647013 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"route-t1-t1-vh1\"\r\nresource_names: \"ingress-http-route-config\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.RouteConfiguration\"\r\nresponse_nonce: \"3960\"\r\n\r\n[2020-08-14 10:35:54.514][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 247 bytes\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7322 bytes\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 873 bytes, end_stream false\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 873\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 30\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 30 bytes into 1 slices\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 30 bytes\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 30 bytes\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:35:54.636][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:54.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:54.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:54.636][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.636][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.636][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:54.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:54.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:54.636][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.636][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:55.039][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:35:56.121][14][debug][router] [source/common/router/router.cc:724] [C1633][S10994230258690132099] upstream timeout\r\n[2020-08-14 10:35:56.121][14][debug][router] [source/common/router/router.cc:1564] [C1633][S10994230258690132099] resetting pool request\r\n[2020-08-14 10:35:56.121][14][debug][client] [source/common/http/codec_client.cc:111] [C1951] request reset\r\n[2020-08-14 10:35:56.121][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:56.121][14][debug][connection] [source/common/network/connection_impl.cc:104] [C1951] closing data_to_write=0 type=1\r\n[2020-08-14 10:35:56.121][14][debug][connection] [source/common/network/connection_impl.cc:193] [C1951] closing socket: 1\r\n[2020-08-14 10:35:56.121][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:298] [C1951] SSL shutdown: rc=0\r\n[2020-08-14 10:35:56.121][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C1951] \r\n[2020-08-14 10:35:56.121][14][debug][client] [source/common/http/codec_client.cc:88] [C1951] disconnect. resetting 0 pending requests\r\n[2020-08-14 10:35:56.121][14][debug][pool] [source/common/http/http1/conn_pool.cc:136] [C1951] client disconnected, failure reason: \r\n[2020-08-14 10:35:56.121][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=2)\r\n[2020-08-14 10:35:56.121][14][debug][http] [source/common/http/conn_manager_impl.cc:1354] [C1633][S10994230258690132099] Sending local reply with details upstream_response_timeout\r\n[2020-08-14 10:35:56.121][14][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1633][S10994230258690132099] encoding headers via codec (end_stream=false):\r\n':status', '504'\r\n'content-length', '24'\r\n'content-type', 'text/plain'\r\n'date', 'Fri, 14 Aug 2020 10:35:56 GMT'\r\n'server', 'envoy'\r\n\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1633] writing 130 bytes, end_stream false\r\n[2020-08-14 10:35:56.121][14][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1633][S10994230258690132099] encoding data via codec (size=24 end_stream=true)\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1633] writing 24 bytes, end_stream false\r\n[2020-08-14 10:35:56.121][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=3)\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1633] readDisable: enabled=false disable=false\r\n[2020-08-14 10:35:56.121][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=3)\r\n2020-08-14T10:35:41.121Z [INFO] [ACCESS_LOG_INGRESS]:[Request_Id: a68e0545-1402-4aa9-b3c9-bd527a89b31d], Protocol: HTTP/1.1, Method : GET, Path: /pthru?d=20000, Response Code: 504, Host: test.host, Duration: 15000, Upstream Service Time: -, Bytes Received: 0, Bytes Sent: 24\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1633] socket event: 2\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1633] write ready\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1633] ssl write returns: 154\r\n[2020-08-14 10:35:56.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1958] new connection\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1958] socket event: 2\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1958] write ready\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1958] socket event: 3\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1958] write ready\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1958] read ready\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1958] read returns: 118\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1958] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1958] parsing 118 bytes\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1958] message begin\r\n[2020-08-14 10:35:56.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1958] new stream\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1958] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1958] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1958] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1958] headers complete\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1958] completed header: key=Connection value=close\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1958] message complete\r\n[2020-08-14 10:35:56.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1958][S9710212564483937592] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:56.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1958][S9710212564483937592] request end stream\r\n[2020-08-14 10:35:56.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1958][S9710212564483937592] request complete: path: /ready\r\n[2020-08-14 10:35:56.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1958][S9710212564483937592] closing connection due to connection close header\r\n[2020-08-14 10:35:56.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1958][S9710212564483937592] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:56 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1958] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1958][S9710212564483937592] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1958] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:56.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:56.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1958] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1958][S9710212564483937592] decode headers called: filter=0x564e4337eba0 status=1\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1958] parsed 118 bytes\r\n[2020-08-14 10:35:56.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1958] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:56.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:56.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1958] socket event: 2\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1958] write ready\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1958] write returns: 243\r\n[2020-08-14 10:35:56.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1958] write flush complete\r\n[2020-08-14 10:35:56.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1958] closing socket: 1\r\n[2020-08-14 10:35:56.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1958] adding to cleanup list\r\n[2020-08-14 10:35:56.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:56.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 7322\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7322 bytes into 1 slices\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7322 bytes\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:59.511][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4636 end_stream=false)\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:35:59.510821916 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3961\"\r\n\r\n[2020-08-14 10:35:59.511][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 232 bytes\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:59.511][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=340 end_stream=false)\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.RouteConfiguration at version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.RouteConfiguration: version_info: \"2020-08-14 10:35:59.510821916 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"route-t1-t1-vh1\"\r\nresource_names: \"ingress-http-route-config\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.RouteConfiguration\"\r\nresponse_nonce: \"3962\"\r\n\r\n[2020-08-14 10:35:59.511][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 247 bytes\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:59.511][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1879 end_stream=false)\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:35:59.511][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:35:59.511][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:35:59.510821916 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3963\"\r\n\r\n[2020-08-14 10:35:59.511][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 193 bytes\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:59.511][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=431 end_stream=false)\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:59.511][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:35:59.511][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:35:59.510821916 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3964\"\r\n\r\n[2020-08-14 10:35:59.511][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7322 bytes\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 873 bytes, end_stream false\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 873\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 43\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 43 bytes into 1 slices\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 43 bytes\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 43 bytes\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:35:59.528][6][trace][upstream] [source/common/upstream/strict_dns_cluster.cc:92] starting async DNS resolution for sapapim-rtcontroller.apim-lrt.svc.cluster.local\r\n[2020-08-14 10:35:59.528][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.529][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.529][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.529][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.530][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.530][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.530][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.530][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.531][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.532][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.532][6][trace][upstream] [source/common/upstream/strict_dns_cluster.cc:99] async DNS resolution complete for sapapim-rtcontroller.apim-lrt.svc.cluster.local\r\n[2020-08-14 10:35:59.532][6][debug][upstream] [source/common/upstream/upstream_impl.cc:250] transport socket match, socket default selected for host with address 100.68.89.88:443\r\n[2020-08-14 10:35:59.532][6][debug][upstream] [source/common/upstream/strict_dns_cluster.cc:156] DNS refresh rate reset for sapapim-rtcontroller.apim-lrt.svc.cluster.local, refresh rate 30000 ms\r\n[2020-08-14 10:35:59.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1959] new connection\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1959] socket event: 2\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1959] write ready\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1959] socket event: 3\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1959] write ready\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1959] read ready\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1959] read returns: 118\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1959] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1959] parsing 118 bytes\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1959] message begin\r\n[2020-08-14 10:35:59.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1959] new stream\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1959] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1959] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1959] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1959] headers complete\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1959] completed header: key=Connection value=close\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1959] message complete\r\n[2020-08-14 10:35:59.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1959][S9808164756817688405] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:59.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1959][S9808164756817688405] request end stream\r\n[2020-08-14 10:35:59.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1959][S9808164756817688405] request complete: path: /ready\r\n[2020-08-14 10:35:59.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1959][S9808164756817688405] closing connection due to connection close header\r\n[2020-08-14 10:35:59.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1959][S9808164756817688405] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:59 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1959] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1959][S9808164756817688405] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1959] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:59.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:59.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1959] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1959][S9808164756817688405] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1959] parsed 118 bytes\r\n[2020-08-14 10:35:59.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1959] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:59.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:59.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1959] socket event: 2\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1959] write ready\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1959] write returns: 243\r\n[2020-08-14 10:35:59.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1959] write flush complete\r\n[2020-08-14 10:35:59.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1959] closing socket: 1\r\n[2020-08-14 10:35:59.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1959] adding to cleanup list\r\n[2020-08-14 10:35:59.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:59.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:00.041][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:36:02.308][6][trace][upstream] [source/common/upstream/strict_dns_cluster.cc:92] starting async DNS resolution for policyengine-t1.apim-lrt.svc.cluster.local\r\n[2020-08-14 10:36:02.308][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.308][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.309][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.309][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.310][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.310][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.310][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.310][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.311][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.311][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.312][6][trace][upstream] [source/common/upstream/strict_dns_cluster.cc:99] async DNS resolution complete for policyengine-t1.apim-lrt.svc.cluster.local\r\n[2020-08-14 10:36:02.312][6][debug][upstream] [source/common/upstream/upstream_impl.cc:250] transport socket match, socket default selected for host with address 100.69.188.67:8443\r\n[2020-08-14 10:36:02.312][6][debug][upstream] [source/common/upstream/strict_dns_cluster.cc:156] DNS refresh rate reset for policyengine-t1.apim-lrt.svc.cluster.local, refresh rate 30000 ms\r\n[2020-08-14 10:36:02.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1960] new connection\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1960] socket event: 2\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1960] write ready\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1960] socket event: 3\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1960] write ready\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1960] read ready\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1960] read returns: 118\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1960] read error: Resource temporarily unavailable\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1960] parsing 118 bytes\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1960] message begin\r\n[2020-08-14 10:36:02.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1960] new stream\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1960] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1960] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1960] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1960] headers complete\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1960] completed header: key=Connection value=close\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1960] message complete\r\n[2020-08-14 10:36:02.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1960][S5133849145229437806] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:02.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1960][S5133849145229437806] request end stream\r\n[2020-08-14 10:36:02.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1960][S5133849145229437806] request complete: path: /ready\r\n[2020-08-14 10:36:02.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1960][S5133849145229437806] closing connection due to connection close header\r\n[2020-08-14 10:36:02.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1960][S5133849145229437806] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:36:02 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1960] writing 228 bytes, end_stream false\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1960][S5133849145229437806] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1960] writing 15 bytes, end_stream false\r\n[2020-08-14 10:36:02.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:02.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1960] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1960][S5133849145229437806] decode headers called: filter=0x564e4337e6c0 status=1\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1960] parsed 118 bytes\r\n[2020-08-14 10:36:02.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1960] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:02.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:36:02.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1960] socket event: 2\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1960] write ready\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1960] write returns: 243\r\n[2020-08-14 10:36:02.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1960] write flush complete\r\n[2020-08-14 10:36:02.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1960] closing socket: 1\r\n[2020-08-14 10:36:02.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1960] adding to cleanup list\r\n[2020-08-14 10:36:02.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:02.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1341] socket event: 3\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1341] write ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1341] read ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1341] ssl read returns: 244\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1341] ssl read returns: -1\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1341] ssl read 244 bytes into 1 slices\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1341] parsing 244 bytes\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1341] message begin\r\n[2020-08-14 10:36:02.954][14][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1341] new stream\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Host value=test.host\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=User-Agent value=PostmanRuntime/7.22.0\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Accept value=*/*\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Cache-Control value=no-cache\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Postman-Token value=3e3bae22-3458-489f-b8df-c909387119d7\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Accept-Encoding value=gzip, deflate, br\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1341] headers complete\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Connection value=keep-alive\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1341] message complete\r\n[2020-08-14 10:36:02.954][14][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1341][S5885534533524513809] request headers complete (end_stream=true):\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n'user-agent', 'PostmanRuntime/7.22.0'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '3e3bae22-3458-489f-b8df-c909387119d7'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'connection', 'keep-alive'\r\n\r\n[2020-08-14 10:36:02.954][14][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1341][S5885534533524513809] request end stream\r\n[2020-08-14 10:36:02.954][14][debug][router] [source/common/router/router.cc:434] [C1341][S5885534533524513809] cluster 'policyengine-t1' match for URL '/pthru?d=20000'\r\n[2020-08-14 10:36:02.954][14][debug][router] [source/common/router/router.cc:549] [C1341][S5885534533524513809] router decoding headers:\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n':scheme', 'https'\r\n'user-agent', 'PostmanRuntime/7.22.0'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '3e3bae22-3458-489f-b8df-c909387119d7'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'x-forwarded-proto', 'https'\r\n'x-envoy-downstream-service-cluster', 'cluster1'\r\n'x-envoy-downstream-service-node', 'ingressnode'\r\n'x-request-id', 'f5f4b35c-0e90-4197-b2c3-6186e4bb0fe5'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2020-08-14 10:36:02.954][14][debug][pool] [source/common/http/http1/conn_pool.cc:95] creating a new connection\r\n[2020-08-14 10:36:02.954][14][debug][client] [source/common/http/codec_client.cc:31] [C1961] connecting\r\n[2020-08-14 10:36:02.954][14][debug][connection] [source/common/network/connection_impl.cc:711] [C1961] connecting to 100.69.188.67:8443\r\n[2020-08-14 10:36:02.954][14][debug][connection] [source/common/network/connection_impl.cc:720] [C1961] connection in progress\r\n[2020-08-14 10:36:02.954][14][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1341][S5885534533524513809] decode headers called: filter=0x564e4337e120 status=1\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1341] parsed 244 bytes\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1341] readDisable: enabled=true disable=true\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1341] socket event: 2\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1341] write ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1961] socket event: 2\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1961] write ready\r\n[2020-08-14 10:36:02.954][14][debug][connection] [source/common/network/connection_impl.cc:559] [C1961] connected\r\n[2020-08-14 10:36:02.954][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C1961] handshake expecting read\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1961] socket event: 3\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1961] write ready\r\n[2020-08-14 10:36:02.954][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:176] [C1961] handshake complete\r\n[2020-08-14 10:36:02.954][14][debug][client] [source/common/http/codec_client.cc:69] [C1961] connected\r\n[2020-08-14 10:36:02.954][14][debug][pool] [source/common/http/http1/conn_pool.cc:249] [C1961] attaching to next request\r\n[2020-08-14 10:36:02.954][14][debug][router] [source/common/router/router.cc:1618] [C1341][S5885534533524513809] pool ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1961] writing 448 bytes, end_stream false\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1961] write ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1961] ssl write returns: 448\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1961] read ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1961] ssl read returns: -1\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1961] ssl read 0 bytes into 0 slices\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1961] socket event: 2\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1961] write ready\r\n[2020-08-14 10:36:04.326][13][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:04.326][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: 0\r\n[2020-08-14 10:36:04.326][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:04.326][13][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.326][13][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.326][13][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:04.326][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:36:04.326][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:04.326][13][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.326][13][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.449][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1962] new connection\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1962] socket event: 2\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1962] write ready\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1962] socket event: 3\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1962] write ready\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1962] read ready\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1962] read returns: 118\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1962] read error: Resource temporarily unavailable\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1962] parsing 118 bytes\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1962] message begin\r\n[2020-08-14 10:36:04.449][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1962] new stream\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1962] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1962] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1962] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1962] headers complete\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1962] completed header: key=Connection value=close\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1962] message complete\r\n[2020-08-14 10:36:04.449][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1962][S2492011494506520494] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:04.449][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1962][S2492011494506520494] request end stream\r\n[2020-08-14 10:36:04.449][6][debug][admin] [source/server/http/admin.cc:1200] [C1962][S2492011494506520494] request complete: path: /ready\r\n[2020-08-14 10:36:04.449][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1962][S2492011494506520494] closing connection due to connection close header\r\n[2020-08-14 10:36:04.449][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1962][S2492011494506520494] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:36:04 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1962] writing 228 bytes, end_stream false\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1962][S2492011494506520494] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1962] writing 15 bytes, end_stream false\r\n[2020-08-14 10:36:04.449][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.449][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1962] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1962][S2492011494506520494] decode headers called: filter=0x564e4337e6c0 status=1\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1962] parsed 118 bytes\r\n[2020-08-14 10:36:04.449][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1962] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:04.449][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:36:04.449Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1962] socket event: 2\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1962] write ready\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1962] write returns: 243\r\n[2020-08-14 10:36:04.449][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1962] write flush complete\r\n[2020-08-14 10:36:04.449][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1962] closing socket: 1\r\n[2020-08-14 10:36:04.449][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1962] adding to cleanup list\r\n[2020-08-14 10:36:04.449][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.449][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 7322\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7322 bytes into 1 slices\r\n[2020-08-14 10:36:04.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7322 bytes\r\n[2020-08-14 10:36:04.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:04.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:04.515][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4636 end_stream=false)\r\n[2020-08-14 10:36:04.515][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.515][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.515][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.515][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:36:04.515233532 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3965\"\r\n\r\n[2020-08-14 10:36:04.515][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 232 bytes\r\n[2020-08-14 10:36:04.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:04.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:04.515][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=431 end_stream=false)\r\n[2020-08-14 10:36:04.515][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:04.516][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:36:04.516][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:36:04.515233532 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3966\"\r\n\r\n[2020-08-14 10:36:04.516][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:04.516][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=340 end_stream=false)\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.RouteConfiguration at version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.RouteConfiguration: version_info: \"2020-08-14 10:36:04.515233532 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"route-t1-t1-vh1\"\r\nresource_names: \"ingress-http-route-config\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.RouteConfiguration\"\r\nresponse_nonce: \"3967\"\r\n\r\n[2020-08-14 10:36:04.516][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 247 bytes\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:04.516][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1879 end_stream=false)\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:36:04.516][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:36:04.516][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:36:04.515233532 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3968\"\r\n\r\n[2020-08-14 10:36:04.516][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 193 bytes\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7322 bytes\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 873 bytes, end_stream false\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 873\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 30\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 30 bytes into 1 slices\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 30 bytes\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 30 bytes\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:36:04.636][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:04.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:36:04.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:04.636][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.636][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.636][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:04.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:36:04.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:04.636][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.636][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:05.041][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1955] socket event: 3\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1955] read ready\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1955] ssl read returns: 144\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1955] ssl read returns: -1\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1955] ssl read 144 bytes into 1 slices\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1955] parsing 144 bytes\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1955] message begin\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1955] completed header: key=Content-Length value=14\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1955] completed header: key=Content-Type value=text/plain\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1955] completed header: key=Date value=Fri, 14 Aug 2020 10:36:05 GMT\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1955] headers complete\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1955] completed header: key=Server value=envoy\r\n[2020-08-14 10:36:05.206][14][debug][router] [source/common/router/router.cc:1036] [C1916][S14993064456358697936] upstream headers complete: end_stream=false\r\n[2020-08-14 10:36:05.206][14][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1916][S14993064456358697936] encoding headers via codec (end_stream=false):\r\n':status', '408'\r\n'content-length', '14'\r\n'content-type', 'text/plain'\r\n'date', 'Fri, 14 Aug 2020 10:36:05 GMT'\r\n'server', 'envoy'\r\n'x-envoy-upstream-service-time', '15001'\r\n\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1916] writing 168 bytes, end_stream false\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1916][S14993064456358697936] encoding data via codec (size=14 end_stream=false)\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1916] writing 14 bytes, end_stream false\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1955] message complete\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:827] [C1955] message complete\r\n[2020-08-14 10:36:05.206][14][debug][client] [source/common/http/codec_client.cc:101] [C1955] response complete\r\n[2020-08-14 10:36:05.206][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1916][S14993064456358697936] encoding data via codec (size=0 end_stream=true)\r\n[2020-08-14 10:36:05.206][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=2)\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1916] readDisable: enabled=false disable=false\r\n[2020-08-14 10:36:05.206][14][debug][pool] [source/common/http/http1/conn_pool.cc:206] [C1955] response complete\r\n[2020-08-14 10:36:05.206][14][debug][pool] [source/common/http/http1/conn_pool.cc:244] [C1955] moving to ready\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1955] parsed 144 bytes\r\n[2020-08-14 10:36:05.206][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=2)\r\n2020-08-14T10:35:50.204Z [INFO] [ACCESS_LOG_INGRESS]:[Request_Id: 7c1115df-3b51-458c-ab2e-ce744b40c6b0], Protocol: HTTP/1.1, Method : GET, Path: /pthru?d=20000, Response Code: 408, Host: test.host, Duration: 15001, Upstream Service Time: 15001, Bytes Received: 0, Bytes Sent: 14\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 2\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1916] ssl write returns: 182\r\n[2020-08-14 10:36:05.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1963] new connection\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1963] socket event: 2\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1963] write ready\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1963] socket event: 3\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1963] write ready\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1963] read ready\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1963] read returns: 118\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1963] read error: Resource temporarily unavailable\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1963] parsing 118 bytes\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1963] message begin\r\n[2020-08-14 10:36:05.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1963] new stream\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1963] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1963] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1963] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1963] headers complete\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1963] completed header: key=Connection value=close\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1963] message complete\r\n[2020-08-14 10:36:05.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1963][S17788979837976716579] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:05.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1963][S17788979837976716579] request end stream\r\n[2020-08-14 10:36:05.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1963][S17788979837976716579] request complete: path: /ready\r\n[2020-08-14 10:36:05.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1963][S17788979837976716579] closing connection due to connection close header\r\n[2020-08-14 10:36:05.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1963][S17788979837976716579] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:36:05 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1963] writing 228 bytes, end_stream false\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1963][S17788979837976716579] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1963] writing 15 bytes, end_stream false\r\n[2020-08-14 10:36:05.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:05.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1963] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1963][S17788979837976716579] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1963] parsed 118 bytes\r\n[2020-08-14 10:36:05.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1963] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:05.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:36:05.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1963] socket event: 2\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1963] write ready\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1963] write returns: 243\r\n[2020-08-14 10:36:05.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1963] write flush complete\r\n[2020-08-14 10:36:05.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1963] closing socket: 1\r\n[2020-08-14 10:36:05.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1963] adding to cleanup list\r\n[2020-08-14 10:36:05.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:05.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 3\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1916] read ready\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1916] ssl read returns: 244\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1916] ssl read returns: -1\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1916] ssl read 244 bytes into 1 slices\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1916] parsing 244 bytes\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1916] message begin\r\n[2020-08-14 10:36:05.726][14][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1916] new stream\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Host value=test.host\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=User-Agent value=PostmanRuntime/7.26.3\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Accept value=*/*\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Cache-Control value=no-cache\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Postman-Token value=353fdf1b-00a7-48f1-a93d-bb552c479330\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Accept-Encoding value=gzip, deflate, br\r\n[2020-08-14 10:36:05.727][14][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1916] headers complete\r\n[2020-08-14 10:36:05.727][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Connection value=keep-alive\r\n[2020-08-14 10:36:05.727][14][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1916] message complete\r\n[2020-08-14 10:36:05.727][14][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1916][S332189432177281115] request headers complete (end_stream=true):\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n'user-agent', 'PostmanRuntime/7.26.3'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '353fdf1b-00a7-48f1-a93d-bb552c479330'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'connection', 'keep-alive'\r\n\r\n[2020-08-14 10:36:05.727][14][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1916][S332189432177281115] request end stream\r\n[2020-08-14 10:36:05.727][14][debug][router] [source/common/router/router.cc:434] [C1916][S332189432177281115] cluster 'policyengine-t1' match for URL '/pthru?d=20000'\r\n[2020-08-14 10:36:05.727][14][debug][router] [source/common/router/router.cc:549] [C1916][S332189432177281115] router decoding headers:\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n':scheme', 'https'\r\n'user-agent', 'PostmanRuntime/7.26.3'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '353fdf1b-00a7-48f1-a93d-bb552c479330'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'x-forwarded-proto', 'https'\r\n'x-envoy-downstream-service-cluster', 'cluster1'\r\n'x-envoy-downstream-service-node', 'ingressnode'\r\n'x-request-id', '247beb82-5d01-43da-8e7a-c49d6901d664'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2020-08-14 10:36:05.727][14][debug][pool] [source/common/http/http1/conn_pool.cc:104] [C1955] using existing connection\r\n[2020-08-14 10:36:05.727][14][debug][router] [source/common/router/router.cc:1618] [C1916][S332189432177281115] pool ready\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1955] writing 448 bytes, end_stream false\r\n[2020-08-14 10:36:05.727][14][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1916][S332189432177281115] decode headers called: filter=0x564e4337e060 status=1\r\n[2020-08-14 10:36:05.727][14][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1916] parsed 244 bytes\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1916] readDisable: enabled=true disable=true\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1955] socket event: 2\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1955] ssl write returns: 448\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 2\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:36:08.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1964] new connection\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1964] socket event: 2\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1964] write ready\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1964] socket event: 3\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1964] write ready\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1964] read ready\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1964] read returns: 118\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1964] read error: Resource temporarily unavailable\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1964] parsing 118 bytes\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1964] message begin\r\n[2020-08-14 10:36:08.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1964] new stream\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1964] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1964] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1964] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1964] headers complete\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1964] completed header: key=Connection value=close\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1964] message complete\r\n[2020-08-14 10:36:08.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1964][S6323848685696604929] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:08.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1964][S6323848685696604929] request end stream\r\n[2020-08-14 10:36:08.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1964][S6323848685696604929] request complete: path: /ready\r\n[2020-08-14 10:36:08.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1964][S6323848685696604929] closing connection due to connection close header\r\n[2020-08-14 10:36:08.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1964][S6323848685696604929] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:36:08 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1964] writing 228 bytes, end_stream false\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1964][S6323848685696604929] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1964] writing 15 bytes, end_stream false\r\n[2020-08-14 10:36:08.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:08.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1964] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1964][S6323848685696604929] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1964] parsed 118 bytes\r\n[2020-08-14 10:36:08.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1964] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:08.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:36:08.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1964] socket event: 2\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1964] write ready\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1964] write returns: 243\r\n[2020-08-14 10:36:08.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1964] write flush complete\r\n[2020-08-14 10:36:08.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1964] closing socket: 1\r\n[2020-08-14 10:36:08.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1964] adding to cleanup list\r\n[2020-08-14 10:36:08.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:08.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 7322\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7322 bytes into 1 slices\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7322 bytes\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:09.513][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4636 end_stream=false)\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:36:09.512752108 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3969\"\r\n\r\n[2020-08-14 10:36:09.513][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 232 bytes\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:09.513][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1879 end_stream=false)\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:36:09.513][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:36:09.513][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:36:09.512752108 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3970\"\r\n\r\n[2020-08-14 10:36:09.513][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 193 bytes\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:09.513][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=431 end_stream=false)\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:09.513][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:36:09.513][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:36:09.512752108 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3971\"\r\n\r\n[2020-08-14 10:36:09.513][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:09.513][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=340 end_stream=false)\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.RouteConfiguration at version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.514][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.RouteConfiguration: version_info: \"2020-08-14 10:36:09.512752108 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"route-t1-t1-vh1\"\r\nresource_names: \"ingress-http-route-config\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.RouteConfiguration\"\r\nresponse_nonce: \"3972\"\r\n\r\n[2020-08-14 10:36:09.514][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 247 bytes\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7322 bytes\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 873 bytes, end_stream false\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 873\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 30\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 30 bytes into 1 slices\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 30 bytes\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 30 bytes\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:36:10.039][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:36:11.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1965] new connection\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1965] socket event: 2\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1965] write ready\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1965] socket event: 3\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1965] write ready\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1965] read ready\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1965] read returns: 118\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1965] read error: Resource temporarily unavailable\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1965] parsing 118 bytes\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1965] message begin\r\n[2020-08-14 10:36:11.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1965] new stream\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1965] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1965] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1965] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1965] headers complete\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1965] completed header: key=Connection value=close\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1965] message complete\r\n[2020-08-14 10:36:11.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1965][S5175704883643059425] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:11.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1965][S5175704883643059425] request end stream\r\n[2020-08-14 10:36:11.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1965][S5175704883643059425] request complete: path: /ready\r\n[2020-08-14 10:36:11.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1965][S5175704883643059425] closing connection due to connection close header\r\n[2020-08-14 10:36:11.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1965][S5175704883643059425] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:36:11 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1965] writing 228 bytes, end_stream false\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1965][S5175704883643059425] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1965] writing 15 bytes, end_stream false\r\n[2020-08-14 10:36:11.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:11.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1965] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1965][S5175704883643059425] decode headers called: filter=0x564e4337f080 status=1\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1965] parsed 118 bytes\r\n[2020-08-14 10:36:11.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1965] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:11.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:36:11.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1965] socket event: 2\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1965] write ready\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1965] write returns: 243\r\n[2020-08-14 10:36:11.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1965] write flush complete\r\n[2020-08-14 10:36:11.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1965] closing socket: 1\r\n[2020-08-14 10:36:11.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1965] adding to cleanup list\r\n[2020-08-14 10:36:11.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:11.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:14.326][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:14.326][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:36:14.327][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:14.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:36:14.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:14.327][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:14.327][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:14.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:14.327][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:14.327][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 7322\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7322 bytes into 1 slices\r\n[2020-08-14 10:36:14.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7322 bytes\r\n[2020-08-14 10:36:14.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:14.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:14.516][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=431 end_stream=false)\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:14.516][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:36:14.516][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.516][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:36:14.515810246 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3973\"\r\n\r\n[2020-08-14 10:36:14.516][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:36:14.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:14.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:14.516][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1879 end_stream=false)\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:36:14.516][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:36:14.517][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.517][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:36:14.515810246 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3974\"\r\n\r\n[2020-08-14 10:36:14.517][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 193 bytes\r\n[2020-08-14 10:36:14.517][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:14.517][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:14.517][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4636 end_stream=false)\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.517][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:36:14.515810246 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3975\"\r\n\r\n</details>",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12665/comments",
    "author": "ankur-anand",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-08-16T21:07:08Z",
        "body": "There have been various changes in this area over the months. Please try with a more recent build and report back. In general though a 408 can happen if idle timeouts, buffer timeouts, etc. are hit."
      },
      {
        "user": "ankur-anand",
        "created_at": "2020-09-01T08:15:51Z",
        "body": "@mattklein123 Thank you for the help. Didn't received such inconsistency in the newer version. "
      }
    ]
  },
  {
    "number": 12632,
    "title": "StopIterationAndWatermark not stopping message flow",
    "created_at": "2020-08-13T16:10:27Z",
    "closed_at": "2021-01-15T04:25:43Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12632",
    "body": "Using envoy with pubsub streamingPullResponse. When StopIterationAndWatermark is returned, messages continue even though continueencoding is never called and the encodeData function also returns StopIterationAndWatermark",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12632/comments",
    "author": "aydanjiwani",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-09-20T17:39:16Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-08T04:22:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-15T04:25:42Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 12543,
    "title": "Envoy does external DNS request for hostname in /etc/hosts",
    "created_at": "2020-08-07T16:38:11Z",
    "closed_at": "2020-09-20T23:39:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12543",
    "body": "I set my domain name in `/etc/hosts`, but envoy still does external DNS query for that domain:\r\n\r\n```\r\n{\"level\":\"debug\",\"service\":\"envoy\",\"name\":\"upstream\",\"time\":\"2020-08-03T09:58:25Z\",\"message\":\"DNS request timed out 4 times\"}\r\n{\"level\":\"debug\",\"service\":\"envoy\",\"name\":\"upstream\",\"time\":\"2020-08-03T09:58:25Z\",\"message\":\"transport socket match, socket default selected for host with address 127.0.0.100:80\"}\r\n{\"level\":\"debug\",\"service\":\"envoy\",\"name\":\"upstream\",\"time\":\"2020-08-03T09:58:25Z\",\"message\":\"DNS refresh rate reset for does-not-exist.com, refresh rate 5000 ms\"}\r\n```\r\n\r\nI expect envoy to skip the DNS query and always use the IP set in `/etc/hosts`. I can't find anywhere in the doc mention about envoy behavior with `/etc/hosts`.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12543/comments",
    "author": "cuonglm",
    "comments": [
      {
        "user": "rshriram",
        "created_at": "2020-08-12T15:52:16Z",
        "body": "I think this is because of c-ares (the DNS library that envoy uses). You might want to look at the c-ares docs to see why it doesn't pick up the /etc/hosts.. They have some env vars to control the behavior"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-11T16:41:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-20T23:39:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "gunsluo",
        "created_at": "2020-12-30T04:12:45Z",
        "body": "I have the same issue.  I use docker/docker-compose to run envoy and get the same error message.\r\n\r\nDocker image: envoyproxy/envoy-dev:7e3b483bbb004bf2b8234e3ec2be6ceef254a2f9\r\nReference To: \r\nConfiguration: Dynamic from filesystem\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 12499,
    "title": "How to avoid communication between threads?",
    "created_at": "2020-08-05T18:40:04Z",
    "closed_at": "2020-08-19T22:35:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12499",
    "body": "We have a requirement where the server will tell our Envoy network filter which source IP and port to use for the outgoing egress TCP connection. \r\n\r\nThe obvious issue that we need to solve is potential address already in use errors (i.e. when a message is received on one thread, and there is already a connection established on another thread with the same IP and port that we are instructed to use).\r\n\r\nWe are thinking of using a map that is shared across all threads, where the key would be the destination address + source address. Each thread would consult this map to see whether an entry exists for a given destination address + source address combination before deciding whether to create a new connection. \r\n\r\nThis approach would involve using locks and would also deviate from Envoy's threading model, where threads don't really communicate with each other. Is there another way that this can be solved? Picking the thread based on which listener accepted the connection perhaps?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12499/comments",
    "author": "kishan-patel",
    "comments": [
      {
        "user": "antoniovicente",
        "created_at": "2020-08-07T21:36:29Z",
        "body": "What is the intended behavior when the requested source IP and port is already in use?\r\n\r\nYou may need to set some socket options to allow multiple connections to bind to the same local ip:port, and fail on connect if a connection with the same src and dest ip:port already exists.\r\n\r\nIt sounds like you need a locked data structure.  If you run into contention problems, I think the most you can do in this case is shard the map by source ip:port to reduce lock contention."
      },
      {
        "user": "kishan-patel",
        "created_at": "2020-08-07T22:05:19Z",
        "body": "Thank you for taking the time to reply.\r\n\r\n> What is the intended behavior when the requested source IP and port is already in use?\r\n\r\nWe are expected to send the message over the already established connection (even though it may have been created on a different thread).\r\n\r\n> It sounds like you need a locked data structure. If you run into contention problems, I think the most you can do in this case is shard the map by source ip:port to reduce lock contention.\r\n\r\nThis is the way that it has been implemented so far in the prototype that we are developing. However, we weren't sure whether it was the best approach due to all of the locking and communication between filters on different threads. Do you see a major problem with this solution or have an alternative suggestion in mind (broadcasting the message in all of the thread's TLS slot perhaps)?"
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-08-07T22:14:02Z",
        "body": "> Thank you for taking the time to reply.\r\n> \r\n> > What is the intended behavior when the requested source IP and port is already in use?\r\n> \r\n> We are expected to send the message over the already established connection (even though it may have been created on a different thread).\r\n\r\nIIRC Buffer::Instance and Network::Connection objects are not thread safe.  I expect you will run into some problems unless you do sends by posting to the thread that owns the relevant connection and verify that the connection still exists in that thread prior to writing to it's output buffer.\r\n\r\n> > It sounds like you need a locked data structure. If you run into contention problems, I think the most you can do in this case is shard the map by source ip:port to reduce lock contention.\r\n> \r\n> This is the way that it has been implemented so far in the prototype that we are developing. However, we weren't sure whether it was the best approach due to all of the locking and communication between filters on different threads. Do you see a major problem with this solution or have an alternative suggestion in mind (broadcasting the message in all of the thread's TLS slot perhaps)?\r\n\r\n"
      },
      {
        "user": "kishan-patel",
        "created_at": "2020-08-16T21:34:34Z",
        "body": "How would I go about posting from one worker thread to another? I was looking at the `runOnAllThreads(Event::PostCb cb)` method inside the `thread_local.h` interface, but it seems like you would only be able to invoke this method from the main thread (i.e. there's this assertion at the start of the method: `ASSERT(std::this_thread::get_id() == main_thread_id_)`).\r\n\r\nEdit: I was able to broadcast incoming messages to all of the threads by using the main thread's dispatcher (a reference is being passed to each network filter that is created). We'll need to see how well it performs, but this solution is definitely simpler and more thread safe compared to the initial approach I was considering (i.e. shared map with locks). "
      }
    ]
  },
  {
    "number": 12457,
    "title": "Envoy filter to extract JWT fields",
    "created_at": "2020-08-04T07:32:01Z",
    "closed_at": "2020-09-13T22:05:41Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12457",
    "body": "Is there a Envoy filter that can extract information from JWT? Our JWT has some custom fields which we would like to extract and process the request only if the JWT has those fields and then pass along them for subsequent downstream requests.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12457/comments",
    "author": "gituserjava",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2020-08-04T14:49:02Z",
        "body": "I do not think a filter like this is part of Envoy. You can write your own either in C++, LUA or WASM (WASM is alpha at this point)."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-06T03:51:19Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-13T22:05:41Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 12436,
    "title": "Prometheus format for /cluster in administration interface",
    "created_at": "2020-08-03T18:22:54Z",
    "closed_at": "2020-09-13T05:42:37Z",
    "labels": [
      "question",
      "stale",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12436",
    "body": "Hi! We want to count requests to each upstreams in cluster. It's possible by /cluster in admin interface, but is it possible to make an interface for prometheus like for stats?format=prometheus (/stats/prometheus) ?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12436/comments",
    "author": "rumanzo",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-09-06T04:51:19Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-13T05:42:36Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "fengyinqiao",
        "created_at": "2020-09-24T03:38:19Z",
        "body": "I have met the problem as you too.So,did you resolve it or find any workaround?"
      },
      {
        "user": "rumanzo",
        "created_at": "2020-09-24T13:20:53Z",
        "body": "> I have met the problem as you too.So,did you resolve it or find any workaround?\r\n\r\nMy colleague wrote a wrapper for sending to graphite.\r\nsee #13249"
      }
    ]
  },
  {
    "number": 12348,
    "title": "Access log log_format.text_format field is not printing logs",
    "created_at": "2020-07-29T05:06:51Z",
    "closed_at": "2020-09-07T08:40:24Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12348",
    "body": "I am trying to enable v3 access logs for my application, where I am using text_format option. Logs are not getting printed if I use text_format, however if I use json_format, i can see the logs. Below is the snippet I used for enabling access logs - \r\n```yaml\r\nfilters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                '@type': >-\r\n                  type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: ingress_http\r\n                codec_type: AUTO\r\n                access_log:\r\n                  name: envoy.access_loggers.http_grpc\r\n                  typed_config:\r\n                    '@type': >-\r\n                      type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                    log_format:\r\n                      text_format: \"plain_text - %REQ(:path)% - %RESPONSE_CODE%\"\r\n                    path: \"/dev/stdout\"\r\n     ```\r\nPlease suggest how can i enable access logs using text option.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12348/comments",
    "author": "hinawatts",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-08-29T10:21:27Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-07T08:40:23Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "jleskovar-tyro",
        "created_at": "2020-09-16T10:30:35Z",
        "body": "Hi, did you find a solution to this issue @hinawatts ?"
      },
      {
        "user": "hinawatts",
        "created_at": "2020-09-20T13:51:17Z",
        "body": "Hi @jleskovar-tyro, no I didn't find a solution to this. I have switched to json_format type."
      },
      {
        "user": "dio",
        "created_at": "2020-09-20T22:51:22Z",
        "body": "@jleskovar-tyro @hinawatts \r\n\r\nExample of access log config for a `v3` `HttpConnectionManager` is as follows:\r\n\r\n```yaml\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          access_log:\r\n            name: \"envoy.access_loggers.http_file\"\r\n            typed_config:\r\n              \"@type\": \"type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\"\r\n              path: \"/dev/stdout\"\r\n              log_format:\r\n                text_format: \"path=%REQ(:path)%\\n\"\r\n```\r\n\r\nAnd for `v2`:\r\n\r\n```yaml\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n          access_log:\r\n            name: \"envoy.access_loggers.http_file\"\r\n            typed_config:\r\n              \"@type\": \"type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\"\r\n              path: \"/dev/stdout\"\r\n              format: \"path=%REQ(:path)%\\n\"\r\n```\r\n\r\nNote that logs printing is buffered, so there will be a delay (for envoy to print a corresponding log entry) if you test it with a single request."
      }
    ]
  },
  {
    "number": 12226,
    "title": "Can't build docker image because of SSL error. ",
    "created_at": "2020-07-22T16:56:02Z",
    "closed_at": "2020-08-30T01:21:30Z",
    "labels": [
      "question",
      "area/build",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12226",
    "body": "Hi. I have a CentOS VM and I try to build image.\r\nI follow steps in instruction and when i call `sudo ./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.dev'`\r\n\r\nI Get a Error:\r\n`INFO: Repository config_validation_pip3 instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule pip_import defined at:\r\n  /build/tmp/_bazel_bazel/b570b5ccd0454dc9af9f65ab1833764d/external/rules_python/python/pip.bzl:51:29: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'config_validation_pip3':\r\n   pip_import failed: Collecting PyYAML==5.3.1 (from -r /build/tmp/_bazel_bazel/b570b5ccd0454dc9af9f65ab1833764d/external/envoy/tools/config_validation/requirements.txt (line 1))\r\n...\r\n...\r\n    raise SSLError(e, request=request)\r\npip._vendor.requests.exceptions.SSLError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Max retries exceeded with url: /packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)'),))\r\n)\r\nINFO: Elapsed time: 12.638s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n`\r\n\r\nI understand, that error from pip3 and I need to add pipy hosts to trusted host. But I have no idea how to do that.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12226/comments",
    "author": "chonbash",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-08-22T21:26:19Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-30T01:21:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 12223,
    "title": "ip match problem",
    "created_at": "2020-07-22T08:08:34Z",
    "closed_at": "2020-09-01T13:27:41Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12223",
    "body": "I want envoy work as a tcp proxy and route packet to destinations if outgoing tcp packet's destination ip in ip_list.\r\nHow can I do that?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12223/comments",
    "author": "rtttech",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-08-23T21:12:26Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 12145,
    "title": "How to do when the downstream protocol is http and the upstream service is tcp",
    "created_at": "2020-07-17T05:44:39Z",
    "closed_at": "2020-09-02T19:05:42Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12145",
    "body": "Hello:\r\n\r\n  Our existing services are exposed externally using tcp.\r\n\r\n Now, we plan to migrate the system to k8s deployment,\r\n\r\n Considering that the tcp service is not convenient to expose outside the k8s cluster,\r\n\r\n So we need an http gateway to perform protocol conversion operations, and connected to the back-end tcp service\r\n\r\n  Please teachl me how to use envoy to done this work conveniently,  THS ~_~!\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12145/comments",
    "author": "zhixiongdu027",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-08-16T22:44:56Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-02T19:05:42Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11982,
    "title": "Change grpcStatus and grpcMessage in the response when encoding data in Envoy::Http::StreamEncoderFilter",
    "created_at": "2020-07-09T17:20:45Z",
    "closed_at": "2020-08-31T18:33:05Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11982",
    "body": "It is possible to send a response with the sendLocalReply method in StreamDecoderFilterCallbacks, but there is no equivalent method in the StreamEncoderFilterCallbacks. Attempted to call sendLocalReply  method of StreamDecoderFilterCallbacks inside encoderTrailers, but received runtime error - something like response header is not in the scope. Is there another way?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11982/comments",
    "author": "aydanjiwani",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-07-13T23:39:13Z",
        "body": "You can just change the headers directly?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-24T04:12:26Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-31T18:33:04Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11947,
    "title": "Meet invalid path error when running envoy proxy for istio ingress gateway ",
    "created_at": "2020-07-08T07:10:11Z",
    "closed_at": "2020-08-31T17:33:03Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11947",
    "body": "Issue Template\r\n\r\nTitle: One line description\r\n\r\nDescription:\r\nMeet invalid path error when running envoy proxy for istio ingress gateway \r\n\r\nDescribe the issue. Please be detailed. If a feature request, please describe the desired behaviour, what scenario it enables and how it would be used.\r\n`2020-07-08T06:49:12.134599Z\tinfo\tEnvoy command: [-c etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster istio-ingressgateway --service-node router~10.244.0.7~istio-ingressgateway-77bc9dbf7d-xwczl.istio-system~istio-system.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format %Y-%m-%dT%T.%fZ\t%l\tenvoy %n\t%v -l warning --component-log-level misc:error]`\r\n\r\n`2020-07-08T06:49:12.935714Z\tcritical\tenvoy main\t[external/envoy/source/server/server.cc:97] error initializing configuration 'etc/istio/proxy/envoy-rev0.json': Invalid path: ./etc/certs/root-cert.pem\r\nInvalid path: ./etc/certs/root-cert.pem`\r\n\r\nI have set the cert path to ./etc/certs to extract or store the certificate so in that case if the path is wrong. Is there a way that we can choose the correct path\r\n\r\nAny extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11947/comments",
    "author": "williamaronli",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-08-24T05:12:26Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-31T17:33:02Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11926,
    "title": "Can filter decodeHeaders be called multiple times? ",
    "created_at": "2020-07-07T16:52:52Z",
    "closed_at": "2020-08-31T10:33:06Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11926",
    "body": "We have code that inserts into FilterState in the decodeHeaders call, which crashes if decodeHeaders is called multiple times in the same test without resetting the filter state. Should we be handling that \"exists in filterstate\" situation in code? Or is decodeHeaders guaranteed to never be called twice for a given filterchain. It does have an `end_stream` variable similar to decodeData, but looking at the code it looks like that just defines whether to expect a body or not -- it seems decodeHeaders expects a complete header map.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11926/comments",
    "author": "auni53",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2020-07-09T12:49:10Z",
        "body": "The 'decodeHeaders' of a filter instance will  never be called twice. But for the robustness of the code, I still recommend that you deal with the 'exists in filterstate' situation."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-24T10:12:26Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-31T10:33:05Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11899,
    "title": "envoy reset downstream connections without sending any data to upstream",
    "created_at": "2020-07-06T13:47:03Z",
    "closed_at": "2020-09-02T12:05:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11899",
    "body": "Description:\r\n\r\n1. Downstream using java http client get some connection errors (connection reset by peer),this maybe envoy reset the connection\r\n1. Then tcpdump in envoy and downstream\r\n1. From the pcap, downstream connect to envoy's port 80 with a random port after shakehands, then downstream make a http post request,envoy side reply with ack. \r\n1. After 500ms, envoy reply to downstream with [RST,ACK]\r\n1. During the 500ms gap, did not find any data to upstream with the POST body.\r\n1. From downstream side, get the POST request to envoy side, ack from envoy side, finally get [RST,ACK] from envoy.\r\n\r\nEnviroment\r\n1. istio 1.3.6,envoy version is 1.12.0\r\n1. use envoy as a edge dateway\r\n1. open the envoy logging ,http=debug,connection=debug. did not find any error\r\n\r\nQuestion:\r\n1. Does anyone know about the problem?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11899/comments",
    "author": "andrewshaoyu",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2020-07-07T13:50:55Z",
        "body": "Were there any logs messages at all? It maybe that connection does not reach envoy at all, due to misconfiguration."
      },
      {
        "user": "andrewshaoyu",
        "created_at": "2020-07-10T07:36:32Z",
        "body": "what's logs could be useful? there too many logs.\r\n\r\nconnection must be connected, kernel didnot pass the data to envoy or there are some problems during the connection close()"
      },
      {
        "user": "yanavlasov",
        "created_at": "2020-07-14T14:06:11Z",
        "body": "Ok, let me ask this then. Does this problem happen to all client requests or only to some requests?"
      },
      {
        "user": "andrewshaoyu",
        "created_at": "2020-07-21T03:39:36Z",
        "body": "Finally，i find the problem,client idletimeout is greater than envoy idletimeout.\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-23T08:26:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-02T12:05:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11874,
    "title": "FractionalPercent when using with RouteMatch",
    "created_at": "2020-07-02T19:02:01Z",
    "closed_at": "2020-08-13T17:42:36Z",
    "labels": [
      "question",
      "stale",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11874",
    "body": "**_Title:_** Log debug/info/whatever message when `FractionalPercent.numerator` greater than `FractionalPercent.denominator`\r\n**_Description:_** When using `FractionalPercent` with `RouteMatch` it is possible to mess up with values on `denominator` / `numerator` so the underlying condition\r\n```cpp\r\n(random_value % denominator) < numerator\r\n```\r\nWill be always evaluated to `true` because of `numerator` exceeds `denominator`. It becomes hard to debug why configuration does not work (no logs, just shows that clusterN has been selected).\r\n\r\nDo you guys think it is make sense to add some kind of logging to simplify configuration debuggability?\r\nIf so I can add this.\r\n\r\nThanks ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11874/comments",
    "author": "belyalov",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2020-07-02T20:44:49Z",
        "body": "Alternatively we could add in configuration validation that will reject configuration where numerator > denominator, unless there are use cases where you'd want to do that?"
      },
      {
        "user": "belyalov",
        "created_at": "2020-07-03T20:49:08Z",
        "body": "This is not clear to me:\r\n- route configuration simply depends on runtime *key*, basically string, seems nothing to validate here\r\n- update runtime values through `rtds` or any other methods could not be validated as well (simply ignore unacceptable values?)"
      },
      {
        "user": "snowp",
        "created_at": "2020-07-06T18:57:38Z",
        "body": "I see, this is about using RuntimeFractionalPercent, not a FractionalPercent proto configured via xDS. \r\n\r\nSeems like logging the numerator/denominator as debug/trace would be the best way to handle this since there is no validation framework around runtime values."
      },
      {
        "user": "belyalov",
        "created_at": "2020-07-06T19:31:42Z",
        "body": "Ok, I'll take care of... :)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-08T09:29:46Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11616,
    "title": "how to enable envoy filter to ingress gateway, yet apply filter to a specific kubernetes pod/service/domainname",
    "created_at": "2020-06-17T13:42:41Z",
    "closed_at": "2020-06-17T22:56:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11616",
    "body": "@rshriram \r\nWould like to enable filter on ingressgateway instead of individualpod. However, is it possible that this filter is run/executed/applied to specific kubernetes service/pod?\r\n\r\nTried routeconfig with domain name , yet it applied to all service withall domain names. please help\r\n\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: hcm-tweaks\r\n  namespace: istio-system\r\nspec:\r\n  workloadSelector:\r\n    labels:\r\n      istio: ingress-gateway\r\n  configPatches:\r\n  - applyTo: HTTP_FILTER # http connection manager is a filter in Envoy\r\n    match:\r\n      context: GATEWAY\r\n      listener:\r\n        filterChainS:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n              stat_prefix: ingress_http\r\n              route_config:\r\n                name: local_route\r\n                virtual_hosts:\r\n                  - name: main\r\n                    **domains: [\"mydomain.com\"]**\r\n                    routes:\r\n                      - match:\r\n                          prefix: \"/\"\r\n              http_filters:\r\n                - name: envoy.ext_authz\r\n                  typed_config:\r\n                    \"@type\": type.googleapis.com/envoy.config.filter.http.ext_authz.v2.ExtAuthz\r\n                    grpc_service:\r\n                      envoy_grpc:\r\n                        google_grpc:\r\n                          target_uri: 10.103.86.44:8585\r\n                          stat_prefix: \"ext_authz\"\r\n\r\nI hope you understand the question.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11616/comments",
    "author": "r-kotagudem",
    "comments": [
      {
        "user": "rshriram",
        "created_at": "2020-06-17T17:50:48Z",
        "body": "please file this in the Istio repository. Envoyproxy/envoy repo is not the right place to ask istio configuration related questions."
      },
      {
        "user": "r-kotagudem",
        "created_at": "2020-06-17T22:56:05Z",
        "body": "Ok thanks"
      }
    ]
  },
  {
    "number": 11592,
    "title": "EnvoyFilter [ lua filter ] is not printing log on console",
    "created_at": "2020-06-15T14:06:47Z",
    "closed_at": "2020-07-25T19:19:21Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11592",
    "body": "This is not an issue but a question.\r\n\r\nI am new in this field. Trying to implement lua filter with OPA [ open policy agent and istio ]\r\nto implement a simple poc, i have created following following filter.\r\n\r\n```\r\n ############################################################\r\n# Envoy External Authorization filter that will query OPA.\r\n############################################################\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: ext-authz\r\n  namespace: istio-system\r\nspec:\r\n  filters:\r\n  - insertPosition:\r\n      index: FIRST\r\n    listenerMatch:\r\n      listenerType: GATEWAY\r\n      listenerProtocol: HTTP\r\n    filterType: HTTP\r\n    filterName: envoy.lua\r\n    filterConfig:\r\n      inlineCode: |\r\n        function envoy_on_request(request_handle)\r\n            request_handle:logWarn(\"envoy_on_request\")\r\n        end\r\n\r\n        function envoy_on_response(response_handle)\r\n            response_handle:logWarn(\"envoy_on_response\")\r\n            response_handle:headers():add(\"x-this\",\"It works\")\r\n        end\r\n---\r\n```\r\n\r\nWhen i am executing my api I am able to get the updated header value in output. [ seems lua filter is invoked in filter chain ]\r\n\r\nbut my problem is, i am not able to find the log “envoy_on_request” anywhere in the container logs.\r\n\r\nneed you suggestion to find the log…\r\n\r\nAm i missing something to enable log .. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11592/comments",
    "author": "ajoysinha",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2020-06-15T15:59:50Z",
        "body": "What log level do you have configured?"
      },
      {
        "user": "ajoysinha",
        "created_at": "2020-06-15T16:03:42Z",
        "body": "I haven't specifically configure any log level, [  I also don't have idea where to set :) ]  but did try to log with Error / Debug / Info all .. it didn't log for any one.  "
      },
      {
        "user": "htuch",
        "created_at": "2020-06-16T14:45:29Z",
        "body": "Do you have the same problem if you try this with standalone Envoy and console logs (which can be controlled with `-l <loglevel>`, rather than when integrated with Istio?"
      },
      {
        "user": "ajoysinha",
        "created_at": "2020-06-16T15:08:47Z",
        "body": "Thanks .. Fortunately I have found that only **request_handle:logCritical** is writing on log. not the other ones .. i am sure that this is standard logging configuration, but i don't have any clue how to do it.."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-18T06:56:05Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-25T19:19:20Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "bsarvan",
        "created_at": "2022-06-23T16:48:49Z",
        "body": "This could be due to logging level not set right for the proxy. For lua, it is required to enable debugging for Lua scripts in the IstioOperator custom resource. Below command using istioctl enables debug, info level logging for lua scripts\r\n\r\nistioctl proxy-config log <pod>.<namespace> --level lua:info\r\n\r\nOptions for --level are lua:debug, lua:info, lua:warning. Hope this helps"
      },
      {
        "user": "xixiangzouyibian",
        "created_at": "2023-01-12T12:50:08Z",
        "body": "> This could be due to logging level not set right for the proxy. For lua, it is required to enable debugging for Lua scripts in the IstioOperator custom resource. Below command using istioctl enables debug, info level logging for lua scripts\r\n> \r\n> istioctl proxy-config log . --level lua:info\r\n> \r\n> Options for --level are lua:debug, lua:info, lua:warning. Hope this helps\r\n\r\nThis really save my life !"
      }
    ]
  },
  {
    "number": 11572,
    "title": "DownstreamConnectionTermination (DC) response flag not set for gRPC",
    "created_at": "2020-06-12T16:33:05Z",
    "closed_at": "2020-07-25T18:19:21Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11572",
    "body": "The DC response flag signifies that the downstream disconnected or cancelled the connection or request and is very useful for understanding what happened to certain requests via access logs.\r\n\r\nIt's currently set with this logic:\r\n```\r\n  // A downstream disconnect can be identified for HTTP requests when the upstream returns with a 0\r\n  // response code and when no other response flags are set.\r\n  if (!stream_info_.hasAnyResponseFlag() && !stream_info_.responseCode()) {\r\n    stream_info_.setResponseFlag(StreamInfo::ResponseFlag::DownstreamConnectionTermination);\r\n  }\r\n```\r\n\r\nThe problem for gRPC is that we may have seen the 200 HTTP response code already, but a cancellation can happen later while receiving the body/trailers.\r\n\r\nProposal: update that logic to add the flag for gRPC requests unless a gRPC status was seen.\r\n\r\nGetting into the details, would it make sense to add a isGrpc() and grpcCode() to StreamInfo? ActiveStream would also need to look for grpc-status in the trailers to set it into the stream info.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11572/comments",
    "author": "mpuncel",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-06-15T21:55:05Z",
        "body": "> The problem for gRPC is that we may have seen the 200 HTTP response code already, but a cancellation can happen later while receiving the body/trailers.\r\n\r\nI think this is the same problem for HTTP though, right? So it seems like potentially this is a larger issue in which downstream disconnect needs to be tracked in a more explicit way?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-18T08:56:05Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-25T18:19:20Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11434,
    "title": "Feature Request: Disable Nagle's algorithm",
    "created_at": "2020-06-03T22:37:42Z",
    "closed_at": "2020-07-12T05:55:29Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11434",
    "body": "It seems like there is currently no way to disable Nagle's algorithm which is relevant for applications that require low latency. To my knowledge, this algorithm is by default enabled for TCP connections.\r\n\r\nNginx also has the feature (tcp_nodelay)\r\n\r\nAre there any plans to implement this?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11434/comments",
    "author": "OneFirefly",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-06-03T22:53:08Z",
        "body": "TCP_NODELAY is set on all proxy connections by default."
      },
      {
        "user": "OneFirefly",
        "created_at": "2020-06-04T01:38:21Z",
        "body": "Thank you\r\nIs there a reason why it's set by default? Isn't Nagle's algorithm there to improve performance?\r\nIs there a way to re-enable it if wished? I didn't find it in the docs."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-04T05:17:17Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-12T05:55:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11418,
    "title": "upstream connect error when cluster address is localhost",
    "created_at": "2020-06-03T10:40:28Z",
    "closed_at": "2020-06-04T06:55:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11418",
    "body": "Hi! I started microservice without docker and i want get this from envoy. Envoy started with docker. And this is not working. Microservice without envoy is available and works.\r\n\r\nMy config:\r\n```admin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address: { address: 0.0.0.0, port_value: 80 }\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              config:\r\n                stat_prefix: ingress_http\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match: { prefix: \"/\" }\r\n                          route: { cluster: my_service }\r\n                http_filters:\r\n                  - name: envoy.router\r\n  clusters:\r\n    - name: my_service\r\n      connect_timeout: 0.25s\r\n      type: LOGICAL_DNS\r\n      lb_policy: ROUND_ROBIN\r\n      hosts: [{ socket_address: { address: localhost, port_value: 8102 }}]\r\n```\r\n\r\nEnvoy logs:\r\n```[2020-06-03 10:38:35.828][17][debug][conn_handler] [source/server/connection_handler_impl.cc:372] [C0] new connection\r\n[2020-06-03 10:38:35.828][17][debug][http] [source/common/http/conn_manager_impl.cc:268] [C0] new stream\r\n[2020-06-03 10:38:35.828][17][debug][http] [source/common/http/conn_manager_impl.cc:781] [C0][S3580059227218846830] request headers complete (end_stream=true):\r\n':authority', 'localhost:8000'\r\n':path', '/app/ping'\r\n':method', 'GET'\r\n'user-agent', 'curl/7.68.0'\r\n'accept', '*/*'\r\n\r\n[2020-06-03 10:38:35.828][17][debug][http] [source/common/http/conn_manager_impl.cc:1333] [C0][S3580059227218846830] request end stream\r\n[2020-06-03 10:38:35.828][17][debug][router] [source/common/router/router.cc:477] [C0][S3580059227218846830] cluster 'my_service' match for URL '/app/ping'\r\n[2020-06-03 10:38:35.828][17][debug][router] [source/common/router/router.cc:634] [C0][S3580059227218846830] router decoding headers:\r\n':authority', 'localhost:8000'\r\n':path', '/app/ping'\r\n':method', 'GET'\r\n':scheme', 'https'\r\n'user-agent', 'curl/7.68.0'\r\n'accept', '*/*'\r\n'x-forwarded-proto', 'http'\r\n'x-request-id', 'b7aacc7c-f391-4c6a-95f8-7b3df0593c46'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2020-06-03 10:38:35.828][17][debug][pool] [source/common/http/conn_pool_base.cc:337] queueing request due to no available connections\r\n[2020-06-03 10:38:35.828][17][debug][pool] [source/common/http/conn_pool_base.cc:47] creating a new connection\r\n[2020-06-03 10:38:35.828][17][debug][client] [source/common/http/codec_client.cc:34] [C1] connecting\r\n[2020-06-03 10:38:35.828][17][debug][connection] [source/common/network/connection_impl.cc:727] [C1] connecting to [::1]:8102\r\n[2020-06-03 10:38:35.828][17][debug][connection] [source/common/network/connection_impl.cc:740] [C1] immediate connection error: 99\r\n[2020-06-03 10:38:35.828][17][debug][connection] [source/common/network/connection_impl.cc:504] [C1] raising immediate error\r\n[2020-06-03 10:38:35.828][17][debug][connection] [source/common/network/connection_impl.cc:200] [C1] closing socket: 0\r\n[2020-06-03 10:38:35.828][17][debug][client] [source/common/http/codec_client.cc:91] [C1] disconnect. resetting 0 pending requests\r\n[2020-06-03 10:38:35.828][17][debug][pool] [source/common/http/conn_pool_base.cc:265] [C1] client disconnected, failure reason: \r\n[2020-06-03 10:38:35.828][17][debug][router] [source/common/router/router.cc:1018] [C0][S3580059227218846830] upstream reset: reset reason connection failure\r\n[2020-06-03 10:38:35.829][17][debug][http] [source/common/http/conn_manager_impl.cc:1475] [C0][S3580059227218846830] Sending local reply with details upstream_reset_before_response_started{connection failure}\r\n[2020-06-03 10:38:35.829][17][debug][http] [source/common/http/conn_manager_impl.cc:1706] [C0][S3580059227218846830] encoding headers via codec (end_stream=false):\r\n':status', '503'\r\n'content-length', '91'\r\n'content-type', 'text/plain'\r\n'date', 'Wed, 03 Jun 2020 10:38:35 GMT'\r\n'server', 'envoy'\r\n\r\n[2020-06-03 10:38:35.829][17][debug][connection] [source/common/network/connection_impl.cc:558] [C0] remote close\r\n[2020-06-03 10:38:35.829][17][debug][connection] [source/common/network/connection_impl.cc:200] [C0] closing socket: 0\r\n[2020-06-03 10:38:35.829][17][debug][conn_handler] [source/server/connection_handler_impl.cc:86] [C0] adding to cleanup list\r\n[2020-06-03 10:38:40.202][1][debug][upstream] [source/common/upstream/logical_dns_cluster.cc:100] starting async DNS resolution for localhost\r\n[2020-06-03 10:38:40.202][1][debug][upstream] [source/common/upstream/logical_dns_cluster.cc:108] async DNS resolution complete for localhost\r\n[2020-06-03 10:38:40.202][1][debug][upstream] [source/common/upstream/logical_dns_cluster.cc:153] DNS refresh rate reset for localhost, refresh rate 5000 ms\r\n[2020-06-03 10:38:40.211][1][debug][main] [source/server/server.cc:177] flushing stats\r\n```\r\n\r\nI don't know what I'm doing wrong",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11418/comments",
    "author": "AlexDaSoul",
    "comments": [
      {
        "user": "AlexDaSoul",
        "created_at": "2020-06-04T06:55:26Z",
        "body": "The trouble was docker's networking"
      },
      {
        "user": "quangthe",
        "created_at": "2022-02-25T10:30:17Z",
        "body": "@AlexDaSoul Could you provide more details on this issue? Is there any workaround for this? Thanks"
      },
      {
        "user": "AlexDaSoul",
        "created_at": "2022-02-27T18:24:59Z",
        "body": "> @AlexDaSoul Could you provide more details on this issue? Is there any workaround for this? Thanks\r\n\r\nThe problem was that the specified hosts are inside the docker network and they are not the same as the addresses on your local computer. "
      }
    ]
  },
  {
    "number": 11415,
    "title": "Request/Response matching",
    "created_at": "2020-06-03T05:31:57Z",
    "closed_at": "2020-07-12T07:55:29Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11415",
    "body": "\r\nAre any possible ways to match request and response in filter?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11415/comments",
    "author": "iinikolaev",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-07-03T15:24:23Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-12T07:55:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11355,
    "title": "Upstream HTTP/2 connection limits",
    "created_at": "2020-05-28T21:56:33Z",
    "closed_at": "2020-07-05T18:06:01Z",
    "labels": [
      "question",
      "stale",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11355",
    "body": "I have been experimenting with Envoy recently. I am trying to understand the following. Thanks in advance for the help - \r\n\r\nI see that the common least request implementation uses the PnC method where n is configurable (randomly choose n hosts and pick the one with least among them). Assuming there is no zone aware routing or custom weights, statistically, traffic would be spread evenly. This means that there will be connections created for most hosts and will be kept alive in the respective pools (one per host for HTTP2 and multiple for HTTP1). I am trying to understand if there are any dynamic limitations for how many such connections can be kept alive (apart form the circuit breaker limits). Following is the behaviour I am observing -\r\n\r\n**Setup** : I have about 400 hosts in a cluster backend. Requests to Envoy are ~30 rps(these requests are time taking ones). Envoy worker count - 4. HTTP/2 (incoming connections to the listener are HTTP1. Outgoing to cluster hosts are HTTP2. I am interested in upstream connections)\r\n\r\n**Observation** : Envoy creates new connections (`upstream_cx_active` stat) as the requests come in and almost immediately, there is an uptick in the number of closed connection metric (`upstream_cx_close_notify`). Relevant circuit breaker settings for limits on cluster wide connections are - `max_connection_pools` and `max_connections`, both of which in my case are not configured, thereby not limiting them at all. So, this must not be the reason for closing connections almost immediately. A few seconds after the incoming traffic is completely cut off, the rate of of closing connections has come down to 0 leaving about 160 connections intact (active connections) for about an hour (I believe this is TCP connection idle timeout). So, all the connections that were created beyond the 160 mark were being actively closed. I am not sure if I am misconfiguring some setting to cause this. I think, the way it is, it would be very inefficient at high request rate to keep closing connections immediately this way. Is there some other setting (a logic that dynamically determines the total number of outstanding connections. I don't have a correlation to 160 anywhere else, so I am guessing this could be dynamically derived from other configurations.).\r\n\r\n**Follow up** : Is there any affinity in the host selection process (I didn't find any such reference and I can see that it could cause the traffic skewed to a certain set of cluster hosts. Maybe, ends up behaving similarly to a tcp proxy?) to pick hosts that already have connections available in their individual pools ? I assume not. In this case, is it fair to assume that for scenarios where the number of available backends is high and the incoming connections that need to routed are sparse, it is likely that we have a high number connections per worker where some/many could be under/not utilized?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11355/comments",
    "author": "praneethvg",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2020-05-28T23:54:12Z",
        "body": "A couple possible reason:\r\n- `max_connections` defaults to 1024 if it is not configured.\r\n- an uptick of `upstream_cx_close_notify` means your upstream is sending GOAWAY, if the protocol is HTTP2, or `connection: close` in case of HTTP1."
      },
      {
        "user": "praneethvg",
        "created_at": "2020-05-29T09:54:50Z",
        "body": "Thanks @lizan . Ya, the max connections that it reached was under 500. I will look into what is happening on the upstream. The upstream is another Envoy instance as well. Maybe there is a limit on the listener there. I will post my findings here soon."
      },
      {
        "user": "praneethvg",
        "created_at": "2020-05-29T17:22:38Z",
        "body": "@lizan , I checked to see if I could find anything with the upstream hosts. Some observations - \r\n\r\n1. Given this is HTTP/2 , there won't be more than one connection per host. The active connections stabilize at 160 upon repeating this experiment multiple time, suggesting that it is unlikely for it to be caused by something on the upstream hosts. Would you agree?\r\n2. In our setup, the `upstream_cx_close_notify` and `upstream_cx_destroy_local` go hand in hand during the entire time this happens. Also, `upstream_cx_destroy_remote` stays at 0 during this time. I am not entirely sure what `upstream_cx_destroy_local` accounts for (for now, assuming that the connection close is initiated locally)."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-28T18:03:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-05T18:06:00Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11310,
    "title": "when run two envoy container in a pod，the hot restart will work incorrectly",
    "created_at": "2020-05-25T03:47:04Z",
    "closed_at": "2020-07-03T07:24:23Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11310",
    "body": "Description:\r\n\r\nwe running a envoyproxy as a business container in k8s and inject istio-proxy for service mesh，it works fine at first startup. but when we change the istio config，istio-proxy hot restart  for load new config，the istio-proxy will not start anymore。\r\n\r\n\r\nRepro steps:\r\n 1.  run two envoyproxy container in one pod\r\n 2. hot restart one of the container，the container will no longer start successfully\r\n```\r\n[2020-05-25 03:16:30.565][70][critical][assert] [external/envoy/source/server/hot_restart_impl.cc:45] panic: cannot open shared memory region /envoy_shared_memory_0 check user permissions. Error: File exists\r\n```\r\n\r\nConfig:\r\n\r\n    istio-proxy normal config\r\n\r\nLogs:\r\n```\r\n2020-05-25T03:16:30.534531Z\tinfo\tEnvoy command: [-c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster envoy-proxy-stress-v3-dev.default --service-node sidecar~9.166.198.232~envoy-proxy-stress-v3-dev-7f6cd56998-lglt8.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --allow-unknown-fields -l warning --component-log-level misc:error --concurrency 2]\r\n[2020-05-25 03:16:30.563][70][warning][config] [external/envoy/source/server/options_impl.cc:193] --allow-unknown-fields is deprecated, use --allow-unknown-static-fields instead.\r\n[2020-05-25 03:16:30.565][70][critical][assert] [external/envoy/source/server/hot_restart_impl.cc:45] panic: cannot open shared memory region /envoy_shared_memory_0 check user permissions. Error: File exists\r\n[2020-05-25 03:16:30.565][70][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:81] Caught Aborted, suspect faulting address 0x53900000046\r\n[2020-05-25 03:16:30.565][70][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:69] Backtrace (use tools/stack_decode.py to get line numbers):\r\n[2020-05-25 03:16:30.565][70][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #0: __restore_rt [0x7f9c73014390]\r\n[2020-05-25 03:16:30.568][70][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #1: Envoy::Server::HotRestartImpl::HotRestartImpl() [0x1a568c1]\r\n[2020-05-25 03:16:30.570][70][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #2: Envoy::MainCommonBase::MainCommonBase() [0xc74427]\r\n[2020-05-25 03:16:30.572][70][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #3: Envoy::MainCommon::MainCommon() [0xc74d95]\r\n[2020-05-25 03:16:30.574][70][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #4: main [0xc73c03]\r\n[2020-05-25 03:16:30.574][70][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #5: __libc_start_main [0x7f9c72c59830]\r\n2020-05-25T03:16:30.576100Z\twarn\tEpoch 0 terminated with an error: signal: aborted\r\n2020-05-25T03:16:30.576137Z\twarn\tAborted all epochs\r\n2020-05-25T03:16:30.576178Z\terror\tPermanent error: budget exhausted trying to fulfill the desired configuration\r\n2020-05-25T03:16:30.576182Z\terror\tcannot start the e with the desired configuration\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11310/comments",
    "author": "maplebeats",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-06-26T05:50:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-03T07:24:22Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "AMagnolia",
        "created_at": "2024-10-18T08:58:10Z",
        "body": "I meet the same question. How do you resolve it finally?"
      }
    ]
  },
  {
    "number": 11302,
    "title": "knative service is not recheable, connection timesout when the pod scales to zero after applying envoy filter configuration to knative service?",
    "created_at": "2020-05-23T06:47:06Z",
    "closed_at": "2020-05-25T23:07:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11302",
    "body": "Expected Behavior\r\nTrying to apply envoy external authorization filter to knative service.\r\nTo be able to authorize requests consistently with envoy ext_authz filter delegating policy evaludation by open policy agent grpc server.\r\n\r\nActual Behavior\r\ndeploy a knative service, which runs fine.\r\napply below envoy filter using target/label selector\r\nwait for pod to scale to zero, then the service is no longer recheable.\r\n\r\nSteps to Reproduce the Problem\r\ndeploy a knative service, which runs fine.\r\n2.apply below envoy filter using target/label selector\r\n3.wait for pod to scale to zero, then the service is no longer recheable.\r\nAdditional Info\r\nWhen below envoy filter configuratoin is applied to individual knative service using name selector/label selector.\r\nKnative service is accessible with route url. However when the pod goes down. knative service is no longer recheable.\r\n\r\nHow can we use knative service selector to consistently apply envoy filter.\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\nname: ext-authz\r\nnamespace: default\r\nspec:\r\ntargets:\r\n\r\nname: example-app\r\nfilters:\r\nlistenerMatch:\r\nportNumber: 80\r\nlistenerType: SIDECAR_INBOUND\r\nfilterType: HTTP\r\nfilterName: \"envoy.ext_authz\"\r\nfilterConfig:\r\ngrpc_service:\r\ngoogle_grpc:\r\ntarget_uri: opa:8585\r\nstat_prefix: \"ext_authz\"\r\nknative service is not recheable, connection timesout when the pod scales to zero\r\n\r\nCould someone explain what is going behind the scenes and if im missing configuration. or knative doesnt support envoy filter configuration\r\nInstall information:\r\n\r\nPlatform (GKE, IKS, AKS, etc.): KUBEADM installed kubernetes cluster.\r\nKnative Version:knative-0.13\r\nIstio version: 1.4.5 , installed with istioctl, default profile. side car enabled in default namespace where knative service are being deployed",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11302/comments",
    "author": "r-kotagudem",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-05-25T22:16:39Z",
        "body": "Please file an issue with knative."
      },
      {
        "user": "r-kotagudem",
        "created_at": "2020-05-25T23:07:14Z",
        "body": "seems to be knative issue. hence closing"
      }
    ]
  },
  {
    "number": 11297,
    "title": "How to log/get Retry mode Failure details (HTTP Response code, Response Time, etc..) ?",
    "created_at": "2020-05-22T17:19:37Z",
    "closed_at": "2020-07-12T08:55:28Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11297",
    "body": "Hi,\r\nI am using envoy v1.12.1 - load_assignment with two endpoints as below,\r\n\r\n```\r\n- route:\r\n                      cluster: cluster1\r\n                      auto_host_rewrite: true\r\n                      prefix_rewrite: \"this/is/new\"\r\n                      retry_policy:\r\n                        retry_on: \"connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes\"\r\n                        num_retries: 3\r\n\r\n                        retriable_status_codes: [\"401\", \"404\", \"500\", \"502\", \"503\", \"504\"]\r\n                        per_try_timeout: \"5s\"\r\n\r\n                        retry_priority:\r\n                          name: envoy.retry_priorities.previous_priorities\r\n                          config:\r\n                            update_frequency: 2\r\n\r\n                    match:\r\n                      prefix: \"this/is/old\"\r\n\r\nclusters:\r\n      - name:  cluster1\r\n        connect_timeout: 10s\r\n        type: STRICT_DNS\r\n        lb_policy: ROUND_ROBIN\r\n        load_assignment:\r\n          cluster_name: cluster1\r\n          endpoints:\r\n          - priority: 1\r\n            lb_endpoints:\r\n\r\n            - endpoint:\r\n                address:\r\n                  socket_address: { address: \"e1.com\", port_value: 443 }\t\t\t\t\r\n          - priority: 0\r\n            lb_endpoints:\r\n\r\n            - endpoint:\r\n                address:\r\n                  socket_address: { address: \"e2.com\", port_value: 443}\r\n```\r\n\r\nAs per my understanding, When priority 0 endpoint fails, it will try to hit priority 1 endpoint. \r\n\r\nIn Such cases, Is it possible to get/log priority 0 endpoint hit failure reasons? As of now we can get logs of priority 1 endpoint hit alone by default. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11297/comments",
    "author": "GaneshKumarKannan",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-05-25T22:15:56Z",
        "body": "I think you need to use upstream access logs."
      },
      {
        "user": "GaneshKumarKannan",
        "created_at": "2020-05-26T08:16:08Z",
        "body": "@mattklein123 Using Upstream access logs, I am able get priority 1 endpoint (as per above) logs. How about priority 0 endpoint failure reason? Is there any way to get that?"
      },
      {
        "user": "GaneshKumarKannan",
        "created_at": "2020-06-03T11:11:35Z",
        "body": "Any update on this issue? "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-03T11:24:23Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-12T08:55:27Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11273,
    "title": "How to get upstream host from custom http filter or network filter?",
    "created_at": "2020-05-20T15:40:17Z",
    "closed_at": "2020-07-18T09:56:07Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11273",
    "body": "Is there a way to get upstream host from http filter or network filter? callbacks_->streamInfo().upstreamHost() is alway nullptr. Basically, the custom filter needs to get lb-selected upstream host. Is it possible? I tried with upstream network filter. But upstreamHost is always nullptr. Please help.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11273/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "yuval-k",
        "created_at": "2020-05-22T18:43:30Z",
        "body": "AFAIK, host selection happens after your filter runs.\r\ntry implementing the `AccessLog::Instance` interface, and you should see it when the `log` method is called."
      },
      {
        "user": "ghost",
        "created_at": "2020-06-09T13:15:11Z",
        "body": "I am able to get this with \"callbacks_->connection().remoteAddress()->asStringView()\" but it gives the resolved IP Address. I am looking for the host name instead of IP Address. @yuval-k, Any help?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-11T08:39:17Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-18T09:56:06Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "moisesrc13",
        "created_at": "2024-06-14T23:39:33Z",
        "body": "@yuval-k \r\nI also have the same question. Is there a way to get the  upstream host or IP and send it back in the envoy response?"
      }
    ]
  },
  {
    "number": 11236,
    "title": "Can we redirect our request to nearest geo located server with the help of envoy?",
    "created_at": "2020-05-18T05:20:57Z",
    "closed_at": "2020-06-28T13:03:39Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11236",
    "body": "Can we redirect our request to nearest geo located server with the help of envoy?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11236/comments",
    "author": "sojo101",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-06-20T17:56:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-28T13:03:38Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11096,
    "title": "reject all listeners ",
    "created_at": "2020-05-07T11:07:11Z",
    "closed_at": "2020-06-11T02:42:19Z",
    "labels": [
      "question",
      "stale",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11096",
    "body": "hi, as we use envoy as k8s internal grpc proxy. when we set some invalid listeners, then all the rules are rejected.. why we design like this ?  here is some err info.\r\n```\r\nsource/common/config/grpc_mux_impl.cc:226] gRPC config for type.googleapis.com/envoy.api.v2.Listener update rejected: Error adding/updating listener listener_internal: route: unknown weighted cluster\r\n``` ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11096/comments",
    "author": "guangbaowan",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-06-06T19:10:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11080,
    "title": "Heathcheck and Outlier detection Behavior During Upstream Cluster Rolling Update in Kubernetes/ZDM for upstream cluster Rolling Update",
    "created_at": "2020-05-06T08:12:16Z",
    "closed_at": "2020-06-13T08:12:21Z",
    "labels": [
      "question",
      "stale",
      "area/health_checking"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11080",
    "body": "**SetUp:**\r\nI have a k8s cluster in which I am using envoyproxy as ingress. I have two running instances of the upstream cluster. \r\nI have configured both health check and outlier detection as part of the cluster configuration.\r\nThis the cluster configuration:\r\n{\r\n   \"name\":\"cluster1\",\r\n   \"type\":\"STRICT_DNS\",\r\n   \"connect_timeout\":\"5s\",\r\n   \"health_checks\":[\r\n      {\r\n         \"timeout\":\"1s\",\r\n         \"interval\":\"1s\",\r\n         \"unhealthy_threshold\":2,\r\n         \"healthy_threshold\":2,\r\n         \"tcp_health_check\":{},\r\n         \"no_traffic_interval\":\"1s\"\r\n      }\r\n   ],\r\n   \"circuit_breakers\":{\r\n      \"thresholds\":[\r\n         {\r\n            \"max_connections\":8000\r\n         },\r\n         {\r\n            \"priority\":\"HIGH\",\r\n            \"max_connections\":10000\r\n         }\r\n      ]\r\n   },\r\n   \"outlier_detection\":{\r\n      \"consecutive_5xx\":3,\r\n      \"consecutive_gateway_failure\":3\r\n   }\r\n}\r\n**Action/Operation:**\r\nI am trying to do the rolling update of my upstream cluster while making the calls continuously in parallel. I make 100 calls/sec and each call take 5 sec to respond. Hence I might have max 500 connections opened per upstream node at ingress.\r\n\r\nI ensure that one instance of upstream cluster is always running when other one is being upgraded to the newer version.\r\nWhen my upstream instance goes down, it doesn't accept any new request but it will serve all the ongoing request successfully and hence terminate gracefully.\r\n**Expected Behavior**\r\nI expect that max 6 calls(3 calls per cluster) should have failed. But I see that I get > 1300 call failures with 503 status code.\r\n**Obesrvation:**\r\nIt might be that during the time outlier detection was taking place, I might have some connections to which the request would have already assigned. and hence I get the call failure.\r\n\r\nI looked into the logs and I saw this error. But I am not sure what it means:\r\n\r\n[debug][connection] [source/common/network/connection_impl.cc:711] [C4398] connecting to 100.96.1.42:443\r\n[debug][connection] [source/common/network/connection_impl.cc:720] [C4398] connection in progress\r\n[debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n**[debug][connection] [source/common/network/connection_impl.cc:568] [C4398] delayed connection error: 111**\r\n[debug][connection] [source/common/network/connection_impl.cc:193] [C4398] closing socket: 0\r\n[debug][client] [source/common/http/codec_client.cc:88] [C4398] disconnect. resetting 0 pending requests\r\n**[debug][pool] [source/common/http/http1/conn_pool.cc:136] [C4398] client disconnected, failure reason: \r\n[debug][pool] [source/common/http/http1/conn_pool.cc:167] [C4398] purge pending, failure reason: \r\n[debug][router] [source/common/router/router.cc:911] [C3654][S17475984688729051914] upstream reset: reset reason connection failure\r\n[debug][http] [source/common/http/conn_manager_impl.cc:1354] [C3654][S17475984688729051914] Sending local reply with details upstream_reset_before_response_started{connection failure}**\r\n[debug][http] [source/common/http/conn_manager_impl.cc:1552] [C3654][S17475984688729051914] encoding headers via codec (end_stream=false):\r\n':status', '503'\r\n'content-length', '91'\r\n'content-type', 'text/plain'\r\n'date', 'Tue, 05 May 2020 10:23:34 GMT'\r\n'server', 'envoy'\r\n\r\n**My question is how can I avoid this behavior and get near to zero call failures during the rolling update. What is it that I have to set to ensure that I get minimum no. of call failures. I am trying to achieve the ZDM behavior. If there is any other way to achieve that, I will gladly try it :)**\r\n\r\n@mattklein123 Any pointer will be helpful.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11080/comments",
    "author": "rohitagg2020",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-06-06T08:10:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-13T08:12:20Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "abhiroop93",
        "created_at": "2023-08-08T06:58:28Z",
        "body": "Hi @ruoshan ,\r\nDid you find a way around this?"
      }
    ]
  },
  {
    "number": 11063,
    "title": "RBAC settings question",
    "created_at": "2020-05-05T09:57:08Z",
    "closed_at": "2020-06-12T07:52:36Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11063",
    "body": "I wanted to set up the RBAC filter but somehow it doesn't work:\r\n\r\n```\r\n...\r\n            http_filters:\r\n            - name: envoy.filters.http.rbac\r\n              config:\r\n                action: ALLOW\r\n                policies:\r\n                  \"admin-user\":\r\n                    permissions:\r\n                      - any: true\r\n                    principals:\r\n                      - header: { name: \"test\", exact_match: \"111\" }\r\n                  \"general-user\":\r\n                    permissions:\r\n                      - not_rule:\r\n                        - path: { prefix: \"/demo.grpc.api.v1.Demo/\" }\r\n                    principals:\r\n                      - any: true\r\n...\r\n```\r\n\r\nI expected only requests with header `test: 111` can access the service with prefix `/demo.grpc.api.v1.Demo/`, but it turns out all requests can still access the service\r\n\r\nCould you let me know where I got it wrong?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11063/comments",
    "author": "freesnowCL",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-06-05T06:06:48Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-12T07:52:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11012,
    "title": "Question: Envoy configured to use V3 connects to /v2/discovery:clusters",
    "created_at": "2020-04-30T12:53:22Z",
    "closed_at": "2020-05-01T10:20:21Z",
    "labels": [
      "question",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11012",
    "body": "Envoy 1.14.1 is configured to use transport_api_version: V3 for REST xDS. However it still sends requests to \"/v2/discovery:routes\" and \"/v2/discovery:clusters\". Am I missing something obvious here ?\r\n\r\n\r\nConfig:\r\n```\r\ndynamic_resources:\r\n  cds_config:\r\n    api_config_source:\r\n      api_type: REST\r\n      cluster_names: [xds_cluster]\r\n      refresh_delay: 5s\r\n      transport_api_version: V3\r\n\r\n...\r\n          rds:\r\n            route_config_name: Route_configuration\r\n            config_source:\r\n              api_config_source:\r\n                api_type: REST\r\n                cluster_names: [xds_cluster]\r\n                refresh_delay: 5s\r\n                transport_api_version: V3\r\n\r\n```\r\n\r\nLog:\r\n\r\nRoutes:\r\n```\r\n[2020-04-30 12:41:44.722][6][debug][router] [source/common/router/router.cc:477] [C0][S2429303885158800883] cluster 'xds_cluster' match for URL '/v2/discovery:routes'\r\n[2020-04-30 12:41:44.722][6][debug][router] [source/common/router/router.cc:634] [C0][S2429303885158800883] router decoding headers:\r\n':method', 'POST'\r\n':path', '/v2/discovery:routes'\r\n':authority', 'xds_cluster'\r\n':scheme', 'http'\r\n'content-type', 'application/json'\r\n'content-length', '11287'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '172.17.0.2'\r\n'x-envoy-expected-rq-timeout-ms', '1000'\r\n\r\n```\r\nClusters:\r\n```\r\n[2020-04-30 12:41:37.982][6][debug][config] [source/common/config/http_subscription_impl.cc:68] Sending REST request for /v2/discovery:clusters\r\n[2020-04-30 12:41:37.983][6][debug][router] [source/common/router/router.cc:477] [C0][S7534484415178178826] cluster 'xds_cluster' match for URL '/v2/discovery:clusters'\r\n[2020-04-30 12:41:37.983][6][debug][router] [source/common/router/router.cc:634] [C0][S7534484415178178826] router decoding headers:\r\n':method', 'POST'\r\n':path', '/v2/discovery:clusters'\r\n':authority', 'xds_cluster'\r\n':scheme', 'http'\r\n'content-type', 'application/json'\r\n'content-length', '11246'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '172.17.0.2'\r\n'x-envoy-expected-rq-timeout-ms', '1000'\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11012/comments",
    "author": "andrewtikhonov",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-04-30T16:30:44Z",
        "body": "cc @htuch @Shikugawa "
      },
      {
        "user": "htuch",
        "created_at": "2020-05-01T01:06:22Z",
        "body": "I think this is definitely not the correct behavior Envoy side, but curious what happens if you set `resource_api_version` to v3 as well?"
      },
      {
        "user": "andrewtikhonov",
        "created_at": "2020-05-01T10:02:50Z",
        "body": "Thanks. `resource_api_version` enabled it.\r\n\r\n```\r\ndynamic_resources:\r\n  cds_config:\r\n    resource_api_version: V3\r\n    api_config_source:\r\n      api_type: REST\r\n      cluster_names: [xds_cluster]\r\n      refresh_delay: 5s\r\n\r\n```"
      }
    ]
  },
  {
    "number": 10967,
    "title": "Clarifications on upstream_rq_time and downstream_rq_time",
    "created_at": "2020-04-27T18:48:00Z",
    "closed_at": "2020-06-06T09:10:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10967",
    "body": "Hi, I am trying to understand better what `upstream_rq_time` and `downstream_rq_time` exactly measure. I referred to the documentation but it wasn't clear to me. For ex. consider  `request: service A -> Envoy A -> Envoy B -> Service B; response: Service B -> Envoy B -> Envoy A -> Service A`, what do `upstream_rq_time` and `downstream_rq_time` mean here? How is the total RTT calculated?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10967/comments",
    "author": "shashankram",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2020-04-27T19:27:39Z",
        "body": "Envoy A's downstream_rq_time measures the time elapsed from when Envoy A starts handling Service A's request until the entire response has sent to Service A.\r\n\r\nEnvoy A's upstream_rq_time measures the time elapsed from the point where Service A's entire request has been received by the HTTP router filter until the entire upstream response from Envoy B has been received.\r\n\r\nThe same is true for Envoy B, except the downstream is Envoy A's request/response and the upstream is Service B.\r\n\r\nSo Envoy A's downstream_rq_time > Envoy A's upstream_rq_time > Envoy B's downtream_rq_time > Envoy B's upstream_rq_time.\r\n\r\nAssuming there are no blocking filters in use (e.g. ext_auth) I expect the times to be reasonably close together."
      },
      {
        "user": "shashankram",
        "created_at": "2020-04-28T00:30:45Z",
        "body": "Thanks for clarifying!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-30T08:22:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-06T09:10:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10955,
    "title": "Web socket disconnection after 100 seconds",
    "created_at": "2020-04-27T06:36:18Z",
    "closed_at": "2020-04-28T06:08:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10955",
    "body": "*Description:*\r\n>I am using a signalR C# client connection (over websocket) to establish a realtime communication with a service signalr hub through envoy proxy (executed as the latest official docker image). From the client to envoy the communication is HTTPs, from envoy to the hub the communication is a simple HTTP. After 100 seconds from the start of the connection, the communication ends abruptly. If i directly connect the client to the service hub this behavior does not occur (the connection remains stable beyond 100 seconds).\r\n>Since this seems to be a configuration problem (more than a bug), i ask if someone can find a possible error in my configuration (it is my first time to use Envoy and i find nothing online specific to SignalR, websocket and Envoy).\r\n>If further informations are needed i will provide it as an update to this issue.\r\n>Thanks for any help you can provide.\r\n\r\n*Configuration*\r\n\r\n\tadmin:\r\n\t  access_log_path: /tmp/admin_access.log\r\n\t  address:\r\n\t\tsocket_address: \r\n\t\t  address: 0.0.0.0\r\n\t\t  port_value: 9901\r\n\r\n\tstatic_resources:\r\n\t  listeners:\r\n\t\t- address:\r\n\t\t\tsocket_address:\r\n\t\t\t  address: 0.0.0.0\r\n\t\t\t  port_value: 443\r\n\t\t  filter_chains:\r\n\t\t\t- filters:\r\n\t\t\t  - name: main_routing\r\n\t\t\t\ttyped_config: \r\n\t\t\t\t  \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n\t\t\t\t  codec_type: auto\r\n\t\t\t\t  access_log:\r\n\t\t\t\t  - name: envoy.access_loggers.file\r\n\t\t\t\t\ttyped_config:\r\n\t\t\t\t\t  \"@type\": type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\r\n\t\t\t\t\t  path: /dev/stdout\r\n\t\t\t\t\t  json_format:\r\n\t\t\t\t\t\ttime: \"%START_TIME%\"\r\n\t\t\t\t\t\tprotocol: \"%PROTOCOL%\"\r\n\t\t\t\t\t\tduration: \"%DURATION%\"\r\n\t\t\t\t\t\trequest_method: \"%REQ(:METHOD)%\"\r\n\t\t\t\t\t\trequest_host: \"%REQ(HOST)%\"\r\n\t\t\t\t\t\tpath: \"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\"\r\n\t\t\t\t\t\tresponse_flags: \"%RESPONSE_FLAGS%\"\r\n\t\t\t\t\t\troute_name: \"%ROUTE_NAME%\"\r\n\t\t\t\t\t\tupstream_host: \"%UPSTREAM_HOST%\"\r\n\t\t\t\t\t\tupstream_cluster: \"%UPSTREAM_CLUSTER%\"\r\n\t\t\t\t\t\tupstream_local_address: \"%UPSTREAM_LOCAL_ADDRESS%\"\r\n\r\n\t\t\t\t  stat_prefix: ingress_http\r\n\t\t\t\t  route_config:\r\n\t\t\t\t\tname: local_config\r\n\t\t\t\t\tvirtual_hosts:\r\n\r\n\t\t\t\t\t- name: proxy_priv_hub\r\n\t\t\t\t\t  domains: \r\n\t\t\t\t\t  - \"my.cluster_domain.com\"\r\n\t\t\t\t\t  routes:\r\n\t\t\t\t\t\t- match: { prefix: \"/evt/\", case_sensitive: false }\r\n\t\t\t\t\t\t  route: \r\n\t\t\t\t\t\t\tcluster: MY_CLUSTER_NAME\r\n\t\t\t\t\t\t\tauto_host_rewrite: true\r\n\t\t\t\t\t\t\tprefix_rewrite: \"/prv/\"\r\n\t\t\t\t\t\t\tupgrade_configs:\r\n\t\t\t\t\t\t\t  upgrade_type: \"websocket\"\r\n\t\t\t\t\t\t\t  enabled: true\r\n\r\n\t\t\t\t  http_filters:\r\n\t\t\t\t\t- name: envoy.filters.http.fault\r\n\t\t\t\t\t  typed_config:\r\n\t\t\t\t\t\t\"@type\": type.googleapis.com/envoy.config.filter.http.fault.v2.HTTPFault\r\n\t\t\t\t\t\tabort:\r\n\t\t\t\t\t\t  http_status: 503\r\n\t\t\t\t\t\t  percentage:\r\n\t\t\t\t\t\t\tnumerator: 0\r\n\t\t\t\t\t\t\tdenominator: HUNDRED\r\n\t\t\t\t\t- name: envoy.filters.http.router\r\n\t\t\t\t\t  typed_config: {}\r\n\t\t\t  tls_context:\r\n\t\t\t\tcommon_tls_context:\r\n\t\t\t\t  tls_certificates:\r\n\t\t\t\t\t- certificate_chain:\r\n\t\t\t\t\t\tfilename: \"/etc/ssl/certs/ca-certificates.crt\"\r\n\t\t\t\t\t  private_key:\r\n\t\t\t\t\t\tfilename: \"/etc/ssl/certs/ca-certificates.key\"\r\n\r\n\t  clusters:\r\n\t\t- name: MY_CLUSTER_NAME\r\n\t\t  connect_timeout: 0.25s\r\n\t\t  type: strict_dns\r\n\t\t  lb_policy: round_robin\r\n\t\t  load_assignment:\r\n\t\t\tcluster_name: MY_CLUSTER_NAME\r\n\t\t\tendpoints:\r\n\t\t\t- lb_endpoints:\r\n\t\t\t  - endpoint:\r\n\t\t\t\t  address:\r\n\t\t\t\t\tsocket_address: { address: MY_CLUSTER_NAME, port_value: 80 }\r\n\r\n\r\n*Logs*\r\n\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:27.979][12][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C67]\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:27.979][12][debug][connection] [source/common/network/connection_impl.cc:558] [C67] remote close\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:27.979][12][debug][connection] [source/common/network/connection_impl.cc:200] [C67] closing socket: 0\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:27.979][12][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:298] [C67] SSL shutdown: rc=0\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:27.979][12][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C67]\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:27.979][12][debug][conn_handler] [source/server/connection_handler_impl.cc:85] [C67] adding to cleanup list\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C69]\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][connection] [source/common/network/connection_impl.cc:558] [C69] remote close\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][connection] [source/common/network/connection_impl.cc:200] [C69] closing socket: 0\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:298] [C69] SSL shutdown: rc=0\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C69]\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][http] [source/common/http/conn_manager_impl.cc:1936] [C69][S1118758786695779674] stream reset\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][router] [source/common/router/upstream_request.cc:263] [C69][S1118758786695779674] resetting pool request\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][client] [source/common/http/codec_client.cc:114] [C70] request reset\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][connection] [source/common/network/connection_impl.cc:109] [C70] closing data_to_write=0 type=1\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][connection] [source/common/network/connection_impl.cc:200] [C70] closing socket: 1\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][client] [source/common/http/codec_client.cc:91] [C70] disconnect. resetting 0 pending requests\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][pool] [source/common/http/conn_pool_base.cc:265] [C70] client disconnected, failure reason:\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][conn_handler] [source/server/connection_handler_impl.cc:85] [C69] adding to cleanup list\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.010][13][debug][pool] [source/common/http/conn_pool_base.cc:93] [C70] destroying stream: 0 remaining\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 06:35:28.065][6][debug][main] [source/server/server.cc:177] flushing stats\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10955/comments",
    "author": "LeonSebastianCoimbra",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2020-04-27T19:14:08Z",
        "body": "That log implies the upstream closed it's connection first, but you'd have to show the start of the connection as well to be more certain."
      },
      {
        "user": "LeonSebastianCoimbra",
        "created_at": "2020-04-27T20:09:26Z",
        "body": "Hi Zuercher, thanks for the fast reply.\r\n\r\nSignalR (the client that create a websocket connection with a service behind envoy) works as follow:\r\n- it does a first call to an handler called \"negotiate\" (it is a simple http request, so it does not last)\r\n- it create a websocket with a second call after the negotiation ended (in this case the entry is \"evt/events\"\r\n\r\nFrom the client to Envoy the traffic is HTTPs with bearer authentication.\r\n\r\nHere are the logs for the initial handshake/creation:\r\n\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.564][13][debug][conn_handler] [source/server/connection_handler_impl.cc:372] [C10] new connection\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.564][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C10] handshake expecting read\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.566][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C10] handshake expecting read\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.567][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C10] handshake expecting read\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.569][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:176] [C10] handshake complete\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.569][13][debug][http] [source/common/http/conn_manager_impl.cc:268] [C10] new stream\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.569][13][debug][http] [source/common/http/conn_manager_impl.cc:781] [C10][S1673674447679062520] request headers complete (end_stream=true):\r\n\tenvoy.main_proxy.debug_1  | ':authority', 'hprv.lselocal.it'\r\n\tenvoy.main_proxy.debug_1  | ':path', '/evt/events/negotiate?negotiateVersion=1'\r\n\tenvoy.main_proxy.debug_1  | ':method', 'POST'\r\n\tenvoy.main_proxy.debug_1  | 'user-agent', 'Microsoft.AspNetCore.Http.Connections.Client/3.1.3'\r\n\tenvoy.main_proxy.debug_1  | 'x-requested-with', 'XMLHttpRequest'\r\n\tenvoy.main_proxy.debug_1  | 'authorization', 'Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6Ijc0OWI3NTkyZDU4NDk5OWU5NjFkYmJhMTFmZTZjNGI1IiwidHlwIjoiSldUIn0.eyJuYmYiOjE1ODgwMTc3ODcsImV4cCI6MTU4ODAyMTM4NywiaXNzIjoiaHR0cHM6Ly9hdXRoLmxzZWxvY2FsLml0IiwiYXVkIjpbImh0dHBzOi8vYXV0aC5sc2Vsb2NhbC5pdC9yZXNvdXJjZXMiLCJoeC5jbC5hcGkiXSwiY2xpZW50X2lkIjoiaHguY2wuYXBpIiwic3ViIjoiMyIsImF1dGhfdGltZSI6MTU4ODAxNzc4NywiaWRwIjoibG9jYWwiLCJoeDM2NS51c2VyLmFjY291bnRfdHlwZS5jbGFpbSI6IkZSRSIsImh4MzY1LnVzZXIuZGF0YS5jbGFpbSI6ImV5SkpaQ0k2TXl3aVRHNGlPaUpwZEdFaUxDSkRiaUk2SWtsVVFTSXNJazV0SWpvaVFtRnpkR2xoYm04aUxDSk5iaUk2SWlJc0lsTnVJam9pUTI5cGJXSnlZU0lzSWtWdElqb2lZaTVqYjJsdFluSmhRSGxoZW1WMlpXUnZMbUp5SWl3aVRXTWlPbnNpU1dRaU9qTXNJazV0SWpvaVFtRnpkR2xoYm04Z1EyOXBiV0p5WVNJc0lrRjBJam9pUmxKRklpd2lVbXdpT2xzaVQxZE9JbDE5TENKRGN5STZXMTBzSWtac0lqb3pNeXdpUVdOamIzVnVkRlI1Y0dVaU9pSkdVa1VpTENKQlkyTnZkVzUwU1c1MFpYSnVZV3dpT21aaGJITmxMQ0pCWTJOdmRXNTBVbVZuZFd4aGNpSTZkSEoxWlN3aVVISnBkbUZqZVVGalkyVndkR1ZrSWpwMGNuVmxMQ0pWYzJWeVJHVnNaWFJsWkNJNlptRnNjMlY5Iiwic2NvcGUiOlsiaHgzNjUudXNlci5kYXRhIiwib3BlbmlkIiwiaHguY2wuYXBpLnNjb3BlIl0sImFtciI6WyJjdXN0b20iXX0.onxM9L8v49buIcRPL7R2NAO1irlySK7c14kdvk2yhpu8LMa1MHNJdLN-vlG5-mGqp8WpFthoNwzVq2p7s2W5vm4UEfdlMSYPmIVae0G0B3aH8BJpghF0cNqAsSLFwxUIOhTJEMEgNoqsDtKBXk5umhWjTCBXgg4Gt96TCVcJI2a3zwsGrGNITUMVXYO-oxm1xIFzJVv1P0jZcLXSgGZ1O_2-tKirLYvxlq0EUfOmiTmBZN6EO3mv1qC89AB9FII3sl_-ec72UXiGGfcZANwwBKuWilPX9eEGkDPXBTUA2RKM2G_9IsvFWrsSpIbKPdWkLIPmjgrjPI_xOG2tOhpd9g'\r\n\tenvoy.main_proxy.debug_1  | 'content-length', '0'\r\n\tenvoy.main_proxy.debug_1  |\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.569][13][debug][http] [source/common/http/conn_manager_impl.cc:1333] [C10][S1673674447679062520] request end stream\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.569][13][debug][router] [source/common/router/router.cc:477] [C10][S1673674447679062520] cluster 'hx.srv.cs.events' match for URL '/evt/events/negotiate?negotiateVersion=1'\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.569][13][debug][router] [source/common/router/router.cc:634] [C10][S1673674447679062520] router decoding headers:\r\n\tenvoy.main_proxy.debug_1  | ':authority', 'hprv.lselocal.it'\r\n\tenvoy.main_proxy.debug_1  | ':path', '/prv/events/negotiate?negotiateVersion=1'\r\n\tenvoy.main_proxy.debug_1  | ':method', 'POST'\r\n\tenvoy.main_proxy.debug_1  | ':scheme', 'http'\r\n\tenvoy.main_proxy.debug_1  | 'user-agent', 'Microsoft.AspNetCore.Http.Connections.Client/3.1.3'\r\n\tenvoy.main_proxy.debug_1  | 'x-requested-with', 'XMLHttpRequest'\r\n\tenvoy.main_proxy.debug_1  | 'authorization', 'Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6Ijc0OWI3NTkyZDU4NDk5OWU5NjFkYmJhMTFmZTZjNGI1IiwidHlwIjoiSldUIn0.eyJuYmYiOjE1ODgwMTc3ODcsImV4cCI6MTU4ODAyMTM4NywiaXNzIjoiaHR0cHM6Ly9hdXRoLmxzZWxvY2FsLml0IiwiYXVkIjpbImh0dHBzOi8vYXV0aC5sc2Vsb2NhbC5pdC9yZXNvdXJjZXMiLCJoeC5jbC5hcGkiXSwiY2xpZW50X2lkIjoiaHguY2wuYXBpIiwic3ViIjoiMyIsImF1dGhfdGltZSI6MTU4ODAxNzc4NywiaWRwIjoibG9jYWwiLCJoeDM2NS51c2VyLmFjY291bnRfdHlwZS5jbGFpbSI6IkZSRSIsImh4MzY1LnVzZXIuZGF0YS5jbGFpbSI6ImV5SkpaQ0k2TXl3aVRHNGlPaUpwZEdFaUxDSkRiaUk2SWtsVVFTSXNJazV0SWpvaVFtRnpkR2xoYm04aUxDSk5iaUk2SWlJc0lsTnVJam9pUTI5cGJXSnlZU0lzSWtWdElqb2lZaTVqYjJsdFluSmhRSGxoZW1WMlpXUnZMbUp5SWl3aVRXTWlPbnNpU1dRaU9qTXNJazV0SWpvaVFtRnpkR2xoYm04Z1EyOXBiV0p5WVNJc0lrRjBJam9pUmxKRklpd2lVbXdpT2xzaVQxZE9JbDE5TENKRGN5STZXMTBzSWtac0lqb3pNeXdpUVdOamIzVnVkRlI1Y0dVaU9pSkdVa1VpTENKQlkyTnZkVzUwU1c1MFpYSnVZV3dpT21aaGJITmxMQ0pCWTJOdmRXNTBVbVZuZFd4aGNpSTZkSEoxWlN3aVVISnBkbUZqZVVGalkyVndkR1ZrSWpwMGNuVmxMQ0pWYzJWeVJHVnNaWFJsWkNJNlptRnNjMlY5Iiwic2NvcGUiOlsiaHgzNjUudXNlci5kYXRhIiwib3BlbmlkIiwiaHguY2wuYXBpLnNjb3BlIl0sImFtciI6WyJjdXN0b20iXX0.onxM9L8v49buIcRPL7R2NAO1irlySK7c14kdvk2yhpu8LMa1MHNJdLN-vlG5-mGqp8WpFthoNwzVq2p7s2W5vm4UEfdlMSYPmIVae0G0B3aH8BJpghF0cNqAsSLFwxUIOhTJEMEgNoqsDtKBXk5umhWjTCBXgg4Gt96TCVcJI2a3zwsGrGNITUMVXYO-oxm1xIFzJVv1P0jZcLXSgGZ1O_2-tKirLYvxlq0EUfOmiTmBZN6EO3mv1qC89AB9FII3sl_-ec72UXiGGfcZANwwBKuWilPX9eEGkDPXBTUA2RKM2G_9IsvFWrsSpIbKPdWkLIPmjgrjPI_xOG2tOhpd9g'\r\n\tenvoy.main_proxy.debug_1  | 'content-length', '0'\r\n\tenvoy.main_proxy.debug_1  | 'x-forwarded-for', '172.26.0.1'\r\n\tenvoy.main_proxy.debug_1  | 'x-forwarded-proto', 'https'\r\n\tenvoy.main_proxy.debug_1  | 'x-envoy-internal', 'true'\r\n\tenvoy.main_proxy.debug_1  | 'x-request-id', '79d9f1ac-5f60-4ed7-9b1f-016d21cc046d'\r\n\tenvoy.main_proxy.debug_1  | 'x-envoy-expected-rq-timeout-ms', '300000'\r\n\tenvoy.main_proxy.debug_1  | 'x-envoy-original-path', '/evt/events/negotiate?negotiateVersion=1'\r\n\tenvoy.main_proxy.debug_1  |\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.570][13][debug][pool] [source/common/http/conn_pool_base.cc:337] queueing request due to no available connections\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.570][13][debug][pool] [source/common/http/conn_pool_base.cc:47] creating a new connection\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.570][13][debug][client] [source/common/http/codec_client.cc:34] [C11] connecting\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.570][13][debug][connection] [source/common/network/connection_impl.cc:727] [C11] connecting to 172.26.0.28:80\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.570][13][debug][connection] [source/common/network/connection_impl.cc:736] [C11] connection in progress\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.570][13][debug][connection] [source/common/network/connection_impl.cc:592] [C11] connected\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.570][13][debug][client] [source/common/http/codec_client.cc:72] [C11] connected\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.570][13][debug][pool] [source/common/http/conn_pool_base.cc:143] [C11] attaching to next request\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.570][13][debug][pool] [source/common/http/conn_pool_base.cc:68] [C11] creating stream\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.570][13][debug][router] [source/common/router/upstream_request.cc:317] [C10][S1673674447679062520] pool ready\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.574][13][debug][router] [source/common/router/router.cc:1149] [C10][S1673674447679062520] upstream headers complete: end_stream=false\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.574][13][debug][http] [source/common/http/conn_manager_impl.cc:1706] [C10][S1673674447679062520] encoding headers via codec (end_stream=false):\r\n\tenvoy.main_proxy.debug_1  | ':status', '200'\r\n\tenvoy.main_proxy.debug_1  | 'date', 'Mon, 27 Apr 2020 20:03:36 GMT'\r\n\tenvoy.main_proxy.debug_1  | 'content-type', 'application/json'\r\n\tenvoy.main_proxy.debug_1  | 'server', 'envoy'\r\n\tenvoy.main_proxy.debug_1  | 'content-length', '252'\r\n\tenvoy.main_proxy.debug_1  | 'x-envoy-upstream-service-time', '4'\r\n\tenvoy.main_proxy.debug_1  |\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.574][13][debug][client] [source/common/http/codec_client.cc:104] [C11] response complete\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.574][13][debug][pool] [source/common/http/http1/conn_pool.cc:48] [C11] response complete\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.574][13][debug][pool] [source/common/http/conn_pool_base.cc:93] [C11] destroying stream: 0 remaining\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.576][12][debug][conn_handler] [source/server/connection_handler_impl.cc:372] [C12] new connection\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.576][12][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C12] handshake expecting read\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.576][12][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C12] handshake expecting read\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.576][12][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C12] handshake expecting read\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.579][12][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:176] [C12] handshake complete\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.579][12][debug][http] [source/common/http/conn_manager_impl.cc:268] [C12] new stream\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.579][12][debug][http] [source/common/http/conn_manager_impl.cc:781] [C12][S10071328960350829753] request headers complete (end_stream=false):\r\n\tenvoy.main_proxy.debug_1  | ':authority', 'hprv.lselocal.it'\r\n\tenvoy.main_proxy.debug_1  | ':path', '/evt/events?id=cUE570z7HRGR0yfEXW2K0Q'\r\n\tenvoy.main_proxy.debug_1  | ':method', 'GET'\r\n\tenvoy.main_proxy.debug_1  | 'connection', 'Upgrade'\r\n\tenvoy.main_proxy.debug_1  | 'upgrade', 'websocket'\r\n\tenvoy.main_proxy.debug_1  | 'sec-websocket-key', 'dytoujS8hi09Kjk3E6tyyQ=='\r\n\tenvoy.main_proxy.debug_1  | 'sec-websocket-version', '13'\r\n\tenvoy.main_proxy.debug_1  | 'x-requested-with', 'XMLHttpRequest'\r\n\tenvoy.main_proxy.debug_1  | 'authorization', 'Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6Ijc0OWI3NTkyZDU4NDk5OWU5NjFkYmJhMTFmZTZjNGI1IiwidHlwIjoiSldUIn0.eyJuYmYiOjE1ODgwMTc3ODcsImV4cCI6MTU4ODAyMTM4NywiaXNzIjoiaHR0cHM6Ly9hdXRoLmxzZWxvY2FsLml0IiwiYXVkIjpbImh0dHBzOi8vYXV0aC5sc2Vsb2NhbC5pdC9yZXNvdXJjZXMiLCJoeC5jbC5hcGkiXSwiY2xpZW50X2lkIjoiaHguY2wuYXBpIiwic3ViIjoiMyIsImF1dGhfdGltZSI6MTU4ODAxNzc4NywiaWRwIjoibG9jYWwiLCJoeDM2NS51c2VyLmFjY291bnRfdHlwZS5jbGFpbSI6IkZSRSIsImh4MzY1LnVzZXIuZGF0YS5jbGFpbSI6ImV5SkpaQ0k2TXl3aVRHNGlPaUpwZEdFaUxDSkRiaUk2SWtsVVFTSXNJazV0SWpvaVFtRnpkR2xoYm04aUxDSk5iaUk2SWlJc0lsTnVJam9pUTI5cGJXSnlZU0lzSWtWdElqb2lZaTVqYjJsdFluSmhRSGxoZW1WMlpXUnZMbUp5SWl3aVRXTWlPbnNpU1dRaU9qTXNJazV0SWpvaVFtRnpkR2xoYm04Z1EyOXBiV0p5WVNJc0lrRjBJam9pUmxKRklpd2lVbXdpT2xzaVQxZE9JbDE5TENKRGN5STZXMTBzSWtac0lqb3pNeXdpUVdOamIzVnVkRlI1Y0dVaU9pSkdVa1VpTENKQlkyTnZkVzUwU1c1MFpYSnVZV3dpT21aaGJITmxMQ0pCWTJOdmRXNTBVbVZuZFd4aGNpSTZkSEoxWlN3aVVISnBkbUZqZVVGalkyVndkR1ZrSWpwMGNuVmxMQ0pWYzJWeVJHVnNaWFJsWkNJNlptRnNjMlY5Iiwic2NvcGUiOlsiaHgzNjUudXNlci5kYXRhIiwib3BlbmlkIiwiaHguY2wuYXBpLnNjb3BlIl0sImFtciI6WyJjdXN0b20iXX0.onxM9L8v49buIcRPL7R2NAO1irlySK7c14kdvk2yhpu8LMa1MHNJdLN-vlG5-mGqp8WpFthoNwzVq2p7s2W5vm4UEfdlMSYPmIVae0G0B3aH8BJpghF0cNqAsSLFwxUIOhTJEMEgNoqsDtKBXk5umhWjTCBXgg4Gt96TCVcJI2a3zwsGrGNITUMVXYO-oxm1xIFzJVv1P0jZcLXSgGZ1O_2-tKirLYvxlq0EUfOmiTmBZN6EO3mv1qC89AB9FII3sl_-ec72UXiGGfcZANwwBKuWilPX9eEGkDPXBTUA2RKM2G_9IsvFWrsSpIbKPdWkLIPmjgrjPI_xOG2tOhpd9g'\r\n\tenvoy.main_proxy.debug_1  |\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.579][12][debug][router] [source/common/router/router.cc:477] [C12][S10071328960350829753] cluster 'hx.srv.cs.events' match for URL '/evt/events?id=cUE570z7HRGR0yfEXW2K0Q'\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.579][12][debug][router] [source/common/router/router.cc:634] [C12][S10071328960350829753] router decoding headers:\r\n\tenvoy.main_proxy.debug_1  | ':authority', 'hprv.lselocal.it'\r\n\tenvoy.main_proxy.debug_1  | ':path', '/prv/events?id=cUE570z7HRGR0yfEXW2K0Q'\r\n\tenvoy.main_proxy.debug_1  | ':method', 'GET'\r\n\tenvoy.main_proxy.debug_1  | ':scheme', 'http'\r\n\tenvoy.main_proxy.debug_1  | 'connection', 'Upgrade'\r\n\tenvoy.main_proxy.debug_1  | 'upgrade', 'websocket'\r\n\tenvoy.main_proxy.debug_1  | 'sec-websocket-key', 'dytoujS8hi09Kjk3E6tyyQ=='\r\n\tenvoy.main_proxy.debug_1  | 'sec-websocket-version', '13'\r\n\tenvoy.main_proxy.debug_1  | 'x-requested-with', 'XMLHttpRequest'\r\n\tenvoy.main_proxy.debug_1  | 'authorization', 'Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6Ijc0OWI3NTkyZDU4NDk5OWU5NjFkYmJhMTFmZTZjNGI1IiwidHlwIjoiSldUIn0.eyJuYmYiOjE1ODgwMTc3ODcsImV4cCI6MTU4ODAyMTM4NywiaXNzIjoiaHR0cHM6Ly9hdXRoLmxzZWxvY2FsLml0IiwiYXVkIjpbImh0dHBzOi8vYXV0aC5sc2Vsb2NhbC5pdC9yZXNvdXJjZXMiLCJoeC5jbC5hcGkiXSwiY2xpZW50X2lkIjoiaHguY2wuYXBpIiwic3ViIjoiMyIsImF1dGhfdGltZSI6MTU4ODAxNzc4NywiaWRwIjoibG9jYWwiLCJoeDM2NS51c2VyLmFjY291bnRfdHlwZS5jbGFpbSI6IkZSRSIsImh4MzY1LnVzZXIuZGF0YS5jbGFpbSI6ImV5SkpaQ0k2TXl3aVRHNGlPaUpwZEdFaUxDSkRiaUk2SWtsVVFTSXNJazV0SWpvaVFtRnpkR2xoYm04aUxDSk5iaUk2SWlJc0lsTnVJam9pUTI5cGJXSnlZU0lzSWtWdElqb2lZaTVqYjJsdFluSmhRSGxoZW1WMlpXUnZMbUp5SWl3aVRXTWlPbnNpU1dRaU9qTXNJazV0SWpvaVFtRnpkR2xoYm04Z1EyOXBiV0p5WVNJc0lrRjBJam9pUmxKRklpd2lVbXdpT2xzaVQxZE9JbDE5TENKRGN5STZXMTBzSWtac0lqb3pNeXdpUVdOamIzVnVkRlI1Y0dVaU9pSkdVa1VpTENKQlkyTnZkVzUwU1c1MFpYSnVZV3dpT21aaGJITmxMQ0pCWTJOdmRXNTBVbVZuZFd4aGNpSTZkSEoxWlN3aVVISnBkbUZqZVVGalkyVndkR1ZrSWpwMGNuVmxMQ0pWYzJWeVJHVnNaWFJsWkNJNlptRnNjMlY5Iiwic2NvcGUiOlsiaHgzNjUudXNlci5kYXRhIiwib3BlbmlkIiwiaHguY2wuYXBpLnNjb3BlIl0sImFtciI6WyJjdXN0b20iXX0.onxM9L8v49buIcRPL7R2NAO1irlySK7c14kdvk2yhpu8LMa1MHNJdLN-vlG5-mGqp8WpFthoNwzVq2p7s2W5vm4UEfdlMSYPmIVae0G0B3aH8BJpghF0cNqAsSLFwxUIOhTJEMEgNoqsDtKBXk5umhWjTCBXgg4Gt96TCVcJI2a3zwsGrGNITUMVXYO-oxm1xIFzJVv1P0jZcLXSgGZ1O_2-tKirLYvxlq0EUfOmiTmBZN6EO3mv1qC89AB9FII3sl_-ec72UXiGGfcZANwwBKuWilPX9eEGkDPXBTUA2RKM2G_9IsvFWrsSpIbKPdWkLIPmjgrjPI_xOG2tOhpd9g'\r\n\tenvoy.main_proxy.debug_1  | 'content-length', '0'\r\n\tenvoy.main_proxy.debug_1  | 'x-forwarded-for', '172.26.0.1'\r\n\tenvoy.main_proxy.debug_1  | 'x-forwarded-proto', 'https'\r\n\tenvoy.main_proxy.debug_1  | 'x-envoy-internal', 'true'\r\n\tenvoy.main_proxy.debug_1  | 'x-request-id', '2eeedfa4-e43c-459e-8ad5-f50ccac4807a'\r\n\tenvoy.main_proxy.debug_1  | 'x-envoy-expected-rq-timeout-ms', '300000'\r\n\tenvoy.main_proxy.debug_1  | 'x-envoy-original-path', '/evt/events?id=cUE570z7HRGR0yfEXW2K0Q'\r\n\tenvoy.main_proxy.debug_1  |\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.579][12][debug][pool] [source/common/http/conn_pool_base.cc:337] queueing request due to no available connections\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.579][12][debug][pool] [source/common/http/conn_pool_base.cc:47] creating a new connection\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.579][12][debug][client] [source/common/http/codec_client.cc:34] [C13] connecting\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.579][12][debug][connection] [source/common/network/connection_impl.cc:727] [C13] connecting to 172.26.0.28:80\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.580][12][debug][connection] [source/common/network/connection_impl.cc:736] [C13] connection in progress\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.580][12][debug][connection] [source/common/network/connection_impl.cc:592] [C13] connected\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.580][12][debug][client] [source/common/http/codec_client.cc:72] [C13] connected\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.580][12][debug][pool] [source/common/http/conn_pool_base.cc:143] [C13] attaching to next request\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.580][12][debug][pool] [source/common/http/conn_pool_base.cc:68] [C13] creating stream\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.580][12][debug][router] [source/common/router/upstream_request.cc:317] [C12][S10071328960350829753] pool ready\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.607][12][debug][router] [source/common/router/router.cc:1149] [C12][S10071328960350829753] upstream headers complete: end_stream=false\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:03:36.607][12][debug][http] [source/common/http/conn_manager_impl.cc:1706] [C12][S10071328960350829753] encoding headers via codec (end_stream=false):\r\n\tenvoy.main_proxy.debug_1  | ':status', '101'\r\n\tenvoy.main_proxy.debug_1  | 'connection', 'Upgrade'\r\n\tenvoy.main_proxy.debug_1  | 'date', 'Mon, 27 Apr 2020 20:03:36 GMT'\r\n\tenvoy.main_proxy.debug_1  | 'server', 'envoy'\r\n\tenvoy.main_proxy.debug_1  | 'upgrade', 'websocket'\r\n\tenvoy.main_proxy.debug_1  | 'sec-websocket-accept', 'g/eRYdasgS88uKBrDbodES87oNo='\r\n\tenvoy.main_proxy.debug_1  | 'content-length', '0'\r\n\tenvoy.main_proxy.debug_1  |\r\n\tenvoy.main_proxy.debug_1  | {\"upstream_local_address\":\"172.26.0.29:38436\",\"duration\":\"4\",\"time\":\"2020-04-27T20:03:36.569Z\",\"route_name\":\"-\",\"response_flags\":\"-\",\"request_method\":\"POST\",\"upstream_host\":\"172.26.0.28:80\",\"upstream_cluster\":\"hx.srv.cs.events\",\"request_host\":\"-\",\"path\":\"/evt/events/negotiate?negotiateVersion=1\",\"protocol\":\"HTTP/1.1\"}\r\n\r\n\r\nHere are the logs at the disconnection (after 100 seconds, they are alike the ones in the initial question):\r\n\r\n\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.586][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C10]\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.586][13][debug][connection] [source/common/network/connection_impl.cc:558] [C10] remote close\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.586][13][debug][connection] [source/common/network/connection_impl.cc:200] [C10] closing socket: 0\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.586][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:298] [C10] SSL shutdown: rc=0\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.586][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C10]\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.586][13][debug][conn_handler] [source/server/connection_handler_impl.cc:86] [C10] adding to cleanup list\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.615][12][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C12]\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.615][12][debug][connection] [source/common/network/connection_impl.cc:558] [C12] remote close\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.615][12][debug][connection] [source/common/network/connection_impl.cc:200] [C12] closing socket: 0\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.616][12][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:298] [C12] SSL shutdown: rc=0\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.616][12][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C12]\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.616][12][debug][http] [source/common/http/conn_manager_impl.cc:1936] [C12][S10071328960350829753] stream reset\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.616][12][debug][router] [source/common/router/upstream_request.cc:263] [C12][S10071328960350829753] resetting pool request\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.616][12][debug][client] [source/common/http/codec_client.cc:114] [C13] request reset\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.616][12][debug][connection] [source/common/network/connection_impl.cc:109] [C13] closing data_to_write=0 type=1\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.616][12][debug][connection] [source/common/network/connection_impl.cc:200] [C13] closing socket: 1\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.616][12][debug][client] [source/common/http/codec_client.cc:91] [C13] disconnect. resetting 0 pending requests\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.616][12][debug][pool] [source/common/http/conn_pool_base.cc:265] [C13] client disconnected, failure reason:\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.616][12][debug][conn_handler] [source/server/connection_handler_impl.cc:86] [C12] adding to cleanup list\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.616][12][debug][pool] [source/common/http/conn_pool_base.cc:93] [C13] destroying stream: 0 remaining\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:17.046][13][debug][conn_handler] [source/server/connection_handler_impl.cc:372] [C14] new connection\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:17.046][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C14] handshake expecting read\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:17.046][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C14] handshake expecting read\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:17.050][13][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:176] [C14] handshake complete\r\n\tenvoy.main_proxy.debug_1  | [2020-04-27 20:05:17.050][13][debug][http] [source/common/http/conn_manager_impl.cc:268] [C14] new stream\r\n"
      },
      {
        "user": "zuercher",
        "created_at": "2020-04-27T23:47:21Z",
        "body": "C12 is the inbound websocket request and eventually we see \r\n\r\n```\r\nenvoy.main_proxy.debug_1  | [2020-04-27 20:05:16.615][12][debug][connection] [source/common/network/connection_impl.cc:558] [C12] remote close\r\n```\r\n\r\nwhich is the client closing the connection.\r\n\r\nOne thing that was fixed recently is #10811, which removes the `Connection-Length: 0` in the 101 response that Envoy is sending. I know from prior experience that some versions of the Microsoft runtime stack have trouble with that header. You might try with a version of Envoy after that PR."
      },
      {
        "user": "LeonSebastianCoimbra",
        "created_at": "2020-04-28T06:08:45Z",
        "body": "Hi Zuercher, thanks again for your help.\r\n\r\nI used the latest dev build of envoy from dockerhub (generate a couple of hours ago) and it seems to work!!\r\n\r\nYour intuition seems to be correct.\r\n\r\nThis, however, makes me think that there are not many microsoft clients that use websocket and envoy, otherwise it would have been corrected a long time ago.\r\n\r\nYou solved the problem just looking at the logs i posted; of course you have great experience about envoy, but i was wondering if there are documents that explain how to read (in a correct way) the logs or enhance them. Apart from the access log and the debug logs (activated by a command line option) i didn't see other logs. \r\n\r\nI close this issue because you already brilliantly solved it, but if you have another couple of minutes to point me to some documentation that i can read about how to better debug such a situation you'll have my thanks (other than the thanks that i already owe you of course)."
      },
      {
        "user": "zuercher",
        "created_at": "2020-04-28T16:54:57Z",
        "body": "Unfortunately, there aren't really any docs discussion debugging using Envoy logs. In general you can use the `[Cn]` markings to track events related to a downstream connection and then find the related `[Cm]` for the upstream connection. Remote close means the non-Envoy side closed the connection. Local close means it was Envoy. Neither of those is necessarily a problem, but if you aren't expecting the closure, it can help determine the cause."
      },
      {
        "user": "LeonSebastianCoimbra",
        "created_at": "2020-04-29T05:55:43Z",
        "body": "Thanks again, i owe you."
      }
    ]
  },
  {
    "number": 10952,
    "title": "examples/jaeger-tracing and examples/zipkin-tracing protobuf Bootstrap has unknown fields",
    "created_at": "2020-04-25T23:05:16Z",
    "closed_at": "2020-04-28T16:01:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10952",
    "body": "```\r\n$ cd examples/jaeger-tracing\r\n\r\n$ docker-compose pull\r\n\r\n$ docker-compose build -d\r\n...\r\nfront-envoy_1  | [2020-04-25 23:00:48.737][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\nfront-envoy_1  | [2020-04-25 23:00:48.737][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10952/comments",
    "author": "chadm-sq",
    "comments": [
      {
        "user": "chadm-sq",
        "created_at": "2020-04-25T23:18:12Z",
        "body": "Same for \"jaeger-native-tracing\" example.\r\n\r\n```\r\nfront-envoy_1  | [2020-04-25 23:17:06.614][10][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\nfront-envoy_1  | [2020-04-25 23:17:06.614][10][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\njaeger-native-tracing_front-envoy_1 exited with code 1\r\n```"
      },
      {
        "user": "mk46",
        "created_at": "2020-04-27T13:29:33Z",
        "body": "@cmiller-sq I have tested Jaeger-tracing and Zipkin-tracing. it's working for me. Here,  you need to remove all previous pulled images regarding envoy. "
      },
      {
        "user": "chadm-sq",
        "created_at": "2020-04-28T16:00:59Z",
        "body": "Well, I can' reproduce it now. Sorry for noise."
      }
    ]
  },
  {
    "number": 10951,
    "title": "examples/ uses new-style filter names in images that don't support them",
    "created_at": "2020-04-25T22:50:59Z",
    "closed_at": "2020-04-30T19:42:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10951",
    "body": "```\r\n$ cd examples/front-end\r\n\r\n$ docker-compose pull\r\n\r\n$ docker-compose up --build\r\n...\r\nfront-envoy_1  | [2020-04-25 22:46:34.317][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\nfront-envoy_1  | [2020-04-25 22:46:34.318][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10951/comments",
    "author": "chadm-sq",
    "comments": [
      {
        "user": "chadm-sq",
        "created_at": "2020-04-26T00:03:12Z",
        "body": "Using `find examples -type f -name docker-compose.yaml -exec echo \\; -exec echo \\; -exec echo found {} and starting it \\; -execdir docker-compose up --quiet-pull --build --abort-on-container-exit \\;`\r\nand `docker kill $(docker ps -q )`\r\n\r\n# examples/front-envoy\r\n\r\n```\r\nfront-envoy_1  | [2020-04-25 23:46:35.190][7][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\nfront-envoy_1  | [2020-04-25 23:46:35.191][7][info][main] [source/server/server.cc:602] exiting\r\n```\r\n\r\n# examples/redis\r\n\r\nok!\r\n\r\n# examples/jaeger-tracing\r\n\r\n```\r\nfront-envoy_1  | [2020-04-25 23:49:58.945][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\nfront-envoy_1  | [2020-04-25 23:49:58.945][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\n```\r\nSee #10952\r\n\r\n# examples/lua\r\n\r\n```\r\nproxy_1        | [2020-04-25 23:50:10.830][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\nproxy_1        | Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\n```\r\n\r\n# examples/load-reporting-service\r\n\r\nok!\r\n\r\n# examples/zipkin-tracing\r\n\r\n```\r\nfront-envoy_1  | [2020-04-25 23:52:23.383][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\nfront-envoy_1  | [2020-04-25 23:52:23.383][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\n```\r\n\r\nSee #10952 \r\n\r\n# examples/mysql\r\n\r\nok!\r\n\r\n# examples/cors/frontend\r\n\r\n```\r\nfront-envoy_1       | [2020-04-25 23:54:08.954][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.cors'\r\nfront-envoy_1       | [2020-04-25 23:54:08.955][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1       | Didn't find a registered implementation for name: 'envoy.filters.http.cors'\r\n```\r\n\r\n# examples/cors/backend\r\n\r\n```\r\nfront-envoy_1      | [2020-04-25 23:54:20.504][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.cors'\r\nfront-envoy_1      | [2020-04-25 23:54:20.505][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1      | Didn't find a registered implementation for name: 'envoy.filters.http.cors'\r\n```\r\n\r\n# examples/jaeger-native-tracing\r\n\r\n```\r\nfront-envoy_1  | [2020-04-25 23:54:35.120][10][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\nfront-envoy_1  | [2020-04-25 23:54:35.120][10][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\n```\r\n\r\nSee #10952 \r\n\r\n# examples/grpc-bridge\r\n\r\n```\r\nbuild github.com/envoyproxy/envoy: cannot load github.com/envoyproxy/envoy/examples/grpc-bridge/server/kv: module github.com/envoyproxy/envoy/examples/grpc-bridge/server@latest (v0.0.0-20200425220349-0b0213fdc38e) found, but does not contain package github.com/envoyproxy/envoy/examples/grpc-bridge/server/kv\r\n```\r\n\r\n# examples/fault-injection\r\n\r\n```\r\nenvoy_1    | [2020-04-25 23:54:54.852][1][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/envoy.yaml': Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\nenvoy_1    | [2020-04-25 23:54:54.852][1][info][main] [source/server/server.cc:602] exiting\r\nenvoy_1    | Didn't find a registered implementation for name: 'envoy.filters.http.router'\r\n```"
      },
      {
        "user": "mk46",
        "created_at": "2020-04-27T07:33:02Z",
        "body": "Hi @cmiller-sq, It seems that docker images are outdated. Are you trying to run docker-compose with master branch?"
      },
      {
        "user": "mk46",
        "created_at": "2020-04-29T20:14:59Z",
        "body": "@cmiller-sq, In my opinion, it's not a bug because of older docker image present in localhost stop pulling the updated image from docker hub. you need to remove it and then proceed further."
      },
      {
        "user": "chadm-sq",
        "created_at": "2020-04-30T19:42:20Z",
        "body": "I didn't do anything special to get old images. I'm willing to think I'm doing something wrong, though. Closing for now."
      }
    ]
  },
  {
    "number": 10924,
    "title": "Consult about rbac filter",
    "created_at": "2020-04-24T02:43:00Z",
    "closed_at": "2020-06-06T11:10:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10924",
    "body": "What fields do the RBAC filters currently support for headers? According to the official documentation, I saw that \": method\" and \": path\" are supported. Is there a filtering mechanism for the payload field in the headers? If so, how to write rules in Envoy's startup configuration file? Looking forward to your reply",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10924/comments",
    "author": "CalbeeMing0530",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-05-30T10:22:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-06T11:10:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10874,
    "title": "Cannot use TLS with TCP Proxy",
    "created_at": "2020-04-21T14:18:28Z",
    "closed_at": "2020-07-19T03:36:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10874",
    "body": "*Title*: *Unable to create a TLS listener with TCP Proxy alone*\r\n\r\n*Description*:\r\nI'm trying to create a TLS termination proxy for RSyslog but I'm being unable to make envoy perform the TLS part of the proxying.\r\n\r\nHere's my config:\r\n```yaml\r\nstatic_resources:\r\n  listeners:\r\n  - name: rsyslog-filter\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 6514 }\r\n    listener_filters:\r\n    - name: \"envoy.filters.listener.tls_inspector\"\r\n      typed_config: {}\r\n    filter_chains:\r\n    - filter_chain_match:\r\n        server_names: [\"syslog.example.com\"]\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.api.v2.auth.DownstreamTlsContext\r\n          common_tls_context:\r\n            tls_certificates:\r\n            - certificate_chain: { filename: \"/certs/tls.crt\" }\r\n              private_key: { filename: \"/certs/tls.key\" }\r\n      filters:\r\n      - name: envoy.filters.network.tcp_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.tcp_proxy.v2.TcpProxy\r\n          stat_prefix: ingress_tcp\r\n          cluster: rsyslog_server\r\n          max_connect_attempts: 3\r\n          access_log:\r\n          - name: rsyslog.access_log\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n              path: /dev/stdout\r\n  clusters:\r\n  - name: rsyslog-server\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 514\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n```\r\n\r\nEnvoy is being launched as a container in a pod and it is now proxying from the 6514 port to the 514 port where there's another container listening. Connections do flow, but the TLS part is not working despite my efforts. I've tried for version 1.14.1 and for `envoy-dev:latest`.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10874/comments",
    "author": "suvl",
    "comments": [
      {
        "user": "suvl",
        "created_at": "2020-05-13T16:15:36Z",
        "body": "@mattklein123 is this a question? it's more kinda bug, envoy shoulnd't do TCP+TLS without the need for anything else?"
      },
      {
        "user": "zuercher",
        "created_at": "2020-05-23T22:06:29Z",
        "body": "The problem is that your TCP proxy config specifies: `cluster: rsyslog_server` and your cluster is `name: rsyslog-server` (e.g., `_` vs `-`), so the TCP proxy immediately closes the connection since the cluster does not exist.\r\n\r\nThe metric `tcp.ingress_tcp.downstream_cx_no_route` is incremented in this case."
      },
      {
        "user": "SanthoshKani",
        "created_at": "2020-06-10T12:37:43Z",
        "body": "For a POC project, I am using envoy as TLS termination proxy for Zookeeper. Our envoy.yaml listed below. I am able to make it work.\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - name: zk_listener\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 2181\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.tcp_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.tcp_proxy.v2.TcpProxy\r\n          stat_prefix: downstream_cx_total\r\n          cluster: zk_service\r\n      transport_socket:\r\n        name: tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.api.v2.auth.DownstreamTlsContext\r\n          common_tls_context:\r\n            tls_certificates:\r\n            - certificate_chain:\r\n                filename: \"/certs/certificate.pem\"\r\n              private_key:\r\n                filename: \"/certs/key.pem\"\r\n            validation_context:\r\n              trusted_ca:\r\n                filename: \"/certs/rootCa/ca.pem\"\r\n              allow_expired_certificate: false\r\n          require_client_certificate: false\r\n  clusters:\r\n  - name: zk_service\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: zk_service\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 0.0.0.0\r\n                port_value: 12181\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 18081\r\n```\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-11T02:39:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-19T03:36:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10804,
    "title": "[Help wanted] how to configure envoy to send http notification to all the members in the cluster without load balancing (broadcast)?",
    "created_at": "2020-04-16T05:06:45Z",
    "closed_at": "2020-05-24T21:28:50Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10804",
    "body": "existing configuration sending a notification to the only one pod based on the lb_policy =Round Robin, but we have a case to send the notification to the all the members of the cluster (like a broadcast) , Kindly do the needful to solve the problem,\r\n\r\n**Envoy version: 1.14.2**\r\n\r\n- '@type': type.googleapis.com/envoy.config.cluster.v3.Cluster\r\n  name: envoy-ueauth-envoy-2-0\r\n  type: STRICT_DNS\r\n  connect_timeout:\r\n    seconds: 18\r\n  http2_protocol_options: {}\r\n  lb_policy: ROUND_ROBIN\r\n  transport_socket:\r\n    name: envoy.transport_sockets.tls\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n      common_tls_context:\r\n        tls_certificates:\r\n        - certificate_chain:\r\n            filename: \"/aus-certs/server.pem\"\r\n          private_key:\r\n            filename: \"/aus-certs/key.pem\"\r\n          password:\r\n            inline_string: \"server@123\"\r\n        validation_context:\r\n          trusted_ca:\r\n            filename: \"/aus-certs/trusted.pem\"\r\n        alpn_protocols: [ \"h2\", \"http/1.1\" ]\r\n  dns_resolvers:\r\n  - socket_address:\r\n      address: 127.0.0.1\r\n      port_value: 8600\r\n      protocol: UDP\r\n  load_assignment:\r\n    cluster_name: envoy-ueauth-envoy-2-0\r\n    endpoints:\r\n    - lb_endpoints:\r\n      - endpoint:\r\n          address:\r\n            socket_address:\r\n              address: \"192.168.25.133\"\r\n              port_value: \"8445\"\r\n              protocol: TCP",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10804/comments",
    "author": "vsharathis",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2020-04-16T13:36:56Z",
        "body": "@snowp do you have any suiggestions?"
      },
      {
        "user": "vsharathis",
        "created_at": "2020-04-16T17:32:44Z",
        "body": "@snowp Kindly share your thoughts to solve the problem. and thanks in advance  "
      },
      {
        "user": "snowp",
        "created_at": "2020-04-16T17:49:13Z",
        "body": "This is not something that's currently supported, so this would be have to be a feature request. \r\n\r\nI can imagine this being implemented as a new route action (broadcast to all hosts for a given cluster) or even as a separate filter if we want to avoid adding the complexity to the router (at the cost of missing out on some router features like `requset_headers_to_add`). \r\n\r\n@mattklein123 might have thoughts on how this would be implemented"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-04-16T17:55:04Z",
        "body": "> @mattklein123 might have thoughts on how this would be implemented\r\n\r\nYeah there is no way to do this today. It would require substantial changes to the router or possibly even a new filter to do it. It's certainly possible but would require a lot of design thought and a full proposal."
      },
      {
        "user": "vsharathis",
        "created_at": "2020-04-17T04:34:01Z",
        "body": "@snowp @mattklein123 @yanavlasov Thank you all for the quick response, is there any chance that this kind of feature (broadcast to all the members in the cluster) will be included in upcoming releases?"
      },
      {
        "user": "yanavlasov",
        "created_at": "2020-04-17T17:32:05Z",
        "body": "@vsharathis It is unclear to me if there is anyone else beside you asking for this feature. Can you try implementing it as a filter yourself?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-17T18:28:05Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-24T21:28:49Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "slonka",
        "created_at": "2020-06-03T12:10:10Z",
        "body": "> @vsharathis It is unclear to me if there is anyone else beside you asking for this feature. Can you try implementing it as a filter yourself?\r\n\r\n@yanavlasov I would be interested in such a feature. Can someone point me to how would I implement it in a filter?"
      },
      {
        "user": "KiranCS-17",
        "created_at": "2020-09-01T15:04:58Z",
        "body": "We have similar requirement for broadcasting the request to multiple endpoints  in the down stream.\r\nWould be good feature if ENVOY provides this "
      },
      {
        "user": "adoyle-h",
        "created_at": "2021-10-13T04:59:57Z",
        "body": "Is this feature implemented so far?"
      },
      {
        "user": "KiranCS-17",
        "created_at": "2022-08-23T12:21:24Z",
        "body": "Could some body help if this facility is given in Envoy or any alternate methods"
      }
    ]
  },
  {
    "number": 10588,
    "title": "circuit-breakers:max_connections in tcp/conn_pool.cc",
    "created_at": "2020-03-31T05:19:10Z",
    "closed_at": "2020-05-11T14:38:00Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10588",
    "body": "My circumstance I have config max_connections like 3,but when I start call(thrift proxy),the existing connections are much more than 3.So I debug the source code,is there any situation when there are existing conns(resource manager's current more then 1), and ready_conns_,busy_conns_,pending_conns_ are all empty?\r\nAny help will be appreciate!!!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10588/comments",
    "author": "pyrl247",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2020-03-31T14:07:41Z",
        "body": "@zuercher might be able to help here with Thrift specifics, but I think we'll probably need more details to investigate."
      },
      {
        "user": "pyrl247",
        "created_at": "2020-03-31T14:44:21Z",
        "body": "Here is my yams config:\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9902\r\n      protocol: TCP\r\nnode:\r\n  cluster: test-cluster\r\n  id: test-id\r\nstatic_resources:\r\n  clusters:\r\n  - circuit_breakers:\r\n      thresholds:\r\n        max_connections: 1\r\n        max_pending_requests: 2\r\n    connect_timeout: 1s\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: service_thrift\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 10.4.45.47\r\n                port_value: 9990\r\n    name: service_thrift\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 10001\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.thrift_proxy\r\n        typed_config:\r\n          '@type': type.googleapis.com/envoy.config.filter.network.thrift_proxy.v2alpha1.ThriftProxy\r\n          protocol: BINARY\r\n          transport: FRAMED\r\n          route_config:\r\n            name: local_route\r\n            routes:\r\n            - match:\r\n                service_name: ''\r\n              route:\r\n                cluster: service_thrift\r\n          stat_prefix: pyrl_server1\r\n          thrift_filters:\r\n          - name: envoy.filters.thrift.router\r\n            typed_config: {}\r\n\r\n```\r\n\r\nWhen I start thrift-client multi-thread,there are always more than 1 connection betweent envoy and thrift-server.seems max_connections didn't work at all."
      },
      {
        "user": "lambdai",
        "created_at": "2020-04-01T02:52:03Z",
        "body": "Drive by: Is it possible the existing 3 connections are among multiple worker thread, while you are looking at a lucky thread which owns no connections"
      },
      {
        "user": "pyrl247",
        "created_at": "2020-04-01T03:42:07Z",
        "body": "Is this config(max_connections) work in each worker thread?not a global config?If I config max_connections as 3, and start envoy with cmd args --concurrency 2,Is that mean the max total connection between envoy and server will be 6(3*2)?"
      },
      {
        "user": "zuercher",
        "created_at": "2020-04-07T16:59:22Z",
        "body": "It's per-worker, per-upstream. So the total number of connections for a cluster will be `max_connections * cluster_size * concurrency`. This is not specific to Thrift, BTW. It's a property of Envoy's connection pooling and applies to the http connection manager and tcp proxy as well."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-07T17:50:06Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10552,
    "title": "Build: bazel build failed in network filters",
    "created_at": "2020-03-27T02:07:03Z",
    "closed_at": "2020-05-06T17:35:02Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10552",
    "body": "*Description*:\r\nI meet the similar error about kafka networkfilter when building envoy-wasm:\r\nthe Traceback :\r\n``` shell\r\nERROR: /home/envoy/source/extensions/filters/network/kafka/BUILD:163:1: Executing genrule //source/extensions/filters/network/kafka:kafka_response_generated_source failed (Exit 1) linux-sandbox failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/sandbox/linux-sandbox/200/execroot/envoy && \\\r\n  exec env - \\\r\n    BAZEL_COMPILER=clang \\\r\n    BAZEL_CXXOPTS='-stdlib=libc++' \\\r\n    BAZEL_LINKLIBS=-l%:libc++.a:-l%:libc++abi.a:-lm \\\r\n    BAZEL_LINKOPTS=-lm \\\r\n    CC=/root/package/llvm-project/build/bin/clang \\\r\n    CXX=/root/package/llvm-project/build/bin/clang++ \\\r\n    CXXFLAGS='-stdlib=libc++ -Wno-error' \\\r\n    LDFLAGS='-stdlib=libc++' \\\r\n    PATH=/root/package/llvm-project/build/bin:/root/package/z3-master/build:/root/package/gn/out:/root/package/llvm-project/build/bin:/usr/local/java/bin:/root/apache-ant-1.9.14/bin:/usr/share/Modules/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin \\\r\n    TMPDIR=/tmp \\\r\n  /root/.cache/bazel/_bazel_root/install/c435ff35b78e9d78b94f06dd7301a644/linux-sandbox -t 15 -w /root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/sandbox/linux-sandbox/200/execroot/envoy -w /tmp -w /dev/shm -D -- /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh;\r\n      ./bazel-out/host/bin/source/extensions/filters/network/kafka/kafka_protocol_code_generator_bin response         bazel-out/aarch64-fastbuild/bin/source/extensions/filters/network/kafka/external/responses.h bazel-out/aarch64-fastbuild/bin/source/extensions/filters/network/kafka/external/kafka_response_resolver.cc         bazel-out/aarch64-fastbuild/bin/source/extensions/filters/network/kafka/external/response_metrics.h external/kafka_source/AddOffsetsToTxnResponse.json external/kafka_source/AddPartitionsToTxnResponse.json external/kafka_source/AlterConfigsResponse.json external/kafka_source/AlterReplicaLogDirsResponse.json external/kafka_source/ApiVersionsResponse.json external/kafka_source/ControlledShutdownResponse.json external/kafka_source/CreateAclsResponse.json external/kafka_source/CreateDelegationTokenResponse.json external/kafka_source/CreatePartitionsResponse.json external/kafka_source/CreateTopicsResponse.json external/kafka_source/DeleteAclsResponse.json external/kafka_source/DeleteGroupsResponse.json external/kafka_source/DeleteRecordsResponse.json external/kafka_source/DeleteTopicsResponse.json external/kafka_source/DescribeAclsResponse.json external/kafka_source/DescribeConfigsResponse.json external/kafka_source/DescribeDelegationTokenResponse.json external/kafka_source/DescribeGroupsResponse.json external/kafka_source/DescribeLogDirsResponse.json external/kafka_source/ElectPreferredLeadersResponse.json external/kafka_source/EndTxnResponse.json external/kafka_source/ExpireDelegationTokenResponse.json external/kafka_source/FetchResponse.json external/kafka_source/FindCoordinatorResponse.json external/kafka_source/HeartbeatResponse.json external/kafka_source/InitProducerIdResponse.json external/kafka_source/JoinGroupResponse.json external/kafka_source/LeaderAndIsrResponse.json external/kafka_source/LeaveGroupResponse.json external/kafka_source/ListGroupsResponse.json external/kafka_source/ListOffsetResponse.json external/kafka_source/MetadataResponse.json external/kafka_source/OffsetCommitResponse.json external/kafka_source/OffsetFetchResponse.json external/kafka_source/OffsetForLeaderEpochResponse.json external/kafka_source/ProduceResponse.json external/kafka_source/RenewDelegationTokenResponse.json external/kafka_source/SaslAuthenticateResponse.json external/kafka_source/SaslHandshakeResponse.json external/kafka_source/StopReplicaResponse.json external/kafka_source/SyncGroupResponse.json external/kafka_source/TxnOffsetCommitResponse.json external/kafka_source/UpdateMetadataResponse.json external/kafka_source/WriteTxnMarkersResponse.json\r\n    ')\r\nsrc/main/tools/linux-sandbox.cc:154: linux-sandbox-pid1 has PID 41442\r\nsrc/main/tools/linux-sandbox-pid1.cc:175: working dir: /root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/sandbox/linux-sandbox/200/execroot/envoy\r\nsrc/main/tools/linux-sandbox-pid1.cc:194: writable: /root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/sandbox/linux-sandbox/200/execroot/envoy\r\nsrc/main/tools/linux-sandbox-pid1.cc:194: writable: /tmp\r\nsrc/main/tools/linux-sandbox-pid1.cc:194: writable: /dev/shm\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/kernel/security\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/unified\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/systemd\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/perf_event\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/freezer\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/devices\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/blkio\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/cpuset\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/net_cls,net_prio\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/cpu,cpuacct\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/pids\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/rdma\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/hugetlb\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/memory\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/cgroup/files\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/pstore\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/firmware/efi/efivars\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/bpf\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/kernel/config\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/fs/selinux\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/kernel/debug\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /sys/kernel/debug/tracing\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /dev\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount rw: /dev/shm\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /dev/pts\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /dev/mqueue\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /dev/hugepages\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /run\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /run/user/0\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /run/docker/netns/default\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /run/docker/netns/7912a705dc8e\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /run/docker/netns/8de312ce079f\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /proc\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /proc/sys/fs/binfmt_misc\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /proc/sys/fs/binfmt_misc\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount rw: /tmp\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /boot\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /boot/efi\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /home\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/6135834168a8967c7a299bab30ca5c8638ed884742446dc3331cb063d9daca87/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/7a79decb12df183f59d27058948b08a966de1596ee1a31d7a8175779811751c8/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/7a2adc7cd48411cd44fbff1b49b442ae952b524a2431e8394dceb2e0ab0f9165/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/6aab83cc6f4bac9784f9c612df26334689b5cfba7419dfd0a429841ee88696f0/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/containers/24f76977d44136beaa6ebda7d434eccf07fbe778e9445e3487551bbf60e6b8ac/mounts/shm\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/containers/3745919e34b650efd7f26629991d87447077016ea438c413e4ef005959fe3869/mounts/shm\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/containers/f455cacc3201749b47c103b7a1b7956feeeb32e4c07de2ba91087325dc413077/mounts/shm\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/containers/f59767aa73ec238bbeedbae48f6e6d7c9658e653a8e5ed38d5a6540e69d35935/mounts/shm\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/206a9e10f13da9affccb3e3bb04881700aafc7a51db48529f4776970e6a9ae1c/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/dce139f77992c441dadba23ea4a9cc31b94527da5777900230dcc86fa148f8e4/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/kubelet/pods/f073bc69-7bc9-4eb0-bb20-e59610de5582/volumes/kubernetes.io~secret/kube-proxy-token-glz8h\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/b01849df3f098c38ac624d8af239ce0f6200890a90dc5a65220d5323442614b4/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/containers/79e23dcccb97442cf1a023c723e85656c8f5f7407ac9c58046090b98ff1d9d10/mounts/shm\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/b7836c251e76c584a02a7f8d4311ca153c0e8e0d72cab4aba8567d53f8c43097/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/kubelet/pods/b2635cb2-f456-477a-bd92-ef9b4682594f/volumes/kubernetes.io~secret/coredns-token-fbhpn\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/kubelet/pods/7f14ae8a-afd7-40ed-8c67-405771945efd/volumes/kubernetes.io~secret/coredns-token-fbhpn\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/44ecff368b1fe24996206e1c51b8d650890b3317b601b0071b1a6454834db94a/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/0383551dce51a4bfb933e7f1a6a696716f5b7d595507d1ebab585a3442eb370c/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/containers/b0d18cdb44f5d0cc5284e912ed813a9ee4725a4caf0b7136021a5d70eb7c5c53/mounts/shm\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/containers/f634fd7eafdb1bc7ffac7634ab379f6d855e64008f224757c3de36966230d745/mounts/shm\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/e652c19d5bbca5d4e3d747cd9d560c708d028424be6fbebb6985dd1679c83ddf/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/b8ae91411cf489656383832991963c1db9d1bbf173b007e1424606f125a51f61/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/a68f97687f416152764fb424bd3ba1f2afce9482d7130f6be4ab62e6bffa7750/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/616c06f65c3d6eac91a97f306eadffea21259d684ccfbbf01811871660836f32/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/kubelet/pods/505687f8-e9d6-4a30-b82f-0202323f4501/volumes/kubernetes.io~secret/flannel-token-5s972\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/fe8e8f9bcca512bab7328e60611cd0e0bed754f9111be9421b1fb6220e23b059/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/containers/059c64b94697667fc91786c31ce8191435299d3ce9ef93947d4c1241a5c4ed66/mounts/shm\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount ro: /var/lib/docker/overlay2/ed397cbd523fc80a27f7e57a6cb6818c392f8d897097ad76d27db09f893a67c4/merged\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount rw: /root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/sandbox/linux-sandbox/200/execroot/envoy\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount rw: /root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/sandbox/linux-sandbox/200/execroot/envoy\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount rw: /tmp\r\nsrc/main/tools/linux-sandbox-pid1.cc:265: remount rw: /dev/shm\r\nsrc/main/tools/process-tools.cc:118: sigaction(32, &sa, nullptr) failed\r\nsrc/main/tools/process-tools.cc:118: sigaction(33, &sa, nullptr) failed\r\nTraceback (most recent call last):\r\n  File \"./bazel-out/host/bin/source/extensions/filters/network/kafka/kafka_protocol_code_generator_bin\", line 349, in <module>\r\n    Main()\r\n  File \"./bazel-out/host/bin/source/extensions/filters/network/kafka/kafka_protocol_code_generator_bin\", line 303, in Main\r\n    'Cannot exec() %r: file not found.' % main_filename\r\nAssertionError: Cannot exec() '/root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/sandbox/linux-sandbox/200/execroot/envoy/./bazel-out/host/bin/source/extensions/filters/network/kafka/kafka_protocol_code_generator_bin.runfiles/envoy/source/extensions/filters/network/kafka/protocol/launcher.py': file not found.\r\n```\r\nIt seems like launcher.py not found, however I found the file\r\n``` shell\r\n[root@localhost envoy]# ll /root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/sandbox/linux-sandbox/200/execroot/envoy/./bazel-out/host/bin/source/extensions/filters/network/kafka/kafka_protocol_code_generator_bin.runfiles/envoy/source/extensions/filters/network/kafka/protocol/launcher.py\r\nlrwxrwxrwx. 1 root root 139 Mar 27 09:48 /root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/sandbox/linux-sandbox/200/execroot/envoy/./bazel-out/host/bin/source/extensions/filters/network/kafka/kafka_protocol_code_generator_bin.runfiles/envoy/source/extensions/filters/network/kafka/protocol/launcher.py -> /root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/execroot/envoy/source/extensions/filters/network/kafka/protocol/launcher.py\r\n[root@localhost envoy]# ll /root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/execroot/envoy/source/extensions/filters/network/kafka/protocol/launcher.py\r\n-rw-------. 1 root root 1836 Mar 26 17:03 /root/.cache/bazel/_bazel_root/9705b42e78be2f32cf7e0ddc5f30c7bd/execroot/envoy/source/extensions/filters/network/kafka/protocol/launcher.py\r\n```\r\nAny idea for what can I do to make it, buddies? ：）\r\nAnd it seems like envoyproxy/envoy#7082 , but no useful information can help me: (",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10552/comments",
    "author": "popozy",
    "comments": [
      {
        "user": "popozy",
        "created_at": "2020-03-27T02:13:30Z",
        "body": "Enviroment:\r\n+ PATH=/root/package/llvm-project/build/bin:/root/package/z3-master/build:/root/package/gn/out:/root/package/llvm-project/build/bin:/usr/local/java/bin:/root/apache-ant-1.9.14/bin:/usr/share/Modules/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin\r\n+ clang++:\r\nlang version 8.0.1\r\nTarget: aarch64-unknown-linux-gnu\r\nThread model: posix\r\nInstalledDir: /root/package/llvm-project/build/bin\r\n+ clang:\r\nclang version 8.0.1\r\nTarget: aarch64-unknown-linux-gnu\r\nThread model: posix\r\nInstalledDir: /root/package/llvm-project/build/bin\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-29T16:37:58Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-06T17:35:01Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-13T18:24:33Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "orangeji11",
        "created_at": "2020-12-03T02:50:54Z",
        "body": "@popozy hi,i meet this question,too.\r\nHave you solved it？"
      },
      {
        "user": "popozy",
        "created_at": "2020-12-03T15:38:24Z",
        "body": "> @popozy hi,i meet this question,too.\r\n> Have you solved it？\r\n\r\npity for us, I didn't found a effective solution to solve it. I found that the proposal to support envoy project on arm arch. was  not an urgent thing. "
      },
      {
        "user": "nickolaev",
        "created_at": "2021-03-17T08:32:25Z",
        "body": "On ubuntu 20.04 I had to install `python-is-python3` for a similar problem. Might not be related though."
      }
    ]
  },
  {
    "number": 10537,
    "title": "How can I determine if the current request  is a health check request when the driver is about to start a span?",
    "created_at": "2020-03-26T16:08:01Z",
    "closed_at": "2020-05-02T22:55:03Z",
    "labels": [
      "question",
      "area/tracing",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10537",
    "body": "The method of traceRequest is invoked in `ConnectionManagerImpl::ActiveStream::decodeHeaders` before the chain of http filter (health check filter, router filter, etc). The `health check` attribution of stream info is updated in health check filter. But the method of traceRequest will determine if the current request  is a health check. So the variable of tracing_decision won’t be {Reason::HealthCheck, false}. How can I determine if the current request  is a health check request when the driver is about to start a span? If my understanding is wrong, please correct me.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10537/comments",
    "author": "SpecialYang",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-04-25T20:46:00Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-02T22:55:02Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10498,
    "title": "Burst of HealthChecking Probes",
    "created_at": "2020-03-24T16:33:45Z",
    "closed_at": "2020-05-02T08:55:03Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10498",
    "body": "*Title*: *Burst of Health Checks on Cluster with Many Replicas of Envoy*\r\n\r\n*Description*:\r\nUsers of upstream services are complaining about bursts of health checks.  The issue is that all replicas of Envoy require to internally maintain the accessibility of all the upstream replicas of a given service, so as to correctly implement the load balancing strategy, removing inaccessible upstreams from consideration, and therefore the number of probes is *n* * *m* in every period for every service.  Where *n* is the number Envoys running and *m* is the number of replicas for the service.  And if all Envoys came up at once, this burst of health check probes occur all at once.\r\n\r\nI can think of multiple ways to address this.\r\n1. Add some randomness to the periodicity, so that all copies of Envoy aren't issuing their health check probes to a given service in lock step.\r\n2. Have the Envoys check the KV store whether the upstream has been checked within the periodicity, and only if no healthy probe was conducted within the timespan will a given Envoy issue a probe.  This approach has the downside of not having each Envoy check that it can reach every instance of an upstream.  Other downsides are its reliance on accessing the KV store, as well as its relative complexity.  The upside is that only *m* requests are done over the period.\r\n3. Stagger the launch of the Envoys.  This approach requires launching Envoy in a way that isn't consistent with the way replicas are typically launched in *k8s*\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10498/comments",
    "author": "PinchasLev",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-03-24T22:27:01Z",
        "body": "Envoy already supports jitter via the `initial_jitter` and `interval_jitter` configuration options.\r\n\r\n(2) Could probably be implemented via a custom health checker. Note also that Envoy can be used without active checking and rely on the control plane to provide centralized health checking."
      },
      {
        "user": "PinchasLev",
        "created_at": "2020-03-25T02:25:13Z",
        "body": "Thanks @mattklein123.  I'll read up on both options you mention."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-25T08:46:02Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-02T08:55:02Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10413,
    "title": "Envoy as front Proxy gives \"socket hang up\" during rolling update.",
    "created_at": "2020-03-17T06:54:36Z",
    "closed_at": "2020-04-25T23:46:02Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10413",
    "body": "I am using envoy as front proxy in my kubernetes Setup. This front proxy is exposed via Load Balancer Service of kubernetes.\r\n\r\nI am firing continuous HTTP Calls on the setup. And in the meantime, I am trying to do a rolling update. My expectation is that until the new pod comes up, All HTTP Calls will be served by the older pod. And once the new Pod is up, All calls will be served by this. Hence I expect that all my calls will pass. But while checking the response, I saw that for some of the calls, I am getting Response as \"Error: Socket Hang Up\".\r\n\r\nI am not sure why this behavior is happening. It might be because of the persistent connection?\r\n\r\nWhat configuration should I do to ensure that none of the calls will be dropped.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10413/comments",
    "author": "singlakirti",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-04-17T02:21:00Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-25T23:46:01Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10401,
    "title": "Is it possible to set separate filters for static and dynamic routes?",
    "created_at": "2020-03-16T08:17:43Z",
    "closed_at": "2020-04-25T18:46:03Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10401",
    "body": "We are introducing an xDS server that will set dynamic routes and cluster details to Envoy. However we want to keep existing static routes in place and may be move them to xDS server in the future. Is there a way to achieve that? \r\nI tried add multiple http connection managers, but Envoy doesn't seem to route to the second one. Thanks for your help! \r\n\r\n```\r\n  listeners:\r\n    - name: gateway-listener-8080\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 8080\r\n          protocol: TCP\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              config:\r\n                stat_prefix: ingress_http\r\n                http_filters:\r\n                  - name: envoy.router\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - domains:\r\n                        - '*'\r\n                      name: backend\r\n                      routes:\r\n                        - match:\r\n                            prefix: /staticbackend\r\n                          route:\r\n                            prefix_rewrite: /\r\n                            cluster: cluster_http___static_backend\r\n            - name: envoy.http_connection_manager                            \r\n              config:\r\n                stat_prefix: ingress_http\r\n                http_filters:\r\n                  - name: envoy.router\r\n                rds:\r\n                  route_config_name: routes-listener-8080\r\n                  config_source:\r\n                    ads: {}\r\n\r\n\r\n  dynamic_resources:\r\n    ads_config:\r\n      api_type: GRPC\r\n      grpc_services:\r\n        - envoy_grpc:\r\n            cluster_name: xds_cluster\r\n    cds_config:\r\n      ads: {}                    \r\n\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10401/comments",
    "author": "ksanghavi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-04-17T06:20:58Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-25T18:46:01Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10346,
    "title": "Question: Malformed IP Address for listener with address \"localhost\"",
    "created_at": "2020-03-11T23:09:19Z",
    "closed_at": "2020-03-13T02:46:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10346",
    "body": "```\r\nstatic_resources:\r\n  listeners:\r\n    - address:\r\n        socket_address:\r\n          address: localhost\r\n          port_value: 19888\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n                codec_type: auto\r\n```\r\n\r\nFor the above config, I see this error when bootstrapping envoy:\r\n\r\n[critical][main] [source/server/server.cc:91] error initializing configuration - malformed IP address: localhost\r\n\r\nWhat is the reason we cannot specify the listen address to be localhost?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10346/comments",
    "author": "sumukhs",
    "comments": [
      {
        "user": "sumukhs",
        "created_at": "2020-03-12T22:19:30Z",
        "body": "@htuch , @mattklein123  - Any insights here? Typically, listening on localhost listens on both the ipv4 and ipv6 address in other places like grpc server. Why isn't this supported in envoy?"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-03-12T22:27:46Z",
        "body": "localhost is a DNS name. We don't support binding on DNS addresses. Use 127.0.0.1, ::1, etc."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-11T22:33:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10304,
    "title": "Method of gRPC Multicast or Channel Mirroring?",
    "created_at": "2020-03-09T07:59:02Z",
    "closed_at": "2020-04-17T15:21:00Z",
    "labels": [
      "question",
      "stale",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10304",
    "body": "This is sort of a weird architecture question but I thought I'd ask here first, related to gRPC channel multicast I guess if that's an accurate descriptor.\r\n\r\nI am looking at Envoy for AWS Lambda function routing via Gloo and I am trying to figure out an optimal way of leveraging gRPC bidirectional streaming with a type of multicast requirement. I have a bunch of gRPC-based mobile clients that will be establishing a bidirectional gRPC streaming connection to an Envoy cluster. Sensor data is being streamed from these clients and with periodic responses from Envoy in return. I have a collection of say 40 AWS Lambda functions that are exposed as services for gRPC -> Gloo -> AWS Lambda function routing; what would be the optimal way of multicast (channel mirroring?) that single stream of sensor data from each mobile client over gRPC without duplicating messages to each 40 routes that invoke AWS Lambda functions? And with periodic responses from those 40 AWS Lambda routes then multiplexed back into the gRPC bidirectional stream that has already been established?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10304/comments",
    "author": "tb438",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-04-08T16:34:08Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-17T15:20:59Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10258,
    "title": "Question on Envoy performing search and actions on http message content",
    "created_at": "2020-03-05T02:09:38Z",
    "closed_at": "2020-04-17T12:21:00Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10258",
    "body": "Envoy has provided a large number of filters, but does Envoy provide the functionality of searching(inspecting) the http content(i.e., HTTP message body) and performing an action on the message body if certain pattern is discovered? \r\n\r\nAnd, does Envoy support passing the http message content to a 3rd party application and the 3rd party application will return the modified message content back to Envoy for upstream delivery? \r\n\r\nIf Envoy does not support searching and actions on the http message content, but if it can export the content to a 3rd party application, the 3rd party application can run the search and perform actions on the http message content, and then pass the modified http message back to Envoy. The actions could be changing the http message content, adding data to the content, removing part of the content, deleting the whole http message, and doing some statistics, etc.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10258/comments",
    "author": "gary-20",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-04-08T21:34:05Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-17T12:20:59Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10202,
    "title": "Is there any good video tutorial available on EnvoyProxy?",
    "created_at": "2020-02-28T02:55:08Z",
    "closed_at": "2020-04-06T06:00:55Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10202",
    "body": "*Title*: Is there any good video tutorial available on EnvoyProxy?\r\n\r\n*Description*:\r\n>Is there any good video tutorial available on EnvoyProxy? I googled and couldn't find any.\r\n\r\n[optional *Relevant Links*:]\r\n>NA\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10202/comments",
    "author": "asinha08",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-03-30T05:52:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-06T06:00:54Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10191,
    "title": "Question on Debug Logs",
    "created_at": "2020-02-27T16:51:08Z",
    "closed_at": "2020-04-06T02:00:55Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10191",
    "body": "Hi All,\r\n\r\nI keep seeing below logs on envoy with DEBUG level\r\n\r\nI am interested to know what is causing this handshake and what is the originating point for this. can log format for debug be updated with setting which can show the source of this handshake?\r\n\r\n[2020-02-27 13:46:36.415][24][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:138] [C340] handshake error: 2\r\n[2020-02-27 13:46:36.433][24][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:138] [C340] handshake error: 2\r\n[2020-02-27 13:46:36.433][24][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:138] [C340] handshake error: 2\r\n[2020-02-27 13:46:36.442][24][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:138] [C340] handshake error: 2\r\n[2020-02-27 13:46:36.442][24][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:138] [C340] handshake error: 2",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10191/comments",
    "author": "vkpardava",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-03-30T01:04:59Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-06T02:00:54Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10172,
    "title": "Deletion of certificates",
    "created_at": "2020-02-26T08:58:29Z",
    "closed_at": "2020-04-03T14:36:55Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10172",
    "body": "Is it possible to delete certificates used by envoy through SDS? Sending a default instance of a Secret as a response to the Secret request, seems that it does not work. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10172/comments",
    "author": "tazmanian10",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-03-27T14:02:54Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-03T14:36:54Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10169,
    "title": "How to configure ratelimit network filter to be able to limit on source ip ?",
    "created_at": "2020-02-26T00:28:22Z",
    "closed_at": "2020-04-03T18:36:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10169",
    "body": "Title: How to configure ratelimit network filter to be able to limit on source ip ?\r\n\r\nDescription:\r\n I am trying to configure rate limiting for redis cluster and I want to limit based on the source ip address. How should the descriptor be configured in this case since we cannot have actions in network level ratelimit filter.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10169/comments",
    "author": "kritishaw",
    "comments": [
      {
        "user": "dio",
        "created_at": "2020-02-26T01:13:05Z",
        "body": "cc. @junr03 "
      },
      {
        "user": "kritishaw",
        "created_at": "2020-02-26T17:08:07Z",
        "body": "@mattklein123 @junr03 - It would be great if you could answer this question either ways so that I can start evaluating other options if this is not possible. Thanks in advance."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-03-27T18:02:54Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-03T18:36:52Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-11T03:33:44Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10155,
    "title": "Cannot load multiple CA certificates through SDS",
    "created_at": "2020-02-25T09:53:29Z",
    "closed_at": "2020-04-06T09:00:55Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10155",
    "body": "Hi all,\r\n\r\ni've been trying to load multiple CA certificates through SDS by concatenating them and setting them as an inline string. However it seems that envoy always takes into account only the first one.\r\n\r\nIn order to test this, I have created two different authorities by using openssl and then created and signed certificates and keys with each one of them. I set the trusted authorities as a reply to the SDS request and get an ACK from envoy. However, when i use curl to send a request towards envoy, TLS handshake is completed only for the request that uses the certificates produced from the CA authority that sits on the top of the list i sent as a reply.  ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10155/comments",
    "author": "tazmanian10",
    "comments": [
      {
        "user": "dio",
        "created_at": "2020-02-25T10:25:17Z",
        "body": "@yxue do you have any ideas on this?"
      },
      {
        "user": "tazmanian10",
        "created_at": "2020-02-25T10:45:40Z",
        "body": "I think that it would be better to add multiple CA through a list, and not through the same string/file. Also the documentation should be updated in order to be clear about the current handling."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-03-30T08:52:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-06T09:00:54Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10141,
    "title": "How to build on windows？",
    "created_at": "2020-02-22T19:07:15Z",
    "closed_at": "2020-04-02T17:15:34Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10141",
    "body": "I have seen the powershell script，but I dont see any docs about windows build？\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10141/comments",
    "author": "engineer1109",
    "comments": [
      {
        "user": "engineer1109",
        "created_at": "2020-02-25T16:41:41Z",
        "body": "I try to powershell        .\\ci\\do_ci.ps1 bazel.release\r\nBut\r\n ERROR: Process exited with status 127\r\n/bin/bash: bazelget_workspace_status: No such file or directory\r\nTarget //source/exe:envoy-static failed to build"
      },
      {
        "user": "engineer1109",
        "created_at": "2020-02-25T17:27:28Z",
        "body": " ./ci/windows_ci_steps.sh\r\nhas compiled successfully\r\nbut no envoy.exe or other useful things"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-03-26T17:13:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-02T17:15:33Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 10083,
    "title": " Exchange data for one-way network",
    "created_at": "2020-02-18T00:16:36Z",
    "closed_at": "2020-03-02T05:40:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10083",
    "body": "i want to exchange data for one-way network.\r\neg. extranet ip A is 114.100.100.100, intranet ip B is 192.168.1.1,\r\ni want to send data from A to B, and B to A.\r\ncan envoy be implemented ？\r\nthanks !",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10083/comments",
    "author": "QiusHosens",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2020-02-18T06:11:43Z",
        "body": "What do you mean by one-way network? B is behind NAT?"
      },
      {
        "user": "QiusHosens",
        "created_at": "2020-02-18T08:59:20Z",
        "body": "yes, only B can access A, A can't access B"
      },
      {
        "user": "QiusHosens",
        "created_at": "2020-02-19T02:29:59Z",
        "body": "of course, B can connect to A with socket.\r\ni mean B can ping A, A can't ping B"
      },
      {
        "user": "QiusHosens",
        "created_at": "2020-02-19T02:31:16Z",
        "body": "> What do you mean by one-way network? B is behind NAT?\r\n\r\nyes. B is behind NAT, i mean B can ping A, A can't ping B. B can connect to A with socket."
      },
      {
        "user": "lizan",
        "created_at": "2020-02-20T01:15:55Z",
        "body": "@QiusHosens What is the protocol are you using, where do you want to deploy Envoy? Generally it depends on the connectivity between A <-> Envoy, and B <-> Envoy, so if you deploy Envoy where NAT happens it could work."
      },
      {
        "user": "QiusHosens",
        "created_at": "2020-02-20T01:49:55Z",
        "body": "@lizan i want to use http, but i cannot deploy Envoy where NAT happens, only can deploy Envoy on A and B, can it work?"
      }
    ]
  },
  {
    "number": 9981,
    "title": "Secured upstream request",
    "created_at": "2020-02-09T13:25:36Z",
    "closed_at": "2020-03-18T17:47:02Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9981",
    "body": "Is there a way to secure the upstream flow. I have a requirement where I need to send to upstream server with OAUTH2 Authorization as upstream servers are secured and can be accessed with only OATH2 Code Grant access.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9981/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-03-11T17:42:16Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-03-18T17:47:01Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9939,
    "title": "Envoy performance when fetching for large certificate files via SDS",
    "created_at": "2020-02-05T18:01:38Z",
    "closed_at": "2020-03-14T00:22:36Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9939",
    "body": "Hi,\r\n\r\nI got a question about any limitation for response size when sending large response from SDS to envoy listener.  Here's my use case: let's say I have an envoy listener that dynamically fetches TLS certs via SDS, which might be sending ~1-3K certs periodically (whenever those are updated) with an approx 40 mb of data, are there any configurations I need to tweak to be able to send this data to be able to avoid response too large errors on the envoy side? Any insights for docs and/or best practices will be greatly appreciated. \r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9939/comments",
    "author": "nmalika5",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-03-07T00:21:06Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-03-14T00:22:35Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9937,
    "title": "Too many Host headers caused via lua headers():add(\":a\",\"value\")",
    "created_at": "2020-02-05T13:53:45Z",
    "closed_at": "2020-02-06T07:39:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9937",
    "body": "*Description*:\r\n\r\nThe lua filters makes it possible to add headers which are available\r\nonly for access logs by adding ':' character to them. However if this\r\nkind of \"hidden\" header starts with \":a\" it is used as \":authority\"\r\nand this causes multiple \"host\" headers to appear with HTTP/1.X\r\nrequests. This in turns causes error with at least golang based http server:\r\n\"HTTP/1.1 400 Bad Request: too many Host headers\".\r\n\r\nWith the envoy virtual hosts routes config it is not possible to\r\nmodify the \":\" prefixed headers but it's nicely possible via lua api.\r\n\r\nThe use of \":\" started headers via lua api may be an unplanned feature\r\nbut it makes it possible to have these hidden/log-only headers like\r\nthe Nginx supports via variables. These variables/hidden headers are\r\nvery convenient especially when using external authorizers or other\r\nexternal subrequests where one needs to log the subrequest result\r\nheaders as part of access logs but not pass all of them to the\r\nupstream.\r\n\r\n*Proposed fix*:\r\n\r\nEnvoy should copy only the \":authority\" header as HTTP/1.X \"host\" header and not all of the \":a*\" headers.\r\n\r\n*Proposed new feature*:\r\n\r\nAdd documented support for using hidden/access-log only headers.\r\n\r\n*Repro steps*:\r\n- Use the configuration below\r\n- Have upstream server ready (written in go), or some other server but verify headers with tcpdump.\r\n- make a request to the envoy\r\n- notice that the server returns 400 error (you can verify the issue by using tcpdump to capture the traffic).\r\n\r\n*Config*:\r\n\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8081\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n          codec_type: auto\r\n          stat_prefix: proxy_http\r\n          http_protocol_options:\r\n            accept_http_10: true\r\n          #Disabled delayed close for the http/1.0 (and all other...)\r\n          delayed_close_timeout: 0.0s\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\r\n              path: /var/log/envoy/envoy-access.log\r\n              format: \"[%START_TIME%] \\\r\n                       host=\\\"%REQ(:AUTHORITY)%\\\" \\\r\n                       m=\\\"%REQ(:METHOD)%\\\" \\\r\n                       st=\\\"%RESPONSE_CODE%\\\" \\\r\n                       ahidden=\\\"%REQ(:AHIDDEN)%\\\" \\\r\n                       bhidden=\\\"%REQ(:BHIDDEN)%\\\" \\\r\n                       \\\"\\n\"\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  path: /envoy-status\r\n                direct_response:\r\n                  status: 200\r\n                  body:\r\n                    inline_string: OK\r\n              - match:\r\n                  prefix: /\r\n                route:\r\n                  cluster: upstream\r\n          http_filters:\r\n          - name: envoy.lua\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.config.filter.http.lua.v2.Lua\r\n              #################################################################\r\n              inline_code: |\r\n                function envoy_on_request(handle)\r\n                     -- testing request header modifications,\r\n                     -- adding a \"hidden\" log only header starting with \":a\" will be used\r\n                     -- as \":authority\" causing \"HTTP/1.1 400 Bad Request: too many Host headers\" error\r\n                     -- Example headers caused by this example:\r\n                     --     host: localhost:8081\r\n                     --     host: a-value\r\n                     handle:headers():replace(\":ahidden\", \"a-value\")\r\n                     handle:headers():replace(\":bhidden\", \"b-value\")\r\n                end \r\n              ##########################################################\r\n          - name: envoy.router\r\n            typed_config: {}\r\n  clusters:\r\n  - name: upstream\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: upstream\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: upstream\r\n                port_value: 9090\r\n### LISTENER FOR ADMIN API\r\nadmin:\r\n  access_log_path: \"/var/log/envoy/envoy-admin.log\"\r\n  address:\r\n    socket_address:\r\n      address: 127.0.0.1\r\n      port_value: 8001\r\n\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9937/comments",
    "author": "jusmaki",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-02-05T17:25:58Z",
        "body": "You are relying on undocumented behavior which I don't think we are going to change. Can you use dynamic metadata for this? IIRC it can also be logged. cc @dio "
      },
      {
        "user": "jusmaki",
        "created_at": "2020-02-06T07:39:57Z",
        "body": "Yes, the dynamic metadata works perfectly for this case. Thanks for the the tip. "
      }
    ]
  },
  {
    "number": 9917,
    "title": "Route Match overhead",
    "created_at": "2020-02-03T23:10:35Z",
    "closed_at": "2020-03-12T16:02:55Z",
    "labels": [
      "question",
      "stale",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9917",
    "body": "We are trying to study the overhead introduced in envoy when there are multiple routes (say 1000) compared to just 1 route. One of the metrics I am using is the duration (Total duration in milliseconds of the request from the first byte read from the upstream host to the last byte sent downstream.). Duration metric is provided by the envoy. \r\n\r\nMy assumption is duration includes time taken by envoy to pick the correct route from the given routes. Can somebody please confirm if this assumption is correct.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9917/comments",
    "author": "pitiwari",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-03-05T15:40:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-03-12T16:02:54Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9907,
    "title": "stats_integration_test failed",
    "created_at": "2020-02-02T05:40:30Z",
    "closed_at": "2020-03-16T04:21:46Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9907",
    "body": "When run `bazel test //test/... --config=libc++`,   stats_integration_test failed because some segmentation fault.\r\n\r\nEnv:\r\n* clang 7 & libc++\r\n* Ubuntu 18.04\r\n\r\nVer:\r\n* 1.12.2\r\n\r\n\r\nLog:\r\n```\r\n[ RUN      ] IpVersions/ClusterMemoryTestRunner.MemoryLargeHostSizeWithStats/IPv6\r\n[warn] evutil_make_internal_pipe_: pipe: Too many open files\r\n[warn] event_base_new_with_config: Unable to make base notifiable.\r\n[2020-02-02 05:22:03.632][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:83] Caught Segmentation fault, suspect faulting address 0x1b0\r\n[2020-02-02 05:22:03.632][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:70] Backtrace (use tools/stack_decode.py to get line numbers):\r\n[2020-02-02 05:22:03.632][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:71] Envoy version: 0/1.14.0-dev/redacted/DEBUG/BoringSSL\r\n[2020-02-02 05:22:03.650][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #0: Envoy::SignalAction::sigHandler() [0x33e46e1]\r\n[2020-02-02 05:22:03.650][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #1: __restore_rt [0x7fa59b683890]\r\n[2020-02-02 05:22:03.667][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #2: evwatch_prepare_new [0x33c660f]\r\n[2020-02-02 05:22:03.684][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #3: Envoy::Event::LibeventScheduler::registerOnPrepareCallback() [0x2feee46]\r\n[2020-02-02 05:22:03.703][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #4: Envoy::Event::DispatcherImpl::DispatcherImpl() [0x2eea4b0]\r\n[2020-02-02 05:22:03.721][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #5: Envoy::Event::DispatcherImpl::DispatcherImpl() [0x2ee9708]\r\n[2020-02-02 05:22:03.738][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #6: Envoy::Api::Impl::allocateDispatcher() [0x2ee8cf2]\r\n[2020-02-02 05:22:03.757][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #7: Envoy::FakeUpstream::FakeUpstream() [0x12bb852]\r\n[2020-02-02 05:22:03.775][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #8: Envoy::FakeUpstream::FakeUpstream() [0x12bd274]\r\n[2020-02-02 05:22:03.793][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #9: Envoy::BaseIntegrationTest::createUpstreams() [0x12fe29f]\r\n[2020-02-02 05:22:03.810][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #10: Envoy::BaseIntegrationTest::initialize() [0x12fbe86]\r\n[2020-02-02 05:22:03.826][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #11: Envoy::(anonymous namespace)::ClusterMemoryTestHelper::clusterMemoryHelper() [0x1258d04]\r\n[2020-02-02 05:22:03.842][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #12: Envoy::(anonymous namespace)::ClusterMemoryTestHelper::computeMemoryDelta() [0x1258ad5]\r\n[2020-02-02 05:22:03.860][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #13: Envoy::(anonymous namespace)::ClusterMemoryTestRunner_MemoryLargeHostSizeWithStats_Test::TestBody() [0x125a930]\r\n[2020-02-02 05:22:03.878][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #14: testing::internal::HandleSehExceptionsInMethodIfSupported<>() [0x481e93e]\r\n[2020-02-02 05:22:03.897][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #15: testing::internal::HandleExceptionsInMethodIfSupported<>() [0x48099fb]\r\n[2020-02-02 05:22:03.917][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #16: testing::Test::Run() [0x47e1933]\r\n[2020-02-02 05:22:03.936][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #17: testing::TestInfo::Run() [0x47e2b12]\r\n[2020-02-02 05:22:03.954][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #18: testing::TestSuite::Run() [0x47e3a67]\r\n[2020-02-02 05:22:03.975][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #19: testing::internal::UnitTestImpl::RunAllTests() [0x47fc835]\r\n[2020-02-02 05:22:03.996][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #20: testing::internal::HandleSehExceptionsInMethodIfSupported<>() [0x4821c0e]\r\n[2020-02-02 05:22:04.013][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #21: testing::internal::HandleExceptionsInMethodIfSupported<>() [0x480c67b]\r\n[2020-02-02 05:22:04.032][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #22: testing::UnitTest::Run() [0x47fc14f]\r\n[2020-02-02 05:22:04.054][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #23: RUN_ALL_TESTS() [0x2de25d1]\r\n[2020-02-02 05:22:04.076][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #24: Envoy::TestRunner::RunTests() [0x2ddfb98]\r\n[2020-02-02 05:22:04.097][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #25: main [0x2ddd4f8]\r\n[2020-02-02 05:22:04.097][16][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #26: __libc_start_main [0x7fa59b2a1b97]\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 310:    16 Segmentation fault      \"${TEST_PATH}\" \"$@\" 2>&1\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9907/comments",
    "author": "wbpcode",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2020-02-03T16:19:41Z",
        "body": "It looks like you are running into the limit on the number of open file descriptors. Check the `ulimit -n` before running the test. If it something low, like 1024 you may need to make it higher to avoid this problem."
      },
      {
        "user": "wbpcode",
        "created_at": "2020-02-08T03:24:40Z",
        "body": "> It looks like you are running into the limit on the number of open file descriptors. Check the `ulimit -n` before running the test. If it something low, like 1024 you may need to make it higher to avoid this problem.\r\n\r\nThanks for your kind reply. Infact I change the value of `ulimit -n` to `4096`, and it still failed.  "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-03-09T03:55:41Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-03-16T04:21:45Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9904,
    "title": "help(build): ./ci/do_ci.sh: Permission denied",
    "created_at": "2020-02-01T13:34:44Z",
    "closed_at": "2020-02-05T05:28:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9904",
    "body": "when I try to build a dev version, I got this error:\r\n\r\n```shell\r\n[root@instance-1 envoy-1.13.0]# pwd\r\n/root/envoy-1.13.0\r\n[root@instance-1 envoy-1.13.0]# ./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.dev'\r\nbash: ./ci/do_ci.sh: Permission denied\r\n[root@instance-1 envoy-1.13.0]# ll ./ci/do_ci.sh\r\n-rwxrwxrwx. 1 root root 14138 Jan 20 22:06 ./ci/do_ci.sh\r\n\r\n[root@instance-1 envoy-1.13.0]# uname -r\r\n3.10.0-1062.9.1.el7.x86_64\r\n[root@instance-1 envoy-1.13.0]# cat /etc/redhat-release\r\nCentOS Linux release 7.7.1908 (Core)\r\n\r\n[root@instance-1 envoy-1.13.0]# docker version\r\nClient:\r\n Version:         1.13.1\r\n API version:     1.26\r\n Package version: docker-1.13.1-108.git4ef4b30.el7.centos.x86_64\r\n Go version:      go1.10.3\r\n Git commit:      4ef4b30/1.13.1\r\n Built:           Tue Jan 21 17:16:25 2020\r\n OS/Arch:         linux/amd64\r\n\r\nServer:\r\n Version:         1.13.1\r\n API version:     1.26 (minimum version 1.12)\r\n Package version: docker-1.13.1-108.git4ef4b30.el7.centos.x86_64\r\n Go version:      go1.10.3\r\n Git commit:      4ef4b30/1.13.1\r\n Built:           Tue Jan 21 17:16:25 2020\r\n OS/Arch:         linux/amd64\r\n Experimental:    false\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9904/comments",
    "author": "membphis",
    "comments": [
      {
        "user": "kylebevans",
        "created_at": "2020-02-01T17:46:47Z",
        "body": "Try this and then build again:\r\n\r\n`mount -o remount,exec /tmp`\r\n\r\nThe run_envoy_docker.sh creates a build dir in /tmp and I bet CentOS mounts tmp with the noexec flag to disallow executing from /tmp for security reasons."
      },
      {
        "user": "membphis",
        "created_at": "2020-02-03T01:27:52Z",
        "body": "It works fine under Ubuntu OS  18.04 :(\r\n\r\nIt seems I have to change my working OS. "
      },
      {
        "user": "kylebevans",
        "created_at": "2020-02-04T15:14:11Z",
        "body": "@membphis glad you got it working. Do you want to close the issue?"
      },
      {
        "user": "membphis",
        "created_at": "2020-02-05T05:28:49Z",
        "body": "many thx"
      }
    ]
  },
  {
    "number": 9876,
    "title": "Envoy SSL/TLS Performance",
    "created_at": "2020-01-30T01:09:45Z",
    "closed_at": "2020-03-04T23:54:02Z",
    "labels": [
      "question",
      "area/tls",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9876",
    "body": "Hi! I'm looking for some insights into Envoy performance when it comes to having thousands of TLS certificates (5-10K, for example) on a single listener. I'm planning on using SDS to dynamically poll that data. Any insight will be helpful!\r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9876/comments",
    "author": "nmalika5",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-02-29T15:55:37Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "nmalika5",
        "created_at": "2020-03-04T23:54:02Z",
        "body": "Following on that issue. I was able to test it out with 10K certs using static configuration on a single envoy listener. It took ~12s for listener to load with total occupied memory as 614mb. "
      }
    ]
  },
  {
    "number": 9794,
    "title": "Multiple validation_context's",
    "created_at": "2020-01-23T08:18:12Z",
    "closed_at": "2020-01-29T02:11:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9794",
    "body": "Is there a way to setup multiple validation_context's with Envoy, particularly for TCP traffic not HTTP?\r\n\r\nWe'd like to provide a single URL, and use mTLS w/ ~3-10 other external systems. We can't influence the other systems to set unique headers, domains, SNI, or anything else.\r\n\r\nWe could associate their CA certs with their hostname/IP to match on based on that, or we could eat the cost of trying every cert till one matched.\r\n\r\nOr Is this possible to do at the moment leveraging filter chains?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9794/comments",
    "author": "steeling",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-01-26T19:03:15Z",
        "body": "As long as you can match on hostname/IP I think you can do this with filter chains? cc @lambdai @PiotrSikora "
      },
      {
        "user": "PiotrSikora",
        "created_at": "2020-01-26T23:16:39Z",
        "body": "Yes, you could use multiple filter chains to do that, but the cost of matching to a particular certificate is negligible (it's just a quick comparison of hashes), so unless you need/want to segregate traffic from those external systems for some reason (it doesn't sound like you do), the best and easiest solution would be to simply provide `trusted_ca` that contains all the trusted CA certificates or provide multiple valid values in `match_subject_alt_names`."
      },
      {
        "user": "steeling",
        "created_at": "2020-01-28T03:03:49Z",
        "body": "can we match on client IP or hostname?\r\n\r\nAll of the traffic will come to a single hostname. At the moment we don't have any influence on enforcing a specific CA to use, so we first request their CA cert with us, and we add it to a list we will verify against"
      },
      {
        "user": "PiotrSikora",
        "created_at": "2020-01-28T03:17:07Z",
        "body": "Yes, you can match on the client (source) IP address or subnet.\r\n\r\nYou cannot match on the client's hostname."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-01-28T16:49:19Z",
        "body": "> You cannot match on the client's hostname.\r\n\r\nJust to clarify, \"hostname\" is an L7/HTTP concept. You can match on SNI."
      },
      {
        "user": "PiotrSikora",
        "created_at": "2020-01-28T17:00:29Z",
        "body": "> Just to clarify, \"hostname\" is an L7/HTTP concept. You can match on SNI.\r\n\r\nHe means client's hostname (reverse DNS of client's IP address), not SNI / `Host` / `:authority`."
      },
      {
        "user": "steeling",
        "created_at": "2020-01-29T02:11:08Z",
        "body": "Thanks all, that helps!"
      }
    ]
  },
  {
    "number": 9793,
    "title": "Envoy routing to statefulsets",
    "created_at": "2020-01-23T08:12:37Z",
    "closed_at": "2020-03-02T02:29:49Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9793",
    "body": "Does envoy support anyway of mapping a URL to a k8's statefulset, based on the domain name.\r\n\r\nAn example would be a rule that could translate\r\n`X.my-domain.com -> X.myservice.default.svc.cluster.local`",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9793/comments",
    "author": "steeling",
    "comments": [
      {
        "user": "steeling",
        "created_at": "2020-01-23T08:20:46Z",
        "body": "also to note that this is TCP traffic, not HTTP"
      },
      {
        "user": "snowp",
        "created_at": "2020-01-24T00:36:39Z",
        "body": "This kind of mapping is possible with Envoy, but Envoy itself doesn't have any knowledge about Kubernetes. You'll probably want to look into control planes that integrate Kubernetes with Envoy, for example Istio or Kuma. "
      },
      {
        "user": "snowp",
        "created_at": "2020-01-24T00:38:38Z",
        "body": "If you're trying to do this kind of mapping on your own you'd just need to generate a virtual host with the desired domain and route that to a cluster that specifies the target (using STRICT_DNS would probably be the easiest). "
      },
      {
        "user": "steeling",
        "created_at": "2020-01-25T01:19:11Z",
        "body": "Ah, to clarify we would like to do this without specifying each domain individually, such that we could set up autoscaling.\r\n\r\nIt would be nice to use a regex rule to take (*).<my-domain> and use the capture group for the result.\r\n\r\nIt's ok if we had to do this in multiple steps with one filter that extracts the data, and places it in dynamic_metadata, and another that routes on that info"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-24T01:44:48Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-03-02T02:29:48Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9759,
    "title": "SNI info is hard requirement for SSL passthrough?",
    "created_at": "2020-01-21T16:52:58Z",
    "closed_at": "2020-02-29T01:29:45Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9759",
    "body": "*Title*: *SNI info is hard requirement for SSL passthrough?*\r\n\r\n*Description*:\r\nfor the SSL passthrough VIPs, i know SNI information need to provided in client hello packet. but since we have some legacy partner which might use quite old lib like even java 1.5, is there some other option or workaround i can go? or is it the mandatory requirement\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9759/comments",
    "author": "Jefle2005",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-02-22T00:30:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-29T01:29:44Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9717,
    "title": "Ratelimit configuration: extract multiple headers and send a tuple of descriptors",
    "created_at": "2020-01-17T11:46:08Z",
    "closed_at": "2020-02-29T02:29:46Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9717",
    "body": "Hi,\r\n\r\nI am looking to extract values from multiple headers and send it as a tuple of descriptors to the rate limit service,  but I only seem to be able to use `request_headers` once; I have tried both\r\n\r\n\r\n```\r\nrate_limits:\r\n- actions:\r\n - {request_headers: {\"header_name\": \"plan\",\"descriptor_key\": \"plan\"}}\r\n- actions:\r\n - {request_headers: {\"header_name\": \"clientId\",\"descriptor_key\": \"clientId\"}}\r\n```\r\n\r\nor\r\n\r\n```\r\nrate_limits:\r\n- actions:\r\n - {request_headers: {\"header_name\": \"plan\",\"descriptor_key\": \"plan\"}}\r\n - {request_headers: {\"header_name\": \"clientId\",\"descriptor_key\": \"clientId\"}}\r\n```\r\n\r\nWith either configuration, I only see one of the `descriptor_key` in the ratelimit service.\r\n\r\nThanks,\r\nBrent",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9717/comments",
    "author": "qbrent",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-02-22T01:30:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-29T02:29:45Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "tricky42",
        "created_at": "2021-11-19T09:38:46Z",
        "body": "@qbrent I am searching for a solution for very similar requirements. Did you find a solution? "
      }
    ]
  },
  {
    "number": 9700,
    "title": "Dubbo connection is temporarily interrupted when the configuration is updated.",
    "created_at": "2020-01-16T08:42:05Z",
    "closed_at": "2020-02-27T17:25:17Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9700",
    "body": "*Title*: *Dubbo connection is temporarily interrupted when the configuration is updated.*\r\n\r\n*Description*:\r\n>Each time I modify the dubbo_proxy configuration in the envoy, the listener containing the filter is updated. According to the listener update mechanism of the LDS, the old listener enters the draining state and waits to be removed, and the new listener enters the warming state and then the active state. However, when the old listener is removed, the new listener does not take effect immediately. It takes about 30 seconds to start the new listener. The dubbo service is unavailable within 30 seconds.\r\n\r\n*Envoy logs*:\r\n>[2019-12-31 02:40:30.998][17][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.105.248.91_20880 hash=17882840085526265571\r\n(Some details of the new listener...)\r\n[2019-12-31 02:40:31.005][17][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] add warming listener: name=10.105.248.91_20880, hash=17882840085526265571, address=10.105.248.91:20880\r\n[2019-12-31 02:40:31.005][17][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] warm complete. updating active listener: name=10.105.248.91_20880, hash=17882840085526265571, address=10.105.248.91:20880\r\n[2019-12-31 02:40:31.005][17][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] draining listener: name=10.105.248.91_20880, hash=16505751281972299251, address=10.105.248.91:20880\r\n\r\n(In 45 seconds...)\r\n>[2019-12-31 02:41:15.043][17][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] removing listener: name=10.105.248.91_20880, hash=16505751281972299251, address=10.105.248.91:20880\r\n[2019-12-31 02:41:15.043][23][debug][connection] [external/envoy/source/common/network/connection_impl.cc:101] [C297] closing data_to_write=0 type=1\r\n[2019-12-31 02:41:15.043][23][debug][connection] [external/envoy/source/common/network/connection_impl.cc:190] [C297] closing socket: 1\r\n[2019-12-31 02:41:15.043][23][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/conn_manager.cc:31] **********ConnectionManager::~ConnectionManager\r\n[2019-12-31 02:41:15.043][17][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] listener removal complete: name=10.105.248.91_20880, hash=16505751281972299251, address=10.105.248.91:20880\r\n\r\n(In 30 seconds...)\r\n>[2019-12-31 02:41:45.616][23][debug][config] [external/envoy/source/extensions/filters/network/dubbo_proxy/config.cc:136] **********ConfigImpl::createProtocol\r\n[2019-12-31 02:41:45.616][23][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/conn_manager.cc:27] **********ConnectionManager::ConnectionManager\r\n\r\n*Dubbo logs*:\r\n>2020-01-16 08:27:36.140 ERROR 1 --- [nio-9080-exec-1] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is org.apache.dubbo.rpc.RpcException: Failed to invoke remote method: recommend, provider: recommendation:20880/com.popomen.dabb.recommendation.api.Recommendation?application=forecast&check=false&generic=false&interface=com.popomen.dabb.recommendation.api.Recommendation&lazy=false&pid=1&register.ip=10.244.3.38&remote.application=&revision=1.0.1-SNAPSHOT&side=consumer&sticky=false&timeout=1000000, cause: message can not send, because channel is closed . url:recommendation:20880/com.popomen.dabb.recommendation.api.Recommendation?application=forecast&check=false&codec=dubbo&generic=false&heartbeat=60000&interface=com.popomen.dabb.recommendation.api.Recommendation&lazy=false&pid=1&register.ip=10.244.3.38&remote.application=&revision=1.0.1-SNAPSHOT&side=consumer&sticky=false&timeout=1000000] with root cause\r\norg.apache.dubbo.remoting.RemotingException: message can not send, because channel is closed . url:recommendation:20880/com.popomen.dabb.recommendation.api.Recommendation?application=forecast&check=false&codec=dubbo&generic=false&heartbeat=60000&interface=com.popomen.dabb.recommendation.api.Recommendation&lazy=false&pid=1&register.ip=10.244.3.38&remote.application=&revision=1.0.1-SNAPSHOT&side=consumer&sticky=false&timeout=1000000",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9700/comments",
    "author": "popomen",
    "comments": [
      {
        "user": "bingohuang",
        "created_at": "2020-01-16T08:48:41Z",
        "body": "+1 Same problem."
      },
      {
        "user": "zyfjeff",
        "created_at": "2020-01-16T08:49:52Z",
        "body": "@popomen Could you upload the whole envoy trace log?"
      },
      {
        "user": "zyfjeff",
        "created_at": "2020-01-16T08:50:05Z",
        "body": "/assign @zyfjeff "
      },
      {
        "user": "popomen",
        "created_at": "2020-01-16T09:13:20Z",
        "body": "[2020-01-16 09:00:10.116][19][debug][config] [external/envoy/source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-01-16T09:00:10Z/32\r\n[2020-01-16 09:00:10.116][19][debug][config] [external/envoy/source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-01-16 09:00:10.116][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:746] begin remove listener: name=6e8bf41d-1b3f-4f30-867b-9ce680bff7e8\r\n[2020-01-16 09:00:10.116][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:754] unknown/locked listener '6e8bf41d-1b3f-4f30-867b-9ce680bff7e8'. no remove\r\n[2020-01-16 09:00:10.117][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.244.3.38_9080 hash=7673960929868624811\r\n[2020-01-16 09:00:10.117][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.244.3.38_9080'. no add/update\r\n[2020-01-16 09:00:10.117][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.244.3.38_15020 hash=18240697577002038327\r\n[2020-01-16 09:00:10.117][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.244.3.38_15020'. no add/update\r\n[2020-01-16 09:00:10.117][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.109.192.161_15029 hash=14165833222769574961\r\n[2020-01-16 09:00:10.117][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.109.192.161_15029'. no add/update\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.100.116.38_6379 hash=7766513077376879158\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.100.116.38_6379'. no add/update\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.98.104.95_20881 hash=4661663114897764\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.98.104.95_20881'. no add/update\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.109.192.161_15443 hash=9010525812596187967\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.109.192.161_15443'. no add/update\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.106.25.97_20880 hash=16274811877132228350\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.106.25.97_20880'. no add/update\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.109.192.161_443 hash=10593898723255843122\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.109.192.161_443'. no add/update\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.103.211.47_443 hash=6371664084190592652\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.103.211.47_443'. no add/update\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.109.192.161_15032 hash=15369889457170035058\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.109.192.161_15032'. no add/update\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.96.0.1_443 hash=13076706859066400394\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.96.0.1_443'. no add/update\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.108.184.209_9000 hash=9539425944796744133\r\n[2020-01-16 09:00:10.118][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.108.184.209_9000'. no add/update\r\n[2020-01-16 09:00:10.119][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.109.192.161_31400 hash=4062997972212896809\r\n[2020-01-16 09:00:10.119][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.109.192.161_31400'. no add/update\r\n[2020-01-16 09:00:10.119][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.111.56.30_20881 hash=12862834683743267431\r\n[2020-01-16 09:00:10.119][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.111.56.30_20881'. no add/update\r\n[2020-01-16 09:00:10.119][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.105.248.91_20880 hash=345980053679949872\r\n[2020-01-16 09:00:10.119][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:57]   filter #0:\r\n[2020-01-16 09:00:10.119][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:58]     name: mixer\r\n[2020-01-16 09:00:10.119][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:61]   config: {}\r\n[2020-01-16 09:00:10.125][19][debug][config] [src/envoy/utils/mixer_control.cc:173] ExtractInfo node metadata missing:NODE_UID fields {\r\n...\r\n[2020-01-16 09:00:10.125][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:57]   filter #1:\r\n[2020-01-16 09:00:10.125][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:58]     name: envoy.filters.network.dubbo_proxy\r\n[2020-01-16 09:00:10.125][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:61]   config: {}\r\n[2020-01-16 09:00:10.126][19][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/router/route_matcher.cc:31] dubbo route matcher: weighted_clusters_size 2\r\n[2020-01-16 09:00:10.126][19][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/router/route_matcher.cc:186] dubbo route matcher: routes list size 1\r\n[2020-01-16 09:00:10.126][19][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/router/route_matcher.cc:31] dubbo route matcher: weighted_clusters_size 2\r\n[2020-01-16 09:00:10.126][19][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/router/route_matcher.cc:186] dubbo route matcher: routes list size 1\r\n[2020-01-16 09:00:10.126][19][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/router/route_matcher.cc:219] route matcher list size 2\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/extensions/filters/network/dubbo_proxy/config.cc:110] using default router filter\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/extensions/filters/network/dubbo_proxy/config.cc:144]     dubbo filter #0\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/extensions/filters/network/dubbo_proxy/config.cc:145]       name: envoy.filters.dubbo.router\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/extensions/filters/network/dubbo_proxy/config.cc:149]       config: {}\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] add warming listener: name=10.105.248.91_20880, hash=345980053679949872, address=10.105.248.91:20880\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] line:608 after new_listener_ref.initialize();: name=10.105.248.91_20880, hash=345980053679949872, address=10.105.248.91:20880\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] warm complete. updating active listener: name=10.105.248.91_20880, hash=345980053679949872, address=10.105.248.91:20880\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] draining listener: name=10.105.248.91_20880, hash=11155164242519234785, address=10.105.248.91:20880\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] line:610 after new_listener_ref.initialize();: name=10.105.248.91_20880, hash=345980053679949872, address=10.105.248.91:20880\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.109.192.161_15031 hash=16855581072399535129\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.109.192.161_15031'. no add/update\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.96.0.10_53 hash=10591784934296408525\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.96.0.10_53'. no add/update\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.106.208.170_443 hash=14544952546773332196\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.106.208.170_443'. no add/update\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.98.151.43_443 hash=7346127626103902085\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.98.151.43_443'. no add/update\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.109.192.161_15030 hash=13158222765463639604\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.109.192.161_15030'. no add/update\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.99.31.162_15011 hash=15180998548318150532\r\n[2020-01-16 09:00:10.126][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.99.31.162_15011'. no add/update\r\n[2020-01-16 09:00:10.127][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.98.151.43_15443 hash=4346197053810508196\r\n[2020-01-16 09:00:10.127][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.98.151.43_15443'. no add/update\r\n[2020-01-16 09:00:10.127][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_15014 hash=15866336662325353105\r\n[2020-01-16 09:00:10.127][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_15014'. no add/update\r\n[2020-01-16 09:00:10.128][26][debug][config] [src/envoy/utils/mixer_control.cc:173] ExtractInfo node metadata missing:NODE_UID fields {\r\n...\r\n[2020-01-16 09:00:10.128][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.99.106.29_20881 hash=17638728219850765834\r\n[2020-01-16 09:00:10.128][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.99.106.29_20881'. no add/update\r\n[2020-01-16 09:00:10.128][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_15010 hash=1075885914505452426\r\n[2020-01-16 09:00:10.128][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_15010'. no add/update\r\n[2020-01-16 09:00:10.129][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.110.97.53_14268 hash=6338815714274005705\r\n[2020-01-16 09:00:10.129][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.110.97.53_14268'. no add/update\r\n[2020-01-16 09:00:10.129][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_8000 hash=8571983533748194430\r\n[2020-01-16 09:00:10.129][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_8000'. no add/update\r\n[2020-01-16 09:00:10.129][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_9555 hash=8840462166335704804\r\n[2020-01-16 09:00:10.129][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_9555'. no add/update\r\n[2020-01-16 09:00:10.129][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_9901 hash=6769688667242573111\r\n[2020-01-16 09:00:10.129][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_9901'. no add/update\r\n[2020-01-16 09:00:10.130][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_5000 hash=15861050667060502850\r\n[2020-01-16 09:00:10.130][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_5000'. no add/update\r\n[2020-01-16 09:00:10.130][28][debug][config] [src/envoy/utils/mixer_control.cc:173] ExtractInfo node metadata missing:NODE_UID fields {\r\n...\r\n[2020-01-16 09:00:10.130][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.110.97.53_14250 hash=11265269227007987849\r\n[2020-01-16 09:00:10.130][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.110.97.53_14250'. no add/update\r\n[2020-01-16 09:00:10.130][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_8080 hash=3399518195252606799\r\n[2020-01-16 09:00:10.130][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_8080'. no add/update\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.109.192.161_15020 hash=2765190426485451733\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.109.192.161_15020'. no add/update\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_5050 hash=12814542077937330149\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_5050'. no add/update\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_80 hash=17739306711842156344\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_80'. no add/update\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_3000 hash=16331582303951039843\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_3000'. no add/update\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_20001 hash=18050329859934360232\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_20001'. no add/update\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_3550 hash=6699166396510953600\r\n[2020-01-16 09:00:10.131][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_3550'. no add/update\r\n[2020-01-16 09:00:10.132][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_7000 hash=11224131873176027840\r\n[2020-01-16 09:00:10.132][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_7000'. no add/update\r\n[2020-01-16 09:00:10.132][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.111.92.220_42422 hash=4444067627767461361\r\n[2020-01-16 09:00:10.132][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.111.92.220_42422'. no add/update\r\n[2020-01-16 09:00:10.132][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_9090 hash=12384742660311281267\r\n[2020-01-16 09:00:10.132][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_9090'. no add/update\r\n[2020-01-16 09:00:10.133][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.98.225.169_16686 hash=204311811491975480\r\n[2020-01-16 09:00:10.133][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.98.225.169_16686'. no add/update\r\n[2020-01-16 09:00:10.133][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=10.110.97.53_14267 hash=12394248318898437492\r\n[2020-01-16 09:00:10.133][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '10.110.97.53_14267'. no add/update\r\n[2020-01-16 09:00:10.133][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_9411 hash=12768722598159185680\r\n[2020-01-16 09:00:10.133][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_9411'. no add/update\r\n[2020-01-16 09:00:10.134][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_50051 hash=13502216266022967903\r\n[2020-01-16 09:00:10.134][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_50051'. no add/update\r\n[2020-01-16 09:00:10.134][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_15004 hash=16816452807393689446\r\n[2020-01-16 09:00:10.134][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_15004'. no add/update\r\n[2020-01-16 09:00:10.134][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_9091 hash=2210769168583554170\r\n[2020-01-16 09:00:10.134][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_9091'. no add/update\r\n[2020-01-16 09:00:10.134][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_8060 hash=9668735146788687059\r\n[2020-01-16 09:00:10.134][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_8060'. no add/update\r\n[2020-01-16 09:00:10.134][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_7070 hash=4375315244493903873\r\n[2020-01-16 09:00:10.134][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_7070'. no add/update\r\n[2020-01-16 09:00:10.135][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=0.0.0.0_9080 hash=8378079875167354930\r\n[2020-01-16 09:00:10.135][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener '0.0.0.0_9080'. no add/update\r\n[2020-01-16 09:00:10.142][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=virtualOutbound hash=15902848354568348468\r\n[2020-01-16 09:00:10.142][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener 'virtualOutbound'. no add/update\r\n[2020-01-16 09:00:10.143][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:499] begin add/update listener: name=virtualInbound hash=2824762179166537073\r\n[2020-01-16 09:00:10.143][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:510] duplicate/locked listener 'virtualInbound'. no add/update\r\n[2020-01-16 09:00:10.143][19][debug][config] [external/envoy/source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-01-16 09:00:10.143][19][debug][config] [external/envoy/source/common/config/grpc_mux_subscription_impl.cc:61] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 52 resources with version 2020-01-16T09:00:10Z/32\r\n"
      },
      {
        "user": "zyfjeff",
        "created_at": "2020-01-16T09:27:21Z",
        "body": "@popomen The logs look incomplete and I can't make a judgment"
      },
      {
        "user": "popomen",
        "created_at": "2020-01-16T09:32:44Z",
        "body": "[2020-01-16 09:00:54.124][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] removing listener: name=10.105.248.91_20880, hash=11155164242519234785, address=10.105.248.91:20880\r\n[2020-01-16 09:00:54.124][28][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/conn_manager.cc:31] **********ConnectionManager::~ConnectionManager\r\n[2020-01-16 09:00:54.124][19][debug][config] [external/envoy/source/server/listener_manager_impl.cc:381] listener removal complete: name=10.105.248.91_20880, hash=11155164242519234785, address=10.105.248.91:20880"
      },
      {
        "user": "popomen",
        "created_at": "2020-01-16T09:32:53Z",
        "body": "[2020-01-16 09:00:57.032][26][debug][config] [external/envoy/source/extensions/filters/network/dubbo_proxy/config.cc:136] **********ConfigImpl::createProtocol\r\n[2020-01-16 09:00:57.032][26][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/conn_manager.cc:27] **********ConnectionManager::ConnectionManager"
      },
      {
        "user": "zyfjeff",
        "created_at": "2020-01-16T11:42:21Z",
        "body": "@popomen Can you upload the full log file?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-20T17:14:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-27T17:25:16Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9689,
    "title": "Leak of information about cluster member",
    "created_at": "2020-01-15T15:44:24Z",
    "closed_at": "2020-01-17T14:47:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9689",
    "body": "Please, how do I retrieve the `envoy_cluster_internal_upstream_rq_xx` by cluster members?\r\n\r\nI have a cluster with 2 members... (load_assignment/lb_endpoints)... but I can't find any information to get errors from each cluster nodes.\r\n\r\nOne of these cluster members is buggy, but I can't find which node. No tip found.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9689/comments",
    "author": "hakuno",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2020-01-15T17:16:15Z",
        "body": "@hakuno does the `/clusters` endpoint provide you the information you need?"
      },
      {
        "user": "hakuno",
        "created_at": "2020-01-15T18:19:22Z",
        "body": "So I can read `rq_error` as bad HTTP status code. Nice!\r\n\r\n```\r\ncluster::node1::rq_active::0\r\ncluster::node1::rq_error::0\r\ncluster::node1::rq_success::94\r\ncluster::node1::rq_timeout::0\r\ncluster::node1::rq_total::94\r\n```\r\n\r\nHow do I read that with the Prometheus/Grafana?"
      },
      {
        "user": "htuch",
        "created_at": "2020-01-15T19:57:44Z",
        "body": "@hakuno I think that question is outside my domain knowledge, so I'll leave this open for others to comment on for the normal question period of time (2 weeks). "
      },
      {
        "user": "mattklein123",
        "created_at": "2020-01-16T01:19:33Z",
        "body": "Per-host stats are not currently exported to the stats sinks, due to cardinality issues."
      },
      {
        "user": "hakuno",
        "created_at": "2020-01-17T14:47:35Z",
        "body": "All right. Thanks!"
      }
    ]
  },
  {
    "number": 9661,
    "title": "Copying out from Buffer Instance to another one",
    "created_at": "2020-01-11T21:52:10Z",
    "closed_at": "2020-02-21T22:30:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9661",
    "body": "Hi,\r\n\r\nI've been looking for a way of copying data from a Buffer::OwnedImpl to another one. What I've done so far is below:\r\n\r\n```c++\r\nfunc(const Envoy::Buffer::Instance& origin_instance){\r\n  std::unique_ptr<uint8_t[]> buff(new uint8_t[1024]);\r\n  origin_instance.copyOut(0, 1024, buff.get());\r\n  target_instance.add(buff.get(),1024);\r\n}\r\n```\r\nHere I want to copy the first 1024 bytes of the `origin_instance` buffer to `target_instance`. However, I could not find a direct solution for that and applied a workaround using `buff`. The above code snipped copies the content two times (for copyOut and add separately). \r\n\r\nIs there a way to prevent the second copy by moving the content of buff directly or something else instead of using Instance::add here? \r\n\r\nThanks in advance,\r\nRaleigh.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9661/comments",
    "author": "cre4tiv3",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2020-01-13T15:42:22Z",
        "body": "Depending on your use case, you could use `getRawSlices()` to access the buffer directly and `add` those slices. That would avoid a second copy. Another option is `linearize()`, but that is potentially expensive. Often `move` makes more sense, which is an operation that `Buffer` supports natively, but depends on use case."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-14T21:45:37Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-21T22:30:52Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9644,
    "title": "Envoy logs `%ROUTE_NAME%` correctly, but also sets `%RESPONSE_FLAGS%` to `NR` and returns 503??",
    "created_at": "2020-01-10T01:38:50Z",
    "closed_at": "2020-01-13T01:20:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9644",
    "body": "We are running envoy as a multi-tenanted edge proxy, which is self-service. One of our customers configured some routes and found that one in particular returned a HTTP 503.\r\n\r\nWe have analyzed the route which was about as simple as:\r\n\r\n```yaml\r\nname: example-route\r\nmatch:\r\n  prefix: /abc-def-ghijklmno\r\nroute:\r\n  cluster: some-cluster\r\n```\r\n\r\nWe verified that the cluster name was valid, and deployed a similar route to another service of ours to make sure there was nothing wrong with the prefix match (there was suspicion that having multiple hyphens was causing some problem). The route worked on an alternative service so we ruled out the possibility of a syntax problem.\r\n\r\nAnyway, what we observed in the logs was that the `NR` flags were set. We know that this means that envoy could not find a route that matches the details of the request... however, we also log the macro `%ROUTE_NAME%`, and it logged the expected route name.\r\n\r\nSo I am perplexed... In what scenario will envoy set the `NR` flag but *also* set the `%ROUTE_NAME%` to something other than the default `-`?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9644/comments",
    "author": "cetanu",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2020-01-10T04:57:15Z",
        "body": "NR is also set if the cluster wasn't known to the cluster manager, e.g. that `some-cluster` wasn't known to Envoy"
      },
      {
        "user": "cetanu",
        "created_at": "2020-01-13T01:20:15Z",
        "body": "I think I might close this one as I don't have enough data on my side to form a timeline of events; what you said makes perfect sense and is likely the cause of the NR/503."
      }
    ]
  },
  {
    "number": 9601,
    "title": "TCP Proxying not working when using EDS dynamic configuration ",
    "created_at": "2020-01-08T14:48:07Z",
    "closed_at": "2020-02-16T18:31:35Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9601",
    "body": "\r\n*Title*: *TCP Proxying in combination with EDS not working - no healthy host for TCP connection pool*\r\n\r\n*Description*:\r\nI am trying to configure envoy to use EDS, by externalizing the endpoint configuration. The purpose of envoy here is to forward TCP packages. Whenever i try to connect to the envoy server, on the configured listener: localhost:9080, i get an unhealthy upstream response, and the message: **_\"no healthy host for TCP connection pool\"_**\r\nI would like to mention that if i configure the endpoints statically, in form of cluster hosts, everything works as intended. \r\nI have added the eds.conf file, looking like this. Am i missing something?\r\n````\r\n{\r\n  \"version_info\": \"0\",\r\n  \"resources\": [{\r\n    \"@type\": \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\"\r\n    \"cluster_name\": \"kafka-0\",\r\n    \"endpoints\": [{\r\n      \"lb_endpoints\": [{\r\n        \"endpoint\": {\r\n          \"address\": {\r\n            \"socket_address\": {\r\n              \"address\": \"10.49.11.101\",\r\n              \"port_value\": 30900\r\n            }\r\n          }\r\n        }\r\n      }]\r\n    }]\r\n  }]\r\n}\r\n```\r\n\r\nAccordingly, the envoy configuration file looks like this:\r\n```\r\nnode:\r\n  id: id_1\r\n  cluster: kafka-0\r\nstatic_resources:\r\n  listeners:\r\n  - name: discovery\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 9080\r\n    filter_chains:\r\n    - filters:\r\n       - name: envoy.tcp_proxy\r\n         config:\r\n          stat_prefix: ingress_tcp\r\n          cluster: kafka-0\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: /dev/stdout\r\n              json_format: {\r\n                \"timestamp\":\"%START_TIME%\",\r\n                \"envoy.protocol\": \"TCP\",\r\n                \"envoy.duration\": \"%DURATION%\",\r\n                \"envoy.bytes_received\": \"%BYTES_RECEIVED%\",\r\n                \"envoy.bytes_sent\": \"%BYTES_SENT%\",\r\n                \"envoy.response_flags\": \"%RESPONSE_FLAGS%\",\r\n                \"envoy.route\": \"%ROUTE_NAME%\",\r\n                \"envoy.upstream_host\": \"%UPSTREAM_HOST%\",\r\n                \"envoy.upstream_cluster\": \"%UPSTREAM_CLUSTER%\",\r\n                \"envoy.downstream_remote_address\": \"%DOWNSTREAM_REMOTE_ADDRESS%\",\r\n                \"envoy.requested_server_name\": \"%REQUESTED_SERVER_NAME%\"\r\n              }\r\n          idle_timeout: 0s\r\n          max_connect_attempts: 5\r\n  clusters:\r\n  - name: kafka-0\r\n    connect_timeout: 5s\r\n    lb_policy: round_robin\r\n    type: EDS\r\n    eds_cluster_config:\r\n      service_name: local\r\n      eds_config:\r\n        path: '/etc/config/envoy/eds.conf'\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8002\r\n```\r\n\r\n\r\n\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n\r\n\r\n\r\n*Logs*:\r\n\r\nHere is the log snippet, in trace mode:\r\n```\r\n\"envoy.upstream_host\":\"-\",\"envoy.downstream_remote_address\":\"192.168.17.149:62912\",\"envoy.requested_server_name\":\"-\",\"timestamp\":\"2020-01-08T14:45:14.337Z\",\"envoy.upstream_cluster\":\"-\",\"envoy.protocol\":\"TCP\",\"envoy.route\":\"-\",\"envoy.response_flags\":\"UH\",\"envoy.bytes_sent\":\"0\",\"envoy.duration\":\"0\",\"envoy.bytes_received\":\"0\"}\r\n{\"envoy.protocol\":\"TCP\",\"envoy.route\":\"-\",\"envoy.response_flags\":\"UH\",\"envoy.bytes_sent\":\"0\",\"envoy.duration\":\"0\",\"envoy.bytes_received\":\"0\",\"envoy.upstream_host\":\"-\",\"envoy.downstream_remote_address\":\"192.168.17.149:62913\",\"envoy.requested_server_name\":\"-\",\"timestamp\":\"2020-01-08T14:45:16.339Z\",\"envoy.upstream_cluster\":\"-\"}\r\n[2020-01-08 09:45:18.350][23310][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:204] [C3] new tcp proxy session\r\n[2020-01-08 09:45:18.350][23310][trace][connection] [external/envoy/source/common/network/connection_impl.cc:293] [C3] readDisable: enabled=true disable=true\r\n[2020-01-08 09:45:18.350][23310][debug][filter] [external/envoy/source/common/tcp_proxy/tcp_proxy.cc:347] [C3] Creating connection to cluster kafka-0\r\n[2020-01-08 09:45:18.350][23310][debug][upstream] [external/envoy/source/common/upstream/cluster_manager_impl.cc:1268] no healthy host for TCP connection pool\r\n[2020-01-08 09:45:18.350][23310][debug][connection] [external/envoy/source/common/network/connection_impl.cc:104] [C3] closing data_to_write=0 type=1\r\n[2020-01-08 09:45:18.350][23310][debug][connection] [external/envoy/source/common/network/connection_impl.cc:193] [C3] closing socket: 1\r\n``` \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9601/comments",
    "author": "alexandru-ersenie",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2020-01-10T17:30:56Z",
        "body": "I modified your example config to for my local environment, and I see two problems. The first is a JSON syntax error:\r\n\r\n```\r\n[2020-01-10 09:21:53.915][1833879][warning][config] [source/common/config/filesystem_subscription_impl.cc:63] Filesystem config update failure: Unable to parse JSON as proto (INVALID_ARGUMENT:Expected , or } after key:value pair.\r\nLoadAssignment\"\r\n    \"cluster_name\": \"kaf\r\n                    ^)\r\n```\r\n\r\n(There's a comma missing after the `\"@type\"` entry in the given eds.conf.)\r\n\r\nSecond, `service_name: local` in the `eds_cluster_config` tells Envoy to use \"local\" (instead of \"kafka-0\") as the cluster name for EDS. The eds.conf contains `cluster_name: kafka-0` and is therefore rejected with this error:\r\n\r\n```\r\n[2020-01-10 09:22:55.657][1834477][warning][config] [source/common/config/filesystem_subscription_impl.cc:39] Filesystem config update rejected: Unexpected EDS cluster (expecting local): kafka-0\r\n```\r\n\r\nYou can fix that either by removing `service_name` from `eds_cluster_config` or changing the cluster name in eds.conf file to local.\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-09T17:39:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-16T18:31:34Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9590,
    "title": "Shadow Mirroring to service not working as intended",
    "created_at": "2020-01-07T20:08:42Z",
    "closed_at": "2020-02-15T05:45:39Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9590",
    "body": "So I am rather new to Envoy, so forgive me. But my team is attempting to mirror requests from multiple services, and split the traffic to a Kafka service we have running, as part of a POC for a larger project. As it currently stands this is not working and I would like to ask for some assistance. Below I'll have our front facing Envoy. We have attempted mirroring and in some places have gotten mirroring to work, though in correctly. What appears to be happening, though it is unclear, is that when we hit an endpoint, say /shadow1, we are making a post request with some junk JSON. the intial endpoint is hit and responds accordingly, but the split to out Kafka service does not appear to be happening and it seems to be that maybe /shadow1 is being added to the Kafka endpoint or something to that effect. Simply hitting the Kafka service endpoint will deliver the proper results and place the JSON on the cue, but mirroring to that endpoint is not working.\r\nAs it stands, the Shadow_service is hitting a local Spring Boot endpoint, while the validation_service is running on a PCF server.\r\nPerhaps someone can assist with this, as it is a major blocker for any continued progress on this project. Hopefully my issue is clear.\r\n\r\n\r\nEDIT: \r\nUpon some further examination . It appears that the validation_service is not able to be hit without 'host_rewrite' pointing to the endpoint. Even if the endpoint is hit directly, not mirrored, the host_rewrite is needed, even though the endpoint is pointed to in the address and transport_socket sections of the validation_service cluster. We made a change to our validation_service PCF endpoint so that it accepts just '/' and does not need anything else. We made the same change to the shadow_service endpoint.\r\nWithout the host_rewrite I receive and error stating \"404 Not Found: Requested route ('localhost:8000') does not exist.\". So without the ability to host_rewrite within the request_mirror, I believe that this is what's happening.\r\nThe mirroring works when we hit the validation_service, with host_rewrite, and pointing the mirror to the shadow_service. But not the other way around, which is what we are looking for.\r\nThe updated envoy.yml is posted below the first.\r\n\r\n\r\n```static_resources:\r\n  listeners:\r\n    - address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 80\r\n\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n                codec_type: auto\r\n                stat_prefix: ingress_http\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: backend\r\n                      domains:\r\n                        - \"*\"\r\n                      routes:\r\n                        - match:\r\n                            prefix: \"/msg/send\"\r\n                          route:\r\n                            host_rewrite: msg-service.apps.com\r\n                            cluster: validation_service\r\n                        - match:\r\n                            prefix: \"/shadow1\"\r\n                          route:\r\n                            cluster: shadow_service\r\n                            host_rewrite: msg-service.apps.com\r\n                            request_mirror_policy:\r\n                               cluster: validation_service\r\n                               runtime_fraction: { default_value: { numerator: 100 } }\r\n                http_filters:\r\n                  - name: envoy.router\r\n                    typed_config: {}\r\n  clusters:\r\n    - name: shadow_service\r\n      connect_timeout: 1s\r\n      type: strict_dns\r\n      lb_policy: round_robin\r\n      load_assignment:\r\n        cluster_name: shadow_service\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: host.docker.internal\r\n                      port_value: 8080\r\n    - name: validation_service\r\n      connect_timeout: 1s\r\n      type: strict_dns\r\n      lb_policy: round_robin\r\n      load_assignment:\r\n        cluster_name: validation_service\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: msg-service.apps.com\r\n                      port_value: 443\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.api.v2.auth.UpstreamTlsContext\r\n          sni: msg-service.apps.com\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001```\r\n\r\n\r\n\r\n```static_resources:\r\n  listeners:\r\n    - address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 80\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n                codec_type: auto\r\n                stat_prefix: ingress_http\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: backend\r\n                      domains:\r\n                        - \"*\"\r\n                      routes:\r\n#                        - match:\r\n#                            prefix: \"/\"\r\n#                          route:\r\n#                            cluster: shadow_service\r\n#                            request_mirror_policy:\r\n#                              cluster: validation_service\r\n#                              runtime_fraction: { default_value: { numerator: 100 } }\r\n                        - match:\r\n                            prefix: \"/\"\r\n                          route:\r\n                            cluster: validation_service\r\n                            host_rewrite: msg-service.apps.com\r\n                            request_mirror_policy:\r\n                              cluster: shadow_service\r\n                              runtime_fraction: { default_value: { numerator: 100 } }\r\n                http_filters:\r\n                  - name: envoy.router\r\n                    typed_config: {}\r\n  clusters:\r\n    - name: shadow_service\r\n      connect_timeout: 1s\r\n      type: logical_dns\r\n      lb_policy: round_robin\r\n      load_assignment:\r\n        cluster_name: shadow_service\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: host.docker.internal\r\n                      port_value: 8080\r\n    - name: validation_service\r\n      connect_timeout: 1s\r\n      type: logical_dns\r\n      lb_policy: round_robin\r\n      load_assignment:\r\n        cluster_name: validation_service\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: msg-service.apps.com\r\n                      port_value: 443\r\n      transport_socket:\r\n        name: envoy.transport_sockets.tls\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.api.v2.auth.UpstreamTlsContext\r\n          sni: msg-service.apps.com\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9590/comments",
    "author": "dlop4life",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-02-08T05:11:47Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-15T05:45:38Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "abhishekpdelhivery",
        "created_at": "2022-12-29T10:48:32Z",
        "body": "help wanted"
      }
    ]
  },
  {
    "number": 9559,
    "title": "envoy does not respect the reason phrase for http1.1 call. ",
    "created_at": "2020-01-04T07:37:22Z",
    "closed_at": "2020-01-21T16:49:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9559",
    "body": "*Title*: *envoy does not respect the reason phrase for http1.1 call. *\r\n\r\n*Description*:\r\nfor http1.1 call, we customized the reason phrase, e.g. 403 = invalid requestorid, instead of forbidden. but when we use envoy, we found such reason phrase is not treated correctly, which is working well in nginx. \r\n\r\non application itself, the curl result to localhost looks like below. \r\n< HTTP/1.1 403 Invalid Requestor ID and Scope\r\n< Content-Length: 0\r\n< Server: Jetty(9.2.15.v20160210)\r\n but when i curl thru envoy, it's:\r\n< HTTP/1.1 403 Forbidden\r\n< content-length: 0\r\n< server: istio-envoy\r\n< x-envoy-upstream-service-time: 0\r\n\r\npls help advise. Thanks!\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9559/comments",
    "author": "Jefle2005",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-01-05T17:17:03Z",
        "body": "Right now we don't proxy the HTTP/1.1 reason phrase. TBH this will be very complicated to do and does not translate to HTTP/1 -> HTTP/2 proxying. If this is an important thing to solve it will require a bunch of thinking on how to do this without requiring major changes."
      },
      {
        "user": "Jefle2005",
        "created_at": "2020-01-21T16:49:22Z",
        "body": "got it. Thanks!"
      }
    ]
  },
  {
    "number": 9523,
    "title": "Adjust sampling rate of Envoy stats",
    "created_at": "2019-12-30T18:23:10Z",
    "closed_at": "2020-02-21T19:30:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9523",
    "body": "Is there a way to adjust the percentage of stats that are generated/pushed to a stats listener using the envoy.dog_statsd stats sink? Similar to how you can specify the sampling method and rate for tracing. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9523/comments",
    "author": "jaegchoi",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-12-31T15:51:04Z",
        "body": "I'm not sure what adjusting a % of the stats that are emitted actually means in practice. You can use the stats flush_interval to control the flush rate though."
      },
      {
        "user": "jaegchoi",
        "created_at": "2020-01-15T17:45:42Z",
        "body": "Thanks for the reply Matt. Is the size of the buffer for the stats configurable? What is the default size of the buffer? We are trying to lessen the memory used by stats"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-14T18:45:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-21T19:30:53Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9483,
    "title": "retry policies \"5xx\", \"gateway-error\", \"reset\"",
    "created_at": "2019-12-26T14:33:19Z",
    "closed_at": "2020-02-14T01:11:37Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9483",
    "body": "Do the retry policies \"5xx\", \"gateway-error\", \"reset\" duplicatе each other?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9483/comments",
    "author": "alex3dm",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2020-01-08T00:24:07Z",
        "body": "\"gateway-error\" is a subset of \"5xx\". Currently, if \"gateway-error\" or \"5xx\" is set, upstream resets are retried. There is a comment in the code indicating this may change in the future. So to be future-proof, set \"5xx,reset\".\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-07T00:47:53Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-14T01:11:36Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9478,
    "title": "Whether to support grpc request mirror to specifle cluster？",
    "created_at": "2019-12-25T12:33:27Z",
    "closed_at": "2020-02-02T00:46:31Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9478",
    "body": "",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9478/comments",
    "author": "bainiu87",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-01-25T23:51:11Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-02T00:46:30Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9423,
    "title": "Ability to force restart of envoy upon certain trigger?",
    "created_at": "2019-12-19T18:05:52Z",
    "closed_at": "2019-12-20T20:53:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9423",
    "body": "*Title*: *Force server restart upon triggered condition?*\r\n\r\n*Description*:\r\nWhen setting up Envoy with a central control plane, the static cluster that serves up the xDS interface is vital to the functioning of the server.  If that cluster isn't up/reachable when Envoy is trying to connect, it goes through it's retries and eventually gets into a state where it's never gonna get updates, but also doesn't shut down.\r\n\r\nThis makes sense from the perspective that the static cluster for xDS is just another cluster, and envoy should stay up.  But now the proxy is in an unrecoverable state and since the server didn't shut down the orchestration tool in use (k8s for instance) won't restart the pod automatically.  \r\n\r\nIs there anything like this in Envoy, or do I need to relegate this to readiness probes in the orchestration tool?  I didn't see anything that could force a shutdown in the docs, but wanted to ask here before I gave up.  \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9423/comments",
    "author": "justincely",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-12-20T16:15:41Z",
        "body": "> Is there anything like this in Envoy, or do I need to relegate this to readiness probes in the orchestration tool? I didn't see anything that could force a shutdown in the docs, but wanted to ask here before I gave up.\r\n\r\nCorrect, there is nothing like this today and we suggest defining readiness probes that do what you want. We have quite a few different admin endpoints that help with writing such probes."
      },
      {
        "user": "justincely",
        "created_at": "2019-12-20T20:43:34Z",
        "body": "thanks @mattklein123, i suspected as much, but appreciate the quick reply.  Happy to have this closed, as my question is answered.  "
      }
    ]
  },
  {
    "number": 9420,
    "title": "Envoy serves new and old config for a period of time",
    "created_at": "2019-12-19T15:36:58Z",
    "closed_at": "2020-01-27T01:58:03Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9420",
    "body": "We are using ambassador to configure routes in kubernetes. We observed seemingly random 404 responses and uppon closer investigation found out that they only occur when we modify a route.\r\nIf we for instance add a new route, for a period of time we sometimes get 404s and sometimes the request succeeds.\r\n\r\nThis can be observed on every route addition/deletion. It looks like the new config slowly takes over. Up to 30 seconds after the new routes have been added we are pretty much at 50% to get the old config. After 30 seconds pass it's much lower.\r\n\r\nWe are pretty sure it's not ambassadors fault, cause the correct requests are logged and the envoy config is correctly updated with no reference to the old configuration.\r\n\r\nWe tried to manually SIGHUP envoy after the configuration has been updated but that didn't change anything.\r\n\r\nWe also tried to update the routes multiple times (from `/first` to `/second` to `/third`) hoping that another update might render the old config obsolete, but that didn't work either. We could still access `/first` for a period of about 30 seconds.\r\n\r\nI hope that is enough information to replicate. If not I guess I can provide our generated envoy config.\r\n\r\nWe tested that using ambassador 0.80.0 and 0.86.0.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9420/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-12-20T22:19:15Z",
        "body": "I would recommend raising this issue to ambassador for help debugging. I think this might be due to listener drain timeout if ambassador is not using RDS, but I'm not sure."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-20T00:57:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-27T01:58:02Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9417,
    "title": "any reason on \"no healthy host for HTTP connection pool\"",
    "created_at": "2019-12-19T13:49:47Z",
    "closed_at": "2019-12-23T01:58:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9417",
    "body": "Hello,\r\n\r\nWe have seen many logs like \"no healthy host for HTTP connection pool\". Envoy failed to establish gRPC to peer side. Could someone shed some light on it?\r\n\r\nThanks a lot!\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9417/comments",
    "author": "mailzyok",
    "comments": [
      {
        "user": "zyfjeff",
        "created_at": "2019-12-20T06:27:27Z",
        "body": "@mailzyok\r\n\r\n\"no healthy\" is generally because the health check is configured and the upstream machine's health status is unhealth, so when there is a request, there is no way to find a healthy host."
      },
      {
        "user": "mailzyok",
        "created_at": "2019-12-20T06:38:36Z",
        "body": "@zyfjeff  Thanks for you comment.\r\n\r\nHere is additional log which indicated the gRPC cannot be established. It was a response from upstream. It is strange the status code is 200. \r\nI am still not clear on what the reason for unhealth, because the upstream service is up and running in K8S. Is there any other log i can check?\r\n\r\n> [2019-12-20 02:39:18.786][18][debug][http] [external/envoy/source/common/http/async_client_impl.cc:94] async http request response headers (end_stream=true):\r\n> ':status', '200'\r\n> 'content-type', 'application/grpc'\r\n> 'grpc-status', '14'\r\n> 'grpc-message', 'no healthy upstream'\r\n> \r\n\r\nThanks."
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-12-20T06:45:11Z",
        "body": "@mailzyok can you upload the complete log and config_dump files?"
      },
      {
        "user": "mailzyok",
        "created_at": "2019-12-20T06:55:29Z",
        "body": "@zyfjeff Thanks. I cannot upload a complete config_dump log here, could you let me know which part you want to see for config_dump, i will copy it here.\r\n\r\nHere is the log for complete log when the problem happens\r\n\r\n> [2019-12-20 02:39:18.784][18][debug][upstream] [external/envoy/source/common/upstream/cluster_manager_impl.cc:74] cm init: adding: cluster=xds-grpc primary=1 secondary=0\r\n> [2019-12-20 02:39:18.784][18][debug][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:42] Establishing new gRPC bidi stream for rpc StreamAggregatedResources(stream .envoy.api.v2.DiscoveryRequest) returns (stream .envoy.api.v2.DiscoveryResponse);\r\n> \r\n> [2019-12-20 02:39:18.784][18][debug][router] [external/envoy/source/common/router/router.cc:332] [C0][S5272412103284310368] cluster 'xds-grpc' match for URL '/envoy.service.discovery.v2.AggregatedDiscoveryService/StreamAggregatedResources'\r\n> [2019-12-20 02:39:18.784][18][debug][upstream] [external/envoy/source/common/upstream/cluster_manager_impl.cc:1124] no healthy host for HTTP connection pool\r\n> [2019-12-20 02:39:18.786][18][debug][http] [external/envoy/source/common/http/async_client_impl.cc:94] async http request response headers (end_stream=true):\r\n> ':status', '200'\r\n> 'content-type', 'application/grpc'\r\n> 'grpc-status', '14'\r\n> 'grpc-message', 'no healthy upstream'\r\n> \r\n> [2019-12-20 02:39:18.786][18][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:86] gRPC config stream closed: 14, no healthy upstream\r\n> [2019-12-20 02:39:18.786][18][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:49] Unable to establish new stream\r\n> \r\n\r\nHere is the cluster info in config_dump\r\n\r\n>      \"clusters\": [\r\n>       {\r\n>        \"name\": \"prometheus_stats\",\r\n>        \"type\": \"STATIC\",\r\n>        \"connect_timeout\": \"0.250s\",\r\n>        \"hosts\": [\r\n>         {\r\n>          \"socket_address\": {\r\n>           \"address\": \"::1\",\r\n>           \"port_value\": 15000\r\n>          }\r\n>         }\r\n>        ]\r\n>       },\r\n>       {\r\n>        \"name\": \"xds-grpc\",\r\n>        \"type\": \"STRICT_DNS\",\r\n>        \"connect_timeout\": \"10s\",\r\n>        \"hosts\": [\r\n>         {\r\n>          \"socket_address\": {\r\n>           \"address\": \"istio-pilot\",\r\n>           \"port_value\": 15010\r\n>          }\r\n>         }\r\n>        ],\r\n>        \"circuit_breakers\": {\r\n>         \"thresholds\": [\r\n>          {\r\n>           \"max_connections\": 100000,\r\n>           \"max_pending_requests\": 100000,\r\n>           \"max_requests\": 100000\r\n>          },\r\n>          {\r\n>           \"priority\": \"HIGH\",\r\n>           \"max_connections\": 100000,\r\n>           \"max_pending_requests\": 100000,\r\n>           \"max_requests\": 100000\r\n>          }\r\n>         ]\r\n>        },\r\n>        \"http2_protocol_options\": {},\r\n>        \"dns_refresh_rate\": \"300s\",\r\n>        \"upstream_connection_options\": {\r\n>         \"tcp_keepalive\": {\r\n>          \"keepalive_time\": 300\r\n>         }\r\n>        }\r\n>       }\r\n>      ]\r\n> "
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-12-20T07:44:28Z",
        "body": "@mailzyok \r\nAccording to your logs, it should be that you cannot connect to the` istio-pilot` to cause \" no health upstream\", you can confirm whether `istio-pilot.istio-system` can resolve the IP"
      },
      {
        "user": "mailzyok",
        "created_at": "2019-12-20T08:59:16Z",
        "body": "@zyfjeff  Thanks a lot, seems DNS resolution problem, see logs below:\r\n\r\nWhen envoy starting resolution for istio-pilot, it takes 150s no response, i think it is a timeout, but i didn't find the default timeout values,\r\n\r\nThen after 300s, envoy start a new resolution, i think 300s is configured as dns_refresh_rate in static cluster configuration. This time the resolution succeeded.\r\n\r\nI will ask the admin to check the issue of DNS resolution. Thanks again and i would take this opportunity to wish you a Merry Xmas and Happy new Year.\r\n\r\n> [2019-12-20 08:32:22.245][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:88] starting async DNS resolution for istio-pilot\r\n> [2019-12-20 08:34:52.249][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:95] async DNS resolution complete for istio-pilot \r\n> [2019-12-20 08:39:52.252][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:88] starting async DNS resolution for istio-pilot\r\n> [2019-12-20 08:39:52.253][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:95] async DNS resolution complete for istio-pilot\r\n> [2019-12-20 08:39:52.253][20][debug][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:117] DNS hosts have changed for istio-pilot "
      },
      {
        "user": "mailzyok",
        "created_at": "2019-12-23T01:58:13Z",
        "body": "Thank you all for the support. The trace log is useful, we figured out the timeout values finally."
      }
    ]
  },
  {
    "number": 9386,
    "title": "503 Connection pool errors",
    "created_at": "2019-12-17T21:15:21Z",
    "closed_at": "2020-01-27T08:20:12Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9386",
    "body": "*Description*:\r\n\r\nHi, we've been running load tests through Envoy which is running as a sidecar to our nodejs server. \r\nMost of the requests are good, except in all of our tests there is a point where Envoy seems to temporarily drop its connections to the upstream (the nodejs server). \r\n\r\nFrom the stats I see \"upstream_rq_pending_failure_eject\" is the culprit of these failures which is described as \"Total requests that were failed due to a connection pool connection failure\" \r\n\r\nWhat does this exactly mean, and what can we do to avoid these errors? \r\n\r\n*Admin and Stats Output*:\r\n\r\nAs this is not our development environment, I do not have direct access to the Envoy stats, and I can only view them through Grafana.\r\n\r\n*Config*:\r\n\r\n```\r\n  clusters:\r\n  - name: xxxxxxxxx\r\n    connect_timeout: 1s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: xxxxxxxxxxx\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: localhost.ssl\r\n                port_value: 8443\r\n    tls_context:\r\n      common_tls_context:\r\n        validation_context:\r\n          trusted_ca: { filename: \"xxxxxxxxxxx\" }\r\n```\r\n\r\n*Logs*:\r\nSample access log: \r\n\r\n> [2019-12-17T16:33:31.362Z] \"GET /xxxxx/ HTTP/1.1\" 503 UF 0 91 0 - \"xx.xxx.xxx.xxx\" \"Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)\" \"c60e035b-157b-98d5-a11d-3610acd7effc\" \"xxxxxx.com\" \"127.0.0.1:8443\"\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9386/comments",
    "author": "jaegchoi",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-12-20T22:16:27Z",
        "body": "Typically this happens when the upstream server disconnects."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-20T02:57:13Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-27T08:20:11Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9363,
    "title": "Failure of Bazel build ",
    "created_at": "2019-12-16T18:07:48Z",
    "closed_at": "2020-01-26T20:58:03Z",
    "labels": [
      "question",
      "area/build",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9363",
    "body": "Hi,\r\n\r\nFirst of all, I'm new to Bazel build environment. I want to contribute envoy and to do so, first I wanted to build the project using the command below in the project directory:\r\n\r\n`bazel build //...`\r\n\r\nHowever I got the error below everytime I try this:\r\n\r\n```\r\nERROR: An error occurred during the fetch of repository 'com_github_gperftools_gperftools':\r\n\r\n//bazel/foreign_cc:gperftools_build depends on @com_github_gperftools_gperftools//:all in repository \r\n@com_github_gperftools_gperftools which failed to fetch. no such package \r\n'@com_github_gperftools_gperftools//': \r\n\r\nERROR: Analysis of target '//bazel/foreign_cc:gperftools_build' failed; build aborted: Analysis failed\r\n```\r\n\r\nI have also tried other methods available on the internet concerning the convertion from Bazel to Cmake but no luck. Bazel plugin for Clion also did not work.\r\n\r\nAlso I wonder if it's possible to include envoy package from a remote repository using cmake in my project. \r\n\r\nI have handled everything on the implementation side and ready to open PR but this problem made me stuck here and could not open PR. \r\n\r\nThanks in advance\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9363/comments",
    "author": "raspberry44",
    "comments": [
      {
        "user": "hongzimao",
        "created_at": "2019-12-20T13:55:05Z",
        "body": "Which `bazel version` are you using? I also hit this error with bazel 2.0.0. Same envoy code built with bazel 0.28.1"
      },
      {
        "user": "hongzimao",
        "created_at": "2019-12-20T20:11:13Z",
        "body": "Actually, although the trigger occurred at the same repo, my error was different:`autoreconf: not found` and I fixed it by\r\n```\r\nsudo apt-get install dh-autoreconf\r\n```"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-19T20:38:02Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-26T20:58:02Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9342,
    "title": "Linking error",
    "created_at": "2019-12-13T19:39:47Z",
    "closed_at": "2020-01-24T03:40:43Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9342",
    "body": "*Title*: *Error on linking envoy 1.12.x using ci/run_envoy_docker.sh*\r\n\r\n*Description*:\r\n> During the linking I run into this errros `requires unsupported dynamic reloc 11; recompile with -fPIC`, implying envoy is being dynamically linked with dependencies.  \r\n\r\nWhen I checked `bazel-out/k8-dbg/bin/envoy-2.params` I don't see `-static` option being emitted but I see `-lstdc++ -lm` flags being included.\r\n\r\nUpon further inspection I found `envoy/bazel/envoy_binary.bzl(envoy_cc_binary)` do set `linkstatic = 1` changing this value to `True` as per the Bazel documentation didn't help either. Hardcoding `-static` option to `envoy/bazel/envoy_binary.bzl(_envoy_linkopts)` did help but linker eventually failed with this error\r\n\r\n>`\r\n/usr/bin/ld.gold: error: /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/libc.a(malloc.o): multiple definition of '__libc_malloc'\r\n/usr/bin/ld.gold: bazel-out/k8-dbg/bin/external/envoy/bazel/foreign_cc/gperftools_build/lib/libtcmalloc_and_profiler.a(libtcmalloc_and_profiler_la-tcmalloc.o): previous definition here\r\n/usr/bin/ld.gold: error: /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/libc.a(malloc.o): multiple definition of 'malloc'\r\n/usr/bin/ld.gold: bazel-out/k8-dbg/bin/external/envoy/bazel/foreign_cc/gperftools_build/lib/libtcmalloc_and_profiler.a(libtcmalloc_and_profiler_la-tcmalloc.o): previous definition here\r\n`\r\n\r\nI was able build with our any issues on Mac, so I presume this is Linux only issue.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9342/comments",
    "author": "kprakasam",
    "comments": [
      {
        "user": "kprakasam",
        "created_at": "2019-12-17T01:07:05Z",
        "body": "I was able to resolve link issue by creating link to envoy/.bazelrc file (I was compiling my filter with envoy as a git sub module)  however I run into additional issue as soon as I created the link\r\n\r\n>root@36d0076241c0:/ebay-envoy# bazel build -c dbg //:envoy\r\nINFO: Writing tracer profile to '/root/.cache/bazel/_bazel_root/98d36195a0ee79abd52c9d0cdadceb3e/command.profile.gz'\r\nERROR: error loading package '': in /root/.cache/bazel/_bazel_root/98d36195a0ee79abd52c9d0cdadceb3e/external/envoy/bazel/api_repositories.bzl: Label '@envoy_api//bazel:repositories.bzl' is invalid because 'bazel' is not a package; perhaps you meant to put the colon here: '@envoy_api//:bazel/repositories.bzl'?\r\nERROR: error loading package '': in /root/.cache/bazel/_bazel_root/98d36195a0ee79abd52c9d0cdadceb3e/external/envoy/bazel/api_repositories.bzl: Label '@envoy_api//bazel:repositories.bzl' is invalid because 'bazel' is not a package; perhaps you meant to put the colon here: '@envoy_api//:bazel/repositories.bzl'?\r\nINFO: Elapsed time: 0.442s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\nSeems this was caused by `build --experimental_remap_main_repo` option inside the envoy/.bazelrc once I removed this option or change it to `build --noincompatible_remap_main_repo` build went though fine"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-16T01:21:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-24T03:40:42Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9296,
    "title": "Is it possbile to get the envoy-static.exe?",
    "created_at": "2019-12-10T12:11:13Z",
    "closed_at": "2020-01-17T07:05:17Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9296",
    "body": "Is it possible to run on windows ? I only find build_test.exe and go_build_test.exe",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9296/comments",
    "author": "makefriend8",
    "comments": [
      {
        "user": "dio",
        "created_at": "2019-12-10T12:14:00Z",
        "body": "cc. @wrowe "
      },
      {
        "user": "wrowe",
        "created_at": "2019-12-10T18:18:40Z",
        "body": "The answer is twofold... The master branch at the project does not yet entirely build on windows. We are currently working on this aspect and have an open slack discussion for collaborators on envoy-windows-dev channel.\r\n\r\nThe second part is that master should be ready for peer development this month. It will not be production ready; only those capable of compiling and then debugging the sources can be of any help until it has reached the point of early community review by users. At that call, envoy-static.exe would become publicly downloadable, which won't happen until 2020."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-10T06:16:06Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-17T07:05:16Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9278,
    "title": "Best use case of the tap filter",
    "created_at": "2019-12-09T13:15:46Z",
    "closed_at": "2020-01-09T08:02:27Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9278",
    "body": "Also, is it a good practice if I use the tap filter as a daily logging tool to record the request/response body? \r\nI still have some doubts about the best use case of the tap filter, and I don't know if anyone has any suggestions.\r\n\r\n@htuch ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9278/comments",
    "author": "wbpcode",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-12-10T04:36:32Z",
        "body": "I think the network TAP filter was only ever designed with developer's needs in minds (as opposed to the HTTP TAP, which does have streaming support). I would not recommend it for production use, I think we probably need to have a streaming interface for that."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-09T04:51:27Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9242,
    "title": "Envoy response_flag=UT returning 200 instead of 504",
    "created_at": "2019-12-05T16:26:21Z",
    "closed_at": "2020-01-16T06:21:34Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9242",
    "body": "*Title*: *Envoy response_flag=UT returning 200 instead of 504*\r\n\r\n*Description*:\r\nWe have an issue where upstream request timedout. Envoy set the right respone_flag as `UT` but response code was set to 200 instead of 504.\r\n\r\n`[05/12/2019T16:14:53+0000 1575562493] method=POST authority=localhost:443 txId=- request=/StudentService/get alpn=HTTP/2 response_flag=UT ssl_protocol=TLSv1.3 ssl_cipher=TLS_AES_128_GCM_SHA256 status=200 status_details=upstream_response_timeout request_size=84 response_size=0 envoy_request_id=52220e61-e045-4dc7-bdc3-c8b5e77dd2ee user-agent=grpc-go/1.24.0 xFor=49.207.48.241 x-envoy-upstream-service-time=- total_time_ms=15001 upstream=127.0.0.1:7000`\r\n\r\nAccess Log format:\r\n\r\n`\"[%START_TIME(%d/%m/%YT%H:%M:%S%z %s)%] method=%REQ(:METHOD)% authority=%REQ(:AUTHORITY)% txId=%RESP(TID)% request=%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% alpn=%PROTOCOL% response_flag=%RESPONSE_FLAGS% ssl_protocol=%DOWNSTREAM_TLS_VERSION% ssl_cipher=%DOWNSTREAM_TLS_CIPHER% status=%RESPONSE_CODE% status_details=%RESPONSE_CODE_DETAILS% request_size=%BYTES_RECEIVED% response_size=%BYTES_SENT% envoy_request_id=%REQ(X-REQUEST-ID)% user-agent=%REQ(USER-AGENT)% xFor=%DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT% x-envoy-upstream-service-time=%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% total_time_ms=%DURATION% upstream=%UPSTREAM_HOST%\\n\"`\r\n\r\nChecking on `RESPONSE_CODE_DETAILS`, it return the cause as `upstream_response_timeout`\r\n\r\n\r\nEnvoy version:\r\n\r\n`envoy  version: 44f8c365a1f1798f0af776f6aa64279dc68f5666/1.12.1/clean-getenvoy-93e425e-envoy/RELEASE/BoringSSL`\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9242/comments",
    "author": "yks0000",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-12-09T23:28:54Z",
        "body": "The timeout can occur after the response headers have been written."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-09T05:51:31Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-16T06:21:33Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "aberres",
        "created_at": "2022-12-13T12:01:58Z",
        "body": "For someone coming here from Google:\r\n\r\nWe are seeing this case with slow downstream clients. Everything is fine upstream, but the client does not read back the payload in time. It can be reproduced, e.g., with the Mac Network Link Conditioner, but not with the throttling tools in FF/Chrome.\r\n\r\nNo idea why this is marked as `UT`, though. It is an issue downstream, not upstream, IMO."
      }
    ]
  },
  {
    "number": 9206,
    "title": "Envoy fails to initialize on DELTA_GRPC ( missing LDS request )",
    "created_at": "2019-12-03T19:56:55Z",
    "closed_at": "2020-01-20T07:57:16Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9206",
    "body": "*Envoy fails to initialize on DELTA_GRPC ( missing LDS request )*: \r\n\r\n*Description*:\r\nEnvoy cannot initialize properly when using a fully dynamic configuration through DELTA_GRPC protocol. CDS and EDS initialization is ok but LDS messages are missing. surprisingly if envoy is bootstrapped without a control plane server forcing a connection failure , when the control plane is back envoy initialize correctly\r\n\r\n*Repro steps*:\r\n\r\n1. Start the control plane\r\n2. Start Envoy ( see envoy configuration below )\r\n3. Envoy will send the CDS and EDS  but not LDS\r\n\r\nif we start envoy before the control plane and  we start the control plane after the first connection failure , envoy will send the  LDS messages and it will init properly.\r\n\r\n1. Start envoy without a control plane ( envoy will fail trying to connect to CP )\r\n2. Start the control plane.\r\n3. Envoy will initialize propertly\r\n\r\n*Envoy version*\r\nenvoyproxy/envoy-alpine:v1.12.1\r\nenvoy  version: 44f8c365a1f1798f0af776f6aa64279dc68f5666/1.12.1/Clean/RELEASE/BoringSSL\r\n\r\n*Config*:\r\n```admin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 127.0.0.1\r\n      port_value: 9901\r\n\r\ndynamic_resources:\r\n  ads_config:\r\n    api_type: DELTA_GRPC\r\n    grpc_services:\r\n      envoy_grpc:\r\n        cluster_name: xds_cluster\r\n\r\n  cds_config:\r\n    ads: {}\r\n  lds_config:\r\n    ads: {}\r\n\r\nnode:\r\n  cluster: service_greeter\r\n  id: test-id\r\n  metadata:\r\n    mode: Edge\r\n\r\nstatic_resources:\r\n  clusters:\r\n    - name: xds_cluster\r\n      connect_timeout: 5s\r\n      type: static\r\n      lb_policy: ROUND_ROBIN\r\n      http2_protocol_options: {}\r\n      upstream_connection_options:\r\n        tcp_keepalive: {}\r\n      load_assignment:\r\n        cluster_name: xds_cluster\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: 127.0.0.1\r\n                      port_value: 8080\r\n```\r\n\r\n*Logs*:\r\n```[2019-12-03 18:54:12.989][12][info][main] [source/server/server.cc:249] initializing epoch 0 (hot restart version=11.104)\r\n[2019-12-03 18:54:12.989][12][info][main] [source/server/server.cc:251] statically linked extensions:\r\n[2019-12-03 18:54:12.989][12][info][main] [source/server/server.cc:253]   access_loggers: envoy.file_access_log,envoy.http_grpc_access_log,envoy.tcp_grpc_access_log\r\n[2019-12-03 18:54:12.989][12][info][main] [source/server/server.cc:256]   filters.http: envoy.buffer,envoy.cors,envoy.csrf,envoy.ext_authz,envoy.fault,envoy.filters.http.adaptive_concurrency,envoy.filters.http.dynamic_forward_proxy,envoy.filters.http.grpc_http1_reverse_bridge,envoy.filters.http.grpc_stats,envoy.filters.http.header_to_metadata,envoy.filters.http.jwt_authn,envoy.filters.http.original_src,envoy.filters.http.rbac,envoy.filters.http.tap,envoy.grpc_http1_bridge,envoy.grpc_json_transcoder,envoy.grpc_web,envoy.gzip,envoy.health_check,envoy.http_dynamo_filter,envoy.ip_tagging,envoy.lua,envoy.rate_limit,envoy.router,envoy.squash\r\n[2019-12-03 18:54:12.989][12][info][main] [source/server/server.cc:259]   filters.listener: envoy.listener.http_inspector,envoy.listener.original_dst,envoy.listener.original_src,envoy.listener.proxy_protocol,envoy.listener.tls_inspector\r\n[2019-12-03 18:54:12.989][12][info][main] [source/server/server.cc:262]   filters.network: envoy.client_ssl_auth,envoy.echo,envoy.ext_authz,envoy.filters.network.dubbo_proxy,envoy.filters.network.mysql_proxy,envoy.filters.network.rbac,envoy.filters.network.sni_cluster,envoy.filters.network.thrift_proxy,envoy.filters.network.zookeeper_proxy,envoy.http_connection_manager,envoy.mongo_proxy,envoy.ratelimit,envoy.redis_proxy,envoy.tcp_proxy\r\n[2019-12-03 18:54:12.989][12][info][main] [source/server/server.cc:264]   stat_sinks: envoy.dog_statsd,envoy.metrics_service,envoy.stat_sinks.hystrix,envoy.statsd\r\n[2019-12-03 18:54:12.989][12][info][main] [source/server/server.cc:266]   tracers: envoy.dynamic.ot,envoy.lightstep,envoy.tracers.datadog,envoy.tracers.opencensus,envoy.tracers.xray,envoy.zipkin\r\n[2019-12-03 18:54:12.989][12][info][main] [source/server/server.cc:269]   transport_sockets.downstream: envoy.transport_sockets.alts,envoy.transport_sockets.raw_buffer,envoy.transport_sockets.tap,envoy.transport_sockets.tls,raw_buffer,tls\r\n[2019-12-03 18:54:12.989][12][info][main] [source/server/server.cc:272]   transport_sockets.upstream: envoy.transport_sockets.alts,envoy.transport_sockets.raw_buffer,envoy.transport_sockets.tap,envoy.transport_sockets.tls,raw_buffer,tls\r\n[2019-12-03 18:54:12.989][12][info][main] [source/server/server.cc:278] buffer implementation: new\r\n[2019-12-03 18:54:12.998][12][info][main] [source/server/server.cc:344] admin address: 127.0.0.1:9901\r\n[2019-12-03 18:54:12.999][12][debug][main] [source/server/overload_manager_impl.cc:183] No overload action is configured for envoy.overload_actions.shrink_heap.\r\n[2019-12-03 18:54:12.999][12][debug][main] [source/server/overload_manager_impl.cc:183] No overload action is configured for envoy.overload_actions.stop_accepting_connections.\r\n[2019-12-03 18:54:12.999][12][debug][main] [source/server/overload_manager_impl.cc:183] No overload action is configured for envoy.overload_actions.stop_accepting_connections.\r\n[2019-12-03 18:54:13.000][12][debug][main] [source/server/overload_manager_impl.cc:183] No overload action is configured for envoy.overload_actions.stop_accepting_connections.\r\n[2019-12-03 18:54:13.000][12][debug][main] [source/server/overload_manager_impl.cc:183] No overload action is configured for envoy.overload_actions.stop_accepting_connections.\r\n[2019-12-03 18:54:13.000][12][debug][main] [source/server/overload_manager_impl.cc:183] No overload action is configured for envoy.overload_actions.stop_accepting_connections.\r\n[2019-12-03 18:54:13.000][12][debug][main] [source/server/overload_manager_impl.cc:183] No overload action is configured for envoy.overload_actions.stop_accepting_connections.\r\n[2019-12-03 18:54:13.000][12][debug][main] [source/server/overload_manager_impl.cc:183] No overload action is configured for envoy.overload_actions.stop_accepting_connections.\r\n[2019-12-03 18:54:13.000][12][debug][main] [source/server/overload_manager_impl.cc:183] No overload action is configured for envoy.overload_actions.stop_accepting_connections.\r\n[2019-12-03 18:54:13.000][12][info][main] [source/server/server.cc:458] runtime: layers:\r\n  - name: base\r\n    static_layer:\r\n      {}\r\n  - name: admin\r\n    admin_layer:\r\n      {}\r\n[2019-12-03 18:54:13.000][12][info][config] [source/server/configuration_impl.cc:62] loading 0 static secret(s)\r\n[2019-12-03 18:54:13.000][12][info][config] [source/server/configuration_impl.cc:68] loading 1 cluster(s)\r\n[2019-12-03 18:54:13.001][16][debug][grpc] [source/common/grpc/google_async_client_impl.cc:44] completionThread running\r\n[2019-12-03 18:54:13.002][12][debug][upstream] [source/common/upstream/upstream_impl.cc:250] transport socket match, socket default selected for host with address 127.0.0.1:8080\r\n[2019-12-03 18:54:13.002][12][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:909] adding TLS initial cluster xds_cluster\r\n[2019-12-03 18:54:13.005][12][trace][upstream] [source/common/upstream/upstream_impl.cc:1140] Local locality: \r\n[2019-12-03 18:54:13.005][12][debug][upstream] [source/common/upstream/upstream_impl.cc:912] initializing secondary cluster xds_cluster completed\r\n[2019-12-03 18:54:13.005][12][debug][init] [source/common/init/manager_impl.cc:45] init manager Cluster xds_cluster contains no targets\r\n[2019-12-03 18:54:13.005][12][debug][init] [source/common/init/watcher_impl.cc:14] init manager Cluster xds_cluster initialized, notifying ClusterImplBase\r\n[2019-12-03 18:54:13.005][12][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:1069] membership update for TLS cluster xds_cluster added 1 removed 0\r\n[2019-12-03 18:54:13.005][12][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:102] cm init: init complete: cluster=xds_cluster primary=0 secondary=0\r\n[2019-12-03 18:54:13.005][12][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:74] cm init: adding: cluster=xds_cluster primary=0 secondary=0\r\n[2019-12-03 18:54:13.005][12][info][upstream] [source/common/upstream/cluster_manager_impl.cc:157] cm init: initializing cds\r\n[2019-12-03 18:54:13.005][12][trace][config] [source/common/config/new_grpc_mux_impl.cc:194] No stream available to send a discovery request for type.googleapis.com/envoy.api.v2.Cluster.\r\n[2019-12-03 18:54:13.005][12][debug][config] [bazel-out/k8-opt/bin/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:47] Establishing new gRPC bidi stream for rpc DeltaAggregatedResources(stream .envoy.api.v2.DeltaDiscoveryRequest) returns (stream .envoy.api.v2.DeltaDiscoveryResponse);\r\n\r\n[2019-12-03 18:54:13.005][12][debug][router] [source/common/router/router.cc:434] [C0][S15852061503706113562] cluster 'xds_cluster' match for URL '/envoy.service.discovery.v2.AggregatedDiscoveryService/DeltaAggregatedResources'\r\n[2019-12-03 18:54:13.005][12][debug][router] [source/common/router/router.cc:549] [C0][S15852061503706113562] router decoding headers:\r\n':method', 'POST'\r\n':path', '/envoy.service.discovery.v2.AggregatedDiscoveryService/DeltaAggregatedResources'\r\n':authority', 'xds_cluster'\r\n':scheme', 'http'\r\n'te', 'trailers'\r\n'content-type', 'application/grpc'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '192.168.0.110'\r\n\r\n[2019-12-03 18:54:13.005][12][debug][client] [source/common/http/codec_client.cc:31] [C0] connecting\r\n[2019-12-03 18:54:13.005][12][debug][connection] [source/common/network/connection_impl.cc:711] [C0] connecting to 127.0.0.1:8080\r\n[2019-12-03 18:54:13.005][12][debug][connection] [source/common/network/connection_impl.cc:720] [C0] connection in progress\r\n[2019-12-03 18:54:13.006][12][debug][http2] [source/common/http/http2/codec_impl.cc:912] [C0] setting stream-level initial window size to 268435456\r\n[2019-12-03 18:54:13.006][12][debug][http2] [source/common/http/http2/codec_impl.cc:934] [C0] updating connection-level initial window size to 268435456\r\n[2019-12-03 18:54:13.006][12][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2019-12-03 18:54:13.006][12][trace][router] [source/common/router/router.cc:1475] [C0][S15852061503706113562] buffering 166 bytes\r\n[2019-12-03 18:54:13.006][12][info][config] [source/server/configuration_impl.cc:72] loading 0 listener(s)\r\n[2019-12-03 18:54:13.006][12][info][config] [source/server/configuration_impl.cc:97] loading tracing configuration\r\n[2019-12-03 18:54:13.006][12][info][config] [source/server/configuration_impl.cc:117] loading stats sink configuration\r\n[2019-12-03 18:54:13.006][12][debug][init] [source/common/init/manager_impl.cc:20] added target LDS to init manager Server\r\n[2019-12-03 18:54:13.006][12][info][main] [source/server/server.cc:549] starting main dispatch loop\r\n[2019-12-03 18:54:13.006][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 3\r\n[2019-12-03 18:54:13.006][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.006][12][debug][connection] [source/common/network/connection_impl.cc:559] [C0] connected\r\n[2019-12-03 18:54:13.006][12][debug][client] [source/common/http/codec_client.cc:69] [C0] connected\r\n[2019-12-03 18:54:13.006][12][debug][pool] [source/common/http/http2/conn_pool.cc:98] [C0] creating stream\r\n[2019-12-03 18:54:13.007][12][debug][router] [source/common/router/router.cc:1618] [C0][S15852061503706113562] pool ready\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C0] send data: bytes=24\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:398] [C0] writing 24 bytes, end_stream false\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C0] about to send frame type=4, flags=0\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C0] send data: bytes=21\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:398] [C0] writing 21 bytes, end_stream false\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C0] sent frame type=4\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C0] about to send frame type=8, flags=0\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C0] send data: bytes=13\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:398] [C0] writing 13 bytes, end_stream false\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C0] sent frame type=8\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C0] about to send frame type=1, flags=4\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C0] send data: bytes=144\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:398] [C0] writing 144 bytes, end_stream false\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C0] sent frame type=1\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:398] [C0] writing 175 bytes, end_stream false\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C0] sent frame type=0\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C0] write returns: 377\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:500] [C0] read ready\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C0] read returns: 21\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C0] read error: Resource temporarily unavailable\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C0] dispatching 21 bytes\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C0] about to recv frame type=4, flags=0\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C0] recv frame type=4\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C0] dispatched 21 bytes\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C0] about to send frame type=4, flags=1\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C0] send data: bytes=9\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:398] [C0] writing 9 bytes, end_stream false\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C0] sent frame type=4\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 2\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C0] write returns: 9\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 3\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/connection_impl.cc:500] [C0] read ready\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C0] read returns: 9\r\n[2019-12-03 18:54:13.007][12][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C0] read error: Resource temporarily unavailable\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C0] dispatching 9 bytes\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C0] about to recv frame type=4, flags=1\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C0] recv frame type=4\r\n[2019-12-03 18:54:13.007][12][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C0] dispatched 9 bytes\r\n[2019-12-03 18:54:13.008][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 3\r\n[2019-12-03 18:54:13.008][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.008][12][trace][connection] [source/common/network/connection_impl.cc:500] [C0] read ready\r\n[2019-12-03 18:54:13.008][12][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C0] read returns: 30\r\n[2019-12-03 18:54:13.008][12][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C0] read error: Resource temporarily unavailable\r\n[2019-12-03 18:54:13.008][12][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C0] dispatching 30 bytes\r\n[2019-12-03 18:54:13.008][12][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C0] about to recv frame type=8, flags=0\r\n[2019-12-03 18:54:13.008][12][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C0] recv frame type=8\r\n[2019-12-03 18:54:13.008][12][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C0] about to recv frame type=6, flags=0\r\n[2019-12-03 18:54:13.008][12][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C0] recv frame type=6\r\n[2019-12-03 18:54:13.008][12][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C0] dispatched 30 bytes\r\n[2019-12-03 18:54:13.008][12][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C0] about to send frame type=6, flags=1\r\n[2019-12-03 18:54:13.008][12][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C0] send data: bytes=17\r\n[2019-12-03 18:54:13.008][12][trace][connection] [source/common/network/connection_impl.cc:398] [C0] writing 17 bytes, end_stream false\r\n[2019-12-03 18:54:13.008][12][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C0] sent frame type=6\r\n[2019-12-03 18:54:13.008][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 2\r\n[2019-12-03 18:54:13.008][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.008][12][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C0] write returns: 17\r\n[2019-12-03 18:54:13.009][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 3\r\n[2019-12-03 18:54:13.009][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.009][12][trace][connection] [source/common/network/connection_impl.cc:500] [C0] read ready\r\n[2019-12-03 18:54:13.009][12][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C0] read returns: 263\r\n[2019-12-03 18:54:13.009][12][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C0] read error: Resource temporarily unavailable\r\n[2019-12-03 18:54:13.009][12][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C0] dispatching 263 bytes\r\n[2019-12-03 18:54:13.009][12][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C0] about to recv frame type=1, flags=4\r\n[2019-12-03 18:54:13.011][12][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C0] recv frame type=1\r\n[2019-12-03 18:54:13.011][12][debug][router] [source/common/router/router.cc:1036] [C0][S15852061503706113562] upstream headers complete: end_stream=false\r\n[2019-12-03 18:54:13.011][12][debug][http] [source/common/http/async_client_impl.cc:93] async http request response headers (end_stream=false):\r\n':status', '200'\r\n'content-type', 'application/grpc'\r\n\r\n[2019-12-03 18:54:13.011][12][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C0] about to recv frame type=0, flags=0\r\n[2019-12-03 18:54:13.011][12][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C0] recv frame type=0\r\n[2019-12-03 18:54:13.011][12][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=231 end_stream=false)\r\n[2019-12-03 18:54:13.011][12][debug][config] [source/common/config/new_grpc_mux_impl.cc:57] Received DeltaDiscoveryResponse for type.googleapis.com/envoy.api.v2.Cluster at version nebula-tcp 1015e8c2346a38720a49e3464655ea5d58096d6b (wip) mar dic  3 18:53:21 UTC 2019\r\n[2019-12-03 18:54:13.011][12][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 0 cluster(s)\r\n[2019-12-03 18:54:13.012][12][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:554] add/update cluster cluster-1 during init\r\n[2019-12-03 18:54:13.012][12][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:592] adding TLS cluster cluster-1\r\n[2019-12-03 18:54:13.012][12][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:74] cm init: adding: cluster=cluster-1 primary=0 secondary=1\r\n[2019-12-03 18:54:13.012][12][info][upstream] [source/common/upstream/cds_api_impl.cc:83] cds: add/update cluster 'cluster-1'\r\n[2019-12-03 18:54:13.012][12][info][upstream] [source/common/upstream/cluster_manager_impl.cc:136] cm init: initializing secondary clusters\r\n[2019-12-03 18:54:13.012][12][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:114] initializing secondary cluster cluster-1\r\n[2019-12-03 18:54:13.012][12][trace][router] [source/common/router/router.cc:1487] [C0][S15852061503706113562] proxying 191 bytes\r\n[2019-12-03 18:54:13.012][12][debug][config] [source/common/config/delta_subscription_state.cc:108] Delta config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources added, 0 removed\r\n[2019-12-03 18:54:13.012][12][trace][router] [source/common/router/router.cc:1487] [C0][S15852061503706113562] proxying 169 bytes\r\n[2019-12-03 18:54:13.012][12][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C0] dispatched 263 bytes\r\n[2019-12-03 18:54:13.012][12][trace][connection] [source/common/network/connection_impl.cc:398] [C0] writing 369 bytes, end_stream false\r\n[2019-12-03 18:54:13.012][12][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C0] sent frame type=0\r\n[2019-12-03 18:54:13.012][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 2\r\n[2019-12-03 18:54:13.012][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.012][12][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C0] write returns: 369\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 3\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:500] [C0] read ready\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C0] read returns: 30\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C0] read error: Resource temporarily unavailable\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C0] dispatching 30 bytes\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C0] about to recv frame type=8, flags=0\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C0] recv frame type=8\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C0] about to recv frame type=6, flags=0\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C0] recv frame type=6\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C0] dispatched 30 bytes\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C0] about to send frame type=6, flags=1\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C0] send data: bytes=17\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:398] [C0] writing 17 bytes, end_stream false\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C0] sent frame type=6\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 2\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C0] write returns: 17\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 3\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:500] [C0] read ready\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C0] read returns: 285\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C0] read error: Resource temporarily unavailable\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C0] dispatching 285 bytes\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C0] about to recv frame type=0, flags=0\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C0] recv frame type=0\r\n[2019-12-03 18:54:13.013][12][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=276 end_stream=false)\r\n[2019-12-03 18:54:13.013][12][debug][config] [source/common/config/new_grpc_mux_impl.cc:57] Received DeltaDiscoveryResponse for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment at version nebula-tcp 1015e8c2346a38720a49e3464655ea5d58096d6b (wip) mar dic  3 18:53:21 UTC 2019\r\n[2019-12-03 18:54:13.013][12][debug][config] [source/common/config/delta_subscription_state.cc:108] Delta config for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment accepted with 1 resources added, 0 removed\r\n[2019-12-03 18:54:13.013][12][trace][router] [source/common/router/router.cc:1487] [C0][S15852061503706113562] proxying 183 bytes\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C0] dispatched 285 bytes\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:398] [C0] writing 192 bytes, end_stream false\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C0] sent frame type=0\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 2\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C0] write returns: 192\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 3\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:500] [C0] read ready\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C0] read returns: 30\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C0] read error: Resource temporarily unavailable\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C0] dispatching 30 bytes\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C0] about to recv frame type=8, flags=0\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C0] recv frame type=8\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C0] about to recv frame type=6, flags=0\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C0] recv frame type=6\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C0] dispatched 30 bytes\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C0] about to send frame type=6, flags=1\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C0] send data: bytes=17\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:398] [C0] writing 17 bytes, end_stream false\r\n[2019-12-03 18:54:13.013][12][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C0] sent frame type=6\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:462] [C0] socket event: 2\r\n[2019-12-03 18:54:13.013][12][trace][connection] [source/common/network/connection_impl.cc:550] [C0] write ready\r\n[2019-12-03 18:54:13.014][12][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C0] write returns: 17\r\n[2019-12-03 18:54:18.006][12][debug][main] [source/server/server.cc:175] flushing stats\r\n[2019-12-03 18:54:18.006][12][debug][main] [source/server/server.cc:185] Envoy is not fully initialized, skipping histogram merge and flushing stats\r\n[2019-12-03 18:54:23.014][12][debug][main] [source/server/server.cc:175] flushing stats\r\n[2019-12-03 18:54:23.014][12][debug][main] [source/server/server.cc:185] Envoy is not fully initialized, skipping histogram merge and flushing stats\r\n[2019-12-03 18:54:28.017][12][debug][main] [source/server/server.cc:175] flushing stats\r\n[2019-12-03 18:54:28.017][12][debug][main] [source/server/server.cc:185] Envoy is not fully initialized, skipping histogram merge and flushing stats\r\n[2019-12-03 18:54:33.020][12][debug][main] [source/server/server.cc:175] flushing stats\r\n[2019-12-03 18:54:33.021][12][debug][main] [source/server/server.cc:185] Envoy is not fully initialized, skipping histogram merge and flushing stats\r\n[2019-12-03 18:54:38.025][12][debug][main] [source/server/server.cc:175] flushing stats\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9206/comments",
    "author": "gabrielvelo",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-12-03T21:07:02Z",
        "body": "cc @fredlas @wgallagher "
      },
      {
        "user": "fredlas",
        "created_at": "2019-12-03T21:36:32Z",
        "body": "In `GrpcMuxImpl::handleStreamEstablishmentFailure()` (NewGrpcMuxImpl in the current code), I included an odd little block of code that was necessary to avoid a crash in an integration test. The comment says:\r\n\r\n\"If this happens while Envoy is still initializing, the onConfigUpdateFailed() we ultimately call on CDS will cause LDS to start up, which adds to subscriptions_ here. So, to avoid a crash, the iteration needs to dance around a little: collect pointers to all SubscriptionStates, call on all those pointers we haven't yet called on, repeat if there are now more SubscriptionStates.\"\r\n\r\nBased on what does/doesn't work, it sounds like hitting that block of code might be what makes things work for you?\r\n\r\nThe non-failure `handleEstablishedStream()` doesn't do anything remotely interesting, though, so there can't be an exact parallel. But, maybe the first successful `onConfigUpdate` for CDS would also triggers the start of LDS in a similar way, and somehow the LDS is similarly left waiting for something to happen that never will?"
      },
      {
        "user": "gabrielvelo",
        "created_at": "2019-12-05T13:04:14Z",
        "body": "@fredlas  I added some extra log lines and I can confirm that the nominal case (  a successfully connection from the control plane ) doesn't work as expected. In this case envoy will only send request for `type.googleapis.com/envoy.api.v2.Cluster` and `type.googleapis.com/envoy.api.v2.ClusterLoadAssignment` . In the other hand when the initial conection fails and the flow hit `GrpcMuxImpl::handleStreamEstablishmentFailure()` the discovery messages sequence looks good."
      },
      {
        "user": "fredlas",
        "created_at": "2019-12-05T15:49:17Z",
        "body": "Ok, thanks for the confirmation, in particular the fact that there is no LDS request sent.\r\n\r\nI think that means this needs to be treated as a bug (or at least particularly delicate finnickiness) in the init machinery, as opposed to the implementation of xDS. When someone starts up a Subscription, an initial request is going to get sent, unless there's a problem with the connectivity (which we know there isn't, since you're using ADS and other xDSes sharing the mux are able to send requests). If no request is sent, then the issue is that Envoy chose not to even try to start up LDS. Given that it takes an initial failure+retry to make things work, I suspect the issue's particular flavor is something like \"there is a complex dance with things waiting on each other and strict dependency/ordering needs, and something is happening in the wrong sequence\".\r\n\r\nUnfortunately, if the init machinery is the source of the problem, that means I can't help further. The logic in `handleStreamEstablishmentFailure()` is the closest I've gotten to that part of Envoy. My best guess redirection is @mergeconflict, just because I'm aware of him having done an overhaul of the structure of the init stuff."
      },
      {
        "user": "fredlas",
        "created_at": "2019-12-13T23:10:59Z",
        "body": "The problem that we have discussed in PR #9189 is looking to be the same, or at least very similar to, this one: init hangs because listeners are not gotten.\r\n\r\nFrom what we're seeing here - Envoy just doesn't even attempt to start LDS, but when nudged by an error things start working - it looks like there was a pre-existing way for the init code to get stuck. It apparently wasn't actually possible to reach it with the old SotW xDS code, so the current state of master is fine, but it suggests there may be some real fragility there.\r\n\r\nI don't know enough about the init process to have ideas, and unfortunately I don't have enough time to learn. Hopefully, attempts to do a piecemeal version of #9189 will resurface this problem in an easier to nail down way. I think that is most likely to work if the init code is getting some scrutiny at the same time."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-12T23:45:45Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-20T07:57:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9180,
    "title": "Question: Can we configure ALPN on PATH based?",
    "created_at": "2019-12-02T14:33:37Z",
    "closed_at": "2020-01-13T00:45:50Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9180",
    "body": "**Issue Template**\r\n\r\n*Title*: *Can we configure Envoy Upstream ALPN on PATH based*\r\n\r\n*Description*:\r\nI have Envoy behind AWS NLB (Network Load Balancer) with TCP Listener. NLB can have Health Check communicate to EC2 only on HTTP1.1 _[Use of TCP Health Check is not really recommended]_. As I we want to restrict codec_type to http2 only, Is it possible to setup http1.1 only for /health route so that NLB Health Check can connect to it on same listener port (443) and all other path follow strict h2.\r\n\r\n/health goes to Upstream \r\n\r\nCurrent Used Config:\r\n\r\n```yaml\r\nstatic_resources:\r\n    listeners:\r\n        address:\r\n            socket_address:\r\n                address: 0.0.0.0\r\n                port_value: 443\r\n        filter_chains:\r\n            filters:\r\n            -   name: envoy.http_connection_manager\r\n                typed_config:\r\n                    '@type': type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n                    codec_type: auto\r\n                    use_remote_address: true\r\n                    stat_prefix: gateway_ingress_http\r\n                    access_log:\r\n                        name: envoy.file_access_log\r\n                        config:\r\n                            path: /var/log/envoy/access.log\r\n                            format: \"[%START_TIME(%d/%m/%YT%H:%M:%S%z %s)%] method=%REQ(:METHOD)%\\\r\n                                \\ authority=%REQ(:AUTHORITY)% txId=%RESP(TNX_ID)%\\\r\n                                \\ request=%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% alpn=%PROTOCOL%\\\r\n                                \\ response_flag=%RESPONSE_FLAGS% ssl_protocol=%DOWNSTREAM_TLS_VERSION%\\\r\n                                \\ ssl_cipher=%DOWNSTREAM_TLS_CIPHER% status=%RESPONSE_CODE%\\\r\n                                \\ request_size=%BYTES_RECEIVED% response_size=%BYTES_SENT%\\\r\n                                \\ envoy_request_id=%REQ(X-REQUEST-ID)% user-agent=%REQ(USER-AGENT)%\\\r\n                                \\ xFor=%DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT% x-envoy-upstream-service-time=%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\\\r\n                                \\ total_time_ms=%DURATION% upstream_cluster=%UPSTREAM_CLUSTER%\\\r\n                                \\ upstream=%UPSTREAM_HOST%\\n\"\r\n                    route_config:\r\n                        name: gateway_portless\r\n                        virtual_hosts:\r\n                        -   name: gateway_portless\r\n                            domains:\r\n                            -   '*'\r\n                            routes:\r\n                            -   match:\r\n                                    prefix: /\r\n                                route:\r\n                                    cluster: gateway_portless\r\n                    http_filters:\r\n                    -   name: envoy.gzip\r\n                        typed_config: {}\r\n                    -   name: envoy.grpc_web\r\n                        typed_config: {}\r\n                    -   name: envoy.grpc_http1_bridge\r\n                        typed_config: {}\r\n                    -   name: envoy.router\r\n                        typed_config: {}\r\n            transport_socket:\r\n                name: envoy.transport_sockets.tls\r\n                typed_config:\r\n                    '@type': type.googleapis.com/envoy.api.v2.auth.DownstreamTlsContext\r\n                    common_tls_context:\r\n                        tls_params:\r\n                            tls_maximum_protocol_version: TLSv1_3\r\n                            tls_minimum_protocol_version: TLSv1_2\r\n                        alpn_protocols: h2\r\n                        tls_certificates:\r\n                        -   certificate_chain:\r\n                                filename: /etc/envoy/ssl/sw10.crt\r\n                            private_key:\r\n                                filename: /etc/envoy/ssl/sw10.key\r\n    clusters:\r\n    -   name: gateway_portless\r\n        http2_protocol_options: {}\r\n        connect_timeout: 1s\r\n        type: static\r\n        dns_lookup_family: V4_ONLY\r\n        lb_policy: round_robin\r\n        load_assignment:\r\n            cluster_name: gateway_portless\r\n            endpoints:\r\n            -   lb_endpoints:\r\n                -   endpoint:\r\n                        address:\r\n                            socket_address:\r\n                                address: 127.0.0.1\r\n                                port_value: 7000\r\nadmin:\r\n    access_log_path: /var/log/envoy/admin.log\r\n    address:\r\n        socket_address:\r\n            address: 127.0.0.1\r\n            port_value: 6000\r\n```\r\n\r\nor, if there is any alternate implementation done, please suggest?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9180/comments",
    "author": "yks0000",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-12-06T22:11:43Z",
        "body": "You can have two filter chain, one matches h2 alpn with `FilterChainMatch` and another matches http1.1 ALPN, in the h2 filter chain use full route config and h1 filter chain config with only /health."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-06T00:14:49Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-13T00:45:49Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9179,
    "title": "redis_proxy: NOAUTH Authentication required while auth ok return",
    "created_at": "2019-12-02T14:20:55Z",
    "closed_at": "2019-12-04T06:54:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9179",
    "body": "*Title*:  (error) NOAUTH Authentication required while auth ok return \r\n\r\n*Description*:\r\n> login the envoy proxy, auth return ok but when type set cmd ,it return error\r\n\r\n\r\n*Config*:\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - name: redis_listener\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 33336\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.redis_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.redis_proxy.v2.RedisProxy\r\n          stat_prefix: egress_redis\r\n          settings:\r\n            op_timeout: 5s\r\n            enable_redirection: \"true\"\r\n          prefix_routes:\r\n            catch_all_route:\r\n              cluster: redis_cluster\r\n          downstream_auth_password:\r\n            inline_string: \"123456\"\r\n  clusters:\r\n  - name: redis_cluster\r\n    connect_timeout: 1s\r\n    type: strict_dns # static\r\n    lb_policy: MAGLEV\r\n    load_assignment:\r\n      cluster_name: redis_cluster\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: dhc-cc-test.cc-test.svc.cluster.local\r\n                port_value: 6379\r\n\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 33337\r\n```\r\n\r\n*Logs*:\r\n```\r\n[root@10-25-83-4 envoy]# redis-cli -h 10.25.152.90 -p 33310 -c -a 123456\r\n10.25.152.90:33310> auth 123\r\n(error) ERR invalid password\r\n10.25.152.90:33310> auth 123456\r\nOK\r\n10.25.152.90:33310> set aaa 123\r\n(error) NOAUTH Authentication required.\r\n10.25.152.90:33310>\r\n```\r\n\r\n\r\n\r\n*admin cmd output *:\r\n```\r\nredis_cluster::default_priority::max_connections::1024\r\nredis_cluster::default_priority::max_pending_requests::1024\r\nredis_cluster::default_priority::max_requests::1024\r\nredis_cluster::default_priority::max_retries::3\r\nredis_cluster::high_priority::max_connections::1024\r\nredis_cluster::high_priority::max_pending_requests::1024\r\nredis_cluster::high_priority::max_requests::1024\r\nredis_cluster::high_priority::max_retries::3\r\nredis_cluster::added_via_api::false\r\nredis_cluster::10.25.232.175:6379::cx_active::0\r\nredis_cluster::10.25.232.175:6379::cx_connect_fail::0\r\nredis_cluster::10.25.232.175:6379::cx_total::0\r\nredis_cluster::10.25.232.175:6379::rq_active::0\r\nredis_cluster::10.25.232.175:6379::rq_error::0\r\nredis_cluster::10.25.232.175:6379::rq_success::0\r\nredis_cluster::10.25.232.175:6379::rq_timeout::0\r\nredis_cluster::10.25.232.175:6379::rq_total::0\r\nredis_cluster::10.25.232.175:6379::hostname::dhc-cc-test.cc-test.svc.cluster.local\r\nredis_cluster::10.25.232.175:6379::health_flags::healthy\r\nredis_cluster::10.25.232.175:6379::weight::1\r\nredis_cluster::10.25.232.175:6379::region::\r\nredis_cluster::10.25.232.175:6379::zone::\r\nredis_cluster::10.25.232.175:6379::sub_zone::\r\nredis_cluster::10.25.232.175:6379::canary::false\r\nredis_cluster::10.25.232.175:6379::priority::0\r\nredis_cluster::10.25.232.175:6379::success_rate::-1\r\nredis_cluster::10.25.232.175:6379::local_origin_success_rate::-1\r\nredis_cluster::10.25.35.221:6379::cx_active::0\r\nredis_cluster::10.25.35.221:6379::cx_connect_fail::0\r\nredis_cluster::10.25.35.221:6379::cx_total::0\r\nredis_cluster::10.25.35.221:6379::rq_active::0\r\nredis_cluster::10.25.35.221:6379::rq_error::0\r\nredis_cluster::10.25.35.221:6379::rq_success::0\r\nredis_cluster::10.25.35.221:6379::rq_timeout::0\r\nredis_cluster::10.25.35.221:6379::rq_total::0\r\nredis_cluster::10.25.35.221:6379::hostname::dhc-cc-test.cc-test.svc.cluster.local\r\nredis_cluster::10.25.35.221:6379::health_flags::healthy\r\nredis_cluster::10.25.35.221:6379::weight::1\r\nredis_cluster::10.25.35.221:6379::region::\r\nredis_cluster::10.25.35.221:6379::zone::\r\nredis_cluster::10.25.35.221:6379::sub_zone::\r\nredis_cluster::10.25.35.221:6379::canary::false\r\nredis_cluster::10.25.35.221:6379::priority::0\r\nredis_cluster::10.25.35.221:6379::success_rate::-1\r\nredis_cluster::10.25.35.221:6379::local_origin_success_rate::-1\r\nredis_cluster::10.25.230.16:6379::cx_active::0\r\nredis_cluster::10.25.230.16:6379::cx_connect_fail::0\r\nredis_cluster::10.25.230.16:6379::cx_total::0\r\nredis_cluster::10.25.230.16:6379::rq_active::0\r\nredis_cluster::10.25.230.16:6379::rq_error::0\r\nredis_cluster::10.25.230.16:6379::rq_success::0\r\nredis_cluster::10.25.230.16:6379::rq_timeout::0\r\nredis_cluster::10.25.230.16:6379::rq_total::0\r\nredis_cluster::10.25.230.16:6379::hostname::dhc-cc-test.cc-test.svc.cluster.local\r\nredis_cluster::10.25.230.16:6379::health_flags::healthy\r\nredis_cluster::10.25.230.16:6379::weight::1\r\nredis_cluster::10.25.230.16:6379::region::\r\nredis_cluster::10.25.230.16:6379::zone::\r\nredis_cluster::10.25.230.16:6379::sub_zone::\r\nredis_cluster::10.25.230.16:6379::canary::false\r\nredis_cluster::10.25.230.16:6379::priority::0\r\nredis_cluster::10.25.230.16:6379::success_rate::-1\r\nredis_cluster::10.25.230.16:6379::local_origin_success_rate::-1\r\nredis_cluster::10.25.94.85:6379::cx_active::0\r\nredis_cluster::10.25.94.85:6379::cx_connect_fail::0\r\nredis_cluster::10.25.94.85:6379::cx_total::0\r\nredis_cluster::10.25.94.85:6379::rq_active::0\r\nredis_cluster::10.25.94.85:6379::rq_error::0\r\nredis_cluster::10.25.94.85:6379::rq_success::0\r\nredis_cluster::10.25.94.85:6379::rq_timeout::0\r\nredis_cluster::10.25.94.85:6379::rq_total::0\r\nredis_cluster::10.25.94.85:6379::hostname::dhc-cc-test.cc-test.svc.cluster.local\r\nredis_cluster::10.25.94.85:6379::health_flags::healthy\r\nredis_cluster::10.25.94.85:6379::weight::1\r\nredis_cluster::10.25.94.85:6379::region::\r\nredis_cluster::10.25.94.85:6379::zone::\r\nredis_cluster::10.25.94.85:6379::sub_zone::\r\nredis_cluster::10.25.94.85:6379::canary::false\r\nredis_cluster::10.25.94.85:6379::priority::0\r\nredis_cluster::10.25.94.85:6379::success_rate::-1\r\nredis_cluster::10.25.94.85:6379::local_origin_success_rate::-1\r\nredis_cluster::10.25.230.247:6379::cx_active::1\r\nredis_cluster::10.25.230.247:6379::cx_connect_fail::0\r\nredis_cluster::10.25.230.247:6379::cx_total::1\r\nredis_cluster::10.25.230.247:6379::rq_active::0\r\nredis_cluster::10.25.230.247:6379::rq_error::0\r\nredis_cluster::10.25.230.247:6379::rq_success::0\r\nredis_cluster::10.25.230.247:6379::rq_timeout::0\r\nredis_cluster::10.25.230.247:6379::rq_total::1\r\nredis_cluster::10.25.230.247:6379::hostname::dhc-cc-test.cc-test.svc.cluster.local\r\nredis_cluster::10.25.230.247:6379::health_flags::healthy\r\nredis_cluster::10.25.230.247:6379::weight::1\r\nredis_cluster::10.25.230.247:6379::region::\r\nredis_cluster::10.25.230.247:6379::zone::\r\nredis_cluster::10.25.230.247:6379::sub_zone::\r\nredis_cluster::10.25.230.247:6379::canary::false\r\nredis_cluster::10.25.230.247:6379::priority::0\r\nredis_cluster::10.25.230.247:6379::success_rate::-1\r\nredis_cluster::10.25.230.247:6379::local_origin_success_rate::-1\r\nredis_cluster::10.25.34.121:6379::cx_active::0\r\nredis_cluster::10.25.34.121:6379::cx_connect_fail::0\r\nredis_cluster::10.25.34.121:6379::cx_total::0\r\nredis_cluster::10.25.34.121:6379::rq_active::0\r\nredis_cluster::10.25.34.121:6379::rq_error::0\r\nredis_cluster::10.25.34.121:6379::rq_success::0\r\nredis_cluster::10.25.34.121:6379::rq_timeout::0\r\nredis_cluster::10.25.34.121:6379::rq_total::0\r\nredis_cluster::10.25.34.121:6379::hostname::dhc-cc-test.cc-test.svc.cluster.local\r\nredis_cluster::10.25.34.121:6379::health_flags::healthy\r\nredis_cluster::10.25.34.121:6379::weight::1\r\nredis_cluster::10.25.34.121:6379::region::\r\nredis_cluster::10.25.34.121:6379::zone::\r\nredis_cluster::10.25.34.121:6379::sub_zone::\r\nredis_cluster::10.25.34.121:6379::canary::false\r\nredis_cluster::10.25.34.121:6379::priority::0\r\nredis_cluster::10.25.34.121:6379::success_rate::-1\r\nredis_cluster::10.25.34.121:6379::local_origin_success_rate::-1\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9179/comments",
    "author": "denghuancong",
    "comments": [
      {
        "user": "denghuancong",
        "created_at": "2019-12-03T01:59:47Z",
        "body": "envoy version:\r\n```\r\nenvoyproxy / envoy : latest\r\nDIGEST:sha256:c485247f63de16331fbe56aec8e9d8a83da76ca0fa8dc872691a09b8d99a314f\r\n```"
      },
      {
        "user": "denghuancong",
        "created_at": "2019-12-04T06:45:48Z",
        "body": "add cluster config :\r\n```\r\n   extension_protocol_options:\r\n      envoy.redis_proxy: { auth_password: { inline_string: \"123456\" }}\r\n```"
      },
      {
        "user": "denghuancong",
        "created_at": "2019-12-04T06:46:40Z",
        "body": "@zuercher @dio  this issue should be close."
      },
      {
        "user": "wluo",
        "created_at": "2021-01-14T00:56:48Z",
        "body": "I'm afraid this is still an issue in 1.17.  I do set the same redis password in both upstream cluster and downstream listener, but after redis-cli connects to envoy and even after issuing AUTH command with OK result, subsequent commands all return with NOAUTH error.  The identical config worked just fine in 1.16.  It's as if Envoy did not keep the client password or did not pass on the client password to the upstream cluster."
      }
    ]
  },
  {
    "number": 9141,
    "title": "Envoy not able to reload successfully in case of GRPC Cluster deletion",
    "created_at": "2019-11-26T14:25:19Z",
    "closed_at": "2020-01-03T00:24:34Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9141",
    "body": "*Description*:\r\n\r\nWe use envoy as a sidecar in our API Gateway, all the ingress and egress(REST And GRPC) calls passes through envoy.\r\nSince many services use envoy, we have common role to support any of the config json or yaml. In this case we are using yaml.\r\n`(/usr/bin/envoy -c /etc/envoy/envoy.json --restart-epoch $RESTART_EPOCH) || (/usr/bin/envoy -c /etc/envoy/envoy.yaml --restart-epoch $RESTART_EPOCH)`\r\n\r\nWe recently faced an issue where on deleting GRPC cluster and reloading the envoy-sidecar(upstart conf for envoy), envoy in its current state is failing (check logs for reference)\r\nalthough YAML configuration is okay and envoy sidecar is also running but everything is failing inside because every process exited for the above OR script case. It requires to first stop and then start the envoy sidecar service to run successfully.\r\nBut if we update that script to pick only one config file it works fine on envoy reload. `/usr/bin/envoy -c /etc/envoy/envoy.yaml --restart-epoch $RESTART_EPOCH`\r\nOn deleting REST clusters and even while adding REST/GRPC cluster, the above OR script works fine though.\r\n\r\nEnvoy version\r\n`envoy version: 56c192d6e0fbba1858f75c68722ffc9b21377110/1.9.0-dev/Clean/RELEASE`\r\n\r\n*Repro steps*:\r\n1) Remove GRPC cluster from envoy.yaml\r\n```\r\n  - name: athena_grpc\r\n    connect_timeout: 5s\r\n    type: strict_dns\r\n    # Comment out the following line to test on v6 networks\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    http2_protocol_options: {}\r\n    hosts:\r\n    - socket_address:\r\n        address: athena.service.consul\r\n        port_value: 7494\r\n```\r\n\r\n2) reload envoy-sidecar\r\n3) Envoy-sidecar will be running but all listeners will be failing\r\n\r\n*Config*:\r\n\r\n\r\nenvoy-sidecar.conf\r\n\r\n```\r\nstart on (local-filesystems and net-device-up IFACE!=lo)\r\nstop on runlevel [06]\r\n\r\nscript\r\n    exec python /etc/envoy/hot-restarter.py /etc/envoy/start_envoy.sh >> /var/log/envoy/envoy.error.log\r\nend script\r\n\r\nrespawn\r\nrespawn limit 10 10\r\nkill timeout 10\r\n\r\n```\r\n\r\n\r\nstart_envoy.sh\r\n\r\n```\r\n#!/bin/bash\r\nulimit -n 102400\r\n# test configuration\r\n#exec /usr/bin/envoy --mode validate -c /etc/envoy/envoy.yaml\r\n# serve traffic\r\n(/usr/bin/envoy -c /etc/envoy/envoy.json --restart-epoch $RESTART_EPOCH)  || (/usr/bin/envoy -c /etc/envoy/envoy.yaml --restart-epoch $RESTART_EPOCH)\r\n```\r\n\r\nenvoy.yaml\r\n\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: UDP\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_6\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: main_repo\r\n          http_filters:\r\n          - name: envoy.cors\r\n          - name: envoy.router\r\n\r\n  - name: listener_1\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 9002\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: egress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/accounts/\"\r\n                route:\r\n                  prefix_rewrite: \"/\"\r\n                  cluster: accounts\r\n          http_filters:\r\n          - name: envoy.router\r\n\r\n  - name: listener_2\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 9003\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          codec_type: auto\r\n          stat_prefix: egress_grpc\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/athena.\"\r\n                route:\r\n                  cluster: athena_grpc\r\n          http_filters:\r\n          - name: envoy.grpc_web          \r\n          - name: envoy.router\r\n\r\n  clusters:\r\n  - name: main_repo\r\n    connect_timeout: 5s\r\n    type: strict_dns\r\n    # Comment out the following line to test on v6 networks\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    hosts:\r\n    - socket_address:\r\n        address: localhost\r\n        port_value: 8999\r\n\r\n  - name: accounts\r\n    connect_timeout: 5s\r\n    type: strict_dns\r\n    # Comment out the following line to test on v6 networks\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    hosts:\r\n    - socket_address:\r\n        address: accounts.service.consul\r\n        port_value: 80\r\n\r\n  - name: athena_grpc\r\n    connect_timeout: 5s\r\n    type: strict_dns\r\n    # Comment out the following line to test on v6 networks\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    http2_protocol_options: {}\r\n    hosts:\r\n    - socket_address:\r\n        address: athena.service.consul\r\n        port_value: 7494\r\n\r\n```\r\n\r\n*Logs*:\r\n\r\nLogs while deleting GRPC cluster\r\n\r\n```\r\n[2019-11-22 20:57:33.998][006771][info][main] [source/server/server.cc:229]   transport_sockets.upstream: envoy.transport_sockets.alts,envoy.transport_sockets.capture,raw_buffer,tls\r\n[2019-11-22 20:57:33.999][006771][critical][main] [source/server/server.cc:86] error initializing configuration '/etc/envoy/envoy.json': unable to read file: /etc/envoy/envoy.json\r\n[2019-11-22 20:57:33.999][006771][info][main] [source/server/server.cc:501] exiting\r\n[2019-11-22 20:57:34.051][006749][warning][main] [source/server/server.cc:400] caught SIGTERM\r\n[2019-11-22 20:57:34.052][006749][info][main] [source/server/server.cc:461] main dispatch loop exited\r\n[2019-11-22 20:57:34.115][006749][info][main] [source/server/server.cc:501] exiting\r\n```\r\n\r\nLogs while deleting REST cluster\r\n\r\n```\r\n[2019-11-22 20:56:40.610][006748][info][main] [source/server/server.cc:229]   transport_sockets.upstream: envoy.transport_sockets.alts,envoy.transport_sockets.capture,raw_buffer,tls\r\n[2019-11-22 20:56:40.611][006748][critical][main] [source/server/server.cc:86] error initializing configuration '/etc/envoy/envoy.json': unable to read file: /etc/envoy/envoy.json\r\n[2019-11-22 20:56:40.611][006748][info][main] [source/server/server.cc:501] exiting\r\n[2019-11-22 20:56:40.632][006749][info][main] [source/server/server.cc:206] initializing epoch 0 (hot restart version=10.200.16384.127.options=capacity=16384, num_slots=8209 hash=228984379728933363 size=2654312)\r\n[2019-11-22 20:56:40.636][006749][info][main] [source/server/server.cc:208] statically linked extensions:\r\n[2019-11-22 20:56:40.636][006749][info][main] [source/server/server.cc:210]   access_loggers: envoy.file_access_log,envoy.http_grpc_access_log\r\n[2019-11-22 20:56:40.636][006749][info][main] [source/server/server.cc:213]   filters.http: envoy.buffer,envoy.cors,envoy.ext_authz,envoy.fault,envoy.filters.http.header_to_metadata,envoy.filters.http.jwt_authn,en\r\nvoy.filters.http.rbac,envoy.grpc_http1_bridge,envoy.grpc_json_transcoder,envoy.grpc_web,envoy.gzip,envoy.health_check,envoy.http_dynamo_filter,envoy.ip_tagging,envoy.lua,envoy.rate_limit,envoy.router,envoy.squash\r\n[2019-11-22 20:56:40.637][006749][info][main] [source/server/server.cc:216]   filters.listener: envoy.listener.original_dst,envoy.listener.proxy_protocol,envoy.listener.tls_inspector\r\n[2019-11-22 20:56:40.637][006749][info][main] [source/server/server.cc:219]   filters.network: envoy.client_ssl_auth,envoy.echo,envoy.ext_authz,envoy.filters.network.dubbo_proxy,envoy.filters.network.rbac,envoy.fi\r\nlters.network.sni_cluster,envoy.filters.network.thrift_proxy,envoy.http_connection_manager,envoy.mongo_proxy,envoy.ratelimit,envoy.redis_proxy,envoy.tcp_proxy\r\n[2019-11-22 20:56:40.637][006749][info][main] [source/server/server.cc:221]   stat_sinks: envoy.dog_statsd,envoy.metrics_service,envoy.stat_sinks.hystrix,envoy.statsd\r\n[2019-11-22 20:56:40.637][006749][info][main] [source/server/server.cc:223]   tracers: envoy.dynamic.ot,envoy.lightstep,envoy.tracers.datadog,envoy.zipkin\r\n[2019-11-22 20:56:40.638][006749][info][main] [source/server/server.cc:226]   transport_sockets.downstream: envoy.transport_sockets.alts,envoy.transport_sockets.capture,raw_buffer,tls\r\n[2019-11-22 20:56:40.638][006749][info][main] [source/server/server.cc:229]   transport_sockets.upstream: envoy.transport_sockets.alts,envoy.transport_sockets.capture,raw_buffer,tls\r\n[2019-11-22 20:56:40.707][006749][info][main] [source/server/server.cc:271] admin address: 0.0.0.0:8001\r\n[2019-11-22 20:56:40.721][006749][info][config] [source/server/configuration_impl.cc:50] loading 0 static secret(s)\r\n[2019-11-22 20:56:40.721][006749][info][config] [source/server/configuration_impl.cc:56] loading 177 cluster(s)\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9141/comments",
    "author": "aditi23",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-12-27T00:21:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-03T00:24:33Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9138,
    "title": "Browser getting static files from rewritten URLs instead of matched route",
    "created_at": "2019-11-26T05:17:37Z",
    "closed_at": "2019-11-27T06:54:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9138",
    "body": "*Title*\r\nQuestion: Browser getting static files from rewritten URLs instead of matched route\r\n\r\n*Description*:\r\nCurrently I want Envoy to route to a backend cluster using this endpoint: <some-domain>.com/cluster1.\r\n\r\nThe virtual host config contains the following:\r\n`\r\n              - match:\r\n                  prefix: \"/cluster1/\"\r\n                route:\r\n                  cluster: cluster1\r\n                  prefix_rewrite: \"/\"\r\n`\r\n\r\nThis routes browser's requests to /cluster1 as expected. However, all the static files (js, css...) from this backend is now resolved to root URL. Ex: <some-domain>.com/index.js instead of <some-domain>.com/cluster1/index.js, resulting in 404 error.\r\n\r\nI want to ask if there is a way to configure Envoy to resolve this issue?\r\n\r\nMany thanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9138/comments",
    "author": "RisingSun777",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2019-11-26T21:37:07Z",
        "body": "Envoy doesn't support rewriting responses.\r\n\r\nIt sounds like the responses from cluster1 contains URLs based on the path of requests sent to cluster1. Perhaps that application could use relative URLs?"
      },
      {
        "user": "RisingSun777",
        "created_at": "2019-11-27T06:54:27Z",
        "body": "Yeah changing to using relative URLs from the application side works.\r\n\r\nThanks for your support."
      }
    ]
  },
  {
    "number": 9126,
    "title": "grpc-json transcoder: HTTP 200 on large response failure",
    "created_at": "2019-11-25T11:59:07Z",
    "closed_at": "2020-01-05T17:33:41Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9126",
    "body": "*Title*: *Large response that fails (due to exceeding per_connection_buffer_limit_bytes) is reported as HTTP status 200 instead of 5xx*\r\n\r\n*Description*:\r\n\r\nSee also #6004. Situation is similar: when using gRPC-JSON transcoder and JSON-encoded response exceeds per_connection_buffer_limit_bytes, this results in empty HTTP  response with status 200. Also, there is no ERROR record in the logs that would clearly indicate the failure.\r\n\r\nWhy a 200 would be returned when the request clearly failed? This is extremely confusing to any HTTP client.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9126/comments",
    "author": "kveretennicov",
    "comments": [
      {
        "user": "qiwzhang",
        "created_at": "2019-11-25T17:08:43Z",
        "body": "This is the problem with http1. The filter has to send the header first before transcoding the response body.   For http2, the error status is send in trailing headers."
      },
      {
        "user": "kveretennicov",
        "created_at": "2019-11-29T16:51:10Z",
        "body": "Thank you, @qiwzhang. So what does it mean in practice? Is there a solution for HTTP/1.1 clients?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-29T17:29:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-05T17:33:40Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9084,
    "title": "Simple Ratelimit doesnt work with envoy frontproxy",
    "created_at": "2019-11-20T15:46:42Z",
    "closed_at": "2019-11-20T16:19:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9084",
    "body": "Simple Ratelimit doesnt work with envoy frontproxy\r\n\r\nenvoy front proxy config:\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          codec_type: auto\r\n          use_remote_address: true\r\n          stat_prefix: ingress_http\r\n          access_log:\r\n            - name: envoy.file_access_log\r\n              config:\r\n                path: /dev/stdout\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  # Send request to an endpoint in the Google cluster\r\n                  cluster: google\r\n                  host_rewrite: www.google.com\r\n                rate_limits:\r\n                  - actions:\r\n                      - remote_address: {}\r\n          http_filters:\r\n          - name: envoy.rate_limit\r\n            config:\r\n              domain: rate_per_ip\r\n              rate_limit_service:\r\n                grpc_service:\r\n                  envoy_grpc:\r\n                    cluster_name: rate_limit_cluster\r\n                  timeout: 0.25s\r\n          - name: envoy.router\r\n            typed_config: {}\r\n  clusters:\r\n  - name: google\r\n    connect_timeout: 1s\r\n    type: logical_dns \r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n    hosts: [{ socket_address: { address: google.com, port_value: 443 }}]\r\n    tls_context: { sni: www.google.com }\r\n\r\n  - name: rate_limit_cluster\r\n    type: STATIC \r\n    connect_timeout: 1s\r\n    lb_policy: ROUND_ROBIN\r\n    http2_protocol_options: {}\r\n    hosts: [{ socket_address: { address: 127.0.0.1 , port_value: 8081 }}]\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n```\r\n\r\n\r\n\r\nratelimit config:\r\n```\r\ndomain: rate_per_ip\r\ndescriptors:\r\n  - key: database\r\n    value: users\r\n    rate_limit:\r\n      unit: minute\r\n      requests_per_unit: 5\r\n  - key: remote_address\r\n    rate_limit:\r\n      unit: minute\r\n      requests_per_unit: 5\r\n```\r\n\r\nI can see both ratelimit and envoy working, and can access them, but envoy doesnt hit ratelimit service at all. Please help me.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9084/comments",
    "author": "nagireddygatla",
    "comments": [
      {
        "user": "junr03",
        "created_at": "2019-11-20T16:10:18Z",
        "body": "in the configuration above the rate_limits field is not indented correctly. The field should be inside of the route field."
      },
      {
        "user": "nagireddygatla",
        "created_at": "2019-11-20T16:19:30Z",
        "body": "Sir, you are such a savior that worked, been struggling with for a week now. very thankful for your help."
      }
    ]
  },
  {
    "number": 9020,
    "title": "TCP health check leads to the error log(Socket hang up) in upstream hosts",
    "created_at": "2019-11-14T05:37:35Z",
    "closed_at": "2019-12-21T18:27:24Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9020",
    "body": "Issue Description:\r\nAfter I config the tcp_health_check in envoy.yaml. I found there were some error logs in the upstream hosts' log file.\r\nDetail log info as below:\r\n{\"type\":\"error\",\"@timestamp\":\"2019-11-14T05:35:31Z\",\"tags\":[\"connection\",\"client\",\"error\"],\"pid\":9,\"level\":\"error\",\"error\":{\"message\":\"socket hang up\",\"name\":\"Error\",\"stack\":\"Error: socket hang up\\n    at TLSSocket.<anonymous> (_tls_wrap.js:890:25)\\n    at emitOne (events.js:121:20)\\n    at TLSSocket.emit (events.js:211:7)\\n    at _handle.close (net.js:561:12)\\n    at Socket.done (_tls_wrap.js:360:7)\\n    at Object.onceWrapper (events.js:315:30)\\n    at emitOne (events.js:116:13)\\n    at Socket.emit (events.js:211:7)\\n    at TCP._handle.close [as _onclose] (net.js:561:12)\",\"code\":\"ECONNRESET\"},\"message\":\"socket hang up\"}\r\nConfig file:\r\n- name: kibana_cluster\r\n        connect_timeout: 0.25s\r\n        type: STRICT_DNS\r\n        dns_lookup_family: V4_ONLY\r\n        lb_policy: ROUND_ROBIN\r\n        hosts:\r\n          socket_address:\r\n            address: kibana-master-svc.default.svc\r\n            port_value: 5601\r\n        health_checks:\r\n          - timeout: {{.Values.envoy.healthCheck.timeout}}\r\n            interval: {{.Values.envoy.healthCheck.interval}}\r\n            interval_jitter: {{.Values.envoy.healthCheck.intervalJitter}}\r\n            unhealthy_threshold: {{.Values.envoy.healthCheck.unhealthyThreshold}}\r\n            healthy_threshold: {{.Values.envoy.healthCheck.healthyThreshold}}\r\n            tcp_health_check: {}\r\n \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9020/comments",
    "author": "charliye",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-11-14T17:40:43Z",
        "body": "@charliye I think these logs are coming from your application? What is the issue with Envoy here specifically?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-14T17:55:02Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-21T18:27:23Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 9010,
    "title": "Can envoy disconnect connection with specific status code?",
    "created_at": "2019-11-13T08:13:30Z",
    "closed_at": "2019-11-19T08:53:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9010",
    "body": "I can set *\"max_requests_per_connection\":1*, but this way is inefficient to me.\r\nI would like to disconnect some connections with status code(like 404). Is there any way to do that?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9010/comments",
    "author": "KwangdeokPark",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-11-13T18:55:22Z",
        "body": "@KwangdeokPark I'm not sure how to interpret the question here; TCP connections aren't terminated with an HTTP status code, HTTP requests are completed with a status code. Can you elaborate?"
      },
      {
        "user": "KwangdeokPark",
        "created_at": "2019-11-14T08:40:59Z",
        "body": "In HTTP connection, envoy makes connection pool and maintain that connection some time(timeout). I want to disconnect some of that connection in connection pool with status code, and make new connection. Is it possible in envoy proxy?"
      },
      {
        "user": "htuch",
        "created_at": "2019-11-14T23:07:12Z",
        "body": "Does the outlier detector match your needs?"
      },
      {
        "user": "KwangdeokPark",
        "created_at": "2019-11-19T08:53:23Z",
        "body": "@htuch it's little bit different, but Thank you!"
      }
    ]
  },
  {
    "number": 8952,
    "title": "integration tests: call to `tcp_client->waitForDisconnect()` gets stuck indefinitely on some Linux builds",
    "created_at": "2019-11-08T12:31:20Z",
    "closed_at": "2020-01-15T00:13:41Z",
    "labels": [
      "question",
      "area/test flakes",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8952",
    "body": "*Title*: call to `tcp_client->waitForDisconnect()` in integration tests gets stuck indefinitely on some Linux builds\r\n\r\n*Description*:\r\n\r\nI'm trying to create an integration test for a negative use case where a client connects to `envoy.tcp_proxy` but connection must be closed because there are no healthy upstreams.\r\n\r\nOriginally, the test case looked like this:\r\n```\r\n  IntegrationTcpClientPtr tcp_client = makeTcpConnection(lookupPort(\"tcp_proxy\"));\r\n  tcp_client->write(\"hello\");\r\n  tcp_client->waitForDisconnect();\r\n\r\n  test_server_->waitForCounterGe(\"cluster.cluster_0.upstream_cx_none_healthy\", 1);\r\n  test_server_->waitForCounterEq(\"cluster.cluster_0.lb_subsets_selected\", 0);\r\n```\r\n\r\nbut it was consistently failing on a subset of Linux builds, i.e. `envoy-linux (bazel compile_time_options)` and `envoy-linux (bazel release)`.\r\n\r\nThe working version looks like this:\r\n```\r\n  IntegrationTcpClientPtr tcp_client = makeTcpConnection(lookupPort(\"tcp_proxy\"));\r\n  tcp_client->write(\"hello\");\r\n\r\n  // commented out on purpose\r\n  // tcp_client->waitForDisconnect();\r\n\r\n  test_server_->waitForCounterGe(\"cluster.cluster_0.upstream_cx_none_healthy\", 1);\r\n  test_server_->waitForCounterEq(\"cluster.cluster_0.lb_subsets_selected\", 0);\r\n\r\n  tcp_client->close();\r\n```\r\n\r\nI think, there might be a problem with integration test client who doesn't detect connection close in that case. \r\n\r\nUnfortunately, I'm not qualified to troubleshoot it further.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8952/comments",
    "author": "yskopets",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-12-08T20:13:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-07T23:39:44Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-15T00:13:40Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 8937,
    "title": "Upstream connect error or disconnect/reset before headers",
    "created_at": "2019-11-07T20:42:19Z",
    "closed_at": "2019-11-10T15:26:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8937",
    "body": "Hi, I'm simply trying to configure envoy as reverse proxy, but when I'm trying to proxy to my docker container, I see error `upstream connect error or disconnect/reset before headers. reset reason: connection failure`. Could you, please, point me, where I misconfigured envoy.\r\n\r\n*docker-compose.yml:*\r\n```\r\nversion: \"3.7\"\r\nservices:\r\n  envoy:\r\n    depends_on: \r\n      - nginx\r\n    networks:\r\n      - proxynet\r\n    volumes:\r\n      - ./envoy.yml:/etc/envoy/envoy.yaml\r\n    ports:\r\n      - \"8081:8081\"\r\n      - \"80:80\"\r\n    container_name: envoy\r\n    image: envoyproxy/envoy-alpine\r\n  nginx:\r\n    networks:\r\n      proxynet:\r\n        aliases:\r\n          - nginx\r\n    ports:\r\n      - \"7000:80\"\r\n    container_name: nginx\r\n    image: nginx\r\nnetworks:\r\n  proxynet:\r\n```\r\n*envoy.yml:*\r\n```\r\nstatic_resources:\r\n  listeners:\r\n    - address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 80\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              config:\r\n                codec_type: auto\r\n                stat_prefix: ingress_http\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: backend\r\n                      domains:\r\n                        - \"*\"\r\n                      routes:\r\n                        - match:\r\n                            prefix: \"/nginx\"\r\n                          route:\r\n                            cluster: nginx\r\n                http_filters:\r\n                  - name: envoy.router\r\n  clusters:\r\n    - name: nginx\r\n      connect_timeout: 0.25s\r\n      type: strict_dns\r\n      lb_policy: round_robin\r\n      hosts:\r\n        - socket_address:\r\n            address: nginx\r\n            port_value: 7000\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8081\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8937/comments",
    "author": "PabloDeCortes",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2019-11-07T20:50:32Z",
        "body": "The error implies that the upstream host (nginx:7000) either refused the connection or accepted it and closed before request headers were forwarded. Seems like a docker config/networking problem rather than a misconfiguration of Envoy. If you look at debug level logs from Envoy (run with `-l debug`) you should be able to see the relevant IP addresses and debug."
      }
    ]
  },
  {
    "number": 8912,
    "title": "Envoy Connections are refreshed when using control plane",
    "created_at": "2019-11-06T05:21:17Z",
    "closed_at": "2019-12-13T20:38:41Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8912",
    "body": "*Description*:\r\nWe are using golang envoy-control-plane to update the envoy routing configs. Envoy acts as a proxy for grpc requests (persistent connections). What we noticed is, whenever the routing configuration is reset after getting the snapshot from the control plane, all the connections are dropped and recreated. \r\n\r\nWe are updating the config snapshot every 10 seconds using SetSnapshot and a new version. This refreshes the configs and the connections and hence we are losing the advantage of persistent connections.\r\n\r\nIs there any way to avoid this connection refresh to upstreams during configuration updates? Or are we doing anything wrong here?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8912/comments",
    "author": "roobalimsab",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-11-06T19:06:39Z",
        "body": "If you use EDS for upstream host changes, there should be no effect to the connections of existing hosts."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-06T19:41:10Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-13T20:38:40Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 8868,
    "title": "change buffer content and headers in reply - encodeData",
    "created_at": "2019-11-03T15:28:08Z",
    "closed_at": "2019-12-11T05:31:39Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8868",
    "body": "I would like to change the content of the buffer in encodeData according to tests that i'm running in my filter. I also need to change the headers because the new buffer content is from different type (JSON). Is there any way to change them both in this phase? I tried to save ptr to HeadersMap in encodeHeaders but it seems like the headers are deleted although encodeHeaders returns stopIteration. I thought this status stops the headers and not just the body. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8868/comments",
    "author": "aby-rd",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-11-04T00:05:28Z",
        "body": "This should work by stopping iteration in headers and then modifying headers and continuing during data processing. There are several filters that do this so I would have a look at the existing examples or provide more info about what you are doing."
      },
      {
        "user": "aby-rd",
        "created_at": "2019-11-04T04:57:09Z",
        "body": "Thanks for your answer!\r\nIt can works but I don't know in headers phase if I need to change the buffer, i only know it in data processing. stop iteration doesn't stop headers until data process complete? or at least data continue?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-04T05:21:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-11T05:31:38Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 8867,
    "title": "option 'close_connections_on_host_set_change' can't destroy long-live stream",
    "created_at": "2019-11-03T09:18:44Z",
    "closed_at": "2019-12-12T20:32:38Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8867",
    "body": "I use envoy to proxy long-live server-side grpc stream, the upstream cluster option 'close_connections_on_host_set_change' has been set to 'true'. When hosts changed, the grpc stream can't destroy. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8867/comments",
    "author": "chijinxina",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-11-04T00:04:29Z",
        "body": "IIRC this setting invokes draining on the connection which tries to do it gracefully vs. forceful closure. for a long lived stream there would never be any graceful closure so I don't think this is going to work, but cc @nezdolik who can provide more info."
      },
      {
        "user": "nezdolik",
        "created_at": "2019-11-05T18:46:23Z",
        "body": "Hello @chijinxina,  'close_connections_on_host_set_change' most likely would not work for long-lived grpc streams. Could you please provide sample Envoy config for me to confirm."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-05T20:30:02Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-12T20:32:37Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "leosunmo",
        "created_at": "2021-06-17T22:59:51Z",
        "body": "I know this is a long-dead issue, but I've just recently run in to an issue where I would like to terminate long-lived gRPC streams when my Cluster changes. Are there any plans to add a `forcefully_close_connections_on_host_set_change`?\r\n\r\nHaving a function that is identical to this, but if it fails to gracefully terminate the connection withing a certain time-frame (ideally configurable as well), it forcefully closes the connection so it can be re-established."
      },
      {
        "user": "mattklein123",
        "created_at": "2021-06-28T03:27:35Z",
        "body": "> I know this is a long-dead issue, but I've just recently run in to an issue where I would like to terminate long-lived gRPC streams when my Cluster changes. Are there any plans to add a forcefully_close_connections_on_host_set_change?\r\n\r\nThis has come up several times, and I'm guessing we have tracking issues about this but I haven't searched yet (assuming it hasn't already been implemented). I think it's a reasonable feature to add and I would open an issue if it's not already tracked. cc @snowp "
      },
      {
        "user": "leosunmo",
        "created_at": "2021-06-28T20:55:18Z",
        "body": "@mattklein123 I'm glad to hear I'm not the only one running in to this problem!\r\n\r\nI am currently in the process of trying to interrogate the Envoy xDS for Cluster updates so I can manually terminate my gRPC streams in my application, which does seem like a hack to me, and it's proving quite difficult to get started _getting_ cluster state/config from Envoy, not _setting_ it. All guides out there are for Control Plane implementations.\r\n\r\nI feel like a way for Envoy to terminate long-running gRPC streams would be a very good addition."
      },
      {
        "user": "krapie",
        "created_at": "2023-03-29T18:15:49Z",
        "body": "Any updates on this issue? I'm having split brain issue on maglev LB rehashing, and `close_connections_on_host_set_change` won't close gRPC stream.\r\n\r\nIs there any other alternatives to close long-lived grpc streams?\r\n\r\nIf not, I would do my best to fix this problem, by introducing new mechanism or enhance `close_connections_on_host_set_change` with `force` to close long-lived grpc streams with somehow..."
      }
    ]
  },
  {
    "number": 8853,
    "title": "Unable to filter HTTP requests before HCM",
    "created_at": "2019-11-01T07:24:53Z",
    "closed_at": "2019-12-08T21:13:22Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8853",
    "body": "### Description\r\n\r\nWe have multiple domains on port 80 and want to customize every domain's HCM setting. For example, separating the access_logs for every domain (`${domain}.${status_code}.log`), enabling different http_filters and enabling websocket support.\r\n\r\nWhen all domains go through TLS protocol, using `server_names` in `FilterChainMatch` can achieve this goal. But `server_names` is based on TLS SNI, and it does not support HTTP requests.\r\n\r\nCurrently, I can separate HTTP requests to different clusters in routing (virtual_hosts layer), but all domains share the same HCM setting. Is there any solution for separating HCM setting?\r\n\r\nThanks in advising.\r\n\r\n### Relative Configs\r\n\r\n<details>\r\n\r\n<summary> Multiple HTTPS Domains </summary>\r\n\r\n```\r\n---\r\nstatic_resources:\r\n  listeners:\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.Listener\r\n    name: listener_443\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 443\r\n    listener_filters:\r\n    - name: envoy.listener.tls_inspector\r\n      typed_config: {}\r\n    filter_chains:\r\n    - filter_chain_match:\r\n        server_names:\r\n        - test1:443\r\n        - test1\r\n      tls_context:\r\n        common_tls_context:\r\n          tls_certificates:\r\n          - certificate_chain:\r\n              filename: \"/self-signed/test1.crt\"\r\n            private_key:\r\n              filename: \"/self-signed/test1.key\"\r\n      filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          generate_request_id: true\r\n          tracing:\r\n            operation_name: egress\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service1\r\n              require_ssl: all\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: service_www_google_com_443\r\n                  host_rewrite: www.google.com\r\n                metadata:\r\n                  filter_metadata:\r\n                    envoy.filters.http.modsecurity: {}\r\n          http_filters:\r\n          - name: envoy.router\r\n    - filter_chain_match:\r\n        server_names:\r\n        - test2:443\r\n        - test2\r\n      tls_context:\r\n        common_tls_context:\r\n          tls_certificates:\r\n          - certificate_chain:\r\n              filename: \"/self-signed/test2.crt\"\r\n            private_key:\r\n              filename: \"/self-signed/test2.key\"\r\n      filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          generate_request_id: true\r\n          tracing:\r\n            operation_name: egress\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service1\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: service_www_yahoo_com_443\r\n                  host_rewrite: www.yahoo.com\r\n          http_filters:\r\n          - name: envoy.router\r\n  clusters:\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.Cluster\r\n    name: service_www_google_com_443\r\n    connect_timeout: 0.25s\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: service_www_google_com_443\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.google.com\r\n                port_value: 443\r\n    tls_context:\r\n      sni: www.google.com\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.Cluster\r\n    name: service_www_yahoo_com_443\r\n    connect_timeout: 0.25s\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: service_www_yahoo_com_443\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.yahoo.com\r\n                port_value: 443\r\n    tls_context:\r\n      sni: www.yahoo.com\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n\r\n<summary> Multiple HTTP Domains </summary>\r\n\r\n```\r\n---\r\nstatic_resources:\r\n  listeners:\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.Listener\r\n    name: listener_80\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          generate_request_id: true\r\n          tracing:\r\n            operation_name: egress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: ingress_test5\r\n              domains:\r\n              - test5:80\r\n              - test5\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: service_www_google_com_443\r\n                  host_rewrite: www.google.com\r\n            - name: ingress_test6\r\n              domains:\r\n              - test6:80\r\n              - test6\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: service_www_yahoo_com_443\r\n                  host_rewrite: www.yahoo.com\r\n          http_filters:\r\n          - name: envoy.router\r\n  clusters:\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.Cluster\r\n    name: service_www_google_com_443\r\n    connect_timeout: 0.25s\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: service_www_google_com_443\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.google.com\r\n                port_value: 443\r\n    tls_context:\r\n      sni: www.google.com\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.Cluster\r\n    name: service_www_yahoo_com_443\r\n    connect_timeout: 0.25s\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: service_www_yahoo_com_443\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.yahoo.com\r\n                port_value: 443\r\n    tls_context:\r\n      sni: www.yahoo.com\r\n```\r\n\r\n</details>\r\n\r\n### Relative Issues\r\n\r\n- Issue #3615: `server_names` is from TLS SNI\r\n- Issue #5920: Multiple http filters don't work",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8853/comments",
    "author": "Quexint",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-11-01T15:46:13Z",
        "body": "At a high level, no. It would be helpful to understand what things you want to specifically customize on a per-route/vhost setting though."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-01T17:18:43Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-08T21:13:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 8833,
    "title": "Support prefix rewrites with after string matches",
    "created_at": "2019-10-31T20:50:00Z",
    "closed_at": "2019-12-08T19:13:22Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8833",
    "body": "*Title*: *Prefix rewrite with after string matches*\r\n\r\n*Description*:\r\n In the case of prefix rewrite, we want to match a prefix string and replace it with another but also insert a new string after the designated string in the original matched full string. We would need a field in the config to provide this string (let's call it after_str). For example, if we want  `www.example.com/foo/bar/zoo/1234/abc` to go to `www.example.com/baz/1234/abc`:\r\n\r\nwe would have config such as: \r\n```\r\n        {\r\n      \"name\": \"www3\",\r\n      \"domains\": [\"example.com\", \"www.example.com\"],\r\n      \"routes\": [\r\n        {\r\n          \"prefix\": \"/foo\",\r\n          \"prefix_rewrite\": \"/baz\",\r\n          \"after_str\": \"/zoo\",      <<<<<<<<<<< \r\n          \"cluster\": \"www3\"\r\n        },\r\n```\r\nSo when the request such as `www.example.com/foo/bar/zoo/1234/abc` comes in, we rewrite `/foo` with `/baz` and then add string after the after_str: zoo, so we form `/baz/1234/abc`. \r\n\r\nI was wondering if there is any other way of doing this? If not, would the upstream community be interested in this feature?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8833/comments",
    "author": "drao9",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-11-01T02:55:05Z",
        "body": "This feels very specific to me and probably not a general enough use case to add a full feature for. I wonder if this could be done by regex rewrite or a Lua filter or something like that? @zuercher WDYT?"
      },
      {
        "user": "zuercher",
        "created_at": "2019-11-01T18:05:05Z",
        "body": "I agree this is something we should support with regex rewrites. There's an open PR for that now (#8462), which will hopefully land eventually.\r\n\r\nOtherwise, yeah I think it's probably more appropriate to use the Lua filter."
      },
      {
        "user": "drao9",
        "created_at": "2019-11-01T18:15:02Z",
        "body": "#8462 should cover use cases like these. Thanks for the response! @mattklein123 @zuercher "
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-01T18:18:43Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-08T19:13:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 8774,
    "title": "Propagate Cluster.alt_stat_name to RouteEntry",
    "created_at": "2019-10-25T22:44:30Z",
    "closed_at": "2019-10-26T04:23:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8774",
    "body": "We use custom `PassThroughEncoderFilter` which emits some cluster-specific stats.\r\nCurrently we use `streamInfo.routeEntry()->clusterName()` for this purpose, but since Cluster.alt_stat_name is not propagated to RouteEntry we can't achieve consistent logging across different code paths.\r\nWould it make sense to provide cluster's alt_stat_name via RouteEntry?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8774/comments",
    "author": "veshij",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-10-25T23:08:22Z",
        "body": "You can look up the cluster and then get the alt stat name from cluster info right?"
      },
      {
        "user": "veshij",
        "created_at": "2019-10-26T04:23:32Z",
        "body": "Yep, that should work, sorry for bothering."
      }
    ]
  },
  {
    "number": 8773,
    "title": "Possible Lua filter memory leak when using body in envoy_on_response()",
    "created_at": "2019-10-25T22:07:50Z",
    "closed_at": "2019-10-28T13:44:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8773",
    "body": "*Title*: *Possible Lua filter memory leak when using body in envoy_on_response()*\r\n\r\n*Description*:\r\nWhen accessing the response body inside the envoy_on_response() Lua function, either with body() or bodyChunks(), the memory does not seem to be freed after Envoy finishes responding to the downstream.\r\n\r\n*Envoy Version*: v1.11.1\r\n\r\n*Repro steps*:\r\n1. Use the standard implementation of the envoy.lua filter with the following code:\r\n```lua\r\nfunction envoy_on_request(handle)\r\n  local metadata = handle:streamInfo():dynamicMetadata()\r\n  local headers = handle:headers()\r\n  local rid = headers:get(\"x-request-id\")\r\n  if rid ~= nil then metadata:set(\"envoy.lua\", \"x-original-request-id\") end\r\nend\r\n\r\nfunction envoy_on_response(handle)\r\n  local cluster = \"logging_server\"\r\n  local headers = {\r\n    [\":method\"] = \"POST\",\r\n    [\":path\"] = \"/logging/response\",\r\n    [\":authority\"] = cluster,\r\n  }\r\n  local metadata = handle:streamInfo():dynamicMetadata():get(\"envoy.lua\")\r\n  for key, value in pairs(metadata) do\r\n    if key == \"x-original-request-id\" then headers[\":path\"] = \"/logging/response/for/\" .. value end\r\n    headers[\"x-int-meta-\" .. key] = value\r\n  end\r\n  if (handle:metadata():get(\"buffer_response\") == \"buffered\") then\r\n    local res = handle:body()\r\n    local body = res:getBytes(0, res:length())\r\n    handle:httpCall(cluster, headers, body, 0)  -- Point A\r\n  else\r\n    local t = {}\r\n    for chunk in handle:bodyChunks() do\r\n      local body = chunk:getBytes(0, chunk:length())\r\n      table.insert(t, body)\r\n    end\r\n    handle:httpCall(cluster, headers, table.concat(t, \"\"), 0)  -- Point B\r\n  end\r\nend\r\n```\r\n2. Configure the listener `per_connection_buffer_limit_bytes` to about 10 MiB.\r\n3. Run the alpine docker image of Envoy acting as a reverse proxy for a server that responds to GET requests with large payloads (120kiB to ~2MiB), and run `docker stats` to observe the memory usage. On my machine, Envoy usually starts with ~8MiB.\r\n4. Run a load testing framework (I used gatling) that exercises the endpoints that return the larger payloads. I ran a test from one machine that throttled to 100 RPS.\r\n5. Observe Envoy's memory usage shoot up to ~1 GiB, which does not go down even an hour after the test is run.\r\n\r\n*Notes*:\r\n- I've ensured that the payload server has enough capacity to handle the requests; Envoy's access log shows 200 status codes and no 503/504 errors are logged.\r\n- The memory usage does not change when *Point A* or *Point B* are commented out.\r\n- Separately, *Point B* will abruptly terminate its connection to the `logging_server` cluster if it is unable to upload the request in time. I will get exceptions from the logging server to the tune of \"the client has disconnected while sending a request\".\r\n\r\n*Config*:\r\n```yaml\r\n# lds.yaml\r\nversion_info: \"0\"\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.api.v2.Listener\r\n  name: vip_listener\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8080\r\n  per_connection_buffer_limit_bytes: 10000000\r\n  filter_chains:\r\n  - filters:\r\n    - name: envoy.http_connection_manager\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n        stat_prefix: vip_http\r\n        codec_type: auto\r\n        rds:\r\n          route_config_name: vip_routes\r\n          config_source:\r\n            path: /envoy/conf/rds.yaml\r\n        http_filters:\r\n          - name: envoy.health_check\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.config.filter.http.health_check.v2.HealthCheck\r\n              pass_through_mode: false\r\n              headers:\r\n                - name: \":path\"\r\n                  exact_match: \"/healthcheck\"\r\n          - name: envoy.lua\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.config.filter.http.lua.v2.Lua\r\n              inline_code: |\r\n                -- << INSERT LUA CODE HERE >>\r\n          - name: envoy.router\r\n            config: {}\r\n        access_log:\r\n          - name: envoy.file_access_log\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\r\n              format: '[%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\r\n                \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\" \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\r\n                '\r\n              path: /dev/stdout\r\n```\r\n\r\n```yaml\r\n# rds.yaml\r\nversion_info: \"0\"\r\nresources:\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n    name: vip_routes\r\n    virtual_hosts:\r\n      - name: incoming\r\n        domains:\r\n          - \"*\"\r\n        routes:\r\n          - match:\r\n              prefix: /log/buffer/\r\n            route:\r\n              prefix_rewrite: /\r\n              cluster: test\r\n              request_mirror_policy:\r\n                cluster: logging_server\r\n            metadata:\r\n              filter_metadata:\r\n                envoy.lua:\r\n                  buffer_response: buffered\r\n          - match:\r\n              prefix: /log/\r\n            route:\r\n              prefix_rewrite: /\r\n              cluster: test\r\n              request_mirror_policy:\r\n                cluster: logging_server\r\n          - match:\r\n              prefix: /\r\n            route:\r\n              cluster: test\r\n```\r\n\r\n```yaml\r\n# cds.yaml\r\nversion_info: \"0\"\r\nresources:\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.Cluster\r\n    name: test\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: test\r\n      endpoints:\r\n        - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: app\r\n                    port_value: 8080\r\n    drain_connections_on_host_removal: true\r\n    circuit_breakers:\r\n      thresholds:\r\n        - max_connections: 10000\r\n          max_pending_requests: 10000\r\n          max_requests: 10000\r\n          max_retries: 0\r\n          priority: DEFAULT\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.Cluster\r\n    name: logging_server\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: logging_server\r\n      endpoints:\r\n        - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: logger\r\n                    port_value: 5000\r\n    drain_connections_on_host_removal: true\r\n    circuit_breakers:\r\n      thresholds:\r\n        - max_connections: 10000\r\n          max_pending_requests: 10000\r\n          max_requests: 10000\r\n          max_retries: 0\r\n          priority: DEFAULT\r\n```\r\n\r\n```yaml\r\n# envoy.yaml\r\nadmin:\r\n  access_log_path: /dev/null\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\ndynamic_resources:\r\n  cds_config:\r\n    path: '/envoy/conf/cds.yaml'\r\n  lds_config:\r\n    path: '/envoy/conf/lds.yaml'\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8773/comments",
    "author": "v1b1",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-10-26T20:47:39Z",
        "body": "What memory stat are you looking at exactly to determine that it is leaking? Unless the memory actually goes up forever, this is most likely a case of tcmalloc not giving memory back to the OS and buffer sizes being set too large."
      },
      {
        "user": "v1b1",
        "created_at": "2019-10-28T13:44:05Z",
        "body": "Thanks @mattklein123, I wrote a different test that utilised 1TPS for a large buffer size and can confirm that the memory holds at approximately that size, plus overhead. "
      }
    ]
  },
  {
    "number": 8730,
    "title": "Have a fallback cluster which matches when the matched upstream cluster for a route is Unhealthy",
    "created_at": "2019-10-23T00:00:56Z",
    "closed_at": "2019-12-07T21:00:17Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8730",
    "body": "\r\n*Title*: *Have a fallback cluster which matches when your matched upstream cluster is Unhealthy*\r\n\r\n*Description*:\r\nWe are running an implementation of this feature in our production env. We wanted to know if this feature is desired upstream.\r\nThe feature is present at `route_config` level, and takes name of a cluster as its value.\r\nIf a route matches an upstream cluster which has `no heathy upstreams`, then instead of sending a 503, we reroute the request to this default cluster (if it is set).\r\n\r\nEnvoy config looks like this:\r\n\r\n```\r\n            \"filter_chains\": [{\r\n                \"filters\": [{\r\n                    \"name\": \"envoy.http_connection_manager\",\r\n                    \"config\": {\r\n                        \"stat_prefix\": \"ingress_http\",\r\n                        \"route_config\": {\r\n                            \"name\": \"envoy_route_config\",\r\n                            \"default_cluster\": \"echo1\", <<<<<<<<<<<<<<<<<<<\r\n                            \"virtual_hosts\": [\r\n                                {\r\n                                    \"name\": \"backend\",\r\n                                    \"domains\": [\r\n                                        \"www.ebay.com\"\r\n                                    ],\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8730/comments",
    "author": "shivanshu21",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-10-23T01:05:51Z",
        "body": "@snowp I guess this can be a retry extension?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-10-23T17:44:03Z",
        "body": "I think this could also be done with an HTTP filter that runs before the router FWIW."
      },
      {
        "user": "shivanshu21",
        "created_at": "2019-10-23T17:59:12Z",
        "body": "@mattklein123 @lizan \r\n\r\nSorry, I didnt quite understand.\r\n```\r\nLets say there is a decoder filter which runs before router.\r\nIt must first \"route\" this request and match it to a cluster.\r\nThen it should check if the matched cluster is up. If not, change its cluster to the default cluster.\r\n```\r\nIn doing so, the new filter will just be another router filter won't it?\r\n\r\nThe way we do it in our private repo is:\r\n\r\n```\r\n// If the cluster is unhealthy then send request to the default cluster.\r\n  if (!conn_pool) {\r\n    Upstream::ThreadLocalCluster* def_cluster = config_.cm_.get(route_entry_->defaultClusterName());\r\n    if (!def_cluster) {\r\n      ENVOY_STREAM_LOG(debug, \"unknown default cluster '{}'\", *callbacks_, route_entry_->defaultClusterName());\r\n    } else {\r\n      cluster_->stats().upstream_rq_default_cluster_.inc();\r\n      ENVOY_STREAM_LOG(debug, \"cluster '{}' unhealthy. Sending to default cluster '{}'\", *callbacks_, route_entry_->clusterName(), route_entry_->defaultClusterName());\r\n      cluster_ = def_cluster->info();\r\n      if (!cluster_->maintenanceMode()) {\r\n        conn_pool = getDefaultConnPool();\r\n      }\r\n    }\r\n  }\r\n\r\n  if (!conn_pool) {  // <<<<<<<<<<<<<<<<<<<<<<< This is envoy's default behavior\r\n    sendNoHealthyUpstreamResponse();\r\n    return Http::FilterHeadersStatus::StopIteration;\r\n  }\r\n```"
      },
      {
        "user": "snowp",
        "created_at": "2019-10-23T18:04:02Z",
        "body": "You can modify the headers and call `callbacks_->clearRouteCache();` to re-resolve the route entry. This would allow you to simplify modify the headers in such a way that a different cluster is resolved, delegating the routing to the router. "
      },
      {
        "user": "mattklein123",
        "created_at": "2019-10-23T18:09:16Z",
        "body": "^ correct"
      },
      {
        "user": "rshriram",
        "created_at": "2019-10-31T19:08:43Z",
        "body": "Why can’t you do this with aggregate cluster ? "
      },
      {
        "user": "shivanshu21",
        "created_at": "2019-10-31T19:22:03Z",
        "body": "Thanks guys, we are exploring different options you have mentioned in responses. For now we have a local fix for this. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-11-30T20:09:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-07T21:00:16Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 8729,
    "title": "How to use SDS with file subscription",
    "created_at": "2019-10-22T23:10:35Z",
    "closed_at": "2019-11-29T02:19:03Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8729",
    "body": "*Description*:\r\nI am using the below configuration to refresh tls certificates from file subscription. But it seems to be not working. Although in the log it seems envoy is watching the file containing inline certs. Any idea what I might be missing here.\r\nI am using kubernetes secrets to store the certs. For refresh I patch the same secret.\r\n\r\nMy Envoy configuration\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\nstatic_resources:\r\n  clusters:\r\n  - connect_timeout: 0.25s\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: main_website\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 127.0.0.1\r\n                port_value: 9091\r\n                protocol: TCP\r\n    name: local_service\r\n    type: STATIC\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8443\r\n        protocol: tcp\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          '@type': type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n          codec_type: AUTO\r\n          forward_client_cert_details: SANITIZE_SET\r\n          http_filters:\r\n          - name: envoy.router\r\n            typed_config: {}\r\n          idle_timeout: 840s\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - domains:\r\n              - '*'\r\n              name: local_service\r\n              routes:\r\n              - match:\r\n                  prefix: /\r\n                route:\r\n                  cluster: local_service\r\n                  timeout: 120s\r\n          stat_prefix: ingress_http\r\n      tls_context:\r\n        common_tls_context:\r\n          tls_certificate_sds_secret_configs:\r\n          - name: server_cert\r\n            sds_config:\r\n              path: /init-data/server_certs.yaml\r\n          validationContext:\r\n            trustedCa:\r\n              filename: /init-data/service.x509.trusted_ca.pem\r\n  secrets:\r\n  - name: validation_context_client\r\n    validation_context:\r\n      trusted_ca:\r\n        filename: /init-data/service.x509.trusted_ca.pem\r\n```\r\n\r\n/init-data/service.x509.trusted_ca.pem content\r\n```\r\n- '@type': type.googleapis.com/envoy.api.v2.auth.Secret\r\n  name: server_cert\r\n  tls_certificate:\r\n    certificate_chain:\r\n      inline_string: |-\r\n         -----BEGIN CERTIFICATE-----\r\n        MIIEETCCAvmgAwIBAgIJALi17MiY8hJKMA0GCSqGSIb3DQEBCwUAMHYxCzAJBgNV\r\n        BAYTAlVTMRMwEQYDVQQIDApDYWxpZm9ybmlhMRYwFAYDVQQHDA1TYW4gRnJhbmNp\r\n        c2NvMQ0wCwYDVQQKDARMeWZ0MRkwFwYDVQQLDBBMeWZ0IEVuZ2luZWVyaW5nMRAw\r\n        DgYDVQQDDAdUZXN0IENBMB4XDTE4MTIxODAxNTAzNFoXDTIwMTIxNzAxNTAzNFow\r\n        ejELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWExFjAUBgNVBAcMDVNh\r\n        biBGcmFuY2lzY28xDTALBgNVBAoMBEx5ZnQxGTAXBgNVBAsMEEx5ZnQgRW5naW5l\r\n        ZXJpbmcxFDASBgNVBAMMC1Rlc3QgU2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOC\r\n        AQ8AMIIBCgKCAQEA9iZuZ4lJbCw6sDIypihrlIm/wYfa6BY4K/eJWUbCCcKT8jhP\r\n        1XQSuoTfqbMVSuoyZxiLFujRz/Wmn1JAbM5myYhJLaVmAyAXUyLLDzzOrexl0n2m\r\n        I133wfrPpcolemRuE0q2zB+ChDn4/o3VwOmXnOc5SL9hAb6TerVkAmAeUAmw15Xc\r\n        WvNhQS0Wq0XA4cy2B7q/z8Ll+76pEuA0aN+MoTbrzHI8keaJyQqlMJO45u+BsJuE\r\n        Gc+9HbV/rpr1CA3y06E38VGXUtzQ20wGEjIdPj0zc5uVtu/DTQRMKO3e8oURcrbQ\r\n        oEGrCMTN7cxqbdR9DZxFGZ7JFwBGbyTwDAzJRwIDAQABo4GdMIGaMAwGA1UdEwEB\r\n        /wQCMAAwCwYDVR0PBAQDAgXgMB0GA1UdJQQWMBQGCCsGAQUFBwMCBggrBgEFBQcD\r\n        ATAeBgNVHREEFzAVghNzZXJ2ZXIxLmV4YW1wbGUuY29tMB0GA1UdDgQWBBRsuFLO\r\n        uLgl1XOmIFJBX2U0P7wNNTAfBgNVHSMEGDAWgBSLgpmApTtprdcKFiTUZxUkLNRh\r\n        ZTANBgkqhkiG9w0BAQsFAAOCAQEAtbE2xVAwOIXY1zGEwpk/ZwJPByqq+dY5pYKN\r\n        vpK1wRT91iDit0ktJwM0ddDBWj1P9IJgz2X4QLHMMLDbmCY1+o2E7aXpdcW2Eima\r\n        EYvzcUD0RZKjb2tqK4CmRvevnr15G3Fw6AnuWP6SDUEVizpTzH6cL83I+nCeQlCB\r\n        CH0Q1/pcdjlsedM7DWohzEGVpjEAs/DGtC2XYFSDPu0pnUHKtBb2B9Njdv9rAJ/y\r\n        Pm5mVbwSEu1GzAzFp3laTn4xwJUejtjZjk41jd6CnmlP+/0hBw3nz86YGYOrb3H/\r\n        yGGZAV+Zg36WH0e1/XkUs+2wyJh8rmoytlvOe+RRxfOyU+KjGA==\r\n        -----END CERTIFICATE-----\r\n    private_key:\r\n      inline_string: |\r\n        -----BEGIN PRIVATE KEY-----\r\n        MIIEpAIBAAKCAQEA9iZuZ4lJbCw6sDIypihrlIm/wYfa6BY4K/eJWUbCCcKT8jhP\r\n        1XQSuoTfqbMVSuoyZxiLFujRz/Wmn1JAbM5myYhJLaVmAyAXUyLLDzzOrexl0n2m\r\n        I133wfrPpcolemRuE0q2zB+ChDn4/o3VwOmXnOc5SL9hAb6TerVkAmAeUAmw15Xc\r\n        WvNhQS0Wq0XA4cy2B7q/z8Ll+76pEuA0aN+MoTbrzHI8keaJyQqlMJO45u+BsJuE\r\n        Gc+9HbV/rpr1CA3y06E38VGXUtzQ20wGEjIdPj0zc5uVtu/DTQRMKO3e8oURcrbQ\r\n        oEGrCMTN7cxqbdR9DZxFGZ7JFwBGbyTwDAzJRwIDAQABAoIBADsK7OCY5au00D8D\r\n        S3Lfry5p/D0HwVJ6Qe8J6UAo4NJGggZPy5Z/yR5EUQH7mtflZUGleXd9URZ47ga5\r\n        TArNV4pvzl3MlrOZNv8SEXMOOuUtFx9uVDOXjp5Q8w3pdyPsgxrNiRtdU4kz/q3Q\r\n        h71GSr+wFuWEQliQxf9hEyQeuiaoM/2nlaAs1MX8PdawMg4n3aqsgVW6eEkz5Hb0\r\n        JLn9awOPltFUumMxLjUsoltmO47E1BBYq7iZbKd5fb3inXZdciaBo0bInpTMxeq1\r\n        2sTJMf1xsPFH+hCTaN0nQRsIY+FAwDl2TWe5OY4pIdCB1J1o71MKlwK8/8f4VUgM\r\n        /EEP98ECgYEA+8KMSOztIdqXt4BZZ9Lli1xRIRREgZP2ad8V7j/Ia5S4iaI6MdOo\r\n        8IwGQHI9HdPZbHO2xraQ+Nh8b8A2A82JmVmUaXuiCHVXQAbIqDEveG9cglmpX88P\r\n        4OScveXfZHGlEycHDVWYP/yESVBbrbqEtTTN+JrUXOCXObiWHNV5V+0CgYEA+kuy\r\n        XX9xIHp/8Y33zOdlmzCnTrREMH6DXebqQ1MixdZBPlKtn8jdjzvMYXxoNeVaCv3k\r\n        JwYkO/ulHA6yqjXHkMgGp3/IvrhWXDSZKh1lSRz8VmDNrX5jku2MdAnW3eFWQ3Aw\r\n        XhioGDMpdXnQiioJWjd8+IIc2U5hRxJ/pHl4l4MCgYEAn4KlBXNvzzpDVBVzEBAU\r\n        Ndb45B+GRB8uSef5ailpX5grz7BEdNtpN3dQYra+uE5IfKlYDdEQS0pGlTtQPdqV\r\n        Kt8R5D9b60YFUOC7c60uKX1NPJ14beawZcoS8VJa1rYWY+sfUbiArVHdsuHCJHF/\r\n        sZvSQIhAstPtnLmyCnTK3AUCgYEAssFfCIFc9NooCAbhQ/i6yyPrST1bOdsQzP5+\r\n        rkhwnCRRgkYf8+7LtWD6rWDhOCStsw3yZKBxPlmRaMFifkdcWJ1Si0uqvfL+0mlK\r\n        PhayYI4P9Uy7d729NZDr72+bgelN52n3USyA2I+pe1ndQt+UwFbCrOSX3+aneM/Q\r\n        U1HheysCgYBQkj1DHb4vzN4eRsKKas9m+L9icEm0P1UHujjL4lgRjbfxE0lNB/eb\r\n        LkNTXDe5YGKe7JGvJ6IIu0Y4RbzFJ4TQxezctvBA+AbF8Yq2CbEFPRXHBW7WMg7D\r\n        Z8dtKmR9HCRTUwJrOFnH1YenQW3ak1wM50hKktEUOY+Ncct6amZK6w==\r\n        -----END PRIVATE KEY-----\r\nversion_info: '0'\r\n```\r\n/certs output\r\n```\r\n{\r\n \"certificates\": [\r\n  {\r\n   \"ca_cert\": [\r\n    {\r\n     \"path\": \"/init-data/service.x509.trusted_ca.pem\",\r\n     \"serial_number\": \"1455663145\",\r\n     \"subject_alt_names\": [],\r\n     \"days_until_expiration\": \"452\",\r\n     \"valid_from\": \"2016-02-12T09:17:10Z\",\r\n     \"expiration_time\": \"2021-01-16T09:17:10Z\"\r\n    }\r\n   ],\r\n   \"cert_chain\": [\r\n    {\r\n     \"path\": \"\\u003cinline\\u003e\",\r\n     \"serial_number\": \"30d28acf90d699db3ff57a35a8696f508feec3cd\",\r\n     \"subject_alt_names\": [\r\n      {\r\n       \"dns\": \"server1.example.com\"\r\n      }\r\n     ],\r\n     \"days_until_expiration\": \"537\",\r\n     \"valid_from\": \"2019-04-11T23:24:24Z\",\r\n     \"expiration_time\": \"2021-04-10T23:24:54Z\"\r\n    }\r\n   ]\r\n  }\r\n ]\r\n}\r\n```\r\n\r\nEnvoy Log statements\r\n```\r\n[2019-10-22 21:59:53.024][9][debug][file] [source/common/filesystem/inotify/watcher_impl.cc:75] notification: fd: 1 mask: 80 file: ..data\r\n\r\n[2019-10-22 22:29:27.486][10][debug][file] [source/common/filesystem/inotify/watcher_impl.cc:50] added watch for directory: '/init-data' file: 'server_certs.yaml' fd: 1\r\n\r\n[2019-10-22 22:28:55.703][10][debug][init] [source/common/init/manager_impl.cc:20] added target SdsApi server_cert to init manager Server\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8729/comments",
    "author": "abhishek-jain11",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-11-22T02:03:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-11-29T02:19:02Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 8644,
    "title": "Redis through envoy gets high response time with redis-benchmark",
    "created_at": "2019-10-17T14:39:47Z",
    "closed_at": "2019-12-07T18:00:17Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8644",
    "body": "Hi, \r\nI’m deploying envoy redis in our environment and I’ve found out that it’s increasing the RTT of the 95 percentile requests to redis in about 7~ms, I have run `redis-benchmark` tool in both configurations, and through envoy the higher percents getting a much higher response time. results attached.\r\nthis is the configuration file:\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 50051\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: local_service_grpc\r\n          http_filters:\r\n          - name: envoy.router\r\n            config: {}\r\n  - name: redis_nrt_listener\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 6379\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.redis_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.redis_proxy.v2.RedisProxy\r\n          stat_prefix: egress_redis\r\n          settings:\r\n            op_timeout: 0.03s\r\n            enable_redirection: true\r\n            enable_hashtagging: true\r\n          prefix_routes:\r\n            catch_all_route:\r\n              cluster: redis_nrt_cluster\r\n  - name: redis_vol_listener\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 6380\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.redis_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.redis_proxy.v2.RedisProxy\r\n          stat_prefix: egress_redis\r\n          settings:\r\n            op_timeout: 0.03s\r\n            enable_redirection: true\r\n            enable_hashtagging: true\r\n          prefix_routes:\r\n            catch_all_route:\r\n              cluster: redis_vol_cluster\r\n  clusters:\r\n  - name: local_service_grpc\r\n    connect_timeout: 0.250s\r\n    type: logical_dns\r\n    lb_policy: round_robin\r\n    http2_protocol_options: {}\r\n    health_checks:\r\n    - timeout: 1s\r\n      interval: 3s\r\n      interval_jitter: 1s\r\n      unhealthy_threshold: 3\r\n      healthy_threshold: 3\r\n      tcp_health_check:\r\n        send:\r\n        receive: []\r\n    hosts:\r\n    - socket_address:\r\n        address: router-us-east4-b-prod.ocddx.com\r\n        port_value: 50051\r\n  - name: redis_vol_cluster\r\n    connect_timeout: 1s\r\n    type: strict_dns # static\r\n    lb_policy: MAGLEV\r\n    load_assignment:\r\n      cluster_name: redis_cluster\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: redis-us-east4-b-prd.ocddx.com\r\n                port_value: 6379\r\n  - name: redis_nrt_cluster\r\n    connect_timeout: 1s\r\n    type: strict_dns # static\r\n    lb_policy: MAGLEV\r\n    load_assignment:\r\n      cluster_name: redis_cluster\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: redis-nrt-us-east4-b-prd.ocddx.com\r\n                port_value: 6379\r\nadmin:\r\n  access_log_path: \"/var/log/envoy_admin_access.log\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\n```\r\n\r\nredis-benchmark running from container to the envoy sidecar:\r\n```\r\nredis-benchmark -h collector_envoy -t set,get\r\n====== SET ======\r\n  100000 requests completed in 7.06 seconds\r\n  50 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n\r\n4.92% <= 1 milliseconds\r\n59.69% <= 2 milliseconds\r\n73.69% <= 3 milliseconds\r\n82.54% <= 4 milliseconds\r\n88.11% <= 5 milliseconds\r\n91.77% <= 6 milliseconds\r\n94.26% <= 7 milliseconds\r\n95.89% <= 8 milliseconds\r\n97.18% <= 9 milliseconds\r\n98.01% <= 10 milliseconds\r\n98.58% <= 11 milliseconds\r\n98.97% <= 12 milliseconds\r\n99.22% <= 13 milliseconds\r\n99.43% <= 14 milliseconds\r\n99.58% <= 15 milliseconds\r\n99.73% <= 16 milliseconds\r\n99.78% <= 17 milliseconds\r\n99.81% <= 18 milliseconds\r\n99.88% <= 19 milliseconds\r\n99.90% <= 20 milliseconds\r\n99.90% <= 21 milliseconds\r\n99.93% <= 22 milliseconds\r\n99.97% <= 23 milliseconds\r\n99.98% <= 24 milliseconds\r\n99.98% <= 28 milliseconds\r\n100.00% <= 29 milliseconds\r\n14164.31 requests per second\r\n\r\n====== GET ======\r\n  100000 requests completed in 7.66 seconds\r\n  50 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n\r\n4.18% <= 1 milliseconds\r\n55.05% <= 2 milliseconds\r\n71.89% <= 3 milliseconds\r\n81.03% <= 4 milliseconds\r\n87.10% <= 5 milliseconds\r\n90.96% <= 6 milliseconds\r\n93.66% <= 7 milliseconds\r\n95.49% <= 8 milliseconds\r\n96.44% <= 9 milliseconds\r\n97.22% <= 10 milliseconds\r\n97.75% <= 11 milliseconds\r\n98.34% <= 12 milliseconds\r\n98.83% <= 13 milliseconds\r\n99.15% <= 14 milliseconds\r\n99.34% <= 15 milliseconds\r\n99.49% <= 16 milliseconds\r\n99.59% <= 17 milliseconds\r\n99.69% <= 18 milliseconds\r\n99.72% <= 19 milliseconds\r\n99.81% <= 20 milliseconds\r\n99.87% <= 21 milliseconds\r\n99.89% <= 22 milliseconds\r\n99.91% <= 23 milliseconds\r\n99.92% <= 24 milliseconds\r\n99.93% <= 25 milliseconds\r\n99.96% <= 26 milliseconds\r\n99.97% <= 27 milliseconds\r\n99.97% <= 28 milliseconds\r\n99.98% <= 29 milliseconds\r\n99.98% <= 34 milliseconds\r\n99.98% <= 35 milliseconds\r\n99.99% <= 36 milliseconds\r\n100.00% <= 37 milliseconds\r\n13063.36 requests per second\r\n```\r\n\r\nredis-benchmark running from container to a single redis host (part of a  masters cluster):\r\n```\r\nbash-4.4# redis-benchmark -h 10.240.15.147 -t set,get\r\n====== SET ======\r\n  100000 requests completed in 4.95 seconds\r\n  50 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n\r\n11.96% <= 1 milliseconds\r\n94.58% <= 2 milliseconds\r\n96.80% <= 3 milliseconds\r\n98.02% <= 4 milliseconds\r\n98.75% <= 5 milliseconds\r\n99.27% <= 6 milliseconds\r\n99.58% <= 7 milliseconds\r\n99.75% <= 8 milliseconds\r\n99.89% <= 9 milliseconds\r\n99.96% <= 10 milliseconds\r\n99.97% <= 11 milliseconds\r\n99.97% <= 12 milliseconds\r\n99.99% <= 13 milliseconds\r\n100.00% <= 13 milliseconds\r\n20197.94 requests per second\r\n\r\n====== GET ======\r\n  100000 requests completed in 5.07 seconds\r\n  50 parallel clients\r\n  3 bytes payload\r\n  keep alive: 1\r\n\r\n15.41% <= 1 milliseconds\r\n90.76% <= 2 milliseconds\r\n95.60% <= 3 milliseconds\r\n97.79% <= 4 milliseconds\r\n98.70% <= 5 milliseconds\r\n99.23% <= 6 milliseconds\r\n99.60% <= 7 milliseconds\r\n99.76% <= 8 milliseconds\r\n99.82% <= 9 milliseconds\r\n99.83% <= 11 milliseconds\r\n99.87% <= 12 milliseconds\r\n99.89% <= 13 milliseconds\r\n99.91% <= 14 milliseconds\r\n99.97% <= 15 milliseconds\r\n99.99% <= 16 milliseconds\r\n100.00% <= 16 milliseconds\r\n19704.43 requests per second\r\n```\r\nAs you can see there's a huge different between the response times, I've been trying to change some configuration for example: type to `logical_dns` instead of `strict_dns`, remove the `lb_type` and add `max_buffer_size_before_flush` and `buffer_flush_timeout` and even change the dns to point to only one member of the redis cluster, the same host I checked in the second test, to ensure the reliability of the `redis-benchmark` test. \r\n\r\nI'd be glad if someone who using redis with envoy will do the same test I did and share the results, and if someone has any recommendations to solve this response time issue \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8644/comments",
    "author": "mosespx",
    "comments": [
      {
        "user": "mosespx",
        "created_at": "2019-10-27T08:45:38Z",
        "body": "@zuercher please add a BUG label, seems like its happening also in other environments "
      },
      {
        "user": "HenryYYang",
        "created_at": "2019-10-31T04:39:42Z",
        "body": "Are you introducing a network hop in the envoy case in your test?\r\n\r\nHere's the result for my test against a local docker container:\r\n> root@8362fe3593b4:/# redis-benchmark -h redis-server -p 7001 -t get,set\r\n> \r\n> ====== SET ======\r\n>   100000 requests completed in 3.07 seconds\r\n>   50 parallel clients\r\n>   3 bytes payload\r\n>   keep alive: 1\r\n> \r\n> 84.07% <= 1 milliseconds\r\n> 99.09% <= 2 milliseconds\r\n> 99.84% <= 3 milliseconds\r\n> 99.93% <= 4 milliseconds\r\n> 99.94% <= 5 milliseconds\r\n> 99.96% <= 6 milliseconds\r\n> 99.96% <= 9 milliseconds\r\n> 99.96% <= 10 milliseconds\r\n> 99.97% <= 11 milliseconds\r\n> 99.99% <= 26 milliseconds\r\n> 100.00% <= 27 milliseconds\r\n> 100.00% <= 27 milliseconds\r\n> 32626.43 requests per second\r\n> \r\n> ====== GET ======\r\n>   100000 requests completed in 3.04 seconds\r\n>   50 parallel clients\r\n>   3 bytes payload\r\n>   keep alive: 1\r\n> \r\n> 85.27% <= 1 milliseconds\r\n> 98.79% <= 2 milliseconds\r\n> 99.85% <= 3 milliseconds\r\n> 99.97% <= 4 milliseconds\r\n> 99.97% <= 9 milliseconds\r\n> 99.99% <= 10 milliseconds\r\n> 100.00% <= 11 milliseconds\r\n> 32894.74 requests per second\r\n> \r\n> \r\n> root@8362fe3593b4:/# redis-benchmark -p 6381 -t get,set\r\n> ====== SET ======\r\n>   100000 requests completed in 3.78 seconds\r\n>   50 parallel clients\r\n>   3 bytes payload\r\n>   keep alive: 1\r\n> \r\n> 0.00% <= -32 milliseconds\r\n> 0.01% <= -30 milliseconds\r\n> 0.02% <= -29 milliseconds\r\n> 0.03% <= -28 milliseconds\r\n> 0.03% <= -26 milliseconds\r\n> 0.04% <= 0 milliseconds\r\n> 24.19% <= 1 milliseconds\r\n> 89.52% <= 2 milliseconds\r\n> 99.04% <= 3 milliseconds\r\n> 99.78% <= 4 milliseconds\r\n> 99.82% <= 5 milliseconds\r\n> 99.84% <= 6 milliseconds\r\n> 99.85% <= 7 milliseconds\r\n> 99.87% <= 8 milliseconds\r\n> 99.89% <= 10 milliseconds\r\n> 99.94% <= 11 milliseconds\r\n> 99.96% <= 12 milliseconds\r\n> 99.97% <= 13 milliseconds\r\n> 99.98% <= 14 milliseconds\r\n> 99.98% <= 15 milliseconds\r\n> 99.98% <= 16 milliseconds\r\n> 99.99% <= 19 milliseconds\r\n> 100.00% <= 20 milliseconds\r\n> 26462.03 requests per second\r\n> \r\n> ====== GET ======\r\n>   100000 requests completed in 3.05 seconds\r\n>   50 parallel clients\r\n>   3 bytes payload\r\n>   keep alive: 1\r\n> \r\n> 28.07% <= 1 milliseconds\r\n> 96.50% <= 2 milliseconds\r\n> 99.51% <= 3 milliseconds\r\n> 99.80% <= 4 milliseconds\r\n> 99.95% <= 5 milliseconds\r\n> 100.00% <= 6 milliseconds\r\n> 100.00% <= 6 milliseconds\r\n> 32797.64 requests per second\r\n\r\nHere's the relevant section of envoy.yaml:\r\n```\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_1\r\n      address:\r\n        socket_address:\r\n          address: 127.0.0.1\r\n          port_value: 6381\r\n      filter_chains:\r\n        filters:\r\n          name: envoy.redis_proxy\r\n          config:\r\n            stat_prefix: redis_stats\r\n            prefix_routes:\r\n              catch_all_route:\r\n                cluster: cluster_1\r\n            settings:\r\n              op_timeout: 5s\r\n  clusters:\r\n    - name: cluster_1\r\n      connect_timeout: 0.25s\r\n      lb_policy: RING_HASH\r\n      hosts:\r\n      - socket_address:\r\n          address: redis-server\r\n          port_value: 7001\r\n      type: STRICT_DNS\r\n```"
      },
      {
        "user": "mosespx",
        "created_at": "2019-10-31T15:01:19Z",
        "body": "envoy is a sidecar container on the application, your second test is through local envoy? "
      },
      {
        "user": "HenryYYang",
        "created_at": "2019-10-31T17:04:12Z",
        "body": "I ran both tests on the envoy container to get an apple to apple measurement."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-11-30T17:09:11Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-07T18:00:16Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "saagar241290",
        "created_at": "2020-04-30T11:49:49Z",
        "body": "@mosespx i am facing the same issue. Did you got any solution or workaround?"
      },
      {
        "user": "mosespx",
        "created_at": "2020-04-30T15:47:26Z",
        "body": "@saagar241290 no, I didn't use this solution because of this issue.\r\nplease share here if you find something interesting"
      },
      {
        "user": "saagar241290",
        "created_at": "2020-05-06T03:41:12Z",
        "body": "@mosespx I tried by increasing number of connections of redis pool to 100 and it gave me a better performance. Earlier there was only a single connection."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2022-10-12T08:48:25Z",
        "body": "@saagar241290 when you says \r\n>  tried by increasing number of connections of redis pool to 100 and it gave me a better performance. Earlier there was only a single connection.\r\n\r\nAre these connections on client?"
      }
    ]
  },
  {
    "number": 8626,
    "title": "gRPC load balancing with Envoy Proxy",
    "created_at": "2019-10-16T11:55:21Z",
    "closed_at": "2019-10-16T18:01:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8626",
    "body": "I am trying to load balance the traffic of two instances of a microservice \"A\" using envoy. \"A\" exposes gRPC services which is being called by another microservice \"B\". I am running a single instance of Envoy with the following configuration.\r\n\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address: { address: 127.0.0.1, port_value: 9901 }\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address: { address: 127.0.0.1, port_value: 10000 }\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          codec_type: AUTO\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match: { prefix: \"/\" }\r\n                route: { cluster: some_service }\r\n          http_filters:\r\n          - name: envoy.router\r\n  clusters:\r\n  - name: some_service\r\n    connect_timeout: 0.25s\r\n    type: STATIC\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: some_service\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 127.0.0.1\r\n                port_value: 8081\r\n\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 127.0.0.1\r\n                port_value: 8082\r\n```\r\n\r\n**However, when the client service makes a gRPC call, the following exception is being thrown.**\r\n\r\nException in thread \"pool-1-thread-5\" io.grpc.StatusRuntimeException: UNAVAILABLE: upstream connect error or disconnect/reset before headers. reset reason: connection termination\r\n\tat io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:235)\r\n\tat io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:216)\r\n\tat io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:141)\r\n\tat com.sample.grpc.HelloServiceGrpc$HelloServiceBlockingStub.hello(HelloServiceGrpc.java:150)\r\n\tat com.example.demo1.MicroserviceAApplication.lambda$runGrpcClient$0(MicroserviceAApplication.java:38)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n \r\nIs this the right way to call a gRPC service keeping Envoy in the middle? What changes do I need to make ? ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8626/comments",
    "author": "iaintamit",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2019-10-16T16:36:03Z",
        "body": "By default the Cluster's `protocol_selection` field will be `USE_CONFIGURED_PROTOCOL` and the default upstream protocol is http1.1. I think what's happening is that Envoy is converting the inbound grpc (http/2) request to http/1.1 and the upstream is rejecting the connection.\r\n\r\nYou can add `http_protocol_options: {}` to your cluster definition to indicate that it is http/2 only and that should fix the immediate problem. You could also specify `protocol_selection: USE_DOWNSTREAM_PROTOCOL` but that only makes sense if the upstreams support both http 1.1 and 2. \r\n\r\nIn addition, if you don't plan on using http/1.1 on the downstream side, you might also consider setting that `codec_type: HTTP2` to avoid forwarding converted http/1.1 requests."
      },
      {
        "user": "iaintamit",
        "created_at": "2019-10-16T18:01:41Z",
        "body": "Thanks for the help! "
      },
      {
        "user": "zuercher",
        "created_at": "2019-10-16T21:35:35Z",
        "body": "Just noticed that my comment had a typo. I'll post here in case anyone else comes across this. It should have said to add `http2_protocol_options: {}`."
      }
    ]
  },
  {
    "number": 8513,
    "title": "envoy crashes at startup with std::bad_cast exception",
    "created_at": "2019-10-07T12:49:36Z",
    "closed_at": "2019-12-14T18:55:04Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8513",
    "body": "*Description*:\r\nI've compiled some of my filters as well but according to the call stack the problem is within the `source/server/http/admin.cc` file in line `139`.\r\nThis is the problematic line: `const std::regex PromRegex(\"[^a-zA-Z0-9_]\");`\r\n\r\nI have a shared library that my filters link with. The .so is compiled with gcc 7 and envoy compiles within its container with gcc 5.\r\n\r\nI`m running on ubuntu 18.04\r\n\r\n*Call Stack*:\r\n```#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n#1  0x00007ffff672a801 in __GI_abort () at abort.c:79\r\n#2  0x000000000189e75d in __gnu_cxx::__verbose_terminate_handler() ()\r\n#3  0x000000000180e366 in __cxxabiv1::__terminate(void (*)()) ()\r\n#4  0x000000000180e3b1 in std::terminate() ()\r\n#5  0x000000000180d5b9 in __cxa_throw ()\r\n#6  0x00000000018182a2 in __cxa_bad_cast ()\r\n#7  0x000000000186c13c in std::__cxx11::collate<char> const& std::use_facet<std::__cxx11::collate<char> >(std::locale const&) ()\r\n#8  0x0000000000afeb98 in std::__cxx11::regex_traits<char>::transform<char*> (this=0x33ae450, __first=0x33e4028 \"\", __last=0x33e4029 \"@>\\003\") at /usr/include/c++/5/bits/regex.h:233\r\n#9  0x0000000000afc974 in std::__cxx11::regex_traits<char>::transform_primary<char*> (this=0x33ae450, __first=0x7fffffffc954 \"\", __last=0x7fffffffc955 \"\\177\") at /usr/include/c++/5/bits/regex.h:266\r\n#10 0x0000000000af8402 in std::__detail::_BracketMatcher<std::__cxx11::regex_traits<char>, false, false>::_M_apply (this=0x7fffffffcb20, __ch=0 '\\000') at /usr/include/c++/5/bits/regex_compiler.tcc:581\r\n#11 0x0000000000af3ebd in std::__detail::_BracketMatcher<std::__cxx11::regex_traits<char>, false, false>::_M_make_cache (this=0x7fffffffcb20) at /usr/include/c++/5/bits/regex_compiler.h:492\r\n#12 0x0000000000af0448 in std::__detail::_BracketMatcher<std::__cxx11::regex_traits<char>, false, false>::_M_ready (this=0x7fffffffcb20) at /usr/include/c++/5/bits/regex_compiler.h:459\r\n#13 0x0000000000af0ffe in std::__detail::_Compiler<std::__cxx11::regex_traits<char> >::_M_insert_bracket_matcher<false, false> (this=0x7fffffffcf80, __neg=true)\r\n    at /usr/include/c++/5/bits/regex_compiler.tcc:428\r\n#14 0x0000000000aed255 in std::__detail::_Compiler<std::__cxx11::regex_traits<char> >::_M_bracket_expression (this=0x7fffffffcf80) at /usr/include/c++/5/bits/regex_compiler.tcc:355\r\n#15 0x0000000000ae8418 in std::__detail::_Compiler<std::__cxx11::regex_traits<char> >::_M_atom (this=0x7fffffffcf80) at /usr/include/c++/5/bits/regex_compiler.tcc:341\r\n#16 0x0000000000ae64f1 in std::__detail::_Compiler<std::__cxx11::regex_traits<char> >::_M_term (this=0x7fffffffcf80) at /usr/include/c++/5/bits/regex_compiler.tcc:139\r\n#17 0x0000000000ae3e63 in std::__detail::_Compiler<std::__cxx11::regex_traits<char> >::_M_alternative (this=0x7fffffffcf80) at /usr/include/c++/5/bits/regex_compiler.tcc:121\r\n#18 0x0000000000ae1615 in std::__detail::_Compiler<std::__cxx11::regex_traits<char> >::_M_disjunction (this=0x7fffffffcf80) at /usr/include/c++/5/bits/regex_compiler.tcc:97\r\n#19 0x0000000000adeda9 in std::__detail::_Compiler<std::__cxx11::regex_traits<char> >::_Compiler (this=0x7fffffffcf80, __b=0x1bfbc71 \"[^a-zA-Z0-9_]\", __e=0x1bfbc7e \"\", __loc=..., __flags=(unknown: 16))\r\n    at /usr/include/c++/5/bits/regex_compiler.tcc:82\r\n#20 0x0000000000adbf18 in std::__detail::__compile_nfa<char const*, std::__cxx11::regex_traits<char> > (__first=0x1bfbc71 \"[^a-zA-Z0-9_]\", __last=0x1bfbc7e \"\", __loc=..., __flags=(unknown: 16))\r\n    at /usr/include/c++/5/bits/regex_compiler.h:194\r\n#21 0x0000000000ad9590 in std::__cxx11::basic_regex<char, std::__cxx11::regex_traits<char> >::basic_regex<char const*> (this=0x29b6a20 <Envoy::Server::(anonymous namespace)::PromRegex>,\r\n    __first=0x1bfbc71 \"[^a-zA-Z0-9_]\", __last=0x1bfbc7e \"\", __loc=..., __f=(unknown: 16)) at /usr/include/c++/5/bits/regex.h:767\r\n#22 0x0000000000ad6a70 in std::__cxx11::basic_regex<char, std::__cxx11::regex_traits<char> >::basic_regex<char const*> (this=0x29b6a20 <Envoy::Server::(anonymous namespace)::PromRegex>,\r\n    __first=0x1bfbc71 \"[^a-zA-Z0-9_]\", __last=0x1bfbc7e \"\", __f=(unknown: 16)) at /usr/include/c++/5/bits/regex.h:512\r\n#23 0x0000000000ad454a in std::__cxx11::basic_regex<char, std::__cxx11::regex_traits<char> >::basic_regex (this=0x29b6a20 <Envoy::Server::(anonymous namespace)::PromRegex>, __p=0x1bfbc71 \"[^a-zA-Z0-9_]\",\r\n    __f=(unknown: 16)) at /usr/include/c++/5/bits/regex.h:445\r\n#24 0x0000000000fc9044 in __static_initialization_and_destruction_0 (__initialize_p=1, __priority=65535) at external/envoy/source/server/http/admin.cc:139\r\n#25 0x0000000000fc9097 in _GLOBAL__sub_I_admin.cc(void) () at external/envoy/source/server/http/admin.cc:1223\r\n#26 0x00000000018b1c6d in __libc_csu_init ()\r\n#27 0x00007ffff670bb28 in __libc_start_main (main=0x8631e0 <main(int, char**)>, argc=12, argv=0x7fffffffd378, init=0x18b1c20 <__libc_csu_init>, fini=<optimized out>, rtld_fini=<optimized out>,\r\n    stack_end=0x7fffffffd368) at ../csu/libc-start.c:266\r\n---Type <return> to continue, or q <return> to quit---\r\n#28 0x0000000000419799 in _start ()```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8513/comments",
    "author": "yurado",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-11-07T06:57:37Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-07T17:00:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-14T18:55:03Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 8410,
    "title": "Envoy should connect to xDS server immediately when it resolves its addr",
    "created_at": "2019-09-27T02:06:22Z",
    "closed_at": "2020-01-03T07:24:35Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8410",
    "body": "We found a big issue, when we have static clusters provided in the bootstrap config. But in our env, there is something wrong with the dns. And the trace shows that envoy takes 25s to resolve the `zipkin.istio-system`. But for pilot, it is instantly.  But what we saw is weird, envoy does not connect to pilot immediately.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8410/comments",
    "author": "hzxuzhonghu",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-10-27T19:09:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "hzxuzhonghu",
        "created_at": "2019-10-28T02:40:26Z",
        "body": "This block the xds sync, anyone please help."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-11-27T03:15:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "hzxuzhonghu",
        "created_at": "2019-11-27T05:55:26Z",
        "body": "/not stale"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-12-27T06:28:59Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-01-03T07:24:34Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 8195,
    "title": "hot restart: set drain time value to 1s",
    "created_at": "2019-09-10T07:00:51Z",
    "closed_at": "2019-10-17T19:47:13Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8195",
    "body": "Description:\r\n_As Envoy Doc says, draining is the process by which Envoy attempts to gracefully shed connections in response to various events. Draining occurs at the following times:\r\n1.The server has been manually health check failed via the healthcheck/fail admin endpoint. \r\n2.The server is being hot restarted.\r\n3.Individual listeners are being modified or removed via LDS._\r\n\r\nWhen individual listeners are being modified, with default drain-time-s value(45s), it will take 1~2 minutes before the new connections are available, but in our scenarios, we prefer the new connections can be established more quickly, so we try to set drain-time-s value to 1s and it will take less time to establish the connections. we wonder if this will bring other problems when set \r\ndrain-time-s value to less value like 1s.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8195/comments",
    "author": "vincentgoat",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-10-10T16:07:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-17T19:47:12Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 8168,
    "title": "Cannot use EDS cluster type for control plane, because ADS only rely on primary clusters",
    "created_at": "2019-09-06T10:59:49Z",
    "closed_at": "2019-10-13T20:00:51Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8168",
    "body": "\r\n*Title*: *Cannot use EDS cluster type for control plane, because ADS only rely on primary clusters.*\r\n\r\n*Description*:\r\nCurrently we are using load balancer address and port to connect to Control-plane, but we observe some problems caused by this setup. We want to switch from STATIC to EDS cluster type for control-plane cluster definition. In code I can see that EDS type cluster is loaded after ADS configuration because EDS cluster may potentially need ADS. \r\nI have configuration like this:\r\n```\r\n clusters:\r\n  - name: control-plane\r\n    type: EDS\r\n    eds_cluster_config:\r\n      eds_config:\r\n        path: '/tmp/envoy/eds.yaml'\r\n\r\ndynamic_resources:\r\n  lds_config:\r\n    ads: {}\r\n  cds_config:\r\n    ads: {}\r\n  ads_config:\r\n    api_type: GRPC\r\n    grpc_services:\r\n      envoy_grpc:\r\n        cluster_name: control-plane\r\n```\r\n\r\nIs it possible to add some (flag/type) which allows to define EDS cluster which will be loaded before ADS setup phase? \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8168/comments",
    "author": "lukidzi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-10-06T17:21:29Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-13T20:00:50Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 8146,
    "title": "[question] Envoy to fail requests if ext_authz is unavailable?",
    "created_at": "2019-09-04T08:11:40Z",
    "closed_at": "2019-09-04T15:50:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8146",
    "body": "By doing some investigation, looks like this is not supported, but wanted to confirm.\r\nIs it possible to configure Envoy to fail http requests in case ext_authz service is unavailable? This could be useful when one runs cluster of envoys behind L7 LB and LB performs http health checks towards envoy instances. Does envoy support health checking towards ext_authz cluster? Could those maybe be propagated to the envoys health check response? ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8146/comments",
    "author": "nezdolik",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-09-04T15:38:30Z",
        "body": "ext_authz should be a cluster like any other cluster, meaning that when Envoy goes to send a request to the cluster in the filter, it should handle a no healthy hosts failure condition and fail the request. "
      },
      {
        "user": "nezdolik",
        "created_at": "2019-09-04T15:50:07Z",
        "body": "Thanks @mattklein123! "
      }
    ]
  },
  {
    "number": 8124,
    "title": "Handshake Error filled Log File",
    "created_at": "2019-09-03T10:55:50Z",
    "closed_at": "2019-09-03T15:23:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8124",
    "body": "I 'm using Ambassador API Gateway which is using envoy as the proxy engine\r\n\r\nand the log file is filled with handshare error : \r\n`[2019-09-03 10:49:18.735][126][debug][main] [source/server/connection_handler_impl.cc:280] [C2051540] new connection\r\n[2019-09-03 10:49:18.735][126][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:168] [C2051540] handshake error: 5\r\n[2019-09-03 10:49:18.735][126][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:201] [C2051540]\r\n[2019-09-03 10:49:18.735][126][debug][connection] [source/common/network/connection_impl.cc:188] [C2051540] closing socket: 0\r\n[2019-09-03 10:49:18.735][126][debug][main] [source/server/connection_handler_impl.cc:80] [C2051540] adding to cleanup list\r\n[2019-09-03 10:49:18.739][125][debug][main] [source/server/connection_handler_impl.cc:280] [C2051541] new connection\r\n[2019-09-03 10:49:18.739][125][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:168] [C2051541] handshake error: 5\r\n[2019-09-03 10:49:18.739][125][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:201] [C2051541]\r\n[2019-09-03 10:49:18.739][125][debug][connection] [source/common/network/connection_impl.cc:188] [C2051541] closing socket: 0\r\n[2019-09-03 10:49:18.739][125][debug][main] [source/server/connection_handler_impl.cc:80] [C2051541] adding to cleanup list\r\n[2019-09-03 10:49:18.781][126][debug][main] [source/server/connection_handler_impl.cc:280] [C2051542] new connection\r\n[2019-09-03 10:49:18.782][126][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:168] [C2051542] handshake error: 5\r\n[2019-09-03 10:49:18.782][126][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:201] [C2051542]\r\n[2019-09-03 10:49:18.782][126][debug][connection] [source/common/network/connection_impl.cc:188] [C2051542] closing socket: 0\r\n[2019-09-03 10:49:18.782][126][debug][main] [source/server/connection_handler_impl.cc:80] [C2051542] adding to cleanup list\r\n[2019-09-03 10:49:18.804][126][debug][main] [source/server/connection_handler_impl.cc:280] [C2051543] new connection`\r\n\r\n\r\nAny Idea how to solve or hide this log ?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8124/comments",
    "author": "fauzan-n",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-09-03T15:09:48Z",
        "body": "Envoy is not designed to be run in production at trace or debug level. Run at info log level."
      },
      {
        "user": "fauzan-n",
        "created_at": "2019-09-08T06:00:51Z",
        "body": "This is not on my production , this is on my development cluster"
      }
    ]
  },
  {
    "number": 8123,
    "title": "Question : Custom filter to split PUT in several multipart requests",
    "created_at": "2019-09-03T08:02:02Z",
    "closed_at": "2019-09-03T17:38:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8123",
    "body": "Let's say I want to upload a large file to Amazon S3 using their multipart API in which the file is sent in several pieces, but I want to do that in a single upload.\r\n\r\nWould it be possible to use Envoy for that ? i.e. the client makes a single PUT request, Envoy splits the request in several pieces (and thus several requests to the upstream), all that in full streaming (no buffering whatsoever).\r\n\r\nI'm guessing it could be feasible if I code a custom filter, but before investing a lot of time in this I want to make sure I'm not missing some constraints that would make this impossible.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8123/comments",
    "author": "s-vivien",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-09-03T15:08:54Z",
        "body": "Yeah you could do this with a custom filter."
      },
      {
        "user": "htuch",
        "created_at": "2019-09-03T15:23:21Z",
        "body": "@s-vivien does this answer your question? If so, can we close out this issue?"
      },
      {
        "user": "s-vivien",
        "created_at": "2019-09-03T15:37:41Z",
        "body": "It does answer my question. Thanks."
      }
    ]
  },
  {
    "number": 8054,
    "title": "envoy.ext_authz with HTTP with POST method doesn't return the exact status.",
    "created_at": "2019-08-27T15:04:53Z",
    "closed_at": "2019-08-27T16:16:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8054",
    "body": "envoy.ext_authz with HTTP with the POST method doesn't return the exact status.\r\nWe set \r\n\r\n- `with_request_body:\r\n                max_request_bytes: 2048\r\n                allow_partial_message: false`\r\n\r\n- And do a JSON POST, from the server 200 is returned, but the envoy gets \r\n`':status', '504'\r\n'content-length', '24'\r\n'content-type', 'text/plain'`\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8054/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-08-27T15:17:27Z",
        "body": "It sounds like the call is timing out, have you tried to increase the timeout to the ext authz server?"
      },
      {
        "user": "ghost",
        "created_at": "2019-08-27T16:16:33Z",
        "body": "it works by increasing the timeout. Thanks!"
      }
    ]
  },
  {
    "number": 8049,
    "title": "Too many sendDiscoveryRequest calls for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment",
    "created_at": "2019-08-27T07:29:12Z",
    "closed_at": "2019-10-04T11:33:26Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8049",
    "body": "When did load test on microservices deployed through istio in GKE the following error occurs:\r\n\r\n[warning][upstream] external/envoy/source/common/config/grpc_mux_impl.cc:65] Too many sendDiscoveryRequest calls for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n\r\nKuberntese version: 1.12.8-gke.10\r\n\r\nistio version : client version: 1.2.4\r\nistio is installed as GKE addon",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8049/comments",
    "author": "charanrajb",
    "comments": [
      {
        "user": "l8huang",
        "created_at": "2019-08-28T05:17:36Z",
        "body": "This is duplicated with #7955"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-09-27T07:47:10Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-04T11:33:25Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7991,
    "title": "How to set a Dubbo Proxy Route Configuration apply to all interface?",
    "created_at": "2019-08-21T09:31:48Z",
    "closed_at": "2019-10-04T10:33:26Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7991",
    "body": "I have a sample  dubbo service which have many interfaces. I want set same route Configuration to all interface. When missing interface In Dubbo Proxy Route Configuration, error occurs in envoy logs as follows.\r\n\"dubbo router: no cluster match for interface 'org.apache.dubbo.demo.DemoService'\"\r\nCan I not specify Interface of Dubbo Proxy Route Configuration? How to set a Dubbo Proxy Route Configuration apply to all interface?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7991/comments",
    "author": "sennyhan",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-08-21T16:13:39Z",
        "body": "cc @zyfjeff "
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-08-23T00:04:15Z",
        "body": "@sennyhan Currently, the interface does not support wildcards. It can only be configured one by one. This can be used as a new feature."
      },
      {
        "user": "sennyhan",
        "created_at": "2019-08-23T02:10:02Z",
        "body": "> @sennyhan Currently, the interface does not support wildcards. It can only be configured one by one. This can be used as a new feature.\r\n\r\n@zyfjeff  The dubbo service which I use has hundreds of interfaces.  It 's too complicated configuring interfaces one by one. Could you tell me when the new feature supporting wildcards of interface will come? And can I submit an issue for tracking it? Thanks alot. "
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-08-23T02:26:36Z",
        "body": "@sennyhan You can create an issue to track it and I will process it in the next two weeks."
      },
      {
        "user": "sennyhan",
        "created_at": "2019-08-26T01:26:57Z",
        "body": "> @sennyhan You can create an issue to track it and I will process it in the next two weeks.\r\n\r\n@zyfjeff  Thanks alot"
      },
      {
        "user": "sennyhan",
        "created_at": "2019-08-28T09:28:32Z",
        "body": "> @sennyhan You can create an issue to track it and I will process it in the next two weeks.\r\n\r\n@zyfjeff  Could you step up it's resolution?  We need to use this feature urgently. Thanks alot!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-09-27T09:47:11Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-04T10:33:25Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "geoleonsh",
        "created_at": "2020-05-08T07:38:16Z",
        "body": "I have solved this problem and I will submit a pull request"
      }
    ]
  },
  {
    "number": 7970,
    "title": "dubbo heartbeat and magic number 20186  issue",
    "created_at": "2019-08-20T01:47:47Z",
    "closed_at": "2019-09-03T20:50:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7970",
    "body": "Description:\r\nWhen I debug envoy with dubbo protocol, found this debug message appear once every minutes,  I wonder how it works and who send this message.\r\n_[2019-08-19 12:19:53.744][29][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/decoder.cc:134] dubbo decoder:17 bytes available\r\n[2019-08-19 12:19:53.744][29][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/decoder.cc:144] dubbo decoder:protocol dubbo, state OnTransportBegin, 17 bytes available\r\n[2019-08-19 12:19:53.744][29][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/decoder.cc:20] dubbo decoder: this is the dubbo heartbeat message\r\n[2019-08-19 12:19:53.744][29][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/heartbeat_response.cc:20] buffer length 16\r\n[2019-08-19 12:19:53.744][29][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/decoder.cc:163] dubbo decoder:data length 1\r\n[2019-08-19 12:19:53.744][29][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/decoder.cc:134] dubbo decoder:1 bytes available\r\n[2019-08-19 12:19:53.744][29][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/decoder.cc:144] dubbo decoder:protocol dubbo, state OnTransportBegin, 1 bytes available\r\n[2019-08-19 12:19:53.744][29][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/decoder.cc:15] dubbo decoder: need more data for dubbo protocol\r\n[2019-08-19 12:19:53.744][29][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/decoder.cc:149] dubbo decoder:wait for data_\r\n\r\nQuestion:\r\nI also found this error below, after heartbeat message, when next message comes, this error will happen, why?\r\n_[2019-08-19 12:20:53.794][29][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/decoder.cc:134] dubbo decoder:18 bytes available\r\n[2019-08-19 12:20:53.794][29][debug][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/decoder.cc:144] dubbo decoder:protocol dubbo, state OnTransportBegin, 18 bytes available\r\n[2019-08-19 12:20:53.794][29][error][dubbo] [external/envoy/source/extensions/filters/network/dubbo_proxy/conn_manager.cc:132] [C44] dubbo error: invalid dubbo message magic number 20186\r\ndefault_words: \"kubernetes://dubboconsumer-fd56fc7d9-6m4m8.default\"_",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7970/comments",
    "author": "vincentgoat",
    "comments": [
      {
        "user": "zyfjeff",
        "created_at": "2019-08-20T02:01:53Z",
        "body": "@awesomelinhhh I suspect it didn't handle the heartbeat request, I'm testing locally., thank you for your feedback"
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-08-20T02:05:13Z",
        "body": "@awesomelinhhh Are you the latest version? This log doesn't match the latest code."
      },
      {
        "user": "vincentgoat",
        "created_at": "2019-08-20T02:09:09Z",
        "body": "> @awesomelinhhh Are you the latest version? This log doesn't match the latest code.\r\n---------\r\nbranch: envoy-wasm\r\ncommit id: f66e557b6e787043e9efde5cfcd88afdce0b9408\r\n\r\n\r\n"
      },
      {
        "user": "vincentgoat",
        "created_at": "2019-08-21T01:53:42Z",
        "body": "@zyfjeff Did you get the magic number error also in local? it looks like envoy can't solve heartbeat msg correctly, I capture the tcp msg when this error appear, heartbeat msg length is 17, but envoy just take heartbeat msg as 16 length, unexpect 4e='N' left to next msg, and magic number error happend(20186=4eda).\r\nda:bb:22:14:00:00:00:00:00:00:00:06:00:00:00:01:**4e**"
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-08-21T02:05:05Z",
        "body": "@awesomelinhhh You are right, the heartbeat processing is wrong, less drain a byte\r\nI will fix this problem this week."
      },
      {
        "user": "vincentgoat",
        "created_at": "2019-08-26T02:56:51Z",
        "body": "@zyfjeff Did you fix this problem, I can't find the pull request."
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-08-26T11:09:52Z",
        "body": "@awesomelinhhh Sorry, for personal reasons, the repair of this problem may have to be postponed for a few days."
      },
      {
        "user": "hzxuzhonghu",
        "created_at": "2019-08-30T06:31:44Z",
        "body": "any update, we meet this too."
      }
    ]
  },
  {
    "number": 7895,
    "title": "Source IP ACLs not working properly with TCP Proxy (Proto Proto enabled)",
    "created_at": "2019-08-12T14:25:07Z",
    "closed_at": "2019-09-18T21:27:15Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7895",
    "body": "*Description*:\r\ni'm working on trying to get some network RBAC filters working and am running into issues where it envoy does look to actually be applying them. I'm guessing it might be config related.  My current setup is  AWS NLB (Proxy Protocol) -> Envoy Proxy (AWS ECS, TLS Termination, TCP Proxying) -> Service (AWS ECS, No TLS)\r\n\r\n*Repro steps*:\r\nHere is my sample config:\r\n\r\n```static_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8080\r\n    filter_chains:\r\n      use_proxy_proto: true\r\n      filters:\r\n      - name: envoy.tcp_proxy\r\n        config:\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n          stat_prefix: ingress_tcp\r\n          cluster: my_service\r\n      - name: envoy.filters.network.rbac\r\n        config:\r\n          stat_prefix: tcp.ip_restrictions.\r\n          rules:\r\n            action: ALLOW\r\n            policies:\r\n              \"allow_trusted_ips\":\r\n                permissions:\r\n                  - any: true\r\n                principals:\r\n                  - source_ip:\r\n                      address_prefix: some.trusted.ip\r\n                      prefix_len: 32\r\n                  - source_ip:\r\n                      address_prefix: 10.9.0.0\r\n                      prefix_len: 16\r\n```\r\n\r\nWhen performing a request through the AWS NLB, the ip restrictions are not applied.  When looking at the stats of the envoy service they look like this:\r\n\r\n```\r\ntcp.ingress_tcp.downstream_cx_no_route: 0\r\ntcp.ingress_tcp.downstream_cx_rx_bytes_buffered: 0\r\ntcp.ingress_tcp.downstream_cx_rx_bytes_total: 0\r\ntcp.ingress_tcp.downstream_cx_total: 2530\r\ntcp.ingress_tcp.downstream_cx_tx_bytes_buffered: 0\r\ntcp.ingress_tcp.downstream_cx_tx_bytes_total: 0\r\ntcp.ingress_tcp.downstream_flow_control_paused_reading_total: 0\r\ntcp.ingress_tcp.downstream_flow_control_resumed_reading_total: 0\r\ntcp.ingress_tcp.idle_timeout: 0\r\ntcp.ingress_tcp.upstream_flush_active: 0\r\ntcp.ingress_tcp.upstream_flush_total: 0\r\ntcp.ip_restrictions.rbac.allowed: 0\r\ntcp.ip_restrictions.rbac.denied: 0\r\ntcp.ip_restrictions.rbac.shadow_allowed: 0\r\ntcp.ip_restrictions.rbac.shadow_denied: 0\r\n```\r\n\r\nThe rbac allowed and denied are both 0 which seems to mean that my RBAC policies didn't actually load properly. \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7895/comments",
    "author": "rongutierrez",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-08-12T19:30:04Z",
        "body": "You need the rbac filter before your tcp_proxy filter, any filter placed behind tcp_proxy has no effect. This is going to be error after #7779 "
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-09-11T21:25:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-09-18T21:27:14Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7894,
    "title": "envoy 1.11 use websocket 503 UR",
    "created_at": "2019-08-12T09:11:54Z",
    "closed_at": "2019-08-16T08:44:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7894",
    "body": "hi, Now I use envoy 1.7 as a websocket proxy is ok，my config is\r\n```\r\n \"filters\": [\r\n        {\r\n          \"type\": \"read\",\r\n          \"name\": \"http_connection_manager\",\r\n          \"config\": {\r\n            \"codec_type\": \"auto\",\r\n            \"stat_prefix\": \"ssl_ingress_http\",\r\n            \"use_remote_address\": true,\r\n            \"http2_settings\": {\"max_concurrent_streams\": 1024000},\r\n            \"idle_timeout_s\": 300,\r\n            \"route_config\":\r\n            {\r\n                \"virtual_hosts\":\r\n                [\r\n                    {\r\n                        \"name\" : \"backend\",\r\n                        \"domains\" : [\"*\"],\r\n                        \"routes\" : [\r\n                            {\r\n\t\t\t\t\"prefix\" : \"/\", \"use_websocket\": true, \"cluster\": \"backend\"\r\n\t\t\t\t  ]\r\n    \t\t\t        }]\r\n    \t\t        }]\r\n                    }\r\n                ]\r\n            },\r\n            \"filters\": [\r\n\t\t{\r\n\t\t  \"type\": \"both\",\r\n\t\t  \"name\": \"health_check\",\r\n\t\t  \"config\": {\r\n\t\t    \"pass_through_mode\": false,\r\n\t\t    \"endpoint\": \"/healthcheck\"\r\n\t\t   }\r\n\t\t},\r\n              {\r\n                \"type\": \"decoder\",\r\n                \"name\": \"router\",\r\n                \"config\": {}\r\n              }\r\n            ],\r\n            \"access_log\" :\r\n            [\r\n                {\"path\" : \"/dev/stdout\"}\r\n            ]\r\n\t  }\r\n        }\r\n      ]\r\n```\r\nNow I want to upgrade to version 1.11，my config is\r\n```\r\nfilter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          http_filters:\r\n          - name: envoy.health_check\r\n            config: {pass_through_mode: true, endpoint: /healthcheck}\r\n          - name: envoy.router\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match: {prefix: \"/\"}\r\n                route:\r\n                  cluster:backend\r\n          codec_type: auto\r\n          upgrade_configs:\r\n          - upgrade_type: websocket\r\n          access_log:\r\n            name: envoy.file_access_log\r\n            config: {path: /dev/stdout}\r\n          stat_prefix: ingress_http\r\n          use_remote_address: true\r\n          idle_timeout: 10s\r\n```\r\n**The request always returns 503 UR, the upstream server is the same。may I know what is the reason？**\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7894/comments",
    "author": "zhiqli",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2019-08-12T17:56:45Z",
        "body": "Offhand I wonder if your upstream doesn't handle the health check filter.   Would it perhaps work better if you used a custom fitter chain in your upgrade config, to only have the router filter?\r\nIf not, please try attaching a trace log where you get the 503.  I suspect there will be useful details as to what's going on."
      },
      {
        "user": "zhiqli",
        "created_at": "2019-08-13T07:06:59Z",
        "body": "@alyssawilk trace log just has `remote reset`\r\n```\r\n{\"log\":\"[2019-08-13 07:02:39.922][15][debug][router] [source/common/router/router.cc:868] [C61][S12200897082752209369] upstream reset: reset reason remote reset\\n\",\"stream\":\"stderr\",\"time\":\"2019-08-13T07:02:39.922679615Z\"}\r\n```"
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-08-13T13:15:21Z",
        "body": "There may be more information in the trace log, but I can't tell from the selected line.  I'd be happy to take a look if you send me a pastebin link or some such?  Meanwhile I'd suggest trying without the health check filter on the upgrade path in case your upstream doesn't allow upgrades on non-first-requests."
      },
      {
        "user": "zhiqli",
        "created_at": "2019-08-16T08:44:19Z",
        "body": "@alyssawilk maybe my sidecar envoy still 1.7。\r\nI will upgrade to the new version and test again.\r\nthank you very much\r\n"
      }
    ]
  },
  {
    "number": 7794,
    "title": "UT case failed on host not supporting IPV6",
    "created_at": "2019-08-01T01:59:17Z",
    "closed_at": "2019-09-14T07:11:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7794",
    "body": "*Title*: *UT case failed on host not supporting IPV6*\r\n\r\n*Description*:\r\nwhen running envoy UT cases on host not supporting IPV6, I used the command: \r\n`bazel test //test/... --test_env=ENVOY_IP_TEST_VERSIONS=v4only`\r\nbut it seems that some cases for IPV6 only have been executed, and failed.\r\n(Envoy was built directly on host, not using docker)\r\n\r\n*Output*:\r\n```\r\n...\r\n[ RUN      ] RouterMatcherHashPolicyTest.HashIpv6DifferentAddresses\r\nunknown file: Failure\r\nC++ exception with description \"IPv6 addresses are not supported on this machine: [2001:db8:85a3::]:0\" thrown in the test body.\r\n[  FAILED  ] RouterMatcherHashPolicyTest.HashIpv6DifferentAddresses (34 ms)\r\n...\r\n```\r\n\r\nso the arg `--test_env=ENVOY_IP_TEST_VERSIONS=v4only` didn't take effect in some UT cases?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7794/comments",
    "author": "smwyzi",
    "comments": [
      {
        "user": "smwyzi",
        "created_at": "2019-08-01T09:16:23Z",
        "body": "this problem can be fixed by adding ip version check before a case:\r\n```\r\nif (!TestEnvironment::shouldRunTestForIpVersion(Network::Address::IpVersion::v6)) {\r\n  return;\r\n}\r\n```\r\n\r\nfor example,  `RouterMatcherHashPolicyTest.HashIpv6DifferentAddresses`  fail on host not supporting IPV6 can be fixed by adding the ip version check before the case:\r\n```\r\nTEST_F(RouterMatcherHashPolicyTest, HashIpv6DifferentAddresses) {\r\n  if (!TestEnvironment::shouldRunTestForIpVersion(Network::Address::IpVersion::v6)) {\r\n    return;\r\n  }\r\n  firstRouteHashPolicy()->mutable_connection_properties()->set_source_ip(true);\r\n  {\r\n    // Different addresses should produce different hashes.\r\n    Network::Address::Ipv6Instance first_ip(\"2001:0db8:85a3:0000:0000::\");\r\n    Network::Address::Ipv6Instance second_ip(\"::1\");\r\n    Http::TestHeaderMapImpl headers = genHeaders(\"www.lyft.com\", \"/foo\", \"GET\");\r\n    const auto hash_policy = config().route(headers, 0)->routeEntry()->hashPolicy();\r\n    const uint64_t hash_1 = hash_policy->generateHash(&first_ip, headers, add_cookie_nop_).value();\r\n    const uint64_t hash_2 = hash_policy->generateHash(&second_ip, headers, add_cookie_nop_).value();\r\n    EXPECT_NE(hash_1, hash_2);\r\n  }\r\n  {\r\n    // Same IP addresses but different ports should produce the same hash.\r\n    Network::Address::Ipv6Instance first_ip(\"1:2:3:4:5::\", 8081);\r\n    Network::Address::Ipv6Instance second_ip(\"1:2:3:4:5::\", 1331);\r\n    Http::TestHeaderMapImpl headers = genHeaders(\"www.lyft.com\", \"/foo\", \"GET\");\r\n    const auto hash_policy = config().route(headers, 0)->routeEntry()->hashPolicy();\r\n    const uint64_t hash_1 = hash_policy->generateHash(&first_ip, headers, add_cookie_nop_).value();\r\n    const uint64_t hash_2 = hash_policy->generateHash(&second_ip, headers, add_cookie_nop_).value();\r\n    EXPECT_EQ(hash_1, hash_2);\r\n  }\r\n}\r\n```\r\n\r\nIs this fix a proper way?\r\nCan I send a pr to fix some cases like this?"
      },
      {
        "user": "lizan",
        "created_at": "2019-08-01T09:24:41Z",
        "body": "What is the host environment are you on? That test shouldn't fail unless you have a kernel/system library that disabled IPv6 completely.\r\n\r\nThe ENVOY_IP_TEST_VERSIONS is to determine the system has an IPv6 address (usually loopback) assigned so integration test can be performed."
      },
      {
        "user": "smwyzi",
        "created_at": "2019-08-01T10:32:32Z",
        "body": "my host is `CentOS 6.3`, and the Linux kernel version is `3.10.0_3-0-0-27`"
      },
      {
        "user": "lizan",
        "created_at": "2019-08-08T04:34:14Z",
        "body": "@smwyzi Thanks, that is beyond the platform we're supporting. Though if you want to patch, use `Network::Address::ipFamilySupported` instead of `TestEnvironment::shouldRunTestForIpVersion`. I believe there are other cases failing your test on CentOS 6 too, no?"
      },
      {
        "user": "smwyzi",
        "created_at": "2019-08-08T05:35:52Z",
        "body": "@lizan \r\nOur product environment is CentOS6, so we must build and test on it :)\r\nYes, it has about 30 tests failing totally due to the Ipv6 syscall like description above.\r\n\r\nOk, i will try the solution as you say using `Network::Address::ipFamilySupported`, if it works, i will try to give back a patch.\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-09-07T06:26:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-09-14T07:11:18Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7770,
    "title": "Questions Lua body replacement",
    "created_at": "2019-07-31T01:53:32Z",
    "closed_at": "2019-09-07T18:07:30Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7770",
    "body": "Hello all,\r\nis there a function to change the body?\r\nI tried to use lua functions that are in the document, but only available to replace headers. Is there an example script to change the body response. example:\r\n**original response body:**\r\n```\r\nfoo bar\r\nbar foo\r\nfoo and baro\r\n```\r\nand i want replace \"bar\" into \"hi\", become:\r\n```\r\nfoo hi\r\nhi foo\r\nfoo and baro\r\n```\r\ntry response_handle:body():replace(\"bar\", \"hi\")\r\nbut not working.\r\nthanks and sorry my bad english.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7770/comments",
    "author": "cwultia",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-08-31T16:44:05Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-09-07T18:07:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7748,
    "title": "Can I use Envoy_log() to print some debug infos or log in the files?",
    "created_at": "2019-07-29T07:49:27Z",
    "closed_at": "2019-09-21T00:46:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7748",
    "body": "for title,i want to know how to use this func.thanks,or other way i can debug in the code.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7748/comments",
    "author": "hbgongen",
    "comments": [
      {
        "user": "derekargueta",
        "created_at": "2019-07-29T18:22:00Z",
        "body": "If you're referring to writing a custom extension then yes, you can use the `ENVOY_LOG` macro to emit your own messages to the Envoy log. The class you log from needs to inherit the `Logger::Loggable` interface with one of the log identifiers in the template. For example:\r\n```\r\nclass MyThing : protected Logger::Loggable<Envoy::Logger::Id::main> {\r\n...\r\n}\r\n```\r\n\r\nTo use the `main` logging identifier."
      },
      {
        "user": "hbgongen",
        "created_at": "2019-08-14T16:40:01Z",
        "body": "@derekargueta.ok.thanks~\r\nI wonder that if envoy has some log function like printf,i can print some logs to std."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-09-13T16:54:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-09-21T00:46:40Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7692,
    "title": "ext_authz http filter does not propagate HttpStatus over http2",
    "created_at": "2019-07-23T10:33:55Z",
    "closed_at": "2019-08-30T21:14:05Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7692",
    "body": "Tested with envoy v1.10.0\r\n\r\n*Description*:\r\nThe explicit http status code specified by external auth service is not respected when request is performed over http2 - instead the pseudo-header of \":status\" always has the value of 200. \r\n\r\n*Details*:\r\nEnvoy is setup to talk to a custom (grpc) auth service that implements the ext_authz interface. Here's the implementation of `Check` method:\r\n\r\n```go\r\nfunc (ea *extauth) Check(ctx context.Context, req *pb.CheckRequest) (*pb.CheckResponse, error) {\r\n\treturn &pb.CheckResponse{\r\n\t\tStatus: &google_rpc.Status{\r\n\t\t\tCode: (int32)(google_rpc.UNAUTHENTICATED),\r\n\t\t},\r\n\t\tHttpResponse: &pb.CheckResponse_DeniedResponse{\r\n\t\t\tDeniedResponse: &pb.DeniedHttpResponse{\r\n\t\t\t\tStatus: &pb_type.HttpStatus{\r\n\t\t\t\t\tCode: pb_type.StatusCode_Unauthorized, // == 401\r\n\t\t\t\t},\r\n\t\t\t},\r\n\t\t},\r\n\t}, nil\r\n}\r\n```\r\n\r\nHere's what shows up in the access log when a request is made over http2:\r\n```\r\n\"POST /service.DummyService/Get HTTP/2\" 200 UAEX 10 0 15 - \"192.168.144.1\" \"grpc-go/1.20.1\" \"7b106f1c-dac0-4ec6-8174-7dbbb0ad4c0e\" \"0.0.0.0:9000\" \"-\"\r\n```\r\n\r\nover http1:\r\n```\r\n\"GET /service.DummyService/Get HTTP/1.1\" 401 UAEX 0 0 2 - \"192.168.144.1\" \"curl/7.54.0\" \"17dbca5b-a430-4922-900c-5f7f36e84dc5\" \"0.0.0.0:9000\" \"-\"\r\n```\r\n\r\nAlthough \":status\" being 200 in the http2 case sort of makes sense (grpc call did fail, but \"transport\" (http2) succeeded), I expect it to match the http1 one. Or, alternatively, allow me to explicitly define http1/http2 response codes. Currently, I do this by setting the \":status\" pseudo header: \r\n\r\n```go\r\nfunc (ea *extauth) Check(ctx context.Context, req *pb.CheckRequest) (*pb.CheckResponse, error) {\r\n\treturn &pb.CheckResponse{\r\n\t\tStatus: &google_rpc.Status{\r\n\t\t\tCode: (int32)(google_rpc.UNAUTHENTICATED),\r\n\t\t},\r\n\t\tHttpResponse: &pb.CheckResponse_DeniedResponse{\r\n\t\t\tDeniedResponse: &pb.DeniedHttpResponse{\r\n\t\t\t\tStatus: &pb_type.HttpStatus{\r\n\t\t\t\t\tCode: pb_type.StatusCode_Unauthorized,\r\n\t\t\t\t},\r\n\t\t\t\tHeaders: []*core.HeaderValueOption{\r\n\t\t\t\t\t{\r\n\t\t\t\t\t\tHeader: &core.HeaderValue{\r\n\t\t\t\t\t\t\tKey: \":status\",\r\n\t\t\t\t\t\t\tValue: \"401\",\r\n\t\t\t\t\t\t},\r\n\t\t\t\t\t},\r\n\t\t\t\t},\r\n\t\t\t},\r\n\t\t},\r\n\t}, nil\r\n}\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7692/comments",
    "author": "alxyav",
    "comments": [
      {
        "user": "dio",
        "created_at": "2019-07-23T10:40:42Z",
        "body": "cc. @gsagula "
      },
      {
        "user": "dio",
        "created_at": "2019-07-23T21:32:15Z",
        "body": "Put it as question first while I'm trying to repro this. Thanks."
      },
      {
        "user": "gsagula",
        "created_at": "2019-07-24T18:21:15Z",
        "body": "@alxyav I think your header key should be `status` instead of `:status`.\r\n```\r\nHeader: &core.HeaderValue{\r\n\tKey: \"status\",\r\n\tValue: \"401\",\r\n}\r\n```"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-23T18:25:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-30T21:14:04Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7649,
    "title": "What is the automatic retry behavior？",
    "created_at": "2019-07-19T07:23:35Z",
    "closed_at": "2019-08-28T03:40:33Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7649",
    "body": "*Title*: *What is the automatic retry behavior?*\r\n\r\n*Description*:\r\nUsing static configuration, when one of the nodes is down, what is envoy's retry behavior? Will it send requests to the downtime node or reject the downtime node and send requests to other nodes for retry?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7649/comments",
    "author": "yangjinjie",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-07-20T19:40:46Z",
        "body": "Envoy does no retries by default."
      },
      {
        "user": "yangjinjie",
        "created_at": "2019-07-22T02:23:33Z",
        "body": "My configuration is like this. When one of the nodes is down, what is envoy's retry behavior? Will it send requests to the downtime node or reject the downtime node and send requests to other nodes for retry?\r\n\r\n```yaml\r\n...\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              retry_policy: { retry_on: \"connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes,5xx\", num_retries: 3 }\r\n              routes:\r\n                - match: { regex: \"/basic.*\" }\r\n                  route: { cluster: basic-rpc }\r\n                - match: { regex: \"/perm.*\" }\r\n                  route: { cluster: perm-rpc }\r\n...\r\n\r\n  - name: basic-rpc\r\n    connect_timeout: 0.5s\r\n    type: STATIC\r\n    lb_policy: ROUND_ROBIN\r\n    http2_protocol_options: {}\r\n    hosts: [\r\n    { socket_address: {address: 172.16.13.110, port_value: 4000 }},\r\n    { socket_address: {address: 172.16.13.120, port_value: 4000 }}\r\n    ]\r\n  - name: perm-rpc\r\n    connect_timeout: 0.5s\r\n    type: STATIC\r\n    lb_policy: ROUND_ROBIN\r\n    http2_protocol_options: {}\r\n    hosts: [\r\n    { socket_address: {address: 172.16.13.110, port_value: 5000 }},\r\n    { socket_address: {address: 172.16.13.120, port_value: 5000 }}\r\n    ]\r\n...\r\n```\r\n\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-21T03:08:10Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-28T03:40:32Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7586,
    "title": "Using a non-HTTP_ONLY build of libcurl with support for SSL",
    "created_at": "2019-07-15T21:19:15Z",
    "closed_at": "2019-08-22T20:51:14Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7586",
    "body": "Hi,\r\n\r\nI noticed recently libcurl was included in the build as a dependency however with `HTTP_ONLY: on` and `CMAKE_USE_OPENSSL: off`.\r\n\r\nIt seems it isn't just a matter of switching these on as enabling it causes issues with BoringSSL we build against. Crashing on `SSL_new` etc.\r\n\r\nCould someone point me to how this could be enabled?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7586/comments",
    "author": "canselcik",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-08-15T20:10:16Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-22T20:51:13Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7556,
    "title": "How should I understand Envoy memory metrics?",
    "created_at": "2019-07-12T08:27:00Z",
    "closed_at": "2019-08-18T13:16:13Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7556",
    "body": "How should understand envoy admin memory metrics?\r\nEnvoy metrics shows:\r\n```\r\nserver.memory_allocated: 4 676 640 bytes\r\nserver.memory_heap_size: 6 291 456 bytes\r\n```\r\n\r\n*ps_mem* shows usage: 26 830 336 bytes\r\n\r\n*top* RES shows 24 704 kb\r\n\r\n*smem*\r\n```\r\nUSS (Unique Set Size): 25 612 kilobytes\r\nPSS (Proportional Set Size): 25 953 kilobytes\r\nRSS (resident set size): 26 888 kilobytes\r\n```\r\n\r\nwhat is real memory usage of envoy? May I find it anywhere in metrics?\r\n\r\n```\r\n/envoy/envoy  version: e95ef6bc43daeda16451ad4ef20979d8e07a5299/1.10.0/Clean/RELEASE/BoringSSL\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7556/comments",
    "author": "andrzejwaw",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-08-11T12:23:56Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-18T13:16:13Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "XanderStrike",
        "created_at": "2019-10-28T17:29:12Z",
        "body": "Hey, I'm curious about envoy's memory usage as well. @alyssawilk do you know where we could go to find answers?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-10-30T15:55:13Z",
        "body": "Unfortunately not offhand.  I think @htuch had aspirations to have every component in Envoy track and report memory but I think that's so far down our priority list I'd be surprised if it ever got done (unless someone sponsors an intern or GSOC mentee to pick it up)"
      }
    ]
  },
  {
    "number": 7554,
    "title": "How can I add HSTS header when Gateway Timeout",
    "created_at": "2019-07-12T03:54:08Z",
    "closed_at": "2019-08-18T12:16:13Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7554",
    "body": "Add HSTS header when Gateway Timeout\r\n\r\n_Description:_\r\nFor security reason, I add HSTS header for every upstream service response. But when gateway timeout, envoy doesn't add HSTS header,  and this leading to a BurpSuite warning. Is there any ways to add HSTS header at this time?\r\n\r\nIt seems like this issue:\r\nEnvoy doesn't add headers in case of Gateway Timeout #4317\r\nbut I found no enhancement for it.\r\n\r\nThanks! \r\n\r\nEnvoy Reply:\r\nHTTP/1.1 503 Service Unavailable\r\ncontent-length: 57\r\ncontent-type: text/plain\r\ndate: Tue, 09 Jul 2019 13:15:03 GMT\r\nserver: envoy\r\nconnection: close\r\nupstream connect error or disconnect/reset before headers\r\n\r\nHSTS:\r\nThe application should instruct web browsers to only access the application using HTTPS. To do this, enable HTTP Strict Transport Security (HSTS) by adding a response header with the name 'Strict-Transport-Security' and the value 'max-age=expireTime', where expireTime is the time in seconds that browsers should remember that the site should only be accessed using HTTPS. Consider adding the 'includeSubDomains' flag if appropriate.\r\n\r\nNote that because HSTS is a \"trust on first use\" (TOFU) protocol, a user who has never accessed the application will never have seen the HSTS header, and will therefore still be vulnerable to SSL stripping attacks. To mitigate this risk, you can optionally add the 'preload' flag to the HSTS header, and submit the domain for review by browser vendors.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7554/comments",
    "author": "gatesking",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-08-11T11:23:56Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-18T12:16:12Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7501,
    "title": "Making outbound http requests from a filter",
    "created_at": "2019-07-09T00:41:15Z",
    "closed_at": "2019-08-30T20:14:05Z",
    "labels": [
      "enhancement",
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7501",
    "body": "Title: Making outbound http requests from a filter\r\n\r\nDescription:\r\nI'm trying to write a Google OAuth 2 filter which requires me to make an outbound HTTP call to www.googleapis.com with the token so as to parse the token's information. I can't seem to find a generic http library in Envoy which allows me to do that. I looked at `Http:AsyncClient` and it seems that it requires specifying a cluster. I'm unsure if that's the expected way to making outbound HTTP calls. I'd expect something similar to what you'd find in other languages such as `send(URL, request)`. Is there a native/compliant library I can use for this? Google has it's own C++ library to make API calls but I'm unsure if it plays well with Envoy's async request-handling model.\r\n\r\nApologies beforehand if this has been answered elsewhere. I tried googling for a couple of hours and looking at other issues here, but I couldn't find anything. Thanks.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7501/comments",
    "author": "alexjames",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2019-07-09T13:27:07Z",
        "body": "Hm, I agree that async client is pretty tied to cluster.  \r\n\r\nWithout doing much digging I'd assume that Google has a gRPC API for what you're doing, and that'd probably be the easiest way to get this working.  You can see other examples of filters doing gRPC side calls, like the oath filter.\r\n\r\nIf that doesn't work I think this will require some code changes.  Last month I probably would have suggested refactoring it to be more neutral, but now it might make more sense to tweak @mattklein123 's forward proxy work since that handles the DNS lookup etc."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-07-09T13:51:50Z",
        "body": "They way I would typically suggest doing this is to setup a logical_dns cluster for www.googleapis.com and then use that via async client. This is probably what you want given that all DNS resolution will then happen asynchronously. However, like @alyssawilk said, with the forward proxy work now well along, it probably would not be that difficult to get that working in the context of async client, though that is not something I'm planning on working on."
      },
      {
        "user": "alexjames",
        "created_at": "2019-07-24T18:41:24Z",
        "body": "Sorry about the delay in response, I was able to get this to work using a logical_dns cluster. Thanks Alyssa and Matt for your assistance."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-23T19:25:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-30T20:14:04Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7487,
    "title": "Envoy sidecar proxy not working for gRPC service",
    "created_at": "2019-07-08T14:52:08Z",
    "closed_at": "2019-07-09T05:08:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7487",
    "body": "**Issue Template**\r\n\r\n*Title*: Envoy sidecar proxy not working for gRPC service\r\n\r\n*Description*:\r\nI have deployed a gRPC service (built in Java) on K8S with a envoy sidecar. When another services calls this service directly as a gRPC client, it works. However, as I configure the client service to calls through the envoy sidecar proxy, it gives error : RPC failed: Status{code=UNAVAILABLE, description=upstream connect error or disconnect/reset before headers. reset reason: connection failure, cause=null}\r\nSo it reached the envoy sidecar, but failed to propagate the call to profileservice (in same pod, configured as localhost)\r\n\r\n*Config*:\r\nMy envoy config is as below:\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8980\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          access_log:\r\n            - name: envoy.file_access_log\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\r\n                path: \"/var/log/access.log\"\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: services\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: frontend_service\r\n              cors:\r\n                allow_origin: [\"*\"]\r\n                allow_methods: GET, PUT, DELETE, POST\r\n                allow_headers: \"keep-alive,user-agent,cache-control,content-type,content-transfer-encoding,x-accept-content-transfer-encoding,x-accept-response-streaming,x-user-agent,x-grpc-web\"\r\n          http_filters:\r\n          - name: envoy.cors\r\n            typed_config: {}\r\n          - name: envoy.router\r\n            typed_config: {}\r\n  clusters:\r\n  - name: frontend_service\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    http2_protocol_options: {}\r\n    load_assignment:\r\n      cluster_name: frontend_service\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: localhost\r\n                port_value: 9980\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\n\r\n\r\nThank you for looking into it, and your suggestions.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7487/comments",
    "author": "sibendu",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2019-07-08T18:17:39Z",
        "body": "You might get more detail if you run your instance with \"-l trace\", which dumps a bunch of debug information.\r\nAlternately @lizan may have some gRPC specific debug tips"
      },
      {
        "user": "sibendu",
        "created_at": "2019-07-09T04:55:49Z",
        "body": "@alyssawilk  thank you for looking into it. I had my config wrong. Now it is working.\r\nRegards."
      },
      {
        "user": "sibendu",
        "created_at": "2019-07-09T05:08:47Z",
        "body": "Here is the corrected one, in case anybody comes across same:\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8980\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: services\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: frontend_service\r\n          http_filters:\r\n          - name: envoy.cors\r\n            typed_config: {}\r\n          - name: envoy.router\r\n            typed_config: {}\r\n  clusters:\r\n  - name: frontend_service\r\n    connect_timeout: 5s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    dns_lookup_family: V4_ONLY\r\n    http2_protocol_options: {}\r\n    load_assignment:\r\n      cluster_name: frontend_service\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: localhost\r\n                port_value: 9980"
      }
    ]
  },
  {
    "number": 7468,
    "title": "Hot restart lead to the abnormal exit of the new process",
    "created_at": "2019-07-04T07:00:40Z",
    "closed_at": "2019-07-18T02:03:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7468",
    "body": "*Title*: *Hot restart lead to the abnormal exit of the new process*\r\n\r\n*Description*:\r\n\r\n\r\nDuring the hot restart, the new Envoy process will start a timer to send `sendParentTerminateRequest` messages after taking over the traffic. but if the parent process quits early, can lead to ` sendParentTerminateRequest ` trigger RELEASE_ASSERT send failure, this kind of behavior is unacceptable, because the new process has already started to take over the traffic.\r\n\r\n```\r\nvoid DrainManagerImpl::startParentShutdownSequence() {\r\n  ASSERT(!parent_shutdown_timer_);\r\n  parent_shutdown_timer_ = server_.dispatcher().createTimer([this]() -> void {\r\n    // Shut down the parent now. It should have already been draining.\r\n    ENVOY_LOG(info, \"shutting down parent after drain\");\r\n    server_.hotRestart().sendParentTerminateRequest();\r\n  });\r\n\r\n  parent_shutdown_timer_->enableTimer(std::chrono::duration_cast<std::chrono::milliseconds>(\r\n      server_.options().parentShutdownTime()));\r\n}\r\n\r\nvoid HotRestartingChild::sendParentTerminateRequest() {\r\n  if (restart_epoch_ == 0 || parent_terminated_) {\r\n    return;\r\n  }\r\n  HotRestartMessage wrapped_request;\r\n  wrapped_request.mutable_request()->mutable_terminate();\r\n  sendHotRestartMessage(parent_address_, wrapped_request);\r\n  parent_terminated_ = true;\r\n  // Once setting parent_terminated_ == true, we can send no more hot restart RPCs, and therefore\r\n  // receive no more responses, including stats. So, now safe to forget our stat transferral state.\r\n  //\r\n  // This destruction is actually important far beyond memory efficiency. The scope-based temporary\r\n  // counter logic relies on the StatMerger getting destroyed once hot restart's stat merging is\r\n  // all done. (See stat_merger.h for details).\r\n  stat_merger_.reset();\r\n}\r\n\r\nvoid HotRestartingBase::sendHotRestartMessage(sockaddr_un& address,\r\n                                              const HotRestartMessage& proto) {\r\n......\r\n    const int rc = sendmsg(my_domain_socket_, &message, 0);\r\n    RELEASE_ASSERT(rc == static_cast<int>(cur_chunk_size),\r\n                   fmt::format(\"hot restart sendmsg() failed: returned {}, errno {}\", rc, errno));\r\n  }\r\n  RELEASE_ASSERT(fcntl(my_domain_socket_, F_SETFL, O_NONBLOCK) != -1,\r\n                 fmt::format(\"Set domain socket nonblocking failed, errno = {}\", errno));\r\n}\r\n```\r\n\r\nI think need to refactor ` sendHotRestartMessage `, send the results of the return, can be treated with the result of sending calls, rather than a unified RELEASE_ASSERT to deal with.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7468/comments",
    "author": "zyfjeff",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-07-05T16:55:30Z",
        "body": "This should never happen, the parent process should never terminate early. The hot restart code is written to assume that any deviation from the normal process puts things into an inconsistent state and everything should come up from scratch. Why is the parent process terminating?"
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-07-06T08:25:20Z",
        "body": "@mattklein123 parent process may be prematurely quit due to bugs, or be killed by mistake, but in any case, the child process has taken over traffic, can work normally, and should not quit."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-07-06T20:13:52Z",
        "body": "IMO it's not worth it to add the extra complexity of potentially inconsistent state between parent and child. I would recommend fixing whatever bug is causing the parent to shutdown early. We have run hot restart like this at Lyft for many years without any issues with inconsistent state."
      }
    ]
  },
  {
    "number": 7441,
    "title": "Trying to add linkopts, Bazel getting in the way...",
    "created_at": "2019-07-02T00:38:22Z",
    "closed_at": "2019-07-02T03:04:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7441",
    "body": "Looks like similar opinions were raised before about Bazel but I am also in the opinion that it is overall a hard ecosystem to navigate.\r\n\r\nSo much so that I am unable to have my project (derived from the `filter-example`) link with an additional system library.\r\n\r\nFor simplicity, let's say it is `curl`. I simply want to link with `libcurl.so`. I don't need to build curl from source (although I tried that and failed to do that with Bazel and there aren't many examples except for `tensorflow` and `googlecartographer/async_grpc` projects), I have it installed on my system.\r\n\r\nI am trying to simply add a `-lcurl` to the `linkopts` but since `envoy_cc_binary` wraps the `cc_binary`, I am unable to specify `linkopts`.\r\n\r\nAny help is appreciated. I feel like this should be rather simple but Bazel is getting in the way. :) ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7441/comments",
    "author": "canselcik",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-07-02T02:44:52Z",
        "body": "Presumably since you are trying to link against a system library, you have private code. You can define your own binary target like so and pass whatever linkops you want. E.g.,\r\n\r\n```\r\nenvoy_cc_binary(\r\n    name = \"envoy\",\r\n    repository = \"@envoy\",\r\n    stamped = True,\r\n    deps = LYFT_CUSTOM_FILTER_CONFIGS + [\r\n        \"@envoy//source/exe:envoy_main_entry_lib\",\r\n    ],\r\n)\r\n```"
      },
      {
        "user": "canselcik",
        "created_at": "2019-07-02T03:04:57Z",
        "body": "Thanks @mattklein123, that was actually the first thing I had tried but that had resulted in an obscure Bazel error so I had figured the `envoy_cc_binary` wrapper didn't accept a list of `linkopts`. Turns out it was something else. Perhaps my `bazel clean --expunge` from earlier addressed something.\r\n\r\nThanks for the quick response."
      }
    ]
  },
  {
    "number": 7432,
    "title": "Disable LuaFilter buffering",
    "created_at": "2019-07-01T15:55:21Z",
    "closed_at": "2019-07-07T09:58:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7432",
    "body": "I am using Envoy in the following scenario :\r\n\r\nSingle listener with a single lua filter.\r\nThe lua filter makes a HTTP call to a third party API, extracts some data from the response and modify upstream headers accordingly. This is what it looks like :\r\n\r\n```\r\nfunction envoy_on_request(request_handle)\r\n  local headers, body = request_handle:httpCall(\r\n  \"storage\",\r\n  {\r\n    [\":method\"] = \"POST\",\r\n    [\":path\"] = \"/api/v1/init\",\r\n    [\":authority\"] = \"storage\",\r\n    [\"Content-Type\"] = \"application/json\",\r\n    [\"wolf-content-length\"] = request_handle:headers():get(\"content-length\"),\r\n    [\"Authorization\"] = request_handle:headers():get(\"Authorization\")\r\n  },\r\n  json.encode(\r\n  {\r\n    [\"wolfContentLength\"] = request_handle:headers():get(\"content-length\")\r\n  }),\r\n  5000)\r\n\r\n  local computedUploadPath = json.decode(body)[\"proxyData\"][\"uploadPath\"]\r\n  request_handle:headers():add(\"uploadPath\", computedUploadPath)\r\nend\r\n\r\n```\r\n\r\nWhat I'm observing from the documentation and the code is that the lua filter is buffering the request body while achieving the httpCall. However, I was expecting the filter to come back to streaming mode as soon as the httpCall is finished, but this doesn't seem to be the case.\r\nIn the Envoy logs, I can see that it keeps buffering during the whole request, like described in the   flow_control documentation :\r\n\r\n```\r\n[2019-07-01 15:18:28.743][26][debug][connection] [source/common/network/connection_impl.cc:422] [C5] onBelowWriteBufferLowWatermark\r\n[2019-07-01 15:18:28.743][26][debug][http] [source/common/http/conn_manager_impl.cc:1817] [C3][S14148771046294272169] Read-enabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:28.744][26][debug][connection] [source/common/network/connection_impl.cc:431] [C5] onAboveWriteBufferHighWatermark\r\n[2019-07-01 15:18:28.744][26][debug][http] [source/common/http/conn_manager_impl.cc:1793] [C3][S14148771046294272169] Read-disabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:29.497][26][debug][connection] [source/common/network/connection_impl.cc:422] [C5] onBelowWriteBufferLowWatermark\r\n[2019-07-01 15:18:29.497][26][debug][http] [source/common/http/conn_manager_impl.cc:1817] [C3][S14148771046294272169] Read-enabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:29.498][26][debug][connection] [source/common/network/connection_impl.cc:431] [C5] onAboveWriteBufferHighWatermark\r\n[2019-07-01 15:18:29.498][26][debug][http] [source/common/http/conn_manager_impl.cc:1793] [C3][S14148771046294272169] Read-disabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:30.239][26][debug][connection] [source/common/network/connection_impl.cc:422] [C5] onBelowWriteBufferLowWatermark\r\n[2019-07-01 15:18:30.239][26][debug][http] [source/common/http/conn_manager_impl.cc:1817] [C3][S14148771046294272169] Read-enabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:30.241][26][debug][connection] [source/common/network/connection_impl.cc:431] [C5] onAboveWriteBufferHighWatermark\r\n[2019-07-01 15:18:30.241][26][debug][http] [source/common/http/conn_manager_impl.cc:1793] [C3][S14148771046294272169] Read-disabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:31.081][26][debug][connection] [source/common/network/connection_impl.cc:422] [C5] onBelowWriteBufferLowWatermark\r\n[2019-07-01 15:18:31.081][26][debug][http] [source/common/http/conn_manager_impl.cc:1817] [C3][S14148771046294272169] Read-enabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:31.083][26][debug][connection] [source/common/network/connection_impl.cc:431] [C5] onAboveWriteBufferHighWatermark\r\n[2019-07-01 15:18:31.083][26][debug][http] [source/common/http/conn_manager_impl.cc:1793] [C3][S14148771046294272169] Read-disabling downstream stream due to filter callbacks.\r\n```\r\n\r\nThis configuration will be used to upload large files, and this buffering behavior makes uploads more than twice longer than they should be.\r\n\r\nIs this a bug or did I miss something in the documentation ?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7432/comments",
    "author": "s-vivien",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-07-01T22:50:49Z",
        "body": "I haven't looked at this code in a while, but AFAICT streaming should resume once the HTTP call succeeds. Are you sure it's not overflowing while the call is happening?\r\n\r\nTBH I'm not sure why we are buffering in this path and not watermarking.\r\n\r\nIf you can confirm that the overflow is happening during the call, we can look into changing to watermarking during HTTP call processing."
      },
      {
        "user": "s-vivien",
        "created_at": "2019-07-02T07:52:26Z",
        "body": "The watermarking seems to start *after* the http call. Here is a snippet of the whole request logs (I didn't mention it in the first place, but I also have a envoy_on_response function in my lua script) :\r\n\r\n```\r\n[2019-07-01 15:17:44.791][26][debug][main] [source/server/connection_handler_impl.cc:257] [C3] new connection\r\n[2019-07-01 15:17:44.791][26][debug][http] [source/common/http/conn_manager_impl.cc:243] [C3] new stream\r\n[2019-07-01 15:17:44.791][26][debug][http] [source/common/http/conn_manager_impl.cc:580] [C3][S14148771046294272169] request headers complete (end_stream=false):\r\n':authority', 'endpoint.namespace.host.com'\r\n':path', '/api/v1/upload'\r\n':method', 'PUT'\r\n'connection', 'close'\r\n'x-real-ip', '100.125.1.88'\r\n'x-forwarded-for', '100.125.1.88'\r\n'x-forwarded-host', 'endpoint.namespace.host.com'\r\n'x-forwarded-port', '443'\r\n'x-forwarded-proto', 'https'\r\n'x-original-uri', '/api/v1/upload'\r\n'x-scheme', 'https'\r\n'content-length', '158775206'\r\n'recipient', 'user1,user2'\r\n'expiration', '12'\r\n'filename', 'my_report.xls'\r\n'cache-control', 'no-cache'\r\n'postman-token', '26da3bcc-9bae-4c9e-ba2c-7de4b5afe435'\r\n'authorization', 'Bearer xxxx.yyyyy.zzzzz'\r\n'content-type', 'text/plain'\r\n'user-agent', 'PostmanRuntime/7.6.0'\r\n'accept', '*/*'\r\n'accept-encoding', 'gzip, deflate'\r\n\r\n[2019-07-01 15:17:44.791][26][debug][lua] [source/extensions/filters/http/lua/lua_filter.cc:532] script log: - START call app.storage - API init\r\n[2019-07-01 15:17:44.791][26][debug][lua] [source/extensions/filters/http/lua/lua_filter.cc:532] script log: /api/v1/upload\r\n[2019-07-01 15:17:44.792][26][debug][router] [source/common/router/router.cc:320] [C0][S15223874834991471007] cluster 'storage' match for URL '/api/v1/storage/init'\r\n[2019-07-01 15:17:44.792][26][debug][router] [source/common/router/router.cc:381] [C0][S15223874834991471007] router decoding headers:\r\n':path', '/api/v1/storage/init'\r\n':method', 'POST'\r\n':authority', 'storage'\r\n':scheme', 'https'\r\n'content-type', 'application/json'\r\n'app-content-length', '158775206'\r\n'authorization', 'Bearer xxxx.yyyyy.zzzzz'\r\n'content-length', '67'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '172.12.9.34'\r\n'x-envoy-expected-rq-timeout-ms', '5000'\r\n\r\n[2019-07-01 15:17:44.792][26][debug][pool] [source/common/http/http1/conn_pool.cc:88] creating a new connection\r\n[2019-07-01 15:17:44.792][26][debug][client] [source/common/http/codec_client.cc:26] [C4] connecting\r\n[2019-07-01 15:17:44.792][26][debug][connection] [source/common/network/connection_impl.cc:644] [C4] connecting to 10.254.183.170:8080\r\n[2019-07-01 15:17:44.792][26][debug][connection] [source/common/network/connection_impl.cc:653] [C4] connection in progress\r\n[2019-07-01 15:17:44.792][26][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2019-07-01 15:17:44.792][26][debug][lua] [source/extensions/filters/common/lua/lua.cc:40] coroutine yielded\r\n[2019-07-01 15:17:44.792][26][debug][connection] [source/common/network/connection_impl.cc:517] [C4] connected\r\n[2019-07-01 15:17:44.792][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C4] handshake error: 2\r\n[2019-07-01 15:17:44.795][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C4] handshake error: 2\r\n[2019-07-01 15:17:44.795][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C4] handshake error: 2\r\n[2019-07-01 15:17:44.795][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C4] handshake error: 2\r\n[2019-07-01 15:17:44.795][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C4] handshake error: 2\r\n[2019-07-01 15:17:44.795][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:131] [C4] handshake complete\r\n[2019-07-01 15:17:44.795][26][debug][client] [source/common/http/codec_client.cc:64] [C4] connected\r\n[2019-07-01 15:17:44.795][26][debug][pool] [source/common/http/http1/conn_pool.cc:245] [C4] attaching to next request\r\n[2019-07-01 15:17:44.795][26][debug][router] [source/common/router/router.cc:1165] [C0][S15223874834991471007] pool ready\r\n[2019-07-01 15:17:44.801][26][debug][router] [source/common/router/router.cc:717] [C0][S15223874834991471007] upstream headers complete: end_stream=false\r\n[2019-07-01 15:17:44.801][26][debug][http] [source/common/http/async_client_impl.cc:94] async http request response headers (end_stream=false):\r\n':status', '200'\r\n'x-content-type-options', 'nosniff'\r\n'x-frame-options', 'DENY'\r\n'x-xss-protection', '1; mode=block'\r\n'x-content-type-options', 'nosniff'\r\n'cache-control', 'no-cache, no-store, max-age=0, must-revalidate'\r\n'pragma', 'no-cache'\r\n'expires', '0'\r\n'strict-transport-security', 'max-age=31536000 ; includeSubDomains'\r\n'content-type', 'application/json;charset=UTF-8'\r\n'transfer-encoding', 'chunked'\r\n'date', 'Mon, 01 Jul 2019 15:17:44 GMT'\r\n'x-envoy-upstream-service-time', '9'\r\n\r\n[2019-07-01 15:17:44.801][26][debug][client] [source/common/http/codec_client.cc:95] [C4] response complete\r\n[2019-07-01 15:17:44.801][26][debug][lua] [source/extensions/filters/http/lua/lua_filter.cc:207] async HTTP response complete\r\n[2019-07-01 15:17:44.802][26][debug][lua] [source/extensions/filters/http/lua/lua_filter.cc:532] script log: - END call app.storage - API init\r\n[2019-07-01 15:17:44.803][26][debug][lua] [source/extensions/filters/common/lua/lua.cc:37] coroutine finished\r\n[2019-07-01 15:17:44.803][26][debug][router] [source/common/router/router.cc:320] [C3][S14148771046294272169] cluster 'obs' match for URL '/obs-app/c85cfc0d-5ae6-41f7-a766-56c0e6fd87cc'\r\n[2019-07-01 15:17:44.803][26][debug][router] [source/common/router/router.cc:381] [C3][S14148771046294272169] router decoding headers:\r\n':method', 'PUT'\r\n':path', '/obs-app/c85cfc0d-5ae6-41f7-a766-56c0e6fd87cc'\r\n':authority', 'server.topkek.com'\r\n':scheme', 'https'\r\n'x-real-ip', '100.125.1.88'\r\n'x-forwarded-for', '100.125.1.88'\r\n'x-forwarded-host', 'endpoint.namespace.host.com'\r\n'x-forwarded-port', '443'\r\n'x-forwarded-proto', 'https'\r\n'x-original-uri', '/api/v1/upload'\r\n'x-scheme', 'https'\r\n'x-request-id', 'c3baa368-7f0b-4520-9029-82cdbc38dd1c'\r\n'authorization', 'AWS4-HMAC-SHA256 Credential=SDQSDQSDQSDQSDQD/20190701/topkek, SignedHeaders=content-length;host;x-amz-content-sha256;x-amz-date, Signature=S5dSd1z5d0s5a4d0s0a84d1fg1z0fd54'\r\n'content-length', '158775206'\r\n'x-amz-content-sha256', '5s0f5z0f85df106z4e0g0dsdgz5e1g2d0g2'\r\n'x-amz-date', '20190701T151744Z'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2019-07-01 15:17:44.803][26][debug][pool] [source/common/http/http1/conn_pool.cc:88] creating a new connection\r\n[2019-07-01 15:17:44.803][26][debug][client] [source/common/http/codec_client.cc:26] [C5] connecting\r\n[2019-07-01 15:17:44.803][26][debug][connection] [source/common/network/connection_impl.cc:644] [C5] connecting to 90.84.40.65:443\r\n[2019-07-01 15:17:44.803][26][debug][connection] [source/common/network/connection_impl.cc:653] [C5] connection in progress\r\n[2019-07-01 15:17:44.803][26][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2019-07-01 15:17:44.803][26][debug][pool] [source/common/http/http1/conn_pool.cc:202] [C4] response complete\r\n[2019-07-01 15:17:44.803][26][debug][pool] [source/common/http/http1/conn_pool.cc:240] [C4] moving to ready\r\n[2019-07-01 15:17:44.826][26][debug][connection] [source/common/network/connection_impl.cc:517] [C5] connected\r\n[2019-07-01 15:17:44.826][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C5] handshake error: 2\r\n[2019-07-01 15:17:44.858][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C5] handshake error: 2\r\n[2019-07-01 15:17:44.858][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C5] handshake error: 2\r\n[2019-07-01 15:17:44.858][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C5] handshake error: 2\r\n[2019-07-01 15:17:44.859][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C5] handshake error: 2\r\n[2019-07-01 15:17:44.859][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C5] handshake error: 2\r\n[2019-07-01 15:17:44.884][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:142] [C5] handshake error: 2\r\n[2019-07-01 15:17:44.884][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:131] [C5] handshake complete\r\n[2019-07-01 15:17:44.885][26][debug][client] [source/common/http/codec_client.cc:64] [C5] connected\r\n[2019-07-01 15:17:44.885][26][debug][pool] [source/common/http/http1/conn_pool.cc:245] [C5] attaching to next request\r\n[2019-07-01 15:17:44.885][26][debug][router] [source/common/router/router.cc:1165] [C3][S14148771046294272169] pool ready\r\n[2019-07-01 15:17:47.190][26][debug][connection] [source/common/network/connection_impl.cc:431] [C5] onAboveWriteBufferHighWatermark\r\n[2019-07-01 15:17:47.190][26][debug][http] [source/common/http/conn_manager_impl.cc:1793] [C3][S14148771046294272169] Read-disabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:17:47.422][26][debug][connection] [source/common/network/connection_impl.cc:422] [C5] onBelowWriteBufferLowWatermark\r\n[2019-07-01 15:17:47.422][26][debug][http] [source/common/http/conn_manager_impl.cc:1817] [C3][S14148771046294272169] Read-enabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:17:47.423][26][debug][connection] [source/common/network/connection_impl.cc:431] [C5] onAboveWriteBufferHighWatermark\r\n[2019-07-01 15:17:47.423][26][debug][http] [source/common/http/conn_manager_impl.cc:1793] [C3][S14148771046294272169] Read-disabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:17:47.708][26][debug][connection] [source/common/network/connection_impl.cc:422] [C5] onBelowWriteBufferLowWatermark\r\n[2019-07-01 15:17:47.708][26][debug][http] [source/common/http/conn_manager_impl.cc:1817] [C3][S14148771046294272169] Read-enabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:17:47.708][26][debug][connection] [source/common/network/connection_impl.cc:431] [C5] onAboveWriteBufferHighWatermark\r\n[2019-07-01 15:17:47.708][26][debug][http] [source/common/http/conn_manager_impl.cc:1793] [C3][S14148771046294272169] Read-disabling downstream stream due to filter callbacks.\r\n[...]\r\n[2019-07-01 15:18:42.597][26][debug][http] [source/common/http/conn_manager_impl.cc:1817] [C3][S14148771046294272169] Read-enabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:42.598][26][debug][connection] [source/common/network/connection_impl.cc:431] [C5] onAboveWriteBufferHighWatermark\r\n[2019-07-01 15:18:42.598][26][debug][http] [source/common/http/conn_manager_impl.cc:1793] [C3][S14148771046294272169] Read-disabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:43.373][26][debug][connection] [source/common/network/connection_impl.cc:422] [C5] onBelowWriteBufferLowWatermark\r\n[2019-07-01 15:18:43.373][26][debug][http] [source/common/http/conn_manager_impl.cc:1817] [C3][S14148771046294272169] Read-enabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:43.374][26][debug][connection] [source/common/network/connection_impl.cc:431] [C5] onAboveWriteBufferHighWatermark\r\n[2019-07-01 15:18:43.374][26][debug][http] [source/common/http/conn_manager_impl.cc:1793] [C3][S14148771046294272169] Read-disabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:43.374][26][debug][http] [source/common/http/conn_manager_impl.cc:1037] [C3][S14148771046294272169] request end stream\r\n[2019-07-01 15:18:43.651][26][debug][connection] [source/common/network/connection_impl.cc:422] [C5] onBelowWriteBufferLowWatermark\r\n[2019-07-01 15:18:43.651][26][debug][http] [source/common/http/conn_manager_impl.cc:1817] [C3][S14148771046294272169] Read-enabling downstream stream due to filter callbacks.\r\n[2019-07-01 15:18:44.135][26][debug][client] [source/common/http/codec_client.cc:95] [C5] response complete\r\n[2019-07-01 15:18:44.135][26][debug][router] [source/common/router/router.cc:717] [C3][S14148771046294272169] upstream headers complete: end_stream=true\r\n[2019-07-01 15:18:44.135][26][debug][lua] [source/extensions/filters/http/lua/lua_filter.cc:532] script log: - START call app.storage - API ack\r\n[2019-07-01 15:18:44.135][26][debug][router] [source/common/router/router.cc:320] [C0][S12838640315204737266] cluster 'storage' match for URL '/api/v1/storage/ack'\r\n[2019-07-01 15:18:44.136][26][debug][router] [source/common/router/router.cc:381] [C0][S12838640315204737266] router decoding headers:\r\n':path', '/api/v1/storage/ack'\r\n':method', 'POST'\r\n':authority', 'storage'\r\n':scheme', 'https'\r\n'content-type', 'application/json'\r\n'authorization', 'Bearer xxxx.yyyyy.zzzzz'\r\n'content-length', '105'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '172.12.9.34'\r\n'x-envoy-expected-rq-timeout-ms', '5000'\r\n\r\n[2019-07-01 15:18:44.136][26][debug][pool] [source/common/http/http1/conn_pool.cc:97] [C4] using existing connection\r\n[2019-07-01 15:18:44.136][26][debug][router] [source/common/router/router.cc:1165] [C0][S12838640315204737266] pool ready\r\n[2019-07-01 15:18:44.136][26][debug][lua] [source/extensions/filters/common/lua/lua.cc:40] coroutine yielded\r\n[2019-07-01 15:18:44.136][26][debug][pool] [source/common/http/http1/conn_pool.cc:202] [C5] response complete\r\n[2019-07-01 15:18:44.136][26][debug][pool] [source/common/http/http1/conn_pool.cc:240] [C5] moving to ready\r\n[2019-07-01 15:18:44.159][26][debug][router] [source/common/router/router.cc:717] [C0][S12838640315204737266] upstream headers complete: end_stream=false\r\n[2019-07-01 15:18:44.159][26][debug][http] [source/common/http/async_client_impl.cc:94] async http request response headers (end_stream=false):\r\n':status', '200'\r\n'x-content-type-options', 'nosniff'\r\n'x-frame-options', 'DENY'\r\n'x-xss-protection', '1; mode=block'\r\n'x-content-type-options', 'nosniff'\r\n'cache-control', 'no-cache, no-store, max-age=0, must-revalidate'\r\n'pragma', 'no-cache'\r\n'expires', '0'\r\n'strict-transport-security', 'max-age=31536000 ; includeSubDomains'\r\n'content-type', 'application/json;charset=UTF-8'\r\n'transfer-encoding', 'chunked'\r\n'date', 'Mon, 01 Jul 2019 15:18:44 GMT'\r\n'x-envoy-upstream-service-time', '23'\r\n\r\n[2019-07-01 15:18:44.160][26][debug][client] [source/common/http/codec_client.cc:95] [C4] response complete\r\n[2019-07-01 15:18:44.160][26][debug][lua] [source/extensions/filters/http/lua/lua_filter.cc:207] async HTTP response complete\r\n[2019-07-01 15:18:44.160][26][debug][lua] [source/extensions/filters/http/lua/lua_filter.cc:532] script log: {\"operationId\":\"c85cfc0d-5ae6-41f7-a766-56c0e6fd87cc\",\"status\":\"success\",\"timestamp\":\"2019-07-01T15:18:44.151+0000\"}\r\n[2019-07-01 15:18:44.160][26][debug][lua] [source/extensions/filters/http/lua/lua_filter.cc:532] script log: - END call app.storage - API ack\r\n[2019-07-01 15:18:44.160][26][debug][lua] [source/extensions/filters/common/lua/lua.cc:37] coroutine finished\r\n[2019-07-01 15:18:44.160][26][debug][http] [source/common/http/conn_manager_impl.cc:1216] [C3][S14148771046294272169] closing connection due to connection close header\r\n[2019-07-01 15:18:44.160][26][debug][http] [source/common/http/conn_manager_impl.cc:1278] [C3][S14148771046294272169] encoding headers via codec (end_stream=true):\r\n':status', '200'\r\n'x-app-message', '{\"operationId\":\"c85cfc0d-5ae6-41f7-a766-56c0e6fd87cc\",\"status\":\"success\",\"timestamp\":\"2019-07-01T15:18:44.151+0000\"}'\r\n'date', 'Mon, 01 Jul 2019 15:18:44 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2019-07-01 15:18:44.160][26][debug][connection] [source/common/network/connection_impl.cc:101] [C3] closing data_to_write=243 type=2\r\n[2019-07-01 15:18:44.160][26][debug][connection] [source/common/network/connection_impl.cc:153] [C3] setting delayed close timer with timeout 1000 ms\r\n[2019-07-01 15:18:44.160][26][debug][pool] [source/common/http/http1/conn_pool.cc:202] [C4] response complete\r\n[2019-07-01 15:18:44.160][26][debug][pool] [source/common/http/http1/conn_pool.cc:240] [C4] moving to ready\r\n[2019-07-01 15:18:44.160][26][debug][connection] [source/common/network/connection_impl.cc:461] [C3] remote early close\r\n[2019-07-01 15:18:44.160][26][debug][connection] [source/common/network/connection_impl.cc:183] [C3] closing socket: 0\r\n[2019-07-01 15:18:44.160][26][debug][main] [source/server/connection_handler_impl.cc:68] [C3] adding to cleanup list\r\n[2019-07-01 15:19:44.220][26][debug][connection] [source/common/network/connection_impl.cc:502] [C4] remote close\r\n[2019-07-01 15:19:44.220][26][debug][connection] [source/common/network/connection_impl.cc:183] [C4] closing socket: 0\r\n[2019-07-01 15:19:44.220][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:244] [C4] SSL shutdown: rc=1\r\n[2019-07-01 15:19:44.220][26][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:175] [C4] \r\n[2019-07-01 15:19:44.220][26][debug][client] [source/common/http/codec_client.cc:82] [C4] disconnect. resetting 0 pending requests\r\n[2019-07-01 15:19:44.220][26][debug][pool] [source/common/http/http1/conn_pool.cc:129] [C4] client disconnected, failure reason: \r\n```\r\n\r\nAdditional information : \r\n- I have this parameter in the listener configuration :\r\n`per_connection_buffer_limit_bytes: 2049264`\r\nI tried changing it to `131152896 `but it behaves the same.\r\n- envoy is deployed on kubernetes and accessed through a nginx ingress.\r\n\r\nHow is watermarking any different from buffering ? The result (time-wise) is the same, if the data-source is cut half of the time."
      },
      {
        "user": "s-vivien",
        "created_at": "2019-07-07T09:58:00Z",
        "body": "I made more in-depth tests, and it appears that my bottleneck is not related to Envoy, but rather to Kubernetes.\r\n\r\nI isolated Envoy from Kubernetes and compared upload times both in direct upload, and via Envoy. The statistics were basically the same.\r\n\r\nSorry for the inconvenience !"
      }
    ]
  },
  {
    "number": 7421,
    "title": "Authz and route config when using RDS",
    "created_at": "2019-06-28T05:38:57Z",
    "closed_at": "2019-09-04T03:04:25Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7421",
    "body": "Not sure if this is a question or a feature request or just a whinge really. Basically I'm looking at using RDS at the moment and realised I have an issue with extauthz.\r\n\r\nIn my current static setup I have something like this:-\r\n\r\n```yaml\r\nstatic_resources:\r\n  listeners:\r\n    - name: \"listener\"\r\n      address:\r\n        socket_address: { address: 0.0.0.0, port_value: 8080 }\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              config:\r\n                stat_prefix: httpmgr\r\n                http_filters:\r\n                  - name: envoy.ext_authz\r\n                    config:\r\n                      grpc_service:\r\n                        envoy_grpc:\r\n                          cluster_name: ext-authz\r\n                  - name: envoy.router\r\n                route_config:\r\n                  name: web_routes\r\n                  virtual_hosts:\r\n                    - name: all_domains\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match: { prefix: /service1 }\r\n                          route: { cluster: service1 }\r\n                        - match: { prefix: /service2 }\r\n                          route: { cluster: service2 }\r\n                          # No extauthz for this one\r\n                          per_filter_config:\r\n                            envoy.ext_authz:\r\n                              disabled: true\r\n```\r\nIf I want to use extauthz and differentiate behaviour based on routes my RDS now needs to know that an extauthz filter is in play.\r\n\r\nWhat I really want to do is replace the above config with something like:-\r\n\r\n```yaml\r\nstatic_resources:\r\n  listeners:\r\n    - name: \"listener\"\r\n      address:\r\n        socket_address: { address: 0.0.0.0, port_value: 8080 }\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              config:\r\n                stat_prefix: httpmgr\r\n                http_filters:\r\n                  - name: envoy.ext_authz\r\n                    config:\r\n                      grpc_service:\r\n                        envoy_grpc:\r\n                          cluster_name: ext-authz\r\n                  - name: envoy.router\r\n                rds:\r\n                  route_config_name: web_port\r\n                  config_source:\r\n                    path: /path/to/xds/rds/rds.yaml\r\n```\r\nand have an rds.yaml file as follows:-\r\n```yaml\r\nversion_info: \"0\"\r\nresources:\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n    name: wf_admin_port\r\n    virtual_hosts:\r\n      - name: wf_admin_port_domains\r\n        domains: [\"*\"]\r\n        routes:\r\n          - match: { prefix: \"/service1\" }\r\n            route: { cluster: service1 }\r\n          - match: { prefix: \"/service2\" }\r\n            route: { cluster: service2 }\r\n```\r\nTo me this is much better but now both of my routes will have the extauthz filter applied to them because the per filter config is no longer there. Although this is file based RDS the same scenario will exist with a grpc based service.\r\n\r\nSo, in order to ensure the `/service2` route doesn't have the extauthz filter applied I need to have the following:-\r\n```yaml\r\nversion_info: \"0\"\r\nresources:\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n    name: wf_admin_port\r\n    virtual_hosts:\r\n      - name: wf_admin_port_domains\r\n        domains: [\"*\"]\r\n        routes:\r\n          - match: { prefix: \"/service1\" }\r\n            route: { cluster: service1 }\r\n          - match: { prefix: \"/service2\" }\r\n            route: { cluster: service2 }\r\n            per_filter_config:\r\n              envoy.ext_authz:\r\n              disabled: true\r\n```\r\nBut this now means I have configuration inside the rds file that, I believe, really doesn't belong in there and is remote from the remaining extauthz configuration.\r\n\r\nIs there something I'm missing here or this an oversight in the design of the extauthz plugin that requires the configuration to be broken up in this way?\r\n\r\nI'm looking at the config for this vs the config for the JWT filter where the rules section allows per route config but keeps it with the other JWT configuration.\r\n\r\nEDITED: Provide a bit more clarity on what was being described / questioned.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7421/comments",
    "author": "andye2004",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-06-28T05:40:56Z",
        "body": "@rshriram can probably speak to the motivation around `per_filter_config` best."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-28T08:22:36Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "andye2004",
        "created_at": "2019-07-29T00:57:16Z",
        "body": "Not really stale, would be nice to get some feedback, @htuch @rshriram ?"
      },
      {
        "user": "htuch",
        "created_at": "2019-07-29T01:48:23Z",
        "body": "I think this speaks to the relatively inflexible nature of how Envoy's filter chains work. They are created per-virtual host, so it's not really possible to have distinct filter chains per virtual host without playign tricks like the per-filter config in the `RouteConfiguration`. Design proposals welcome if this is a serious impediment at your end."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-28T02:40:33Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-09-04T03:04:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7410,
    "title": "DNS request timed out 4 times.",
    "created_at": "2019-06-27T04:03:32Z",
    "closed_at": "2019-08-08T03:42:14Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7410",
    "body": "I use k8s ambassador NodePort to expose serive.  I open firewalld and then close. the ambassador pod log show DNS request timed out 4 times. \r\n\r\nI don't know how the firewalld influence the udp socket(DNS request). when open firewalld, the udp socket can close, but when close firewalld, the socket never close.\r\n\r\nI kubectl exec ambassador pod: \r\n/var/log # netstat -nal| grep udp | grep -v 127\r\nudp        0      0 177.177.162.240:60021   10.96.0.10:53           ESTABLISHED\r\n\r\nI want to ask: DNS request timed out 4 times, why not closeSocket?\r\n\r\n\r\n[root@node1 ~]# kubectl get svc --all-namespaces\r\nNAMESPACE      NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                 AGE\r\nbase-service   ambassador               NodePort       10.96.140.102   <none>        443:10443/TCP,80:10080/TCP                              1d\r\nbase-service   cas-server               LoadBalancer   10.96.3.228     <pending>     3000:51487/TCP \r\nkube-system    kube-dns                 ClusterIP      10.96.0.10      <none>        53/UDP,53/TCP \r\n\r\n\r\n[root@node1 opt]#systemctl stop firewalld\r\n\r\n[root@node1 opt]# kubectl exec  ambassador-ngvc9 -n base-service -ti -- /bin/sh\r\n\r\n/ambassador # netstat -nalp |grep udp | grep -v 127\r\nudp        0      0 177.177.166.134:41963   10.96.0.10:53           ESTABLISHED 438/envoy\r\n/ambassador # netstat -nalp |grep udp | grep -v 127\r\nudp        0      0 177.177.166.134:41963   10.96.0.10:53           ESTABLISHED 438/envoy\r\n/ambassador # netstat -nalp |grep udp | grep -v 127\r\nudp        0      0 177.177.166.134:41963   10.96.0.10:53           ESTABLISHED 438/envoy\r\n/ambassador # netstat -nalp |grep udp | grep -v 127\r\nudp        0      0 177.177.166.134:41963   10.96.0.10:53           ESTABLISHED 438/envoy\r\n/ambassador # netstat -nalp |grep udp | grep -v 127\r\nudp        0      0 177.177.166.134:41963   10.96.0.10:53           ESTABLISHED 438/envoy\r\n/ambassador # netstat -nalp |grep udp | grep -v 127\r\nudp        0      0 177.177.166.134:41963   10.96.0.10:53           ESTABLISHED 438/envoy\r\n/ambassador # netstat -nalp |grep udp | grep -v 127\r\nudp        0      0 177.177.166.134:41963   10.96.0.10:53           ESTABLISHED 438/envoy\r\n/ambassador # netstat -nalp |grep udp | grep -v 127\r\nudp        0      0 177.177.166.134:41963   10.96.0.10:53           ESTABLISHED 438/envoy\r\n/ambassador #\r\n/ambassador #\r\n/ambassador # netstat -nalp |grep udp | grep -v 127\r\nudp        0      0 177.177.166.134:41963   10.96.0.10:53           ESTABLISHED 438/envoy\r\n/ambassador #\r\n/ambassador #\r\n/ambassador # netstat -nalp |grep udp | grep -v 127\r\nudp        0      0 177.177.166.134:41963   10.96.0.10:53           ESTABLISHED 438/envoy\r\n\r\n\r\n\r\nthe pod log:\r\n[2019-06-27 05:42:38.313] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 upstream: source/common/network/dns_impl.cc:147] Setting DNS resolution timer for 0 milliseconds\r\n[2019-06-27 05:42:38.313] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 upstream: source/common/network/dns_impl.cc:147] Setting DNS resolution timer for 0 milliseconds\r\n[2019-06-27 05:42:38.313] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 upstream: source/common/network/dns_impl.cc:147] Setting DNS resolution timer for 0 milliseconds\r\n[2019-06-27 05:42:38.313] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 upstream: source/common/network/dns_impl.cc:147] Setting DNS resolution timer for 3793 milliseconds\r\n[2019-06-27 05:42:38.882] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 main: source/server/server.cc:119] flushing stats\r\n[2019-06-27 05:42:39.883] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 main: source/server/server.cc:119] flushing stats\r\n[2019-06-27 05:42:40.884] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 main: source/server/server.cc:119] flushing stats\r\n[2019-06-27 05:42:41.659] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 upstream: source/common/upstream/upstream_impl.cc:866] starting async DNS resolution for 127.0.0.1\r\n[2019-06-27 05:42:41.659] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 upstream: source/common/upstream/upstream_impl.cc:873] async DNS resolution complete for 127.0.0.1\r\n[2019-06-27 05:42:41.885] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 main: source/server/server.cc:119] flushing stats\r\n[2019-06-27 05:42:42.107] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 upstream: source/common/network/dns_impl.cc:118] DNS request timed out 4 times\r\n[2019-06-27 05:42:42.107] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 upstream: source/common/upstream/upstream_impl.cc:873] async DNS resolution complete for dnec-ui.dnec\r\n[2019-06-27 05:42:42.107] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 upstream: source/common/network/dns_impl.cc:147] Setting DNS resolution timer for 2008 milliseconds\r\n[2019-06-27 05:42:42.886] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 main: source/server/server.cc:119] flushing stats\r\n[2019-06-27 05:42:43.149] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 main: source/server/connection_handler_impl.cc:217] [C42459] new connection\r\n[2019-06-27 05:42:43.149] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 http: source/common/http/conn_manager_impl.cc:187] [C42459] new stream\r\n[2019-06-27 05:42:43.149] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 http: source/common/http/conn_manager_impl.cc:798] [C42459][S2738831110447437719] request end stream\r\n[2019-06-27 05:42:43.149] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 http: source/common/http/conn_manager_impl.cc:454] [C42459][S2738831110447437719] request headers complete (end_stream=true):\r\n':authority', '127.0.0.1:8001'\r\n':path', '/logging'\r\n':method', 'POST'\r\n'user-agent', 'python-requests/2.18.4'\r\n'accept-encoding', 'gzip, deflate'\r\n'accept', '*/*'\r\n'connection', 'keep-alive'\r\n'content-length', '0'\r\n\r\n[2019-06-27 05:42:43.149] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 admin: source/server/http/admin.cc:755] [C42459][S2738831110447437719] request complete: path: /logging\r\n[2019-06-27 05:42:43.149] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 http: source/common/http/conn_manager_impl.cc:984] [C42459][S2738831110447437719] encoding headers via codec (end_stream=false):\r\n':status', '404'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Thu, 27 Jun 2019 05:42:43 GMT'\r\n'server', 'envoy'\r\n\r\n[2019-06-27 05:42:43.150] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 connection: source/common/network/connection_impl.cc:451] [C42459] remote close\r\n[2019-06-27 05:42:43.150] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 connection: source/common/network/connection_impl.cc:133] [C42459] closing socket: 0\r\n[2019-06-27 05:42:43.150] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 main: source/server/connection_handler_impl.cc:50] [C42459] adding to cleanup list\r\n[2019-06-27 05:42:43.153] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 main: source/server/connection_handler_impl.cc:217] [C42460] new connection\r\n[2019-06-27 05:42:43.153] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 http: source/common/http/conn_manager_impl.cc:187] [C42460] new stream\r\n[2019-06-27 05:42:43.153] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 http: source/common/http/conn_manager_impl.cc:798] [C42460][S11881029595277275926] request end stream\r\n[2019-06-27 05:42:43.153] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 http: source/common/http/conn_manager_impl.cc:454] [C42460][S11881029595277275926] request headers complete (end_stream=true):\r\n':authority', '127.0.0.1:8001'\r\n':path', '/stats'\r\n':method', 'GET'\r\n'user-agent', 'python-requests/2.18.4'\r\n'accept-encoding', 'gzip, deflate'\r\n'accept', '*/*'\r\n'connection', 'keep-alive'\r\n\r\n[2019-06-27 05:42:43.153] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 admin: source/server/http/admin.cc:755] [C42460][S11881029595277275926] request complete: path: /stats\r\n[2019-06-27 05:42:43.158] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 http: source/common/http/conn_manager_impl.cc:984] [C42460][S11881029595277275926] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Thu, 27 Jun 2019 05:42:43 GMT'\r\n'server', 'envoy'\r\n\r\n[2019-06-27 05:42:43.159] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 connection: source/common/network/connection_impl.cc:451] [C42460] remote close\r\n[2019-06-27 05:42:43.159] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 connection: source/common/network/connection_impl.cc:133] [C42460] closing socket: 0\r\n[2019-06-27 05:42:43.159] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 main: source/server/connection_handler_impl.cc:50] [C42460] adding to cleanup list\r\n[2019-06-27 05:42:43.887] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 main: source/server/server.cc:119] flushing stats\r\n[2019-06-27 05:42:44.114] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 upstream: source/common/network/dns_impl.cc:147] Setting DNS resolution timer for 1 milliseconds\r\n[2019-06-27 05:42:44.117] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 upstream: source/common/network/dns_impl.cc:147] Setting DNS resolution timer for 4288 milliseconds\r\n[2019-06-27 05:42:44.887] ambassador-ngvc9 ambassador/debug/AMBSSADOR_LOG: 438 main: source/server/server.cc:119] flushing stats",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7410/comments",
    "author": "YanLemon",
    "comments": [
      {
        "user": "YanLemon",
        "created_at": "2019-06-27T09:20:13Z",
        "body": "@htuch  @mattklein123  thank you . please give me some advice."
      },
      {
        "user": "htuch",
        "created_at": "2019-06-27T13:59:19Z",
        "body": "CC @kflynn "
      },
      {
        "user": "YanLemon",
        "created_at": "2019-07-02T02:20:24Z",
        "body": "1、start or stop firewalld，the dns lookup will resolve fail for a while ,and then back to normal.\r\n2、start firewalld,the envoy will close the socket (for DNS request)\r\n3、There are some phenomena when stop the firewalld:\r\n1) stop the fiewalld, the same source port (same as the port before stop firewalld ) can receive DNS response\r\n2) stop the fiewalld,the same port has no response from dns\r\n3) stop the fiewalld,the envoy change the source port and has no response from dns\r\n4、when envoy has no dns response, the socket never close (the source port never changed)\r\n\r\n[root@node1 ~]# systemctl stop firewalld.service\r\n[root@node1 ~]# systemctl start firewalld.service"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-01T02:39:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-08T03:42:13Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7381,
    "title": "configurable default circuit breaking config",
    "created_at": "2019-06-24T15:11:40Z",
    "closed_at": "2019-06-24T15:13:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7381",
    "body": "*configurable default circuit breaking config*\r\n\r\n*Description*:\r\nContour allows circuit breaking fields such as max connections to be configured via annotations. If unset then the Envoy defaults are used. These defaults are much too low, and were one of the callouts at EnvoyCon with regard to sensible defaults in Envoy.\r\n\r\nProviding a way to increase these for all IngressRoutes would be ideal. As operators of Contour we'd like to provide some more sensible defaults rather than go out to dozens of service teams and ask them to provide the annotations.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7381/comments",
    "author": "phylake",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-06-24T15:13:13Z",
        "body": "I think this is a Contour issue. Please file an issue there to allow the defaults to be changed."
      },
      {
        "user": "phylake",
        "created_at": "2019-06-24T15:13:42Z",
        "body": "Whoops I was on the wrong tab. Sorry about that!"
      }
    ]
  },
  {
    "number": 7380,
    "title": "ext-auth and redirect",
    "created_at": "2019-06-24T14:08:23Z",
    "closed_at": "2019-08-15T17:10:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7380",
    "body": "Hey Folks,\r\n\r\nI’m trying to use the ext_authz filter. The behaviour I’m seeing – requests get routed to my ext-auth cluster, get auth’d but then return 403.\r\n\r\nIf I disable the ext-auth filter on my auth route I can ‘see’ my auth payload.\r\n\r\nI guess my question is – how do I control what header values mean a request is auth’d or denied?\r\n\r\nThx!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7380/comments",
    "author": "RocIngersol555",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-06-24T23:03:18Z",
        "body": "@RocIngersol555 can you share your ext_auhz filter? That would probably make things a little easier to grok."
      },
      {
        "user": "RocIngersol555",
        "created_at": "2019-06-30T18:20:11Z",
        "body": "Thx. Here’s some addiotnal context and configs.\r\n\r\n-\tI’m currently running Envoy as a sidecar and it’s fronting a web app.\r\n-\tI have an OIDC identify provider and I want to pass web connections off to it for authentication (and then back to Envoy for routing etc).\r\n-\tI have a web app (listening on 9090) that will talk to my identity provider, authenticate the user and do a call back with the user token and claims.\r\n\r\nIf I watch the Envoy logs I see;\r\n-\tConnections are passed to ‘ext-authz’ and then to my auth provider.\r\n-\tThe call back from my auth provider back to Envoy is then met with a 403..\r\n\r\nMy config is pretty much like the example one from the docs;\r\n\r\n`static_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8090\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: my_app\r\n\r\n          http_filters:\r\n          - name: envoy.ext_authz\r\n            config:\r\n              http_service:\r\n                server_uri:\r\n                  uri: 127.0.0.1:9090\r\n                  cluster: ext-authz\r\n                  timeout: 0.25s\r\n          - name: envoy.router\r\n            typed_config: {}\r\n  clusters:\r\n  - name: my_app\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: my_app\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 127.0.0.1\r\n                port_value: 10902\r\n  - name: ext-authz\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 9090\r\n\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n`\r\n"
      },
      {
        "user": "htuch",
        "created_at": "2019-07-09T15:52:27Z",
        "body": "Do you know what your auth provider is returning to Envoy?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-08T16:42:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-08-15T17:10:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "ides15",
        "created_at": "2022-03-18T15:58:42Z",
        "body": "@RocIngersol555 would you happen to remember what the issue here ended up being? I'm getting the same behavior with my extauth setup."
      },
      {
        "user": "sahrul6121",
        "created_at": "2022-07-08T08:27:22Z",
        "body": "@ides15 had same issue, did you find how to resolve it ?"
      }
    ]
  },
  {
    "number": 7309,
    "title": "Compilation stalls when building Envoy in a container",
    "created_at": "2019-06-18T00:59:00Z",
    "closed_at": "2019-06-19T14:37:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7309",
    "body": "*Title*: Build progress stalls in the *envoyproxy/envoy-build-ubuntu* container.\r\n\r\n*Description*:\r\n\r\nI checked out the latest master and followed the instructions in `ci` directory - `./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.dev` - to build envoy from source inside the ubuntu container. My host OS is a macOS 10.14.5 with docker desktop installed.\r\n\r\nAfter triggering the above command the container is successfully launched but the compilation stalled at:\r\n\r\n```\r\n[2,182 / 2,334] 151 actions, 2 running\r\n    Compiling source/extensions/filters/http/jwt_authn/extractor.cc; 22s local\r\n    Compiling source/common/router/header_formatter.cc; 17s local\r\n    [Analy] Compiling external/io_opencensus_cpp/opencensus/exporters/trace/stackdriver/internal/stackdriver_exporter.cc; 1828s\r\n    [Analy] Compiling source/common/http/context_impl.cc; 1778s\r\n    [Analy] Compiling source/extensions/tracers/common/ot/opentracing_driver_impl.cc; 1778s\r\n    [Analy] Compiling source/common/router/header_parser.cc; 1778s\r\n    [Analy] Compiling source/extensions/filters/http/buffer/buffer_filter.cc; 1778s\r\n    [Analy] Compiling source/extensions/filters/http/lua/wrappers.cc; 1778s ...\r\n``` \r\n\r\nIf I send Ctrl-C on the terminal, it seems Bazel complained as:\r\n\r\n```\r\n^C\r\nSession terminated, terminating shell...\r\nBazel caught interrupt signal; shutting down.\r\n\r\n\r\nCould not interrupt server (Deadline Exceeded)\r\n```\r\nAnd if I use `docker stop [envoy-build-container-id]`, it doesn't work as well.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7309/comments",
    "author": "InfoHunter",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-06-18T15:06:33Z",
        "body": "I suspect your docker VM doesn't have enough memory, causing compilation to grind to a halt. Maybe try increasing the resources given to the VM?"
      },
      {
        "user": "InfoHunter",
        "created_at": "2019-06-19T14:37:04Z",
        "body": "Yes, you are right. After I increased the memory from 2GB to 8GB, the problem was gone. Thanks for the help!"
      }
    ]
  },
  {
    "number": 7275,
    "title": "front-proxy example configuration works for sample python service, but not with simple service written using Java Spring Boot",
    "created_at": "2019-06-14T07:45:11Z",
    "closed_at": "2019-06-21T13:58:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7275",
    "body": "**Issue Template**\r\n\r\n*Title*: front-proxy example configuration works for sample python service, but not with simple service written using Java Spring Boot\r\n\r\n*Description*:\r\nI am trying Envoy as a front-proxy to a simple API built using Java Spring Boot. But it returns HTTP status 503: upstream connect error or disconnect/reset before headers. reset reason: connection termination\r\nSame configuration works when I replace my service with python service provided in examples.\r\n\r\n[optional *Relevant Links*:]\r\nI do not think it is a bug. I am probably missing some trivial configuration - to proxy a REST-JSON service. \r\nMy Java code is as simple as this-\r\n\r\n@RestController\r\n@RequestMapping(value = \"/customer\")\r\npublic class Application {\r\n\tpublic static void main(String[] args) {\r\n\t\tSpringApplication.run(Application.class, args);\r\n\t}\r\n\t\r\n\t@RequestMapping(value = \"/{id}\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE)\r\n\tpublic Customer getCustomer(@PathVariable(\"id\") Integer id) throws Exception {\r\n\r\n\t\tCustomer cust = new Customer(id, \"My Name\", \"My Address\", new ArrayList<Order>());\r\n                 return cust;\r\n        }\r\n}\r\n\r\n\r\n*Config*:\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 7080\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/service/ab\"\r\n                route:\r\n                  cluster: serviceab\r\n          http_filters:\r\n          - name: envoy.router\r\n            typed_config: {}\r\n  clusters:\r\n  - name: serviceab\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    http2_protocol_options: {}\r\n    load_assignment:\r\n      cluster_name: serviceab\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: serviceab\r\n                port_value: 8080\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\n\r\n\r\n*Logs*:\r\n[2019-06-14 07:44:21.227][18][debug][http] [source/common/http/conn_manager_impl.cc:1155] [C43][S8392129109895463059] Sending local reply with details upstream_reset_before_response_started{connection termination}\r\n[2019-06-14 07:44:21.227][18][debug][http] [source/common/http/conn_manager_impl.cc:1347] [C43][S8392129109895463059] encoding headers via codec (end_stream=false):\r\n':status', '503'\r\n'content-length', '95'\r\n'content-type', 'text/plain'\r\n'date', 'Fri, 14 Jun 2019 07:44:21 GMT'\r\n'server', 'envoy'\r\n [2019-06-14 07:44:21.227][18][debug][pool] [source/common/http/http2/conn_pool.cc:149] [C44] client disconnected\r\n[2019-06-14 07:44:21.227][18][debug][pool] [source/common/http/http2/conn_pool.cc:171] [C44] destroying primary client\r\n[2019-06-14 07:44:22.183][8][debug][main] [source/server/server.cc:168] flushing stats\r\n[2019-06-14 07:44:22.707][18][debug][connection] [source/common/network/connection_impl.cc:518] [C42] remote close\r\n[2019-06-14 07:44:22.707][18][debug][connection] [source/common/network/connection_impl.cc:188] [C42] closing socket: 0\r\n[2019-06-14 07:44:22.707][18][debug][main] [source/server/connection_handler_impl.cc:80] [C42] adding to cleanup list\r\n[2019-06-14 07:44:22.707][18][debug][connection] [source/common/network/connection_impl.cc:518] [C43] remote close\r\n[2019-06-14 07:44:22.707][18][debug][connection] [source/common/network/connection_impl.cc:188] [C43] closing socket: 0\r\n[2019-06-14 07:44:22.707][18][debug][main] [source/server/connection_handler_impl.cc:80] [C43] adding to cleanup list",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7275/comments",
    "author": "sibendu",
    "comments": [
      {
        "user": "sibendu",
        "created_at": "2019-06-19T06:29:52Z",
        "body": "Hi All,\r\nI am quite stuck on this; and seemingly it must be something simple in config I am missing. Any clue/pointer please?  Thanks a lot in advance.\r\nRegards.\r\nsibendu"
      },
      {
        "user": "rene-m-hernandez",
        "created_at": "2019-06-20T17:05:44Z",
        "body": "The `serviceab` Cluster definition has `http2_protocol_options: {}`. Did you configure your Java server to handle Http/2?"
      },
      {
        "user": "sibendu",
        "created_at": "2019-06-21T13:58:20Z",
        "body": "Hi, many thanks for looking into it. yes the service works with Http2. I had some issues with networking setup in my environment. Now I am able to make it work. I am closing the issue. \r\nThank you."
      },
      {
        "user": "ghost",
        "created_at": "2020-02-12T15:01:24Z",
        "body": "@sibendu, what was the issue with network set up? I am facing the same issue now."
      },
      {
        "user": "sibendu",
        "created_at": "2020-02-12T16:03:44Z",
        "body": "hi. as far as i can recall, it was nothing related to envoy. i was initially having the java service outside kubernetes on separate cloud VMs; and networking rules were not configured properly.    "
      },
      {
        "user": "ghost",
        "created_at": "2020-02-12T16:12:20Z",
        "body": "Got it resolved - it was something to do with http2 options!! Thanks!!!"
      }
    ]
  },
  {
    "number": 7257,
    "title": "envoy http ratelimit is work,but https ratelimit not work",
    "created_at": "2019-06-13T09:39:24Z",
    "closed_at": "2019-07-21T17:04:07Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7257",
    "body": "static_resources:\r\nlisteners:\r\n\r\naddress:\r\nsocket_address:\r\naddress: 0.0.0.0\r\nport_value: 443\r\nfilter_chains:\r\n\r\nfilters:\r\n\r\nname: envoy.http_connection_manager\r\ntyped_config:\r\n\"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\ncodec_type: auto\r\nstat_prefix: ingress_https\r\naccess_log:\r\n- name: envoy.file_access_log\r\nconfig:\r\npath: \"/var/log/access.log\"\r\nroute_config:\r\nname: local_route\r\nvirtual_hosts:\r\n- name: backend\r\ndomains:\r\n- \"test.com\"\r\nroutes:\r\n- match:\r\nprefix: \"/service/1\"\r\nroute:\r\ncluster: service1\r\n- match:\r\nprefix: \"/service/2\"\r\nroute:\r\ncluster: service2\r\nrate_limits:\r\n- stage: 0\r\nactions:\r\n- generic_key:\r\ndescriptor_value: \"default\"\r\nhttp_filters:\r\n\r\nname: envoy.rate_limit\r\nconfig:\r\ndomain: apis\r\nstage: 0\r\nfailure_mode_deny: true\r\nrate_limit_service:\r\ngrpc_service:\r\nenvoy_grpc:\r\ncluster_name: rate_limit_cluster\r\ntimeout: 0.25s\r\nhttp_filters:\r\n\r\nname: envoy.router\r\ntyped_config: {}\r\ntls_context:\r\ncommon_tls_context:\r\nalpn_protocols:\r\n- h2\r\n- http/1.1\r\ntls_certificates:\r\n- certificate_chain:\r\nfilename: \"/etc/test.com.pem\"\r\nprivate_key:\r\nfilename: \"/etc/test.com.key\"\r\nclusters:\r\n\r\nname: service1\r\nconnect_timeout: 0.25s\r\ntype: strict_dns\r\nlb_policy: round_robin\r\nhttp2_protocol_options: {}\r\nload_assignment:\r\ncluster_name: service1\r\nendpoints:\r\n\r\nlb_endpoints:\r\nendpoint:\r\naddress:\r\nsocket_address:\r\naddress: service1\r\nport_value: 80\r\nname: service2\r\nconnect_timeout: 0.25s\r\ntype: strict_dns\r\nlb_policy: round_robin\r\nhttp2_protocol_options: {}\r\nload_assignment:\r\ncluster_name: service2\r\nendpoints:\r\n\r\nlb_endpoints:\r\nendpoint:\r\naddress:\r\nsocket_address:\r\naddress: service2\r\nport_value: 80\r\nname: rate_limit_cluster\r\ntype: STATIC\r\nconnect_timeout: 0.25s\r\nlb_policy: round_robin\r\nhttp2_protocol_options: {}\r\nhosts:\r\n\r\nsocket_address:\r\naddress: 192.168.1.213\r\nport_value: 8081\r\nadmin:\r\naccess_log_path: \"/dev/null\"\r\naddress:\r\nsocket_address:\r\naddress: 0.0.0.0\r\nport_value: 8001",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7257/comments",
    "author": "0521ak47",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-07-14T16:19:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-21T17:04:06Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7219,
    "title": "downstream_pre_cx_timeout stat is incorrectly incremented",
    "created_at": "2019-06-10T05:10:02Z",
    "closed_at": "2019-07-18T05:13:50Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7219",
    "body": "While debugging a `no healthy upstream` issue, I figured out that  `downstream_pre_cx_timeout` is incorrectly incremented. We send a request to upstream and it fails with `no healthy upstream` error. The following stats are incremented correctly\r\n\r\n`listener.0.0.0.0_8443.http.0.0.0.0_8443.downstream_rq_5xx: 103\r\nlistener.0.0.0.0_8443.http.0.0.0.0_8443.downstream_rq_completed: 108`\r\n\r\nHowever after 15s (which is the default listener filter timeout) the stat `listener.0.0.0.0_8443.downstream_pre_cx_timeout` is incremented. \r\n\r\nI think this stat should not be incremented in this case. Is this a bug or am I missing some thing?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7219/comments",
    "author": "ramaraochavali",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-06-10T18:03:25Z",
        "body": "I doubt these two events are related, but definitely take a look at the code and let us know if you see any timer bugs."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2019-06-11T03:27:28Z",
        "body": "@mattklein123 I have not looked at the code yet, but I think it may be a timer issue. As soon as the request fails, I see `no heathy upstream` response and `http.0.0.0.0_8443.downstream_rq_5xx` and `http.0.0.0.0_8443.downstream_rq_completed` are incremented immediately. This makes me think that  filter chain is complete and after 15s `downstream_pre_cx_timeout` is incremented. \r\n\r\nWhat do you think? I will look at the code to see if there is a timer issue and respond back here."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-11T04:35:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-18T05:13:49Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7217,
    "title": "Terminate upstream connection after every request/stream",
    "created_at": "2019-06-09T06:32:22Z",
    "closed_at": "2019-07-18T01:13:51Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7217",
    "body": "**Issue Template**\r\n\r\n*Title*: Feature request - Terminate upstream connection after every request/stream\r\n\r\n*Description*:\r\nCurrently envoy keeps the connection open to upstream cluster. But we want to disable it. Already tried `max_requests_per_connection:1` and `max_concurrent_streams: 1`. We want the upstream connection to close after every request.\r\nOne use case is: Upstream instance has multi-process model  and uses SO_REUSEPORT to balance requests between connections. With one persistent connection open to a process, all requests will end up being served by same worker process.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7217/comments",
    "author": "conqerAtapple",
    "comments": [
      {
        "user": "conqerAtapple",
        "created_at": "2019-06-09T14:13:39Z",
        "body": "@mattklein123 "
      },
      {
        "user": "mattklein123",
        "created_at": "2019-06-10T02:40:32Z",
        "body": "@conqerAtapple assuming you are talking about HTTP the settings you specify should work. Will need more config details, etc. to understand what is going on."
      },
      {
        "user": "conqerAtapple",
        "created_at": "2019-06-10T13:26:20Z",
        "body": "@mattklien123 no the request is for gRPC proxying. Since h2 connections are persistent to upstream, it effects upstreams who are load balanced by single-socket multi-processs architecture. How do we opt out of the upstream persistent connection? Essentially a connection pool size of 0."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-06-10T20:34:36Z",
        "body": "The settings you identify should work, effectively max requests per connection."
      },
      {
        "user": "conqerAtapple",
        "created_at": "2019-06-10T23:34:26Z",
        "body": "@mattklein123 We already tried that configuration(max_requests_per_connection). What we see is that -\r\n- Without envoy (grpc client -> grpc server) requests have increasing streamid. This is because grpc client increases streamId on the same connection.\r\n- With envoy in between as gRPC proxy, envoy does not terminate the connection to the gRPC server after the request. Our observation is that envoy sometimes reuses the old connection if the request is sent after a certain time. This causes gRPC server to fail as it sees a duplicate streamid for the same connection.\r\n\r\nSo the question is : is there a \"time\" associated with the max_requests_per_connection within which the request is sent to a new connection?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-06-10T23:39:16Z",
        "body": "> So the question is : is there a \"time\" associated with the max_requests_per_connection within which the request is sent to a new connection?\r\n\r\nNo, I don't think so. I would take a look at the http/2 connection pool code."
      },
      {
        "user": "conqerAtapple",
        "created_at": "2019-06-10T23:43:57Z",
        "body": "The connection is definitely not closed after each request. We do see new connections (with old connections still open). The \"time\" factor is something we are still investigating. Your insight will save a lot of time on our end. Thanks! "
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-11T00:35:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-18T01:13:50Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7187,
    "title": "How to restart envoy at docker container?",
    "created_at": "2019-06-06T06:56:46Z",
    "closed_at": "2019-07-18T04:13:50Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7187",
    "body": "**How to restart envoy at docker container?**\r\n\r\n*Description*:\r\n>when envoy running at docker container, __and pid is 1__,if execute `docker exec -d {container_id} envoy -c etc/envoy/envoy.yaml --restart-epoch 1 --log-level info --drain-time-s 60 --parent-shutdown-time-s 90` when parent shutdown, the current container also shutdow",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7187/comments",
    "author": "xxm404",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-06-07T21:57:04Z",
        "body": "@mattklein123 I think you previously mentioned container based hot restart is possible?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-06-10T02:44:02Z",
        "body": "At a high level, in order to use hot restart the containers need to be able to talk to each other over unix domain sockets. This means they likely have to use host networking."
      },
      {
        "user": "derekargueta",
        "created_at": "2019-06-11T02:14:52Z",
        "body": "I implemented this a bit over a year ago - we do hot restarts across containers. The 2 container requirements I recall are 1) host networking (as Matt mentioned) and shared IPC for shared memory.\r\n\r\nA few more things to consider\r\n1) Both containers have to be running during the draining period, so the container names need to be different\r\n2) If you have a container restart policy, it needs to not restart containers on clean exit, since a clean exit is expected on hot restart."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-11T02:35:27Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-18T04:13:49Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "easyfmxu",
        "created_at": "2020-06-19T19:33:44Z",
        "body": "> I implemented this a bit over a year ago - we do hot restarts across containers. The 2 container requirements I recall are 1) host networking (as Matt mentioned) and shared IPC for shared memory.\r\n> \r\n> A few more things to consider\r\n> \r\n> 1. Both containers have to be running during the draining period, so the container names need to be different\r\n> 2. If you have a container restart policy, it needs to not restart containers on clean exit, since a clean exit is expected on hot restart.\r\n\r\n@derekargueta We are interested in hot-restart with containers. do you have more details how do you do that? Also for things to consider. 2.  If you have a container restart policy, it needs to not restart containers on clean exit, since a clean exit is expected on hot restart. what does it exactly mean? Thanks\r\n"
      },
      {
        "user": "ks864148379",
        "created_at": "2020-08-12T08:20:45Z",
        "body": "> I implemented this a bit over a year ago - we do hot restarts across containers. The 2 container requirements I recall are 1) host networking (as Matt mentioned) and shared IPC for shared memory.\r\n> \r\n> A few more things to consider\r\n> \r\n> 1. Both containers have to be running during the draining period, so the container names need to be different\r\n> 2. If you have a container restart policy, it needs to not restart containers on clean exit, since a clean exit is expected on hot restart.\r\n\r\n@derekargueta  Why should use host networking between containers? If I mount /dev and share IPC namespace and mount .sock between containers, envoy can hot restart too? "
      }
    ]
  },
  {
    "number": 7144,
    "title": "What's the best way to send additional side-requests in an ActiveHealthCheckSession?",
    "created_at": "2019-06-03T20:16:42Z",
    "closed_at": "2019-07-10T23:35:29Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7144",
    "body": "Hi,\r\n\r\nI am trying to healthcheck a service behind Envoy that requires more than one request to be made from Envoy.\r\n\r\nBasically:\r\n\r\n1- send request to origin\r\n2- get response, use that response to send another request to the same origin\r\n3- assert something about that response and based on that mark the origin active or not\r\n\r\nAnd I am wondering what would be the best way to do (2).\r\n\r\nI could re-use \r\n```\r\nrequest_encoder_ = &client_->newStream(*this);\r\nrequest_encoder_->getStream().addCallbacks(*this);\r\n```\r\n\r\nin the same `ActiveHealthCheckSession` however I feel like there might be an easier way. Perhaps something using the `StreamDecoderWrapper` or `AsyncClientImpl`.\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7144/comments",
    "author": "canselcik",
    "comments": [
      {
        "user": "canselcik",
        "created_at": "2019-06-03T20:17:12Z",
        "body": "@mattklein123 feel free to mark it as a \"question\". Any pointers are appreciated. :)"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-06-03T22:17:02Z",
        "body": "@canselcik I would potentially take a look at integration tests if you want some inspiration. Fundamentally, yes, you need to make multiple streams/requests against the upstream server."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-03T23:10:54Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-10T23:35:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7138,
    "title": "thrift method return void will increase response_error",
    "created_at": "2019-06-03T11:30:18Z",
    "closed_at": "2019-07-10T17:35:29Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7138",
    "body": "When thrift method return void ,the metirc response_error will increase .As I debug the envoy,the method ConnectionManager::ResponseDecoder::fieldBegin won't be called ,I think there is something worng with it.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7138/comments",
    "author": "pyrl247",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-06-03T16:10:49Z",
        "body": "@zuercher "
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-03T17:10:53Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-10T17:35:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7085,
    "title": "thrift-request blocked when restart client",
    "created_at": "2019-05-28T10:10:03Z",
    "closed_at": "2019-07-06T10:14:46Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7085",
    "body": "I used thrift server as envoy upstream and thrift client as envoy downstream.\r\nhere is my yaml:\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 10000\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.thrift_proxy\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.thrift_proxy.v2alpha1.ThriftProxy\r\n          transport: UNFRAMED\r\n          protocol: BINARY\r\n          stat_prefix: ingress_proxy\r\n          route_config:\r\n            name: local_route\r\n            routes:\r\n            - match:\r\n                method_name: ping\r\n              route:\r\n                cluster: service_thrift\r\n          thrift_filters:\r\n            name: envoy.filters.thrift.router\r\n\r\n  clusters:\r\n  - name: service_thrift\r\n    connect_timeout: 0.25s\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: service_thrift\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 10.4.44.88\r\n                port_value: 9999\r\n```\r\nWhen I stop the client , the connection between server and envoy will not release.Is that correct?\r\nAnd when I start the client again ,it block the thrift request.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7085/comments",
    "author": "pyrl247",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-05-29T02:00:31Z",
        "body": "cc @zuercher "
      },
      {
        "user": "pyrl247",
        "created_at": "2019-05-29T03:10:20Z",
        "body": "It happened when I used thrift simpleserver,the request rpc blocked after I restart thrift client."
      },
      {
        "user": "pyrl247",
        "created_at": "2019-05-30T09:24:12Z",
        "body": "I think I may know way this happen.The second thrift request dispatch to different worker thread by libevent,so when thread switches ,the ready_conns_ is empty, so it need create new connection with server,and the server is simple server which can not accept new connection.Is that correct?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-29T09:26:54Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-06T10:14:45Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 7066,
    "title": "TLS and SNI for TCP proxy connections",
    "created_at": "2019-05-24T10:27:40Z",
    "closed_at": "2019-07-14T10:03:59Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7066",
    "body": "### Description\r\n> While connecting via protocols other than HTTP(say MQTT), the method for TLS termination/origination and SNI are not well documented. This is helpful when we have to manage a TCP proxy. Allowing us to use domain/server name instead of IPs all the while ensuring our connection cannot be snooped.\r\n\r\n<details>\r\n<summary>Here is the config with which we can connect with upstream TCP proxy using IP, port</summary>\r\n\r\n```yaml\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 127.0.0.1\r\n      port_value: 9901\r\nstatic_resources:\r\n  listeners:\r\n  ##- name: listener_other\r\n  - name: listener_mqtt\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 1883\r\n    # listener_filters:\r\n    # - name: \"envoy.listener.tls_inspector\"\r\n      # config: {}\r\n    filter_chains:\r\n    # - filter_chain_match: \r\n        # server_names: [\"dev.8hoot.com\"]\r\n        # transport_protocol: tls\r\n        # application_protocols: []\r\n      # tls_context:\r\n        # common_tls_context:\r\n          # tls_certificates:\r\n          # - certificate_chain:\r\n              # filename: \"./docker_volume_path/cert.pem\"\r\n            # private_key:\r\n              # filename: \"./docker_volume_path/privkey.pem\"\r\n      filters:\r\n      - name: envoy.tcp_proxy\r\n        config:\r\n          stat_prefix: vernemq\r\n          cluster: service_mqtt\r\n\r\n  clusters:\r\n  ##- name: service_other\r\n  - name: service_mqtt\r\n    connect_timeout: 0.25s\r\n    type: STATIC\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    load_assignment:\r\n      cluster_name: service_mqtt\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 172.17.0.10\r\n                port_value: 1883\r\n```\r\n</details>\r\n\r\n### Other relevant details\r\n\r\n- The above configuration allows connecting with the upstream TCP connection using IP. Connecting with MQTT server behind Envoy's TCP proxy looks like this\r\n    ```shell\r\n    $ export HOST='1.1.1.1' # any IP\r\n    $ mosquitto_pub -h ${HOST} -p 1883 -t topic -m payload -q 2 -d\r\n    ```\r\n- With the help SNI we will be able to connect with the TCP proxy using domain names instead. It would look like this\r\n    ```shell\r\n    $ export HOST='test.mosquitto.org' # any domain/server name\r\n    $ mosquitto_pub -h ${HOST} -p 1883 -t topic -m payload -q 2 -d\r\n    ```\r\n- Further TLS temination/origination will allow securing TCP proxy connection between client and Envoy.\r\n\r\n- Comments with single `#` are what I tried for implementing TLS and SNI.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7066/comments",
    "author": "chintan-mishra",
    "comments": [
      {
        "user": "chintan-mishra",
        "created_at": "2019-06-05T09:38:00Z",
        "body": "Evidently, SNI works when it has been set in the nameserver settings and **no changes are required in Envoy's config**. However, TLS for TCP is still bugging me. I will update this if I make further discoveries."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-07T09:45:02Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-14T10:03:58Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "alochym01",
        "created_at": "2019-08-03T02:42:54Z",
        "body": "> Evidently, SNI works when it has been set in the nameserver settings and **no changes are required in Envoy's config**. However, TLS for TCP is still bugging me. I will update this if I make further discoveries.\r\n\r\nany news information?? i had same issue"
      },
      {
        "user": "chintan-mishra",
        "created_at": "2019-08-03T04:14:53Z",
        "body": "I made no discoveries. Still facing the same issue."
      },
      {
        "user": "alochym01",
        "created_at": "2019-08-03T06:20:50Z",
        "body": "@chintan-mishra i got working and i use \r\n\r\n```\r\n[root@localhost envoy]# envoy --version\r\n\r\nenvoy  version: 845923714078e5a3528e85d4b58f2cc95850a1c6/1.8.0-dev/Distribution/RELEASE\r\n```\r\nrunning envoy check configured file\r\n```\r\n[root@localhost envoy]# envoy -c envoy-apiqueue-mqtt.yaml --mode validate\r\n[2019-08-03 13:17:51.011][5539][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)\r\n[2019-08-03 13:17:51.013][5539][info][upstream] source/common/upstream/cluster_manager_impl.cc:133] cm init: all clusters initialized\r\n[2019-08-03 13:17:51.013][5539][info][config] source/server/configuration_impl.cc:60] loading 2 listener(s)\r\n[2019-08-03 13:17:51.110][5539][warning][config] source/server/listener_manager_impl.cc:257] adding listener 'x.x.x.x:8883': filter chain match rules require TLS Inspector listener filter, but it isn't configured, trying to inject it (this might fail if Envoy is compiled without it)\r\n[2019-08-03 13:17:51.110][5539][info][config] source/server/configuration_impl.cc:94] loading tracing configuration\r\n[2019-08-03 13:17:51.110][5539][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration\r\nconfiguration 'envoy-apiqueue-mqtt.yaml' OK\r\n\r\n```\r\n\r\nmy envoy config\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: your_public_ip\r\n        port_value: 8883\r\n      listener_filters:\r\n      - name: \"envoy.listener.tls_inspector\"\r\n        config: {}\r\n    filter_chains:\r\n    - filter_chain_match:\r\n        server_names: [\"yourdomain\"]\r\n        transport_protocol: tls\r\n    - filters:\r\n      - name: envoy.tcp_proxy\r\n        config:\r\n          stat_prefix: mqtt_server\r\n          cluster: mqtt_server\r\n      tls_context:\r\n        common_tls_context:\r\n          tls_certificates:\r\n            certificate_chain: {filename: \"/etc/ssl/certs/certificate.cert\"}\r\n            private_key: {filename: \"/etc/ssl/private/private.key\"}\r\n  clusters:\r\n  - name: mqtt_server\r\n    connect_timeout: 0.25s\r\n    type: STATIC\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: mqtt_service\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: x.x.x.x\r\n                port_value: 1883\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8000\r\n```\r\nmy mqtt config\r\n```\r\nuser mosquitto\r\n\r\npid_file /var/run/mosquitto/mosquitto.pid\r\n\r\n#bind_address localhost\r\n\r\nautosave_interval 3600\r\nautosave_on_changes false\r\npersistence true\r\npersistence_file mosquitto.db\r\npersistence_location /mnt/ssd/mosquitto/\r\n\r\n#connection_messages false\r\nallow_anonymous false\r\nclientid_prefixes xxx-\r\n\r\nlistener 1883 your_ip_address\r\n\r\nlistener 9001\r\nprotocol websockets\r\n\r\nauth_plugin /etc/mosquitto/libs/auth-plug.so\r\nauth_opt_backends http\r\nauth_opt_acl_cacheseconds 0\r\nauth_opt_auth_cacheseconds 0\r\nauth_opt_acl_cachejitter 0\r\nauth_opt_auth_cachejitter 0\r\n\r\n#HTTP\r\nauth_opt_http_ip 127.0.0.1\r\nauth_opt_http_port 8002\r\nauth_opt_http_getuser_uri /queue/auth\r\nauth_opt_http_superuser_uri /queue/superuser\r\nauth_opt_http_aclcheck_uri /queue/acl\r\nauth_opt_http_with_tls false\r\n```\r\n\r\nhope it helps"
      },
      {
        "user": "chintan-mishra",
        "created_at": "2019-08-05T09:56:44Z",
        "body": "Testing this out tomorrow. I was occupied in the last few days. I will update soon."
      }
    ]
  },
  {
    "number": 6950,
    "title": "Envoy is crashed when try to log if there is no space",
    "created_at": "2019-05-15T08:30:01Z",
    "closed_at": "2019-05-16T06:09:45Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6950",
    "body": "**Envoy is crashed when try to log if there is no space**\r\n\r\n*Envoy is crashed when try to log if there is no space*: *Enabled access log and if no space left, envoy is crashed immediately*\r\n\r\n*Description*:\r\nI enabled access logs and on load traffic, the log partition can be full in sometimes. In that time, envoy is crashed and the log is like following. The expected behaviour is not crashed and not try to log if no space left.\r\n\r\nThanks in advance\r\nSincerely\r\n\r\n*Logs*:\r\n[2019-05-14 16:41:00.934][6193][critical][assert] [external/envoy/source/common/access_log/access_log_manager_impl.cc:95] assert failure: result.rc_ == static_cast<ssize_t>(slice.len_).\r\n[2019-05-14 16:41:00.934][6193][critical][backtrace] [bazel-out/k8-dbg/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:81] Caught Aborted, suspect faulting address 0x181e\r\n[2019-05-14 16:41:00.934][6193][critical][backtrace] [bazel-out/k8-dbg/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:69] Backtrace (use tools/stack_decode.py to get line\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6950/comments",
    "author": "cihankom",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-05-15T14:37:38Z",
        "body": "@cihankom that's a debug assert. It will be compiled out on release builds. IMO it's reasonable to keep that assert in this case. In release builds the data will be dropped."
      },
      {
        "user": "cihankom",
        "created_at": "2019-05-16T05:58:52Z",
        "body": "Ok, I see. thanks for your help"
      }
    ]
  },
  {
    "number": 6930,
    "title": "Envoy doesn't execute automatic retries using 5xx Envoy retry policy ",
    "created_at": "2019-05-14T10:40:44Z",
    "closed_at": "2019-06-20T20:45:10Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6930",
    "body": "**Envoy doesn't execute automatic retries using 5xx Envoy retry policy**\r\n\r\n*Description*:\r\n\r\nI'm interested in some envoy behavior. I implemented email sending service that is used as cluster in envoy configuration.  I want to retry requests using envoy, if each of 500, 502, 503, 504 status codes return back from service to envoy. \r\n\r\n**Bug Template**\r\n\r\n*Description*:\r\n\r\nIn the first case I'm sending several request in parallel with binary file to service through envoy and get 500 status code in response from service or 503 service code, if the service is shut down. I except that envoy automatically retry requests, but retries are not occur.\r\n\r\nIn the second case I'am implement POST request with empty data and in this case I get 500 status code and retries are successfully happen.\r\n\r\n*Repro steps*:\r\n> There are 20 parallel requests in the first case with data binary and in the second case with empty data\r\n\r\n```\r\nfile=$1\r\ncurl --data-binary @${file}.post  -L --post301 --connect-timeout 300 $envoy-url \r\n```\r\n*Admin and Stats Output*:\r\n\r\n/stats/prometheus | grep email-common\r\n>The first case: sending data binary file\r\n```\r\nenvoy_cluster_upstream_cx_tx_bytes_total{envoy_cluster_name=\"email-common\"} 4183272\r\nenvoy_cluster_upstream_rq_pending_failure_eject{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_removed{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_attempt{envoy_cluster_name=\"email-common\"} 36\r\nenvoy_cluster_internal_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_subsets_fallback{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_destroy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_change{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_tx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_local_cluster_not_ok{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_fail{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http2_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_total{envoy_cluster_name=\"email-common\"} 8380\r\nenvoy_cluster_bind_errors{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_no_rebuild{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_max_requests{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_drained_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_no_capacity_left{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_attempts_exceeded{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_per_try_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http1_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_rx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_internal_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_number_differs{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_close_notify{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_routing_sampled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_original_dst_host_invalid{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_idle_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_cluster_too_small{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_resumed_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_internal_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_retry_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote_with_active_rq{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_none_healthy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_failure{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local_with_active_rq{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_connect_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_routing_all_directly{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_protocol_error{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_cancelled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_empty{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry_success{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_backed_up_total{envoy_cluster_name=\"email-common\"} 40\r\nenvoy_cluster_lb_recalculate_zone_structures{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_success{envoy_cluster_name=\"email-common\"} 36\r\nenvoy_cluster_retry_or_shadow_abandoned{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_subsets_created{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_with_active_rq{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_routing_cross_zone{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_maintenance_mode{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_paused_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_healthy_panic{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_selected{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_total{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_circuit_breakers_default_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_tx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_healthy{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_upstream_rq_pending_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_max_host_weight{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_version{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_active{envoy_cluster_name=\"email-common\"} 0\r\n\r\n```\r\n>The second case: sending request with empty data\r\n```\r\nenvoy_cluster_upstream_cx_tx_bytes_total{envoy_cluster_name=\"email-common\"} 26950\r\nenvoy_cluster_upstream_rq_pending_failure_eject{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_removed{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_attempt{envoy_cluster_name=\"email-common\"} 30\r\nenvoy_cluster_internal_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_retry_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_subsets_fallback{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_destroy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_change{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_tx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_local_cluster_not_ok{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_fail{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http2_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_total{envoy_cluster_name=\"email-common\"} 23045\r\nenvoy_cluster_bind_errors{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_no_rebuild{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_max_requests{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_drained_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_no_capacity_left{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_attempts_exceeded{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_per_try_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http1_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_upstream_rq_rx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_internal_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_number_differs{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_close_notify{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_routing_sampled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_original_dst_host_invalid{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_idle_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_zone_cluster_too_small{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_resumed_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_internal_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_retry_overflow{envoy_cluster_name=\"email-common\"} 13\r\nenvoy_cluster_upstream_rq_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_retry_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_cx_destroy_remote{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote_with_active_rq{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_retry_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_rq_total{envoy_cluster_name=\"email-common\"} 55\r\nenvoy_cluster_upstream_cx_none_healthy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_failure{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local_with_active_rq{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_cx_connect_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_lb_zone_routing_all_directly{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_protocol_error{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_cancelled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_empty{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry_success{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_backed_up_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_recalculate_zone_structures{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_success{envoy_cluster_name=\"email-common\"} 31\r\nenvoy_cluster_retry_or_shadow_abandoned{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_created{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_with_active_rq{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_zone_routing_cross_zone{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_maintenance_mode{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_paused_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_healthy_panic{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_selected{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_total{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_circuit_breakers_default_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_tx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_buffered{envoy_cluster_name=\"email-common\"} 3771\r\nenvoy_cluster_membership_healthy{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_upstream_rq_pending_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_max_host_weight{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_version{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_active{envoy_cluster_name=\"email-common\"} 9\r\n```\r\n*Config*:\r\nEnvoy version is v1.9.0, pulled from docker hub\r\n\r\n```\r\nadmin:\r\n  access_log_path: /app/admin_access.log\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 8081 }\r\nstatic_resources:\r\n  listeners:\r\n  - name: https_listener\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 8080 }\r\n    filter_chains:\r\n      - filters:\r\n        - name: envoy.http_connection_manager\r\n          config:\r\n            codec_type: AUTO\r\n            stat_prefix: ingress\r\n            route_config:\r\n              name: router\r\n              virtual_hosts:\r\n              - name: common\r\n                domains: [\"*\"]\r\n                routes:\r\n                - match: { prefix: \"/\" }\r\n                  route:\r\n                    cluster: email-common\r\n                    auto_host_rewrite: true\r\n                    timeout: 50s\r\n                    retry_policy:\r\n                      retry_on: \"5xx\"\r\n                      num_retries: 5\r\n                      per_try_timeout: 10s\r\n            http_filters:\r\n            - name: envoy.router\r\n              config: { deprecated_v1: true }\r\n  clusters:\r\n    name: email-common\r\n    type: LOGICAL_DNS\r\n    connect_timeout: 1s\r\n    tls_context:\r\n      common_tls_context:\r\n        validation_context:\r\n          trusted_ca:\r\n            filename: /app/data/fullchain.pem\r\n    hosts:\r\n      - socket_address: { address: email-service, port_value: 443 }\r\n```\r\n\r\n\r\n*Logs*:\r\n>Logs of requests for requests with data binary:\r\n\r\n```\r\n[2019-05-14 10:25:28.054][000036][debug][router] [source/common/router/router.cc:1023] [C782][S9375733934444584087] pool ready\r\n--\r\n  | [2019-05-14 10:25:28.056][000036][debug][router] [source/common/router/router.cc:615] [C782][S9375733934444584087] upstream headers complete: end_stream=false\r\n  | [2019-05-14 10:25:28.056][000036][debug][router] [source/common/router/router.cc:968] [C782][S9375733934444584087] resetting pool request\r\n  | [2019-05-14 10:25:28.074][000036][debug][router] [source/common/router/router.cc:270] [C784][S6563403434288646526] cluster 'email-common' match for URL '/email/'\r\n  | [2019-05-14 10:25:28.074][000036][debug][router] [source/common/router/router.cc:328] [C784][S6563403434288646526] router decoding headers:\r\n  | ':authority', 'envoy-proxy'\r\n  | ':path', '/email/'\r\n  | ':method', 'POST'\r\n  | ':scheme', 'http'\r\n  | 'content-length', '2266446'\r\n  | 'user-agent', 'curl/7.64.0'\r\n  | 'accept', '*/*'\r\n  | 'authorization', 'none'\r\n  | 'content-type', 'application/json'\r\n  | 'x-request-id', '693db9a5-46ee-4bd7-9bb3-0766fbe90ed4'\r\n  | 'x-envoy-expected-rq-timeout-ms', '10000'\r\n  | 'x-forwarded-host', 'envoy-proxy.host.ru;'\r\n  | 'x-forwarded-port', '80'\r\n  | 'x-forwarded-proto', 'http'\r\n  | 'forwarded', 'for=10.233.53.119;host=envoy-proxy.host.ru;;proto=http'\r\n  | 'x-forwarded-for', '10.233.53.119'\r\n  | 'x-envoy-internal', 'true'\r\n```\r\n>Logs of requests for requests with empty data:\r\n```\r\n[2019-05-14 10:10:08.241][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n--\r\n  | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n  | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n  | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n  | [2019-05-14 10:10:08.248][000040][debug][router] [source/common/router/router.cc:1023] [C254][S6944810802321297265] pool ready\r\n  | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:615] [C254][S6944810802321297265] upstream headers complete: end_stream=false\r\n  | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:779] [C254][S6944810802321297265] performing retry\r\n  | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:968] [C254][S6944810802321297265] resetting pool request\r\n  | [2019-05-14 10:10:08.260][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n  | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n  | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n  | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n  | [2019-05-14 10:10:08.271][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n  | [2019-05-14 10:10:08.276][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n  | [2019-05-14 10:10:08.284][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n  | [2019-05-14 10:10:08.284][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n  | [2019-05-14 10:10:08.290][000039][debug][router] [source/common/router/router.cc:270] [C263][S2736688878384099272] cluster 'email-common' match for URL '/email/'\r\n  | [2019-05-14 10:10:08.290][000039][debug][router] [source/common/router/router.cc:328] [C263][S2736688878384099272] router decoding headers:\r\n  | ':authority', 'envoy-proxy'\r\n  | ':path', '/email/'\r\n  | ':method', 'POST'\r\n  | ':scheme', 'http'\r\n  | 'content-length', '0'\r\n  | 'user-agent', 'curl/7.64.0'\r\n  | 'accept', '*/*'\r\n  | 'authorization', 'none'\r\n  | 'content-type', 'application/json'\r\n  | 'x-request-id', 'f627a2c5-a91d-4c71-aa56-61c04163eb88'\r\n  | 'x-envoy-expected-rq-timeout-ms', '10000'\r\n  | 'x-forwarded-host', 'envoy-proxy.host.ru'\r\n  | 'x-forwarded-port', '80'\r\n  | 'x-forwarded-proto', 'http'\r\n  | 'forwarded', 'for=10.233.53.119;host=envoy-proxy.host.ru;proto=http'\r\n  | 'x-forwarded-for', '10.233.53.119'\r\n  | 'x-envoy-internal', 'true'\r\n```\r\n\r\nQuestions:\r\n\r\n1. Why envoy don't execute retries, if I send to it some data binary in requests? The target service return 500 error code, and the envoy can executing retries according '5xx', for example for requests with empty data.\r\n\r\n1. What it can be changed to retry occurring for requests with binary data? Can it is due with some retry policy parameters, such as 'per_try_timeout' or some clusters parameters like type or connect timeout? .\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6930/comments",
    "author": "PetrovMikhail",
    "comments": [
      {
        "user": "PetrovMikhail",
        "created_at": "2019-05-14T13:21:34Z",
        "body": "Size of sending file in request is in the first case ~2 MB."
      },
      {
        "user": "snowp",
        "created_at": "2019-05-14T13:27:21Z",
        "body": "I suspect you're running into the limitation that Envoy is unable to retry requests that are too large to fit into its buffer, evident by the fact that `envoy_cluster_retry_or_shadow_abandoned` is non-zero. Can you try setting `per_connection_buffer_limit_bytes` on the cluster to something higher? The default is 1MiB."
      },
      {
        "user": "PetrovMikhail",
        "created_at": "2019-05-14T14:24:32Z",
        "body": "Yes, I think that your advise in right way, thank you! but I increased this value, and it did not work yet, I think that i have to increase limit file value in another parameters of Envoy too. Do you where it is may be? "
      },
      {
        "user": "PetrovMikhail",
        "created_at": "2019-05-14T15:19:26Z",
        "body": "@snowp , thank you very match, this was a right solution. Besides envoy we have nginx in series, and after increasing `client_max_body_size`  it works fine."
      },
      {
        "user": "PetrovMikhail",
        "created_at": "2019-05-14T20:29:23Z",
        "body": "This is my folder, I didn't understood correct results of tests, which I describe in two previous comments. I already change `per_connection_buffer_limit_bytes` to 30 000 000 value, which just about equal 30 MiB, but when I was testing envoy behavior next, I discovered that envoy still doesn't retry request with files > 1 MiB. Maybe another parameter is occurs which changes buffer size of cluster and so on? "
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-13T20:44:23Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-20T20:45:09Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "feature-id",
        "created_at": "2021-03-15T12:01:49Z",
        "body": "For all, who got the same problem: there's also per_connection_buffer_limit_bytes option on a listener level, which is 1MB by default."
      }
    ]
  },
  {
    "number": 6887,
    "title": "Envoy Asserted when continueDecoding called in async callback",
    "created_at": "2019-05-10T08:23:56Z",
    "closed_at": "2019-06-16T15:59:58Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6887",
    "body": "**Envoy Asserted when continueDecoding called in async callback**\r\n\r\n*Title*: Envoy Asserted when continueDecoding called in async callback\r\n\r\n*Description*:\r\nI have implemented a http filter. I am making some request as async. When response is come, I need to call decoder_callback->continueDecoding. In that time, envoy is asserted at `ConnectionManagerImpl::ActiveStreamFilterBase::commonContinue` -> `ASSERT(!canIterate());` I am using some third part library and this library is using libevent. When event is triggered, it calls a callback.\r\n\r\nBut there is also another type of async callback and this is `httpAsyncClientForCluster`. When I create a async http request with this , it makes no problem and I can call continueDecoding in `httpAsyncClientForCluster` -> callback .\r\n\r\nI could not understand why there is no problem for `httpAsyncClientForCluster` . I have investigate source code also httpAsync to see difference. I could not find anything , I should overlooked something else.\r\n\r\nWhat should be done in async callback before calling continueDecoding ? there should be difference between `httpAsyncClientForCluster`  and libevent callbacks or std::async ?\r\n\r\nThanks in advance\r\nSincerely",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6887/comments",
    "author": "cihankom",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-06-09T15:45:05Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-16T15:59:57Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6867,
    "title": "EnvoyProxy Configuration With DC/OS",
    "created_at": "2019-05-09T06:22:51Z",
    "closed_at": "2019-06-15T14:36:44Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6867",
    "body": "Hi All,\r\nI am new to envoy proxy. Is any one know how to configured envoyproxy with DC/OS ? \r\nPlease ping me.\r\nThanks in advanced.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6867/comments",
    "author": "II-VSB-II",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-06-08T14:17:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-15T14:36:43Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6845,
    "title": "Add ability to support dynamic metadata while initializing grpc stream",
    "created_at": "2019-05-07T21:44:13Z",
    "closed_at": "2019-06-19T05:48:25Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6845",
    "body": "Description: While writing custom filters when there is a need to use grpc stream, it would be great to add custom \"dynamic\" headers while initializing a grpc stream. Currently it supports only static initial_metadata.\r\n```\r\nheaders_message_ = Common::prepareHeaders(\r\n      parent_.remote_cluster_name_, service_method_.service()->full_name(), service_method_.name(),\r\n      absl::optional<std::chrono::milliseconds>(timeout_));\r\n  // Fill service-wide initial metadata.\r\n  for (const auto& header_value : parent_.initial_metadata_) {\r\n    headers_message_->headers().addCopy(Http::LowerCaseString(header_value.key()),\r\n                                        header_value.value());\r\n  }\r\n  callbacks_.onCreateInitialMetadata(headers_message_->headers());\r\n  stream_->sendHeaders(headers_message_->headers(), false);\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6845/comments",
    "author": "rbkumar88",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-05-13T04:05:54Z",
        "body": "`callbacks_.onCreateInitialMetadata(headers_message_->headers());` is where you can add/remove/modify headers in a passed in callback."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-12T05:00:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-19T05:48:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6835,
    "title": "compile envoy of branch 1.10.0 failed",
    "created_at": "2019-05-07T06:24:06Z",
    "closed_at": "2019-06-23T14:04:28Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6835",
    "body": "I compile envoy of version 1.10.0 failed, the output is below\r\n\r\n\r\n\r\nINFO: From Generating Descriptor Set proto_library @envoy_api//envoy/service/ratelimit/v2:rls:\r\nexternal/com_google_protobuf: warning: directory does not exist.\r\nINFO: From Generating Descriptor Set proto_library @envoy_api//envoy/config/filter/network/rate_limit/v2:rate_limit:\r\nexternal/com_google_protobuf: warning: directory does not exist.\r\nINFO: From GoLink external/com_lyft_protoc_gen_validate/linux_amd64_stripped/protoc-gen-validate [for host]:\r\nGoLink: warning: package \"github.com/golang/protobuf/ptypes/any\" is provided by more than one rule:\r\n    @com_github_golang_protobuf//ptypes/any:go_default_library\r\n    @io_bazel_rules_go//proto/wkt:any_go_proto\r\nSet \"importmap\" to different paths in each library.\r\nThis will be an error in the future.\r\nGoLink: warning: package \"github.com/golang/protobuf/ptypes/duration\" is provided by more than one rule:\r\n    @com_github_golang_protobuf//ptypes/duration:go_default_library\r\n    @io_bazel_rules_go//proto/wkt:duration_go_proto\r\nSet \"importmap\" to different paths in each library.\r\nThis will be an error in the future.\r\nGoLink: warning: package \"github.com/golang/protobuf/ptypes/timestamp\" is provided by more than one rule:\r\n    @com_github_golang_protobuf//ptypes/timestamp:go_default_library\r\n    @io_bazel_rules_go//proto/wkt:timestamp_go_proto\r\nSet \"importmap\" to different paths in each library.\r\nThis will be an error in the future.\r\nERROR: /home/linuxlover/C++/envoy/source/exe/BUILD:26:1: Linking of rule '//source/exe:envoy-static' failed (Exit 1) envoy_cc_wrapper failed: error executing command /home/linuxlover/.cache/bazel/_bazel_linuxlover/decf774929bcae4e6bd155910598679a/external/local_config_cc/extra_tools/envoy_cc_wrapper -o bazel-out/k8-dbg/bin/source/exe/envoy-static -pthread ... (remaining 62 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\n/usr/bin/ld.gold: internal error in write_build_id, at ../../gold/layout.cc:5542\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //source/exe:envoy-static failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1220.023s, Critical Path: 98.24s\r\nINFO: 1970 processes: 1970 linux-sandbox.\r\nFAILED: Build did NOT complete successfully\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6835/comments",
    "author": "zplinuxlover",
    "comments": [
      {
        "user": "HarinadhD",
        "created_at": "2019-05-16T15:05:57Z",
        "body": "#6835 envoy-1.10.0 compilation failed\r\n\r\n I'm using below command to build envoy-1.10.0\r\nbazel build -c dbg //source/exe:envoy-static\r\n\r\n\r\nBuild failed , ld.gold failed to find libstd++ , So further execution failed to resolve symbols\r\n\r\nbelow is the output \r\n---------------------\r\nINFO: From Compiling external/envoy_api/envoy/data/tap/v2alpha/http.pb.cc:\r\nbazel-out/k8-opt/genfiles/external/envoy_api/envoy/data/tap/v2alpha/http.pb.cc:199:13: warning: 'dynamic_init_dummy_envoy_2fdata_2ftap_2fv2alpha_2fhttp_2eproto' defined but not used [-Wunused-variable]\r\n static bool dynamic_init_dummy_envoy_2fdata_2ftap_2fv2alpha_2fhttp_2eproto = []() { AddDescriptors_envoy_2fdata_2ftap_2fv2alpha_2fhttp_2eproto(); return true; }();\r\n             ^\r\n[2,667 / 2,668] Linking source/exe/envoy-static; 38s processwrapper-sandbox\r\n**ERROR: /usr/src/photon/BUILD/envoy-v1.10.0/envoy-1.10.0/source/exe/BUILD:26:1: Linking of rule '//source/exe:envoy-static' failed (Exit 1) envoy_cc_wrapper failed: error executing command /root/.cache/bazel/_bazel_root/8875cd1ec28c696ed4d7a4cb16d80bcf/external/local_config_cc/extra_tools/envoy_cc_wrapper -o bazel-out/k8-opt/bin/source/exe/envoy-static -pthread '-Wl,--hash-style=gnu' ... (remaining 63 argument(s) skipped)**\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\n**/bin/ld.gold: error: cannot find -lstdc++**\r\nbazel-out/k8-**opt/bin/source/common/common/_virtual_includes/utility_lib/common/common/utility.h:103: error: undefined reference to 'std::chrono::_V2::steady_clock::now()'\r\nbazel-out/k8-opt/bin/source/common/common/_virtual_includes/utility_lib/common/common/utility.h:102: error: undefined reference to 'std::chrono::_V2::system_clock::now()'\r\nexternal/com_github_fmtlib_fmt/include/fmt/format.h:365: error: undefined reference to 'std::runtime_error::~runtime_error()'**\r\nexternal/com_github_fmtlib_fmt/include/fmt/format.h:365: error: undefined reference to 'std::runtime_error::~runtime_error()'\r\nexternal/com_github_fmtlib_fmt/include/fmt/format-inl.h:930: error: undefined reference to '__cxa_allocate_exception'\r\nexternal/com_github_fmtlib_fmt/include/fmt/format.h:368: error: undefined reference to 'std::runtime_error::runtime_error(char const*)'\r\nexternal/com_github_fmtlib_fmt/include/fmt/format-inl.h:930: error: undefined reference to '__cxa_throw'\r\nexternal/com_github_fmtlib_fmt/include/fmt/format-inl.h:930: error: undefined reference to '__cxa_free_exception'\r\nexternal/com_github_fmtlib_fmt/include/fmt/format.h:2493: error: undefined reference to '__cxa_allocate_exception'\r\nexternal/com_github_fmtlib_fmt/include/fmt/format.h:368: error: undefined reference to 'std::runtime_error::runtime_error(char const*)'\r\nexternal/com_github_fmtlib_fmt/include/fmt/format.h:2493: error: undefined reference to '__cxa_throw'\r\nexternal/com_github_fmtlib_fmt/include/fmt/format.h:2493: error: undefined reference to '__cxa_free_exception'\r\nbazel-out/k8-opt/bin/source/exe/_virtual_includes/terminate_handler_lib/exe/terminate_handler.h:13: error: undefined reference to 'std::set_terminate(void (*)())'\r\nsource/exe/main.cc:27: error: undefined reference to '__cxa_begin_catch'\r\nsource/exe/main.cc:27: error: undefined reference to '__cxa_end_catch'\r\nsource/exe/main.cc:32: error: undefined reference to '__cxa_begin_catch'\r\nsource/exe/main.cc:33: error: undefined reference to 'std::cerr'\r\nsource/exe/main.cc:33: error: undefined reference to 'std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'\r\n/usr/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/../../../../include/c++/5.3.0/ostream:113: error: undefined reference to 'std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)' "
      },
      {
        "user": "HarinadhD",
        "created_at": "2019-05-17T12:21:00Z",
        "body": "\r\n\r\n\r\n> #6835 envoy-1.10.0 compilation failed\r\n> \r\n> I'm using below command to build envoy-1.10.0\r\n> bazel build -c dbg //source/exe:envoy-static\r\n> \r\n> Build failed , ld.gold failed to find libstd++ , So further execution failed to resolve symbols\r\n> \r\n> ## below is the output\r\n> INFO: From Compiling external/envoy_api/envoy/data/tap/v2alpha/http.pb.cc:\r\n> bazel-out/k8-opt/genfiles/external/envoy_api/envoy/data/tap/v2alpha/http.pb.cc:199:13: warning: 'dynamic_init_dummy_envoy_2fdata_2ftap_2fv2alpha_2fhttp_2eproto' defined but not used [-Wunused-variable]\r\n> static bool dynamic_init_dummy_envoy_2fdata_2ftap_2fv2alpha_2fhttp_2eproto = { AddDescriptors_envoy_2fdata_2ftap_2fv2alpha_2fhttp_2eproto(); return true; }();\r\n> ^\r\n> [2,667 / 2,668] Linking source/exe/envoy-static; 38s processwrapper-sandbox\r\n> **ERROR: /usr/src/photon/BUILD/envoy-v1.10.0/envoy-1.10.0/source/exe/BUILD:26:1: Linking of rule '//source/exe:envoy-static' failed (Exit 1) envoy_cc_wrapper failed: error executing command /root/.cache/bazel/_bazel_root/8875cd1ec28c696ed4d7a4cb16d80bcf/external/local_config_cc/extra_tools/envoy_cc_wrapper -o bazel-out/k8-opt/bin/source/exe/envoy-static -pthread '-Wl,--hash-style=gnu' ... (remaining 63 argument(s) skipped)**\r\n> \r\n> Use --sandbox_debug to see verbose messages from the sandbox\r\n> **/bin/ld.gold: error: cannot find -lstdc++**\r\n> bazel-out/k8-**opt/bin/source/common/common/_virtual_includes/utility_lib/common/common/utility.h:103: error: undefined reference to 'std::chrono::_V2::steady_clock::now()' bazel-out/k8-opt/bin/source/common/common/_virtual_includes/utility_lib/common/common/utility.h:102: error: undefined reference to 'std::chrono::_V2::system_clock::now()' external/com_github_fmtlib_fmt/include/fmt/format.h:365: error: undefined reference to 'std::runtime_error::~runtime_error()'**\r\n> external/com_github_fmtlib_fmt/include/fmt/format.h:365: error: undefined reference to 'std::runtime_error::~runtime_error()'\r\n> external/com_github_fmtlib_fmt/include/fmt/format-inl.h:930: error: undefined reference to '__cxa_allocate_exception'\r\n> external/com_github_fmtlib_fmt/include/fmt/format.h:368: error: undefined reference to 'std::runtime_error::runtime_error(char const*)'\r\n> external/com_github_fmtlib_fmt/include/fmt/format-inl.h:930: error: undefined reference to '__cxa_throw'\r\n> external/com_github_fmtlib_fmt/include/fmt/format-inl.h:930: error: undefined reference to '__cxa_free_exception'\r\n> external/com_github_fmtlib_fmt/include/fmt/format.h:2493: error: undefined reference to '__cxa_allocate_exception'\r\n> external/com_github_fmtlib_fmt/include/fmt/format.h:368: error: undefined reference to 'std::runtime_error::runtime_error(char const*)'\r\n> external/com_github_fmtlib_fmt/include/fmt/format.h:2493: error: undefined reference to '__cxa_throw'\r\n> external/com_github_fmtlib_fmt/include/fmt/format.h:2493: error: undefined reference to '__cxa_free_exception'\r\n> bazel-out/k8-opt/bin/source/exe/_virtual_includes/terminate_handler_lib/exe/terminate_handler.h:13: error: undefined reference to 'std::set_terminate(void (_)())' source/exe/main.cc:27: error: undefined reference to '__cxa_begin_catch' source/exe/main.cc:27: error: undefined reference to '__cxa_end_catch' source/exe/main.cc:32: error: undefined reference to '__cxa_begin_catch' source/exe/main.cc:33: error: undefined reference to 'std::cerr' source/exe/main.cc:33: error: undefined reference to 'std::basic_ostream<char, std::char_traits >& std::operator<< <std::char_traits >(std::basic_ostream<char, std::char_traits >&, char const_)'\r\n> /usr/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/../../../../include/c++/5.3.0/ostream:113: error: undefined reference to 'std::basic_ostream<char, std::char_traits >& std::endl<char, std::char_traits >(std::basic_ostream<char, std::char_traits >&)'\r\n\r\n#issue got resolved after updating gcc version from 5+ to 6+"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-16T12:59:56Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-23T14:04:27Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6830,
    "title": "Pass user_id from ext-authz to rate-limit",
    "created_at": "2019-05-06T21:54:45Z",
    "closed_at": "2019-06-13T14:44:23Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6830",
    "body": "We want to enforce rate limiting at Envoy based on user-id. The user-id is retrieved by the ext-authz service. Is there a way we can pass the user-id from the ext-authz to the rate-limit service?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6830/comments",
    "author": "hanyu-liu",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-06-06T13:16:23Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-13T14:44:22Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6814,
    "title": "Envoy statsd sink, fails ",
    "created_at": "2019-05-06T05:47:51Z",
    "closed_at": "2019-06-12T20:38:29Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6814",
    "body": "*Title*: *Envoy statsd sink fails*\r\n\r\n*Description*:\r\n>Envoy proxy fails when configured with statsd sink for stats sinks. The error is:\r\n\r\n```\r\nproxy_1            | [2019-05-06 04:50:38.006][27][info][main] [source/server/server.cc:516] exiting\r\nproxy_1            | tcp statsd: node 'id' and 'cluster' are required. Set it either in 'node'\r\nconfig or via --service-node and --service-cluster options.\r\ntemplate-starter-windows_proxy_1 exited with code 1\r\n```\r\n\r\nI have the following configuration envoy.yaml:\r\n```\r\nadmin:\r\n  access_log_path: /logs/envoy_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n\r\nstats_sinks:\r\n  name: envoy.statsd\r\n  config:\r\n    tcp_cluster_name: statsd-exporter\r\n\r\nstatic_resources:\r\n  ...\r\n\r\n\r\n  clusters:\r\n\r\n    - name: app\r\n      connect_timeout: 0.25s\r\n      type: strict_dns\r\n      lb_policy: round_robin\r\n      hosts:\r\n        - socket_address:\r\n            address: {{appName}}\r\n            port_value: {{appPort}}\r\n\r\n    - name: statsd-exporter\r\n      connect_timeout: 0.25s\r\n      type: strict_dns\r\n      lb_policy: round_robin\r\n      hosts:\r\n        - socket_address:\r\n            address: statsd_exporter\r\n            port_value: 9125\r\n```\r\nI am not sure if I have made a mistake with configuration or if this is a bug.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6814/comments",
    "author": "nmjmdr",
    "comments": [
      {
        "user": "nmjmdr",
        "created_at": "2019-05-06T06:47:57Z",
        "body": "Update:\r\n\r\nI was able to resolve the error by setting the --service-cluster and --service-node parameters for envoy command: envoy -c /etc/envoy/envoy.yaml --service-cluster 'front-envoy' --service-node 'front-envoy'\r\n\r\nI am not sure why using statsd sink would require these parameters to be set. and The documentation for envoy does not mention this information,"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-05T15:04:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-12T20:38:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "soheilade",
        "created_at": "2019-09-26T08:49:09Z",
        "body": "hi @nmjmdr , I followed your path but the graphite statsd admin on port 8126 is not reachable, did you manage to get this port working and observe statsd admin? same for port 8125(statsd). Any comment is very useful.\r\n"
      }
    ]
  },
  {
    "number": 6780,
    "title": "HTTPS endpoint for Zipkin",
    "created_at": "2019-05-02T16:27:14Z",
    "closed_at": "2019-06-08T22:17:41Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6780",
    "body": "*Title*: *HTTPS endpoint for Zipkin*\r\n\r\n*Description*:\r\nI looked at the code `zipkin_tracer_impl.cc` and found that the code currently uses:\r\n```\r\n    driver_.clusterManager()\r\n        .httpAsyncClientForCluster(driver_.cluster()->name())\r\n```\r\nto create an http client. I then followed into the http client implementation and found some code related to SSL.\r\n\r\nMy main question is this: Is it possible to use a Zipkin HTTPS endpoint for tracing?\r\nIf currently not possible, could you provide some pointer where could I start hacking to add this kind of functionality. I plan to use envoy to trace services in a Google Serverless environment that supports only the HTTPS endpoint. Thank you.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6780/comments",
    "author": "chanwit",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-06-01T21:35:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-08T22:17:40Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6772,
    "title": "Building envoy filters with docker CI scripts",
    "created_at": "2019-05-01T21:05:26Z",
    "closed_at": "2019-06-08T20:17:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6772",
    "body": "Do we have a guide somewhere that shows you how to build a custom filter (and link to envoy binary) using the `run_envoy_docker.sh` wrapper or some variation of it? It would be really nice to be able to do a containerized build of custom filters on our CI system.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6772/comments",
    "author": "kuroneko25",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-06-01T19:35:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-08T20:17:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6755,
    "title": "How to access Envoy API's for dynamic configuration",
    "created_at": "2019-04-30T18:48:02Z",
    "closed_at": "2019-06-07T05:16:24Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6755",
    "body": "I need to  find a way to access Envoy API's for dynamic configuration on AKS. I am deploying envoy on AKS. Do i need to create my own control plane to access these API's? What is best way to do it? ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6755/comments",
    "author": "monubamb",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-05-31T04:21:50Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-07T05:16:23Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6754,
    "title": "Envoy POD on AKS refusing connection",
    "created_at": "2019-04-30T18:36:20Z",
    "closed_at": "2019-06-07T06:16:24Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6754",
    "body": "I have envoy proxy pod listening on port 6004 inside AKS proxying connection to an external VM endpoint. When I connect to the POD, I get connection refused.\r\n\r\nE0429 19:44:45.579885     277 portforward.go:331] an error occurred forwarding 6004 -> 6004: error forwarding port 6004 to pod 872cfaa98e82be7644cca89e85ea5b2ac9d5a5b382fb20e7e3e1b72afa54e7a0, uid : exit status 1: 2019/04/30 02:44:44 socat[14233] E connect(6, AF=2 127.0.0.1:6004, 16): Connection refused\r\n\r\nSame container runs fine on my local machine proxying connection to the endpoint without any issue.\r\n\r\nWhat could be wrong or missing? Tried exposing the POD as a load balanced service.. but same error.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6754/comments",
    "author": "monubamb",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-05-31T05:21:50Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-07T06:16:23Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6706,
    "title": "Expose stats store singleton via Api::Api",
    "created_at": "2019-04-25T17:00:51Z",
    "closed_at": "2019-05-16T23:22:00Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6706",
    "body": "*Title*: Expose stats store singleton via Api::Api\r\n\r\n*Description*: We have a use case where we want to read stats exported by the xDS APIs from a filter. The stats store isn't globally accessible right now, and we'd rather not make admin requests to /stats from the filter. Could we expose the stats store via Api::Api, thus making it accessible to the filter via FactoryContext?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6706/comments",
    "author": "ahedberg",
    "comments": [
      {
        "user": "junr03",
        "created_at": "2019-04-25T17:10:20Z",
        "body": "@jmarantz I think you are the best person to comment on this probably."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-04-25T17:16:53Z",
        "body": "I would be OK exposing the root scope (not store) as a const object only. @jmarantz WDYT?"
      },
      {
        "user": "jmarantz",
        "created_at": "2019-04-25T17:34:28Z",
        "body": "Root Scope SGTM. Store inherits from Scope and adds the counters(), gauges(), and histograms() APIs . Ashley -- is that sufficient? You would just look up stats you know the name of?\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-04-25T17:38:51Z",
        "body": "Oh yeah I guess if it needs to list out all stats, doing the store might be needed. As long as it's const sounds OK."
      },
      {
        "user": "ahedberg",
        "created_at": "2019-04-25T18:06:35Z",
        "body": "We know the names of the stats we care about, so root scope should be sufficient."
      },
      {
        "user": "ahedberg",
        "created_at": "2019-04-26T14:27:08Z",
        "body": "While working on a test in #6712, I realized that the counter/gauge/histogram APIs in Stats::Scope are not const. I'm not sure it's reasonable to affix the const qualifier to those member functions--Envoy::Stats::IsolatedStoreImpl is an eventual subclass, and does decidedly non-const things in its implementations (like adding cached stats to a hash map). This unfortunately makes a const Stats::Scope& not particularly useful.\r\n\r\nThoughts?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-04-26T14:55:54Z",
        "body": "@ahedberg it would be more work, but one option would be to add a const get method to the scope which will only get a stat and not create it. This would probably be generally useful. I don't think we should provide a non-const root scope to all extensions. The other option would be to just return the const store (not scope) which allows you to iterate over all stats. Maybe this would be enough? cc @jmarantz for thoughts."
      },
      {
        "user": "jmarantz",
        "created_at": "2019-04-26T15:34:31Z",
        "body": "Having const getters sgtm. Now; should the returned Counter& also be const, so that you can observe the value but not increment?\r\n\r\nWhat's the usage model for const stats?"
      },
      {
        "user": "jmarantz",
        "created_at": "2019-04-26T17:40:45Z",
        "body": "Sorry I didn't read Matt's and Ashley's comments closely enough and missed a key point -- I think if we want to provide a conceptually const stat store or scope, that provides read-only name lookups, it should still have the same caching behavior and we should annotate the hash tables as mutable, adding new behavior that will avoid creating new stats for a new name, but will populate the caches as needed.  \r\n\r\nThe new method would probably return pointers rather than references, as a non-existing name has to have some behavior. My personal bias is to prefer that to raising exceptions, but if we decide to make the getCounter() method return a ref or throw, we could always wrap it at Google.\r\n\r\nI don't have a strong feeling about whether we need to provide the versions that return a vector of counters, in the API.\r\n\r\n"
      },
      {
        "user": "ahedberg",
        "created_at": "2019-04-30T14:45:41Z",
        "body": "I think adding `Counter* const getCounter(const std::string& name) const;` and friends for gauges and histograms to the scope, which will populate the underlying cache but not create new stats, makes sense and is worth investing time in. Otherwise I suspect we'll see folks reinventing the \"iterate over all counters to find the one(s) we care about\" loop.\r\n\r\nI'll move forward with that plan if that sounds good to you.\r\n\r\n"
      },
      {
        "user": "jmarantz",
        "created_at": "2019-04-30T14:50:14Z",
        "body": "SGTM overall, modulo 2 things:\r\n * can you make the implementation take a Stats::StatName rather than a const std::string& name? All new calls should use StatName.\r\n * Note that the implementation of this in thread_local_store.cc will likely merge-conflict with @fredlas #5910 . That's not a reason not to do it, we should just be aware of it. Hopefully that will merge before too long."
      },
      {
        "user": "ahedberg",
        "created_at": "2019-05-06T21:04:31Z",
        "body": "So, fun discovery after a few days of debugging: `BaseIntegrationTest` has an `Api` object that it uses for various things, including constructing an `IntegrationTestServerImpl`. However, the `IntegrationTestServerImpl` creates its own stats store, symbol table, etc., completely ignoring whatever its `Api` was constructed with. This is fine...until you're writing an integration test and are trying to read stats out of the `Api`'s scope, only to find that they're stored somewhere else.\r\n\r\ntl;dr: Yak-shaving continues, but I haven't dropped this."
      }
    ]
  },
  {
    "number": 6633,
    "title": "Using request_headers_to_ad to set Host header, the port value is removed!?",
    "created_at": "2019-04-18T09:13:31Z",
    "closed_at": "2019-04-19T16:00:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6633",
    "body": "I'm trying to perform this configuration:\r\n`        request_headers_to_add:\r\n            - header:\r\n                key: \"Host\"\r\n                value: \"127.0.0.1:10000\"\r\n              append: false\r\n`\r\nBut on the http request the \"Host\" header lost the value port:\r\n\"Host\": \"127.0.0.1\"\r\nIs this an intentional Envoy behavior? Or I'm using a wrong configuration?\r\nThanks in advance.\r\nAlessandro",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6633/comments",
    "author": "alessandro-vincelli",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-04-18T12:19:48Z",
        "body": "I think you probably want to use `host_rewrite` instead of `request_headers_to_add`. The `host` header is special and has a bunch of interactions with HTTP connection manager and the codecs."
      },
      {
        "user": "alessandro-vincelli",
        "created_at": "2019-04-19T16:00:33Z",
        "body": "Thanks hutch, Evoy it works correctly, my fault, the header was override by another system."
      }
    ]
  },
  {
    "number": 6600,
    "title": "Change headers after encodeData.",
    "created_at": "2019-04-16T11:00:24Z",
    "closed_at": "2019-05-23T15:52:28Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6600",
    "body": "Can I change HTTP headers like \"Content-Length\" after encodeHeaders?\r\n\r\n The length changed after encodeData, but I can't change headers even if I stored the pointer of headers' reference passed in encodeHeaders().\r\n\r\nI found a similar scene in gzip_filter, which remove the Content-Length in encodeHeaders(). But this will cause chunked http response. So, what can I do change the header according to body. \r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6600/comments",
    "author": "panzhongxian",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-04-16T15:13:20Z",
        "body": "@mattklein123 @alyssawilk is there a clean way to do this?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-04-16T15:45:37Z",
        "body": "Once you call `encodeHeaders()` the headers may be written to the wire, so they cannot be changed after that fact. If you need to change the headers you need to buffer until you know the final header values and then they can be continued."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-16T15:52:23Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-23T15:52:27Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6598,
    "title": "data-plane-api lack tracing.operation_name: ingress",
    "created_at": "2019-04-16T07:11:51Z",
    "closed_at": "2019-04-24T02:40:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6598",
    "body": "***Title***: *data-plane-api lack tracing.operation_name: ingress*\r\n\r\n***Description*:**\r\n>When I send Ads to all local envoy dynamiclly，I find that lacking tracing.operation_name: ingress\r\n\r\n***Repro steps*:**\r\n```\r\nimport (  \r\n        ...\r\n    http_conn_manager \"github.com/envoyproxy/go-control-plane/envoy/config/filter/network/http_connection_manager/v2\"\r\n        ...\r\n)\r\n    listenFilterHttpConn.Tracing = &http_conn_manager.HttpConnectionManager_Tracing{\r\n        OperationName:  http_conn_manager.INGRESS,  \r\n        RandomSampling: &_type.Percent{Value: 1.0},\r\n    }\r\n\r\n    listenFilterHttpConnConv, err := util.MessageToStruct(listenFilterHttpConn)\r\n```\r\n\r\n***Config dump*:**\r\nI expect \r\n```\r\n    tracing:\r\n        operation_name: ingress\r\n        random_sampling: 1.0\r\n``` \r\n\r\nbut  in fact, as below\r\n```\r\n    tracing:\r\n        random_sampling: 1.0\r\n```\r\n\r\n\r\n***Guess*:**\r\nI guess， maybe  \r\n```\r\nOperationName HttpConnectionManager_Tracing_OperationName `protobuf:\"varint,1,opt,name=operation_name,json=operationName,proto3,enum=envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager_Tracing_OperationName\" json:\"operation_name,omitempty\"`\r\n```\r\n`omitempty` cause this promble, becase `http_conn_manager.INGRESS` is 0 in actually. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6598/comments",
    "author": "chainhelen",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-04-16T15:15:12Z",
        "body": "@chainhelen that sounds right; is this an actual problem for you or were you curious as to why it wasn't appearing?"
      },
      {
        "user": "chainhelen",
        "created_at": "2019-04-23T05:07:54Z",
        "body": "@htuch Thx for your reply.\r\n\r\nI expect\r\n```\r\n    tracing:\r\n        operation_name: ingress\r\n        random_sampling: 1.0\r\n```\r\nbut in fact, as below\r\n```\r\n    tracing:\r\n        random_sampling: 1.0\r\n```\r\nEn, I just care，does this affect the final effect for envoy?"
      },
      {
        "user": "htuch",
        "created_at": "2019-04-23T15:45:10Z",
        "body": "@chainhelen no, Envoy will fill in the default when the proto is parsed from the wire (or YAML)."
      },
      {
        "user": "chainhelen",
        "created_at": "2019-04-24T02:40:34Z",
        "body": "@htuch ok, get it.Thx for your reply"
      }
    ]
  },
  {
    "number": 6522,
    "title": "Network filter ext-authz gRPC not adding header",
    "created_at": "2019-04-09T13:07:58Z",
    "closed_at": "2019-05-22T10:50:05Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6522",
    "body": "Hi all!\r\n\r\n*Title*: *Network filter ext-authz gRPC not adding header*\r\n\r\n*Description*:\r\n>I am using Envoy as sidecar of one of my pods in order to contact an external gRPC service for authorization and proxy the http flow to different clusters depending on a header added by the Authorization server.\r\n>For the auth logic I am using the network filter **ext-authz gRPC** and I created a very simple gRPC Auth service using as base your proto files. The problem comes when I cannot see the header added in the response from the auth server. I guess I am not adding the header properly (it is my first time playing with RPC), so I would appreciate if you could tell me if I am understanding properly how to build the CheckResponse object.\r\n> I was using Envoy v1.9.0 but the same is happening with v1.10.0\r\n\r\n**Dockerfile**:\r\n```\r\nFROM envoyproxy/envoy:v1.9.0\r\nCMD envoy -c /etc/envoy/envoy.yaml -l debug --component-log-level \"grpc:trace,router:trace\"\r\n```\r\n\r\nThis is my **envoy.yaml**:\r\n```\r\n    admin:\r\n      access_log_path: /tmp/admin_access.log\r\n      address:\r\n        socket_address: { address: 0.0.0.0, port_value: 9901 }\r\n\r\n    static_resources:\r\n      listeners:\r\n      - name: listener_0\r\n        address:\r\n          socket_address:\r\n            address: 0.0.0.0\r\n            port_value: 8888\r\n        filter_chains:\r\n          - filters:\r\n            - name: envoy.ext_authz\r\n              config:\r\n                stat_prefix: ext_authz\r\n                grpc_service:\r\n                  envoy_grpc:\r\n                    cluster_name: ext-authz-grpc\r\n            - name: envoy.http_connection_manager\r\n              config:\r\n                codec_type: auto\r\n                stat_prefix: ingress_http\r\n                access_log:\r\n                  - name: envoy.file_access_log\r\n                    config:\r\n                      path: \"/var/log/access.log\"\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: backend\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match:\r\n                            prefix: \"/\"\r\n                            headers:\r\n                              name: \"next_hop\"\r\n                              exact_match: \"human\"\r\n                          route:\r\n                            cluster: human\r\n                        - match:\r\n                            prefix: \"/\"\r\n                            headers:\r\n                              name: \"next_hop\"\r\n                              exact_match: \"cat\"\r\n                          route:\r\n                            cluster: cat\r\n                http_filters:\r\n                  - name: envoy.router\r\n      clusters:\r\n      - name: human\r\n        connect_timeout: 0.25s\r\n        type: strict_dns\r\n        http2_protocol_options:  {}\r\n        hosts: [{ socket_address: { address: human, port_value: 8080 }}]\r\n      - name: cat\r\n        connect_timeout: 0.25s\r\n        type: strict_dns\r\n        http2_protocol_options:  {}\r\n        hosts: [{ socket_address: { address: cat, port_value: 8080 }}]\r\n      - name: ext-authz-grpc\r\n        connect_timeout: 0.25s\r\n        type: strict_dns\r\n        http2_protocol_options:  {}\r\n        hosts: [{ socket_address: { address: authz-grpc, port_value: 50051 }}]\r\n```\r\n\r\nAnd this is the code I did in the Auth server:\r\n```\r\n  static class AuthorizationImpl extends AuthorizationImplBase {\r\n\r\n    @Override\r\n    public void check(CheckRequest req, StreamObserver<CheckResponse> responseObserver) {\r\n      Status statusOk = Status.newBuilder().setCode(Code.OK_VALUE).build();\r\n      HeaderValue header = HeaderValue.newBuilder().setKey(\"next_hop\").setValue(\"cat\").build();\r\n      HeaderValueOption newHeader = HeaderValueOption.newBuilder().setAppend(BoolValue.of(true)).setHeader(header).build();\r\n      OkHttpResponse httpResponse = OkHttpResponse.newBuilder().addHeaders(newHeader).build();\r\n      CheckResponse response = CheckResponse.newBuilder()\r\n        .setStatus(statusOk)\r\n        .setOkResponse(httpResponse)\r\n        .build();\r\n\r\n      responseObserver.onNext(response);\r\n      responseObserver.onCompleted();\r\n    }\r\n  }\r\n```\r\n\r\nAs you can see, I am authorizing all the queries and adding the header \"**next_hop**\" with the hardcoded value \"cat\". The purpose is to add the header and see how envoy proxies the connection to the cat cluster.\r\n\r\nTaking a look to the logs from Envoy:\r\n```\r\n2019-04-09 10:45:17.311][000016][debug][router] [source/common/router/router.cc:270] [C0][S18166035339641235100] cluster 'ext-authz-grpc' match for URL '/envoy.service.auth.v2alpha.Authorization/Check'\r\n[2019-04-09 10:45:17.312][000016][debug][router] [source/common/router/router.cc:328] [C0][S18166035339641235100] router decoding headers:\r\n':method', 'POST'\r\n':path', '/envoy.service.auth.v2alpha.Authorization/Check'\r\n':authority', 'ext-authz-grpc'\r\n':scheme', 'http'\r\n'te', 'trailers'\r\n'grpc-timeout', '200m'\r\n'content-type', 'application/grpc'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '192.168.0.9'\r\n'x-envoy-expected-rq-timeout-ms', '200'\r\n\r\n[2019-04-09 10:45:17.312][000016][debug][client] [source/common/http/codec_client.cc:26] [C1] connecting\r\n[2019-04-09 10:45:17.312][000016][debug][connection] [source/common/network/connection_impl.cc:634] [C1] connecting to 10.111.95.67:50051\r\n[2019-04-09 10:45:17.312][000016][debug][connection] [source/common/network/connection_impl.cc:643] [C1] connection in progress\r\n[2019-04-09 10:45:17.312][000016][debug][http2] [source/common/http/http2/codec_impl.cc:721] [C1] setting stream-level initial window size to 268435456\r\n[2019-04-09 10:45:17.312][000016][debug][http2] [source/common/http/http2/codec_impl.cc:743] [C1] updating connection-level initial window size to 268435456\r\n[2019-04-09 10:45:17.312][000016][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2019-04-09 10:45:17.312][000016][trace][router] [source/common/router/router.cc:913] [C0][S18166035339641235100] buffering 50 bytes\r\n[2019-04-09 10:45:17.312][000016][debug][connection] [source/common/network/connection_impl.cc:516] [C1] connected\r\n[2019-04-09 10:45:17.312][000016][debug][client] [source/common/http/codec_client.cc:64] [C1] connected\r\n[2019-04-09 10:45:17.312][000016][debug][pool] [source/common/http/http2/conn_pool.cc:83] [C1] creating stream\r\n[2019-04-09 10:45:17.312][000016][debug][router] [source/common/router/router.cc:1023] [C0][S18166035339641235100] pool ready\r\n[2019-04-09 10:45:17.513][000016][debug][router] [source/common/router/router.cc:462] [C0][S18166035339641235100] upstream timeout\r\n[2019-04-09 10:45:17.513][000016][debug][router] [source/common/router/router.cc:968] [C0][S18166035339641235100] resetting pool request\r\n[2019-04-09 10:45:17.513][000016][debug][client] [source/common/http/codec_client.cc:105] [C1] request reset\r\n[2019-04-09 10:45:17.513][000016][debug][pool] [source/common/http/http2/conn_pool.cc:222] [C1] destroying stream: 0 remaining\r\n[2019-04-09 10:45:17.513][000016][debug][http2] [source/common/http/http2/codec_impl.cc:518] [C1] sent reset code=0\r\n[2019-04-09 10:45:17.513][000016][debug][http2] [source/common/http/http2/codec_impl.cc:563] [C1] stream closed: 0\r\n[2019-04-09 10:45:17.517][000016][debug][http] [source/common/http/async_client_impl.cc:93] async http request response headers (end_stream=true):\r\n':status', '200'\r\n'content-type', 'application/grpc'\r\n'grpc-status', '14'\r\n'grpc-message', 'upstream request timeout'\r\n\r\n[2019-04-09 10:45:17.517][000016][debug][connection] [source/common/network/connection_impl.cc:101] [C0] closing data_to_write=0 type=1\r\n[2019-04-09 10:45:17.517][000016][debug][connection] [source/common/network/connection_impl.cc:183] [C0] closing socket: 1\r\n[2019-04-09 10:45:17.517][000016][debug][main] [source/server/connection_handler_impl.cc:68] [C0] adding to cleanup list\r\n[2019-04-09 10:45:18.388][000007][debug][main] [source/server/server.cc:143] flushing stats\r\n```\r\n\r\nI can see the filter is calling my Auth service and it is answering 200 OK, but I cannot see the \"next_hop\" header. Could you please tell me if I am adding the header properly in the CheckResponse object?\r\n\r\nThanks a lot.\r\n\\Mariano.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6522/comments",
    "author": "kikogolk",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-04-09T13:54:34Z",
        "body": "@gsagula "
      },
      {
        "user": "gsagula",
        "created_at": "2019-04-09T16:18:18Z",
        "body": "@kikogolk This is a problem with the append logic. If the header that you are trying to append is already part of the request, it will work, otherwise filter won't create a new header. I already have a fix for that, but I still need to open a proper PR. Meanwhile, you can try `HeaderValueOption.newBuilder().setAppend(BoolValue.of(false))` if append is not important in your case.\r\n\r\nPlease, let me know if it works."
      },
      {
        "user": "kikogolk",
        "created_at": "2019-04-10T08:20:02Z",
        "body": "Thanks @gsagula for you quick answer!\r\n\r\nI have tested your proposal (setting append to false) and I am afraid the header is still not there. These are the logs using Envoy v1.10.0 (sorry because I pasted a wrong log in my first message, in that log the grpc was answering timeout. I guess I have to tune the timeout values because the first attemp is always returning timeout):\r\n\r\n```\r\n[2019-04-10 08:12:51.389][13][debug][router] [source/common/router/router.cc:320] [C0][S3071285557094235203] cluster 'ext-authz-grpc' match for URL '/envoy.service.auth.v2.Authorization/Check'\r\n[2019-04-10 08:12:51.389][13][debug][router] [source/common/router/router.cc:381] [C0][S3071285557094235203] router decoding headers:\r\n':method', 'POST'\r\n':path', '/envoy.service.auth.v2.Authorization/Check'\r\n':authority', 'ext-authz-grpc'\r\n':scheme', 'http'\r\n'te', 'trailers'\r\n'grpc-timeout', '200m'\r\n'content-type', 'application/grpc'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '192.168.0.9'\r\n'x-envoy-expected-rq-timeout-ms', '200'\r\n\r\n[2019-04-10 08:12:51.389][13][debug][client] [source/common/http/codec_client.cc:26] [C3] connecting\r\n[2019-04-10 08:12:51.389][13][debug][connection] [source/common/network/connection_impl.cc:644] [C3] connecting to 10.98.128.244:50051\r\n[2019-04-10 08:12:51.390][13][debug][connection] [source/common/network/connection_impl.cc:653] [C3] connection in progress\r\n[2019-04-10 08:12:51.390][13][debug][http2] [source/common/http/http2/codec_impl.cc:735] [C3] setting stream-level initial window size to 268435456\r\n[2019-04-10 08:12:51.390][13][debug][http2] [source/common/http/http2/codec_impl.cc:757] [C3] updating connection-level initial window size to 268435456\r\n[2019-04-10 08:12:51.390][13][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2019-04-10 08:12:51.390][13][trace][router] [source/common/router/router.cc:1058] [C0][S3071285557094235203] buffering 50 bytes\r\n[2019-04-10 08:12:51.390][13][debug][connection] [source/common/network/connection_impl.cc:517] [C3] connected\r\n[2019-04-10 08:12:51.390][13][debug][client] [source/common/http/codec_client.cc:64] [C3] connected\r\n[2019-04-10 08:12:51.390][13][debug][pool] [source/common/http/http2/conn_pool.cc:96] [C3] creating stream\r\n[2019-04-10 08:12:51.390][13][debug][router] [source/common/router/router.cc:1165] [C0][S3071285557094235203] pool ready\r\n[2019-04-10 08:12:51.402][13][debug][router] [source/common/router/router.cc:717] [C0][S3071285557094235203] upstream headers complete: end_stream=false\r\n[2019-04-10 08:12:51.402][13][debug][http] [source/common/http/async_client_impl.cc:94] async http request response headers (end_stream=false):\r\n':status', '200'\r\n'content-type', 'application/grpc'\r\n'grpc-encoding', 'identity'\r\n'grpc-accept-encoding', 'gzip'\r\n'x-envoy-upstream-service-time', '12'\r\n\r\n[2019-04-10 08:12:51.402][13][debug][client] [source/common/http/codec_client.cc:95] [C3] response complete\r\n[2019-04-10 08:12:51.402][13][debug][pool] [source/common/http/http2/conn_pool.cc:237] [C3] destroying stream: 0 remaining\r\n[2019-04-10 08:12:51.402][13][debug][http] [source/common/http/async_client_impl.cc:109] async http request response trailers:\r\n'grpc-status', '0'\r\n\r\n[2019-04-10 08:12:51.402][13][debug][http] [source/common/http/conn_manager_impl.cc:243] [C2] new stream\r\n[2019-04-10 08:12:51.402][13][debug][http] [source/common/http/conn_manager_impl.cc:580] [C2][S18017938375058352559] request headers complete (end_stream=true):\r\n':authority', '10.109.131.210:8080'\r\n':path', '/'\r\n':method', 'GET'\r\n'cache-control', 'no-cache'\r\n'postman-token', '9087dbf5-bf75-4791-9fa6-465ea6af27b6'\r\n'user-agent', 'PostmanRuntime/7.6.1'\r\n'accept', '*/*'\r\n'accept-encoding', 'gzip, deflate'\r\n'connection', 'keep-alive'\r\n\r\n[2019-04-10 08:12:51.402][13][debug][http] [source/common/http/conn_manager_impl.cc:1037] [C2][S18017938375058352559] request end stream\r\n[2019-04-10 08:12:51.402][13][debug][router] [source/common/router/router.cc:277] [C2][S18017938375058352559] no cluster match for URL '/'\r\n[2019-04-10 08:12:51.402][13][debug][http] [source/common/http/conn_manager_impl.cc:1278] [C2][S18017938375058352559] encoding headers via codec (end_stream=true):\r\n':status', '404'\r\n'date', 'Wed, 10 Apr 2019 08:12:51 GMT'\r\n'server', 'envoy'\r\n```"
      },
      {
        "user": "kikogolk",
        "created_at": "2019-04-10T17:08:33Z",
        "body": "Hi!\r\n\r\nI was playing a little today with your filter code and adding some logs to trace how it was working (the ones with KIKOGOLK string). I do not have too much experience coding in C++ and Bazel, but finally I could check that the header is in the CheckResponse object:\r\n\r\n```\r\n\r\n\r\n[2019-04-10 16:55:04.216][13][debug][config] [source/extensions/filters/common/ext_authz/ext_authz_grpc_impl.cc:26] KIKOGOLK >>> CREATING GrpcClientImpl\r\n[2019-04-10 16:55:04.216][13][debug][main] [source/server/connection_handler_impl.cc:257] [C2] new connection\r\n[2019-04-10 16:55:04.217][13][debug][config] [source/extensions/filters/common/ext_authz/ext_authz_grpc_impl.cc:43] KIKOGOLK >>> GrpcClientImpl::check()\r\n[2019-04-10 16:55:04.217][13][debug][router] [source/common/router/router.cc:320] [C0][S6735508158765147663] cluster 'ext-authz-grpc' match for URL '/envoy.service.auth.v2.Authorization/Check'\r\n[2019-04-10 16:55:04.217][13][debug][router] [source/common/router/router.cc:381] [C0][S6735508158765147663] router decoding headers:\r\n':method', 'POST'\r\n':path', '/envoy.service.auth.v2.Authorization/Check'\r\n':authority', 'ext-authz-grpc'\r\n':scheme', 'http'\r\n'te', 'trailers'\r\n'grpc-timeout', '200m'\r\n'content-type', 'application/grpc'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '192.168.0.10'\r\n'x-envoy-expected-rq-timeout-ms', '200'\r\n\r\n[2019-04-10 16:55:04.217][13][debug][client] [source/common/http/codec_client.cc:26] [C3] connecting\r\n[2019-04-10 16:55:04.217][13][debug][connection] [source/common/network/connection_impl.cc:644] [C3] connecting to 10.106.153.146:50051\r\n[2019-04-10 16:55:04.217][13][debug][connection] [source/common/network/connection_impl.cc:653] [C3] connection in progress\r\n[2019-04-10 16:55:04.217][13][debug][http2] [source/common/http/http2/codec_impl.cc:735] [C3] setting stream-level initial window size to 268435456\r\n[2019-04-10 16:55:04.217][13][debug][http2] [source/common/http/http2/codec_impl.cc:757] [C3] updating connection-level initial window size to 268435456\r\n[2019-04-10 16:55:04.217][13][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2019-04-10 16:55:04.217][13][trace][router] [source/common/router/router.cc:1058] [C0][S6735508158765147663] buffering 51 bytes\r\n[2019-04-10 16:55:04.217][13][debug][connection] [source/common/network/connection_impl.cc:517] [C3] connected\r\n[2019-04-10 16:55:04.217][13][debug][client] [source/common/http/codec_client.cc:64] [C3] connected\r\n[2019-04-10 16:55:04.217][13][debug][pool] [source/common/http/http2/conn_pool.cc:96] [C3] creating stream\r\n[2019-04-10 16:55:04.217][13][debug][router] [source/common/router/router.cc:1165] [C0][S6735508158765147663] pool ready\r\n[2019-04-10 16:55:04.230][13][debug][router] [source/common/router/router.cc:717] [C0][S6735508158765147663] upstream headers complete: end_stream=false\r\n[2019-04-10 16:55:04.231][13][debug][http] [source/common/http/async_client_impl.cc:94] async http request response headers (end_stream=false):\r\n':status', '200'\r\n'content-type', 'application/grpc'\r\n'grpc-encoding', 'identity'\r\n'grpc-accept-encoding', 'gzip'\r\n'x-envoy-upstream-service-time', '13'\r\n\r\n[2019-04-10 16:55:04.231][13][debug][client] [source/common/http/codec_client.cc:95] [C3] response complete\r\n[2019-04-10 16:55:04.231][13][debug][pool] [source/common/http/http2/conn_pool.cc:237] [C3] destroying stream: 0 remaining\r\n[2019-04-10 16:55:04.231][13][debug][http] [source/common/http/async_client_impl.cc:109] async http request response trailers:\r\n'grpc-status', '0'\r\n\r\n[2019-04-10 16:55:04.231][13][debug][config] [source/extensions/filters/common/ext_authz/ext_authz_grpc_impl.cc:53] KIKOGOLK >>> GrpcClientImpl::onSuccess()\r\n[2019-04-10 16:55:04.231][13][debug][config] [source/extensions/filters/common/ext_authz/ext_authz_grpc_impl.cc:56] KIKOGOLK >>> GrpcClientImpl::onSuccess() - GRPC STATUS OK\r\n[2019-04-10 16:55:04.231][13][debug][config] [source/extensions/filters/common/ext_authz/ext_authz_grpc_impl.cc:60] KIKOGOLK >>> GrpcClientImpl::onSuccess() - HAS OK RESPONSE\r\n[2019-04-10 16:55:04.231][13][debug][config] [source/extensions/filters/common/ext_authz/ext_authz_grpc_impl.cc:93] KIKOGOLK >>> GrpcClientImpl::toAuthzResponseHeader()\r\n[2019-04-10 16:55:04.231][13][debug][config] [source/extensions/filters/common/ext_authz/ext_authz_grpc_impl.cc:95] KIKOGOLK >>> header [next_hop, cat]\r\n[2019-04-10 16:55:04.231][13][debug][config] [source/extensions/filters/common/ext_authz/ext_authz_grpc_impl.cc:101] KIKOGOLK >>> GrpcClientImpl::toAuthzResponseHeader() - Adding the header\r\n[2019-04-10 16:55:04.231][13][debug][http] [source/common/http/conn_manager_impl.cc:243] [C2] new stream\r\n[2019-04-10 16:55:04.231][13][debug][http] [source/common/http/conn_manager_impl.cc:580] [C2][S1913616105050349090] request headers complete (end_stream=true):\r\n':authority', '10.103.22.144:8080'\r\n':path', '/'\r\n':method', 'GET'\r\n'cache-control', 'no-cache'\r\n'postman-token', '3eb60a26-880f-4d91-be0e-12ddacbb8c3e'\r\n'user-agent', 'PostmanRuntime/7.6.1'\r\n'accept', '*/*'\r\n'accept-encoding', 'gzip, deflate'\r\n'connection', 'keep-alive'\r\n\r\n[2019-04-10 16:55:04.231][13][debug][http] [source/common/http/conn_manager_impl.cc:1037] [C2][S1913616105050349090] request end stream\r\n[2019-04-10 16:55:04.231][13][debug][router] [source/common/router/router.cc:277] [C2][S1913616105050349090] no cluster match for URL '/'\r\n[2019-04-10 16:55:04.231][13][debug][http] [source/common/http/conn_manager_impl.cc:1278] [C2][S1913616105050349090] encoding headers via codec (end_stream=true):\r\n':status', '404'\r\n'date', 'Wed, 10 Apr 2019 16:55:03 GMT'\r\n'server', 'envoy'\r\n```\r\n\r\nI can see in the logs that the header is in the response, when `onSuccess` method is called.\r\n\r\nFrom here I am a little lost. Hope this can help you.\r\n\r\nThanks!\r\n\r\n\r\n"
      },
      {
        "user": "gsagula",
        "created_at": "2019-04-10T18:22:41Z",
        "body": "@kikogolk This helps, thanks! I'm stuck with a bunch of other stuff right now. I can take a look at this a bit later. Please, feel free to ping me if for some reason I forget to get back to you."
      },
      {
        "user": "kikogolk",
        "created_at": "2019-04-15T10:01:09Z",
        "body": "Hi!\r\n\r\nNo problem @gsagula, it was not my intention to rush you. I will update the ticket with my findings and you can take a look when you want.\r\n\r\nI realized that I could not see any HTTP Request data inside the CheckRequest object so, after thinking a little, I realized this could be due to I was using the gRPC Ext Auth filter in layer 3. It makes sense. So probably I did my testing not properly and this ticket could be closed.\r\n\r\nAnyway, I changed my Envoy's configuration and I put the gRPC Ext Auth filter as part of HTTP Connection Manager Filter (I was checking another open issue you are working on it #6481). Now I can see the HTTP Request inside the CheckRequest gRPC message object and I can see the header I added inside the Response. But, as issue #6481 declares, routing cannot be done using headers added by the Auth Server.\r\n\r\nThanks a lot!\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-15T10:04:22Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-22T10:50:03Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6493,
    "title": "Access listener metadata from lua script",
    "created_at": "2019-04-05T19:24:24Z",
    "closed_at": "2019-05-12T22:40:45Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6493",
    "body": "\r\n*Description*:\r\n> I would like to know if it is possible to access to the defined listener metadata from a lua script.\r\n\r\n*Config*:\r\n>\r\n``` static_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    metadata:\r\n      filter_metadata:\r\n        envoy.lua:\r\n          xxx: yyy <- access to this config\r\n    filter_chains:\r\n    - filters: \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6493/comments",
    "author": "ajzach",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-05-05T22:18:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-12T22:40:44Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6486,
    "title": "Upstream request header modification",
    "created_at": "2019-04-05T14:17:09Z",
    "closed_at": "2019-05-19T02:15:44Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6486",
    "body": "I would like to have a way of modifying upstream request headers _including_ `host/:authority`.\r\nI can alter headers via `request_headers_to_add` or lua code, but changing `host/:authority` is done before virtual host matching and so it changes virtual host picked, or results in 404.\r\nI need virtual host to be picked using downstream request's `host/:authority` and only then alter headers sent to upstream.\r\n\r\nWhy do I need it: I would like to multiplex traffic from many services to envoy through single listener with one virtual host per service. Envoy is not aware of all domains (with possible subdomains) of these services, so I would substitute original host with known per service vhost name storing original host in some other header first. Envoy selects exact virtual hosts efficiently with a map and so I would like not to use wildcard domains, or long route chains matching custom headers.\r\n\r\nExample of traffic:\r\n```\r\nOriginal request      | Request from load balancer to Envoy         | Request from Envoy to upstream\r\n------------------------------------------------------------------------------------------------------\r\nHost: a.example.com   | Host: a,  X-Original-Host: a.example.com    | vhost a -> Host: a.example.com\r\nHost: b.example.com   | Host: b,  X-Original-Host: b.example.com    | vhost b -> Host: b.example.com\r\nHost: a.c.example.com | Host: c,  X-Original-Host: a.c.example.com  | vhost c -> Host: a.c.example.com\r\nHost: b.c.example.com | Host: c,  X-Original-Host: b.c.example.com  | vhost c -> Host: b.c.example.com\r\n```\r\nIs it possible?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6486/comments",
    "author": "bartebor",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-04-05T16:33:54Z",
        "body": "I think this might be possible with a custom filter that does an internal redirect. @alyssawilk WDYT?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-04-08T19:12:26Z",
        "body": "I'm not 100% sure I'm following the request.  Is the problem that by the time the request reaches Envoy it's doing matching based on a, where you want to do a.example.com, or you want to pick on a but forward a.example.com?   You can definitely swap the host with a custom filter, but the question of if you need an internal redirect or just a catch-all service and to rewrite authority and clear/reset the cached route depends on what you're aiming for."
      },
      {
        "user": "bartebor",
        "created_at": "2019-04-09T06:57:09Z",
        "body": "@alyssawilk If I understood you correctly, it is the latter scenario you describe: I want Envoy to match virtual host using `a`, but forward `a.example.com` to upstream service.\r\n\r\nWe hava a (non-Envoy) load balancer handling many services on multiple IPs. This LB should forward traffic to Envoy, which in turn handles multiple upstreams (clusters) in kubernetes cluster. I would like to have a simple, but efficient interface between LB and Envoy, avoiding maintaning multiple listeners, multitude of service domains etc. at the Envoy's layer.\r\n\r\nI thought it would be best to assign each application fixed domain for LB to store in `host/:authority` so Envoy could select virtual host efficiently. At the same time I would like to preserve original domain in a way that upstream services relying on `host/:authority` continue to work with no changes."
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-04-09T13:32:24Z",
        "body": "Using HTTP/1 semantics just for simplicity (swap :authority for H2) I think if your LB is fowarding to Envoy with \r\nHost: a\r\nAnd you want to route select on \r\nHost: a\r\nand then forward\r\nHost: a.example.com\r\nyou can just configure Envoy to route based on A, and add a custom filter which on decodeHeaders replaces the authority from the x-original-host header.  Alternately if there aren't infinite combinations and permutations you might be able to configure host_rewrite rules and skip the custom filter but I'm not terribly familiar with that feature so unsure if it'd meet your needs."
      },
      {
        "user": "bartebor",
        "created_at": "2019-04-10T05:40:55Z",
        "body": "@alyssawilk Thank you for your response. Unfortunately neither `host_rewrite` nor `auto_host_rewrite` meet my needs, because of using fixed values. If only some kind of `host_rewrite_header` option existed, I would have what I need. Do you think it is reasonable to implement? If not, I will probably have to follow custom filter path.\r\n"
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-04-10T13:27:58Z",
        "body": "Hm, I think it would be generically useful but also somewhat scary from a security perspective.  @envoyproxy/senior-maintainers thoughts on this?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-04-11T18:37:23Z",
        "body": "Couldn't this be done in a custom filter?"
      },
      {
        "user": "bartebor",
        "created_at": "2019-04-11T21:18:30Z",
        "body": "I will try creating custom filter when I find some time, but still it seems that copying/composing upstream headers (including authority) from other headers would be very handy feature to have in vanilla Envoy. I agree that allowing modification of authority header may pose some security risk, but there are other envoy configuration options which may be misused or abused. I think it is administrator's responsibility to configure their Envoy instance in a safe manner in given environment."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-11T21:22:54Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-19T02:15:43Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "bartebor",
        "created_at": "2019-06-07T14:15:42Z",
        "body": "I have finally found some time to tackle this.\r\n\r\nIt seems that it is not possible to achieve described scenario with custom filter. The problem is that route filter contains whole process of selecting vhost and forwarding request to upstream and must be the last filter on http connection manager's filter list. \r\n\r\nAny :authority/host header modification done with custom filter executing before route filter will affect selected vhost. At the same time there is no way to hook into `RouteEntryImplBase::finalizeRequestHeaders`. Existing header alteration methods either do not allow to modify :authority/host header (`request_header_to_add`) or use constant strings only (`host_rewrite`, `auto_host_rewrite`).\r\n\r\nInternal redirect won't work because of above reasons (and it needs separate service to supply 302s, what seems like a not a very good idea in terms of performance).\r\n\r\nI decided to add another rewrite option, `auto_host_rewrite_header` and I will open a PR for evaluation. I made it clear in the docs that using this option requires special care.\r\n\r\n"
      },
      {
        "user": "markschmid",
        "created_at": "2019-10-18T09:59:53Z",
        "body": "I'm interested in accessing the original Host information at the cluster level, instead of having it rewritten by envoy. Actually I have not found a solution for this so far (is there really no way?).\r\n\r\nWill the following do the trick for me? (From the docs: \"If header value is empty, host header is left intact.\")\r\n```auto_host_rewrite_header: \"\"```\r\n\r\nFor some reason, I have not been able to get it to work, envoy (v1.11.2) fails to start, stating \r\n```auto_host_rewrite_header: Cannot find field.```"
      },
      {
        "user": "bartebor",
        "created_at": "2019-10-18T10:40:50Z",
        "body": "AFAIK envoy does not normally alter Host information unless you make it to; upstream request will have whatever Host came from downstream. My PR is about doing something opposite: I _want_ to alter Host being sent to cluster endpoint with other header's value. The field you used should contain this other header name, not it's value which is taken from downstream request."
      },
      {
        "user": "markschmid",
        "created_at": "2019-10-18T11:09:01Z",
        "body": "Thanks for the clarification, it was really helpful and helped me to realize that it is in fact a different component in my stack which has been altering the host information all this time..."
      },
      {
        "user": "randavidovitz",
        "created_at": "2020-12-07T22:48:58Z",
        "body": "@markschmid are you sure that the HOST is not manipulated ?"
      }
    ]
  },
  {
    "number": 6476,
    "title": "max_connections circuit breaker not always honored when using multiple priorities",
    "created_at": "2019-04-03T20:27:22Z",
    "closed_at": "2019-04-04T04:37:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6476",
    "body": "When a stream is created, the http1 connection pool first attempts to use an existing connection. If one is not available, then the max_pending and max_connections circuit breaker is consulted before creating a new connection.\r\n\r\nWhen using multiple priorities with different values for max_connections this can let the lower value exceed its circuit breaker:\r\n\r\nDEFAULT has max 10\r\nHIGH has max 100\r\n\r\nEnvoy gets a bunch of requests at once, depletes the 10 for the DEFAULT requests, but allow the HIGH requests to make new connections. At this point there are >10 available connections, allowing a higher than desired concurrency for DEFAULT requests. \r\n\r\nNote that this is mostly theoretical based on me reading the code, I could totally be wrong here. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6476/comments",
    "author": "snowp",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-04-03T20:29:43Z",
        "body": "I came over this trying to figure out why setting max_connections to 0 doesn't seem to do what I'd expect (fail all traffic). Guessing there's somehow a connection being created somewhere before checking the resource count, and since we don't look at it before assigning a request to an existing connection it will happily assign the request to the open connection."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-04-04T03:33:31Z",
        "body": "@snowp IIRC the different priorities use different resource managers, so I don't think they interact at all? Can you point me at the code you think is doing this?"
      },
      {
        "user": "snowp",
        "created_at": "2019-04-04T04:37:27Z",
        "body": "My bad - I missed the part that there is one connection pool per priority. My thinking was that there was a shared pool, so that connections could be created through quotas in one priority and then used in the other. \r\n\r\ntl;dr I didn't realize `priority_` in `conn_pool.cc` was a member variable. :) Sorry for the noise"
      }
    ]
  },
  {
    "number": 6471,
    "title": "Help with gRPC HTTP / 1.1 reverse bridge: grpc-message mapping.",
    "created_at": "2019-04-03T13:45:12Z",
    "closed_at": "2019-05-11T14:14:48Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6471",
    "body": "**Help with gRPC HTTP / 1.1 reverse bridge.**\r\n\r\n*Title*: *Help with gRPC HTTP / 1.1 reverse bridge: grpc-message mapping.*\r\n\r\n*Description*:\r\nI'm using the gRPC HTTP / 1.1 reverse bridge and I have the next doubt:\r\n\r\nIn case of error, (besides the **grpc-status** \"auto\" map) is there any way to map an error message to the trailer **grpc-message** when my upstream does not understand any gRPC semantics? \r\n\r\nI would like to avoid passing errors in the body and use the standard way.\r\n\r\nMany thanks in advance and excuse my ignorance.\r\n\r\n*Config*:\r\nThis is the config I'm using it and working correctly:\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 127.0.0.1\r\n      port_value: 9901\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          protocol: TCP\r\n          address: 0.0.0.0\r\n          port_value: 10000\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n                stat_prefix: ingress_http\r\n                codec_type: AUTO\r\n                route_config:\r\n                  name: backend\r\n                  virtual_hosts:\r\n                    - name: backend\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match: { prefix: \"/\" }\r\n                          route: { host_rewrite: nginx, cluster: backend, timeout: 59.99s }\r\n                http_filters:\r\n                  - name: envoy.filters.http.grpc_http1_reverse_bridge\r\n                    config:\r\n                      content_type: application/grpc+proto\r\n                      withhold_grpc_frames: true\r\n                  - name: envoy.router\r\n                    typed_config: {}\r\n  clusters:\r\n  - name: backend\r\n    connect_timeout: 59.99s\r\n    type: logical_dns\r\n    dns_lookup_family: v4_only\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: backend\r\n      endpoints:\r\n        - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: nginx\r\n                    port_value: 80\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6471/comments",
    "author": "sp-manuel-jurado",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-04-03T14:30:13Z",
        "body": "The filter already injects error messages into grpc-message when it receives an unsupported response. Are you talking about being able to respond with a custom message?"
      },
      {
        "user": "sp-manuel-jurado",
        "created_at": "2019-04-03T14:45:23Z",
        "body": "@snowp yes, I was referring to custom message error (even the possibility to customize grpc-status too).\r\n\r\nNote: my upstream does not understand any gRPC semantics, I'm using PHP."
      },
      {
        "user": "snowp",
        "created_at": "2019-04-03T15:21:03Z",
        "body": "To send a custom message you can just send a header only response (no body) with a header named `grpc-message` with your message. As log as the content-type matches what the filter expects it should just pass through the `grpc-message`.\r\n\r\nFor `grpc-status` we'd have to make some code changes. Making the filter just pass through the status if the upstream provided one sounds reasonable, would that work for you?"
      },
      {
        "user": "sp-manuel-jurado",
        "created_at": "2019-04-04T10:41:28Z",
        "body": "Hi @snowp \r\nI've checked grpc-message pass through using your directions and works as expected.\r\n\r\nFor example:\r\n\r\nHTTP/1.1 request/response (within content-type, content-length: 0, grpc-message headers set)\r\n```\r\n*   Trying ::1...\r\n* TCP_NODELAY set\r\n* Connected to localhost (::1) port 10000 (#0)\r\n> POST /TestAPI/TestMethod HTTP/1.1\r\n> Host: localhost:10000\r\n> User-Agent: curl/7.54.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 400 Bad Request\r\n< server: envoy\r\n< content-type: application/grpc+proto\r\n< content-length: 0\r\n< x-powered-by: PHP/7.2.15\r\n< grpc-message: custom message for 400 http status code\r\n< cache-control: no-cache, private\r\n< date: Thu, 04 Apr 2019 10:19:12 GMT\r\n< access-control-allow-origin:\r\n< x-envoy-upstream-service-time: 966\r\n<\r\n* Connection #0 to host localhost left intact\r\n```\r\n\r\ngRPC response status (as PHP array, sorry for that. I'm testing with PHP client too)\r\n```\r\n(\r\n    [metadata] => Array\r\n        (\r\n            [:status] => Array\r\n                (\r\n                    [0] => 400\r\n                )\r\n\r\n            [server] => Array\r\n                (\r\n                    [0] => envoy\r\n                )\r\n\r\n            [content-type] => Array\r\n                (\r\n                    [0] => application/grpc\r\n                )\r\n\r\n            [content-length] => Array\r\n                (\r\n                    [0] => 5\r\n                )\r\n\r\n            [x-powered-by] => Array\r\n                (\r\n                    [0] => PHP/7.2.15\r\n                )\r\n\r\n            [grpc-message] => Array\r\n                (\r\n                    [0] => custom message for 400 http status code\r\n                )\r\n\r\n            [cache-control] => Array\r\n                (\r\n                    [0] => no-cache, private\r\n                )\r\n\r\n            [date] => Array\r\n                (\r\n                    [0] => Thu, 04 Apr 2019 10:22:16 GMT\r\n                )\r\n\r\n            [access-control-allow-origin] => Array\r\n                (\r\n                    [0] =>\r\n                )\r\n\r\n            [x-envoy-upstream-service-time] => Array\r\n                (\r\n                    [0] => 1255\r\n                )\r\n\r\n        )\r\n\r\n    [code] => 1\r\n    [details] => Received http2 header with status: 400\r\n)\r\n```\r\n\r\nI can see:\r\n- http2 status (:status) set to 400\r\n- grpc custom message (grpc-message) set to \"custom message for 400 http status code\"\r\nThis is ok and very useful for me.\r\n\r\nBut I miss the grpc-status in metadata. Related to this: ¿Is it the standard behaviour of the filter (and only sets the grpc-status trailer when response content is not empty)? Or I'm doing or understanding something wrong.\r\n\r\nRelated to:\r\nsnowp: \"For grpc-status we'd have to make some code changes. Making the filter just pass through the status if the upstream provided one sounds reasonable, would that work for you?\"\r\nI guess this is related to what I said above. The standard mapping would work for me (in that case 11 (400 Bad Request)). If I would want a custom error I could add it into grpc-message using a protocolbuffer serialized as string or checking http2 generic \":status\".\r\n\r\nMany thanks in advance and apologize for the inconveniences.\r\n"
      },
      {
        "user": "sp-manuel-jurado",
        "created_at": "2019-04-04T10:54:33Z",
        "body": "Also, I've checked to add grpc-status as response header to do mapping manually and I get the next grpc status:\r\n\r\n```\r\n(\r\n    [metadata] => Array\r\n        (\r\n            [server] => Array\r\n                (\r\n                    [0] => envoy\r\n                )\r\n\r\n            [content-length] => Array\r\n                (\r\n                    [0] => 5\r\n                )\r\n\r\n            [x-powered-by] => Array\r\n                (\r\n                    [0] => PHP/7.2.15\r\n                )\r\n\r\n            [cache-control] => Array\r\n                (\r\n                    [0] => no-cache, private\r\n                )\r\n\r\n            [date] => Array\r\n                (\r\n                    [0] => Thu, 04 Apr 2019 10:46:08 GMT\r\n                )\r\n\r\n            [access-control-allow-origin] => Array\r\n                (\r\n                    [0] =>\r\n                )\r\n\r\n            [x-envoy-upstream-service-time] => Array\r\n                (\r\n                    [0] => 1293\r\n                )\r\n\r\n        )\r\n\r\n    [code] => 11\r\n    [details] => custom message for 400 http status code\r\n)\r\n```\r\n\r\nNow is not added in metadata but is added in code and details.\r\nIs this behaviour correct?\r\nOr I'm doing or understanding something wrong.\r\n\r\nI attach the headers example too (HTTP/1.1 request/response):\r\n\r\n```\r\n*   Trying ::1...\r\n* TCP_NODELAY set\r\n* Connected to localhost (::1) port 10000 (#0)\r\n> POST /TestAPI/TestMethod HTTP/1.1\r\n> Host: localhost:10000\r\n> User-Agent: curl/7.54.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 400 Bad Request\r\n< server: envoy\r\n< content-type: application/grpc+proto\r\n< content-length: 0\r\n< x-powered-by: PHP/7.2.15\r\n< grpc-message: custom message for 400 http status code\r\n< grpc-status: 11\r\n< cache-control: no-cache, private\r\n< date: Thu, 04 Apr 2019 10:52:22 GMT\r\n< access-control-allow-origin:\r\n< x-envoy-upstream-service-time: 1411\r\n<\r\n* Connection #0 to host localhost left intact\r\n```"
      },
      {
        "user": "snowp",
        "created_at": "2019-04-04T13:31:23Z",
        "body": "Ah yeah, looking over the code again we don't do anything fancy if it's a header only response, so it makes sense that grpc-status is propagated in that case. \r\n\r\nI would expect them to not show up in metadata since they are headers with special meaning in gRPC."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-04T13:34:30Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-11T14:14:47Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6456,
    "title": "upstream connect error or disconnect/reset before headers",
    "created_at": "2019-04-02T01:47:21Z",
    "closed_at": "2019-05-09T17:04:48Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6456",
    "body": "**Issue Template**\r\n\r\n*Title*: Connection issues with grpc server when using envoy with TLS\r\n\r\n*Description*:\r\n>I got a java client talking to a python grpc server. I deployed envoy to act as a load balancer. Everything was working after a little bit of tweaking when TLS is not enabled. Once I added TLS context, I started seeing ALPN negotiation failed issue. I added alpn_protocols:[h2] in my envoy config which pushed me to next blocker. Now I am seeing *RPC failed: Status{code=UNAVAILABLE, description=upstream connect error or disconnect/reset before headers, cause=null}* error. \r\nI got all these services (java client app, envoy, python server) running on OpenShift environment. \r\nMy config yaml:\r\n*\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 10000 }\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              require_tls: ALL\r\n              routes:\r\n              - match: { prefix: \"/\" }\r\n                route: { cluster: python-server, timeout: 60s}\r\n          http_filters:\r\n          - name: envoy.router\r\n      tls_context:\r\n        common_tls_context:\r\n          tls_certificates:\r\n            certificate_chain: { \"filename\": \"/envoy_server.cert\" }\r\n            private_key: { \"filename\": \"/envoy_server.key\" }\r\n          alpn_protocols: [ \"h2\" ]\r\n          validation_context:\r\n            trusted_ca:\r\n              filename: /python-server.cert.pem\r\n  clusters:\r\n  - name: python-server\r\n    connect_timeout: 15s\r\n    type: STRICT_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    http2_protocol_options: { }\r\n    lb_policy: LEAST_REQUEST\r\n    hosts: [{ socket_address: { address: python-server, port_value: 10000 }}]\r\n*\r\nAny pointers are appreciated",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6456/comments",
    "author": "kvkk",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-05-02T16:27:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-09T17:04:47Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "Duanzhiwei",
        "created_at": "2021-10-10T09:17:35Z",
        "body": "I encountered a similar problem, did you solve it?"
      }
    ]
  },
  {
    "number": 6451,
    "title": "Problems with subversion help",
    "created_at": "2019-04-01T18:41:00Z",
    "closed_at": "2019-05-09T00:04:47Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6451",
    "body": "Hello, so, I have problems when I put a svn subversion service behind the envoy. When I enter to subversion by web https I do not have any problem, but when I start to download things through the software tortoise svn the problem starts as the download begins correctly and after about 3 minutes the file transfer is cut.\r\n\r\n**My cluster config**\r\n```\r\n  - name: svn-cluster\r\n    connect_timeout: 300s\r\n    type: STRICT_DNS\r\n    hosts: [{ socket_address: { address: svn.example.net, port_value: 443 }}]\r\n    tls_context: { sni: svn.example.net, allow_renegotiation: true }\r\n```\r\nI'm not sure but I think it has something to do with the exchange of certificates.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6451/comments",
    "author": "barajas-victor",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-05-01T23:27:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-09T00:04:46Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6448,
    "title": "how to use envoy to proxy thrift request",
    "created_at": "2019-04-01T11:10:56Z",
    "closed_at": "2019-05-09T04:04:47Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6448",
    "body": "how can i use envoy to proxy thrift request, \r\nis there an example config for thrift?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6448/comments",
    "author": "zplinuxlover",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-04-01T15:49:39Z",
        "body": "@zuercher "
      },
      {
        "user": "zplinuxlover",
        "created_at": "2019-04-02T02:36:10Z",
        "body": "can you offer a detailed config file for envoy thrift?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-02T03:27:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-09T04:04:46Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "sribalakumar",
        "created_at": "2020-01-27T12:56:26Z",
        "body": "@zplinuxlover Any luck on this ? Will help if there is any example configuration."
      }
    ]
  },
  {
    "number": 6427,
    "title": "circuit breaking support grpc?",
    "created_at": "2019-03-29T10:37:21Z",
    "closed_at": "2019-05-05T15:41:50Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6427",
    "body": "*Title*: \r\n\r\ncircuit breaker does not take effect on grpc.\r\n\r\n*Description*:\r\nDoes the envoy circuit breaker support GRPC? I test the circuit breaker does not take effect on grpc.\r\n\r\n*Config*:\r\nThis is my envoy config.\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        protocol: TCP\r\n        address: 0.0.0.0\r\n        port_value: 10000\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                  grpc: {}\r\n                route:\r\n                  cluster: service_grpc\r\n          http_filters:\r\n          - name: envoy.router\r\n  clusters:\r\n  - name: service_grpc\r\n    connect_timeout: 0.25s\r\n    type: LOGICAL_DNS\r\n    # Comment out the following line to test on v6 networks\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    http2_protocol_options: {}\r\n    max_requests_per_connection: 1\r\n    outlier_detection:\r\n      interval: \"5s\"\r\n      base_ejection_time: \"180s\"\r\n      enforcing_consecutive_5xx: 1\r\n      consecutive_gateway_failure: 1\r\n      enforcing_consecutive_gateway_failure: 100\r\n    circuit_breakers:\r\n      thresholds:\r\n        - max_connections: 1\r\n          max_pending_requests: 1\r\n          max_requests: 1\r\n          max_retries: 1\r\n    hosts:\r\n      - socket_address:\r\n          address: 192.168.1.4\r\n          port_value: 8081\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6427/comments",
    "author": "yiqinguo",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-03-29T14:30:22Z",
        "body": "Yes, circuit breaking is supported similar to HTTP/2, as gRPC is just HTTP/2."
      },
      {
        "user": "yiqinguo",
        "created_at": "2019-03-29T15:00:35Z",
        "body": "@mattklein123 Is my configuration wrong? Why is my configuration not taking effect?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-04-28T15:11:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-05T15:41:49Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6412,
    "title": "envoy  proxy Segmentation fault",
    "created_at": "2019-03-28T08:42:50Z",
    "closed_at": "2019-03-29T02:18:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6412",
    "body": "*Title*:  envoy proxy Segmentation fault when I learn examples/redis\r\n\r\n*Description*:\r\n>this is  log\r\n\r\nproxy_1  | [2019-03-28 08:30:11.919][12][debug][redis] [source/extensions/filters/network/redis_proxy/command_splitter_impl.cc:403] redis: splitting '[\"set\", \"name\", \"envoy\"]'\r\nproxy_1  | [2019-03-28 08:30:11.919][12][debug][connection] [source/common/network/connection_impl.cc:644] [C4] connecting to 172.20.0.2:6379\r\nproxy_1  | [2019-03-28 08:30:11.921][12][debug][connection] [source/common/network/connection_impl.cc:653] [C4] connection in progress\r\nproxy_1  | [2019-03-28 08:30:11.921][12][debug][connection] [source/common/network/connection_impl.cc:517] [C4] connected\r\nproxy_1  | [2019-03-28 08:30:11.923][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:81] Caught Segmentation fault, suspect faulting address 0xa\r\nproxy_1  | [2019-03-28 08:30:11.923][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:69] Backtrace (use tools/stack_decode.py to get line numbers):\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #0: __restore_rt [0x7f78d87f7390]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #1: [0x88058f]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #2: [0x889b3b]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #3: [0x88b8ee]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #4: [0x88be9f]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #5: [0x8897a9]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #6: [0x88985d]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #7: [0xaa5b4c]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #8: [0xaa23ba]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #9: [0xaa2aca]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #10: [0xa9c78a]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #11: [0xde74c9]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #12: [0xde79ff]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #13: [0xde9608]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #14: [0xa9bdfd]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #15: [0xa96c9e]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #16: [0xfc7575]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #17: start_thread [0x7f78d87ed6ba]\r\nproxy_1  | Segmentation fault (core dumped)\r\nredis_proxy_1 exited with code 139\r\n\r\nthis is my test steps:\r\n[root@localhost ~]# redis-cli -p 1999\r\n127.0.0.1:1999> set name envoy\r\n(error) no upstream host\r\n127.0.0.1:1999>\r\n\r\nwhat's trouble, I  not charge config  and  follow the examples/redis steps to operate 。\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6412/comments",
    "author": "skywli",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-03-28T16:42:34Z",
        "body": "Please try current master the bug here was reverted."
      },
      {
        "user": "skywli",
        "created_at": "2019-03-29T02:17:26Z",
        "body": "@mattklein123 thank your help,the new image is ok."
      }
    ]
  },
  {
    "number": 6403,
    "title": "panic: listener accept failure: Too many open files on high load",
    "created_at": "2019-03-27T18:57:36Z",
    "closed_at": "2019-04-11T09:23:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6403",
    "body": "**Issue Template**\r\n\r\n*Title*: *too many open files on high load*\r\n\r\n*Description*:\r\n\r\nI deployed envoy on Kubernetes on a dedicated workers where only envoy runs. I used 3 nodes with 36 vCPU and 96 GB RAM. I disabled circuit breaking by setting high threshold as described in FAQ. \r\n\r\nWe had 15 millions requests per *minute* during the run, it went fine for about 15 min till envoy crashed with too many open files. \r\n\r\nWe set nofile to maximum that the kernel would allow 2^20\r\n```\r\nulimit -n\r\n1,048,576\r\n```\r\n\r\nincreasing this limit further is bad idea and will break things. \r\n\r\nthe stack or backtrace is not relevant it just printed too many open files and crashed. \r\n\r\nCPU was around 25% and load around 16.0, pretty safe and normal.\r\n\r\nis it happening because we disabled the circuit breaker? \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6403/comments",
    "author": "benishak",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-03-27T19:56:07Z",
        "body": "It's very hard to say what is happening here without more information. I would recommend taking a look at Envoy stats to see how many connections are open, etc. which will allow us to better understand where the FDs are being used."
      },
      {
        "user": "benishak",
        "created_at": "2019-03-28T11:06:41Z",
        "body": "Thanks I will try to get that metrics when I rerun the test\r\nhere are some logs from the system\r\n\r\n```\r\n[Wed Mar 27 15:33:40 2019] TCP: too many orphaned sockets\r\n....\r\n[Wed Mar 27 15:32:05 2019] nf_conntrack: nf_conntrack: table full, dropping packet\r\n....\r\n[Wed Mar 27 15:32:05 2019] nf_conntrack: nf_conntrack: table full, dropping packet\r\n[Wed Mar 27 15:33:44 2019] TCP: too many orphaned sockets\r\n[Wed Mar 27 15:33:44 2019] TCP: request_sock_TCP: Possible SYN flooding on port 80. Sending cookies.  Check SNMP counters.\r\n[Wed Mar 27 15:33:44 2019] TCP: request_sock_TCP: Possible SYN flooding on port 443. Sending cookies.  Check SNMP counters.\r\n```\r\n\r\nand here our kernel settings\r\n\r\n```\r\nnet.ipv4.tcp_mem = 1638400 1638400 1638400\r\nnet.netfilter.nf_conntrack_tcp_timeout_established = 300\r\nnet.netfilter.nf_conntrack_tcp_timeout_time_wait = 1\r\nnet.netfilter.nf_conntrack_tcp_timeout_close_wait = 1\r\nnet.netfilter.nf_conntrack_max = 524288\r\nnet.nf_conntrack_max = 524288\r\nnet.core.somaxconn = 1024\r\nnet.ipv4.tcp_max_syn_backlog = 8192\r\n```\r\n\r\nI will try to increase `net.core.somaxconn`"
      },
      {
        "user": "benishak",
        "created_at": "2019-03-28T14:01:28Z",
        "body": "envoy stats\r\n```\r\naccess_log_file.flushed_by_timer: 2\r\naccess_log_file.reopen_failed: 0\r\naccess_log_file.write_buffered: 5\r\naccess_log_file.write_completed: 3\r\naccess_log_file.write_total_buffered: 124\r\ncluster.gloo-system.default-auctioncore-8081.bind_errors: 0\r\ncluster.gloo-system.default-auctioncore-8081.circuit_breakers.default.cx_open: 0\r\ncluster.gloo-system.default-auctioncore-8081.circuit_breakers.default.rq_open: 0\r\ncluster.gloo-system.default-auctioncore-8081.circuit_breakers.default.rq_pending_open: 0\r\ncluster.gloo-system.default-auctioncore-8081.circuit_breakers.default.rq_retry_open: 0\r\ncluster.gloo-system.default-auctioncore-8081.circuit_breakers.high.cx_open: 0\r\ncluster.gloo-system.default-auctioncore-8081.circuit_breakers.high.rq_open: 0\r\ncluster.gloo-system.default-auctioncore-8081.circuit_breakers.high.rq_pending_open: 0\r\ncluster.gloo-system.default-auctioncore-8081.circuit_breakers.high.rq_retry_open: 0\r\ncluster.gloo-system.default-auctioncore-8081.external.upstream_rq_200: 2098520\r\ncluster.gloo-system.default-auctioncore-8081.external.upstream_rq_2xx: 2098519\r\ncluster.gloo-system.default-auctioncore-8081.external.upstream_rq_404: 5868\r\ncluster.gloo-system.default-auctioncore-8081.external.upstream_rq_4xx: 5868\r\ncluster.gloo-system.default-auctioncore-8081.external.upstream_rq_500: 263\r\ncluster.gloo-system.default-auctioncore-8081.external.upstream_rq_503: 25592\r\ncluster.gloo-system.default-auctioncore-8081.external.upstream_rq_5xx: 25855\r\ncluster.gloo-system.default-auctioncore-8081.external.upstream_rq_completed: 2130242\r\ncluster.gloo-system.default-auctioncore-8081.lb_healthy_panic: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_local_cluster_not_ok: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_recalculate_zone_structures: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_subsets_active: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_subsets_created: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_subsets_fallback: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_subsets_fallback_panic: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_subsets_removed: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_subsets_selected: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_zone_cluster_too_small: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_zone_no_capacity_left: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_zone_number_differs: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_zone_routing_all_directly: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_zone_routing_cross_zone: 0\r\ncluster.gloo-system.default-auctioncore-8081.lb_zone_routing_sampled: 0\r\ncluster.gloo-system.default-auctioncore-8081.max_host_weight: 1\r\ncluster.gloo-system.default-auctioncore-8081.membership_change: 1\r\ncluster.gloo-system.default-auctioncore-8081.membership_degraded: 0\r\ncluster.gloo-system.default-auctioncore-8081.membership_healthy: 521\r\ncluster.gloo-system.default-auctioncore-8081.membership_total: 521\r\ncluster.gloo-system.default-auctioncore-8081.original_dst_host_invalid: 0\r\ncluster.gloo-system.default-auctioncore-8081.retry_or_shadow_abandoned: 0\r\ncluster.gloo-system.default-auctioncore-8081.update_attempt: 2\r\ncluster.gloo-system.default-auctioncore-8081.update_empty: 0\r\ncluster.gloo-system.default-auctioncore-8081.update_failure: 0\r\ncluster.gloo-system.default-auctioncore-8081.update_no_rebuild: 0\r\ncluster.gloo-system.default-auctioncore-8081.update_rejected: 0\r\ncluster.gloo-system.default-auctioncore-8081.update_success: 1\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_active: 52860\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_close_notify: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_connect_attempts_exceeded: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_connect_fail: 116231\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_connect_timeout: 116231\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_destroy: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_destroy_local: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_destroy_local_with_active_rq: 15818\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_destroy_remote: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_destroy_remote_with_active_rq: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_destroy_with_active_rq: 15818\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_http1_total: 184909\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_http2_total: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_idle_timeout: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_max_requests: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_none_healthy: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_overflow: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_protocol_error: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_rx_bytes_buffered: 23114775\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_rx_bytes_total: 914565585\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_total: 184909\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_tx_bytes_buffered: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_tx_bytes_total: 2211748249\r\ncluster.gloo-system.default-auctioncore-8081.upstream_flow_control_backed_up_total: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_flow_control_drained_total: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_flow_control_paused_reading_total: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_flow_control_resumed_reading_total: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_internal_redirect_failed_total: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_internal_redirect_succeeded_total: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_200: 2098516\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_2xx: 2098516\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_404: 5868\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_4xx: 5868\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_500: 263\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_503: 25592\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_5xx: 25855\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_active: 9982\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_cancelled: 45201\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_completed: 2130239\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_maintenance_mode: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_pending_active: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_pending_failure_eject: 25592\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_pending_overflow: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_pending_total: 184909\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_per_try_timeout: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_retry: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_retry_overflow: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_retry_success: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_rx_reset: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_timeout: 0\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_total: 2130443\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_tx_reset: 0\r\ncluster.gloo-system.default-auctioncore-8081.version: 17666663386481859172\r\ncluster.gloo-system.incoming-mediation.bind_errors: 0\r\ncluster.gloo-system.incoming-mediation.circuit_breakers.default.cx_open: 0\r\ncluster.gloo-system.incoming-mediation.circuit_breakers.default.rq_open: 0\r\ncluster.gloo-system.incoming-mediation.circuit_breakers.default.rq_pending_open: 0\r\ncluster.gloo-system.incoming-mediation.circuit_breakers.default.rq_retry_open: 0\r\ncluster.gloo-system.incoming-mediation.circuit_breakers.high.cx_open: 0\r\ncluster.gloo-system.incoming-mediation.circuit_breakers.high.rq_open: 0\r\ncluster.gloo-system.incoming-mediation.circuit_breakers.high.rq_pending_open: 0\r\ncluster.gloo-system.incoming-mediation.circuit_breakers.high.rq_retry_open: 0\r\ncluster.gloo-system.incoming-mediation.external.upstream_rq_200: 44507\r\ncluster.gloo-system.incoming-mediation.external.upstream_rq_204: 20648\r\ncluster.gloo-system.incoming-mediation.external.upstream_rq_2xx: 65155\r\ncluster.gloo-system.incoming-mediation.external.upstream_rq_503: 65\r\ncluster.gloo-system.incoming-mediation.external.upstream_rq_5xx: 65\r\ncluster.gloo-system.incoming-mediation.external.upstream_rq_completed: 65220\r\ncluster.gloo-system.incoming-mediation.lb_healthy_panic: 0\r\ncluster.gloo-system.incoming-mediation.lb_local_cluster_not_ok: 0\r\ncluster.gloo-system.incoming-mediation.lb_recalculate_zone_structures: 0\r\ncluster.gloo-system.incoming-mediation.lb_subsets_active: 0\r\ncluster.gloo-system.incoming-mediation.lb_subsets_created: 0\r\ncluster.gloo-system.incoming-mediation.lb_subsets_fallback: 0\r\ncluster.gloo-system.incoming-mediation.lb_subsets_fallback_panic: 0\r\ncluster.gloo-system.incoming-mediation.lb_subsets_removed: 0\r\ncluster.gloo-system.incoming-mediation.lb_subsets_selected: 0\r\ncluster.gloo-system.incoming-mediation.lb_zone_cluster_too_small: 0\r\ncluster.gloo-system.incoming-mediation.lb_zone_no_capacity_left: 0\r\ncluster.gloo-system.incoming-mediation.lb_zone_number_differs: 0\r\ncluster.gloo-system.incoming-mediation.lb_zone_routing_all_directly: 0\r\ncluster.gloo-system.incoming-mediation.lb_zone_routing_cross_zone: 0\r\ncluster.gloo-system.incoming-mediation.lb_zone_routing_sampled: 0\r\ncluster.gloo-system.incoming-mediation.max_host_weight: 1\r\ncluster.gloo-system.incoming-mediation.membership_change: 1\r\ncluster.gloo-system.incoming-mediation.membership_degraded: 0\r\ncluster.gloo-system.incoming-mediation.membership_healthy: 2\r\ncluster.gloo-system.incoming-mediation.membership_total: 2\r\ncluster.gloo-system.incoming-mediation.original_dst_host_invalid: 0\r\ncluster.gloo-system.incoming-mediation.retry_or_shadow_abandoned: 0\r\ncluster.gloo-system.incoming-mediation.update_attempt: 5\r\ncluster.gloo-system.incoming-mediation.update_empty: 0\r\ncluster.gloo-system.incoming-mediation.update_failure: 0\r\ncluster.gloo-system.incoming-mediation.update_no_rebuild: 4\r\ncluster.gloo-system.incoming-mediation.update_success: 5\r\ncluster.gloo-system.incoming-mediation.upstream_cx_active: 945\r\ncluster.gloo-system.incoming-mediation.upstream_cx_close_notify: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_connect_attempts_exceeded: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_connect_fail: 3783\r\ncluster.gloo-system.incoming-mediation.upstream_cx_connect_timeout: 3783\r\ncluster.gloo-system.incoming-mediation.upstream_cx_destroy: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_destroy_local: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_destroy_local_with_active_rq: 403\r\ncluster.gloo-system.incoming-mediation.upstream_cx_destroy_remote: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_destroy_remote_with_active_rq: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_destroy_with_active_rq: 403\r\ncluster.gloo-system.incoming-mediation.upstream_cx_http1_total: 5131\r\ncluster.gloo-system.incoming-mediation.upstream_cx_http2_total: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_idle_timeout: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_max_requests: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_none_healthy: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_overflow: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_protocol_error: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_rx_bytes_buffered: 350784\r\ncluster.gloo-system.incoming-mediation.upstream_cx_rx_bytes_total: 25732995\r\ncluster.gloo-system.incoming-mediation.upstream_cx_total: 5131\r\ncluster.gloo-system.incoming-mediation.upstream_cx_tx_bytes_buffered: 0\r\ncluster.gloo-system.incoming-mediation.upstream_cx_tx_bytes_total: 74229241\r\ncluster.gloo-system.incoming-mediation.upstream_flow_control_backed_up_total: 0\r\ncluster.gloo-system.incoming-mediation.upstream_flow_control_drained_total: 0\r\ncluster.gloo-system.incoming-mediation.upstream_flow_control_paused_reading_total: 0\r\ncluster.gloo-system.incoming-mediation.upstream_flow_control_resumed_reading_total: 0\r\ncluster.gloo-system.incoming-mediation.upstream_internal_redirect_failed_total: 0\r\ncluster.gloo-system.incoming-mediation.upstream_internal_redirect_succeeded_total: 0\r\ncluster.gloo-system.incoming-mediation.upstream_rq_200: 44507\r\ncluster.gloo-system.incoming-mediation.upstream_rq_204: 20648\r\ncluster.gloo-system.incoming-mediation.upstream_rq_2xx: 65155\r\ncluster.gloo-system.incoming-mediation.upstream_rq_503: 65\r\ncluster.gloo-system.incoming-mediation.upstream_rq_5xx: 65\r\ncluster.gloo-system.incoming-mediation.upstream_rq_active: 421\r\ncluster.gloo-system.incoming-mediation.upstream_rq_cancelled: 1049\r\ncluster.gloo-system.incoming-mediation.upstream_rq_completed: 65220\r\ncluster.gloo-system.incoming-mediation.upstream_rq_maintenance_mode: 0\r\ncluster.gloo-system.incoming-mediation.upstream_rq_pending_active: 0\r\ncluster.gloo-system.incoming-mediation.upstream_rq_pending_failure_eject: 65\r\ncluster.gloo-system.incoming-mediation.upstream_rq_pending_overflow: 0\r\ncluster.gloo-system.incoming-mediation.upstream_rq_pending_total: 5131\r\ncluster.gloo-system.incoming-mediation.upstream_rq_per_try_timeout: 0\r\ncluster.gloo-system.incoming-mediation.upstream_rq_retry: 0\r\ncluster.gloo-system.incoming-mediation.upstream_rq_retry_overflow: 0\r\ncluster.gloo-system.incoming-mediation.upstream_rq_retry_success: 0\r\ncluster.gloo-system.incoming-mediation.upstream_rq_rx_reset: 0\r\ncluster.gloo-system.incoming-mediation.upstream_rq_timeout: 0\r\ncluster.gloo-system.incoming-mediation.upstream_rq_total: 65978\r\ncluster.gloo-system.incoming-mediation.upstream_rq_tx_reset: 0\r\ncluster.gloo-system.incoming-mediation.version: 0\r\ncluster.xds_cluster.bind_errors: 0\r\ncluster.xds_cluster.circuit_breakers.default.cx_open: 0\r\ncluster.xds_cluster.circuit_breakers.default.rq_open: 0\r\ncluster.xds_cluster.circuit_breakers.default.rq_pending_open: 0\r\ncluster.xds_cluster.circuit_breakers.default.rq_retry_open: 0\r\ncluster.xds_cluster.circuit_breakers.high.cx_open: 0\r\ncluster.xds_cluster.circuit_breakers.high.rq_open: 0\r\ncluster.xds_cluster.circuit_breakers.high.rq_pending_open: 0\r\ncluster.xds_cluster.circuit_breakers.high.rq_retry_open: 0\r\ncluster.xds_cluster.http2.header_overflow: 0\r\ncluster.xds_cluster.http2.headers_cb_no_stream: 0\r\ncluster.xds_cluster.http2.rx_messaging_error: 0\r\ncluster.xds_cluster.http2.rx_reset: 0\r\ncluster.xds_cluster.http2.too_many_header_frames: 0\r\ncluster.xds_cluster.http2.trailers: 0\r\ncluster.xds_cluster.http2.tx_reset: 0\r\ncluster.xds_cluster.internal.upstream_rq_200: 1\r\ncluster.xds_cluster.internal.upstream_rq_2xx: 1\r\ncluster.xds_cluster.internal.upstream_rq_503: 1\r\ncluster.xds_cluster.internal.upstream_rq_5xx: 1\r\ncluster.xds_cluster.internal.upstream_rq_completed: 2\r\ncluster.xds_cluster.lb_healthy_panic: 1\r\ncluster.xds_cluster.lb_local_cluster_not_ok: 0\r\ncluster.xds_cluster.lb_recalculate_zone_structures: 0\r\ncluster.xds_cluster.lb_subsets_active: 0\r\ncluster.xds_cluster.lb_subsets_created: 0\r\ncluster.xds_cluster.lb_subsets_fallback: 0\r\ncluster.xds_cluster.lb_subsets_fallback_panic: 0\r\ncluster.xds_cluster.lb_subsets_removed: 0\r\ncluster.xds_cluster.lb_subsets_selected: 0\r\ncluster.xds_cluster.lb_zone_cluster_too_small: 0\r\ncluster.xds_cluster.lb_zone_no_capacity_left: 0\r\ncluster.xds_cluster.lb_zone_number_differs: 0\r\ncluster.xds_cluster.lb_zone_routing_all_directly: 0\r\ncluster.xds_cluster.lb_zone_routing_cross_zone: 0\r\ncluster.xds_cluster.lb_zone_routing_sampled: 0\r\ncluster.xds_cluster.max_host_weight: 1\r\ncluster.xds_cluster.membership_change: 1\r\ncluster.xds_cluster.membership_degraded: 0\r\ncluster.xds_cluster.membership_healthy: 1\r\ncluster.xds_cluster.membership_total: 1\r\ncluster.xds_cluster.original_dst_host_invalid: 0\r\ncluster.xds_cluster.retry_or_shadow_abandoned: 0\r\ncluster.xds_cluster.update_attempt: 3\r\ncluster.xds_cluster.update_empty: 0\r\ncluster.xds_cluster.update_failure: 0\r\ncluster.xds_cluster.update_no_rebuild: 2\r\ncluster.xds_cluster.update_success: 3\r\ncluster.xds_cluster.upstream_cx_active: 1\r\ncluster.xds_cluster.upstream_cx_close_notify: 0\r\ncluster.xds_cluster.upstream_cx_connect_attempts_exceeded: 0\r\ncluster.xds_cluster.upstream_cx_connect_fail: 0\r\ncluster.xds_cluster.upstream_cx_connect_timeout: 0\r\ncluster.xds_cluster.upstream_cx_destroy: 0\r\ncluster.xds_cluster.upstream_cx_destroy_local: 0\r\ncluster.xds_cluster.upstream_cx_destroy_local_with_active_rq: 0\r\ncluster.xds_cluster.upstream_cx_destroy_remote: 0\r\ncluster.xds_cluster.upstream_cx_destroy_remote_with_active_rq: 0\r\ncluster.xds_cluster.upstream_cx_destroy_with_active_rq: 0\r\ncluster.xds_cluster.upstream_cx_http1_total: 0\r\ncluster.xds_cluster.upstream_cx_http2_total: 1\r\ncluster.xds_cluster.upstream_cx_idle_timeout: 0\r\ncluster.xds_cluster.upstream_cx_max_requests: 0\r\ncluster.xds_cluster.upstream_cx_none_healthy: 1\r\ncluster.xds_cluster.upstream_cx_overflow: 0\r\ncluster.xds_cluster.upstream_cx_protocol_error: 0\r\ncluster.xds_cluster.upstream_cx_rx_bytes_buffered: 30\r\ncluster.xds_cluster.upstream_cx_rx_bytes_total: 21835\r\ncluster.xds_cluster.upstream_cx_total: 1\r\ncluster.xds_cluster.upstream_cx_tx_bytes_buffered: 0\r\ncluster.xds_cluster.upstream_cx_tx_bytes_total: 2411\r\ncluster.xds_cluster.upstream_flow_control_backed_up_total: 0\r\ncluster.xds_cluster.upstream_flow_control_drained_total: 0\r\ncluster.xds_cluster.upstream_flow_control_paused_reading_total: 0\r\ncluster.xds_cluster.upstream_flow_control_resumed_reading_total: 0\r\ncluster.xds_cluster.upstream_internal_redirect_failed_total: 0\r\ncluster.xds_cluster.upstream_internal_redirect_succeeded_total: 0\r\ncluster.xds_cluster.upstream_rq_200: 1\r\ncluster.xds_cluster.upstream_rq_2xx: 1\r\ncluster.xds_cluster.upstream_rq_503: 1\r\ncluster.xds_cluster.upstream_rq_5xx: 1\r\ncluster.xds_cluster.upstream_rq_active: 1\r\ncluster.xds_cluster.upstream_rq_cancelled: 0\r\ncluster.xds_cluster.upstream_rq_completed: 2\r\ncluster.xds_cluster.upstream_rq_maintenance_mode: 0\r\ncluster.xds_cluster.upstream_rq_pending_active: 0\r\ncluster.xds_cluster.upstream_rq_pending_failure_eject: 0\r\ncluster.xds_cluster.upstream_rq_pending_overflow: 0\r\ncluster.xds_cluster.upstream_rq_pending_total: 1\r\ncluster.xds_cluster.upstream_rq_per_try_timeout: 0\r\ncluster.xds_cluster.upstream_rq_retry: 0\r\ncluster.xds_cluster.upstream_rq_retry_overflow: 0\r\ncluster.xds_cluster.upstream_rq_retry_success: 0\r\ncluster.xds_cluster.upstream_rq_rx_reset: 0\r\ncluster.xds_cluster.upstream_rq_timeout: 0\r\ncluster.xds_cluster.upstream_rq_total: 1\r\ncluster.xds_cluster.upstream_rq_tx_reset: 0\r\ncluster.xds_cluster.version: 0\r\ncluster_manager.active_clusters: 3\r\ncluster_manager.cds.update_attempt: 2\r\ncluster_manager.cds.update_failure: 0\r\ncluster_manager.cds.update_rejected: 0\r\ncluster_manager.cds.update_success: 1\r\ncluster_manager.cds.version: 17666663386481859172\r\ncluster_manager.cluster_added: 3\r\ncluster_manager.cluster_modified: 0\r\ncluster_manager.cluster_removed: 0\r\ncluster_manager.cluster_updated: 0\r\ncluster_manager.cluster_updated_via_merge: 0\r\ncluster_manager.update_merge_cancelled: 0\r\ncluster_manager.update_out_of_merge_window: 0\r\ncluster_manager.warming_clusters: 0\r\ncontrol_plane.connected_state: 1\r\ncontrol_plane.pending_requests: 0\r\ncontrol_plane.rate_limit_enforced: 0\r\nhttp.admin.downstream_cx_active: 1\r\nhttp.admin.downstream_cx_delayed_close_timeout: 0\r\nhttp.admin.downstream_cx_destroy: 5\r\nhttp.admin.downstream_cx_destroy_active_rq: 0\r\nhttp.admin.downstream_cx_destroy_local: 0\r\nhttp.admin.downstream_cx_destroy_local_active_rq: 0\r\nhttp.admin.downstream_cx_destroy_remote: 5\r\nhttp.admin.downstream_cx_destroy_remote_active_rq: 0\r\nhttp.admin.downstream_cx_drain_close: 0\r\nhttp.admin.downstream_cx_http1_active: 1\r\nhttp.admin.downstream_cx_http1_total: 6\r\nhttp.admin.downstream_cx_http2_active: 0\r\nhttp.admin.downstream_cx_http2_total: 0\r\nhttp.admin.downstream_cx_idle_timeout: 0\r\nhttp.admin.downstream_cx_overload_disable_keepalive: 0\r\nhttp.admin.downstream_cx_protocol_error: 0\r\nhttp.admin.downstream_cx_rx_bytes_buffered: 84\r\nhttp.admin.downstream_cx_rx_bytes_total: 504\r\nhttp.admin.downstream_cx_ssl_active: 0\r\nhttp.admin.downstream_cx_ssl_total: 0\r\nhttp.admin.downstream_cx_total: 6\r\nhttp.admin.downstream_cx_tx_bytes_buffered: 0\r\nhttp.admin.downstream_cx_tx_bytes_total: 167910\r\nhttp.admin.downstream_cx_upgrades_active: 0\r\nhttp.admin.downstream_cx_upgrades_total: 0\r\nhttp.admin.downstream_flow_control_paused_reading_total: 0\r\nhttp.admin.downstream_flow_control_resumed_reading_total: 0\r\nhttp.admin.downstream_rq_1xx: 0\r\nhttp.admin.downstream_rq_2xx: 5\r\nhttp.admin.downstream_rq_3xx: 0\r\nhttp.admin.downstream_rq_4xx: 0\r\nhttp.admin.downstream_rq_5xx: 0\r\nhttp.admin.downstream_rq_active: 1\r\nhttp.admin.downstream_rq_completed: 5\r\nhttp.admin.downstream_rq_http1_total: 6\r\nhttp.admin.downstream_rq_http2_total: 0\r\nhttp.admin.downstream_rq_idle_timeout: 0\r\nhttp.admin.downstream_rq_non_relative_path: 0\r\nhttp.admin.downstream_rq_overload_close: 0\r\nhttp.admin.downstream_rq_response_before_rq_complete: 0\r\nhttp.admin.downstream_rq_rx_reset: 0\r\nhttp.admin.downstream_rq_timeout: 0\r\nhttp.admin.downstream_rq_too_large: 0\r\nhttp.admin.downstream_rq_total: 6\r\nhttp.admin.downstream_rq_tx_reset: 0\r\nhttp.admin.downstream_rq_ws_on_non_ws_route: 0\r\nhttp.admin.rs_too_large: 0\r\nhttp.async-client.no_cluster: 0\r\nhttp.async-client.no_route: 0\r\nhttp.async-client.rq_direct_response: 0\r\nhttp.async-client.rq_redirect: 0\r\nhttp.async-client.rq_reset_after_downstream_response_started: 0\r\nhttp.async-client.rq_total: 2\r\nhttp.http.cors.origin_invalid: 0\r\nhttp.http.cors.origin_valid: 0\r\nhttp.http.downstream_cx_active: 303950\r\nhttp.http.downstream_cx_delayed_close_timeout: 1784\r\nhttp.http.downstream_cx_destroy: 565392\r\nhttp.http.downstream_cx_destroy_active_rq: 60822\r\nhttp.http.downstream_cx_destroy_local: 1786\r\nhttp.http.downstream_cx_destroy_local_active_rq: 0\r\nhttp.http.downstream_cx_destroy_remote: 563624\r\nhttp.http.downstream_cx_destroy_remote_active_rq: 60822\r\nhttp.http.downstream_cx_drain_close: 0\r\nhttp.http.downstream_cx_http1_active: 151753\r\nhttp.http.downstream_cx_http1_total: 652026\r\nhttp.http.downstream_cx_http2_active: 88151\r\nhttp.http.downstream_cx_http2_total: 93294\r\nhttp.http.downstream_cx_idle_timeout: 0\r\nhttp.http.downstream_cx_overload_disable_keepalive: 0\r\nhttp.http.downstream_cx_protocol_error: 1176\r\nhttp.http.downstream_cx_rx_bytes_buffered: 143532589\r\nhttp.http.downstream_cx_rx_bytes_total: 1630349285\r\nhttp.http.downstream_cx_ssl_active: 134178\r\nhttp.http.downstream_cx_ssl_total: 180169\r\nhttp.http.downstream_cx_total: 869343\r\nhttp.http.downstream_cx_tx_bytes_buffered: 0\r\nhttp.http.downstream_cx_tx_bytes_total: 902443702\r\nhttp.http.downstream_cx_upgrades_active: 0\r\nhttp.http.downstream_cx_upgrades_total: 0\r\nhttp.http.downstream_flow_control_paused_reading_total: 0\r\nhttp.http.downstream_flow_control_resumed_reading_total: 0\r\nhttp.http.downstream_rq_1xx: 0\r\nhttp.http.downstream_rq_2xx: 2163682\r\nhttp.http.downstream_rq_3xx: 0\r\nhttp.http.downstream_rq_4xx: 6217\r\nhttp.http.downstream_rq_5xx: 25920\r\nhttp.http.downstream_rq_active: 10445\r\nhttp.http.downstream_rq_completed: 2195816\r\nhttp.http.downstream_rq_http1_total: 1643857\r\nhttp.http.downstream_rq_http2_total: 626011\r\nhttp.http.downstream_rq_idle_timeout: 0\r\nhttp.http.downstream_rq_non_relative_path: 3\r\nhttp.http.downstream_rq_overload_close: 0\r\nhttp.http.downstream_rq_response_before_rq_complete: 0\r\nhttp.http.downstream_rq_rx_reset: 63645\r\nhttp.http.downstream_rq_timeout: 0\r\nhttp.http.downstream_rq_too_large: 0\r\nhttp.http.downstream_rq_total: 2269871\r\nhttp.http.downstream_rq_tx_reset: 0\r\nhttp.http.downstream_rq_ws_on_non_ws_route: 0\r\nhttp.http.fault.aborts_injected: 0\r\nhttp.http.fault.active_faults: 0\r\nhttp.http.fault.delays_injected: 0\r\nhttp.http.fault.faults_overflow: 0\r\nhttp.http.fault.response_rl_injected: 0\r\nhttp.http.no_cluster: 0\r\nhttp.http.no_route: 0\r\nhttp.http.rds.listener-__-443-routes.config_reload: 1\r\nhttp.http.rds.listener-__-443-routes.update_attempt: 2\r\nhttp.http.rds.listener-__-443-routes.update_empty: 0\r\nhttp.http.rds.listener-__-443-routes.update_failure: 0\r\nhttp.http.rds.listener-__-443-routes.update_rejected: 0\r\nhttp.http.rds.listener-__-443-routes.update_success: 1\r\nhttp.http.rds.listener-__-443-routes.version: 17666663386481859172\r\nhttp.http.rds.listener-__-80-routes.config_reload: 1\r\nhttp.http.rds.listener-__-80-routes.update_attempt: 2\r\nhttp.http.rds.listener-__-80-routes.update_empty: 0\r\nhttp.http.rds.listener-__-80-routes.update_failure: 0\r\nhttp.http.rds.listener-__-80-routes.update_rejected: 0\r\nhttp.http.rds.listener-__-80-routes.update_success: 1\r\nhttp.http.rds.listener-__-80-routes.version: 17666663386481859172\r\nhttp.http.rq_direct_response: 0\r\nhttp.http.rq_redirect: 0\r\nhttp.http.rq_reset_after_downstream_response_started: 0\r\nhttp.http.rq_total: 2268338\r\nhttp.http.rs_too_large: 0\r\nhttp.http.tracing.client_enabled: 0\r\nhttp.http.tracing.health_check: 0\r\nhttp.http.tracing.not_traceable: 0\r\nhttp.http.tracing.random_sampling: 0\r\nhttp.http.tracing.service_forced: 0\r\nhttp.http.user_agent.android.downstream_cx_destroy_remote_active_rq: 0\r\nhttp.http.user_agent.android.downstream_cx_total: 9\r\nhttp.http.user_agent.android.downstream_rq_total: 9\r\nhttp.http.user_agent.ios.downstream_cx_destroy_remote_active_rq: 2125\r\nhttp.http.user_agent.ios.downstream_cx_total: 13715\r\nhttp.http.user_agent.ios.downstream_rq_total: 13715\r\nhttp2.header_overflow: 0\r\nhttp2.headers_cb_no_stream: 0\r\nhttp2.rx_messaging_error: 0\r\nhttp2.rx_reset: 1621\r\nhttp2.too_many_header_frames: 0\r\nhttp2.trailers: 0\r\nhttp2.tx_reset: 0\r\nlistener.[__]_443.downstream_cx_active: 134178\r\nlistener.[__]_443.downstream_cx_destroy: 45991\r\nlistener.[__]_443.downstream_cx_total: 180169\r\nlistener.[__]_443.downstream_pre_cx_active: 0\r\nlistener.[__]_443.downstream_pre_cx_timeout: 0\r\nlistener.[__]_443.http.http.downstream_rq_1xx: 0\r\nlistener.[__]_443.http.http.downstream_rq_2xx: 702817\r\nlistener.[__]_443.http.http.downstream_rq_3xx: 0\r\nlistener.[__]_443.http.http.downstream_rq_4xx: 5121\r\nlistener.[__]_443.http.http.downstream_rq_5xx: 9565\r\nlistener.[__]_443.http.http.downstream_rq_completed: 717501\r\nlistener.[__]_443.no_filter_chain_match: 0\r\nlistener.[__]_443.server_ssl_socket_factory.downstream_context_secrets_not_ready: 0\r\nlistener.[__]_443.server_ssl_socket_factory.ssl_context_update_by_sds: 0\r\nlistener.[__]_443.server_ssl_socket_factory.upstream_context_secrets_not_ready: 0\r\nlistener.[__]_443.ssl.ciphers.AEAD-AES128-GCM-SHA256: 47365\r\nlistener.[__]_443.ssl.ciphers.AEAD-CHACHA20-POLY1305-SHA256: 9587\r\nlistener.[__]_443.ssl.ciphers.AES128-GCM-SHA256: 3\r\nlistener.[__]_443.ssl.ciphers.AES128-SHA: 2\r\nlistener.[__]_443.ssl.ciphers.ECDHE-RSA-AES128-GCM-SHA256: 74442\r\nlistener.[__]_443.ssl.ciphers.ECDHE-RSA-AES128-SHA: 610\r\nlistener.[__]_443.ssl.ciphers.ECDHE-RSA-CHACHA20-POLY1305: 12496\r\nlistener.[__]_443.ssl.connection_error: 175\r\nlistener.[__]_443.ssl.curves.P-256: 17327\r\nlistener.[__]_443.ssl.curves.X25519: 127173\r\nlistener.[__]_443.ssl.fail_verify_cert_hash: 0\r\nlistener.[__]_443.ssl.fail_verify_error: 0\r\nlistener.[__]_443.ssl.fail_verify_no_cert: 0\r\nlistener.[__]_443.ssl.fail_verify_san: 0\r\nlistener.[__]_443.ssl.handshake: 144505\r\nlistener.[__]_443.ssl.no_certificate: 144505\r\nlistener.[__]_443.ssl.session_reused: 6565\r\nlistener.[__]_443.ssl.versions.TLSv1: 548\r\nlistener.[__]_443.ssl.versions.TLSv1.1: 6\r\nlistener.[__]_443.ssl.versions.TLSv1.2: 86999\r\nlistener.[__]_443.ssl.versions.TLSv1.3: 56952\r\nlistener.[__]_80.downstream_cx_active: 169771\r\nlistener.[__]_80.downstream_cx_destroy: 519401\r\nlistener.[__]_80.downstream_cx_total: 689174\r\nlistener.[__]_80.downstream_pre_cx_active: 0\r\nlistener.[__]_80.downstream_pre_cx_timeout: 0\r\nlistener.[__]_80.http.http.downstream_rq_1xx: 0\r\nlistener.[__]_80.http.http.downstream_rq_2xx: 1460868\r\nlistener.[__]_80.http.http.downstream_rq_3xx: 0\r\nlistener.[__]_80.http.http.downstream_rq_4xx: 1096\r\nlistener.[__]_80.http.http.downstream_rq_5xx: 16355\r\nlistener.[__]_80.http.http.downstream_rq_completed: 1478319\r\nlistener.[__]_80.no_filter_chain_match: 0\r\nlistener.admin.downstream_cx_active: 1\r\nlistener.admin.downstream_cx_destroy: 5\r\nlistener.admin.downstream_cx_total: 6\r\nlistener.admin.downstream_pre_cx_active: 0\r\nlistener.admin.downstream_pre_cx_timeout: 0\r\nlistener.admin.http.admin.downstream_rq_1xx: 0\r\nlistener.admin.http.admin.downstream_rq_2xx: 5\r\nlistener.admin.http.admin.downstream_rq_3xx: 0\r\nlistener.admin.http.admin.downstream_rq_4xx: 0\r\nlistener.admin.http.admin.downstream_rq_5xx: 0\r\nlistener.admin.http.admin.downstream_rq_completed: 5\r\nlistener.admin.no_filter_chain_match: 0\r\nlistener_manager.lds.update_attempt: 2\r\nlistener_manager.lds.update_failure: 0\r\nlistener_manager.lds.update_rejected: 0\r\nlistener_manager.lds.update_success: 1\r\nlistener_manager.lds.version: 17666663386481859172\r\nlistener_manager.listener_added: 2\r\nlistener_manager.listener_create_failure: 0\r\nlistener_manager.listener_create_success: 72\r\nlistener_manager.listener_modified: 0\r\nlistener_manager.listener_removed: 0\r\nlistener_manager.total_listeners_active: 2\r\nlistener_manager.total_listeners_draining: 0\r\nlistener_manager.total_listeners_warming: 0\r\nruntime.admin_overrides_active: 0\r\nruntime.deprecated_feature_use: 2886\r\nruntime.load_error: 0\r\nruntime.load_success: 0\r\nruntime.num_keys: 0\r\nruntime.override_dir_exists: 0\r\nruntime.override_dir_not_exists: 0\r\nserver.concurrency: 36\r\nserver.days_until_first_cert_expiring: 268\r\nserver.debug_assertion_failures: 0\r\nserver.hot_restart_epoch: 0\r\nserver.live: 1\r\nserver.memory_allocated: 4379749248\r\nserver.memory_heap_size: 4562354176\r\nserver.parent_connections: 0\r\nserver.total_connections: 943485\r\nserver.uptime: 25\r\nserver.version: 6894057\r\nserver.watchdog_mega_miss: 0\r\nserver.watchdog_miss: 1\r\nstats.overflow: 0\r\ncluster.gloo-system.default-auctioncore-8081.external.upstream_rq_time: P0(0,0) P25(14.7606,13.7269) P50(81.8527,82.3807) P75(185.613,207.036) P90(275.104,345.433) P95(386.468,489.214) P99(523.314,1612.93) P99.5(726.377,2310.33) P99.9(958.452,3668.89) P100(1200,5900)\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_connect_ms: P0(0,0) P25(0,0) P50(0,29.0089) P75(0,781.791) P90(2.06543,2144.27) P95(7.1,2747.26) P99(33.6,3072) P99.5(49.2,3086) P99.9(82.48,3097.2) P100(130,3100)\r\ncluster.gloo-system.default-auctioncore-8081.upstream_cx_length_ms: P0(88,0) P25(12821.6,5020.69) P50(15974.9,5049.39) P75(18788.5,5078.08) P90(20794.6,5095.29) P95(21743,6800.68) P99(22954.9,16931.4) P99.5(23426,19085.7) P99.9(23885.2,21840.9) P100(24000,24000)\r\ncluster.gloo-system.default-auctioncore-8081.upstream_rq_time: P0(0,0) P25(14.7606,13.7269) P50(81.8527,82.3807) P75(185.613,207.036) P90(275.104,345.433) P95(386.468,489.214) P99(523.314,1612.93) P99.5(726.377,2310.33) P99.9(958.452,3668.89) P100(1200,5900)\r\ncluster.gloo-system.incoming-mediation.external.upstream_rq_time: P0(13,9) P25(27.25,28.0421) P50(165.443,180.801) P75(260.719,262.236) P90(271.365,276.049) P95(278.229,298.856) P99(303.395,772.194) P99.5(313.748,983.936) P99.9(353.87,1549.06) P100(490,2100)\r\ncluster.gloo-system.incoming-mediation.upstream_cx_connect_ms: P0(0,0) P25(0,0) P50(0,1.07279) P75(0,158.125) P90(0,1183.75) P95(1.05,2297.5) P99(2.05,3026.5) P99.5(2.075,3063.25) P99.9(2.095,3092.65) P100(2.1,3100)\r\ncluster.gloo-system.incoming-mediation.upstream_cx_length_ms: P0(13000,2) P25(18083.3,5016.36) P50(18833.3,5044.48) P75(19583.3,5072.59) P90(20100,5089.45) P95(20550,5095.08) P99(20910,5099.57) P99.5(20955,6908.5) P99.9(20991,18939) P100(21000,21000)\r\ncluster.gloo-system.incoming-mediation.upstream_rq_time: P0(13,9) P25(27.25,28.0421) P50(165.443,180.801) P75(260.719,262.236) P90(271.365,276.049) P95(278.229,298.856) P99(303.395,772.194) P99.5(313.748,983.936) P99.9(353.87,1549.06) P100(490,2100)\r\ncluster.xds_cluster.upstream_cx_connect_ms: P0(nan,0) P25(nan,0) P50(nan,0) P75(nan,0) P90(nan,0) P95(nan,0) P99(nan,0) P99.5(nan,0) P99.9(nan,0) P100(nan,0)\r\ncluster.xds_cluster.upstream_cx_length_ms: No recorded values\r\nhttp.admin.downstream_cx_length_ms: P0(410,410) P25(412.5,420.833) P50(415,425) P75(417.5,429.167) P90(419,525) P95(419.5,527.5) P99(419.9,529.5) P99.5(419.95,529.75) P99.9(419.99,529.95) P100(420,530)\r\nhttp.admin.downstream_rq_time: P0(1,0) P25(1.025,1.00625) P50(1.05,1.0375) P75(1.075,1.06875) P90(1.09,1.0875) P95(1.095,1.09375) P99(1.099,1.09875) P99.5(1.0995,1.09937) P99.9(1.0999,1.09987) P100(1.1,1.1)\r\nhttp.http.downstream_cx_length_ms: P0(0,0) P25(2716.88,473.846) P50(10414.2,4773.56) P75(12769.3,10733.6) P90(17191.6,13233.8) P95(18866.4,15766.8) P99(21492,19517.9) P99.5(22153.3,20691.3) P99.9(22958.7,22433.9) P100(24000,24000)\r\nhttp.http.downstream_cx_length_ms: P0(0,0) P25(263.877,278.826) P50(439.095,451.77) P75(734.862,838.797) P90(4804.95,2429.53) P95(11279.7,5348.68) P99(18812.7,13661) P99.5(19969.5,16332.4) P99.9(21857.9,20095.2) P100(24000,24000)\r\nhttp.http.downstream_rq_time: P0(0,0) P25(20.9334,20.1753) P50(81.8221,84.8523) P75(176.09,196.652) P90(288.748,409.977) P95(405.828,671.004) P99(739.656,2336.93) P99.5(916.228,3141.9) P99.9(973.535,4489.47) P100(1200,5900)\r\nhttp.http.downstream_rq_time: P0(0,0) P25(13.1269,13.1496) P50(84.1121,86.5577) P75(197.912,228.72) P90(271.741,326.495) P95(362.258,487.716) P99(495.636,1733.87) P99.5(521.998,2414.23) P99.9(890.717,3992.53) P100(1200,5600)\r\nhttp.http.user_agent.android.downstream_cx_length_ms: P0(nan,690) P25(nan,842.5) P50(nan,935) P75(nan,1075) P90(nan,3010) P95(nan,3055) P99(nan,3091) P99.5(nan,3095.5) P99.9(nan,3099.1) P100(nan,3100)\r\nhttp.http.user_agent.ios.downstream_cx_length_ms: P0(190,190) P25(226.875,316.346) P50(345,1062.56) P75(10825,1707.5) P90(17633.3,8930) P95(19862.5,12016.7) P99(21945,19157.5) P99.5(22445,20407.5) P99.9(22889,22163) P100(23000,23000)\r\nhttp.http.user_agent.ios.downstream_cx_length_ms: P0(8,3) P25(146.029,159.488) P50(274.643,347.031) P75(611.324,897.727) P90(1023.33,1724.51) P95(2113.12,2416.75) P99(15242,6327) P99.5(17526.3,11292.8) P99.9(18870.2,17313.5) P100(22000,22000)\r\nlistener.[__]_443.downstream_cx_length_ms: P0(0,0) P25(2716.88,473.846) P50(10414.2,4773.56) P75(12769.3,10733.6) P90(17191.6,13233.8) P95(18866.4,15766.8) P99(21492,19517.9) P99.5(22153.3,20691.3) P99.9(22958.7,22433.9) P100(24000,24000)\r\nlistener.[__]_80.downstream_cx_length_ms: P0(0,0) P25(263.868,278.811) P50(439.086,451.753) P75(734.862,838.787) P90(4804.95,2429.53) P95(11279.7,5348.68) P99(18812.7,13661) P99.5(19969.5,16332.4) P99.9(21857.9,20095.2) P100(24000,24000)\r\nlistener.admin.downstream_cx_length_ms: P0(410,410) P25(412.5,420.833) P50(415,425) P75(417.5,429.167) P90(419,525) P95(419.5,527.5) P99(419.9,529.5) P99.5(419.95,529.75) P99.9(419.99,529.95) P100(420,530)\r\n```\r\n\r\nafter crashing this one became\r\n```\r\nserver.total_connections: 269773\r\n```\r\nnot sure if these aggregated stats from all 3 Envoys or just from 1\r\n\r\nbefore it crashed tcp orphans jumped to high number and inuse dropped from high number to something 15 and tw increased also\r\n```\r\ncat /proc/net/sockstat\r\nsockets: used 480\r\nTCP: inuse 15 orphan 113206 tw 223149 alloc 113229 mem 113807\r\nUDP: inuse 4 mem 1\r\nUDPLITE: inuse 0\r\nRAW: inuse 0\r\nFRAG: inuse 0 memory 0\r\n```\r\n\r\nbacktrace:\r\n\r\n```\r\n[2019-03-28 11:41:53.324][8][info][main] [external/envoy/source/server/drain_manager_impl.cc:63] shutting down parent after drain\r\n[2019-03-28 13:24:14.801][57][critical][assert] [external/envoy/source/common/network/listener_impl.cc:81] panic: listener accept failure: Too many open files\r\n[2019-03-28 13:24:14.801][63][critical][assert] [external/envoy/source/common/network/address_impl.cc:165] assert failure: result.rc_ != -1. Details: socket(2) failed, got error: Too many open files\r\n[2019-03-28 13:24:14.801][57][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:81] Caught Aborted, suspect faulting address 0x8\r\n[2019-03-28 13:24:14.801][57][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:69] Backtrace (use tools/stack_decode.py to get line numbers):\r\n[2019-03-28 13:24:14.802][85][critical][assert] [external/envoy/source/common/network/listener_impl.cc:81] panic: listener accept failure: Too many open files\r\n[2019-03-28 13:24:14.802][41][critical][assert] [external/envoy/source/common/network/listener_impl.cc:81] panic: listener accept failure: Too many open files\r\n[2019-03-28 13:24:14.802][23][critical][assert] [external/envoy/source/common/network/listener_impl.cc:81] panic: listener accept failure: Too many open files\r\n[2019-03-28 13:24:14.802][67][critical][assert] [external/envoy/source/common/network/listener_impl.cc:81] panic: listener accept failure: Too many open files\r\n[2019-03-28 13:24:14.802][18][critical][assert] [external/envoy/source/common/network/listener_impl.cc:81] panic: listener accept failure: Too many open files\r\n[2019-03-28 13:24:14.802][27][critical][assert] [external/envoy/source/common/network/listener_impl.cc:81] panic: listener accept failure: Too many open files\r\n[2019-03-28 13:24:14.802][28][critical][assert] [external/envoy/source/common/network/listener_impl.cc:81] panic: listener accept failure: Too many open files\r\n[2019-03-28 13:24:14.802][51][critical][assert] [external/envoy/source/common/network/listener_impl.cc:81] panic: listener accept failure: Too many open files\r\n[2019-03-28 13:24:14.802][45][critical][assert] [external/envoy/source/common/network/listener_impl.cc:81] panic: listener accept failure: Too many open files\r\n[2019-03-28 13:24:14.802][87][critical][assert] [external/envoy/source/common/network/listener_impl.cc:81] panic: listener accept failure: Too many open files\r\n```"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-28T16:57:53Z",
        "body": "From looking quickly, especially at `cx_active` it looks like there are maybe 400K open connections between downstream and upstream, so theoretically there should be enough fd space without crashing. \r\n\r\nI do wonder if either FDs are getting orphaned between accept and connection creation somehow (maybe via listener filter) or the process is getting overrun or something else. I'm not sure without more digging. I might also follow up with the Gloo folks since I'm not sure if they are doing anything else beyond what is in pure OSS."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-28T16:59:46Z",
        "body": "Also, in newer Envoy, there is a listener stat called `downstream_pre_cx_active` which would be helpful here which I don't see in your stat dump. I would work with the Gloo people to upgrade to current Envoy master and then we can look further. Thank you."
      },
      {
        "user": "benishak",
        "created_at": "2019-03-28T18:43:16Z",
        "body": "we managed to increase FD to something like 5,000,000 this helped much but I'm not comfortable having such high limit.\r\n\r\nI will try last master branch to give more insight. "
      }
    ]
  },
  {
    "number": 6381,
    "title": "Cannot bind a list to map for field 'typed_per_filter_config'",
    "created_at": "2019-03-26T15:39:44Z",
    "closed_at": "2019-03-27T14:36:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6381",
    "body": "I'm trying to write custom envoy control plane and I need per-route config for **ExtAuthz** filter.\r\nHowever, when sending **Map(\"envoy.ext_authz\" -> ExtAuthzPerRoute)** in **typed_per_filter_config** field of a **Route**, I'm getting an error in envoy logs:\r\n\r\n> [2019-03-26 18:31:18.949][030564][warning][config] [bazel-out/k8-fastbuild/bin/source/common/config/_virtual_includes/grpc_mux_subscription_lib/common/config/grpc_mux_subscription_impl.h:70] gRPC config for type.googleapis.com/envoy.api.v2.Listener rejected: Error adding/updating listener rin-listener: Unable to parse JSON as proto (INVALID_ARGUMENT:(route_config.virtual_hosts[0].routes[0]): invalid value Cannot bind a list to map for field 'typed_per_filter_config'. for type Map): {\"http_filters\":[{\"name\":\"envoy.router\"}],\"route_config\":{\"response_headers_to_add\":[],\"name\":\"\",\"virtual_hosts\":[{\"rate_limits\":[],\"response_headers_to_remove\":[],\"routes\":[{\"response_headers_to_remove\":[],\"route\":{\"upgrade_configs\":[],\"request_headers_to_add\":[],\"cluster\":\"cop\",\"hash_policy\":[],\"response_headers_to_add\":[],\"prefix_rewrite\":\"\",\"rate_limits\":[],\"response_headers_to_remove\":[]},\"request_headers_to_remove\":[],\"per_filter_config\":[],\"request_headers_to_add\":[],\"typed_per_filter_config\":[{\"value\":{\"type_url\":\"type.googleapis.com/envoy.config.filter.http.ext_authz.v2.ExtAuthzPerRoute\"},\"key\":\"envoy.ext_authz\"}],\"response_headers_to_add\":[],\"match\":{\"headers\":[{\"name\":\":method\",\"exact_match\":\"PATCH\"}],\"query_parameters\":[],\"regex\":\"/api/v1/a/([^/]+)/b/([^/]+)/c/([^/]+)([/]?)\"}}],\"request_headers_to_remove\":[],\"request_headers_to_add\":[],\"virtual_clusters\":[],\"domains\":[\"*\"],\"response_headers_to_add\":[],\"name\":\"rin\"}],\"response_headers_to_remove\":[],\"request_headers_to_remove\":[],\"request_headers_to_add\":[],\"internal_only_headers\":[]},\"via\":\"\",\"stat_prefix\":\"rin-router-stats\",\"upgrade_configs\":[],\"access_log\":[],\"server_name\":\"\"}\r\n\r\nAny suggestions?.\r\nI'm also tried setting **per_filter_config** - no luck.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6381/comments",
    "author": "rnd4222",
    "comments": [
      {
        "user": "rnd4222",
        "created_at": "2019-03-26T17:13:03Z",
        "body": "And I'm getting another error when trying to set **http_connection_error.HttpFilter.typed_config**:\r\n\r\n> Unable to parse JSON as proto (INVALID_ARGUMENT:(http_filters[1].typed_config): invalid value Missing @type for any field in envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager for type Any)\r\n\r\nSo how do I set **Any** or **Struct** typed fields?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-26T17:33:04Z",
        "body": "cc @htuch @lizan @moderation "
      },
      {
        "user": "lizan",
        "created_at": "2019-03-26T21:55:42Z",
        "body": "@rnd4222 can you post your config or a part of it?"
      },
      {
        "user": "rnd4222",
        "created_at": "2019-03-26T22:07:05Z",
        "body": "@lizan I can post it tomorrow, but it's pretty standard bootstrap config which uses LDS/CDS via gRPC ADS."
      },
      {
        "user": "rnd4222",
        "created_at": "2019-03-27T09:30:09Z",
        "body": "@lizan here is the config:\r\n\r\n```\r\nnode:\r\n  id: node-id-1\r\n  cluster: dc1\r\ndynamic_resources:\r\n  cds_config: { ads: {} }\r\n  lds_config: { ads: {} }\r\n  ads_config:\r\n    api_type: GRPC\r\n    grpc_services:\r\n      envoy_grpc:\r\n        cluster_name: ads_cluster\r\nstatic_resources:\r\n  clusters:\r\n  - name: ads_cluster\r\n    connect_timeout: { seconds: 5 }\r\n    type: STATIC\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 10003\r\n    lb_policy: ROUND_ROBIN\r\n    http2_protocol_options: {}\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address: { address: 127.0.0.1, port_value: 12346 }\r\n```"
      },
      {
        "user": "rnd4222",
        "created_at": "2019-03-27T14:36:46Z",
        "body": "OK, got it - it seems that .proto files are not really backwards-compatible, and control plane always should use proto files from specific envoy version.\r\nClosing the issue."
      },
      {
        "user": "yangyangpig",
        "created_at": "2019-05-23T11:42:15Z",
        "body": "@rnd4222 the issue how to slove it ? which proto version can you use??"
      }
    ]
  },
  {
    "number": 6349,
    "title": "rate limit panic",
    "created_at": "2019-03-21T20:53:28Z",
    "closed_at": "2019-04-28T01:11:26Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6349",
    "body": "the Envoy is panicing when any client is trying to connect to it , it is happening after adding rate_limit to the yaml file :\r\n\r\nthis is the part that i added :\r\n\r\n    filter_chains:\r\n      filters:\r\n      - name: envoy.ratelimit\r\n        config:\r\n          domain: rate_limit_test\r\n          stat_prefix: \"/rate_limit\"\r\n          timeout: 2s\r\n          failure_mode_deny: true\r\n          descriptors: \r\n          - entries:\r\n            - key: \"path\"\r\n              value: \"/not_allow\"\r\n\r\n\r\nafter this change , envoy started with no problem , but when i tried to call \"localhost/not_allow\" , it paniced :\r\n\r\n\r\n[2019-03-21 19:04:22.477][16][critical][assert] [source/common/grpc/async_client_manager_impl.cc:89] panic: not reached\r\n[2019-03-21 19:04:22.477][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:125] Caught Aborted, suspect faulting address 0x1\r\n[2019-03-21 19:04:22.477][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:94] Backtrace thr<16> obj</lib/x86_64-linux-gnu/libc.so.6> (If unsymbolized, use tools/stack_decode.py):\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #0 0x7fbf1d8a7428 (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #1 0x7fbf1d8a9029 (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<16> obj<envoy>\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #2 0x83d686 (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #3 0x7b0823 (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #4 0x60a98c (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #5 0x7adcef (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #6 0x7d2d39 (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #7 0x7d301b (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #8 0x7d220e (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #9 0x7e16b0 (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #10 0xb363c7 (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #11 0xb28c14 (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #12 0xb2935e (unknown)\r\n[2019-03-21 19:04:22.478][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #13 0xb2c0e7 (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #14 0x7d4662 (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #15 0x7d0968 (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #16 0xdcabc4 (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<16> obj</lib/x86_64-linux-gnu/libpthread.so.0>\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<16> #17 0x7fbf1df4c6b9 start_thread\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<16> obj</lib/x86_64-linux-gnu/libc.so.6>\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #18 0x7fbf1d97941c (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:121] end backtrace thread 16\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:125] Caught Segmentation fault, suspect faulting address 0x0\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:94] Backtrace thr<16> obj</lib/x86_64-linux-gnu/libc.so.6> (If unsymbolized, use tools/stack_decode.py):\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #0 0x7fbf1d8a9196 (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<16> obj<envoy>\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #1 0x83d686 (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #2 0x7b0823 (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #3 0x60a98c (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #4 0x7adcef (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #5 0x7d2d39 (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #6 0x7d301b (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #7 0x7d220e (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #8 0x7e16b0 (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #9 0xb363c7 (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #10 0xb28c14 (unknown)\r\n[2019-03-21 19:04:22.479][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #11 0xb2935e (unknown)\r\n[2019-03-21 19:04:22.480][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #12 0xb2c0e7 (unknown)\r\n[2019-03-21 19:04:22.480][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #13 0x7d4662 (unknown)\r\n[2019-03-21 19:04:22.480][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #14 0x7d0968 (unknown)\r\n[2019-03-21 19:04:22.480][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #15 0xdcabc4 (unknown)\r\n[2019-03-21 19:04:22.480][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<16> obj</lib/x86_64-linux-gnu/libpthread.so.0>\r\n[2019-03-21 19:04:22.480][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<16> #16 0x7fbf1df4c6b9 start_thread\r\n[2019-03-21 19:04:22.480][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<16> obj</lib/x86_64-linux-gnu/libc.so.6>\r\n[2019-03-21 19:04:22.480][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<16> #17 0x7fbf1d97941c (unknown)\r\n[2019-03-21 19:04:22.480][16][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:121] end backtrace thread 16\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6349/comments",
    "author": "msarajam",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-03-21T22:47:08Z",
        "body": "I think this is because you have not setup an upstream rate limit server, and you are probably using a build that is an intermediate state with no required annotations. Can you try current master and I think you will get an error?"
      },
      {
        "user": "msarajam",
        "created_at": "2019-03-22T00:22:07Z",
        "body": "I'lll try that , thanks"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-04-21T01:06:41Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-04-28T01:11:25Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6345,
    "title": "TCP Connection Load Balancing",
    "created_at": "2019-03-21T18:25:31Z",
    "closed_at": "2019-05-08T16:52:08Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6345",
    "body": "Title: Envoy Support for TCP Proxy Load Balancing for long-lived connections\r\n\r\nDescription: If one of the upstream endpoint (load balanced) goes down, does envoy make sure it keeps the TCP connection alive using any other live/healthy upstream endpoint.Our use-case is to use envoy as edge proxy and we don't want want the TCP connection to be dropped at all in case of upstream restarts/patches.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6345/comments",
    "author": "rbkumar88",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-04-01T15:54:35Z",
        "body": "@rbkumar88 not sure I fully understand what you are looking for. TCP connections are point to point, so if the upstream goes away it must be dropped."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-01T16:19:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-08T16:52:07Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6270,
    "title": "When a gRPC method returns a stream, the HTTP1 status code always returns 200 OK.",
    "created_at": "2019-03-12T18:35:55Z",
    "closed_at": "2019-05-24T20:26:56Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6270",
    "body": "*Title*: When a gRPC method returns a stream, the HTTP1 status code always returns 200 OK.\r\n\r\n*Description*:\r\n>When invoking a gRPC service method that returns a stream of some message, the HTTP 1.x status code is always 200 OK and the response payload an empty JSON array.  This is when using the **envoy.grpc_json_transcoder** and supplying the protobin file generated from the .proto file.\r\n\r\n*Steps to Reproduce*\r\n>Create a simple .proto service like below:\r\n```\r\n\trpc Print(google.protobuf.Empty) returns (stream StringMessage) {\r\n\t   option (google.api.http) = {\r\n\t       get: \"/v1/example/print\"\r\n\t   };\r\n\t}\r\n```\r\n>Implement the service in Java which always throws an **UNAUTHENTICATED** StatusException:\r\n```\r\n    @Override\r\n    public void print(Empty request, StreamObserver<StringMessage> responseObserver) {\r\n        responseObserver.onError(Status.UNAUTHENTICATED.asException());\r\n    }\r\n```\r\n>Use a REST client (like Postman) to hit the service through a properly configure instance of Envoy Proxy (with envoy.grpc_json_transoder enabled and pointed to the proto descriptor).  Note that even though the HTTP1 response header \"**grpc-status: 16**\" is present, the HTTP1 status code is still 200.\r\n\r\n*Workaround*\r\n>You can work around this issue by returning a message type which wraps a **repeated** message field.  This limits the HTTP2 clients to processing the repeated items only after all of the responses are sent.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6270/comments",
    "author": "jbf154",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-03-12T19:20:15Z",
        "body": "This is due to the limitation of HTTP 1, which must encode the status at the beginning of the stream, while gRPC may send the status at the end of the stream."
      },
      {
        "user": "jbf154",
        "created_at": "2019-03-12T19:39:32Z",
        "body": "@lizan, is the stream not buffered in Envoy such that the headers aren't sent back to the HTTP 1 client until after the grpc-status is received?  I don't think Envoy can return the response body back to the HTTP 1 client until it has reached the end of the stream anyways."
      },
      {
        "user": "lizan",
        "created_at": "2019-03-12T20:43:01Z",
        "body": "No, in streaming case envoy doesn't buffer the stream until grpc-status, that's the limitation of transcoding streaming, you'll have to encode the error in the body if desired. Response trailer will be omitted."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-04-11T21:40:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "jbf154",
        "created_at": "2019-04-17T19:36:58Z",
        "body": "This behavior was also brought up in issue #5011 and it was supposed to be looked in to, but never was.  Basically, if the trailers are sent right away (before any response is sent back) wouldn't it be possible to correctly set the HTTP 1 status code?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-17T20:01:29Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-24T20:26:55Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "mittalvikas",
        "created_at": "2019-05-29T06:16:56Z",
        "body": "I am also stuck at this point, any suggestions?"
      }
    ]
  },
  {
    "number": 6257,
    "title": "Error building envoy",
    "created_at": "2019-03-11T23:19:55Z",
    "closed_at": "2019-04-19T06:10:57Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6257",
    "body": "Build envoy by `./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.release.server_only'`\r\n\r\nThe error message is not very straight forward.\r\nFrom `source/common/common/BUILD`\r\n\r\n```\r\ngenrule(\r\n    name = \"generate_version_number\",\r\n    srcs = [\"//:VERSION\"],\r\n    outs = [\"version_number.h\"],\r\n    cmd = \"\"\"echo \"#define BUILD_VERSION_NUMBER \\\\\"$$(cat $<)\\\\\"\" >$@\"\"\",\r\n)\r\n```\r\n\r\n\r\n> \r\n> [2019-03-11T23:15:06Z] + ./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.release.server_only'\r\n> --\r\n>   | [2019-03-11T23:15:07Z] No remote cache bucket is set, skipping setup remote cache.\r\n>   | [2019-03-11T23:15:07Z] ENVOY_SRCDIR=/source\r\n>   | [2019-03-11T23:15:07Z] $TEST_TMPDIR defined: output root default is '/build/tmp' and max_idle_secs default is '15'.\r\n>   | [2019-03-11T23:15:07Z] WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n>   | [2019-03-11T23:15:07Z] /source/tools/bazel.rc\r\n>   | [2019-03-11T23:15:07Z] Extracting Bazel installation...\r\n>   | [2019-03-11T23:15:13Z] Starting local Bazel server and connecting to it...\r\n>   | [2019-03-11T23:15:32Z] Cloning into '/build/envoy-filter-example'...\r\n>   | [2019-03-11T23:15:33Z] remote: Enumerating objects: 38, done.\r\n>   | remote: Counting objects: 100% (38/38), done.\r\n>   | remote: Compressing objects: 100% (29/29), done.\r\n>   | remote: Total 3722 (delta 36), reused 11 (delta 9), pack-reused 3684\r\n>   | Receiving objects: 100% (3722/3722), 372.68 KiB \\| 0 bytes/s, done.\r\n>   | Resolving deltas: 100% (3601/3601), done.  0% (0/3601)\r\n>   | [2019-03-11T23:15:33Z] Checking connectivity... done.\r\n>   | [2019-03-11T23:15:33Z] Note: checking out '6c0625cb4cc9a21df97cef2a1d065463f2ae81ae'.\r\n>   | [2019-03-11T23:15:33Z]\r\n>   | [2019-03-11T23:15:33Z] You are in 'detached HEAD' state. You can look around, make experimental\r\n>   | [2019-03-11T23:15:33Z] changes and commit them, and you can discard any commits you make in this\r\n>   | [2019-03-11T23:15:33Z] state without impacting any branches by performing another checkout.\r\n>   | [2019-03-11T23:15:33Z]\r\n>   | [2019-03-11T23:15:33Z] If you want to create a new branch to retain commits you create, you may\r\n>   | [2019-03-11T23:15:33Z] do so (now or later) by using -b with the checkout command again. Example:\r\n>   | [2019-03-11T23:15:33Z]\r\n>   | [2019-03-11T23:15:33Z]   git checkout -b <new-branch-name>\r\n>   | [2019-03-11T23:15:33Z]\r\n>   | [2019-03-11T23:15:33Z] HEAD is now at 6c0625c... integration: make test use 3-arg base class ctor (#69)\r\n>   | [2019-03-11T23:15:33Z] building using 2 CPUs\r\n>   | [2019-03-11T23:15:33Z] gcc/g++ toolchain configured\r\n>   | [2019-03-11T23:15:33Z] bazel release build...\r\n>   | [2019-03-11T23:15:33Z] Building...\r\n>   | [2019-03-11T23:15:33Z] $TEST_TMPDIR defined: output root default is '/build/tmp' and max_idle_secs default is '15'.\r\n>   | [2019-03-11T23:15:33Z] Starting local Bazel server and connecting to it...\r\n>   | [2019-03-11T23:15:35Z] INFO: Writing tracer profile to '/build/tmp/_bazel_bazel/436badd4919a15958fa3800a4e21074a/command.profile'\r\n>   | ERROR: /source/source/common/common/BUILD:247:1: no such package '': BUILD file not found on package path and referenced by '//source/common/common:generate_version_number'\r\n>   | ERROR: Analysis of target '//source/exe:envoy-static' failed; build aborted: Analysis failed\r\n>   | INFO: Elapsed time: 37.785s\r\n>   | INFO: 0 processes.\r\n>   | FAILED: Build did NOT complete successfully (185 packages loaded, 4914 targets\\\r\n>   | [2019-03-11T23:16:11Z]  configured)\r\n>   | [2019-03-11T23:16:11Z] 🚨 Error: The command exited with status 1\r\n> \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6257/comments",
    "author": "hanyu-liu",
    "comments": [
      {
        "user": "hanyu-liu",
        "created_at": "2019-03-12T22:51:06Z",
        "body": "The generated file seems should be `#define BUILD_VERSION_NUMBER \"1.10.0-dev\"`"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-04-11T23:40:19Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-04-19T06:10:56Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6218,
    "title": "How to open the tcp keepalive when envoy is the server?",
    "created_at": "2019-03-08T09:40:32Z",
    "closed_at": "2019-04-18T09:09:07Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6218",
    "body": "",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6218/comments",
    "author": "devincd",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-04-11T06:23:13Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-04-18T09:09:06Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 6212,
    "title": "Feedback mechanism when Endpoints are changing [question]",
    "created_at": "2019-03-07T18:32:32Z",
    "closed_at": "2019-03-11T19:21:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6212",
    "body": "**Issue Template**\r\n\r\nI am investigating the feasibility of using Envoy as the gateway/reverse proxy in a distributed systems framework at Microsoft.\r\n\r\nWe have a system that fail's over primary replicas to a new node within a few milliseconds. When this happens, the fastest way to identify the new endpoint is for the client to \"pull\" the new location when the old endpoint returns an error code. For this to work, the client has to write code that triggers a http endpoint which we can use as a signal to \"pull\" the new endpoint location and configure envoy correctly.\r\n\r\nWithout the pull model, all nodes in the cluster learn about the new endpoint in 5 seconds and hence Envoy would eventually be configured to route to the correct endpoint on all the nodes. (There is 1 envoy on each node).\r\n\r\nI am trying to improve this mechanism to instead re-configure Envoy as soon as the new endpoint becomes available, rather than having to wait for the 5 second interval we have. In order to write simpler clients who don't have to signal to \"pull\" the new endpoint, is there a way for Envoy to provide a notification when an endpoint returns a 404 or other connection related error code so that we can proactively do a 'pull' without involving the client?\r\n\r\nLooking forward to some ideas here.\r\n\r\nThanks!\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6212/comments",
    "author": "sumukhs",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-03-08T16:35:54Z",
        "body": "@sumukhs there is nothing built-in to do this today, however why not just use gRPC streaming discovery and push updated endpoints when they are ready?"
      },
      {
        "user": "sumukhs",
        "created_at": "2019-03-08T17:57:19Z",
        "body": "@mattklein123 - We have a central service which runs on a single node (with secondary replicas on other nodes) that learns about the updated endpoints first. On this particular node, it is possible to push the update to envoy using gRPC ASAP.\r\n\r\nHowever, the other nodes in the cluster discover this update after a configured interval (5 seconds) through a reliable broadcast message we send out. We could reduce this interval to a smaller value, but at the cost of too many updates when a whole node goes down, taking down many services. The batching is done to reduce the number of broadcasts being processed.\r\n\r\nSo if a client was connected to an envoy proxy on a random node, it would take ~5 seconds to receive the updated endpoints. This latency is okay for nodes if there are no active clients to the service, but I am looking for ways to improve this time for nodes that do have an active client connected.\r\n\r\nOne option I was thinking of exploring was to add an envoy filter that identifies an error code corresponding to 'HTTP endpoint missing' and signals our process on the local node to \"pull\" the required update immediately from the central service. This way, client critical envoy's get their updates immediately, while other envoy processes get them lazily. Do you see any fundamental issues with this approach?\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-08T21:05:44Z",
        "body": "> Do you see any fundamental issues with this approach?\r\n\r\nI think that could work without too much complexity. I still wonder why you require broadcasts vs. just everyone getting streamed updates, but I obviously don't understand the details of your architecture."
      },
      {
        "user": "sumukhs",
        "created_at": "2019-03-08T21:16:40Z",
        "body": "Thanks. Are there pointers to examples of writing such filters?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-11T01:02:54Z",
        "body": "@sumukhs I might take a look at the health check filter, as it is an encoding filter that inspects headers which I think is mostly what you need."
      }
    ]
  },
  {
    "number": 6159,
    "title": "Bazel build failed in foreign_cc",
    "created_at": "2019-03-04T18:31:44Z",
    "closed_at": "2019-03-04T22:14:04Z",
    "labels": [
      "question",
      "area/build"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6159",
    "body": "**Bug Template**\r\n\r\n*Title*: Bazel build failed in foreign_cc\r\n\r\n*Description*:\r\nWith latest tree, and with bazel 0.21.0,   when \"bazel build //souce/...\", I got\r\n\r\nbazel build //source/exe:envoy-static\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/home/qiwzhang/github/envoyproxy/envoy/tools/bazel.rc\r\nERROR: /home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/envoy/bazel/foreign_cc/BUILD:86:1: in cmake_external rule @envoy//bazel/foreign_cc:yaml: \r\nTraceback (most recent call last):\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/envoy/bazel/foreign_cc/BUILD\", line 86\r\n                cmake_external(name = 'yaml')\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/rules_foreign_cc/tools/build_defs/cmake.bzl\", line 47, in _cmake_external\r\n                cc_external_rule_impl(ctx, attrs)\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/foreign_cc_impl/framework.bzl\", line 209, in cc_external_rule_impl\r\n                _define_out_cc_info(ctx, attrs, inputs, outputs)\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/foreign_cc_impl/framework.bzl\", line 627, in _define_out_cc_info\r\n                cc_common.create_compilation_context(ctx = ctx, headers = depset([outpu...]), <2 more arguments>)\r\nunexpected keyword 'ctx', for call to method create_compilation_context(headers = unbound, system_includes = unbound, includes = unbound, quote_includes = unbound, defines = unbound) of 'cc_common'\r\nERROR: Analysis of target '//source/exe:envoy-static' failed; build aborted: Analysis of target '@envoy//bazel/foreign_cc:yaml' failed; build aborted\r\nINFO: Elapsed time: 1.483s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (213 packages loaded, 4783 targets configured)\r\n    Fetching @com_github_gperftools_gperftools; Patching repository\r\n    Fetching @com_github_nghttp2_nghttp2; fetching\r\n    Fetching @com_github_c_ares_c_ares; fetching\r\n    Fetching @com_github_circonus_labs_libcircllhist; fetching\r\n    Fetching @boringssl; fetching\r\n    Fetching @com_github_madler_zlib; fetching\r\n\r\n\r\n\r\nBTW,  I also tried with bazel 0.23.0. Same issue\r\n\r\nAny clues?\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6159/comments",
    "author": "qiwzhang",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-03-04T20:28:55Z",
        "body": "Did you try `bazel clean --expunge`? 0.21 doesn't work with latest, but 0.22 or 0.23 should work and they are tested in CI."
      },
      {
        "user": "qiwzhang",
        "created_at": "2019-03-04T22:13:58Z",
        "body": "Thanks.   It works now after \"bazel clean --expunge\"\r\n"
      }
    ]
  },
  {
    "number": 6130,
    "title": "Bug in tracking active gRPC requests?",
    "created_at": "2019-02-28T16:17:14Z",
    "closed_at": "2019-04-07T16:27:29Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6130",
    "body": "See Update below for what I think may be a bug in tracking active gRPC requests. Original issue shown below:\r\n\r\nThis is a question more than anything else -- is it expected that the least request algorithm can properly load balance gRPC stream requests? I think I've run into a scenario where it does not. Below I've captured some statistics from a gRPC cluster receiving stream requests, the cluster has 15 servicers and each second (approximately) the servicers output how many active requests they are handling/queuing. \r\n\r\n```\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-4: 0\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-14: 0\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-13: 28\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-5: 0\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-6: 0\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-12: 1\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-1: 0\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-2: 0\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-3: 0\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-7: 0\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-9: 36\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-11: 0\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-15: 30\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-10: 0\r\n2019-02-28 16:18:25 - [INFO] root [redacted.run]: RedactedWrapper-8: 13\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-4: 0\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-14: 0\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-13: 28\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-5: 0\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-6: 0\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-12: 0\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-1: 0\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-2: 0\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-3: 0\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-7: 0\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-9: 37\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-11: 0\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-15: 30\r\n2019-02-28 16:18:26 - [INFO] root [redacted.run]: RedactedWrapper-10: 0\r\n2019-02-28 16:18:27 - [INFO] root [redacted.run]: RedactedWrapper-8: 14\r\n2019-02-28 16:18:27 - [INFO] root [redacted.run]: RedactedWrapper-4: 0\r\n2019-02-28 16:18:27 - [INFO] root [redacted.run]: RedactedWrapper-14: 0\r\n2019-02-28 16:18:27 - [INFO] root [redacted.run]: RedactedWrapper-13: 28\r\n2019-02-28 16:18:27 - [INFO] root [redacted.run]: RedactedWrapper-5: 0\r\n2019-02-28 16:18:27 - [INFO] root [redacted.run]: RedactedWrapper-6: 0\r\n2019-02-28 16:18:27 - [INFO] root [redacted.run]: RedactedWrapper-12: 1\r\n2019-02-28 16:18:27 - [INFO] root [redacted.run]: RedactedWrapper-1: 0\r\n2019-02-28 16:18:27 - [INFO] root [redacted.run]: RedactedWrapper-2: 0\r\n2019-02-28 16:18:27 - [INFO] root [redacted.run]: RedactedWrapper-3: 0\r\n```\r\n\r\nYou can see that, for example, RedactedWrapper-9 continues to receive requests despite it having the most active requests -- nearly all of the other servicers have no active requests, which (according to least request) should mean that RedactedWrapper-9 should _not_ receive new requests. \r\n\r\nAm I missing something about how the least request algorithm should work? I can provide stats information as needed, but all servicers in this cluster are shown as \"healthy\" (according to a TCP health check). \r\n\r\nI'll also mention that one conclusion I haven't resolved yet is that requests are so numerous and being handled so quickly that the frequency of reporting the active RPC count every second is deceiving. However, I still think it'd be helpful to get confirmation that the least request algo should work in this situation.\r\n\r\n**Update**:\r\nHere's a bit more info I'm able to glean from cluster stats:\r\n\r\nFirst, it seems that the number of RPCs reported by gRPC does not match the number of active requests reported in Envoy. Not sure what to make of that (if anything at all). I'm seeing numbers reported in gRPC that a single servicer has ~230 active RPCs while Envoy says it has ~20 active requests. Here's more details on that particular servicer:\r\n\r\n```\r\nredacted::172.17.0.2:8510::rq_active::20\r\nredacted::172.17.0.2:8510::rq_error::43\r\nredacted::172.17.0.2:8510::rq_success::4900\r\nredacted::172.17.0.2:8510::rq_timeout::0\r\nredacted::172.17.0.2:8510::rq_total::5189\r\n```\r\n\r\nMost striking to me about that servicer is that the number of active and error requests do not sum to the difference between the success and total. total - success = 289. active + error = 63. 289 - 63 is 226, which is the approximate number of RPCs reported by gRPC. Where did the rest of the 226 requests go in Envoy? Is the _true number_ of \"active\" requests actually 246?\r\n\r\nSecond, here's an example of active requests for the cluster. `172.17.0.2:8504` bounces in the range of 24-26 while the other servicers bounce in the range of 0-3 -- I collected these stats every second, which means it can have the same problem I described earlier with monitoring the count of RPCs (requests may be handled faster than the reporting frequency). All that said, that outlier servicer does appear to get requests when other servicers appear to have no active requests. \r\n```\r\nredacted::172.17.0.2:8500::rq_active::0\r\nredacted::172.17.0.2:8501::rq_active::0\r\nredacted::172.17.0.2:8502::rq_active::0\r\nredacted::172.17.0.2:8503::rq_active::0\r\nredacted::172.17.0.2:8504::rq_active::26\r\nredacted::172.17.0.2:8505::rq_active::0\r\nredacted::172.17.0.2:8506::rq_active::0\r\nredacted::172.17.0.2:8507::rq_active::0\r\nredacted::172.17.0.2:8508::rq_active::0\r\nredacted::172.17.0.2:8509::rq_active::0\r\nredacted::172.17.0.2:8510::rq_active::0\r\nredacted::172.17.0.2:8511::rq_active::0\r\nredacted::172.17.0.2:8512::rq_active::0\r\nredacted::172.17.0.2:8513::rq_active::0\r\nredacted::172.17.0.2:8514::rq_active::0\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6130/comments",
    "author": "jshlbrd",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-03-01T04:54:49Z",
        "body": "> This is a question more than anything else -- is it expected that the least request algorithm can properly load balance gRPC stream requests\r\n\r\nYes, gRPC requests are really just HTTP requests so they should be load balanced correctly. I think it would be useful to have a full stat dump, envoy version, etc. IIRC rq_error for per-host doesn't actually capture all errors (I think @ramaraochavali encountered this recently).\r\n\r\nI suspect what is happening here is that Envoy is failing/cleaning up the requests due to circuit breaking, timeouts, etc. but the Python process never cleans them up (or something like that)."
      },
      {
        "user": "jshlbrd",
        "created_at": "2019-03-01T15:11:26Z",
        "body": "@mattklein123 thanks for your reply, here're some dumps from a cluster currently exhibiting this behavior. `172.17.0.2:8501` has ~90 \"active\" RPCs at the time this info was dumped.\r\n\r\n**stats**\r\n```\r\n\r\ncluster.redacted.circuit_breakers.default.cx_open: 0\r\ncluster.redacted.circuit_breakers.default.rq_open: 0\r\ncluster.redacted.circuit_breakers.default.rq_pending_open: 0\r\ncluster.redacted.circuit_breakers.default.rq_retry_open: 0\r\ncluster.redacted.circuit_breakers.high.cx_open: 0\r\ncluster.redacted.circuit_breakers.high.rq_open: 0\r\ncluster.redacted.circuit_breakers.high.rq_pending_open: 0\r\ncluster.redacted.circuit_breakers.high.rq_retry_open: 0\r\ncluster.redacted.external.upstream_rq_200: 547115\r\ncluster.redacted.external.upstream_rq_2xx: 547115\r\ncluster.redacted.external.upstream_rq_completed: 547115\r\ncluster.redacted.health_check.attempt: 8595\r\ncluster.redacted.health_check.degraded: 0\r\ncluster.redacted.health_check.failure: 0\r\ncluster.redacted.health_check.healthy: 15\r\ncluster.redacted.health_check.network_failure: 0\r\ncluster.redacted.health_check.passive_failure: 0\r\ncluster.redacted.health_check.success: 8595\r\ncluster.redacted.health_check.verify_cluster: 0\r\ncluster.redacted.http2.header_overflow: 0\r\ncluster.redacted.http2.headers_cb_no_stream: 0\r\ncluster.redacted.http2.rx_messaging_error: 0\r\ncluster.redacted.http2.rx_reset: 70\r\ncluster.redacted.http2.too_many_header_frames: 0\r\ncluster.redacted.http2.trailers: 0\r\ncluster.redacted.http2.tx_reset: 1805\r\ncluster.redacted.lb_healthy_panic: 0\r\ncluster.redacted.lb_local_cluster_not_ok: 0\r\ncluster.redacted.lb_recalculate_zone_structures: 0\r\ncluster.redacted.lb_subsets_active: 0\r\ncluster.redacted.lb_subsets_created: 0\r\ncluster.redacted.lb_subsets_fallback: 0\r\ncluster.redacted.lb_subsets_removed: 0\r\ncluster.redacted.lb_subsets_selected: 0\r\ncluster.redacted.lb_zone_cluster_too_small: 0\r\ncluster.redacted.lb_zone_no_capacity_left: 0\r\ncluster.redacted.lb_zone_number_differs: 0\r\ncluster.redacted.lb_zone_routing_all_directly: 0\r\ncluster.redacted.lb_zone_routing_cross_zone: 0\r\ncluster.redacted.lb_zone_routing_sampled: 0\r\ncluster.redacted.max_host_weight: 0\r\ncluster.redacted.membership_change: 1\r\ncluster.redacted.membership_degraded: 0\r\ncluster.redacted.membership_healthy: 15\r\ncluster.redacted.membership_total: 15\r\ncluster.redacted.original_dst_host_invalid: 0\r\ncluster.redacted.outlier_detection.ejections_active: 0\r\ncluster.redacted.outlier_detection.ejections_consecutive_5xx: 0\r\ncluster.redacted.outlier_detection.ejections_detected_consecutive_5xx: 0\r\ncluster.redacted.outlier_detection.ejections_detected_consecutive_gateway_failure: 0\r\ncluster.redacted.outlier_detection.ejections_detected_success_rate: 0\r\ncluster.redacted.outlier_detection.ejections_enforced_consecutive_5xx: 0\r\ncluster.redacted.outlier_detection.ejections_enforced_consecutive_gateway_failure: 0\r\ncluster.redacted.outlier_detection.ejections_enforced_success_rate: 0\r\ncluster.redacted.outlier_detection.ejections_enforced_total: 0\r\ncluster.redacted.outlier_detection.ejections_overflow: 0\r\ncluster.redacted.outlier_detection.ejections_success_rate: 0\r\ncluster.redacted.outlier_detection.ejections_total: 0\r\ncluster.redacted.retry_or_shadow_abandoned: 0\r\ncluster.redacted.update_attempt: 0\r\ncluster.redacted.update_empty: 0\r\ncluster.redacted.update_failure: 0\r\ncluster.redacted.update_no_rebuild: 0\r\ncluster.redacted.update_success: 0\r\ncluster.redacted.upstream_cx_active: 30\r\ncluster.redacted.upstream_cx_close_notify: 0\r\ncluster.redacted.upstream_cx_connect_attempts_exceeded: 0\r\ncluster.redacted.upstream_cx_connect_fail: 0\r\ncluster.redacted.upstream_cx_connect_timeout: 0\r\ncluster.redacted.upstream_cx_destroy: 0\r\ncluster.redacted.upstream_cx_destroy_local: 0\r\ncluster.redacted.upstream_cx_destroy_local_with_active_rq: 0\r\ncluster.redacted.upstream_cx_destroy_remote: 0\r\ncluster.redacted.upstream_cx_destroy_remote_with_active_rq: 0\r\ncluster.redacted.upstream_cx_destroy_with_active_rq: 0\r\ncluster.redacted.upstream_cx_http1_total: 0\r\ncluster.redacted.upstream_cx_http2_total: 30\r\ncluster.redacted.upstream_cx_idle_timeout: 0\r\ncluster.redacted.upstream_cx_max_requests: 0\r\ncluster.redacted.upstream_cx_none_healthy: 0\r\ncluster.redacted.upstream_cx_overflow: 0\r\ncluster.redacted.upstream_cx_protocol_error: 0\r\ncluster.redacted.upstream_cx_rx_bytes_buffered: 35354\r\ncluster.redacted.upstream_cx_rx_bytes_total: 1082767812\r\ncluster.redacted.upstream_cx_total: 30\r\ncluster.redacted.upstream_cx_tx_bytes_buffered: 0\r\ncluster.redacted.upstream_cx_tx_bytes_total: 18354081289\r\ncluster.redacted.upstream_flow_control_backed_up_total: 512\r\ncluster.redacted.upstream_flow_control_drained_total: 509\r\ncluster.redacted.upstream_flow_control_paused_reading_total: 275\r\ncluster.redacted.upstream_flow_control_resumed_reading_total: 270\r\ncluster.redacted.upstream_internal_redirect_failed_total: 0\r\ncluster.redacted.upstream_internal_redirect_succeeded_total: 0\r\ncluster.redacted.upstream_rq_200: 547115\r\ncluster.redacted.upstream_rq_2xx: 547115\r\ncluster.redacted.upstream_rq_active: 43\r\ncluster.redacted.upstream_rq_cancelled: 0\r\ncluster.redacted.upstream_rq_completed: 547115\r\ncluster.redacted.upstream_rq_maintenance_mode: 0\r\ncluster.redacted.upstream_rq_pending_active: 0\r\ncluster.redacted.upstream_rq_pending_failure_eject: 0\r\ncluster.redacted.upstream_rq_pending_overflow: 0\r\ncluster.redacted.upstream_rq_pending_total: 30\r\ncluster.redacted.upstream_rq_per_try_timeout: 0\r\ncluster.redacted.upstream_rq_retry: 0\r\ncluster.redacted.upstream_rq_retry_overflow: 0\r\ncluster.redacted.upstream_rq_retry_success: 0\r\ncluster.redacted.upstream_rq_rx_reset: 0\r\ncluster.redacted.upstream_rq_timeout: 0\r\ncluster.redacted.upstream_rq_total: 548920\r\ncluster.redacted.upstream_rq_tx_reset: 1762\r\ncluster.redacted.version: 0\r\ncluster_manager.active_clusters: 1\r\ncluster_manager.cluster_added: 1\r\ncluster_manager.cluster_modified: 0\r\ncluster_manager.cluster_removed: 0\r\ncluster_manager.cluster_updated: 0\r\ncluster_manager.cluster_updated_via_merge: 0\r\ncluster_manager.update_merge_cancelled: 0\r\ncluster_manager.update_out_of_merge_window: 0\r\ncluster_manager.warming_clusters: 0\r\nfilesystem.flushed_by_timer: 179\r\nfilesystem.reopen_failed: 0\r\nfilesystem.write_buffered: 5\r\nfilesystem.write_completed: 4\r\nfilesystem.write_total_buffered: 0\r\nhttp.admin.downstream_cx_active: 1\r\nhttp.admin.downstream_cx_delayed_close_timeout: 0\r\nhttp.admin.downstream_cx_destroy: 5\r\nhttp.admin.downstream_cx_destroy_active_rq: 0\r\nhttp.admin.downstream_cx_destroy_local: 0\r\nhttp.admin.downstream_cx_destroy_local_active_rq: 0\r\nhttp.admin.downstream_cx_destroy_remote: 5\r\nhttp.admin.downstream_cx_destroy_remote_active_rq: 0\r\nhttp.admin.downstream_cx_drain_close: 0\r\nhttp.admin.downstream_cx_http1_active: 1\r\nhttp.admin.downstream_cx_http1_total: 6\r\nhttp.admin.downstream_cx_http2_active: 0\r\nhttp.admin.downstream_cx_http2_total: 0\r\nhttp.admin.downstream_cx_idle_timeout: 0\r\nhttp.admin.downstream_cx_overload_disable_keepalive: 0\r\nhttp.admin.downstream_cx_protocol_error: 0\r\nhttp.admin.downstream_cx_rx_bytes_buffered: 83\r\nhttp.admin.downstream_cx_rx_bytes_total: 506\r\nhttp.admin.downstream_cx_ssl_active: 0\r\nhttp.admin.downstream_cx_ssl_total: 0\r\nhttp.admin.downstream_cx_total: 6\r\nhttp.admin.downstream_cx_tx_bytes_buffered: 0\r\nhttp.admin.downstream_cx_tx_bytes_total: 49268\r\nhttp.admin.downstream_cx_upgrades_active: 0\r\nhttp.admin.downstream_cx_upgrades_total: 0\r\nhttp.admin.downstream_flow_control_paused_reading_total: 0\r\nhttp.admin.downstream_flow_control_resumed_reading_total: 0\r\nhttp.admin.downstream_rq_1xx: 0\r\nhttp.admin.downstream_rq_2xx: 4\r\nhttp.admin.downstream_rq_3xx: 0\r\nhttp.admin.downstream_rq_4xx: 1\r\nhttp.admin.downstream_rq_5xx: 0\r\nhttp.admin.downstream_rq_active: 1\r\nhttp.admin.downstream_rq_completed: 5\r\nhttp.admin.downstream_rq_http1_total: 6\r\nhttp.admin.downstream_rq_http2_total: 0\r\nhttp.admin.downstream_rq_idle_timeout: 0\r\nhttp.admin.downstream_rq_non_relative_path: 0\r\nhttp.admin.downstream_rq_overload_close: 0\r\nhttp.admin.downstream_rq_response_before_rq_complete: 0\r\nhttp.admin.downstream_rq_rx_reset: 0\r\nhttp.admin.downstream_rq_timeout: 0\r\nhttp.admin.downstream_rq_too_large: 0\r\nhttp.admin.downstream_rq_total: 6\r\nhttp.admin.downstream_rq_tx_reset: 0\r\nhttp.admin.downstream_rq_ws_on_non_ws_route: 0\r\nhttp.admin.rs_too_large: 0\r\nhttp.async-client.no_cluster: 0\r\nhttp.async-client.no_route: 0\r\nhttp.async-client.rq_direct_response: 0\r\nhttp.async-client.rq_redirect: 0\r\nhttp.async-client.rq_reset_after_downstream_response_started: 0\r\nhttp.async-client.rq_total: 0\r\nhttp.ingress_http.downstream_cx_active: 2\r\nhttp.ingress_http.downstream_cx_delayed_close_timeout: 0\r\nhttp.ingress_http.downstream_cx_destroy: 573\r\nhttp.ingress_http.downstream_cx_destroy_active_rq: 0\r\nhttp.ingress_http.downstream_cx_destroy_local: 0\r\nhttp.ingress_http.downstream_cx_destroy_local_active_rq: 0\r\nhttp.ingress_http.downstream_cx_destroy_remote: 573\r\nhttp.ingress_http.downstream_cx_destroy_remote_active_rq: 0\r\nhttp.ingress_http.downstream_cx_drain_close: 0\r\nhttp.ingress_http.downstream_cx_http1_active: 0\r\nhttp.ingress_http.downstream_cx_http1_total: 0\r\nhttp.ingress_http.downstream_cx_http2_active: 2\r\nhttp.ingress_http.downstream_cx_http2_total: 2\r\nhttp.ingress_http.downstream_cx_idle_timeout: 0\r\nhttp.ingress_http.downstream_cx_overload_disable_keepalive: 0\r\nhttp.ingress_http.downstream_cx_protocol_error: 0\r\nhttp.ingress_http.downstream_cx_rx_bytes_buffered: 6102\r\nhttp.ingress_http.downstream_cx_rx_bytes_total: 24421432135\r\nhttp.ingress_http.downstream_cx_ssl_active: 0\r\nhttp.ingress_http.downstream_cx_ssl_total: 0\r\nhttp.ingress_http.downstream_cx_total: 575\r\nhttp.ingress_http.downstream_cx_tx_bytes_buffered: 0\r\nhttp.ingress_http.downstream_cx_tx_bytes_total: 1076449705\r\nhttp.ingress_http.downstream_cx_upgrades_active: 0\r\nhttp.ingress_http.downstream_cx_upgrades_total: 0\r\nhttp.ingress_http.downstream_flow_control_paused_reading_total: 512\r\nhttp.ingress_http.downstream_flow_control_resumed_reading_total: 509\r\nhttp.ingress_http.downstream_rq_1xx: 0\r\nhttp.ingress_http.downstream_rq_2xx: 547115\r\nhttp.ingress_http.downstream_rq_3xx: 0\r\nhttp.ingress_http.downstream_rq_4xx: 0\r\nhttp.ingress_http.downstream_rq_5xx: 0\r\nhttp.ingress_http.downstream_rq_active: 43\r\nhttp.ingress_http.downstream_rq_completed: 547115\r\nhttp.ingress_http.downstream_rq_http1_total: 0\r\nhttp.ingress_http.downstream_rq_http2_total: 548920\r\nhttp.ingress_http.downstream_rq_idle_timeout: 0\r\nhttp.ingress_http.downstream_rq_non_relative_path: 0\r\nhttp.ingress_http.downstream_rq_overload_close: 0\r\nhttp.ingress_http.downstream_rq_response_before_rq_complete: 43\r\nhttp.ingress_http.downstream_rq_rx_reset: 1805\r\nhttp.ingress_http.downstream_rq_timeout: 0\r\nhttp.ingress_http.downstream_rq_too_large: 0\r\nhttp.ingress_http.downstream_rq_total: 548920\r\nhttp.ingress_http.downstream_rq_tx_reset: 0\r\nhttp.ingress_http.downstream_rq_ws_on_non_ws_route: 0\r\nhttp.ingress_http.no_cluster: 0\r\nhttp.ingress_http.no_route: 0\r\nhttp.ingress_http.rq_direct_response: 0\r\nhttp.ingress_http.rq_redirect: 0\r\nhttp.ingress_http.rq_reset_after_downstream_response_started: 0\r\nhttp.ingress_http.rq_total: 548920\r\nhttp.ingress_http.rs_too_large: 0\r\nhttp.ingress_http.tracing.client_enabled: 0\r\nhttp.ingress_http.tracing.health_check: 0\r\nhttp.ingress_http.tracing.not_traceable: 0\r\nhttp.ingress_http.tracing.random_sampling: 0\r\nhttp.ingress_http.tracing.service_forced: 0\r\nhttp2.header_overflow: 0\r\nhttp2.headers_cb_no_stream: 0\r\nhttp2.rx_messaging_error: 0\r\nhttp2.rx_reset: 1762\r\nhttp2.too_many_header_frames: 0\r\nhttp2.trailers: 0\r\nhttp2.tx_reset: 43\r\nlistener.0.0.0.0_8443.downstream_cx_active: 2\r\nlistener.0.0.0.0_8443.downstream_cx_destroy: 573\r\nlistener.0.0.0.0_8443.downstream_cx_total: 575\r\nlistener.0.0.0.0_8443.downstream_pre_cx_active: 0\r\nlistener.0.0.0.0_8443.downstream_pre_cx_timeout: 0\r\nlistener.0.0.0.0_8443.http.ingress_http.downstream_rq_1xx: 0\r\nlistener.0.0.0.0_8443.http.ingress_http.downstream_rq_2xx: 547115\r\nlistener.0.0.0.0_8443.http.ingress_http.downstream_rq_3xx: 0\r\nlistener.0.0.0.0_8443.http.ingress_http.downstream_rq_4xx: 0\r\nlistener.0.0.0.0_8443.http.ingress_http.downstream_rq_5xx: 0\r\nlistener.0.0.0.0_8443.http.ingress_http.downstream_rq_completed: 547115\r\nlistener.0.0.0.0_8443.no_filter_chain_match: 0\r\nlistener.admin.downstream_cx_active: 1\r\nlistener.admin.downstream_cx_destroy: 5\r\nlistener.admin.downstream_cx_total: 6\r\nlistener.admin.downstream_pre_cx_active: 0\r\nlistener.admin.downstream_pre_cx_timeout: 0\r\nlistener.admin.http.admin.downstream_rq_1xx: 0\r\nlistener.admin.http.admin.downstream_rq_2xx: 4\r\nlistener.admin.http.admin.downstream_rq_3xx: 0\r\nlistener.admin.http.admin.downstream_rq_4xx: 1\r\nlistener.admin.http.admin.downstream_rq_5xx: 0\r\nlistener.admin.http.admin.downstream_rq_completed: 5\r\nlistener.admin.no_filter_chain_match: 0\r\nlistener_manager.listener_added: 1\r\nlistener_manager.listener_create_failure: 0\r\nlistener_manager.listener_create_success: 16\r\nlistener_manager.listener_modified: 0\r\nlistener_manager.listener_removed: 0\r\nlistener_manager.total_listeners_active: 1\r\nlistener_manager.total_listeners_draining: 0\r\nlistener_manager.total_listeners_warming: 0\r\nruntime.admin_overrides_active: 0\r\nruntime.deprecated_feature_use: 17\r\nruntime.load_error: 0\r\nruntime.load_success: 0\r\nruntime.num_keys: 0\r\nruntime.override_dir_exists: 0\r\nruntime.override_dir_not_exists: 0\r\nserver.concurrency: 16\r\nserver.days_until_first_cert_expiring: 2147483647\r\nserver.debug_assertion_failures: 0\r\nserver.hot_restart_epoch: 0\r\nserver.live: 1\r\nserver.memory_allocated: 194654112\r\nserver.memory_heap_size: 379584512\r\nserver.parent_connections: 0\r\nserver.total_connections: 2\r\nserver.uptime: 34354\r\nserver.version: 9480099\r\nserver.watchdog_mega_miss: 0\r\nserver.watchdog_miss: 0\r\nstats.overflow: 0\r\ncluster.redacted.external.upstream_rq_time: P0(13,8) P25(16.9167,15.161) P50(20.1667,18.4887) P75(24.75,29.1587) P90(41.6,297.911) P95(164.333,366.677) P99(442.6,2442.77) P99.5(446.3,6259.12) P99.9(449.26,72395) P100(450,910000)\r\ncluster.redacted.upstream_cx_connect_ms: P0(nan,0) P25(nan,0) P50(nan,0) P75(nan,0) P90(nan,0) P95(nan,0) P99(nan,0) P99.5(nan,0) P99.9(nan,0) P100(nan,0)\r\ncluster.redacted.upstream_cx_length_ms: No recorded values\r\ncluster.redacted.upstream_rq_time: P0(13,8) P25(16.9167,15.161) P50(20.1667,18.4887) P75(24.75,29.1587) P90(41.6,297.911) P95(164.333,366.677) P99(442.6,2442.77) P99.5(446.3,6259.12) P99.9(449.26,72395) P100(450,910000)\r\nhttp.admin.downstream_cx_length_ms: P0(nan,0) P25(nan,0) P50(nan,0) P75(nan,0) P90(nan,1.05) P95(nan,1.075) P99(nan,1.095) P99.5(nan,1.0975) P99.9(nan,1.0995) P100(nan,1.1)\r\nhttp.admin.downstream_rq_time: P0(nan,0) P25(nan,0) P50(nan,0) P75(nan,0) P90(nan,0) P95(nan,0) P99(nan,0) P99.5(nan,0) P99.9(nan,0) P100(nan,0)\r\nhttp.ingress_http.downstream_cx_length_ms: P0(nan,0) P25(nan,0) P50(nan,0) P75(nan,0) P90(nan,0) P95(nan,0) P99(nan,0) P99.5(nan,0) P99.9(nan,0) P100(nan,0)\r\nhttp.ingress_http.downstream_rq_time: P0(13,9) P25(17.0714,15.3489) P50(20.2857,18.7243) P75(24.8333,30.146) P90(41.6,300.839) P95(164.333,370.276) P99(442.6,2499.46) P99.5(446.3,6518.87) P99.9(449.26,72386.4) P100(450,910000)\r\nlistener.0.0.0.0_8443.downstream_cx_length_ms: P0(nan,0) P25(nan,0) P50(nan,0) P75(nan,0) P90(nan,0) P95(nan,0) P99(nan,0) P99.5(nan,0) P99.9(nan,0) P100(nan,0)\r\nlistener.admin.downstream_cx_length_ms: P0(nan,0) P25(nan,0) P50(nan,0) P75(nan,0) P90(nan,0) P95(nan,0) P99(nan,0) P99.5(nan,0) P99.9(nan,0) P100(nan,0)\r\n```\r\n\r\n**clusters**\r\n```\r\nredacted::outlier::success_rate_ejection_threshold::-1\r\nredacted::default_priority::max_connections::1024\r\nredacted::default_priority::max_pending_requests::1024\r\nredacted::default_priority::max_requests::1024\r\nredacted::default_priority::max_retries::0\r\nredacted::high_priority::max_connections::1024\r\nredacted::high_priority::max_pending_requests::1024\r\nredacted::high_priority::max_requests::1024\r\nredacted::high_priority::max_retries::3\r\nredacted::added_via_api::false\r\nredacted::172.17.0.2:8500::cx_active::2\r\nredacted::172.17.0.2:8500::cx_connect_fail::0\r\nredacted::172.17.0.2:8500::cx_total::2\r\nredacted::172.17.0.2:8500::rq_active::0\r\nredacted::172.17.0.2:8500::rq_error::9\r\nredacted::172.17.0.2:8500::rq_success::37398\r\nredacted::172.17.0.2:8500::rq_timeout::0\r\nredacted::172.17.0.2:8500::rq_total::37471\r\nredacted::172.17.0.2:8500::health_flags::healthy\r\nredacted::172.17.0.2:8500::weight::1\r\nredacted::172.17.0.2:8500::region::\r\nredacted::172.17.0.2:8500::zone::\r\nredacted::172.17.0.2:8500::sub_zone::\r\nredacted::172.17.0.2:8500::canary::false\r\nredacted::172.17.0.2:8500::success_rate::-1\r\nredacted::172.17.0.2:8501::cx_active::2\r\nredacted::172.17.0.2:8501::cx_connect_fail::0\r\nredacted::172.17.0.2:8501::cx_total::2\r\nredacted::172.17.0.2:8501::rq_active::27\r\nredacted::172.17.0.2:8501::rq_error::41\r\nredacted::172.17.0.2:8501::rq_success::35065\r\nredacted::172.17.0.2:8501::rq_timeout::0\r\nredacted::172.17.0.2:8501::rq_total::35266\r\nredacted::172.17.0.2:8501::health_flags::healthy\r\nredacted::172.17.0.2:8501::weight::1\r\nredacted::172.17.0.2:8501::region::\r\nredacted::172.17.0.2:8501::zone::\r\nredacted::172.17.0.2:8501::sub_zone::\r\nredacted::172.17.0.2:8501::canary::false\r\nredacted::172.17.0.2:8501::success_rate::-1\r\nredacted::172.17.0.2:8502::cx_active::2\r\nredacted::172.17.0.2:8502::cx_connect_fail::0\r\nredacted::172.17.0.2:8502::cx_total::2\r\nredacted::172.17.0.2:8502::rq_active::0\r\nredacted::172.17.0.2:8502::rq_error::15\r\nredacted::172.17.0.2:8502::rq_success::36759\r\nredacted::172.17.0.2:8502::rq_timeout::0\r\nredacted::172.17.0.2:8502::rq_total::36894\r\nredacted::172.17.0.2:8502::health_flags::healthy\r\nredacted::172.17.0.2:8502::weight::1\r\nredacted::172.17.0.2:8502::region::\r\nredacted::172.17.0.2:8502::zone::\r\nredacted::172.17.0.2:8502::sub_zone::\r\nredacted::172.17.0.2:8502::canary::false\r\nredacted::172.17.0.2:8502::success_rate::-1\r\nredacted::172.17.0.2:8503::cx_active::2\r\nredacted::172.17.0.2:8503::cx_connect_fail::0\r\nredacted::172.17.0.2:8503::cx_total::2\r\nredacted::172.17.0.2:8503::rq_active::0\r\nredacted::172.17.0.2:8503::rq_error::17\r\nredacted::172.17.0.2:8503::rq_success::36849\r\nredacted::172.17.0.2:8503::rq_timeout::0\r\nredacted::172.17.0.2:8503::rq_total::36983\r\nredacted::172.17.0.2:8503::health_flags::healthy\r\nredacted::172.17.0.2:8503::weight::1\r\nredacted::172.17.0.2:8503::region::\r\nredacted::172.17.0.2:8503::zone::\r\nredacted::172.17.0.2:8503::sub_zone::\r\nredacted::172.17.0.2:8503::canary::false\r\nredacted::172.17.0.2:8503::success_rate::-1\r\nredacted::172.17.0.2:8504::cx_active::2\r\nredacted::172.17.0.2:8504::cx_connect_fail::0\r\nredacted::172.17.0.2:8504::cx_total::2\r\nredacted::172.17.0.2:8504::rq_active::7\r\nredacted::172.17.0.2:8504::rq_error::36\r\nredacted::172.17.0.2:8504::rq_success::35194\r\nredacted::172.17.0.2:8504::rq_timeout::0\r\nredacted::172.17.0.2:8504::rq_total::35433\r\nredacted::172.17.0.2:8504::health_flags::healthy\r\nredacted::172.17.0.2:8504::weight::1\r\nredacted::172.17.0.2:8504::region::\r\nredacted::172.17.0.2:8504::zone::\r\nredacted::172.17.0.2:8504::sub_zone::\r\nredacted::172.17.0.2:8504::canary::false\r\nredacted::172.17.0.2:8504::success_rate::-1\r\nredacted::172.17.0.2:8505::cx_active::2\r\nredacted::172.17.0.2:8505::cx_connect_fail::0\r\nredacted::172.17.0.2:8505::cx_total::2\r\nredacted::172.17.0.2:8505::rq_active::0\r\nredacted::172.17.0.2:8505::rq_error::17\r\nredacted::172.17.0.2:8505::rq_success::35847\r\nredacted::172.17.0.2:8505::rq_timeout::0\r\nredacted::172.17.0.2:8505::rq_total::36050\r\nredacted::172.17.0.2:8505::health_flags::healthy\r\nredacted::172.17.0.2:8505::weight::1\r\nredacted::172.17.0.2:8505::region::\r\nredacted::172.17.0.2:8505::zone::\r\nredacted::172.17.0.2:8505::sub_zone::\r\nredacted::172.17.0.2:8505::canary::false\r\nredacted::172.17.0.2:8505::success_rate::-1\r\nredacted::172.17.0.2:8506::cx_active::2\r\nredacted::172.17.0.2:8506::cx_connect_fail::0\r\nredacted::172.17.0.2:8506::cx_total::2\r\nredacted::172.17.0.2:8506::rq_active::0\r\nredacted::172.17.0.2:8506::rq_error::23\r\nredacted::172.17.0.2:8506::rq_success::36572\r\nredacted::172.17.0.2:8506::rq_timeout::0\r\nredacted::172.17.0.2:8506::rq_total::36693\r\nredacted::172.17.0.2:8506::health_flags::healthy\r\nredacted::172.17.0.2:8506::weight::1\r\nredacted::172.17.0.2:8506::region::\r\nredacted::172.17.0.2:8506::zone::\r\nredacted::172.17.0.2:8506::sub_zone::\r\nredacted::172.17.0.2:8506::canary::false\r\nredacted::172.17.0.2:8506::success_rate::-1\r\nredacted::172.17.0.2:8507::cx_active::2\r\nredacted::172.17.0.2:8507::cx_connect_fail::0\r\nredacted::172.17.0.2:8507::cx_total::2\r\nredacted::172.17.0.2:8507::rq_active::0\r\nredacted::172.17.0.2:8507::rq_error::16\r\nredacted::172.17.0.2:8507::rq_success::36635\r\nredacted::172.17.0.2:8507::rq_timeout::0\r\nredacted::172.17.0.2:8507::rq_total::36738\r\nredacted::172.17.0.2:8507::health_flags::healthy\r\nredacted::172.17.0.2:8507::weight::1\r\nredacted::172.17.0.2:8507::region::\r\nredacted::172.17.0.2:8507::zone::\r\nredacted::172.17.0.2:8507::sub_zone::\r\nredacted::172.17.0.2:8507::canary::false\r\nredacted::172.17.0.2:8507::success_rate::-1\r\nredacted::172.17.0.2:8508::cx_active::2\r\nredacted::172.17.0.2:8508::cx_connect_fail::0\r\nredacted::172.17.0.2:8508::cx_total::2\r\nredacted::172.17.0.2:8508::rq_active::1\r\nredacted::172.17.0.2:8508::rq_error::34\r\nredacted::172.17.0.2:8508::rq_success::35294\r\nredacted::172.17.0.2:8508::rq_timeout::0\r\nredacted::172.17.0.2:8508::rq_total::35482\r\nredacted::172.17.0.2:8508::health_flags::healthy\r\nredacted::172.17.0.2:8508::weight::1\r\nredacted::172.17.0.2:8508::region::\r\nredacted::172.17.0.2:8508::zone::\r\nredacted::172.17.0.2:8508::sub_zone::\r\nredacted::172.17.0.2:8508::canary::false\r\nredacted::172.17.0.2:8508::success_rate::-1\r\nredacted::172.17.0.2:8509::cx_active::2\r\nredacted::172.17.0.2:8509::cx_connect_fail::0\r\nredacted::172.17.0.2:8509::cx_total::2\r\nredacted::172.17.0.2:8509::rq_active::0\r\nredacted::172.17.0.2:8509::rq_error::18\r\nredacted::172.17.0.2:8509::rq_success::35730\r\nredacted::172.17.0.2:8509::rq_timeout::0\r\nredacted::172.17.0.2:8509::rq_total::35878\r\nredacted::172.17.0.2:8509::health_flags::healthy\r\nredacted::172.17.0.2:8509::weight::1\r\nredacted::172.17.0.2:8509::region::\r\nredacted::172.17.0.2:8509::zone::\r\nredacted::172.17.0.2:8509::sub_zone::\r\nredacted::172.17.0.2:8509::canary::false\r\nredacted::172.17.0.2:8509::success_rate::-1\r\nredacted::172.17.0.2:8510::cx_active::2\r\nredacted::172.17.0.2:8510::cx_connect_fail::0\r\nredacted::172.17.0.2:8510::cx_total::2\r\nredacted::172.17.0.2:8510::rq_active::0\r\nredacted::172.17.0.2:8510::rq_error::18\r\nredacted::172.17.0.2:8510::rq_success::36523\r\nredacted::172.17.0.2:8510::rq_timeout::0\r\nredacted::172.17.0.2:8510::rq_total::36677\r\nredacted::172.17.0.2:8510::health_flags::healthy\r\nredacted::172.17.0.2:8510::weight::1\r\nredacted::172.17.0.2:8510::region::\r\nredacted::172.17.0.2:8510::zone::\r\nredacted::172.17.0.2:8510::sub_zone::\r\nredacted::172.17.0.2:8510::canary::false\r\nredacted::172.17.0.2:8510::success_rate::-1\r\nredacted::172.17.0.2:8511::cx_active::2\r\nredacted::172.17.0.2:8511::cx_connect_fail::0\r\nredacted::172.17.0.2:8511::cx_total::2\r\nredacted::172.17.0.2:8511::rq_active::0\r\nredacted::172.17.0.2:8511::rq_error::10\r\nredacted::172.17.0.2:8511::rq_success::37210\r\nredacted::172.17.0.2:8511::rq_timeout::0\r\nredacted::172.17.0.2:8511::rq_total::37310\r\nredacted::172.17.0.2:8511::health_flags::healthy\r\nredacted::172.17.0.2:8511::weight::1\r\nredacted::172.17.0.2:8511::region::\r\nredacted::172.17.0.2:8511::zone::\r\nredacted::172.17.0.2:8511::sub_zone::\r\nredacted::172.17.0.2:8511::canary::false\r\nredacted::172.17.0.2:8511::success_rate::-1\r\nredacted::172.17.0.2:8512::cx_active::2\r\nredacted::172.17.0.2:8512::cx_connect_fail::0\r\nredacted::172.17.0.2:8512::cx_total::2\r\nredacted::172.17.0.2:8512::rq_active::7\r\nredacted::172.17.0.2:8512::rq_error::7\r\nredacted::172.17.0.2:8512::rq_success::37689\r\nredacted::172.17.0.2:8512::rq_timeout::0\r\nredacted::172.17.0.2:8512::rq_total::37740\r\nredacted::172.17.0.2:8512::health_flags::healthy\r\nredacted::172.17.0.2:8512::weight::1\r\nredacted::172.17.0.2:8512::region::\r\nredacted::172.17.0.2:8512::zone::\r\nredacted::172.17.0.2:8512::sub_zone::\r\nredacted::172.17.0.2:8512::canary::false\r\nredacted::172.17.0.2:8512::success_rate::-1\r\nredacted::172.17.0.2:8513::cx_active::2\r\nredacted::172.17.0.2:8513::cx_connect_fail::0\r\nredacted::172.17.0.2:8513::cx_total::2\r\nredacted::172.17.0.2:8513::rq_active::0\r\nredacted::172.17.0.2:8513::rq_error::20\r\nredacted::172.17.0.2:8513::rq_success::37301\r\nredacted::172.17.0.2:8513::rq_timeout::0\r\nredacted::172.17.0.2:8513::rq_total::37385\r\nredacted::172.17.0.2:8513::health_flags::healthy\r\nredacted::172.17.0.2:8513::weight::1\r\nredacted::172.17.0.2:8513::region::\r\nredacted::172.17.0.2:8513::zone::\r\nredacted::172.17.0.2:8513::sub_zone::\r\nredacted::172.17.0.2:8513::canary::false\r\nredacted::172.17.0.2:8513::success_rate::-1\r\nredacted::172.17.0.2:8514::cx_active::2\r\nredacted::172.17.0.2:8514::cx_connect_fail::0\r\nredacted::172.17.0.2:8514::cx_total::2\r\nredacted::172.17.0.2:8514::rq_active::0\r\nredacted::172.17.0.2:8514::rq_error::19\r\nredacted::172.17.0.2:8514::rq_success::36235\r\nredacted::172.17.0.2:8514::rq_timeout::0\r\nredacted::172.17.0.2:8514::rq_total::36393\r\nredacted::172.17.0.2:8514::health_flags::healthy\r\nredacted::172.17.0.2:8514::weight::1\r\nredacted::172.17.0.2:8514::region::\r\nredacted::172.17.0.2:8514::zone::\r\nredacted::172.17.0.2:8514::sub_zone::\r\nredacted::172.17.0.2:8514::canary::false\r\nredacted::172.17.0.2:8514::success_rate::-1\r\n```"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-31T15:50:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-04-07T16:27:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5995,
    "title": "Route two services on same domain ",
    "created_at": "2019-02-18T20:32:33Z",
    "closed_at": "2019-03-29T01:14:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5995",
    "body": "I have to do something like this in my application  , how can i do that \r\n\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 10000 }\r\n\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match: { prefix: \"/\" }\r\n                route: { host_rewrite: www.google.com, cluster: service_google }\r\n              - match: { prefix: \"/site/fb\" }\r\n                route: { host_rewrite: www.facebook.com, cluster: service_fb }\r\n          http_filters:\r\n          - name: envoy.router\r\n\r\n  clusters:\r\n  - name: service_google\r\n    connect_timeout: 0.25s\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    hosts: [{ socket_address: { address: google.com, port_value: 443 }}]\r\n    tls_context: { sni: www.google.com }\r\n  - name: service_fb\r\n    connect_timeout: 0.25s\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    hosts: [{ socket_address: { address: facebook.com, port_value: 443 }}]\r\n    tls_context: { sni: www.facebook.com }\r\n\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5995/comments",
    "author": "manvindar",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-03-22T01:09:07Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-29T01:14:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5930,
    "title": "Redis rate limit example",
    "created_at": "2019-02-13T07:20:14Z",
    "closed_at": "2019-04-19T04:10:56Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5930",
    "body": "*Title*: *Redis reatelimit sample*\r\n\r\n*Description*:\r\n1) How to use rate limit in redis and tcp network?\r\n2) and how use below configuration?\r\n```\r\nrate_limit:\r\n   unit: second\r\n   requests_per_unit: 500\r\n```\r\n\r\n3) What is the best replacement for \"xxxx\", \"yyyy\"?\r\n\r\nConfig:\r\n```\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n\r\nstatic_resources:\r\n  listeners:\r\n    - name: redis_listener\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 6000\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.redis_proxy\r\n              config:\r\n                stat_prefix: egress_redis\r\n                cluster: redis_cluster\r\n                settings:\r\n                  op_timeout: 5s\r\n\r\n            - name: envoy.ratelimit\r\n              config:\r\n                stat_prefix: rate_redis\r\n                domain: rate_per_ip\r\n                descriptors:\r\n                  entries:\r\n                  - key: xxxx\r\n                    value: yyyy\r\n\r\n  clusters:\r\n    - name: redis_cluster\r\n      connect_timeout: 1s\r\n      type: strict_dns\r\n      lb_policy: MAGLEV\r\n      hosts:\r\n        - socket_address:\r\n            address: proxy-redis1\r\n            port_value: 6379\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5930/comments",
    "author": "poyaz",
    "comments": [
      {
        "user": "yutongp",
        "created_at": "2019-03-13T02:54:34Z",
        "body": "bump.\r\nI have similar question about how to set up network ratelimit for tcp proxy if I want rate limit on remote address"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-04-12T03:40:19Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-04-19T04:10:55Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "seguidor777",
        "created_at": "2019-06-19T15:42:25Z",
        "body": "Hello I have the same question, I am looking for examples of network rate limit"
      },
      {
        "user": "sandy724",
        "created_at": "2019-08-19T19:05:57Z",
        "body": "I'm also looking for implementing simple network rate limiting, the official documentation is pretty confusing"
      },
      {
        "user": "ivans3",
        "created_at": "2019-09-23T23:21:51Z",
        "body": " I am looking for examples of network rate limit as applied to the default egress PassthroughCluster\r\n"
      },
      {
        "user": "kritishaw",
        "created_at": "2020-02-24T16:32:51Z",
        "body": "@poyaz were you able to configure it?"
      }
    ]
  },
  {
    "number": 5891,
    "title": "Support for retry policy per cluster",
    "created_at": "2019-02-09T02:16:22Z",
    "closed_at": "2019-04-05T23:52:08Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5891",
    "body": "Support for retry policy configuration per cluster:\r\n\r\nRight now retry policy is supported per route and it doesn't play along well with WeightedClusters.\r\nIf I want different clusters to adapt different retry policies, it's not possible with the current configuration. \r\n\r\nAccording to @htuch, when an Envoy retries a failed request, it will retry on the same cluster in the list of WeightedClusters. If the retries are going to the same cluster, retry policy per cluster config makes more sense than per route.\r\n\r\n Is this something that you guys are considering of adding as a feature at all?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5891/comments",
    "author": "Y0Username",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-02-09T21:14:57Z",
        "body": "> If the retries are going to the same cluster, retry policy per cluster config makes more sense than per route.\r\n\r\nI'm not sure I full understand the request here. Can you clarify with an example?"
      },
      {
        "user": "Y0Username",
        "created_at": "2019-02-25T22:58:33Z",
        "body": "Hi @mattklein123, \r\n\r\nCurrently the retry policy is applied at the route level. If I have a WeightedCluster in the route and I want each cluster to respect a different retry policy, there's no way to do it. \r\n\r\nIt would make sense if retry policy is configured under the cluster for following reasons:\r\n1. When the user has multiple clusters(within a route) with different versions of the software, the user might want different retry policies for each version of the software.\r\n2. When the envoy retries a failed request; it will retry to the same cluster instead of picking a different cluster from the list of WeightedCluster (this implicitly means that the retry policy is applied per cluster).\r\n\r\nSo the current configuration has all the clusters under a route being configured with the same retry policy. Would it be possible to move retry policy under cluster configs?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-02-27T22:44:50Z",
        "body": "> Would it be possible to move retry policy under cluster configs?\r\n\r\nI'm not sure this makes sense, since we still need retrie policies to be route/vhost specific. I think it would be better to consider adding override retry policies to the weighted cluster route construct if that is desired. cc @rshriram for thoughts on this."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-29T23:01:38Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-04-05T23:52:07Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5874,
    "title": "Atomicity behavior of /reset_counters API  ",
    "created_at": "2019-02-07T15:45:46Z",
    "closed_at": "2019-03-20T12:31:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5874",
    "body": "Query on atomicity behavior of /reset_counters API  \r\n\r\nWe are using 1.8 version of envoy in our product and the integration is really doing good.\r\nHowever, I need to understand the behavior of **/reset_counters** API little more. Does this API always guarantee that counters are being reset atomically - especially the cumulative counters of like 2xx, 5xx?\r\n\r\nSo in a running system if 5XX is sum of 503 and 504 counters -  is it possible that 503, 504 and 5xx counters (together in a group) are not being reset atomically?\r\n\r\nExpectation is that at any point of time, the 5XX counter should always show correct value as the addition of 503 and 504 counters, even in case counters are being reset very frequently.\r\n\r\nAny possibility, if this expectation goes wrong?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5874/comments",
    "author": "thevks",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-02-07T19:04:04Z",
        "body": "It's just a for loop that runs through the counters and resets them individually, so it's definitely not atomic on a global level."
      },
      {
        "user": "thevks",
        "created_at": "2019-02-08T06:03:51Z",
        "body": "Thanks for the response..\r\n"
      },
      {
        "user": "thevks",
        "created_at": "2019-02-08T06:25:34Z",
        "body": "Does the **/reset_counters** API reset the shared memory or not?"
      },
      {
        "user": "thevks",
        "created_at": "2019-02-11T10:56:13Z",
        "body": "@mattklein123 \r\nWhat is the behaviour of /reset_counters API? Does it reset the whole sort of counters, even if they are not appearing (for the time being) in the context of newly created envoy process, after hot_restart?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-13T11:52:22Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-20T12:31:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5788,
    "title": "Envoy \"Connection: close\" causes 1s rq_time overhead",
    "created_at": "2019-01-31T12:05:04Z",
    "closed_at": "2019-02-01T12:13:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5788",
    "body": "Given the below configuration:\r\n\r\nRequest with __Connection: close__, causes envoy to delays the sending the tcp (FIN) / closing the socket by 1s.\r\n\r\nEnvoy sends the response data, rightaway (no delay visible in tcpdump / wireshark), the FIN / closing of the connection is the issue.\r\n```bash\r\ntime fortio curl -loglevel=debug -keepalive=false localhost:8080/\r\n```\r\n\r\nRequest without 'Connection: close', works as expected\r\n```bash\r\ntime fortio curl -loglevel=debug localhost:8080/\r\n```\r\n\r\n```yaml\r\n---\r\nnode:\r\n  locality:\r\n    zone: default-zone\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: default_listener\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8080\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress_proxy\r\n          access_log:\r\n            name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n              format: >\r\n                [%START_TIME%] %PROTOCOL% %REQ(:METHOD)% %REQ(:authority)% %REQ(:PATH)% %RESPONSE_CODE% %RESPONSE_FLAGS%\r\n                %BYTES_RECEIVED%b %BYTES_SENT%b %DURATION%ms \"%DOWNSTREAM_REMOTE_ADDRESS%\" -> \"%UPSTREAM_HOST%\"\r\n          route_config:\r\n            name: \"ingress_routes\"\r\n            virtual_hosts:\r\n              - name: \"local_service\"\r\n                domains:\r\n                  - \"*\"\r\n                routes:\r\n                  - match:\r\n                      prefix: \"/\"\r\n                    route:\r\n                      cluster: \"example\"\r\n          http_filters:\r\n            - name: envoy.router\r\n          http_protocol_options:\r\n            allow_absolute_url: true\r\n\r\n  clusters:\r\n  - name: example\r\n    type: STRICT_DNS\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 9091\r\n    connect_timeout:\r\n      seconds: 1\r\n\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9091\r\n```\r\n\r\nThis looks similar to #234, we've noticed unhealthy instances in varnish, when probes had .timeout < 1s.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5788/comments",
    "author": "fkowal",
    "comments": [
      {
        "user": "fkowal",
        "created_at": "2019-01-31T12:37:02Z",
        "body": "```\r\ntcpdump -nni lo0 -s 0 -A tcp portrange 8080\r\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\r\nlistening on lo0, link-type NULL (BSD loopback), capture size 262144 bytes\r\n13:15:10.238945 IP6 ::1.65046 > ::1.8080: Flags [S], seq 1696331147, win 65535, options [mss 16324,nop,wscale 6,nop,nop,TS val 803548502 ecr 0,sackOK,eol], length 0\r\n`..0.,.@....................................e............4....?........\r\n/.-V........\r\n13:15:10.239025 IP6 ::1.8080 > ::1.65046: Flags [S.], seq 2050231525, ack 1696331148, win 65535, options [mss 16324,nop,wscale 6,nop,nop,TS val 803548502 ecr 803548502,sackOK,eol], length 0\r\n`....,.@....................................z4..e........4....?........\r\n/.-V/.-V....\r\n13:15:10.239037 IP6 ::1.65046 > ::1.8080: Flags [.], ack 1, win 6371, options [nop,nop,TS val 803548502 ecr 803548502], length 0\r\n`..0. .@....................................e...z4.......(.....\r\n/.-V/.-V\r\n13:15:10.239045 IP6 ::1.8080 > ::1.65046: Flags [.], ack 1, win 6371, options [nop,nop,TS val 803548502 ecr 803548502], length 0\r\n`.... .@....................................z4..e........(.....\r\n/.-V/.-V\r\n13:15:10.239089 IP6 ::1.65046 > ::1.8080: Flags [P.], seq 1:97, ack 1, win 6371, options [nop,nop,TS val 803548502 ecr 803548502], length 96: HTTP: GET / HTTP/1.1\r\n`..0...@....................................e...z4.............\r\n/.-V/.-VGET / HTTP/1.1\r\nHost: localhost:8080\r\nConnection: close\r\nUser-Agent: fortio.org/fortio-1.3.0\r\n\r\n\r\n13:15:10.239101 IP6 ::1.8080 > ::1.65046: Flags [.], ack 97, win 6370, options [nop,nop,TS val 803548502 ecr 803548502], length 0\r\n`.... .@....................................z4..e........(.....\r\n/.-V/.-V\r\n13:15:10.247298 IP6 ::1.8080 > ::1.65046: Flags [P.], seq 1:4807, ack 97, win 6370, options [nop,nop,TS val 803548510 ecr 803548502], length 4806: HTTP: HTTP/1.1 200 OK\r\n`......@....................................z4..e..............\r\n/.-^/.-VHTTP/1.1 200 OK\r\ncontent-type: text/html; charset=UTF-8\r\ncache-control: no-cache, max-age=0\r\nx-content-type-options: nosniff\r\ndate: Thu, 31 Jan 2019 12:15:10 GMT\r\nserver: envoy\r\nx-envoy-upstream-service-time: 0\r\nconnection: close\r\ntransfer-encoding: chunked\r\n\r\n11b4\r\n<head>\r\n  <title>Envoy Admin</title> etc\r\n</body>\r\n\r\n0\r\n\r\n\r\n13:15:10.247329 IP6 ::1.65046 > ::1.8080: Flags [.], ack 4807, win 6296, options [nop,nop,TS val 803548510 ecr 803548510], length 0\r\n`..0. .@....................................e...z4.......(.....\r\n/.-^/.-^\r\n13:15:11.250726 IP6 ::1.8080 > ::1.65046: Flags [.], ack 97, win 6370, length 0\r\n`......@....................................z4..e...P.......\r\n13:15:11.250767 IP6 ::1.65046 > ::1.8080: Flags [.], ack 4807, win 6296, options [nop,nop,TS val 803549510 ecr 803548510], length 0\r\n`..0. .@....................................e...z4.......(.....\r\n/.1F/.-^\r\n13:15:11.252739 IP6 ::1.8080 > ::1.65046: Flags [F.], seq 4807, ack 97, win 6370, options [nop,nop,TS val 803549512 ecr 803549510], length 0\r\n`.... .@....................................z4..e........(.....\r\n/.1H/.1F\r\n13:15:11.252769 IP6 ::1.65046 > ::1.8080: Flags [.], ack 4808, win 6296, options [nop,nop,TS val 803549512 ecr 803549512], length 0\r\n`..0. .@....................................e...z4.......(.....\r\n/.1H/.1H\r\n13:15:11.252815 IP6 ::1.65046 > ::1.8080: Flags [F.], seq 97, ack 4808, win 6296, options [nop,nop,TS val 803549512 ecr 803549512], length 0\r\n`..0. .@....................................e...z4.......(.....\r\n/.1H/.1H\r\n13:15:11.252866 IP6 ::1.8080 > ::1.65046: Flags [.], ack 98, win 6370, options [nop,nop,TS val 803549512 ecr 803549512], length 0\r\n`.... .@....................................z4..e........(.....\r\n/.1H/.1H\r\n```\r\n\r\nHere is the tcpdump"
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-01-31T15:56:19Z",
        "body": "If it's a close due to early response, it's likely ConnectionCloseType::FlushWriteAndDelay which was introduced to address the race described in #2929  \r\n\r\nIf it's not a close due to early response there might be some other corner case triggering the delay.\r\n\r\nIf that's the problem you could fix by adjusting  delayed_close_timeout in your HCM config but depending on how your client handles resets you might reintroduce the race described in that issue\r\n\r\ncc @AndresGuedez "
      },
      {
        "user": "fkowal",
        "created_at": "2019-02-01T12:13:44Z",
        "body": "Changing the __delayed_close_timeout__ did helped lowering the delay with the responses.\r\n\r\nThese metrics keep increasing, which I belive is an indication that varnish is not issuing a FIN after receiving the response.\r\n*http.ingress_http.downstream_cx_delayed_close_timeout* and *http.ingress_http.downstream_cx_destroy_local*\r\n\r\nThank you @alyssawilk "
      }
    ]
  },
  {
    "number": 5777,
    "title": "bazel build //api/... fails in clean client",
    "created_at": "2019-01-30T21:23:30Z",
    "closed_at": "2019-03-09T03:04:06Z",
    "labels": [
      "question",
      "area/build",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5777",
    "body": "```\r\nbazel build //api/...\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/usr/local/google/home/jmarantz/git4/envoy/tools/bazel.rc\r\nINFO: Invocation ID: 6f21355e-91e3-42fe-bca2-2323b997fc27\r\nLoading: \r\nLoading: 0 packages loaded\r\nERROR: error loading package 'api/test/build': Unable to load file '//bazel:api_build_system.bzl': file doesn't exist\r\nINFO: Elapsed time: 1.061s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (5 packages loaded)\r\n```\r\n\r\nNote: there is a broken reference in api/envoy/admin/v2alpha/BUILD, which looks like it ought to be fixable like this:\r\n```\r\ndiff --git a/api/envoy/admin/v2alpha/BUILD b/api/envoy/admin/v2alpha/BUILD\r\nindex a6b403fdd..56a6f541b 100644\r\n--- a/api/envoy/admin/v2alpha/BUILD\r\n+++ b/api/envoy/admin/v2alpha/BUILD\r\n@@ -1,4 +1,4 @@\r\n-load(\"//bazel:api_build_system.bzl\", \"api_proto_library_internal\")\r\n+load(\"//bazel/api:api_build_system.bzl\", \"api_proto_library_internal\")\r\n \r\n licenses([\"notice\"])  # Apache 2\r\n ```\r\n\r\nBut then there are more failures. It appears this might be related to some zero-sized BUILD files under //bazel/api/...\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5777/comments",
    "author": "jmarantz",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-01-31T01:31:44Z",
        "body": "It is supposed to build with `@envoy_api//...`, it is structured to make `data-plane-api` independent from the structure of this repo. Perhaps add some document around it?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-02T02:57:23Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-09T03:04:05Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5751,
    "title": "Proposal: Apply one-class-per-file to conn_manager_impl.cc",
    "created_at": "2019-01-29T14:02:11Z",
    "closed_at": "2019-03-07T18:01:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5751",
    "body": "In my experience, the many class definitions all grouped together within conn_manager_impl.cc has made it way more difficult to understand the conn manager and its relationship to ActiveStream, callbacks, decoders, etc. Just recently I was trying to understand what an ActiveStream does, but got confused sorting through all the ActiveStream*Callback classes as well. This will get worse as the conn manager becomes more complex, so I would propose moving all of the other classes to their own files. Thoughts?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5751/comments",
    "author": "auni53",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-01-29T16:27:47Z",
        "body": "These are all private classes and are very tightly coupled to the parent class. I'm not really sure that moving things into their own file is going to help with understanding?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-02-28T17:07:50Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-07T18:01:52Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5746,
    "title": "Envoy as Redis proxy: does that affect throughput?",
    "created_at": "2019-01-29T02:21:39Z",
    "closed_at": "2019-03-10T19:29:42Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5746",
    "body": "*Title*: *Envoy as Redis proxy: does that affect throughput?*\r\n\r\n*Description*:\r\nWe're currently using the open source Redis version (not cluster mode) and are considering switching to Envoy as a Redis proxy with the hope of more horizontal scalability to handle more traffic. Based on some simple load testing, it seemed that the throughput didn't improve after introducing Envoy as a proxy (a.k.a the number of timeout errors didn't decrease as a result, if anything, it increased slightly). I wonder if that's a false expectation from us or maybe we didn't use Envoy properly.\r\n\r\nFor context, our current redis setup is one primary + one replica. Our throughput is at the scale of thousands reads per second + thousands writes per second + close to hundreds of large batch reads and writes. Error rate ranges between 1%-3%.\r\n\r\nWould appreciate any insights you could provide. Thanks!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5746/comments",
    "author": "jwang0830",
    "comments": [
      {
        "user": "HenryYYang",
        "created_at": "2019-01-31T00:48:41Z",
        "body": "The Redis proxy shards both the read/write commands to multiple Redis instances, it doesn't handle replica.  Since you only have 1 primary, there's nothing for Redis proxy to horizontally scale to.\r\n\r\nAlso can you share some details on your test setup and how you're generating the load please?  Depends on the key/value size and the size of your batch, Redis might not be the bottleneck here."
      },
      {
        "user": "jwang0830",
        "created_at": "2019-01-31T01:09:56Z",
        "body": "Thanks for your reply!\r\n\r\nTo provide more context: one primary + one replica is the setup with the open source Redis (no Envoy). Our setup with Envoy in test is two nodes behind the proxy (which should handle both read and write requests in my understanding). \r\n\r\nWe're generating the load using JMeter (apache's load testing tool), which hits an endpoint handled by test machines that then talk to Redis (hosted on different machines). The key/value size are both < 1KB, though the batch size is quite big, ranging from thousands to couple thousands."
      },
      {
        "user": "HenryYYang",
        "created_at": "2019-01-31T21:29:25Z",
        "body": "What kind of machines are you running these on?  Also, how many test machines?  \r\n\r\nThe large batch size is probably the reason for your high timeout.  When you shard each of those large batches to 2 nodes, each batch request will have to wait for responses from 2 redis nodes instead of 1.  That said, % timeout is not a good measure of throughput."
      },
      {
        "user": "jwang0830",
        "created_at": "2019-01-31T23:15:07Z",
        "body": "We have 2 test machines with apache servers handling incoming calls.\r\nWould you suggest breaking up the large batch calls or routing all request to one node to help mitigate the timeout issue?\r\nAlso, what could be a reasonable measure of throughput?"
      },
      {
        "user": "HenryYYang",
        "created_at": "2019-02-01T19:19:55Z",
        "body": "The number of operations per second would be a good measure.  I'd suggest limit the batch size to <1000 in general.  The larger your batch size the larger the buffer that's needed and the longer other requests have to wait.\r\n\r\nBut before you do that, I'd suggest that you monitor you network bandwidth, cpu and memory on both the client machine and redis first.  Then you can figure out the bottleneck and scale accordingly."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-03T19:23:58Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-10T19:29:41Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5695,
    "title": "How to create request based connection from envoy to verticals ?",
    "created_at": "2019-01-23T16:10:16Z",
    "closed_at": "2019-03-01T17:51:58Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5695",
    "body": "Description:\r\nHello. \r\nI am having tough time to solve the request based connections in the following deployment scenario.\r\n\r\nOur deployment on Kubernetes, has IC(ingress controller) as  Ambassador (which uses envoy) and the back-end servers are Vert.x based.\r\nUsing H2Load for testing the scenario. \r\n    \r\nProblem\r\nThere are 2 instance of back-end Service1 (with 2 verticles each) and 3 instance of back-end service2 (with 3 verticles)\r\nIf thee are many request (say 100000 request) from 1 client to IC then how to configure envoy to open 2 HTTP connections to each verticles of service1 (so total 4 HTTP connections) and 3 connections to each verticles of service 2(so total 9 HTTP connections) and distribute the load evenly on those connections?\r\n\r\nThere is --concurrency option in Envoy which configures fixed number of connections to all servers but does not allow to have variable connections across services\r\nIf there any configuration option in envoy to support this?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5695/comments",
    "author": "dineshkr3",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-02-22T17:02:00Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-01T17:51:57Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5683,
    "title": "handshake error: 2",
    "created_at": "2019-01-22T17:31:36Z",
    "closed_at": "2019-03-07T16:01:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5683",
    "body": "Hi guys, \r\n\r\nCould someone please describe what means **handshake error: 2** in this case? \r\n\r\nIs it ok? \r\n\r\n```\r\n[2019-01-22 17:23:34.712][1][debug][connection] [source/common/network/connection_impl.cc:638] [C36391] connecting to 13.52.114.241:21213\r\n[2019-01-22 17:23:34.712][1][debug][connection] [source/common/network/connection_impl.cc:647] [C36391] connection in progress\r\n[2019-01-22 17:23:34.712][1][debug][connection] [source/common/network/connection_impl.cc:516] [C36391] connected\r\n[2019-01-22 17:23:34.712][1][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:138] [C36391] handshake error: 2\r\n[2019-01-22 17:23:34.713][1][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:127] [C36391] handshake complete\r\n[2019-01-22 17:23:34.713][1][debug][connection] [source/common/network/connection_impl.cc:101] [C36391] closing data_to_write=2 type=1\r\n[2019-01-22 17:23:34.713][1][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:236] [C36391] SSL shutdown: rc=0\r\n[2019-01-22 17:23:34.713][1][debug][connection] [source/common/network/connection_impl.cc:183] [C36391] closing socket: 1\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5683/comments",
    "author": "Mykolaichenko",
    "comments": [
      {
        "user": "ipuustin",
        "created_at": "2019-01-29T14:12:56Z",
        "body": "The `error: 2` here is `SSL_ERROR_WANT_READ` from `SSL_do_handshake()`. This means that the handshake requires more data (socket becoming readable) and then someone to call `SSL_do_handshake()` again. When this happens you see the `handshake complete` message in the log."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-02-28T15:07:50Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-07T16:01:52Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5665,
    "title": "how can we collect envoy access log?",
    "created_at": "2019-01-19T16:40:28Z",
    "closed_at": "2019-02-25T17:55:37Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5665",
    "body": "We deploy fluentd on every node collect pod's logs to avoid too many log be sent to a center at one time. Envoy output logs to a file or send logs by grpc. In a kubernetes-istio cluster, so many pods' envoy on a node will have the same configuration about access log. this will make collecting and analysing them be hard.  So can we find a better manner to send logs to es?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5665/comments",
    "author": "knightXun",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-02-18T17:10:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-02-25T17:55:36Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5620,
    "title": "Performance improvement in data-path - proposal",
    "created_at": "2019-01-16T12:39:49Z",
    "closed_at": "2019-02-27T17:02:00Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5620",
    "body": "*Background:*\r\nIn my investigation, I have found that there is a significant impact of a user-space proxy (i.e., envoy) in terms of performance degradation. That is, if we could move the data from the client egress socket directly to the envoy egress socket w/o sending it through the envoy user-space proxy, we can get a significant performance improvement (e.g., tens of percents in a synthetic benchmark) for buffers of 16KB and up.\r\n\r\nIn many use-cases, envoy needs ONLY the meta-data of the message and does NOT need to read the data itself. so, we actually do not need to pass the data through envoy proxy and we can just forward it in the kernel from the client socket directly to the envoy egress socket.\r\n\r\n*Proposal:*\r\nI would like to suggest to add a simple logic into envoy which forwards the data (if it is large enough - 16K and up) in the kernel from the client socket to the envoy egress socket by using the *splice* syscall.\r\n\r\nIs this interesting ? If yes, just let me know and I'll prepare a PR.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5620/comments",
    "author": "zachidan",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-01-16T16:54:42Z",
        "body": "@zachidan this has come up before, but the architecture of Envoy (decoupling of upstream and downstream) will make it very difficult to actually do this without a large number of changes. If you are interested in trying to make this happen, I would suggest producing a small design document on how you would go about the change and we can all evaluate?"
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-01-17T01:51:31Z",
        "body": "@zachidan I am very interested in this, I have also researched before, but I found that envoy will modify the original buffer anyway, so there is no way directly do splice directly."
      },
      {
        "user": "zachidan",
        "created_at": "2019-01-21T08:41:12Z",
        "body": "@mattklein123 I think that for **tcp_proxy** listener it is straightforward because there is 1:1 mapping between the downstream socket and the upstream socket (do I miss something ?). I agree that for **http** listener it is a bit more complicated but still feasible because we just need to _splice_ the data of each message/frame.\r\n\r\nHowever, there is some constraint when a **TLS** is used:\r\n\r\n- TLS termination: we couldn't use _splice_ because we need to let envoy to decrypt the data.\r\n- TLS origination: in order to use _splice_ with TLS origination, envoy needs to switch to kernel TLS (kTLS).\r\n\r\nIn the bottom line: _splice_ can be used without TLS or with TLS origination + kTLS.\r\n\r\nAny thoughts ?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-01-21T16:31:43Z",
        "body": "The complications go beyond TLS even for basic tcp_proxy due to filters. There can be filters that run before tcp_proxy that can buffer, etc. There really is no easy way to do this without quite a bit of special casing. I think if you start to actually do the change you will see the problems. Feel free to put together a design doc on the proposed changes if you like. It's possible, but it will require a lot of changes, special casing, and complexity."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-02-20T16:55:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-02-27T17:01:59Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5614,
    "title": "stale endpoint with old ip address still exists after changing endpoint ip address",
    "created_at": "2019-01-15T23:24:31Z",
    "closed_at": "2019-02-22T03:07:21Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5614",
    "body": "We are using v1.7 currently. I found the issue where if i change ip address of the endpoint and then do localhost:8001/clusters it shows old ip for the endpoint as well. I do see log message from eds saying \"membership update for TLS cluster ep1\". Restarting envoy resolved the issue. \r\n\r\nep1::10.0.0.1:8080    // old ip \r\nep1::10.0.0.2:8080   // new ip",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5614/comments",
    "author": "pitiwari",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-01-16T01:43:06Z",
        "body": "Please try on master and report back. Thank you!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-02-15T02:34:01Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-02-22T03:07:20Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5595,
    "title": "Websocket connection does not work between envoy sidecars",
    "created_at": "2019-01-14T15:16:20Z",
    "closed_at": "2019-01-16T15:05:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5595",
    "body": "Hi, I am trying to do a simple WS example which includes `front-envoy` and `service1`. The `service1` is a WS server coupling with an envoy sidecar. `front-proxy` is for passing through the WS request to `service1`. I use the new version to configure WS with `upgrade_type: websocket`. It works when a WS client sends a request to the envoy sidecar of `service1`, but it does not work when sending request to `front-proxy`. It return handshake status 503. Below are the envoy files, and I use the `envoy-alpine:lastest` image. \r\n\r\n*Config envoy for `front-proxy`*:\r\n```yaml\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          upgrade_configs:\r\n          - upgrade_type: websocket\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/service1\"\r\n                route:\r\n                  cluster: service1\r\n          http_filters:\r\n          - name: envoy.router\r\n            config: {}\r\n  clusters:\r\n  - name: service1\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    http2_protocol_options: {}\r\n    hosts:\r\n    - socket_address:\r\n        address: service1\r\n        port_value: 80\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n```\r\n*Config envoy for `service1`*:\r\n```yaml\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          upgrade_configs:\r\n          - upgrade_type: websocket\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: service\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n                - match:\r\n                    prefix: \"/service1\"\r\n                  route:\r\n                    cluster: local_service\r\n          http_filters:\r\n            - name: envoy.router\r\n              config: {}\r\n  clusters:\r\n  - name: local_service\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 8888\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5595/comments",
    "author": "pltanhthu",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-01-15T00:12:06Z",
        "body": "@alyssawilk any quick ideas on this one?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-01-15T14:46:33Z",
        "body": "I assume from port 80 these are all http but given the codec is auto, if either is H2 you need to explicitly set `allow_connect in http2_protocol_options.  \r\n\r\nI don't think there's many websocket failure paths which return 50x though - upgrade failures should be 403, and show up in stats as rejected upgrades.    I think I'd need to see header traces to do better here, sorry :-/"
      },
      {
        "user": "pltanhthu",
        "created_at": "2019-01-16T10:37:01Z",
        "body": "Hi @alyssawilk, it works when I set *allow_connect: true* in *http2_protocol_options*. Thanks so much for the hint. "
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-01-16T15:05:36Z",
        "body": "Sweet, glad to hear it!  Sadly there's not so much we can do about that from a discoverability standpoint other than maybe eventually flipping that true by default - nghttp2 rejects the headers as invalid before we inspect them so it's not obvious to the Envoy code base that an upgrade was rejected in that case."
      }
    ]
  },
  {
    "number": 5421,
    "title": "tcp_proxy filter should return some message while onInitFailure",
    "created_at": "2018-12-26T06:43:00Z",
    "closed_at": "2019-02-07T13:16:52Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5421",
    "body": "*Title*: *tcp_proxy filter should return the error message while onInitFailure*\r\n\r\n*Description*:\r\n>tcp_proxy filter should return the error message while onInitFailure, because the client wants to know who closed the connection and why.\r\nThe original:\r\n```\r\n  virtual void onInitFailure(UpstreamFailureReason) {\r\n    read_callbacks_->connection().close(Network::ConnectionCloseType::NoFlush);\r\n  }\r\n```\r\nAnd can be  modified as follows:\r\n```\r\n  virtual void onInitFailure(UpstreamFailureReason reason) {\r\n    std::string str(\"envoy tcp_proxy error: \");\r\n    switch(reason) {\r\n    case UpstreamFailureReason::CONNECT_FAILED:\r\n      str.append(\"CONNECT_FAILED\");\r\n      break;\r\n    case UpstreamFailureReason::NO_HEALTHY_UPSTREAM:\r\n      str.append(\"NO_HEALTHY_UPSTREAM\");\r\n      break;\r\n    case UpstreamFailureReason::RESOURCE_LIMIT_EXCEEDED:\r\n      str.append(\"RESOURCE_LIMIT_EXCEEDED\");\r\n      break;\r\n    case UpstreamFailureReason::NO_ROUTE:\r\n      str.append(\"NO_ROUTE\");\r\n      break;\r\n    default:\r\n      str.append(\"UNKNOWN\");\r\n      break;\r\n    }\r\n    str.append(\"\\n\");\r\n    Buffer::OwnedImpl response;\r\n    response.add(str.c_str());\r\n    read_callbacks_->connection().write(response, false);\r\n    read_callbacks_->connection().close(Network::ConnectionCloseType::NoFlush);\r\n  }\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5421/comments",
    "author": "hello-jianghongke",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2018-12-26T18:27:47Z",
        "body": "I'm not sure it's a good idea to put this in the standard tcp proxy filter - I imagine most things establishing a connection would not expect this kind of response so you'd probably get \"invalid protocol/handshake\"-type errors.\r\n\r\nIf you have a specific application that wants to know why the connection failed I'd suggest putting in a separate filter specific to your use case."
      },
      {
        "user": "zyfjeff",
        "created_at": "2018-12-27T03:36:08Z",
        "body": "I don't think I need to return the error message here, because the returned content will not necessarily be parsed, it is not universal, and it may be better to record a log."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-12-28T03:17:13Z",
        "body": "+1 to @snowp and @zyfjeff, I don't think we can do this generically."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-01-27T05:18:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-02-07T13:16:51Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5410,
    "title": "Building envoy on OS X",
    "created_at": "2018-12-24T15:16:55Z",
    "closed_at": "2018-12-25T09:52:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5410",
    "body": "I'm trying to build envoy on OS X mojave `10.14.2 (18C54)` without success.\r\n\r\nHere is the executed command :\r\n`bazel build --incompatible_package_name_is_a_function=false --action_env=PATH=/usr/local/bin:/opt/local/bin:/usr/bin:/bin //source/exe:envoy-static`\r\n\r\nHere is the result: \r\n```shell\r\nINFO: Invocation ID: 9d5adc00-d36e-4957-b2c0-3f984d6cdc44\r\n/private/var/tmp/_bazel_nicolassterchele/19b5387e66de067346e6f0614dfa8b23/external/envoy_deps/./repositories.sh: line 8: md5: command not found\r\nExternal dependency cache directory /private/var/tmp/_bazel_nicolassterchele/19b5387e66de067346e6f0614dfa8b23/external/envoy_deps_cache_\r\n./build_and_install_deps.sh: line 12: sysctl: command not found\r\nmake: the `-j' option requires a positive integral argument\r\n```\r\nI guess that there is specifics commands for linux that is not available on darwin... Am I right ?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5410/comments",
    "author": "sterchelen",
    "comments": [
      {
        "user": "moderation",
        "created_at": "2018-12-24T16:34:14Z",
        "body": "@sterchelen What version of Bazel are your running? You should upgrade to 0.21.0 if you are on something earlier. There were some recent changes for MacOS using that build that work. I build with `bazel build -c opt //source/exe:envoy-static.stripped` and `.bazelrc` is:\r\n```\r\nimport %workspace%/tools/bazel.rc\r\nbuild --announce_rc\r\nbuild --define google_grpc=disabled\r\nbuild --define signal_trace=disabled\r\nbuild --action_env=PATH=/bin:/opt/local/bin:/usr/bin:/usr/local/bin\r\n```"
      },
      {
        "user": "snowp",
        "created_at": "2018-12-24T18:24:48Z",
        "body": "This: \r\n\r\n```\r\n./build_and_install_deps.sh: line 12: sysctl: command not found\r\n```\r\n\r\nshould have been fixed on latest master to use the full path, can you try to pull latest?"
      },
      {
        "user": "sterchelen",
        "created_at": "2018-12-25T09:52:47Z",
        "body": "In fact, getting last version of master resolved it. Thanks."
      }
    ]
  },
  {
    "number": 5359,
    "title": "[Question] Meaning of [C~] character included in connection log and stream log etc",
    "created_at": "2018-12-20T02:41:56Z",
    "closed_at": "2018-12-20T03:37:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5359",
    "body": "*Title*: *[Question] Meaning of [C~] character included in connection log and stream log etc*\r\n\r\n*Description*:\r\nYou could see a character string [C (number)] as follows in Envoy's log. Would you tell this meaning?\r\nI think logs with the same id are logs in the same connection since I recognize it's like a connection id, but is it right?\r\nThanks.\r\n\r\n```\r\n[2018-12-20 01:52:58.930][000011][debug][router] [source/common/router/router.cc:322] [C0][S539228372188944921] router decoding headers:\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5359/comments",
    "author": "nakabonne",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-12-20T03:30:54Z",
        "body": "Yes that's right, it's an internal connection ID."
      },
      {
        "user": "nakabonne",
        "created_at": "2018-12-20T03:36:36Z",
        "body": "Helped a lot, thanks!"
      }
    ]
  },
  {
    "number": 5343,
    "title": "Headers stripped out on redirect",
    "created_at": "2018-12-18T16:48:22Z",
    "closed_at": "2019-01-25T18:31:05Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5343",
    "body": "*Title*: *Headers stripped out on redirect*\r\n\r\n*Description*:\r\n> When using route with redirect any custom headers inserted are not passed to end host. I've tried to research this, but did not find anything concrete. Maybe I'm missing come configuration or this behavior is intended.\r\n\r\nThis is my sample config to replicate the issue:\r\n```yaml\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9090\r\n\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 8080\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              config:\r\n                codec_type: auto\r\n                stat_prefix: ingress_http\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match:\r\n                            prefix: \"/\"\r\n                          redirect:\r\n                            host_redirect: headers.jsontest.com\r\n                http_filters:\r\n                  - name: envoy.lua\r\n                    config:\r\n                      inline_code: |\r\n                        function envoy_on_request(request_handle)\r\n                          request_handle:headers():add(\"apikey\", \"my-key\")\r\n                        end\r\n                  - name: envoy.router\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5343/comments",
    "author": "Fadelis",
    "comments": [
      {
        "user": "zyfjeff",
        "created_at": "2018-12-19T14:18:26Z",
        "body": "@Fadelis I tested, found that if using redirects, won't execute filter, directly to the redirection address launched after the request."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-01-18T18:27:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-01-25T18:31:05Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5226,
    "title": "HTTPS front proxy to reroute traffic to http listener - fail with 503",
    "created_at": "2018-12-05T15:53:01Z",
    "closed_at": "2018-12-05T20:06:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5226",
    "body": "HTTPS front proxy to reroute traffic to http listener - fail with 503\r\n\r\n*Description*:\r\n>Try to use Envoy as front proxy to listen HTTPS request and re-route to the HTTP listener. Both listeners are docker instance on the same host. Test client send request from another host via SOAP UI.\r\n\r\nThe Envoy returns 503 error to the SOAP UI. Thr SOAP UI log:\r\n```\r\nWed Dec 05 10:36:36 EST 2018:DEBUG:<< \"HTTP/1.1 503 Service Unavailable[\\r][\\n]\"\r\nWed Dec 05 10:36:36 EST 2018:DEBUG:<< \"content-length: 57[\\r][\\n]\"\r\nWed Dec 05 10:36:36 EST 2018:DEBUG:<< \"content-type: text/plain[\\r][\\n]\"\r\nWed Dec 05 10:36:36 EST 2018:DEBUG:<< \"date: Wed, 05 Dec 2018 15:36:36 GMT[\\r][\\n]\"\r\nWed Dec 05 10:36:36 EST 2018:DEBUG:<< \"server: envoy[\\r][\\n]\"\r\nWed Dec 05 10:36:36 EST 2018:DEBUG:<< \"[\\r][\\n]\"\r\nWed Dec 05 10:36:36 EST 2018:DEBUG:<< \"upstream connect error or disconnect/reset before headers\"\r\n```\r\n\r\n*Envoy docker file*:\r\n```\r\nFROM envoyproxy/envoy-alpine:latest\r\n\r\nADD ./pem/envoy-front-ssl.crt /etc/\r\nADD ./pem/envoy-front-ssl.key /etc/\r\nADD edge.yaml /etc/\r\n\r\nCMD [\"/usr/local/bin/envoy\", \"-c\", \"/etc/edge.yaml\", \"-l\", \"debug\"]\r\n\r\n```\r\n\r\n*Envoy Config yaml file*:\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 10000 }\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress_http\r\n          codec_type: AUTO\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/mock-domain\" # a test for mock-domain\r\n                route:\r\n                  cluster: mock-domain\r\n          http_filters:\r\n          - name: envoy.router\r\n      tls_context:\r\n        common_tls_context:\r\n            #alpn_protocols: \"h2\"\r\n            tls_certificates:\r\n            - certificate_chain: { filename: \"/etc/envoy-front-ssl.crt\" }\r\n              private_key: { filename: \"/etc/envoy-front-ssl.key\" }\r\n  clusters:\r\n  - name: mock-domain\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    hosts:\r\n    - socket_address:\r\n        address: mock-domain\r\n        port_value: 18080\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n\r\n```\r\n\r\n*Envoy Console Log*:\r\n```\r\n[2018-12-05 15:36:30.805][000005][info][main] [source/server/server.cc:207] initializing epoch 0 (hot restart version=10.200.16384.127.options=capacity=16384, num_slots=8209 hash=228984379728933363 size=2654312)\r\n[2018-12-05 15:36:30.805][000005][info][main] [source/server/server.cc:209] statically linked extensions:\r\n[2018-12-05 15:36:30.805][000005][info][main] [source/server/server.cc:211]   access_loggers: envoy.file_access_log,envoy.http_grpc_access_log\r\n[2018-12-05 15:36:30.805][000005][info][main] [source/server/server.cc:214]   filters.http: envoy.buffer,envoy.cors,envoy.ext_authz,envoy.fault,envoy.filters.http.header_to_metadata,envoy.filters.http.jwt_authn,envoy.filters.http.rbac,envoy.grpc_http1_bridge,envoy.grpc_json_transcoder,envoy.grpc_web,envoy.gzip,envoy.health_check,envoy.http_dynamo_filter,envoy.ip_tagging,envoy.lua,envoy.rate_limit,envoy.router,envoy.squash\r\n[2018-12-05 15:36:30.805][000005][info][main] [source/server/server.cc:217]   filters.listener: envoy.listener.original_dst,envoy.listener.proxy_protocol,envoy.listener.tls_inspector\r\n[2018-12-05 15:36:30.805][000005][info][main] [source/server/server.cc:220]   filters.network: envoy.client_ssl_auth,envoy.echo,envoy.ext_authz,envoy.filters.network.dubbo_proxy,envoy.filters.network.rbac,envoy.filters.network.sni_cluster,envoy.filters.network.thrift_proxy,envoy.http_connection_manager,envoy.mongo_proxy,envoy.ratelimit,envoy.redis_proxy,envoy.tcp_proxy\r\n[2018-12-05 15:36:30.805][000005][info][main] [source/server/server.cc:222]   stat_sinks: envoy.dog_statsd,envoy.metrics_service,envoy.stat_sinks.hystrix,envoy.statsd\r\n[2018-12-05 15:36:30.805][000005][info][main] [source/server/server.cc:224]   tracers: envoy.dynamic.ot,envoy.lightstep,envoy.tracers.datadog,envoy.zipkin\r\n[2018-12-05 15:36:30.805][000005][info][main] [source/server/server.cc:227]   transport_sockets.downstream: envoy.transport_sockets.alts,envoy.transport_sockets.capture,raw_buffer,tls\r\n[2018-12-05 15:36:30.805][000005][info][main] [source/server/server.cc:230]   transport_sockets.upstream: envoy.transport_sockets.alts,envoy.transport_sockets.capture,raw_buffer,tls\r\n[2018-12-05 15:36:30.809][000005][info][main] [source/server/server.cc:272] admin address: 0.0.0.0:8001\r\n[2018-12-05 15:36:30.811][000005][debug][main] [source/server/overload_manager_impl.cc:171] No overload action configured for envoy.overload_actions.stop_accepting_connections.\r\n[2018-12-05 15:36:30.811][000005][debug][main] [source/server/overload_manager_impl.cc:171] No overload action configured for envoy.overload_actions.stop_accepting_connections.\r\n[2018-12-05 15:36:30.811][000005][info][config] [source/server/configuration_impl.cc:51] loading 0 static secret(s)\r\n[2018-12-05 15:36:30.811][000005][info][config] [source/server/configuration_impl.cc:57] loading 1 cluster(s)\r\n[2018-12-05 15:36:30.812][000005][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:818] adding TLS initial cluster mock-domain\r\n[2018-12-05 15:36:30.812][000005][debug][upstream] [source/common/upstream/upstream_impl.cc:1183] starting async DNS resolution for mock-domain\r\n[2018-12-05 15:36:30.813][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 3437 milliseconds\r\n[2018-12-05 15:36:30.813][000005][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:63] cm init: adding: cluster=mock-domain primary=1 secondary=0\r\n[2018-12-05 15:36:30.813][000005][info][config] [source/server/configuration_impl.cc:62] loading 1 listener(s)\r\n[2018-12-05 15:36:30.813][000005][debug][config] [source/server/configuration_impl.cc:64] listener #0:\r\n[2018-12-05 15:36:30.813][000005][debug][config] [source/server/listener_manager_impl.cc:640] begin add/update listener: name=listener_0 hash=10827106893954255580\r\n[2018-12-05 15:36:30.813][000005][debug][config] [source/server/listener_manager_impl.cc:40]   filter #0:\r\n[2018-12-05 15:36:30.813][000005][debug][config] [source/server/listener_manager_impl.cc:41]     name: envoy.http_connection_manager\r\n[2018-12-05 15:36:30.813][000005][debug][config] [source/server/listener_manager_impl.cc:44]   config: {\"codec_type\":\"AUTO\",\"route_config\":{\"name\":\"local_route\",\"virtual_hosts\":[{\"name\":\"local_service\",\"domains\":[\"*\"],\"routes\":[{\"route\":{\"cluster\":\"mock-domain\"},\"match\":{\"prefix\":\"/mock-domain\"}}]}]},\"stat_prefix\":\"ingress_http\",\"http_filters\":[{\"name\":\"envoy.router\"}]}\r\n[2018-12-05 15:36:30.815][000005][debug][config] [source/extensions/filters/network/http_connection_manager/config.cc:312]     http filter #0\r\n[2018-12-05 15:36:30.815][000005][debug][config] [source/extensions/filters/network/http_connection_manager/config.cc:313]       name: envoy.router\r\n[2018-12-05 15:36:30.815][000005][debug][config] [source/extensions/filters/network/http_connection_manager/config.cc:317]     config: {}\r\n[2018-12-05 15:36:30.819][000005][debug][config] [source/server/listener_manager_impl.cc:527] add active listener: name=listener_0, hash=10827106893954255580, address=0.0.0.0:10000\r\n[2018-12-05 15:36:30.819][000005][info][config] [source/server/configuration_impl.cc:95] loading tracing configuration\r\n[2018-12-05 15:36:30.819][000005][info][config] [source/server/configuration_impl.cc:115] loading stats sink configuration\r\n[2018-12-05 15:36:30.819][000005][info][main] [source/server/server.cc:458] starting main dispatch loop\r\n[2018-12-05 15:36:30.819][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 4687 milliseconds\r\n[2018-12-05 15:36:30.819][000009][debug][grpc] [source/common/grpc/google_async_client_impl.cc:41] completionThread running\r\n[2018-12-05 15:36:30.842][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 3437 milliseconds\r\n[2018-12-05 15:36:30.848][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 4375 milliseconds\r\n[2018-12-05 15:36:30.848][000005][debug][upstream] [source/common/upstream/upstream_impl.cc:1190] async DNS resolution complete for mock-domain\r\n[2018-12-05 15:36:30.848][000005][debug][upstream] [source/common/upstream/upstream_impl.cc:1212] DNS hosts have changed for mock-domain\r\n[2018-12-05 15:36:30.848][000005][debug][upstream] [source/common/upstream/upstream_impl.cc:587] initializing secondary cluster mock-domain completed\r\n[2018-12-05 15:36:30.848][000005][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:953] membership update for TLS cluster mock-domain\r\n[2018-12-05 15:36:30.848][000005][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:91] cm init: init complete: cluster=mock-domain primary=0 secondary=0\r\n[2018-12-05 15:36:30.848][000005][info][upstream] [source/common/upstream/cluster_manager_impl.cc:136] cm init: all clusters initialized\r\n[2018-12-05 15:36:30.848][000005][info][main] [source/server/server.cc:430] all clusters initialized. initializing init manager\r\n[2018-12-05 15:36:30.848][000005][info][config] [source/server/listener_manager_impl.cc:910] all dependencies initialized. starting workers\r\n[2018-12-05 15:36:30.849][000011][debug][main] [source/server/worker_impl.cc:98] worker entering dispatch loop\r\n[2018-12-05 15:36:30.849][000011][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:818] adding TLS initial cluster mock-domain\r\n[2018-12-05 15:36:30.849][000011][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:953] membership update for TLS cluster mock-domain\r\n[2018-12-05 15:36:30.849][000012][debug][main] [source/server/worker_impl.cc:98] worker entering dispatch loop\r\n[2018-12-05 15:36:30.849][000012][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:818] adding TLS initial cluster mock-domain\r\n[2018-12-05 15:36:30.849][000012][debug][upstream] [source/common/upstream/cluster_manager_impl.cc:953] membership update for TLS cluster mock-domain\r\n[2018-12-05 15:36:30.849][000013][debug][grpc] [source/common/grpc/google_async_client_impl.cc:41] completionThread running\r\n[2018-12-05 15:36:30.849][000014][debug][grpc] [source/common/grpc/google_async_client_impl.cc:41] completionThread running\r\n[2018-12-05 15:36:35.820][000005][debug][main] [source/server/server.cc:144] flushing stats\r\n[2018-12-05 15:36:35.849][000005][debug][upstream] [source/common/upstream/upstream_impl.cc:1183] starting async DNS resolution for mock-domain\r\n[2018-12-05 15:36:35.850][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 5000 milliseconds\r\n[2018-12-05 15:36:35.852][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 3125 milliseconds\r\n[2018-12-05 15:36:35.864][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 2812 milliseconds\r\n[2018-12-05 15:36:35.875][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 3750 milliseconds\r\n[2018-12-05 15:36:35.876][000005][debug][upstream] [source/common/upstream/upstream_impl.cc:1190] async DNS resolution complete for mock-domain\r\n[2018-12-05 15:36:36.258][000012][debug][main] [source/server/connection_handler_impl.cc:236] [C0] new connection\r\n[2018-12-05 15:36:36.258][000012][debug][connection] [source/common/ssl/ssl_socket.cc:135] [C0] handshake error: 2\r\n[2018-12-05 15:36:36.288][000012][debug][connection] [source/common/ssl/ssl_socket.cc:135] [C0] handshake error: 2\r\n[2018-12-05 15:36:36.288][000012][debug][connection] [source/common/ssl/ssl_socket.cc:135] [C0] handshake error: 2\r\n[2018-12-05 15:36:36.309][000012][debug][connection] [source/common/ssl/ssl_socket.cc:135] [C0] handshake error: 2\r\n[2018-12-05 15:36:36.309][000012][debug][connection] [source/common/ssl/ssl_socket.cc:135] [C0] handshake error: 2\r\n[2018-12-05 15:36:36.321][000012][debug][connection] [source/common/ssl/ssl_socket.cc:124] [C0] handshake complete\r\n[2018-12-05 15:36:36.329][000012][debug][http] [source/common/http/conn_manager_impl.cc:200] [C0] new stream\r\n[2018-12-05 15:36:36.333][000012][debug][http] [source/common/http/conn_manager_impl.cc:529] [C0][S8599161127663960637] request headers complete (end_stream=false):\r\n':authority', '192.168.64.135:10000'\r\n':path', '/mock-domain'\r\n':method', 'POST'\r\n'accept-encoding', 'gzip,deflate'\r\n'content-type', 'text/xml;charset=UTF-8'\r\n'content-length', '799'\r\n'connection', 'Keep-Alive'\r\n'user-agent', 'Apache-HttpClient/4.1.1 (java 1.5)'\r\n\r\n[2018-12-05 15:36:36.334][000012][debug][router] [source/common/router/router.cc:264] [C0][S8599161127663960637] cluster 'mock-domain' match for URL '/mock-domain'\r\n[2018-12-05 15:36:36.334][000012][debug][router] [source/common/router/router.cc:322] [C0][S8599161127663960637] router decoding headers:\r\n':authority', '192.168.64.135:10000'\r\n':path', '/mock-domain'\r\n':method', 'POST'\r\n':scheme', 'http'\r\n'accept-encoding', 'gzip,deflate'\r\n'content-type', 'text/xml;charset=UTF-8'\r\n'content-length', '799'\r\n'user-agent', 'Apache-HttpClient/4.1.1 (java 1.5)'\r\n'x-forwarded-proto', 'https'\r\n'x-request-id', 'aec12380-e00c-4e2a-8085-cc9ef792abc7'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2018-12-05 15:36:36.334][000012][debug][pool] [source/common/http/http1/conn_pool.cc:80] creating a new connection\r\n[2018-12-05 15:36:36.334][000012][debug][client] [source/common/http/codec_client.cc:26] [C1] connecting\r\n[2018-12-05 15:36:36.335][000012][debug][connection] [source/common/network/connection_impl.cc:634] [C1] connecting to 172.19.0.2:18080\r\n[2018-12-05 15:36:36.335][000012][debug][connection] [source/common/network/connection_impl.cc:643] [C1] connection in progress\r\n[2018-12-05 15:36:36.335][000012][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2018-12-05 15:36:36.335][000012][debug][http] [source/common/http/conn_manager_impl.cc:945] [C0][S8599161127663960637] request end stream\r\n[2018-12-05 15:36:36.335][000012][debug][connection] [source/common/network/connection_impl.cc:525] [C1] delayed connection error: 111\r\n[2018-12-05 15:36:36.335][000012][debug][connection] [source/common/network/connection_impl.cc:183] [C1] closing socket: 0\r\n[2018-12-05 15:36:36.335][000012][debug][client] [source/common/http/codec_client.cc:82] [C1] disconnect. resetting 0 pending requests\r\n[2018-12-05 15:36:36.335][000012][debug][pool] [source/common/http/http1/conn_pool.cc:121] [C1] client disconnected\r\n[2018-12-05 15:36:36.335][000012][debug][router] [source/common/router/router.cc:475] [C0][S8599161127663960637] upstream reset\r\n[2018-12-05 15:36:36.335][000012][debug][http] [source/common/http/conn_manager_impl.cc:1180] [C0][S8599161127663960637] encoding headers via codec (end_stream=false):\r\n':status', '503'\r\n'content-length', '57'\r\n'content-type', 'text/plain'\r\n'date', 'Wed, 05 Dec 2018 15:36:36 GMT'\r\n'server', 'envoy'\r\n\r\n[2018-12-05 15:36:40.821][000005][debug][main] [source/server/server.cc:144] flushing stats\r\n[2018-12-05 15:36:40.877][000005][debug][upstream] [source/common/upstream/upstream_impl.cc:1183] starting async DNS resolution for mock-domain\r\n[2018-12-05 15:36:40.877][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 3437 milliseconds\r\n[2018-12-05 15:36:40.878][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 4375 milliseconds\r\n[2018-12-05 15:36:40.885][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 4375 milliseconds\r\n[2018-12-05 15:36:40.891][000005][debug][upstream] [source/common/network/dns_impl.cc:158] Setting DNS resolution timer for 4375 milliseconds\r\n[2018-12-05 15:36:40.891][000005][debug][upstream] [source/common/upstream/upstream_impl.cc:1190] async DNS resolution complete for mock-domain\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5226/comments",
    "author": "bnlcnd",
    "comments": [
      {
        "user": "taion809",
        "created_at": "2018-12-05T18:49:09Z",
        "body": "Hello,\r\nSo here is what I see in your logs\r\n\r\n```\r\n[2018-12-05 15:36:36.335][000012][debug][connection] [source/common/network/connection_impl.cc:634] [C1] connecting to 172.19.0.2:18080\r\n[2018-12-05 15:36:36.335][000012][debug][connection] [source/common/network/connection_impl.cc:643] [C1] connection in progress\r\n[2018-12-05 15:36:36.335][000012][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2018-12-05 15:36:36.335][000012][debug][http] [source/common/http/conn_manager_impl.cc:945] [C0][S8599161127663960637] request end stream\r\n[2018-12-05 15:36:36.335][000012][debug][connection] [source/common/network/connection_impl.cc:525] [C1] delayed connection error: 111\r\n[2018-12-05 15:36:36.335][000012][debug][connection] [source/common/network/connection_impl.cc:183] [C1] closing socket: 0\r\n[2018-12-05 15:36:36.335][000012][debug][client] [source/common/http/codec_client.cc:82] [C1] disconnect. resetting 0 pending requests\r\n[2018-12-05 15:36:36.335][000012][debug][pool] [source/common/http/http1/conn_pool.cc:121] [C1] client disconnected\r\n[2018-12-05 15:36:36.335][000012][debug][router] [source/common/router/router.cc:475] [C0][S8599161127663960637] upstream reset\r\n```\r\n\r\nThere appears to be a connection refused here, error 111 which IIRC is ECONNREFUSED.\r\n\r\nIf you check the output of the `/clusters` command it will give you the IP:PORT information for the cluster in question.  Double check that the server is actually listening on this port."
      },
      {
        "user": "bnlcnd",
        "created_at": "2018-12-05T20:06:37Z",
        "body": "Thanks Nicholas. @taion809 \r\nIt's another silly mistake. Sorry for the newbies question. \r\n"
      },
      {
        "user": "taion809",
        "created_at": "2018-12-05T20:09:31Z",
        "body": "Np, glad it was easy to catch 😅 "
      }
    ]
  },
  {
    "number": 5213,
    "title": "Does envoy.ext_authz support cookies?",
    "created_at": "2018-12-04T19:52:39Z",
    "closed_at": "2019-02-14T10:07:11Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5213",
    "body": "**Issue Template**\r\n\r\n*Title*: *Envoy doesn't seem to send the cookie after auth*\r\n\r\n*Description*:\r\n> I know I can use `allowed_authorization_headers` and `allowed_request_headers` to allow headers going to a service to auth with envoy.ext_authz but \"set-cookie\" doesn't seem to be passed back to the client after a successful login.\r\n\r\n```yaml\r\n          http_filters:\r\n          - name: envoy.ext_authz\r\n            config:\r\n              http_service:\r\n                  server_uri:\r\n                    uri: hostname:8080\r\n                    cluster: ext-authz\r\n                    timeout: 0.25s\r\n                  path_prefix: \"/auth\"\r\n                  allowed_authorization_headers: [\"www-authenticate\", \"set-cookie\"]\r\n                  allowed_request_headers: [ \"authorization\", \"cookie\" ]\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5213/comments",
    "author": "bbigras",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-12-05T00:18:06Z",
        "body": "@gsagula "
      },
      {
        "user": "gsagula",
        "created_at": "2018-12-06T16:18:58Z",
        "body": "@bbigras You should be able to do that. Are you doing any kind of redirect? Let me try this on my end too and I get back to you."
      },
      {
        "user": "bbigras",
        "created_at": "2018-12-14T18:56:14Z",
        "body": "Sorry for the late reply. No I don't do any redirect. My \"auth\" process uses \"basic\" auth.\r\n"
      },
      {
        "user": "gsagula",
        "created_at": "2019-01-02T08:27:40Z",
        "body": "You should be able to send a `set-cookie` header back to the client."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-02-07T09:16:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-02-14T10:07:10Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5173,
    "title": "ADS Server Reconnect Never Occurs After Hard Reboot",
    "created_at": "2018-11-30T22:05:40Z",
    "closed_at": "2019-03-18T03:46:45Z",
    "labels": [
      "question",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5173",
    "body": "*ADS Server Reconnect Never Occurs After Hard Reboot*: *When the ADS server is abruptly rebooted with `sudo reboot -f` the envoy proxy does not attempt to reconnect to the ADS hosts. However, when using `sudo reboot` (without `-f`) ADS properly attempts a reconnect.*\r\n\r\n*Description*:\r\n>`sudo reboot -f` on Ubuntu 16.04 of the ADS server causes the envoy proxy to no longer attempt a reconnect to the cluster hosts for ADS. It seems like some sort of a TCP timeout is not properly handled on envoy. I also don't see any issue in the envoy logs. The envoy /cluster endpoint also shows the correct ADS cluster configuration, that the ADS host has moved, and indicates that no connection attempts have been made. The only known solution is to restart the envoy proxy.\r\n\r\n*Config*:\r\n>   \r\n```\r\n    dynamic_resources:\r\n      cds_config: {ads: {}}\r\n      lds_config: {ads: {}}\r\n      ads_config:\r\n        api_type: GRPC\r\n        grpc_services:\r\n          envoy_grpc:\r\n            cluster_name: {{ cluster_name }}_ads\r\n\r\n    static_resources:\r\n      clusters:\r\n      - name: {{ cluster_name }}_ads\r\n        connect_timeout: { seconds: 1 }\r\n        dns_refresh_rate: { seconds: 10 }\r\n        type: STRICT_DNS\r\n        lb_policy: LEAST_REQUEST\r\n        health_checks:\r\n          healthy_threshold: 1\r\n          interval: { seconds: 10 }\r\n          timeout: { seconds: 10 }\r\n          tcp_health_check: {}\r\n          unhealthy_threshold: 1\r\n        http2_protocol_options: {}\r\n        hosts:\r\n        - socket_address:\r\n            address: ads.envoy.marathon.slave.mesos\r\n            port_value: 9902\r\n```\r\n> /cluster endpoint\r\n```\r\n...\r\nsystem-test_ads::default_priority::max_connections::1024\r\nsystem-test_ads::default_priority::max_pending_requests::1024\r\nsystem-test_ads::default_priority::max_requests::1024\r\nsystem-test_ads::default_priority::max_retries::3\r\nsystem-test_ads::high_priority::max_connections::1024\r\nsystem-test_ads::high_priority::max_pending_requests::1024\r\nsystem-test_ads::high_priority::max_requests::1024\r\nsystem-test_ads::high_priority::max_retries::3\r\nsystem-test_ads::added_via_api::false\r\nsystem-test_ads::172.27.1.192:9902::cx_active::0\r\nsystem-test_ads::172.27.1.192:9902::cx_connect_fail::0\r\nsystem-test_ads::172.27.1.192:9902::cx_total::0\r\nsystem-test_ads::172.27.1.192:9902::rq_active::0\r\nsystem-test_ads::172.27.1.192:9902::rq_error::0\r\nsystem-test_ads::172.27.1.192:9902::rq_success::0\r\nsystem-test_ads::172.27.1.192:9902::rq_timeout::0\r\nsystem-test_ads::172.27.1.192:9902::rq_total::0\r\nsystem-test_ads::172.27.1.192:9902::health_flags::healthy\r\nsystem-test_ads::172.27.1.192:9902::weight::1\r\nsystem-test_ads::172.27.1.192:9902::region::\r\nsystem-test_ads::172.27.1.192:9902::zone::\r\nsystem-test_ads::172.27.1.192:9902::sub_zone::\r\nsystem-test_ads::172.27.1.192:9902::canary::false\r\nsystem-test_ads::172.27.1.192:9902::success_rate::-1\r\n...\r\n```\r\n\r\n*Environment*:\r\n> OS: Ubuntu 16.04\r\nEnvoy Version: v1.8.0\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5173/comments",
    "author": "zanes2016",
    "comments": [
      {
        "user": "zanes2016",
        "created_at": "2018-12-10T19:56:59Z",
        "body": "The behavior suggests that envoy's cluster connection manager fails to tolerate a TCP connection loss. \r\n\r\nIs it possible that for the XDS configuration that a TTL or Timeout is missing and not being set?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-01-09T20:38:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-01-19T02:23:42Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "jeff-merrick",
        "created_at": "2019-02-16T00:31:57Z",
        "body": "@mattklein123 this still seems to be an issue with Envoy Version: v1.9.0. Can you reopen this issue and add the \"help wanted\" label? Thanks."
      },
      {
        "user": "suhailpatel",
        "created_at": "2019-03-09T21:37:26Z",
        "body": "This is usually happening because of a TCP Half Open connection. There is no detection of this. ADS is particularly prone because of long periods of idleness and communication is initiated by the ADS server to push snapshot changes via ADS.\r\n\r\nWe ran into a similar issue and we fixed it by enabling a TCP keep alive.\r\n```\r\n    static_resources:\r\n      clusters:\r\n      - name: {{ cluster_name }}_ads\r\n        connect_timeout: { seconds: 1 }\r\n        dns_refresh_rate: { seconds: 10 }\r\n        type: STRICT_DNS\r\n        ...\r\n        upstream_connection_options:\r\n          tcp_keepalive:\r\n            keepalive_probes: 3\r\n            keepalive_time: 30\r\n            keepalive_interval: 5\r\n```\r\n\r\nThis is especially bad for ADS because Envoy will merrily think it's connected in a TCP Half Open mode and there is no way to re-establish that connection since Envoy is the one that connects to the ADS server not the other way round.\r\n\r\nI'll submit a patch to update some of the docs to note this fact."
      },
      {
        "user": "zsilver",
        "created_at": "2019-03-10T21:18:53Z",
        "body": "Thanks @suhailpatel , we'll give this a try and see if it solves our issue."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-17T16:38:46Z",
        "body": "Note that for searching this issue also applies to non-ADS xDS."
      },
      {
        "user": "suhailpatel",
        "created_at": "2019-03-17T18:25:21Z",
        "body": "@zsilver @zanes2016 Would be interested to hear if the suggested config solved your issues? "
      },
      {
        "user": "jeff-merrick",
        "created_at": "2019-03-18T21:28:37Z",
        "body": "> @zsilver @zanes2016 Would be interested to hear if the suggested config solved your issues?\r\n\r\n@suhailpatel yup this worked for us, thanks!"
      }
    ]
  },
  {
    "number": 5134,
    "title": "Envoy JWT_Auth Connectivity issue",
    "created_at": "2018-11-27T17:24:16Z",
    "closed_at": "2019-01-04T22:21:24Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5134",
    "body": "We're not able to connect to the envoy using the jwt_Auth token, but we able to hit the target url without the jwt token. \r\n\r\n```\r\n{\r\n    \"static_resources\": {\r\n      \"listeners\": [\r\n        {\r\n          \"address\": {\r\n            \"socket_address\": {\r\n              \"address\": \"0.0.0.0\",\r\n              \"port_value\": 10083\r\n            }\r\n          },\r\n          \"filter_chains\": [\r\n            {\r\n              \"filters\": [\r\n                {\r\n                  \"name\": \"envoy.http_connection_manager\",\r\n                  \"config\": {\r\n                    \"codec_type\": \"auto\",\r\n                    \"stat_prefix\": \"ingress_http\",\r\n                    \"route_config\": {\r\n                      \"name\": \"local_route\",\r\n                      \"virtual_hosts\": [\r\n                        {\r\n                          \"name\": \"service\",\r\n                          \"domains\": [\r\n                            \"*\"\r\n                          ],\r\n                          \"routes\": [\r\n                            {\r\n                              \"match\": {\r\n                                \"prefix\": \"/\"\r\n                              },\r\n                              \"route\": {\r\n                                  \"timeout\": \"3s\",\r\n                                \"cluster\": \"local_service\",\r\n                                \"retry_policy\":{\r\n                                  \"retry_on\":\"5xx\",\r\n                                  \"num_retries\":\"3\"\r\n                                }\r\n                              }\r\n                            }\r\n                          ]\r\n                        }\r\n                      ]\r\n                    },\r\n                    \"http_filters\": [\r\n                      {\r\n                        \"name\": \"envoy.filters.http.jwt_authn\",\r\n                        \"config\": {\r\n                          \"providers\": {\r\n                            \"google-jwt\": {\r\n                              \"issuer\": \"628645741881-noabiu23f5a8m8ovd8ucv698lj78vv0l@developer.gserviceaccount.com\",\r\n                              \"audiences\": [ ],\r\n                              \"forward\": true,\r\n                              \"local_jwks\": {\r\n                                \"inline_string\": \"{ \\\"keys\\\" : [ {\\\"e\\\":   \\\"AQAB\\\", \\\"kid\\\": \\\"b3319a147514df7ee5e4bcdee51350cc890cc89e\\\", \\\"kty\\\": \\\"RSA\\\",\\\"n\\\":   \\\"qDi7Tx4DhNvPQsl1ofxxc2ePQFcs-L0mXYo6TGS64CY_2WmOtvYlcLNZjhuddZVV2X88m0MfwaSA16wE-RiKM9hqo5EY8BPXj57CMiYAyiHuQPp1yayjMgoE1P2jvp4eqF-BTillGJt5W5RuXti9uqfMtCQdagB8EC3MNRuU_KdeLgBy3lS3oo4LOYd-74kRBVZbk2wnmmb7IhP9OoLc1-7-9qU1uhpDxmE6JwBau0mDSwMnYDS4G_ML17dC-ZDtLd1i24STUw39KH0pcSdfFbL2NtEZdNeam1DDdk0iUtJSPZliUHJBI_pj8M-2Mn_oA8jBuI8YKwBqYkZCN1I95Q\\\"}]}\"\r\n                                      },\r\n                                \"from_headers\": \"test-jwt-payload-output\"\r\n                            }\r\n                          },\r\n                          \"rules\": [\r\n                            {\r\n                              \"match\": {\r\n                                \"prefix\": \"/Orders\"\r\n                              },\r\n                              \"requires\": {\r\n                                \"provider_name\": \"google-jwt\"\r\n                              }\r\n                            }\r\n                          ]\r\n                        }\r\n                      },\r\n              \r\n                      {\r\n                        \"name\": \"envoy.router\",\r\n                        \"config\": {\r\n                        }\r\n                      }\r\n                    ]\r\n                  }\r\n                }\r\n              ]\r\n            }\r\n          ]\r\n        }\r\n      ],\r\n      \"clusters\": [\r\n        {\r\n          \"name\": \"local_service\",\r\n          \"connect_timeout\": \"0.25s\",\r\n          \"type\": \"strict_dns\",\r\n          \"lb_policy\": \"round_robin\",\r\n  \r\n          \"tls_context\": {\r\n              \"common_tls_context\": {\r\n                \"validation_context\": {\r\n                  \"trusted_ca\": {\r\n                    \"filename\": \"/usr/local/share/ca-certificates/hqidlsspwa01-com.crt\"\r\n                  }\r\n                }\r\n              }\r\n          },\r\n          \"circuit_breakers\":{\r\n            \"thresholds\" : {\r\n                    \"max_connections\" : 1,\r\n                    \"max_requests\": 1,\r\n                    \"max_pending_requests\":1,\r\n                    \"max_retries\":0\r\n            }\r\n    },        \r\n          \"common_lb_config\": {\r\n            \"healthy_panic_threshold\": {\r\n              \"value\": 45\r\n            }\r\n          },\r\n          \"outlier_detection\": {\r\n            \"consecutive_5xx\": 1,\r\n            \"base_ejection_time\": \"20s\"\r\n          },\r\n          \"load_assignment\": {\r\n            \"cluster_name\": \"local_service\",\r\n            \"endpoints\": [\r\n              {\r\n                \"lb_endpoints\": [\r\n                  {\r\n                    \"endpoint\": {\r\n                      \"health_check_config\": {\r\n                        \"port_value\": 8080\r\n                      },\r\n                      \"address\": {\r\n                        \"socket_address\": {\r\n                          \"address\": \"localhost\",\r\n                          \"port_value\": 10084\r\n                        }\r\n                      }\r\n                    }\r\n                  },\r\n                  {\r\n                    \"endpoint\": {\r\n                      \"health_check_config\": {\r\n                        \"port_value\": 8080\r\n                      },\r\n                      \"address\": {\r\n                        \"socket_address\": {\r\n                          \"address\": \"localhost\",\r\n                          \"port_value\": 10081\r\n                        }\r\n                      }\r\n                    }\r\n                  }\r\n                ]\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        {\r\n          \"name\": \"jwt\",\r\n          \"type\": \"LOGICAL_DNS\",\r\n          \"dns_lookup_family\": \"V4_ONLY\",\r\n          \"connect_timeout\": \"2s\",\r\n          \"lb_policy\": \"ROUND_ROBIN\",\r\n          \"hosts\": [\r\n            {\r\n              \"socket_address\": {\r\n                \"address\": \"localhost\",\r\n                \"port_value\": 443\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      ]\r\n    },\r\n    \"admin\": {\r\n      \"access_log_path\": \"/tmp/admin_tmp.log\",\r\n      \"address\": {\r\n        \"socket_address\": {\r\n          \"address\": \"0.0.0.0\",\r\n          \"port_value\": 8082\r\n        }\r\n      }\r\n    }\r\n  }\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5134/comments",
    "author": "rayepudi",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2018-11-27T17:48:25Z",
        "body": "@qiwzhang can you help out here?"
      },
      {
        "user": "qiwzhang",
        "created_at": "2018-11-27T18:18:14Z",
        "body": "@rayepudi Are you saying that a request to Envoy listener port 10083 with JWT  is rejected, but the one without JWT can reach the service in cluster \"local_service\"?   Since your JWT rules has \"/Order\" prefix,  were your request path has that prefix? \r\n\r\nCould you turn on envoy debug with \"-l debug\" and post the envoy log for me?  One with JWT and one without JWT?\r\n\r\nThanks"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-12-27T20:06:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-01-04T22:21:23Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5113,
    "title": "Http connection reset, but the TCP is succeed when using Valgrind in Istio",
    "created_at": "2018-11-26T11:08:02Z",
    "closed_at": "2019-01-02T19:15:25Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5113",
    "body": "Http connection reset, but the TCP is succeed when using Valgrind in Istio\r\n\r\n### *Description*:\r\nI’m trying to use the tool callgrind in Valgrind to get the real-time call graph of envoy, which is running under the Istio proxy. I revised the envoy start code from _./envoy + parameters_ to _valgrind --tool=callgrind ./envoy + parameters_. But when I use curl to visit nginx http server behind envoy, the HTTP connection is reset so that I could not visit the webpage:\r\n\r\n>$ curl 172.16.0.33:80\r\ncurl: (7) Failed connect to 172.16.0.33:80; Connection refused\r\n\r\n### *Logs*:\r\n\r\n>[2018-11-22 01:44:29.551][13][info][main] external/envoy/source/server/server.cc:203]   transport_sockets.downstream: alts,envoy.transport_sockets.capture,raw_buffer,tls\r\n[2018-11-22 01:44:29.552][13][info][main] external/envoy/source/server/server.cc:206]   transport_sockets.upstream: alts,envoy.transport_sockets.capture,raw_buffer,tls\r\n[2018-11-22 01:44:29.944][13][info][config] external/envoy/source/server/configuration_impl.cc:50] loading 0 static secret(s)\r\n[2018-11-22 01:44:30.325][13][warning][upstream] external/envoy/source/common/config/grpc_mux_impl.cc:240] gRPC config stream closed: 14, no healthy upstream\r\n[2018-11-22 01:44:30.334][13][warning][upstream] external/envoy/source/common/config/grpc_mux_impl.cc:41] Unable to establish new stream\r\n[2018-11-22 01:44:30.335][13][info][config] external/envoy/source/server/configuration_impl.cc:60] loading 0 listener(s)\r\n[2018-11-22 01:44:30.336][13][info][config] external/envoy/source/server/configuration_impl.cc:94] loading tracing configuration\r\n[2018-11-22 01:44:30.337][13][info][config] external/envoy/source/server/configuration_impl.cc:103]   loading tracing driver: envoy.zipkin\r\n[2018-11-22 01:44:30.445][13][info][config] external/envoy/source/server/configuration_impl.cc:116] loading stats sink configuration\r\n[2018-11-22 01:44:30.516][13][info][main] external/envoy/source/server/server.cc:418] starting main dispatch loop\r\n[2018-11-22 01:44:30.615][13][info][upstream] external/envoy/source/common/upstream/cluster_manager_impl.cc:130] cm init: initializing cds\r\n**[2018-11-22 01:44:41.793][13][warning][upstream] external/envoy/source/common/config/grpc_mux_impl.cc:240] gRPC config stream closed: 14, upstream connect error or disconnect/reset before headers\r\n[2018-11-22 01:44:53.281][13][warning][upstream] external/envoy/source/common/config/grpc_mux_impl.cc:240] gRPC config stream closed: 14, upstream connect error or disconnect/reset before headers**\r\n\r\n\r\nIn the proxy container the tcpdump shows that the stream is reset, return the reset flag:\r\n\r\n>$ tcpdump -p tcp\r\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\r\nlistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes\r\n03:08:14.097846 IP 172.16.0.17.44844 > nginx-v1-64dc9b7dc5-9tj8w.http: Flags [S], seq 100742906, win 29200, options [mss 1460,sackOK,TS val 3970652136 ecr 0,nop,wscale 7], length 0\r\n03:08:14.097898 IP nginx-v1-64dc9b7dc5-9tj8w.http > 172.16.0.17.44844: Flags [R.], seq 0, ack 100742907, win 0, length 0\r\n^C\r\n2 packets captured\r\n2 packets received by filter\r\n0 packets dropped by kernel\r\n\r\nWhen I change the backend server to iperf3 to build a **tcp connection**. The connection is successful:\r\n\r\n>$ iperf3 -c 172.16.0.35 -p 5201\r\nConnecting to host 172.16.0.35, port 5201\r\n [  4] local 172.16.0.17 port 55066 connected to 172.16.0.35 port 5201\r\n[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\r\n[  4]   0.00-1.00   sec  1.94 GBytes  16.7 Gbits/sec    0   1.91 MBytes\r\n[  4]   1.00-2.00   sec  1.48 GBytes  12.7 Gbits/sec    0   2.00 Mbytes\r\n\r\n\r\nAnd I can get the normal tcp stream in istio-proxy with tcpdump:\r\n\r\n\r\n>03:16:08.673976 IP 172.16.0.17.55064 > iperf.5201: Flags [S], seq 1098387993, win 28200, options [mss 1410,sackOK,TS val 3971126712 ecr 0,nop,wscale 7], length 0\r\n03:16:08.674029 IP iperf.5201 > 172.16.0.17.55064: Flags [S.], seq 3298436655, ack 1098387994, win 27800, options [mss 1402,sackOK,TS val 3971126713 ecr 3971126712,nop,wscale 7], length 0\r\n03:16:08.674056 IP 172.16.0.17.55064 > iperf.5201: Flags [.], ack 1, win 221, options [nop,nop,TS val 3971126713 ecr 3971126713], length 0\r\n03:16:08.674645 IP 172.16.0.17.55064 > iperf.5201: Flags [P.], seq 1:38, ack 1, win 221, options [nop,nop,TS val 3971126713 ecr 3971126713], length 37\r\n03:16:08.676844 IP iperf.5201 > 172.16.0.17.55064: Flags [.], ack 38, win 218, options [nop,nop,TS val 3971126715 ecr 3971126713], length 0\r\n\r\nThe pods do not restart in both two situations.\r\nIt seems like that the L4 TCP connection is successful but the L7 HTTP connection is not working.\r\nAnyone can figure out what happens?\r\nOr can you recommand other tools to get the call graph?\r\nThank you.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5113/comments",
    "author": "BigYao-c",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-12-26T12:45:00Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-01-02T19:15:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 5097,
    "title": "connect timeout overrides route timeout & per try timeout",
    "created_at": "2018-11-21T21:01:41Z",
    "closed_at": "2019-01-01T04:02:14Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5097",
    "body": "It seems that when per try timeout is less than connect timeout, the connect timeout overrides route timeout.\r\n\r\nVersion: 1.8.0\r\n\r\nConfiguration\r\n================================================\r\nadmin:\r\n  access_log_path: /var/log/envoy/admin_access.log\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9903\r\n      protocol: TCP\r\nstatic_resources:\r\n  clusters:\r\n  - circuit_breakers:\r\n      thresholds:\r\n      - max_connections: 1024\r\n        max_pending_requests: 1024\r\n        max_requests: 1024\r\n        max_retries: 3\r\n    connect_timeout: 50s\r\n    dns_lookup_family: V4_ONLY\r\n    health_checks:\r\n    - event_log_path: /var/log/envoy/health.log\r\n      healthy_threshold: 2\r\n      http_health_check:\r\n        path: /\r\n      interval: 300s\r\n      no_traffic_interval: 300s\r\n      timeout: 3s\r\n      unhealthy_threshold: 1\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n        port_value: '9250'\r\n    lb_policy: ROUND_ROBIN\r\n    name: service_elasticsearch\r\n    outlier_detection:\r\n      base_ejection_time: 60s\r\n      consecutive_5xx: 1\r\n    type: STATIC\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 9200\r\n        protocol: TCP\r\n    filter_chains:\r\n    - filters:\r\n      - config:\r\n          http_filters:\r\n          - name: envoy.router\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - domains:\r\n              - '*'\r\n              name: local_service\r\n              routes:\r\n              - match:\r\n                  prefix: /\r\n                route:\r\n                  cluster: service_elasticsearch\r\n                  timeout: 75s\r\n                  retry_policy:\r\n                    num_retries: 1\r\n                    per_try_timeout: 35s\r\n                    retry_host_predicate: []\r\n                    retry_on: 5xx\r\n          stat_prefix: ingress_http\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n                path: \"/var/log/envoy/egress_http.log\"\r\n        name: envoy.http_connection_manager\r\n    name: listener_0\r\n\r\nLogs\r\n==================================================\r\n[2018-11-21 20:54:04.233][21988][debug][main] source/server/connection_handler_impl.cc:218] [C1] new connection\r\n[2018-11-21 20:54:04.233][21988][trace][connection] source/common/network/connection_impl.cc:437] [C1] socket event: 3\r\n[2018-11-21 20:54:04.233][21988][trace][connection] source/common/network/connection_impl.cc:505] [C1] write ready\r\n[2018-11-21 20:54:04.233][21988][trace][connection] source/common/network/connection_impl.cc:475] [C1] read ready\r\n[2018-11-21 20:54:04.233][21988][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C1] read returns: 78\r\n[2018-11-21 20:54:04.233][21988][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C1] read returns: -1\r\n[2018-11-21 20:54:04.233][21988][trace][connection] source/common/network/raw_buffer_socket.cc:29] [C1] read error: 11\r\n[2018-11-21 20:54:04.233][21988][trace][http] source/common/http/http1/codec_impl.cc:346] [C1] parsing 78 bytes\r\n[2018-11-21 20:54:04.233][21988][trace][http] source/common/http/http1/codec_impl.cc:444] [C1] message begin\r\n[2018-11-21 20:54:04.233][21988][debug][http] source/common/http/conn_manager_impl.cc:203] [C1] new stream\r\n[2018-11-21 20:54:04.233][21988][trace][http] source/common/http/http1/codec_impl.cc:314] [C1] completed header: key=User-Agent value=curl/7.29.0\r\n[2018-11-21 20:54:04.233][21988][trace][http] source/common/http/http1/codec_impl.cc:314] [C1] completed header: key=Host value=localhost:9200\r\n[2018-11-21 20:54:04.233][21988][trace][http] source/common/http/http1/codec_impl.cc:410] [C1] headers complete\r\n[2018-11-21 20:54:04.233][21988][trace][http] source/common/http/http1/codec_impl.cc:314] [C1] completed header: key=Accept value=*/*\r\n[2018-11-21 20:54:04.233][21988][trace][http] source/common/http/http1/codec_impl.cc:431] [C1] message complete\r\n[2018-11-21 20:54:04.233][21988][debug][http] source/common/http/conn_manager_impl.cc:920] [C1][S8320371981278207510] request end stream\r\n[2018-11-21 20:54:04.233][21988][debug][http] source/common/http/conn_manager_impl.cc:521] [C1][S8320371981278207510] request headers complete (end_stream=true):\r\n':authority', 'localhost:9200'\r\n':path', '/'\r\n':method', 'GET'\r\n'user-agent', 'curl/7.29.0'\r\n'accept', '*/*'\r\n\r\n[2018-11-21 20:54:04.233][21988][debug][router] source/common/router/router.cc:252] [C1][S8320371981278207510] cluster 'service_elasticsearch' match for URL '/'\r\n[2018-11-21 20:54:04.233][21988][debug][router] source/common/router/router.cc:303] [C1][S8320371981278207510] router decoding headers:\r\n':authority', 'localhost:9200'\r\n':path', '/'\r\n':method', 'GET'\r\n':scheme', 'http'\r\n'user-agent', 'curl/7.29.0'\r\n'accept', '*/*'\r\n'x-forwarded-proto', 'http'\r\n'x-request-id', 'c84b8090-8d63-4993-a366-625c29ebbb71'\r\n'x-envoy-expected-rq-timeout-ms', '35000'\r\n\r\n[2018-11-21 20:54:04.233][21988][debug][pool] source/common/http/http1/conn_pool.cc:78] creating a new connection\r\n[2018-11-21 20:54:04.233][21988][debug][client] source/common/http/codec_client.cc:25] [C2] connecting\r\n[2018-11-21 20:54:04.233][21988][debug][connection] source/common/network/connection_impl.cc:632] [C2] connecting to 127.0.0.1:9250\r\n[2018-11-21 20:54:04.233][21988][debug][connection] source/common/network/connection_impl.cc:641] [C2] connection in progress\r\n[2018-11-21 20:54:04.233][21988][debug][pool] source/common/http/http1/conn_pool.cc:106] queueing request due to no available connections\r\n[2018-11-21 20:54:04.233][21988][trace][http] source/common/http/conn_manager_impl.cc:750] [C1][S8320371981278207510] decode headers called: filter=0x2a3c8c0 status=1\r\n[2018-11-21 20:54:04.233][21988][trace][http] source/common/http/http1/codec_impl.cc:367] [C1] parsed 78 bytes\r\n[2018-11-21 20:54:04.233][21988][trace][connection] source/common/network/connection_impl.cc:280] [C1] readDisable: enabled=true disable=true\r\n[2018-11-21 20:54:04.233][21988][trace][connection] source/common/network/connection_impl.cc:437] [C1] socket event: 2\r\n[2018-11-21 20:54:04.233][21988][trace][connection] source/common/network/connection_impl.cc:505] [C1] write ready\r\n[2018-11-21 20:54:08.237][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:54:13.238][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:54:18.237][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:54:23.237][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:54:28.237][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:54:33.237][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:54:38.237][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:54:39.232][21988][debug][router] source/common/router/router.cc:945] [C1][S8320371981278207510] upstream per try timeout\r\n[2018-11-21 20:54:39.232][21988][debug][router] source/common/router/router.cc:919] [C1][S8320371981278207510] cancelling pool request\r\n[2018-11-21 20:54:39.232][21988][debug][pool] source/common/http/http1/conn_pool.cc:201] cancelling pending request\r\n[2018-11-21 20:54:39.232][21988][debug][router] source/common/router/router.cc:744] [C1][S8320371981278207510] performing retry\r\n[2018-11-21 20:54:39.238][21988][debug][pool] source/common/http/http1/conn_pool.cc:78] creating a new connection\r\n[2018-11-21 20:54:39.238][21988][debug][client] source/common/http/codec_client.cc:25] [C3] connecting\r\n[2018-11-21 20:54:39.238][21988][debug][connection] source/common/network/connection_impl.cc:632] [C3] connecting to 127.0.0.1:9250\r\n[2018-11-21 20:54:39.238][21988][debug][connection] source/common/network/connection_impl.cc:641] [C3] connection in progress\r\n[2018-11-21 20:54:39.238][21988][debug][pool] source/common/http/http1/conn_pool.cc:106] queueing request due to no available connections\r\n[2018-11-21 20:54:43.237][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:54:48.237][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:54:53.238][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:54:54.232][21988][debug][pool] source/common/http/http1/conn_pool.cc:351] [C2] connect timeout\r\n[2018-11-21 20:54:54.232][21988][debug][connection] source/common/network/connection_impl.cc:99] [C2] closing data_to_write=0 type=1\r\n[2018-11-21 20:54:54.232][21988][debug][connection] source/common/network/connection_impl.cc:181] [C2] closing socket: 1\r\n[2018-11-21 20:54:54.232][21988][debug][client] source/common/http/codec_client.cc:81] [C2] disconnect. resetting 0 pending requests\r\n[2018-11-21 20:54:54.232][21988][debug][pool] source/common/http/http1/conn_pool.cc:122] [C2] client disconnected\r\n[2018-11-21 20:54:54.232][21988][debug][router] source/common/router/router.cc:457] [C1][S8320371981278207510] upstream reset\r\n[2018-11-21 20:54:54.232][21971][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21980][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21979][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21942][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21985][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21970][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21975][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21988][debug][http] source/common/http/conn_manager_impl.cc:1121] [C1][S8320371981278207510] encoding headers via codec (end_stream=false):\r\n':status', '503'\r\n'content-length', '57'\r\n'content-type', 'text/plain'\r\n'date', 'Wed, 21 Nov 2018 20:54:53 GMT'\r\n'server', 'envoy'\r\n\r\n[2018-11-21 20:54:54.232][21972][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21978][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21989][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21988][trace][connection] source/common/network/connection_impl.cc:374] [C1] writing 134 bytes, end_stream false\r\n[2018-11-21 20:54:54.232][21983][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21988][trace][http] source/common/http/conn_manager_impl.cc:1208] [C1][S8320371981278207510] encoding data via codec (size=57 end_stream=true)\r\n[2018-11-21 20:54:54.232][21992][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21988][trace][connection] source/common/network/connection_impl.cc:374] [C1] writing 57 bytes, end_stream false\r\n[2018-11-21 20:54:54.232][21988][trace][main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=1)\r\n[2018-11-21 20:54:54.232][21988][trace][connection] source/common/network/connection_impl.cc:280] [C1] readDisable: enabled=false disable=false\r\n[2018-11-21 20:54:54.232][21988][trace][main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=2)\r\n[2018-11-21 20:54:54.232][21988][debug][upstream] source/common/upstream/cluster_manager_impl.cc:952] membership update for TLS cluster service_elasticsearch\r\n[2018-11-21 20:54:54.232][21988][trace][main] source/common/event/dispatcher_impl.cc:52] clearing deferred deletion list (size=2)\r\n[2018-11-21 20:54:54.233][21988][trace][connection] source/common/network/connection_impl.cc:437] [C1] socket event: 2\r\n[2018-11-21 20:54:54.233][21988][trace][connection] source/common/network/connection_impl.cc:505] [C1] write ready\r\n[2018-11-21 20:54:54.233][21988][trace][connection] source/common/network/raw_buffer_socket.cc:62] [C1] write returns: 191\r\n[2018-11-21 20:54:54.233][21988][trace][connection] source/common/network/connection_impl.cc:437] [C1] socket event: 3\r\n[2018-11-21 20:54:54.233][21988][trace][connection] source/common/network/connection_impl.cc:505] [C1] write ready\r\n[2018-11-21 20:54:54.233][21988][trace][connection] source/common/network/connection_impl.cc:475] [C1] read ready\r\n[2018-11-21 20:54:54.233][21988][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C1] read returns: 0\r\n[2018-11-21 20:54:54.233][21988][debug][connection] source/common/network/connection_impl.cc:499] [C1] remote close\r\n[2018-11-21 20:54:54.233][21988][debug][connection] source/common/network/connection_impl.cc:181] [C1] closing socket: 0\r\n[2018-11-21 20:54:54.233][21988][debug][main] source/server/connection_handler_impl.cc:51] [C1] adding to cleanup list\r\n[2018-11-21 20:54:54.233][21988][trace][main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=1)\r\n[2018-11-21 20:54:54.233][21988][trace][main] source/common/event/dispatcher_impl.cc:52] clearing deferred deletion list (size=1)\r\n[2018-11-21 20:54:58.238][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:55:03.238][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:55:08.238][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:55:13.239][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:55:18.239][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:55:23.240][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:55:28.240][21942][debug][main] source/server/server.cc:139] flushing stats\r\n[2018-11-21 20:55:29.238][21988][debug][pool] source/common/http/http1/conn_pool.cc:351] [C3] connect timeout\r\n[2018-11-21 20:55:29.238][21988][debug][connection] source/common/network/connection_impl.cc:99] [C3] closing data_to_write=0 type=1\r\n[2018-11-21 20:55:29.238][21988][debug][connection] source/common/network/connection_impl.cc:181] [C3] closing socket: 1\r\n[2018-11-21 20:55:29.238][21988][debug][client] source/common/http/codec_client.cc:81] [C3] disconnect. resetting 0 pending requests\r\n[2018-11-21 20:55:29.238][21988][debug][pool] source/common/http/http1/conn_pool.cc:122] [C3] client disconnected\r\n[2018-11-21 20:55:29.238][21988][trace][main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=1)\r\n[2018-11-21 20:55:29.238][21988][trace][main] source/common/event/dispatcher_impl.cc:52] clearing deferred deletion list (size=1)\r\n\r\nObservations\r\n==============================================\r\nAs expected the per try timeout triggered and a retry was fired. But even before the 2nd per timeout, the first connection timeout was triggered and a local response was forced.  ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5097/comments",
    "author": "ashimrana",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-11-22T14:20:41Z",
        "body": "Right now connect timeout is orthogonal to route timeouts, per-try timeouts, etc. IMO this is the right behavior and changing it will not be easy. Can you explain why you think the behavior should be changed?"
      },
      {
        "user": "ashimrana",
        "created_at": "2018-11-22T15:41:38Z",
        "body": "The documentation does not state that connect timeout has priority over other time outs. Also it seems that the per-try time is supposed to cancel the association between pending request and the underlying connection so that while pending request goes on for a retry (if any) on another connection, the state of the old connection does not interfere with the now disassociated request.\r\n\r\nSay, while the host1 does not respond within the per-try timeout, I want it to be retried on host-2 for min(connect timeout, per-try time out, remaining route time-out). Currently, the host-2 gets only (connect timeout - per try time out) time to respond. If connect timeout and per-try time out are very close, I am afraid host-2 would not get sufficient time to respond, even though as per the envoy configuration it is supposed to get.\r\n\r\n "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-11-24T14:54:30Z",
        "body": "OK I see what you are saying. Sorry, can you clarify why you think the connect timeout is overriding the other timeouts? I don't think it should be."
      },
      {
        "user": "snowp",
        "created_at": "2018-11-24T18:46:16Z",
        "body": "Perhaps this is related to #4903 where a slow connection setup would eat into the per try timeout? "
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-12-24T22:09:50Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-01-01T04:02:13Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4987,
    "title": "Support request timeout with retries",
    "created_at": "2018-11-07T17:22:46Z",
    "closed_at": "2018-11-08T17:07:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4987",
    "body": "*Description*:\r\n> The request timeout likely only spans first attempt. We should determine the desired behaviour for request timeouts with retries and how to configure that.\r\n\r\nThis is pending #4456.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4987/comments",
    "author": "auni53",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-11-07T18:52:39Z",
        "body": "@auni53 per my comment in the related issue I'm not sure there is anything to do here. Can you clarify?"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-11-08T17:07:51Z",
        "body": "Closing per related discussion."
      }
    ]
  },
  {
    "number": 4986,
    "title": "Help needed to write EnvoyFilter which can intercepts the packets based upon IP address or interface ",
    "created_at": "2018-11-07T14:43:18Z",
    "closed_at": "2018-12-14T20:16:17Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4986",
    "body": "\r\nHelp needed to write EnvoyFilter which can intercepts the packets based upon IP address or interface of POD\r\n\r\nDescription:\r\n\r\nI wish to write a EnvoyFilter which can intercepts the packets based upon IP address or interface of POD . Can someone please refer me such  examples ? I think the syntax will be something like this below ?But not not sure about the lua code ? If someone can show the examples of lua code to intercepts IP's or interface of POD , it will be awesome . Thanks in advance,\r\n\r\n\r\napiVersion: networking.istio.io/v1alpha3\r\nkind: EnvoyFilter\r\nmetadata:\r\n  name: reviews-lua\r\nspec:\r\n  workloadLabels:\r\n    app: reviews\r\n  filters:\r\n  - listenerMatch:\r\n      portNumber: 8080\r\n      listenerType: SIDECAR_INBOUND #will match with the inbound listener for reviews:8080\r\n    filterName: envoy.lua\r\n    filterType: HTTP\r\n    filterConfig:\r\n      inlineCode: |\r\n        ... lua code ...\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4986/comments",
    "author": "marhatha",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-12-07T19:53:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-12-14T20:16:16Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4961,
    "title": "how to  dynamicly config route to match the header of the request to select the instance",
    "created_at": "2018-11-05T07:01:44Z",
    "closed_at": "2019-01-18T17:36:46Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4961",
    "body": "**Issue Template**\r\nin multi-tenant scene, instances of a service may have different label, we could config roure configuration for every instance to match header, however the config is troublesome and we should update the config in its life-cycle. so if exist a way to config it easy?\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\n>Describe the issue. Please be detailed. If a feature request, please\r\ndescribe the desired behaviour, what scenario it enables and how it\r\nwould be used.\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4961/comments",
    "author": "wansuiye",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-12-05T21:05:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "utkarsh-simha",
        "created_at": "2018-12-11T20:48:18Z",
        "body": "If I understand correctly, the requirement is to route to a specific instance of a service dynamically?\r\n\r\nOne solution, as you mentioned is to use `cluster_header: foo` to route dynamically based on the value of the given header `foo`. If the configuration needs to be updated, you can use a control plane to do so\r\n\r\nThe other solution is to use subset load balancing which allows you to build subsets based on key-value \"tags\". Each service instance can be tagged with a unique ID. You route your request to a cluster which defines your service where the service instances are endpoints. Now, you can choose a specific instance of a service by providing the value for the key using a header, and use the header-to-metadata filter to convert the header to the subset load balancer metadata."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-01-10T22:16:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-01-18T17:36:45Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "xiaoma2015",
        "created_at": "2021-09-18T06:25:40Z",
        "body": "> If I understand correctly, the requirement is to route to a specific instance of a service dynamically?\r\n> \r\n> One solution, as you mentioned is to use `cluster_header: foo` to route dynamically based on the value of the given header `foo`. If the configuration needs to be updated, you can use a control plane to do so\r\n> \r\n> The other solution is to use subset load balancing which allows you to build subsets based on key-value \"tags\". Each service instance can be tagged with a unique ID. You route your request to a cluster which defines your service where the service instances are endpoints. Now, you can choose a specific instance of a service by providing the value for the key using a header, and use the header-to-metadata filter to convert the header to the subset load balancer metadata.\r\n\r\n@utkarsh-simha   I  understand  the dynamicly not just this meaning.  It's we can  dynamicly route the request to the right instance  accordingly to its tags. for example:  in subset lb， all the endpoint has the tag “version” filter metadata with as many as 100 values, we can just route  the. request according to  its tag's (\"version\") value dynamicly .  its not necessary to list all the route cases in rds ,  but envoy does not support this case now。\r\n"
      }
    ]
  },
  {
    "number": 4955,
    "title": "Is there a binary that coule be installed directly on ubuntu and centos?",
    "created_at": "2018-11-03T13:51:43Z",
    "closed_at": "2018-12-12T15:29:44Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4955",
    "body": "Is there a binary that coule be installed directly on ubuntu or centos?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4955/comments",
    "author": "zwl1619",
    "comments": [
      {
        "user": "taion809",
        "created_at": "2018-11-05T14:33:08Z",
        "body": "there aren't any published binaries.  You can use docker cp to pull the compiled binary from the published docker image though\r\n\r\nEx:\r\n\r\n```\r\nARG envoy_version=latest\r\n\r\nFROM envoyproxy/envoy:${envoy_version}\r\nENV PATH=$PATH:/usr/local/go/path\r\n\r\nWORKDIR /dist/envoy\r\nRUN cp /usr/local/bin/envoy .\r\n\r\nFROM fpm-ubuntu:latest\r\n\r\nRUN mkdir -p /build\r\nCOPY --from=0 /dist/envoy/envoy /build/envoy\r\n```"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-12-05T15:05:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-12-12T15:29:43Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4902,
    "title": "unhealthy_interval doesn't work",
    "created_at": "2018-10-29T22:30:34Z",
    "closed_at": "2018-12-07T04:05:34Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4902",
    "body": "*Title*: *unhealthy_interval doesn't work*\r\n\r\n*Description*:\r\n\r\nFor health check config:\r\n```\r\n       health_checks:\r\n      - timeout: 60s\r\n        interval: 60s\r\n        unhealthy_interval: 0.1s\r\n        unhealthy_threshold: 1\r\n        healthy_threshold: 1\r\n        tcp_health_check: {}\r\n```\r\n\r\nI expect a TCP connection attempt every 0.1 seconds for an unhealthy backend. It looks like it is happening every 1 minute.\r\n\r\n*Repro steps*:\r\n\r\n1.  I created a health check config like the following:\r\n     ```\r\n       health_checks:\r\n      - timeout: 60s\r\n        interval: 60s\r\n        unhealthy_interval: 0.1s\r\n        unhealthy_threshold: 1\r\n        healthy_threshold: 1\r\n        tcp_health_check: {}\r\n     ```\r\n     As a reverse proxy to port 8080\r\n     ```\r\n       filter_chains:\r\n       - filters:\r\n         - name: envoy.http_connection_manager\r\n           config:\r\n             stat_prefix: ingress_http\r\n             codec_type: AUTO\r\n             stream_idle_timeout: 3600s\r\n             route_config:\r\n               name: local_route\r\n               virtual_hosts:\r\n               - name: local_service\r\n                 domains:\r\n                   - \"*\"\r\n                 routes:\r\n                 - match:\r\n                     prefix: \"/\"\r\n                   route:\r\n                     cluster: worker_endpoint\r\n                     timeout: 300s\r\n    clusters:\r\n     - name: worker_endpoint\r\n       connect_timeout: 1s\r\n       type: STRICT_DNS\r\n      lb_policy: ROUND_ROBIN\r\n       hosts:\r\n         socket_address:\r\n           address: 127.0.0.1\r\n           port_value: 8080\r\n     ```\r\n1.  Launch Envoy.\r\n1.  Run tcpdump on 8080\r\n     ```\r\n     $ sudo tcpdump -i lo -nn -v port 8080\r\n     ```\r\n1.  Look at clusters/ verify worker_container is marked as unhealthy\r\n\r\nWhat I expect to see is that tcpdump shows a TCP connection attempt every 0.1 seconds, but I don't see that. It looks more like every 1 minute.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4902/comments",
    "author": "misterwilliam",
    "comments": [
      {
        "user": "tak2siva",
        "created_at": "2018-10-30T05:03:38Z",
        "body": "@misterwilliam Is there a traffic on this upstream cluster ? If not `no_traffic_interval `(default 60 sec) will kick in.\r\n\r\n@mattklein123 If its not the above case. Can you assign this to me ? will look into it"
      },
      {
        "user": "aschran",
        "created_at": "2018-10-30T17:19:47Z",
        "body": "@tak2siva I think the suggestion here is that when `no_traffic_interval` is not set, then if upstream has never yet passed a health check successfully, `unhealthy_interval` should apply. Does that match your understanding? Or are you saying that there is no way to disable `no_traffic_interval`, and if it's not explicitly set then it will be treated the same as if it were explicitly set to 60 sec? (If so, is that deliberate, or can we make it possible to configure `no_traffic_interval` as \"undefined\"?)"
      },
      {
        "user": "misterwilliam",
        "created_at": "2018-10-30T18:10:11Z",
        "body": "@tak2siva what @aschran said is correct. There was no traffic on this upstream cluster. I was hoping that if `no_traffic_interval` was not set set, and the upstream cluster is unhealthy. The `unhealthy_interval` would apply."
      },
      {
        "user": "qiannawang",
        "created_at": "2018-10-31T03:11:04Z",
        "body": "+1. I would expect unhealthy_interval is independent from the other interval settings and the traffic amount. In particular, it would apply no matter there is traffic or not or whether no_traffic_interval is set or not."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-11-30T03:33:46Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-12-07T04:05:33Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4901,
    "title": "Prefix rewrite and prefix",
    "created_at": "2018-10-29T21:26:21Z",
    "closed_at": "2018-12-05T23:05:30Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4901",
    "body": "For some reason I can't get it working.\r\n\r\n```\r\n                    - name: backend\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                          - match:\r\n                              prefix: \"/prefix/\"\r\n                            route:\r\n                              prefix_rewrite: \"/\"\r\n                              cluster: service1\r\n                          - match:\r\n                              prefix: \"/prefix\"\r\n                            route:\r\n                              prefix_rewrite: \"/\"\r\n                              cluster: service1\r\n```\r\nI'm trying to forward `/prefix/hello => /hello` or `/prefix => /`\r\n\r\nEnvoy shows this:\r\n```\r\n[2018-10-29 21:16:33.778][000040][debug][router] [source/common/router/router.cc:261] [C2][S11568900481930739193] cluster 'service1' match for URL '/prefix/hello'\r\n| [2018-10-29 21:16:33.778][000040][debug][router] [source/common/router/router.cc:317] [C2][S11568900481930739193] router decoding headers:\r\n| ':authority', 'localhost:8000'\r\n| ':path', '//hello'\r\n| ':method', 'GET'\r\n| ':scheme', 'http'\r\n| 'upgrade-insecure-requests', '1'\r\n| 'user-agent', 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.67 Safari/537.36'\r\n| 'accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'\r\n| 'accept-encoding', 'gzip, deflate, br'\r\n| 'accept-language', 'en-US,en;q=0.9'\r\n| 'x-forwarded-proto', 'http'\r\n| 'x-request-id', '39831c26-12ef-44b6-9319-60df0ca61a46'\r\n| 'x-envoy-expected-rq-timeout-ms', '15000'\r\n| 'x-envoy-original-path', '/prefix/hello'\r\n|\r\n| [2018-10-29 21:16:33.778][000040][debug][pool] [source/common/http/http2/conn_pool.cc:97] [C1] creating stream\r\n| [2018-10-29 21:16:33.778][000040][debug][router] [source/common/router/router.cc:1000] [C2][S11568900481930739193] pool ready\r\n| [2018-10-29 21:16:33.778][000040][debug][client] [source/common/http/codec_client.cc:94] [C1] response complete\r\n| [2018-10-29 21:16:33.778][000040][debug][pool] [source/common/http/http2/conn_pool.cc:189] [C1] destroying stream: 0 remaining\r\n| [2018-10-29 21:16:33.778][000040][debug][router] [source/common/router/router.cc:600] [C2][S11568900481930739193] upstream headers complete: end_stream=true\r\n| [2018-10-29 21:16:33.778][000040][debug][http] [source/common/http/conn_manager_impl.cc:1095] [C2][S11568900481930739193] encoding headers via codec (end_stream=true):\r\n| ':status', '404'\r\n| 'date', 'Mon, 29 Oct 2018 21:16:33 GMT'\r\n| 'server', 'envoy'\r\n| 'x-envoy-upstream-service-time', '0'\r\n```\r\nits throwing 404 because for some reason its doing `/prefix/hello => //hello`. \r\n\r\nWhats the proper way to do it? Thanks",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4901/comments",
    "author": "rislah",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-11-28T22:25:52Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-12-05T23:05:29Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4900,
    "title": "[question] streaming gRPC requests or responses and JSON transcoder ",
    "created_at": "2018-10-29T20:51:59Z",
    "closed_at": "2018-11-01T21:53:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4900",
    "body": "\r\n*Title*: *Is it possible to deal with the streaming gRPC requests or responses using JSON transcoder and avoid buffering?*\r\n\r\n*Description*:\r\nJSON transcoder seems to be doing a great job of, essentially, allowing people to have a service that only implements gRPC-based API to be accessible via both gRPC and REST/JSON. As we know, gRPC has a streaming feature that can be used for the request, response or both. The documentation for the transcoder clearly says that, if I understand it correctly, a streaming request would become an array and the same happens to the response. Thus, the streams would be buffered and converted into a single request/response.\r\n\r\nFrom what I understand, the thing closest to gRPC streaming over HTTP/2 in HTTP < 2.0 world are multipart messages. Is it possible to use the filter (or develop such a filter) that would transcode the streamed gRPC messages to JSON and send them as parts of a multi-part request or response? \r\n\r\nI suspect it may be a long shot. I think it would be great to do something like this, or state it clearly that it is beyond the limits of what can be done with the filter.\r\n\r\nThanks!\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4900/comments",
    "author": "ngrigoriev",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-10-29T21:22:54Z",
        "body": "@lizan "
      },
      {
        "user": "lizan",
        "created_at": "2018-10-29T21:41:54Z",
        "body": "> a streaming request would become an array and the same happens to the response.\r\n\r\nThis is right, while\r\n\r\n> Thus, the streams would be buffered and converted into a single request/response.\r\n\r\nis not always true. The gRPC-JSON filter converts each array element (gRPC frame) when they are available and send to upstream/downstream. In HTTP/2 this will be DATA frames, in HTTP/1.1 this will be chunked transfer encoding.\r\n\r\n"
      },
      {
        "user": "ngrigoriev",
        "created_at": "2018-10-29T22:44:24Z",
        "body": "@lizan \r\n\r\nBut a chunk does not really mean a complete fragment, while MIME part does. In HTTP/2 case it would be just a bunch of DATA frames carrying a complete protobuf message.\r\n\r\nAnyway, the next question is: does the transcoder buffer the entire message or sends the data as it comes? Lets consider the example when the server sends back a gRPC stream - stream of protobuf messages. Will transcoder wait before the end of stream (or the end of some kind of buffer) before sending out the first byte? Or will it convert the first message to JSON and send it back to the client as a set of one or more chunks?\r\n\r\n"
      },
      {
        "user": "lizan",
        "created_at": "2018-10-29T23:17:03Z",
        "body": "@ngrigoriev It would buffer each message in the stream, but not the entire request/response. It wouldn't wait for end of stream before sending out first byte in streaming case.\r\n"
      },
      {
        "user": "ngrigoriev",
        "created_at": "2018-10-30T00:40:52Z",
        "body": "Thanks for the clarification, I am going to try it this way to see how it behaves. Probably will post a little demo :)"
      },
      {
        "user": "lizan",
        "created_at": "2018-11-01T21:53:54Z",
        "body": "Closing as answered, feel free to open another issue or comment on this one to follow up."
      }
    ]
  },
  {
    "number": 4890,
    "title": "ratelimit actions can't function correct",
    "created_at": "2018-10-29T06:54:16Z",
    "closed_at": "2018-12-05T18:05:31Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4890",
    "body": "When two header_value_match actions exists , envoy proxy will not generate any desccripter !\r\n\r\nfollowing is my config \r\n__________________________________________\r\n virtual_hosts:\r\n            - name: backend\r\n              domains:\r\n              - \"*\"\r\n              rate_limits:\r\n                - stage: 0\r\n                  actions:\r\n                    - header_value_match: {\"descriptor_value\": \"loginuser\",\"expect_match\":true,\"headers\": [{ \"name\": \"jsessionid\"}]}\r\n                    - header_value_match: {\"descriptor_value\": \"guest\",\"expect_match\":false,\"headers\": [{ \"name\": \"jsessionid\"}]}\r\n________________\r\nwhen test with curl ,no ratelimit debug log occured \r\n\r\n[root@kub-master ~]# curl localhost:30082/service/1\r\nHello from behind Envoy (service 1)! hostname: service1 resolvedhostname: 10.32.0.16\r\n[root@kub-master ~]# curl -H \"jsessionid:333\" localhost:30082/service/1\r\nHello from behind Envoy (service 1)! hostname: service1 resolvedhostname: 10.32.0.16\r\n\r\nwhen I remove second action and restart , It's OK.\r\n\r\n^[[A[2018-10-29 14:53:00.220][10][debug][router] source/common/router/router.cc:252] [C0][S16081177476653114950] cluster 'rate_limit_cluster' match for URL '/envoy.service.ratelimit.v2.RateLimitService/ShouldRateLimit'\r\n':path', '/envoy.service.ratelimit.v2.RateLimitService/ShouldRateLimit'\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4890/comments",
    "author": "leader-us",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-11-28T17:25:52Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-12-05T18:05:30Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4834,
    "title": "Envoy Ext Auth Filter does not add/modify original headers in the client in v1.6",
    "created_at": "2018-10-23T20:45:29Z",
    "closed_at": "2018-10-24T16:31:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4834",
    "body": "*Description*:\r\n> I have an auth service which does external authorization for the rails client. The auth svc responds with 200 code but I don't see modified http request headers when I log them in the app. For example, I want to access \"x-foo-bar\" and override \"x-baz\" header in the app which are being modified/added in the ok_reponse on auth svc.\r\n\r\nNotes:\r\n1. auth svc is a GRPC service\r\n2. Client is a rails app.\r\n\r\n*Config*:\r\nIngress listener\r\n{\r\n                      \"name\": \"envoy.ext_authz\",\r\n                      \"config\": {\r\n                        \"grpc_service\": {\r\n                          \"envoy_grpc\": { \"cluster_name\": \"auth\" }\r\n                        }\r\n}\r\n\r\ncluster:\r\n\"name\": \"auth\",\r\n\"connect_timeout\": \"0.25s\",\r\n\"type\": \"EDS\",\r\n\r\nThere are other configs in the cluster that Im not allowed to show here.\r\n\r\nGRPC response from auth looks like this:\r\n { http_response: 'ok_response',\r\n  status: \r\n   { code: 0,\r\n     message: 'blah blah blah',\r\n     details: [] },\r\n  denied_response: null,\r\n  ok_response: { headers: [ [Object], [Object] ] } }\r\n\r\neach object looks like this:\r\n{ header: { key: 'x-foo-bar', value: 'test' },\r\n  append: { value: false } }\r\n\r\n\r\nQuestions:\r\n1. Is there any functionality to do this for grpc auth services?\r\n2. If yes, is there a bug in the config or am I using an old version (v1.6)?\r\n3. If no, any workaround for this on envoy?\r\n\r\nAny suggestions? @gsagula (saw few issues related to this where you were tagged)",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4834/comments",
    "author": "abhi9git",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-10-24T01:06:54Z",
        "body": "Please use 1.8.0/current master. There has been a lot of changes since the version you are using."
      }
    ]
  },
  {
    "number": 4733,
    "title": "Building a shared library?",
    "created_at": "2018-10-15T21:02:29Z",
    "closed_at": "2018-11-23T03:03:52Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4733",
    "body": "**Issue Template**\r\n\r\n*Title*: *Building a shared library?*\r\n\r\n*Description*:\r\nIs there a way to ask the build system to build a shared library that one can link a program written using the C++ API against? Since in my company, we already have a plugin manager works with .so to handle the life cycle of a shared library, so we want to take the best effort to reuse it. \r\n\r\n*Relevant Links*:\r\nAfter several surveys with bazel build system, I tried to add a ``linkshared=1`` in cc_binary, then try to build the origin envoy-static as an envoy-static.so.\r\nHowever, the bazel system outputs maily with two different kinds of errors:\r\nThe first one seems caused by linking a shared library (which requires position-independent code, PIC) to a static library (which has not been compiled with PIC)?\r\n```bash\r\n.....\r\n/usr/bin/ld.gold: error: external/envoy_deps/thirdparty_build/lib/libyaml-cpp.a(parser.cpp.o): requires dynamic R_X86_64_PC32 reloc against 'memcmp' which may overflow at runtime; recompile with -fPIC\r\n/usr/bin/ld.gold: error: external/envoy_deps/thirdparty_build/lib/libyaml-cpp.a(parser.cpp.o): requires dynamic R_X86_64_32 reloc which may overflow at runtime; recompile with -fPIC\r\n/usr/bin/ld.gold: error: external/envoy_deps/thirdparty_build/lib/libyaml-cpp.a(parser.cpp.o): requires dynamic R_X86_64_32 reloc which may overflow at runtime; recompile with -fPIC\r\n/usr/bin/ld.gold: error: external/envoy_deps/thirdparty_build/lib/libyaml-cpp.a(regex_yaml.cpp.o): requires dynamic R_X86_64_PC32 reloc against '_Znwm' which may overflow at runtime; recompile with -fPIC\r\n/usr/bin/ld.gold: error: external/envoy_deps/thirdparty_build/lib/libyaml-cpp.a(regex_yaml.cpp.o): requires dynamic R_X86_64_PC32 reloc against '_Znwm' which may overflow at runtime; recompile with -fPIC\r\n/usr/bin/ld.gold: error: external/envoy_deps/thirdparty_build/lib/libyaml-cpp.a(regex_yaml.cpp.o): requires dynamic R_X86_64_32 reloc which may overflow at runtime; recompile with -fPIC\r\n/usr/bin/ld.gold: error: external/envoy_deps/thirdparty_build/lib/libyaml-cpp.a(regex_yaml.cpp.o): requires dynamic R_X86_64_32 reloc which may overflow at runtime; recompile with -fPIC\r\n.....\r\n```\r\nThe other kind error seems caused by multiple definitions:\r\n```bash\r\n/usr/bin/ld.gold: bazel-out/k8-fastbuild/bin/external/lightstep_vendored_googleapis/_objs/googleapis_proto/http.pb.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-fastbuild/bin/external/googleapis/_objs/http_api_protos/http.pb.pic.o: multiple definition of 'google::api::HttpRule::IsInitialized() const'\r\n/usr/bin/ld.gold: bazel-out/k8-fastbuild/bin/external/lightstep_vendored_googleapis/_objs/googleapis_proto/http.pb.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-fastbuild/bin/external/googleapis/_objs/http_api_protos/http.pb.pic.o: multiple definition of 'google::api::HttpRule::Swap(google::api::HttpRule*)'\r\n/usr/bin/ld.gold: bazel-out/k8-fastbuild/bin/external/lightstep_vendored_googleapis/_objs/googleapis_proto/http.pb.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-fastbuild/bin/external/googleapis/_objs/http_api_protos/http.pb.pic.o: multiple definition of 'google::api::HttpRule::InternalSwap(google::api::HttpRule*)'\r\n/usr/bin/ld.gold: bazel-out/k8-fastbuild/bin/external/lightstep_vendored_googleapis/_objs/googleapis_proto/http.pb.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-fastbuild/bin/external/googleapis/_objs/http_api_protos/http.pb.pic.o: multiple definition of 'google::api::HttpRule::UnsafeArenaSwap(google::api::HttpRule*)'\r\n```\r\n\r\nWill envoy team suggest or support build shared library formally?\r\nI'm really glad to hear your advice! Tks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4733/comments",
    "author": "tony-shi",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-11-16T02:31:31Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-11-23T03:03:51Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4718,
    "title": "continuous update rejects could flood envoy with discovery requests",
    "created_at": "2018-10-14T10:05:19Z",
    "closed_at": "2018-10-30T01:24:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4718",
    "body": "If for a particular reason, an update is rejected, Envoy sends back a response immediately indicating the failure to management server. However if management server, responds again with incorrect data with possibility of update being rejected, Envoy would send the update rejected response immediately.\r\nThis continuous flow of rejections can be quite chatty and could flood envoy with discovery requests.\r\n\r\nIt is better to implement back-off for update rejected cases with back-off starting immediately after the first failure.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4718/comments",
    "author": "ramaraochavali",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2018-10-15T01:53:40Z",
        "body": "Arguably the management server should implement back-off here @ramaraochavali. "
      },
      {
        "user": "ramaraochavali",
        "created_at": "2018-10-15T03:18:20Z",
        "body": "@htuch I was also thinking that management server should implement the back-off here. However following (especially the first one) was the motivation for the issue and the PR.\r\n- Should Envoy protect itself from a management server misbehaviour possibly because of a bug in it's implementation or some other conditions?\r\n- Would it be useful for all management servers if Envoy implements this instead of every one doing it?\r\nI am Ok if we say it is a conscious design choice that management server should take care of this."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2018-10-15T12:19:20Z",
        "body": "Another point to consider here request back-off vs. response back-off which one is better as a design choice? I tried both with this PR in envoy and implementing it in our management server on the response side - request side looked more natural to me, but it's just me. If it is true rate limiting server side might look natural. But here we need to definitely respond but with a back-off so that's where I felt, request side may be better.\r\n\r\nI am definitely open to suggestions and ideas here.\r\n\r\nLMK what you think?"
      },
      {
        "user": "junr03",
        "created_at": "2018-10-15T16:46:25Z",
        "body": "@htuch jumping in from the left field here. ~I vaguely remember someone bringing up a similar line of conversation a few months ago. I think we discussed \"ratelimiting\" discovery interactions back then.~ Found the discussion in #2169 (if you are interested, @ramaraochavali). "
      },
      {
        "user": "htuch",
        "created_at": "2018-10-15T19:36:01Z",
        "body": "Yeah, I think we can do back-off in Envoy, we just need to make sure it doesn't affect normal rapid successful updates. I still think Envoy doesn't need to protect itself as the management server is essentially trusted (a broken management server can do a lot more bad things than cause Envoy to spin..). But, if you want to add this, seems legit."
      }
    ]
  },
  {
    "number": 4672,
    "title": "[Question] /envoy_shared_memory_110 check user permissions. Error: File exists",
    "created_at": "2018-10-10T14:28:45Z",
    "closed_at": "2018-10-11T01:10:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4672",
    "body": "**Issue Template**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\nI started envoy with root user by mistake. I killed it.\r\nThen when i start envoy again with my own user, it says:\r\n[!184 06:42:56  ~]$ [2018-10-10 06:42:56.702][23046][critical][assert] source/server/hot_restart_impl.cc:62] panic: cannot open shared memory region /envoy_shared_memory_110 check user permissions. Error: File exists\r\n\r\nWhich file should i remove before i start envoy with my own users? Thanks\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4672/comments",
    "author": "huxiaobabaer",
    "comments": [
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-10T14:30:15Z",
        "body": "Full screen output\r\n===============\r\n[!185 07:29:28  ~]$ /home/zapp/apps/envoy -c /home/zapp/apps/11/envoy/config.yaml --base-id 11 --v2-config-only\r\n[2018-10-10 07:29:31.869][26733][critical][assert] source/server/hot_restart_impl.cc:62] panic: cannot open shared memory region /envoy_shared_memory_110 check user permissions. Error: File exists\r\n[2018-10-10 07:29:31.871][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:125] Caught Aborted, suspect faulting address 0x1f40000686d\r\n[2018-10-10 07:29:31.873][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:94] Backtrace thr<0> obj</lib64/libc.so.6> (If unsymbolized, use tools/stack_decode.py):\r\n[2018-10-10 07:29:31.887][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #0 0x7f4a4c829277 raise\r\n[2018-10-10 07:29:31.899][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #1 0x7f4a4c82a967 abort\r\n[2018-10-10 07:29:31.899][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</home/zapp/apps/envoy>\r\n[2018-10-10 07:29:31.984][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #2 0x6cf1c4 Envoy::Server::SharedMemory::initialize()\r\n[2018-10-10 07:29:32.068][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #3 0x6cf8b2 Envoy::Server::HotRestartImpl::HotRestartImpl()\r\n[2018-10-10 07:29:32.153][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #4 0x48e3d7 Envoy::MainCommonBase::MainCommonBase()\r\n[2018-10-10 07:29:32.238][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #5 0x48e732 Envoy::MainCommon::MainCommon()\r\n[2018-10-10 07:29:32.322][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #6 0x419905 main\r\n[2018-10-10 07:29:32.322][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</lib64/libc.so.6>\r\n[2018-10-10 07:29:32.335][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #7 0x7f4a4c815444 __libc_start_main\r\n[2018-10-10 07:29:32.335][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</home/zapp/apps/envoy>\r\n[2018-10-10 07:29:32.420][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<0> #8 0x484834 (unknown)\r\n[2018-10-10 07:29:32.420][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:121] end backtrace thread 0\r\nAborted (core dumped)\r\n[!186 07:29:32 zapp@5.fet.stg.slv.zuora ~]$\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-10T16:30:01Z",
        "body": "`/dev/shm/envoy_shared_memory_110` on Linux."
      },
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-11T01:10:40Z",
        "body": "@mattklein123   Thank you very much!!!"
      }
    ]
  },
  {
    "number": 4651,
    "title": "jwt-authn exception",
    "created_at": "2018-10-09T13:13:53Z",
    "closed_at": "2018-10-09T16:29:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4651",
    "body": "**Issue Template**\r\n\r\n*Title*: *jwt-authn exception*\r\n\r\n*Description*:\r\n>Im using the jwt-authn http-filter for validating JWT. Im using the version 2 API reference of envoy and the following envoy image version tag : fdfa5bde3343372ad662a830da0bdc3aea806f4d\r\n\r\n>Im getting following exception : \r\n[critical][main] source/server/server.cc:80] error initializing configuration '/etc/front-envoy.yaml': Didn't find a registered implementation for name: 'envoy.jwt_authn'\r\n\r\n*config*:\r\n```\r\n- name: envoy.jwt_authn\r\n            config:\r\n```\r\n      \r\nAm i using the wrong name ?\r\n\r\nthanks     \r\n\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4651/comments",
    "author": "githubYasser",
    "comments": [
      {
        "user": "qiwzhang",
        "created_at": "2018-10-09T16:04:30Z",
        "body": "It should be:  envoy.filters.http.jwt_authn"
      },
      {
        "user": "githubYasser",
        "created_at": "2018-10-09T16:29:38Z",
        "body": "thank you , working now."
      }
    ]
  },
  {
    "number": 4640,
    "title": "How to run multi envoy process on one linux server",
    "created_at": "2018-10-08T16:30:21Z",
    "closed_at": "2018-10-10T14:19:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4640",
    "body": "*Title*: *One line description*\r\nHow to run multi envoy process on one linux server\r\n\r\n*Description*:\r\nI am not using docker container.\r\nI have the need to want to start two envoy processes on one linux server.\r\n\r\nSteps:\r\n1) create two folder on a centos server\r\n/root/envoy1 and /root/envoy2\r\n2) scp envoy binary (built on centos7) to the server under /root/envoy1 and /root/envoy2\r\n3) Prepare config.yaml files with diff ports \r\n4) Start envoy from folder 1. it works well\r\n/root/envoy1/envoy -c /root/envoy1/config.yaml --service-cluster myapp1 --service-node myapp1 \r\n5) Start envoy from folder 2. Nothing happens. And there is no screen output and log for further information\r\n/root/envoy2/envoy -c /root/envoy2/config.yaml --service-cluster myapp2 --service-node myapp2\r\n\r\nAny suggestion?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4640/comments",
    "author": "huxiaobabaer",
    "comments": [
      {
        "user": "moderation",
        "created_at": "2018-10-08T16:36:34Z",
        "body": "@huxiaobabaer from `envoy --help`. Works well and I run multiple Envoy's on a single host.\r\n\r\n```\r\n   --base-id <uint32_t>\r\n     base ID so that multiple envoys can run on the same host if needed\r\n```"
      },
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-08T16:43:25Z",
        "body": "@moderation What is the default value of base id if not specified? TKS"
      },
      {
        "user": "moderation",
        "created_at": "2018-10-08T16:52:15Z",
        "body": "I don't believe there is a default. I start with 0 and count up from there. 0 to 4294967295 works."
      },
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-08T16:59:25Z",
        "body": "Thank you very much for your help!!! @moderation "
      }
    ]
  },
  {
    "number": 4586,
    "title": "Envoy filter config only at route level and not at global level",
    "created_at": "2018-10-02T17:52:20Z",
    "closed_at": "2018-11-08T19:20:54Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4586",
    "body": "Hi,\r\n\r\nI could see buffer and rbac filters where we define at global level and are overridable/controllable at per route level. Are there any filters which we don't define at global level but available only at route level. In short, the filter exists/applies only for that route.\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4586/comments",
    "author": "rbkumar88",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-11-01T18:22:43Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-11-08T19:20:53Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4580,
    "title": "Is it possible to do an http health check, for each member of the cluster ?",
    "created_at": "2018-10-01T21:06:03Z",
    "closed_at": "2018-11-08T00:28:25Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4580",
    "body": "*Description*:\r\n\r\nIn my scenario I have 200 clusters with 5 servers each (with xds).\r\n\r\nIs there a feature that makes it possible to create an \"http health check\" for each server (member of a cluster), pointing to a check path (example: \"/ healthcheck\")?\r\n\r\nSuggestion:\r\n  If \"endpoint.Endpoint.HealthCheckConfig\" has the path of the \"healthcheck\" route I believe it would suffice.\r\n\r\nThanks for the help.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4580/comments",
    "author": "fernandodiassp",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-10-31T23:30:29Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-11-08T00:28:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4567,
    "title": "upstream connect error or disconnect/reset before headers",
    "created_at": "2018-09-30T04:35:09Z",
    "closed_at": "2018-11-06T17:03:12Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4567",
    "body": "*Title*: *upstream connect error or disconnect/reset before headers*\r\n\r\n*Description*:\r\nI downloaded the Docker image for Envoy and simply tried to run:\r\n`docker container run --rm -d -p 10000:10000 --name envoy envoyproxy/envoy`\r\nI tried to visit localhost:10000 on my browser to check redirection but threw an error saying:\r\n**upstream connect error or disconnect/reset before headers**\r\nAlso tried with `curl` and with Docker `--network=host` flags. But didn't work. Any idea?\r\n\r\nThanks. :)",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4567/comments",
    "author": "kjanshair",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-10-30T16:29:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-11-06T17:03:11Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4542,
    "title": "RBAC Filter Envoy Proxy Usage",
    "created_at": "2018-09-26T20:55:08Z",
    "closed_at": "2018-11-06T08:03:09Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4542",
    "body": "**Help Wanted**\r\n\r\n*Title*: *I want some sample example of RBAC network filter usage with complete envoy yml configuration*\r\n\r\n*Description*:\r\n>I want to implement something like this in a RBAC filter\r\n\r\nBelow is just a sudo config of what I want:\r\n```\r\nnetwork_filters:\r\n     - name: envoy.rbac\r\n       config:  \r\n             rules:\r\n                 action: ALLOW\r\n                 policies:\r\n                     \"service-access\":\r\n                         principals:\r\n                            source_ip: 192.168.135.211/32\r\n                         permissions:\r\n                            - destination_ip: 0.0.0.0/32\r\n                            - destination_port: 443\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4542/comments",
    "author": "gupash",
    "comments": [
      {
        "user": "mimiasd",
        "created_at": "2018-09-30T07:45:29Z",
        "body": "```\r\nstatic_resources:\r\n  listeners:\r\n  - name: \"ingress listener\"\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 9001\r\n    filter_chains:\r\n      filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"  \r\n                route:\r\n                  cluster: local_service\r\n                per_filter_config:\r\n                  envoy.filters.http.rbac:\r\n                    rbac:\r\n                      rules:\r\n                        action: ALLOW\r\n                        policies:\r\n                          \"per-route-rule\":\r\n                            permissions:\r\n                            - any: true\r\n                            principals:\r\n                            - any: true\r\n          http_filters:\r\n          - name: envoy.filters.http.rbac \r\n            config: \r\n              rules: \r\n                action: ALLOW\r\n                policies:\r\n                  \"general-rules\":\r\n                    permissions:\r\n                    - any: true\r\n                    principals:\r\n                    - any: true\r\n          - name: envoy.router\r\n            config: {}\r\n          access_log:\r\n            name: envoy.file_access_log\r\n            config: {path: /dev/stdout}\r\n\r\n  clusters:\r\n  - name: local_service\r\n    connect_timeout: 0.250s\r\n    type: static\r\n    lb_policy: round_robin\r\n    http2_protocol_options: {}\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 9000\r\n\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8080\r\n\r\n```\r\n@gupash can do like this.  RBAC is a great feature. @mattklein123 "
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-10-30T07:55:13Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-11-06T08:03:08Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4523,
    "title": "got build error when build envoy 1.7.1 with envoyproxy/envoy-build-centos:latest",
    "created_at": "2018-09-25T06:36:21Z",
    "closed_at": "2018-11-29T08:25:55Z",
    "labels": [
      "question",
      "area/build",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4523",
    "body": "**Issue Template**\r\n\r\n*Title*: *I got build error when build envoy 1.7.1 with envoyproxy/envoy-build-centos:LATEST *\r\n\r\n*Description*:\r\n>I want to build envoy v1.7.1 of centos. If i choose envoyproxy/envoy-build-centos:latest, i got below error\r\nERROR: missing input file '@envoy//ci/prebuilt:thirdparty_build/lib/libevent_pthreads.a'\r\nERROR: /source/source/exe/BUILD:21:1: //source/exe:envoy-static: missing input file '@envoy//ci/prebuilt:thirdparty_build/lib/libevent_pthreads.a'\r\nTarget //source/exe:envoy-static failed to build\r\nERROR: /source/source/exe/BUILD:21:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 33.270s, Critical Path: 1.39s\r\nINFO: 2 processes: 2 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\nscript returned exit code 1\r\n\r\nGuess i may use the build image which matches envoy 1.7.1 code\r\nHow can i find out centos build image tag to build envoy v1.7.1?\r\n\r\n*Repro steps*:\r\nCheck out envoy code with tag v1.7.1\r\n IMAGE_NAME=envoyproxy/envoy-build-centos IMAGE_ID=latest ./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.release.server_only' ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4523/comments",
    "author": "huxiaobabaer",
    "comments": [
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-23T07:46:33Z",
        "body": "Any update on this? TKS"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-11-22T08:09:36Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-11-29T08:25:54Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4428,
    "title": "Lua HTTP Filter Cookies missing from Headers",
    "created_at": "2018-09-14T21:52:39Z",
    "closed_at": "2018-09-17T20:06:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4428",
    "body": "**Issue Template**\r\n\r\n*Title*: *Lua HTTP Filter Cookies missing from Headers*\r\n\r\n*Description*:\r\n\r\nTrying to extract cookies from the incomming headers however they aren't in ```request_handle:headers()```.\r\n\r\nI know that they are a part of the request because downstream apps can access the cookies.\r\n\r\nWe're trying to forward the cookie headers to a downstream auth service via the following code:\r\n\r\n```\r\n    local edge_headers = {\r\n        [\":method\"] = \"POST\",\r\n        [\":path\"] = \"/edge\",\r\n        [\":authority\"] = http_host,\r\n    }\r\n\r\n    for header, value in pairs(request_handle:headers()) do\r\n        local normalized_header = header:match(\"^[:]*(.+)\")\r\n        edge_headers[\"Edge-\" .. normalized_header] = value\r\n    end\r\n\r\n    local response_headers = request_handle:httpCall(\r\n        cluster,\r\n        edge_headers,\r\n        \"\",                             -- empty body\r\n        {{ edge_processors_timeout_ms }}\r\n    )\r\n```\r\n\r\nIs there some special trick with cookie access in the proxy code?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4428/comments",
    "author": "oahayder",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2018-09-17T18:34:34Z",
        "body": "@oahayder I'm a bit confused about what is upstream and what is downstream in your example. Normally, in Envoy, the client is the downstream and the backends are the upstreams. Can you clarify? There should be no need for any special treatment to access cookies, they should appear to filters as normal headers."
      },
      {
        "user": "oahayder",
        "created_at": "2018-09-17T20:06:36Z",
        "body": "Apologies this was user error. Thanks for looking @htuch "
      }
    ]
  },
  {
    "number": 4427,
    "title": "Spurious connection failures",
    "created_at": "2018-09-14T20:00:02Z",
    "closed_at": "2018-10-25T23:14:42Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4427",
    "body": "*Title*: *Spurious Connection Failures*\r\n\r\n*Description*:\r\nAs we're rolling our microservices onto Envoy we've gotten some reports of spurious 503s. These seem to be due to connection failures. Access logs show the UF flag which means there was a connection failure.\r\n\r\nDebug logging shows a lot of TLS handshake errors:\r\n\r\n```\r\n2018-09-14_19:22:08.51521 [2018-09-14 19:22:08.515][355175][debug][connection] external/envoy/source/common/network/connection_impl.cc:570] [C140] connecting to 10.1.5.128:443\r\n2018-09-14_19:22:08.51530 [2018-09-14 19:22:08.515][355175][debug][connection] external/envoy/source/common/network/connection_impl.cc:579] [C140] connection in progress\r\n2018-09-14_19:22:08.51540 [2018-09-14 19:22:08.515][355175][debug][connection] external/envoy/source/common/network/connection_impl.cc:466] [C140] connected\r\n2018-09-14_19:22:08.51549 [2018-09-14 19:22:08.515][355175][debug][connection] external/envoy/source/common/ssl/ssl_socket.cc:111] [C140] handshake error: 2\r\n2018-09-14_19:22:08.54690 [2018-09-14 19:22:08.546][355175][debug][connection] external/envoy/source/common/ssl/ssl_socket.cc:111] [C140] handshake error: 2\r\n2018-09-14_19:22:08.54696 [2018-09-14 19:22:08.546][355175][debug][connection] external/envoy/source/common/ssl/ssl_socket.cc:111] [C140] handshake error: 2\r\n2018-09-14_19:22:09.41950 [2018-09-14 19:22:09.419][355175][debug][connection] external/envoy/source/common/ssl/ssl_socket.cc:100] [C140] handshake complete\r\n```\r\n\r\nWe're unable to reproduce the error with the `nc` or with `openssl s_client` straight to the backend, so it's seeming like the issue is within Envoy.\r\n\r\nEnvoy version: c92a3017017dfa31241dec13dc6a1479090318d0\r\n\r\nWe're not sure what else to look at at the moment, let me know if there is more pertinent information I can gather.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4427/comments",
    "author": "mpuncel",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2018-09-17T18:35:27Z",
        "body": "Do you have `/stats`?"
      },
      {
        "user": "lizan",
        "created_at": "2018-09-18T21:13:21Z",
        "body": "`handshake error: 2` is not real error, it is `SSL_ERROR_WANT_READ` returned from BoringSSL means waiting more data. /stats or log from other places might help."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-10-18T22:37:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-10-25T23:14:41Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "senthilkrajagopal",
        "created_at": "2019-01-30T01:45:20Z",
        "body": "Helped wanted. Having this issue."
      }
    ]
  },
  {
    "number": 4414,
    "title": "\"No healthy upstream\" seen sometimes for static localhost cluster.",
    "created_at": "2018-09-13T06:29:57Z",
    "closed_at": "2018-10-20T21:08:22Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4414",
    "body": "*Description*:\r\nWe have some \"No healthy upstream\" errors happening in production. They are rare and we can't reproduce them or figure out why they are happening.\r\nThe most concerning case is the one where the remote cluster is a \"STATIC\" type cluster with 127.0.0.1 host.\r\n\r\nHere's the cluster definition we see UH flag with\r\n\r\n```\r\n      - circuit_breakers:\r\n          thresholds:\r\n          - max_connections: 8192\r\n            max_pending_requests: 300\r\n        common_lb_config:\r\n          healthy_panic_threshold:\r\n            value: 0.25\r\n        connect_timeout: 0.1s\r\n        dns_lookup_family: V4_ONLY\r\n        health_checks:\r\n        - healthy_threshold: 2\r\n          http_health_check:\r\n            path: /status\r\n          interval: 5s\r\n          no_traffic_interval: 5s\r\n          timeout: 5s\r\n          unhealthy_threshold: 3\r\n        hosts:\r\n        - socket_address:\r\n            address: 127.0.0.1\r\n            port_value: 8080\r\n        lb_policy: LEAST_REQUEST\r\n        name: local-appserver\r\n        type: STATIC\r\n```\r\n\r\nOur expectation was that for at least static clusters we should never see UH flag, and that panic mode will route the request whatever happens.\r\n\r\nThis also happens for remote clusters which use STRICT_DNS discovery. For those I would understand if temporary DNS blips would cause this behavior somehow, although I suspect it's the same reason we see it for localhost cluster.\r\n\r\nWhat can be causing this?\r\n\r\nVersion of envoy:\r\nnot sure. We are using the dockerhub envoy docker image \r\nenvoyproxy/envoy-alpine@sha256:fee7f5cd71f82e01957921a27c8fbb370e4e0bbb5646d88603dd7ab62d630f5a\r\nversion: 2d744738c8ba57c189ac5444b3e48f45c442c8ee/1.7.0-dev/Clean/RELEASE",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4414/comments",
    "author": "ikatson",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-10-13T20:40:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-10-20T21:08:21Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4380,
    "title": "is there any way use different sampling strategy for different listener ?",
    "created_at": "2018-09-10T05:53:29Z",
    "closed_at": "2018-10-28T06:36:45Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4380",
    "body": "I use native jaeger tracing, I want different listener use different sampling strategy. \r\nfor example:\r\n listener1 ratelimit 100\r\n listener2 ratelimit 10\r\n listener3 no tracing\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4380/comments",
    "author": "xmm1989218",
    "comments": [
      {
        "user": "zyfjeff",
        "created_at": "2018-09-10T06:24:36Z",
        "body": "tracing by default setting is global,  you can run multiple envoy instance, each envoy responsible for a listener has a own tracing configuration."
      },
      {
        "user": "xmm1989218",
        "created_at": "2018-09-10T14:17:13Z",
        "body": "@zyfjeff run multiple envoy instance for every listener is not friendly to resource utilization and management. now in my enverioment, each physical machine has one envoy for all listener. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-10-21T06:24:06Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-10-28T06:36:44Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4358,
    "title": "envoy proxy grpc server over domain not work",
    "created_at": "2018-09-06T03:06:19Z",
    "closed_at": "2018-09-07T00:51:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4358",
    "body": "*Title*: *envoy proxy grpc server over domain not work*\r\n\r\n*Description*:\r\nI use envoy to proxy multi backend grpc services, so i want to use different domain to distinguish different backend grpc service. when change domain from \"*\" to \"grpc.service.com\", It seems that envoy is not working properly, my grpc client get error like this:\r\n```shell\r\nException in thread \"main\" io.grpc.StatusRuntimeException: UNIMPLEMENTED\r\n        at io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:230)\r\n        at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:211)\r\n        at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:144)\r\n        at com.connect.generate.TimedTaskExecutorGrpc$TimedTaskExecutorBlockingStub.execTimedTask(TimedTaskExecutorGrpc.java:150)\r\n        at com.connect.TimeClient.createTimeTask(TimeClient.java:81)\r\n        at com.connect.TimeClient.main(TimeClient.java:132)\r\n```\r\n\r\nhere is my envoy config file:\r\n```shell static_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 10000\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains:\r\n              - \"grpc.service.com\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                  headers:\r\n                  - name: content-type\r\n                    exact_match: application/grpc\r\n                route:\r\n                  cluster: local_service_grpc\r\n          http_filters:\r\n          - name: envoy.router\r\n            config: {}\r\n  clusters:\r\n  - name: local_service_grpc\r\n    connect_timeout: 25s\r\n    type: static\r\n    lb_policy: round_robin\r\n    http2_protocol_options: {}\r\n    hosts:\r\n    - socket_address:\r\n        address: 192.168.201.99\r\n        port_value: 50052\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9901 ```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4358/comments",
    "author": "inetkiller",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2018-09-06T15:13:57Z",
        "body": "Try use domains with port number, i.e. `grpc.service.com:10000`, gRPC Java is always adding port number to `:authority` header. Or you can use `overrideAuthority` when building channel in your gRPC client."
      },
      {
        "user": "inetkiller",
        "created_at": "2018-09-07T00:51:19Z",
        "body": "@lizan yes, it works. It's releated #886.  thx"
      }
    ]
  },
  {
    "number": 4162,
    "title": "Initialization failure when TLS context is configured",
    "created_at": "2018-08-15T14:47:45Z",
    "closed_at": "2018-08-23T21:50:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4162",
    "body": "*Title*: *Initialization failure when TLS context is configured*\r\n\r\n*Description*:\r\n\r\nWhen I add the TLS context as part of the configuration envoy fails to load the private key \r\nfrom the specified path. Envoy should load the private key from the path provided in the \r\nconfiguration. \r\n\r\n*Repro steps*:\r\n\r\nI'm running envoy as a docker image as follows:\r\n\r\n```\r\nFROM envoyproxy/envoy:latest\r\n\r\nCOPY cert.pem /etc/envoy/server-cert.pem\r\nCOPY cert.pem /etc/envoy/server-key.pem\r\n\r\nCOPY service-envoy.yaml /etc/envoy/service-envoy.yaml\r\nCMD /usr/local/bin/envoy -c /etc/envoy/service-envoy.yaml\r\n```\r\n\r\n*Config*:\r\n```\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                redirect:\r\n                  path_redirect: \"/\"\r\n                  https_redirect: true\r\n          http_filters:\r\n          - name: envoy.router\r\n            config: {}\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 443\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          codec_type: auto\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains:\r\n              - \"*\"\r\n              # ...\r\n      tls_context:\r\n        common_tls_context:\r\n          tls_certificates:\r\n            - certificate_chain: { filename: \"/etc/envoy/server-cert.pem\" }\r\n              private_key: { filename: \"/etc/envoy/server-key.pem\" }\r\n  clusters:\r\n  # ...\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8081\r\n```\r\n\r\n*Logs*:\r\n```\r\n\r\n[2018-08-15 14:40:58.814][9][info][main] source/server/server.cc:183] initializing epoch 0 (hot restart version=10.200.16384.127.options=capacity=16384, num_slots=8209 hash=228984379728933363 size=2654312)\r\n[2018-08-15 14:40:58.814][9][info][main] source/server/server.cc:185] statically linked extensions:\r\n[2018-08-15 14:40:58.814][9][info][main] source/server/server.cc:187]   access_loggers: envoy.file_access_log,envoy.http_grpc_access_log\r\n[2018-08-15 14:40:58.814][9][info][main] source/server/server.cc:190]   filters.http: envoy.buffer,envoy.cors,envoy.ext_authz,envoy.fault,envoy.filters.http.header_to_metadata,envoy.filters.http.jwt_authn,envoy.filters.http.rbac,envoy.grpc_http1_bridge,envoy.grpc_json_transcoder,envoy.grpc_web,envoy.gzip,envoy.health_check,envoy.http_dynamo_filter,envoy.ip_tagging,envoy.lua,envoy.rate_limit,envoy.router,envoy.squash\r\n[2018-08-15 14:40:58.814][9][info][main] source/server/server.cc:193]   filters.listener: envoy.listener.original_dst,envoy.listener.proxy_protocol,envoy.listener.tls_inspector\r\n[2018-08-15 14:40:58.814][9][info][main] source/server/server.cc:196]   filters.network: envoy.client_ssl_auth,envoy.echo,envoy.ext_authz,envoy.filters.network.thrift_proxy,envoy.http_connection_manager,envoy.mongo_proxy,envoy.ratelimit,envoy.redis_proxy,envoy.tcp_proxy\r\n[2018-08-15 14:40:58.814][9][info][main] source/server/server.cc:198]   stat_sinks: envoy.dog_statsd,envoy.metrics_service,envoy.stat_sinks.hystrix,envoy.statsd\r\n[2018-08-15 14:40:58.814][9][info][main] source/server/server.cc:200]   tracers: envoy.dynamic.ot,envoy.lightstep,envoy.zipkin\r\n[2018-08-15 14:40:58.814][9][info][main] source/server/server.cc:203]   transport_sockets.downstream: envoy.transport_sockets.capture,raw_buffer,tls\r\n[2018-08-15 14:40:58.814][9][info][main] source/server/server.cc:206]   transport_sockets.upstream: envoy.transport_sockets.capture,raw_buffer,tls\r\n[2018-08-15 14:40:58.820][9][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)\r\n[2018-08-15 14:40:58.822][9][info][upstream] source/common/upstream/cluster_manager_impl.cc:134] cm init: all clusters initialized\r\n[2018-08-15 14:40:58.822][9][info][config] source/server/configuration_impl.cc:60] loading 2 listener(s)\r\n[2018-08-15 14:40:58.892][9][critical][main] source/server/server.cc:78] error initializing configuration '/etc/envoy/service-envoy.yaml': Failed to load private key from /etc/envoy/server-key.pem\r\n[2018-08-15 14:40:58.892][9][info][main] source/server/server.cc:465] exiting\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4162/comments",
    "author": "ovigio",
    "comments": [
      {
        "user": "PiotrSikora",
        "created_at": "2018-08-15T19:49:58Z",
        "body": "What are the file system permissions for that file (and the whole path, for that matter)?"
      },
      {
        "user": "ovigio",
        "created_at": "2018-08-15T20:11:02Z",
        "body": "@PiotrSikora \r\nThe permissions on the file are: -rwx------  server-key.pem\r\nThe whole path to the file is at: /etc/envoy/server-key.pem"
      },
      {
        "user": "PiotrSikora",
        "created_at": "2018-08-22T22:15:11Z",
        "body": "Sorry for the delay, I've missed you reply.\r\n\r\nCould you paste output of:\r\n```\r\n$ ls -ld /etc/ /etc/envoy/\r\n```\r\n```\r\n$ ls -l /etc/envoy/server-key.*\r\n```\r\n\r\nAlso, what user is the Envoy running as? Who's starting the process?"
      },
      {
        "user": "ovigio",
        "created_at": "2018-08-23T16:07:17Z",
        "body": "@PiotrSikora \r\nHere is output the from running those commands:\r\n\r\n$ ls -l /etc/envoy/server-key.*\r\n`drwxr-xr-x    1 root     root          4096 Aug 23 14:41 /etc/\r\ndrwxr-xr-x    1 root     root          4096 Aug 21 22:54 /etc/envoy/`\r\n\r\n$ls -l /etc/envoy/server-key.*\r\n`-rw-rw-r-- 1 root root 1204 Aug 23 14:30 /etc/envoy/server-key.pem`\r\n\r\nThe envoy process is running as root.\r\n\r\nI was able to fix the issue by generating certificate and key pem files separately. Before I was generating a crt and key file and concatenating them to a pem file. That was the reason it wasn't able to load the private key."
      },
      {
        "user": "PiotrSikora",
        "created_at": "2018-08-23T20:22:05Z",
        "body": "Ah, well, that's not supposed to work.\r\n\r\nI think this can be closed now."
      }
    ]
  },
  {
    "number": 4139,
    "title": "Questions around Lua filter",
    "created_at": "2018-08-14T01:14:40Z",
    "closed_at": "2018-08-16T15:22:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4139",
    "body": "Does the lua filter load required modules on every request ?\r\n\r\nI am trying to write a lua filter that writes the content of incoming requests into kafka or aws-kinesis. I've never used lua before but it looks very simple compared to trying to build a custom envoy filter, but i am not sure if required modules are cached somehow ?\r\n\r\nI only need to have a lua filter sending my requests to an upstream destination in code, can i configure envoy to not route traffic to any destination ?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4139/comments",
    "author": "sirajmansour",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-08-14T03:01:18Z",
        "body": "Code is loaded/JITd once per worker thread and then reused for all requests."
      },
      {
        "user": "sirajmansour",
        "created_at": "2018-08-14T04:03:21Z",
        "body": "Thanks for your prompt response."
      }
    ]
  },
  {
    "number": 3942,
    "title": "support Zeroc ice",
    "created_at": "2018-07-24T07:01:10Z",
    "closed_at": "2018-08-30T17:50:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3942",
    "body": "I use zeroc ice as the rpc amount backend servers.\r\n\r\nDoes envoy support zeroc ice?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3942/comments",
    "author": "gtexbackend",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-07-24T15:07:17Z",
        "body": "No, not currently."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-23T16:34:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-30T17:50:17Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 3928,
    "title": "core.ConfigSource file Path is not working in kubernetes (symlink)",
    "created_at": "2018-07-23T15:45:14Z",
    "closed_at": "2018-08-29T17:34:15Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3928",
    "body": "I trying to source CDS, LDS, RDS from filesystem.\r\n\r\nI have CDS,LDS,RDS configuration in Kubernetes by mounting `ConfigMap` as `volume`. So when ever I update the k8s `ConfigMap` of xDS its getting updated in the volume mount perfectly but **envoy watch is not able to detect the file change and reload it**. I'm guessing symlink & inotify isn't going well together.\r\n\r\nAny workaround suggestions ?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3928/comments",
    "author": "tak2siva",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-08-22T16:43:00Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-29T17:34:14Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 3899,
    "title": "[question] Does the request timeout includes connection time or not ?",
    "created_at": "2018-07-19T08:44:18Z",
    "closed_at": "2018-08-25T16:19:24Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3899",
    "body": "*Description*:\r\nDoes the `request_timeout` in envoy includes `connection_time` or not ? Lets say I have `request_timeout` as 5s my upstream cluster connection takes more than 5s. Will I get req timeout or not ? (say I have 20s timeout for clusters so I wont get cluster_connection_timeout)\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3899/comments",
    "author": "tak2siva",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-07-19T14:35:20Z",
        "body": "Yes, if a new upstream connection has to be made connection time is counted against the request timeout."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-18T15:28:04Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-25T16:19:23Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 3886,
    "title": "How to verify if LEAST_REQUEST LB policy is working?",
    "created_at": "2018-07-18T16:33:06Z",
    "closed_at": "2018-07-23T09:32:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3886",
    "body": "This is a question, and not a bug or feature request. If this is not a proper place to ask questions, I can move this somwhere else (I checked SO and Serverfault, but didn't see much traction about Envoy there).\r\n\r\nI'm using Envoy as a Load balancer in front of a service consisting of ~40 nodes. I have 2 nodes running Envoy. I'm using `STRICT_DNS` discovery type, and the proxying works fine without any issues.\r\n\r\nThe upstream service is doing some CPU-intensive processing in the implementation of its endpoint, where the requests are relatively long running, and their duration is pretty varied (anywhere between 300ms-5s).  \r\nBefore using Envoy we were just using a simple round robin load balancer approach, and what we experienced was that the distribution of the CPU-load was very uneven among the nodes, and at any time some nodes were in parallel processing much more requests than some others. I contributed this simply to the randomness of round robin, so this looked like a good candidate to utilize a least connection algorithm instead.\r\n\r\nSo I've put in place Envoy to be able to use the Least Request LB algorithm.  \r\nI started with the LB policy set to `ROUND_ROBIN`, and I wanted to compare it to `LEAST_REQUEST`, so after a while I changed it to `LEAST_REQUEST`, and kept monitoring the upstream cluster.\r\n\r\nI was expecting to get better overall resource utilization, and a more flat distribution of outstanding requests across the upstream nodes.  \r\nOn the other hand, what happens is that I can see absolutely no difference in response times, CPU-usage, or the variance in the distribution of in progress requests among the upstream nodes.  \r\nI can accept if it turns out my assumptions were wrong, but it's hard to believe that switching from Round Robin to Least Requests on the LB level doesn't make *any* observable difference.\r\n\r\nIs there a way for me to \"verify\" if Envoy is really operating in Least Request LB mode? Is there any situation in which Envoy doesn't really do Least Request, but it falls back to Round Robin?\r\n\r\nThis is the configuration I'm using:\r\n\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 80 }\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress_http\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match: { prefix: \"/\" }\r\n                route: { host_rewrite: my-upstream-domain-name, cluster: service_my_upstream }\r\n          http_filters:\r\n          - name: envoy.router\r\n  clusters:\r\n  - name: service_my_upstream\r\n    connect_timeout: 0.25s\r\n    type: STRICT_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: LEAST_REQUEST\r\n    hosts: [{ socket_address: { address: my-upstream-domain-name, port_value: 80 }}]\r\n```\r\n\r\n(Where `my-upstream-domain-name` is the domain name having the A record for all the upstream nodes.)",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3886/comments",
    "author": "markvincze",
    "comments": [
      {
        "user": "markvincze",
        "created_at": "2018-07-19T14:10:31Z",
        "body": "One thing I tried now: I replaced the LB policy with an incorrect value (I just added `lb_policy: invalid_policy`), but I didn't receive any errors either on startup or later when sending requests to Envoy, even if I set the logging level to `debug`, so I'm doubting that the `lb_policy` setting is picked up by Envoy. So maybe something is wrong in my config?"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-07-19T14:31:12Z",
        "body": "@markvincze yes something seems wrong. Use the `/config_dump` admin endpoint to double check the applied configuration.\r\n\r\nAlso, if you look at per host stats in `/clusters` output it should be pretty clear if it's RR vs. LR, because LR will look a lot more random for `rq_total` vs. the roughly even increasing of RR."
      },
      {
        "user": "markvincze",
        "created_at": "2018-07-23T09:32:36Z",
        "body": "@mattklein123 Thanks for the suggestions!  \r\nWith `/config_dump` I could verify that the `LEAST_REQUEST` setting was indeed picked up. And also, when testing it with a dummy CPU-intensive API I created just for testing, I could also verify that the overall performance was improved significantly with `LEAST_REQUEST` compared to `ROUND_ROBIN`. So it seems that it's just that with my real production application, `LEAST_REQUEST` really doesn't bring a noticable improvement."
      }
    ]
  },
  {
    "number": 3860,
    "title": "rolling update with envoy",
    "created_at": "2018-07-15T07:10:58Z",
    "closed_at": "2018-08-24T03:05:24Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3860",
    "body": "*Title*: *What is the best way to do a rolling update with envoy?*\r\n\r\n*Description*:\r\nWe have envoy setup as a side car container to each pod in our services. Now whenever there is a config change for envoy (configmap change) and we do a helm update.\r\nWe see a spike in 503s and then eventually it goes away. \r\nBefore envoy refreshes it set of hosts(dns_refresh_rate 5s), does it still send requests to the pods being terminated or already terminated? If so, how to cleanly do rolling update when there is a config change?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3860/comments",
    "author": "dnivra26",
    "comments": [
      {
        "user": "rshriram",
        "created_at": "2018-07-18T02:04:58Z",
        "body": "You could play around with drain times but YMMV based on the types of clients (long running vs short lived connections). Alternatively, look at using the XDS APIs to update state programmatically."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-17T02:31:26Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-24T03:05:24Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 3772,
    "title": "Zipkin Custom Headers",
    "created_at": "2018-07-02T14:30:49Z",
    "closed_at": "2018-08-09T00:46:58Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3772",
    "body": "I have Envoy configured to send traces via the builtin Zipkin tracer, but my Zipkin instance is in front of a (non-envoy) reverse-proxy that requires an authentication token to provide some measure of protection to Zipkin.\r\n\r\nI have created the following config based on the basic google.com proxy example that opens up a local listener (pointed to by cluster `zipkin_inbound`) that zipkin is configured to send to in order to get the header added, before forwarding it to the `zipkin_outbound` cluster where it goes to the real zipkin server.\r\n\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 9901 }\r\n\r\nstatic_resources:\r\n  listeners:\r\n.....\r\n  - name: trace_0\r\n    address:\r\n      socket_address: { address: 127.0.0.1, port_value: 19876 }\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: zipkin_http\r\n          route_config:\r\n            name: local_route\r\n            request_headers_to_add:\r\n              - header: {key: MY-AUTH-HEADER, value: MY-AUTH-TOKEN}\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"zipkin_inbound\"]\r\n              routes:\r\n              - match: { path: \"/api/v1/spans\" }\r\n                route: { host_rewrite: 'my.real.zipkin.server', cluster: zipkin_outbound }\r\n          http_filters:\r\n          - name: envoy.router\r\n\r\n  clusters:\r\n.....\r\n  - name: zipkin_inbound\r\n    connect_timeout: 1s\r\n    type: static\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 19876\r\n\r\n  - name: zipkin_outbound\r\n    connect_timeout: 2s\r\n    type: LOGICAL_DNS\r\n    lb_policy: ROUND_ROBIN\r\n    hosts:\r\n    - socket_address:\r\n        address: my.real.zipkin.server\r\n        port_value: 9411\r\ntracing:\r\n  http:\r\n    name: envoy.zipkin\r\n    config:\r\n      collector_cluster: zipkin_inbound\r\n      collector_endpoint: \"/api/v1/spans\"\r\n```\r\n\r\nThis works but I was wondering if there was a less roundabout way of doing this?  I am willing to contribute config options to add arbitrary headers under the `tracing.http.[name=envoy.zipkin]` section if that is the better way to go about this.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3772/comments",
    "author": "keitwb",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-07-03T00:15:27Z",
        "body": "This is probably the only way right now. We could potentially add a header addition config option to the zipkin tracer."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-02T00:29:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-09T00:46:57Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 3768,
    "title": "[ISSUE] use cluster healthcheck makes upstream delay",
    "created_at": "2018-06-30T13:43:13Z",
    "closed_at": "2018-08-08T04:05:03Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3768",
    "body": "Description:\r\n\r\nversion: 1.7\r\nwhen cluster healthcheck enabled, upstream servers will delay to response.\r\n\r\nConfig:\r\nhealth_checks:\r\n- timeout:\r\nseconds: 10\r\ninterval:\r\nseconds: 5\r\nunhealthy_threshold: 3\r\nhealthy_threshold: 3\r\ntcp_health_check: {}",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3768/comments",
    "author": "liyehaha",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-07-02T03:54:45Z",
        "body": "@liyehaha sorry I can't parse this question/problem. Please expand?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-01T03:56:13Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-08T04:05:02Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 3733,
    "title": "verbose logs when management server is not reachable ",
    "created_at": "2018-06-26T05:45:36Z",
    "closed_at": "2018-06-27T21:00:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3733",
    "body": "When Envoy is disconnected from management server, we get these logs (mainly coming from statically defined EDS clusters or pending CLA requests)\r\n`[warning][config] bazel-out/k8-opt/bin/source/common/config/_virtual_includes/grpc_mux_subscription_lib/common/config/grpc_mux_subscription_impl.h:67] gRPC update for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment failed `\r\n\r\nSince this is just a failed update (not reject), should we reduce this to debug? Any way, we do have stats like `update_failure` to detect this .\r\n\r\nOn a side note, should this reconnection to management server implement some kind of back-off strategy rather than a fixed aggressive interval?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3733/comments",
    "author": "ramaraochavali",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-06-26T13:21:44Z",
        "body": "@htuch thoughts here?"
      },
      {
        "user": "htuch",
        "created_at": "2018-06-26T21:15:15Z",
        "body": "I think these both sound like totally valid suggestions. PRs welcome :) I think the back-off strategy should be implemented in some generic way so we can do back-off in other places as well where we have retries."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2018-06-27T04:04:12Z",
        "body": "@htuch Thanks. Let us split this in to two. I will push a PR for log level change shortly as it is simpler :-). \r\nWill create a separate issue for back-off strategy which I will work in my free time in coming weeks."
      }
    ]
  },
  {
    "number": 3706,
    "title": "helloworld of grpc example can't run at grpc-bridge of envoy source",
    "created_at": "2018-06-23T15:29:23Z",
    "closed_at": "2018-07-30T16:26:16Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3706",
    "body": "environment buiding:\r\n1. download envoy source\r\n     cd examples/grpc-bridge\r\n     script/bootstrap\r\n     script/build\r\n     docker-compose up --build\r\nit can build two containers    grpc-bridge_python_1   and    grpc-bridge_grpc_1\r\nit also can run the example, just  set foo bar, get foo.  but I want to run grpc example, so\r\n2. docker exec -ti grpc-bridge_python_1/grpc bash，download the grpc source，and make it\r\n3. modify:\r\ngrpc/examples/cpp/helloworld/greeter_client.cc\r\n--------------------------------------------------------------------\r\n    // The actual RPC.\r\n+    context.set_authority(\"grpc\");\r\n    Status status = stub_->SayHello(&context, request, &reply);\r\n\r\n    // Act upon its status.\r\n    if (status.ok()) {\r\n      return reply.message();\r\n    } else {\r\n      std::cout << status.error_code() << \": \" << status.error_message()\r\n---------------------------------------------------------------------\r\n  // localhost at port 50051). We indicate that the channel isn't authenticated \r\n  // (use of InsecureChannelCredentials()).                                        \r\n  GreeterClient greeter(grpc::CreateChannel(                                       \r\n-      \"localhost:50051\", grpc::InsecureChannelCredentials()));                     \r\n+      \"localhost:9001\", grpc::InsecureChannelCredentials()));\r\n  std::string user(\"world\");                                                       \r\n  std::string reply = greeter.SayHello(user);\r\n-----------------------------------------------------------------------\r\nat container grpc-bridge_grpc_1 /etc/s2s-grpc-envoy.yaml\r\n-----------------------------------------------------------------------\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n-        port_value: 8081\r\n+        port_value: 50051\r\nadmin:\r\n  access_log_path: \"/var/log/envoy/admin_access.log\"\r\n  address:\r\n-----------------------------------------------------------------------\r\n4. copy greet_client to container grpc-bridge_python_1 and copy greet_server to container grpc-bridge_grpc_1\r\nafter modify the yaml, Just restart container\r\nnow I run greet_client return message:\r\nGreeter received: RPC failed, cost time is:3 ms.\r\nI just use tcpdump capture packet. then I am surely that server can recv the request of client, and answer it, but the client can't analyze the answer. I think maybe the envoy modify the header of packet, but I don't know where and how to fix it.\r\n5. copy the log here,\r\n------------ server-------------------------\r\n[2018-06-23 11:40:56.614][493][debug][main] source/server/connection_handler_impl.cc:217] [C0] new connection\r\n[2018-06-23 11:40:56.614][493][trace][connection] source/common/network/connection_impl.cc:389] [C0] socket event: 3\r\n[2018-06-23 11:40:56.614][493][trace][connection] source/common/network/connection_impl.cc:457] [C0] write ready\r\n[2018-06-23 11:40:56.614][493][trace][connection] source/common/network/connection_impl.cc:427] [C0] read ready\r\n[2018-06-23 11:40:56.614][493][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C0] read returns: 505\r\n[2018-06-23 11:40:56.614][493][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C0] read returns: -1\r\n[2018-06-23 11:40:56.615][493][trace][connection] source/common/network/raw_buffer_socket.cc:29] [C0] read error: 11\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:305] [C0] parsing 505 bytes\r\n[2018-06-23 11:40:56.615][493][debug][http] source/common/http/conn_manager_impl.cc:187] [C0] new stream\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=host value=grpc\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=te value=trailers\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=content-type value=application/grpc\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=user-agent value=grpc-c++/1.12.1 grpc-c/6.0.0 (linux; chttp2; glorious)\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=grpc-accept-encoding value=identity,deflate,gzip\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=accept-encoding value=identity,gzip\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=x-forwarded-for value=125.39.207.17\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=x-forwarded-proto value=http\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=x-envoy-internal value=true\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=x-envoy-downstream-service-cluster value=\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=x-request-id value=cea162bc-6bf4-4b76-ac4c-4f6d35ea52d2\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=x-envoy-expected-rq-timeout-ms value=15000\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:361] [C0] headers complete\r\n[2018-06-23 11:40:56.615][493][trace][http] source/common/http/http1/codec_impl.cc:292] [C0] completed header: key=transfer-encoding value=chunked\r\n[2018-06-23 11:40:56.615][493][debug][http] source/common/http/conn_manager_impl.cc:454] [C0][S5133098934978251612] request headers complete (end_stream=false):\r\n':authority', 'grpc'\r\n':path', '/helloworld.Greeter/SayHello'\r\n':method', 'POST'\r\n'te', 'trailers'\r\n'content-type', 'application/grpc'\r\n'user-agent', 'grpc-c++/1.12.1 grpc-c/6.0.0 (linux; chttp2; glorious)'\r\n'grpc-accept-encoding', 'identity,deflate,gzip'\r\n'accept-encoding', 'identity,gzip'\r\n'x-forwarded-for', '125.39.207.17'\r\n'x-forwarded-proto', 'http'\r\n'x-envoy-internal', 'true'\r\n'x-envoy-downstream-service-cluster', ''\r\n'x-request-id', 'cea162bc-6bf4-4b76-ac4c-4f6d35ea52d2'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n'transfer-encoding', 'chunked'\r\n\r\n[2018-06-23 11:40:56.615][493][debug][router] source/common/router/router.cc:253] [C0][S5133098934978251612] cluster 'local_service_grpc' match for URL '/helloworld.Greeter/SayHello'\r\n[2018-06-23 11:40:56.615][493][debug][router] source/common/router/router.cc:304] [C0][S5133098934978251612] router decoding headers:\r\n':authority', 'grpc'\r\n':path', '/helloworld.Greeter/SayHello'\r\n':method', 'POST'\r\n':scheme', 'http'\r\n'te', 'trailers'\r\n'content-type', 'application/grpc'\r\n'user-agent', 'grpc-c++/1.12.1 grpc-c/6.0.0 (linux; chttp2; glorious)'\r\n'grpc-accept-encoding', 'identity,deflate,gzip'\r\n'accept-encoding', 'identity,gzip'\r\n'x-forwarded-for', '125.39.207.17'\r\n'x-forwarded-proto', 'http'\r\n'x-envoy-downstream-service-cluster', ''\r\n'x-request-id', 'cea162bc-6bf4-4b76-ac4c-4f6d35ea52d2'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2018-06-23 11:40:56.615][493][debug][client] source/common/http/codec_client.cc:25] [C1] connecting\r\n[2018-06-23 11:40:56.615][493][debug][connection] source/common/network/connection_impl.cc:569] [C1] connecting to 0.0.0.0:50051\r\n[2018-06-23 11:40:56.615][493][debug][connection] source/common/network/connection_impl.cc:578] [C1] connection in progress\r\n[2018-06-23 11:40:56.616][493][debug][http2] source/common/http/http2/codec_impl.cc:621] [C1] setting stream-level initial window size to 268435456\r\n[2018-06-23 11:40:56.616][493][debug][http2] source/common/http/http2/codec_impl.cc:643] [C1] updating connection-level initial window size to 268435456\r\n[2018-06-23 11:40:56.616][493][debug][pool] source/common/http/http2/conn_pool.cc:97] [C1] creating stream\r\n[2018-06-23 11:40:56.616][493][debug][router] source/common/router/router.cc:972] [C0][S5133098934978251612] pool ready\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:492] [C1] send data: bytes=24\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:326] [C1] writing 24 bytes, end_stream false\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:492] [C1] send data: bytes=21\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:326] [C1] writing 21 bytes, end_stream false\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:446] [C1] sent frame type=4\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:492] [C1] send data: bytes=13\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:326] [C1] writing 13 bytes, end_stream false\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:446] [C1] sent frame type=8\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:492] [C1] send data: bytes=289\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:326] [C1] writing 289 bytes, end_stream false\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:446] [C1] sent frame type=1\r\n[2018-06-23 11:40:56.616][493][trace][http] source/common/http/conn_manager_impl.cc:660] [C0][S5133098934978251612] decode headers called: filter=0x2db5d60 status=1\r\n[2018-06-23 11:40:56.616][493][trace][http] source/common/http/http1/codec_impl.cc:535] [C0] body size=8\r\n[2018-06-23 11:40:56.616][493][trace][router] source/common/router/router.cc:872] [C0][S5133098934978251612] proxying 8 bytes\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:326] [C1] writing 17 bytes, end_stream false\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:446] [C1] sent frame type=0\r\n[2018-06-23 11:40:56.616][493][trace][http] source/common/http/conn_manager_impl.cc:722] [C0][S5133098934978251612] decode data called: filter=0x2db5d60 status=3\r\n[2018-06-23 11:40:56.616][493][trace][http] source/common/http/http1/codec_impl.cc:543] [C0] message complete\r\n[2018-06-23 11:40:56.616][493][debug][http] source/common/http/conn_manager_impl.cc:788] [C0][S5133098934978251612] request end stream\r\n[2018-06-23 11:40:56.616][493][trace][router] source/common/router/router.cc:872] [C0][S5133098934978251612] proxying 0 bytes\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:326] [C1] writing 9 bytes, end_stream false\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:446] [C1] sent frame type=0\r\n[2018-06-23 11:40:56.616][493][trace][http] source/common/http/conn_manager_impl.cc:722] [C0][S5133098934978251612] decode data called: filter=0x2db5d60 status=3\r\n[2018-06-23 11:40:56.616][493][trace][http] source/common/http/http1/codec_impl.cc:322] [C0] parsed 505 bytes\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:232] [C0] readDisable: enabled=true disable=true\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:389] [C1] socket event: 3\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:457] [C1] write ready\r\n[2018-06-23 11:40:56.616][493][debug][connection] source/common/network/connection_impl.cc:466] [C1] connected\r\n[2018-06-23 11:40:56.616][493][debug][client] source/common/http/codec_client.cc:63] [C1] connected\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:457] [C1] write ready\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/raw_buffer_socket.cc:63] [C1] write returns: 373\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:427] [C1] read ready\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C1] read returns: 63\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C1] read returns: -1\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/raw_buffer_socket.cc:29] [C1] read error: 11\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:277] [C1] dispatching 63 bytes\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:335] [C1] recv frame type=4\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:335] [C1] recv frame type=8\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:335] [C1] recv frame type=6\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:292] [C1] dispatched 63 bytes\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:492] [C1] send data: bytes=9\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:326] [C1] writing 9 bytes, end_stream false\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:446] [C1] sent frame type=4\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:492] [C1] send data: bytes=17\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:326] [C1] writing 17 bytes, end_stream false\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:446] [C1] sent frame type=6\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:389] [C0] socket event: 2\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:457] [C0] write ready\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:389] [C1] socket event: 2\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:457] [C1] write ready\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/raw_buffer_socket.cc:63] [C1] write returns: 26\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:389] [C1] socket event: 3\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:457] [C1] write ready\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:427] [C1] read ready\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C1] read returns: 191\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C1] read returns: -1\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/raw_buffer_socket.cc:29] [C1] read error: 11\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:277] [C1] dispatching 191 bytes\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:335] [C1] recv frame type=4\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:335] [C1] recv frame type=1\r\n[2018-06-23 11:40:56.616][493][debug][router] source/common/router/router.cc:584] [C0][S5133098934978251612] upstream headers complete: end_stream=false\r\n[2018-06-23 11:40:56.616][493][debug][http] source/common/http/conn_manager_impl.cc:974] [C0][S5133098934978251612] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'application/grpc'\r\n'grpc-accept-encoding', 'identity,deflate,gzip'\r\n'accept-encoding', 'identity,gzip'\r\n'x-envoy-upstream-service-time', '0'\r\n'date', 'Sat, 23 Jun 2018 11:40:56 GMT'\r\n'server', 'envoy'\r\n\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:326] [C0] writing 242 bytes, end_stream false\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:335] [C1] recv frame type=0\r\n[2018-06-23 11:40:56.616][493][trace][http] source/common/http/conn_manager_impl.cc:1029] [C0][S5133098934978251612] encoding data via codec (size=8 end_stream=false)\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:326] [C0] writing 13 bytes, end_stream false\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:335] [C1] recv frame type=1\r\n[2018-06-23 11:40:56.616][493][debug][client] source/common/http/codec_client.cc:94] [C1] response complete\r\n[2018-06-23 11:40:56.616][493][trace][main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=1)\r\n[2018-06-23 11:40:56.616][493][debug][pool] source/common/http/http2/conn_pool.cc:189] [C1] destroying stream: 0 remaining\r\n[2018-06-23 11:40:56.616][493][debug][http] source/common/http/conn_manager_impl.cc:1051] [C0][S5133098934978251612] encoding trailers via codec:\r\n'grpc-status', '0'\r\n\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:326] [C0] writing 5 bytes, end_stream false\r\n[2018-06-23 11:40:56.616][493][trace][main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=2)\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:232] [C0] readDisable: enabled=false disable=false\r\n[2018-06-23 11:40:56.616][493][debug][http2] source/common/http/http2/codec_impl.cc:501] [C1] stream closed: 0\r\n[2018-06-23 11:40:56.616][493][trace][main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=3)\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:335] [C1] recv frame type=8\r\n[2018-06-23 11:40:56.616][493][trace][http2] source/common/http/http2/codec_impl.cc:292] [C1] dispatched 191 bytes\r\n[2018-06-23 11:40:56.616][493][trace][main] source/common/event/dispatcher_impl.cc:52] clearing deferred deletion list (size=3)\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:389] [C0] socket event: 2\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/connection_impl.cc:457] [C0] write ready\r\n[2018-06-23 11:40:56.616][493][trace][connection] source/common/network/raw_buffer_socket.cc:63] [C0] write returns: 260\r\n\r\n----------------------------client--------------------------------\r\n[2018-06-23 12:49:19.736][3991][debug][main] source/server/connection_handler_impl.cc:217] [C0] new connection\r\n[2018-06-23 12:49:19.736][3991][trace][connection] source/common/network/connection_impl.cc:389] [C0] socket event: 2\r\n[2018-06-23 12:49:19.736][3991][trace][connection] source/common/network/connection_impl.cc:457] [C0] write ready\r\n[2018-06-23 12:49:19.736][3991][trace][connection] source/common/network/connection_impl.cc:389] [C0] socket event: 3\r\n[2018-06-23 12:49:19.736][3991][trace][connection] source/common/network/connection_impl.cc:457] [C0] write ready\r\n[2018-06-23 12:49:19.736][3991][trace][connection] source/common/network/connection_impl.cc:427] [C0] read ready\r\n[2018-06-23 12:49:19.736][3991][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C0] read returns: 405\r\n[2018-06-23 12:49:19.737][3991][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C0] read returns: -1\r\n[2018-06-23 12:49:19.737][3991][trace][connection] source/common/network/raw_buffer_socket.cc:29] [C0] read error: 11\r\n[2018-06-23 12:49:19.737][3991][debug][http2] source/common/http/http2/codec_impl.cc:621] [C0] setting stream-level initial window size to 268435456\r\n[2018-06-23 12:49:19.737][3991][debug][http2] source/common/http/http2/codec_impl.cc:643] [C0] updating connection-level initial window size to 268435456\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:277] [C0] dispatching 405 bytes\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:335] [C0] recv frame type=4\r\n[2018-06-23 12:49:19.737][3991][debug][http] source/common/http/conn_manager_impl.cc:187] [C0] new stream\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:335] [C0] recv frame type=1\r\n[2018-06-23 12:49:19.737][3991][debug][http] source/common/http/conn_manager_impl.cc:454] [C0][S14746386230218570168] request headers complete (end_stream=false):\r\n':scheme', 'http'\r\n':method', 'POST'\r\n':path', '/helloworld.Greeter/SayHello'\r\n':authority', 'grpc'\r\n'te', 'trailers'\r\n'content-type', 'application/grpc'\r\n'user-agent', 'grpc-c++/1.12.1 grpc-c/6.0.0 (linux; chttp2; glorious)'\r\n'grpc-accept-encoding', 'identity,deflate,gzip'\r\n'accept-encoding', 'identity,gzip'\r\n\r\n[2018-06-23 12:49:19.737][3991][trace][http] source/common/http/conn_manager_impl.cc:660] [C0][S14746386230218570168] decode headers called: filter=0x38e9220 status=0\r\n[2018-06-23 12:49:19.737][3991][debug][router] source/common/router/router.cc:253] [C0][S14746386230218570168] cluster 'grpc' match for URL '/helloworld.Greeter/SayHello'\r\n[2018-06-23 12:49:19.737][3991][debug][router] source/common/router/router.cc:304] [C0][S14746386230218570168] router decoding headers:\r\n':scheme', 'http'\r\n':method', 'POST'\r\n':path', '/helloworld.Greeter/SayHello'\r\n':authority', 'grpc'\r\n'te', 'trailers'\r\n'content-type', 'application/grpc'\r\n'user-agent', 'grpc-c++/1.12.1 grpc-c/6.0.0 (linux; chttp2; glorious)'\r\n'grpc-accept-encoding', 'identity,deflate,gzip'\r\n'accept-encoding', 'identity,gzip'\r\n'x-forwarded-for', '125.39.207.17'\r\n'x-forwarded-proto', 'http'\r\n'x-envoy-internal', 'true'\r\n'x-envoy-downstream-service-cluster', ''\r\n'x-request-id', '99c7e683-e643-4e7e-9322-875645a0965c'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2018-06-23 12:49:19.737][3991][debug][pool] source/common/http/http1/conn_pool.cc:79] creating a new connection\r\n[2018-06-23 12:49:19.737][3991][debug][client] source/common/http/codec_client.cc:25] [C1] connecting\r\n[2018-06-23 12:49:19.737][3991][debug][connection] source/common/network/connection_impl.cc:569] [C1] connecting to 10.169.139.235:9211\r\n[2018-06-23 12:49:19.737][3991][debug][connection] source/common/network/connection_impl.cc:578] [C1] connection in progress\r\n[2018-06-23 12:49:19.737][3991][debug][pool] source/common/http/http1/conn_pool.cc:105] queueing request due to no available connections\r\n[2018-06-23 12:49:19.737][3991][trace][http] source/common/http/conn_manager_impl.cc:660] [C0][S14746386230218570168] decode headers called: filter=0x38e9270 status=1\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:335] [C0] recv frame type=8\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:335] [C0] recv frame type=0\r\n[2018-06-23 12:49:19.737][3991][debug][http] source/common/http/conn_manager_impl.cc:788] [C0][S14746386230218570168] request end stream\r\n[2018-06-23 12:49:19.737][3991][trace][http] source/common/http/conn_manager_impl.cc:722] [C0][S14746386230218570168] decode data called: filter=0x38e9220 status=0\r\n[2018-06-23 12:49:19.737][3991][trace][router] source/common/router/router.cc:862] [C0][S14746386230218570168] buffering 8 bytes\r\n[2018-06-23 12:49:19.737][3991][trace][http] source/common/http/conn_manager_impl.cc:722] [C0][S14746386230218570168] decode data called: filter=0x38e9270 status=3\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:335] [C0] recv frame type=8\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:335] [C0] recv frame type=6\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:292] [C0] dispatched 405 bytes\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:492] [C0] send data: bytes=15\r\n[2018-06-23 12:49:19.737][3991][trace][connection] source/common/network/connection_impl.cc:326] [C0] writing 15 bytes, end_stream false\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:446] [C0] sent frame type=4\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:492] [C0] send data: bytes=9\r\n[2018-06-23 12:49:19.737][3991][trace][connection] source/common/network/connection_impl.cc:326] [C0] writing 9 bytes, end_stream false\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:446] [C0] sent frame type=4\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:492] [C0] send data: bytes=17\r\n[2018-06-23 12:49:19.737][3991][trace][connection] source/common/network/connection_impl.cc:326] [C0] writing 17 bytes, end_stream false\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:446] [C0] sent frame type=6\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:492] [C0] send data: bytes=13\r\n[2018-06-23 12:49:19.737][3991][trace][connection] source/common/network/connection_impl.cc:326] [C0] writing 13 bytes, end_stream false\r\n[2018-06-23 12:49:19.737][3991][trace][http2] source/common/http/http2/codec_impl.cc:446] [C0] sent frame type=8\r\n[2018-06-23 12:49:19.737][3991][trace][connection] source/common/network/connection_impl.cc:389] [C0] socket event: 2\r\n[2018-06-23 12:49:19.737][3991][trace][connection] source/common/network/connection_impl.cc:457] [C0] write ready\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/raw_buffer_socket.cc:63] [C0] write returns: 54\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/connection_impl.cc:389] [C1] socket event: 2\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/connection_impl.cc:457] [C1] write ready\r\n[2018-06-23 12:49:19.738][3991][debug][connection] source/common/network/connection_impl.cc:466] [C1] connected\r\n[2018-06-23 12:49:19.738][3991][debug][client] source/common/http/codec_client.cc:63] [C1] connected\r\n[2018-06-23 12:49:19.738][3991][debug][pool] source/common/http/http1/conn_pool.cc:249] [C1] attaching to next request\r\n[2018-06-23 12:49:19.738][3991][debug][router] source/common/router/router.cc:972] [C0][S14746386230218570168] pool ready\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/connection_impl.cc:326] [C1] writing 487 bytes, end_stream false\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/connection_impl.cc:326] [C1] writing 18 bytes, end_stream false\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/connection_impl.cc:457] [C1] write ready\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/raw_buffer_socket.cc:63] [C1] write returns: 505\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/connection_impl.cc:389] [C1] socket event: 2\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/connection_impl.cc:457] [C1] write ready\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/connection_impl.cc:389] [C1] socket event: 3\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/connection_impl.cc:457] [C1] write ready\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/connection_impl.cc:427] [C1] read ready\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C1] read returns: 260\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/raw_buffer_socket.cc:21] [C1] read returns: -1\r\n[2018-06-23 12:49:19.738][3991][trace][connection] source/common/network/raw_buffer_socket.cc:29] [C1] read error: 11\r\n[2018-06-23 12:49:19.738][3991][trace][http] source/common/http/http1/codec_impl.cc:305] [C1] parsing 260 bytes\r\n[2018-06-23 12:49:19.738][3991][trace][http] source/common/http/http1/codec_impl.cc:292] [C1] completed header: key=content-type value=application/grpc\r\n[2018-06-23 12:49:19.738][3991][trace][http] source/common/http/http1/codec_impl.cc:292] [C1] completed header: key=grpc-accept-encoding value=identity,deflate,gzip\r\n[2018-06-23 12:49:19.738][3991][trace][http] source/common/http/http1/codec_impl.cc:292] [C1] completed header: key=accept-encoding value=identity,gzip\r\n[2018-06-23 12:49:19.738][3991][trace][http] source/common/http/http1/codec_impl.cc:292] [C1] completed header: key=x-envoy-upstream-service-time value=0\r\n[2018-06-23 12:49:19.738][3991][trace][http] source/common/http/http1/codec_impl.cc:292] [C1] completed header: key=date value=Sat, 23 Jun 2018 12:49:19 GMT\r\n[2018-06-23 12:49:19.738][3991][trace][http] source/common/http/http1/codec_impl.cc:292] [C1] completed header: key=server value=envoy\r\n[2018-06-23 12:49:19.738][3991][trace][http] source/common/http/http1/codec_impl.cc:361] [C1] headers complete\r\n[2018-06-23 12:49:19.738][3991][trace][http] source/common/http/http1/codec_impl.cc:292] [C1] completed header: key=transfer-encoding value=chunked\r\n[2018-06-23 12:49:19.738][3991][debug][router] source/common/router/router.cc:584] [C0][S14746386230218570168] upstream headers complete: end_stream=false\r\n[2018-06-23 12:49:19.739][3991][trace][http] source/common/http/conn_manager_impl.cc:893] [C0][S14746386230218570168] encode headers called: filter=0x390f780 status=0\r\n[2018-06-23 12:49:19.739][3991][debug][http] source/common/http/conn_manager_impl.cc:974] [C0][S14746386230218570168] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'application/grpc'\r\n'grpc-accept-encoding', 'identity,deflate,gzip'\r\n'accept-encoding', 'identity,gzip'\r\n'x-envoy-upstream-service-time', '1'\r\n'date', 'Sat, 23 Jun 2018 12:49:19 GMT'\r\n'server', 'envoy'\r\n\r\n[2018-06-23 12:49:19.739][3991][trace][http2] source/common/http/http2/codec_impl.cc:492] [C0] send data: bytes=123\r\n[2018-06-23 12:49:19.739][3991][trace][connection] source/common/network/connection_impl.cc:326] [C0] writing 123 bytes, end_stream false\r\n[2018-06-23 12:49:19.739][3991][trace][http2] source/common/http/http2/codec_impl.cc:446] [C0] sent frame type=1\r\n[2018-06-23 12:49:19.739][3991][trace][http] source/common/http/conn_manager_impl.cc:1022] [C0][S14746386230218570168] encode data called: filter=0x390f780 status=0\r\n[2018-06-23 12:49:19.739][3991][trace][http] source/common/http/conn_manager_impl.cc:1029] [C0][S14746386230218570168] encoding data via codec (size=8 end_stream=false)\r\n[2018-06-23 12:49:19.739][3991][trace][connection] source/common/network/connection_impl.cc:326] [C0] writing 17 bytes, end_stream false\r\n[2018-06-23 12:49:19.739][3991][trace][http2] source/common/http/http2/codec_impl.cc:446] [C0] sent frame type=0\r\n[2018-06-23 12:49:19.739][3991][trace][http] source/common/http/http1/codec_impl.cc:662] [C1] message complete\r\n[2018-06-23 12:49:19.739][3991][debug][client] source/common/http/codec_client.cc:94] [C1] response complete\r\n[2018-06-23 12:49:19.739][3991][trace][main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=1)\r\n[2018-06-23 12:49:19.739][3991][trace][http] source/common/http/conn_manager_impl.cc:1022] [C0][S14746386230218570168] encode data called: filter=0x390f780 status=0\r\n[2018-06-23 12:49:19.739][3991][trace][http] source/common/http/conn_manager_impl.cc:1029] [C0][S14746386230218570168] encoding data via codec (size=0 end_stream=true)\r\n[2018-06-23 12:49:19.739][3991][trace][connection] source/common/network/connection_impl.cc:326] [C0] writing 9 bytes, end_stream false\r\n[2018-06-23 12:49:19.739][3991][trace][http2] source/common/http/http2/codec_impl.cc:446] [C0] sent frame type=0\r\n[2018-06-23 12:49:19.739][3991][debug][http2] source/common/http/http2/codec_impl.cc:501] [C0] stream closed: 0\r\n[2018-06-23 12:49:19.739][3991][trace][main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=2)\r\n[2018-06-23 12:49:19.739][3991][trace][main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=3)\r\n[2018-06-23 12:49:19.739][3991][debug][pool] source/common/http/http1/conn_pool.cc:206] [C1] response complete\r\n[2018-06-23 12:49:19.739][3991][debug][pool] source/common/http/http1/conn_pool.cc:244] [C1] moving to ready\r\n[2018-06-23 12:49:19.739][3991][trace][http] source/common/http/http1/codec_impl.cc:322] [C1] parsed 260 bytes\r\n[2018-06-23 12:49:19.739][3991][trace][connection] source/common/network/connection_impl.cc:389] [C0] socket event: 2\r\n[2018-06-23 12:49:19.739][3991][trace][connection] source/common/network/connection_impl.cc:457] [C0] write ready\r\n[2018-06-23 12:49:19.739][3991][trace][connection] source/common/network/raw_buffer_socket.cc:63] [C0] write returns: 149\r\n[2018-06-23 12:49:19.739][3991][trace][main] source/common/event/dispatcher_impl.cc:52] clearing deferred deletion list (size=3)\r\n----------------------------------------------------------\r\n\r\nwhy the grpc helloworld can't run at envoy\r\nPlease help me to fix it , Thanks",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3706/comments",
    "author": "yuansx",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-07-23T16:13:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-07-30T16:26:15Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 3652,
    "title": "bazel build //source/exe:envoy-static, error for org_golang_google_grpc",
    "created_at": "2018-06-18T11:37:58Z",
    "closed_at": "2018-09-08T16:13:42Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3652",
    "body": "[root@localhost envoy]# bazel build --package_path %workspace%:/root/src/envoy/ //source/exe:envoy-static\r\nINFO: Build options have changed, discarding analysis cache.\r\nExternal dependency cache directory /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25\r\nmake: Entering directory `/root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps'\r\nNo need to rebuild /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25/cares.dep.env\r\nNo need to rebuild /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25/benchmark.dep.env\r\nNo need to rebuild /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25/libevent.dep.env\r\nNo need to rebuild /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25/gperftools.dep.env\r\nNo need to rebuild /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25/luajit.dep.env\r\nBuilding in /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25/luajit.dep.build, logs at /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25/luajit.dep.log\r\nSuccessful build of /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25/luajit.dep\r\ncat: /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25/nghttp2.dep.env: No such file or directory\r\nBuilding in /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25/nghttp2.dep.build, logs at /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/envoy_deps_cache_fbe7fd77b8354b9a6f47b8e24c1a5f25/nghttp2.dep.log\r\nERROR: /root/.cache/bazel/_bazel_root/b4ce55150c2905dc1fa0886156c33084/external/io_bazel_rules_go/proto/BUILD.bazel:21:1: no such package '@org_golang_google_grpc//': failed to fetch org_golang_google_grpc: 2018/06/09 23:04:16 unrecognized import path \"google.golang.org/grpc\"\r\n and referenced by '@io_bazel_rules_go//proto:go_grpc'\r\nERROR: Analysis of target '//source/exe:envoy-static' failed; build aborted: no such package '@org_golang_google_grpc//': failed to fetch org_golang_google_grpc: 2018/06/09 23:04:16 unrecognized import path \"google.golang.org/grpc\"\r\nINFO: Elapsed time: 47.221s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (248 packages loaded)\r\n\r\nall the error message, I don't know how to fix it. who can tell me how to do, thanks",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3652/comments",
    "author": "yuansx",
    "comments": [
      {
        "user": "alonlong",
        "created_at": "2018-06-23T11:41:52Z",
        "body": "ENVOY_SRCDIR=/source\r\nINFO: $TEST_TMPDIR defined: output root default is '/build/tmp' and max_idle_secs default is '15'.\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\n...............\r\nCloning into '/build/envoy-filter-example'...\r\nremote: Counting objects: 850, done.\r\nremote: Compressing objects: 100% (23/23), done.\r\nremote: Total 850 (delta 28), reused 9 (delta 7), pack-reused 820\r\nReceiving objects: 100% (850/850), 97.91 KiB | 99.00 KiB/s, done.\r\nResolving deltas: 100% (762/762), done.\r\nNote: checking out '4b6c55b726eda8a1f99e6f4ca1a87f6ce670604f'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n\r\n  git checkout -b new_branch_name\r\n\r\nHEAD is now at 4b6c55b... cleanup: move filter wrapper into Http namespace. (#49)\r\nbuilding using 2 CPUs\r\ngcc/g++ toolchain configured\r\nbazel release build...\r\nBuilding...\r\nINFO: $TEST_TMPDIR defined: output root default is '/build/tmp' and max_idle_secs default is '15'.\r\nDEBUG: /build/tmp/_bazel_bazel/436badd4919a15958fa3800a4e21074a/external/io_bazel_rules_go/proto/def.bzl:138:3: You no longer need to call proto_register_toolchains(), it does nothing\r\nERROR: /build/tmp/_bazel_bazel/436badd4919a15958fa3800a4e21074a/external/io_bazel_rules_go/proto/BUILD.bazel:21:1: no such package '@org_golang_google_grpc//': failed to fetch org_golang_google_grpc: 2018/06/23 11:38:14 unrecognized import path \"google.golang.org/grpc\"\r\n and referenced by '@io_bazel_rules_go//proto:go_grpc'\r\nERROR: Analysis of target '//source/exe:envoy-static' failed; build aborted: no such package '@org_golang_google_grpc//': failed to fetch org_golang_google_grpc: 2018/06/23 11:38:14 unrecognized import path \"google.golang.org/grpc\"\r\nINFO: Elapsed time: 197.996s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (248 packages loaded)"
      },
      {
        "user": "alonlong",
        "created_at": "2018-06-23T11:42:10Z",
        "body": "I have the same issue"
      },
      {
        "user": "bengol",
        "created_at": "2018-07-06T07:59:38Z",
        "body": "got this error too"
      },
      {
        "user": "xiaoma2100",
        "created_at": "2018-07-26T06:30:33Z",
        "body": "got this error too"
      },
      {
        "user": "agile6v",
        "created_at": "2018-08-02T15:04:04Z",
        "body": "@yuansx @yuansx @bengol @xiaoma2100 \r\n\r\nI've solved this problem by setting \r\n> git config —system http.proxy \"proxy_ip\"\r\n> git config —system https.proxy \"proxy_ip\"\r\n\r\nHope to be useful for you."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-09-01T15:50:51Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-09-08T16:13:41Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 3594,
    "title": "How does envoy do retry on grpc request?",
    "created_at": "2018-06-12T03:17:15Z",
    "closed_at": "2018-08-05T00:37:48Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3594",
    "body": "*Title*: *How does envoy do retry on grpc request?*\r\n\r\n*Description*:\r\n>The grpc-client send a grpc request to local sidecar:envoy, then envoy proxy it to target service(envoy). To be more specific, I assume the local envoy will decode bytes to http2 header & data frames, then proxy these frames to target service(envoy), right? If response failed/timeout, then envoy will do the retry. \r\nQuestions are:\r\n1.For unary call: Envoy will re-send the header & data frames again, right?\r\n2.For streaming call which might contains endless data frames, how to do the retry?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3594/comments",
    "author": "laosijikaichele",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-06-28T23:48:52Z",
        "body": "Streaming requests essentially cannot be retried in any sensible way and retries should not be configured."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-07-29T00:11:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-05T00:37:47Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 3512,
    "title": "[QUESTION] use cluster healthcheck makes upstream delay",
    "created_at": "2018-05-31T04:51:00Z",
    "closed_at": "2018-06-07T04:07:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3512",
    "body": "*Description*:\r\n>\r\nversion: 1.7\r\nwhen cluster healthcheck enabled, upstream servers will delay to response.\r\n\r\n*Config*:\r\nhealth_checks:\r\n    - timeout:\r\n        seconds: 10\r\n      interval:\r\n        seconds: 5\r\n      unhealthy_threshold: 3\r\n      healthy_threshold: 3\r\n      tcp_health_check: {}\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3512/comments",
    "author": "liyehaha",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-05-31T19:38:54Z",
        "body": "@liyehaha sorry, can't parse the question here."
      }
    ]
  },
  {
    "number": 3487,
    "title": "Question about running original_dst serving traffic from different docker containers",
    "created_at": "2018-05-25T14:17:24Z",
    "closed_at": "2018-08-10T01:37:02Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3487",
    "body": "hello! I'm trying to implement an outbound transparent proxy with envoy, running in a different container from the application which sends the traffic to be proxied.\r\n\r\nI'm using this configuration:\r\n```{\r\n  \"listeners\": [\r\n    {\r\n      \"address\": \"tcp://0.0.0.0:80\",\r\n      \"bind_to_port\": false,\r\n      \"filters\": [\r\n        {\r\n          \"type\": \"read\",\r\n          \"name\": \"http_connection_manager\",\r\n          \"config\": {\r\n            \"access_log\": [\r\n                {\r\n                \"path\": \"/tmp/envoy.log\"\r\n                }\r\n            ],\r\n            \"codec_type\": \"auto\",\r\n            \"stat_prefix\": \"forward_http\",\r\n            \"route_config\": {\r\n              \"virtual_hosts\": [\r\n                {\r\n                  \"name\": \"default_http\",\r\n                  \"domains\": [\"*\"],\r\n                  \"routes\": [\r\n                    {\r\n                      \"timeout_ms\": 0,\r\n                      \"prefix\": \"/\",\r\n                      \"cluster\": \"outbound_forward_proxy_http\"\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            \"filters\": [\r\n              {\r\n                \"type\": \"decoder\",\r\n                \"name\": \"router\",\r\n                \"config\": {}\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    },\r\n    {\r\n        \"address\": \"tcp://0.0.0.0:15001\",\r\n        \"filters\": [],\r\n        \"bind_to_port\": true,\r\n        \"use_original_dst\": true\r\n    }\r\n  ],\r\n  \"admin\": {\r\n    \"access_log_path\": \"/tmp/access_log\",\r\n    \"address\": \"tcp://0.0.0.0:8001\"\r\n  },\r\n  \"cluster_manager\": {\r\n      \"clusters\": [\r\n          {\r\n              \"name\": \"outbound_forward_proxy_http\",\r\n              \"connect_timeout_ms\": 2500,\r\n              \"type\": \"original_dst\",\r\n              \"lb_type\": \"original_dst_lb\"\r\n          }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nThis is an docker-compose example yaml:\r\n```\r\nversion: '2'\r\nservices:\r\n  envoy:\r\n    build:\r\n      context: ./envoy\r\n      dockerfile: Dockerfile-envoy\r\n    cap_add:\r\n      - NET_ADMIN\r\n    ports:\r\n      - \"80:80\"\r\n    expose:\r\n      - \"80\"\r\n  application:\r\n    build:\r\n      context: ./api\r\n      dockerfile: Dockerfile-api\r\n    cap_add:\r\n      - NET_ADMIN\r\n\r\n```\r\n\r\nI use IPTABLES in order to redirect all the traffic outgoing from application to envoy.  If I use locally (the http requester application and envoy are in the same container) it works like a charm.\r\n\r\nIn the other hand, if I run envoy in a different container, I receive this message from envoy when I redirect the application requests:\r\n\r\nupstream connect error or disconnect/reset before headers.\r\n   \r\nThe request arrives to envoy ( I can see in the access_log).\r\n\r\nMy IPTABLES rules in envoy:\r\n```\r\niptables -t nat -N ISTIO_REDIRECT\r\niptables -t nat -A ISTIO_REDIRECT -p tcp -j REDIRECT --to-port $ENVOY_PORT\r\niptables -t nat -A PREROUTING -j ISTIO_REDIRECT\r\n\r\niptables -t nat -N ISTIO_OUTPUT\r\niptables -t nat -A OUTPUT -p tcp -j ISTIO_OUTPUT\r\niptables -t nat -A ISTIO_OUTPUT -m owner --uid-owner ${ENVOY_UID} -j RETURN\r\niptables -t nat -A ISTIO_OUTPUT -j ISTIO_REDIRECT.\r\n```  \r\n\r\nIn application:\r\n```iptables -t nat -I OUTPUT -p tcp --dport 80 -j DNAT --to-destination $ENVOY_IP:80```\r\n\r\n\r\nCould you give me an advice on how can I configure envoy as a transparent proxy running in another container?  \r\n\r\n\r\nThanks in advance!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3487/comments",
    "author": "alejandropal",
    "comments": [
      {
        "user": "jolisper",
        "created_at": "2018-06-27T21:00:04Z",
        "body": "I'm facing the same issue, any suggestion @mattklein123?"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-06-29T16:54:14Z",
        "body": "Sorry I don't have any experience w/ the transparent proxy stuff. @lizan @PiotrSikora @kyessenov @rshriram might have some ideas."
      },
      {
        "user": "rshriram",
        "created_at": "2018-07-04T00:00:08Z",
        "body": "Are these two containers in same network namespace? Try putting them in the same namespace to see if it works. My hunch is that the SO_ORIGINAL_DST option works within the same net namespace only"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-03T00:22:42Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-08-10T01:37:02Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 3421,
    "title": "Question: Can I use envoy as a tcp proxy for multiple backends?",
    "created_at": "2018-05-17T17:50:10Z",
    "closed_at": "2018-05-21T02:39:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3421",
    "body": "Hi team,\r\n\r\nTotal newbie here! \r\n\r\nI 'm not trying to envoy as a side-car (which I get from the blogs I read on envoy) but mostly as reverse proxy with health check and load-balancing for tcp connection. The connection is Grpc based. I 've using haproxy for a long time and thought of introducing some exciting to my stack. :) So that I can later introduce features like rate-limiting etc. \r\n\r\nWhat I want is something like this. \r\n```\r\nClient ---> Envoy ---> Server1\r\n                  ---> Server2\r\n                  --->  Server-n.\r\n```\r\n\r\nNow, I spent a full day with the documentation but could no where come close to a configuration, my docker doesn't start with my configurations (sighs). I think the configuration is difficult to do and I couldn't get the APIs, xDS and management server concepts. \r\n\r\nCan someone please help me out?\r\n\r\nThanks. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3421/comments",
    "author": "ashish235",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-05-17T18:31:38Z",
        "body": "@ashish235 for such a general query I think you might have better luck in Slack."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-21T02:39:38Z",
        "body": "Closing out. Please reopen if you have a specific bug to track, but for general user questions we prefer slack or envoy-users@. Thank you."
      }
    ]
  },
  {
    "number": 3416,
    "title": "Can't remove server, date and content-length headers via Lua script",
    "created_at": "2018-05-17T14:10:20Z",
    "closed_at": "2018-05-21T02:40:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3416",
    "body": "*Title*: *Can't remove server, date and content-length headers via Lua script*\r\n\r\n*Description*:\r\n> I want to remove all headers if response code is 204 to reduce traffic costs but I can remove only x-envoy-upstream-service-time and location headers on response. I use the envoyproxy/envoy-alpine:latest container\r\n\r\n*Config*:\r\n```\r\nadmin:\r\n  access_log_path: /dev/stdout\r\n  address:\r\n    socket_address:\r\n      address: 127.0.0.1\r\n      port_value: 8001\r\nstatic_resources:\r\n  listeners:\r\n  - name: http\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8080\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: envoy_edge\r\n          codec_type: AUTO\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: envoy-sidecar\r\n          http_filters:\r\n          - name: envoy.lua\r\n            config:\r\n              inline_code: |\r\n                function envoy_on_response(response_handle)\r\n                  status = response_handle:headers():get(\":status\")\r\n                  if status == \"404\" or status == \"503\" or status == \"204\" then\r\n                    response_handle:logInfo(\"STATUS IS \" .. status)\r\n                    response_handle:headers():remove(\"server\")\r\n                    response_handle:headers():remove(\"date\")\r\n                    response_handle:headers():remove(\"content-length\")\r\n                    response_handle:headers():remove(\"location\")\r\n                    response_handle:headers():remove(\"x-envoy-upstream-service-time\")\r\n                    response_handle:headers():replace(\":status\", \"204\")\r\n                  end\r\n                end\r\n          - name: envoy.router\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3416/comments",
    "author": "username1366",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-05-17T18:28:12Z",
        "body": "This is not possible currently as these headers are set by the connection manager. Removing them would need to be a connection manager option (though content-length could be elided from HTTP/1.1 responses where it is added for 204 responses if we want to patch that)."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-21T02:40:19Z",
        "body": "Closing as answered. Feel free to open feature request issues if you like."
      }
    ]
  },
  {
    "number": 3410,
    "title": "Non-TLS http_health_check on cluster with tls_context",
    "created_at": "2018-05-16T22:26:35Z",
    "closed_at": "2018-05-21T02:40:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3410",
    "body": "*Description*:\r\nIt seems a http_health_check will attempt a TLS connection if the cluster is configured with a tls_context. The ability to allow a non-TLS http_health_check in such a situation would be nice. Apologies if I am simply not reading the documentation close enough or if such an issue exists (I did a quick search for `health` in the open issues.\r\n\r\nConfiguration (please note my EDS is returning an endpoint definition that uses the health_check_config { port_value: port } option)\r\n```\r\n  clusters:\r\n  - name: upstream\r\n    connect_timeout:\r\n      seconds: 1\r\n    lb_policy: ROUND_ROBIN\r\n    health_checks:\r\n    - timeout:\r\n        seconds: 1\r\n      interval:\r\n        seconds: 5\r\n      interval_jitter:\r\n        nanos: 500000000\r\n      no_traffic_interval:\r\n        seconds: 15\r\n      unhealthy_threshold: 1\r\n      healthy_threshold: 2\r\n      http_health_check:\r\n        path: \"/\"\r\n        request_headers_to_add: []\r\n    common_lb_config:\r\n      healthy_panic_threshold: \r\n        value: 0.0\r\n    type: EDS\r\n    eds_cluster_config:\r\n      eds_config:\r\n        api_config_source:\r\n          api_type: GRPC\r\n          grpc_services:\r\n          - envoy_grpc:\r\n              cluster_name: xds_cluster\r\n    tls_context:\r\n      sni: some.domain.com\r\n```\r\nOutput from envoy:\r\n```\r\n[client] source/common/http/codec_client.cc:25] [C41] connecting\r\n[connection] source/common/network/connection_impl.cc:568] [C41] connecting to x.x.x.x:port\r\n[connection] source/common/network/connection_impl.cc:577] [C41] connection in progress\r\n[connection] source/common/network/connection_impl.cc:322] [C41] writing 78 bytes, end_stream false\r\n[connection] source/common/network/connection_impl.cc:385] [C41] socket event: 2\r\n[connection] source/common/network/connection_impl.cc:453] [C41] write ready\r\n[connection] source/common/network/connection_impl.cc:462] [C41] connected\r\n[connection] source/common/ssl/ssl_socket.cc:110] [C41] handshake error: 2\r\n[connection] source/common/network/connection_impl.cc:385] [C41] socket event: 3\r\n[connection] source/common/network/connection_impl.cc:453] [C41] write ready\r\n[connection] source/common/ssl/ssl_socket.cc:110] [C41] handshake error: 1\r\n[connection] source/common/ssl/ssl_socket.cc:138] [C41] SSL error: 268435703:SSL routines:OPENSSL_internal:WRONG_VERSION_NUMBER\r\n[connection] source/common/network/connection_impl.cc:134] [C41] closing socket: 0\r\n[http] source/common/http/http1/codec_impl.cc:305] [C41] parsing 0 bytes\r\n[http] source/common/http/http1/codec_impl.cc:322] [C41] parsed 0 bytes\r\n[client] source/common/http/codec_client.cc:81] [C41] disconnect. resetting 1 pending requests\r\n[client] source/common/http/codec_client.cc:104] [C41] request reset\r\n[main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=1)\r\n[hc] source/common/upstream/health_checker_impl.cc:155] [C41] connection/stream error health_flags=/failed_active_hc\r\n[main] source/common/event/dispatcher_impl.cc:126] item added to deferred deletion list (size=2)\r\n[main] source/common/event/dispatcher_impl.cc:52] clearing deferred deletion list (size=2)\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3410/comments",
    "author": "rene-m-hernandez",
    "comments": [
      {
        "user": "PiotrSikora",
        "created_at": "2018-05-16T22:45:47Z",
        "body": "I'd argue that there is little to no-value from performing non-TLS health-check on a TLS server.\r\n\r\nWhy do you need that?"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-16T22:47:59Z",
        "body": "I tend to agree with @PiotrSikora but either way it's not supported currently."
      },
      {
        "user": "rene-m-hernandez",
        "created_at": "2018-05-16T23:25:26Z",
        "body": "I have an application deployed to a single IP, listening on multiple ports: one is secured, one is not. Plus, it is listening on a third, non-secure port for health check because I see no reason to secure it. Maybe better described as a heartbeat in my case: it doesn't trigger/execute functionality that needs to be secured nor does it return anything of value (200/503).\r\n\r\nThe workaround is simple enough: I listen on a fourth, secured port for health checks."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-21T02:40:56Z",
        "body": "Closing as answered. Feel free to open a feature request issue if you like."
      }
    ]
  },
  {
    "number": 3394,
    "title": "http2 protocolOptions not set unless using ALPN",
    "created_at": "2018-05-15T22:49:51Z",
    "closed_at": "2018-05-16T00:08:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3394",
    "body": "*Title*: *http2 protocolOptions not sent unless using ALPN*\r\n\r\n*Description*:\r\nI have 2 H2/TLS upstream clusters which are identical in all respects except one specifies \r\n`AlpnProtocols: h2, http/1.1` and the other one does not.\r\n\r\nWhat I observe is that both clusters use http2 client to communicate with the upstream. However `http2 protocolOptions` like `maxConcurrentStreams` is only set when ALPN is used.\r\n\r\nIt seems inconsistent to ignore `protocolOptions` when not using `alpn`\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3394/comments",
    "author": "mandarjog",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-05-15T22:58:39Z",
        "body": "@mandarjog AFAIK this is not how the code works so I doubt this is true. Please provide a full config example."
      },
      {
        "user": "mandarjog",
        "created_at": "2018-05-16T00:01:53Z",
        "body": "with alpn\r\n```\r\nrouter] external/envoy/source/common/router/router.cc:250]   C88487]  S11977125130324865691] cluster 'outbound|80||b.istio-system.svc.cluster.local' match for URL '/abc'\r\n...\r\nrouter] external/envoy/source/common/router/router.cc:298]   C88487]  S11977125130324865691]   'x-envoy-expected-rq-timeout-ms':'15000'\r\nrouter] external/envoy/source/common/router/router.cc:298]   C88487]  S11977125130324865691]   ':scheme':'https'\r\nclient] external/envoy/source/common/http/codec_client.cc:25]    connecting\r\nconnection] external/envoy/source/common/network/connection_impl.cc:565]    connecting to 10.48.1.37:80\r\nconnection] external/envoy/source/common/network/connection_impl.cc:574]    connection in progress\r\nhttp2] external/envoy/source/common/http/http2/codec_impl.cc:628]    setting max concurrent streams to 1073741824\r\nhttp2] external/envoy/source/common/http/http2/codec_impl.cc:635]    setting stream-level initial window size to 268435456\r\nhttp2] external/envoy/source/common/http/http2/codec_impl.cc:657]    updating connection-level initial window size to 268435456\r\npool] external/envoy/source/common/http/http2/conn_pool.cc:97]    creating stream\r\nrouter] external/envoy/source/common/router/router.cc:968]    pool ready\r\nhttp2] external/envoy/source/common/http/http2/codec_impl.cc:506]    send data: bytes=24\r\nconnection] external/envoy/source/common/network/connection_impl.cc:322]    writing 24 bytes, end_stream false\r\nhttp2] external/envoy/source/common/http/http2/codec_impl.cc:506]    send data: bytes=27\r\n```\r\nwithout alpn\r\n```\r\nrouter] external/envoy/source/common/router/router.cc:250]   C88487]  S11977125130324865691] cluster 'outbound|80||b.istio-system.svc.cluster.local' match for URL '/abc'\r\n...\r\nrouter] external/envoy/source/common/router/router.cc:298]   C88487]  S11977125130324865691]   'x-envoy-expected-rq-timeout-ms':'15000'\r\nrouter] external/envoy/source/common/router/router.cc:298]   C88487]  S11977125130324865691]   ':scheme':'https'\r\npool] external/envoy/source/common/http/http2/conn_pool.cc:97]   C27] creating stream\r\nrouter] external/envoy/source/common/router/router.cc:968]   C88487]  S11977125130324865691] pool ready\r\nhttp2] external/envoy/source/common/http/http2/codec_impl.cc:506]   C27] send data: bytes=80\r\nconnection] external/envoy/source/common/network/connection_impl.cc:322]   C27] writing 80 bytes, end_stream false\r\nhttp2] external/envoy/source/common/http/http2/codec_impl.cc:460]   C27] sent frame type=1\r\nhttp] external/envoy/source/common/http/conn_manager_impl.cc:670]   C88487]  S11977125130324865691] decode headers called: filter=0x37a1540 status=1\r\nhttp] external/envoy/source/common/http/http1/codec_impl.cc:322]   C88487] parsed 68 bytes\r\nconnection] external/envoy/source/common/network/connection_impl.cc:228]   C88487] readDisable: enabled=true disable=true\r\nconnection] external/envoy/source/common/network/connection_impl.cc:385]   C27] socket event: 2\r\nconnection] external/envoy/source/common/network/connection_impl.cc:453]   C27] write ready\r\nconnection] external/envoy/source/common/ssl/ssl_socket.cc:172]   C27] ssl write returns: 80\r\nconnection] external/envoy/source/common/network/connection_impl.cc:385]   C88487] socket event: 2\r\nconnection] external/envoy/source/common/network/connection_impl.cc:453]   C88487] write ready\r\nconnection] external/envoy/source/common/network/connection_impl.cc:385]   C27] socket event: 3\r\nconnection] external/envoy/source/common/network/connection_impl.cc:453]   C27] write ready\r\nconnection] external/envoy/source/common/network/connection_impl.cc:423]   C27] read ready\r\n```\r\n\r\nconfig \r\n```json\r\n{\r\n  \"name\": \"outbound|80||b.istio-system.svc.cluster.local\",\r\n  \"type\": \"EDS\",\r\n  \"edsClusterConfig\": {\r\n    \"edsConfig\": {\r\n      \"ads\": {\r\n\r\n      }\r\n    },\r\n    \"serviceName\": \"outbound|80||b.istio-system.svc.cluster.local\"\r\n  },\r\n  \"connectTimeout\": \"1.000s\",\r\n  \"tlsContext\": {\r\n    \"commonTlsContext\": {\r\n      \"tlsCertificates\": [\r\n        {\r\n          \"certificateChain\": {\r\n            \"filename\": \"/etc/certs/cert-chain.pem\"\r\n          },\r\n          \"privateKey\": {\r\n            \"filename\": \"/etc/certs/key.pem\"\r\n          }\r\n        }\r\n      ],\r\n      \"validationContext\": {\r\n        \"trustedCa\": {\r\n          \"filename\": \"/etc/certs/root-cert.pem\"\r\n        },\r\n        \"verifySubjectAltName\": [\r\n          \"spiffe://cluster.local/ns/istio-system/sa/default\"\r\n        ]\r\n      },\r\n      \"alpnProtocols\": [ \"h2\", \"http/1.1\" ]\r\n    }\r\n  },\r\n  \"http2ProtocolOptions\": {\r\n    \"maxConcurrentStreams\": 1073741824\r\n  }\r\n}\r\n```"
      },
      {
        "user": "mandarjog",
        "created_at": "2018-05-16T00:08:27Z",
        "body": "Aah ok, my bad. It seems that the `without alpn` log snippet was captured at trace level *after* h2 connection was already established. I was able to confirm thru new logs that `http2 protocolOptions` are honored regadless of `alpn` settings.\r\nThanks."
      }
    ]
  },
  {
    "number": 3341,
    "title": "can envoy health check re-use traffic connection",
    "created_at": "2018-05-10T09:39:57Z",
    "closed_at": "2018-05-16T19:12:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3341",
    "body": "**Issue Template**\r\n\r\n*Title*: *One line description*\r\ncan envoy health check re-use traffic connection\r\n\r\n*Description*:\r\n\r\nI had set up two envoys as client/server and both of them are using only 1 working thread (--concurrency 1). I also set http type health check for them. I want the http HC and traffic use only one connection (that means the http HC re-use traffic connection). Is it possible? \r\n\r\nI'm using envoy  version: 4dd49d8809f7aaa580538b3c228dd99a2fae92a4/1.6.0/Clean/RELEASE\r\n\r\nthanks.\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3341/comments",
    "author": "futangw",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-05-10T15:23:07Z",
        "body": "No, it's not currently possible, and I'm not sure it ever will be without a very large number of changes."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-16T19:12:50Z",
        "body": "Closing as answered."
      }
    ]
  },
  {
    "number": 3237,
    "title": "Envoy is not complaining missing mongo cluster",
    "created_at": "2018-04-27T09:31:25Z",
    "closed_at": "2018-05-04T05:29:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3237",
    "body": "When we are using `http_connection_manager`, envoy will complaining if the cluster is missing. But `tcp_proxy` looks not. I think this should consider as a bug.\r\n\r\nversion: `envoy  version: 4dd49d8809f7aaa580538b3c228dd99a2fae92a4/1.6.0/Clean/RELEASE`\r\n\r\nconfig:\r\n\r\n```yaml\r\nadmin:\r\n  access_log_path: /dev/null\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: mongo_listener\r\n    address:\r\n      socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 27071\r\n    filter_chains:\r\n    - use_proxy_proto: false\r\n      filters:\r\n      - name: envoy.mongo_proxy\r\n        config:\r\n          stat_prefix: stat.mongo\r\n      - name: envoy.tcp_proxy\r\n        config:\r\n          stat_prefix: stat.mongo_tcp\r\n          cluster: mongo_cluster\r\n\r\n  clusters: []\r\n```\r\n\r\noutput:\r\n\r\n```\r\n[2018-04-27 09:30:45.605][30668][info][main] source/server/server.cc:178] initializing epoch 0 (hot restart version=9.200.16384.127.options=capacity=16384, num_slots=8209 hash=228984379728933363)\r\n[2018-04-27 09:30:45.608][30668][info][upstream] source/common/upstream/cluster_manager_impl.cc:131] cm init: all clusters initialized\r\n[2018-04-27 09:30:45.608][30668][info][config] source/server/configuration_impl.cc:52] loading 1 listener(s)\r\n[2018-04-27 09:30:45.609][30668][info][config] source/server/configuration_impl.cc:92] loading tracing configuration\r\n[2018-04-27 09:30:45.610][30668][info][config] source/server/configuration_impl.cc:119] loading stats sink configuration\r\n[2018-04-27 09:30:45.610][30668][info][main] source/server/server.cc:337] all clusters initialized. initializing init manager\r\n[2018-04-27 09:30:45.611][30668][info][config] source/server/listener_manager_impl.cc:583] all dependencies initialized. starting workers\r\n[2018-04-27 09:30:45.611][30668][info][main] source/server/server.cc:353] starting main dispatch loop\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3237/comments",
    "author": "winguse",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-04-27T14:42:05Z",
        "body": "This is the way it used to work, but then we relaxed the error so that tcp_proxy can work with CDS. I would be in favor of adding some type of config option like \"validate_cluster\" that would optionally enforce this check of desired (we discussed this a while back). cc @ggreenway "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-04T05:29:47Z",
        "body": "Closing as answered"
      }
    ]
  },
  {
    "number": 3203,
    "title": "envoy cannot handle max_stream_per_connection.",
    "created_at": "2018-04-25T09:20:20Z",
    "closed_at": "2018-05-04T05:28:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3203",
    "body": "acutally, I want to send 1 streams per 1 connection. (http2).\r\n\r\nso I configured as.\r\n```\r\n.\r\n.\r\n.\r\n clusters:\r\n  - name: asr_mobile_en-US_Service\r\n    max_requests_per_connection: 1\r\n    connect_timeout: 2s\r\n    cleanup_interval: 2s\r\n    type: STATIC\r\n    lb_policy: ROUND_ROBIN\r\n    http2_protocol_options:\r\n      hpack_table_size: 4096\r\n      max_concurrent_streams: 1\r\n    circuit_breakers:\r\n      thresholds:\r\n      - max_connections: 4096\r\n        max_pending_requests: 6000\r\n        max_requests: 6000\r\n    hosts: [\r\n      { socket_address: { address: 172.17.0.1, port_value: 10001 }},\r\n      { socket_address: { address: 172.17.0.1, port_value: 10002 }},\r\n      { socket_address: { address: 172.17.0.1, port_value: 10003 }},\r\n      { socket_address: { address: 172.17.0.1, port_value: 10004 }},\r\n      { socket_address: { address: 172.17.0.1, port_value: 10005 }},\r\n      { socket_address: { address: 172.17.0.1, port_value: 10006 }},\r\n    ]\r\n\r\n```\r\n\r\nas you can see,\r\nI set `max_requests__per_connection=1` and `max_concurrent_streams=1`\r\nthere are only 6 machines and I requested to envoy with 7 requests.\r\nand expect 6 requests sends to machine each, and reject 1 request.\r\n\r\nand run as\r\n`envoy -c /etc/envoy.yaml --concurrency 1`\r\n\r\nbut log is\r\n```\r\n{ \"ts\": \"2018-04-25T09:10:19.727Z\", \"upstreamHost\": \"172.17.0.1:10002\" } }\r\n{ \"ts\": \"2018-04-25T09:10:19.725Z\", \"upstreamHost\": \"172.17.0.1:10002\" } }\r\n{ \"ts\": \"2018-04-25T09:10:19.726Z\", \", \"upstreamHost\": \"172.17.0.1:10001\" } }\r\n{ \"ts\": \"2018-04-25T09:10:19.726Z\",  \"upstreamHost\": \"172.17.0.1:10006\" } }\r\n{ \"ts\": \"2018-04-25T09:10:19.726Z\", \"upstreamHost\": \"172.17.0.1:10005\" } }\r\n{ \"ts\": \"2018-04-25T09:10:19.726Z\",  \"upstreamHost\": \"172.17.0.1:10004\" } }\r\n{ \"ts\": \"2018-04-25T09:10:19.725Z\",  \"upstreamHost\": \"172.17.0.1:10003\" } }\r\n```\r\n\r\nas you can see, envoy sent to `172.17.0.1:10002` twice before finishing first request.\r\nas again, I set `max_requests__per_connection=1` and `max_concurrent_streams=1`\r\nwhat am I missing? \r\n \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3203/comments",
    "author": "hehe717",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-04-25T16:38:25Z",
        "body": "Envoy does load balancing and connections per worker. Check your --concurrency setting."
      },
      {
        "user": "hehe717",
        "created_at": "2018-04-26T06:55:29Z",
        "body": "yes, you can see `--concurrency 1` in run commend.\r\nso 1 worker is running for that envoy I think"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-04-29T07:10:41Z",
        "body": "Currently:\r\n\r\n`max_concurrent_streams` for upstream connections basically doesn't do anything, as it just sets the HTTP/2 settings frame which really only applies to push promise, which we don't support.\r\n\r\n`max_requests__per_connection=1` does not limit overall requests, just requests per connection, so the connection pool will make a new one as needed.\r\n\r\nI don't think it's possible to do exactly what you are looking for currently unfortunately."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-04T05:28:39Z",
        "body": "Closing as answered"
      }
    ]
  },
  {
    "number": 3164,
    "title": "hot-restarter.py throws SIGCHLD",
    "created_at": "2018-04-23T14:39:50Z",
    "closed_at": "2018-05-04T05:28:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3164",
    "body": "First start\r\n\r\n```\r\nroot@b09d29ce90d1:/# ./hot-restarter.py start_envoy.sh &\r\n[1] 12\r\nroot@b09d29ce90d1:/# starting hot-restarter with target: start_envoy.sh\r\nforking and execing new child process at epoch 0\r\nforked new child process with PID=13\r\n[2018-04-23 14:34:11.929][13][info][main] source/server/server.cc:188] initializing epoch 0 (hot restart version=9.200.16384.127.options=capacity=16384, num_slots=8209 hash=228984379728933363)\r\n[2018-04-23 14:34:11.929][13][info][main] source/server/server.cc:190] statically linked extensions:\r\n[2018-04-23 14:34:11.929][13][info][main] source/server/server.cc:192]   access_loggers: envoy.file_access_log,envoy.http_grpc_access_log\r\n[2018-04-23 14:34:11.929][13][info][main] source/server/server.cc:195]   filters.http: envoy.buffer,envoy.cors,envoy.ext_authz,envoy.fault,envoy.grpc_http1_bridge,envoy.grpc_json_transcoder,envoy.grpc_web,envoy.gzip,envoy.health_check,envoy.http_dynamo_filter,envoy.ip_tagging,envoy.lua,envoy.rate_limit,envoy.router,envoy.squash\r\n[2018-04-23 14:34:11.929][13][info][main] source/server/server.cc:198]   filters.listener: envoy.listener.original_dst,envoy.listener.proxy_protocol\r\n[2018-04-23 14:34:11.929][13][info][main] source/server/server.cc:201]   filters.network: envoy.client_ssl_auth,envoy.echo,envoy.ext_authz,envoy.http_connection_manager,envoy.mongo_proxy,envoy.ratelimit,envoy.redis_proxy,envoy.tcp_proxy\r\n[2018-04-23 14:34:11.929][13][info][main] source/server/server.cc:203]   stat_sinks: envoy.dog_statsd,envoy.metrics_service,envoy.statsd\r\n[2018-04-23 14:34:11.929][13][info][main] source/server/server.cc:205]   tracers: envoy.dynamic.ot,envoy.lightstep,envoy.zipkin\r\n[2018-04-23 14:34:11.929][13][info][main] source/server/server.cc:208]   transport_sockets.downstream: raw_buffer,ssl\r\n[2018-04-23 14:34:11.929][13][info][main] source/server/server.cc:211]   transport_sockets.upstream: raw_buffer,ssl\r\n[2018-04-23 14:34:11.932][13][info][upstream] source/common/upstream/cluster_manager_impl.cc:131] cm init: all clusters initialized\r\n[2018-04-23 14:34:11.932][13][info][config] source/server/configuration_impl.cc:52] loading 0 listener(s)\r\n[2018-04-23 14:34:11.932][13][info][config] source/server/configuration_impl.cc:92] loading tracing configuration\r\n[2018-04-23 14:34:11.932][13][info][config] source/server/configuration_impl.cc:114] loading stats sink configuration\r\n[2018-04-23 14:34:11.932][13][info][main] source/server/server.cc:371] all clusters initialized. initializing init manager\r\n[2018-04-23 14:34:11.933][13][info][config] source/server/listener_manager_impl.cc:602] all dependencies initialized. starting workers\r\n[2018-04-23 14:34:11.933][13][info][main] source/server/server.cc:387] starting main dispatch loop\r\n```\r\n\r\nWhen I try to restart \r\n\r\n```\r\nroot@b09d29ce90d1:/# export RESTART_EPOCH=1\r\nroot@b09d29ce90d1:/# ./hot-restarter.py start_envoy.sh\r\nstarting hot-restarter with target: start_envoy.sh\r\nforking and execing new child process at epoch 0\r\nforked new child process with PID=49\r\ngot SIGCHLD\r\nPID=49 exited with code=1\r\nDue to abnormal exit, force killing all child processes and exiting\r\nexiting due to lack of child processes\r\nroot@b09d29ce90d1:/#\r\n\r\n```\r\n\r\nAm I doing anything wrong or is there a problem with restarter script ? ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3164/comments",
    "author": "tak2siva",
    "comments": [
      {
        "user": "bplotnick",
        "created_at": "2018-05-02T16:44:43Z",
        "body": "The hot restarter script is meant to be constantly running. To hot restart, send `SIGHUP` to the hot-restarter process. It stores the epoch number and will increment it when it is restarted."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-04T05:28:13Z",
        "body": "Closing as answered"
      }
    ]
  },
  {
    "number": 3032,
    "title": "Add \"extension_name\" parameter for non-test targets",
    "created_at": "2018-04-09T19:22:59Z",
    "closed_at": "2018-05-21T02:45:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3032",
    "body": "*Description*:\r\nThe new extension refactor has made our site-specific modifications much easier.  However, it might make it even better if there was an \"extension_name\" parameter for non-test targets as well.  That would mean a build across the entire repo would not build unwanted extensions because the targets would not exist.  Are there any concerns about doing something like this?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3032/comments",
    "author": "mrice32",
    "comments": [
      {
        "user": "mrice32",
        "created_at": "2018-04-09T19:23:46Z",
        "body": "cc @htuch "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-04-09T19:32:39Z",
        "body": "@mrice32 why would they build? Bazel dependency tracking should take care of it. No?"
      },
      {
        "user": "mrice32",
        "created_at": "2018-04-09T21:18:57Z",
        "body": "Sorry, I should've been more clear.  I'm talking about if you were to run `bazel build //source/...`, I think bazel will attempt to build all targets - including ones that aren't used by any binaries.  Our internal CI does this (as does Envoy CI IIUC).  If this is atypical for site-specific deployments in the Envoy community, we're happy to just handle it internally. "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-04-09T21:27:44Z",
        "body": "@mrice32 I see. I can't speak for others, but for our production build we just build our exe target and let bazel figure out the rest."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-21T02:45:15Z",
        "body": "@mrice32 I'm going to close this for now. Feel free to reopen if you still want this and we can discuss."
      }
    ]
  },
  {
    "number": 2990,
    "title": "my service failed with 503 err and msg \"upstream connect error or disconnect/reset before headers\"",
    "created_at": "2018-04-04T06:48:26Z",
    "closed_at": "2018-04-08T14:17:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2990",
    "body": "## my service failed with 503 err and msg \"upstream connect error or disconnect/reset before headers\"\r\n\r\nHi,\r\n\r\nI deployed istio 0.6.0 on a K8s cluster version 1.8.3, and the official sample BookInfo worked fine on the cluster. I have written my own service named \"hello-test\" in python and manually injected istio sidecar (enovy-debug).  \r\n\r\nNow I'm trying to deploy two instances of \"hello-test\" hoping every one sends a hello message(in HTTP GET request) to each other and waits response from peer.\r\n\r\nBut I get the http 503 response and error message \"upstream connect error or disconnect/reset before headers\". \r\n\r\nBTW, they work fine if I run two instances directly in K8s cluster whitout any istio sidecar, so i guess it has nothing to do with my code or service.yaml config file \r\n\r\nIs this an envoy config issue? Or some other?\r\nCan someone please take a look?\r\n\r\nThanks!\r\n\r\n\r\n### Here is the yaml file content of my service before injection:\r\n```yaml\r\n##########################################################   \r\n#one instance named hello-test-a   ---- service-a.yaml\r\n##########################################################   \r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: hello-test-a\r\n  namespace: default\r\n  labels:\r\n    app: hello-test-a\r\nspec:\r\n  selector:\r\n    app: hello-test-a\r\n  ports:\r\n  - name: http\r\n    port: 41302\r\n    targetPort: 41302    \r\n---\r\napiVersion: extensions/v1beta1\r\nkind: Deployment\r\nmetadata:\r\n  name: hello-test-a\r\nspec:\r\n  replicas: 1\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: hello-test-a\r\n        version: v1\r\n    spec:\r\n      nodeSelector:\r\n        kaml: support\r\n      containers:\r\n      - name: hello-test-a\r\n        image: capu-svr:5000/hello-test\r\n        command: [\"/bin/bash\"]\r\n        args:\r\n        - -c\r\n        - cd /home/capu/dist/hello-test; ./hello-test hello-test-a hello-test-b\r\n        ports:\r\n        - containerPort: 41302        \r\n      restartPolicy: Always\r\n---\r\napiVersion: extensions/v1beta1\r\nkind: Ingress\r\nmetadata:\r\n  name: hello-test-a\r\n  annotations:\r\n    kubernetes.io/ingress.class: \"istio\"\r\n    ingress.kubernetes.io/ssl-redirect: \"false\"\r\nspec:\r\n  rules:\r\n  - http:\r\n      paths:\r\n      - path: /123/req-from-hello-test-b\r\n        backend:\r\n          serviceName: hello-test-a\r\n          servicePort: 41302\r\n---\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2990/comments",
    "author": "c52662",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-04-04T17:05:28Z",
        "body": "@c52662 I would recommend asking this question over at the Istio project. "
      },
      {
        "user": "c52662",
        "created_at": "2018-04-08T14:16:13Z",
        "body": "@mattklein123 OK, Thanks!"
      }
    ]
  },
  {
    "number": 2819,
    "title": "Re-Balancing Connections when using a hash-based load balancer",
    "created_at": "2018-03-15T02:32:53Z",
    "closed_at": "2018-03-17T19:15:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2819",
    "body": "Been exploring the envoy docs a little bit and haven't seen anything on this topic so thought I would ask. If I have a load balancer set up that uses the ring-hash algorithm, how does re-balancing work, with regards to upstreams coming up or down?\r\n\r\nSpecifically, if I have upstream A and upstream B, with persistent connections (say, websockets) open to both, and A goes down, will all traffic be routed to B? What happens when A comes back up? Will the LB kill any connections that are open to B that are meant to go to A? Or will it keep those connections open and only **new** connections will go to A?\r\n\r\nBasically, we want to guarantee that for a specific header value, **all** clients are **always** connected to the same upstream, regardless of what the upstream actually is.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2819/comments",
    "author": "asgoel",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-03-15T15:49:53Z",
        "body": "It's the former. No existing requests are effected during rehashing. For long lives request reams (websocket/gRPC) you have a split brain situation. It's possible we could kill all connections on rehash, but that would be a feature request and need a bunch of thinking."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-03-17T19:15:48Z",
        "body": "Closing as answered. Please feel free to open a feature request issue with more details on the requirements and use cases."
      }
    ]
  },
  {
    "number": 2773,
    "title": "1",
    "created_at": "2018-03-10T02:48:06Z",
    "closed_at": "2018-03-13T16:52:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2773",
    "body": "",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2773/comments",
    "author": "lixiaobing1",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-03-10T22:57:59Z",
        "body": "@lixiaobing1 I'm sorry but I don't follow the question. Envoy does not create pods. Can you provide ore information? Thank you."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-03-13T16:52:40Z",
        "body": "Closing."
      }
    ]
  },
  {
    "number": 2638,
    "title": "mTLS and traffic split",
    "created_at": "2018-02-17T20:08:53Z",
    "closed_at": "2018-02-21T02:41:18Z",
    "labels": [
      "question",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2638",
    "body": "# Background\r\n\r\nI am preparing a traffic shift from a web API in a Swarm cluster (let's call it `swarm_api`) to the same API in a Kubernetes cluster (`k8s_api`). The two clusters are on distinct private networks. The API's clients are on the same private network as the Swarm cluster, making requests over HTTP. However, the Kubernetes cluster is isolated (and fully managed by a cloud provider; the nodes' network is abstracted); the connection must therefore be secured.\r\n\r\nMy plan is to add an edge envoy in the Swarm cluster (`swarm_proxy`) to split traffic between `swarm_api` and a second edge envoy in the Kubernetes cluster (`k8s_proxy`), which would route traffic to `k8s_api`. The connection between `swarm_proxy` and `k8s_proxy` must be secured with mutual TLS as it goes through the Internet.\r\n\r\n```\r\nPrivate network 1\r\n                      (swarm_proxy)---HTTP--->(swarm_api)\r\n____________________________|__________________________\r\nInternet                    |\r\n                          HTTPS\r\n____________________________|__________________________\r\nPrivate network 2           |\r\n                            ⌄\r\n                       (k8s_proxy)----HTTP---->(k8s_api)\r\n```\r\n\r\n# What I have so far\r\n\r\n1) One the one hand, I have successfully prototyped a traffic split over HTTP, *without* mTLS:\r\n    ```\r\n    Private network 1\r\n                          (swarm_proxy)---HTTP--->(swarm_api)\r\n    ____________________________|__________________________\r\n    Internet                    |\r\n                              HTTP\r\n    ____________________________|__________________________\r\n    Private network 2           |\r\n                                ⌄\r\n                           (k8s_proxy)----HTTP---->(k8s_api)\r\n    ```\r\n    `swarm_proxy` uses an `http_connection_manager` to split traffic between `swarm_api` and `k8s_proxy`; `k8s_proxy` simply routes traffic to `k8s_api` (see configurations below).\r\n2) On the other hand, I have successfully prototyped simple routing with mTLS:\r\n    ```\r\n    Private network 1\r\n                          (swarm_proxy)\r\n    ____________________________|__________________________\r\n    Internet                    |\r\n                              HTTPS\r\n    ____________________________|__________________________\r\n    Private network 2           |\r\n                                ⌄\r\n                           (k8s_proxy)----HTTP---->(k8s_api)\r\n    ```\r\n    `swarm_proxy` uses a `tcp_proxy` to route traffic to `k8s_proxy`; the envoy _cluster_ adds a `tls_context` including a client certificate; `k8s_proxy` requires TLS with client certificate, then routes traffic to `k8s_api` (see configurations below).\r\n\r\n# Issue\r\n\r\nNow I'm struggling to make both traffic split and mTLS work at the same time: if I try to combine the two approaches in a third prototype, `swarm_proxy` returns 301 Moved Permanently when it routes traffic to `k8s_proxy` (see configurations below).\r\n\r\nIs there something I'm missing / don't understand, or is this a bug?\r\n\r\n# Envoy Configurations\r\n\r\n## Prototype 1: traffic split\r\n\r\n```yaml\r\n# swarm_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: api\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  weighted_clusters:\r\n                    runtime_key_prefix: routing.traffic_split.api\r\n                    clusters:\r\n                    - name: swarm_api\r\n                      weight: 50\r\n                    - name: k8s_proxy\r\n                      weight: 50\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: swarm_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: swarm_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\n  - name: k8s_proxy\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_proxy # TODO replace with public DNS when implemented\r\n        port_value: 80\r\n    connect_timeout: 1s\r\nadmin:\r\n  access_log_path: \"/dev/stdout\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\n```\r\n\r\n```yaml\r\n# k8s_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: k8s_api\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  cluster: k8s_api\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: k8s_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\nadmin: {...}\r\n```\r\n\r\n## Prototype 2: mutual TLS\r\n\r\n```yaml\r\n# swarm_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.tcp_proxy\r\n        config:\r\n          stat_prefix: ingress\r\n          cluster: k8s_proxy\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: k8s_proxy\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_proxy # TODO replace with public DNS when implemented\r\n        port_value: 443\r\n    connect_timeout: 1s\r\n    tls_context:\r\n      common_tls_context:\r\n        validation_context:\r\n          trusted_ca:\r\n            filename: /etc/certs/ca.crt.pem\r\n        tls_certificates:\r\n        - certificate_chain:\r\n            filename: /etc/certs/swarm_proxy.crt.pem\r\n          private_key:\r\n            filename: /etc/certs/swarm_proxy.key.pem\r\nadmin: {...}\r\n```\r\n\r\n```yaml\r\n# k8s_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 443\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: k8s_api\r\n              domains:\r\n              - \"*\"\r\n              require_tls: ALL\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  cluster: k8s_api\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n      tls_context:\r\n        require_client_certificate: true\r\n        common_tls_context:\r\n          validation_context:\r\n            trusted_ca:\r\n              filename: /etc/certs/ca.crt.pem\r\n          tls_certificates:\r\n          - certificate_chain:\r\n              filename: /etc/certs/k8s_proxy.crt.pem\r\n            private_key:\r\n              filename: /etc/certs/k8s_proxy.key.pem\r\n  clusters:\r\n  - name: k8s_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\nadmin: {...}\r\n```\r\n\r\n## Prototype 3: traffic split and mutual TLS (NOT WORKING)\r\n\r\n```yaml\r\n# swarm_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: api\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  weighted_clusters:\r\n                    runtime_key_prefix: routing.traffic_split.api\r\n                    clusters:\r\n                    - name: swarm_api\r\n                      weight: 50\r\n                    - name: k8s_proxy\r\n                      weight: 50\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: swarm_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: swarm_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\n  - name: k8s_proxy\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_proxy # TODO replace with public DNS when implemented\r\n        port_value: 443\r\n    connect_timeout: 1s\r\n    tls_context:\r\n      common_tls_context:\r\n        validation_context:\r\n          trusted_ca:\r\n            filename: /etc/certs/ca.crt.pem\r\n        tls_certificates:\r\n        - certificate_chain:\r\n            filename: /etc/certs/swarm_proxy.crt.pem\r\n          private_key:\r\n            filename: /etc/certs/swarm_proxy.key.pem\r\nadmin: {...}\r\n```\r\n\r\n```yaml\r\n# k8s_proxy : same as prototype 2\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2638/comments",
    "author": "adrienjt",
    "comments": [
      {
        "user": "PiotrSikora",
        "created_at": "2018-02-21T01:44:44Z",
        "body": "You need to either add `use_remote_address: true` to `k8s_proxy` or remove `require_tls: ALL` from it.\r\n\r\nIf you're using `use_remote_address: false` (default), then `k8s_proxy` is going to receive and accept `X-Forwarded-Proto: http` from `swarm_proxy` and reject client's HTTP request, since it doesn't fulfill the `require_tls` restriction, which applies to client's HTTP request and not to the connection to `k8s_proxy`, `require_client_certificate: true` is enough to enforce mTLS between the proxies.\r\n\r\nAlso, you should add `use_remote_address: true` to `swarm_proxy`, since it's acting as an edge proxy."
      },
      {
        "user": "adrienjt",
        "created_at": "2018-02-21T02:41:00Z",
        "body": "Thanks for the solution *and explanations* @PiotrSikora! That worked."
      }
    ]
  },
  {
    "number": 2505,
    "title": "prometheus2 scrape envoy stats  show no token found , why did I do?",
    "created_at": "2018-02-01T08:07:56Z",
    "closed_at": "2018-02-02T08:25:28Z",
    "labels": [
      "question",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2505",
    "body": "prometheus config\r\n        metrics_path: /stats\r\n        params:\r\n          format: ['prometheus']\r\nthe prometheus targets show status down and error is no token found,I know the reason is the metrics format, but how did I do to  solve this problem ?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2505/comments",
    "author": "marsty",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2018-02-01T18:31:21Z",
        "body": "Can you use curl (or similar) to query the stats endpoint on Envoy? Prometheus is very restrictive about metric names (they may only contain `[A-Za-z0-9:_]` and must start with `[A-Za-z_]`). I suspect one of your metrics has an illegal character that Envoy should be filtering out. "
      },
      {
        "user": "marsty",
        "created_at": "2018-02-02T02:01:56Z",
        "body": "I use prom tool check the envoy stats (format prometheus) found that have the same name of the metrics envoy_listener_http_downstream_rq_xx{envoy_response_code_class=\"2\",envoy_http_conn_manager_prefix=\"ingress_http\",envoy_listener_address=\"0_0_0_0_80\"} 0\r\nenvoy_listener_http_downstream_rq_xx{envoy_response_code_class=\"4\",envoy_http_conn_manager_prefix=\"ingress_http\",envoy_listener_address=\"0_0_0_0_80\"} 4\r\nhow did I  change the name ?"
      },
      {
        "user": "marsty",
        "created_at": "2018-02-02T08:25:21Z",
        "body": "I have solved, in another thinking of a solution,thanks!"
      }
    ]
  },
  {
    "number": 2462,
    "title": "Question on license of envoy 1.3.0 dependencies",
    "created_at": "2018-01-26T16:24:25Z",
    "closed_at": "2018-01-30T14:53:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2462",
    "body": "I've a question on one of the envoy 1.3.0 dependencies: rapidjson  1.1.0 \r\n\r\nIt's license states: \r\n\r\n**_If you have downloaded a copy of the RapidJSON source code from Tencent, please note that RapidJSON source code is licensed under the MIT License, except for the third-party components listed below which are subject to different license terms.  Your integration of RapidJSON into your own projects may require compliance with the MIT License, as well as the other licenses applicable to the third-party components included within RapidJSON. To avoid the problematic JSON license in your own projects, it's sufficient to exclude the bin/jsonchecker/ directory, as it's the only code under the JSON license.\r\nA copy of the MIT License is included in this file._** \r\n\r\nSo my question is:  is the bin/jsonchecker/ directory included in the envoy 1.3.0 binary referenced as:\r\n\r\n**_proxy/envoy-1.3.0.tg: \r\nsize:2266298 \r\nobject_id:c10f7dcc-4010-4dfe-460a-250a0e1cde1 \r\nsha: 45d667aa64a876ab857853b112f065a8800d3161_**     ?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2462/comments",
    "author": "luisapace",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-01-26T16:29:54Z",
        "body": "@rshriram can you or someone else from IBM answer this? Thank you."
      },
      {
        "user": "rshriram",
        "created_at": "2018-01-27T03:30:44Z",
        "body": "Why ibm? \r\nEither way, Envoy was approved internally a year ago. "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-01-27T18:07:23Z",
        "body": "@rshriram because @luisapace works at IBM and has been emailing me. :)"
      },
      {
        "user": "luisapace",
        "created_at": "2018-01-29T09:15:23Z",
        "body": "I'm sorry, but really I do not understand your point... if I've a question on the build of Envoy (not produced by IBM), why do I have to write to IBM instead of the developers of that package that has produced that binary?  Could you please clarify? I've done this several times for other packages and always their developers have answered me. Please let me know if you know the answer to my question or not. Thanks a lot for your time and help. \r\n\r\n\r\n"
      },
      {
        "user": "rshriram",
        "created_at": "2018-01-29T20:25:52Z",
        "body": "Short answer: no.\r\nContents of bazel-envoy/external/com_github_tencent_rapidjson/bin/jsonchecker/ are\r\n```\r\nfail1.json   fail13.json  fail17.json  fail20.json  fail24.json  fail28.json  fail31.json  fail5.json   fail9.json   readme.txt   \r\nfail10.json  fail14.json  fail18.json  fail21.json  fail25.json  fail29.json  fail32.json  fail6.json   pass1.json   \r\nfail11.json  fail15.json  fail19.json  fail22.json  fail26.json  fail3.json   fail33.json  fail7.json   pass2.json   \r\nfail12.json  fail16.json  fail2.json   fail23.json  fail27.json  fail30.json  fail4.json   fail8.json   pass3.json   \r\n```\r\n\r\nwhich are not part of envoy binary."
      },
      {
        "user": "luisapace",
        "created_at": "2018-01-30T13:58:24Z",
        "body": "Great, thanks a lot for your help!"
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-01-30T14:53:52Z",
        "body": "I think this answers your question so closing this off, but please reopen if I'm wrong!"
      }
    ]
  },
  {
    "number": 2426,
    "title": "1",
    "created_at": "2018-01-22T08:26:20Z",
    "closed_at": "2018-01-23T20:37:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2426",
    "body": "",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2426/comments",
    "author": "lixiaobing1",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-01-22T18:28:12Z",
        "body": "@lixiaobing1 Do you have a question? Not much to go on here. Thank you."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-01-23T20:37:17Z",
        "body": "Going to close this out. Please feel free to reopen if you have a more complete question."
      }
    ]
  },
  {
    "number": 2425,
    "title": "openssl: command not found causing multiple test failures on envoy_build_centos",
    "created_at": "2018-01-21T22:26:33Z",
    "closed_at": "2018-01-29T20:03:16Z",
    "labels": [
      "question",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2425",
    "body": "When building envoy as of commit d076db68e2cda1f46c2256f6c9ef6c932811057e using envoy_build_centos there are two test failures due to openssl not being found.  Is the docker image including openssl and/or is test runtime not configured to point to openssl?\r\n\r\n/build/tmp/_bazel_bazel/436badd4919a15958fa3800a4e21074a/execroot/ci/bazel-out/k8-opt/bin/test/common/ssl/context_impl_test.runfiles/ci/test/common/ssl/gen_unittest_certs.sh: line 41: openssl: command not found\r\nFailed PYTHONPATH=$(dirname /build/tmp/_bazel_bazel/436badd4919a15958fa3800a4e21074a/execroot/ci/bazel-out/k8-opt/bin/test/common/ssl/context_impl_test.runfiles/ci/test/common/ssl/gen_unittest_certs.sh) /build/tmp/_bazel_bazel/436badd4919a15958fa3800a4e21074a/execroot/ci/bazel-out/k8-opt/bin/test/common/ssl/context_impl_test.runfiles/ci/test/common/ssl/gen_unittest_certs.sh\r\n[2018-01-21 22:02:55.951][12008][critical][assert] test/test_common/environment.cc:210] assert failure: false\r\n[2018-01-21 22:02:55.951][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:101] Caught Aborted, suspect faulting address 0xc3ad00002ee8\r\n[2018-01-21 22:02:55.951][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:85] Backtrace obj</lib64/libc.so.6> thr<0> (use tools/stack_decode.py):\r\n[2018-01-21 22:02:55.951][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #0 0x7f26e98cb1f7\r\n[2018-01-21 22:02:55.951][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #1 0x7f26e98cc8e7\r\n[2018-01-21 22:02:55.951][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:93] thr<0> obj</build/tmp/_bazel_bazel/436badd4919a15958fa3800a4e21074a/execroot/ci/bazel-out/k8-opt/bin/test/common/ssl/context_impl_test.runfiles/ci/test/common/ssl/context_impl_test>\r\n[2018-01-21 22:02:55.951][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #2 0x4485c3\r\n[2018-01-21 22:02:55.951][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #3 0x41fede\r\n[2018-01-21 22:02:55.951][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #4 0x71a527\r\n[2018-01-21 22:02:55.952][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #5 0x71a718\r\n[2018-01-21 22:02:55.952][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #6 0x71c71c\r\n[2018-01-21 22:02:55.952][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #7 0x71a8f7\r\n[2018-01-21 22:02:55.952][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #8 0x71aab2\r\n[2018-01-21 22:02:55.952][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #9 0x445753\r\n[2018-01-21 22:02:55.952][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #10 0x40d2c7\r\n[2018-01-21 22:02:55.952][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:93] thr<0> obj</lib64/libc.so.6>\r\n[2018-01-21 22:02:55.952][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #11 0x7f26e98b7c04\r\n[2018-01-21 22:02:55.952][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:93] thr<0> obj</build/tmp/_bazel_bazel/436badd4919a15958fa3800a4e21074a/execroot/ci/bazel-out/k8-opt/bin/test/common/ssl/context_impl_test.runfiles/ci/test/common/ssl/context_impl_test>\r\n[2018-01-21 22:02:55.952][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #12 0x41bba4\r\n[2018-01-21 22:02:55.952][12008][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:97] end backtrace thread 0\r\nAborted (core dumped)\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2425/comments",
    "author": "toaler",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-01-22T18:27:39Z",
        "body": "@toaler I would imagine the image needs something else installed. I'm marking this help wanted as the centos build is community driven and none of the maintainers use it."
      }
    ]
  },
  {
    "number": 2398,
    "title": "Linker error (wrapped_ar) on macOS during build of libprotobuf",
    "created_at": "2018-01-18T03:16:10Z",
    "closed_at": "2018-01-22T02:41:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2398",
    "body": "Hi!\r\n\r\nsomehow `wrapped_ar` does not get called with the right parameters on my box here. It should be called as `wrapped_ar rcs bazel-out/file1.o bazel-out/file2.o ...` but the `rcs` is being omitted and `ar` barfs at the `z` in ba`z`el-out.\r\n\r\n@zuercher Any idea what is going on?\r\n\r\nThanks a lot!\r\n\r\n```\r\nINFO: Analysed target //source/exe:envoy-static (1 packages loaded).\r\nINFO: Found 1 target...\r\nSUBCOMMAND: # @com_google_protobuf//:protoc_lib [action 'Linking external/com_google_protobuf/libprotoc_lib.a [for host]']\r\n(cd /private/var/tmp/_bazel_dignabbit/f1230d944ef1a2f5e5e43295c7aeb52c/execroot/envoy && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM='' \\\r\n    APPLE_SDK_VERSION_OVERRIDE='' \\\r\n    PATH=/usr/bin:/bin:/usr/sbin:/sbin \\\r\n    TMPDIR=/var/folders/_0/hklc1gfx4w51_6rxchyp3mlh0000gn/T/ \\\r\n    XCODE_VERSION_OVERRIDE=9.2.0 \\\r\n  external/local_config_cc/wrapped_ar bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/code_generator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/command_line_interface.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_enum.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_enum_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_extension.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_file.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_generator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_helpers.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_map_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_message.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_message_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_padding_optimizer.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_primitive_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_service.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_string_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_doc_comment.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_enum.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_enum_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_field_base.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_generator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_helpers.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_map_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_message.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_message_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_primitive_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_reflection_class.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_repeated_enum_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_repeated_message_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_repeated_primitive_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_source_generator_base.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/csharp/csharp_wrapper_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_context.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_doc_comment.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_enum.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_enum_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_enum_field_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_enum_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_extension.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_extension_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_file.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_generator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_generator_factory.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_helpers.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_lazy_message_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_lazy_message_field_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_map_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_map_field_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_message.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_message_builder.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_message_builder_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_message_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_message_field_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_message_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_name_resolver.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_primitive_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_primitive_field_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_service.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_shared_code_generator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_string_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/java/java_string_field_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/javanano/javanano_enum.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/javanano/javanano_enum_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/javanano/javanano_extension.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/javanano/javanano_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/javanano/javanano_file.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/javanano/javanano_generator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/javanano/javanano_helpers.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/javanano/javanano_map_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/javanano/javanano_message.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/javanano/javanano_message_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/javanano/javanano_primitive_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/js/js_generator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/js/well_known_types_embed.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_enum.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_enum_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_extension.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_file.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_generator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_helpers.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_map_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_message.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_message_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_oneof.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/objectivec/objectivec_primitive_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/php/php_generator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/plugin.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/plugin.pb.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/python/python_generator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/ruby/ruby_generator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/subprocess.o bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/external/com_google_protobuf/src/google/protobuf/compiler/zip_writer.o)\r\nSUBCOMMAND: # @com_google_protobuf//:protobuf_lite [action 'Linking external/com_google_protobuf/libprotobuf_lite.a [for host]']\r\n(cd /private/var/tmp/_bazel_dignabbit/f1230d944ef1a2f5e5e43295c7aeb52c/execroot/envoy && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM='' \\\r\n    APPLE_SDK_VERSION_OVERRIDE='' \\\r\n    PATH=/usr/bin:/bin:/usr/sbin:/sbin \\\r\n    TMPDIR=/var/folders/_0/hklc1gfx4w51_6rxchyp3mlh0000gn/T/ \\\r\n    XCODE_VERSION_OVERRIDE=9.2.0 \\\r\n  external/local_config_cc/wrapped_ar bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/arena.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/arenastring.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/extension_set.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/generated_message_table_driven_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/generated_message_util.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/io/coded_stream.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/io/zero_copy_stream.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/io/zero_copy_stream_impl_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/message_lite.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/repeated_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/atomicops_internals_x86_gcc.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/atomicops_internals_x86_msvc.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/bytestream.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/common.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/int128.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/io_win32.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/once.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/status.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/statusor.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/stringpiece.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/stringprintf.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/structurally_valid.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/strutil.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/stubs/time.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/external/com_google_protobuf/src/google/protobuf/wire_format_lite.o)\r\nSUBCOMMAND: # @com_google_protobuf//:protobuf [action 'Linking external/com_google_protobuf/libprotobuf.a [for host]']\r\n(cd /private/var/tmp/_bazel_dignabbit/f1230d944ef1a2f5e5e43295c7aeb52c/execroot/envoy && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM='' \\\r\n    APPLE_SDK_VERSION_OVERRIDE='' \\\r\n    PATH=/usr/bin:/bin:/usr/sbin:/sbin \\\r\n    TMPDIR=/var/folders/_0/hklc1gfx4w51_6rxchyp3mlh0000gn/T/ \\\r\n    XCODE_VERSION_OVERRIDE=9.2.0 \\\r\n  external/local_config_cc/wrapped_ar bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/any.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/any.pb.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/api.pb.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/compiler/importer.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/compiler/parser.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/descriptor.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/descriptor.pb.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/descriptor_database.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/duration.pb.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/dynamic_message.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/empty.pb.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/extension_set_heavy.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/field_mask.pb.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/generated_message_reflection.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/generated_message_table_driven.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/io/gzip_stream.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/io/printer.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/io/strtod.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/io/tokenizer.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/io/zero_copy_stream_impl.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/map_field.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/message.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/reflection_ops.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/service.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/source_context.pb.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/struct.pb.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/stubs/mathlimits.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/stubs/substitute.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/text_format.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/timestamp.pb.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/type.pb.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/unknown_field_set.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/delimited_message_util.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/field_comparator.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/field_mask_util.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/datapiece.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/default_value_objectwriter.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/error_listener.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/field_mask_utility.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/json_escaping.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/json_objectwriter.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/json_stream_parser.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/object_writer.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/proto_writer.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/protostream_objectsource.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/protostream_objectwriter.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/type_info.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/type_info_test_helper.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/internal/utility.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/json_util.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/message_differencer.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/time_util.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/util/type_resolver_util.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/wire_format.o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/external/com_google_protobuf/src/google/protobuf/wrappers.pb.o)\r\nINFO: From Linking external/com_google_protobuf/libprotobuf_lite.a [for host]:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ar: illegal option -- z\r\nusage:  ar -d [-TLsv] archive file ...\r\n\tar -m [-TLsv] archive file ...\r\n\tar -m [-abiTLsv] position archive file ...\r\n\tar -p [-TLsv] archive [file ...]\r\n\tar -q [-cTLsv] archive file ...\r\n\tar -r [-cuTLsv] archive file ...\r\n\tar -r [-abciuTLsv] position archive file ...\r\n\tar -t [-TLsv] archive [file ...]\r\n\tar -x [-ouTLsv] archive [file ...]\r\nINFO: From Linking external/com_google_protobuf/libprotoc_lib.a [for host]:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ar: illegal option -- z\r\nusage:  ar -d [-TLsv] archive file ...\r\n\tar -m [-TLsv] archive file ...\r\n\tar -m [-abiTLsv] position archive file ...\r\n\tar -p [-TLsv] archive [file ...]\r\n\tar -q [-cTLsv] archive file ...\r\n\tar -r [-cuTLsv] archive file ...\r\n\tar -r [-abciuTLsv] position archive file ...\r\n\tar -t [-TLsv] archive [file ...]\r\n\tar -x [-ouTLsv] archive [file ...]\r\nERROR: /private/var/tmp/_bazel_dignabbit/f1230d944ef1a2f5e5e43295c7aeb52c/external/com_google_protobuf/BUILD:274:1: output 'external/com_google_protobuf/libprotoc_lib.a' was not created\r\nERROR: /private/var/tmp/_bazel_dignabbit/f1230d944ef1a2f5e5e43295c7aeb52c/external/com_google_protobuf/BUILD:274:1: not all outputs were created or valid\r\nERROR: /private/var/tmp/_bazel_dignabbit/f1230d944ef1a2f5e5e43295c7aeb52c/external/com_google_protobuf/BUILD:64:1: output 'external/com_google_protobuf/libprotobuf_lite.a' was not created\r\nTarget //source/exe:envoy-static failed to build\r\nINFO: Elapsed time: 1.700s, Critical Path: 0.41s\r\nFAILED: Build did NOT complete successfully\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2398/comments",
    "author": "dignabbit",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2018-01-19T17:47:19Z",
        "body": "I suspect you are using an older version of bazel. Version 0.9.0 is known to work, but older versions contain a bug causing bazel to use invalid flags for `ar` and `strip`. For a while we had workarounds in place, but they were removed."
      },
      {
        "user": "dignabbit",
        "created_at": "2018-01-20T00:48:06Z",
        "body": "@zuercher : Looks like my 0.9.0 does not work. The output I pasted is from this version:\r\n\r\n```\r\nBuild label: 0.9.0-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Sun Jul 12 12:24:01 +49936 (1513677414241)\r\nBuild timestamp: 1513677414241\r\nBuild timestamp as int: 1513677414241\r\n```\r\n\r\nShould I try a stock non homebrew 0.9.0?"
      },
      {
        "user": "dignabbit",
        "created_at": "2018-01-20T20:16:20Z",
        "body": "@zuercher : FYI, both bazel 0.8.0-homebrew and 0.8.1-homebrew work fine for me.\r\n\r\n```\r\nBuild label: 0.8.0-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Nov 27 20:38:20 2017 (1511815100)\r\nBuild timestamp: 1511815100\r\nBuild timestamp as int: 1511815100\r\n\r\nBuild label: 0.8.1-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Dec 5 19:29:04 2017 (1512502144)\r\nBuild timestamp: 1512502144\r\nBuild timestamp as int: 1512502144\r\n```\r\n"
      },
      {
        "user": "dignabbit",
        "created_at": "2018-01-22T02:41:13Z",
        "body": "Well, my apologies. Bazel 0.9.0 works fine against current master. My branch was lagging behind too much."
      }
    ]
  },
  {
    "number": 2230,
    "title": "Extend the admin interface to support requesting CDS, RDS sync/reload",
    "created_at": "2017-12-19T13:32:53Z",
    "closed_at": "2017-12-20T16:15:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2230",
    "body": "I have a use case where I need to add a new cluster and as set of routed to the envoy configuration and then signal envoy to consume the new configuration so I can use new configuration immediately after. \r\n\r\nI wanted to use CDS and RDS for this but currently envoy queries xDS periodically which means that there will be some un-deterministic time before the new clusters and routes take effect. \r\n\r\nI think it would be useful to have /clusters/reload /routes/reload  etc.  functionality in the admin interface which allows deterministic reload/update of the configuration.\r\n\r\nI think these methods should return/complete after the reload has completed not just when a sync has been initiated.   \r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2230/comments",
    "author": "georgi-d",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2017-12-19T13:38:20Z",
        "body": "Would gRPC streaming in the v2 API suffice for your use case?"
      },
      {
        "user": "georgi-d",
        "created_at": "2017-12-19T14:36:38Z",
        "body": "Could you elaborate on what you mean by \r\n> gRPC streaming in the v2 API\r\n\r\nI would like if it is possible to send a simple REST request possibly containing JSON body. I would rather avoid having to rely on protocol that requires using a library for encoding the request.\r\n\r\nEither requesting a resync over RDS, CDS or directly adding the new configuration over REST would work for me. \r\n\r\nThanks\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2017-12-19T16:29:38Z",
        "body": "Although this feature is possible, it's a large amount of work, and IMO not worth it when the v2 APIs have been designed to be streaming with ACKs as @htuch mentions. @georgi-d can you please take a look at the v2 streaming APIs to see if that works for you. I think it's unlikely that any of the core Envoy developers are going to add this so if it did get added it would be a (complicated) community addition."
      },
      {
        "user": "georgi-d",
        "created_at": "2017-12-20T14:04:10Z",
        "body": "I read into the v2 streaming API and as far as I understand it it should cover what I want to achieve. \r\n\r\nI will attempt the solution with the v2 API.\r\n\r\nThanks for the reply. "
      },
      {
        "user": "mattklein123",
        "created_at": "2017-12-20T16:15:56Z",
        "body": "OK going ahead and closing. Thank you."
      }
    ]
  },
  {
    "number": 2106,
    "title": "envoy won't start as a user if /dev/shm/envoy_shared_memory_0 is owned by root",
    "created_at": "2017-11-26T23:43:58Z",
    "closed_at": "2017-12-01T02:34:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2106",
    "body": "If you normally run envoy as a user (say: istio-proxy), but you run it as root to debug something, you won't be able to then start it again as the non-root user.\r\n\r\n/dev/shm/envoy_shared_memory_0 is owned by root and Istio crashes when it can't read it.\r\n\r\nDeleting that file and restarting brings everything back to life.\r\n\r\n```\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.366][15786][critical][assert] external/envoy/source/server/hot_restart_impl.cc:43] panic: cannot open shared memory region /envoy_shared_memory_0 check user permissions\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.367][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:101] Caught Aborted, suspect faulting address 0x6d00003daa\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.367][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:85] Backtrace obj</lib/x86_64-linux-gnu/libc.so.6> thr<0> (use tools/stack_decode.py):\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.367][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #0 0x7f2993968fcf\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.367][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #1 0x7f299396a3f9\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.367][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:93] thr<0> obj</usr/local/bin/envoy>\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.368][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #2 0x5fa612\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.368][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #3 0x5fa83b\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.368][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #4 0x4c44bb\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.368][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #5 0x41adb5\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.369][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:93] thr<0> obj</lib/x86_64-linux-gnu/libc.so.6>\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.369][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #6 0x7f29939562b0\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.369][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:93] thr<0> obj</usr/local/bin/envoy>\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.369][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:95] thr<0> #7 0x44ca88\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.369][15786][critical][backtrace] bazel-out/local-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:97] end backtrace thread 0\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: W1126 23:36:42.370554   15781 agent.go:204] Epoch 0 terminated with an error: signal: aborted\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: W1126 23:36:42.370624   15781 agent.go:303] Aborted all epochs\r\nNov 26 23:36:42 localhost istio-start.sh[15754]: [2017-11-26 23:36:42.577][15789][critical][assert] external/envoy/source/server/hot_restart_impl.cc:43] panic: cannot open shared memory region /envoy_shared_memory_0 check user permissions\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2106/comments",
    "author": "craigbox",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-11-27T01:38:24Z",
        "body": "@craigbox not really sure what we can do better here. Do you want the error message without the panic/abort (clean exit(1))?"
      },
      {
        "user": "craigbox",
        "created_at": "2017-11-28T12:31:51Z",
        "body": "Your call - in this case it appears you know you can't start but then crash trying. You could either exit cleanly, or open an new shared memory file? \r\n\r\nDoesn't matter to me, I'm unlikely to make the mistake again :)"
      },
      {
        "user": "mattklein123",
        "created_at": "2017-12-01T02:34:44Z",
        "body": "Going to close for now as the error message is clear before the panic. If this comes up again we can get rid of the abort."
      }
    ]
  },
  {
    "number": 2068,
    "title": "How to configure tcp_proxy with CDS?",
    "created_at": "2017-11-16T00:33:55Z",
    "closed_at": "2017-11-16T22:16:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2068",
    "body": "I originally posted this in Slack but wasn't able to get a reply from those who were online at the time. Putting it here for asynchronous conversation purposes.\r\n\r\nIs it possible to get `tcp_proxy` working with clusters as defined via CDS? I am able to get it working with `http_connection_manager` just fine. Here is the error I'm encountering:\r\n\r\n```\r\n[2017-11-15 01:18:26.445][251][critical][main] source/server/server.cc:78] error \\\r\n  initializing configuration '/etc/envoy.json': tcp proxy : unknown cluster \\\r\n  'TCPSERVICE' in TCP route\r\n```\r\n\r\nHowever that `TCPSERVICE` service is available in CDS. The CDS response provides information about hundreds of different clusters. Here's a truncated result with just the applicable one:\r\n\r\n```json\r\n{\r\n  \"clusters\": [\r\n    \"...many HTTP services\",\r\n    {\r\n      \"connect_timeout_ms\": 2000,\r\n      \"hosts\": [\r\n        {\r\n          \"url\": \"tcp://instance1.datacenter.opentable.com:1234\"\r\n        }\r\n      ],\r\n      \"lb_type\": \"random\",\r\n      \"name\": \"TCPSERVICE\",\r\n      \"type\": \"strict_dns\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nAnd finally, here is what my generated configuration file looks like. If the `egress_listener-TCPSERVICE` section is removed then the file compiles just fine and I'm able to make egress HTTP connections.\r\n\r\n```json\r\n{\r\n  \"listeners\": [\r\n    {\r\n      \"name\": \"ingress_http_listener\",\r\n      \"address\": \"tcp://0.0.0.0:3002\",\r\n      \"filters\": [\r\n        {\r\n          \"name\": \"http_connection_manager\",\r\n          \"config\": {\r\n            \"codec_type\": \"auto\",\r\n            \"stat_prefix\": \"CONSUMERNAME-ingress\",\r\n            \"route_config\": {\r\n              \"virtual_hosts\": [\r\n                {\r\n                  \"name\": \"route_vhost_local_service\",\r\n                  \"domains\": [\r\n                    \"*\"\r\n                  ],\r\n                  \"routes\": [\r\n                    {\r\n                      \"timeout_ms\": 0,\r\n                      \"prefix\": \"/\",\r\n                      \"host_rewrite\": \"tcp://127.0.0.1:3000\",\r\n                      \"cluster\": \"CONSUMERNAME-cluster-ingress\"\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            \"filters\": [\r\n              {\r\n                \"name\": \"router\",\r\n                \"config\": {}\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    },\r\n    {\r\n      \"name\": \"egress_http_listener\",\r\n      \"address\": \"tcp://127.0.0.1:80\",\r\n      \"filters\": [\r\n        {\r\n          \"name\": \"http_connection_manager\",\r\n          \"config\": {\r\n            \"codec_type\": \"auto\",\r\n            \"stat_prefix\": \"CONSUMERNAME-egress\",\r\n            \"route_config\": {\r\n              \"virtual_hosts\": [\r\n                {\r\n                  \"name\": \"route_vhost_remote_services\",\r\n                  \"domains\": [\r\n                    \"*\"\r\n                  ],\r\n                  \"routes\": [\r\n                    {\r\n                      \"prefix\": \"/\",\r\n                      \"cluster_header\": \"Envoy-Discovery-Target\",\r\n                      \"timeout_ms\": 15000,\r\n                      \"retry_policy\": {\r\n                        \"retry_on\": \"5xx\",\r\n                        \"num_retries\": 3,\r\n                        \"per_try_timeout_ms\": 5000\r\n                      }\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            \"filters\": [\r\n              {\r\n                \"name\": \"router\",\r\n                \"config\": {}\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    },\r\n    {\r\n      \"name\": \"egress_listener-TCPSERVICE\",\r\n      \"address\": \"tcp://127.0.0.1:6379\",\r\n      \"filters\": [\r\n        {\r\n          \"name\": \"tcp_proxy\",\r\n          \"config\": {\r\n            \"stat_prefix\": \"CONSUMERNAME-egress-TCPSERVICE\",\r\n            \"route_config\": {\r\n              \"routes\": [\r\n                {\r\n                  \"cluster\": \"TCPSERVICE\"\r\n                }\r\n              ]\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ],\r\n  \"cluster_manager\": {\r\n    \"cds\": {\r\n      \"refresh_delay_ms\": 10000,\r\n      \"cluster\": {\r\n        \"name\": \"envoy-discovery.api.opentable.com\",\r\n        \"type\": \"logical_dns\",\r\n        \"connect_timeout_ms\": 250,\r\n        \"lb_type\": \"round_robin\",\r\n        \"hosts\": [\r\n          {\r\n            \"url\": \"tcp://envoy-discovery.api.opentable.com:80\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    \"clusters\": [\r\n      {\r\n        \"name\": \"CONSUMERNAME-cluster-ingress\",\r\n        \"connect_timeout_ms\": 250,\r\n        \"type\": \"logical_dns\",\r\n        \"lb_type\": \"round_robin\",\r\n        \"hosts\": [\r\n          {\r\n            \"url\": \"tcp://127.0.0.1:3000\"\r\n          }\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nMy intention with this approach is to provide a catch-all for all HTTP traffic, routing based on the `Envoy-Discovery-Target` header. When it comes to non-HTTP traffic I'll generate a list of those `tcp_proxy` entries based on the name of the cluster and an internal port to listen on.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2068/comments",
    "author": "tlhunter",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-11-16T05:26:34Z",
        "body": "Currently, tcp_proxy clusters must be defined when the listener is created. They cannot come later via CDS. Thus, in the current implementation you will need to use LDS also. It would be possible to relax this and allow the tcp_proxy to specify clusters that are not yet defined, but that would require a code change."
      },
      {
        "user": "tlhunter",
        "created_at": "2017-11-16T17:21:17Z",
        "body": "Thanks for the reply, I'll checkout LDS."
      },
      {
        "user": "tlhunter",
        "created_at": "2017-11-16T18:59:08Z",
        "body": "It looks like I can output the following JSON from an LDS to get my desired outcome:\r\n\r\n```json\r\n{\r\n  \"listeners\": [\r\n    {\r\n      \"name\": \"%TCP_SERVICE_NAME%-listener\",\r\n      \"address\": \"127.0.0.1:%DESIRED_PORT%\",\r\n      \"filters\": [{\r\n        \"name\": \"%TCP_SERVICE_NAME%-filter\",\r\n        \"config\": {\r\n          \"name\": \"tcp_proxy\",\r\n          \"config\": {\r\n            \"stat_prefix\": \"%TCP_SERVICE_NAME%-egress\",\r\n            \"route_config\": {\r\n              \"routes\": [{\r\n                \"cluster\": \"%TCP_SERVICE_NAME%\"\r\n              }]\r\n            }\r\n          }\r\n        }\r\n      }]\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nHowever with this scenario the LDS needs to somehow keep a registry of relationships between services. For example if I have consumers C1, C2, C3, then LDS needs to know that C1 needs to talk to producers P4 and P5, C2 needs to talk to P6, etc. Registering these requirements ahead of time seems a little dangerous, especially when C1 version 2.0 is released and needs to talk to P6 and no longer P4. Also, what happens when a developer does one-off local testing?\r\n\r\nI do know, at the point in time the envoy.json file is created, which servicename/ports I want to access from a given consumer. Like I know that C1 wants to access P4 and that C1 is expecting to do so over port 1234. The listeners I need won't really be dynamic; C1 will always need to talk to P4 and P5 through the same ports.\r\n\r\nDoes this still sound like LDS is the best approach? Are most users somehow registering the producers they need to communicate with ahead of time? Does my use-case even make sense or am I beginning to misuse Envoy?"
      },
      {
        "user": "mattklein123",
        "created_at": "2017-11-16T20:07:30Z",
        "body": "> Does my use-case even make sense or am I beginning to misuse Envoy?\r\n\r\nUnfortunately there is no right answer here. Your use case makes sense, but every use case is different in terms of whether you can use centralized control or service local based configuration. I would say that in general most people are moving towards feeding intention into the central management system and serving it from there.\r\n\r\nIt's not a very difficult change to add a configuration option to allow TCP proxy config to load with a cluster that does not exist and gracefully fail at runtime if the cluster continues to not exist. If you want to turn this issue into a feature request that's fine also."
      },
      {
        "user": "tlhunter",
        "created_at": "2017-11-16T22:16:08Z",
        "body": "@mattklein123 thanks for the recommendation, glad to hear it wouldn't be a difficult change. I've created #2075."
      }
    ]
  },
  {
    "number": 1989,
    "title": "envoy clusters problem when cx_connect_fail not zero",
    "created_at": "2017-11-02T04:16:17Z",
    "closed_at": "2017-11-07T19:54:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1989",
    "body": "Envoy took a long time (sometimes nearly 1 minute) to get hosts for all upstream clusters (~30) ready via sds in my environment. I found the cx_connect_fail was not zero when the issue occurred. I suspected it may be caused by connection timeout, so I adjusted the connect_timeout_ms of sds settings from 250 ms to 2500 ms. The problem never appeared again during my test.\r\n\r\n```\r\nsds_cluster::172.24.1.22:9019::cx_active::18\r\nsds_cluster::172.24.1.22:9019::cx_connect_fail::15\r\nsds_cluster::172.24.1.22:9019::cx_total::33\r\n```\r\nAlthough the problem has been fixed, I still have several questions in my mind:\r\n\r\n1. in what situation the  cx_connect_fail could greater than zero?\r\n\r\n2. why the envoy takes a long time to get hosts for all upstream clusters when there have failed connection to sds server?\r\n\r\nThanks in advance!",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1989/comments",
    "author": "jinuxstyle",
    "comments": [
      {
        "user": "jinuxstyle",
        "created_at": "2017-11-02T12:53:17Z",
        "body": "update: found connect_timeout in stats\r\n\r\n$ curl xxx/stats|grep sds |grep timeout\r\n\r\ncluster.sds_cluster.upstream_cx_connect_timeout: 15\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2017-11-02T16:24:47Z",
        "body": "Sounds like you are having networking problems between Envoy and the SDS server. I would take a look at that."
      },
      {
        "user": "mattklein123",
        "created_at": "2017-11-07T19:54:55Z",
        "body": "Closing. Please reopen if there is more to look at it here."
      }
    ]
  },
  {
    "number": 1988,
    "title": "Stateless ADS push semantics",
    "created_at": "2017-11-02T01:04:13Z",
    "closed_at": "2017-11-03T18:55:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1988",
    "body": "**Issue Template**\r\n\r\n*Title*: Push semantics in ADS/xDS\r\n\r\n*Description*:\r\nSeems that the aggressive EDS/RDS request behavior makes it hard to implement a stateless management server.\r\n\r\nCurrently, Envoy requests EDS aggressively. As soon as it receives a response, it makes another request. For a server that's trying to push an EDS update, this creates a lot of complexity. First, the server needs to leave one of the requests open (otherwise, Envoy spins forever). That means it needs to know that all resources made it through EDS. Unfortunately, Envoy only accepts resources that have been asked as `resource_name` in EDS request. In case that Envoy requests resources one by one, the server now needs to track how many resources have already been acknowledged by the remote Envoy node. That information will have to be embedded in the version or be tracked in some shared cache between management server instances. This tracking per node/resource requires some state and prevents writing a stateless push server.\r\n\r\ncc @htuch @mattklein123 \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1988/comments",
    "author": "kyessenov",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-11-02T01:36:45Z",
        "body": "@kyessenov I'm sorry but this is a very complicated issue and you will need to be much more specific. Can you please provide the exact sequence of events (in as much detail as possible) that is problematic.\r\n\r\nIs the issue in ADS that Envoy collapses all of the EDS requests into a single request? E.g., First {E1}, then {E1, E2}, etc.? \r\n\r\nIs the issue that you don't know when Envoy doesn't care about an EDS anymore? {E1, E2} -> {E1}? (Basically you don't know when a watch is gone)\r\n\r\n@htuch wrote all this code and is in a much better position to follow up on this, but I don't think I can help until I better understand the problem. I can see many possible solutions here specific to ADS in terms of how Envoy sends and receives messages, since internally, Envoy is already doing a substantial amount of logic to do the muxing."
      },
      {
        "user": "mattklein123",
        "created_at": "2017-11-02T02:10:49Z",
        "body": "@kyessenov I talked to @htuch offline and I think the concerns are basically ^ and he has a good plan to fix, but for other readers probably useful if you can provide more color."
      },
      {
        "user": "kyessenov",
        "created_at": "2017-11-02T02:26:12Z",
        "body": "Sorry about the limited context in the question. I filed it right after chatting with @htuch. The basic premise is that we are looking to implement a management server that can safely ignore `resource_names`.  If the config server is pre-populated with some responses, then we need to push them to proxies with a fresh version. That works well for LDS/CDS since the response should have all new listeners and clusters. But for EDS, there is a bunch requests coming for different subsets of fresh and new clusters. If we respond to all requests with the same desired set of endpoints, Envoy spins forever asking for more and more updates. So it means we should look at the requested resource names, and then make sure that all updated resources have been asked by a particular Envoy node."
      }
    ]
  },
  {
    "number": 1908,
    "title": "[ASK] Can envoy send grpc error code in REST response",
    "created_at": "2017-10-20T03:37:45Z",
    "closed_at": "2017-10-26T19:03:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1908",
    "body": "I have a case that when I send grpc request via REST the log said that it 200 code which mean success, but I'm not sure it's clearly success becaus when I look the grpc log it said error 401. So can envoy  send also grpc error code response? ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1908/comments",
    "author": "syukur91",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-10-20T16:13:18Z",
        "body": "I'm sorry I don't understand the question. Can you rephrase with more detail?"
      },
      {
        "user": "mattklein123",
        "created_at": "2017-10-26T19:03:33Z",
        "body": "Closing this out. Feel free to reopen with more info."
      }
    ]
  },
  {
    "number": 1863,
    "title": "How do I clone traffic (shadow proxy) while still able to keep the XFF and remote client ip?",
    "created_at": "2017-10-16T08:00:00Z",
    "closed_at": "2017-10-24T00:13:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1863",
    "body": "How do I clone traffic (shadow proxy) while still able to keep the XFF and remote client ip?\r\n\r\nCurrently I'm running envoy inside a docker container (172.17.0.1) but the forwarded traffic to varnish etc always show originating IP from 172.17.0.1 instead from the end user client ip. Please help. Thanks",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1863/comments",
    "author": "unisqu",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-10-16T15:49:05Z",
        "body": "The shadow feature does not alter XFF. It uses whatever has already been defined by the connection manager settings."
      },
      {
        "user": "mattklein123",
        "created_at": "2017-10-24T00:13:52Z",
        "body": "Closing as answered."
      }
    ]
  },
  {
    "number": 1855,
    "title": "Load balancing and persistent connections",
    "created_at": "2017-10-13T21:35:01Z",
    "closed_at": "2017-10-18T17:24:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1855",
    "body": "Hi! I looked for documentation around this and wasn't able to find anything salient.\r\n\r\nHow does Envoy behave when load balancing persistent HTTP/2 connections to an upstream cluster? Specifically, how does it handle cases where hosts are added or removed to a cluster that uses strict DNS service discovery? Will it rebalance traffic when hosts are added, or will it only balance new connections to new hosts? \r\n\r\nWe're looking to address issues we have with using Kubernetes services to load balance persistent gRPC connections, where we see uneven load balancing behavior that persists after a rolling restart of pod backends.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1855/comments",
    "author": "natetarrh",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-10-14T01:09:39Z",
        "body": "I thought we had docs on this but I can't quickly find them. Will do an update at some point. \r\n\r\nThe answer is that load balancing is at the request level, not the connection level. Traffic will be rebalanced as hosts are added and removed."
      },
      {
        "user": "natetarrh",
        "created_at": "2017-10-18T17:24:15Z",
        "body": "Thanks Matt!"
      }
    ]
  },
  {
    "number": 1850,
    "title": "Build error: no such package '@protobuf_bzl//'",
    "created_at": "2017-10-13T12:37:21Z",
    "closed_at": "2017-10-24T00:13:38Z",
    "labels": [
      "question",
      "area/build"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1850",
    "body": "I get the following build error with Bazel:\r\n\r\n    bazel build //source/exe:envoy-static\r\n    External dependency cache directory /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f\r\n    make: Entering directory '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps'\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/cares.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/cares.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/cares.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/backward.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/backward.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/backward.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/libevent.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/libevent.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/libevent.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/gcovr.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/gcovr.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/gcovr.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/googletest.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/googletest.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/googletest.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/gperftools.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/gperftools.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/gperftools.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/http-parser.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/http-parser.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/http-parser.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/protobuf.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/protobuf.dep.log\r\n    DEBUG: /root/envoy/bazel/repositories.bzl:18:5: Fetching external dependencies...\r\n    DEBUG: /root/envoy/bazel/repositories.bzl:24:5: External dep build exited with return code: 256\r\n    DEBUG: /root/envoy/bazel/repositories.bzl:25:5:\r\n    DEBUG: /root/envoy/bazel/repositories.bzl:26:5: Timed out; also encountered an error while attempting to retrieve output\r\n    ERROR: Skipping '//source/exe:envoy-static': error loading package 'source/exe': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_bzl//': no such package '@envoy_deps//thirdparty/protobuf': External dep build failed\r\n    WARNING: Target pattern parsing failed.\r\n    ERROR: error loading package 'source/exe': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_bzl//': no such package '@envoy_deps//thirdparty/protobuf': External dep build failed\r\n    INFO: Elapsed time: 705.921s\r\n    FAILED: Build did NOT complete successfully (0 packages loaded)\r\n        currently loading: source/exe\r\n\r\non `v1.4.0` branch. I've tried clearing the Bazel cache and restarting the build - but it then fails on this step.\r\n\r\nThe build is running on Debian version:\r\n\r\n    Linux 4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u1 (2017-06-18) x86_64 GNU/Linux\r\n\r\nwith Bazel version:\r\n\r\n    Build label: 0.6.1\r\n    Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n    Build time: Thu Oct 5 21:54:59 2017 (1507240499)\r\n    Build timestamp: 1507240499\r\n    Build timestamp as int: 1507240499",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1850/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "ghost",
        "created_at": "2017-10-13T12:45:11Z",
        "body": "Sometimes, the protobuf dep passes, and I get this error:\r\n\r\n    bazel build //source/exe:envoy-static\r\n    External dependency cache directory /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f\r\n    make: Entering directory '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps'\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/cares.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/backward.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/libevent.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/gcovr.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/googletest.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/gperftools.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/http-parser.dep' is up to date.\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/lightstep.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/lightstep.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/lightstep.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/nghttp2.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/nghttp2.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/nghttp2.dep\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/protobuf.dep' is up to date.\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/rapidjson.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/rapidjson.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/rapidjson.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/spdlog.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/spdlog.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/spdlog.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/boringssl.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/boringssl.dep.log\r\n    Loading: 0 packages loaded\r\n        currently loading: source/exe\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/boringssl.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/tclap.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/tclap.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/tclap.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/yaml-cpp.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/yaml-cpp.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/yaml-cpp.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/zlib.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/zlib.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/zlib.dep\r\n    make: Leaving directory '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps'\r\n\r\n    real    7m57.674s\r\n    user    6m25.900s\r\n    sys 0m24.180s\r\n    DEBUG: /root/envoy/bazel/repositories.bzl:18:5: Fetching external dependencies...\r\n    DEBUG: /root/envoy/bazel/repositories.bzl:24:5: External dep build exited with return code: 0\r\n    DEBUG: /root/envoy/bazel/repositories.bzl:25:5:\r\n    DEBUG: /root/envoy/bazel/repositories.bzl:26:5: External dependency cache directory /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f\r\n    make: Entering directory '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps'\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/cares.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/backward.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/libevent.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/gcovr.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/googletest.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/gperftools.dep' is up to date.\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/http-parser.dep' is up to date.\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/lightstep.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/lightstep.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/lightstep.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/nghttp2.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/nghttp2.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/nghttp2.dep\r\n    make: '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/protobuf.dep' is up to date.\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/rapidjson.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/rapidjson.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/rapidjson.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/spdlog.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/spdlog.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/spdlog.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/boringssl.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/boringssl.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/boringssl.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/tclap.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/tclap.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/tclap.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/yaml-cpp.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/yaml-cpp.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/yaml-cpp.dep\r\n    Building in /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/zlib.dep.build, logs at /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/zlib.dep.log\r\n    Successful build of /root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f/zlib.dep\r\n    make: Leaving directory '/root/.cache/bazel/_bazel_root/4e47b77d977b209382cf04b3ae32963e/external/envoy_deps'\r\n\r\n    real    7m57.674s\r\n    user    6m25.900s\r\n    sys 0m24.180s\r\n    ERROR: in target '//external:cc_toolchain': error loading package '@local_config_cc//': Extension file not found. Unable to load file '@local_config_cc//:dummy_toolchain.bzl': file doesn't exist or isn't a file\r\n    INFO: Elapsed time: 478.660s\r\n    FAILED: Build did NOT complete successfully (5 packages loaded)\r\n        currently loading: @local_config_cc//"
      },
      {
        "user": "ghost",
        "created_at": "2017-10-13T13:35:03Z",
        "body": "Small update: The build fails with this error for tag `v1.4.0` but not for master. "
      },
      {
        "user": "htuch",
        "created_at": "2017-10-13T13:40:40Z",
        "body": "Bazel 0.6.1 will work with master. For v1.4.0, you probably want something more like 0.5.2 or 0.5.3. "
      },
      {
        "user": "mattklein123",
        "created_at": "2017-10-24T00:13:38Z",
        "body": "Closing as answered. "
      }
    ]
  },
  {
    "number": 1392,
    "title": "Questions on enabling envoy for gRPC with TLS",
    "created_at": "2017-08-04T00:46:00Z",
    "closed_at": "2017-08-08T03:21:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1392",
    "body": "Hi,\r\n\r\nWe are trying out envoy to proxy gRPC with TLS. We tried gRPC without TLS and it works well.\r\nWhen we added TLS on both the listeners and cluster, we are getting this on the client call:\r\n```\r\nrpc error: code = FailedPrecondition desc = transport: received the unexpected content-type \"text/plain\"\r\n```\r\n\r\nIs there any docs on how gRPC/TLS is supposed to work with envoy?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1392/comments",
    "author": "leon-g-xu",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-08-04T01:43:10Z",
        "body": "Please provide your configuration, and a dump of /clusters and /stats."
      },
      {
        "user": "leon-g-xu",
        "created_at": "2017-08-04T02:02:26Z",
        "body": "This is my envoy json. I am running a envoy docker container:\r\n```\r\n{\r\n  \"listeners\": [\r\n    {\r\n      \"address\": \"tcp://0.0.0.0:80\",\r\n      \"ssl_context\": {\r\n        \"private_key_file\": \"/etc/envoy-security/key.pem\",\r\n        \"ca_cert_file\": \"/etc/envoy-security/ca.cer\",\r\n        \"cert_chain_file\": \"/etc/envoy-security/cert.cer\",\r\n        \"alpn_protocols\": \"h2\"\r\n      },\r\n      \"filters\": [\r\n        {\r\n          \"type\": \"read\",\r\n          \"name\": \"http_connection_manager\",\r\n          \"config\": {\r\n            \"codec_type\": \"http2\",\r\n            \"stat_prefix\": \"router\",\r\n            \"route_config\": {\r\n              \"virtual_hosts\": [\r\n                {\r\n                  \"name\": \"default\",\r\n                  \"domains\": [ \"*\" ],\r\n                  \"routes\": [\r\n                    {\r\n                      \"prefix\": \"/\",\r\n                      \"cluster\": \"grpcsample\"\r\n                    }\r\n                  ]\r\n                }\r\n              ]\r\n            },\r\n            \"access_log\": [\r\n              {\r\n                \"path\": \"/dev/stdout\"\r\n              }\r\n            ],\r\n            \"filters\": [\r\n              { \"type\": \"decoder\", \"name\": \"router\", \"config\": {} }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ],\r\n  \"admin\": {\r\n    \"access_log_path\": \"/dev/null\",\r\n    \"address\": \"tcp://0.0.0.0:8001\"\r\n  },\r\n  \"cluster_manager\": {\r\n    \"clusters\": [\r\n      {\r\n        \"name\": \"grpcsample\",\r\n        \"connect_timeout_ms\": 250,\r\n        \"type\": \"strict_dns\",\r\n        \"lb_type\": \"round_robin\",\r\n        \"ssl_context\": {\r\n          \"private_key_file\": \"/etc/envoy-security/key.pem\",\r\n          \"ca_cert_file\": \"/etc/envoy-security/ca.cer\",\r\n          \"cert_chain_file\": \"/etc/envoy-security/cert.cer\",\r\n          \"alpn_protocols\": \"h2\"\r\n        },\r\n        \"features\": \"http2\",\r\n        \"hosts\": [\r\n          {\r\n            \"url\": \"tcp://grpcsample.testing.dev:50050\"\r\n          }\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n}\r\n\r\n```"
      },
      {
        "user": "leon-g-xu",
        "created_at": "2017-08-04T20:43:54Z",
        "body": "are `/clusters` and `/stats` two available endpoints in envoy? \r\nAlso I tried disabling tls in the upstream server and remove the ssl_context in the cluster_manager, and it's working. Wondering if envoy is changing the request if it is tls.  "
      },
      {
        "user": "mattklein123",
        "created_at": "2017-08-04T20:46:03Z",
        "body": "/clusters and /stats are admin endpoints. Please look at docs. I'm guessing that something is not configured correctly and the TLS handshake is not actually working between envoy and upstream. "
      },
      {
        "user": "leon-g-xu",
        "created_at": "2017-08-04T21:48:49Z",
        "body": "stat dump: \r\n```\r\ncluster.grpcsample.external.upstream_rq_503: 1\r\ncluster.grpcsample.external.upstream_rq_5xx: 1\r\ncluster.grpcsample.http2.header_overflow: 0\r\ncluster.grpcsample.http2.headers_cb_no_stream: 0\r\ncluster.grpcsample.http2.rx_reset: 0\r\ncluster.grpcsample.http2.trailers: 0\r\ncluster.grpcsample.http2.tx_reset: 0\r\ncluster.grpcsample.lb_healthy_panic: 0\r\ncluster.grpcsample.lb_local_cluster_not_ok: 0\r\ncluster.grpcsample.lb_recalculate_zone_structures: 0\r\ncluster.grpcsample.lb_zone_cluster_too_small: 0\r\ncluster.grpcsample.lb_zone_no_capacity_left: 0\r\ncluster.grpcsample.lb_zone_number_differs: 0\r\ncluster.grpcsample.lb_zone_routing_all_directly: 0\r\ncluster.grpcsample.lb_zone_routing_cross_zone: 0\r\ncluster.grpcsample.lb_zone_routing_sampled: 0\r\ncluster.grpcsample.max_host_weight: 1\r\ncluster.grpcsample.membership_change: 1\r\ncluster.grpcsample.membership_healthy: 1\r\ncluster.grpcsample.membership_total: 1\r\ncluster.grpcsample.ssl.connection_error: 0\r\ncluster.grpcsample.ssl.fail_verify_cert_hash: 0\r\ncluster.grpcsample.ssl.fail_verify_error: 1\r\ncluster.grpcsample.ssl.fail_verify_no_cert: 0\r\ncluster.grpcsample.ssl.fail_verify_san: 0\r\ncluster.grpcsample.ssl.handshake: 0\r\ncluster.grpcsample.ssl.no_certificate: 0\r\ncluster.grpcsample.update_attempt: 772\r\ncluster.grpcsample.update_failure: 0\r\ncluster.grpcsample.update_success: 772\r\ncluster.grpcsample.upstream_cx_active: 0\r\ncluster.grpcsample.upstream_cx_close_notify: 0\r\ncluster.grpcsample.upstream_cx_connect_fail: 1\r\ncluster.grpcsample.upstream_cx_connect_timeout: 0\r\ncluster.grpcsample.upstream_cx_destroy: 0\r\ncluster.grpcsample.upstream_cx_destroy_local: 0\r\ncluster.grpcsample.upstream_cx_destroy_local_with_active_rq: 0\r\ncluster.grpcsample.upstream_cx_destroy_remote: 0\r\ncluster.grpcsample.upstream_cx_destroy_remote_with_active_rq: 1\r\ncluster.grpcsample.upstream_cx_destroy_with_active_rq: 1\r\ncluster.grpcsample.upstream_cx_http1_total: 0\r\ncluster.grpcsample.upstream_cx_http2_total: 1\r\ncluster.grpcsample.upstream_cx_max_requests: 0\r\ncluster.grpcsample.upstream_cx_none_healthy: 0\r\ncluster.grpcsample.upstream_cx_overflow: 0\r\ncluster.grpcsample.upstream_cx_protocol_error: 0\r\ncluster.grpcsample.upstream_cx_rx_bytes_buffered: 0\r\ncluster.grpcsample.upstream_cx_rx_bytes_total: 0\r\ncluster.grpcsample.upstream_cx_total: 1\r\ncluster.grpcsample.upstream_cx_tx_bytes_buffered: 0\r\ncluster.grpcsample.upstream_cx_tx_bytes_total: 0\r\ncluster.grpcsample.upstream_flow_control_paused_reading_total: 0\r\ncluster.grpcsample.upstream_flow_control_resumed_reading_total: 0\r\ncluster.grpcsample.upstream_rq_503: 1\r\ncluster.grpcsample.upstream_rq_5xx: 1\r\ncluster.grpcsample.upstream_rq_active: 0\r\ncluster.grpcsample.upstream_rq_cancelled: 0\r\ncluster.grpcsample.upstream_rq_maintenance_mode: 0\r\ncluster.grpcsample.upstream_rq_pending_active: 0\r\ncluster.grpcsample.upstream_rq_pending_failure_eject: 1\r\ncluster.grpcsample.upstream_rq_pending_overflow: 0\r\ncluster.grpcsample.upstream_rq_pending_total: 0\r\ncluster.grpcsample.upstream_rq_per_try_timeout: 0\r\ncluster.grpcsample.upstream_rq_retry: 0\r\ncluster.grpcsample.upstream_rq_retry_overflow: 0\r\ncluster.grpcsample.upstream_rq_retry_success: 0\r\ncluster.grpcsample.upstream_rq_rx_reset: 0\r\ncluster.grpcsample.upstream_rq_timeout: 0\r\ncluster.grpcsample.upstream_rq_total: 1\r\ncluster.grpcsample.upstream_rq_tx_reset: 0\r\ncluster_manager.cluster_added: 1\r\ncluster_manager.cluster_modified: 0\r\ncluster_manager.cluster_removed: 0\r\ncluster_manager.total_clusters: 1\r\nfilesystem.flushed_by_timer: 381\r\nfilesystem.reopen_failed: 0\r\nfilesystem.write_buffered: 2\r\nfilesystem.write_completed: 2\r\nfilesystem.write_total_buffered: 0\r\nhttp.admin.downstream_cx_active: 1\r\nhttp.admin.downstream_cx_destroy: 6\r\nhttp.admin.downstream_cx_destroy_active_rq: 0\r\nhttp.admin.downstream_cx_destroy_local: 4\r\nhttp.admin.downstream_cx_destroy_local_active_rq: 0\r\nhttp.admin.downstream_cx_destroy_remote: 2\r\nhttp.admin.downstream_cx_destroy_remote_active_rq: 0\r\nhttp.admin.downstream_cx_drain_close: 0\r\nhttp.admin.downstream_cx_http1_active: 1\r\nhttp.admin.downstream_cx_http1_total: 6\r\nhttp.admin.downstream_cx_http2_active: 0\r\nhttp.admin.downstream_cx_http2_total: 0\r\nhttp.admin.downstream_cx_idle_timeout: 0\r\nhttp.admin.downstream_cx_protocol_error: 4\r\nhttp.admin.downstream_cx_rx_bytes_buffered: 98\r\nhttp.admin.downstream_cx_rx_bytes_total: 2264\r\nhttp.admin.downstream_cx_ssl_active: 0\r\nhttp.admin.downstream_cx_ssl_total: 0\r\nhttp.admin.downstream_cx_total: 7\r\nhttp.admin.downstream_cx_tx_bytes_buffered: 0\r\nhttp.admin.downstream_cx_tx_bytes_total: 8739\r\nhttp.admin.downstream_rq_2xx: 1\r\nhttp.admin.downstream_rq_3xx: 0\r\nhttp.admin.downstream_rq_4xx: 0\r\nhttp.admin.downstream_rq_5xx: 0\r\nhttp.admin.downstream_rq_active: 1\r\nhttp.admin.downstream_rq_http1_total: 2\r\nhttp.admin.downstream_rq_http2_total: 0\r\nhttp.admin.downstream_rq_non_relative_path: 0\r\nhttp.admin.downstream_rq_response_before_rq_complete: 0\r\nhttp.admin.downstream_rq_rx_reset: 0\r\nhttp.admin.downstream_rq_total: 2\r\nhttp.admin.downstream_rq_tx_reset: 0\r\nhttp.admin.failed_generate_uuid: 0\r\nhttp.admin.tracing.tracing.client_enabled: 0\r\nhttp.admin.tracing.tracing.health_check: 0\r\nhttp.admin.tracing.tracing.not_traceable: 0\r\nhttp.admin.tracing.tracing.random_sampling: 0\r\nhttp.admin.tracing.tracing.service_forced: 0\r\nhttp.async-client.no_cluster: 0\r\nhttp.async-client.no_route: 0\r\nhttp.async-client.rq_redirect: 0\r\nhttp.async-client.rq_total: 0\r\nhttp.router.downstream_cx_active: 0\r\nhttp.router.downstream_cx_destroy: 17\r\nhttp.router.downstream_cx_destroy_active_rq: 0\r\nhttp.router.downstream_cx_destroy_local: 0\r\nhttp.router.downstream_cx_destroy_local_active_rq: 0\r\nhttp.router.downstream_cx_destroy_remote: 17\r\nhttp.router.downstream_cx_destroy_remote_active_rq: 0\r\nhttp.router.downstream_cx_drain_close: 0\r\nhttp.router.downstream_cx_http1_active: 0\r\nhttp.router.downstream_cx_http1_total: 0\r\nhttp.router.downstream_cx_http2_active: 0\r\nhttp.router.downstream_cx_http2_total: 1\r\nhttp.router.downstream_cx_idle_timeout: 0\r\nhttp.router.downstream_cx_protocol_error: 0\r\nhttp.router.downstream_cx_rx_bytes_buffered: 0\r\nhttp.router.downstream_cx_rx_bytes_total: 186\r\nhttp.router.downstream_cx_ssl_active: 0\r\nhttp.router.downstream_cx_ssl_total: 17\r\nhttp.router.downstream_cx_total: 17\r\nhttp.router.downstream_cx_tx_bytes_buffered: 0\r\nhttp.router.downstream_cx_tx_bytes_total: 178\r\nhttp.router.downstream_rq_2xx: 0\r\nhttp.router.downstream_rq_3xx: 0\r\nhttp.router.downstream_rq_4xx: 0\r\nhttp.router.downstream_rq_5xx: 1\r\nhttp.router.downstream_rq_active: 0\r\nhttp.router.downstream_rq_http1_total: 0\r\nhttp.router.downstream_rq_http2_total: 1\r\nhttp.router.downstream_rq_non_relative_path: 0\r\nhttp.router.downstream_rq_response_before_rq_complete: 0\r\nhttp.router.downstream_rq_rx_reset: 0\r\nhttp.router.downstream_rq_total: 1\r\nhttp.router.downstream_rq_tx_reset: 0\r\nhttp.router.failed_generate_uuid: 0\r\nhttp.router.no_cluster: 0\r\nhttp.router.no_route: 0\r\nhttp.router.rq_redirect: 0\r\nhttp.router.rq_total: 1\r\nhttp.router.tracing.client_enabled: 0\r\nhttp.router.tracing.health_check: 0\r\nhttp.router.tracing.not_traceable: 0\r\nhttp.router.tracing.random_sampling: 0\r\nhttp.router.tracing.service_forced: 0\r\nhttp2.header_overflow: 0\r\nhttp2.headers_cb_no_stream: 0\r\nhttp2.rx_reset: 0\r\nhttp2.trailers: 0\r\nhttp2.tx_reset: 0\r\nlistener.0.0.0.0_80.downstream_cx_active: 0\r\nlistener.0.0.0.0_80.downstream_cx_destroy: 17\r\nlistener.0.0.0.0_80.downstream_cx_proxy_proto_error: 0\r\nlistener.0.0.0.0_80.downstream_cx_total: 17\r\nlistener.0.0.0.0_80.ssl.ciphers.ECDHE-RSA-AES128-GCM-SHA256: 1\r\nlistener.0.0.0.0_80.ssl.connection_error: 15\r\nlistener.0.0.0.0_80.ssl.fail_verify_cert_hash: 0\r\nlistener.0.0.0.0_80.ssl.fail_verify_error: 1\r\nlistener.0.0.0.0_80.ssl.fail_verify_no_cert: 0\r\nlistener.0.0.0.0_80.ssl.fail_verify_san: 0\r\nlistener.0.0.0.0_80.ssl.handshake: 1\r\nlistener.0.0.0.0_80.ssl.no_certificate: 1\r\nlistener.admin.downstream_cx_active: 1\r\nlistener.admin.downstream_cx_destroy: 6\r\nlistener.admin.downstream_cx_proxy_proto_error: 0\r\nlistener.admin.downstream_cx_total: 7\r\nlistener_manager.listener_added: 1\r\nlistener_manager.listener_create_failure: 0\r\nlistener_manager.listener_modified: 0\r\nlistener_manager.listener_removed: 0\r\nlistener_manager.total_listeners_active: 1\r\nlistener_manager.total_listeners_draining: 0\r\nlistener_manager.total_listeners_warming: 0\r\nserver.days_until_first_cert_expiring: 3400\r\nserver.live: 1\r\nserver.memory_allocated: 1792520\r\nserver.memory_heap_size: 4194304\r\nserver.parent_connections: 0\r\nserver.total_connections: 0\r\nserver.uptime: 3859\r\nserver.version: 2907298\r\nserver.watchdog_mega_miss: 0\r\nserver.watchdog_miss: 14\r\nstats.overflow: 0\r\n```"
      },
      {
        "user": "leon-g-xu",
        "created_at": "2017-08-04T21:49:22Z",
        "body": "cluster dump:\r\n```\r\ngrpcsample::default_priority::max_connections::1024\r\ngrpcsample::default_priority::max_pending_requests::1024\r\ngrpcsample::default_priority::max_requests::1024\r\ngrpcsample::default_priority::max_retries::3\r\ngrpcsample::high_priority::max_connections::1024\r\ngrpcsample::high_priority::max_pending_requests::1024\r\ngrpcsample::high_priority::max_requests::1024\r\ngrpcsample::high_priority::max_retries::3\r\ngrpcsample::172.25.0.2:50050::cx_active::0\r\ngrpcsample::172.25.0.2:50050::cx_connect_fail::1\r\ngrpcsample::172.25.0.2:50050::cx_total::1\r\ngrpcsample::172.25.0.2:50050::rq_active::0\r\ngrpcsample::172.25.0.2:50050::rq_timeout::0\r\ngrpcsample::172.25.0.2:50050::rq_total::1\r\ngrpcsample::172.25.0.2:50050::health_flags::healthy\r\ngrpcsample::172.25.0.2:50050::weight::1\r\ngrpcsample::172.25.0.2:50050::zone::\r\ngrpcsample::172.25.0.2:50050::canary::false\r\ngrpcsample::172.25.0.2:50050::success_rate::-1\r\n```"
      },
      {
        "user": "mattklein123",
        "created_at": "2017-08-07T04:54:45Z",
        "body": "```\r\ncluster.grpcsample.ssl.fail_verify_error: 1\r\ncluster.grpcsample.upstream_cx_connect_fail: 1\r\n```\r\n\r\nI would check your TLS settings."
      },
      {
        "user": "mattklein123",
        "created_at": "2017-08-08T03:21:34Z",
        "body": "Closing. Feel free to reopen if you have additional questions and/or suspect a bug in Envoy."
      }
    ]
  },
  {
    "number": 1189,
    "title": "TLS between Envoys is not working",
    "created_at": "2017-06-29T00:34:06Z",
    "closed_at": "2017-06-29T16:27:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1189",
    "body": "Hi,\r\n\r\nWe are trying to initiate a TCP connection through two Envoys, like so:\r\n```\r\nClient (nc) --> \"Client\" Envoy ==> \"Server\" Envoy --> Server (nc -l)\r\n```\r\nWhere the `==>` arrow is TLS (server certs, no client certs).\r\n\r\nThe client connection looks like\r\n```\r\ndate | nc -v 127.0.0.1 10000\r\n```\r\n\r\nAnd the server runs as\r\n```\r\nnc -l -k 8080\r\n```\r\n\r\nWhen trying to connect, we get inconsistent behavior.   Sometimes the date appears on the server side `nc`, but often it does not.\r\n\r\nHowever, the client side `nc` always reports \r\n```\r\nConnection to 127.0.0.1 10000 port [tcp/webmin] succeeded!\r\n```\r\n\r\n\r\nOn the server side, the Envoy debug logs are:\r\n```\r\n[2017-06-29 00:32:55.981][116][info][main] source/server/connection_handler_impl.cc:109] [C1] new connection\r\n[2017-06-29 00:32:55.981][116][info][filter] source/common/filter/tcp_proxy.cc:117] [C1] new tcp proxy session\r\n[2017-06-29 00:32:55.981][116][debug][filter] source/common/filter/tcp_proxy.cc:133] [C1] Creating connection to cluster service_local\r\n[2017-06-29 00:32:55.981][116][debug][connection] source/common/network/connection_impl.cc:418] [C2] connecting to 127.0.0.1:8080\r\n[2017-06-29 00:32:55.981][116][debug][connection] source/common/network/connection_impl.cc:427] [C2] connection in progress\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/ssl/connection_impl.cc:128] [C1] handshake error: 5\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:341] [C1] remote close\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:135] [C1] closing socket: 1\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:102] [C2] closing data_to_write=0 type=1\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:135] [C2] closing socket: 2\r\n[2017-06-29 00:32:55.983][116][info][main] source/server/connection_handler_impl.cc:51] [C1] adding to cleanup list\r\n```\r\n\r\nAnd the client side, the Envoy debug logs are:\r\n\r\n```\r\n[2017-06-29 00:32:55.980][894][info][main] source/server/connection_handler_impl.cc:109] [C1] new connection\r\n[2017-06-29 00:32:55.980][894][info][filter] source/common/filter/tcp_proxy.cc:117] [C1] new tcp proxy session\r\n[2017-06-29 00:32:55.980][894][debug][filter] source/common/filter/tcp_proxy.cc:133] [C1] Creating connection to cluster service_remote\r\n[2017-06-29 00:32:55.980][894][debug][connection] source/common/network/connection_impl.cc:418] [C2] connecting to 172.17.0.2:10001\r\n[2017-06-29 00:32:55.980][894][debug][connection] source/common/network/connection_impl.cc:427] [C2] connection in progress\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:341] [C1] remote close\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:135] [C1] closing socket: 1\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:102] [C2] closing data_to_write=29 type=1\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/ssl/connection_impl.cc:128] [C2] handshake error: 2\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:135] [C2] closing socket: 2\r\n[2017-06-29 00:32:55.981][894][info][main] source/server/connection_handler_impl.cc:51] [C1] adding to cleanup list\r\n```\r\n\r\n\r\n\r\nConfig for \"server\" Envoy:\r\n```json\r\n{\r\n    \"listeners\": [{\r\n        \"address\": \"tcp://0.0.0.0:10001\",\r\n        \"filters\": [{\r\n            \"type\": \"read\",\r\n            \"name\": \"tcp_proxy\",\r\n            \"config\": {\r\n              \"stat_prefix\": \"ingress_tcp\",\r\n              \"route_config\": {\r\n                  \"routes\": [{\r\n                    \"cluster\": \"service_local\"\r\n               }]\r\n            }\r\n          }\r\n        }],\r\n        \"ssl_context\": {\r\n          \"cert_chain_file\": \"certs/server.crt\",\r\n          \"private_key_file\": \"certs/server.key\"\r\n        }\r\n    }],\r\n    \"admin\": {\r\n        \"access_log_path\": \"/tmp/admin_access.log\",\r\n        \"address\": \"tcp://127.0.0.1:9901\"\r\n    },\r\n    \"cluster_manager\": {\r\n        \"clusters\": [{\r\n            \"name\": \"service_local\",\r\n            \"connect_timeout_ms\": 2500,\r\n            \"type\": \"static\",\r\n            \"lb_type\": \"round_robin\",\r\n            \"hosts\": [{\r\n                \"url\": \"tcp://127.0.0.1:8080\"\r\n            }]\r\n        }]\r\n    }\r\n}\r\n```\r\n\r\nConfig for \"client\" Envoy:\r\n```json\r\n{\r\n    \"listeners\": [{\r\n        \"address\": \"tcp://0.0.0.0:10000\",\r\n        \"filters\": [{\r\n            \"type\": \"read\",\r\n            \"name\": \"tcp_proxy\",\r\n            \"config\": {\r\n                \"stat_prefix\": \"ingress_tcp\",\r\n                \"route_config\": {\r\n                    \"routes\": [{\r\n                            \"cluster\": \"service_remote\"\r\n                    }]\r\n                }\r\n            }\r\n        }]\r\n    }],\r\n    \"admin\": {\r\n        \"access_log_path\": \"/tmp/admin_access.log\",\r\n        \"address\": \"tcp://127.0.0.1:9901\"\r\n    },\r\n    \"cluster_manager\": {\r\n        \"clusters\": [{\r\n            \"name\": \"service_remote\",\r\n            \"connect_timeout_ms\": 2500,\r\n            \"type\": \"static\",\r\n            \"lb_type\": \"round_robin\",\r\n            \"hosts\": [{\r\n                \"url\": \"tcp://172.17.0.2:10001\"\r\n            }],\r\n            \"ssl_context\": {\r\n              \"ca_cert_file\": \"certs/ca.crt\"\r\n            }\r\n        }]\r\n    }\r\n}\r\n```\r\n\r\n\r\nAny idea why this setup doesn't work consistently?\r\n\r\nThanks,\r\nAngela and @rosenhouse\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1189/comments",
    "author": "angelachin",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-06-29T14:42:37Z",
        "body": "When you run `date | nc -v 127.0.0.1 10000` the connection will be closed after writing the data. Envoy detects this close and terminates the proxied connection leading to a timing issue. Basically, Envoy does not currently support this case in the TCP proxy (accept client connection, write data, and guarantee that all data gets written if client closes connection). \r\n\r\nIs this a real use case or just a test? If just a test, try `telnet` or `nc` without a pipe so you can keep the connection open."
      },
      {
        "user": "angelachin",
        "created_at": "2017-06-29T16:27:58Z",
        "body": "Ok, that makes sense. It was just a test-- works fine when we don't pipe. Thanks!\r\n\r\n- Angela and @rosenhouse"
      }
    ]
  },
  {
    "number": 1146,
    "title": "What suggest to  deploy a 'service pod' on non-docker environment?",
    "created_at": "2017-06-20T10:31:19Z",
    "closed_at": "2017-06-26T14:44:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1146",
    "body": "We deploy a 'service pod' with a service instance and the sidecar envoy, and other surrounding stuff.\r\n\r\nServicePod => |Service|Envoy |\r\n\r\nIt's very easy to deploy in k8s or other docker situation, though, not so easy in non-docker machine or vm. We have develop a tool to patch everything required in the service home directory. However,  It's to too weird, I think.\r\n\r\nAny good solutions?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1146/comments",
    "author": "spgyip",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-06-20T16:14:21Z",
        "body": "@anticpp this is a bit out of scope for what the Envoy project can provide. It really comes down to the specifics of your deployment (e.g., puppet vs. salt, how you do deploys, etc.). cc @louiscryan"
      },
      {
        "user": "louiscryan",
        "created_at": "2017-06-20T18:59:30Z",
        "body": "Are you using anything for deployment / orchestration? Chef, Puppet etc ?"
      },
      {
        "user": "spgyip",
        "created_at": "2017-06-21T02:18:45Z",
        "body": "No, we are not yet using any deployment utilities. It will be in our future plan."
      },
      {
        "user": "spgyip",
        "created_at": "2017-06-21T02:22:24Z",
        "body": "The point is, we have implemented an initial version to our project, we don't want to introduce a 'heavy' deployment in our system at this moment. So, we are looking forward a relative 'light' solution."
      },
      {
        "user": "mattklein123",
        "created_at": "2017-06-26T14:44:24Z",
        "body": "Closing this for now as we have no plans to take on this type of ops work in the primary envoy project."
      }
    ]
  },
  {
    "number": 1092,
    "title": "build error on centos 6",
    "created_at": "2017-06-12T18:08:45Z",
    "closed_at": "2017-06-26T14:46:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1092",
    "body": "I tried to build envoy-1.3.0 release on centos 6, but some third party failed:\r\n\r\n```\r\n[fliu@jumphost002 envoy-1.3.0]$ bazel version\r\nBuild label: 0.5.1- (@non-git)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Sun Jun 11 22:11:06 2017 (1497219066)\r\nBuild timestamp: 1497219066\r\nBuild timestamp as int: 1497219066\r\n[fliu@jumphost002 envoy-1.3.0]$ gcc --version\r\ngcc (GCC) 6.2.1 20160916 (Red Hat 6.2.1-3)\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n[fliu@jumphost002 envoy-1.3.0]$ bazel fetch //source/...\r\nWARNING: /home/fliu/src/envoy/envoy-1.3.0/bazel/repositories.bzl:17:5: Fetching external dependencies...\r\nWARNING: /home/fliu/src/envoy/envoy-1.3.0/bazel/repositories.bzl:30:5: External dep build exited with return code: 256.\r\nWARNING: /home/fliu/src/envoy/envoy-1.3.0/bazel/repositories.bzl:31:5: \r\nWARNING: /home/fliu/src/envoy/envoy-1.3.0/bazel/repositories.bzl:32:5: Process terminated by signal 15; also encountered an error while attempting to retrieve output.\r\nERROR: /home/fliu/src/envoy/envoy-1.3.0/bazel/repositories.bzl:128:13: no such package '@envoy_deps//': External dep build failed and referenced by '//external:nghttp2'.\r\nERROR: /home/fliu/src/envoy/envoy-1.3.0/bazel/repositories.bzl:128:13: no such package '@envoy_deps//': External dep build failed and referenced by '//external:spdlog'.\r\nERROR: Evaluation of query \"deps(//source/...)\" failed: errors were encountered while computing transitive closure.\r\n[fliu@jumphost002 envoy-1.3.0]$ \r\n\r\n```\r\nNow if I try to build envoy static, it gives git error\r\n```\r\n[fliu@jumphost002 envoy-1.3.0]$ bazel build //source/exe:envoy-static\r\nWARNING: /home/fliu/src/envoy/envoy-1.3.0/bazel/repositories.bzl:17:5: Fetching external dependencies...\r\nWARNING: /home/fliu/src/envoy/envoy-1.3.0/bazel/repositories.bzl:30:5: External dep build exited with return code: 0.\r\nWARNING: /home/fliu/src/envoy/envoy-1.3.0/bazel/repositories.bzl:31:5: make: Entering directory '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps'\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/cares.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/backward.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/libevent.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/gcovr.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/googletest.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/gperftools.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/http-parser.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/lightstep.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/nghttp2.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/protobuf.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/rapidjson.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/spdlog.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/boringssl.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/tclap.dep' is up to date.\r\nmake: Leaving directory '/home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps'\r\n\r\nreal\t0m0.116s\r\nuser\t0m0.012s\r\nsys\t0m0.015s\r\n.\r\nWARNING: /home/fliu/src/envoy/envoy-1.3.0/bazel/repositories.bzl:32:5: External dependency cache directory /home/fliu/.cache/bazel/_bazel_fliu/8b151bc5d038aae775aa9cf563410cae/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d\r\n.\r\nINFO: Found 1 target...\r\nERROR: Process exited with status 1: Process exited with status 1.\r\nfatal: Not a git repository (or any of the parent directories): .git\r\nTarget //source/exe:envoy-static failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3.468s, Critical Path: 0.03s\r\n[fliu@jumphost002 envoy-1.3.0]$ \r\n```\r\n\r\nNext I tried the git master branch instead of 1.3.0 release.\r\nexternal third party build fails with different errors\r\n```\r\n[fliu@jumphost002 envoy]$ bazel fetch //source/...\r\n.......................\r\nWARNING: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:17:5: Fetching external dependencies...\r\nWARNING: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:30:5: External dep build exited with return code: 256.\r\nWARNING: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:31:5: \r\nWARNING: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:32:5: Process terminated by signal 15; also encountered an error while attempting to retrieve output.\r\nERROR: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:165:13: no such package '@envoy_deps//': External dep build failed and referenced by '//external:rapidjson'.\r\nERROR: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:165:13: no such package '@envoy_deps//': External dep build failed and referenced by '//external:ssl'.\r\nERROR: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:165:13: no such package '@envoy_deps//': External dep build failed and referenced by '//external:spdlog'.\r\nERROR: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:165:13: no such package '@envoy_deps//': External dep build failed and referenced by '//external:tclap'.\r\nERROR: Evaluation of query \"deps(//source/...)\" failed: errors were encountered while computing transitive closure.\r\n[fliu@jumphost002 envoy]$ \r\n```\r\n\r\nbut this time, the envoy-static build seems to complete\r\n```\r\n[fliu@jumphost002 envoy]$ bazel build //source/exe:envoy-static\r\nWARNING: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:17:5: Fetching external dependencies...\r\nWARNING: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:30:5: External dep build exited with return code: 0.\r\nWARNING: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:31:5: make: Entering directory '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps'\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/cares.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/backward.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/libevent.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/gcovr.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/googletest.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/gperftools.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/http-parser.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/lightstep.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/nghttp2.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/protobuf.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/rapidjson.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/spdlog.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/boringssl.dep' is up to date.\r\nmake: '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d/tclap.dep' is up to date.\r\nmake: Leaving directory '/home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps'\r\n\r\nreal\t0m0.116s\r\nuser\t0m0.014s\r\nsys\t0m0.013s\r\n.\r\nWARNING: /home/fliu/src/envoy/envoy/bazel/repositories.bzl:32:5: External dependency cache directory /home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/external/envoy_deps_cache_48f656afceeef09c874ef0ab944d102d\r\n.\r\nINFO: Found 1 target...\r\nTarget //source/exe:envoy-static up-to-date:\r\n  bazel-bin/source/exe/envoy-static\r\nINFO: Elapsed time: 340.796s, Critical Path: 12.98s\r\n[fliu@jumphost002 envoy]$ \r\n[fliu@jumphost002 envoy]$ ls -l /home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/execroot/envoy/bazel-out/local-fastbuild/bin/source/exe/envoy-static\r\n-r-xr-xr-x 1 fliu staff 16051104 Jun 12 18:04 /home/fliu/.cache/bazel/_bazel_fliu/2afcc55f8d999d11560b483bc7eefdd4/execroot/envoy/bazel-out/local-fastbuild/bin/source/exe/envoy-static\r\n\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1092/comments",
    "author": "gfrankliu",
    "comments": [
      {
        "user": "gfrankliu",
        "created_at": "2017-06-12T18:11:01Z",
        "body": "Two questions:\r\n1) is bazel build not supported with the release version 1.3.0?\r\n2) are those third party fetch/build errors harmless and should they be ignored?"
      },
      {
        "user": "mattklein123",
        "created_at": "2017-06-14T04:29:54Z",
        "body": "In general the only officially supported builds are what we provide in CI. For everything else you are on your own. But for RH stuff I think we are going to try to get some help from RH sometime soon so stay tuned. cc @christian-posta "
      },
      {
        "user": "mattklein123",
        "created_at": "2017-06-26T14:46:44Z",
        "body": "Closing for now as we don't have the resources to help with build beyond the CI setup."
      }
    ]
  },
  {
    "number": 874,
    "title": "CDS API is not called when setting up cluster manager",
    "created_at": "2017-05-02T00:24:11Z",
    "closed_at": "2017-05-02T17:28:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/874",
    "body": "Im attempting to setup the CDS API with envoy. I have succeeded with static config first and see my static configs work fine.\r\n\r\nVersion: /usr/local/bin/envoy  version: 3bc959c440a7e6903110faf4fd53942ca2ac8a07/Clean/RELEASE\r\n\r\nI have confirmed that SDS and RDS API both make requests to service.\r\n\r\nPlease see the following config as what I am using:\r\n\r\n```json\r\n{\r\n  \"listeners\": [\r\n    {\r\n      \"address\": \"tcp://0.0.0.0:443\",\r\n      \"filters\": [\r\n        {\r\n          \"type\": \"read\",\r\n          \"name\": \"http_connection_manager\",\r\n          \"config\": {\r\n            \"codec_type\": \"auto\",\r\n            \"stat_prefix\": \"ingress_http\",\r\n            \"access_log\": [\r\n              {\r\n                \"path\": \"/var/log/envoy.log\"\r\n              }\r\n            ],\r\n            \"rds\": {\r\n              \"cluster\": \"sds\",\r\n              \"route_config_name\": \"envoyCluster\"\r\n            },\r\n            \"filters\": [\r\n              {\r\n                \"type\": \"decoder\",\r\n                \"name\": \"router\",\r\n                \"config\": {}\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ],\r\n  \"admin\": {\r\n    \"access_log_path\": \"/dev/null\",\r\n    \"address\": \"tcp://0.0.0.0:8001\"\r\n  },\r\n  \"cluster_manager\": {\r\n    \"clusters\": [\r\n      {\r\n        \"name\": \"MyService\",\r\n        \"service_name\": \"MyService\",\r\n        \"connect_timeout_ms\": 250,\r\n        \"type\": \"sds\",\r\n        \"lb_type\": \"round_robin\",\r\n        \"features\": \"http2\",\r\n        \"ssl_context\": {\r\n          \"alpn_protocols\": \"h2,http/1.1\",\r\n          \"private_key_file\": \"/etc/ssl/private/some_pkey.pem\",\r\n          \"cert_chain_file\": \"/etc/ssl/certs/some_cert.pem\"\r\n        }\r\n\r\n      }\r\n    ],\r\n    \"cds\": {\r\n      \"cluster\": {\r\n        \"name\": \"cds\",\r\n        \"connect_timeout_ms\": 250,\r\n        \"type\": \"strict_dns\",\r\n        \"lb_type\": \"round_robin\",\r\n        \"hosts\": [\r\n          {\r\n            \"url\": \"tcp://some.host:8085\"\r\n          }\r\n        ]\r\n      },\r\n      \"refresh_delay_ms\": 2\r\n    },\r\n    \"sds\": {\r\n      \"refresh_delay_ms\": 2000,\r\n      \"cluster\": {\r\n        \"name\": \"sds\",\r\n        \"connect_timeout_ms\": 250,\r\n        \"type\": \"strict_dns\",\r\n        \"lb_type\": \"round_robin\",\r\n        \"hosts\": [\r\n          {\r\n            \"url\": \"tcp://192.168.99.100:8085\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}```\r\n\r\nThe use case I am after is to dynamically modify the cluster as new services are made available. SDS gets me half way there with host:ip membership but knowledge of the cluster I am after.\r\n\r\nAs a second issue I notice I can not use CDS with zero clusters:[]. I actually had to define 1 to get the RDS and SDS api's to call.\r\n\r\nAny guidance or examples or suggestions on what is wrong and how to effectively enable / debug CDS API queries.\r\n\r\nThanks and awesome project here!\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/874/comments",
    "author": "robertfreund",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-05-02T14:52:37Z",
        "body": "I'm guessing `tcp://some.host:8085` is not the host you want for the CDS server. /clusters and /stats admin output will typically help you debug."
      },
      {
        "user": "robertfreund",
        "created_at": "2017-05-02T16:54:22Z",
        "body": "Ok thanks. I removed actual host. I tried with \"static\" and \"192.168.99.100:8085\" as well and see requests made. In my desired config RDS , SDS, CDS all point at same server. I tried \"static\" \"192.168.99.100:8085\" for all. Again the RDS and SDS properly request. I can't seem to get CDS to make any requests."
      },
      {
        "user": "mattklein123",
        "created_at": "2017-05-02T16:57:31Z",
        "body": "Sorry don't know. Will need complete final config, and admin output of /clusters and /stats to help further. Or you can ask for help in gitter."
      },
      {
        "user": "robertfreund",
        "created_at": "2017-05-02T17:28:52Z",
        "body": "Thanks! /clusters helped. For completeness here is the final config that worked for me to get SDS,RDS,CDS all working. \r\n\r\n```json\r\n{\r\n  \"listeners\": [\r\n    {\r\n      \"address\": \"tcp://0.0.0.0:9443\",\r\n      \"filters\": [\r\n        {\r\n          \"type\": \"read\",\r\n          \"name\": \"http_connection_manager\",\r\n          \"config\": {\r\n            \"codec_type\": \"auto\",\r\n            \"stat_prefix\": \"ingress_http\",\r\n            \"access_log\": [\r\n              {\r\n                \"path\": \"/var/log/envoy.log\"\r\n              }\r\n            ],\r\n            \"rds\": {\r\n              \"cluster\": \"cds\",\r\n              \"route_config_name\": \"envoyCluster\",\r\n              \"refresh_delay_ms\": 2000\r\n            },\r\n            \"filters\": [\r\n              {\r\n                \"type\": \"decoder\",\r\n                \"name\": \"router\",\r\n                \"config\": {}\r\n              }\r\n            ]\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  ],\r\n  \"admin\": {\r\n    \"access_log_path\": \"/dev/null\",\r\n    \"address\": \"tcp://0.0.0.0:8001\"\r\n  },\r\n  \"cluster_manager\": {\r\n    \"clusters\": [],\r\n    \"cds\": {\r\n      \"cluster\": {\r\n        \"name\": \"cds\",\r\n        \"connect_timeout_ms\": 250,\r\n        \"type\": \"static\",\r\n        \"lb_type\": \"round_robin\",\r\n        \"hosts\": [\r\n          {\r\n            \"url\": \"tcp://192.168.99.100:8085\"\r\n          }\r\n        ]\r\n      },\r\n      \"refresh_delay_ms\": 2000\r\n    },\r\n    \"sds\": {\r\n      \"refresh_delay_ms\": 2000,\r\n      \"cluster\": {\r\n        \"name\": \"sds\",\r\n        \"connect_timeout_ms\": 250,\r\n        \"type\": \"strict_dns\",\r\n        \"lb_type\": \"round_robin\",\r\n        \"hosts\": [\r\n          {\r\n            \"url\": \"tcp://192.168.99.100:8085\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n```\r\ntime=\"2017-05-02T17:23:35Z\" level=info msg=\"ListRoutes for: serviceCluster:\\\"envoy_cluster\\\" serviceNode:\\\"envoy_node\\\" routeConfigName:\\\"envoyCluster\\\"\r\ntime=\"2017-05-02T17:23:35Z\" level=info msg=\"Got request for: /v1/clusters/envoy_cluster/envoy_node\"\r\n\r\ntime=\"2017-05-02T17:23:35Z\" level=info msg=\"ListClusters for: serviceCluster:\\\"envoy_cluster\\\" serviceNode:\\\"envoy_node\\\" \"\r\ntime=\"2017-05-02T17:23:38Z\" level=info msg=\"Got request for: /v1/clusters/envoy_cluster/envoy_node\" \r\n\r\ntime=\"2017-05-02T17:23:37Z\" level=info msg=\"Got request for: /v1/registration/MyService\" \r\ntime=\"2017-05-02T17:23:37Z\" level=info msg=\"ListHosts for: serviceName:\\\"MyService\\\" \"  \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 547,
    "title": "Error Building from sources",
    "created_at": "2017-03-08T19:20:33Z",
    "closed_at": "2017-03-09T01:21:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/547",
    "body": "hi,\r\n\r\nI am trying to build envoy on Centos7, post installing deps, \r\n\r\n`cmake -L`\r\n=========\r\n\r\n```\r\ncmake -L\r\n-- cotire 1.7.8 loaded.\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/vagrant/envoy-src/envoy\r\n-- Cache values\r\nCLANG-FORMAT:FILEPATH=clang-format\r\nCMAKE_BUILD_TYPE:STRING=\r\nCMAKE_INSTALL_PREFIX:PATH=/usr/local\r\nCOTIRE_ADDITIONAL_PREFIX_HEADER_IGNORE_EXTENSIONS:STRING=inc;inl;ipp\r\nCOTIRE_ADDITIONAL_PREFIX_HEADER_IGNORE_PATH:STRING=\r\nCOTIRE_DEBUG:BOOL=OFF\r\nCOTIRE_MAXIMUM_NUMBER_OF_UNITY_INCLUDES:STRING=0\r\nCOTIRE_MINIMUM_NUMBER_OF_TARGET_SOURCES:STRING=3\r\nCOTIRE_UNITY_SOURCE_EXCLUDE_EXTENSIONS:STRING=m;mm\r\nCOTIRE_VERBOSE:BOOL=OFF\r\nENVOY_CARES_INCLUDE_DIR:FILEPATH=\r\nENVOY_CODE_COVERAGE:BOOL=OFF\r\nENVOY_COTIRE_MODULE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty/cotire-cotire-1.7.8/CMake\r\nENVOY_DEBUG:BOOL=OFF\r\nENVOY_EXE_EXTRA_LINKER_FLAGS:STRING=-L/home/vagrant/envoy-src/thirdparty_build/lib\r\nENVOY_GCOVR:FILEPATH=/home/vagrant/envoy-src/thirdparty/gcovr-3.3/scripts/gcovr\r\nENVOY_GCOVR_EXTRA_ARGS:STRING=-e test/* -e build/*\r\nENVOY_GMOCK_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty_build/include\r\nENVOY_GPERFTOOLS_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty_build/include\r\nENVOY_GTEST_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty_build/include\r\nENVOY_HTTP_PARSER_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty_build/include\r\nENVOY_LIBEVENT_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty_build/include\r\nENVOY_LIGHTSTEP_TRACER_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty_build/include\r\nENVOY_NGHTTP2_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty_build/include\r\nENVOY_OPENSSL_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty_build/include\r\nENVOY_PROTOBUF_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty_build/include\r\nENVOY_PROTOBUF_PROTOC:FILEPATH=/home/vagrant/envoy-src/thirdparty_build/bin/protoc\r\nENVOY_RAPIDJSON_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty/rapidjson-1.1.0/include\r\nENVOY_SANITIZE:BOOL=OFF\r\nENVOY_SPDLOG_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty/spdlog-0.11.0/include\r\nENVOY_STRIP:BOOL=ON\r\nENVOY_TCLAP_INCLUDE_DIR:FILEPATH=/home/vagrant/envoy-src/thirdparty/tclap-1.2.1/include\r\nENVOY_TCMALLOC:BOOL=ON\r\nENVOY_TEST_EXTRA_LINKER_FLAGS:STRING=-L/home/vagrant/envoy-src/thirdparty_build/lib\r\nENVOY_USE_CCACHE:BOOL=OFF\r\n```\r\n\r\nand triggering build using \r\n\r\n`make -j $NUM_CPUS envoy`, i get the following error.\r\n\r\n```\r\n/home/vagrant/envoy-src/envoy/source/common/event/file_event_impl.cc: In member function ‘virtual void Event::FileEventImpl::activate(uint32_t)’:\r\n/home/vagrant/envoy-src/envoy/source/common/event/file_event_impl.cc:28:24: error: ‘EV_CLOSED’ was not declared in this scope\r\n     libevent_events |= EV_CLOSED;\r\n                        ^\r\n/home/vagrant/envoy-src/envoy/source/common/event/file_event_impl.cc: In member function ‘void Event::FileEventImpl::assignEvents(uint32_t)’:\r\n/home/vagrant/envoy-src/envoy/source/common/event/file_event_impl.cc:40:54: error: ‘EV_CLOSED’ was not declared in this scope\r\n                    (events & FileReadyType::Closed ? EV_CLOSED : 0),\r\n                                                      ^\r\n/home/vagrant/envoy-src/envoy/source/common/event/file_event_impl.cc: In lambda function:\r\n/home/vagrant/envoy-src/envoy/source/common/event/file_event_impl.cc:52:29: error: ‘EV_CLOSED’ is not captured\r\n                  if (what & EV_CLOSED) {\r\n                             ^\r\n[ 15%] Building CXX object source/common/CMakeFiles/envoy-common.dir/event/libevent.cc.o\r\nmake[3]: *** [source/common/CMakeFiles/envoy-common.dir/event/file_event_impl.cc.o] Error 1\r\nmake[3]: *** Waiting for unfinished jobs....\r\nmake[2]: *** [source/common/CMakeFiles/envoy-common.dir/all] Error 2\r\nmake[1]: *** [source/exe/CMakeFiles/envoy.dir/rule] Error 2\r\nmake: *** [envoy] Error 2\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/547/comments",
    "author": "kameshsampath",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-03-09T01:21:37Z",
        "body": "We recently upgraded to libevent 2.1.8. Please upgrade your local dependency."
      }
    ]
  },
  {
    "number": 320,
    "title": "HTTP2 Load balancing affinity",
    "created_at": "2017-01-05T16:27:21Z",
    "closed_at": "2017-01-11T03:51:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/320",
    "body": "This could be a beginner question so I apologise.\r\n\r\nIt was not clear to me in the documentation how HTTP2 load balancing works. \r\n\r\nAs new HTTP2 requests come in from a single client on a HTTP2 connection to a envoty route does each client request continue to communicate with the original backend instance (say 7 of 9) or does each request within a client connection to Envoy get load balanced to all 9 backend instances of a cluster. \r\n\r\nIs this also configurable as I could imagine many architecture scenarios where different affinity makes sense",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/320/comments",
    "author": "andrewwebber",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-01-05T16:53:42Z",
        "body": "Requests get load balanced to all backends depending on the load balancing policy. Currently we do not support any consistent hashing load balancing policies, however we will support hash based load balancing for HTTP based on a header \"soon\" (~4-6 weeks)."
      },
      {
        "user": "andrewwebber",
        "created_at": "2017-01-05T17:00:31Z",
        "body": "This is great information, thank you.\r\n\r\nCurrently a bidirectional gRpc stream seems to simulate session affinity although it's application is very limited"
      },
      {
        "user": "mattklein123",
        "created_at": "2017-01-11T03:51:29Z",
        "body": "Going to close this out, let us know if you have further questions on this topic. "
      }
    ]
  },
  {
    "number": 240,
    "title": "envoy binary file size - currently 127MB",
    "created_at": "2016-11-22T19:52:25Z",
    "closed_at": "2016-12-13T19:05:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/240",
    "body": "Hi guys, \r\n\r\nI;m thinking about to use envoy in a kubernetes setup. \r\n\r\nOne of the challenges in such a setup is to keep the docker / rtk container image size small and therefore I was a bit surprised that the envoy binary is about 127 MB.\r\n\r\nHowever, I was able to \"shrink\" the binary down to about 8 MB using the ELF \"strip\" approach : \r\n\r\n`/tmp$ strip -S --strip-unneeded --remove-section=.note.gnu.gold-version --remove-section=.comment --remove-section=.note --remove-section=.note.gnu.build-id --remove-section=.note.ABI-tag envoy`\r\n\r\n-rwxr-xr-x 1 jj jj 127M Nov 22 20:17 envoy.orig\r\n-rwxr-xr-x 1 jj jj 7,7M Nov 22 20:18 envoy\r\n\r\nThe question is, what makes the binary such large ?\r\n\r\nThank you. \r\n\r\nCheers, \r\njj\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/240/comments",
    "author": "ramtej",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2016-11-22T19:56:35Z",
        "body": "The default build includes debug symbols and is statically linked. If you strip symbols that's what takes you down to 8MB or so. If you want to go down further than that you should dynamically link against system libraries.\r\n\r\nFWIW, we haven't really focused very much on the build/package/install side of things. I'm hoping the community can help out there. Different deployments are going to need different kinds of compiles."
      },
      {
        "user": "mattklein123",
        "created_at": "2016-11-22T19:58:24Z",
        "body": "cc @enricoschiattarella @louiscryan I think you are already probably thinking about ^^^ not sure if you have any thoughts. "
      },
      {
        "user": "ramtej",
        "created_at": "2016-11-22T20:01:45Z",
        "body": "Hi Matt,\r\n\r\nthank you for your feedback !\r\n\r\nAgree, different deployment scenarios has different requirements. I also like the static linking approach, to keep things consistent and independent. \r\n\r\nAt least for my usecase the \"strip\" approach fine, but evtl. a kind of \"included\" solution will be better. \r\n\r\nCheers, \r\njj\r\n"
      },
      {
        "user": "moderation",
        "created_at": "2016-11-22T20:27:47Z",
        "body": "I think statically linked is important as if you wanted to run in a Docker container you could in theory create a minimal image using `from scratch`. I haven't tested this yet."
      },
      {
        "user": "ramtej",
        "created_at": "2016-11-22T20:34:23Z",
        "body": "yes, this is exactly what I;m doing and therefore the dynamic approach will not work. Also such a critical component should be self-contained. "
      },
      {
        "user": "mattklein123",
        "created_at": "2016-11-22T21:13:39Z",
        "body": "At the very least we can put in a cmake option to strip debug symbols. That is easy to do."
      },
      {
        "user": "mattklein123",
        "created_at": "2016-12-13T19:05:50Z",
        "body": "fixed"
      }
    ]
  },
  {
    "number": 185,
    "title": "Document the minimal interfaces for providing new discovery modes",
    "created_at": "2016-11-02T16:35:01Z",
    "closed_at": "2017-01-05T04:04:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/185",
    "body": "@mattklein123 as we discussed last night, if we could get some documentation that shows the bare minimum interface(s) for providing a new mode of discovery, that would be excellent! Preferably it would be something that is not mixed up with the DNS part, which is far more complicated. Having such documentation would make contributing new modes of discovery far easier. \r\n\r\nThanks in advance. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/185/comments",
    "author": "timperrett",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2016-11-02T16:57:59Z",
        "body": "Yup will do.\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2017-01-05T04:04:06Z",
        "body": "It seems like everyone is getting on the SDS/CDS/RDS bandwagon so I'm going to close this out. I would like to revisit general plugin documentation in depth at a later time."
      }
    ]
  },
  {
    "number": 35785,
    "title": "Add User-Agent to jwks http requests",
    "created_at": "2024-08-21T23:55:22Z",
    "closed_at": "2024-09-11T19:07:05Z",
    "labels": [
      "help wanted",
      "area/jwt_authn"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/35785",
    "body": "*Title*: Add User-Agent to jwks http requests\r\n\r\n*Description*:\r\nRequests to get the jwks url fail because the requests lack a user-agent header. This is a requirement from the IDP. I'm currently using Envoy Gateway in EKS. \r\n\r\nThe below request fails, I'd like to add a user-agent to get that working.\r\n\r\n```\r\n[2024-08-21 22:46:23.522][1][debug][router] [source/common/router/router.cc:750] [Tags: \"ConnectionId\":\"0\",\"StreamId\":\"17207847609171885090\"] router decoding headers:\r\n':path', '/path/oauth/jwks'\r\n':authority', 'auth.url.dev'\r\n':method', 'GET'\r\n':scheme', 'http'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '10.100.4.34'\r\n'x-envoy-expected-rq-timeout-ms', '10000'\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/35785/comments",
    "author": "yerkulees",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2024-08-23T20:39:03Z",
        "body": "cc @tyxia as code owner"
      },
      {
        "user": "Athishpranav2003",
        "created_at": "2024-08-30T05:01:34Z",
        "body": "I guess I can work on this issue\n@KBaichoo is this issue still open"
      },
      {
        "user": "Athishpranav2003",
        "created_at": "2024-08-30T05:04:26Z",
        "body": "Incase of programs what should we mention in the UA string? Some generic browser UA string or the go library we are using?"
      },
      {
        "user": "yerkulees",
        "created_at": "2024-08-30T13:59:13Z",
        "body": "For my case, the generic go browser UA would work.\r\n\r\nThank you for your time."
      },
      {
        "user": "tyxia",
        "created_at": "2024-09-05T02:26:09Z",
        "body": "@Athishpranav2003 I think it is still open. Your help/contribution would be appreciated. Thanks!"
      },
      {
        "user": "Athishpranav2003",
        "created_at": "2024-09-05T02:55:48Z",
        "body": "I will work on it this week ✌️"
      },
      {
        "user": "Athishpranav2003",
        "created_at": "2024-09-05T05:00:30Z",
        "body": "@KBaichoo @tyxia could you guys please check this. I am not sure how i can test it.\r\n"
      }
    ]
  },
  {
    "number": 34305,
    "title": "Silence LRS steam closures",
    "created_at": "2024-05-22T18:25:40Z",
    "closed_at": "2024-11-27T21:42:09Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/load reporting"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/34305",
    "body": "LRS prints a warning on any stream closure, including OK. This is triggering alerts for a normal behavior, re-balancing of long living streams by a server.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/34305/comments",
    "author": "kyessenov",
    "comments": [
      {
        "user": "adisuissa",
        "created_at": "2024-11-27T21:42:09Z",
        "body": "Closing, as this is fixed by #37076 "
      }
    ]
  },
  {
    "number": 34102,
    "title": "Unable to see Trace Id in Access Logs when request is rate limited (429)",
    "created_at": "2024-05-13T03:38:17Z",
    "closed_at": "2024-05-23T00:30:13Z",
    "labels": [
      "enhancement",
      "area/tracing",
      "help wanted",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/34102",
    "body": "`%REQ(traceparent)%` is not working when request is rate limited\r\n\r\ncc @wbpcode ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/34102/comments",
    "author": "zirain",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2024-05-16T01:56:51Z",
        "body": "Yeah. We can extend the substitution formatter to output the trace id and span id (downstream request span id) from the active span directly. As far as I know, the active span is part of the `HttpFormatterContext` now? So, it should be easy to do that."
      },
      {
        "user": "zirain",
        "created_at": "2024-05-16T02:03:48Z",
        "body": "ok, will add a new formatter `%TRACE_ID%`."
      }
    ]
  },
  {
    "number": 33846,
    "title": "Move datadog example (Docker) out of `/source`",
    "created_at": "2024-04-29T08:12:37Z",
    "closed_at": "2024-08-26T10:50:10Z",
    "labels": [
      "help wanted",
      "no stalebot",
      "area/examples",
      "area/datadog"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/33846",
    "body": "Currently the datadog extension has a docker demo in the `/source` directory\r\n\r\nTo say this is unexpected would be to put it mildly - and unless im mistaken this is not part of the source at all.\r\n\r\nIf this is a demo then it should be moved to `/examples` and have the relevant documentation/testing added",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/33846/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2024-04-29T08:12:59Z",
        "body": "cc @dmehala @mattklein123\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-05-29T12:01:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "dmehala",
        "created_at": "2024-05-29T13:35:57Z",
        "body": "no stalebot\r\n\r\nEDIT: Quick head up. I am working on it this week."
      },
      {
        "user": "phlax",
        "created_at": "2024-05-29T14:19:42Z",
        "body": "brilliant thanks so much @dmehala that is really appreciated - i can help with creating a sandbox"
      }
    ]
  },
  {
    "number": 29740,
    "title": "New feature: Generic proxy transmit data by block",
    "created_at": "2023-09-21T01:50:26Z",
    "closed_at": "2024-01-08T02:05:28Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29740",
    "body": "New feature: Generic proxy transmit data by block\r\n\r\nWe find the new feature, generic proxy in v1.27.0, is only process data (request/response) after its content is received completely. Otherwise, if we transmit data before all data is received, it will lead to the subsequent data loss .The above way of data transmission will cause the huge memory occupation and performance degradation when the data size is much big. So we want to know whether there are other ways to transmit data by generic proxy. And we have tried to add a new interface in the generic proxy for data transmission by block rather than waiting receiving all data.  The data transmission by block is to process data as the data is read from rx buffer.\r\n\r\nLooking forward to your reply.\r\n@KBaichoo  @soulxu @wbpcode \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29740/comments",
    "author": "liming95",
    "comments": [
      {
        "user": "soulxu",
        "created_at": "2023-09-22T01:35:35Z",
        "body": "cc @wbpcode also "
      },
      {
        "user": "wbpcode",
        "created_at": "2023-09-22T01:55:47Z",
        "body": "Thanks for your suggestion. This is a reasonable requirement. And I am also want it. The streamline single request/response would be very helpful for big request/response.\r\n\r\nBut in our prod env, our requests/responses are not that big and this feature will make the L7 filter chain very complex. So, this feature only has a low priority on my list.\r\n\r\nMarked this with `help wanted` in case the community wants to help it."
      },
      {
        "user": "liming95",
        "created_at": "2023-09-22T02:52:53Z",
        "body": "Thank you for replying. We have developed a common interface support this feature by some simple modification and it can work normally in our environment. And I want to know whether we can add these modification in the project and I also want to know what is the problem of these modification in other scenarios .  The following is crucial component of our modification.\r\n\r\n1.add the new interace onDecodingSuccess().\r\nvoid onDecodingSuccess(ResponsePtr response, ExtendedOptions options, bool end_stream);\r\n\r\n2. when the above function is called, the succeeding operation for clearing ‘stream’ will only be executed when the end_stream is true. For example, \r\n\r\nvoid UpstreamRequest::onDecodingSuccess(ResponsePtr response, ExtendedOptions options,\r\n                                        bool end_stream) {\r\n  if (end_stream) {\r\n    clearStream(options.drainClose());\r\n  }\r\n  parent_.onUpstreamResponse(std::move(response), options);\r\n}\r\n\r\nOur implementation is for the big response transmission by block and the request still need to receive all request data. The main reason is based on a fact that request is usually small and respond is more big.\r\n@wbpcode "
      },
      {
        "user": "wbpcode",
        "created_at": "2023-09-22T07:53:46Z",
        "body": "Happy to see the generic proxy be used in your environment. And any contributions are welcome.\r\n\r\nIf we want to merge the modifications to the upstream, we need to design the interface more carefully to make sure it's reasonable and meaningful.\r\n\r\n~~Rather than refactor current `onDecodingSuccess`,  I think maybe the better way is provide a set of new methods to provide more fine grained control to the decoding processing.~~\r\n\r\nupdated at 2023.09.26: Considering the multiplexing, only a set of new methods also aren't enough."
      },
      {
        "user": "wbpcode",
        "created_at": "2023-09-26T07:12:13Z",
        "body": "Because I will have some free time in next week. So I improved the priority of this task. But after thinking in more detail, I found it's harder than my initial thought, esp considering the possible multiplexing in some protocols.\r\n\r\nI have created a PR for discussion for the possible new codec API. Could you also take a look when you have free time? cc @SoSoSorry to if it meets your requirement. "
      },
      {
        "user": "wbpcode",
        "created_at": "2024-01-08T02:05:28Z",
        "body": "#29806"
      }
    ]
  },
  {
    "number": 28834,
    "title": "Add into dynamic metadata the filter name that generate a local replay",
    "created_at": "2023-08-04T12:25:11Z",
    "closed_at": "2023-11-09T02:14:44Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/metadata"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28834",
    "body": "Add into dynamic metadata the filter name that generate a local replay\r\n\r\nIt could be useful lo allow http_connection_manager add metadata for the filter name that stop iteration and generate a local replay\r\nThat metadata could be consumed by access_log or eventually other custom filter\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28834/comments",
    "author": "juanmolle",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2023-08-04T20:23:33Z",
        "body": "I think this seems reasonable. I'm not sure if dynamic metadata is the right choice or whether it should be some kind of loggable stream attribute though."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2023-08-06T09:03:27Z",
        "body": "FILTER_STATE may be better because DYNAMIC_METADATA is not accessible from WASM filters or any other new stream attributes"
      },
      {
        "user": "juanmolle",
        "created_at": "2023-08-07T18:13:18Z",
        "body": "I guess dynamic_metadata was accessible, but not for write due to performance impact, probably FILTER_STATES could be a better place. Is it possible to access any data from wasm filter? apparently, accessing from wasm to write or read add \"wasm.\" namespace."
      }
    ]
  },
  {
    "number": 27801,
    "title": "generic proxy: initial stats support",
    "created_at": "2023-06-05T09:03:08Z",
    "closed_at": "2023-08-07T07:17:56Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/27801",
    "body": "*Title*: *generic proxy: initial stats support*\r\n\r\n*Description*:\r\nAdd initial simple stats support to generic proxy.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/27801/comments",
    "author": "wbpcode",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-06-05T09:15:55Z",
        "body": "cc @LaurenLiu123"
      },
      {
        "user": "liulanyi",
        "created_at": "2023-06-05T12:18:15Z",
        "body": "Hi @wbpcode  I will take this task. Thanks"
      },
      {
        "user": "liulanyi",
        "created_at": "2023-06-05T12:18:16Z",
        "body": "Hi @wbpcode  I will take this task. Thanks"
      }
    ]
  },
  {
    "number": 26994,
    "title": "wasm: assertion failure on close request in response header callback",
    "created_at": "2023-04-26T17:06:47Z",
    "closed_at": "2024-01-16T14:19:10Z",
    "labels": [
      "bug",
      "help wanted",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26994",
    "body": "Closing a request during a callback triggers an assertion failure:\r\n\r\n```\r\n[2023-04-26 17:05:33.565][2621768][critical][assert] [external/envoy/source/common/http/http1/codec_impl.cc:351] assert failure: 0UL == output_buffer_->length().\r\n```\r\n\r\nMinimal plugin code:\r\n\r\n```c++\r\n#include \"proxy_wasm_intrinsics.h\"\r\n\r\n// PluginRootContext is the root context for all streams processed by the\r\n// thread. It has the same lifetime as the VM and acts as target for\r\n// interactions that outlives individual stream, e.g. timer, async calls.\r\nclass PluginRootContext : public RootContext {\r\n public:\r\n  explicit PluginRootContext(uint32_t id, std::string_view root_id)\r\n      : RootContext(id, root_id) {}\r\n\r\n  bool onConfigure(size_t) override;\r\n};\r\n\r\n// Per-stream context.\r\nclass PluginContext : public Context {\r\n public:\r\n  explicit PluginContext(uint32_t id, RootContext* root) : Context(id, root) {}\r\n  FilterHeadersStatus onResponseHeaders(uint32_t, bool) override {\r\n    closeRequest();\r\n    return FilterHeadersStatus::Continue;\r\n  }\r\n\r\n private:\r\n  inline PluginRootContext* rootContext() {\r\n    return dynamic_cast<PluginRootContext*>(this->root());\r\n  }\r\n};\r\n\r\nstatic RegisterContextFactory register_Scaffold(\r\n    CONTEXT_FACTORY(PluginContext), ROOT_FACTORY(PluginRootContext));\r\n\r\nbool PluginRootContext::onConfigure(size_t) { return true; }\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26994/comments",
    "author": "kyessenov",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2023-04-26T17:08:03Z",
        "body": "CC @mpwarres "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-26T20:01:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      }
    ]
  },
  {
    "number": 26525,
    "title": "Use rst snippets to build `changelogs/current.yaml`",
    "created_at": "2023-04-03T22:21:50Z",
    "closed_at": "2023-05-11T04:01:25Z",
    "labels": [
      "enhancement",
      "area/docs",
      "help wanted",
      "area/release"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26525",
    "body": "As suggested by @ravenblackx offline we could shift `current.yaml` to folders containing rst snippets\r\n\r\nthis would resolve the ongoing changelog conflict dance (at least in most cases) and would also mean that devs can edit rst in rst files\r\n\r\ni think this is an incredibly good idea",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26525/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-04T00:02:53Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-05-11T04:01:25Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 25480,
    "title": "Pending stream queue timing",
    "created_at": "2023-02-09T19:30:08Z",
    "closed_at": "2023-03-30T22:36:30Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/http",
      "area/observability"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25480",
    "body": "Pending Stream  Queue Timing\r\n\r\nWhen a stream is pending on a connection pool we should gather a timestamp. When it is not longer pending as there's stream capacity or it's cancelled we should take another timestamp and get the delta in order to record how long the stream spent in pending state.\r\n\r\nThis can help diagnose issues such as high Envoy e2e latency due to insufficient backend capacity.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25480/comments",
    "author": "KBaichoo",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2023-03-11T20:01:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      }
    ]
  },
  {
    "number": 25330,
    "title": "ext_authz.failure_mode_allow should optionally append a header on failure",
    "created_at": "2023-02-03T00:58:14Z",
    "closed_at": "2024-07-25T17:10:11Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/ext_authz"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25330",
    "body": "*Title*: \r\n*failure_mode_allow should optionally append a header on failure*\r\n\r\n*Description*:\r\nWhen using `ext_authz` filters with `failure_mode_allow: true`, requests that could not be completed are allowed to pass through rather than being denied. It would be useful in cases where `ext_authz` filters are used sequentially, for example, to know if a request that has made it <this far> was passed along as a result of an allowed failure vs auth success. \r\n\r\nAppending a header such as `x-envoy-failure-mode-allowed: true` would allow for special handling or custom metric emission for those cases. \r\n\r\nWhether or not to add this header on failures could be configurable, and disabled by default. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25330/comments",
    "author": "bgerson-square",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-02-06T02:00:49Z",
        "body": "make sense to me."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-02-06T08:39:38Z",
        "body": "If no one is working on this, i can pick it up, thanks."
      },
      {
        "user": "bgerson-square",
        "created_at": "2023-02-06T18:01:37Z",
        "body": "i actually wouldn't mind taking a crack at this myself, have been looking for an opportunity to contribute here and this seems like a good one. i'll try and have a PR up by EOW or comment here if i am unable to do so"
      },
      {
        "user": "StarryVae",
        "created_at": "2023-03-09T06:53:30Z",
        "body": "> i actually wouldn't mind taking a crack at this myself, have been looking for an opportunity to contribute here and this seems like a good one. i'll try and have a PR up by EOW or comment here if i am unable to do so\r\n\r\nSorry to interrupt, but do you have time to solve this problem now? @bgerson-square "
      },
      {
        "user": "bgerson-square",
        "created_at": "2023-03-13T17:28:26Z",
        "body": "apologies! I have been not been able to make progress here. if you'd like to pick this up, i'd appreciate it!"
      },
      {
        "user": "StarryVae",
        "created_at": "2023-03-14T03:31:06Z",
        "body": "yeah, thanks."
      },
      {
        "user": "ggreenway",
        "created_at": "2024-07-25T17:10:11Z",
        "body": "This was added in #26326 "
      }
    ]
  },
  {
    "number": 25116,
    "title": "local_ratelimit listener filter",
    "created_at": "2023-01-24T06:36:14Z",
    "closed_at": "2023-03-15T22:16:24Z",
    "labels": [
      "help wanted",
      "area/ratelimit"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25116",
    "body": "*Title*: *local_ratelimit listener filter*\r\n\r\n*Description*:\r\nWe saw CPU usage could be high if there's too many downstream connections which requesting handshake in a short period. Especially while a host is already busy, there downstream hosts could disconnect by timeout and connect again.\r\n\r\nThis is the perfect scenario of local rate limit. However, local rate_limit network filter is too late since the filter is involved after handshake.\r\n\r\nTherefore, I'd like to move on a local_ratelimit listener filter.\r\nIdeally we want to block too many handshake in a short period but I haven't seen through the implementation.\r\nA possible way is to count all `onAccept` no matter whether it is TLS.\r\nNext step could limit on the `requestedApplicationProtocols` or `detectedTransportProtocol` from `tls_inspector`\r\n\r\ncc local ratelimit codeowners @mattklein123 @wbpcode ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25116/comments",
    "author": "JuniorHsu",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2023-01-24T16:13:47Z",
        "body": "Yeah this makes sense to me."
      },
      {
        "user": "kyessenov",
        "created_at": "2023-01-24T18:01:20Z",
        "body": "I've seen an issue similar to this for some of our customers: a single request costs more for the proxy with TLS than for the fast backend. My request would be to improve telemetry. Without a stream, the standard logging and metrics break down, so we'd need a separate metric prior to stream creation."
      }
    ]
  },
  {
    "number": 24218,
    "title": "Add filename to \"Filesystem config update failure\"  warning",
    "created_at": "2022-11-27T05:56:44Z",
    "closed_at": "2022-12-29T11:51:09Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/xds",
      "area/filesystem"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24218",
    "body": "*Title*: *Misconfigured file is hard to find on \"Filesystem config update failure\"*\r\n\r\n*Description*:\r\n\r\nWhen I deploy a new service to the testing environment and I misconfigured something, I find a line like the following in the logs.\r\n```log\r\n[2022-11-27 04:51:34.312][1][warning][config] [source/common/config/filesystem_subscription_impl.cc:98] Filesystem config update failure: yaml-cpp: error at line 5, column 5: end of sequence not found\r\n```\r\nThe line tells me, I made a mistake and where in the file the mistake happened, but doesn't describe which file has issues.\r\n\r\nIf the warning was changed to something like the following, it wouldn't be so frustrating.\r\n\r\n```log\r\n[2022-11-27 04:51:34.312][1][warning][config] [source/common/config/filesystem_subscription_impl.cc:98] Filesystem config update failure: yaml-cpp: error in /foo/bar/rds.yaml at line 5, column 5: end of sequence not found\r\n```\r\n(In my example I've added `in /foo/bar/rds.yaml`)\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24218/comments",
    "author": "MichaelSasser",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-11-27T11:22:05Z",
        "body": "Sounds reasonable requirement."
      },
      {
        "user": "MichaelSasser",
        "created_at": "2022-12-29T10:31:57Z",
        "body": "I think the issue can be closed as #24503 was merged."
      }
    ]
  },
  {
    "number": 24217,
    "title": "consider host as active when active health check passes with both active/passive HC are enabled",
    "created_at": "2022-11-26T08:40:40Z",
    "closed_at": "2023-01-20T16:44:16Z",
    "labels": [
      "help wanted",
      "area/health_checking"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24217",
    "body": "When we have both active and passive health checks enabled, when host is ejected out of pool by passive health check failure, it will come back to pool only after ejection time is over, even though in between active health checks pass. Consider a case a where the ejection time is 2 min, the host wont be considered again for load balancing even if active health checks detect it healthy after say 30s.\r\n\r\nI understand this is done to cover up for cases where active health check may not be going to actual server, Can this made configurable behaviour for services like Redis that wants \r\n- Early ejection based on passive health checks (with high ejection times)\r\n- Come back to pool based on active health check success (instead of waiting for ejection timeout)?\r\n\r\nAny existing config knobs that help this case or any down sides of doing this?\r\n\r\ncc: @wbpcode @mattklein123 @snowp ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24217/comments",
    "author": "ramaraochavali",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-11-26T18:37:54Z",
        "body": "Seems reasonable to me."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2022-11-28T04:08:34Z",
        "body": "Should we have api config to drive this or runtime override is good enough?"
      },
      {
        "user": "mattklein123",
        "created_at": "2022-11-28T15:44:52Z",
        "body": "> Should we have api config to drive this or runtime override is good enough?\r\n\r\nMy first thought was that it's OK to do this all the time. My only concern is situations in which health check is passing but the data plane traffic is always failing. I suppose in that case it would just get ejected again quickly so it's probably fine. I would probably just do it always with a runtime guard and see if anyone complains?"
      },
      {
        "user": "ramaraochavali",
        "created_at": "2022-11-28T16:11:28Z",
        "body": "> I would probably just do it always with a runtime guard and see if anyone complains?\r\n\r\n+1. This is what I was thinking as well. I will push a change."
      },
      {
        "user": "bernardoVale",
        "created_at": "2023-05-10T11:37:24Z",
        "body": "Hi, I was reading the release notes and bumped into this new behavior.\r\n\r\nIf I understood correctly, if an API endpoint starts failing on a specific host and gets ejected by the outlier, but the health check remains healthy, Envoy will uneject the host on the next health check evaluation.\r\n\r\nIsn't the example above one of the values of using an outlier detector? Certain types of failures aren't easily covered by a health check endpoint."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2023-05-10T11:57:29Z",
        "body": "This change will not wait for the outlier detection time i.e. if a host is ejected because of outlier detection, it will be ejected for base_ejectoin time * iteration\r\n- with out this change, envoy will wait for that duration before it brings it back to load balancer\r\n- with this change, if a health check passes before that interval, it will consider host as healthy and brings it back.\r\n"
      }
    ]
  },
  {
    "number": 24013,
    "title": "Reflection for the filter state",
    "created_at": "2022-11-15T19:18:36Z",
    "closed_at": "2023-09-01T01:58:45Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/metadata"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/24013",
    "body": "The main advantage of the dynamic metadata over the filter state is the ability to introspect the content without having the definition struct compiled-in. That enables a bunch of applications: access logs can traverse fields, CEL can operate on it as JSON, Wasm filters can avoid full content serialization. I think it's time to add the same capability to the filter state objects. The simplest proposal I can come up with is the following:\r\n* Add `DynamicValue` modeled after `CelValue` with the CEL type system, to support basic computations and traversals without too much overhead.\r\n* Add `Reflection` factory keyed by the filter state object key. Its purpose is to mirror a filter state native object into `DynamicValue`.\r\n* Integrate with filters, mainly to permit generic traversals in Wasm, access log, CEL shim, etc.\r\n\r\nAs a side advantage, with this, we can drop the dependency on flatbuffers, since that makes the downstream responsible to provide a reflection registration for the filter state objects backed by flatbuffers.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/24013/comments",
    "author": "kyessenov",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-12-16T00:03:02Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "tyxia",
        "created_at": "2023-02-22T20:15:29Z",
        "body": "/cc @tyxia "
      }
    ]
  },
  {
    "number": 23306,
    "title": "Missing distinct oauth filter logging option",
    "created_at": "2022-09-29T13:17:15Z",
    "closed_at": "2023-08-22T06:01:55Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/oauth"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23306",
    "body": "Cannot configure logging level for Oauth filter\r\n\r\nWhen starting the envoy using --component-log-level option, you cannot set a distinct value for oauth filter.\r\n\r\nTwo fixes needed:\r\n\r\n- add oauth value in Enum in logger.h in the list\r\n#define ALL_LOGGER_IDS(FUNCTION)\r\n\r\n- configure new value (oauth instead upstream) in oauth_client.h in the line:\r\nclass OAuth2ClientImpl : public OAuth2Client, Logger::Loggable<Logger::Id::upstream> {\r\n\r\nSources were inspected on main branch.\r\n\r\nThank you in advance.\r\nDan.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23306/comments",
    "author": "DanApostol",
    "comments": [
      {
        "user": "StarryVae",
        "created_at": "2022-10-12T09:12:27Z",
        "body": "So, is it resonable to fix this, and is any one working on it? if no, i will try to fix."
      },
      {
        "user": "kanurag94",
        "created_at": "2023-08-22T03:41:35Z",
        "body": "I think this is done, should we close this issue? @StarryVae @DanApostol "
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-22T03:43:16Z",
        "body": "yeah, i think we can close it."
      },
      {
        "user": "DanApostol",
        "created_at": "2023-08-22T06:01:55Z",
        "body": "Ok, closed."
      }
    ]
  },
  {
    "number": 23302,
    "title": "ratelimit supports query parameter match",
    "created_at": "2022-09-29T03:53:01Z",
    "closed_at": "2022-12-21T06:39:42Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/ratelimit"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23302",
    "body": "*Title*: ratelimit supports query parameter match\r\n\r\n*Description*:\r\nis it reasonable for ratelimit to support query parameter match?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23302/comments",
    "author": "StarryVae",
    "comments": [
      {
        "user": "StarryVae",
        "created_at": "2022-09-29T03:53:25Z",
        "body": "cc @wbpcode @mattklein123 "
      },
      {
        "user": "wbpcode",
        "created_at": "2022-09-29T08:51:53Z",
        "body": "If new matcher is necessary, should we just use xds common matcher here?"
      },
      {
        "user": "yanavlasov",
        "created_at": "2022-09-29T14:23:05Z",
        "body": "I think you can make it work with header match using `:path` header and reg ex expression. But for convenience I think it is worth adding query matcher."
      },
      {
        "user": "StarryVae",
        "created_at": "2022-10-11T04:09:04Z",
        "body": "> I think you can make it work with header match using `:path` header and reg ex expression. But for convenience I think it is worth adding query matcher.\r\n\r\nyes, thanks, i will add query matcher to fix this."
      }
    ]
  },
  {
    "number": 22875,
    "title": "Use alphabetic ordering for tocs in API docs",
    "created_at": "2022-08-29T15:00:21Z",
    "closed_at": "2022-10-18T15:18:36Z",
    "labels": [
      "enhancement",
      "area/docs",
      "help wanted",
      "api"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22875",
    "body": "Currently the tocs in the api docs are a bit messy, as is navigation in the API more generally\r\n\r\nOne possible cleanup/improvement is to use alphabetic ordering",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22875/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-08-29T15:01:04Z",
        "body": "cc @mattklein123 "
      },
      {
        "user": "krupaJari",
        "created_at": "2022-09-10T18:34:53Z",
        "body": "Hey @phlax, \r\nI looked into your project and would like to start working on this issue."
      },
      {
        "user": "phlax",
        "created_at": "2022-09-28T10:45:21Z",
        "body": "hi @krupaJari - apologies i missed your response - is this something you would still like to look at?\r\n"
      },
      {
        "user": "75asu",
        "created_at": "2022-10-02T03:52:05Z",
        "body": "HI @phlax , I would like to work on this issue, could you plaese provide the file path which needs to be modified"
      },
      {
        "user": "phlax",
        "created_at": "2022-10-03T10:59:00Z",
        "body": "Looking through its not immediately clear which pages should/not have their TOCs alphabeticised - some definitely do tho\r\n\r\nMostly i think its the rst files under the `docs/root/api-v3` path - eg `docs/root/api-v3/config/config.rst` is particularly badly sorted\r\n\r\n\r\n"
      },
      {
        "user": "phlax",
        "created_at": "2022-10-03T10:59:40Z",
        "body": "@measutosh i would v much like to land a cleanup for this before the next release (a week or so from now)"
      },
      {
        "user": "75asu",
        "created_at": "2022-10-04T03:49:30Z",
        "body": "@phlax , I would love to contribute something atleast, let's connect on slack and get this done"
      },
      {
        "user": "phlax",
        "created_at": "2022-10-14T13:42:44Z",
        "body": "as noone has raised any PRs to address this yet and the 1.24 release is imminent i will probably look at this over this weekend if noone else has already"
      }
    ]
  },
  {
    "number": 22009,
    "title": "The oauth2_filter should use client_secret_basic to access token endpoint by default.",
    "created_at": "2022-07-04T07:25:33Z",
    "closed_at": "2022-07-25T18:03:15Z",
    "labels": [
      "bug",
      "help wanted",
      "area/oauth"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22009",
    "body": "*Title*:\n\nThe oauth2_filter should use client_secret_basic to access token endpoint by default.\n\n*Description*:\n\nCurrently the oauth2_filter use client_secret_post as the default authentication method to access oauth2 token endpoint. By RFC6749 and RFC8414, client_secret_basic is the default authentication method. And the client_secret_post method is not recommended and should be limited.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22009/comments",
    "author": "figroc",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-07-05T09:24:41Z",
        "body": "cc @snowp "
      },
      {
        "user": "snowp",
        "created_at": "2022-07-05T14:18:01Z",
        "body": "I suspect we use that just for historical reasons (cc @rgs1), we can probably do better here"
      },
      {
        "user": "Amila-Rukshan",
        "created_at": "2022-07-10T16:42:31Z",
        "body": "I'm going to start working on this ticket. Support Basic auth as the default method and keep the existing method (sending credentials as url encoded body values) to set under a config.\r\n```\r\nconfig:\r\n  ...\r\n  auth_type: \"URL_ENCODED_BODY\". // default to \"BASIC_AUTH\"\r\n  ...\r\n```"
      }
    ]
  },
  {
    "number": 21265,
    "title": "Add option for Custom headers to append empty string when dynamic value cannot be resolved",
    "created_at": "2022-05-12T15:41:00Z",
    "closed_at": "2022-05-31T14:30:28Z",
    "labels": [
      "help wanted",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21265",
    "body": "*Title*: Add option for Custom headers to append empty string when dynamic value cannot be resolved.\r\n\r\n*Description*:\r\nCustom headers currently do not append anything if `%DYNAMIC_VALUE%` does not resolve to a non-empty string. It would be helpful to have an additional option in `HeaderValueOption` to append \"header-name:\" in these cases. Happy to send a PR myself.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21265/comments",
    "author": "paul-r-gall",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-05-13T15:33:13Z",
        "body": "Sure sounds fine to me."
      }
    ]
  },
  {
    "number": 21261,
    "title": "http filters: make the http filter can be configured repeatly and work correctly",
    "created_at": "2022-05-12T06:41:25Z",
    "closed_at": "2022-05-25T15:31:41Z",
    "labels": [
      "help wanted",
      "area/http_filter",
      "area/router"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21261",
    "body": "*Title*: *http filters: make the http filter can be configured repeatly and work correctly*\r\n\r\n*Description*:\r\n\r\nAlthough the http filters can be configured repeatly in the `http_filters`, they cannot work perfectly. Because the route configs in the `typed_per_filter_config` are indexed by the filter name. So for the same type filters in the `http_filters`, they can only share the same route-level config.\r\n\r\nAfter #21030, route configs in the `typed_per_filter_config` can be indexed by other keys, for example, `HttpFilter.name`.\r\n\r\nThis would be a big change. And it would be a long time to deprecate the old behaviours.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21261/comments",
    "author": "wbpcode",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-05-12T06:42:56Z",
        "body": "cc @kyessenov @mattklein123 "
      },
      {
        "user": "wbpcode",
        "created_at": "2022-05-12T07:03:31Z",
        "body": "Some typical scenes:\r\n- multiple `ext_authz` refered by @kyessenov \r\n- multiple lua scripts for single route\r\n- route config support for wasm filters\r\n- etc"
      },
      {
        "user": "wbpcode",
        "created_at": "2022-05-12T07:15:35Z",
        "body": "We can achieve this step by step.\r\n- [ ] Ensure the `HttpFilter.name` can be the unique flag in the http filter list.\r\n- [ ] Make the config name and filter name are available for the filters/StreamFilterCallbacks.\r\n- [ ] New API to get route level filter config.\r\n- [ ] Update all the filter to use new API.\r\n- [ ] Switch to use `HttpFilter.name` by runtime flag.\r\n- [ ] Deprecate old behaviours finally."
      },
      {
        "user": "wbpcode",
        "created_at": "2022-05-24T16:24:26Z",
        "body": "Hi, I am trying to land this. \n\n**Before the first PR, I want to get some inputs from community. Does this make sense for the community?**\n\ncc @mattklein123 @kyessenov \n\n(My ultimate goal is per-route or per-vh http filter chain. After complete this work, we can configure same filter repeatedly with different config. And then by #20867, we can enable an array of filters based on route config. For example, for route A, an ext_authz filter backed by auth server A can be enabled, and for route B, an ext_authz filter backed by auth server B can be enabled.)"
      },
      {
        "user": "mattklein123",
        "created_at": "2022-05-25T15:25:08Z",
        "body": "I agree we should fix this, but I'm not sure we should change the existing behavior as it would probably break a lot of people. Would there be some way to opt-in the new behavior using an additional configuration field that would somehow allow matching up different versions of the route config?\r\n\r\nI'm thinking that we can add a field called something like \"route_config_key\" or whatever and then use this for additional matching if it's there. cc @envoyproxy/api-shepherds also. "
      },
      {
        "user": "markdroth",
        "created_at": "2022-05-25T15:30:11Z",
        "body": "This looks like a duplicate of #12274.  Can we close this and track the changes there?\r\n\r\nNote that there is already some discussion in the other issue about how to handle backward compatibility."
      },
      {
        "user": "wbpcode",
        "created_at": "2022-05-25T15:31:27Z",
        "body": "cc @markdroth you are right."
      },
      {
        "user": "markdroth",
        "created_at": "2022-05-25T15:31:41Z",
        "body": "Okay, then I'll close this."
      }
    ]
  },
  {
    "number": 21065,
    "title": "Use structured data (ie yaml) for changelog (version histories)",
    "created_at": "2022-04-28T11:49:51Z",
    "closed_at": "2022-05-03T16:43:59Z",
    "labels": [
      "enhancement",
      "area/docs",
      "help wanted",
      "area/release"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21065",
    "body": "### description\r\n\r\nSurfacing a conversation on slack with @mattklein123 ...\r\n\r\nWhen adding a changelog entry, instead of editing `current.rst` as we do now we could instead add entries into a eg `changelog.yaml` file\r\n\r\nThe `current.rst` file could then be built from the yaml\r\n\r\nThis has the advantage that it would make it easier to validate/manipulate the data in the yaml file\r\n\r\nThe changelog entries would still want to be valid `rst` \r\n\r\nIf we go down this path one question it throws up is how we store historical changelogs - ie do we just generate the `v1.X.Y.rst` file and store it as we do now, or do we retain the yaml files and leave them to be built/rendered in bazel\r\n\r\nif we take the latter path we probs want to shift existing rst -> `changelog.v1.X.Y.yaml` and make the version_history rst entirely dynamic and not just `current.rst`\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21065/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-04-28T12:29:09Z",
        "body": "possible changelog format\r\n\r\n```yaml\r\n\r\nbehavior_changes:\r\n# Changes that are expected to cause an incompatibility if applicable; deployment changes are likely required\r\n- area: tls-inspector\r\n  change: |\r\n    the listener filter tls inspector's stats ``connection_closed`` and ``read_error`` are removed. The new stats are introduced for listener, ``downstream_peek_remote_close`` and ``read_error`` :ref:`listener stats <config_listener_stats>`.\r\n\r\nminor_behavour_changes:\r\n# Changes that may cause incompatibilities for some users, but should not for most\r\n- area: thrift\r\n  change: |\r\n    add validate_clusters in :ref:`RouteConfiguration <envoy_v3_api_msg_extensions.filters.network.thrift_proxy.v3.RouteConfiguration>` to override the default behavior of cluster validation.\r\n- area: tls\r\n  change: |\r\n    if both :ref:`match_subject_alt_names <envoy_v3_api_field_extensions.transport_sockets.tls.v3.CertificateValidationContext.match_subject_alt_names>` and :ref:`match_typed_subject_alt_names <envoy_v3_api_field_extensions.transport_sockets.tls.v3.CertificateValidationContext.match_typed_subject_alt_names>` are specified, the former (deprecated) field is ignored. Previously, setting both fields would result in an error.\r\n- area: tls\r\n  change: removed SHA-1 cipher suites from the server-side defaults.\r\n\r\nbug_fixes:\r\n# Changes expected to improve the state of the world and are unlikely to have negative effects\r\n\r\nremoved_config_or_runtime:\r\n# Normally occurs at the end of the deprecation period\r\n- area: compressor\r\n  change: removed ``envoy.reloadable_features.fix_added_trailers`` and legacy code paths.\r\n- area: dns\r\n  change: removed ``envoy.reloadable_features.use_dns_ttl`` and legacy code paths.\r\n- area: ext_authz\r\n  change: removed ``envoy.reloadable_features.http_ext_authz_do_not_skip_direct_response_and_redirect`` runtime guard and legacy code paths.\r\n- area: http\r\n  change: deprecated ``envoy.reloadable_features.correct_scheme_and_xfp`` and legacy code paths.\r\n- area: http\r\n  change: deprecated ``envoy.reloadable_features.validate_connect`` and legacy code paths.\r\n- area: tcp_proxy\r\n  change: removed ``envoy.reloadable_features.new_tcp_connection_pool`` and legacy code paths.\r\n\r\nnew_features:\r\n- area: access_log\r\n  change: added new access_log command operators to retrieve upstream connection information change: ``%UPSTREAM_PROTOCOL%``, ``%UPSTREAM_PEER_SUBJECT%``, ``%UPSTREAM_PEER_ISSUER%``, ``%UPSTREAM_TLS_SESSION_ID%``, ``%UPSTREAM_TLS_CIPHER%``, ``%UPSTREAM_TLS_VERSION%``, ``%UPSTREAM_PEER_CERT_V_START%``, ``%UPSTREAM_PEER_CERT_V_END%`` and ``%UPSTREAM_PEER_CERT%``.\r\n- area: dns_resolver\r\n  change: added :ref:`include_unroutable_families<envoy_v3_api_field_extensions.network.dns_resolver.apple.v3.AppleDnsResolverConfig.include_unroutable_families>` to the Apple DNS resolver.\r\n- area: ext_proc\r\n  change: added support for per-route :ref:`grpc_service <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.ExtProcOverrides.grpc_service>`.\r\n- area: thrift\r\n  change: added flag to router to control downstream local close. :ref:`close_downstream_on_upstream_error <envoy_v3_api_field_extensions.filters.network.thrift_proxy.router.v3.Router.close_downstream_on_upstream_error>`.\r\n\r\ndeprecated:\r\n```"
      },
      {
        "user": "phlax",
        "created_at": "2022-04-28T13:34:00Z",
        "body": "testing this out locally and it works pretty well - i think this will eliminate quite a few steps in terms of \"make sure version history...\"\r\n\r\nans possibly no need to cycle version files"
      },
      {
        "user": "mattklein123",
        "created_at": "2022-04-28T14:57:13Z",
        "body": "Huge +1, I'm regretting not thinking about this and implementing this much sooner. cc @envoyproxy/envoy-maintainers for comment. I think this will make many things massively easier in terms of linting, sorting, generating, consistency, etc."
      },
      {
        "user": "phlax",
        "created_at": "2022-04-28T15:01:51Z",
        "body": "also separates the changes from the presentation - so as we change how we want to present the version histories we dont have to go and edit every file"
      },
      {
        "user": "phlax",
        "created_at": "2022-04-29T07:14:35Z",
        "body": "the one downside to this is that you lose the rst highlighting etc in your editor - at least i do by default\r\n\r\nyou can probably workaround with format-in-format highlighting or similar (eg emacs has polymode)"
      },
      {
        "user": "phlax",
        "created_at": "2022-04-30T10:59:08Z",
        "body": "another pro - i think this will mitigate merge conflicts in the current changelog file as there will be no need to maintain order etc"
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-05-05T17:01:52Z",
        "body": "I'm still seeing references to current.rst in a bunch of docs - CONTRIBUTING instrctions, PULL_REQUESTS, RELEASES etc.  You on top of those clean ups in follow up PRs?"
      }
    ]
  },
  {
    "number": 20934,
    "title": "Metrics List Request",
    "created_at": "2022-04-22T01:56:31Z",
    "closed_at": "2022-11-10T08:31:20Z",
    "labels": [
      "help wanted",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20934",
    "body": "*Title*: *Metrics List Request*\r\n\r\n*Description*:\r\n\r\nNow the only way I know to get the metrics list is to deploy Envoy and post requests to the metric endpoint, please tell me if something missing.\r\n\r\nWhen metrics change, a monitor system like Prometheus would raise the error for the missing metrics, which is not conducive to upgrade and maintenance.\r\n\r\nI think we can add a doc of metrics list and update it when we release,  which makes metrics obvious and easy to check what/which we need or not. Potentially we will reduce some metrics and improve performance.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20934/comments",
    "author": "daixiang0",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-04-22T02:26:46Z",
        "body": "In general this would be really nice, but it's no so easy as metrics can be created dynamically. With that said I would love to figure out a way to auto generate docs for all standard metrics. cc @jmarantz "
      },
      {
        "user": "jmarantz",
        "created_at": "2022-04-22T03:25:42Z",
        "body": "A few things to unpack here:\r\n * defining doc for each stat in the code where we create the stat -- not sure if that's what Matt meant but I think it's easy to add a new stat and not remember to add documentation for it.\r\n * a more robust way to prune unneeded stats out of the system than what's currently available via the matcher API, as some stats are read in production code, which would misbehave if they were eliminated -- there's a separate bug about this\r\n * enumerating the \"core\" stats we expect to be there, thought that's made tricky by some potentially important ones being created on-demand."
      },
      {
        "user": "daixiang0",
        "created_at": "2022-04-22T04:22:50Z",
        "body": "I want to split stats into two kinds: static and dynamic.\r\n\r\nFor the static one, we list all of them, for the dynamic one, we list all patterns with examples, that would be helpful.\r\n\r\nThe way to generate docs may be like how API does, or we can update one doc by hand when stats update as the first step, at least cover static stats."
      },
      {
        "user": "zhangbo1882",
        "created_at": "2022-04-29T03:50:27Z",
        "body": "@daixiang0 , what are the dynamic metrics ?\r\n\r\n"
      },
      {
        "user": "daixiang0",
        "created_at": "2022-04-29T03:54:56Z",
        "body": "> @daixiang0 , what are the dynamic metrics ?\r\n\r\nMetrics which are generated according to certain rules, "
      }
    ]
  },
  {
    "number": 20665,
    "title": "corrupt stats for internal listener",
    "created_at": "2022-04-05T04:30:43Z",
    "closed_at": "2022-08-10T15:32:43Z",
    "labels": [
      "bug",
      "help wanted",
      "area/listener"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20665",
    "body": "Stats produced by an internal listener seem to have \"envoy://\" in their name, which is confusing. Example:\r\n```\r\nlistener.envoy_//internal_outbound.worker_39.downstream_cx_total: 0\r\n````\r\n\r\n/cc @lambdai ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20665/comments",
    "author": "kyessenov",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2022-04-06T00:09:15Z",
        "body": "As a workaround, stat_prefix can be specified \r\n```\r\n      listener_scope_(server_.stats().createScope(\r\n          fmt::format(\"listener.{}.\",\r\n                      !config.stat_prefix().empty()\r\n                          ? config.stat_prefix()\r\n                          : Network::Address::resolveProtoAddress(config.address())->asString()))),\r\n```\r\n\r\nThe goal of address `envoy://` as listener name is to avoid conflicting. \r\nThere is no guaranteed collision by default, I think I will simply remove \"//\" in the default address representation.\r\n\r\nNote that `:` is replaced to `_` as part of the metric name sanitize. This sanitize is enforce anyway b/c ipv6 address contains \":\""
      },
      {
        "user": "daixiang0",
        "created_at": "2022-04-06T01:32:44Z",
        "body": "/assign"
      },
      {
        "user": "lambdai",
        "created_at": "2022-04-06T18:38:28Z",
        "body": "CC @daixiang0 \r\nSame for pipe: when the address if \"/tmp/testpipe\". See below\r\n\r\nSo we'd better sanitize the address string in `listener_impl` rather than changing the `Address::asString()`;\r\n```\r\nlistener./tmp/testpipe.downstream_cx_length_ms: No recorded values\r\n```"
      },
      {
        "user": "daixiang0",
        "created_at": "2022-04-07T00:24:48Z",
        "body": "I will take a look and update, thanks!"
      }
    ]
  },
  {
    "number": 20506,
    "title": "Single backticks are no longer being caught in version_history",
    "created_at": "2022-03-24T12:21:43Z",
    "closed_at": "2022-05-03T16:44:00Z",
    "labels": [
      "bug",
      "area/docs",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20506",
    "body": "The code in `tools/docs/rst_check.py` is a bit brittle and is earmarked to be moved/improved to pytooling fairly soon\r\n\r\nOne thing that seems to have stopped working is the single backtick checker, and as a result we are getting quite a few PRs landing with only single backticks\r\n\r\nIdeally we dont do this with regexes - altho in this particular case i havent found any of the static rst(check/lint) tools to work - i think probably this could be done with a sphinx plugin at render time\r\n\r\nEither way, i think its quite important that we get this working again ASAP as it leads to bad docs and PR noise",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20506/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-03-24T12:22:02Z",
        "body": "cc @danzh2010 "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-23T16:01:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      }
    ]
  },
  {
    "number": 20466,
    "title": "thrift: deprecate ttwitter",
    "created_at": "2022-03-22T19:45:23Z",
    "closed_at": "2022-03-25T15:02:44Z",
    "labels": [
      "help wanted",
      "area/thrift"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20466",
    "body": "As a follow-up to discussions in #10701, let's initiate the process to deprecate ttwitter support in Thrift Proxy. We believe that the protocol is not widely used and supporting it makes Thrift Proxy more complex. Plus, to test it we need to bring in twitter-commons which is deprecated and doesn't properly support Python3. So here's the proposed plan:\r\n\r\n1) I'll send a PR with changelog entry to give notice of the deprecation along with a log warning when ttwitter is enabled\r\n2) I'll send an email to the appropriate lists giving a heads up about the deprecation\r\n3) I'll remove tests related to ttwitter, which effectively unblocks the py3 dependencies update \r\n4) after a year has gone by, we'll delete the code that supports ttwitter\r\n\r\ncc: @zuercher @mattklein123 @fishcakez @davinci26 @phlax \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20466/comments",
    "author": "rgs1",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-03-22T19:51:02Z",
        "body": "+1 thanks a ton @rgs1 for driving this."
      },
      {
        "user": "rgs1",
        "created_at": "2023-05-24T13:12:31Z",
        "body": "Alright, sounds like we are ready to delete TTwitter. I'll send something (unless @JuniorHsu beats me to it).\r\n\r\ncc: @fishcakez @zuercher "
      }
    ]
  },
  {
    "number": 20064,
    "title": "router check tool: add an option to write test results to a file in a structured format",
    "created_at": "2022-02-21T14:47:46Z",
    "closed_at": "2022-03-24T19:22:24Z",
    "labels": [
      "help wanted",
      "area/route_check_tool"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20064",
    "body": "*Title*: router check tool: add an option to write test results to a file in a structured format\r\n\r\n*Description*:\r\nAt the present, the tool writes test results to the standard output streams in a text format.\r\nWe'd like to integrate with the tool in our validation workflows, so it might be better to have an option to make the tool write test results to files in some structured way for further processing.\r\n\r\nWe could have an option `--output` with the file path where to save the test results. Test results could be represented with a proto message `ValidationResult` (to be crafted). Setting the `--output` option wouldn't affect the standard output the tool produces.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20064/comments",
    "author": "seventhscream",
    "comments": [
      {
        "user": "daixiang0",
        "created_at": "2022-02-22T05:25:17Z",
        "body": "I think you can use a pipe command like `route_check | tee <any_script>`."
      },
      {
        "user": "seventhscream",
        "created_at": "2022-02-22T09:53:54Z",
        "body": "Sure, this is one of the options.\r\nSome benefits of the structured output:\r\n- It would be easier to parse results, meaning less errors.\r\n- It would also be safer to use as the proto is expected to be backward compatible.\r\n\r\nIt's not mandatory, but might be nice to have. Besides, I wanted to implement it myself. WDYT?"
      },
      {
        "user": "daixiang0",
        "created_at": "2022-02-23T01:53:24Z",
        "body": "I think it is good.\r\n\r\n/cc @envoyproxy/envoy-maintainers "
      },
      {
        "user": "adisuissa",
        "created_at": "2022-02-23T20:19:40Z",
        "body": "cc @mattklein123 "
      },
      {
        "user": "mattklein123",
        "created_at": "2022-02-24T17:23:02Z",
        "body": "Sure SGTM"
      }
    ]
  },
  {
    "number": 19829,
    "title": "Request timeout metric not getting emitted for chunked responses",
    "created_at": "2022-02-04T21:11:57Z",
    "closed_at": "2022-02-23T15:40:58Z",
    "labels": [
      "bug",
      "help wanted",
      "beginner",
      "area/http",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19829",
    "body": "## Title\r\nRequest timeout metric is not getting emitted from clusters or virtual clusters for chunked responses\r\n\r\n## Description\r\n\r\nWe realized that the request timeout metrics are not getting emitted for clusters & virtual clusters in case of chunking where the upstream server immediately returns a **200 OK** with all the other headers but keep sending the data in chunks. If the response times out later than we do get a `UT` in the response flags but none of the other metrics like `upstream_rq_timeout` gets emitted.\r\n\r\nI was looking at the code and see that we do have a condition on only emitting these metrics for those requests which are awaiting headers but it won't work in the above described scenario.\r\n\r\n```\r\n// Don't do work for upstream requests we've already seen headers for.\r\nif (upstream_request->awaitingHeaders()) {\r\n  cluster_->stats().upstream_rq_timeout_.inc();\r\n  if (request_vcluster_) {\r\n    request_vcluster_->stats().upstream_rq_timeout_.inc();\r\n  }\r\n...\r\n```\r\n\r\nIs this intentional? Just trying to understand if it's possible to also record these metrics for such cases or if there is any other metric we can look at for such cases.\r\n\r\n## Repro Steps\r\n\r\nImplement a backend API which immediately returns a 200 OK with all the headers and return the data in chunks and then make that timeout somehow.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19829/comments",
    "author": "agrawroh",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2022-02-06T18:48:30Z",
        "body": "@alyssawilk @yanavlasov "
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-02-07T14:01:48Z",
        "body": "Yeah I guess this was written not taking into account bidirectional streaming, and assuming once the response had started we didn't want to time out requests.  We do support bidi streaming through (MultiplexedUpstreamIntegrationTest.LargeBidirectionalStreamingWithBufferLimits) so I think the correct thing to do in that case is continue to allow request timeouts until the request is fully complete.  cc @snowp @mattklein123 for a second opinion"
      },
      {
        "user": "mattklein123",
        "created_at": "2022-02-07T16:18:32Z",
        "body": "> so I think the correct thing to do in that case is continue to allow request timeouts until the request is fully complete. cc @snowp @mattklein123 for a second opinion\r\n\r\nI'm pretty sure we still do the request timeouts, but I think the issue is that the stat is not incremented in that case? Assuming that is true, to be perfectly honest I don't remember the history here and why we only increment the stat if nothing has been received. I think it's probably OK to increment the stat in all cases but we should definitely runtime guard the fix."
      },
      {
        "user": "agrawroh",
        "created_at": "2022-02-08T23:54:36Z",
        "body": "@mattklein123 Would the right fix here be to just remove this condition which looks for `awaitingHeaders()` before increasing these stats?\r\n\r\n```\r\n// Don't do work for upstream requests we've already seen headers for.\r\nif (upstream_request->awaitingHeaders()) { ...\r\n```"
      },
      {
        "user": "mattklein123",
        "created_at": "2022-02-09T00:04:05Z",
        "body": "Yes I think that is right. I can't really see any reason to guard that, but it must be that way historically for some reason. I would definitely guard this change behind a runtime flag."
      }
    ]
  },
  {
    "number": 19729,
    "title": "code coverage for contrib extension",
    "created_at": "2022-01-28T09:08:50Z",
    "closed_at": "2022-02-08T14:20:12Z",
    "labels": [
      "area/build",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19729",
    "body": "*Title*: code coverage for contrib extension\r\n\r\n*Description*:\r\nThe \"bazel coverage\" could not generate the code coverage report of contrib extension,  is there any plan to implement it?  \r\n\r\n[optional *Relevant Links*:]\r\nNo",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19729/comments",
    "author": "dorisd0102",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-01-28T15:53:58Z",
        "body": "This would be nice to have somehow, but it's not implemented yet. I will mark it help wanted."
      },
      {
        "user": "daixiang0",
        "created_at": "2022-01-29T02:45:41Z",
        "body": "/assign"
      },
      {
        "user": "daixiang0",
        "created_at": "2022-01-29T02:49:29Z",
        "body": "@dorisd0102 you can use `bazel.coverage //contrib/...` target to get it."
      },
      {
        "user": "dorisd0102",
        "created_at": "2022-02-07T02:36:10Z",
        "body": "Hi @daixiang0 ,\r\nFor non-contrib code under source, after bazel coverage, there is code coverage in bazel-out/_coverage/_coverage_report.dat , then  I could use \"genhtml bazel-out/_coverage/_coverage_report.dat -o generated/coverage\" to generate a coverage report. \r\n\r\nBut for contrib code, it seems no code coverage reported in bazel-out/_coverage/_coverage_report.dat after bazel coverage run.\r\n\r\nThis is my execuction:\r\n$ bazel coverage //contrib/sip_proxy/filters/network/test:cov_test\r\nINFO: Build completed successfully, 3 total actions\r\n//contrib/sip_proxy/filters/network/test:cov_test               (cached) PASSED in 9.1s\r\n  /home/felixdu/.cache/bazel/_bazel_felixdu/29c3a0bddff37a5f1a60349652c66c6a/execroot/envoy/bazel-out/k8-fastbuild/testlogs/contrib/sip_proxy/filters/network/test/cov_test/coverage.dat\r\n$ grep contrib bazel-out/_coverage/_coverage_report.dat | wc\r\n      0       0       0\r\n\r\n\r\n"
      },
      {
        "user": "daixiang0",
        "created_at": "2022-02-07T03:50:15Z",
        "body": "Please run ` bazel.coverage //contrib/sip_proxy/filters/network/...`:\r\n```\r\nWriting directory view page.\r\nOverall coverage rate:\r\n  lines......: 5.0% (2307 of 46145 lines)\r\n  functions..: 6.0% (494 of 8266 functions)\r\n\r\n````"
      },
      {
        "user": "daixiang0",
        "created_at": "2022-02-07T03:51:18Z",
        "body": "There is no target about `cov_test`:\r\n```\r\n//contrib/sip_proxy/filters/network/test:app_exception_impl_test         PASSED in 24.5s\r\n  /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/execroot/envoy/bazel-out/k8-fastbuild/testlogs/contrib/sip_proxy/filters/network/test/app_exception_impl_test/coverage.dat\r\n//contrib/sip_proxy/filters/network/test:config_test                     PASSED in 29.0s\r\n  /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/execroot/envoy/bazel-out/k8-fastbuild/testlogs/contrib/sip_proxy/filters/network/test/config_test/coverage.dat\r\n//contrib/sip_proxy/filters/network/test:conn_manager_test               PASSED in 31.3s\r\n  /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/execroot/envoy/bazel-out/k8-fastbuild/testlogs/contrib/sip_proxy/filters/network/test/conn_manager_test/coverage.dat\r\n//contrib/sip_proxy/filters/network/test:decoder_test                    PASSED in 31.3s\r\n  /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/execroot/envoy/bazel-out/k8-fastbuild/testlogs/contrib/sip_proxy/filters/network/test/decoder_test/coverage.dat\r\n//contrib/sip_proxy/filters/network/test:router_test                     PASSED in 26.8s\r\n  /build/tmp/_bazel_envoybuild/b570b5ccd0454dc9af9f65ab1833764d/execroot/envoy/bazel-out/k8-fastbuild/testlogs/contrib/sip_proxy/filters/network/test/router_test/coverage.dat\r\n```"
      },
      {
        "user": "dorisd0102",
        "created_at": "2022-02-07T06:31:03Z",
        "body": "@daixiang0  Many thanks for your quick response./so\r\n\r\n\r\nIt seems overall coverage rate reported in bazel.coverage only include the code coverage in source/ that contrib/sip-proxy called. It dose not include the source code under contrib.\r\n\r\nyou could check the detailed  coverage data in bazel-out/_coverage/_coverage_report.dat.\r\n\r\n\r\n"
      },
      {
        "user": "daixiang0",
        "created_at": "2022-02-08T05:30:10Z",
        "body": "#19867 may help."
      }
    ]
  },
  {
    "number": 19542,
    "title": "make 0-rtt configurable",
    "created_at": "2022-01-13T21:43:26Z",
    "closed_at": "2024-01-04T01:05:37Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/quic",
      "cronvoy-mvp"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19542",
    "body": "In case folks want to do it for non-idempotent requests, etc.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19542/comments",
    "author": "alyssawilk",
    "comments": [
      {
        "user": "danzh2010",
        "created_at": "2022-03-09T15:35:11Z",
        "body": "Can you clarify a bit more about what the config knob should guard? Allowing non-safe requests to be sent over 0-RTT?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-03-09T15:37:28Z",
        "body": "yeah I think there's times folks have wanted to to 0-rtt for gRPC\r\nThat said this one is for mvp so I don't think we need to tackle it yet. "
      },
      {
        "user": "ggreenway",
        "created_at": "2024-01-04T01:05:37Z",
        "body": "This was fixed in #20242 and #20167"
      }
    ]
  },
  {
    "number": 19429,
    "title": "Document Dynamic Forward Proxy misconfiguration pitfalls",
    "created_at": "2022-01-06T17:44:03Z",
    "closed_at": "2022-05-09T15:29:09Z",
    "labels": [
      "area/docs",
      "area/security",
      "help wanted",
      "area/forward proxy",
      "area/rbac"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19429",
    "body": "DFP presents a potential confused deputy attack vector against networks if not correctly configured. For example, it's possible to reach private network, loopback or link-local addresses (including cloud metadata servers), unless either Envoy RBAC or external kernel / container / network firewall rules prevent this. There are best practices, e.g. using a default deny RBAC with DFP, that prevent this. At a minimum, operators should be aware that DFP should not be used without careful attention to non-public IP routability and resolution.\r\n\r\nThis issue tracks improving documentation around this. I have raised his issue with the Envoy OSS PST and confirmed that this makes sense to address in the open as a documentation exercise, since DFP is strictly WAI in these cases.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19429/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-02-05T20:01:21Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-08T16:01:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-15T16:05:48Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ]
  },
  {
    "number": 19099,
    "title": "Question about different listeners listen same port when enable_reuse_port set to true.",
    "created_at": "2021-11-25T06:26:11Z",
    "closed_at": "2021-12-01T18:30:40Z",
    "labels": [
      "bug",
      "help wanted",
      "area/listener"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/19099",
    "body": "When enable_reuse_port set to ture,different listeners can listen same port without error,and the accept behavior depand on kernel,maybe cause route unpredictable?\r\n```\r\nadmin:\r\n  address:\r\n    socket_address: { address: 127.0.0.1, port_value: 9901 }\r\nstats_config:\r\n  stats_matcher:\r\n    reject_all: true\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 8888\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.tcp_proxy\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n                stat_prefix: destination\r\n                cluster: cluster_0\r\n    - name: listener_1\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 8888\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.tcp_proxy\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n                stat_prefix: destination\r\n                cluster: cluster_1\r\n  clusters:\r\n    - name: cluster_0\r\n      connect_timeout: 30s\r\n      type: STATIC\r\n      load_assignment:\r\n        cluster_name: cluster_0\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: 127.0.0.1\r\n                      port_value: 5001\r\n    - name: cluster_1\r\n      connect_timeout: 30s\r\n      type: STATIC\r\n      load_assignment:\r\n        cluster_name: cluster_1\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: 127.0.0.1\r\n                      port_value: 5002\r\n```\r\nSince envoy1.20 ,enable_reuse_port is default to true.\r\nWhen use the static config above,the request to port 8888 will randomly route to cluster_0 or cluster_1.\r\nIs that a reasonable behavior?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/19099/comments",
    "author": "pyrl247",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2021-11-29T19:39:37Z",
        "body": "No, this is a bug. The configuration should have failed validation in this case."
      },
      {
        "user": "ggreenway",
        "created_at": "2021-11-29T19:40:03Z",
        "body": "cc @mattklein123 "
      },
      {
        "user": "mattklein123",
        "created_at": "2021-11-29T20:04:34Z",
        "body": "Yeah definitely a bug. I can look into fixing."
      }
    ]
  },
  {
    "number": 18880,
    "title": "handle http/2 with no initial streams",
    "created_at": "2021-11-03T15:51:57Z",
    "closed_at": "2022-01-13T13:45:36Z",
    "labels": [
      "bug",
      "help wanted",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18880",
    "body": "right now it looks like we won't increase max streams.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18880/comments",
    "author": "alyssawilk",
    "comments": [
      {
        "user": "YaoZengzeng",
        "created_at": "2021-11-05T07:09:51Z",
        "body": "@alyssawilk @mattklein123 maybe I'd like to fix this:)"
      },
      {
        "user": "alyssawilk",
        "created_at": "2021-11-08T14:41:25Z",
        "body": "Feel free to take a pass at it and let me know if you need pointers."
      },
      {
        "user": "YaoZengzeng",
        "created_at": "2021-11-11T13:16:04Z",
        "body": "AFAK, now the **concurrent_stream_limit_** could only adjust downward and when it drops to zero, the state of client will transist to **DRAINING**. \r\n\r\nSo what I should do is make the **concurrent_stream_limit_** configurable, not just downward. And transist the state of client to **BUSY** other than **DRAINING**, when **concurrent_stream_limit_** become zero.\r\n\r\nIs it correct? @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2021-11-11T21:03:10Z",
        "body": "That sounds right!  You should be able to turn up the 0 streams integration test to make sure you have that logic right.\r\nThe other gotcha is if the Envoy is say configured to have a max concurrent streams of 100, and we start at 10 and then the client renegotiates to 150, we still want to cap it at 100, so be careful not to exceed the configured bounds :-)"
      },
      {
        "user": "YaoZengzeng",
        "created_at": "2021-11-12T13:04:55Z",
        "body": "@alyssawilk thanks for your reminder :)\r\n\r\nAnother question, if now we have a max concurrent streams of 100 and 80 of them are active streams, then the client renegotiates to 60. So **part I**: the state of client should transist from READY to BUSY, right? On the contrary, **part II**: if now the state of client is BUSY and the max concurrent streams get increased, the state should transist to READY?\r\n\r\nIf it is right, **part I** seems have not implemented yet, I could start from it."
      }
    ]
  },
  {
    "number": 18726,
    "title": "Add link to closed issue when creating newer in deps release checker",
    "created_at": "2021-10-22T09:47:50Z",
    "closed_at": "2021-10-25T10:42:44Z",
    "labels": [
      "enhancement",
      "help wanted",
      "dependencies"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18726",
    "body": "### description\r\n\r\nThe check-deps release checker very usefully creates newer issues as they become available, and closes any existing issues\r\n\r\nFor PRs that have been created to resolve the existing issues, assuming you update the PR to target the newer version, you also need to change the issue that it fixes\r\n\r\nIt would be helpful to be able to find the newer issue from the old one to update the PR with\r\n\r\n### related\r\n\r\n#16705 #17876 \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18726/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2021-10-22T09:50:47Z",
        "body": "cc @ME-ON1 "
      },
      {
        "user": "ME-ON1",
        "created_at": "2021-10-25T09:35:13Z",
        "body": "i dont get it. Which link do we want in the closed issue ? "
      },
      {
        "user": "phlax",
        "created_at": "2021-10-25T10:42:44Z",
        "body": "apologies it is acutally there already - i opened this ticket to follow up the offline conversation we had - but it seems like its not needed - apologies for noise\r\n"
      }
    ]
  },
  {
    "number": 18691,
    "title": "network external auth failure reason not set in access logs",
    "created_at": "2021-10-20T15:25:56Z",
    "closed_at": "2021-10-28T15:07:26Z",
    "labels": [
      "bug",
      "help wanted",
      "area/ext_authz"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18691",
    "body": "\r\n*Description*:\r\n\r\nI've configured network external auth to validate a TLS client certificate. It's working as expected, however when the extauthz validation fails, I'm not setting the failure reason in the normal access log or the ALS tcp log.\r\n\r\nMy log:\r\nAuthorizationServerCheck error=\"no peer cert\" \r\n\r\nAccess Log:\r\n\r\n```\r\n{\r\n\t\"downstream_peer_uri_san\": null,\r\n\t\"duration\": 4,\r\n\t\"flags_rx\": \"-\",\r\n\t\"downstream_remote_address\": \"127.0.0.1\",\r\n\t\"req_host\": null,\r\n\t\"req_path\": null,\r\n\t\"downstream_tls_cipher\": \"TLS_AES_256_GCM_SHA384\",\r\n\t\"resp_duration\": null,\r\n\t\"bytes_tx\": 0,\r\n\t\"req_method\": null,\r\n\t\"downstream_direct_remote_address\": \"127.0.0.1\",\r\n\t\"req_duration\": null,\r\n\t\"upstream_transport_failure_reason\": null,\r\n\t\"downstream_local_address\": \"127.0.0.1:443\",\r\n\t\"resp_code\": 0,\r\n\t\"downstream_peer_subject\": null,\r\n\t\"downstream_tls_session_id\": null,\r\n\t\"resp_code_details\": null,\r\n\t\"host\": \"...\",\r\n\t\"downstream_tls_version\": \"TLSv1.3\",\r\n\t\"req_server_name\": \"localhost\",\r\n\t\"upstream_local_address\": \"127.0.0.1:61857\",\r\n\t\"bytes_rx\": 134,\r\n\t\"upstream_cluster\": \"test\",\r\n\t\"ts\": \"2021-10-20T14:36:00.1634740560.155+0000\",\r\n\t\"route_name\": null,\r\n\t\"upstream_host\": \"127.0.0.1:8443\",\r\n\t\"downstream_peer_issuer\": null\r\n}\r\n```\r\n\r\nALS:\r\n\r\n```\r\n{\r\n\t\"common\": {\r\n\t\t\"downstream-direct-remote-address\": {\r\n\t\t\t\"socket-address\": {\r\n\t\t\t\t\"address\": \"127.0.0.1\",\r\n\t\t\t\t\"ipv4-compat\": false,\r\n\t\t\t\t\"named-port\": \"\",\r\n\t\t\t\t\"port-value\": 62241,\r\n\t\t\t\t\"protocol\": \"TCP\",\r\n\t\t\t\t\"resolver\": \"\"\r\n\t\t\t}\r\n\t\t},\r\n\t\t\"downstream-local-address\": {\r\n\t\t\t\"socket-address\": {\r\n\t\t\t\t\"address\": \"127.0.0.1\",\r\n\t\t\t\t\"ipv4-compat\": false,\r\n\t\t\t\t\"named-port\": \"\",\r\n\t\t\t\t\"port-value\": 443,\r\n\t\t\t\t\"protocol\": \"TCP\",\r\n\t\t\t\t\"resolver\": \"\"\r\n\t\t\t}\r\n\t\t},\r\n\t\t\"downstream-remote-address\": {\r\n\t\t\t\"socket-address\": {\r\n\t\t\t\t\"address\": \"127.0.0.1\",\r\n\t\t\t\t\"ipv4-compat\": false,\r\n\t\t\t\t\"named-port\": \"\",\r\n\t\t\t\t\"port-value\": 62241,\r\n\t\t\t\t\"protocol\": \"TCP\",\r\n\t\t\t\t\"resolver\": \"\"\r\n\t\t\t}\r\n\t\t},\r\n\t\t\"response-flags\": {},\r\n\t\t\"route-name\": \"\",\r\n\t\t\"sample-rate\": 0,\r\n\t\t\"start-time\": \"2021-10-20T15:00:23Z\",\r\n\t\t\"tls\": {\r\n\t\t\t\"cipher-suite\": \"TLS_AES_256_GCM_SHA384\",\r\n\t\t\t\"local-certificate\": {\r\n\t\t\t\t\"subject\": \"O=TEST\"\r\n\t\t\t},\r\n\t\t\t\"peer-certificate\": {\r\n\t\t\t\t\"subject\": \"\"\r\n\t\t\t},\r\n\t\t\t\"session-id\": \"\",\r\n\t\t\t\"sni\": \"localhost\",\r\n\t\t\t\"version\": \"TLSv1_3\"\r\n\t\t},\r\n\t\t\"upstream-cluster\": \"test\",\r\n\t\t\"upstream-local-address\": {\r\n\t\t\t\"socket-address\": {\r\n\t\t\t\t\"address\": \"127.0.0.1\",\r\n\t\t\t\t\"ipv4-compat\": false,\r\n\t\t\t\t\"named-port\": \"\",\r\n\t\t\t\t\"port-value\": 62242,\r\n\t\t\t\t\"protocol\": \"TCP\",\r\n\t\t\t\t\"resolver\": \"\"\r\n\t\t\t}\r\n\t\t},\r\n\t\t\"upstream-remote-address\": {\r\n\t\t\t\"socket-address\": {\r\n\t\t\t\t\"address\": \"127.0.0.1\",\r\n\t\t\t\t\"ipv4-compat\": false,\r\n\t\t\t\t\"named-port\": \"\",\r\n\t\t\t\t\"port-value\": 8443,\r\n\t\t\t\t\"protocol\": \"TCP\",\r\n\t\t\t\t\"resolver\": \"\"\r\n\t\t\t}\r\n\t\t},\r\n\t\t\"upstream-transport-failure-reason\": \"\"\r\n\t},\r\n\t\"connection\": {\r\n\t\t\"received-bytes\": 134,\r\n\t\t\"sent-bytes\": 0\r\n\t}\r\n}\r\n```\r\n\r\nListener Config Excerpt\r\n\r\n```\r\n{\r\n\t\"dynamic_listeners\": [{\r\n\t\t\"name\": \"listener_443:0.0.0.0:443\",\r\n\t\t\"active_state\": {\r\n\t\t\t\"version_info\": \"1634700180450893000\",\r\n\t\t\t\"listener\": {\r\n\t\t\t\t\"@type\": \"type.googleapis.com/envoy.config.listener.v3.Listener\",\r\n\t\t\t\t\"name\": \"listener_443:0.0.0.0:443\",\r\n\t\t\t\t\"address\": {\r\n\t\t\t\t\t\"socket_address\": {\r\n\t\t\t\t\t\t\"address\": \"0.0.0.0\",\r\n\t\t\t\t\t\t\"port_value\": 443\r\n\t\t\t\t\t}\r\n\t\t\t\t},\r\n\t\t\t\t\"filter_chains\": [{\r\n\t\t\t\t\t\t\"filter_chain_match\": {\r\n\t\t\t\t\t\t\t\"server_names\": [\r\n\t\t\t\t\t\t\t\t\"localhost\"\r\n\t\t\t\t\t\t\t]\r\n\t\t\t\t\t\t},\r\n\t\t\t\t\t\t\"filters\": [{\r\n\t\t\t\t\t\t\t\t\"name\": \"envoy.filters.network.ext_authz\",\r\n\t\t\t\t\t\t\t\t\"typed_config\": {\r\n\t\t\t\t\t\t\t\t\t\"@type\": \"type.googleapis.com/envoy.extensions.filters.network.ext_authz.v3.ExtAuthz\",\r\n\t\t\t\t\t\t\t\t\t\"stat_prefix\": \"network_authz\",\r\n\t\t\t\t\t\t\t\t\t\"grpc_service\": {\r\n\t\t\t\t\t\t\t\t\t\t\"envoy_grpc\": {\r\n\t\t\t\t\t\t\t\t\t\t\t\"cluster_name\": \"extauthz\"\r\n\t\t\t\t\t\t\t\t\t\t},\r\n\t\t\t\t\t\t\t\t\t\t\"timeout\": \"10s\"\r\n\t\t\t\t\t\t\t\t\t},\r\n\t\t\t\t\t\t\t\t\t\"include_peer_certificate\": true,\r\n\t\t\t\t\t\t\t\t\t\"transport_api_version\": \"V3\"\r\n\t\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\t},\r\n\t\t\t\t\t\t\t{\r\n\t\t\t\t\t\t\t\t\"name\": \"envoy.filters.network.tcp_proxy\",\r\n\t\t\t\t\t\t\t\t\"typed_config\": {\r\n\t\t\t\t\t\t\t\t\t\"@type\": \"type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\",\r\n\t\t\t\t\t\t\t\t\t\"stat_prefix\": \"ingress_tcp\",\r\n\t\t\t\t\t\t\t\t\t\"cluster\": \"test\",\r\n\t\t\t\t\t\t\t\t\t\"access_log\": [{\r\n\t\t\t\t\t\t\t\t\t\t\"name\": \"envoy.file_access_log\",\r\n\t\t\t\t\t\t\t\t\t\t\"typed_config\": {\r\n\t\t\t\t\t\t\t\t\t\t\t\"@type\": \"type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\",\r\n\t\t\t\t\t\t\t\t\t\t\t\"path\": \"/dev/stdout\",\r\n\t\t\t\t\t\t\t\t\t\t\t\"log_format\": {\r\n\t\t\t\t\t\t\t\t\t\t\t\t\"json_format\": {\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"duration\": \"%DURATION%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"req_server_name\": \"%REQUESTED_SERVER_NAME%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"upstream_cluster\": \"%UPSTREAM_CLUSTER%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"downstream_local_address\": \"%DOWNSTREAM_LOCAL_ADDRESS%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"resp_duration\": \"%RESPONSE_DURATION%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"bytes_tx\": \"%BYTES_SENT%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"downstream_peer_uri_san\": \"%DOWNSTREAM_PEER_URI_SAN%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"ts\": \"%START_TIME(%Y-%m-%dT%H:%M:%S.%s.%3f%z)%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"req_method\": \"%REQ(:METHOD)%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"req_path\": \"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"req_duration\": \"%REQUEST_DURATION%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"resp_code_details\": \"%RESPONSE_CODE_DETAILS%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"downstream_direct_remote_address\": \"%DOWNSTREAM_DIRECT_REMOTE_ADDRESS_WITHOUT_PORT%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"resp_code\": \"%RESPONSE_CODE%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"bytes_rx\": \"%BYTES_RECEIVED%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"upstream_local_address\": \"%UPSTREAM_LOCAL_ADDRESS%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"upstream_transport_failure_reason\": \"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"downstream_peer_subject\": \"%DOWNSTREAM_PEER_SUBJECT%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"host\": \"%HOSTNAME%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"upstream_host\": \"%UPSTREAM_HOST%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"req_host\": \"%REQ(:AUTHORITY)%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"downstream_tls_session_id\": \"%DOWNSTREAM_TLS_SESSION_ID%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"downstream_tls_version\": \"%DOWNSTREAM_TLS_VERSION%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"downstream_remote_address\": \"%DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"route_name\": \"%ROUTE_NAME%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"downstream_peer_issuer\": \"%DOWNSTREAM_PEER_ISSUER%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"downstream_tls_cipher\": \"%DOWNSTREAM_TLS_CIPHER%\",\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\"flags_rx\": \"%RESPONSE_FLAGS%\"\r\n\t\t\t\t\t\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\t\t\t}]\r\n\t\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t],\r\n\t\t\t\t\t\t\"transport_socket\": {\r\n\t\t\t\t\t\t\t\"name\": \"envoy.transport_sockets.tls\",\r\n\t\t\t\t\t\t\t\"typed_config\": {\r\n\t\t\t\t\t\t\t\t\"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\",\r\n\t\t\t\t\t\t\t\t\"common_tls_context\": {\r\n\t\t\t\t\t\t\t\t\t\"tls_params\": {\r\n\t\t\t\t\t\t\t\t\t\t\"tls_minimum_protocol_version\": \"TLSv1_2\"\r\n\t\t\t\t\t\t\t\t\t},\r\n\t\t\t\t\t\t\t\t\t\"tls_certificates\": [{\r\n\t\t\t\t\t\t\t\t\t\t\"REDACTED\": \"REDACTED\"\r\n\t\t\t\t\t\t\t\t\t}],\r\n\t\t\t\t\t\t\t\t\t\"validation_context\": {\r\n\t\t\t\t\t\t\t\t\t\t\"trust_chain_verification\": \"ACCEPT_UNTRUSTED\"\r\n\t\t\t\t\t\t\t\t\t},\r\n\t\t\t\t\t\t\t\t\t\"alpn_protocols\": [\r\n\t\t\t\t\t\t\t\t\t\t\"h2\",\r\n\t\t\t\t\t\t\t\t\t\t\"http/1.1\"\r\n\t\t\t\t\t\t\t\t\t]\r\n\t\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t},\r\n\t\t\t\t\t\t\"name\": \"tls_server_filter_chain\"\r\n\t\t\t\t\t}\r\n\r\n\t\t\t\t],\r\n\t\t\t\t\"per_connection_buffer_limit_bytes\": 32768,\r\n\t\t\t\t\"listener_filters\": [{\r\n\t\t\t\t\t\"name\": \"envoy.filters.listener.tls_inspector\"\r\n\t\t\t\t}]\r\n\t\t\t}\r\n\t\t}\r\n\t}]\r\n}\r\n\r\n```\r\n\r\nCluster Config Excerpt\r\n\r\n```\r\n{\r\n\t\"dynamic_active_clusters\": [{\r\n\t\t\t\"cluster\": {\r\n\t\t\t\t\"@type\": \"type.googleapis.com/envoy.config.cluster.v3.Cluster\",\r\n\t\t\t\t\"name\": \"extauthz\",\r\n\t\t\t\t\"type\": \"STRICT_DNS\",\r\n\t\t\t\t\"connect_timeout\": \"10s\",\r\n\t\t\t\t\"per_connection_buffer_limit_bytes\": 32768,\r\n\t\t\t\t\"lb_policy\": \"LEAST_REQUEST\",\r\n\t\t\t\t\"circuit_breakers\": {\r\n\t\t\t\t\t\"thresholds\": [{}]\r\n\t\t\t\t},\r\n\t\t\t\t\"dns_refresh_rate\": \"300s\",\r\n\t\t\t\t\"dns_lookup_family\": \"V4_ONLY\",\r\n\t\t\t\t\"outlier_detection\": {},\r\n\t\t\t\t\"upstream_connection_options\": {\r\n\t\t\t\t\t\"tcp_keepalive\": {}\r\n\t\t\t\t},\r\n\t\t\t\t\"load_assignment\": {\r\n\t\t\t\t\t\"cluster_name\": \"extauthz\",\r\n\t\t\t\t\t\"endpoints\": [{\r\n\t\t\t\t\t\t\"lb_endpoints\": [{\r\n\t\t\t\t\t\t\t\"endpoint\": {\r\n\t\t\t\t\t\t\t\t\"address\": {\r\n\t\t\t\t\t\t\t\t\t\"socket_address\": {\r\n\t\t\t\t\t\t\t\t\t\t\"address\": \"127.0.0.1\",\r\n\t\t\t\t\t\t\t\t\t\t\"port_value\": 7001\r\n\t\t\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\t},\r\n\t\t\t\t\t\t}]\r\n\t\t\t\t\t}]\r\n\t\t\t\t},\r\n\t\t\t\t\"typed_extension_protocol_options\": {\r\n\t\t\t\t\t\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\": {\r\n\t\t\t\t\t\t\"@type\": \"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\r\n\t\t\t\t\t\t\"common_http_protocol_options\": {\r\n\t\t\t\t\t\t\t\"headers_with_underscores_action\": \"REJECT_REQUEST\"\r\n\t\t\t\t\t\t},\r\n\t\t\t\t\t\t\"upstream_http_protocol_options\": {},\r\n\t\t\t\t\t\t\"explicit_http_config\": {\r\n\t\t\t\t\t\t\t\"http2_protocol_options\": {\r\n\t\t\t\t\t\t\t\t\"max_concurrent_streams\": 100,\r\n\t\t\t\t\t\t\t\t\"initial_stream_window_size\": 65536,\r\n\t\t\t\t\t\t\t\t\"initial_connection_window_size\": 1048576\r\n\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t}\r\n\t\t\t\t},\r\n\t\t\t\t\"respect_dns_ttl\": true\r\n\t\t\t},\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"cluster\": {\r\n\t\t\t\t\"@type\": \"type.googleapis.com/envoy.config.cluster.v3.Cluster\",\r\n\t\t\t\t\"name\": \"test\",\r\n\t\t\t\t\"type\": \"STRICT_DNS\",\r\n\t\t\t\t\"connect_timeout\": \"10s\",\r\n\t\t\t\t\"per_connection_buffer_limit_bytes\": 32768,\r\n\t\t\t\t\"lb_policy\": \"LEAST_REQUEST\",\r\n\t\t\t\t\"circuit_breakers\": {\r\n\t\t\t\t\t\"thresholds\": [{}]\r\n\t\t\t\t},\r\n\t\t\t\t\"dns_refresh_rate\": \"300s\",\r\n\t\t\t\t\"dns_lookup_family\": \"V4_ONLY\",\r\n\t\t\t\t\"outlier_detection\": {},\r\n\t\t\t\t\"transport_socket\": {\r\n\t\t\t\t\t\"name\": \"envoy.transport_sockets.tls\",\r\n\t\t\t\t\t\"typed_config\": {\r\n\t\t\t\t\t\t\"@type\": \"type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\",\r\n\t\t\t\t\t\t\"common_tls_context\": {\r\n\t\t\t\t\t\t\t\"validation_context\": {}\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t}\r\n\t\t\t\t},\r\n\t\t\t\t\"upstream_connection_options\": {\r\n\t\t\t\t\t\"tcp_keepalive\": {}\r\n\t\t\t\t},\r\n\t\t\t\t\"load_assignment\": {\r\n\t\t\t\t\t\"cluster_name\": \"test\",\r\n\t\t\t\t\t\"endpoints\": [{\r\n\t\t\t\t\t\t\"lb_endpoints\": [{\r\n\t\t\t\t\t\t\t\"endpoint\": {\r\n\t\t\t\t\t\t\t\t\"address\": {\r\n\t\t\t\t\t\t\t\t\t\"socket_address\": {\r\n\t\t\t\t\t\t\t\t\t\t\"address\": \"127.0.0.1\",\r\n\t\t\t\t\t\t\t\t\t\t\"port_value\": 8443\r\n\t\t\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\t},\r\n\t\t\t\t\t\t\t\"health_status\": \"HEALTHY\",\r\n\t\t\t\t\t\t}]\r\n\t\t\t\t\t}]\r\n\t\t\t\t},\r\n\t\t\t\t\"respect_dns_ttl\": true\r\n\t\t\t},\r\n\t\t}\r\n\t]\r\n}\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18691/comments",
    "author": "marcosrmendezthd",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2021-10-21T04:31:52Z",
        "body": "@dio @ggreenway any thoughts on this one?"
      },
      {
        "user": "ggreenway",
        "created_at": "2021-10-21T15:48:53Z",
        "body": "Which field are you expecting something to be in? Response flags? \r\n\r\nI took a brief look at the code, and it appears that the network ext_auth does not set `UAEX`, but http ext_auth does. This seems like a bug; they should both set that response flag."
      },
      {
        "user": "marcosrmendezthd",
        "created_at": "2021-10-21T19:12:20Z",
        "body": "agreed. i think the response code details should also be set:\r\n\r\nenvoy access log\r\n\r\n```\r\n\"resp_code_details\": \"ext_authz_denied\"\r\n```\r\n\r\nals\r\n```\r\n\"response\": {\r\n\t\"code-details\": \"ext_authz_denied\",\r\n}\r\n```\r\n"
      },
      {
        "user": "ggreenway",
        "created_at": "2021-10-21T20:37:30Z",
        "body": "Yep, that makes sense to me. If anyone wants to fix, it should be a pretty simple change to make."
      }
    ]
  },
  {
    "number": 18066,
    "title": "stats scope for grpc access log should be envoy wide",
    "created_at": "2021-09-10T03:52:27Z",
    "closed_at": "2021-10-01T01:53:32Z",
    "labels": [
      "bug",
      "help wanted",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18066",
    "body": "*Description*:\r\n\r\nCurrently an grpc access logger is instantiate using the creator' context. Usually the owner is an filter that can be destroyed.\r\nThe logger use the stats scope of the owner. It's fine if the logger is destroyed along with the owner.\r\nHowever, the grpc access logger is expensive so that logger is cached. The logger can outlive its creator and the logger stats could refer a destroyed counter.\r\n\r\nSince the grpc access loggers are shared for good, perhaps we should maintain an grpc access logger manager wide scope.\r\nAnd generate server wide `access_logs.grpc_access_log.logs_written`.\r\n\r\nThough we lost the detailed \"<per_filter_prefix>.access_logs.grpc_access_log.logs_written\" stats, but that counter is broken already when the cache is introduced.\r\n\r\ncc @mattklein123 @ggreenway @jmarantz ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18066/comments",
    "author": "lambdai",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2021-09-10T05:59:34Z",
        "body": "#18067 explains how to pin the scope in access logger cache.\r\nHowever, #18067 finds the root scope by using the dynamic cast.\r\nNot ideal but it avoid introducing new method in CommonFactoryContext for now.\r\n\r\n#18067 on top of the reverted #17931 passes the extra tests in go-control-plane"
      },
      {
        "user": "mattklein123",
        "created_at": "2021-09-10T15:24:52Z",
        "body": "There are 2 possible fixes:\r\n1) If logger stats need to be scoped to the creator, we can figure out a way of having the stats be passed by the creator, and not created by the logger.\r\n2) If logger stats can be global, then I think your fix is OK."
      },
      {
        "user": "lambdai",
        "created_at": "2021-09-10T19:14:40Z",
        "body": "> If logger stats need to be scoped to the creator, we can figure out a way of having the stats be passed by the creator, and not created by the logger.\r\n\r\nThis approach need some work with the the logger cache. A logger attached to the stats prefix A should not be used by stats_prefix B. \r\n\r\nFollowing this path, perhaps we can split the stats part from the cached logger as in #18067 \r\n\r\nThe logger user  gets a shared logger from logger cache. \r\nThe logger user create a new `logger with stats`, wrapping the above shared logger and offer a stat."
      },
      {
        "user": "jmarantz",
        "created_at": "2021-09-10T20:00:32Z",
        "body": "qq: in the current system, how often is the Stats Scope created/destroyed?\r\n\r\nThere's a fair amount of overhead for that so I wanted to make sure that was not happening per-request."
      },
      {
        "user": "lambdai",
        "created_at": "2021-09-10T21:35:17Z",
        "body": "> qq: in the current system, how often is the Stats Scope created/destroyed?\r\n\r\nI don't think there are many Scope created per-request. Perhaps 0 if not close to 0.\r\n\r\ncreateScope()/scopeFromElements()/scopeFromStatNames() occurrences are rare."
      },
      {
        "user": "ggreenway",
        "created_at": "2021-09-13T15:51:45Z",
        "body": "Yeah, as far as I can tell, we only create/destroy scopes on config updates."
      }
    ]
  },
  {
    "number": 17690,
    "title": "Seeing stats for http connection manager that has been removed from the filter chain",
    "created_at": "2021-08-12T00:11:54Z",
    "closed_at": "2021-09-02T03:00:49Z",
    "labels": [
      "bug",
      "help wanted",
      "area/stats",
      "area/listener"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17690",
    "body": "Hi all,\r\n\r\nI am seeing the following behavior with HTTP connection manager stats and wanted to check if the behavior is expected. \r\nBelow is my dynamic listener config\r\n\r\n```\r\n\"dynamic_listeners\": [\r\n    {\r\n     \"name\": \"listener\",\r\n     \"active_state\": {\r\n      \"listener\": {\r\n       \"@type\": \"type.googleapis.com/envoy.config.listener.v3.Listener\",\r\n       \"name\": \"listener\",\r\n       },\r\n       \"filter_chains\": [\r\n        {\r\n         \"filter_chain_match\": {\r\n          \"server_names\": [\r\n           \"foo.com\"\r\n          ]\r\n         },\r\n        \"filters\": [\r\n        {\r\n        \"name\": \"envoy.http_connection_manager\",\r\n           \"typed_config\": {\r\n            \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\r\n            \"stat_prefix\": \"foo\",\r\n            \"rds\": {\r\n             \"config_source\": {\r\n              \"ads\": {},\r\n              \"resource_api_version\": \"V3\"\r\n             },\r\n             \"route_config_name\": \"foo\"\r\n            },\r\n       ]\r\n       },\r\n       {\r\n         \"filter_chain_match\": {\r\n          \"server_names\": [\r\n           \"bar.com\"\r\n          ]\r\n         },\r\n        \"filters\": [\r\n        {\r\n        \"name\": \"envoy.http_connection_manager\",\r\n           \"typed_config\": {\r\n            \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\r\n            \"stat_prefix\": \"bar\",\r\n            \"rds\": {\r\n             \"config_source\": {\r\n              \"ads\": {},\r\n              \"resource_api_version\": \"V3\"\r\n             },\r\n             \"route_config_name\": \"bar\"\r\n            },\r\n          ]\r\n         }\r\n ]\r\n```\r\nand with this config, on the stats endpoint, I was seeing the following stats:\r\n```\r\nhttp.foo.downstream_cx_active: 0\r\nhttp.bar.downstream_cx_active:0\r\n.\r\n```\r\n\r\nI have then made an update to the filter chain wherein I removed the filter chain corresponding to `bar.com`, i.e there's only one HTTP connection manager now matching on `foo.com`, I was expecting that there won't be anymore HTTP connection manager related stats from stat_prefix `bar`, but that's not happening. Even after filter chain and listener update, I am seeing stats such as `http.bar.downstream_cx_active: 0`. The envoy config also seems to be correct, but I am able to see stats corresponding to deleted HTTP connection manager.\r\n\r\nMay I know if this is the expected behavior from envoy stats?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17690/comments",
    "author": "akhila1012",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2021-08-13T17:04:31Z",
        "body": "Do the extra stats stay there forever, or only for awhile? I think it is expected that they would remain while the old listener config is draining. I think the default time for that is 15 minutes."
      },
      {
        "user": "akhila1012",
        "created_at": "2021-08-13T18:13:29Z",
        "body": "Hello @ggreenway, it's not for a while, I don't see them vanishing until I re-start envoy again. I have enabled debug logs and checked that the stats stay around even after the old listener is completely drained. \r\n\r\nI am observing this happen with all the downstream stats. For example, I have had other filters in the filter chain whose stats also stay around after the filters are removed. Same is not the case for upstream ones i..e when I remove a cluster from a config, the upstream stats corresponding to the cluster are removed once the cluster is removed."
      },
      {
        "user": "ggreenway",
        "created_at": "2021-08-13T18:15:38Z",
        "body": "cc @mattklein123. This seems unexpected to me; is it possible the listener never finishes draining?"
      },
      {
        "user": "mattklein123",
        "created_at": "2021-08-13T18:20:40Z",
        "body": "This is not expected. It's possible this is a bug around the listener intelligent / filter chain update only feature? cc @jmarantz @lambdai "
      },
      {
        "user": "mattklein123",
        "created_at": "2021-08-13T18:21:26Z",
        "body": "(I'm wondering if a scope is shared across the entire listener, but really it should be per-filter chain.)"
      },
      {
        "user": "akhila1012",
        "created_at": "2021-08-13T18:27:49Z",
        "body": "These were the envoy debug logs from the time I removed the filter chain\r\n```\r\n[2021-08-13T17:51:00.694194Z][Envoy][1][debug][config]begin add/update listener: name=listener hash=11615887453658398091\r\n[2021-08-13T17:51:00.694233Z][Envoy][1][debug][config]use in place update filter chain update path for listener name=listener hash=11615887453658398091\r\n[2021-08-13T17:51:00.694261Z][Envoy][1][debug][config]  filter #0:\r\n[2021-08-13T17:51:00.694262Z][Envoy][1][debug][config]    name: envoy.filters.listener.tls_inspector\r\n[2021-08-13T17:51:00.694277Z][Envoy][1][debug][config]  config: {}\r\n\r\n[2021-08-13T17:51:00.703653Z][Envoy][1][debug][config]new fc_contexts has 9 filter chains, including 0 newly built\r\n[2021-08-13T17:51:00.703664Z][Envoy][1][debug][config]add warming listener: name=listener, hash=11615887453658398091, address=[::]:10000\r\n[2021-08-13T17:51:00.703667Z][Envoy][1][debug][misc]Initialize listener listener local-init-manager.\r\n[2021-08-13T17:51:00.703669Z][Envoy][1][debug][init]init manager Listener-local-init-manager listener 11615887453658398091 contains no targets\r\n[2021-08-13T17:51:00.703670Z][Envoy][1][debug][init]init manager Listener-local-init-manager listener 11615887453658398091 initialized, notifying Listener-local-init-watcher listener\r\n[2021-08-13T17:51:00.703671Z][Envoy][1][debug][config]execute in place filter chain update: name=listener, hash=11615887453658398091, address=[::]:10000\r\n[2021-08-13T17:51:00.703672Z][Envoy][1][debug][config]replacing existing listener 1\r\n[2021-08-13T17:51:00.703706Z][Envoy][1][debug][config]replacing existing listener 1\r\n[2021-08-13T17:51:00.703730Z][Envoy][1][debug][config]replacing existing listener 1\r\n[2021-08-13T17:51:00.703752Z][Envoy][1][debug][config]replacing existing listener 1\r\n[2021-08-13T17:51:00.703788Z][Envoy][1][debug][config]replacing existing listener 1\r\n[2021-08-13T17:51:00.703812Z][Envoy][1][debug][config]replacing existing listener 1\r\n[2021-08-13T17:51:00.703842Z][Envoy][1][debug][config]replacing existing listener 1\r\n[2021-08-13T17:51:00.703867Z][Envoy][1][debug][config]replacing existing listener 1\r\n[2021-08-13T17:51:00.706894Z][Envoy][1][debug][config]draining 1 filter chains in listener listener: name=listener, hash=12608909188243545912, address=[::]:10000\r\n[2021-08-13T17:51:00.706901Z][Envoy][1][info][upstream]lds: add/update listener 'listener'\r\n[2021-08-13T17:51:00.706915Z][Envoy][1][debug][config]Resuming discovery requests for type.googleapis.com/envoy.config.route.v3.RouteConfiguration (previous count 1)\r\n[2021-08-13T17:51:00.706915Z][Envoy][1][debug][config]Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration (previous count 1)\r\n[2021-08-13T17:51:00.706918Z][Envoy][1][debug][config]gRPC config for type.googleapis.com/envoy.config.listener.v3.Listener accepted with 1 resources with version d7a307b4-20b3-4c6d-8f97-67184a2a8591\r\n[2021-08-13T17:51:00.706958Z][Envoy][1][debug][config]Resuming discovery requests for type.googleapis.com/envoy.config.listener.v3.Listener (previous count 1)\r\n[2021-08-13T17:51:03.329722Z][Envoy][1][debug][main]flushing stats\r\n[2021-08-13T17:53:00.700681Z][Envoy][1][debug][config]removing draining filter chains from listener listener: name=listener, hash=12608909188243545912, address=[::]:10000\r\n[2021-08-13T17:53:00.701440Z][Envoy][1][debug][config]draining filter chains from listener listener complete: name=listener, hash=12608909188243545912, address=[::]:10000\r\n[2021-08-13T17:53:00.701496Z][Envoy][1][debug][init]Listener-local-init-watcher listener destroyed\r\n[2021-08-13T17:53:00.702126Z][Envoy][1][debug][init]init manager RDS local-init-manager foo destroyed\r\n[2021-08-13T17:53:00.702137Z][Envoy][1][debug][init]target RdsRouteConfigSubscription local-init-target foo destroyed\r\n[2021-08-13T17:53:00.702139Z][Envoy][1][debug][init]RDS local-init-watcher foo destroyed\r\n[2021-08-13T17:53:00.702142Z][Envoy][1][debug][init]shared target RdsRouteConfigSubscription init foo destroyed\r\n[2021-08-13T17:53:00.703057Z][Envoy][1][debug][init]init manager Listener-local-init-manager listener 12608909188243545912 destroyed\r\n[2021-08-13T17:53:00.703063Z][Envoy][1][debug][init]target Listener-init-target listener destroyed\r\n```"
      },
      {
        "user": "mattklein123",
        "created_at": "2021-08-13T19:09:42Z",
        "body": "Yeah I just looked and I believe the issue is we have a scope for the entire listener, when we actually need it to be per-filter chain. Will switch this to bug, thanks for reporting! cc @lambdai for a potential fix."
      },
      {
        "user": "nitgoy",
        "created_at": "2021-08-14T01:05:17Z",
        "body": "Plugging in the related feature ask #15185. Not sure if that'd help or suffer from exact same issue. But if metrics could be split by filter chain names then we don't need to rely on stat_prefix supplied to individual filters."
      },
      {
        "user": "lambdai",
        "created_at": "2021-08-14T01:14:10Z",
        "body": "Is there any concerns around the overhead on introducing new stats scopes?"
      },
      {
        "user": "mattklein123",
        "created_at": "2021-08-16T14:59:51Z",
        "body": "> Is there any concerns around the overhead on introducing new stats scopes?\r\n\r\nIMO no, scopes are pretty lightweight and I don't see an issue with having one per filter chain. @jmarantz wdyt?"
      },
      {
        "user": "jmarantz",
        "created_at": "2021-08-16T15:02:23Z",
        "body": "What's the lifetime of the filter chain? The filters are request-scoped and recycling scopes at that level might have some overhead. But if they last until an xDS update changes the filter configuration that seems fine."
      },
      {
        "user": "lambdai",
        "created_at": "2021-08-17T21:00:08Z",
        "body": "Thank you for the confirm! The lifetime should be along with filter chain config. It lasts 1+ LDS update.\r\n\r\n"
      }
    ]
  },
  {
    "number": 17687,
    "title": "TEST_P(XfccIntegrationTest, TagExtractedNameGenerationTest) has lots of stale test cases",
    "created_at": "2021-08-11T22:24:06Z",
    "closed_at": "2022-03-17T21:05:23Z",
    "labels": [
      "tech debt",
      "help wanted",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17687",
    "body": "This test contains a list of stat names to check how they are tag extracted. While making a small change to this test, I noticed that many of the elements in the list are never checked at all (I think the count was 174). \r\n\r\nThis test should be cleaned up to reflect current stats that are emitted, and then modified so that un-tested cases make the test fail, so that it doesn't become stale in the future.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17687/comments",
    "author": "ggreenway",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2021-08-11T22:24:15Z",
        "body": "cc @jmarantz "
      },
      {
        "user": "ankatare",
        "created_at": "2021-09-24T16:48:13Z",
        "body": "@ggreenway can you elaborate it more. I would like to understand and work on this"
      },
      {
        "user": "ggreenway",
        "created_at": "2021-09-27T16:02:16Z",
        "body": "If you look at the test, there's a big list of cases to test. Many of them are unintentionally not hit at all, due to changes to stats over the years since the test was written.\r\n\r\nI think the test should keep track of which cases have been hit, and fail at the end of the test if any case has not been hit. Anytime a case isn't hit, it probably means that someone made some change to stats, and a corresponding change in this test is needed also."
      },
      {
        "user": "ankatare",
        "created_at": "2021-09-28T05:10:02Z",
        "body": "@ggreenway Thanks for explaining it. Let me understand it , will get back for any doubts. "
      }
    ]
  },
  {
    "number": 17656,
    "title": "Publish Windows Server 2022 Envoy images ",
    "created_at": "2021-08-10T16:28:51Z",
    "closed_at": "2021-10-04T20:35:37Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/windows",
      "area/docker"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17656",
    "body": "Windows Server 2022 comes with many new features but the most notable one is support for `SO_ORIGINAL_DST` and traffic redirection via HNS policies. These features are an absolute requirement to be able to run Envoy as a sidecar in a service mesh solution.\r\n\r\nWe should publish windows server 2022 images of Envoy (**ideally** based on Nanoserver)\r\n\r\nLimitations:\r\n\r\n1. windows-server-latest CI is not compatible with window server 2022 which means that we can't build the image from a windows host\r\n2. `docker buildx` is not compatible with `RUN` commands which we use to set the path.\r\n\r\nProblem (1) is not possible to solve, so we will have to solve (2).\r\n\r\nWe use `RUN` commands for two reasons:\r\n\r\n1. Setting up the Envoy user, which can be moved into a runtime script.\r\n2. Place `envoy.exe` into a location that is already in the path and thus remove the need for running `setx` commands.\r\n\r\n\r\n\r\ntl;dr: We should build Windows images from linux with `docker buildx`\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17656/comments",
    "author": "davinci26",
    "comments": [
      {
        "user": "davinci26",
        "created_at": "2021-08-10T16:29:47Z",
        "body": "@wrowe and @phlax this come up on the windows-dev channel. I think that this is going to be a big win on many different levels"
      },
      {
        "user": "davinci26",
        "created_at": "2021-08-10T16:32:18Z",
        "body": "@thhous-msft this should be a good issue if you find it interesting\r\n\r\ncc @envoyproxy/windows-dev "
      },
      {
        "user": "sunjayBhatia",
        "created_at": "2021-08-10T16:42:23Z",
        "body": "If you need CI/build tools for Windows Server 2022 might want to also make a tracking issue for envoy-build-tools, might be good to get started for the future but probably not a blocker to just publish builds"
      },
      {
        "user": "davinci26",
        "created_at": "2021-09-10T20:59:12Z",
        "body": "I want to solicit some feedback here on the implementation that I am thinking of here:\r\n\r\n### Problem \r\n\r\nThe biggest problem to port the Windows Dockerfile to use buildx is the usage of \r\n```\r\nUSER ContainerAdministrator\r\nRUN net user /add \"EnvoyUser\"\r\nRUN net localgroup \"Network Configuration Operators\" \"EnvoyUser\" /add\r\n```\r\n\r\nThis part is really important though for all cases Envoy runs as a sidecar and it **needs** to be running on as `EnvoyUser`\r\n\r\n### Proposed Solution\r\n\r\ntl;dr: Move the user creation to the container startup.\r\n\r\nThat would work in the following way:\r\n\r\n1. Create a new executable `Envoy-privilege-dropper` which does the following\r\n1.1 Create `EnvoyUser`\r\n1.2 Add `EnvoyUser` to `Network Configuration Operators`\r\n1.3 Create a **child process** to run Envoy as `EnvoyUser`\r\n1.4 Redirect the standard output/error of child process to the parent process\r\n\r\nI am positive that the above solution should work but I think that this additional layer of abstraction might complicate things in terms of debugging.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 17617,
    "title": "Add way to get more detailed OS level network stats as Envoy metrics",
    "created_at": "2021-08-06T18:27:26Z",
    "closed_at": "2021-10-25T15:37:22Z",
    "labels": [
      "enhancement",
      "design proposal",
      "help wanted",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17617",
    "body": "I would like a way to have Envoy provide more OS level network metrics, attributed to a listener or cluster.\r\n\r\nExamples: RTT, TCP retransmission rate, amount of data buffered in the OS.\r\n\r\nOn linux, the following can be retrieved via getsockopt(..., TCP_INFO):\r\n\r\n```\r\nstruct tcp_info {\r\n\t__u8\ttcpi_state;\r\n\t__u8\ttcpi_ca_state;\r\n\t__u8\ttcpi_retransmits;\r\n\t__u8\ttcpi_probes;\r\n\t__u8\ttcpi_backoff;\r\n\t__u8\ttcpi_options;\r\n\t__u8\ttcpi_snd_wscale : 4, tcpi_rcv_wscale : 4;\r\n\t__u8\ttcpi_delivery_rate_app_limited:1, tcpi_fastopen_client_fail:2;\r\n\r\n\t__u32\ttcpi_rto;\r\n\t__u32\ttcpi_ato;\r\n\t__u32\ttcpi_snd_mss;\r\n\t__u32\ttcpi_rcv_mss;\r\n\r\n\t__u32\ttcpi_unacked;\r\n\t__u32\ttcpi_sacked;\r\n\t__u32\ttcpi_lost;\r\n\t__u32\ttcpi_retrans;\r\n\t__u32\ttcpi_fackets;\r\n\r\n\t/* Times. */\r\n\t__u32\ttcpi_last_data_sent;\r\n\t__u32\ttcpi_last_ack_sent;     /* Not remembered, sorry. */\r\n\t__u32\ttcpi_last_data_recv;\r\n\t__u32\ttcpi_last_ack_recv;\r\n\r\n\t/* Metrics. */\r\n\t__u32\ttcpi_pmtu;\r\n\t__u32\ttcpi_rcv_ssthresh;\r\n\t__u32\ttcpi_rtt;\r\n\t__u32\ttcpi_rttvar;\r\n\t__u32\ttcpi_snd_ssthresh;\r\n\t__u32\ttcpi_snd_cwnd;\r\n\t__u32\ttcpi_advmss;\r\n\t__u32\ttcpi_reordering;\r\n\r\n\t__u32\ttcpi_rcv_rtt;\r\n\t__u32\ttcpi_rcv_space;\r\n\r\n\t__u32\ttcpi_total_retrans;\r\n\r\n\t__u64\ttcpi_pacing_rate;\r\n\t__u64\ttcpi_max_pacing_rate;\r\n\t__u64\ttcpi_bytes_acked;    /* RFC4898 tcpEStatsAppHCThruOctetsAcked */\r\n\t__u64\ttcpi_bytes_received; /* RFC4898 tcpEStatsAppHCThruOctetsReceived */\r\n\t__u32\ttcpi_segs_out;\t     /* RFC4898 tcpEStatsPerfSegsOut */\r\n\t__u32\ttcpi_segs_in;\t     /* RFC4898 tcpEStatsPerfSegsIn */\r\n\r\n\t__u32\ttcpi_notsent_bytes;\r\n\t__u32\ttcpi_min_rtt;\r\n\t__u32\ttcpi_data_segs_in;\t/* RFC4898 tcpEStatsDataSegsIn */\r\n\t__u32\ttcpi_data_segs_out;\t/* RFC4898 tcpEStatsDataSegsOut */\r\n\r\n\t__u64   tcpi_delivery_rate;\r\n\r\n\t__u64\ttcpi_busy_time;      /* Time (usec) busy sending data */\r\n\t__u64\ttcpi_rwnd_limited;   /* Time (usec) limited by receive window */\r\n\t__u64\ttcpi_sndbuf_limited; /* Time (usec) limited by send buffer */\r\n\r\n\t__u32\ttcpi_delivered;\r\n\t__u32\ttcpi_delivered_ce;\r\n\r\n\t__u64\ttcpi_bytes_sent;     /* RFC4898 tcpEStatsPerfHCDataOctetsOut */\r\n\t__u64\ttcpi_bytes_retrans;  /* RFC4898 tcpEStatsPerfOctetsRetrans */\r\n\t__u32\ttcpi_dsack_dups;     /* RFC4898 tcpEStatsStackDSACKDups */\r\n\t__u32\ttcpi_reord_seen;     /* reordering events seen */\r\n\r\n\t__u32\ttcpi_rcv_ooopack;    /* Out-of-order packets received */\r\n\r\n\t__u32\ttcpi_snd_wnd;\t     /* peer's advertised receive window after\r\n\t\t\t\t      * scaling (bytes)\r\n\t\t\t\t      */\r\n};\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17617/comments",
    "author": "ggreenway",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2021-08-06T18:35:56Z",
        "body": "Proposal: Add a new network filter. The filter would not look at either read or write data at all. This would make the cost (both memory for the stats, and CPU for the getsockopt() calls) opt-in and have no cost by default.\r\n\r\nThe filter would create metrics, in a configurable stat_prefix.\r\n\r\nFor gauges and counters, the filter would have a configurable update duration which would poll the TCP_INFO at the specified period to update metrics for that connection. Stats will always update when the connection is closed. An unset update duration means to only update at connection close.\r\n\r\nHistograms would only be updated at connection close.\r\n\r\nInitial set of metrics:\r\n\r\nGauges:\r\nData in OS read buffer\r\nData in OS write buffer\r\n\r\nCounters:\r\nRetansmitted packets\r\nSent packets\r\nReceived packets\r\nReordered packets\r\n\r\nHistograms:\r\nLast RTT\r\nPercent of packets retransmitted\r\n\r\nI think it makes the most sense to make this a linux-specific filter. The available OS-level network stats are not consistent between platforms. If there's interest, a similar filter could be created for other OSs that provide the metrics those platforms support."
      },
      {
        "user": "mattklein123",
        "created_at": "2021-08-06T22:08:39Z",
        "body": "+1 this sounds great."
      }
    ]
  },
  {
    "number": 17611,
    "title": "test queuing requests in DFP when stream limit is maxed out",
    "created_at": "2021-08-05T18:51:03Z",
    "closed_at": "2021-10-25T19:46:40Z",
    "labels": [
      "help wanted",
      "area/forward proxy"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17611",
    "body": "When a connection pool's stream limit is reached requests will fail. For DFP this is undesirable. It would instead be better if requests could be queued until other streams close and a new stream may be created.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17611/comments",
    "author": "RyanTheOptimist",
    "comments": [
      {
        "user": "RyanTheOptimist",
        "created_at": "2021-08-05T18:51:33Z",
        "body": "/assign @alyssawilk "
      },
      {
        "user": "RyanTheOptimist",
        "created_at": "2021-08-05T18:52:07Z",
        "body": "Assigning to Alyssa as per offline discussion."
      },
      {
        "user": "alyssawilk",
        "created_at": "2021-10-25T16:09:56Z",
        "body": "so if the resource manager requests the request we insta-fail, but we do otherwise appear to queue.  I'll add a regression test."
      },
      {
        "user": "RyanTheOptimist",
        "created_at": "2021-10-25T16:20:50Z",
        "body": "@alyssawilk great find! What is the resource manager? That's a term I'm not yet familiar with."
      }
    ]
  },
  {
    "number": 17321,
    "title": "wasm: make vm_config.runtime default to V8 (or Wasmtime/WAVM/WAMR).",
    "created_at": "2021-07-14T00:34:14Z",
    "closed_at": "2023-11-08T15:40:42Z",
    "labels": [
      "enhancement",
      "help wanted",
      "no stalebot",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17321",
    "body": "Given that almost all Wasm plugin users would put `runtime: envoy.wasm.runtime.v8`, it would make sense to me that make `vm_config.runtime` optional and default to V8 or any runtime which Envoy is compiled with. @PiotrSikora any thoughts?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17321/comments",
    "author": "mathetake",
    "comments": [
      {
        "user": "PiotrSikora",
        "created_at": "2021-07-14T02:48:16Z",
        "body": "Yes, we should definitely make `runtime` field optional, and if there is only one Wasm runtime built-in (which is currently the only option), then it should be used without any extra configuration.\r\n\r\nIt gets a little tricky because of NullVM, but we can always try to load NullVM plugin first before falling back to loading from file/remote."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-08-13T04:01:25Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "vikaschoudhary16",
        "created_at": "2023-11-08T08:59:13Z",
        "body": "this is done and can be closed."
      }
    ]
  },
  {
    "number": 16751,
    "title": "Support keep-alive in QUIC downstream and make it configurable",
    "created_at": "2021-06-01T17:45:56Z",
    "closed_at": "2021-10-22T14:14:40Z",
    "labels": [
      "help wanted",
      "no stalebot",
      "area/quic",
      "quic-mvp"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16751",
    "body": "Currently QUIC server codec doesn't support sending keep-alive PING to client like Http2 does. We probably need similar configurable feature in gQUIC and using PATH_CHALLENGE and PATH_RESPONSE in IETF QUIC.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16751/comments",
    "author": "danzh2010",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-01T20:01:32Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-07-09T00:01:34Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "danzh2010",
        "created_at": "2021-07-12T15:43:00Z",
        "body": "Can someone make it non-stale? \r\nAs we will not support Google QUIC soon, implementing it using PATH_CHALLENGE and PATH_RESPONSE is enough."
      },
      {
        "user": "danzh2010",
        "created_at": "2021-07-20T21:09:38Z",
        "body": "Can someone re-open this?"
      }
    ]
  },
  {
    "number": 16275,
    "title": "Deprecate Envoy Alpine Docker images",
    "created_at": "2021-05-03T07:56:22Z",
    "closed_at": "2022-02-10T09:39:10Z",
    "labels": [
      "area/docs",
      "help wanted",
      "deprecation",
      "area/docker"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/16275",
    "body": "### description\r\n\r\nOnce #16268 lands we will have a distroless Docker image and we can think about deprecating the Alpine image, which doesnt automatically get kept up to date in terms of sec issues  #14869 #16170 \r\n\r\nCurrently the alpine image is used for some backend services in sandbox examples, although there is no reason for that - they dont use the Envoy binary etc \r\n\r\nWe will also need to announce the intention to deprecate somehow and document removal\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/16275/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2021-05-03T08:24:55Z",
        "body": "cc @oleksiyp "
      },
      {
        "user": "oleksiyp",
        "created_at": "2021-05-05T05:52:10Z",
        "body": "Will take this one, but not sure I understand scope of \"some backend services\". Can you be more specific what to convert? All examples or some specific set?"
      },
      {
        "user": "phlax",
        "created_at": "2021-05-05T05:54:53Z",
        "body": "yep - try:\r\n\r\n```\r\n$ git grep envoy-alpine-dev examples\r\nexamples/cache/Dockerfile-service:FROM envoyproxy/envoy-alpine-dev:latest\r\nexamples/cors/backend/Dockerfile-service:FROM envoyproxy/envoy-alpine-dev:latest\r\nexamples/cors/frontend/Dockerfile-service:FROM envoyproxy/envoy-alpine-dev:latest\r\nexamples/csrf/crosssite/Dockerfile-service:FROM envoyproxy/envoy-alpine-dev:latest\r\nexamples/csrf/samesite/Dockerfile-service:FROM envoyproxy/envoy-alpine-dev:latest\r\nexamples/front-proxy/Dockerfile-jaeger-service:FROM envoyproxy/envoy-alpine-dev:latest\r\nexamples/front-proxy/Dockerfile-service:FROM envoyproxy/envoy-alpine-dev:latest\r\nexamples/load-reporting-service/Dockerfile-http-server:FROM envoyproxy/envoy-alpine-dev:latest\r\n\r\n```\r\n\r\nthese are almost all flask services so dont need to use envoy-alpine-dev at all"
      },
      {
        "user": "phlax",
        "created_at": "2021-05-05T05:58:58Z",
        "body": "hmm checking `start_service.sh`in these im not sure - checking further..."
      },
      {
        "user": "phlax",
        "created_at": "2021-05-05T06:22:29Z",
        "body": "@oleksiyp see  #16325 for an example of what  i think is needed in terms of examples' use of `envoy-alpine-dev` and removal"
      },
      {
        "user": "phlax",
        "created_at": "2021-05-05T06:24:26Z",
        "body": "if you want to test an individual example locally best way is (eg with cache example)\r\n\r\n```console\r\n$ cd examples/cache\r\n$ ./verify.sh\r\n```"
      },
      {
        "user": "daixiang0",
        "created_at": "2022-02-09T07:57:17Z",
        "body": "#19836 may close this."
      },
      {
        "user": "phlax",
        "created_at": "2022-02-10T09:37:45Z",
        "body": "@daixiang0 i think we still need to do something with the dockerhub page - somewhere at least - that lets people know that this is no longer supported - ie dont start using it"
      },
      {
        "user": "phlax",
        "created_at": "2022-02-10T09:39:09Z",
        "body": "actually just realizing that we have #19781 to track that, closing..."
      }
    ]
  },
  {
    "number": 15887,
    "title": "UDP: add support for multiple filters",
    "created_at": "2021-04-08T11:02:13Z",
    "closed_at": "2021-10-19T15:27:21Z",
    "labels": [
      "help wanted",
      "area/udp"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15887",
    "body": "Does UDP support network filters? If not, why not? Are there some issues with it, or someone has to implement it? \r\n\r\n**Network filters:**\r\nI'd like to write my own filter that works with UDP as well but currently as I know network filters won't apply for UDP listeners.\r\nI know that for now, only 1 UDP listener filter per listener supported, but what about network filters?\r\n\r\n**Listener filters:**\r\nIn #11656 it was mentioned that \r\n>Someone would need to finish the UDP filter manager work so that we can support multiple filters.\r\n\r\nIs there any progress on this? What are your plans with UDP in the future?\r\n\r\nSorry for a lot of questions. But I really want to know what should I expect.\r\nThanks for your time and help in advance  \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15887/comments",
    "author": "davidkornel",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-04-08T17:15:43Z",
        "body": "I know we have some support for UDP listeners, but not sure how sophisticated the support is compared to TCP. \r\n\r\n@alyssawilk @mattklein123 might know more"
      },
      {
        "user": "mattklein123",
        "created_at": "2021-04-08T20:21:08Z",
        "body": "I would love to have multiple UDP filters implemented, but someone needs to work on it. There is no high level reason it is not done, it's just that no one has cared to do it yet. I will repurpose this issue to track that."
      },
      {
        "user": "davidkornel",
        "created_at": "2021-04-09T11:50:34Z",
        "body": "Alright, thanks. Yes, that would be very nice to have multiple UDP filters. I'd say I'll do it but unfortunately to this point, I only changed a small portion of the code to implement a smaller feature I need. And I definitely don't see where and how should these changes be done. So I see two options right now. We'll wait until someone - who's capable - get the job done, or if someone gives me help I'll try it myself. WDYT how challenging it would be to implement the support for multiple UDP filters? "
      },
      {
        "user": "mattklein123",
        "created_at": "2021-04-09T16:26:12Z",
        "body": "> WDYT how challenging it would be to implement the support for multiple UDP filters?\r\n\r\nFor experienced contributors I would say medium, for new contributors hard. I can try to find some time to write down some implementation notes but I'm not sure when I will be able to get to it."
      },
      {
        "user": "zhxie",
        "created_at": "2021-10-11T07:24:38Z",
        "body": "I would like to follow this issue and take a try working on it, but I think there are still a few questions need to be discussed.\r\nIn the current design, each UDP listener can only support 1 UDP listener filter and does not support network filter (or filter chain), do we have some proposals, notes or drafts around multiple UDP filters?\r\n\r\nAnd if not, I would like to ask,\r\n\r\n- Support multiple UDP listener filters: create UDP filter manager for UDP listener filter chain (for RBAC, which means we will implement a solely separated UDP RBAC listener filter)\r\n- Support network filter (or filter chain): reuse certain existed connectionless network filters in UDP (we have to find some way make filters work connectionless and convert UDP listener filters like `UDP Proxy` to network filters)\r\n\r\n@mattklein123 which one looks more reasonable?"
      },
      {
        "user": "mattklein123",
        "created_at": "2021-10-11T14:55:42Z",
        "body": "> @mattklein123 which one looks more reasonable?\r\n\r\nIMO I would go with the 1st option. My intuition is that the interface is going to be enough different (for example we don't want to allow modifying buffers beyond MTU size (in the initial version maybe we don't allow buffer modification at all, just stop iteration/dropping).\r\n\r\nI think we could end up sharing a bunch of other code like RBAC matchers in utility classes but drive them from different filters."
      },
      {
        "user": "zhxie",
        "created_at": "2021-10-12T01:30:22Z",
        "body": "Sounds great. Then I think it may be not difficult to realize UDP multiple filters.\r\n\r\nBut, in my opinion, I think network socket, transport socket, listener filter and network filter would better be decoupled. Though there are a lot of major differences between TCP and UDP, one is stream-based, the other is datagram-based, one is reliable, the other only delivers with best-effort. But no one says you cannot put HTTP over UDP, or DNS over TCP. I think it would better if we could apply existed network filters on UDP, to make network socket unified. You mentioned MTU, but I do not think it is the thing we, a proxy, should consider. Users should be aware of MTU and the OS is responsible for fragmentation."
      },
      {
        "user": "mattklein123",
        "created_at": "2021-10-12T15:26:45Z",
        "body": "> I think it would better if we could apply existed network filters on UDP, to make network socket unified. You mentioned MTU, but I do not think it is the thing we, a proxy, should consider. Users should be aware of MTU and the OS is responsible for fragmentation.\r\n\r\nI agree with you in theory, but in practice I'm skeptical the extra complexity is actually worthwhile. I don't think people will make use of it. If there is an actual use case for this, let's discuss, but I would rather solve the immediate problems people are having which I think can be done with a very simple datagram based filter interface."
      },
      {
        "user": "zhxie",
        "created_at": "2021-10-12T15:35:47Z",
        "body": "Yes, I agree with you too. It's like a tech debt, maybe in the future Envoy 2.0, 3.0, we will be able to do it (laugh)."
      }
    ]
  },
  {
    "number": 15830,
    "title": "SimpleHttpCache: age header missing on cached responses after validation",
    "created_at": "2021-04-03T09:46:13Z",
    "closed_at": "2021-08-27T17:19:23Z",
    "labels": [
      "bug",
      "help wanted",
      "area/cache",
      "area/examples"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15830",
    "body": "*Description*:\r\nI would expect the `age` header to contain the time since the cache was last validated (i.e. when `date` is updated), and it does, except when the max-age is exceeded, but the response has not changed.\r\n\r\nSeries of events:\r\n* Initial request, response does not contain `age` header ✅\r\n* Second request (within `max-age`), response contains `age` header ✅\r\n* Third request (without `max-age`), response no longer contains `age` header. The response has been validated, but remains the same, the `date` gets reset ✅\r\n* Fourth request (within `max-age` of the new `date`), response does not contain `age` header despite being from cache ❌\r\n\r\n*Repro steps*:\r\n\r\nNote: The example has a max-age of 60 seconds.\r\n\r\n```bash\r\ncd examples/cache\r\ndocker-compose build --pull\r\ndocker-compose up -d\r\n\r\n# Make the four requests detailed above\r\ncurl -i localhost:8000/service/1/valid-for-minute \\\r\n  && sleep 30 \\\r\n  && curl -i localhost:8000/service/1/valid-for-minute \\\r\n  && sleep 40 \\\r\n  && curl -i localhost:8000/service/1/valid-for-minute \\\r\n  && sleep 10 \\\r\n  && curl -i localhost:8000/service/1/valid-for-minute\r\n```\r\n\r\nResult\r\n```bash\r\n# First response, no cache used ✅\r\nHTTP/1.1 200 OK\r\ncontent-type: text/html; charset=utf-8\r\ncontent-length: 101\r\ncache-control: max-age=60\r\ncustom-header: any value\r\netag: \"65ca6c562d20a7860e9da71cefa8ce58\"\r\ndate: Sat, 03 Apr 2021 09:19:04 GMT\r\nserver: envoy\r\nx-envoy-upstream-service-time: 13\r\n\r\nThis response will stay fresh for one minute!!\r\n\r\n# Second response, cached used, age header set ✅\r\nResponse generated at: Sat, 03 Apr 2021 09:19:04 GMT\r\nHTTP/1.1 200 OK\r\ncontent-type: text/html; charset=utf-8\r\ncontent-length: 101\r\ncache-control: max-age=60\r\ncustom-header: any value\r\netag: \"65ca6c562d20a7860e9da71cefa8ce58\"\r\ndate: Sat, 03 Apr 2021 09:19:04 GMT\r\nserver: envoy\r\nx-envoy-upstream-service-time: 13\r\nage: 30\r\n\r\nThis response will stay fresh for one minute!!\r\n\r\n# Third response, cached validated, date reset ✅\r\nResponse generated at: Sat, 03 Apr 2021 09:19:04 GMT\r\nHTTP/1.1 200 OK\r\ncache-control: max-age=60\r\ncustom-header: any value\r\netag: \"65ca6c562d20a7860e9da71cefa8ce58\"\r\ndate: Sat, 03 Apr 2021 09:20:14 GMT\r\nserver: envoy\r\nx-envoy-upstream-service-time: 9\r\ncontent-length: 101\r\ncontent-type: text/html; charset=utf-8\r\n\r\nThis response will stay fresh for one minute!!\r\n\r\n# Fourth response, cached used, age not set ❌\r\nResponse generated at: Sat, 03 Apr 2021 09:19:04 GMT\r\nHTTP/1.1 200 OK\r\ncache-control: max-age=60\r\ncustom-header: any value\r\netag: \"65ca6c562d20a7860e9da71cefa8ce58\"\r\ndate: Sat, 03 Apr 2021 09:20:24 GMT\r\nserver: envoy\r\nx-envoy-upstream-service-time: 8\r\ncontent-length: 101\r\ncontent-type: text/html; charset=utf-8\r\n\r\nThis response will stay fresh for one minute!!\r\n\r\nResponse generated at: Sat, 03 Apr 2021 09:19:04 GMT\r\n```\r\n\r\nThe other requested information from the bug template information doesn't seem relevant given that this is demonstrable with one of the stock examples.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15830/comments",
    "author": "dpwrussell",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-05-05T16:01:44Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "dpwrussell",
        "created_at": "2021-05-07T07:57:28Z",
        "body": "This is still an issue and should not be closed."
      }
    ]
  },
  {
    "number": 15794,
    "title": "listener: switch default for SO_REUSEPORT to true",
    "created_at": "2021-03-31T20:26:20Z",
    "closed_at": "2021-07-20T02:03:48Z",
    "labels": [
      "help wanted",
      "area/listener"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15794",
    "body": "Switch the default to SO_REUSEPORT to true as at this point functional kernel support is widespread, and it's an ongoing support burden for Envoy with people complaining about connection imbalance. Plan:\r\n\r\n1) Deprecate `reuse_port`\r\n2) Add WKT `enable_reuse_port`\r\n3) Default for `enable_reuse_port` will be true, with a runtime feature flag to flip the default to false (preserve current behavior).\r\n\r\ncc @envoyproxy/api-shepherds @antoniovicente ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15794/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-03-31T21:25:26Z",
        "body": "I'm realizing that flipping the default to true probably also has to be aware of hot restart. I don't think hot restarting onto a different default is going to go well."
      },
      {
        "user": "daixiang0",
        "created_at": "2021-06-28T01:34:14Z",
        "body": "Still available?"
      }
    ]
  },
  {
    "number": 15784,
    "title": "general tracing support",
    "created_at": "2021-03-31T06:40:43Z",
    "closed_at": "2021-09-03T07:52:36Z",
    "labels": [
      "area/tracing",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15784",
    "body": "*Title*: *general tracing support*\r\n\r\n*Description*:\r\n\r\nEnvoy provides powerful tracing capabilities. However, it is currently only available for the HTTP protocol. Because Tracer uses Http::RequestHeaderMap to pass the tracing context of the request.\r\nWhile other protocols can implement an Http::RequestHeaderMap to directly reuse the existing Tracer, this requires a significant amount of work.\r\nBut if a simpler abstraction can be provided, then other protocols can more easily integrate the tracer into it. \r\nFor example, simply implementing the following method:\r\n\r\n```\r\nclass TraceableStream {\r\n    virtual void setContext(const std::string_view key, const std::string_view value) PURE;\r\n    virtual std::optional<std::string_view> getContext(const std::string_view key) const PURE;\r\n};\r\n\r\n```\r\n\r\nAnd then create tracing span from it:\r\n\r\n```\r\n  /**\r\n   * Start driver specific span.\r\n   */\r\n  virtual SpanPtr startSpan(const Config& config, TraceableStream& stream,\r\n                            const std::string& operation_name, SystemTime start_time,\r\n                            const Tracing::Decision tracing_decision) PURE;\r\n```\r\n\r\n[optional *Relevant Links*:]\r\n>Any extra documentation required to understand the issue.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15784/comments",
    "author": "wbpcode",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-03-31T18:20:01Z",
        "body": "Yeah this is something I would love to clean up. Would definitely appreciate cleanup work here to make this code all generic."
      },
      {
        "user": "wbpcode",
        "created_at": "2021-04-01T01:49:55Z",
        "body": "I can try to figure it out. Because I just want to enhance the tracing of the dubbo protocol. General tracing support is a very important pre-work.  🤔 @mattklein123 "
      },
      {
        "user": "wbpcode",
        "created_at": "2021-09-03T07:52:36Z",
        "body": "This is ok now."
      }
    ]
  },
  {
    "number": 15637,
    "title": "Per connection rate limiting of HTTP requests",
    "created_at": "2021-03-24T00:29:49Z",
    "closed_at": "2021-05-25T00:47:05Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/ratelimit"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15637",
    "body": "Currently, the HTTP local rate limiter's token bucket is shared across all workers, thus causing the rate limits to be applied per Envoy instance/process. This could potentially result in bad actors quickly exhausting limits on a given instance before legitimate users have had a fair chance and in extreme cases even lead to starvation. We need a way to allocate token buckets for each connection thereby allowing us to rate limit requests per connection. \r\n\r\nOne possible option would be to allow specifying a `scope` on the local rate limit configuration, along the lines of:\r\n\r\n```javascript\r\nname: envoy.filters.http.local_ratelimit\r\ntyped_config:\r\n  \"@type\": type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit\r\n  token_bucket:\r\n    max_tokens: 10000\r\n    tokens_per_fill: 1000\r\n    fill_interval: 1s\r\n    scope: per_connection/per_process etc ...\r\n```\r\n\r\nFollowing this, depending on the configured `scope`, we could maybe add an instance of the `LocalRateLimit::LocalRateLimiterImpl` to the connection object that each HTTP request belongs to, if there isn't one already (perhaps via `callbacks_->connection().streamInfo().filterState()->setData`?)\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15637/comments",
    "author": "gokulnair",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-03-24T13:22:53Z",
        "body": "Seems like a reasonable enhancement, marking help wanted.\r\n\r\ncc @rgs1 "
      },
      {
        "user": "mattklein123",
        "created_at": "2021-03-24T22:01:15Z",
        "body": "+1 I would like to see this happen. I think doing it via filter data sounds like a good plan."
      }
    ]
  },
  {
    "number": 15351,
    "title": "Use jinja for protodoc",
    "created_at": "2021-03-07T09:01:55Z",
    "closed_at": "2022-02-07T07:01:41Z",
    "labels": [
      "area/docs",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15351",
    "body": "### description\r\n\r\nthe complexity of protodoc seems to have outgrown the python templating pattern used\r\n\r\nthis is resulting in a lot of `\\n` and `{indent}` fu and making it increasingly difficult to work with\r\n\r\none possible solution to this problem is to use python only to create structured variables that a proper templating/markup lang like jinja can consume",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15351/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2021-03-07T09:02:07Z",
        "body": "cc @htuch "
      },
      {
        "user": "htuch",
        "created_at": "2021-03-08T03:41:41Z",
        "body": "@phlax seems totally reasonable."
      },
      {
        "user": "phlax",
        "created_at": "2022-02-07T07:01:41Z",
        "body": "protodoc uses jinja now"
      }
    ]
  },
  {
    "number": 15264,
    "title": "enforce pragma once in header file",
    "created_at": "2021-03-02T19:14:32Z",
    "closed_at": "2023-09-05T17:46:29Z",
    "labels": [
      "enhancement",
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15264",
    "body": "I find my newly added header file not following the style guide `Header guards should use #pragma once.`\r\n\r\nLooks like this is not unique \r\n```\r\n$ for f in `find include/ source/ test/ -name \"*.h\"`; do grep \"pragma once\" $f >/dev/null|| echo $f; done | wc -l\r\n81\r\n```\r\n\r\nI will blindly add the pragma directive to make the future include no-brain.\r\n\r\nDo we need a rule formatter to check/fix after the above change?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15264/comments",
    "author": "lambdai",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2021-03-02T20:48:36Z",
        "body": "yeah, looks like it's mostly but not all test code.  It'd definitely be worth adding to the fix format script IMO."
      },
      {
        "user": "lambdai",
        "created_at": "2021-03-03T19:04:55Z",
        "body": "An quick fix is added formatted is not ready.\r\n\r\nAlso the exceptions are listed below for track\r\n\r\n1. chromium url\r\n```\r\nsource/common/chromium_url/url_parse.h\r\nsource/common/chromium_url/url_parse_internal.h\r\nsource/common/chromium_url/url_canon_stdstring.h\r\nsource/common/chromium_url/url_canon_internal.h\r\nsource/common/chromium_url/url_canon.h\r\n```\r\n2. test/config/integration/certs\r\n```\r\ntest/config/integration/certs/clientcert_hash.h\r\n...\r\n```\r\n3. test/extensions/transport_sockets/tls/test_data/\r\n```\r\ntest/extensions/transport_sockets/tls/test_data/\r\n```"
      },
      {
        "user": "yanavlasov",
        "created_at": "2021-03-05T03:59:22Z",
        "body": "The `source/common/chromium_url` should go away soon. I'm not quite certain when deprecation period has started for the code that uses it."
      },
      {
        "user": "botengyao",
        "created_at": "2023-09-05T17:43:18Z",
        "body": "We can close this issue in favor of the merged PR above."
      }
    ]
  },
  {
    "number": 15107,
    "title": "Upstream max connection duration should be configurable.",
    "created_at": "2021-02-18T18:51:01Z",
    "closed_at": "2021-10-14T12:43:47Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15107",
    "body": "We currently support max connection duration for downstream connections, but not yet for upstream connections. Allowing upstream connections to have a max duration provides users a guarantee that after a certain amount of time, connections will be cycled. This allows for service discovery through Kubernetes Service IPs, for example, to work more reliably.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15107/comments",
    "author": "esmet",
    "comments": [
      {
        "user": "antoniovicente",
        "created_at": "2021-02-18T23:30:09Z",
        "body": "cc @snowp @alyssawilk \r\n\r\nSeems like a reasonable feature request.\r\n\r\nI see you have an early draft. Do you plan to provide a fix for this issue?"
      },
      {
        "user": "esmet",
        "created_at": "2021-02-19T17:39:12Z",
        "body": "I do, yes. The draft is intended to fix this issue but I was hoping for some early feedback on the approach while I worked on other things, before pulling it out of draft."
      },
      {
        "user": "esmet",
        "created_at": "2021-08-31T17:02:49Z",
        "body": "/assign"
      }
    ]
  },
  {
    "number": 14922,
    "title": "Listener access logs missing %REQUESTED_SERVER_NAME% when no filter chain matches",
    "created_at": "2021-02-03T01:35:43Z",
    "closed_at": "2021-06-30T14:44:53Z",
    "labels": [
      "bug",
      "help wanted",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14922",
    "body": "**If you are reporting *any* crash or *any* potential security issue, *do not*\r\nopen an issue in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\nWhen I send a request that matches a filter chain, I see access logs:\r\n`{\"requested_server_name\":\"hello.example.com\", ...}`\r\n\r\nHowever, if I send a request that does *not* match any filter chains:\r\n`{\"requested_server_name\":null,...}`\r\n\r\n*Repro steps*:\r\nAccess log config, at the listener level:\r\n```json\r\n       \"access_log\": [\r\n        {\r\n         \"name\": \"envoy.access_loggers.file\",\r\n         \"filter\": {\r\n          \"response_flag_filter\": {\r\n           \"flags\": [\r\n            \"NR\"\r\n           ]\r\n          }\r\n         },\r\n         \"typed_config\": {\r\n          \"@type\": \"type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\",\r\n          \"path\": \"/dev/stdout\",\r\n          \"log_format\": {\r\n           \"json_format\": {\r\n            \"authority\": \"%REQ(:AUTHORITY)%\",\r\n            \"bytes_received\": \"%BYTES_RECEIVED%\",\r\n            \"bytes_sent\": \"%BYTES_SENT%\",\r\n            \"downstream_local_address\": \"%DOWNSTREAM_LOCAL_ADDRESS%\",\r\n            \"downstream_remote_address\": \"%DOWNSTREAM_REMOTE_ADDRESS%\",\r\n            \"duration\": \"%DURATION%\",\r\n            \"method\": \"%REQ(:METHOD)%\",\r\n            \"path\": \"%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%\",\r\n            \"protocol\": \"%PROTOCOL%\",\r\n            \"request_id\": \"%REQ(X-REQUEST-ID)%\",\r\n            \"requested_server_name\": \"%REQUESTED_SERVER_NAME%\",\r\n            \"response_code\": \"%RESPONSE_CODE%\",\r\n            \"response_flags\": \"%RESPONSE_FLAGS%\",\r\n            \"route_name\": \"%ROUTE_NAME%\",\r\n            \"start_time\": \"%START_TIME%\",\r\n            \"upstream_cluster\": \"%UPSTREAM_CLUSTER%\",\r\n            \"upstream_host\": \"%UPSTREAM_HOST%\",\r\n            \"upstream_local_address\": \"%UPSTREAM_LOCAL_ADDRESS%\",\r\n            \"upstream_service_time\": \"%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%\",\r\n            \"upstream_transport_failure_reason\": \"%UPSTREAM_TRANSPORT_FAILURE_REASON%\",\r\n            \"user_agent\": \"%REQ(USER-AGENT)%\",\r\n            \"x_forwarded_for\": \"%REQ(X-FORWARDED-FOR)%\"\r\n           }\r\n          }\r\n         }\r\n        }\r\n       ]\r\n      },\r\n```\r\n\r\nMy expectation is that the REQUESTED_SERVER_NAME field would be populated here. Given tls_inspector runs before we do filter chain matching, I would expect this information is available?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14922/comments",
    "author": "howardjohn",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2021-02-04T21:12:00Z",
        "body": "I guess it's because ActiveTcpListener::newConnection didn't populate the REQUESTED_SERVER_NAME. \r\nThe logic is tricky there:\r\nIf a filter chain is selected, the transport socket in the chosen filter chain should populate the `REQUESTED_SERVER_NAME`. The value sniffed by tls_inspector should be ignored.\r\nIf no filter chain is selected, this server name should be populated with the best knowledge, namely what tls inspector sniffed. "
      },
      {
        "user": "howardjohn",
        "created_at": "2021-02-04T21:18:52Z",
        "body": "By the way, real world case that impacted me: I was trying to debug no filter chain match. I concluded, based on log, that client does not set any SNI. However, it actually did, but set to one that did not match any filter chain. I was only able to figure this out by swapping envoy with mitmproxy"
      }
    ]
  },
  {
    "number": 14894,
    "title": "Inconsistency in per-route filter config resolution across filters",
    "created_at": "2021-02-01T18:23:59Z",
    "closed_at": "2021-06-01T15:52:56Z",
    "labels": [
      "bug",
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14894",
    "body": "Some filters use `resolveMostSpecificPerFilterConfig()`, others use `mostSpecificPerFilterConfigTyped`. They both have independent logic around most-specific route config resolution, which is hard to follow and possibly keep inline. Is it plausible we can converge these?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14894/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2021-02-01T18:24:06Z",
        "body": "CC @markdroth "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-04T20:03:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14873,
    "title": "Allow envoy to set 'authority' header's value to the endpoint's hostname for clusters with type: STRICT_DNS and lb_policy: MAGLEV/RING_HASH ",
    "created_at": "2021-01-30T10:51:46Z",
    "closed_at": "2021-02-27T23:36:50Z",
    "labels": [
      "bug",
      "help wanted",
      "area/cluster_manager"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14873",
    "body": "*Title*: Currently requests coming to endpoints of cluster with type=STRICT_DNS and lb_policy=MAGLEV have 'authority' header equal to the name of the cluster ('dcs-cluster' as in the example below) and not to the hostname of the endpoint.\r\n\r\n*Description*: \r\nIn my configuration:\r\n```\r\n  clusters:\r\n    - name: dcs-cluster\r\n      type: STRICT_DNS\r\n      lb_policy: MAGLEV\r\n      common_lb_config:\r\n        consistent_hashing_lb_config:\r\n          use_hostname_for_hashing: true\r\n      load_assignment:\r\n        cluster_name: dcs-cluster\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: dc1\r\n                      port_value: 8080\r\n                  hostname: dc1 # ignored for STRICT_DNS\r\n                  health_check_config:\r\n                    hostname: dc1 # only applies to health requests\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: dc2\r\n                      port_value: 8080\r\n                  hostname: dc2 # ignored for STRICT_DNS\r\n                  health_check_config:\r\n                    hostname: dc2 # only applies to health requests\r\n```\r\n\r\nI would like the 'authority' header in the requests sent to dc1 endpoint was equal to 'dc1' (not 'dcs-cluster') - the hostname of the endpoint. So in case I want to route these requests back to the envoy, I could distinguish from which endpoint the request is coming from (based on the 'authority' header) to perform additional routing.\r\n\r\nEnvoy already allows to specify hostname for the endpoint, but it is ignored if cluster's type = STRICT_DNS/LOGICAL_DNS although it allows to specify hostname for endpoint's health_check_config.\r\n\r\nI need this because I use hostnames dc1, dc2 in the endpoints as aliases and they point to different hostnames on different envoy servers. I need those aliases so the Maglev balancing with property 'use_hostname_for_hashing'=true would work identically on all our envoy servers.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14873/comments",
    "author": "denisanfimov",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-01-31T22:04:34Z",
        "body": "I've been looking for a related issue to this, but I can't find it, so maybe I'm not remembering correctly. Doesn't auto_host_rewrite do what you want?"
      },
      {
        "user": "denisanfimov",
        "created_at": "2021-02-02T13:40:54Z",
        "body": "@mattklein123 auto_host_rewrite=true on route allows to use hostname from \r\n'endpoint.address.socket_address.address' \r\nbut ignores the value form\r\n'endpoint.hostname'\r\nI would like envoy to \r\n1) resolve IP address from 'endpoint.address.socket_address.address' via custom dns_resolver.\r\nMy custom dns_resolver actually resolves CNAME records not for dc1 (alias) but for real FQDN (rewritten in custom dns_resolver. I use coredns with rewrite plugin) \r\n2) use real FQDN from 'endpoint.hostname' to insert in 'authority' header in the requests to the upstream.\r\n\r\nso if my config is like this:\r\n```\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: dc1\r\n                      port_value: 8080\r\n                  hostname: real-gw1.fqdn\r\n                  health_check_config:\r\n                    hostname: real-gw1.fqdn\r\n```\r\nhealth requests will contain 'authority' header = real-gw1.fqdn\r\nother requests will contain 'authority' header = dc1\r\n\r\nbut I want both health requests and other requests contain 'authority' header = real-gw1.fqdn\r\nDoes it make sense?\r\n\r\n\r\nWait...\r\nI need such config only to have the same hostname used in calculation during maglev load balancing, and I need dc1(alias) to be used, but if you will get value of hostname from 'endpoint.hostname' then the config above won't do what I initially planned it to do."
      },
      {
        "user": "mattklein123",
        "created_at": "2021-02-03T00:41:52Z",
        "body": "I know I had a thread about this in some other issue but I can't find it now. The issue is that STRICT_DNS ignores `hostname` when it's set and always set the hostname of each host to the DNS address in socket_address. IMO the correct behavior is to set the hostname of the host to `hostname` if it's set, otherwise to use the DNS address. I think this is effectively what the other clusters do so I would consider this a bug. I can look into fixing this unless someone gets to it first."
      },
      {
        "user": "lambdai",
        "created_at": "2021-02-04T21:16:34Z",
        "body": "A related issue is about the different semantic between google-grpc and envoy-grpc.\r\n\r\nEnvoy-grpc uses cluster backend. Currently it's envoy-grpc client that set :authority, but it's good to have another source of :authority that is bind with cluster."
      }
    ]
  },
  {
    "number": 14782,
    "title": "Add consistent hashing InputMatcher",
    "created_at": "2021-01-21T16:01:45Z",
    "closed_at": "2021-03-08T20:23:37Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/matching"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14782",
    "body": "Tracks adding a simple InputMatcher that takes a configurable threshold and will consistently hash the input string, matching if the resulting hash value is above the configured threshold:\r\n\r\n```\r\nmatch = (hash(input) % 100) > threshold\r\n```\r\n\r\nThis would allow users to configure a match rule that will consistently match X% of requests based on a random input string (e.g. a tracing header). \r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14782/comments",
    "author": "snowp",
    "comments": [
      {
        "user": "kailun-qin",
        "created_at": "2021-01-22T09:25:15Z",
        "body": "@snowp \r\nAt which level would you prefer this matching should be added to? Supposing there are actually 3 opts:\r\n1) extending as a separate match logic for `SinglePredicate` InputMatcher;\r\n2) extending as a new match rule for `StringMatcher`;\r\n3) adding as a new `ConsistentHashMatcher` type.\r\n\r\nThanks."
      },
      {
        "user": "snowp",
        "created_at": "2021-01-28T16:00:42Z",
        "body": "I would add this as a CustomMatcher extension for use within the SinglePredicate. I don't recall off the top of my head whether we've wired up the extension factory logic here, so that might be part of it"
      }
    ]
  },
  {
    "number": 14781,
    "title": "Add environment variable DataInput extension",
    "created_at": "2021-01-21T15:58:32Z",
    "closed_at": "2021-04-01T16:05:14Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/matching"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14781",
    "body": "Tracks adding a DataInput extension that allows matching on an environment variable. \r\n\r\nThis would allow users to do things such as following a match path only for a specific instance (i.e. by matching on HOSTNAME) while still sending out the same xDS configuration to a large set of Envoy instances. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14781/comments",
    "author": "snowp",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-01-21T16:04:03Z",
        "body": "Currently the easiest way to add this would be to add it as a `DataInput<HttpMatchingData>`, but it would be interesting to support this more generically by having a different category of `DataInput`s that doesn't require protocol data (other examples would be things like the current time, information from Envoy's bootstrap, etc.) that would automatically work regardless of which protocol data the match tree is using. "
      },
      {
        "user": "kailun-qin",
        "created_at": "2021-01-22T13:30:16Z",
        "body": "We might not be able to abstract one single category of `DataInput` for all the potential usages here (env vars, current time, info from bootstrap etc.). I guess the requirement here is to start w/ the env vars at the first place?"
      },
      {
        "user": "snowp",
        "created_at": "2021-01-28T16:05:01Z",
        "body": "When I mentioned having a different category of DataInput I was talking about how the templating of DataInput works today. \r\n\r\nRight now each DataInput is templated by the protocol specific input (e.g. `DataInput<HttpMatchingData>`), so if we wanted to support an env var input we'd have to make a `EnvironmentInput : public DataInput<HttpMatchingData>` and register it. \r\n\r\nThis typing means that we'd need to do the same if we ever wanted to use it in another context, e.g. if we introduced `DataInput<NetworkMatchingData>`, then we'd need to create a new `EnvironmentNetworkInput : public DataInput<NetworkMatchingData>`.  As you can see this is a bit cumbersome, so it would be nicer if we could define a `EnvironmentInput : public GenericDataInput` which wasn't dependent on the protocol type.\r\n\r\nWhen I listed the examples I was listing other input types that would fit under this `GenericDataInput` category, not insinuating that they should be the same input type."
      }
    ]
  },
  {
    "number": 14736,
    "title": "Support internal redirects with body to support the semantics of 307 redirects",
    "created_at": "2021-01-17T23:45:30Z",
    "closed_at": "2021-04-08T23:12:49Z",
    "labels": [
      "help wanted",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14736",
    "body": "*Title*: Support internal redirects with body to support the semantics of 307 redirects\r\n\r\n*Description*:\r\nEnvoy today handles internal redirects based on 3xx status codes from upstream clusters. There are a some limitation with this. While it can internally redirect 307s, it will not allow the incoming request to have a body and will fallback to return the 307 to the downstream server. Since 307s mean that the original body and method should be used, it is severely limiting the usage of internal redirect.\r\n\r\nBeing able to have request with a  body and with the ability to handle this through an internal redirect would for example be able to transparently handle cases where functionality has been migrated to a different service. This would also allow other use cases where transparent re-processing in a different service is desired and changing the client to understand the 307 and resubmit is not possible.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14736/comments",
    "author": "jespersoderlund",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-01-18T17:25:31Z",
        "body": "cc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2021-01-19T13:50:15Z",
        "body": "Yeah, we definitely want to support internal redirects with bodies in the long run, it's just a question of prioritization.\r\nSome recent refactors to allow streaming retries should make this easier, but I still don't think it's on anyone's task list.  If you want to pick it up, I'd be happy to do reviews!\r\ncc @penguingao as well."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-02-22T20:04:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-02T00:07:05Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "derekargueta",
        "created_at": "2021-03-04T07:18:22Z",
        "body": "@alyssawilk can we get this issue re-opened?"
      }
    ]
  },
  {
    "number": 14366,
    "title": "test flake in overload_integration_test",
    "created_at": "2020-12-10T19:21:18Z",
    "closed_at": "2022-06-24T01:22:45Z",
    "labels": [
      "help wanted",
      "area/test flakes",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14366",
    "body": "It looks like `OverloadScaledTimerIntegrationTest.CloseIdleHttpConnections` is flakey on the master branch:\r\n\r\n```\r\nbazel test //test/integration:overload_integration_test --test_env=ENVOY_IP_TEST_VERSIONS=v4only --runs_per_test=1000 --test_filter=Protocols/OverloadScaledTimerIntegrationTest.CloseIdleHttpConnections/IPv4_Http2Downstream_HttpUpstream\r\n<snip>\r\n//test/integration:overload_integration_test                             FAILED in 1 out of 1000 in 14.3s\r\n  Stats over 1000 runs: max = 14.3s, min = 6.6s, avg = 8.4s, dev = 1.3s\r\n\r\nExecuted 1 out of 1 test: 1 fails locally.\r\nINFO: Build completed, 1 test FAILED, 1027 total actions\r\n```\r\n\r\nTest output:\r\n\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\nExecuting tests from //test/integration:overload_integration_test\r\n-----------------------------------------------------------------------------\r\nNote: Google Test filter = Protocols/OverloadScaledTimerIntegrationTest.CloseIdleHttpConnections/IPv4_Http2Downstream_HttpUpstream\r\n[==========] Running 1 test from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from Protocols/OverloadScaledTimerIntegrationTest\r\n[ RUN      ] Protocols//IPv4_Http2Downstream_HttpUpstream\r\ntest/integration/overload_integration_test.cc:314: Failure\r\nValue of: codec_client_->sawGoAway()\r\n  Actual: false\r\nExpected: true\r\nStack trace:\r\n  0x20b109a: Envoy::OverloadScaledTimerIntegrationTest_CloseIdleHttpConnections_Test::TestBody()\r\n  0x4b0faf4: testing::internal::HandleSehExceptionsInMethodIfSupported<>()\r\n  0x4affdee: testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n  0x4ae8473: testing::Test::Run()\r\n  0x4ae906d: testing::TestInfo::Run()\r\n... Google Test internal frames ...\r\n\r\n[  FAILED  ] Protocols/OverloadScaledTimerIntegrationTest.CloseIdleHttpConnections/IPv4_Http2Downstream_HttpUpstream, where GetParam() = 12-byte object <00-00 00-00 01-00 00-00 00-00 00-00> (8044 ms)\r\n[----------] 1 test from Protocols/OverloadScaledTimerIntegrationTest (8046 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test suite ran. (8047 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] Protocols/OverloadScaledTimerIntegrationTest.CloseIdleHttpConnections/IPv4_Http2Downstream_HttpUpstream, where GetParam() = 12-byte object <00-00 00-00 01-00 00-00 00-00 00-00>\r\n\r\n 1 FAILED TEST\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14366/comments",
    "author": "mpuncel",
    "comments": [
      {
        "user": "ankatare",
        "created_at": "2021-12-07T11:05:29Z",
        "body": "seems passing  in latest code. \r\nsee below results\r\n\r\n```\r\n:~/new_folder/envoy$ bazel test //test/integration:overload_integration_test --test_env=ENVOY_IP_TEST_VERSIONS=v4only --runs_per_test=1000 --test_filter=Protocols/OverloadScaledTimerIntegrationTest.CloseIdleHttpConnections/IPv4_Http2Downstream_HttpUpstream\r\nINFO: Build options --runs_per_test, --test_env, and --test_filter have changed, discarding analysis cache.\r\nINFO: Analyzed target //test/integration:overload_integration_test (1 packages loaded, 19165 targets configured).\r\nINFO: Found 1 test target...\r\nTarget //test/integration:overload_integration_test up-to-date:\r\n  bazel-bin/test/integration/overload_integration_test\r\nINFO: Elapsed time: 116.408s, Critical Path: 51.81s\r\nINFO: 2013 processes: 4 internal, 2009 linux-sandbox.\r\nINFO: Build completed successfully, 2013 total actions\r\n//test/integration:overload_integration_test                             PASSED in 0.8s\r\n  Stats over 2000 runs: max = 0.8s, min = 0.2s, avg = 0.3s, dev = 0.0s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\nINFO: Build completed successfully, 2013 total actions\r\n\r\n```"
      },
      {
        "user": "ggreenway",
        "created_at": "2022-06-22T14:47:21Z",
        "body": "Saw this again today on Windows in main branch CI.\r\n\r\n```\r\n[ RUN      ] Protocols/OverloadScaledTimerIntegrationTest.CloseIdleHttpConnections/IPv4_Http2Downstream_Http2UpstreamNghttp2NoDeferredProcessing\r\ntest/integration/overload_integration_test.cc(250): error: Value of: codec_client_->sawGoAway()\r\n  Actual: false\r\nExpected: true\r\n```"
      }
    ]
  },
  {
    "number": 14316,
    "title": "subset_selectors: fallback policy support healthy threshold",
    "created_at": "2020-12-08T07:06:27Z",
    "closed_at": "2023-11-10T05:32:21Z",
    "labels": [
      "design proposal",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14316",
    "body": "*Title*: *subset_selectors: fallback policy support healthy threshold*\r\n\r\n*Description*:\r\n\r\nWhen using the subset fallback capability, is it possible to add an option fallback_healthy_threshold, if the number of healthy nodes in the current subset is lower than the threshold, the fallback will be forced, for example\r\n\r\n```\r\nlb_subset_config:\r\n  fallback_policy: NO_FALLBACK\r\n  subset_selectors:\r\n  - keys:\r\n    - stage\r\n    - region\r\n    fallback_policy: KEYS_SUBSET\r\n    fallback_healthy_threshold: 20%\r\n    fallback_keys_subset:\r\n      - stage\r\n```\r\n\r\nSubset uses the site and area to filter the upstream nodes. When the current number of healthy nodes in the site is less than 20%, force fallback logic to increase the overall success rate.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14316/comments",
    "author": "drawing",
    "comments": [
      {
        "user": "dio",
        "created_at": "2020-12-08T07:38:11Z",
        "body": "cc. @snowp "
      },
      {
        "user": "snowp",
        "created_at": "2020-12-08T12:03:35Z",
        "body": "This wouldn't be that hard to get working in some fashion, but the presence of multiple priorities etc., make it trickier to come up with a great solution that doesn't have odd edge cases. For example, if the primary subset has two priorities, and all the unhealthy hosts are in the second priority that would normally get 0 traffic, do you still want to fail over? \r\n\r\nIt kinda seems like we'd want this to work more closely to how priorities work for general load balancing, where you might want to define multiple levels of subsets and have traffic spill over between the desired subsets as the preferred ones go unhealthy. This would be more work but seems to have nicer properties around graceful failover.\r\n\r\n@zuercher might have thoughts as well here."
      },
      {
        "user": "drawing",
        "created_at": "2020-12-11T14:23:50Z",
        "body": "I'm describing the usage scenarios, such as using the subset label to distinguish between us-east and us-west machines. In normal scenarios, the envoy of us-east forwards to the us-east machine of the service, and the minimum delay can be obtained. When the service us-east machine is abnormal, in order to ensure availability, the service can sacrifice time delay and go to the fallback logic without distinguishing between regions.\r\n\r\nWhen there are too many machines hanging up, the capacity of the remaining surviving machines is not enough to support the traffic in an area. or sometimes the exception does not cause all machines to be completely down, but intermittent service interruption. Since not all machines interrupt service at the same time, it is impossible to reach the fallback logic of subset."
      }
    ]
  },
  {
    "number": 14310,
    "title": "DNS resolver as an extension point",
    "created_at": "2020-12-07T23:47:35Z",
    "closed_at": "2021-10-15T20:15:29Z",
    "labels": [
      "design proposal",
      "help wanted",
      "area/dns"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14310",
    "body": "We currently have c-ares and Apple iOS DNS resolver support as compile time options. We might also be interested in a site-specific DNS resolver library. I think it's reasonable to make this a standard extension point.\r\n\r\n@junr03 since you were last digging into this, any thoughts on whether it would be feasible to accommodate the various DNS resolvers this way?\r\n\r\nCC @yanavlasov @antoniovicente @asraa @adisuissa @KBaichoo @akonradi ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14310/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "junr03",
        "created_at": "2020-12-08T00:16:31Z",
        "body": "It would certainly be feasible. @mattklein123 and I discussed it somewhere  (I am trying to find where) when I was adding the Apple Resolver, and decided for the current setup for expediency. However, there aren't any **major** technical reasons why the resolvers couldn't be formalized as an extension point. In fact, it would help clean up configuration that doesn't apply to all resolvers.\r\n\r\nResolvers are created via the dispatcher, so there is already a small surface for resolver creation. In terms of configuration consumption there are a few places where the extension would have to be accessible: some clusters, dns cache, udp dns filter; and additionally the \"default\" server resolver.\r\n\r\nSo overall, my assessment is that while there is some cleaning up to do it would actually be nice to elevate resolvers as an extension point, and something I would have wanted to do if I had had more time in the past. Happy to help whomever wants to take this on."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-12-08T17:33:09Z",
        "body": "Yes I agree making the DNS resolver a full extension would be good."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-07T20:08:46Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "yanjunxiang-google",
        "created_at": "2021-06-30T19:23:30Z",
        "body": "I am looking at this issue now."
      },
      {
        "user": "yanjunxiang-google",
        "created_at": "2021-06-30T19:23:48Z",
        "body": "@yanavlasov "
      },
      {
        "user": "KBaichoo",
        "created_at": "2021-06-30T19:26:40Z",
        "body": "/assign @yanjunxiang-google "
      },
      {
        "user": "htuch",
        "created_at": "2021-07-19T01:52:19Z",
        "body": "Reopening, since #17272 was just the API changes (@yanjunxiang-google best to put \"Part of #xyzabc\" instead of \"Fixes #xyzabc\" when working on a series of patches on an issue)."
      }
    ]
  },
  {
    "number": 14309,
    "title": "alt_stat_name as a standardized observability label for clusters",
    "created_at": "2020-12-07T23:38:39Z",
    "closed_at": "2021-03-02T17:52:24Z",
    "labels": [
      "area/tracing",
      "design proposal",
      "help wanted",
      "area/stats",
      "area/access_log",
      "area/observability"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14309",
    "body": "Currently, `Cluster` has an `alt_stat_name` field that is used for stats only. It seems that this would be useful for both tracing and access logs (to the extent that service names appear that at all). This allows for a semantically meaningful label to be consumed by monitoring and humans, regardless of the resource name (which might be reflecting addressing or config pipeline artifacts, e.g. when we start using `xdstp://` URLs).\r\n\r\nI'd like to propose we either repurpose `alt_stat_name` as the general observability name or create an explicit, more general, `observability_name` that applies across stats/logging/tracing. Within Envoy, we would have a clean C++ abstraction to access this in `ClusterInfo`. \r\n\r\nUltimately this would apply to other resources as well, but clusters is where we (Google) have a specific interest today.\r\n\r\nCC @envoyproxy/api-shepherds ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14309/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-12-08T16:46:47Z",
        "body": "I think it would be fine to use `alt_stat_name` for general observability naming and tag the field for a name change in the next major version. I'm not sure it's worth the churn to change the name now."
      },
      {
        "user": "htuch",
        "created_at": "2020-12-08T23:52:49Z",
        "body": "Sounds good. I imagine we should set a runtime flag to control its use in tracing/logs, in case this breaks anyone, but we can default it true?"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-12-09T00:39:58Z",
        "body": "Sure runtime flag sgtm but IMO we can default it to on as you say."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-08T04:24:16Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      }
    ]
  },
  {
    "number": 14307,
    "title": "Remove memory  allocation in Win32 WatcherImpl::directoryChangeCompletion",
    "created_at": "2020-12-07T19:15:26Z",
    "closed_at": "2020-12-29T00:03:21Z",
    "labels": [
      "bug",
      "tech debt",
      "help wanted",
      "area/windows"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14307",
    "body": "In `WatcherImpl::directoryChangeCompletion` we allocate a `Buffer::OwnedImpl` to call `ioHandle` `write`.\r\n\r\nThis should be changed to `writev` to prevent memory allocation or we should implement and use #14244",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14307/comments",
    "author": "davinci26",
    "comments": [
      {
        "user": "davinci26",
        "created_at": "2020-12-07T19:15:55Z",
        "body": "cc @envoyproxy/windows-dev "
      },
      {
        "user": "rmiller14",
        "created_at": "2020-12-14T23:34:44Z",
        "body": "I don't have the ability to assign this, but I went ahead and submitted a PR for this issue."
      }
    ]
  },
  {
    "number": 14226,
    "title": "Create upstream response body in LUA when it is NIL",
    "created_at": "2020-12-01T10:42:17Z",
    "closed_at": "2021-01-06T17:24:03Z",
    "labels": [
      "help wanted",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14226",
    "body": "The LUA filter documentation states that you can override the upstream response body by doing this :\r\n\r\n```\r\nfunction envoy_on_response(response_handle)\r\n  local content_length = response_handle:body():setBytes(\"<html><b>Not Found<b></html>\")\r\n  response_handle:headers():replace(\"content-length\", content_length)\r\n  response_handle:headers():replace(\"content-type\", \"text/html\")\r\nend\r\n```\r\n\r\nHowever, in some cases, the upstream response has no body, thus making it impossible to change its content.\r\n\r\nIs there a workaround for this ? If not, that could be an interesting feature.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14226/comments",
    "author": "s-vivien",
    "comments": [
      {
        "user": "vaibhavkumbhar1",
        "created_at": "2024-10-11T05:26:17Z",
        "body": "There is a way, you can achieve by passing true as a param to the body.\r\nAdding example here so that it can be helpful to other\r\n\r\n\r\n```\r\n                        function envoy_on_response(response_handle)\r\n                          -- Check if the status code is 429 and response came from envoy rate limit service\r\n                          local status = response_handle:headers():get(\":status\")\r\n                          local rate_limit_header = response_handle:headers():get(\"x-envoy-ratelimited\")\r\n                          if status == \"429\" and rate_limit_header then\r\n                            -- Set a custom response body\r\n                            response_handle:headers():replace(\"content-type\", \"application/json\")\r\n                            local content_length = response_handle:body(true):setBytes(\"{\\\"error\\\":{\\\"description\\\":\\\"Too Many Requests\\\",\\\"code\\\":\\\"400040000429\\\",\\\"additionalInfo\\\":{}}}\")\r\n                            response_handle:headers():replace(\"content-length\", content_length)\r\n                          end\r\n                        end\r\n\r\n```\r\n"
      }
    ]
  },
  {
    "number": 14141,
    "title": "Handle container started as non-root better",
    "created_at": "2020-11-23T05:49:43Z",
    "closed_at": "2021-03-02T17:07:20Z",
    "labels": [
      "area/docs",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14141",
    "body": "### description\r\n\r\nIt seems like a common issue that people try to start the container as non-root, which doesnt work with the `ENTRYPOINT` currently.\r\n\r\nUsers then seem to set `ENVOY_UID` and that only confuses the issue further.\r\n\r\nI think we can do a couple of things. \r\n\r\n- print a very big warning in the entrypoint if it  detects that it has been started by a user other than root - perhaps with link to docs\r\n- allow the container to be started (with an explicit flag) as non-root in which case it wont try to drop perms or chown stuff etc\r\n\r\nstarting as non-root will probably work in most situations, esp if you ensure the env yourself - not sure off hand about stdout/err\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14141/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2020-11-23T05:56:09Z",
        "body": "this may be an issue for some specific frameworks or at least common setups - eg openshift"
      },
      {
        "user": "phlax",
        "created_at": "2020-11-23T05:57:56Z",
        "body": "one idea i was thinking of for starting as non-root is that if the ENVOY_UID is set *and* the container is started with the same uid, perhaps it could skip the permissions dropping etc "
      },
      {
        "user": "phlax",
        "created_at": "2020-11-23T05:58:37Z",
        "body": "@lizan any thoughts on this ?"
      },
      {
        "user": "phlax",
        "created_at": "2020-11-23T19:18:50Z",
        "body": "/assign phlax"
      },
      {
        "user": "lizan",
        "created_at": "2020-12-12T06:53:35Z",
        "body": "I forgot why we had chown in the first place, in most environment it isn't needed anyway... /dev/stdout is a symlink to /proc/self/fd/1"
      },
      {
        "user": "phlax",
        "created_at": "2020-12-12T07:06:15Z",
        "body": "im trying to rem too - my vague recollection was that it was needed\r\n\r\ni guess there are two options that might improve this - remove it (if unneeded), or add some logic to conditionally do whatever perms/user checking/dropping\r\n\r\ni think the  main issue is that users are trying to start the container (from outside) as non-root, which can work i think, but wont with the current permission dropping\r\n"
      },
      {
        "user": "lizan",
        "created_at": "2020-12-12T10:24:29Z",
        "body": "agreed, I'm ok with either way."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-11T12:13:24Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "phlax",
        "created_at": "2021-01-13T10:13:02Z",
        "body": "bump"
      },
      {
        "user": "mtttcgcg",
        "created_at": "2021-02-19T01:31:48Z",
        "body": "This is more important for us now that Envoy 1.14's xDS v2 API is deprecated. 1.14 was the last version we were able to run without encountering this issue."
      },
      {
        "user": "phlax",
        "created_at": "2021-02-19T21:44:42Z",
        "body": "@mtttcgcg there is a fix i think in #15115 \r\n\r\nill follow up with an update to docs once that has landed"
      },
      {
        "user": "phlax",
        "created_at": "2021-02-26T08:46:27Z",
        "body": "@ToniCipriani @mtttcgcg if you can confirm that this issue was resolved in #15115 ill add a note in docs and we can close this ticket i think"
      },
      {
        "user": "ToniCipriani",
        "created_at": "2021-02-26T14:14:07Z",
        "body": "> @ToniCipriani @mtttcgcg if you can confirm that this issue was resolved in #15115 ill add a note in docs and we can close this ticket i think\r\n\r\nLooks good. Deployed envoyproxy/envoy-alpine-dev, container did not complain and I can see logs. Did a few basic tests and seems to work fine.\r\n\r\nThanks."
      },
      {
        "user": "ToniCipriani",
        "created_at": "2021-02-26T19:27:04Z",
        "body": "Also, will this be part of the next 1.17 release?"
      },
      {
        "user": "phlax",
        "created_at": "2021-03-01T12:39:21Z",
        "body": "@Shikugawa would we be able to backport #15115 and/or include in current release ?"
      },
      {
        "user": "Shikugawa",
        "created_at": "2021-03-01T13:04:59Z",
        "body": "@phlax It makes sense. Affected versions are 1.15, 1.16 and 1.17, right?"
      },
      {
        "user": "phlax",
        "created_at": "2021-03-01T13:10:41Z",
        "body": "great, thanks\r\n\r\n>  Affected versions are 1.15, 1.16 and 1.17, right?\r\n\r\nyep - just checked the `v1.15-latest` image and that has the problem entrypoint"
      },
      {
        "user": "ToniCipriani",
        "created_at": "2021-08-25T10:58:33Z",
        "body": "@phlax We noticed the same problem again on one of our clusters. However it doesn't mention the additional message, and it tried to drop the permissions, resulting in the chown error again. I will have to check the service account configuration on what it's trying to run as."
      },
      {
        "user": "phlax",
        "created_at": "2021-08-25T11:19:20Z",
        "body": "im a little confused - im trying to figure out how either, it could be running as non-root but get a uid of 0, *or*, its running as root but cant chown"
      }
    ]
  },
  {
    "number": 14098,
    "title": "Allow usage of xDS data from LUA",
    "created_at": "2020-11-19T12:00:50Z",
    "closed_at": "2020-12-20T09:46:10Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14098",
    "body": "*Title*: Allow usage of xDS data from LUA\r\n\r\n*Description*:\r\n\r\nCurrently we are using LUA to do URL mapping. It requires a large list of URL patterns and for each a replacement pattern. We have around 1000+ of these. It cannot make use of regex, as this would not be performant enough.\r\n\r\nThe LUA script initially loads a local file (which is filled by `consul-template`) and sets up a lookup tree for the URL patterns. Every time the set of patterns changes, Envoy needs to fully reload in order for LUA to reload the file. Reloading Envoy is not as stable, as triggering reloads too fast would cause `previous envoy process is still initializing`.\r\n\r\nIdeally we want LUA to be able to have some kind of xDS service available where we can expose the required data and let LUA reload the lookup-tree when it changes or reload LUA itself with the data being passed to the script. From what I'm gathering this is not possible with the current LUA APIs.\r\n\r\nThis is a request for allowing LUA to access dynamic data from xDS. However, suggestions are very welcome for an alternative implementation. Should we move to WASM where such features are more viable?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14098/comments",
    "author": "bobvanderlinden",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-20T00:07:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "dio",
        "created_at": "2020-12-20T02:00:36Z",
        "body": "I think you can send a per-route Lua script via RDS (or via LDS if it is a global script). You can embed the data inside the script, prepared by the xDS Server. "
      },
      {
        "user": "bobvanderlinden",
        "created_at": "2020-12-20T09:46:10Z",
        "body": "This is indeed how we eventually solved it just recently. It does feel a bit dirty, but it seems work quite well.\r\n\r\nOur setup now adds lua files in the working directory of Envoy to avoid sending over multiple files. We use ECDS to just update the lua http filter. We eventually chose to use file-based xDS. The file is generated from Consul using consul-template, which writes YAML with a large table in the embedded lua and ends with requiring the local files and passing the table to that module.\r\n\r\nWhat might make the process a bit cleaner is adding initialization parameters alongside the inline script in the configuration. This allows embedding data in yaml/json. This avoids needing to generate safe lua code.\r\n\r\nThat said, for us this is solved. I think this issue can be closed."
      }
    ]
  },
  {
    "number": 14091,
    "title": "Lua filter causes a crash when the script uses streamInfo():downstreamSslConnection()",
    "created_at": "2020-11-19T08:43:49Z",
    "closed_at": "2020-11-23T08:54:38Z",
    "labels": [
      "bug",
      "help wanted",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14091",
    "body": "Envoy crashes with segmentation fault, when Lua script contains `streamInfo():downstreamSslConnection()`.\r\nThe crash happens after issuing many https requests.\r\n\r\nThe bug exists in v1.16.0 as well as in the latest master version.\r\n\r\nI have a permission from envoy-security@googlegroups.com for fixing this publicly.\r\n\r\n\r\n*Repro steps*:\r\nRun envoy with attached config and make 1000 https requests.\r\n\r\n*Config*:\r\n<details>\r\n  <summary>Click to expand!</summary>\r\n\r\n```yaml\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 10000\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: ingress_http\r\n                http_filters:\r\n                  - name: envoy.filters.http.lua\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n                      inline_code: |\r\n                        function envoy_on_request(handle)\r\n                          if handle:connection():ssl() ~= nil then\r\n                            if handle:streamInfo() ~= nil then\r\n                              if handle:streamInfo():downstreamSslConnection() ~= nil then\r\n                                handle:logInfo(\"lua - envoy_on_request - SSL; streamInfo == present; downstreamSslConnection == present\")\r\n                              else\r\n                                handle:logInfo(\"lua - envoy_on_request - SSL; streamInfo == present; downstreamSslConnection == nil\")\r\n                              end\r\n                            else\r\n                              handle:logInfo(\"lua - envoy_on_request - SSL; streamInfo == nil\")\r\n                            end\r\n                          else\r\n                            handle:logInfo(\"lua - envoy_on_request - RAW BUFFER\")\r\n                          end\r\n                        end\r\n                  - name: envoy.filters.http.router\r\n                route_config:\r\n                  name: local_route\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match:\r\n                            prefix: \"/\"\r\n                          direct_response:\r\n                            status: 200\r\n                            body:\r\n                              inline_string: \"Envoy direct response\"\r\n          transport_socket:\r\n            name: envoy.transport_sockets.tls\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n              common_tls_context:\r\n                tls_certificates:\r\n                  certificate_chain:\r\n                    filename: ./certs/rsa-2048-signed-rsa-server/cert.crt\r\n                  private_key:\r\n                    filename: ./certs/rsa-2048-signed-rsa-server/key.prv\r\n\r\n```\r\n\r\n</details>\r\n\r\n*Call Stack*:\r\n<details>\r\n  <summary>Click to expand!</summary>\r\n\r\n```\r\nThread 11 \"wrk:worker_2\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7ffff280f700 (LWP 71)]\r\n0x00005555567d8ef0 in lua_rawgeti ()\r\n#0  0x00005555567d8ef0 in lua_rawgeti ()\r\n#1  0x000055555682905c in luaL_unref ()\r\n#2  0x00005555567b8790 in Envoy::Extensions::Filters::Common::Lua::LuaRef<Envoy::Extensions::Filters::Common::Lua::SslConnectionWrapper>::unref (\r\n    this=<optimized out>)\r\n    at bazel-out/k8-opt/bin/source/extensions/filters/common/lua/_virtual_includes/lua_lib/extensions/filters/common/lua/lua.h:277\r\n#3  Envoy::Extensions::Filters::Common::Lua::LuaRef<Envoy::Extensions::Filters::Common::Lua::SslConnectionWrapper>::~LuaRef (this=<optimized out>)\r\n    at bazel-out/k8-opt/bin/source/extensions/filters/common/lua/_virtual_includes/lua_lib/extensions/filters/common/lua/lua.h:239\r\n#4  Envoy::Extensions::Filters::Common::Lua::LuaDeathRef<Envoy::Extensions::Filters::Common::Lua::SslConnectionWrapper>::~LuaDeathRef (this=<optimized out>)\r\n    at bazel-out/k8-opt/bin/source/extensions/filters/common/lua/_virtual_includes/lua_lib/extensions/filters/common/lua/lua.h:294\r\n#5  Envoy::Extensions::HttpFilters::Lua::StreamInfoWrapper::~StreamInfoWrapper\r\n    (this=0x7ffff7e23c38)\r\n    at bazel-out/k8-opt/bin/source/extensions/filters/http/lua/_virtual_includes/wrappers_lib/extensions/filters/http/lua/wrappers.h:181\r\n#6  0x00005555567b4b27 in Envoy::Extensions::Filters::Common::Lua::BaseLuaObject<Envoy::Extensions::HttpFilters::Lua::StreamInfoWrapper>::registerType(lua_State*)::{lambda(lua_State*)#1}::operator()(lua_State*) const (\r\n    this=<optimized out>, state=<optimized out>)\r\n    at bazel-out/k8-opt/bin/source/extensions/filters/common/lua/_virtual_includes/lua_lib/extensions/filters/common/lua/lua.h:146\r\n#7  0x00005555567c3b7a in lj_BC_FUNCC ()\r\n#8  0x00005555567c62ec in gc_finalize ()\r\n#9  0x00005555567c6a58 in gc_onestep ()\r\n#10 0x00005555567c6628 in lj_gc_step ()\r\n#11 0x00005555567d8929 in lua_newuserdata ()\r\n#12 0x00005555567abd90 in Envoy::Extensions::Filters::Common::Lua::allocateLuaUserData<Envoy::Extensions::HttpFilters::Lua::StreamHandleWrapper> (state=\r\n    0x7ffff7e23ce8)\r\n    at bazel-out/k8-opt/bin/source/extensions/filters/common/lua/_virtual_includes/lua_lib/extensions/filters/common/lua/lua.h:82\r\n#13 Envoy::Extensions::Filters::Common::Lua::BaseLuaObject<Envoy::Extensions::HttpFilters::Lua::StreamHandleWrapper>::create<Envoy::Extensions::Filters::Common::Lua::Coroutine&, Envoy::Http::HeaderMap&, bool&, Envoy::Extensions::HttpFilters::Lua::Filter&, Envoy::Extensions::HttpFilters::Lua::FilterCallbacks&> (\r\n    state=0x7ffff7e23ce8, args=..., args=..., args=..., args=..., args=...)\r\n    at bazel-out/k8-opt/bin/source/extensions/filters/common/lua/_virtual_includes/lua_lib/extensions/filters/common/lua/lua.h:121\r\n#14 0x00005555567abc44 in Envoy::Extensions::HttpFilters::Lua::Filter::doHeaders (this=0x2d78bf1a3778, handle=..., coroutine=..., callbacks=...,\r\n    function_ref=1, setup=<optimized out>, headers=..., end_stream=true)\r\n    at source/extensions/filters/http/lua/lua_filter.cc:684\r\n#15 0x00005555567ad6c5 in Envoy::Extensions::HttpFilters::Lua::Filter::decodeHeaders (this=0x2d78bf1a3778, headers=..., end_stream=true)\r\n    at bazel-out/k8-opt/bin/source/extensions/filters/http/lua/_virtual_includes/lua_filter_lib/extensions/filters/http/lua/lua_filter.h:442\r\n#16 0x000055555749a02f in Envoy::Http::ActiveStreamDecoderFilter::decodeHeaders (this=0x2d78bfa12900, headers=..., end_stream=true)\r\n    at bazel-out/k8-opt/bin/source/common/http/_virtual_includes/filter_manager_lib/common/http/filter_manager.h:199\r\n#17 Envoy::Http::FilterManager::decodeHeaders (this=<optimized out>,\r\n    filter=<optimized out>, headers=..., end_stream=<optimized out>)\r\n    at source/common/http/filter_manager.cc:452\r\n#18 0x000055555748dc93 in Envoy::Http::FilterManager::decodeHeaders (\r\n    this=0x2d78bf217278, headers=..., end_stream=<optimized out>)\r\n    at bazel-out/k8-opt/bin/source/common/http/_virtual_includes/filter_manager_lib/common/http/filter_manager.h:582\r\n#19 Envoy::Http::ConnectionManagerImpl::ActiveStream::decodeHeaders (\r\n    this=0x2d78bf217200, headers=..., end_stream=<optimized out>)\r\n    at source/common/http/conn_manager_impl.cc:1024\r\n#20 0x00005555574ac6f6 in Envoy::Http::Legacy::Http1::ServerConnectionImpl::onMessageComplete (this=0x2d78bf15e680)\r\n    at source/common/http/http1/codec_impl_legacy.cc:970\r\n#21 0x00005555574aa9e5 in Envoy::Http::Legacy::Http1::ConnectionImpl::onMessageCompleteBase (this=0x2d78bf15e690)\r\n    at source/common/http/http1/codec_impl_legacy.cc:764\r\n#22 0x00005555574a78dd in Envoy::Http::Legacy::Http1::ConnectionImpl::$_8::operator() (this=<optimized out>, parser=<optimized out>)\r\n    at source/common/http/http1/codec_impl_legacy.cc:434\r\n#23 Envoy::Http::Legacy::Http1::ConnectionImpl::$_8::__invoke (\r\n    parser=<optimized out>)\r\n    at source/common/http/http1/codec_impl_legacy.cc:433\r\n#24 0x0000555557631beb in http_parser_execute (parser=0x2d78bf15e6d8,\r\n    settings=<optimized out>, data=<optimized out>, len=<optimized out>)\r\n    at external/com_github_nodejs_http_parser/http_parser.c:2167\r\n#25 0x00005555574a91fa in Envoy::Http::Legacy::Http1::ConnectionImpl::dispatchSlice (this=0x2d78bf15e690, slice=0x0, len=140737261838648)\r\n    at source/common/http/http1/codec_impl_legacy.cc:572\r\n#26 0x00005555574a8d9f in Envoy::Http::Legacy::Http1::ConnectionImpl::innerDispatch (this=0x2d78bf15e690, data=...)\r\n    at source/common/http/http1/codec_impl_legacy.cc:548\r\n#27 0x00005555574b1552 in Envoy::Http::Legacy::Http1::ConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_14::operator()(Envoy::Buffer::Instance&) const (\r\n    this=<optimized out>, data=...)\r\n    at source/common/http/http1/codec_impl_legacy.cc:531\r\n#28 _ZNSt3__18__invokeIRZN5Envoy4Http6Legacy5Http114ConnectionImpl8dispatchERNS1_6Buffer8InstanceEE4$_14JS8_EEEDTclclsr3std3__1E7forwardIT_Efp_Espclsr3std3__1E7forwardIT0_Efp0_EEEOSB_DpOSC_ (__f=..., __args=...)\r\n    at /opt/llvm/bin/../include/c++/v1/type_traits:3539\r\n#29 std::__1::__invoke_void_return_wrapper<absl::Status>::__call<Envoy::Http::Legacy::Http1::ConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_14(Envoy::Buffer::Instance&)&>(Envoy::Http::Legacy::Http1::ConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_14(Envoy::Buffer::Instance&)&) (__args=..., __args=...)\r\n    at /opt/llvm/bin/../include/c++/v1/__functional_base:317\r\n#30 std::__1::__function::__alloc_func<Envoy::Http::Legacy::Http1::ConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_14(std::__1::allocator<std::__1::allocator>, absl::Status (Envoy::Buffer::Instance&))>::operator()(Envoy::Buffer::Instance&) (this=<optimized out>, __arg=...)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:1540\r\n#31 std::__1::__function::__func<Envoy::Http::Legacy::Http1::ConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_14(std::__1::allocator<std::__1::allocator>, absl::Status (Envoy::Buffer::Instance&))>::operator()(Envoy::Buffer::Instance&) (this=<optimized out>, __arg=...)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:1714\r\n#32 0x0000555557603847 in std::__1::__function::__value_func<absl::Status (Envoy::Buffer::Instance&)>::operator()(Envoy::Buffer::Instance&) const (\r\n    this=<optimized out>, __args=...)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:1867\r\n#33 std::__1::function<absl::Status (Envoy::Buffer::Instance&)>::operator()(Envoy::Buffer::Instance&) const (this=<optimized out>, __arg=...)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:2473\r\n#34 Envoy::Http::Utility::exceptionToStatus(std::__1::function<absl::Status (Envoy::Buffer::Instance&)>, Envoy::Buffer::Instance&) (dispatch=..., data=...)\r\n    at source/common/http/utility.cc:43\r\n#35 0x00005555574a8a51 in virtual thunk to Envoy::Http::Legacy::Http1::ConnectionImpl::dispatch(Envoy::Buffer::Instance&) ()\r\n    at source/common/http/http1/codec_impl_legacy.cc:530\r\n#36 0x0000555557489e2c in Envoy::Http::ConnectionManagerImpl::onData (\r\n    this=0x2d78bfb483c0, data=...)\r\n    at source/common/http/conn_manager_impl.cc:282\r\n#37 0x0000555557202873 in Envoy::Network::FilterManagerImpl::onContinueReading\r\n    (this=0x2d78bf163bf8, filter=<optimized out>, buffer_source=...)\r\n    at source/common/network/filter_manager_impl.cc:66\r\n#38 0x00005555571fe845 in Envoy::Network::ConnectionImpl::onRead (\r\n    this=0x2d78bf163b80, read_buffer_size=256)\r\n    at source/common/network/connection_impl.cc:292\r\n#39 Envoy::Network::ConnectionImpl::onReadReady (this=0x2d78bf163b80)\r\n    at source/common/network/connection_impl.cc:574\r\n#40 0x00005555571fc815 in Envoy::Network::ConnectionImpl::onFileEvent (\r\n    this=0x2d78bf163b80, events=<optimized out>)\r\n    at source/common/network/connection_impl.cc:534\r\n#41 0x00005555571f2366 in std::__1::__function::__value_func<void (unsigned int)>::operator()(unsigned int&&) const (this=0x2d78bf127330,\r\n    __args=@0x7ffff27fb0cc: 3)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:1867\r\n#42 std::__1::function<void (unsigned int)>::operator()(unsigned int) const (\r\n    this=0x2d78bf127330, __arg=3)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:2473\r\n#43 Envoy::Event::FileEventImpl::mergeInjectedEventsAndRunCb (\r\n    this=0x2d78bf1272b0, events=3)\r\n    at source/common/event/file_event_impl.cc:133\r\n#44 Envoy::Event::FileEventImpl::assignEvents(unsigned int, event_base*)::$_1::operator()(int, short, void*) const (this=<optimized out>,\r\n    what=<optimized out>, arg=0x2d78bf1272b0)\r\n    at source/common/event/file_event_impl.cc:106\r\n#45 Envoy::Event::FileEventImpl::assignEvents(unsigned int, event_base*)::$_1::__invoke(int, short, void*) (what=<optimized out>, arg=0x2d78bf1272b0)\r\n    at source/common/event/file_event_impl.cc:90\r\n#46 0x000055555762afe8 in event_process_active_single_queue ()\r\n#47 0x00005555576299be in event_base_loop ()\r\n#48 0x00005555571e3838 in Envoy::Server::WorkerImpl::threadRoutine (\r\n    this=0x2d78bfbcd1d0, guard_dog=...) at source/server/worker_impl.cc:132\r\n#49 0x00005555577d88c3 in std::__1::__function::__value_func<void ()>::operator()() const (this=<optimized out>)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:1867\r\n#50 std::__1::function<void ()>::operator()() const (this=<optimized out>)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:2473\r\n#51 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, std::__1::optional<Envoy::Thread::Options> const&)::{lambda(void*)#1}::operator()(void*) const (this=<optimized out>, arg=<optimized out>)\r\n    at source/common/common/posix/thread_impl.cc:49\r\n#52 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, std::__1::optional<Envoy::Thread::Options> const&)::{lambda(void*)#1}::__invoke(void*) (arg=<optimized out>)\r\n    at source/common/common/posix/thread_impl.cc:48\r\n#53 0x00007ffff74116db in start_thread (arg=0x7ffff280f700)\r\n    at pthread_create.c:463\r\n#54 0x00007ffff713a71f in clone ()\r\n    at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\nA debugging session is active.\r\n\r\n```\r\n\r\n</details>\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14091/comments",
    "author": "MarcinFalkowski",
    "comments": [
      {
        "user": "ccannell67",
        "created_at": "2020-11-22T12:46:37Z",
        "body": "Experiencing the same SegFault when using downstreamSslConnection:subjectPeerCertificate(), but at a lower request threshold. SegFaults at < 20 requests. Can provide config if  needed. "
      }
    ]
  },
  {
    "number": 13778,
    "title": "Docs are published before Docker images - leaving dangling links",
    "created_at": "2020-10-27T15:23:30Z",
    "closed_at": "2020-11-10T05:44:21Z",
    "labels": [
      "area/docs",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13778",
    "body": "### description\r\n\r\nraised by a user in the slack channel...\r\n\r\nwhen docs are published, they publish links/instructions for a docker image `envoy-dev:SHA` with the `sha` corresponding to the last commit on master\r\n\r\nunfortunately when the docs are published the Docker images are still cooking, and the links dont work (for maybe an hour or so)\r\n\r\nas we are constantly updating master, i would imagine that we are quite frequently publishing links that dont work in the latest docs.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13778/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-10-27T15:27:13Z",
        "body": "cc @lizan I think it would be reasonable to split out docs publish from docs CI and do that after docker publish?"
      },
      {
        "user": "lizan",
        "created_at": "2020-10-27T21:17:13Z",
        "body": "yeah that sgtm."
      }
    ]
  },
  {
    "number": 13422,
    "title": "Give the option to increase the maximun size of the HTTP body in responses to be bigger than 4 KB",
    "created_at": "2020-10-07T08:37:26Z",
    "closed_at": "2021-02-02T02:56:55Z",
    "labels": [
      "help wanted",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13422",
    "body": "I know it is for keeping the proxy’s memory footprint from growing too large but it would help to have the option to use a body bigger than 4 KB for the HTTP responses. \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13422/comments",
    "author": "Andres-Escalante",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-10-07T15:28:17Z",
        "body": "I thought we had an issue tracking this but I can't find it. Yes it would be fine to make this configurable."
      }
    ]
  },
  {
    "number": 13285,
    "title": "Support for per route \"with_request_body\" configuration for ext_authz",
    "created_at": "2020-09-26T15:42:27Z",
    "closed_at": "2020-10-06T06:26:00Z",
    "labels": [
      "help wanted",
      "area/ext_authz"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13285",
    "body": "*Title*: Support for per route \"with_request_body\" configuration for ext_authz\r\n\r\n*Description*:\r\nFor the ext_authz extension we would like to be able to configure with_request_body for specific routes. Our use case is for enabling SAML from our ext_authz service which requires posting the SAML assertion in the body for authentication to a specific endpoint. We would like to be able to do this without the additional overhead of passing the body on every request other request.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13285/comments",
    "author": "nickhiggs",
    "comments": [
      {
        "user": "frasercobb",
        "created_at": "2020-10-06T22:12:52Z",
        "body": "Thank you for addressing this so quickly.\r\n\r\nCould you help me understand the design decision to have a disable flag rather than setting \"with_request_body\" ?\r\nI was originally thinking of using this feature to disable with request body by default, and enable it for a specific route.\r\n\r\nThanks again."
      },
      {
        "user": "dio",
        "created_at": "2020-10-07T02:42:42Z",
        "body": "@frasercobb yeah, I was about to have per route `with_request_body` but config wise it will be a bit complicated since we need to think of merging these two if both are enabled (global and per-route). And implementation wise this is easier. But yeah, of course, we can always revisit that."
      },
      {
        "user": "frasercobb",
        "created_at": "2020-10-07T02:50:37Z",
        "body": "@dio Thank you for the explanation, I would prefer a slightly clunky feature now rather than a perfect one later so your decision sits well with me. Thanks again."
      }
    ]
  },
  {
    "number": 13232,
    "title": "Add a welcome message/ DCO nag from the repo bot to new contributors",
    "created_at": "2020-09-23T15:14:09Z",
    "closed_at": "2021-01-07T21:41:04Z",
    "labels": [
      "area/community",
      "help wanted",
      "area/repokitteh"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13232",
    "body": "## Description\r\n\r\nFirst time contributors often havent added DCO - when reviewers point  this out it can be misunderstood as a criticism or as a review of the PR.\r\n\r\nIf the repo bot automatically posted a message saying something like \"welcome @newcontributor - please read contributing guidelines if you havent already...\" - it could also nag about DCO if that hasnt been added already\r\n\r\nnot sure if this is possible - might make it easier for new contribs - also flags to reviewers more explicitly that its a new contributor\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13232/comments",
    "author": "phlax",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-09-23T17:30:28Z",
        "body": "Agreed this would be neat. cc @itayd for thoughts on how to do this with RepoKitteh"
      },
      {
        "user": "itayd",
        "created_at": "2020-09-23T17:52:26Z",
        "body": "can you define explicitly what needs to be done here?\r\nwhen would the message be emitted? only for first time contributors? every pr?"
      },
      {
        "user": "phlax",
        "created_at": "2020-09-23T18:03:26Z",
        "body": ">  when would the message be emitted? only for first time contributors? every pr?\r\n\r\nthe welcome message would be for first time contributors (i guess if they havent opened a PR before too ?)\r\n\r\nfor the DCO nag - i guess either whenever DCO will fail (i need to look to see if this checks PR or commits...) or at least when a PR was opened and DCO would fail\r\n"
      },
      {
        "user": "phlax",
        "created_at": "2020-09-23T19:33:43Z",
        "body": "> i need to look to see if this checks PR or commits...\r\n\r\nthinking more about this - i think if *any commits* are missing DCO would probs be the rule to trigger (ie on any PR at any time)"
      },
      {
        "user": "itayd",
        "created_at": "2020-09-24T06:20:51Z",
        "body": "@phlax i think it makes sense on every time DCO emits, we should add a message that:\r\n1. explain nicely that we don't hate the contributor, it's just DCO\r\n2. explain how to resolve the DCO issue\r\n\r\nRe the welcome message, why not just point to it from a PR template? Add a link to a document welcoming people."
      },
      {
        "user": "phlax",
        "created_at": "2020-09-24T06:28:04Z",
        "body": "> Re the welcome message, why not just point to it from a PR template? Add a link to a document welcoming people.\r\n\r\ni was thinking this for a few reasons\r\n\r\n- there are some links/messages in PR template already i think - but not everyone reads that properly\r\n- in the case that DCO is missing, they will get a thankyou and welcome message first, so its less frightening\r\n- makes it obvious to reviewers that its a new contributor\r\n\r\ni think the DCO messaging is more important tho"
      },
      {
        "user": "phlax",
        "created_at": "2020-09-24T07:53:49Z",
        "body": "...another reason...\r\n\r\nif you create a new PR for the first time and it messages you immediately i think in most cases you will immediately receive an email with an `@contributor` message - and contrib guidelines etc"
      },
      {
        "user": "phlax",
        "created_at": "2020-09-25T14:08:23Z",
        "body": "I was thinking for messages...\r\n\r\n### welcome\r\n\r\n```\r\nWelcome @newcontributor and thankyou for your contribution.\r\n\r\nWe will try to review your Pull Request as quickly as possible.\r\n\r\nIn the meantime please read the contribution guidelines if you haven't already (link...)\r\n```\r\n### DCO\r\n```\r\nHi @dcolesscontributor it seems that one or more of the commits in your Pull Request has not been signed.\r\n\r\nWe require this to ensure we know and remember who made each contribution.\r\n\r\nYou can fix this with the following commands\r\n\\```\r\nINSTRUCTIONS\r\n\\```\r\nFull information can be found here (link...)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "itayd",
        "created_at": "2020-09-28T06:56:50Z",
        "body": "This makes sense. @phlax would you like to write this behavior yourself? I will teach you how."
      },
      {
        "user": "phlax",
        "created_at": "2020-09-28T06:58:34Z",
        "body": "> would you like to write this behavior yourself? I will teach you how.\r\n\r\ni would love to! \r\n\r\ni had a quick look to see if i could figure it out - but it wasnt immediately obvious and i was working on other things\r\n\r\nlmk what you want me to do to get started"
      },
      {
        "user": "mattklein123",
        "created_at": "2021-01-07T21:41:04Z",
        "body": "I think this is working now."
      }
    ]
  },
  {
    "number": 13196,
    "title": "X-Envoy-Expected-Rq-Timeout-Ms incorrect for retries",
    "created_at": "2020-09-20T04:57:26Z",
    "closed_at": "2021-12-07T13:56:00Z",
    "labels": [
      "bug",
      "help wanted",
      "area/http",
      "area/retry"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13196",
    "body": "I would expect the `X-Envoy-Expected-Rq-Timeout-Ms` header to reflect the remaining deadline \"budget\". Currently it seems to:\r\n- copy `x-envoy-upstream-rq-per-try-timeout-ms` when it's set and hedging is off\r\n- copy `x-envoy-upstream-rq-timeout-ms` when heding is on / no per-try timeout\r\n\r\nThis doesn't account for the reduced deadline, nor for the extra \"padding\" due to backoff.\r\n\r\n## Non-hedged example:\r\nRequest:\r\n```\r\nx-envoy-retry-on: 5xx\r\nx-envoy-max-retries: 2\r\nx-envoy-upstream-rq-timeout-ms: 250\r\nx-envoy-upstream-rq-per-try-timeout-ms: 100\r\n```\r\nCurrent behavior:\r\n```\r\nAttempt #1: X-Envoy-Expected-Rq-Timeout-Ms: 100\r\nAttempt #2: X-Envoy-Expected-Rq-Timeout-Ms: 100\r\nAttempt #3: X-Envoy-Expected-Rq-Timeout-Ms: 100\r\n```\r\nExpected behavior:\r\n```\r\nAttempt #1: X-Envoy-Expected-Rq-Timeout-Ms: 100\r\nAttempt #2: X-Envoy-Expected-Rq-Timeout-Ms: 100\r\nAttempt #3: X-Envoy-Expected-Rq-Timeout-Ms: 50\r\n```\r\n(not accounting for backoff)\r\n\r\n## Hedged example:\r\nRequest:\r\n```\r\nx-envoy-retry-on: 5xx\r\nx-envoy-max-retries: 2\r\nx-envoy-hedge-on-per-try-timeout: true\r\nx-envoy-upstream-rq-timeout-ms: 250\r\nx-envoy-upstream-rq-per-try-timeout-ms: 100\r\n```\r\nCurrent behavior:\r\n```\r\nAttempt #1: X-Envoy-Expected-Rq-Timeout-Ms: 250\r\nAttempt #2: X-Envoy-Expected-Rq-Timeout-Ms: 250\r\nAttempt #3: X-Envoy-Expected-Rq-Timeout-Ms: 250\r\n```\r\nExpected behavior:\r\n```\r\nAttempt #1: X-Envoy-Expected-Rq-Timeout-Ms: 250\r\nAttempt #2: X-Envoy-Expected-Rq-Timeout-Ms: 150\r\nAttempt #3: X-Envoy-Expected-Rq-Timeout-Ms: 50\r\n```\r\n(not accounting for backoff)",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13196/comments",
    "author": "ikonst",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-09-21T14:00:29Z",
        "body": "Agreed we should make this more accurate."
      }
    ]
  },
  {
    "number": 13166,
    "title": "Retrieving socket options in access loggers",
    "created_at": "2020-09-17T23:31:14Z",
    "closed_at": "2021-03-24T12:51:32Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13166",
    "body": "Problem we're trying to solve: extract downstream/upstream remote socket options for use in an access logger.\r\n\r\nI think we can do this for downstream via a custom listener filter that has access to the Socket and place this in dynamic metadata, which the access logger can then consume.\r\n\r\nI don't see a mechanism to do this upstream, upstream network filters don't have a way to reach into the Socket.\r\n\r\n1. Is this a correct assessment of the current situation?\r\n2. Would it be reasonable to provide upstream network filters the ability to peek at the Socket for getsockopt (possibly wrapped to limit interactions to only this method)?\r\n3. Is there a more principled way we could ask Envoy to collection a set of socket options and make them available for logging? I can imagine a config option to specify which socket options to collect, alongside which ones to set as we have today.\r\n\r\nCC @florincoras @mattklein123 @eziskind ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13166/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2020-09-18T16:53:51Z",
        "body": "It may depend on what socket options are of interest. If this comes from a specific request can we get a list of options?"
      },
      {
        "user": "eziskind",
        "created_at": "2020-09-18T17:38:58Z",
        "body": "This is for a Google-internal use-case where we want to get a custom socket option that has been set by the virtual networking layer."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-09-18T17:54:25Z",
        "body": "> Would it be reasonable to provide upstream network filters the ability to peek at the Socket for getsockopt (possibly wrapped to limit interactions to only this method)?\r\n\r\nThis seems OK to me at the network filter level, especially if it's only const access.\r\n\r\n> Is there a more principled way we could ask Envoy to collection a set of socket options and make them available for logging? I can imagine a config option to specify which socket options to collect, alongside which ones to set as we have today.\r\n\r\nI think we could do this, but I'm not sure it's worth it if we do the previous item, at leas for now."
      },
      {
        "user": "htuch",
        "created_at": "2020-09-21T14:54:38Z",
        "body": "We discussed offline and the preferred option is network filter access to socket options, assigned to @eziskind."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-09T03:11:29Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "htuch",
        "created_at": "2020-12-10T00:26:50Z",
        "body": "@eziskind are you planning on tackling this in the short term or should we park it as help wanted?"
      },
      {
        "user": "eziskind",
        "created_at": "2020-12-10T00:31:05Z",
        "body": "Yes, I'll probably get to this next quarter. You can mark it \"no stalebot\"?"
      }
    ]
  },
  {
    "number": 13114,
    "title": "CONNECT request with IP address as hostname fails with invalid_url",
    "created_at": "2020-09-15T20:45:43Z",
    "closed_at": "2020-11-10T21:13:41Z",
    "labels": [
      "bug",
      "help wanted",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13114",
    "body": "*Description*:\r\nIf the browser/client is configured to use an explicit HTTPS proxy and the destination is an IP address then envoy fails the connection with a \"400 Bad Request\" due to a \"invalid_url\" error.\r\n\r\n*Logs*:\r\n```\r\n[2020-09-15 20:37:09.669][1608][trace][http] [external/envoy/source/common/http/http1/codec_impl_legacy.cc:632] [C0] onHeadersCompleteBase\r\n[2020-09-15 20:37:09.669][1608][trace][http] [external/envoy/source/common/http/http1/codec_impl_legacy.cc:470] [C0] completed header: key=Host value=10.0.0.1:4443\r\n[2020-09-15 20:37:09.669][1608][trace][http] [external/envoy/source/common/http/http1/codec_impl_legacy.cc:676] [C0] codec entering upgrade mode for CONNECT request.\r\n[2020-09-15 20:37:09.669][1608][trace][http] [external/envoy/source/common/http/http1/codec_impl_legacy.cc:851] [C0] Server: onHeadersComplete size=4\r\n[2020-09-15 20:37:09.669][1608][debug][http] [external/envoy/source/common/http/filter_manager.cc:773] [C0][S6861130327348177454] Sending local reply with details http1.invalid_url\r\n[2020-09-15 20:37:09.669][1608][debug][http] [external/envoy/source/common/http/conn_manager_impl.cc:1255] [C0][S6861130327348177454] closing connection due to connection close header\r\n[2020-09-15 20:37:09.669][1608][debug][http] [external/envoy/source/common/http/conn_manager_impl.cc:1311] [C0][S6861130327348177454] encoding headers via codec (end_stream=false):\r\n':status', '400'\r\n'content-length', '11'\r\n'content-type', 'text/plain'\r\n'date', 'Tue, 15 Sep 2020 20:37:09 GMT'\r\n'connection', 'close'\r\n```\r\n\r\n*Detail*:\r\nI traced it to `Url::initialize` in url_utility.cc, which uses GURL.  The problem is that the GURL parser decodes the \"hostname:port\" string as a scheme (as also stated on url_utility.cc:65) and that GURL performs a sanity check on the \"scheme\" to make sure the first character is an alpha.  See IsSchemeFirstChar() in google URL url/url_canon_etc.cc:95.  In other words an IP address would fail the scheme sanity check.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13114/comments",
    "author": "roelfdutoit",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-09-16T23:38:37Z",
        "body": "Hmm this does seem like something we should fix somehow but not sure of the RFC details. cc @yanavlasov @dio @alyssawilk who have been looking at similar things recently."
      },
      {
        "user": "yanavlasov",
        "created_at": "2020-09-17T02:54:29Z",
        "body": "Yes, it looks like a bug. This code is going to be changed very soon. We will keep this bug open to make sure we look into this case. \r\n"
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-10-29T15:27:06Z",
        "body": "IPv4 was fixed a ways back.  IPv6 was fixed by #13798 so you should be good to go!\r\n"
      }
    ]
  },
  {
    "number": 12957,
    "title": "grpc_http1_reverse_bridge: support getting message length from upstream header",
    "created_at": "2020-09-02T20:12:42Z",
    "closed_at": "2021-08-11T03:47:39Z",
    "labels": [
      "help wanted",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12957",
    "body": "*Title*: *grpc_http1_reverse_bridge: support getting message length from upstream header*\r\n\r\n*Description*:\r\nRight now the gRPC reverse bridge filter can operate in 2 modes based on the withhold_grpc_frames config option. If it's false, then the upstream must prefix gRPC messages with the message-length frame header. If it's true, then the filter buffers the entire upstream response so it can count the length and add the prefix itself.\r\n\r\nI think it would be useful to have a 3rd option which would be to get the message length from an upstream header, which would allow the upstream to avoid having knowledge of the gRPC message format, but still let the filter stream rather than buffer.\r\n\r\nOne weird wrinkle is we may not want to use the usual `content-length` header, because the upstream may want to use a `transfer-encoding: chunked` still which means that the `content-length` header should not be set.\r\n\r\nProposal:\r\n\r\nAdd a string `response_size_header` option, which lets users specify a custom header to use to get the length in bytes of the response message. If this is set, ignore `withhold_grpc_frames`.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12957/comments",
    "author": "mpuncel",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-09-07T04:47:25Z",
        "body": "Sounds reasonable to me!"
      }
    ]
  },
  {
    "number": 12927,
    "title": "Different guard dogs for threads with different behavior",
    "created_at": "2020-09-01T20:29:08Z",
    "closed_at": "2020-09-17T17:44:23Z",
    "labels": [
      "help wanted",
      "area/watchdog"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12927",
    "body": "Different Guarddogs : a guard dog for worker threads and a guard dog for the main thread.\r\n\r\nThe main thread has different workloads than worker threads, this causes friction in systems that set thresholds for both of them as it’s “one-size fits all”.\r\n\r\nFor example, the main thread often spends a significant amount of time on parsing new config. That amount of time would be considered excessive for a worker thread to not have finished it’s event loop iteration and check in with its watchdog.\r\n\r\nThis “one size fits all” guard dog doesn’t work. Instead we need two guard dogs, one that’ll monitor the main thread and (other auxiliary threads)?, and another for the worker threads. This would allow us to better fine tune the thresholds depending on the guarddog (and what threads it watches) -- better allowing us to deploy watchdog capabilities such as watchdog actions, kill timeouts, etc.\r\n\r\nThe main trade off would be that it’d be harder to enact cross-guard dog policy that would trigger if we’d aggregate across guard dogs (such as multikill events), but won’t trigger because they aren’t sufficient within their individual guard dog to trigger that event.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12927/comments",
    "author": "KBaichoo",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2020-09-02T14:47:25Z",
        "body": "This seems reasonable to me, assuming we put this all behind configuration"
      },
      {
        "user": "KBaichoo",
        "created_at": "2020-09-02T18:30:25Z",
        "body": "/assign @KBaichoo "
      },
      {
        "user": "KBaichoo",
        "created_at": "2020-09-02T19:13:16Z",
        "body": "To implement this I propose the following:\r\n\r\nSteps:\r\n\r\n1. Encapsulate Watchdog Configuration in `Server::Configuration::Watchdog` instead of  `Server::Configuration::Main`, though we’ll still allow the Main configuration to have method(s) to return the watchdog config(s). This will look similar to the Admin interface, and they'll be a new class encapping watchdog info (timeouts, etc)\r\n    1. The method to return guarddog configs will likely look like `const Server::Configuration::Watchdog& getWatchdogConfig()` and an `std::optional<const Server::Configuration::Watchdog&> getWorkerWatchdogConfig()` for the additional guarddog that’ll watch only worker threads if both watchdog configs are specified.\r\n    2. Add a method on `Server::Configuration::Main` regarding whether we are using multiple guarddogs. \r\n2. Modify Guarddog ctor to take `Server::Configuration::Watchdog` instead of `Server::Configuration::Main`; this would simplify the interface since currently the guarddog just uses its specific config information from the main config and it avoids adding an additional param / mechanism for clarifying which `Server::Configuration::Watchdog` a guarddog need to boot up as its own.\r\n3. Add an additional watchdog in bootstrap.proto, if it is specified then we’ll have a separate guarddog for worker threads.\r\n4. Augment the server.cc to have a `unique_ptr` to another guarddog that’ll be focused only on worker threads if both are specified \r\n5. Modify `InstanceImpl::startWorkers()`  use the appropriate guarddog for watching the worker threads based off of information is `Server::Configuration::Main`.\r\n\r\nIf the additional watchdog isn’t specified in the `Bootstrap` then we’ll fall back to the default behavior of a single guarddog.\r\n\r\n\r\nThought @antoniovicente @envoyproxy/api-shepherds ? Thanks"
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-09-03T02:08:13Z",
        "body": "My thoughts about API:\r\n\r\nLike you mention in item (3) above, we need a Watchdog config message in bootstrap.proto for each type.  One possible option is the one you suggest of having a second field for the worker watchdog parameters.  Some alternate options include having the new parameter specify the auxiliary thread watchdog parameters instead.  Or add 2 new parameters(one for workers, one for auxiliary) and slowly deprecate the current parameter.\r\n\r\nAnother option would be to change the type of the watchdog config field in bootstrap.proto to be a repeated field and add some type enum to the Watchdog message to specify if it applies to workers, main thread, or is the default set of parameters for threads that don't have a more specific watchdog type.  A possible problem with this last suggestion is that Envoy coding convention does not allow changing type of proto fields from optional to repeated because it breaks compat with certain config sources.  Also, the naming convention for repeated fields is plural.\r\n\r\nIt seems fine for multi-kill to only pay attention to a specific type of thread like worker threads.  Multi-kill may not make as much sense for auxiliary threads."
      },
      {
        "user": "KBaichoo",
        "created_at": "2020-09-03T14:24:39Z",
        "body": "Thanks for the input @antoniovicente.\r\n\r\nI believe we only create WDs right now for workers, and for the main thread. Perhaps we'd want to in the future create it for other threads with similar workloads (say async io threads)?\r\n\r\n> One possible option is the one you suggest of having a second field for the worker watchdog parameters.\r\n\r\nWe'd be able to keep the existing field (breaking downstream a little less), but it would have additional cognitive load as we'd have a single knob that changes in complex ways depending on the value of another knob. \r\n\r\n> Some alternate options include having the new parameter specify the auxiliary thread watchdog parameters instead. \r\n\r\nThe main drawback with this approach (IIUC the suggestion), is that we'd have an explosion of parameters for every new field we'd need an additional one for the auxiliary.\r\n \r\n> Or add 2 new parameters(one for workers, one for auxiliary) and slowly deprecate the current parameter.\r\n\r\nThis would likely be cleaner from a code hygiene perspective, but it won't be future proof to say adding a guarddog for threads with another set of behaviors (say logger threads). That is to say, often times when we want 2 of some X, it sometimes becomes a multiple > 2 in the future.\r\n\r\n> Another option would be to change the type of the watchdog config field in bootstrap.proto to be a repeated field and add some type enum to the Watchdog message to specify if it applies to workers, main thread, or is the default set of parameters for threads that don't have a more specific watchdog type. A possible problem with this last suggestion is that Envoy coding convention does not allow changing type of proto fields from optional to repeated because it breaks compat with certain config sources. Also, the naming convention for repeated fields is plural.\r\n\r\nThis generalizes from the 2 new parameter solution, but we run into issues from coding conventions (I believe changing the name doesn't matter to the protos as the tag is what matters). \r\n\r\nSince we can't retrofit the old field, we could just add a new field `watchdogs` and have an enum for configs for different watchdog types; this would also simplify the `Server::Configuration::Main` interface to have a single function `std::optional<WatchdogConfig> GetWatchdogConfig(WatchdogTypeEnum)`.\r\n\r\nA drawback to this is configuration validation could get messy (for example users could have multiple configs for the same watchdog type provided), but we could just take the first one and warn them about their misconfiguration.\r\n\r\n"
      },
      {
        "user": "htuch",
        "created_at": "2020-09-03T20:13:48Z",
        "body": "I'd avoid enum, I think the cleanest here is to add a `MultiWatchdog multi_watchdog = N;` to bootstrap and a new message with:\r\n```\r\nmessage MultiWatchdog {\r\n  Watchdog aux_watchdog = 1;\r\n  Watchdog worker_watchdog = 2;\r\n  ...\r\n}\r\n```\r\nThis way we can deprecate the existing `watchdog` field. This is similar to what we did with `Runtime` when introducing explicit layer control."
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-09-03T21:12:01Z",
        "body": "Adding a new message to group together the multiple watchdog configs seems like a fine plan.  Is that the preferred option?  If yes, let's move on to implementation."
      },
      {
        "user": "KBaichoo",
        "created_at": "2020-09-03T21:15:43Z",
        "body": "Yea upon implementation that seemed to be cleaner than repeated field. I've started implementation with the new message. "
      }
    ]
  },
  {
    "number": 12923,
    "title": "QUIC/HTTP3: docs",
    "created_at": "2020-09-01T18:59:14Z",
    "closed_at": "2021-05-07T12:28:09Z",
    "labels": [
      "area/docs",
      "help wanted",
      "area/quic",
      "quic-mvp",
      "quic-alpha"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12923",
    "body": "Arch overview docs, config docs, etc.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12923/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-09-01T18:59:29Z",
        "body": "cc @ggreenway @danzh2010 @moderation "
      },
      {
        "user": "ggreenway",
        "created_at": "2020-09-15T20:53:06Z",
        "body": "Also document runtime flag `envoy.reloadable_features.prefer_quic_kernel_bpf_packet_routing`, added in #13086 "
      },
      {
        "user": "danzh2010",
        "created_at": "2020-11-03T23:09:26Z",
        "body": "/assign @danzh2010 "
      }
    ]
  },
  {
    "number": 12899,
    "title": "network tls: alternative to using socket BIOs in SslSocket",
    "created_at": "2020-08-31T18:51:11Z",
    "closed_at": "2020-09-05T00:36:52Z",
    "labels": [
      "help wanted",
      "area/tls"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12899",
    "body": "TLS sockets are currently using BIOs that rely on access to a socket file descriptor, namely:\r\n\r\n```\r\nBIO* bio = BIO_new_socket(callbacks_->ioHandle().fdDoNotUse(), 0);\r\n```\r\n\r\nTo avoid direct use of the fd and to ensure that `SslSocket`s can work with any type of `IoHandle`s, probably mem BIOs should be used instead. I am willing to do the work but does anybody know if this comes with any significant downsides? Are there any better alternatives?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12899/comments",
    "author": "florincoras",
    "comments": [
      {
        "user": "florincoras",
        "created_at": "2020-08-31T18:52:04Z",
        "body": "cc @mattklein123 @antoniovicente "
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-08-31T19:06:29Z",
        "body": "One possible option would be to add a method to IoHandle that creates an SSL bio that is appropriate for the IoHandle in question.  It does pollute the IoHandle interface in some ways.  We should keep in mind that IoHandle::readv and writev should not be used on IoHandles used by SSL libraries."
      },
      {
        "user": "florincoras",
        "created_at": "2020-08-31T19:25:22Z",
        "body": "Interesting idea! Personally, I like the option of being able to abstract any type of transport with IoHandles, i.e., being able to readv/writev to an IoHandle that internally implements TLS/QUIC seems good to me. But, agreed, that could lead to a significant amount of api pollution. "
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-08-31T19:27:24Z",
        "body": "Writing a BIO that calls into IoHandle for peek/read/write operations may also be an option."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-09-01T00:06:22Z",
        "body": "cc @ggreenway @lizan @PiotrSikora @davidben for thoughts.\r\n\r\nI don't know about the available options. Some type of shim over an IOHandle would be great if it's possible to implement a custom one as @antoniovicente suggests."
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-09-01T00:18:06Z",
        "body": "See:\r\nBIO_METHOD acts like a vtable, allows you to specify read/write/etc functions\r\nBIO *BIO_new(const BIO_METHOD *method);\r\n\r\nBIO::ptr would point to the IoHandle*"
      },
      {
        "user": "florincoras",
        "created_at": "2020-09-01T00:27:08Z",
        "body": "Looking at what @antoniovicente suggested! "
      },
      {
        "user": "florincoras",
        "created_at": "2020-09-01T03:18:22Z",
        "body": "Surprisingly enough, something as simple as #12911 seems to be working."
      }
    ]
  },
  {
    "number": 12861,
    "title": "Test100AndDisconnectLegacy flake (ARM release)",
    "created_at": "2020-08-27T22:37:00Z",
    "closed_at": "2020-09-03T22:20:20Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12861",
    "body": "```\r\n[ RUN      ] Protocols/DownstreamProtocolIntegrationTest.Test100AndDisconnectLegacy/IPv4_HttpDownstream_HttpUpstream\r\n[2020-08-27 21:11:58.717][30877][critical][assert] [test/integration/http_integration.cc:317] assert failure: result. Details: The connection disconnected unexpectedly, and allow_unexpected_disconnects_ is false.\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12861/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-08-27T22:37:20Z",
        "body": "cc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-08-31T19:50:54Z",
        "body": "I don't recall this assert having caught anything useful  How would you feel if we just allowed unexpected disconnects?"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-08-31T20:18:38Z",
        "body": "> I don't recall this assert having caught anything useful How would you feel if we just allowed unexpected disconnects?\r\n\r\nBig +1"
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-09-01T12:06:31Z",
        "body": "Excellent.  I'll pick that up Wednesday"
      }
    ]
  },
  {
    "number": 12666,
    "title": "ext_authz: Support filter naming and per-filter statistics",
    "created_at": "2020-08-15T15:47:43Z",
    "closed_at": "2020-09-30T22:03:47Z",
    "labels": [
      "help wanted",
      "area/ext_authz"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12666",
    "body": "*Title*: Support filter naming and per-filter statistics\r\n\r\n*Description*:\r\nMy HTTP filter chain contains multiple instances of `ext_authz`, each fulfilling a completely different task.\r\n\r\nMy real-life use case:\r\n```yaml\r\nhttp_filters:\r\n  # Filter #1 - WAF sidecar\r\n  - name: envoy.filters.http.ext_authz\r\n    config: {...}\r\n\r\n  # Filter #2 - request authorization service\r\n  - name: envoy.filters.http.ext_authz\r\n    config: {...}\r\n```\r\n\r\nIn this case, I can access relevant statistics about the External Authorization executions in general, but I cannot distinguish between the two external services, and most importantly I cannot measure the allow/deny rates of each of the filters.\r\n\r\nMy suggestion is to have the option of **naming each of the filters**, and having that name **added as a dimension** to the statistics produced by the filter, for example:\r\n\r\n```yaml\r\nhttp_filters:\r\n  # Filter #1 - WAF sidecar\r\n  - name: envoy.filters.http.ext_authz\r\n    config:\r\n      filter_stat_name: waf\r\n      ...\r\n\r\n  # Filter #2 - request authorization service\r\n  - name: envoy.filters.http.ext_authz\r\n    config:\r\n      filter_stat_name: authorizer\r\n      ...\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12666/comments",
    "author": "danielmittelman",
    "comments": [
      {
        "user": "dio",
        "created_at": "2020-08-15T16:17:54Z",
        "body": "I think this is a reasonable ask. And i believe saw some TODOs related to this."
      }
    ]
  },
  {
    "number": 12532,
    "title": "TSAN DelayActivation/TimerImplTest.ChangeTimerForwardsToNonZeroBeforeRun/0 flake",
    "created_at": "2020-08-07T04:59:19Z",
    "closed_at": "2020-08-24T20:44:59Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12532",
    "body": "```\r\n[ RUN      ] DelayActivation/TimerImplTest.ChangeTimerForwardsToNonZeroBeforeRun/0\r\nunknown file: Failure\r\n\r\nUnexpected mock function call - returning directly.\r\n    Function call: ready()\r\nGoogle Mock tried the following 1 expectation, but it didn't match:\r\n\r\ntest/common/event/dispatcher_impl_test.cc:613: EXPECT_CALL(watcher2, ready())...\r\n         Expected: all pre-requisites are satisfied\r\n           Actual: the following immediate pre-requisites are not satisfied:\r\ntest/common/event/dispatcher_impl_test.cc:612: pre-requisite #0\r\n                   (end of pre-requisites)\r\n         Expected: to be called once\r\n           Actual: never called - unsatisfied and active\r\n```\r\n\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\nExecuting tests from //test/common/event:dispatcher_impl_test\r\n-----------------------------------------------------------------------------\r\n[==========] Running 44 tests from 9 test suites.\r\n[----------] Global test environment set-up.\r\n[----------] 4 tests from SchedulableCallbackImplTest\r\n[ RUN      ] SchedulableCallbackImplTest.ScheduleCurrentAndCancel\r\n[       OK ] SchedulableCallbackImplTest.ScheduleCurrentAndCancel (1 ms)\r\n[ RUN      ] SchedulableCallbackImplTest.ScheduleNextAndCancel\r\n[       OK ] SchedulableCallbackImplTest.ScheduleNextAndCancel (1 ms)\r\n[ RUN      ] SchedulableCallbackImplTest.ScheduleOrder\r\n[       OK ] SchedulableCallbackImplTest.ScheduleOrder (1 ms)\r\n[ RUN      ] SchedulableCallbackImplTest.ScheduleChainingAndCancellation\r\n[       OK ] SchedulableCallbackImplTest.ScheduleChainingAndCancellation (2 ms)\r\n[----------] 4 tests from SchedulableCallbackImplTest (5 ms total)\r\n\r\n[----------] 1 test from DeferredDeleteTest\r\n[ RUN      ] DeferredDeleteTest.DeferredDelete\r\n[       OK ] DeferredDeleteTest.DeferredDelete (1 ms)\r\n[----------] 1 test from DeferredDeleteTest (1 ms total)\r\n\r\n[----------] 1 test from DeferredTaskTest\r\n[ RUN      ] DeferredTaskTest.DeferredTask\r\n[       OK ] DeferredTaskTest.DeferredTask (1 ms)\r\n[----------] 1 test from DeferredTaskTest (1 ms total)\r\n\r\n[----------] 6 tests from DispatcherImplTest\r\n[ RUN      ] DispatcherImplTest.InitializeStats\r\n[       OK ] DispatcherImplTest.InitializeStats (5 ms)\r\n[ RUN      ] DispatcherImplTest.Post\r\n[       OK ] DispatcherImplTest.Post (2 ms)\r\n[ RUN      ] DispatcherImplTest.RunPostCallbacksLocking\r\n[       OK ] DispatcherImplTest.RunPostCallbacksLocking (3 ms)\r\n[ RUN      ] DispatcherImplTest.Timer\r\n[       OK ] DispatcherImplTest.Timer (3 ms)\r\n[ RUN      ] DispatcherImplTest.TimerWithScope\r\n[       OK ] DispatcherImplTest.TimerWithScope (53 ms)\r\n[ RUN      ] DispatcherImplTest.IsThreadSafe\r\n[       OK ] DispatcherImplTest.IsThreadSafe (3 ms)\r\n[----------] 6 tests from DispatcherImplTest (70 ms total)\r\n\r\n[----------] 1 test from NotStartedDispatcherImplTest\r\n[ RUN      ] NotStartedDispatcherImplTest.IsThreadSafe\r\n[       OK ] NotStartedDispatcherImplTest.IsThreadSafe (0 ms)\r\n[----------] 1 test from NotStartedDispatcherImplTest (1 ms total)\r\n\r\n[----------] 2 tests from DispatcherMonotonicTimeTest\r\n[ RUN      ] DispatcherMonotonicTimeTest.UpdateApproximateMonotonicTime\r\n[       OK ] DispatcherMonotonicTimeTest.UpdateApproximateMonotonicTime (0 ms)\r\n[ RUN      ] DispatcherMonotonicTimeTest.ApproximateMonotonicTime\r\n[       OK ] DispatcherMonotonicTimeTest.ApproximateMonotonicTime (1 ms)\r\n[----------] 2 tests from DispatcherMonotonicTimeTest (1 ms total)\r\n\r\n[----------] 1 test from TimerImplTimingTest\r\n[ RUN      ] TimerImplTimingTest.TheoreticalTimerTiming\r\nTestRandomGenerator running with seed 67947800\r\n[       OK ] TimerImplTimingTest.TheoreticalTimerTiming (32613 ms)\r\n[----------] 1 test from TimerImplTimingTest (32613 ms total)\r\n\r\n[----------] 2 tests from TimerUtilsTest\r\n[ RUN      ] TimerUtilsTest.TimerNegativeValueThrows\r\n[       OK ] TimerUtilsTest.TimerNegativeValueThrows (0 ms)\r\n[ RUN      ] TimerUtilsTest.TimerValueConversion\r\n[       OK ] TimerUtilsTest.TimerValueConversion (0 ms)\r\n[----------] 2 tests from TimerUtilsTest (0 ms total)\r\n\r\n[----------] 26 tests from DelayActivation/TimerImplTest\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerEnabledDisabled/0\r\n[       OK ] DelayActivation/TimerImplTest.TimerEnabledDisabled/0 (11 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerEnabledDisabled/1\r\n[       OK ] DelayActivation/TimerImplTest.TimerEnabledDisabled/1 (8 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.ChangeTimerBackwardsBeforeRun/0\r\n[       OK ] DelayActivation/TimerImplTest.ChangeTimerBackwardsBeforeRun/0 (14 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.ChangeTimerBackwardsBeforeRun/1\r\n[       OK ] DelayActivation/TimerImplTest.ChangeTimerBackwardsBeforeRun/1 (15 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.ChangeTimerForwardsToZeroBeforeRun/0\r\n[       OK ] DelayActivation/TimerImplTest.ChangeTimerForwardsToZeroBeforeRun/0 (15 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.ChangeTimerForwardsToZeroBeforeRun/1\r\n[       OK ] DelayActivation/TimerImplTest.ChangeTimerForwardsToZeroBeforeRun/1 (14 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.ChangeTimerForwardsToNonZeroBeforeRun/0\r\nunknown file: Failure\r\n\r\nUnexpected mock function call - returning directly.\r\n    Function call: ready()\r\nGoogle Mock tried the following 1 expectation, but it didn't match:\r\n\r\ntest/common/event/dispatcher_impl_test.cc:613: EXPECT_CALL(watcher2, ready())...\r\n         Expected: all pre-requisites are satisfied\r\n           Actual: the following immediate pre-requisites are not satisfied:\r\ntest/common/event/dispatcher_impl_test.cc:612: pre-requisite #0\r\n                   (end of pre-requisites)\r\n         Expected: to be called once\r\n           Actual: never called - unsatisfied and active\r\nStack trace:\r\n  0x3d46b45: testing::internal::GoogleTestFailureReporter::ReportFailure()\r\n  0x152d158: testing::internal::Expect()\r\n  0x3d497cf: testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()\r\n  0x1485ce8: testing::internal::FunctionMocker<>::Invoke()\r\n  0x1485c68: Envoy::ReadyWatcher::ready()\r\n  0x141ff7c: Envoy::Event::(anonymous namespace)::TimerImplTest_ChangeTimerForwardsToNonZeroBeforeRun_Test::TestBody()::$_35::operator()()\r\n  0x141fed1: std::__1::__invoke<>()\r\n  0x141fe31: std::__1::__invoke_void_return_wrapper<>::__call<>()\r\n  0x141fdd1: std::__1::__function::__alloc_func<>::operator()()\r\n  0x141e0a0: std::__1::__function::__func<>::operator()()\r\n  0x1491777: std::__1::__function::__value_func<>::operator()()\r\n  0x14916c9: std::__1::function<>::operator()()\r\n  0x1cd6906: Envoy::Event::TimerImpl::TimerImpl()::$_0::operator()()\r\n  0x1cd6867: Envoy::Event::TimerImpl::TimerImpl()::$_0::__invoke()\r\n  0x3c13639: event_process_active_single_queue\r\n  0x3c07ee8: event_process_active\r\n  0x3c058a0: event_base_loop\r\n  0x1cd2350: Envoy::Event::LibeventScheduler::run()\r\n  0x1926002: Envoy::Event::DispatcherImpl::run()\r\n  0x141ad04: Envoy::Event::(anonymous namespace)::TimerImplTest_ChangeTimerForwardsToNonZeroBeforeRun_Test::TestBody()\r\n  0x3dbc11d: testing::internal::HandleSehExceptionsInMethodIfSupported<>()\r\n  0x3d9ffbf: testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n  0x3d84c52: testing::Test::Run()\r\n  0x3d85af4: testing::TestInfo::Run()\r\n... Google Test internal frames ...\r\n\r\ntest/common/event/dispatcher_impl_test.cc:613: Failure\r\nActual function call count doesn't match EXPECT_CALL(watcher2, ready())...\r\n         Expected: to be called once\r\n           Actual: never called - unsatisfied and active\r\nStack trace:\r\n  0x3d46b45: testing::internal::GoogleTestFailureReporter::ReportFailure()\r\n  0x152d158: testing::internal::Expect()\r\n  0x3d4a02d: testing::internal::UntypedFunctionMockerBase::VerifyAndClearExpectationsLocked()\r\n  0x15c696b: testing::internal::FunctionMocker<>::~FunctionMocker()\r\n  0x1cbe489: Envoy::ReadyWatcher::~ReadyWatcher()\r\n  0x141ad2d: Envoy::Event::(anonymous namespace)::TimerImplTest_ChangeTimerForwardsToNonZeroBeforeRun_Test::TestBody()\r\n  0x3dbc11d: testing::internal::HandleSehExceptionsInMethodIfSupported<>()\r\n  0x3d9ffbf: testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n  0x3d84c52: testing::Test::Run()\r\n  0x3d85af4: testing::TestInfo::Run()\r\n... Google Test internal frames ...\r\n\r\n[  FAILED  ] DelayActivation/TimerImplTest.ChangeTimerForwardsToNonZeroBeforeRun/0, where GetParam() = false (1239 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.ChangeTimerForwardsToNonZeroBeforeRun/1\r\n[       OK ] DelayActivation/TimerImplTest.ChangeTimerForwardsToNonZeroBeforeRun/1 (14 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.ChangeLargeTimerForwardToZeroBeforeRun/0\r\n[       OK ] DelayActivation/TimerImplTest.ChangeLargeTimerForwardToZeroBeforeRun/0 (8 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.ChangeLargeTimerForwardToZeroBeforeRun/1\r\n[       OK ] DelayActivation/TimerImplTest.ChangeLargeTimerForwardToZeroBeforeRun/1 (8 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.ChangeLargeTimerForwardToNonZeroBeforeRun/0\r\n[       OK ] DelayActivation/TimerImplTest.ChangeLargeTimerForwardToNonZeroBeforeRun/0 (15 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.ChangeLargeTimerForwardToNonZeroBeforeRun/1\r\n[       OK ] DelayActivation/TimerImplTest.ChangeLargeTimerForwardToNonZeroBeforeRun/1 (15 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerOrdering/0\r\n[       OK ] DelayActivation/TimerImplTest.TimerOrdering/0 (15 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerOrdering/1\r\n[       OK ] DelayActivation/TimerImplTest.TimerOrdering/1 (15 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerOrderAndDisableAlarm/0\r\n[       OK ] DelayActivation/TimerImplTest.TimerOrderAndDisableAlarm/0 (16 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerOrderAndDisableAlarm/1\r\n[       OK ] DelayActivation/TimerImplTest.TimerOrderAndDisableAlarm/1 (14 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerOrderDisableAndReschedule/0\r\n[       OK ] DelayActivation/TimerImplTest.TimerOrderDisableAndReschedule/0 (27 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerOrderDisableAndReschedule/1\r\n[       OK ] DelayActivation/TimerImplTest.TimerOrderDisableAndReschedule/1 (26 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerOrderAndReschedule/0\r\n[       OK ] DelayActivation/TimerImplTest.TimerOrderAndReschedule/0 (22 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerOrderAndReschedule/1\r\n[       OK ] DelayActivation/TimerImplTest.TimerOrderAndReschedule/1 (26 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerChaining/0\r\n[       OK ] DelayActivation/TimerImplTest.TimerChaining/0 (11 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerChaining/1\r\n[       OK ] DelayActivation/TimerImplTest.TimerChaining/1 (10 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerChainDisable/0\r\n[       OK ] DelayActivation/TimerImplTest.TimerChainDisable/0 (12 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerChainDisable/1\r\n[       OK ] DelayActivation/TimerImplTest.TimerChainDisable/1 (9 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerChainDelete/0\r\n[       OK ] DelayActivation/TimerImplTest.TimerChainDelete/0 (10 ms)\r\n[ RUN      ] DelayActivation/TimerImplTest.TimerChainDelete/1\r\n[       OK ] DelayActivation/TimerImplTest.TimerChainDelete/1 (9 ms)\r\n[----------] 26 tests from DelayActivation/TimerImplTest (1599 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 44 tests from 9 test suites ran. (34292 ms total)\r\n[  PASSED  ] 43 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] DelayActivation/TimerImplTest.ChangeTimerForwardsToNonZeroBeforeRun/0, where GetParam() = false\r\n\r\n 1 FAILED TEST\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12532/comments",
    "author": "dio",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-08-07T16:24:50Z",
        "body": "cc @antoniovicente "
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-08-10T16:23:30Z",
        "body": "/assign @antoniovicente \r\n\r\nI can reproduce misc flakiness in TimerImplTest across multiple test cases.  I don't understand the failures yet, but will try to get to the bottom of it as soon as I can."
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-08-11T16:35:29Z",
        "body": "I should add: this is not a TSAN issue.  Flakiness happens outside TSAN.  At least 4 different test cases are affected, ChangeTimerForwardsToZeroBeforeRun just happens to be the one that is most likely to flake.  I don't fully understand the flakiness yet or how to mitigate it.  It may make sense to disable some of these very picky and relatively unessential tests to avoid presubmit problems."
      }
    ]
  },
  {
    "number": 12430,
    "title": "Proposal: Introduce host_rewrite_path support in route config",
    "created_at": "2020-08-02T15:58:05Z",
    "closed_at": "2020-08-11T12:30:16Z",
    "labels": [
      "design proposal",
      "help wanted",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12430",
    "body": "**Rational**\r\n\r\nUsing subdomains for tenant-id in a multi-tenant application is a relatively popular strategy, our own MediaWiki (the software behind Wikipedia) uses `Host` header to distinguish between different language wikis. Ability to rewrite `Host` based on path would provide flexibility in moving the tenant-id from subdomain to path when deploying existing software behind the proxy.\r\n\r\n**Proposal**\r\n\r\nIntroduce a new `host_rewrite_path` option in `RouteAction:host_rewrite_specifier`. \r\n\r\n```\r\n// Indicates that during forwarding, the host header will be swapped with the result of the\r\n// given regex substitution run agains the request path. \r\ntype.matcher.v3.RegexMatchAndSubstitute host_rewrite_path = 36;\r\n```\r\n\r\nThe behavior of the new property would be the following: `Host` header will be set to the result of the `RegexMatchAndSubstitute`, executed agains the path. However, this would need to be applied with care, since the regex matcher needs to be guaranteed to match the whole path string. Alternatively, a new `RegexMatchAndReplace` message could introduced, where the result would replace the input string with capture groups applied, or null if the regex did not match the input - in that case the host header would not be swapped if the regex did not match.\r\n\r\nWe are going to provide implementation and testing. Seeking approval from the maintainers and guidance on whether to reuse `RegexMatchAndSubstitute` or introduce a new message type.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12430/comments",
    "author": "Pchelolo",
    "comments": [
      {
        "user": "dio",
        "created_at": "2020-08-02T22:25:47Z",
        "body": "cc. @snowp "
      },
      {
        "user": "mattklein123",
        "created_at": "2020-08-03T16:23:22Z",
        "body": "At a high level this feature makes sense to me. \r\n\r\n> However, this would need to be applied with care, since the regex matcher needs to be guaranteed to match the whole path string. Alternatively, a new RegexMatchAndReplace message could introduced, where the result would replace the input string with capture groups applied, or null if the regex did not match the input - in that case the host header would not be swapped if the regex did not match.\r\n\r\nI'm a little hazy on this part. Can you explain in more detail? Could adding additional parameters to `RegexMatchAndSubstitute` also help change the behavior in a way that would work for you?"
      },
      {
        "user": "Pchelolo",
        "created_at": "2020-08-03T16:28:36Z",
        "body": "> I'm a little hazy on this part. Can you explain in more detail? Could adding additional parameters to RegexMatchAndSubstitute also help change the behavior in a way that would work for you?\r\n\r\nOn a second thought, `RegexMatchAndSubstitute` would work just fine as is. I was being too worried about missconfiguring this, but introducing new flags wouldn't really make it any easier to configure. I'll implement a version with just `RegexMatchAndSubstitute`. "
      }
    ]
  },
  {
    "number": 12391,
    "title": "TimerImplTest flake",
    "created_at": "2020-07-30T22:49:51Z",
    "closed_at": "2020-08-03T23:13:21Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12391",
    "body": "```\r\n2020-07-30T22:30:54.3352783Z [ RUN      ] TimerImplTest.TimerOrderDisableAndReschedule\r\n2020-07-30T22:30:54.3353357Z unknown file: Failure\r\n2020-07-30T22:30:54.3353635Z \r\n2020-07-30T22:30:54.3377861Z Unexpected mock function call - returning directly.\r\n2020-07-30T22:30:54.3384978Z     Function call: ready()\r\n2020-07-30T22:30:54.3385946Z Google Mock tried the following 1 expectation, but it didn't match:\r\n2020-07-30T22:30:54.3386381Z \r\n2020-07-30T22:30:54.3386836Z test/common/event/dispatcher_impl_test.cc:600: EXPECT_CALL(watcher2, ready())...\r\n2020-07-30T22:30:54.3387585Z          Expected: all pre-requisites are satisfied\r\n2020-07-30T22:30:54.3388389Z            Actual: the following immediate pre-requisites are not satisfied:\r\n2020-07-30T22:30:54.3389223Z test/common/event/dispatcher_impl_test.cc:599: pre-requisite #0\r\n2020-07-30T22:30:54.3389915Z                    (end of pre-requisites)\r\n2020-07-30T22:30:54.3390356Z          Expected: to be called once\r\n2020-07-30T22:30:54.3391027Z            Actual: never called - unsatisfied and active\r\n2020-07-30T22:30:54.3391479Z Stack trace:\r\n2020-07-30T22:30:54.3391979Z   0x563b6bbd77f9: testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()\r\n2020-07-30T22:30:54.3392580Z   0x563b6b41d42d: std::_Function_handler<>::_M_invoke()\r\n2020-07-30T22:30:54.3393107Z   0x563b6bba8156: event_process_active_single_queue.isra.0\r\n2020-07-30T22:30:54.3393572Z   0x563b6bba87bf: event_base_loop\r\n2020-07-30T22:30:54.3394196Z   0x563b6b438674: Envoy::Event::(anonymous namespace)::TimerImplTest_TimerOrderDisableAndReschedule_Test::TestBody()\r\n2020-07-30T22:30:54.3395071Z   0x563b6bbcea40: testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n2020-07-30T22:30:54.3395608Z   0x563b6bbcec58: testing::Test::Run()\r\n2020-07-30T22:30:54.3396073Z   0x563b6bbcef39: testing::TestInfo::Run()\r\n2020-07-30T22:30:54.3396548Z   0x563b6bbcf1a5: testing::TestSuite::Run()\r\n2020-07-30T22:30:54.3396991Z ... Google Test internal frames ...\r\n2020-07-30T22:30:54.3397283Z \r\n2020-07-30T22:30:54.3397617Z unknown file: Failure\r\n2020-07-30T22:30:54.3397894Z \r\n2020-07-30T22:30:54.3416273Z Unexpected mock function call - returning directly.\r\n2020-07-30T22:30:54.3416985Z     Function call: ready()\r\n2020-07-30T22:30:54.3417822Z Google Mock tried the following 1 expectation, but it didn't match:\r\n2020-07-30T22:30:54.3418258Z \r\n2020-07-30T22:30:54.3419736Z test/common/event/dispatcher_impl_test.cc:601: EXPECT_CALL(watcher3, ready())...\r\n2020-07-30T22:30:54.3420595Z          Expected: all pre-requisites are satisfied\r\n2020-07-30T22:30:54.3421399Z            Actual: the following immediate pre-requisites are not satisfied:\r\n2020-07-30T22:30:54.3422232Z test/common/event/dispatcher_impl_test.cc:600: pre-requisite #0\r\n2020-07-30T22:30:54.3423108Z                    (end of pre-requisites)\r\n2020-07-30T22:30:54.3423565Z          Expected: to be called once\r\n2020-07-30T22:30:54.3424231Z            Actual: never called - unsatisfied and active\r\n2020-07-30T22:30:54.3424675Z Stack trace:\r\n2020-07-30T22:30:54.3425159Z   0x563b6bbd77f9: testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()\r\n2020-07-30T22:30:54.3425761Z   0x563b6b41ceed: std::_Function_handler<>::_M_invoke()\r\n2020-07-30T22:30:54.3426300Z   0x563b6bba8156: event_process_active_single_queue.isra.0\r\n2020-07-30T22:30:54.3426767Z   0x563b6bba87bf: event_base_loop\r\n2020-07-30T22:30:54.3427583Z   0x563b6b438674: Envoy::Event::(anonymous namespace)::TimerImplTest_TimerOrderDisableAndReschedule_Test::TestBody()\r\n2020-07-30T22:30:54.3428338Z   0x563b6bbcea40: testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n2020-07-30T22:30:54.3428872Z   0x563b6bbcec58: testing::Test::Run()\r\n2020-07-30T22:30:54.3429341Z   0x563b6bbcef39: testing::TestInfo::Run()\r\n2020-07-30T22:30:54.3429801Z   0x563b6bbcf1a5: testing::TestSuite::Run()\r\n2020-07-30T22:30:54.3430241Z ... Google Test internal frames ...\r\n2020-07-30T22:30:54.3430544Z \r\n2020-07-30T22:30:54.3430924Z test/common/event/dispatcher_impl_test.cc:600: Failure\r\n2020-07-30T22:30:54.3431695Z Actual function call count doesn't match EXPECT_CALL(watcher2, ready())...\r\n2020-07-30T22:30:54.3432212Z          Expected: to be called once\r\n2020-07-30T22:30:54.3432865Z            Actual: never called - unsatisfied and active\r\n2020-07-30T22:30:54.3433468Z Stack trace:\r\n2020-07-30T22:30:54.3433991Z   0x563b6bbd3b68: testing::internal::UntypedFunctionMockerBase::VerifyAndClearExpectationsLocked()\r\n2020-07-30T22:30:54.3434614Z   0x563b6b5405e5: Envoy::ReadyWatcher::~ReadyWatcher()\r\n2020-07-30T22:30:54.3435266Z   0x563b6b4386b0: Envoy::Event::(anonymous namespace)::TimerImplTest_TimerOrderDisableAndReschedule_Test::TestBody()\r\n2020-07-30T22:30:54.3435985Z   0x563b6bbcea40: testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n2020-07-30T22:30:54.3436520Z   0x563b6bbcec58: testing::Test::Run()\r\n2020-07-30T22:30:54.3436985Z   0x563b6bbcef39: testing::TestInfo::Run()\r\n2020-07-30T22:30:54.3437445Z   0x563b6bbcf1a5: testing::TestSuite::Run()\r\n2020-07-30T22:30:54.3437887Z ... Google Test internal frames ...\r\n2020-07-30T22:30:54.3438189Z \r\n2020-07-30T22:30:54.3438568Z test/common/event/dispatcher_impl_test.cc:601: Failure\r\n2020-07-30T22:30:54.3439327Z Actual function call count doesn't match EXPECT_CALL(watcher3, ready())...\r\n2020-07-30T22:30:54.3439874Z          Expected: to be called once\r\n2020-07-30T22:30:54.3440546Z            Actual: never called - unsatisfied and active\r\n2020-07-30T22:30:54.3440976Z Stack trace:\r\n2020-07-30T22:30:54.3441495Z   0x563b6bbd3b68: testing::internal::UntypedFunctionMockerBase::VerifyAndClearExpectationsLocked()\r\n2020-07-30T22:30:54.3442111Z   0x563b6b5405e5: Envoy::ReadyWatcher::~ReadyWatcher()\r\n2020-07-30T22:30:54.3442874Z   0x563b6b4386ce: Envoy::Event::(anonymous namespace)::TimerImplTest_TimerOrderDisableAndReschedule_Test::TestBody()\r\n2020-07-30T22:30:54.3443585Z   0x563b6bbcea40: testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n2020-07-30T22:30:54.3444112Z   0x563b6bbcec58: testing::Test::Run()\r\n2020-07-30T22:30:54.3444575Z   0x563b6bbcef39: testing::TestInfo::Run()\r\n2020-07-30T22:30:54.3445048Z   0x563b6bbcf1a5: testing::TestSuite::Run()\r\n2020-07-30T22:30:54.3445470Z ... Google Test internal frames ...\r\n2020-07-30T22:30:54.3445773Z \r\n2020-07-30T22:30:54.3446155Z [  FAILED  ] TimerImplTest.TimerOrderDisableAndReschedule (27 ms)\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12391/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-07-30T22:50:00Z",
        "body": "cc @antoniovicente "
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-07-30T23:48:22Z",
        "body": "/assign @antoniovicente "
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-07-31T14:31:08Z",
        "body": "I now see the error in my reasoning regarding use of sleep to ensure enough separation between registrations and Dispatcher::run.  Time can go backwards before or after the sleep, resulting in run not picking up all the necessary events in the first iteration.  I looked at the libevent API in the hope of there being some way to ensure that the monotonic time advances past the necessary points, but the gettime functions used by the event loop are not exposed via libevent's public API.  Another alternative would be to rewrite the test to allow for some form of internal retries until the expected termination condition is reached.  Some of the known non-deterministic simulated timer tests take a similar approach to verify that all possible outcomes are reached within a small number of attempts."
      }
    ]
  },
  {
    "number": 12311,
    "title": "ActiveQuicListenerTest.ReceiveCHLO/IPv6_UseGQuicWithTLS flake",
    "created_at": "2020-07-27T21:27:38Z",
    "closed_at": "2023-03-20T16:05:47Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12311",
    "body": "```\r\n[ RUN      ] ActiveQuicListenerTests/ActiveQuicListenerTest.ReceiveCHLO/IPv6_UseGQuicWithTLS\r\nTestRandomGenerator running with seed -1579339208\r\ntest/extensions/quic_listeners/quiche/active_quic_listener_test.cc:297: Failure\r\nValue of: quic_dispatcher_->session_map().empty()\r\n  Actual: true\r\nExpected: false\r\nStack trace:\r\n  0x7d9950: Envoy::Quic::ActiveQuicListenerTest_ReceiveCHLO_Test::TestBody()\r\n  0x12a8568: testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n  0x12a8495: testing::Test::Run()\r\n  0x12a92e0: testing::TestInfo::Run()\r\n... Google Test internal frames ...\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12311/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-07-27T21:27:50Z",
        "body": "cc @danzh2010 would appreciate if you could take a look"
      },
      {
        "user": "danzh2010",
        "created_at": "2020-07-27T21:57:02Z",
        "body": "/assign @danzh2010 "
      }
    ]
  },
  {
    "number": 12282,
    "title": "dubbo_proxy clang-tidy CI build regression since 7/20",
    "created_at": "2020-07-24T16:29:48Z",
    "closed_at": "2020-07-27T15:10:30Z",
    "labels": [
      "help wanted",
      "area/dubbo"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12282",
    "body": "The dubbo_proxy was updated on 7/20, and broke pre-commit clang-tidy build.\r\n\r\n@alyssawilk \r\n\r\n```\r\n2020-07-23T21:11:20.7931109Z /source/source/extensions/filters/network/dubbo_proxy/deserializer.h:34:27: error: no member named 'Hessian' in 'Envoy::Extensions::NetworkFilters::DubboProxy::SerializationType'; did you mean 'Hessian2'? [clang-diagnostic-error]\r\n2020-07-23T21:11:20.7932514Z       {SerializationType::Hessian, \"hessian\"},\r\n2020-07-23T21:11:20.7932937Z                           ^~~~~~~\r\n2020-07-23T21:11:20.7933290Z                           Hessian2\r\n2020-07-23T21:11:20.7934270Z bazel-out/k8-fastbuild/bin/source/extensions/filters/network/dubbo_proxy/_virtual_includes/message_lib/extensions/filters/network/dubbo_proxy/message.h:48:3: note: 'Hessian2' declared here\r\n2020-07-23T21:11:20.7935187Z   Hessian2 = 2,\r\n2020-07-23T21:11:20.7935479Z   ^\r\n2020-07-23T21:11:20.7936250Z /source/source/extensions/filters/network/dubbo_proxy/deserializer.h:54:7: error: redefinition of 'RpcInvocation' [clang-diagnostic-error]\r\n2020-07-23T21:11:20.7936818Z class RpcInvocation {\r\n2020-07-23T21:11:20.7937111Z       ^\r\n2020-07-23T21:11:20.7938011Z bazel-out/k8-fastbuild/bin/source/extensions/filters/network/dubbo_proxy/_virtual_includes/message_lib/extensions/filters/network/dubbo_proxy/message.h:118:7: note: previous definition is here\r\n2020-07-23T21:11:20.7938664Z class RpcInvocation {\r\n2020-07-23T21:11:20.7939089Z       ^\r\n2020-07-23T21:11:20.7939854Z /source/source/extensions/filters/network/dubbo_proxy/deserializer.h:69:7: error: redefinition of 'RpcResult' [clang-diagnostic-error]\r\n2020-07-23T21:11:20.7940438Z class RpcResult {\r\n2020-07-23T21:11:20.7940728Z       ^\r\n2020-07-23T21:11:20.7941618Z bazel-out/k8-fastbuild/bin/source/extensions/filters/network/dubbo_proxy/_virtual_includes/message_lib/extensions/filters/network/dubbo_proxy/message.h:135:7: note: previous definition is here\r\n2020-07-23T21:11:20.7942262Z class RpcResult {\r\n2020-07-23T21:11:20.7942541Z       ^\r\n2020-07-23T21:11:20.7943493Z bazel-out/k8-fastbuild/bin/source/common/config/_virtual_includes/utility_lib/common/config/utility.h:239:52: error: no member named 'typed_config' in 'std::__1::basic_string<char>' [clang-diagnostic-error]\r\n2020-07-23T21:11:20.7944263Z     const ProtobufWkt::Any& typed_config = message.typed_config();\r\n2020-07-23T21:11:20.7944710Z                                                    ^\r\n2020-07-23T21:11:20.7946056Z /source/source/extensions/filters/network/dubbo_proxy/deserializer.h:154:36: note: in instantiation of function template specialization 'Envoy::Config::Utility::getAndCheckFactory<Envoy::Extensions::NetworkFilters::DubboProxy::NamedDeserializerConfigFactory, std::__1::basic_string<char> >' requested here\r\n2020-07-23T21:11:20.7947122Z     return Envoy::Config::Utility::getAndCheckFactory<NamedDeserializerConfigFactory>(name);\r\n2020-07-23T21:11:20.7947616Z                                    ^\r\n2020-07-23T21:11:20.7948603Z bazel-out/k8-fastbuild/bin/source/common/config/_virtual_includes/utility_lib/common/config/utility.h:259:63: error: no member named 'name' in 'std::__1::basic_string<char>' [clang-diagnostic-error]\r\n2020-07-23T21:11:20.7949345Z     return Utility::getAndCheckFactoryByName<Factory>(message.name());\r\n2020-07-23T21:11:20.7949831Z                                                               ^\r\n2020-07-23T21:11:20.7950143Z \r\n2020-07-23T21:11:20.7950468Z 67872 warnings and 5 errors generated.\r\n2020-07-23T21:11:20.7950933Z Error while processing /source/source/extensions/filters/network/dubbo_proxy/deserializer.h.\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12282/comments",
    "author": "wrowe",
    "comments": [
      {
        "user": "wrowe",
        "created_at": "2020-07-24T16:29:58Z",
        "body": "@sunjayBhatia "
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-07-27T13:41:35Z",
        "body": "Sorry about that.  After a few minutes of being baffled (the Hessian option was removed ages ago!) the only way I see this compiling is if the file isn't used at all.  Sent out a PR to remove it."
      }
    ]
  },
  {
    "number": 12253,
    "title": "tcp tunneling integration flake",
    "created_at": "2020-07-23T17:06:16Z",
    "closed_at": "2020-07-29T17:48:20Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12253",
    "body": "```\r\n[ RUN      ] IpVersions/TcpTunnelingIntegrationTest.TcpProxyUpstreamFlush/IPv6\r\n[2020-07-23 13:00:06.534][10423][critical][backtrace] [bazel-out/aarch64-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] Caught Segmentation fault, suspect faulting address 0x61616161616179\r\n[2020-07-23 13:00:06.534][10423][critical][backtrace] [bazel-out/aarch64-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:91] Backtrace (use tools/stack_decode.py to get line numbers):\r\n[2020-07-23 13:00:06.534][10423][critical][backtrace] [bazel-out/aarch64-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:92] Envoy version: 0/1.16.0-dev/test/RELEASE/BoringSSL\r\n[2020-07-23 13:00:06.534][10423][critical][backtrace] [bazel-out/aarch64-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:96] #0: __kernel_rt_sigreturn [0xffffbd4cb5b8]\r\n[2020-07-23 13:00:06.539][10423][critical][backtrace] [bazel-out/aarch64-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:96] #1: Envoy::Event::DispatcherImpl::runPostCallbacks() [0xe60c5c]\r\n[2020-07-23 13:00:06.544][10423][critical][backtrace] [bazel-out/aarch64-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:96] #2: event_process_active_single_queue [0x1570b00]\r\n[2020-07-23 13:00:06.549][10423][critical][backtrace] [bazel-out/aarch64-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:96] #3: event_base_loop [0x156f624]\r\n[2020-07-23 13:00:06.553][10423][critical][backtrace] [bazel-out/aarch64-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:96] #4: Envoy::FakeUpstream::threadRoutine() [0x872914]\r\n[2020-07-23 13:00:06.558][10423][critical][backtrace] [bazel-out/aarch64-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:96] #5: Envoy::Thread::ThreadImplPosix::ThreadImplPosix()::{lambda()#1}::__invoke() [0x156a8e0]\r\n[2020-07-23 13:00:06.558][10423][critical][backtrace] [bazel-out/aarch64-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:96] #6: start_thread [0xffffbd396088]\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12253/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-07-23T17:06:24Z",
        "body": "cc @alyssawilk "
      },
      {
        "user": "mattklein123",
        "created_at": "2020-07-27T16:37:43Z",
        "body": "There are still more issues here. I saw the same crash on master after my fix."
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-08-03T21:29:57Z",
        "body": "Trying to repo under ASAN.  I suspect it has something to do with FakeUpstream post callbacks which may result in use-after-free.\r\n\r\n[ RUN      ] IpVersions/ConnectTerminationIntegrationTest.BasicMaxStreamDuration/IPv6\r\n==12==ERROR: AddressSanitizer: heap-use-after-free on address 0x6080019a2d48 at pc 0x0000171145db bp 0x7f7161de14e0 sp 0x7f7161de14d8\r\nREAD of size 8 at 0x6080019a2d48 thread T131\r\n    #0 0x171145da in Envoy::Thread::LockGuard::LockGuard(Envoy::Thread::BasicLockable&) ??:?\r\n    #1 0x1751cebf in Envoy::FakeRawConnection::ReadFilter::onData(Envoy::Buffer::Instance&, bool) ??:?\r\n    #2 0x1ed88933 in Envoy::Network::FilterManagerImpl::onContinueReading(Envoy::Network::FilterManagerImpl::ActiveReadFilter*, Envoy::Network::ReadBufferSource&) ??:?\r\n    #3 0x1ed89222 in Envoy::Network::FilterManagerImpl::onRead() ??:?\r\n    #4 0x1ed16a8c in Envoy::Network::ConnectionImpl::onRead(unsigned long) ??:?\r\n    #5 0x1ed2698b in Envoy::Network::ConnectionImpl::onReadReady() ??:?\r\n    #6 0x1ed20833 in Envoy::Network::ConnectionImpl::onFileEvent(unsigned int) ??:?\r\n    #7 0x1ed5fbfd in Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6::operator()(unsigned int) const connection_impl.cc:?\r\n    #8 0x1ed5fb26 in _ZNSt3__18__invokeIRZN5Envoy7Network14ConnectionImplC1ERNS1_5Event10DispatcherEONS_10unique_ptrINS2_16ConnectionSocketENS_14default_deleteIS8_EEEEONS7_INS2_15TransportSocketENS9_ISD_EEEERNS1_10StreamInfo10StreamInfoEbE3$_6JjEEEDTclclsr3std3__1E7forwardIT_Efp_Espclsr3std3__1E7forwardIT0_Efp0_EEEOSM_DpOSN_ connection_impl.cc:?\r\n    #9 0x1ed5f8a6 in void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&, unsigned int&&) connection_impl.cc:?\r\n    #10 0x1ed5f76c in std::__1::__function::__alloc_func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6>, void (unsigned int)>::operator()(unsigned int&&) connection_impl.cc:?\r\n    #11 0x1ed5a2f7 in std::__1::__function::__func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6>, void (unsigned int)>::operator()(unsigned int&&) connection_impl.cc:?\r\n    #12 0x1b0f1070 in std::__1::__function::__value_func<void (unsigned int)>::operator()(unsigned int&&) const mocks.cc:?\r\n    #13 0x1b0f0d32 in std::__1::function<void (unsigned int)>::operator()(unsigned int) const ??:?\r\n    #14 0x1ecd8f8c in Envoy::Event::FileEventImpl::mergeInjectedEventsAndRunCb(unsigned int) ??:?\r\n    #15 0x1ecd9586 in Envoy::Event::FileEventImpl::assignEvents(unsigned int, event_base*)::$_1::operator()(int, short, void*) const file_event_impl.cc:?\r\n    #16 0x1ecd8fc5 in Envoy::Event::FileEventImpl::assignEvents(unsigned int, event_base*)::$_1::__invoke(int, short, void*) file_event_impl.cc:?\r\n    #17 0x25085ead in event_persist_closure event.c:?\r\n    #18 0x2507ff34 in event_process_active_single_queue event.c:?\r\n    #19 0x25057e30 in event_process_active event.c:?\r\n    #20 0x2504f709 in event_base_loop ??:?\r\n    #21 0x1f511e4b in Envoy::Event::LibeventScheduler::run(Envoy::Event::Dispatcher::RunType) ??:?\r\n    #22 0x1ec9d779 in Envoy::Event::DispatcherImpl::run(Envoy::Event::Dispatcher::RunType) ??:?\r\n    #23 0x17513a9f in Envoy::FakeUpstream::threadRoutine() ??:?\r\n    #24 0x1757eb27 in Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12::operator()() const fake_upstream.cc:?\r\n    #25 0x1757ea59 in _ZNSt3__18__invokeIRZN5Envoy12FakeUpstreamC1EONS_10unique_ptrINS1_7Network22TransportSocketFactoryENS_14default_deleteIS5_EEEEONS3_INS4_6SocketENS6_ISA_EEEENS1_18FakeHttpConnection4TypeERNS1_5Event14TestTimeSystemEbE4$_12JEEEDTclclsr3std3__1E7forwardIT_Efp_Espclsr3std3__1E7forwardIT0_Efp0_EEEOSL_DpOSM_ fake_upstream.cc:?\r\n    #26 0x1757e8a9 in void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&>(Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&) fake_upstream.cc:?\r\n    #27 0x1757e7ff in std::__1::__function::__alloc_func<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12, std::__1::allocator<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12>, void ()>::operator()() fake_upstream.cc:?\r\n    #28 0x1757940a in std::__1::__function::__func<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12, std::__1::allocator<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12>, void ()>::operator()() fake_upstream.cc:?\r\n    #29 0x174ba8e3 in std::__1::__function::__value_func<void ()>::operator()() const autonomous_upstream.cc:?\r\n    #30 0x174b9fea in std::__1::function<void ()>::operator()() const ??:?\r\n    #31 0x24ff195e in Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, std::__1::optional<Envoy::Thread::Options> const&)::{lambda(void*)#1}::operator()(void*) const ??:?\r\n    #32 0x24ff18b4 in Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, std::__1::optional<Envoy::Thread::Options> const&)::{lambda(void*)#1}::__invoke(void*) ??:?\r\n    #33 0x7f716d601f26 in start_thread /build/glibc-M65Gwz/glibc-2.30/nptl/pthread_create.c:479\r\n    #34 0x7f716d5172ee in __GI___clone /build/glibc-M65Gwz/glibc-2.30/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n\r\n0x6080019a2d48 is located 40 bytes inside of 96-byte region [0x6080019a2d20,0x6080019a2d80)\r\nfreed by thread T0 here:\r\n    #0 0x16e5e3ed in __interceptor_free ??:?\r\n    #1 0x1765f850 in Envoy::FakeRawConnection::~FakeRawConnection() ??:?\r\n    #2 0x170fee4a in std::__1::default_delete<Envoy::FakeRawConnection>::operator()(Envoy::FakeRawConnection*) const tcp_tunneling_integration_test.cc:?\r\n    #3 0x170feb7c in std::__1::unique_ptr<Envoy::FakeRawConnection, std::__1::default_delete<Envoy::FakeRawConnection> >::reset(Envoy::FakeRawConnection*) tcp_tunneling_integration_test.cc:?\r\n    #4 0x170fe368 in std::__1::unique_ptr<Envoy::FakeRawConnection, std::__1::default_delete<Envoy::FakeRawConnection> >::~unique_ptr() tcp_tunneling_integration_test.cc:?\r\n    #5 0x16ea2a96 in Envoy::(anonymous namespace)::ConnectTerminationIntegrationTest::~ConnectTerminationIntegrationTest() tcp_tunneling_integration_test.cc:?\r\n    #6 0x16eb38a7 in Envoy::(anonymous namespace)::ConnectTerminationIntegrationTest_BasicMaxStreamDuration_Test::~ConnectTerminationIntegrationTest_BasicMaxStreamDuration_Test() tcp_tunneling_integration_test.cc:?\r\n    #7 0x16eb3904 in Envoy::(anonymous namespace)::ConnectTerminationIntegrationTest_BasicMaxStreamDuration_Test::~ConnectTerminationIntegrationTest_BasicMaxStreamDuration_Test() tcp_tunneling_integration_test.cc:?\r\n    #8 0x254de08d in testing::Test::DeleteSelf_() ??:?\r\n    #9 0x2552759b in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) ??:?\r\n    #10 0x254dc320 in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) ??:?\r\n    #11 0x2549a267 in testing::TestInfo::Run() ??:?\r\n    #12 0x2549ba74 in testing::TestSuite::Run() ??:?\r\n    #13 0x254c347a in testing::internal::UnitTestImpl::RunAllTests() ??:?\r\n    #14 0x2553cb9b in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) ??:?\r\n    #15 0x254e5085 in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) ??:?\r\n    #16 0x254c2012 in testing::UnitTest::Run() ??:?\r\n    #17 0x1ea414a1 in RUN_ALL_TESTS() ??:?\r\n    #18 0x1ea3f243 in Envoy::TestRunner::RunTests(int, char**) ??:?\r\n    #19 0x1ea3a7b6 in main ??:?\r\n    #20 0x7f716d440e0a in __libc_start_main /build/glibc-M65Gwz/glibc-2.30/csu/../csu/libc-start.c:308\r\n\r\npreviously allocated by thread T0 here:\r\n    #0 0x16e5e66d in __interceptor_malloc ??:?\r\n    #1 0x7f716d6d8a37 in operator new(unsigned long) ??:?\r\n    #2 0x175181b0 in Envoy::FakeUpstream::waitForRawConnection(std::__1::unique_ptr<Envoy::FakeRawConnection, std::__1::default_delete<Envoy::FakeRawConnection> >&, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l> >) ??:?\r\n    #3 0x16ea384d in Envoy::(anonymous namespace)::ConnectTerminationIntegrationTest::setUpConnection() tcp_tunneling_integration_test.cc:?\r\n    #4 0x16eb3e19 in Envoy::(anonymous namespace)::ConnectTerminationIntegrationTest_BasicMaxStreamDuration_Test::TestBody() tcp_tunneling_integration_test.cc:?\r\n    #5 0x2552759b in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) ??:?\r\n    #6 0x254dc320 in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) ??:?\r\n    #7 0x25497d2f in testing::Test::Run() ??:?\r\n    #8 0x2549a09f in testing::TestInfo::Run() ??:?\r\n    #9 0x2549ba74 in testing::TestSuite::Run() ??:?\r\n    #10 0x254c347a in testing::internal::UnitTestImpl::RunAllTests() ??:?\r\n    #11 0x2553cb9b in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) ??:?\r\n    #12 0x254e5085 in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) ??:?\r\n    #13 0x254c2012 in testing::UnitTest::Run() ??:?\r\n    #14 0x1ea414a1 in RUN_ALL_TESTS() ??:?\r\n    #15 0x1ea3f243 in Envoy::TestRunner::RunTests(int, char**) ??:?\r\n    #16 0x1ea3a7b6 in main ??:?\r\n    #17 0x7f716d440e0a in __libc_start_main /build/glibc-M65Gwz/glibc-2.30/csu/../csu/libc-start.c:308\r\n\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-08-03T21:52:42Z",
        "body": "@antoniovicente I was able to repro on an ARM machine with opt build (???). I have a fix soon."
      }
    ]
  },
  {
    "number": 12242,
    "title": "filter_chain_benchmark_test_benchmark_test tsan flake",
    "created_at": "2020-07-23T02:43:54Z",
    "closed_at": "2020-07-24T04:05:21Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12242",
    "body": "```\r\n2020-07-23T00:06:20.5948156Z TIMEOUT: //test/server:filter_chain_benchmark_test_benchmark_test (Summary)\r\n2020-07-23T00:06:20.5998332Z       /build/tmp/_bazel_bazel/b570b5ccd0454dc9af9f65ab1833764d/execroot/envoy/bazel-out/k8-dbg/testlogs/test/server/filter_chain_benchmark_test_benchmark_test/test.log\r\n2020-07-23T00:06:20.5998879Z ==================== Test output for //test/server:filter_chain_benchmark_test_benchmark_test:\r\n2020-07-23T00:06:20.5999508Z [2020-07-22 23:51:22.419][19][critical][misc] Expensive benchmarks are being skipped; see test/README.md for more information\r\n2020-07-23T00:06:20.5999952Z 2020-07-22 23:51:22\r\n2020-07-23T00:06:20.6000481Z Running /b/f/w/bazel-out/k8-dbg/bin/test/server/filter_chain_benchmark_test_benchmark_test.runfiles/envoy/test/server/filter_chain_benchmark_test\r\n2020-07-23T00:06:20.6000836Z Run on (2 X 2250 MHz CPU s)\r\n2020-07-23T00:06:20.6001025Z CPU Caches:\r\n2020-07-23T00:06:20.6001348Z   L1 Data 32K (x1)\r\n2020-07-23T00:06:20.6001541Z   L1 Instruction 32K (x1)\r\n2020-07-23T00:06:20.6001735Z   L2 Unified 512K (x1)\r\n2020-07-23T00:06:20.6001917Z   L3 Unified 16384K (x1)\r\n2020-07-23T00:06:20.6002125Z Load Average: 1.38, 1.18, 1.17\r\n2020-07-23T00:06:20.6002385Z ***WARNING*** Library was built as DEBUG. Timings may be affected.\r\n2020-07-23T00:06:20.6002911Z [2020-07-22 23:51:23.233][19][debug][config] new fc_contexts has 3 filter chains, including 3 newly built\r\n2020-07-23T00:06:20.6003449Z -------------------------------------------------------------------------------------------------------\r\n2020-07-23T00:06:20.6003841Z Benchmark                                                             Time             CPU   Iterations\r\n2020-07-23T00:06:20.6004458Z -------------------------------------------------------------------------------------------------------\r\n2020-07-23T00:06:20.6004841Z FilterChainBenchmarkFixture/FilterChainManagerBuildTest/1      19192398 ns     18954121 ns            1\r\n2020-07-23T00:06:20.6005390Z [2020-07-22 23:51:24.760][19][debug][config] new fc_contexts has 10 filter chains, including 10 newly built\r\n2020-07-23T00:06:20.6005781Z FilterChainBenchmarkFixture/FilterChainManagerBuildTest/8      71343691 ns     71345320 ns            1\r\n2020-07-23T00:06:20.6006329Z [2020-07-22 23:51:31.722][19][debug][config] new fc_contexts has 66 filter chains, including 66 newly built\r\n2020-07-23T00:06:20.6006730Z FilterChainBenchmarkFixture/FilterChainManagerBuildTest/64    552462296 ns    552178573 ns            1\r\n2020-07-23T00:06:20.6007282Z [2020-07-22 23:52:21.731][19][debug][config] new fc_contexts has 514 filter chains, including 514 newly built\r\n2020-07-23T00:06:20.6007690Z FilterChainBenchmarkFixture/FilterChainManagerBuildTest/512  4090699827 ns   4089874909 ns            1\r\n2020-07-23T00:06:20.6008239Z [2020-07-22 23:58:51.792][19][debug][config] new fc_contexts has 4098 filter chains, including 4098 newly built\r\n2020-07-23T00:06:20.6068120Z FilterChainBenchmarkFixture/FilterChainManagerBuildTest/4096 32562148278 ns   32556390826 ns            1\r\n2020-07-23T00:06:20.6069516Z [2020-07-22 23:58:55.921][19][debug][config] new fc_contexts has 3 filter chains, including 3 newly built\r\n2020-07-23T00:06:20.6070175Z FilterChainBenchmarkFixture/FilterChainFindTest/1                109550 ns       106830 ns            1\r\n2020-07-23T00:06:20.6070965Z [2020-07-22 23:58:57.479][19][debug][config] new fc_contexts has 10 filter chains, including 10 newly built\r\n2020-07-23T00:06:20.6071583Z FilterChainBenchmarkFixture/FilterChainFindTest/8                560229 ns       557259 ns            1\r\n2020-07-23T00:06:20.6072765Z [2020-07-22 23:59:04.058][19][debug][config] new fc_contexts has 66 filter chains, including 66 newly built\r\n2020-07-23T00:06:20.6073950Z FilterChainBenchmarkFixture/FilterChainFindTest/64              3571219 ns      3569130 ns            1\r\n2020-07-23T00:06:20.6074667Z [2020-07-22 23:59:54.815][19][debug][config] new fc_contexts has 514 filter chains, including 514 newly built\r\n2020-07-23T00:06:20.6075154Z FilterChainBenchmarkFixture/FilterChainFindTest/512            28730589 ns     28731850 ns            1\r\n2020-07-23T00:06:20.6075685Z -- Test timed out at 2020-07-23 00:06:17 UTC --\r\n2020-07-23T00:06:20.6076046Z ================================================================================\r\n2020-07-23T00:06:20.6076446Z INFO: From Testing //test/server:filter_chain_benchmark_test_benchmark_test:\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12242/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-07-23T02:44:21Z",
        "body": "I've seen this flake multiple times on TSAN. I'm guessing the test is too expensive for that sanitizer. @antoniovicente any recommendations on how to fix?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-07-23T18:26:55Z",
        "body": "Do we need a benchmark on TSAN?  This is the sort of thing I think we could just compile disable."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-07-23T18:29:28Z",
        "body": "Theoretically it seems like it would catch some issues, but I would be fine disabling this type of thing on TSAN."
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-07-23T18:39:48Z",
        "body": "We have a flag that pgenera added to skip expensive benchmarks when running the smoke tests, I'll look into how to reduce the runtime for this specific test."
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-07-23T18:40:01Z",
        "body": "/assign @antoniovicente "
      }
    ]
  },
  {
    "number": 12140,
    "title": "CacheFilter: Add date metadata/ internal header to cached responses",
    "created_at": "2020-07-16T22:26:38Z",
    "closed_at": "2020-09-11T00:21:58Z",
    "labels": [
      "help wanted",
      "area/cache"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12140",
    "body": "When a response is being inserted to the cache, the cache filter should add a date internal header or metadata to the cached response. This should be used instead of the response `Date` header throughout the CacheFilter module as there might be a clock skew between the origin and Envoy.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12140/comments",
    "author": "yosrym93",
    "comments": [
      {
        "user": "yosrym93",
        "created_at": "2020-07-16T22:32:55Z",
        "body": "Among the updates that should be made when this is implemented:\r\n- Update CacheabilityUtils::isCacheableResponse() has_validation_data conditions.\r\n- Update LookupRequest::requiresValidation() and remove the check for a future response_time."
      },
      {
        "user": "toddmgreer",
        "created_at": "2020-07-17T18:04:25Z",
        "body": "I think we should add things like this to the plugin API. Plugins can turn them into header if needed, and we can avoid the cost of modifying headers for in-memory caches.\r\n\r\nOffhand, I think we should add a Metadata struct (or proto?) that would be included in makeInsertContext and LookupResult."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-16T21:44:56Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 11801,
    "title": "IpVersions/Http2FloodMitigationTest.RST_STREAM flake",
    "created_at": "2020-06-29T17:16:57Z",
    "closed_at": "2023-03-20T16:05:25Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11801",
    "body": "```\r\n[ RUN      ] IpVersions/Http2FloodMitigationTest.RST_STREAM/IPv6\r\n-- Test timed out at 2020-06-29 16:41:57 UTC --\r\ntest/integration/http2_integration_test.cc:1659: Failure\r\nExpected equality of these values:\r\n  1\r\n  test_server_->counter(\"http.config_test.downstream_cx_delayed_close_timeout\")->value()\r\n    Which is: 0\r\nStack trace:\r\n  0x2ab8f65: Envoy::Http2FloodMitigationTest_RST_STREAM_Test::TestBody()\r\n  0x7d6442d: testing::internal::HandleSehExceptionsInMethodIfSupported<>()\r\n  0x7d484bf: testing::internal::HandleExceptionsInMethodIfSupported<>()\r\n  0x7d2d7e2: testing::Test::Run()\r\n  0x7d2e684: testing::TestInfo::Run()\r\n... Google Test internal frames ...\r\n\r\n[2020-06-29 16:41:57.709][16][critical][backtrace] [bazel-out/k8-dbg/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] Caught Segmentation fault, suspect faulting address 0x0\r\n[2020-06-29 16:41:57.709][16][critical][backtrace] [bazel-out/k8-dbg/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:91] Backtrace (use tools/stack_decode.py to get line numbers):\r\n[2020-06-29 16:41:57.709][16][critical][backtrace] [bazel-out/k8-dbg/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:92] Envoy version: 0/1.15.0-dev/redacted/DEBUG/BoringSSL\r\n[2020-06-29 16:41:57.823][16][critical][backtrace] [bazel-out/k8-dbg/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:96] #0: Envoy::SignalAction::sigHandler() [0x6217fe7]\r\n[2020-06-29 16:41:57.934][16][critical][backtrace] [bazel-out/k8-dbg/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:96] #1: __tsan::CallUserSignalHandler() [0x29ff010]\r\n================================================================================\r\n\r\nTIMEOUT: //test/integration:http2_integration_test (Summary)\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11801/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-06-29T17:17:11Z",
        "body": "cc @yanavlasov @alyssawilk @antoniovicente I've seen this a few times recently."
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-06-29T18:21:10Z",
        "body": "failing on any particular build?  traditional deflake instructions aren't getting me anything on a pretty recent build.\r\n\r\nbazel test //test/integration:http2_integration_test --runs_per_test=3000 --test_arg=--gtest_filter=IpVersions/Http2FloodMitigationTest.RST_STREAM/IPv6 --jobs=100 --local_ram_resources=1000000000 --local_cpu_resources=1000000000\r\n\r\nGiven that we could either sprinkle logs to get a better idea of what's failing where, or make more of the \"do this until success\" calls \"do this for 5s or fail\" to get better failure information"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-06-29T18:28:36Z",
        "body": "@alyssawilk I've seen it a bunch on TSAN. It must be a timing issue on that build."
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-06-29T20:09:42Z",
        "body": "ugh, can't repro with --config=clang-tsan either.  I'll work on making the tcp client read and write not infinite hang if things fail.  Leaving as unassigned as Antonio/Yan might have ideas of what if anything changed regarding code."
      },
      {
        "user": "yanavlasov",
        "created_at": "2020-06-30T02:28:16Z",
        "body": "No ideas at this point. I'll poke at it."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-06-30T20:06:40Z",
        "body": "This is a failure from a coverage build which I assume is a similar issue:\r\n```\r\n2020-06-30T19:35:01.8047752Z [ RUN      ] IpVersions/Http2FloodMitigationTest.Data/IPv4\r\n2020-06-30T19:35:01.8048404Z #0 Envoy::SignalAction::sigHandler() [0x453f584]\r\n2020-06-30T19:35:01.8049017Z #1 __restore_rt [0x7f0d6071b890]\r\n2020-06-30T19:35:01.8049631Z #2 Envoy::AutonomousStream::~AutonomousStream() [0x1f71843]\r\n2020-06-30T19:35:01.8050340Z #3 Envoy::AutonomousStream::~AutonomousStream() [0x1f718bc]\r\n2020-06-30T19:35:01.8050988Z #4 __llvm_coverage_mapping [0x1ecaa03]\r\n2020-06-30T19:35:01.8051593Z #5 __llvm_coverage_mapping [0x1eca938]\r\n2020-06-30T19:35:01.8052144Z #6 __llvm_coverage_mapping [0x1df10ee]\r\n2020-06-30T19:35:01.8052667Z #7 __llvm_coverage_mapping [0x1ef14bd]\r\n2020-06-30T19:35:01.8053282Z #8 std::__1::allocator_traits<>::__destroy<>() [0x1ef1481]\r\n2020-06-30T19:35:01.8054018Z #9 std::__1::allocator_traits<>::destroy<>() [0x1ef1441]\r\n2020-06-30T19:35:01.8054695Z #10 __llvm_coverage_mapping [0x1ef13e2]\r\n2020-06-30T19:35:01.8055257Z #11 __llvm_coverage_mapping [0x1ef12d0]\r\n2020-06-30T19:35:01.8055887Z #12 std::__1::__vector_base<>::~__vector_base() [0x1ef1040]\r\n2020-06-30T19:35:01.8056551Z #13 __llvm_coverage_mapping [0x1df369d]\r\n2020-06-30T19:35:01.8057381Z #14 Envoy::AutonomousHttpConnection::~AutonomousHttpConnection() [0x1f7cd87]\r\n2020-06-30T19:35:01.8058127Z #15 Envoy::AutonomousHttpConnection::~AutonomousHttpConnection() [0x1f748e3]\r\n2020-06-30T19:35:01.8058892Z #16 Envoy::AutonomousHttpConnection::~AutonomousHttpConnection() [0x1f7491c]\r\n2020-06-30T19:35:01.8059608Z #17 __llvm_coverage_mapping [0x1f83d13]\r\n2020-06-30T19:35:01.8060189Z #18 __llvm_coverage_mapping [0x1f83c48]\r\n2020-06-30T19:35:01.8060723Z #19 __llvm_coverage_mapping [0x1f73f8e]\r\n2020-06-30T19:35:01.8061272Z #20 __llvm_coverage_mapping [0x1f813bd]\r\n2020-06-30T19:35:01.8062067Z #21 std::__1::allocator_traits<>::__destroy<>() [0x1f81381]\r\n2020-06-30T19:35:01.8062725Z #22 std::__1::allocator_traits<>::destroy<>() [0x1f81341]\r\n2020-06-30T19:35:01.8063357Z #23 __llvm_coverage_mapping [0x1f812e2]\r\n2020-06-30T19:35:01.8063933Z #24 __llvm_coverage_mapping [0x1f811d0]\r\n2020-06-30T19:35:01.8064507Z #25 __llvm_coverage_mapping [0x1f73a1b]\r\n2020-06-30T19:35:01.8065130Z #26 Envoy::AutonomousUpstream::~AutonomousUpstream() [0x1f7270e]\r\n2020-06-30T19:35:01.8065823Z #27 Envoy::AutonomousUpstream::~AutonomousUpstream() [0x1f7285c]\r\n2020-06-30T19:35:01.8066463Z #28 __llvm_coverage_mapping [0x1eeb2f3]\r\n2020-06-30T19:35:01.8067080Z #29 __llvm_coverage_mapping [0x1eeb2a8]\r\n2020-06-30T19:35:01.8067626Z #30 __llvm_coverage_mapping [0x1eeb20e]\r\n2020-06-30T19:35:01.8068175Z #31 __llvm_coverage_mapping [0x1eeb1cd]\r\n2020-06-30T19:35:01.8068825Z #32 std::__1::allocator_traits<>::__destroy<>() [0x1eeb191]\r\n2020-06-30T19:35:01.8069516Z #33 std::__1::allocator_traits<>::destroy<>() [0x1eeb151]\r\n2020-06-30T19:35:01.8070128Z #34 __llvm_coverage_mapping [0x1f41082]\r\n2020-06-30T19:35:01.8070678Z #35 __llvm_coverage_mapping [0x1f40ff0]\r\n2020-06-30T19:35:01.8071308Z #36 std::__1::__vector_base<>::~__vector_base() [0x1f40f80]\r\n2020-06-30T19:35:01.8072127Z #37 __llvm_coverage_mapping [0x1f40bcd]\r\n2020-06-30T19:35:01.8072820Z #38 Envoy::BaseIntegrationTest::~BaseIntegrationTest() [0x1f3c2fa]\r\n2020-06-30T19:35:01.8073560Z #39 Envoy::HttpIntegrationTest::~HttpIntegrationTest() [0x1f01279]\r\n2020-06-30T19:35:01.8074289Z #40 Envoy::Http2FloodMitigationTest::~Http2FloodMitigationTest() [0x1e4fd93]\r\n2020-06-30T19:35:01.8075071Z #41 Envoy::Http2FloodMitigationTest_Data_Test::~Http2FloodMitigationTest_Data_Test() [0x1df9768]\r\n2020-06-30T19:35:01.8076104Z #42 Envoy::Http2FloodMitigationTest_Data_Test::~Http2FloodMitigationTest_Data_Test() [0x1df978c]\r\n2020-06-30T19:35:01.8076828Z #43 testing::Test::DeleteSelf_() [0x5ac3b10]\r\n2020-06-30T19:35:01.8077461Z #44 testing::internal::HandleSehExceptionsInMethodIfSupported<>() [0x5ad7b56]\r\n2020-06-30T19:35:01.8078218Z #45 testing::internal::HandleExceptionsInMethodIfSupported<>() [0x5ac30f1]\r\n2020-06-30T19:35:01.8078819Z #46 testing::TestInfo::Run() [0x5aae225]\r\n2020-06-30T19:35:01.8079410Z #47 testing::TestSuite::Run() [0x5aae939]\r\n2020-06-30T19:35:01.8080054Z #48 testing::internal::UnitTestImpl::RunAllTests() [0x5abb946]\r\n2020-06-30T19:35:01.8080770Z #49 testing::internal::HandleSehExceptionsInMethodIfSupported<>() [0x5adcfd6]\r\n2020-06-30T19:35:01.8081512Z #50 testing::internal::HandleExceptionsInMethodIfSupported<>() [0x5ac5f41]\r\n2020-06-30T19:35:01.8082202Z #51 testing::UnitTest::Run() [0x5abb390]\r\n2020-06-30T19:35:01.8082801Z #52 RUN_ALL_TESTS() [0x3e10435]\r\n2020-06-30T19:35:01.8083357Z #53 Envoy::TestRunner::RunTests() [0x3e0fb2d]\r\n2020-06-30T19:35:01.8083894Z #54 main [0x3e0dcc5]\r\n2020-06-30T19:35:01.8084391Z #55 __libc_start_main [0x7f0d60339b97]\r\n2020-06-30T19:35:01.8085111Z external/bazel_tools/tools/test/collect_coverage.sh: line 131: 24501 Aborted                 (core dumped) \"$@\"```"
      }
    ]
  },
  {
    "number": 11734,
    "title": "CacheFilter: Upgrade GET requests whose upgrade headers are not removed should bypass the cache",
    "created_at": "2020-06-24T19:28:21Z",
    "closed_at": "2020-08-27T18:49:28Z",
    "labels": [
      "help wanted",
      "area/cache"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11734",
    "body": "The CacheFilter (#7198) should bypass cache for any upgrade GET requests that Envoy does not strip the upgrade header for before proxying it upstream. The reason for this is that if the upgrade header is forwarded upstream, then this means that this is a request that wants the origin (as opposed to the proxy) to change the communication protocol. Responses to these requests should not be cached.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11734/comments",
    "author": "yosrym93",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-07-25T14:19:18Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "yosrym93",
        "created_at": "2020-08-27T18:49:25Z",
        "body": "We don't have enough information for why this is needed at the moment. I'll close the issue."
      }
    ]
  },
  {
    "number": 11538,
    "title": "http2_integration_test flood test flake",
    "created_at": "2020-06-10T15:06:39Z",
    "closed_at": "2020-06-11T21:01:13Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11538",
    "body": "I've seen this a few times in CI:\r\n```\r\n2020-06-10T14:23:40.3629074Z [ RUN      ] IpVersions/Http2FloodMitigationTest.Data/IPv6\r\n2020-06-10T14:23:40.3629467Z ==================\r\n2020-06-10T14:23:40.3629850Z WARNING: ThreadSanitizer: data race (pid=15)\r\n2020-06-10T14:23:40.3630294Z   Read of size 8 at 0x0000087c1178 by thread T3:\r\n2020-06-10T14:23:40.3631800Z     #0 Envoy::InjectableSingleton<Envoy::Runtime::Loader>::getExisting() /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/singleton/_virtual_includes/threadsafe_singleton/common/singleton/threadsafe_singleton.h:60:36 (http2_integration_test+0x2e92f1f)\r\n2020-06-10T14:23:40.3632816Z     #1 Envoy::Runtime::getInteger(absl::string_view, unsigned long) /proc/self/cwd/source/common/runtime/runtime_features.cc:26:7 (http2_integration_test+0x5bcd09a)\r\n2020-06-10T14:23:40.3633669Z     #2 Envoy::Buffer::WatermarkBuffer::setWatermarks(unsigned int, unsigned int) /proc/self/cwd/source/common/buffer/watermark_buffer.cc:75:7 (http2_integration_test+0x57e752f)\r\n2020-06-10T14:23:40.3635061Z     #3 Envoy::Http::Http2::ConnectionImpl::StreamImpl::setWriteBufferWatermarks(unsigned int, unsigned int) /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/http/http2/_virtual_includes/codec_lib/common/http/http2/codec_impl.h:252:26 (http2_integration_test+0x55ee24e)\r\n2020-06-10T14:23:40.3636169Z     #4 Envoy::Http::Http2::ConnectionImpl::StreamImpl::StreamImpl(Envoy::Http::Http2::ConnectionImpl&, unsigned int) /proc/self/cwd/source/common/http/http2/codec_impl.cc:99:5 (http2_integration_test+0x55c718f)\r\n2020-06-10T14:23:40.3638284Z     #5 Envoy::Http::Http2::ConnectionImpl::ServerStreamImpl::ServerStreamImpl(Envoy::Http::Http2::ConnectionImpl&, unsigned int) /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/http/http2/_virtual_includes/codec_lib/common/http/http2/codec_impl.h:359:11 (http2_integration_test+0x55f2bbc)\r\n2020-06-10T14:23:40.3639470Z     #6 Envoy::Http::Http2::ServerConnectionImpl::onBeginHeaders(nghttp2_frame const*) /proc/self/cwd/source/common/http/http2/codec_impl.cc:1256:34 (http2_integration_test+0x55d708e)\r\n2020-06-10T14:23:40.3641194Z     #7 non-virtual thunk to Envoy::Http::Http2::ServerConnectionImpl::onBeginHeaders(nghttp2_frame const*) /proc/self/cwd/source/common/http/http2/codec_impl.cc (http2_integration_test+0x55d746f)\r\n2020-06-10T14:23:40.3642354Z     #8 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_10::operator()(nghttp2_session*, nghttp2_frame const*, void*) const /proc/self/cwd/source/common/http/http2/codec_impl.cc:1037:57 (http2_integration_test+0x55da7cc)\r\n2020-06-10T14:23:40.3643418Z     #9 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_10::__invoke(nghttp2_session*, nghttp2_frame const*, void*) /proc/self/cwd/source/common/http/http2/codec_impl.cc:1036:19 (http2_integration_test+0x55da728)\r\n2020-06-10T14:23:40.3644298Z     #10 session_call_on_begin_headers /b/f/w/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:3314:10 (http2_integration_test+0x5cdaad0)\r\n2020-06-10T14:23:40.3645018Z     #11 nghttp2_session_on_request_headers_received /b/f/w/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:3915:8 (http2_integration_test+0x5cda63b)\r\n2020-06-10T14:23:40.3645720Z     #12 session_process_headers_frame /b/f/w/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:4059:12 (http2_integration_test+0x5ce94a0)\r\n2020-06-10T14:23:40.3647204Z     #13 nghttp2_session_mem_recv /b/f/w/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:5570:14 (http2_integration_test+0x5ce0df3)\r\n2020-06-10T14:23:40.3648168Z     #14 Envoy::Http::Http2::ConnectionImpl::innerDispatch(Envoy::Buffer::Instance&) /proc/self/cwd/source/common/http/http2/codec_impl.cc:495:9 (http2_integration_test+0x55cf25f)\r\n2020-06-10T14:23:40.3649298Z     #15 Envoy::Http::Http2::ServerConnectionImpl::innerDispatch(Envoy::Buffer::Instance&) /proc/self/cwd/source/common/http/http2/codec_impl.cc:1382:26 (http2_integration_test+0x55d9424)\r\n2020-06-10T14:23:40.3650462Z     #16 Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23::operator()(Envoy::Buffer::Instance&) const /proc/self/cwd/source/common/http/http2/codec_impl.cc:1368:60 (http2_integration_test+0x55eb284)\r\n2020-06-10T14:23:40.3652045Z     #17 decltype(std::__1::forward<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23&>(fp)(std::__1::forward<Envoy::Buffer::Instance&>(fp0))) std::__1::__invoke<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23&, Envoy::Buffer::Instance&>(Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23&, Envoy::Buffer::Instance&) /opt/llvm/bin/../include/c++/v1/type_traits:3530:1 (http2_integration_test+0x55eb194)\r\n2020-06-10T14:23:40.3654153Z     #18 absl::Status std::__1::__invoke_void_return_wrapper<absl::Status>::__call<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23&, Envoy::Buffer::Instance&>(Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23&, Envoy::Buffer::Instance&) /opt/llvm/bin/../include/c++/v1/__functional_base:317:16 (http2_integration_test+0x55eb0c4)\r\n2020-06-10T14:23:40.3655815Z     #19 std::__1::__function::__alloc_func<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23, std::__1::allocator<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23>, absl::Status (Envoy::Buffer::Instance&)>::operator()(Envoy::Buffer::Instance&) /opt/llvm/bin/../include/c++/v1/functional:1533:16 (http2_integration_test+0x55eb034)\r\n2020-06-10T14:23:40.3657741Z     #20 std::__1::__function::__func<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23, std::__1::allocator<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23>, absl::Status (Envoy::Buffer::Instance&)>::operator()(Envoy::Buffer::Instance&) /opt/llvm/bin/../include/c++/v1/functional:1707:12 (http2_integration_test+0x55e92d3)\r\n2020-06-10T14:23:40.3659239Z     #21 std::__1::__function::__value_func<absl::Status (Envoy::Buffer::Instance&)>::operator()(Envoy::Buffer::Instance&) const /opt/llvm/bin/../include/c++/v1/functional:1860:16 (http2_integration_test+0x47740eb)\r\n2020-06-10T14:23:40.3660430Z     #22 std::__1::function<absl::Status (Envoy::Buffer::Instance&)>::operator()(Envoy::Buffer::Instance&) const /opt/llvm/bin/../include/c++/v1/functional:2419:12 (http2_integration_test+0x4773fff)\r\n2020-06-10T14:23:40.3661762Z     #23 Envoy::Http::Utility::exceptionToStatus(std::__1::function<absl::Status (Envoy::Buffer::Instance&)>, Envoy::Buffer::Instance&) /proc/self/cwd/source/common/http/utility.cc:41:14 (http2_integration_test+0x5bce840)\r\n2020-06-10T14:23:40.3663065Z     #24 Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&) /proc/self/cwd/source/common/http/http2/codec_impl.cc:1367:10 (http2_integration_test+0x55d8ed2)\r\n2020-06-10T14:23:40.3663906Z     #25 Envoy::FakeHttpConnection::ReadFilter::onData(Envoy::Buffer::Instance&, bool) /proc/self/cwd/./test/integration/fake_upstream.h:453:45 (http2_integration_test+0x2cffbc9)\r\n2020-06-10T14:23:40.3664859Z     #26 Envoy::Network::FilterManagerImpl::onContinueReading(Envoy::Network::FilterManagerImpl::ActiveReadFilter*, Envoy::Network::ReadBufferSource&) /proc/self/cwd/source/common/network/filter_manager_impl.cc:66:48 (http2_integration_test+0x57e0358)\r\n2020-06-10T14:23:40.3665780Z     #27 Envoy::Network::FilterManagerImpl::onRead() /proc/self/cwd/source/common/network/filter_manager_impl.cc:76:3 (http2_integration_test+0x57e0651)\r\n2020-06-10T14:23:40.3666504Z     #28 Envoy::Network::ConnectionImpl::onRead(unsigned long) /proc/self/cwd/source/common/network/connection_impl.cc:297:19 (http2_integration_test+0x57b8bb2)\r\n2020-06-10T14:23:40.3667443Z     #29 Envoy::Network::ConnectionImpl::onReadReady() /proc/self/cwd/source/common/network/connection_impl.cc:579:5 (http2_integration_test+0x57bddc0)\r\n2020-06-10T14:23:40.3668389Z     #30 Envoy::Network::ConnectionImpl::onFileEvent(unsigned int) /proc/self/cwd/source/common/network/connection_impl.cc:539:5 (http2_integration_test+0x57bc310)\r\n2020-06-10T14:23:40.3669981Z     #31 Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6::operator()(unsigned int) const /proc/self/cwd/source/common/network/connection_impl.cc:77:74 (http2_integration_test+0x57d2157)\r\n2020-06-10T14:23:40.3673640Z     #32 decltype(std::__1::forward<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&>(fp)(std::__1::forward<unsigned int>(fp0))) std::__1::__invoke<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&, unsigned int&&) /opt/llvm/bin/../include/c++/v1/type_traits:3530:1 (http2_integration_test+0x57d209c)\r\n2020-06-10T14:23:40.3678431Z     #33 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&, unsigned int&&) /opt/llvm/bin/../include/c++/v1/__functional_base:348:9 (http2_integration_test+0x57d1fcd)\r\n2020-06-10T14:23:40.3682690Z     #34 std::__1::__function::__alloc_func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6>, void (unsigned int)>::operator()(unsigned int&&) /opt/llvm/bin/../include/c++/v1/functional:1533:16 (http2_integration_test+0x57d1f4d)\r\n2020-06-10T14:23:40.3686283Z     #35 std::__1::__function::__func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6>, void (unsigned int)>::operator()(unsigned int&&) /opt/llvm/bin/../include/c++/v1/functional:1707:12 (http2_integration_test+0x57d01fc)\r\n2020-06-10T14:23:40.3689215Z     #36 std::__1::__function::__value_func<void (unsigned int)>::operator()(unsigned int&&) const /opt/llvm/bin/../include/c++/v1/functional:1860:16 (http2_integration_test+0x47a88f4)\r\n2020-06-10T14:23:40.3690155Z     #37 std::__1::function<void (unsigned int)>::operator()(unsigned int) const /opt/llvm/bin/../include/c++/v1/functional:2419:12 (http2_integration_test+0x47a87e8)\r\n2020-06-10T14:23:40.3691548Z     #38 Envoy::Event::FileEventImpl::assignEvents(unsigned int, event_base*)::$_0::operator()(int, short, void*) const /proc/self/cwd/source/common/event/file_event_impl.cc:66:9 (http2_integration_test+0x57a6d44)\r\n2020-06-10T14:23:40.3692492Z     #39 Envoy::Event::FileEventImpl::assignEvents(unsigned int, event_base*)::$_0::__invoke(int, short, void*) /proc/self/cwd/source/common/event/file_event_impl.cc:50:7 (http2_integration_test+0x57a6ac6)\r\n2020-06-10T14:23:40.3693266Z     #40 event_persist_closure /b/f/w/external/com_github_libevent_libevent/event.c:1639:9 (http2_integration_test+0x7a1a1f0)\r\n2020-06-10T14:23:40.3693901Z     #41 event_process_active_single_queue /b/f/w/external/com_github_libevent_libevent/event.c:1698:4 (http2_integration_test+0x7a18c18)\r\n2020-06-10T14:23:40.3694520Z     #42 event_process_active /b/f/w/external/com_github_libevent_libevent/event.c:1799:9 (http2_integration_test+0x7a0e458)\r\n2020-06-10T14:23:40.3695121Z     #43 event_base_loop /b/f/w/external/com_github_libevent_libevent/event.c:2041:12 (http2_integration_test+0x7a0c103)\r\n2020-06-10T14:23:40.3695841Z     #44 Envoy::Event::LibeventScheduler::run(Envoy::Event::Dispatcher::RunType) /proc/self/cwd/source/common/event/libevent_scheduler.cc:47:3 (http2_integration_test+0x5a8e9ef)\r\n2020-06-10T14:23:40.3696800Z     #45 Envoy::Event::DispatcherImpl::run(Envoy::Event::Dispatcher::RunType) /proc/self/cwd/source/common/event/dispatcher_impl.cc:203:19 (http2_integration_test+0x57926a1)\r\n2020-06-10T14:23:40.3697771Z     #46 Envoy::FakeUpstream::threadRoutine() /proc/self/cwd/test/integration/fake_upstream.cc:494:16 (http2_integration_test+0x2cc2174)\r\n2020-06-10T14:23:40.3699206Z     #47 Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12::operator()() const /proc/self/cwd/test/integration/fake_upstream.cc:457:67 (http2_integration_test+0x2ce7a5b)\r\n2020-06-10T14:23:40.3702670Z     #48 decltype(std::__1::forward<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&>(fp)()) std::__1::__invoke<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&>(Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&) /opt/llvm/bin/../include/c++/v1/type_traits:3530:1 (http2_integration_test+0x2ce79b0)\r\n2020-06-10T14:23:40.3706672Z     #49 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&>(Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&) /opt/llvm/bin/../include/c++/v1/__functional_base:348:9 (http2_integration_test+0x2ce7910)\r\n2020-06-10T14:23:40.3710891Z     #50 std::__1::__function::__alloc_func<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12, std::__1::allocator<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12>, void ()>::operator()() /opt/llvm/bin/../include/c++/v1/functional:1533:16 (http2_integration_test+0x2ce78b0)\r\n2020-06-10T14:23:40.3714212Z     #51 std::__1::__function::__func<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12, std::__1::allocator<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12>, void ()>::operator()() /opt/llvm/bin/../include/c++/v1/functional:1707:12 (http2_integration_test+0x2ce5b7f)\r\n2020-06-10T14:23:40.3716360Z     #52 std::__1::__function::__value_func<void ()>::operator()() const /opt/llvm/bin/../include/c++/v1/functional:1860:16 (http2_integration_test+0x2ca9936)\r\n2020-06-10T14:23:40.3717556Z     #53 std::__1::function<void ()>::operator()() const /opt/llvm/bin/../include/c++/v1/functional:2419:12 (http2_integration_test+0x2ca95c8)\r\n2020-06-10T14:23:40.3719134Z     #54 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&)::'lambda'(void*)::operator()(void*) const /proc/self/cwd/source/common/common/posix/thread_impl.cc:49:11 (http2_integration_test+0x79f283b)\r\n2020-06-10T14:23:40.3721653Z     #55 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&)::'lambda'(void*)::__invoke(void*) /proc/self/cwd/source/common/common/posix/thread_impl.cc:48:9 (http2_integration_test+0x79f27c8)\r\n2020-06-10T14:23:40.3722495Z \r\n2020-06-10T14:23:40.3722840Z   Previous write of size 8 at 0x0000087c1178 by thread T6:\r\n2020-06-10T14:23:40.3723969Z     #0 Envoy::InjectableSingleton<Envoy::Runtime::Loader>::clear() /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/singleton/_virtual_includes/threadsafe_singleton/common/singleton/threadsafe_singleton.h:67:33 (http2_integration_test+0x306208b)\r\n2020-06-10T14:23:40.3725427Z     #1 Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader>::~ScopedInjectableLoader() /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/singleton/_virtual_includes/threadsafe_singleton/common/singleton/threadsafe_singleton.h:81:31 (http2_integration_test+0x306203c)\r\n2020-06-10T14:23:40.3726794Z     #2 std::__1::default_delete<Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader> >::operator()(Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader>*) const /opt/llvm/bin/../include/c++/v1/memory:2338:5 (http2_integration_test+0x3061f76)\r\n2020-06-10T14:23:40.3728496Z     #3 std::__1::unique_ptr<Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader>, std::__1::default_delete<Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader> > >::reset(Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader>*) /opt/llvm/bin/../include/c++/v1/memory:2593:7 (http2_integration_test+0x3061ea0)\r\n2020-06-10T14:23:40.3729924Z     #4 std::__1::unique_ptr<Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader>, std::__1::default_delete<Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader> > >::~unique_ptr() /opt/llvm/bin/../include/c++/v1/memory:2547:19 (http2_integration_test+0x304412c)\r\n2020-06-10T14:23:40.3731085Z     #5 Envoy::Server::InstanceImpl::~InstanceImpl() /proc/self/cwd/source/server/server.cc:133:1 (http2_integration_test+0x3003e62)\r\n2020-06-10T14:23:40.3732502Z     #6 Envoy::IntegrationTestServerImpl::createAndRunEnvoyServer(Envoy::OptionsImpl&, Envoy::Event::TimeSystem&, std::__1::shared_ptr<Envoy::Network::Address::Instance const>, Envoy::ListenerHooks&, Envoy::Thread::BasicLockable&, Envoy::Server::ComponentFactory&, std::__1::unique_ptr<Envoy::Runtime::RandomGenerator, std::__1::default_delete<Envoy::Runtime::RandomGenerator> >&&, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >) /proc/self/cwd/test/integration/server.cc:221:3 (http2_integration_test+0x2e1899d)\r\n2020-06-10T14:23:40.3734402Z     #7 Envoy::IntegrationTestServer::threadRoutine(Envoy::Network::Address::IpVersion, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >) /proc/self/cwd/test/integration/server.cc:181:3 (http2_integration_test+0x2e180e3)\r\n2020-06-10T14:23:40.3736186Z     #8 Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0::operator()() const /proc/self/cwd/test/integration/server.cc:93:9 (http2_integration_test+0x2e1c2aa)\r\n2020-06-10T14:23:40.3739805Z     #9 decltype(std::__1::forward<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0&>(fp)()) std::__1::__invoke<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0&>(Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0&) /opt/llvm/bin/../include/c++/v1/type_traits:3530:1 (http2_integration_test+0x2e1c0e0)\r\n2020-06-10T14:23:40.3743979Z     #10 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0&>(Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0&) /opt/llvm/bin/../include/c++/v1/__functional_base:348:9 (http2_integration_test+0x2e1c040)\r\n2020-06-10T14:23:40.3747526Z     #11 std::__1::__function::__alloc_func<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0, std::__1::allocator<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0>, void ()>::operator()() /opt/llvm/bin/../include/c++/v1/functional:1533:16 (http2_integration_test+0x2e1bfe0)\r\n2020-06-10T14:23:40.3751144Z     #12 std::__1::__function::__func<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0, std::__1::allocator<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0>, void ()>::operator()() /opt/llvm/bin/../include/c++/v1/functional:1707:12 (http2_integration_test+0x2e1a6af)\r\n2020-06-10T14:23:40.3753283Z     #13 std::__1::__function::__value_func<void ()>::operator()() const /opt/llvm/bin/../include/c++/v1/functional:1860:16 (http2_integration_test+0x2ca9936)\r\n2020-06-10T14:23:40.3754101Z     #14 std::__1::function<void ()>::operator()() const /opt/llvm/bin/../include/c++/v1/functional:2419:12 (http2_integration_test+0x2ca95c8)\r\n2020-06-10T14:23:40.3755525Z     #15 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&)::'lambda'(void*)::operator()(void*) const /proc/self/cwd/source/common/common/posix/thread_impl.cc:49:11 (http2_integration_test+0x79f283b)\r\n2020-06-10T14:23:40.3757371Z     #16 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&)::'lambda'(void*)::__invoke(void*) /proc/self/cwd/source/common/common/posix/thread_impl.cc:48:9 (http2_integration_test+0x79f27c8)\r\n2020-06-10T14:23:40.3758154Z \r\n2020-06-10T14:23:40.3759022Z   Location is global 'Envoy::InjectableSingleton<Envoy::Runtime::Loader>::loader_' of size 8 at 0x0000087c1178 (http2_integration_test+0x0000087c1178)\r\n2020-06-10T14:23:40.3759572Z \r\n2020-06-10T14:23:40.3759966Z   Thread T3 (tid=1262, running) created by main thread at:\r\n2020-06-10T14:23:40.3761029Z     #0 pthread_create /tmp/final/llvm.src/projects/compiler-rt/lib/tsan/rtl/tsan_interceptors.cc:967:3 (http2_integration_test+0x297678b)\r\n2020-06-10T14:23:40.3762091Z     #1 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&) /proc/self/cwd/source/common/common/posix/thread_impl.cc:46:20 (http2_integration_test+0x79f1267)\r\n2020-06-10T14:23:40.3763588Z     #2 std::__1::__unique_if<Envoy::Thread::ThreadImplPosix>::__unique_single std::__1::make_unique<Envoy::Thread::ThreadImplPosix, std::__1::function<void ()>&, absl::optional<Envoy::Thread::Options> const&>(std::__1::function<void ()>&, absl::optional<Envoy::Thread::Options> const&) /opt/llvm/bin/../include/c++/v1/memory:3003:32 (http2_integration_test+0x79f0bbf)\r\n2020-06-10T14:23:40.3764966Z     #3 Envoy::Thread::ThreadFactoryImplPosix::createThread(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&) /proc/self/cwd/source/common/common/posix/thread_impl.cc:121:10 (http2_integration_test+0x79f07f3)\r\n2020-06-10T14:23:40.3766491Z     #4 Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool) /proc/self/cwd/test/integration/fake_upstream.cc:457:35 (http2_integration_test+0x2cc04fb)\r\n2020-06-10T14:23:40.3768560Z     #5 Envoy::FakeUpstream::FakeUpstream(std::__1::shared_ptr<Envoy::Network::Address::Instance const> const&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool, bool) /proc/self/cwd/test/integration/fake_upstream.cc:420:7 (http2_integration_test+0x2cc0bfd)\r\n2020-06-10T14:23:40.3769957Z     #6 Envoy::AutonomousUpstream::AutonomousUpstream(std::__1::shared_ptr<Envoy::Network::Address::Instance const> const&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool) /proc/self/cwd/./test/integration/autonomous_upstream.h:57:9 (http2_integration_test+0x2d7f802)\r\n2020-06-10T14:23:40.3771443Z     #7 Envoy::BaseIntegrationTest::createUpstreams() /proc/self/cwd/test/integration/integration.cc:314:40 (http2_integration_test+0x2d5f0e5)\r\n2020-06-10T14:23:40.3772178Z     #8 Envoy::BaseIntegrationTest::initialize() /proc/self/cwd/test/integration/integration.cc:305:3 (http2_integration_test+0x2d5e781)\r\n2020-06-10T14:23:40.3772912Z     #9 Envoy::Http2FloodMitigationTest::beginSession() /proc/self/cwd/test/integration/http2_integration_test.cc:1500:3 (http2_integration_test+0x2a163c4)\r\n2020-06-10T14:23:40.3773836Z     #10 Envoy::Http2FloodMitigationTest_Data_Test::TestBody() /proc/self/cwd/test/integration/http2_integration_test.cc:1625:3 (http2_integration_test+0x2a18d01)\r\n2020-06-10T14:23:40.3774834Z     #11 void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2433:10 (http2_integration_test+0x7b90d2c)\r\n2020-06-10T14:23:40.3776010Z     #12 void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2469:14 (http2_integration_test+0x7b7542e)\r\n2020-06-10T14:23:40.3776936Z     #13 testing::Test::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2508:5 (http2_integration_test+0x7b5a511)\r\n2020-06-10T14:23:40.3777866Z     #14 testing::TestInfo::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2684:11 (http2_integration_test+0x7b5b3b3)\r\n2020-06-10T14:23:40.3778653Z     #15 testing::TestSuite::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2816:28 (http2_integration_test+0x7b5bdea)\r\n2020-06-10T14:23:40.3779475Z     #16 testing::internal::UnitTestImpl::RunAllTests() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:5338:44 (http2_integration_test+0x7b6b58d)\r\n2020-06-10T14:23:40.3780780Z     #17 bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2433:10 (http2_integration_test+0x7b97cdc)\r\n2020-06-10T14:23:40.3782255Z     #18 bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2469:14 (http2_integration_test+0x7b792fe)\r\n2020-06-10T14:23:40.3783338Z     #19 testing::UnitTest::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:4925:10 (http2_integration_test+0x7b6ae4b)\r\n2020-06-10T14:23:40.3784041Z     #20 RUN_ALL_TESTS() /proc/self/cwd/external/com_google_googletest/googletest/include/gtest/gtest.h:2473:46 (http2_integration_test+0x56c3177)\r\n2020-06-10T14:23:40.3784861Z     #21 Envoy::TestRunner::RunTests(int, char**) /proc/self/cwd/test/test_runner.cc:134:10 (http2_integration_test+0x56c24ed)\r\n2020-06-10T14:23:40.3785473Z     #22 main /proc/self/cwd/test/main.cc:34:10 (http2_integration_test+0x56bfe91)\r\n2020-06-10T14:23:40.3785813Z \r\n2020-06-10T14:23:40.3786178Z   Thread T6 (tid=1263, running) created by main thread at:\r\n2020-06-10T14:23:40.3787327Z     #0 pthread_create /tmp/final/llvm.src/projects/compiler-rt/lib/tsan/rtl/tsan_interceptors.cc:967:3 (http2_integration_test+0x297678b)\r\n2020-06-10T14:23:40.3788513Z     #1 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&) /proc/self/cwd/source/common/common/posix/thread_impl.cc:46:20 (http2_integration_test+0x79f1267)\r\n2020-06-10T14:23:40.3789962Z     #2 std::__1::__unique_if<Envoy::Thread::ThreadImplPosix>::__unique_single std::__1::make_unique<Envoy::Thread::ThreadImplPosix, std::__1::function<void ()>&, absl::optional<Envoy::Thread::Options> const&>(std::__1::function<void ()>&, absl::optional<Envoy::Thread::Options> const&) /opt/llvm/bin/../include/c++/v1/memory:3003:32 (http2_integration_test+0x79f0bbf)\r\n2020-06-10T14:23:40.3791544Z     #3 Envoy::Thread::ThreadFactoryImplPosix::createThread(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&) /proc/self/cwd/source/common/common/posix/thread_impl.cc:121:10 (http2_integration_test+0x79f07f3)\r\n2020-06-10T14:23:40.3793374Z     #4 Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >) /proc/self/cwd/test/integration/server.cc:91:28 (http2_integration_test+0x2e16c02)\r\n2020-06-10T14:23:40.3795786Z     #5 Envoy::IntegrationTestServer::create(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Network::Address::IpVersion, std::__1::function<void (Envoy::IntegrationTestServer&)>, std::__1::function<void ()>, bool, Envoy::Event::TestTimeSystem&, Envoy::Api::Api&, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >, bool) /proc/self/cwd/test/integration/server.cc:67:11 (http2_integration_test+0x2e16671)\r\n2020-06-10T14:23:40.3798793Z     #6 Envoy::BaseIntegrationTest::createGeneratedApiTestServer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, Envoy::Server::FieldValidationConfig, bool) /proc/self/cwd/test/integration/integration.cc:457:18 (http2_integration_test+0x2d603a3)\r\n2020-06-10T14:23:40.3800519Z     #7 Envoy::BaseIntegrationTest::createEnvoy() /proc/self/cwd/test/integration/integration.cc:375:3 (http2_integration_test+0x2d5fed6)\r\n2020-06-10T14:23:40.3801396Z     #8 Envoy::BaseIntegrationTest::initialize() /proc/self/cwd/test/integration/integration.cc:307:3 (http2_integration_test+0x2d5e7b6)\r\n2020-06-10T14:23:40.3802267Z     #9 Envoy::Http2FloodMitigationTest::beginSession() /proc/self/cwd/test/integration/http2_integration_test.cc:1500:3 (http2_integration_test+0x2a163c4)\r\n2020-06-10T14:23:40.3804491Z     #10 Envoy::Http2FloodMitigationTest_Data_Test::TestBody() /proc/self/cwd/test/integration/http2_integration_test.cc:1625:3 (http2_integration_test+0x2a18d01)\r\n2020-06-10T14:23:40.3805688Z     #11 void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2433:10 (http2_integration_test+0x7b90d2c)\r\n2020-06-10T14:23:40.3807113Z     #12 void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2469:14 (http2_integration_test+0x7b7542e)\r\n2020-06-10T14:23:40.3808345Z     #13 testing::Test::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2508:5 (http2_integration_test+0x7b5a511)\r\n2020-06-10T14:23:40.3809129Z     #14 testing::TestInfo::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2684:11 (http2_integration_test+0x7b5b3b3)\r\n2020-06-10T14:23:40.3809904Z     #15 testing::TestSuite::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2816:28 (http2_integration_test+0x7b5bdea)\r\n2020-06-10T14:23:40.3810722Z     #16 testing::internal::UnitTestImpl::RunAllTests() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:5338:44 (http2_integration_test+0x7b6b58d)\r\n2020-06-10T14:23:40.3812025Z     #17 bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2433:10 (http2_integration_test+0x7b97cdc)\r\n2020-06-10T14:23:40.3813465Z     #18 bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2469:14 (http2_integration_test+0x7b792fe)\r\n2020-06-10T14:23:40.3814727Z     #19 testing::UnitTest::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:4925:10 (http2_integration_test+0x7b6ae4b)\r\n2020-06-10T14:23:40.3815474Z     #20 RUN_ALL_TESTS() /proc/self/cwd/external/com_google_googletest/googletest/include/gtest/gtest.h:2473:46 (http2_integration_test+0x56c3177)\r\n2020-06-10T14:23:40.3816190Z     #21 Envoy::TestRunner::RunTests(int, char**) /proc/self/cwd/test/test_runner.cc:134:10 (http2_integration_test+0x56c24ed)\r\n2020-06-10T14:23:40.3816953Z     #22 main /proc/self/cwd/test/main.cc:34:10 (http2_integration_test+0x56bfe91)\r\n2020-06-10T14:23:40.3817523Z \r\n2020-06-10T14:23:40.3818734Z SUMMARY: ThreadSanitizer: data race /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/singleton/_virtual_includes/threadsafe_singleton/common/singleton/threadsafe_singleton.h:60:36 in Envoy::InjectableSingleton<Envoy::Runtime::Loader>::getExisting()\r\n2020-06-10T14:23:40.3819561Z ==================\r\n2020-06-10T14:23:40.3819893Z ==================\r\n2020-06-10T14:23:40.3820571Z WARNING: ThreadSanitizer: data race (pid=15)\r\n2020-06-10T14:23:40.3821009Z   Write of size 8 at 0x0000087c1178 by thread T6:\r\n2020-06-10T14:23:40.3822163Z     #0 Envoy::InjectableSingleton<Envoy::Runtime::Loader>::clear() /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/singleton/_virtual_includes/threadsafe_singleton/common/singleton/threadsafe_singleton.h:67:33 (http2_integration_test+0x306208b)\r\n2020-06-10T14:23:40.3823701Z     #1 Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader>::~ScopedInjectableLoader() /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/singleton/_virtual_includes/threadsafe_singleton/common/singleton/threadsafe_singleton.h:81:31 (http2_integration_test+0x306203c)\r\n2020-06-10T14:23:40.3824997Z     #2 std::__1::default_delete<Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader> >::operator()(Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader>*) const /opt/llvm/bin/../include/c++/v1/memory:2338:5 (http2_integration_test+0x3061f76)\r\n2020-06-10T14:23:40.3826291Z     #3 std::__1::unique_ptr<Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader>, std::__1::default_delete<Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader> > >::reset(Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader>*) /opt/llvm/bin/../include/c++/v1/memory:2593:7 (http2_integration_test+0x3061ea0)\r\n2020-06-10T14:23:40.3827829Z     #4 std::__1::unique_ptr<Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader>, std::__1::default_delete<Envoy::ScopedInjectableLoader<Envoy::Runtime::Loader> > >::~unique_ptr() /opt/llvm/bin/../include/c++/v1/memory:2547:19 (http2_integration_test+0x304412c)\r\n2020-06-10T14:23:40.3828877Z     #5 Envoy::Server::InstanceImpl::~InstanceImpl() /proc/self/cwd/source/server/server.cc:133:1 (http2_integration_test+0x3003e62)\r\n2020-06-10T14:23:40.3830647Z     #6 Envoy::IntegrationTestServerImpl::createAndRunEnvoyServer(Envoy::OptionsImpl&, Envoy::Event::TimeSystem&, std::__1::shared_ptr<Envoy::Network::Address::Instance const>, Envoy::ListenerHooks&, Envoy::Thread::BasicLockable&, Envoy::Server::ComponentFactory&, std::__1::unique_ptr<Envoy::Runtime::RandomGenerator, std::__1::default_delete<Envoy::Runtime::RandomGenerator> >&&, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >) /proc/self/cwd/test/integration/server.cc:221:3 (http2_integration_test+0x2e1899d)\r\n2020-06-10T14:23:40.3832615Z     #7 Envoy::IntegrationTestServer::threadRoutine(Envoy::Network::Address::IpVersion, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >) /proc/self/cwd/test/integration/server.cc:181:3 (http2_integration_test+0x2e180e3)\r\n2020-06-10T14:23:40.3834355Z     #8 Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0::operator()() const /proc/self/cwd/test/integration/server.cc:93:9 (http2_integration_test+0x2e1c2aa)\r\n2020-06-10T14:23:40.3838246Z     #9 decltype(std::__1::forward<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0&>(fp)()) std::__1::__invoke<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0&>(Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0&) /opt/llvm/bin/../include/c++/v1/type_traits:3530:1 (http2_integration_test+0x2e1c0e0)\r\n2020-06-10T14:23:40.3843489Z     #10 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0&>(Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0&) /opt/llvm/bin/../include/c++/v1/__functional_base:348:9 (http2_integration_test+0x2e1c040)\r\n2020-06-10T14:23:40.3847325Z     #11 std::__1::__function::__alloc_func<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0, std::__1::allocator<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0>, void ()>::operator()() /opt/llvm/bin/../include/c++/v1/functional:1533:16 (http2_integration_test+0x2e1bfe0)\r\n2020-06-10T14:23:40.3851001Z     #12 std::__1::__function::__func<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0, std::__1::allocator<Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >)::$_0>, void ()>::operator()() /opt/llvm/bin/../include/c++/v1/functional:1707:12 (http2_integration_test+0x2e1a6af)\r\n2020-06-10T14:23:40.3853031Z     #13 std::__1::__function::__value_func<void ()>::operator()() const /opt/llvm/bin/../include/c++/v1/functional:1860:16 (http2_integration_test+0x2ca9936)\r\n2020-06-10T14:23:40.3853809Z     #14 std::__1::function<void ()>::operator()() const /opt/llvm/bin/../include/c++/v1/functional:2419:12 (http2_integration_test+0x2ca95c8)\r\n2020-06-10T14:23:40.3855246Z     #15 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&)::'lambda'(void*)::operator()(void*) const /proc/self/cwd/source/common/common/posix/thread_impl.cc:49:11 (http2_integration_test+0x79f283b)\r\n2020-06-10T14:23:40.3857053Z     #16 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&)::'lambda'(void*)::__invoke(void*) /proc/self/cwd/source/common/common/posix/thread_impl.cc:48:9 (http2_integration_test+0x79f27c8)\r\n2020-06-10T14:23:40.3857800Z \r\n2020-06-10T14:23:40.3858337Z   Previous read of size 8 at 0x0000087c1178 by thread T3:\r\n2020-06-10T14:23:40.3859773Z     #0 Envoy::InjectableSingleton<Envoy::Runtime::Loader>::getExisting() /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/singleton/_virtual_includes/threadsafe_singleton/common/singleton/threadsafe_singleton.h:60:36 (http2_integration_test+0x2e92f1f)\r\n2020-06-10T14:23:40.3861122Z     #1 Envoy::Runtime::getInteger(absl::string_view, unsigned long) /proc/self/cwd/source/common/runtime/runtime_features.cc:26:7 (http2_integration_test+0x5bcd09a)\r\n2020-06-10T14:23:40.3861955Z     #2 Envoy::Buffer::WatermarkBuffer::setWatermarks(unsigned int, unsigned int) /proc/self/cwd/source/common/buffer/watermark_buffer.cc:75:7 (http2_integration_test+0x57e752f)\r\n2020-06-10T14:23:40.3863316Z     #3 Envoy::Http::Http2::ConnectionImpl::StreamImpl::setWriteBufferWatermarks(unsigned int, unsigned int) /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/http/http2/_virtual_includes/codec_lib/common/http/http2/codec_impl.h:251:26 (http2_integration_test+0x55ee231)\r\n2020-06-10T14:23:40.3864573Z     #4 Envoy::Http::Http2::ConnectionImpl::StreamImpl::StreamImpl(Envoy::Http::Http2::ConnectionImpl&, unsigned int) /proc/self/cwd/source/common/http/http2/codec_impl.cc:99:5 (http2_integration_test+0x55c718f)\r\n2020-06-10T14:23:40.3866408Z     #5 Envoy::Http::Http2::ConnectionImpl::ServerStreamImpl::ServerStreamImpl(Envoy::Http::Http2::ConnectionImpl&, unsigned int) /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/http/http2/_virtual_includes/codec_lib/common/http/http2/codec_impl.h:359:11 (http2_integration_test+0x55f2bbc)\r\n2020-06-10T14:23:40.3867555Z     #6 Envoy::Http::Http2::ServerConnectionImpl::onBeginHeaders(nghttp2_frame const*) /proc/self/cwd/source/common/http/http2/codec_impl.cc:1256:34 (http2_integration_test+0x55d708e)\r\n2020-06-10T14:23:40.3868820Z     #7 non-virtual thunk to Envoy::Http::Http2::ServerConnectionImpl::onBeginHeaders(nghttp2_frame const*) /proc/self/cwd/source/common/http/http2/codec_impl.cc (http2_integration_test+0x55d746f)\r\n2020-06-10T14:23:40.3869884Z     #8 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_10::operator()(nghttp2_session*, nghttp2_frame const*, void*) const /proc/self/cwd/source/common/http/http2/codec_impl.cc:1037:57 (http2_integration_test+0x55da7cc)\r\n2020-06-10T14:23:40.3871005Z     #9 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_10::__invoke(nghttp2_session*, nghttp2_frame const*, void*) /proc/self/cwd/source/common/http/http2/codec_impl.cc:1036:19 (http2_integration_test+0x55da728)\r\n2020-06-10T14:23:40.3871931Z     #10 session_call_on_begin_headers /b/f/w/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:3314:10 (http2_integration_test+0x5cdaad0)\r\n2020-06-10T14:23:40.3872680Z     #11 nghttp2_session_on_request_headers_received /b/f/w/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:3915:8 (http2_integration_test+0x5cda63b)\r\n2020-06-10T14:23:40.3873447Z     #12 session_process_headers_frame /b/f/w/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:4059:12 (http2_integration_test+0x5ce94a0)\r\n2020-06-10T14:23:40.3874189Z     #13 nghttp2_session_mem_recv /b/f/w/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:5570:14 (http2_integration_test+0x5ce0df3)\r\n2020-06-10T14:23:40.3875014Z     #14 Envoy::Http::Http2::ConnectionImpl::innerDispatch(Envoy::Buffer::Instance&) /proc/self/cwd/source/common/http/http2/codec_impl.cc:495:9 (http2_integration_test+0x55cf25f)\r\n2020-06-10T14:23:40.3876172Z     #15 Envoy::Http::Http2::ServerConnectionImpl::innerDispatch(Envoy::Buffer::Instance&) /proc/self/cwd/source/common/http/http2/codec_impl.cc:1382:26 (http2_integration_test+0x55d9424)\r\n2020-06-10T14:23:40.3877180Z     #16 Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23::operator()(Envoy::Buffer::Instance&) const /proc/self/cwd/source/common/http/http2/codec_impl.cc:1368:60 (http2_integration_test+0x55eb284)\r\n2020-06-10T14:23:40.3878807Z     #17 decltype(std::__1::forward<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23&>(fp)(std::__1::forward<Envoy::Buffer::Instance&>(fp0))) std::__1::__invoke<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23&, Envoy::Buffer::Instance&>(Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23&, Envoy::Buffer::Instance&) /opt/llvm/bin/../include/c++/v1/type_traits:3530:1 (http2_integration_test+0x55eb194)\r\n2020-06-10T14:23:40.3880788Z     #18 absl::Status std::__1::__invoke_void_return_wrapper<absl::Status>::__call<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23&, Envoy::Buffer::Instance&>(Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23&, Envoy::Buffer::Instance&) /opt/llvm/bin/../include/c++/v1/__functional_base:317:16 (http2_integration_test+0x55eb0c4)\r\n2020-06-10T14:23:40.3882722Z     #19 std::__1::__function::__alloc_func<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23, std::__1::allocator<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23>, absl::Status (Envoy::Buffer::Instance&)>::operator()(Envoy::Buffer::Instance&) /opt/llvm/bin/../include/c++/v1/functional:1533:16 (http2_integration_test+0x55eb034)\r\n2020-06-10T14:23:40.3884610Z     #20 std::__1::__function::__func<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23, std::__1::allocator<Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&)::$_23>, absl::Status (Envoy::Buffer::Instance&)>::operator()(Envoy::Buffer::Instance&) /opt/llvm/bin/../include/c++/v1/functional:1707:12 (http2_integration_test+0x55e92d3)\r\n2020-06-10T14:23:40.3886010Z     #21 std::__1::__function::__value_func<absl::Status (Envoy::Buffer::Instance&)>::operator()(Envoy::Buffer::Instance&) const /opt/llvm/bin/../include/c++/v1/functional:1860:16 (http2_integration_test+0x47740eb)\r\n2020-06-10T14:23:40.3887011Z     #22 std::__1::function<absl::Status (Envoy::Buffer::Instance&)>::operator()(Envoy::Buffer::Instance&) const /opt/llvm/bin/../include/c++/v1/functional:2419:12 (http2_integration_test+0x4773fff)\r\n2020-06-10T14:23:40.3888485Z     #23 Envoy::Http::Utility::exceptionToStatus(std::__1::function<absl::Status (Envoy::Buffer::Instance&)>, Envoy::Buffer::Instance&) /proc/self/cwd/source/common/http/utility.cc:41:14 (http2_integration_test+0x5bce840)\r\n2020-06-10T14:23:40.3889534Z     #24 Envoy::Http::Http2::ServerConnectionImpl::dispatch(Envoy::Buffer::Instance&) /proc/self/cwd/source/common/http/http2/codec_impl.cc:1367:10 (http2_integration_test+0x55d8ed2)\r\n2020-06-10T14:23:40.3890490Z     #25 Envoy::FakeHttpConnection::ReadFilter::onData(Envoy::Buffer::Instance&, bool) /proc/self/cwd/./test/integration/fake_upstream.h:453:45 (http2_integration_test+0x2cffbc9)\r\n2020-06-10T14:23:40.3891701Z     #26 Envoy::Network::FilterManagerImpl::onContinueReading(Envoy::Network::FilterManagerImpl::ActiveReadFilter*, Envoy::Network::ReadBufferSource&) /proc/self/cwd/source/common/network/filter_manager_impl.cc:66:48 (http2_integration_test+0x57e0358)\r\n2020-06-10T14:23:40.3892676Z     #27 Envoy::Network::FilterManagerImpl::onRead() /proc/self/cwd/source/common/network/filter_manager_impl.cc:76:3 (http2_integration_test+0x57e0651)\r\n2020-06-10T14:23:40.3893463Z     #28 Envoy::Network::ConnectionImpl::onRead(unsigned long) /proc/self/cwd/source/common/network/connection_impl.cc:297:19 (http2_integration_test+0x57b8bb2)\r\n2020-06-10T14:23:40.3894243Z     #29 Envoy::Network::ConnectionImpl::onReadReady() /proc/self/cwd/source/common/network/connection_impl.cc:579:5 (http2_integration_test+0x57bddc0)\r\n2020-06-10T14:23:40.3895126Z     #30 Envoy::Network::ConnectionImpl::onFileEvent(unsigned int) /proc/self/cwd/source/common/network/connection_impl.cc:539:5 (http2_integration_test+0x57bc310)\r\n2020-06-10T14:23:40.3896559Z     #31 Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6::operator()(unsigned int) const /proc/self/cwd/source/common/network/connection_impl.cc:77:74 (http2_integration_test+0x57d2157)\r\n2020-06-10T14:23:40.3900945Z     #32 decltype(std::__1::forward<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&>(fp)(std::__1::forward<unsigned int>(fp0))) std::__1::__invoke<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&, unsigned int&&) /opt/llvm/bin/../include/c++/v1/type_traits:3530:1 (http2_integration_test+0x57d209c)\r\n2020-06-10T14:23:40.3905525Z     #33 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6&, unsigned int&&) /opt/llvm/bin/../include/c++/v1/__functional_base:348:9 (http2_integration_test+0x57d1fcd)\r\n2020-06-10T14:23:40.3909945Z     #34 std::__1::__function::__alloc_func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6>, void (unsigned int)>::operator()(unsigned int&&) /opt/llvm/bin/../include/c++/v1/functional:1533:16 (http2_integration_test+0x57d1f4d)\r\n2020-06-10T14:23:40.3914067Z     #35 std::__1::__function::__func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, Envoy::StreamInfo::StreamInfo&, bool)::$_6>, void (unsigned int)>::operator()(unsigned int&&) /opt/llvm/bin/../include/c++/v1/functional:1707:12 (http2_integration_test+0x57d01fc)\r\n2020-06-10T14:23:40.3916486Z     #36 std::__1::__function::__value_func<void (unsigned int)>::operator()(unsigned int&&) const /opt/llvm/bin/../include/c++/v1/functional:1860:16 (http2_integration_test+0x47a88f4)\r\n2020-06-10T14:23:40.3917579Z     #37 std::__1::function<void (unsigned int)>::operator()(unsigned int) const /opt/llvm/bin/../include/c++/v1/functional:2419:12 (http2_integration_test+0x47a87e8)\r\n2020-06-10T14:23:40.3918560Z     #38 Envoy::Event::FileEventImpl::assignEvents(unsigned int, event_base*)::$_0::operator()(int, short, void*) const /proc/self/cwd/source/common/event/file_event_impl.cc:66:9 (http2_integration_test+0x57a6d44)\r\n2020-06-10T14:23:40.3919629Z     #39 Envoy::Event::FileEventImpl::assignEvents(unsigned int, event_base*)::$_0::__invoke(int, short, void*) /proc/self/cwd/source/common/event/file_event_impl.cc:50:7 (http2_integration_test+0x57a6ac6)\r\n2020-06-10T14:23:40.3920939Z     #40 event_persist_closure /b/f/w/external/com_github_libevent_libevent/event.c:1639:9 (http2_integration_test+0x7a1a1f0)\r\n2020-06-10T14:23:40.3921711Z     #41 event_process_active_single_queue /b/f/w/external/com_github_libevent_libevent/event.c:1698:4 (http2_integration_test+0x7a18c18)\r\n2020-06-10T14:23:40.3922415Z     #42 event_process_active /b/f/w/external/com_github_libevent_libevent/event.c:1799:9 (http2_integration_test+0x7a0e458)\r\n2020-06-10T14:23:40.3923062Z     #43 event_base_loop /b/f/w/external/com_github_libevent_libevent/event.c:2041:12 (http2_integration_test+0x7a0c103)\r\n2020-06-10T14:23:40.3923827Z     #44 Envoy::Event::LibeventScheduler::run(Envoy::Event::Dispatcher::RunType) /proc/self/cwd/source/common/event/libevent_scheduler.cc:47:3 (http2_integration_test+0x5a8e9ef)\r\n2020-06-10T14:23:40.3924692Z     #45 Envoy::Event::DispatcherImpl::run(Envoy::Event::Dispatcher::RunType) /proc/self/cwd/source/common/event/dispatcher_impl.cc:203:19 (http2_integration_test+0x57926a1)\r\n2020-06-10T14:23:40.3925488Z     #46 Envoy::FakeUpstream::threadRoutine() /proc/self/cwd/test/integration/fake_upstream.cc:494:16 (http2_integration_test+0x2cc2174)\r\n2020-06-10T14:23:40.3926834Z     #47 Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12::operator()() const /proc/self/cwd/test/integration/fake_upstream.cc:457:67 (http2_integration_test+0x2ce7a5b)\r\n2020-06-10T14:23:40.3930919Z     #48 decltype(std::__1::forward<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&>(fp)()) std::__1::__invoke<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&>(Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&) /opt/llvm/bin/../include/c++/v1/type_traits:3530:1 (http2_integration_test+0x2ce79b0)\r\n2020-06-10T14:23:40.3935319Z     #49 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&>(Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12&) /opt/llvm/bin/../include/c++/v1/__functional_base:348:9 (http2_integration_test+0x2ce7910)\r\n2020-06-10T14:23:40.3939469Z     #50 std::__1::__function::__alloc_func<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12, std::__1::allocator<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12>, void ()>::operator()() /opt/llvm/bin/../include/c++/v1/functional:1533:16 (http2_integration_test+0x2ce78b0)\r\n2020-06-10T14:23:40.3943412Z     #51 std::__1::__function::__func<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12, std::__1::allocator<Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool)::$_12>, void ()>::operator()() /opt/llvm/bin/../include/c++/v1/functional:1707:12 (http2_integration_test+0x2ce5b7f)\r\n2020-06-10T14:23:40.3945531Z     #52 std::__1::__function::__value_func<void ()>::operator()() const /opt/llvm/bin/../include/c++/v1/functional:1860:16 (http2_integration_test+0x2ca9936)\r\n2020-06-10T14:23:40.3946276Z     #53 std::__1::function<void ()>::operator()() const /opt/llvm/bin/../include/c++/v1/functional:2419:12 (http2_integration_test+0x2ca95c8)\r\n2020-06-10T14:23:40.3948110Z     #54 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&)::'lambda'(void*)::operator()(void*) const /proc/self/cwd/source/common/common/posix/thread_impl.cc:49:11 (http2_integration_test+0x79f283b)\r\n2020-06-10T14:23:40.3949902Z     #55 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&)::'lambda'(void*)::__invoke(void*) /proc/self/cwd/source/common/common/posix/thread_impl.cc:48:9 (http2_integration_test+0x79f27c8)\r\n2020-06-10T14:23:40.3950943Z \r\n2020-06-10T14:23:40.3951763Z   Location is global 'Envoy::InjectableSingleton<Envoy::Runtime::Loader>::loader_' of size 8 at 0x0000087c1178 (http2_integration_test+0x0000087c1178)\r\n2020-06-10T14:23:40.3952294Z \r\n2020-06-10T14:23:40.3952647Z   Thread T6 (tid=1263, running) created by main thread at:\r\n2020-06-10T14:23:40.3953519Z     #0 pthread_create /tmp/final/llvm.src/projects/compiler-rt/lib/tsan/rtl/tsan_interceptors.cc:967:3 (http2_integration_test+0x297678b)\r\n2020-06-10T14:23:40.3954437Z     #1 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&) /proc/self/cwd/source/common/common/posix/thread_impl.cc:46:20 (http2_integration_test+0x79f1267)\r\n2020-06-10T14:23:40.3955772Z     #2 std::__1::__unique_if<Envoy::Thread::ThreadImplPosix>::__unique_single std::__1::make_unique<Envoy::Thread::ThreadImplPosix, std::__1::function<void ()>&, absl::optional<Envoy::Thread::Options> const&>(std::__1::function<void ()>&, absl::optional<Envoy::Thread::Options> const&) /opt/llvm/bin/../include/c++/v1/memory:3003:32 (http2_integration_test+0x79f0bbf)\r\n2020-06-10T14:23:40.3957524Z     #3 Envoy::Thread::ThreadFactoryImplPosix::createThread(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&) /proc/self/cwd/source/common/common/posix/thread_impl.cc:121:10 (http2_integration_test+0x79f07f3)\r\n2020-06-10T14:23:40.3959781Z     #4 Envoy::IntegrationTestServer::start(Envoy::Network::Address::IpVersion, std::__1::function<void ()>, bool, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >) /proc/self/cwd/test/integration/server.cc:91:28 (http2_integration_test+0x2e16c02)\r\n2020-06-10T14:23:40.3962555Z     #5 Envoy::IntegrationTestServer::create(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Network::Address::IpVersion, std::__1::function<void (Envoy::IntegrationTestServer&)>, std::__1::function<void ()>, bool, Envoy::Event::TestTimeSystem&, Envoy::Api::Api&, bool, absl::optional<std::__1::reference_wrapper<Envoy::ProcessObject> >, Envoy::Server::FieldValidationConfig, unsigned int, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1l> >, bool) /proc/self/cwd/test/integration/server.cc:67:11 (http2_integration_test+0x2e16671)\r\n2020-06-10T14:23:40.3965308Z     #6 Envoy::BaseIntegrationTest::createGeneratedApiTestServer(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, Envoy::Server::FieldValidationConfig, bool) /proc/self/cwd/test/integration/integration.cc:457:18 (http2_integration_test+0x2d603a3)\r\n2020-06-10T14:23:40.3967161Z     #7 Envoy::BaseIntegrationTest::createEnvoy() /proc/self/cwd/test/integration/integration.cc:375:3 (http2_integration_test+0x2d5fed6)\r\n2020-06-10T14:23:40.3968103Z     #8 Envoy::BaseIntegrationTest::initialize() /proc/self/cwd/test/integration/integration.cc:307:3 (http2_integration_test+0x2d5e7b6)\r\n2020-06-10T14:23:40.3968889Z     #9 Envoy::Http2FloodMitigationTest::beginSession() /proc/self/cwd/test/integration/http2_integration_test.cc:1500:3 (http2_integration_test+0x2a163c4)\r\n2020-06-10T14:23:40.3969755Z     #10 Envoy::Http2FloodMitigationTest_Data_Test::TestBody() /proc/self/cwd/test/integration/http2_integration_test.cc:1625:3 (http2_integration_test+0x2a18d01)\r\n2020-06-10T14:23:40.3970991Z     #11 void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2433:10 (http2_integration_test+0x7b90d2c)\r\n2020-06-10T14:23:40.3972302Z     #12 void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2469:14 (http2_integration_test+0x7b7542e)\r\n2020-06-10T14:23:40.3973572Z     #13 testing::Test::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2508:5 (http2_integration_test+0x7b5a511)\r\n2020-06-10T14:23:40.3974355Z     #14 testing::TestInfo::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2684:11 (http2_integration_test+0x7b5b3b3)\r\n2020-06-10T14:23:40.3975144Z     #15 testing::TestSuite::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2816:28 (http2_integration_test+0x7b5bdea)\r\n2020-06-10T14:23:40.3976301Z     #16 testing::internal::UnitTestImpl::RunAllTests() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:5338:44 (http2_integration_test+0x7b6b58d)\r\n2020-06-10T14:23:40.3977521Z     #17 bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2433:10 (http2_integration_test+0x7b97cdc)\r\n2020-06-10T14:23:40.3979397Z     #18 bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2469:14 (http2_integration_test+0x7b792fe)\r\n2020-06-10T14:23:40.3980549Z     #19 testing::UnitTest::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:4925:10 (http2_integration_test+0x7b6ae4b)\r\n2020-06-10T14:23:40.3981619Z     #20 RUN_ALL_TESTS() /proc/self/cwd/external/com_google_googletest/googletest/include/gtest/gtest.h:2473:46 (http2_integration_test+0x56c3177)\r\n2020-06-10T14:23:40.3982295Z     #21 Envoy::TestRunner::RunTests(int, char**) /proc/self/cwd/test/test_runner.cc:134:10 (http2_integration_test+0x56c24ed)\r\n2020-06-10T14:23:40.3983253Z     #22 main /proc/self/cwd/test/main.cc:34:10 (http2_integration_test+0x56bfe91)\r\n2020-06-10T14:23:40.3983620Z \r\n2020-06-10T14:23:40.3984010Z   Thread T3 (tid=1262, running) created by main thread at:\r\n2020-06-10T14:23:40.3985032Z     #0 pthread_create /tmp/final/llvm.src/projects/compiler-rt/lib/tsan/rtl/tsan_interceptors.cc:967:3 (http2_integration_test+0x297678b)\r\n2020-06-10T14:23:40.3986178Z     #1 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&) /proc/self/cwd/source/common/common/posix/thread_impl.cc:46:20 (http2_integration_test+0x79f1267)\r\n2020-06-10T14:23:40.3987998Z     #2 std::__1::__unique_if<Envoy::Thread::ThreadImplPosix>::__unique_single std::__1::make_unique<Envoy::Thread::ThreadImplPosix, std::__1::function<void ()>&, absl::optional<Envoy::Thread::Options> const&>(std::__1::function<void ()>&, absl::optional<Envoy::Thread::Options> const&) /opt/llvm/bin/../include/c++/v1/memory:3003:32 (http2_integration_test+0x79f0bbf)\r\n2020-06-10T14:23:40.3989478Z     #3 Envoy::Thread::ThreadFactoryImplPosix::createThread(std::__1::function<void ()>, absl::optional<Envoy::Thread::Options> const&) /proc/self/cwd/source/common/common/posix/thread_impl.cc:121:10 (http2_integration_test+0x79f07f3)\r\n2020-06-10T14:23:40.3991165Z     #4 Envoy::FakeUpstream::FakeUpstream(std::__1::unique_ptr<Envoy::Network::TransportSocketFactory, std::__1::default_delete<Envoy::Network::TransportSocketFactory> >&&, std::__1::unique_ptr<Envoy::Network::Socket, std::__1::default_delete<Envoy::Network::Socket> >&&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool) /proc/self/cwd/test/integration/fake_upstream.cc:457:35 (http2_integration_test+0x2cc04fb)\r\n2020-06-10T14:23:40.3992953Z     #5 Envoy::FakeUpstream::FakeUpstream(std::__1::shared_ptr<Envoy::Network::Address::Instance const> const&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool, bool) /proc/self/cwd/test/integration/fake_upstream.cc:420:7 (http2_integration_test+0x2cc0bfd)\r\n2020-06-10T14:23:40.3994483Z     #6 Envoy::AutonomousUpstream::AutonomousUpstream(std::__1::shared_ptr<Envoy::Network::Address::Instance const> const&, Envoy::FakeHttpConnection::Type, Envoy::Event::TestTimeSystem&, bool) /proc/self/cwd/./test/integration/autonomous_upstream.h:57:9 (http2_integration_test+0x2d7f802)\r\n2020-06-10T14:23:40.3995785Z     #7 Envoy::BaseIntegrationTest::createUpstreams() /proc/self/cwd/test/integration/integration.cc:314:40 (http2_integration_test+0x2d5f0e5)\r\n2020-06-10T14:23:40.3996678Z     #8 Envoy::BaseIntegrationTest::initialize() /proc/self/cwd/test/integration/integration.cc:305:3 (http2_integration_test+0x2d5e781)\r\n2020-06-10T14:23:40.3997434Z     #9 Envoy::Http2FloodMitigationTest::beginSession() /proc/self/cwd/test/integration/http2_integration_test.cc:1500:3 (http2_integration_test+0x2a163c4)\r\n2020-06-10T14:23:40.3998491Z     #10 Envoy::Http2FloodMitigationTest_Data_Test::TestBody() /proc/self/cwd/test/integration/http2_integration_test.cc:1625:3 (http2_integration_test+0x2a18d01)\r\n2020-06-10T14:23:40.3999543Z     #11 void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2433:10 (http2_integration_test+0x7b90d2c)\r\n2020-06-10T14:23:40.4000952Z     #12 void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2469:14 (http2_integration_test+0x7b7542e)\r\n2020-06-10T14:23:40.4002217Z     #13 testing::Test::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2508:5 (http2_integration_test+0x7b5a511)\r\n2020-06-10T14:23:40.4003196Z     #14 testing::TestInfo::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2684:11 (http2_integration_test+0x7b5b3b3)\r\n2020-06-10T14:23:40.4003991Z     #15 testing::TestSuite::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2816:28 (http2_integration_test+0x7b5bdea)\r\n2020-06-10T14:23:40.4004801Z     #16 testing::internal::UnitTestImpl::RunAllTests() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:5338:44 (http2_integration_test+0x7b6b58d)\r\n2020-06-10T14:23:40.4006142Z     #17 bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2433:10 (http2_integration_test+0x7b97cdc)\r\n2020-06-10T14:23:40.4007587Z     #18 bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2469:14 (http2_integration_test+0x7b792fe)\r\n2020-06-10T14:23:40.4008921Z     #19 testing::UnitTest::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:4925:10 (http2_integration_test+0x7b6ae4b)\r\n2020-06-10T14:23:40.4009692Z     #20 RUN_ALL_TESTS() /proc/self/cwd/external/com_google_googletest/googletest/include/gtest/gtest.h:2473:46 (http2_integration_test+0x56c3177)\r\n2020-06-10T14:23:40.4010430Z     #21 Envoy::TestRunner::RunTests(int, char**) /proc/self/cwd/test/test_runner.cc:134:10 (http2_integration_test+0x56c24ed)\r\n2020-06-10T14:23:40.4011222Z     #22 main /proc/self/cwd/test/main.cc:34:10 (http2_integration_test+0x56bfe91)\r\n2020-06-10T14:23:40.4011583Z \r\n2020-06-10T14:23:40.4012724Z SUMMARY: ThreadSanitizer: data race /proc/self/cwd/bazel-out/k8-dbg/bin/source/common/singleton/_virtual_includes/threadsafe_singleton/common/singleton/threadsafe_singleton.h:67:33 in Envoy::InjectableSingleton<Envoy::Runtime::Loader>::clear()\r\n2020-06-10T14:23:40.4013696Z ==================\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11538/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-06-10T15:06:54Z",
        "body": "cc @alyssawilk @yanavlasov "
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-06-10T15:23:22Z",
        "body": "Looks like the recent watermark change.  I think the runtime code *should* be safe to use the way it was used - I'll take a look this morning and see if I can sort it out."
      }
    ]
  },
  {
    "number": 11388,
    "title": "[watchdog] Provide additional watchdog actions and/or extension points",
    "created_at": "2020-06-01T19:32:31Z",
    "closed_at": "2020-08-13T17:11:48Z",
    "labels": [
      "help wanted",
      "area/watchdog"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11388",
    "body": "The thread watchdog is already an important mechanism to detect and recover from coding errors that results in infinite loops, blocking API calls and very long computations in worker threads.  There are a few simple improvements that would make the watchdog even more awesome:\r\n- Option to capture a 5sec to 10sec CPU profile after a series of watchdog misses or mega misses, and either write it to disk or make it available via admin interface.  If writing to disk, provide parameter for max number of profiles to generate to avoid filling up the disk.\r\n- Option to capture and log the current stack of the watched thread or all thread stacks on mega miss.\r\n- Option to terminate the process by sending SIGABRT to the stuck thread instead calling PANIC on the guarddog thread.\r\n- Registration mechanism for additional callbacks to invoke on watchdog miss or megamiss which could be used to implement some of the prior ideas and/or integrate with third party systems.  Callback arguments may include the list of threads that have experienced recent megamiss events and info about when they were last reported alive.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11388/comments",
    "author": "antoniovicente",
    "comments": [
      {
        "user": "KBaichoo",
        "created_at": "2020-07-15T20:43:03Z",
        "body": "/assign @KBaichoo "
      },
      {
        "user": "KBaichoo",
        "created_at": "2020-07-17T19:05:43Z",
        "body": "For a general mechanism I'm proposing the following changes:\r\n\r\nBootstrap.proto\r\n\r\n```\r\nMessage WatchDog {\r\n        Enum WatchDogEvent {\r\n                Abort\r\n                Miss\r\n                MegaMiss\r\n        }\r\n        \r\n        Message WatchDogActions {\r\n                String name;\r\n                WatchDogEvent evt_type;\r\n        }\r\n\r\n        repeated WatchDogActions actions;\r\n}\r\n```\r\n\r\nIn GuardDogImpl:\r\n- Step() function needs to have vectors that’ll hold the (tid, last time touch) for the different miss categories\r\n    - 3 vectors:\r\n        - Miss\r\n        - Mega Miss\r\n        - MultiKill\r\n    - Have the miss / megamiss callbacks occur outside the critical section, and the multi kill / kill inside\r\n- Need to have access to the registry mechanism and then create a mapping from ‘Event Type’: [Array of Callbacks]\r\n\r\nThe callbacks will have the following signature:\r\n`void Function(WatchDogEvent event, std::vector<std::pair<Thread::ThreadID, MonotonicTime>> tid_ltt_pairs, MonotonicTime now)`"
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-07-17T19:14:08Z",
        "body": "You may want to have the events be: MultiKill, Kill, Miss and Megamiss to match the current actions.\r\n\r\nGetting the full list of threads that are in a miss/megamiss state seems very useful.  Similarly, list of threads involved in the Kill or MultiKill event seems useful and would be consistent with the API used for miss/megamiss, even if the kind of operations I expect we would run on Kill/MultiKill possibly not requiring the thread id information."
      },
      {
        "user": "KBaichoo",
        "created_at": "2020-07-17T19:21:53Z",
        "body": "Good Idea, that would add more granularity and be more consistent than Abort."
      },
      {
        "user": "KBaichoo",
        "created_at": "2020-07-20T16:30:41Z",
        "body": "IIUC, the best way to provide the 'extensions' (for particular watchdog events) is via Factories and using utilities such as `GetAndCheckFactory` to then get the callbacks.\r\n\r\nI don't see a class that derives from `TypedFactory` that best suits WatchDog, so I'll create a subclass off of that which all specific factories will derive from similar to `NamedNetworkFilterConfigFactory`."
      },
      {
        "user": "antoniovicente",
        "created_at": "2020-07-20T16:38:25Z",
        "body": "@envoyproxy/api-shepherds for input"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-07-20T18:05:41Z",
        "body": "+1 to using an extension/typed_config interface if we think that we will want this to be extensible in the future with different actions/events."
      },
      {
        "user": "KBaichoo",
        "created_at": "2020-07-31T19:41:02Z",
        "body": "I've turned the Extension PR from a draft into an actual PR. Since it was getting quite big I've decided to implement one of the extensions in another PR.\r\n\r\nFor implementing CPU profiling based on WatchDogEvents:\r\n\r\n- Add WD’s dispatcher ref to the context that can be used by WDActions to plan callbacks\r\n    - We’ll be able to trigger the action from the GuardDog’s `step()` which will StartProfiling, and can then schedule the `stop()` functionality on the GuardDog’s dispatcher. We need to schedule the `stop()` there as it’s possible we don’t invoke the action while profiling, and thus might not stop profiling.\r\n\r\n- A proto for the configuration for the WD action:\r\n    - `Profile durations` -- duration on how long to run the profile for\r\n    - `Profile_path` -- path to file to output the profiles on\r\n    - `Max_profiles_per_thread` -- limits max number of profiles we’ll generate for a given thread to avoid filling disks\r\n- Implement a WatchDogProfileAction and Factory\r\n    - Action tracks whether it’s running a profile current, as well as tids associated with profiles and counts\r\n"
      }
    ]
  },
  {
    "number": 11141,
    "title": "Add dynamic metadata kafka",
    "created_at": "2020-05-11T13:41:51Z",
    "closed_at": "2020-05-20T13:10:22Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/kafka"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11141",
    "body": "Title: Add dynamic metadata\r\nDescription:\r\n\r\n    i would like to use RBAC filter or ext_authz to authorize kafka actions",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11141/comments",
    "author": "p53",
    "comments": [
      {
        "user": "dio",
        "created_at": "2020-05-12T02:11:58Z",
        "body": "@p53 do you want to implement it? Will wait for your PR 🙂 \r\n\r\ncc. @adamkotwasinski "
      },
      {
        "user": "p53",
        "created_at": "2020-05-12T06:13:30Z",
        "body": "i would like to, but i don't have any experience with C++"
      }
    ]
  },
  {
    "number": 11038,
    "title": "Teach transcoding filter to be aware of per vhost / per route settings ",
    "created_at": "2020-05-01T23:49:31Z",
    "closed_at": "2021-01-29T19:59:33Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11038",
    "body": "Currently `envoy.filters.http.grpc_json_transcoder` is only applied on the 'global' level, meaning that it's not possible / easy to setup envoy's  configuration for the following scenarios:\r\n- having two routes for two different domains configured with the same path value (assuming they use transcoding), example: say you have domain1.com/foo and domain2.com/foo and want requests to each of them be transcoded and then be forwarded to different upstreams\r\n- using RDS with transcoded paths\r\n- gracefully migrating from a setting that does not use transcoding to a setting that does (or the other way round), example: having a path that accepts some json in request and currently upstream is responsible for deserialization and being in the process to migrate over to a proto-based solution and leveraging transcoding plugin, for the time of the migration part of the traffic would go directly to the upstream, while the other part would be transcoded and sent to a newly implemented upstream\r\n\r\nPotential solution:\r\nAdd similar logic to the transcoding filter as it is, for example, in `Buffer` ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11038/comments",
    "author": "gagata",
    "comments": [
      {
        "user": "gagata",
        "created_at": "2020-05-06T06:45:50Z",
        "body": "we need it at work, so I'm happy to send a patch for it - feel free to assign to me if there's no other concerns"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-05-06T16:01:49Z",
        "body": "@gagata go for it!"
      },
      {
        "user": "gagata",
        "created_at": "2020-06-18T00:16:21Z",
        "body": "Hey @mattklein123 \r\nCan you please advise what's the best way to approach the filter configuration where the filter is only specified on the virtual host level?\r\n\r\nWe've been testing the implementation from the linked diff for a while now, and the use case I'm stuck on is as follows:\r\n- the general filter is basically empty (disabled)\r\n- the filter is set on per-virtual host basis only (not for routes)\r\nWhen trying to access the filter configuration, `decoder_callbacks_->route()` will return `nullptr` for a route `/something/that/will/soon/be/transcoded` (the route in the route configuration is the grpc one, after transcoding). \r\n\r\nThe issue was kinda shadowed and hard to figure before, since in our config we used `/` to fallback to the old system (so the path was matched to something, and filter could've been applied). \r\n\r\nQuestions:\r\n- is it considered a valid use case?\r\n- what would you suggest doing to solve the problem (some way of figuring out the per-virtual host config that's not realying on `route()` entry?)\r\n- should I create a new issue to track this separately?"
      }
    ]
  },
  {
    "number": 10926,
    "title": "Envoy /stats/prometheus?filter not working with prometheus due to url percent encoding.",
    "created_at": "2020-04-24T07:54:51Z",
    "closed_at": "2020-07-10T23:48:10Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10926",
    "body": "When a prometheus job is configured to scrape envoy using `filter` query arg, it sends the requests with the params url percent encoded:\r\n\r\n```\r\nGET /stats/prometheus?filter=upstream%7Cdownstream HTTP/1.1\r\nHost: X.Y.Z.Q:8524\r\nUser-Agent: Prometheus/2.15.2\r\nAccept: application/openmetrics-text; version=0.0.1,text/plain;version=0.0.4;q=0.5,*/*;q=0.1\r\nAccept-Encoding: gzip\r\n```\r\n\r\n`upstream%7Cdownstream` doesn't get urldecoded properly and no metrics are returned.\r\n\r\n`upstream|downstream` works fine, but prometheus can't be configured to not url percent encode the params from its scrape requests.\r\n\r\nI'm willing to work on this if it's considered a bug and not expected behaviour. Assuming there already are some bits that do urldecoding in envoy, i think it's approachable even for someone that doesn't know c++ well (me).\r\n\r\n\r\nLater edit:\r\n\r\nuse case: On large enough envoy deployments, we can't fit all the metrics we get from all envoys on just one prom box without eventually trashing it. So we're forced to rely on stats filtering so that each prom sees just a subset of the metrics. The lack of url decoding for `|`-based regex filters forces us to do quite a few unnecessary scrape jobs and various workarounds.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10926/comments",
    "author": "nugasescuserbun",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2020-04-27T19:33:21Z",
        "body": "I'm not sure we have URL decoding anywhere, but it's definitely not happening in the handler from prom stats. "
      },
      {
        "user": "magederek",
        "created_at": "2020-06-29T12:27:56Z",
        "body": "This is really a problem as we use envoy as the ingress proxy in a large kubernetes cluster. Thousands of endpoints are registered to envoy so the statistics of them are so many, make it takes too much time for prometheus to pull all the metrics. We use the 'filter' parameter to filter the statistics on the envoy side so that it can lower the duration of pulling.\r\n\r\nHowever, the value of request query parameters from prometheus is urlencoded, and envoy does not recognize it:\r\n\r\n```bash\r\n# without urlencode, got response from envoy\r\ncurl -s '127.0.0.1:10002/stats?filter=(cluster.upstream_(rq_total|rq_time_sum|rq_time_count|rq_time_bucket|rq_xx|rq_complete|rq_active|cx_active))|(server.version)'\r\ncluster.dynamic_forward_proxy_cluster.upstream_cx_active: 0\r\ncluster.dynamic_forward_proxy_cluster.upstream_rq_active: 0\r\ncluster.dynamic_forward_proxy_cluster.upstream_rq_completed: 0\r\ncluster.dynamic_forward_proxy_cluster.upstream_rq_total: 0\r\ncluster.envolve_mate_cluster.upstream_cx_active: 0\r\ncluster.envolve_mate_cluster.upstream_rq_active: 0\r\ncluster.envolve_mate_cluster.upstream_rq_completed: 0\r\ncluster.envolve_mate_cluster.upstream_rq_total: 0\r\ncluster.envolve_xds_cluster.upstream_cx_active: 1\r\ncluster.envolve_xds_cluster.upstream_rq_active: 1\r\ncluster.envolve_xds_cluster.upstream_rq_completed: 2\r\ncluster.envolve_xds_cluster.upstream_rq_total: 1\r\ncluster.local_envoy_admin_cluster.upstream_cx_active: 10\r\ncluster.local_envoy_admin_cluster.upstream_rq_active: 2\r\ncluster.local_envoy_admin_cluster.upstream_rq_completed: 22\r\ncluster.local_envoy_admin_cluster.upstream_rq_total: 24\r\ncluster.local_envoy_http1_cluster.upstream_cx_active: 0\r\ncluster.local_envoy_http1_cluster.upstream_rq_active: 0\r\ncluster.local_envoy_http1_cluster.upstream_rq_completed: 0\r\ncluster.local_envoy_http1_cluster.upstream_rq_total: 0\r\ncluster.local_envoy_http2_cluster.upstream_cx_active: 0\r\ncluster.local_envoy_http2_cluster.upstream_rq_active: 0\r\ncluster.local_envoy_http2_cluster.upstream_rq_completed: 0\r\ncluster.local_envoy_http2_cluster.upstream_rq_total: 0\r\nserver.version: 6166960\r\n\r\n# no response with urlencode\r\ncurl '127.0.0.1:10002/stats?filter=%28cluster.upstream_%28rq_total%7Crq_time_sum%7Crq_time_count%7Crq_time_bucket%7Crq_xx%7Crq_complete%7Crq_active%7Ccx_active%29%29%7C%28server.version%29'\r\n```\r\n\r\nIt is very helpful if someone can fix this or give some suggestion to make prometheus not to send request with urlencode."
      }
    ]
  },
  {
    "number": 10921,
    "title": "SO_REUSEPORT crash in envoy 1.13.1",
    "created_at": "2020-04-23T20:35:20Z",
    "closed_at": "2020-06-16T17:04:26Z",
    "labels": [
      "bug",
      "help wanted",
      "area/envoy_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10921",
    "body": "Description:\r\n> Envoy is crashing when SO_REUSEPORT socket option is used for our TCP listeners. Below is a related backtrace of a segfault in envoy version 1.13.1.\r\n\r\nRepro steps:\r\n> Our envoys are running on baremetal ubuntu 16.04 LTS with kernel 4.14.130. \r\n\r\nLogs:\r\n> Apr 22 10:59:52 lb-l7-5.ko tt-envoy[22257]: [2020-04-22 10:59:52.681][22259][info][upstream] [source/common/upstream/cds_api_impl.cc:90] cds: add/update cluster 'k8s2_ko__labrador_stable__service_freshness_svc__n__one_url0'\r\nApr 22 10:59:53 lb-l7-5.ko tt-envoy[22257]: [2020-04-22 10:59:53.830][22259][warning][config] [source/common/config/grpc_subscription_impl.cc:87] gRPC config for type.googleapis.com/envoy.api.v2.Listener rejected: Error adding/updating listener(s) k8s2_ko__labrador_stable__service_freshness_svc__n__one: route: unknown cluster 'k8s2_ko__labrador_stable__service_freshness_svc__n__one_url0'\r\nApr 22 10:59:54 lb-l7-5.ko tt-envoy[22257]: [2020-04-22 10:59:54.440][22259][info][upstream] [source/common/upstream/cds_api_impl.cc:74] cds: add 1955 cluster(s), remove 2 cluster(s)\r\nApr 22 10:59:55 lb-l7-5.ko tt-envoy[22257]: [2020-04-22 10:59:55.372][22259][info][upstream] [source/server/lds_api.cc:73] lds: add/update listener 'k8s2_ko__labrador_stable__service_freshness_svc__n__one'\r\nApr 22 10:59:55 lb-l7-5.ko tt-envoy[22257]: [2020-04-22 10:59:55.411][22516][critical][assert] [source/exe/main_common.cc:81] panic: out of memory\r\nApr 22 10:59:55 lb-l7-5.ko tt-envoy[22257]: [2020-04-22 10:59:55.411][22513][critical][assert] [source/exe/main_common.cc:81] panic: out of memory\r\nApr 22 10:59:55 lb-l7-5.ko tt-envoy[22257]: [2020-04-22 10:59:55.411][22503][critical][assert] [source/exe/main_common.cc:81] panic: out of memory\r\nApr 22 10:59:55 lb-l7-5.ko tt-envoy[22257]: [2020-04-22 10:59:55.411][22516][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:83] Caught Aborted, suspect faulting address 0x56f3\r\nApr 22 10:59:55 lb-l7-5.ko tt-envoy[22257]: [2020-04-22 10:59:55.411][22516][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:70] Backtrace (use tools/stack_decode.py to get line numbers):\r\n\r\nBacktrace:\r\n```\r\nCore was generated by `/www/envoy/bin/envoy -c /www/envoy/conf/conf.yaml --restart-epoch 0 --drain-tim'.\r\nProgram terminated with signal SIGABRT, Aborted.\r\n#0  0x00007f2a71b15428 in _IO_vfprintf_internal (s=0x1,\r\n    format=0x56002b801a30 <Envoy::MainCommonBase::MainCommonBase(Envoy::OptionsImpl const&, Envoy::Event::TimeSystem&, Envoy::ListenerHooks&, Envoy::Server::ComponentFactory&, std::__1::unique_ptr<Envoy::Runtime::RandomGenerator, std::__1::default_delete<Envoy::Runtime::RandomGenerator> >&&, Envoy::Thread::ThreadFactory&, Envoy::Filesystem::Instance&, std::__1::unique_ptr<Envoy::ProcessContext, std::__1::default_delete<Envoy::ProcessContext> >)::$_0::__invoke()> \"UH\\211\\345\\350\\a\",\r\n    ap=0x56002cd98340 <(anonymous namespace)::set_new_handler_lock>) at vfprintf.c:1621\r\n1621 vfprintf.c: No such file or directory.\r\n[Current thread is 1 (LWP 5081)]\r\n(gdb)\r\n(gdb) bt\r\n#0  0x00007f2a71b15428 in _IO_vfprintf_internal (s=0x1,\r\n    format=0x56002b801a30 <Envoy::MainCommonBase::MainCommonBase(Envoy::OptionsImpl const&, Envoy::Event::TimeSystem&, Envoy::ListenerHooks&, Envoy::Server::ComponentFactory&, std::__1::unique_ptr<Envoy::Runtime::RandomGenerator, std::__1::default_delete<Envoy::Runtime::RandomGenerator> >&&, Envoy::Thread::ThreadFactory&, Envoy::Filesystem::Instance&, std::__1::unique_ptr<Envoy::ProcessContext, std::__1::default_delete<Envoy::ProcessContext> >)::$_0::__invoke()> \"UH\\211\\345\\350\\a\",\r\n    ap=0x56002cd98340 <(anonymous namespace)::set_new_handler_lock>) at vfprintf.c:1621\r\n#1  0x000056002b801a39 in Envoy::MainCommonBase::MainCommonBase(Envoy::OptionsImpl const&, Envoy::Event::TimeSystem&, Envoy::ListenerHooks&, Envoy::Server::ComponentFactory&, std::__1::unique_ptr<Envoy::Runtime::RandomGenerator, std::__1::default_delete<Envoy::Runtime::RandomGenerator> >&&, Envoy::Thread::ThreadFactory&, Envoy::Filesystem::Instance&, std::__1::unique_ptr<Envoy::ProcessContext, std::__1::default_delete<Envoy::ProcessContext> >)::$_0::__invoke() () at source/exe/main_common.cc:81\r\n#2  0x000056002cb74785 in (anonymous namespace)::handle_oom(void* (*)(void*), void*, bool, bool) ()\r\n#3  0x000056002cbdc090 in tcmalloc::allocate_full_cpp_throw_oom(unsigned long) ()\r\n#4  0x000056002b801f1d in std::__1::__libcpp_allocate (__size=7451599682960305765, __align=1) at /opt/llvm/bin/../include/c++/v1/new:253\r\n#5  std::__1::allocator<char>::allocate (this=0x7f2a5caa0278, __n=7451599682960305765) at /opt/llvm/bin/../include/c++/v1/memory:1813\r\n#6  std::__1::allocator_traits<std::__1::allocator<char> >::allocate (__a=..., __n=7451599682960305765) at /opt/llvm/bin/../include/c++/v1/memory:1546\r\n#7  fmt::v5::internal::allocate<std::__1::allocator<char> > (alloc=..., n=7451599682960305765) at external/com_github_fmtlib_fmt/include/fmt/format.h:284\r\n#8  fmt::v5::basic_memory_buffer<char, 500ul, std::__1::allocator<char> >::grow (this=0x7f2a5caa0278, size=5081) at external/com_github_fmtlib_fmt/include/fmt/format.h:541\r\n#9  0x000056002b802f08 in fmt::v5::internal::basic_buffer<char>::reserve (this=0x7f2a5caa0278, new_capacity=7451599682960305765) at external/com_github_fmtlib_fmt/include/fmt/core.h:275\r\n#10 fmt::v5::internal::basic_buffer<char>::resize (this=0x7f2a5caa0278, new_size=7451599682960305765) at external/com_github_fmtlib_fmt/include/fmt/core.h:265\r\n#11 fmt::v5::internal::reserve<fmt::v5::internal::basic_buffer<char> > (it=..., n=7451599682960305765) at external/com_github_fmtlib_fmt/include/fmt/format.h:599\r\n#12 fmt::v5::basic_writer<fmt::v5::back_insert_range<fmt::v5::internal::basic_buffer<char> > >::reserve (this=0x7f2a5caa0210, n=7451599682960305765) at external/com_github_fmtlib_fmt/include/fmt/format.h:2272\r\n#13 fmt::v5::basic_writer<fmt::v5::back_insert_range<fmt::v5::internal::basic_buffer<char> > >::write (this=0x7f2a5caa0210, value=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:2623\r\n#14 fmt::v5::internal::arg_formatter_base<fmt::v5::back_insert_range<fmt::v5::internal::basic_buffer<char> > >::operator() (this=0x7f2a5caa0210, value=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:1433\r\n#15 0x000056002b8022e9 in fmt::v5::internal::parse_format_string<false, char, fmt::v5::format_handler<fmt::v5::arg_formatter<fmt::v5::back_insert_range<fmt::v5::internal::basic_buffer<char> > >, char, fmt::v5::basic_format_context<std::__1::back_insert_iterator<fmt::v5::internal::basic_buffer<char> >, char> >&> (format_str=..., handler=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:3097\r\n#16 0x000056002b82c755 in fmt::v5::vformat_to<fmt::v5::arg_formatter<fmt::v5::back_insert_range<fmt::v5::internal::basic_buffer<char> > >, char, fmt::v5::basic_format_context<std::__1::back_insert_iterator<fmt::v5::internal::basic_buffer<char> >, char> > (out=...,\r\n    format_str=..., args=..., loc=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:3130\r\n#17 fmt::v5::internal::vformat_to<char> (buf=..., format_str=..., args=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:3237\r\n#18 fmt::v5::internal::vformat<char> (format_str=..., args=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:3414\r\n#19 0x000056002c18014e in fmt::v5::format<char [30], std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, char const*> (format_str=..., args=..., args=...) at external/com_github_fmtlib_fmt/include/fmt/core.h:1456\r\n#20 Envoy::Server::ListenSocketFactoryImpl::createListenSocketAndApplyOptions (this=0x56003fb1de98) at source/server/listener_impl.cc:85\r\n#21 0x000056002c180518 in Envoy::Server::ListenSocketFactoryImpl::getListenSocket (this=0x56003fb1de98) at source/server/listener_impl.cc:123\r\n#22 0x000056002c1ce942 in Envoy::Server::ConnectionHandlerImpl::ActiveTcpListener::ActiveTcpListener (this=0x56003ba5b260, parent=..., config=...) at source/server/connection_handler_impl.cc:118\r\n#23 0x000056002c1cda09 in std::__1::make_unique<Envoy::Server::ConnectionHandlerImpl::ActiveTcpListener, Envoy::Server::ConnectionHandlerImpl&, Envoy::Network::ListenerConfig&> (__args=..., __args=...) at /opt/llvm/bin/../include/c++/v1/memory:3003\r\n#24 Envoy::Server::ConnectionHandlerImpl::addListener (this=0x56002e598f00, config=...) at source/server/connection_handler_impl.cc:33\r\n#25 0x000056002c1cd19f in Envoy::Server::WorkerImpl::addListener(Envoy::Network::ListenerConfig&, std::__1::function<void (bool)>)::$_1::operator()() const (this=0x56003ffff500) at source/server/worker_impl.cc:44\r\n#26 _ZNSt3__18__invokeIRZN5Envoy6Server10WorkerImpl11addListenerERNS1_7Network14ListenerConfigENS_8functionIFvbEEEE3$_1JEEEDTclclsr3std3__1E7forwardIT_Efp_Espclsr3std3__1E7forwardIT0_Efp0_EEEOSC_DpOSD_ (__f=...) at /opt/llvm/bin/../include/c++/v1/type_traits:3530\r\n#27 std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Server::WorkerImpl::addListener(Envoy::Network::ListenerConfig&, std::__1::function<void (bool)>)::$_1&>(Envoy::Server::WorkerImpl::addListener(Envoy::Network::ListenerConfig&, std::__1::function<void (bool)>)::$_1&) (__args=...) at /opt/llvm/bin/../include/c++/v1/__functional_base:348\r\n#28 std::__1::__function::__alloc_func<Envoy::Server::WorkerImpl::addListener(Envoy::Network::ListenerConfig&, std::__1::function<void (bool)>)::$_1(std::__1::allocator<std::__1::allocator>, void ())>::operator()() (this=0x56003ffff500)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:1533\r\n#29 std::__1::__function::__func<Envoy::Server::WorkerImpl::addListener(Envoy::Network::ListenerConfig&, std::__1::function<void (bool)>)::$_1(std::__1::allocator<std::__1::allocator>, void ())>::operator()() (this=0x56003ffff4f0)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:1707\r\n#30 0x000056002c1d5b66 in std::__1::__function::__value_func<void ()>::operator()() const (this=0x7f2a5caa06f0) at /opt/llvm/bin/../include/c++/v1/functional:1860\r\n#31 std::__1::function<void ()>::operator()() const (this=0x7f2a5caa06f0) at /opt/llvm/bin/../include/c++/v1/functional:2419\r\n#32 Envoy::Event::DispatcherImpl::runPostCallbacks (this=0x56002e6b6c00) at source/common/event/dispatcher_impl.cc:224\r\n#33 0x000056002c4ed0b6 in event_process_active_single_queue ()\r\n#34 0x000056002c4ebc3e in event_base_loop ()\r\n#35 0x000056002c1cc8f8 in Envoy::Server::WorkerImpl::threadRoutine (this=0x56002e421e60, guard_dog=...) at source/server/worker_impl.cc:110\r\n#36 0x000056002c6b8363 in std::__1::__function::__value_func<void ()>::operator()() const (this=<optimized out>) at /opt/llvm/bin/../include/c++/v1/functional:1860\r\n#37 std::__1::function<void ()>::operator()() const (this=<optimized out>) at /opt/llvm/bin/../include/c++/v1/functional:2419\r\n#38 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>)::$_0::operator()(void*) const (this=<optimized out>, arg=<optimized out>) at source/common/common/posix/thread_impl.cc:33\r\n#39 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>)::$_0::__invoke(void*) (arg=<optimized out>) at source/common/common/posix/thread_impl.cc:32\r\n#40 0x00007f2a71eb16ba in pthread_getattr_np (thread_id=94558776739096, attr=0x0) at pthread_getattr_np.c:88\r\n#41 0x0000000000000000 in ?? ()\r\n(gdb) bt full\r\n#0  0x00007f2a71b15428 in _IO_vfprintf_internal (s=0x1,\r\n    format=0x56002b801a30 <Envoy::MainCommonBase::MainCommonBase(Envoy::OptionsImpl const&, Envoy::Event::TimeSystem&, Envoy::ListenerHooks&, Envoy::Server::ComponentFactory&, std::__1::unique_ptr<Envoy::Runtime::RandomGenerator, std::__1::default_delete<Envoy::Runtime::RandomGenerator> >&&, Envoy::Thread::ThreadFactory&, Envoy::Filesystem::Instance&, std::__1::unique_ptr<Envoy::ProcessContext, std::__1::default_delete<Envoy::ProcessContext> >)::$_0::__invoke()> \"UH\\211\\345\\350\\a\",\r\n    ap=0x56002cd98340 <(anonymous namespace)::set_new_handler_lock>) at vfprintf.c:1621\r\n        string = <optimized out>\r\n        width = 8\r\n        signed_number = <optimized out>\r\n        is_short = 1023504608\r\n        spec = 101 'e'\r\n        ptr = <optimized out>\r\n        ptr = <optimized out>\r\n        outc = <optimized out>\r\n        step0_jumps = {0, 160, 64, 1664, 1576, 1464, 1360, 2216, 2912, 208, 2032, 1848, 1760, -768, -760, 3024, 3000, 3104, 2128, 3120, -304, -704, 384, 312, -80, -1256, 2624, -1344, -1344, 2536}\r\n        base = <optimized out>\r\n        pad = <optimized out>\r\n        offset = <optimized out>\r\n        offset = <optimized out>\r\n        step1_jumps = {0, 0, 0, 0, 0, 0, 0, 0, 0, 208, 2032, 1848, 1760, -768, -760, 3024, 3000, 3104, 2128, 3120, -304, -704, 384, 312, -80, -1256, 2624, -1344, -1344, 0}\r\n        args_value = <optimized out>\r\n        is_negative = <optimized out>\r\n        number = <optimized out>\r\n        use_outdigits = <optimized out>\r\n        step2_jumps = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2032, 1848, 1760, -768, -760, 3024, 3000, 3104, 2128, 3120, -304, -704, 384, 312, -80, -1256, 2624, -1344, -1344, 0}\r\n        alt = 733045535\r\n        left = 1331545088\r\n        showsign = 1267700866\r\n        group = 1554644144\r\n        is_char = 14\r\n        step3a_jumps = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1936, 0, 0, 0, -760, 3024, 3000, 3104, 2128, 0, 0, 0, 0, 312, 0, 0, 0, 0, 0, 0}\r\n        the_arg = {pa_wchar = 94 L'^', pa_int = 94, pa_long_int = 94, pa_long_long_int = 94, pa_u_int = 94, pa_u_long_int = 94, pa_u_long_long_int = 94, pa_double = 4.6442170709077175e-322, pa_long_double = <invalid float value>,\r\n          pa_string = 0x5e <error: Cannot access memory at address 0x5e>, pa_wstring = 0x5e <error: Cannot access memory at address 0x5e>, pa_pointer = 0x5e, pa_user = 0x5e}\r\n        space = 1075408736\r\n        is_long_double = 1\r\n        is_long = 1\r\n        step3b_jumps = {0 <repeats 11 times>, 1760, 0, 0, -760, 3024, 3000, 3104, 2128, 3120, -304, -704, 384, 312, -80, -1256, 2624, 0, 0, 0}\r\n        step4_jumps = {0 <repeats 14 times>, -760, 3024, 3000, 3104, 2128, 3120, -304, -704, 384, 312, -80, -1256, 2624, 0, 0, 0}\r\n        prec = 1907446824\r\n        _buffer = {__routine = 0x40, __arg = 0x2e72657473756c63, __canceltype = 64, __prev = 0x56002b814178 <fmt::v5::internal::arg_formatter_base<fmt::v5::back_insert_range<fmt::v5::internal::basic_buffer<char> > >::write(char const*)+40>}\r\n        _avail = <optimized out>\r\n        thousands_sep = 0x5d31383a63632e6e <error: Cannot access memory at address 0x5d31383a63632e6e>\r\n        grouping = 0x6d20666f2074756f <error: Cannot access memory at address 0x6d20666f2074756f>\r\n        done = 750209056\r\n        f = 0x6 <error: Cannot access memory at address 0x6>\r\n        lead_str_end = 0x9ddfea08eb382d69 <error: Cannot access memory at address 0x9ddfea08eb382d69>\r\n        end_of_spec = <optimized out>\r\n        work_buffer = \"@U\\352q*\\177\\000\\000\\037شq*\\177\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\200\\000\\000\\000\\000\\000\\000\\000\\216R\\353q*\\177\\000\\000\\340\\374\\251\\\\*\\177\\000\\000\\220\\240;.\\000V\\000\\000\\070\\375\\251\\\\*\\177\\000\\000\\360\\375\\251\\\\*\\177\\000\\000\\260\\016\\276,\\000V\\000\\000\\220\\240;.\\000V\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000p\\375\\251\\\\*\\177\\000\\000\\364K\\271,\\000V\\000\\000\\370Y\\364,\\000V\\000\\000\\376=\\271,\\000V\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\001}\\025,\\000V\\000\\000\\370Y\\364,\\000V\\000\\000\\030EL.\\000V\\000\\000\\220\\240;.\\000V\\000\\000\\063w\\263,\\000V\\000\\000\\200\\240;.\\000V\\000\\000\\220\\240;.\\000V\\000\\000\\220\\375\\251\\\\*\\177\\000\\000\\251\"...\r\n        workstart = <optimized out>\r\n        workend = <optimized out>\r\n        ap_save = {{gp_offset = 3, fp_offset = 0, overflow_arg_area = 0x7f2a71ea5620 <_nl_global_locale+160>, reg_save_area = 0x7f2a71b4d81f <_int_malloc+1327>}}\r\n        nspecs_done = 2681\r\n        save_errno = 1554644056\r\n        readonly_format = 1919905125\r\n        __PRETTY_FUNCTION__ = \"_IO_vfprintf_internal\"\r\n        __result = <optimized out>\r\n#1  0x000056002b801a39 in Envoy::MainCommonBase::MainCommonBase(Envoy::OptionsImpl const&, Envoy::Event::TimeSystem&, Envoy::ListenerHooks&, Envoy::Server::ComponentFactory&, std::__1::unique_ptr<Envoy::Runtime::RandomGenerator, std::__1::default_delete<Envoy::Runtime::RandomGenerator> >&&, Envoy::Thread::ThreadFactory&, Envoy::Filesystem::Instance&, std::__1::unique_ptr<Envoy::ProcessContext, std::__1::default_delete<Envoy::ProcessContext> >)::$_0::__invoke() () at source/exe/main_common.cc:81\r\nNo locals.\r\n#2  0x000056002cb74785 in (anonymous namespace)::handle_oom(void* (*)(void*), void*, bool, bool) ()\r\nNo symbol table info available.\r\n#3  0x000056002cbdc090 in tcmalloc::allocate_full_cpp_throw_oom(unsigned long) ()\r\nNo symbol table info available.\r\n---Type <return> to continue, or q <return> to quit---\r\n#4  0x000056002b801f1d in std::__1::__libcpp_allocate (__size=7451599682960305765, __align=1) at /opt/llvm/bin/../include/c++/v1/new:253\r\nNo locals.\r\n#5  std::__1::allocator<char>::allocate (this=0x7f2a5caa0278, __n=7451599682960305765) at /opt/llvm/bin/../include/c++/v1/memory:1813\r\nNo locals.\r\n#6  std::__1::allocator_traits<std::__1::allocator<char> >::allocate (__a=..., __n=7451599682960305765) at /opt/llvm/bin/../include/c++/v1/memory:1546\r\nNo locals.\r\n#7  fmt::v5::internal::allocate<std::__1::allocator<char> > (alloc=..., n=7451599682960305765) at external/com_github_fmtlib_fmt/include/fmt/format.h:284\r\nNo locals.\r\n#8  fmt::v5::basic_memory_buffer<char, 500ul, std::__1::allocator<char> >::grow (this=0x7f2a5caa0278, size=5081) at external/com_github_fmtlib_fmt/include/fmt/format.h:541\r\n        old_data = 0x7f2a5caa0298 \"\\320ۧ?\"\r\n        old_capacity = 0\r\n        new_capacity = 7451599682960305765\r\n        new_data = <optimized out>\r\n#9  0x000056002b802f08 in fmt::v5::internal::basic_buffer<char>::reserve (this=0x7f2a5caa0278, new_capacity=7451599682960305765) at external/com_github_fmtlib_fmt/include/fmt/core.h:275\r\nNo locals.\r\n#10 fmt::v5::internal::basic_buffer<char>::resize (this=0x7f2a5caa0278, new_size=7451599682960305765) at external/com_github_fmtlib_fmt/include/fmt/core.h:265\r\nNo locals.\r\n#11 fmt::v5::internal::reserve<fmt::v5::internal::basic_buffer<char> > (it=..., n=7451599682960305765) at external/com_github_fmtlib_fmt/include/fmt/format.h:599\r\n        c = @0x7f2a5caa0278: warning: RTTI symbol not found for class 'fmt::v5::basic_memory_buffer<char, 500ul, std::__1::allocator<char> >'\r\n{_vptr$basic_buffer = 0x56002cbe0eb0 <vtable for fmt::v5::basic_memory_buffer<char, 500ul, std::__1::allocator<char> >+16>, ptr_ = 0x7f2a5caa0298 \"\\320ۧ?\", size_ = 0, capacity_ = 500}\r\n        size = 0\r\n#12 fmt::v5::basic_writer<fmt::v5::back_insert_range<fmt::v5::internal::basic_buffer<char> > >::reserve (this=0x7f2a5caa0210, n=7451599682960305765) at external/com_github_fmtlib_fmt/include/fmt/format.h:2272\r\nNo locals.\r\n#13 fmt::v5::basic_writer<fmt::v5::back_insert_range<fmt::v5::internal::basic_buffer<char> > >::write (this=0x7f2a5caa0210, value=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:2623\r\n        it = <optimized out>\r\n#14 fmt::v5::internal::arg_formatter_base<fmt::v5::back_insert_range<fmt::v5::internal::basic_buffer<char> > >::operator() (this=0x7f2a5caa0210, value=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:1433\r\nNo locals.\r\n#15 0x000056002b8022e9 in fmt::v5::internal::parse_format_string<false, char, fmt::v5::format_handler<fmt::v5::arg_formatter<fmt::v5::back_insert_range<fmt::v5::internal::basic_buffer<char> > >, char, fmt::v5::basic_format_context<std::__1::back_insert_iterator<fmt::v5::internal::basic_buffer<char> >, char> >&> (format_str=..., handler=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:3097\r\n        p = 0x56002af9f97a \"}: Setting socket options {}\"\r\n        begin = <optimized out>\r\n        end = 0x56002af9f996 \"\"\r\n        write = {handler_ = @0x7f2a5caa0490}\r\n#16 0x000056002b82c755 in fmt::v5::vformat_to<fmt::v5::arg_formatter<fmt::v5::back_insert_range<fmt::v5::internal::basic_buffer<char> > >, char, fmt::v5::basic_format_context<std::__1::back_insert_iterator<fmt::v5::internal::basic_buffer<char> >, char> > (out=...,\r\n    format_str=..., args=..., loc=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:3130\r\n        h = {<fmt::v5::internal::error_handler> = {<No data fields>},\r\n          context = {<fmt::v5::internal::context_base<std::__1::back_insert_iterator<fmt::v5::internal::basic_buffer<char> >, fmt::v5::basic_format_context<std::__1::back_insert_iterator<fmt::v5::internal::basic_buffer<char> >, char>, char>> = {\r\n              parse_context_ = {<fmt::v5::internal::error_handler> = {<No data fields>}, format_str_ = {data_ = 0x56002af9f97a \"}: Setting socket options {}\", size_ = 28}, next_arg_id_ = 1},\r\n              out_ = {<std::__1::iterator<std::__1::output_iterator_tag, void, void, void, void>> = {<No data fields>}, container = 0x7f2a5caa0278}, args_ = {types_ = 171, {values_ = 0x7f2a5caa0580, args_ = 0x7f2a5caa0580}}, loc_ = {locale_ = 0x0}}, map_ = {map_ = 0x0,\r\n              size_ = 0}}, arg = {value_ = {{int_value = 1920298835, uint_value = 1920298835, long_long_value = 4472748857728790355, ulong_long_value = 4472748857728790355, double_value = 1.0707909644342217e-09, long_double_value = <invalid float value>,\r\n                pointer = 0x3e12656372756f53, string = {value = 0x3e12656372756f53 <error: Cannot access memory at address 0x3e12656372756f53>, size = 7451599682960305765}, sstring = {value = 0x3e12656372756f53 <error: Cannot access memory at address 0x3e12656372756f53>,\r\n                  size = 7451599682960305765}, ustring = {value = 0x3e12656372756f53 <error: Cannot access memory at address 0x3e12656372756f53>, size = 7451599682960305765}, custom = {value = 0x3e12656372756f53, format = 0x6769666e6f432e65}}},\r\n            type_ = fmt::v5::internal::string_type}}\r\n#17 fmt::v5::internal::vformat_to<char> (buf=..., format_str=..., args=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:3237\r\nNo locals.\r\n#18 fmt::v5::internal::vformat<char> (format_str=..., args=...) at external/com_github_fmtlib_fmt/include/fmt/format.h:3414\r\n        buffer = warning: RTTI symbol not found for class 'fmt::v5::basic_memory_buffer<char, 500ul, std::__1::allocator<char> >'\r\n{<std::__1::allocator<char>> = {<No data fields>}, <fmt::v5::internal::basic_buffer<char>> = {_vptr$basic_buffer = 0x56002cbe0eb0 <vtable for fmt::v5::basic_memory_buffer<char, 500ul, std::__1::allocator<char> >+16>, ptr_ = 0x7f2a5caa0298 \"\\320ۧ?\",\r\n            size_ = 0, capacity_ = 500},\r\n          store_ = \"\\320ۧ?\\000V\\000\\000\\360\\002\\252\\\\*\\177\\000\\000NmN,\\000V\\000\\000\\255\\001\\000\\000\\000\\000\\000\\000\\000\\304\\313,\\000V\\000\\000@\\003\\252\\\\*\\177\\000\\000\\274\\066\\071,\\000V\\000\\000XK\\220;\\000V\\000\\000\\031kN,\\000V\\000\\000\\020\\000\\000\\000\\000\\000\\000\\000\\000\\304\\313,\\000V\\000\\000\\060\\003\\252\\\\*\\177\\000\\000P\\034N,\\000V\\000\\000P\\336\\262+\\000V\\000\\000P\\336\\262+\\000V\\000\\000\\230\\236\\243?\\000V\\000\\000XK\\220;\\000V\\000\\000\\300ޱ?\\000V\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\\240\\003\\252\\\\*\\177\\000\\000,QD,\\000V\\000\\000\\200\\003\\252\\\\*\\177\\000\\000\\336RD,\\000V\\000\\000\\200\\234\\313,\\000V\\000\\000\\221`\\\",\\000V\\000\\000\\370\\003\\252\\\\\"...}\r\n#19 0x000056002c18014e in fmt::v5::format<char [30], std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, char const*> (format_str=..., args=..., args=...) at external/com_github_fmtlib_fmt/include/fmt/core.h:1456\r\nNo locals.\r\n#20 Envoy::Server::ListenSocketFactoryImpl::createListenSocketAndApplyOptions (this=0x56003fb1de98) at source/server/listener_impl.cc:85\r\n        ok = false\r\n        message = <optimized out>\r\n        socket = <optimized out>\r\n#21 0x000056002c180518 in Envoy::Server::ListenSocketFactoryImpl::getListenSocket (this=0x56003fb1de98) at source/server/listener_impl.cc:123\r\n        socket = {__ptr_ = 0x0, __cntrl_ = 0x0}\r\n#22 0x000056002c1ce942 in Envoy::Server::ConnectionHandlerImpl::ActiveTcpListener::ActiveTcpListener (this=0x56003ba5b260, parent=..., config=...) at source/server/connection_handler_impl.cc:118\r\nNo locals.\r\n#23 0x000056002c1cda09 in std::__1::make_unique<Envoy::Server::ConnectionHandlerImpl::ActiveTcpListener, Envoy::Server::ConnectionHandlerImpl&, Envoy::Network::ListenerConfig&> (__args=..., __args=...) at /opt/llvm/bin/../include/c++/v1/memory:3003\r\nNo locals.\r\n#24 Envoy::Server::ConnectionHandlerImpl::addListener (this=0x56002e598f00, config=...) at source/server/connection_handler_impl.cc:33\r\n---Type <return> to continue, or q <return> to quit---\r\n        tcp_listener = <optimized out>\r\n        details = <optimized out>\r\n#25 0x000056002c1cd19f in Envoy::Server::WorkerImpl::addListener(Envoy::Network::ListenerConfig&, std::__1::function<void (bool)>)::$_1::operator()() const (this=0x56003ffff500) at source/server/worker_impl.cc:44\r\n        e = <optimized out>\r\n#26 _ZNSt3__18__invokeIRZN5Envoy6Server10WorkerImpl11addListenerERNS1_7Network14ListenerConfigENS_8functionIFvbEEEE3$_1JEEEDTclclsr3std3__1E7forwardIT_Efp_Espclsr3std3__1E7forwardIT0_Efp0_EEEOSC_DpOSD_ (__f=...) at /opt/llvm/bin/../include/c++/v1/type_traits:3530\r\nNo locals.\r\n#27 std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Server::WorkerImpl::addListener(Envoy::Network::ListenerConfig&, std::__1::function<void (bool)>)::$_1&>(Envoy::Server::WorkerImpl::addListener(Envoy::Network::ListenerConfig&, std::__1::function<void (bool)>)::$_1&) (__args=...) at /opt/llvm/bin/../include/c++/v1/__functional_base:348\r\nNo locals.\r\n#28 std::__1::__function::__alloc_func<Envoy::Server::WorkerImpl::addListener(Envoy::Network::ListenerConfig&, std::__1::function<void (bool)>)::$_1(std::__1::allocator<std::__1::allocator>, void ())>::operator()() (this=0x56003ffff500)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:1533\r\nNo locals.\r\n#29 std::__1::__function::__func<Envoy::Server::WorkerImpl::addListener(Envoy::Network::ListenerConfig&, std::__1::function<void (bool)>)::$_1(std::__1::allocator<std::__1::allocator>, void ())>::operator()() (this=0x56003ffff4f0)\r\n    at /opt/llvm/bin/../include/c++/v1/functional:1707\r\nNo locals.\r\n#30 0x000056002c1d5b66 in std::__1::__function::__value_func<void ()>::operator()() const (this=0x7f2a5caa06f0) at /opt/llvm/bin/../include/c++/v1/functional:1860\r\nNo locals.\r\n#31 std::__1::function<void ()>::operator()() const (this=0x7f2a5caa06f0) at /opt/llvm/bin/../include/c++/v1/functional:2419\r\nNo locals.\r\n#32 Envoy::Event::DispatcherImpl::runPostCallbacks (this=0x56002e6b6c00) at source/common/event/dispatcher_impl.cc:224\r\n        callback = {<std::__1::__function::__maybe_derive_from_unary_function<void ()>> = {<No data fields>}, <std::__1::__function::__maybe_derive_from_binary_function<void ()>> = {<No data fields>}, __f_ = {__buf_ = {\r\n              __lx = \"\\371\\276\\071\\001\\000\\000\\000\\000\\\\\\240\\005\", '\\000' <repeats 13 times>, \"H\\350\\211:\\000V\\000\"}, __f_ = 0x56003ffff4f0}}\r\n#33 0x000056002c4ed0b6 in event_process_active_single_queue ()\r\nNo symbol table info available.\r\n#34 0x000056002c4ebc3e in event_base_loop ()\r\nNo symbol table info available.\r\n#35 0x000056002c1cc8f8 in Envoy::Server::WorkerImpl::threadRoutine (this=0x56002e421e60, guard_dog=...) at source/server/worker_impl.cc:110\r\nNo locals.\r\n#36 0x000056002c6b8363 in std::__1::__function::__value_func<void ()>::operator()() const (this=<optimized out>) at /opt/llvm/bin/../include/c++/v1/functional:1860\r\nNo locals.\r\n#37 std::__1::function<void ()>::operator()() const (this=<optimized out>) at /opt/llvm/bin/../include/c++/v1/functional:2419\r\nNo locals.\r\n#38 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>)::$_0::operator()(void*) const (this=<optimized out>, arg=<optimized out>) at source/common/common/posix/thread_impl.cc:33\r\nNo locals.\r\n#39 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>)::$_0::__invoke(void*) (arg=<optimized out>) at source/common/common/posix/thread_impl.cc:32\r\nNo locals.\r\n#40 0x00007f2a71eb16ba in pthread_getattr_np (thread_id=94558776739096, attr=0x0) at pthread_getattr_np.c:88\r\n        rl = {rlim_cur = 0, rlim_max = 0}\r\n        fp = 0x0\r\n        thread = 0x56002e4c4518\r\n        iattr = 0x0\r\n        ret = 0\r\n#41 0x0000000000000000 in ?? ()\r\nNo symbol table info available.\r\n(gdb)\r\n```\r\n\r\nNote:\r\n> The above crash happens when listener is added by LDS.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10921/comments",
    "author": "oblazek",
    "comments": [
      {
        "user": "l8huang",
        "created_at": "2020-05-19T02:41:55Z",
        "body": "I'm using 1.13.1 and don't have crash issue with reuseport enabled. I tried last commit in maser branch(32b8445b8fac9e46f67c2f2ce884716b7bc9dde8/1.15.0-dev/Clean/DEBUG/BoringSSL), couldn't reproduce it with a minimal static config.\r\n\r\n@oblazek Could you please send the commit version you are using? If a static config can reproduce the issue, it could be helpful."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-06-16T17:04:26Z",
        "body": "In looking at this again this looks like an OOM so I'm going to close. If you have a self contained repro please let us know."
      },
      {
        "user": "oblazek",
        "created_at": "2020-06-17T06:31:52Z",
        "body": "That doesn't sound right.. we are running envoy on a baremetal machines with approx 192GB RAM and the whole system including envoy is consuming around 10GB so not sure how this can OOM."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-06-17T14:48:46Z",
        "body": "> That doesn't sound right.. we are running envoy on a baremetal machines with approx 192GB RAM and the whole system including envoy is consuming around 10GB so not sure how this can OOM.\r\n\r\nI'm working on fixing related issues right now and I can't repro this. Are you still able to repro this on current master? It's possible this is a bug in the format library."
      },
      {
        "user": "oblazek",
        "created_at": "2020-06-17T14:53:28Z",
        "body": "We've rolled it back, but yeah np I will try that again with current master. "
      }
    ]
  },
  {
    "number": 10807,
    "title": "request_id: Allow always setting x-request-id header in response",
    "created_at": "2020-04-16T07:39:54Z",
    "closed_at": "2020-04-22T16:02:51Z",
    "labels": [
      "help wanted",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10807",
    "body": "*Description*:\r\nCurrently non-configurable Envoy behavior is to return `x-request-id` header in response iff the tracing is forced.\r\nIt would be nice to have a configuration option to allow always returning the header in response.\r\n\r\nIt is especially useful for cross-correlating client-side debug logs with access logs on server side.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10807/comments",
    "author": "euroelessar",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-04-16T16:15:21Z",
        "body": "I thought we already had an issue on this but I can't quickly find it, +1."
      }
    ]
  },
  {
    "number": 10764,
    "title": "access log: grpc access log grows unboundly if backend is slow",
    "created_at": "2020-04-14T00:29:54Z",
    "closed_at": "2020-05-12T16:58:44Z",
    "labels": [
      "help wanted",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10764",
    "body": "gRPC access log has no back pressure in case if backend can not keep up with the load. In this case it starts buffering all entries without any upper bound.\r\n\r\nIn a sense it is similar to #9458 but for other types of loggers.\r\n\r\nIn order to avoid OOM situations Envoy should:\r\n1. Limit memory usage of buffered entries.\r\n2. Drop new entries after some threshold (which can default to smth like 1MB).\r\n3. Emit metric for number of dropped log entries.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10764/comments",
    "author": "euroelessar",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-04-14T15:35:28Z",
        "body": "I think the issue here is that we don't actually expose the high/low watermark events in the async client case. I think the correct fix here is to make sure that the router watermark events are exposed in the async client, and then in streaming cases the users of the client can respond to them and do what they think is best (reset, drop, etc.). cc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-04-20T20:28:50Z",
        "body": "I'm happy to do the \"exposing to user of async client\" bit, but do you have thoughts on what the drop bits would look like?  If it's just early return and increment a stat on the log call I can pick that up.  If we're going to plumb something more complicated through so high value logs get logged during overload, I'd pass it off to someone with greater logging enthusiam :-P"
      },
      {
        "user": "euroelessar",
        "created_at": "2020-04-20T20:36:18Z",
        "body": "I was thinking about something as simple as just dropping all log entries while buffer is above some watermark and incrementing corresponding stat."
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-04-21T18:50:52Z",
        "body": "SG.  This is mildly complicated by the fact the access loggers didn't have stats, and I'm not sure if I can get away with only adding stats for the gRPC access logger. We'll see how the review goes :-P"
      }
    ]
  },
  {
    "number": 10701,
    "title": "thrift proxy test driver dependencies",
    "created_at": "2020-04-08T17:25:22Z",
    "closed_at": "2022-04-26T00:54:08Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10701",
    "body": "Envoy's thrift proxy network filter uses some python code to generate a variety of requests and responses in the various combinations of thrift transport and protocol that are support. One of the supported protocols, colloquially known as \"ttwitter\", is no longer supported in python and is not compatible with python3, blocking #4552.\r\n\r\nThe actual python3 incompatibility is in the twitter.common.rpc package and involves a type check against `long`, which no longer exists in python3. The fix is simple, but as the package is no longer support I don't expect we'll see an update. \r\n\r\nThis issue enumerates so possible paths forward:\r\n\r\n1. It's fairly simple to patch the library to remove the check for `long`. I think this is reasonable in the short-term.\r\n\r\n2. Bring the unsupported twitter.common.rpc code into the envoyproxy org (not necessarily envoyproxy/envoy), and fix it. I dislike this path because the entire point of using external libraries was to test against a different implementation of the protocol. If the protocol were ever updated (and it does have a versioning provision) we'd be implementing both sides of the integration test.\r\n\r\n3. Thrift supports other languages besides python and it should be possible to rewrite the code in `test/extensions/filters/network/thrift_proxy/driver` in another language. Java seems the mostly likely candidate since it's supported by bazel and has support for all the variations of thrift. I think we'd want to put that code in a new repository (under the envoyproxy org) and treat the entire payload generating structure as an external dependency.\r\n\r\n4. Deprecate ttwitter support and delete usage of the abandoned libraries. I don't have a sense of how much the ttwitter thrift protocol is used in conjunction with Envoy so I don't know how to gauge how painful this would be to end users.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10701/comments",
    "author": "zuercher",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2020-04-08T17:29:17Z",
        "body": "#10702 implements the first option above."
      },
      {
        "user": "rgs1",
        "created_at": "2022-03-22T19:13:57Z",
        "body": "I think we should get started with option 4 (deprecate ttwitter and remove the abandoned tests), I am happy to drive. I pinged @mattklein123 offline to get a sense of whether other deployments are relying on ttwitter.\r\n\r\ncc: @fishcakez @davinci26 @tkovacs-2 @caitong93  "
      },
      {
        "user": "rgs1",
        "created_at": "2022-04-25T23:00:58Z",
        "body": "@zuercher I think we can close this now that #20466 is done. \r\n\r\ncc: @phlax "
      },
      {
        "user": "zuercher",
        "created_at": "2022-04-26T00:54:08Z",
        "body": "For anyone going across this bug in the future -- we chose option 4 and have deprecated the ttwitter protocol."
      }
    ]
  },
  {
    "number": 10680,
    "title": "//test/integration:filter_manager_integration_test flake",
    "created_at": "2020-04-07T16:44:08Z",
    "closed_at": "2020-04-07T19:45:03Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10680",
    "body": "I've seen this multiple times:\r\n```\r\n[ RUN      ] Params/InjectDataWithEchoFilterIntegrationTest.FilterChainMismatch/IPv4_inject_data_outside_filter_callback\r\n[2020-04-07 10:32:01.429][17][critical][assert] [test/integration/integration.cc:557] assert failure: 0. Details: Timed out waiting for access log\r\nAddressSanitizer:DEADLYSIGNAL\r\n=================================================================\r\n==17==ERROR: AddressSanitizer: ABRT on unknown address 0xfffe00000011 (pc 0x7f3c911ff428 bp 0x7ffe5b1b2cf0 sp 0x7ffe5b1b2928 T0)\r\n    #0 0x7f3c911ff427 in raise (/lib/x86_64-linux-gnu/libc.so.6+0x35427)\r\n    #1 0x7f3c91201029 in abort (/lib/x86_64-linux-gnu/libc.so.6+0x37029)\r\n    #2 0x15f9da7c in Envoy::BaseIntegrationTest::waitForAccessLog(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) /proc/self/cwd/test/integration/integration.cc:557:3\r\n    #3 0x15462ba2 in Envoy::(anonymous namespace)::InjectDataWithEchoFilterIntegrationTest_FilterChainMismatch_Test::TestBody() /proc/self/cwd/test/integration/filter_manager_integration_test.cc:481:3\r\n    #4 0x252aaa9d in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2433:10\r\n    #5 0x2525af95 in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2469:14\r\n    #6 0x252067c6 in testing::Test::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2508:5\r\n    #7 0x252094cf in testing::TestInfo::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2684:11\r\n    #8 0x2520bc3b in testing::TestSuite::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2816:28\r\n    #9 0x2523cce1 in testing::internal::UnitTestImpl::RunAllTests() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:5338:44\r\n    #10 0x252be19b in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2433:10\r\n    #11 0x25266335 in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:2469:14\r\n    #12 0x2523ab07 in testing::UnitTest::Run() /proc/self/cwd/external/com_google_googletest/googletest/src/gtest.cc:4925:10\r\n    #13 0x1e34edcf in RUN_ALL_TESTS() /proc/self/cwd/external/com_google_googletest/googletest/include/gtest/gtest.h:2473:46\r\n    #14 0x1e34c525 in Envoy::TestRunner::RunTests(int, char**) /proc/self/cwd/test/test_runner.cc:134:10\r\n    #15 0x1e3449a6 in main /proc/self/cwd/test/main.cc:46:10\r\n    #16 0x7f3c911ea82f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2082f)\r\n    #17 0x15349028 in _start (/b/f/w/bazel-out/k8-dbg/bin/test/integration/filter_manager_integration_test.runfiles/envoy/test/integration/filter_manager_integration_test+0x15349028)\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10680/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-04-07T16:44:44Z",
        "body": "cc @kyessenov I think you added this recently ^?"
      },
      {
        "user": "kyessenov",
        "created_at": "2020-04-07T16:58:52Z",
        "body": "I can bump the timeout. This test looks to be pretty heavy, and the test check seems to be polling for a callback for a short time. Let me see if I can also maybe move it out to a smaller test suite."
      }
    ]
  },
  {
    "number": 10612,
    "title": "Support file refactoring within a package in v3",
    "created_at": "2020-04-01T18:10:29Z",
    "closed_at": "2021-10-22T04:41:40Z",
    "labels": [
      "help wanted",
      "api/v3"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10612",
    "body": "As a result of the internal shadow construction technique used in #10355, it will not be possible to move protos between files (at least when they have deprecated v2 fields as reserved fields). This is a known limitation that should be addressed as needed, this issue tracks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10612/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "ankatare",
        "created_at": "2020-10-13T18:23:46Z",
        "body": "@htuch is this issue still need help or ok to take for 1.17 ? "
      },
      {
        "user": "htuch",
        "created_at": "2020-10-13T23:53:03Z",
        "body": "@ankatare I'd wait until we're done with the v2 deprecation in Q1 to take this on."
      },
      {
        "user": "ankatare",
        "created_at": "2021-10-21T10:05:31Z",
        "body": "@htuch Pls suggest if it is ready to take Now. "
      },
      {
        "user": "htuch",
        "created_at": "2021-10-22T04:41:40Z",
        "body": "I'm going to close this one out as we no longer have the tooling issues since the v2 removal."
      }
    ]
  },
  {
    "number": 10343,
    "title": "Explicit v2/v3 versioning in YAML/JSON files",
    "created_at": "2020-03-11T20:04:15Z",
    "closed_at": "2020-04-23T18:11:28Z",
    "labels": [
      "tech debt",
      "help wanted",
      "api/v3"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10343",
    "body": "After the recent v3 cut, Envoy attempts to upgrade to v3 any unparseable v2 config. This leads to v3 related messages to config that the user intends as v2, a bad UX. There are a few options to make this explicit:\r\n\r\n1. Allow version to be included in the filename, e.g. `foo.v2.yaml`, `bar.v3.yaml`.\r\n\r\n2. Add a CLI flag to Envoy to imply an explicit version for bootstrap JSON/YAML (and others?)\r\n\r\nCC @snowp ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10343/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "Mythra",
        "created_at": "2020-03-17T17:16:11Z",
        "body": "FWIW I think Option #2 here is by far the better option. I've definitely seen some people use: `.v2.yaml` in reference to the version of the file placed on disk through whatever generated the file. Don't think it's safe to rely on that."
      },
      {
        "user": "snowp",
        "created_at": "2020-04-13T19:19:33Z",
        "body": "I also like #2, let me see if I can get a PR up for this"
      },
      {
        "user": "htuch",
        "created_at": "2020-04-13T19:37:16Z",
        "body": "I think #2 is fine for bootstrap. For file-based xDS, we probably want to make use of the resource API version present in `ConfigSource`."
      }
    ]
  },
  {
    "number": 10266,
    "title": "/reopen_logs admin handler",
    "created_at": "2020-03-05T17:45:39Z",
    "closed_at": "2020-03-18T16:39:10Z",
    "labels": [
      "help wanted",
      "area/access_log"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10266",
    "body": "In our environment processes run in isolated process namespaces and cannot send signals to each other.\r\nWe need to implement correct log rotation for envoy, and possible solution is to expose /reopen_logs admin handler which triggers AccessLogManager's reopen() similar to SIGUSR1 handler.\r\n\r\nImplementation might be pretty straightforward:\r\n```\r\nHttp::Code AdminImpl::handlerReopenLogs(absl::string_view, Http::HeaderMap&,\r\n                                          Buffer::Instance& response, AdminStream&) {\r\n  server_.accessLogManager().reopen();\r\n  response.add(\"OK\\n\");\r\n  return Http::Code::OK;\r\n}\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10266/comments",
    "author": "veshij",
    "comments": [
      {
        "user": "veshij",
        "created_at": "2020-03-05T17:46:19Z",
        "body": "I can work on a PR if that sounds reasonable to upstream."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-03-05T17:48:52Z",
        "body": "Sure SGTM, thanks."
      }
    ]
  },
  {
    "number": 10108,
    "title": "coverage_tests fail in specific order",
    "created_at": "2020-02-19T21:23:28Z",
    "closed_at": "2020-06-09T04:15:59Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10108",
    "body": "when running coverage_tests as of 1223a2b4aef6eb90f3e34eebf496aead4930c4ab in shard 27 of 50:\r\n\r\nbacktrace from gdb:\r\n```\r\n#0  raise (sig=<optimized out>) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n#1  0x000000000f7c8f1e in Envoy::SignalAction::sigHandler(int, siginfo_t*, void*) ()\r\n#2  <signal handler called>\r\n#3  0x000000000e00f98f in grpc_core::LockfreeEvent::SetReady() ()\r\n#4  0x000000000e0067e2 in fd_become_readable ()\r\n#5  0x000000000e008e7e in pollable_process_events(grpc_pollset*, pollable*, bool) ()\r\n#6  0x000000000e006bf1 in pollset_work ()\r\n#7  0x000000000dffd034 in pollset_work(grpc_pollset*, grpc_pollset_worker**, long) ()\r\n#8  0x000000000e010105 in grpc_pollset_work(grpc_pollset*, grpc_pollset_worker**, long) ()\r\n#9  0x000000000e04af5a in cq_next(grpc_completion_queue*, gpr_timespec, void*) ()\r\n#10 0x000000000e04a0ce in grpc_completion_queue_next ()\r\n#11 0x000000000dde24ba in grpc_impl::CompletionQueue::AsyncNextInternal(void**, bool*, gpr_timespec) ()\r\n#12 0x000000000dd9dbd1 in grpc_impl::CompletionQueue::Next(void**, bool*) ()\r\n#13 0x000000000dd95bd5 in Envoy::Grpc::GoogleAsyncClientThreadLocal::completionThread() ()\r\n#14 0x000000000dd9b91c in Envoy::Grpc::GoogleAsyncClientThreadLocal::GoogleAsyncClientThreadLocal(Envoy::Api::Api&)::$_0::operator()() const ()\r\n#15 0x000000000dd9b8b1 in _ZNSt3__18__invokeIRZN5Envoy4Grpc28GoogleAsyncClientThreadLocalC1ERNS1_3Api3ApiEE3$_0JEEEDTclclsr3std3__1E7forwardIT_Efp_Espclsr3std3__1E7forwardIT0_Efp0_EEEOS9_DpOSA_ ()\r\n#16 0x000000000dd9b831 in void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Grpc::GoogleAsyncClientThreadLocal::GoogleAsyncClientThreadLocal(Envoy::Api::Api&)::$_0&>(Envoy::Grpc::GoogleAsyncClientThreadLocal::GoogleAsyncClientThreadLocal(Envoy::Api::Api&)::$_0&) ()\r\n#17 0x000000000dd9b7f1 in std::__1::__function::__alloc_func<Envoy::Grpc::GoogleAsyncClientThreadLocal::GoogleAsyncClientThreadLocal(Envoy::Api::Api&)::$_0(std::__1::allocator<std::__1::allocator>, void ())>::operator()() ()\r\n#18 0x000000000dd9a213 in std::__1::__function::__func<Envoy::Grpc::GoogleAsyncClientThreadLocal::GoogleAsyncClientThreadLocal(Envoy::Api::Api&)::$_0(std::__1::allocator<std::__1::allocator>, void ())>::operator()()\r\n    ()\r\n#19 0x00000000054bb1be in std::__1::__function::__value_func<void ()>::operator()() const ()\r\n#20 0x00000000054bb0a9 in std::__1::function<void ()>::operator()() const ()\r\n#21 0x000000000f78bbf6 in Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>)::$_0::operator()(void*) const ()\r\n#22 0x000000000f78bbb5 in Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>)::$_0::__invoke(void*) ()\r\n#23 0x00007efe89c6e6db in start_thread (arg=0x7efe83068700) at pthread_create.c:463\r\n#24 0x00007efe8999788f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n```\r\n\r\nReproduce steps:\r\n```\r\nHEAPCHECK= TEST_SRCDIR=bazel-out/k8-fastbuild/bin/test/coverage/coverage_tests.runfiles TEST_TMPDIR=/tmp GTEST_TOTAL_SHARDS=50 GTEST_SHARD_INDEX=26 ENVOY_IP_TEST_VERSIONS=v4only bazel-bin/test/coverage/coverage_tests\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10108/comments",
    "author": "lizan",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-06-09T04:15:59Z",
        "body": "I think this is no longer an issue with split binary."
      }
    ]
  },
  {
    "number": 10048,
    "title": "Lua receives empty body on httpCall to GRPC",
    "created_at": "2020-02-13T18:07:40Z",
    "closed_at": "2020-09-22T01:34:38Z",
    "labels": [
      "help wanted",
      "area/lua"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10048",
    "body": "When calling a GRPC service from a lua filter the returned body is always empty. Digging into this I think its related to the response handling code that copies the c++ string to a c string as the body of the http request starts with a null char because the grpc wire format is uncompressed. This doesn't happen though on the request sending, the lua string handles having a null as the first char.\r\n\r\n```lua\r\nlocal bit = require(\"bit\")\r\n\r\nfunction codeToProto(code)\r\n  return \"\\n\\n\" .. code\r\nend\r\n\r\nfunction protoToGRPC(proto)\r\n  local protoLength = string.len(proto)\r\n  local protoLengthBytes = {}\r\n  for i = 0, 3 do\r\n      protoLengthBytes[i] = bit.band(bit.rshift(protoLength,i * 8), 0xff)\r\n  end\r\n  return string.char(0, protoLengthBytes[3], protoLengthBytes[2], protoLengthBytes[1], protoLengthBytes[0]) .. proto\r\nend\r\n\r\nfunction envoy_on_request(request_handle)\r\n  local code = request_handle:headers():get(\"code\")\r\n\r\n  print(\"Making request for cluster of \" .. code)\r\n\r\n  local headers, response = request_handle:httpCall(\r\n    \"servicediscovery\",\r\n    {\r\n      [\":method\"] = \"POST\",\r\n      [\":path\"] = \"/envoy.Discovery/Lookup\",\r\n      [\":authority\"] = \"routing\",\r\n      [\"Content-Type\"] = \"application/grpc+proto\",\r\n      [\"TE\"] = \"trailers\"\r\n    },\r\n    protoToGRPC(codeToProto(code)),\r\n    5000\r\n  )\r\n  for key,value in pairs(headers) do print(key,value) end\r\n  print(string.byte(response, 1))\r\n  cluster = string.sub(response, 6)\r\n  print(string.len(response))\r\n\r\n  request_handle:headers():add(\"cluster\", cluster)\r\nend\r\n```\r\n\r\nends up with the following debug print\r\n\r\n```\r\nMaking request for cluster of 1234567\r\n:status\t200\r\ngrpc-accept-encoding\tgzip\r\nx-envoy-upstream-service-time\t221\r\ngrpc-encoding\tidentity\r\ncontent-type\tapplication/grpc\r\n\r\n0\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10048/comments",
    "author": "mtalbot",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2020-02-14T00:24:50Z",
        "body": "I expect you're getting a GRPC error, which is encoded in trailers. Trailers, unfortunately, are not exposed by the Envoy's `request_handle:httpCall`.  You could confirm this by turning on debug logging in Envoy. The trailers should be logged, including grpc-status and grpc-message trailers."
      },
      {
        "user": "mtalbot",
        "created_at": "2020-02-14T09:52:28Z",
        "body": "No, the response is fine\r\n\r\nThe request packet is\r\n\r\n```\r\n0000   02 00 00 00 45 00 01 2a 00 00 40 00 40 06 00 00   ....E..*..@.@...\r\n0010   7f 00 00 01 7f 00 00 01 da b9 23 1d 37 55 17 b9   ..........#.7U..\r\n0020   6f 69 9b b6 80 18 18 eb ff 1e 00 00 01 01 08 0a   oi..............\r\n0030   3d 2c d8 58 3d 2c d8 57 50 52 49 20 2a 20 48 54   =,.X=,.WPRI * HT\r\n0040   54 50 2f 32 2e 30 0d 0a 0d 0a 53 4d 0d 0a 0d 0a   TP/2.0....SM....\r\n0050   00 00 0c 04 00 00 00 00 00 00 04 10 00 00 00 00   ................\r\n0060   02 00 00 00 00 00 00 04 08 00 00 00 00 00 0f ff   ................\r\n0070   00 01 00 00 99 01 04 00 00 00 01 04 a7 60 87 a5   .............`..\r\n0080   72 1e a2 5b 62 f4 9a 44 98 b6 2e 5a bb 9f d2 f8   r..[b..D...Z....\r\n0090   2a 49 93 ea f9 90 43 f7 2d 9e 98 ce 73 f5 b6 bc   *I....C.-...s...\r\n00a0   15 24 c9 f5 83 41 87 2d 29 b0 f6 a4 d5 4d 86 5f   .$...A.-)....M._\r\n00b0   90 1d 75 d0 62 0d 26 3d 4c 4d 65 64 ff 75 d8 74   ..u.b.&=LMed.u.t\r\n00c0   9f 40 02 74 65 86 4d 83 35 05 b1 1f 0f 0d 02 31   .@.te.M.5......1\r\n00d0   37 40 8c f2 b1 6a ee 7f 4b 1a a4 96 ca 87 47 83   7@...j..K.....G.\r\n00e0   4d 96 97 40 8b f2 b4 a7 b3 c0 ec 90 b2 2d 29 ec   M..@.........-).\r\n00f0   87 0b a2 5c 2e ae 05 c5 40 96 f2 b1 6a ee 7f 4b   ...\\....@...j..K\r\n0100   17 cd 65 22 4b 22 d6 76 59 26 a4 a7 b5 2b 52 8f   ..e\"K\".vY&...+R.\r\n0110   83 6c 00 07 00 00 11 00 01 00 00 00 01 00 00 00   .l..............\r\n0120   00 0c 0a 0a 70 68 6f 73 31 32 33 34 38 38         ....phos123488\r\n``` \r\n\r\nThe response packet is\r\n```\r\n0000   02 00 00 00 45 00 00 c4 00 00 40 00 40 06 00 00   ....E.....@.@...\r\n0010   7f 00 00 01 7f 00 00 01 23 1d da b9 6f 69 9b e7   ........#...oi..\r\n0020   37 55 18 b8 80 18 18 e7 fe b8 00 00 01 01 08 0a   7U..............\r\n0030   3d 2c d9 2e 3d 2c d9 2c 00 00 4b 01 24 00 00 00   =,..=,.,..K.$...\r\n0040   01 00 00 00 00 0f 88 5f 10 61 70 70 6c 69 63 61   ......._.applica\r\n0050   74 69 6f 6e 2f 67 72 70 63 40 0d 67 72 70 63 2d   tion/grpc@.grpc-\r\n0060   65 6e 63 6f 64 69 6e 67 08 69 64 65 6e 74 69 74   encoding.identit\r\n0070   79 40 14 67 72 70 63 2d 61 63 63 65 70 74 2d 65   y@.grpc-accept-e\r\n0080   6e 63 6f 64 69 6e 67 04 67 7a 69 70 00 00 16 00   ncoding.gzip....\r\n0090   00 00 00 00 01 00 00 00 00 11 0a 0f 63 69 72 63   ............circ\r\n00a0   75 69 74 2d 64 65 66 61 75 6c 74 00 00 14 01 25   uit-default....%\r\n00b0   00 00 00 01 00 00 00 00 0f 40 0b 67 72 70 63 2d   .........@.grpc-\r\n00c0   73 74 61 74 75 73 01 30                           status.0\r\n```\r\n\r\n```\r\n[2020-02-14 09:48:25.976][26][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C97][S7104143247181254501] request end stream\r\nMaking request for cluster of phos123488\r\n[2020-02-14 09:48:25.983][26][debug][router] [source/common/router/router.cc:434] [C0][S14126135176097527152] cluster 'emtservicediscovery' match for URL '/com.concur.midtier.envoy.EntityDiscovery/LookupEntity'\r\n[2020-02-14 09:48:25.984][26][debug][router] [source/common/router/router.cc:549] [C0][S14126135176097527152] router decoding headers:\r\n':path', '/com.concur.midtier.envoy.EntityDiscovery/LookupEntity'\r\n':method', 'POST'\r\n':authority', 'emtrouting'\r\n':scheme', 'http'\r\n'content-type', 'application/grpc+proto'\r\n'te', 'trailers'\r\n'content-length', '17'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '172.17.0.2'\r\n'x-envoy-expected-rq-timeout-ms', '5000'\r\n\r\n[2020-02-14 09:48:25.985][26][debug][client] [source/common/http/codec_client.cc:31] [C98] connecting\r\n[2020-02-14 09:48:25.985][26][debug][connection] [source/common/network/connection_impl.cc:711] [C98] connecting to 192.168.65.2:8989\r\n[2020-02-14 09:48:25.985][26][debug][connection] [source/common/network/connection_impl.cc:720] [C98] connection in progress\r\n[2020-02-14 09:48:25.987][26][debug][http2] [source/common/http/http2/codec_impl.cc:912] [C98] setting stream-level initial window size to 268435456\r\n[2020-02-14 09:48:25.988][26][debug][http2] [source/common/http/http2/codec_impl.cc:934] [C98] updating connection-level initial window size to 268435456\r\n[2020-02-14 09:48:25.988][26][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2020-02-14 09:48:25.988][26][debug][lua] [source/extensions/filters/common/lua/lua.cc:40] coroutine yielded\r\n[2020-02-14 09:48:25.988][26][debug][connection] [source/common/network/connection_impl.cc:559] [C98] connected\r\n[2020-02-14 09:48:25.989][26][debug][client] [source/common/http/codec_client.cc:69] [C98] connected\r\n[2020-02-14 09:48:25.989][26][debug][pool] [source/common/http/http2/conn_pool.cc:98] [C98] creating stream\r\n[2020-02-14 09:48:25.989][26][debug][router] [source/common/router/router.cc:1618] [C0][S14126135176097527152] pool ready\r\n[2020-02-14 09:48:26.020][26][debug][router] [source/common/router/router.cc:1036] [C0][S14126135176097527152] upstream headers complete: end_stream=false\r\n[2020-02-14 09:48:26.020][26][debug][http] [source/common/http/async_client_impl.cc:93] async http request response headers (end_stream=false):\r\n':status', '200'\r\n'content-type', 'application/grpc'\r\n'grpc-encoding', 'identity'\r\n'grpc-accept-encoding', 'gzip'\r\n'x-envoy-upstream-service-time', '32'\r\n\r\n[2020-02-14 09:48:26.021][26][debug][client] [source/common/http/codec_client.cc:101] [C98] response complete\r\n[2020-02-14 09:48:26.021][26][debug][pool] [source/common/http/http2/conn_pool.cc:236] [C98] destroying stream: 0 remaining\r\n[2020-02-14 09:48:26.021][26][debug][http] [source/common/http/async_client_impl.cc:119] async http request response trailers:\r\n'grpc-status', '0'\r\n\r\n[2020-02-14 09:48:26.021][26][debug][lua] [source/extensions/filters/http/lua/lua_filter.cc:218] async HTTP response complete\r\n:status\t200\r\ngrpc-accept-encoding\tgzip\r\nx-envoy-upstream-service-time\t32\r\ngrpc-encoding\tidentity\r\ncontent-type\tapplication/grpc\r\n\r\n0\r\n[2020-02-14 09:48:26.022][26][debug][lua] [source/extensions/filters/common/lua/lua.cc:37] coroutine finished\r\n```\r\n\r\nThe body just isn't being returned\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-03-15T13:21:44Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-03-22T13:54:39Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "james-thomson",
        "created_at": "2020-09-02T10:53:51Z",
        "body": "We're seeing the same underlying issue without GRPC. \r\nIt seems that the httpCall function allows null byte in the request body but the response body is a null terminated string.\r\n\r\nCan this ticket be reopened so that the entire response body can be accessed in the lua script?"
      },
      {
        "user": "dio",
        "created_at": "2020-09-21T11:22:07Z",
        "body": "@mtalbot @james-thomson I have a draft here:  #13199, which I believe can solve this issue. However, to be sure, could you try this image: `dio123/envoy:lua-httpcall-raw-data` (note that this image is ubuntu-based image) and see if this works for you? Thank you!"
      }
    ]
  },
  {
    "number": 10012,
    "title": "http buffer filter integration test flake",
    "created_at": "2020-02-11T21:25:00Z",
    "closed_at": "2020-07-25T00:29:13Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10012",
    "body": "This seems to happen only in coverage tests. The last test logged as starting is always:\r\n\r\n```\r\n[----------] 1 test from Protocols/BufferIntegrationTest\r\n[ RUN      ] Protocols/BufferIntegrationTest.RouterRequestPopulateContentLengthOnTrailers/IPv4_Http2Downstream_HttpUpstream\r\nexternal/bazel_tools/tools/test/collect_coverage.sh: line 128: 27497 Segmentation fault      (core dumped) \"$@\"\r\n+ TEST_STATUS=139\r\n+ touch /build/tmp/_bazel_bazel/b570b5ccd0454dc9af9f65ab1833764d/sandbox/processwrapper-sandbox/4581/execroot/*****/bazel-out/k8-fastbuild/testlogs/test/coverage/coverage_tests/shard_24_of_50/coverage.dat\r\n+ [[ 139 -ne 0 ]]\r\n+ echo --\r\n--\r\n+ echo Coverage runner: Not collecting coverage for failed test.\r\nCoverage runner: Not collecting coverage for failed test.\r\n+ echo The following commands failed with status 139\r\nThe following commands failed with status 139\r\n+ echo /build/tmp/_bazel_bazel/b570b5ccd0454dc9af9f65ab1833764d/sandbox/processwrapper-sandbox/4581/execroot/*****/bazel-out/k8-fastbuild/bin/test/coverage/coverage_tests.runfiles/*****/test/coverage/coverage_tests --gmock_default_mock_behavior=2 '--log-path /dev/null' '-l trace'\r\n/build/tmp/_bazel_bazel/b570b5ccd0454dc9af9f65ab1833764d/sandbox/processwrapper-sandbox/4581/execroot/*****/bazel-out/k8-fastbuild/bin/test/coverage/coverage_tests.runfiles/*****/test/coverage/coverage_tests --gmock_default_mock_behavior=2 --log-path /dev/null -l trace\r\n+ exit 139\r\n```\r\n\r\nMaster commits where this failure occurred:\r\nenvoyproxy/envoy@4e1753fd683e8c0a901da62251640a434042d2d5\r\nenvoyproxy/envoy@7e60f33641cb005aaee6c8b878c7bf0476a863d9\r\nenvoyproxy/envoy@527853b4faa3106497a695a40c2322137546c142\r\nenvoyproxy/envoy@f75d47e72034f5db7aceeb3aad02ad1f57b34d8d\r\nenvoyproxy/envoy@9105aead0f5c89419bd2aca789b3738967c0baa1\r\nenvoyproxy/envoy@bbdc33e53723dc02b6d51bb0f329c5b369adfe03\r\nenvoyproxy/envoy@7801d058b2d3d9fc91cd8730cdbba0f76f88a0ee",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10012/comments",
    "author": "zuercher",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-07-25T00:29:13Z",
        "body": "Haven't seen this in a while, closing for now."
      }
    ]
  },
  {
    "number": 9998,
    "title": "proposal: make configuration of a tracing provider dynamic",
    "created_at": "2020-02-10T21:51:48Z",
    "closed_at": "2020-04-16T20:17:19Z",
    "labels": [
      "area/tracing",
      "design proposal",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9998",
    "body": "*Title*: Make configuration of a tracing provider dynamic\r\n\r\n*Context*:\r\n* at the moment, tracing provider is configured statically as part of bootstrap config\r\n* while this approach is very pragmatic, it breaks overall user experience that Envoy can reconfigured on-the-fly\r\n* although it would be possible to achieve a similar user experience through a hot restart, it feels like a workaround and doesn't seem a long-term solution\r\n \r\n*Proposal*:\r\n* Make configuration of a tracing provider dynamic\r\n\r\n*Example use cases*:\r\n* change a tracing provider on-the-fly (e.g., `zipkin` to `opentracing`)\r\n* reconfigure an active tracing provider (e.g., address of a collector)\r\n\r\n*Anticipated scope of changes*:\r\n* extend Envoy bootstrap config with a new trace driver - `envoy.tracers.dynamic`\r\n* implement configuration model for `dynamic` tracer, e.g. as a new independent xDS API or as a higher-level abstraction on top of RTDS (Runtime Discovery)\r\n* implement a `dynamic` tracer as a smart reference to the actual implementation\r\n\r\n*Open questions*:\r\n* which configuration model to choose: new xDS, RTDS or something else? Does Envoy already have an example of turning a part of bootstrap config into a dynamic configuration ?\r\n* it's possible that some tracing providers might not support being configured multiple times (e.g., `envoy.dynamic.ot` and `envoy.tracers.opencensus` look questionable). Hopefully, it's acceptable to sacrifice them in the beginning and let them improve over time",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9998/comments",
    "author": "yskopets",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-02-10T23:02:13Z",
        "body": "@yskopets can we just remove tracing from bootstrap entirely and just define it as part of the HCM configuration that would come down as part of LDS? This is similar to what @ramaraochavali did around the rate limit service configuration. Can we follow the same pattern?"
      },
      {
        "user": "yskopets",
        "created_at": "2020-02-11T07:13:12Z",
        "body": "@mattklein123 Let me take a look into rate limit example"
      },
      {
        "user": "yskopets",
        "created_at": "2020-02-11T16:43:03Z",
        "body": "@mattklein123 Here is the updated proposal that follows rate limit example:\r\n\r\n* Update configuration of model of `HttpConnectionManager` to support in-place definition\r\n  of a `tracing_provider`, e.g.\r\n  ```\r\n  message HttpConnectionManager {\r\n    ...\r\n    message Tracing {\r\n      ...\r\n\r\n      // Configuration for an external tracing provider.\r\n      //\r\n      // Within depracation window, this field may be omitted,\r\n      // in which case Envoy will fall back the to tracing provider configuration\r\n      // from the :ref:`Bootstrap <envoy_api_msg_config.bootstrap.v3.Bootstrap>` config.\r\n      //\r\n      // Once deprecation window is over, tracing provider configuration will be excluded\r\n      // from the :ref:`Bootstrap <envoy_api_msg_config.bootstrap.v3.Bootstrap>` config\r\n      // and this field will become mandatory.\r\n      trace.v3.Tracing tracing_provider = 9;\r\n    }\r\n  }\r\n  ```\r\n  Sample configuration will look the following way:\r\n  ```yaml\r\n  filter_chains:\r\n  - filters:\r\n    - name: envoy.http_connection_manager\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v3.HttpConnectionManager\r\n        tracing:\r\n          tracing_provider:\r\n            http:\r\n              name: envoy.zipkin\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.config.trace.v2.ZipkinConfig\r\n                collector_cluster: zipkin\r\n                collector_endpoint: \"/api/v2/spans\"\r\n                collector_endpoint_version: HTTP_JSON\r\n        codec_type: auto\r\n        ...\r\n        http_filters:\r\n        - name: envoy.router\r\n          typed_config: {}  \r\n  ```\r\n  Notice `tracing :: tracing_provider :: http` path. Probably, it should be simplified.\r\n* Introduce `HttpTracerCache` interface\r\n  ```\r\n  /**\r\n   * Interface for a cache of HttpTracers.\r\n   *\r\n   * The cache is responsible for de-duplicating tracers with identical configuration.\r\n   *\r\n   * Notice that the cache doesn't deal with threading since HttpTracers are required to be thread-safe\r\n   * and already address this problem internally.\r\n   */\r\n  class HttpTracerCache {\r\n  public:\r\n    virtual ~HttpTracerCache() = default;\r\n  \r\n    /**\r\n     * Get existing HttpTracer or create a new one for a given configuration.\r\n     * @param config supplies the configuration for the tracing provider.\r\n     * @return HttpTracerSharedPtr.\r\n     */\r\n    virtual HttpTracerSharedPtr getOrCreateHttpTracer(\r\n        const envoy::config::trace::v3::Tracing& config) PURE;\r\n  };\r\n  ```\r\n* Introduce `HttpTracerCacheImpl` as a singleton\r\n  ```\r\n  class HttpTracerCacheImpl : public Singleton::Instance, public HttpTracerCache {\r\n    ...\r\n  }\r\n  ```\r\n* Refactor server startup process to initialize `HttpTracerCache` singleton\r\n  and use it to create an `HttpTracer` out of Bootstrap config\r\n* Exclude `HttpTracer` from everywhere, namely from\r\n  * `Envoy::Http::Context`\r\n  * `Envoy::Server::Configuration::Main`\r\n  * `Envoy::Server::Configuration::FactoryContext`\r\n* Update `HttpConnectionManagerConfig` to use `HttpTracerCache` singleton to get/create\r\n  an `HttpTracer` and store it in the `HttpConnectionManagerConfig`\r\n* Update `Http::ConnectionManagerImpl::ActiveStream` to use `HttpTracer` from\r\n  `HttpConnectionManagerConfig`\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-02-11T17:11:05Z",
        "body": "+1 looks great. @ramaraochavali WDYT?"
      },
      {
        "user": "yskopets",
        "created_at": "2020-03-03T19:07:08Z",
        "body": "@mattklein123 I'm actively working on this feature"
      },
      {
        "user": "yskopets",
        "created_at": "2020-04-13T12:04:58Z",
        "body": "@kyessenov @dio Tracing configuration is now fully dynamic, both in `v2` and `v3` APIs.\r\nThe remaining PRs are optimizations and refactorings.\r\n\r\nIf you were planning to use this feature, you can give it a try already."
      },
      {
        "user": "kyessenov",
        "created_at": "2020-04-13T17:03:39Z",
        "body": "Thanks for all your work!"
      },
      {
        "user": "aldecarolis",
        "created_at": "2023-11-20T16:44:26Z",
        "body": "Hi, I have a problem with this feature, on envoy proxy I see sometimes: \r\n- instantiating a new tracer: envoy.dynamic.ot -> the zipkin tracer will not work\r\n- instantiating a new tracer: envoy.tracers.zipkin -> the zipkin tracer will work\r\n\r\nIt is still possible to force a static parameter to force the use of zipkin tracer on the envoy sidecar?"
      }
    ]
  },
  {
    "number": 9872,
    "title": "CacheFilter: Cache::Utils methods need more tests",
    "created_at": "2020-01-29T20:59:24Z",
    "closed_at": "2020-08-12T22:38:12Z",
    "labels": [
      "help wanted",
      "area/cache"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9872",
    "body": "The tests for Cache::Utils are too thin, and should be enhanced, though some of this may be supplanted by #9833.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9872/comments",
    "author": "toddmgreer",
    "comments": [
      {
        "user": "yosrym93",
        "created_at": "2020-07-27T19:48:36Z",
        "body": "After #11727, the only remaining function that needs more tests is `CacheHeadersUtils::httpTime`."
      }
    ]
  },
  {
    "number": 9778,
    "title": "Remove x-envoy-decorator-operation response header?",
    "created_at": "2020-01-22T18:43:38Z",
    "closed_at": "2020-02-08T00:43:36Z",
    "labels": [
      "area/tracing",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9778",
    "body": "Is there a way to remove the x-envoy-decorator-operation header from the response to the client? I'm not directly setting that header anywhere, I assume it is being set by Envoy using the decorator value. I've tried setting the response_headers_to_remove setting in the route_config but that has had no effect. Here is the route_config I am using:\r\n\r\n>           \r\n          route_config:\r\n            name: xxxx\r\n            response_headers_to_remove:\r\n              - x-envoy-decorator-operation\r\n            virtual_hosts:\r\n            - name: xxxx\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  timeout: 300s\r\n                  idle_timeout:\r\n                  cluster: xxxx\r\n                decorator:\r\n                  operation: xxxx\r\n          http_filters:\r\n          - name: envoy.router\r\n            config:\r\n              suppress_envoy_headers: true",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9778/comments",
    "author": "jaegchoi",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-01-22T23:51:46Z",
        "body": "IIRC there is no way to do this today. cc @objectiser "
      },
      {
        "user": "objectiser",
        "created_at": "2020-01-23T09:30:31Z",
        "body": "@jaegchoi If you remove,\r\n```\r\n            decorator:\r\n              operation: xxxx\r\n```\r\nfrom your route config, that should do it. This is specifying the span name/operation to be used in the tracing data produced by the envoy proxy handling the service request - and then the response header is returning the same information to the envoy proxy handling the client side of the request, so that it can use the same span name/operation in its tracing data - designed to provide consistency in the tracing data.\r\n"
      },
      {
        "user": "jaegchoi",
        "created_at": "2020-01-23T18:38:20Z",
        "body": "@objectiser We use the decorators to differentiate our different spans in Jaeger. We would like to keep the decorator for the use of tracing, but remove it from the response headers to the client after Envoy has sent all the tracing information to the collector. \r\n\r\nIf this is not yet possible, could I make this a feature request? We would like to hide all knowledge of Envoy being used from the client for security reasons"
      },
      {
        "user": "objectiser",
        "created_at": "2020-01-24T09:58:57Z",
        "body": "@jaegchoi Its not currently possible, so would need to be a feature request.\r\n\r\n@mattklein123 Could this be labelled as feature/enhancement?"
      }
    ]
  },
  {
    "number": 9712,
    "title": "protobuf: update pertinent sites to anyConvertAndValidate",
    "created_at": "2020-01-17T01:53:56Z",
    "closed_at": "2020-02-19T17:23:03Z",
    "labels": [
      "tech debt",
      "help wanted",
      "beginner",
      "area/configuration"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9712",
    "body": "*Description*:\r\nThere are several places that use anyConvert and then validate the typed message. #9516 introduced `anyConvertAndValidate`. Update call pertinent callsites to the new function.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9712/comments",
    "author": "junr03",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-02-16T02:51:26Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "junr03",
        "created_at": "2020-02-19T17:21:48Z",
        "body": "@mattklein123 did you mean to re-open this? "
      },
      {
        "user": "mattklein123",
        "created_at": "2020-02-19T17:23:03Z",
        "body": "Sorry I didn't realize it is done."
      }
    ]
  },
  {
    "number": 9709,
    "title": "RTDS should be fully warmed before ClusterManager initialization",
    "created_at": "2020-01-16T21:16:56Z",
    "closed_at": "2020-04-20T22:02:59Z",
    "labels": [
      "tech debt",
      "help wanted",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9709",
    "body": "Today, RTDS warms at the same time as the `ClusterManager`, on the server `InitManager`. There may be resources in the first CDS response that require RTDS override. In eventually consistent systems, this makes initialization non-deterministic when override is intended. We should probably add another server init warming stage where we only start CDS after RTDS has succeeded/failed/timed out.\r\n\r\nCC @alyssawilk @mattklein123 @rfaulk",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9709/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-01-16T21:20:33Z",
        "body": "Yeah +1"
      },
      {
        "user": "stevenzzzz",
        "created_at": "2020-02-11T16:13:25Z",
        "body": "/cc stevenzzzz\r\n"
      }
    ]
  },
  {
    "number": 9573,
    "title": "CI check for fuzz coverage",
    "created_at": "2020-01-06T18:22:01Z",
    "closed_at": "2020-07-19T09:39:42Z",
    "labels": [
      "area/security",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9573",
    "body": "Similar to what we have today for test coverage, a separate job that measures fuzz coverage and prevents check-in when we drop below a certain level will incentivize more fuzzing work done across the developer community. We could narrow this to just extensions and data plane, or make it proxy wide. It won't start as 98%, but even if only 30%, it would provide a useful measure for how we are fairing and prevent regression. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9573/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "asraa",
        "created_at": "2020-03-03T18:03:09Z",
        "body": "We might also want to target certain specific files to have a higher fuzz coverage % -- codec_impls, tokenization libraries, etc."
      }
    ]
  },
  {
    "number": 9567,
    "title": "API boosting via clang-tidy",
    "created_at": "2020-01-06T04:00:32Z",
    "closed_at": "2023-01-06T04:35:53Z",
    "labels": [
      "tech debt",
      "help wanted",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9567",
    "body": "To prevent regression from latest API version, we should regularly run the API booster. If we can transform it to be a standard clang-tidy plugin, we can make this part of the CI clang-tidy phase.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9567/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "ankatare",
        "created_at": "2020-07-22T05:38:22Z",
        "body": "@htuch would like to understand more on requirements. "
      },
      {
        "user": "lgyhit",
        "created_at": "2022-12-22T09:45:34Z",
        "body": "what API boosting used for in envoy ?"
      },
      {
        "user": "htuch",
        "created_at": "2023-01-06T04:35:53Z",
        "body": "This issue is stale due to the move to freeze the major version at 3 permanently."
      }
    ]
  },
  {
    "number": 9560,
    "title": "ENVOY_LOG(warn, ...) vs -l warning inconsistency",
    "created_at": "2020-01-04T15:18:40Z",
    "closed_at": "2020-02-06T19:24:08Z",
    "labels": [
      "bug",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9560",
    "body": "In our C++ code we issue warnings by using the `warn` warg to` ENVOY_LOG*` macros.\r\n\r\nBut when specifying the loglevel we say `-l warning`, and there is no sanity-check for a reasonable `-l foo` arg.\r\n\r\nWe should make these consistent (at least allowing `-l warn`) and also issue a startup error if you give a bogus argument to `-l`.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9560/comments",
    "author": "jmarantz",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-01-05T17:19:35Z",
        "body": "Agreed we should make this better."
      },
      {
        "user": "rulex123",
        "created_at": "2020-01-31T10:07:31Z",
        "body": "Hi! I would like to help out with this."
      }
    ]
  },
  {
    "number": 9352,
    "title": "gRPC status operator for file access logs",
    "created_at": "2019-12-14T03:45:27Z",
    "closed_at": "2020-04-14T17:26:15Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9352",
    "body": "Right now the only way to infer the failure reason for gRPC requests in file access logs is to output `%TRAILER(grpc-status)%`, which outputs the gRPC status as a number. It would be very useful to have an operator that output the error name (e.g. `UNAVAILABLE` instead of `14`).\r\n\r\nThis would also make it easier to handle the fact that the `grpc-status` header might appear in either the headers or in the trailers: the `%GRPC_STATUS%` operator would just check either.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9352/comments",
    "author": "snowp",
    "comments": [
      {
        "user": "Shikugawa",
        "created_at": "2020-02-20T02:09:45Z",
        "body": "@snowp Is anyone working on this? If not, I will implement."
      },
      {
        "user": "snowp",
        "created_at": "2020-02-20T02:10:32Z",
        "body": "Not that I'm aware, feel free to take this one. "
      }
    ]
  },
  {
    "number": 9269,
    "title": "SubjectAlternateName protobuf message omits IP address names",
    "created_at": "2019-12-09T00:39:31Z",
    "closed_at": "2020-03-06T16:34:54Z",
    "labels": [
      "help wanted",
      "area/admin"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9269",
    "body": "*Title*: \r\n\r\nSubjectAlternateName protobuf message omits IP address names\r\n\r\n*Description*:\r\n\r\nThe `SubjectAlternateName` protobuf introduced in #4566 omits the IP address alternate names. These should be included for completeness and consistency.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9269/comments",
    "author": "jpeach",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-01-08T04:26:16Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "ramaraochavali",
        "created_at": "2020-01-08T04:42:34Z",
        "body": "not stale. Will work in coming weeks"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-07T04:47:53Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "ramaraochavali",
        "created_at": "2020-02-07T06:37:17Z",
        "body": "@dio Thanks for adding help wanted. I meant to do it - but busy with other stuff :-)"
      }
    ]
  },
  {
    "number": 9230,
    "title": "quiche: add a reloadable feature to enable/disable QUIC support",
    "created_at": "2019-12-04T22:32:49Z",
    "closed_at": "2020-05-22T14:31:29Z",
    "labels": [
      "help wanted",
      "area/quic"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9230",
    "body": "Current QUIC support can only be turned on/off via LDS. We can do better here by adding an Envoy reloadable feature for QUIC to turn the switch via runtime dynamic config push. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9230/comments",
    "author": "danzh2010",
    "comments": [
      {
        "user": "nezdolik",
        "created_at": "2019-12-16T22:50:26Z",
        "body": "@mattklein123, could help with this task."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-12-19T16:27:36Z",
        "body": "@nezdolik sgtm. TBH though I'm not completely sure how we can do this easily since there is no fallback. I think the only thing we could potentially do is just have a kill-switch for the UDP processing in general and then assume the client has a fallback. @alyssawilk is that what you were thinking?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-01-06T14:50:30Z",
        "body": "Yeah.  Internally I _think_ we have two modes of shut-off.  One for \"we would like to turn QUIC off gracefully\" which stops reading and sends public resets to active sessions (a few per event loop), and the \"UDP is deemed dangerous\" which just stops all UDP traffic.\r\nEither way all clients should always fail over to TCP because you can never trust UDP on the internet, so even with silent shut-off it should time out, re-try handshake, and fail over.  It just takes a bit more time without the reset."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-05-22T14:31:29Z",
        "body": "Fixed."
      }
    ]
  },
  {
    "number": 9054,
    "title": "Assertion on non-empty upstream_requests in Router::Filter destructor in stress test ",
    "created_at": "2019-11-18T06:43:35Z",
    "closed_at": "2019-11-22T01:51:21Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9054",
    "body": "*Title*: Envoy crash with assertion on upstream_requests not empty in Router::Filter destructor in stress test using wrk\r\n\r\n*Description*:\r\n\r\n\\During stress test using wrk with command similar to following:\r\n\r\nwrk -t 1 --timeout 2m -c 10 -d 60s\r\n\r\nwith these data\r\n\r\n= 1 threads and 10 connections\r\n= Thread Stats   Avg      Stdev     Max   +/- Stdev\r\n= Latency    38.91ms    5.71ms 101.70ms   90.33%\r\n= Req/Sec   257.68     28.93   303.00     71.83%\r\n= 15411 requests in 1.00m, 3.48MB read\r\n= Requests/sec:    256.63\r\n= Transfer/sec:     59.40KB\r\n\r\n*Call Stack*:\r\n<pre>\r\n#0  raise (sig=6) at ../sysdeps/unix/sysv/linux/raise.c:50\r\n#1  0x000055db2583a4bb in Envoy::SignalAction::sigHandler (sig=6, info=0x7f7f22b47e70, context=0x7f7f22b47d40) at external/envoy/source/common/signal/signal_action.cc:74\r\n#2  <signal handler called>\r\n#3  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50\r\n#4  0x00007f7f25bc9535 in __GI_abort () at abort.c:79\r\n#5  0x000055db2530a25c in Envoy::Router::Filter::~Filter (this=0x55db28cf5b90, __in_chrg=<optimized out>) at external/envoy/source/common/router/router.cc:253\r\n#6  0x000055db25308894 in Envoy::Router::ProdFilter::~ProdFilter (this=0x55db28cf5b90, __in_chrg=<optimized out>) at bazel-out/k8-dbg/bin/external/envoy/source/common/router/_virtual_includes/router_lib/common/router/router.h:594\r\n#7  0x000055db24a09493 in __gnu_cxx::new_allocator<Envoy::Router::ProdFilter>::destroy<Envoy::Router::ProdFilter> (this=0x55db28cf5b90, __p=0x55db28cf5b90) at /usr/include/c++/7/ext/new_allocator.h:140\r\n#8  0x000055db24a09301 in std::allocator_traits<std::allocator<Envoy::Router::ProdFilter> >::destroy<Envoy::Router::ProdFilter> (__a=..., __p=0x55db28cf5b90) at /usr/include/c++/7/bits/alloc_traits.h:487\r\n#9  0x000055db24a08ffd in std::_Sp_counted_ptr_inplace<Envoy::Router::ProdFilter, std::allocator<Envoy::Router::ProdFilter>, (__gnu_cxx::_Lock_policy)2>::_M_dispose (this=0x55db28cf5b80) at /usr/include/c++/7/bits/shared_ptr_base.h:535\r\n#10 0x000055db24618270 in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release (this=0x55db28cf5b80) at /usr/include/c++/7/bits/shared_ptr_base.h:154\r\n#11 0x000055db24616ceb in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count (this=0x55db2b770588, __in_chrg=<optimized out>) at /usr/include/c++/7/bits/shared_ptr_base.h:684\r\n#12 0x000055db24616586 in std::__shared_ptr<Envoy::Http::StreamDecoderFilter, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr (this=0x55db2b770580, __in_chrg=<optimized out>) at /usr/include/c++/7/bits/shared_ptr_base.h:1123\r\n#13 0x000055db246165a2 in std::shared_ptr<Envoy::Http::StreamDecoderFilter>::~shared_ptr (this=0x55db2b770580, __in_chrg=<optimized out>) at /usr/include/c++/7/bits/shared_ptr.h:93\r\n#14 0x000055db253de61e in Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter::~ActiveStreamDecoderFilter (this=0x55db2b770540, __in_chrg=<optimized out>, __vtt_parm=<optimized out>)\r\n    at bazel-out/k8-dbg/bin/external/envoy/source/common/http/_virtual_includes/conn_manager_lib/common/http/conn_manager_impl.h:206\r\n#15 0x000055db253de692 in Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter::~ActiveStreamDecoderFilter (this=0x55db2b770540, __in_chrg=<optimized out>, __vtt_parm=<optimized out>)\r\n    at bazel-out/k8-dbg/bin/external/envoy/source/common/http/_virtual_includes/conn_manager_lib/common/http/conn_manager_impl.h:206\r\n#16 0x000055db253de6e2 in std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter>::operator() (this=0x55db28f35a30, __ptr=0x55db2b770540) at /usr/include/c++/7/bits/unique_ptr.h:78\r\n#17 0x000055db253dc6fb in std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter> >::~unique_ptr (this=0x55db28f35a30,\r\n    __in_chrg=<optimized out>) at /usr/include/c++/7/bits/unique_ptr.h:268\r\n#18 0x000055db253e2dfe in __gnu_cxx::new_allocator<std::_List_node<std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter> > > >::destroy<std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter> > > (this=0x55db2975f238, __p=0x55db28f35a30)\r\n    at /usr/include/c++/7/ext/new_allocator.h:140\r\n#19 0x000055db253e0673 in std::allocator_traits<std::allocator<std::_List_node<std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter> > > > >::destroy<std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter> > > (__a=..., __p=0x55db28f35a30)\r\n    at /usr/include/c++/7/bits/alloc_traits.h:487\r\n#20 0x000055db253de364 in std::__cxx11::_List_base<std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter> >, std::allocator<std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter> > > >::_M_clear (this=0x55db2975f238) at /usr/include/c++/7/bits/list.tcc:76\r\n#21 0x000055db253dc448 in std::__cxx11::_List_base<std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter> >, std::allocator<std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter> > > >::~_List_base (this=0x55db2975f238, __in_chrg=<optimized out>)\r\n    at /usr/include/c++/7/bits/stl_list.h:442\r\n#22 0x000055db253db200 in std::__cxx11::list<std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter> >, std::allocator<std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter> > > >::~list (this=0x55db2975f238, __in_chrg=<optimized out>)\r\n    at /usr/include/c++/7/bits/stl_list.h:733\r\n#23 0x000055db253c9ee9 in Envoy::Http::ConnectionManagerImpl::ActiveStream::~ActiveStream (this=0x55db2975f180, __in_chrg=<optimized out>) at external/envoy/source/common/http/conn_manager_impl.cc:542\r\n#24 0x000055db253ca08e in Envoy::Http::ConnectionManagerImpl::ActiveStream::~ActiveStream (this=0x55db2975f180, __in_chrg=<optimized out>) at external/envoy/source/common/http/conn_manager_impl.cc:587\r\n#25 0x000055db246c77ca in std::default_delete<Envoy::Event::DeferredDeletable>::operator() (this=0x55db29ea8800, __ptr=0x55db2975f180) at /usr/include/c++/7/bits/unique_ptr.h:78\r\n#26 0x000055db24f5ab97 in std::unique_ptr<Envoy::Event::DeferredDeletable, std::default_delete<Envoy::Event::DeferredDeletable> >::reset (this=0x55db29ea8800, __p=0x55db2975f180) at /usr/include/c++/7/bits/unique_ptr.h:376\r\n#27 0x000055db24f58392 in Envoy::Event::DispatcherImpl::clearDeferredDeleteList (this=0x55db283986e0) at external/envoy/source/common/event/dispatcher_impl.cc:95\r\n#28 0x000055db24f576cd in Envoy::Event::DispatcherImpl::<lambda()>::operator()(void) const (__closure=0x55db28509b50) at external/envoy/source/common/event/dispatcher_impl.cc:42\r\n#29 0x000055db24f59ad1 in std::_Function_handler<void(), Envoy::Event::DispatcherImpl::DispatcherImpl(Envoy::Buffer::WatermarkFactoryPtr&&, Envoy::Api::Api&, Envoy::Event::TimeSystem&)::<lambda()> >::_M_invoke(const std::_Any_data &) (\r\n    __functor=...) at /usr/include/c++/7/bits/std_function.h:316\r\n#30 0x000055db246bf992 in std::function<void ()>::operator()() const (this=0x55db28509b50) at /usr/include/c++/7/bits/std_function.h:706\r\n#31 0x000055db24f94a00 in Envoy::Event::TimerImpl::<lambda(int, short int, void*)>::operator()(int, short, void *) const (__closure=0x0, arg=0x55db28509ad0) at external/envoy/source/common/event/timer_impl.cc:23\r\n#32 0x000055db24f94ab9 in Envoy::Event::TimerImpl::<lambda(int, short int, void*)>::_FUN(int, short, void *) () at external/envoy/source/common/event/timer_impl.cc:23\r\n#33 0x000055db255f67ba in event_process_active_single_queue (base=0x55db2848c580, activeq=0x55db283646d0, max_to_process=2147483647, endtime=0x0)\r\n    at /home/qiwzhang/.cache/bazel/_bazel_qiwzhang/60020ca4eb4bbfdb82eb82dc35e1335d/sandbox/linux-sandbox/1893/execroot/gcpproxy/external/com_github_libevent_libevent/event.c:1713\r\n#34 0x000055db255f6ca1 in event_process_active (base=0x55db2848c580)\r\n    at /home/qiwzhang/.cache/bazel/_bazel_qiwzhang/60020ca4eb4bbfdb82eb82dc35e1335d/sandbox/linux-sandbox/1893/execroot/gcpproxy/external/com_github_libevent_libevent/event.c:1805\r\n#35 0x000055db255f7557 in event_base_loop (base=0x55db2848c580, flags=0)\r\n    at /home/qiwzhang/.cache/bazel/_bazel_qiwzhang/60020ca4eb4bbfdb82eb82dc35e1335d/sandbox/linux-sandbox/1893/execroot/gcpproxy/external/com_github_libevent_libevent/event.c:2047\r\n#36 0x000055db24f93516 in Envoy::Event::LibeventScheduler::run (this=0x55db28398730, mode=Envoy::Event::Dispatcher::RunType::Block) at external/envoy/source/common/event/libevent_scheduler.cc:47\r\n#37 0x000055db24f59707 in Envoy::Event::DispatcherImpl::run (this=0x55db283986e0, type=Envoy::Event::Dispatcher::RunType::Block) at external/envoy/source/common/event/dispatcher_impl.cc:194\r\n#38 0x000055db24f4225c in Envoy::Server::WorkerImpl::threadRoutine (this=0x55db2855a150, guard_dog=...) at external/envoy/source/server/worker_impl.cc:110\r\n#39 0x000055db24f41b00 in Envoy::Server::WorkerImpl::<lambda()>::operator()(void) const (__closure=0x55db28a01268) at external/envoy/source/server/worker_impl.cc:75\r\n#40 0x000055db24f42b6e in std::_Function_handler<void(), Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/7/bits/std_function.h:316\r\n#41 0x000055db246bf992 in std::function<void ()>::operator()() const (this=0x55db28a01268) at /usr/include/c++/7/bits/std_function.h:706\r\n#42 0x000055db2583fae8 in Envoy::Thread::ThreadImplPosix::<lambda(void*)>::operator()(void *) const (__closure=0x0, arg=0x55db28a01260) at external/envoy/source/common/common/posix/thread_impl.cc:33\r\n#43 0x000055db2583fb0c in Envoy::Thread::ThreadImplPosix::<lambda(void*)>::_FUN(void *) () at external/envoy/source/common/common/posix/thread_impl.cc:35\r\n#44 0x00007f7f25d6ffa3 in start_thread (arg=<optimized out>) at pthread_create.c:486\r\n#45 0x00007f7f25ca04cf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n</pre>",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9054/comments",
    "author": "qiwzhang",
    "comments": [
      {
        "user": "qiwzhang",
        "created_at": "2019-11-18T16:34:14Z",
        "body": "This is from \r\n\r\nENVOY_SHA1 = \"619eb5735f038a568df6497f28fac1bf809b2379\"  # 11.06.2019\r\n\r\n"
      },
      {
        "user": "junr03",
        "created_at": "2019-11-18T18:58:37Z",
        "body": "@snowp could you take a look at this?"
      },
      {
        "user": "snowp",
        "created_at": "2019-11-18T22:24:57Z",
        "body": "It seems like somehow we're handling the defer delete for the HCM ActiveStream without all the `upstream_requests_` being removed. The only place we call `deferredDelete` we very explicitly call `onDestroy` (which resets all upstream requests) for all the HCM filters before moving the stream onto the list of deferred deletions. \r\n\r\nI'm wondering if what's going on is that the ActiveStream is being shut down between `newStream` being called on the HCM and `decodeHeaders` called on the stream: since we defer delete the ActiveStream, I think it might be possible for a `decodeHeaders` event to be enqueued while the event to destroy (e.g. due to a timeout, which wouldn't yet stop the headers from being processed) the stream has yet to be executed, resulting in a new upstream request being created between `onDestroy` and the list of deferred deletes being cleared.\r\n\r\nIf this is the case we should probably just bail out early in `decodeHeaders` based on `state_.destroyed_`\r\n\r\n@alyssawilk "
      },
      {
        "user": "qiwzhang",
        "created_at": "2019-11-18T23:10:38Z",
        "body": "In our test setup, we had a  http filter making async_client call.  While the async_client call is in progress,  the filter is paused with STOP.    This filter is not handling onDestroy() to cancel active async_client call.  Could this be the root cause?\r\n\r\nBut, the upsteram_stream_ in Router is created on the Router::decodeHeader(). I assume if my filter decodeHeader() is paused with STOP,  Router::decodeHeader() should not be called.  Since Router::upstream_requests is not empty when crash,  it means my filter async_client call should have returned, it should have returned CONTINUE so that Router::decodeHeader() is called.\r\n"
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-11-18T23:12:37Z",
        "body": "If this is reproducible, any chance you can rerun with -l trace and share the logs of the offending connection with us?  It might shortcut a lot of debugging!"
      },
      {
        "user": "qiwzhang",
        "created_at": "2019-11-18T23:20:04Z",
        "body": "It is a stress test.  with over 10k requests within 1m,   with \"-l trace\", it will generate too much logs.  problem may not happen.  "
      },
      {
        "user": "qiwzhang",
        "created_at": "2019-11-18T23:22:07Z",
        "body": "reproduce steps is: run a stress test with over 10k request per minute for a couple hours."
      },
      {
        "user": "qiwzhang",
        "created_at": "2019-11-20T18:48:12Z",
        "body": "Update:  I am trying to cancel the async_client in-flight request when filter is gone in my http custom filter to see if it fixes the problem.  \r\n\r\nWill update the result here."
      },
      {
        "user": "qiwzhang",
        "created_at": "2019-11-22T01:51:21Z",
        "body": "After I cancel the in-flight async_client request on my custom Http Filter::onDestroy(), the problem went away.   Hence I close the issue."
      }
    ]
  },
  {
    "number": 8940,
    "title": "TSAN warning",
    "created_at": "2019-11-07T22:32:38Z",
    "closed_at": "2019-11-14T22:36:47Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8940",
    "body": "Detected in Envoy 1.12:\r\n```\r\nWARNING: ThreadSanitizer: data race (pid=7313)\r\n  Read of size 8 at 0x7b1800014778 by thread T9:\r\n    #0 std::__1::unique_ptr<Envoy::Event::Timer, std::__1::default_delete<Envoy::Event::Timer> >::operator bool() const <null> (envoy+0x2e4c417)\r\n    #1 bool std::__1::operator!=<Envoy::Event::Timer, std::__1::default_delete<Envoy::Event::Timer> >(std::__1::unique_ptr<Envoy::Event::Timer, std::__1::default_delete<Envoy::Event::Timer> > const&, std::nullptr_t) <null> (envoy+0x461d9a4)\r\n    #2 Envoy::Server::DrainManagerImpl::draining() const <null> (envoy+0x60e5996)\r\n    #3 Envoy::Server::DrainManagerImpl::drainClose() const <null> (envoy+0x60e0388)\r\n    #4 Envoy::Server::ListenerImpl::drainClose() const <null> (envoy+0x608cb55)\r\n    #5 non-virtual thunk to Envoy::Server::ListenerImpl::drainClose() const <null> (envoy+0x608cc5f)\r\n    #6 Envoy::Http::ConnectionManagerImpl::ActiveStream::encodeHeaders(Envoy::Http::ConnectionManagerImpl::ActiveStreamEncoderFilter*, Envoy::Http::HeaderMap&, bool) <null> (envoy+0x6bcba47)\r\n    #7 Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter::encodeHeaders(std::__1::unique_ptr<Envoy::Http::HeaderMap, std::__1::default_delete<Envoy::Http::HeaderMap> >&&, bool) <null> (envoy+0x6bd27f5)\r\n    #8 non-virtual thunk to Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter::encodeHeaders(std::__1::unique_ptr<Envoy::Http::HeaderMap, std::__1::default_delete<Envoy::Http::HeaderMap> >&&, bool) <null> (envoy+0x6bd287f)\r\n    #9 Envoy::Router::Filter::onUpstreamHeaders(unsigned long, std::__1::unique_ptr<Envoy::Http::HeaderMap, std::__1::default_delete<Envoy::Http::HeaderMap> >&&, Envoy::Router::Filter::UpstreamRequest&, bool) <null> (envoy+0x6a2e288)\r\n    #10 Envoy::Router::Filter::UpstreamRequest::decodeHeaders(std::__1::unique_ptr<Envoy::Http::HeaderMap, std::__1::default_delete<Envoy::Http::HeaderMap> >&&, bool) <null> (envoy+0x6a3341b)\r\n    #11 Envoy::Http::StreamDecoderWrapper::decodeHeaders(std::__1::unique_ptr<Envoy::Http::HeaderMap, std::__1::default_delete<Envoy::Http::HeaderMap> >&&, bool) <null> (envoy+0x67dd17f)\r\n    #12 Envoy::Http::Http2::ConnectionImpl::StreamImpl::decodeHeaders() <null> (envoy+0x6c3b8d4)\r\n    #13 Envoy::Http::Http2::ConnectionImpl::onFrameReceived(nghttp2_frame const*) <null> (envoy+0x6c41c06)\r\n    #14 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_11::operator()(nghttp2_session*, nghttp2_frame const*, void*) const <null> (envoy+0x6c4bbc0)\r\n    #15 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_11::__invoke(nghttp2_session*, nghttp2_frame const*, void*) <null> (envoy+0x6c4bb38)\r\n    #16 session_call_on_frame_received <null> (envoy+0x6f1bb31)\r\n    #17 session_after_header_block_received <null> (envoy+0x6f2a8be)\r\n    #18 nghttp2_session_mem_recv <null> (envoy+0x6f249d9)\r\n    #19 Envoy::Http::Http2::ConnectionImpl::dispatch(Envoy::Buffer::Instance&) <null> (envoy+0x6c3fda4)\r\n    #20 virtual thunk to Envoy::Http::Http2::ConnectionImpl::dispatch(Envoy::Buffer::Instance&) <null> (envoy+0x6c402f0)\r\n    #21 Envoy::Http::CodecClient::onData(Envoy::Buffer::Instance&) <null> (envoy+0x69b3a5c)\r\n    #22 Envoy::Http::CodecClient::CodecReadFilter::onData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x69b8cbc)\r\n    #23 Envoy::Network::FilterManagerImpl::onContinueReading(Envoy::Network::FilterManagerImpl::ActiveReadFilter*, Envoy::Network::ReadBufferSource&) <null> (envoy+0x62008e5)\r\n    #24 Envoy::Network::FilterManagerImpl::onRead() <null> (envoy+0x6200b54)\r\n    #25 Envoy::Network::ConnectionImpl::onRead(unsigned long) <null> (envoy+0x61e5e34)\r\n    #26 Envoy::Network::ConnectionImpl::onReadReady() <null> (envoy+0x61e9d70)\r\n    #27 Envoy::Network::ConnectionImpl::onFileEvent(unsigned int) <null> (envoy+0x61e9928)\r\n    #28 Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2::operator()(unsigned int) const <null> (envoy+0x61f2d57)\r\n    #29 decltype(std::__1::forward<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&>(fp)(std::__1::forward<unsigned int>(fp0))) std::__1::__invoke<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int&&) <null> (envoy+0x61f2ccc)\r\n    #30 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int&&) <null> (envoy+0x61f2bfd)\r\n    #31 std::__1::__function::__alloc_func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2>, void (unsigned int)>::operator()(unsigned int&&) <null> (envoy+0x61f2b7d)\r\n    #32 std::__1::__function::__func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2>, void (unsigned int)>::operator()(unsigned int&&) <null> (envoy+0x61f11bc)\r\n    #33 std::__1::__function::__value_func<void (unsigned int)>::operator()(unsigned int&&) const <null> (envoy+0x61d4be4)\r\n    #34 std::__1::function<void (unsigned int)>::operator()(unsigned int) const <null> (envoy+0x61d4268)\r\n    #35 Envoy::Event::FileEventImpl::assignEvents(unsigned int)::$_0::operator()(int, short, void*) const <null> (envoy+0x61d3ceb)\r\n    #36 Envoy::Event::FileEventImpl::assignEvents(unsigned int)::$_0::__invoke(int, short, void*) <null> (envoy+0x61d3af6)\r\n    #37 event_persist_closure <null> (envoy+0x70df520)\r\n    #38 event_process_active_single_queue <null> (envoy+0x70ddf48)\r\n    #39 event_process_active <null> (envoy+0x70d3898)\r\n    #40 event_base_loop <null> (envoy+0x70d1563)\r\n    #41 Envoy::Event::LibeventScheduler::run(Envoy::Event::Dispatcher::RunType) <null> (envoy+0x622808f)\r\n    #42 Envoy::Event::DispatcherImpl::run(Envoy::Event::Dispatcher::RunType) <null> (envoy+0x61c1521)\r\n    #43 Envoy::Server::WorkerImpl::threadRoutine(Envoy::Server::GuardDog&) <null> (envoy+0x6183ba0)\r\n    #44 Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&)::$_3::operator()() const <null> (envoy+0x618d8d7)\r\n    #45 decltype(std::__1::forward<Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&)::$_3&>(fp)()) std::__1::__invoke<Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&)::$_3&>(Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&)::$_3&) <null> (envoy+0x618d840)\r\n    #46 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&)::$_3&>(Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&)::$_3&) <null> (envoy+0x618d7a0)\r\n    #47 std::__1::__function::__alloc_func<Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&)::$_3, std::__1::allocator<Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&)::$_3>, void ()>::operator()() <null> (envoy+0x618d740)\r\n    #48 std::__1::__function::__func<Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&)::$_3, std::__1::allocator<Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&)::$_3>, void ()>::operator()() <null> (envoy+0x618bd9f)\r\n    #49 std::__1::__function::__value_func<void ()>::operator()() const <null> (envoy+0x2ad2bb6)\r\n    #50 std::__1::function<void ()>::operator()() const <null> (envoy+0x2ad2b08)\r\n    #51 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>)::$_0::operator()(void*) const <null> (envoy+0x758918b)\r\n    #52 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>)::$_0::__invoke(void*) <null> (envoy+0x7589118)\r\n  Previous write of size 8 at 0x7b1800014778 by main thread:\r\n    #0 std::__1::unique_ptr<Envoy::Event::Timer, std::__1::default_delete<Envoy::Event::Timer> >::reset(Envoy::Event::Timer*) <null> (envoy+0x2d2b0e5)\r\n    #1 std::__1::unique_ptr<Envoy::Event::Timer, std::__1::default_delete<Envoy::Event::Timer> >::operator=(std::__1::unique_ptr<Envoy::Event::Timer, std::__1::default_delete<Envoy::Event::Timer> >&&) <null> (envoy+0x2e4c138)\r\n    #2 Envoy::Server::DrainManagerImpl::startDrainSequence(std::__1::function<void ()>) <null> (envoy+0x60e0be8)\r\n    #3 Envoy::Server::ListenerManagerImpl::drainListener(std::__1::unique_ptr<Envoy::Server::ListenerImpl, std::__1::default_delete<Envoy::Server::ListenerImpl> >&&) <null> (envoy+0x60a945a)\r\n    #4 Envoy::Server::ListenerManagerImpl::onListenerWarmed(Envoy::Server::ListenerImpl&) <null> (envoy+0x60a9f8c)\r\n    #5 Envoy::Server::ListenerImpl::ListenerImpl(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Server::ListenerManagerImpl&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, unsigned long, Envoy::ProtobufMessage::ValidationVisitor&)::$_0::operator()() const <null> (envoy+0x608f859)\r\n    #6 decltype(std::__1::forward<Envoy::Server::ListenerImpl::ListenerImpl(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Server::ListenerManagerImpl&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, unsigned long, Envoy::ProtobufMessage::ValidationVisitor&)::$_0&>(fp)()) std::__1::__invoke<Envoy::Server::ListenerImpl::ListenerImpl(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Server::ListenerManagerImpl&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, unsigned long, Envoy::ProtobufMessage::ValidationVisitor&)::$_0&>(Envoy::Server::ListenerImpl::ListenerImpl(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Server::ListenerManagerImpl&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, unsigned long, Envoy::ProtobufMessage::ValidationVisitor&)::$_0&) <null> (envoy+0x608f7c0)\r\n    #7 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Server::ListenerImpl::ListenerImpl(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Server::ListenerManagerImpl&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, unsigned long, Envoy::ProtobufMessage::ValidationVisitor&)::$_0&>(Envoy::Server::ListenerImpl::ListenerImpl(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Server::ListenerManagerImpl&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, unsigned long, Envoy::ProtobufMessage::ValidationVisitor&)::$_0&) <null> (envoy+0x608f720)\r\n    #8 std::__1::__function::__alloc_func<Envoy::Server::ListenerImpl::ListenerImpl(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Server::ListenerManagerImpl&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, unsigned long, Envoy::ProtobufMessage::ValidationVisitor&)::$_0, std::__1::allocator<Envoy::Server::ListenerImpl::ListenerImpl(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Server::ListenerManagerImpl&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, unsigned long, Envoy::ProtobufMessage::ValidationVisitor&)::$_0>, void ()>::operator()() <null> (envoy+0x608f6c0)\r\n    #9 std::__1::__function::__func<Envoy::Server::ListenerImpl::ListenerImpl(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Server::ListenerManagerImpl&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, unsigned long, Envoy::ProtobufMessage::ValidationVisitor&)::$_0, std::__1::allocator<Envoy::Server::ListenerImpl::ListenerImpl(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Server::ListenerManagerImpl&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, unsigned long, Envoy::ProtobufMessage::ValidationVisitor&)::$_0>, void ()>::operator()() <null> (envoy+0x608dd1f)\r\n    #10 std::__1::__function::__value_func<void ()>::operator()() const <null> (envoy+0x2ad2bb6)\r\n    #11 std::__1::function<void ()>::operator()() const <null> (envoy+0x2ad2b08)\r\n    #12 Envoy::Init::WatcherHandleImpl::ready() const <null> (envoy+0x69eb765)\r\n    #13 Envoy::Init::ManagerImpl::ready() <null> (envoy+0x69e6db9)\r\n    #14 Envoy::Init::ManagerImpl::initialize(Envoy::Init::Watcher const&) <null> (envoy+0x69e6b61)\r\n    #15 Envoy::Server::ListenerImpl::initialize() <null> (envoy+0x608ce89)\r\n    #16 Envoy::Server::ListenerManagerImpl::addOrUpdateListener(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool) <null> (envoy+0x60a8a22)\r\n    #17 Envoy::Server::LdsApiImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<envoy::api::v2::Resource> const&, google::protobuf::RepeatedPtrField<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x617a531)\r\n    #18 Envoy::Server::LdsApiImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x617b007)\r\n    #19 non-virtual thunk to Envoy::Server::LdsApiImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x617b0c7)\r\n    #20 Envoy::Config::GrpcMuxSubscriptionImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x693ee5b)\r\n    #21 non-virtual thunk to Envoy::Config::GrpcMuxSubscriptionImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x693f147)\r\n    #22 Envoy::Config::GrpcMuxImpl::onDiscoveryResponse(std::__1::unique_ptr<envoy::api::v2::DiscoveryResponse, std::__1::default_delete<envoy::api::v2::DiscoveryResponse> >&&) <null> (envoy+0x6927d75)\r\n    #23 non-virtual thunk to Envoy::Config::GrpcMuxImpl::onDiscoveryResponse(std::__1::unique_ptr<envoy::api::v2::DiscoveryResponse, std::__1::default_delete<envoy::api::v2::DiscoveryResponse> >&&) <null> (envoy+0x69283cf)\r\n    #24 Envoy::Config::GrpcStream<envoy::api::v2::DiscoveryRequest, envoy::api::v2::DiscoveryResponse>::onReceiveMessage(std::__1::unique_ptr<envoy::api::v2::DiscoveryResponse, std::__1::default_delete<envoy::api::v2::DiscoveryResponse> >&&) <null> (envoy+0x692bf9a)\r\n    #25 Envoy::Grpc::AsyncStreamCallbacks<envoy::api::v2::DiscoveryResponse>::onReceiveMessageRaw(std::__1::unique_ptr<Envoy::Buffer::Instance, std::__1::default_delete<Envoy::Buffer::Instance> >&&) <null> (envoy+0x692bc86)\r\n    #26 Envoy::Grpc::AsyncStreamImpl::onData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x69ef51f)\r\n    #27 non-virtual thunk to Envoy::Grpc::AsyncStreamImpl::onData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x69ef6af)\r\n    #28 Envoy::Http::AsyncStreamImpl::encodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x6a02e91)\r\n    #29 non-virtual thunk to Envoy::Http::AsyncStreamImpl::encodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x6a02f9f)\r\n    #30 Envoy::Router::Filter::onUpstreamData(Envoy::Buffer::Instance&, Envoy::Router::Filter::UpstreamRequest&, bool) <null> (envoy+0x6a2fd83)\r\n    #31 Envoy::Router::Filter::UpstreamRequest::decodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x6a3385f)\r\n    #32 Envoy::Http::StreamDecoderWrapper::decodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x67dde59)\r\n    #33 Envoy::Http::Http2::ConnectionImpl::onFrameReceived(nghttp2_frame const*) <null> (envoy+0x6c4231f)\r\n    #34 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_11::operator()(nghttp2_session*, nghttp2_frame const*, void*) const <null> (envoy+0x6c4bbc0)\r\n    #35 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_11::__invoke(nghttp2_session*, nghttp2_frame const*, void*) <null> (envoy+0x6c4bb38)\r\n    #36 session_call_on_frame_received <null> (envoy+0x6f1bb31)\r\n    #37 nghttp2_session_on_data_received <null> (envoy+0x6f1f447)\r\n    #38 session_process_data_frame <null> (envoy+0x6f2badb)\r\n    #39 nghttp2_session_mem_recv <null> (envoy+0x6f27077)\r\n    #40 Envoy::Http::Http2::ConnectionImpl::dispatch(Envoy::Buffer::Instance&) <null> (envoy+0x6c3fda4)\r\n    #41 virtual thunk to Envoy::Http::Http2::ConnectionImpl::dispatch(Envoy::Buffer::Instance&) <null> (envoy+0x6c402f0)\r\n    #42 Envoy::Http::CodecClient::onData(Envoy::Buffer::Instance&) <null> (envoy+0x69b3a5c)\r\n    #43 Envoy::Http::CodecClient::CodecReadFilter::onData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x69b8cbc)\r\n    #44 Envoy::Network::FilterManagerImpl::onContinueReading(Envoy::Network::FilterManagerImpl::ActiveReadFilter*, Envoy::Network::ReadBufferSource&) <null> (envoy+0x62008e5)\r\n    #45 Envoy::Network::FilterManagerImpl::onRead() <null> (envoy+0x6200b54)\r\n    #46 Envoy::Network::ConnectionImpl::onRead(unsigned long) <null> (envoy+0x61e5e34)\r\n    #47 Envoy::Network::ConnectionImpl::onReadReady() <null> (envoy+0x61e9d70)\r\n    #48 Envoy::Network::ConnectionImpl::onFileEvent(unsigned int) <null> (envoy+0x61e9928)\r\n    #49 Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2::operator()(unsigned int) const <null> (envoy+0x61f2d57)\r\n    #50 decltype(std::__1::forward<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&>(fp)(std::__1::forward<unsigned int>(fp0))) std::__1::__invoke<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int&&) <null> (envoy+0x61f2ccc)\r\n    #51 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int&&) <null> (envoy+0x61f2bfd)\r\n    #52 std::__1::__function::__alloc_func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2>, void (unsigned int)>::operator()(unsigned int&&) <null> (envoy+0x61f2b7d)\r\n    #53 std::__1::__function::__func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2>, void (unsigned int)>::operator()(unsigned int&&) <null> (envoy+0x61f11bc)\r\n    #54 std::__1::__function::__value_func<void (unsigned int)>::operator()(unsigned int&&) const <null> (envoy+0x61d4be4)\r\n    #55 std::__1::function<void (unsigned int)>::operator()(unsigned int) const <null> (envoy+0x61d4268)\r\n    #56 Envoy::Event::FileEventImpl::assignEvents(unsigned int)::$_0::operator()(int, short, void*) const <null> (envoy+0x61d3ceb)\r\n    #57 Envoy::Event::FileEventImpl::assignEvents(unsigned int)::$_0::__invoke(int, short, void*) <null> (envoy+0x61d3af6)\r\n    #58 event_persist_closure <null> (envoy+0x70df520)\r\n    #59 event_process_active_single_queue <null> (envoy+0x70ddf48)\r\n    #60 event_process_active <null> (envoy+0x70d3898)\r\n    #61 event_base_loop <null> (envoy+0x70d1563)\r\n    #62 Envoy::Event::LibeventScheduler::run(Envoy::Event::Dispatcher::RunType) <null> (envoy+0x622808f)\r\n    #63 Envoy::Event::DispatcherImpl::run(Envoy::Event::Dispatcher::RunType) <null> (envoy+0x61c1521)\r\n    #64 Envoy::Server::InstanceImpl::run() <null> (envoy+0x601b69b)\r\n    #65 Envoy::MainCommonBase::run() <null> (envoy+0x312c824)\r\n    #66 Envoy::MainCommon::run() <null> (envoy+0x3128e72)\r\n    #67 main <null> (envoy+0x3128956)\r\n  Location is heap block of size 96 at 0x7b1800014760 allocated by main thread:\r\n    #0 malloc <null> (envoy+0x288cec4)\r\n    #1 operator new(unsigned long) <null> (libc++.so.1+0x887f4)\r\n    #2 Envoy::Server::ListenerImpl::ListenerImpl(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Envoy::Server::ListenerManagerImpl&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, unsigned long, Envoy::ProtobufMessage::ValidationVisitor&) <null> (envoy+0x60872c1)\r\n    #3 Envoy::Server::ListenerManagerImpl::addOrUpdateListener(envoy::api::v2::Listener const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool) <null> (envoy+0x60a7238)\r\n    #4 Envoy::Server::LdsApiImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<envoy::api::v2::Resource> const&, google::protobuf::RepeatedPtrField<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x617a531)\r\n    #5 Envoy::Server::LdsApiImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x617b007)\r\n    #6 non-virtual thunk to Envoy::Server::LdsApiImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x617b0c7)\r\n    #7 Envoy::Config::GrpcMuxSubscriptionImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x693ee5b)\r\n    #8 non-virtual thunk to Envoy::Config::GrpcMuxSubscriptionImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x693f147)\r\n    #9 Envoy::Config::GrpcMuxImpl::onDiscoveryResponse(std::__1::unique_ptr<envoy::api::v2::DiscoveryResponse, std::__1::default_delete<envoy::api::v2::DiscoveryResponse> >&&) <null> (envoy+0x6927d75)\r\n    #10 non-virtual thunk to Envoy::Config::GrpcMuxImpl::onDiscoveryResponse(std::__1::unique_ptr<envoy::api::v2::DiscoveryResponse, std::__1::default_delete<envoy::api::v2::DiscoveryResponse> >&&) <null> (envoy+0x69283cf)\r\n    #11 Envoy::Config::GrpcStream<envoy::api::v2::DiscoveryRequest, envoy::api::v2::DiscoveryResponse>::onReceiveMessage(std::__1::unique_ptr<envoy::api::v2::DiscoveryResponse, std::__1::default_delete<envoy::api::v2::DiscoveryResponse> >&&) <null> (envoy+0x692bf9a)\r\n    #12 Envoy::Grpc::AsyncStreamCallbacks<envoy::api::v2::DiscoveryResponse>::onReceiveMessageRaw(std::__1::unique_ptr<Envoy::Buffer::Instance, std::__1::default_delete<Envoy::Buffer::Instance> >&&) <null> (envoy+0x692bc86)\r\n    #13 Envoy::Grpc::AsyncStreamImpl::onData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x69ef51f)\r\n    #14 non-virtual thunk to Envoy::Grpc::AsyncStreamImpl::onData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x69ef6af)\r\n    #15 Envoy::Http::AsyncStreamImpl::encodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x6a02e91)\r\n    #16 non-virtual thunk to Envoy::Http::AsyncStreamImpl::encodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x6a02f9f)\r\n    #17 Envoy::Router::Filter::onUpstreamData(Envoy::Buffer::Instance&, Envoy::Router::Filter::UpstreamRequest&, bool) <null> (envoy+0x6a2fd83)\r\n    #18 Envoy::Router::Filter::UpstreamRequest::decodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x6a3385f)\r\n    #19 Envoy::Http::StreamDecoderWrapper::decodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x67dde59)\r\n    #20 Envoy::Http::Http2::ConnectionImpl::onFrameReceived(nghttp2_frame const*) <null> (envoy+0x6c4231f)\r\n    #21 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_11::operator()(nghttp2_session*, nghttp2_frame const*, void*) const <null> (envoy+0x6c4bbc0)\r\n    #22 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_11::__invoke(nghttp2_session*, nghttp2_frame const*, void*) <null> (envoy+0x6c4bb38)\r\n    #23 session_call_on_frame_received <null> (envoy+0x6f1bb31)\r\n    #24 nghttp2_session_on_data_received <null> (envoy+0x6f1f447)\r\n    #25 session_process_data_frame <null> (envoy+0x6f2badb)\r\n    #26 nghttp2_session_mem_recv <null> (envoy+0x6f27077)\r\n    #27 Envoy::Http::Http2::ConnectionImpl::dispatch(Envoy::Buffer::Instance&) <null> (envoy+0x6c3fda4)\r\n    #28 virtual thunk to Envoy::Http::Http2::ConnectionImpl::dispatch(Envoy::Buffer::Instance&) <null> (envoy+0x6c402f0)\r\n    #29 Envoy::Http::CodecClient::onData(Envoy::Buffer::Instance&) <null> (envoy+0x69b3a5c)\r\n    #30 Envoy::Http::CodecClient::CodecReadFilter::onData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x69b8cbc)\r\n    #31 Envoy::Network::FilterManagerImpl::onContinueReading(Envoy::Network::FilterManagerImpl::ActiveReadFilter*, Envoy::Network::ReadBufferSource&) <null> (envoy+0x62008e5)\r\n    #32 Envoy::Network::FilterManagerImpl::onRead() <null> (envoy+0x6200b54)\r\n    #33 Envoy::Network::ConnectionImpl::onRead(unsigned long) <null> (envoy+0x61e5e34)\r\n    #34 Envoy::Network::ConnectionImpl::onReadReady() <null> (envoy+0x61e9d70)\r\n    #35 Envoy::Network::ConnectionImpl::onFileEvent(unsigned int) <null> (envoy+0x61e9928)\r\n    #36 Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2::operator()(unsigned int) const <null> (envoy+0x61f2d57)\r\n    #37 decltype(std::__1::forward<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&>(fp)(std::__1::forward<unsigned int>(fp0))) std::__1::__invoke<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int&&) <null> (envoy+0x61f2ccc)\r\n    #38 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int&&) <null> (envoy+0x61f2bfd)\r\n    #39 std::__1::__function::__alloc_func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2>, void (unsigned int)>::operator()(unsigned int&&) <null> (envoy+0x61f2b7d)\r\n    #40 std::__1::__function::__func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2>, void (unsigned int)>::operator()(unsigned int&&) <null> (envoy+0x61f11bc)\r\n    #41 std::__1::__function::__value_func<void (unsigned int)>::operator()(unsigned int&&) const <null> (envoy+0x61d4be4)\r\n    #42 std::__1::function<void (unsigned int)>::operator()(unsigned int) const <null> (envoy+0x61d4268)\r\n    #43 Envoy::Event::FileEventImpl::assignEvents(unsigned int)::$_0::operator()(int, short, void*) const <null> (envoy+0x61d3ceb)\r\n    #44 Envoy::Event::FileEventImpl::assignEvents(unsigned int)::$_0::__invoke(int, short, void*) <null> (envoy+0x61d3af6)\r\n    #45 event_persist_closure <null> (envoy+0x70df520)\r\n    #46 event_process_active_single_queue <null> (envoy+0x70ddf48)\r\n    #47 event_process_active <null> (envoy+0x70d3898)\r\n    #48 event_base_loop <null> (envoy+0x70d1563)\r\n    #49 Envoy::Event::LibeventScheduler::run(Envoy::Event::Dispatcher::RunType) <null> (envoy+0x622808f)\r\n    #50 Envoy::Event::DispatcherImpl::run(Envoy::Event::Dispatcher::RunType) <null> (envoy+0x61c1521)\r\n    #51 Envoy::Server::InstanceImpl::run() <null> (envoy+0x601b69b)\r\n    #52 Envoy::MainCommonBase::run() <null> (envoy+0x312c824)\r\n    #53 Envoy::MainCommon::run() <null> (envoy+0x3128e72)\r\n    #54 main <null> (envoy+0x3128956)\r\n  Thread T9 (tid=7323, running) created by main thread at:\r\n    #0 pthread_create <null> (envoy+0x288e7ab)\r\n    #1 Envoy::Thread::ThreadImplPosix::ThreadImplPosix(std::__1::function<void ()>) <null> (envoy+0x7588c8e)\r\n    #2 std::__1::__unique_if<Envoy::Thread::ThreadImplPosix>::__unique_single std::__1::make_unique<Envoy::Thread::ThreadImplPosix, std::__1::function<void ()>&>(std::__1::function<void ()>&) <null> (envoy+0x758947e)\r\n    #3 Envoy::Thread::ThreadFactoryImplPosix::createThread(std::__1::function<void ()>) <null> (envoy+0x7589023)\r\n    #4 Envoy::Server::WorkerImpl::start(Envoy::Server::GuardDog&) <null> (envoy+0x6183372)\r\n    #5 Envoy::Server::ListenerManagerImpl::startWorkers(Envoy::Server::GuardDog&) <null> (envoy+0x60aab9e)\r\n    #6 Envoy::Server::InstanceImpl::startWorkers() <null> (envoy+0x601a301)\r\n    #7 Envoy::Server::InstanceImpl::run()::$_11::operator()() const <null> (envoy+0x6038031)\r\n    #8 decltype(std::__1::forward<Envoy::Server::InstanceImpl::run()::$_11&>(fp)()) std::__1::__invoke<Envoy::Server::InstanceImpl::run()::$_11&>(Envoy::Server::InstanceImpl::run()::$_11&) <null> (envoy+0x6037f70)\r\n    #9 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Server::InstanceImpl::run()::$_11&>(Envoy::Server::InstanceImpl::run()::$_11&) <null> (envoy+0x6037ed0)\r\n    #10 std::__1::__function::__alloc_func<Envoy::Server::InstanceImpl::run()::$_11, std::__1::allocator<Envoy::Server::InstanceImpl::run()::$_11>, void ()>::operator()() <null> (envoy+0x6037e70)\r\n    #11 std::__1::__function::__func<Envoy::Server::InstanceImpl::run()::$_11, std::__1::allocator<Envoy::Server::InstanceImpl::run()::$_11>, void ()>::operator()() <null> (envoy+0x60364cf)\r\n    #12 std::__1::__function::__value_func<void ()>::operator()() const <null> (envoy+0x2ad2bb6)\r\n    #13 std::__1::function<void ()>::operator()() const <null> (envoy+0x2ad2b08)\r\n    #14 Envoy::Server::RunHelper::RunHelper(Envoy::Server::Instance&, Envoy::Server::Options const&, Envoy::Event::Dispatcher&, Envoy::Upstream::ClusterManager&, Envoy::AccessLog::AccessLogManager&, Envoy::Init::Manager&, Envoy::Server::OverloadManager&, std::__1::function<void ()>)::$_5::operator()() const <null> (envoy+0x602a68d)\r\n    #15 decltype(std::__1::forward<Envoy::Server::RunHelper::RunHelper(Envoy::Server::Instance&, Envoy::Server::Options const&, Envoy::Event::Dispatcher&, Envoy::Upstream::ClusterManager&, Envoy::AccessLog::AccessLogManager&, Envoy::Init::Manager&, Envoy::Server::OverloadManager&, std::__1::function<void ()>)::$_5&>(fp)()) std::__1::__invoke<Envoy::Server::RunHelper::RunHelper(Envoy::Server::Instance&, Envoy::Server::Options const&, Envoy::Event::Dispatcher&, Envoy::Upstream::ClusterManager&, Envoy::AccessLog::AccessLogManager&, Envoy::Init::Manager&, Envoy::Server::OverloadManager&, std::__1::function<void ()>)::$_5&>(Envoy::Server::RunHelper::RunHelper(Envoy::Server::Instance&, Envoy::Server::Options const&, Envoy::Event::Dispatcher&, Envoy::Upstream::ClusterManager&, Envoy::AccessLog::AccessLogManager&, Envoy::Init::Manager&, Envoy::Server::OverloadManager&, std::__1::function<void ()>)::$_5&) <null> (envoy+0x602a5d0)\r\n    #16 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Server::RunHelper::RunHelper(Envoy::Server::Instance&, Envoy::Server::Options const&, Envoy::Event::Dispatcher&, Envoy::Upstream::ClusterManager&, Envoy::AccessLog::AccessLogManager&, Envoy::Init::Manager&, Envoy::Server::OverloadManager&, std::__1::function<void ()>)::$_5&>(Envoy::Server::RunHelper::RunHelper(Envoy::Server::Instance&, Envoy::Server::Options const&, Envoy::Event::Dispatcher&, Envoy::Upstream::ClusterManager&, Envoy::AccessLog::AccessLogManager&, Envoy::Init::Manager&, Envoy::Server::OverloadManager&, std::__1::function<void ()>)::$_5&) <null> (envoy+0x602a530)\r\n    #17 std::__1::__function::__alloc_func<Envoy::Server::RunHelper::RunHelper(Envoy::Server::Instance&, Envoy::Server::Options const&, Envoy::Event::Dispatcher&, Envoy::Upstream::ClusterManager&, Envoy::AccessLog::AccessLogManager&, Envoy::Init::Manager&, Envoy::Server::OverloadManager&, std::__1::function<void ()>)::$_5, std::__1::allocator<Envoy::Server::RunHelper::RunHelper(Envoy::Server::Instance&, Envoy::Server::Options const&, Envoy::Event::Dispatcher&, Envoy::Upstream::ClusterManager&, Envoy::AccessLog::AccessLogManager&, Envoy::Init::Manager&, Envoy::Server::OverloadManager&, std::__1::function<void ()>)::$_5>, void ()>::operator()() <null> (envoy+0x602a4d0)\r\n    #18 std::__1::__function::__func<Envoy::Server::RunHelper::RunHelper(Envoy::Server::Instance&, Envoy::Server::Options const&, Envoy::Event::Dispatcher&, Envoy::Upstream::ClusterManager&, Envoy::AccessLog::AccessLogManager&, Envoy::Init::Manager&, Envoy::Server::OverloadManager&, std::__1::function<void ()>)::$_5, std::__1::allocator<Envoy::Server::RunHelper::RunHelper(Envoy::Server::Instance&, Envoy::Server::Options const&, Envoy::Event::Dispatcher&, Envoy::Upstream::ClusterManager&, Envoy::AccessLog::AccessLogManager&, Envoy::Init::Manager&, Envoy::Server::OverloadManager&, std::__1::function<void ()>)::$_5>, void ()>::operator()() <null> (envoy+0x6028b9f)\r\n    #19 std::__1::__function::__value_func<void ()>::operator()() const <null> (envoy+0x2ad2bb6)\r\n    #20 std::__1::function<void ()>::operator()() const <null> (envoy+0x2ad2b08)\r\n    #21 Envoy::Init::WatcherHandleImpl::ready() const <null> (envoy+0x69eb765)\r\n    #22 Envoy::Init::ManagerImpl::ready() <null> (envoy+0x69e6db9)\r\n    #23 Envoy::Init::ManagerImpl::onTargetReady() <null> (envoy+0x69e6f54)\r\n    #24 Envoy::Init::ManagerImpl::ManagerImpl(absl::string_view)::$_0::operator()() const <null> (envoy+0x69e924b)\r\n    #25 decltype(std::__1::forward<Envoy::Init::ManagerImpl::ManagerImpl(absl::string_view)::$_0&>(fp)()) std::__1::__invoke<Envoy::Init::ManagerImpl::ManagerImpl(absl::string_view)::$_0&>(Envoy::Init::ManagerImpl::ManagerImpl(absl::string_view)::$_0&) <null> (envoy+0x69e91d0)\r\n    #26 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Init::ManagerImpl::ManagerImpl(absl::string_view)::$_0&>(Envoy::Init::ManagerImpl::ManagerImpl(absl::string_view)::$_0&) <null> (envoy+0x69e9130)\r\n    #27 std::__1::__function::__alloc_func<Envoy::Init::ManagerImpl::ManagerImpl(absl::string_view)::$_0, std::__1::allocator<Envoy::Init::ManagerImpl::ManagerImpl(absl::string_view)::$_0>, void ()>::operator()() <null> (envoy+0x69e90d0)\r\n    #28 std::__1::__function::__func<Envoy::Init::ManagerImpl::ManagerImpl(absl::string_view)::$_0, std::__1::allocator<Envoy::Init::ManagerImpl::ManagerImpl(absl::string_view)::$_0>, void ()>::operator()() <null> (envoy+0x69e772f)\r\n    #29 std::__1::__function::__value_func<void ()>::operator()() const <null> (envoy+0x2ad2bb6)\r\n    #30 std::__1::function<void ()>::operator()() const <null> (envoy+0x2ad2b08)\r\n    #31 Envoy::Init::WatcherHandleImpl::ready() const <null> (envoy+0x69eb765)\r\n    #32 Envoy::Init::TargetImpl::ready() <null> (envoy+0x7110722)\r\n    #33 Envoy::Server::LdsApiImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<envoy::api::v2::Resource> const&, google::protobuf::RepeatedPtrField<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x617a71e)\r\n    #34 Envoy::Server::LdsApiImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x617b007)\r\n    #35 non-virtual thunk to Envoy::Server::LdsApiImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x617b0c7)\r\n    #36 Envoy::Config::GrpcMuxSubscriptionImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x693ee5b)\r\n    #37 non-virtual thunk to Envoy::Config::GrpcMuxSubscriptionImpl::onConfigUpdate(google::protobuf::RepeatedPtrField<google::protobuf::Any> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) <null> (envoy+0x693f147)\r\n    #38 Envoy::Config::GrpcMuxImpl::onDiscoveryResponse(std::__1::unique_ptr<envoy::api::v2::DiscoveryResponse, std::__1::default_delete<envoy::api::v2::DiscoveryResponse> >&&) <null> (envoy+0x6927d75)\r\n    #39 non-virtual thunk to Envoy::Config::GrpcMuxImpl::onDiscoveryResponse(std::__1::unique_ptr<envoy::api::v2::DiscoveryResponse, std::__1::default_delete<envoy::api::v2::DiscoveryResponse> >&&) <null> (envoy+0x69283cf)\r\n    #40 Envoy::Config::GrpcStream<envoy::api::v2::DiscoveryRequest, envoy::api::v2::DiscoveryResponse>::onReceiveMessage(std::__1::unique_ptr<envoy::api::v2::DiscoveryResponse, std::__1::default_delete<envoy::api::v2::DiscoveryResponse> >&&) <null> (envoy+0x692bf9a)\r\n    #41 Envoy::Grpc::AsyncStreamCallbacks<envoy::api::v2::DiscoveryResponse>::onReceiveMessageRaw(std::__1::unique_ptr<Envoy::Buffer::Instance, std::__1::default_delete<Envoy::Buffer::Instance> >&&) <null> (envoy+0x692bc86)\r\n    #42 Envoy::Grpc::AsyncStreamImpl::onData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x69ef51f)\r\n    #43 non-virtual thunk to Envoy::Grpc::AsyncStreamImpl::onData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x69ef6af)\r\n    #44 Envoy::Http::AsyncStreamImpl::encodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x6a02e91)\r\n    #45 non-virtual thunk to Envoy::Http::AsyncStreamImpl::encodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x6a02f9f)\r\n    #46 Envoy::Router::Filter::onUpstreamData(Envoy::Buffer::Instance&, Envoy::Router::Filter::UpstreamRequest&, bool) <null> (envoy+0x6a2fd83)\r\n    #47 Envoy::Router::Filter::UpstreamRequest::decodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x6a3385f)\r\n    #48 Envoy::Http::StreamDecoderWrapper::decodeData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x67dde59)\r\n    #49 Envoy::Http::Http2::ConnectionImpl::onFrameReceived(nghttp2_frame const*) <null> (envoy+0x6c4231f)\r\n    #50 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_11::operator()(nghttp2_session*, nghttp2_frame const*, void*) const <null> (envoy+0x6c4bbc0)\r\n    #51 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_11::__invoke(nghttp2_session*, nghttp2_frame const*, void*) <null> (envoy+0x6c4bb38)\r\n    #52 session_call_on_frame_received <null> (envoy+0x6f1bb31)\r\n    #53 nghttp2_session_on_data_received <null> (envoy+0x6f1f447)\r\n    #54 session_process_data_frame <null> (envoy+0x6f2badb)\r\n    #55 nghttp2_session_mem_recv <null> (envoy+0x6f27077)\r\n    #56 Envoy::Http::Http2::ConnectionImpl::dispatch(Envoy::Buffer::Instance&) <null> (envoy+0x6c3fda4)\r\n    #57 virtual thunk to Envoy::Http::Http2::ConnectionImpl::dispatch(Envoy::Buffer::Instance&) <null> (envoy+0x6c402f0)\r\n    #58 Envoy::Http::CodecClient::onData(Envoy::Buffer::Instance&) <null> (envoy+0x69b3a5c)\r\n    #59 Envoy::Http::CodecClient::CodecReadFilter::onData(Envoy::Buffer::Instance&, bool) <null> (envoy+0x69b8cbc)\r\n    #60 Envoy::Network::FilterManagerImpl::onContinueReading(Envoy::Network::FilterManagerImpl::ActiveReadFilter*, Envoy::Network::ReadBufferSource&) <null> (envoy+0x62008e5)\r\n    #61 Envoy::Network::FilterManagerImpl::onRead() <null> (envoy+0x6200b54)\r\n    #62 Envoy::Network::ConnectionImpl::onRead(unsigned long) <null> (envoy+0x61e5e34)\r\n    #63 Envoy::Network::ConnectionImpl::onReadReady() <null> (envoy+0x61e9d70)\r\n    #64 Envoy::Network::ConnectionImpl::onFileEvent(unsigned int) <null> (envoy+0x61e9928)\r\n    #65 Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2::operator()(unsigned int) const <null> (envoy+0x61f2d57)\r\n    #66 decltype(std::__1::forward<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&>(fp)(std::__1::forward<unsigned int>(fp0))) std::__1::__invoke<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int&&) <null> (envoy+0x61f2ccc)\r\n    #67 void std::__1::__invoke_void_return_wrapper<void>::__call<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int>(Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2&, unsigned int&&) <null> (envoy+0x61f2bfd)\r\n    #68 std::__1::__function::__alloc_func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2>, void (unsigned int)>::operator()(unsigned int&&) <null> (envoy+0x61f2b7d)\r\n    #69 std::__1::__function::__func<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2, std::__1::allocator<Envoy::Network::ConnectionImpl::ConnectionImpl(Envoy::Event::Dispatcher&, std::__1::unique_ptr<Envoy::Network::ConnectionSocket, std::__1::default_delete<Envoy::Network::ConnectionSocket> >&&, std::__1::unique_ptr<Envoy::Network::TransportSocket, std::__1::default_delete<Envoy::Network::TransportSocket> >&&, bool)::$_2>, void (unsigned int)>::operator()(unsigned int&&) <null> (envoy+0x61f11bc)\r\n    #70 std::__1::__function::__value_func<void (unsigned int)>::operator()(unsigned int&&) const <null> (envoy+0x61d4be4)\r\n    #71 std::__1::function<void (unsigned int)>::operator()(unsigned int) const <null> (envoy+0x61d4268)\r\n    #72 Envoy::Event::FileEventImpl::assignEvents(unsigned int)::$_0::operator()(int, short, void*) const <null> (envoy+0x61d3ceb)\r\n    #73 Envoy::Event::FileEventImpl::assignEvents(unsigned int)::$_0::__invoke(int, short, void*) <null> (envoy+0x61d3af6)\r\n    #74 event_persist_closure <null> (envoy+0x70df520)\r\n    #75 event_process_active_single_queue <null> (envoy+0x70ddf48)\r\n    #76 event_process_active <null> (envoy+0x70d3898)\r\n    #77 event_base_loop <null> (envoy+0x70d1563)\r\n    #78 Envoy::Event::LibeventScheduler::run(Envoy::Event::Dispatcher::RunType) <null> (envoy+0x622808f)\r\n    #79 Envoy::Event::DispatcherImpl::run(Envoy::Event::Dispatcher::RunType) <null> (envoy+0x61c1521)\r\n    #80 Envoy::Server::InstanceImpl::run() <null> (envoy+0x601b69b)\r\n    #81 Envoy::MainCommonBase::run() <null> (envoy+0x312c824)\r\n    #82 Envoy::MainCommon::run() <null> (envoy+0x3128e72)\r\n    #83 main <null> (envoy+0x3128956)\r\nSUMMARY: ThreadSanitizer: data race (/home/bootstrap/.cache/bazel/_bazel_root/45ae47b4a0e12d1e81c831ece04d820d/execroot/io_istio_proxy/bazel-out/k8-fastbuild/bin/src/envoy/envoy+0x2e4c417) in std::__1::unique_ptr<Envoy::Event::Timer, std::__1::default_delete<Envoy::Event::Timer> >::operator bool() const\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8940/comments",
    "author": "kyessenov",
    "comments": [
      {
        "user": "jplevyak",
        "created_at": "2019-11-08T17:29:02Z",
        "body": "I can do take this one."
      },
      {
        "user": "kyessenov",
        "created_at": "2019-11-14T22:36:47Z",
        "body": "The fix was merged apparently, closing."
      }
    ]
  },
  {
    "number": 8743,
    "title": "config dump: allow search/filtering",
    "created_at": "2019-10-23T21:37:16Z",
    "closed_at": "2019-12-23T03:31:53Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8743",
    "body": "It would be nice to have query strings that allow dumping only listeners, only clusters, and potentially searching for specific names within those dumps. \r\n\r\nThis could be done by a different tool but it seems useful to have some basic ability built in.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8743/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "spenceral",
        "created_at": "2019-11-16T00:08:51Z",
        "body": "I can work on this. "
      },
      {
        "user": "mattklein123",
        "created_at": "2019-11-16T00:32:40Z",
        "body": "@spenceral awesome! Go for it. LMK if you have any questions. This will be super useful."
      }
    ]
  },
  {
    "number": 8636,
    "title": "Run fix_format in ci and produce diff",
    "created_at": "2019-10-17T01:09:59Z",
    "closed_at": "2019-10-31T04:32:46Z",
    "labels": [
      "enhancement",
      "area/build",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8636",
    "body": "We're adding more stuff in the format CI and it become heavier and with more dependencies to run. We can produce a diff in azp in case of check_format fails, so the diff gives what CI expect for the fixable format without environment dependent bit.\r\n\r\n@htuch @jmarantz ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8636/comments",
    "author": "lizan",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-10-17T22:18:31Z",
        "body": "Yes, I think this is a good idea, but will require some work to get `check_format.py` into a position to provide a patchable diff; right now its output formatting isn't quite right for this. I think this would be an awesome task for anyone who wants to take on Python dev infrastructure work, will mark it as help wanted."
      },
      {
        "user": "jmarantz",
        "created_at": "2019-10-17T22:22:38Z",
        "body": "I think the easiest way to approach this is run the format fixer, and then use git to make the diff.\r\n\r\nSome of the check_format tests are not able to automatically fix any problems, in which case the format fixer should return an error."
      },
      {
        "user": "htuch",
        "created_at": "2019-10-17T22:29:42Z",
        "body": "+1; in CI we can run the format fixer without worrying about local state as well. Let's take that approach."
      },
      {
        "user": "lizan",
        "created_at": "2019-10-18T17:12:40Z",
        "body": "> I think the easiest way to approach this is run the format fixer, and then use git to make the diff.\r\n\r\nYeah that is what was in my mind. Simply run fix_format and git diff."
      }
    ]
  },
  {
    "number": 8545,
    "title": "listener/http-manager downstream_rq_4xx counter not incremented when envoy returns a 431 response after hitting a header size limit",
    "created_at": "2019-10-09T06:07:57Z",
    "closed_at": "2020-06-25T15:31:36Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8545",
    "body": "\r\n**Title**: *listener/http-manager downstream_rq_4xx counter not incremented when envoy returns a 431 response after hitting a header size limit*\r\n\r\n**Description**:\r\nWhen rolling out v1.11.2 which includes the max 100 headers count limit we wanted to know if we were breaking any existing service (i.e. a service that expects requests with >100 headers) by looking at the `response_code_class` tag in the `listener.http.downstream_rq_xx` and `http.downstream_rq_xx` metrics. But we noticed these counters are not incremented when envoy returns a 431.\r\n\r\nIs this working as designed?\r\n\r\nWe did notice that `http.downstream_cx_protocol_error` gets incremented but that seems too generic to tell if the cause is headers size (to be fair, 4xx is also too generic, **is there a way to get specific status code tags as with the cluster metrics?**)\r\n\r\n**Repro steps**:\r\n\r\nTested with `v1.11.1` (max header kb exceeded) and v1.11.2 (both max headers count and kb exeeded)\r\n\r\n 1. Set  `max_request_headers_kb` to  `1` to easily trigger the 431s (see full envoy config below)\r\n\r\n 2. Perform a bunch of requests with a long header:\r\n\r\n```\r\n$ curl -I -H \"x-long-header: $(printf 'a%.0s' {1..1200})\" localhost:8080\r\nHTTP/1.1 431 Request Header Fields Too Large\r\ncontent-length: 0\r\nconnection: close\r\n```\r\n\r\n**Admin and Stats Output**:\r\n\r\nThese are the http manager and listener stats (after making 10  status 431 requests):\r\n\r\n```\r\nhttp.ingress_http.downstream_cx_active: 0\r\nhttp.ingress_http.downstream_cx_delayed_close_timeout: 0\r\nhttp.ingress_http.downstream_cx_destroy: 10\r\nhttp.ingress_http.downstream_cx_destroy_active_rq: 0\r\nhttp.ingress_http.downstream_cx_destroy_local: 0\r\nhttp.ingress_http.downstream_cx_destroy_local_active_rq: 0\r\nhttp.ingress_http.downstream_cx_destroy_remote: 10\r\nhttp.ingress_http.downstream_cx_destroy_remote_active_rq: 0\r\nhttp.ingress_http.downstream_cx_drain_close: 0\r\nhttp.ingress_http.downstream_cx_http1_active: 0\r\nhttp.ingress_http.downstream_cx_http1_total: 10\r\nhttp.ingress_http.downstream_cx_http2_active: 0\r\nhttp.ingress_http.downstream_cx_http2_total: 0\r\nhttp.ingress_http.downstream_cx_idle_timeout: 0\r\nhttp.ingress_http.downstream_cx_overload_disable_keepalive: 0\r\nhttp.ingress_http.downstream_cx_protocol_error: 10\r\nhttp.ingress_http.downstream_cx_rx_bytes_buffered: 0\r\nhttp.ingress_http.downstream_cx_rx_bytes_total: 12960\r\nhttp.ingress_http.downstream_cx_ssl_active: 0\r\nhttp.ingress_http.downstream_cx_ssl_total: 0\r\nhttp.ingress_http.downstream_cx_total: 10\r\nhttp.ingress_http.downstream_cx_tx_bytes_buffered: 0\r\nhttp.ingress_http.downstream_cx_tx_bytes_total: 860\r\nhttp.ingress_http.downstream_cx_upgrades_active: 0\r\nhttp.ingress_http.downstream_cx_upgrades_total: 0\r\nhttp.ingress_http.downstream_flow_control_paused_reading_total: 0\r\nhttp.ingress_http.downstream_flow_control_resumed_reading_total: 0\r\nhttp.ingress_http.downstream_rq_1xx: 0\r\nhttp.ingress_http.downstream_rq_2xx: 0\r\nhttp.ingress_http.downstream_rq_3xx: 0\r\nhttp.ingress_http.downstream_rq_4xx: 0\r\nhttp.ingress_http.downstream_rq_5xx: 0\r\nhttp.ingress_http.downstream_rq_active: 0\r\nhttp.ingress_http.downstream_rq_completed: 0\r\nhttp.ingress_http.downstream_rq_http1_total: 10\r\nhttp.ingress_http.downstream_rq_http2_total: 0\r\nhttp.ingress_http.downstream_rq_idle_timeout: 0\r\nhttp.ingress_http.downstream_rq_non_relative_path: 0\r\nhttp.ingress_http.downstream_rq_overload_close: 0\r\nhttp.ingress_http.downstream_rq_response_before_rq_complete: 0\r\nhttp.ingress_http.downstream_rq_rx_reset: 10\r\nhttp.ingress_http.downstream_rq_timeout: 0\r\nhttp.ingress_http.downstream_rq_too_large: 0\r\nhttp.ingress_http.downstream_rq_total: 10\r\nhttp.ingress_http.downstream_rq_tx_reset: 0\r\nhttp.ingress_http.downstream_rq_ws_on_non_ws_route: 0\r\nhttp.ingress_http.no_cluster: 0\r\nhttp.ingress_http.no_route: 0\r\nhttp.ingress_http.rq_direct_response: 0\r\nhttp.ingress_http.rq_redirect: 0\r\nhttp.ingress_http.rq_reset_after_downstream_response_started: 0\r\nhttp.ingress_http.rq_total: 0\r\nhttp.ingress_http.rs_too_large: 0\r\nhttp.ingress_http.tracing.client_enabled: 0\r\nhttp.ingress_http.tracing.health_check: 0\r\nhttp.ingress_http.tracing.not_traceable: 0\r\nhttp.ingress_http.tracing.random_sampling: 0\r\nhttp.ingress_http.tracing.service_forced: 0\r\nlistener.0.0.0.0_8080.downstream_cx_active: 0\r\nlistener.0.0.0.0_8080.downstream_cx_destroy: 10\r\nlistener.0.0.0.0_8080.downstream_cx_total: 10\r\nlistener.0.0.0.0_8080.downstream_pre_cx_active: 0\r\nlistener.0.0.0.0_8080.downstream_pre_cx_timeout: 0\r\nlistener.0.0.0.0_8080.http.ingress_http.downstream_rq_1xx: 0\r\nlistener.0.0.0.0_8080.http.ingress_http.downstream_rq_2xx: 0\r\nlistener.0.0.0.0_8080.http.ingress_http.downstream_rq_3xx: 0\r\nlistener.0.0.0.0_8080.http.ingress_http.downstream_rq_4xx: 0\r\nlistener.0.0.0.0_8080.http.ingress_http.downstream_rq_5xx: 0\r\nlistener.0.0.0.0_8080.http.ingress_http.downstream_rq_completed: 0\r\nlistener.0.0.0.0_8080.no_filter_chain_match: 0\r\n```\r\n\r\n**Config**:\r\n\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n\r\nstats_config:\r\n  use_all_default_tags: true\r\n\r\nstats_flush_interval: \"10s\"\r\n\r\nstatic_resources:\r\n  listeners:\r\n    - name: \"ingress_listener\"\r\n      address:\r\n        socket_address:\r\n          protocol: \"TCP\"\r\n          address: \"0.0.0.0\"\r\n          port_value: 8080\r\n      filter_chains:\r\n        - filters:\r\n            - name: \"envoy.http_connection_manager\"\r\n              config:\r\n                max_request_headers_kb: 1\r\n                stat_prefix: \"ingress_http\"\r\n                route_config:\r\n                  name: \"ingress_route\"\r\n                  virtual_hosts:\r\n                    - name: \"virtual_host_service\"\r\n                      domains:\r\n                        - \"*\"\r\n                      routes:\r\n                        - match:\r\n                            prefix: \"/\"\r\n                          route:\r\n                            cluster: \"service\"\r\n                http_filters:\r\n                  - name: \"envoy.router\"\r\n\r\n  clusters:\r\n    - name: \"service\"\r\n      connect_timeout: \"0.25s\"\r\n      type: \"LOGICAL_DNS\"\r\n      dns_lookup_family: \"V4_ONLY\"\r\n      load_assignment:\r\n        cluster_name: \"service\"\r\n        endpoints:\r\n          - lb_endpoints:\r\n              - endpoint:\r\n                  address:\r\n                    socket_address:\r\n                      address: \"127.0.0.1\"\r\n                      port_value: 8001\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8545/comments",
    "author": "argos83",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2019-10-09T14:33:38Z",
        "body": "It looks like the problem here is that generally the 4xx stats are handled in the HCM, but for overly large headers (and other classes of errors) the codec_impl.cc does sendProtocolError and the stats aren't incremented.  I think it would be good to unify that handling."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-10-09T19:02:14Z",
        "body": "Yeah +1 we should definitely fix this."
      }
    ]
  },
  {
    "number": 8429,
    "title": "Next free field annotations via protoxform",
    "created_at": "2019-09-29T12:55:29Z",
    "closed_at": "2019-10-18T20:35:36Z",
    "labels": [
      "help wanted",
      "beginner",
      "api/v3"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8429",
    "body": "`protoxform` could fairly easily generate annotations such as:\r\n```\r\n// [#comment:next free field: 21]\r\n```\r\nfor every message (maybe only over a given size). This is a great beginner task for anyone interested in API tooling work.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8429/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "yittg",
        "created_at": "2019-09-29T14:39:45Z",
        "body": "@htuch I think I can take it, that would help me get familiar with the tool chain."
      }
    ]
  },
  {
    "number": 8366,
    "title": "UdpListenerImplTest flaked on macos",
    "created_at": "2019-09-25T15:49:52Z",
    "closed_at": "2019-12-13T16:49:48Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8366",
    "body": "[ RUN      ] IpVersions/UdpListenerImplTest.UdpEcho/IPv4\r\ntest/common/network/udp_listener_impl_test.cc:217: Failure\r\nExpected: (test_peer_address) != (nullptr), actual: 0x0 vs (nullptr)\r\nTerminated: 15\r\n-- Test timed out at 2019-09-24 22:38:26 UTC --\r\n\r\n@danzh2010 @conqerAtapple could one of you check this out?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8366/comments",
    "author": "alyssawilk",
    "comments": [
      {
        "user": "danzh2010",
        "created_at": "2019-09-25T18:58:37Z",
        "body": "`test_peer_address` is populated in onData_(), could it be possible that a write event triggers onWriteReady_() before onData_()?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-12-13T16:49:48Z",
        "body": "This was fixed."
      }
    ]
  },
  {
    "number": 8302,
    "title": "http: Allow upper bounding lifetime of downstream connections",
    "created_at": "2019-09-20T01:37:58Z",
    "closed_at": "2019-10-26T20:52:35Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8302",
    "body": "*Description*:\r\nWe use envoy as user-facing edge proxy behind lvs/ipvs.\r\nDuring the restart of the envoy proxy connections tend to migrate to other processes.\r\nThis leads to cascading load imbalance (the last process to restart has most connections, typically several times more than in steady mode).\r\n\r\nDoes it make sense to add configuration option to HTTP Connection Manager to gracefully drain a connection after certain timeout since connection establishment? (Similar to idle timeout)",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8302/comments",
    "author": "veshij",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2019-09-20T16:32:48Z",
        "body": "Sounds like you're asking for a \"max connection age\" or \"max requests\" features for http connection manager.\r\n\r\nYou could also look at using the hot restart mechanism to avoid shutting down listeners. There may periodically be versions of Envoy that cannot be upgraded with hot-restart, though."
      },
      {
        "user": "veshij",
        "created_at": "2019-09-20T16:43:48Z",
        "body": ">Sounds like you're asking for a \"max connection age\" or \"max requests\" features for http connection manager.\r\n\r\nYes, \"max connection age\" sounds right.\r\n\r\n>You could also look at using the hot restart mechanism to avoid shutting down listeners. There may periodically be versions of Envoy that cannot be upgraded with hot-restart, though.\r\n\r\nEven with hot restart we'll see imbalance for nodes being allocated/rebooted.\r\n"
      },
      {
        "user": "euroelessar",
        "created_at": "2019-09-20T19:31:47Z",
        "body": "> You could also look at using the hot restart mechanism to avoid shutting down listeners. There may periodically be versions of Envoy that cannot be upgraded with hot-restart, though.\r\n\r\nWe're using SO_REUSEPORT-based graceful deploy logic, it has exactly the same properties in regard of traffic imbalance as hot restart."
      }
    ]
  },
  {
    "number": 8286,
    "title": "fix the broken test/common/crypto/utility_test",
    "created_at": "2019-09-18T19:52:23Z",
    "closed_at": "2019-11-15T07:45:22Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8286",
    "body": "**Bug Template**\r\n\r\n*Title*: fix the bug in common/crypto/utility. \r\n\r\n*Description*:\r\n> Run the unit test test/common/crypto/utility_test with asan, the test fails w/ error \r\n\"allocated with new being deallocated with free\"\r\nIt's because around source/extensions/common/crypto/utility.cc:62\r\nThe wrapper is a dynamically cast pointer to the referenced \"key\" obj, in this case it's a PublicKeyObject() object \"new-d\" in the crypto/utility_test.cc\r\n```\r\n  auto pkey_wrapper = Common::Crypto::Access::getTyped<Common::Crypto::PublicKeyObject>(key);\r\n  EVP_PKEY* pkey = pkey_wrapper->getEVP_PKEY();\r\n\r\n  if (pkey == nullptr) {\r\n    free(pkey_wrapper);\r\n``` \r\n\r\n\r\n*Repro steps*:\r\n> bazel test --config=asan test/common/crypto:utility_test\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8286/comments",
    "author": "stevenzzzz",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-10-18T22:07:36Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stevenzzzz",
        "created_at": "2019-10-19T01:17:40Z",
        "body": "make it alive again.\r\nwe just need to  use delete here that should at least get rid of the error."
      }
    ]
  },
  {
    "number": 8202,
    "title": "Support HTTP request mirroring to multiple clusters",
    "created_at": "2019-09-10T23:13:00Z",
    "closed_at": "2019-12-26T20:58:39Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8202",
    "body": "*Title*: Support HTTP request mirroring to multiple clusters\r\n\r\n*Description*:\r\nCurrently a route action can only have one request mirror policy going to one cluster. This ticket is to file a design proposal // enhancement ticket to be able to specify a list of request mirroring policies. It should be a fairly straightforward-change, the router would have a `std::vector<..>` of ShadowWriters instead of just one, and on `maybeDoShadowing` iterate through the vector and call `.shadow(` on each. The API transition should be smooth as I _believe_ if you only provide a single object, protobuf can naturally translate it into a list of 1.\r\n\r\nCurrent concrete use-case we have is wanting to mirror 1% to cluster A and 3% to cluster B.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8202/comments",
    "author": "derekargueta",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2019-10-11T18:24:16Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-18T21:07:37Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "derekargueta",
        "created_at": "2019-11-27T00:44:29Z",
        "body": "@envoyproxy/api-shepherds just got a bump from an internal team that would really like the ability to mirror to 2 clusters at once (HTTP). Would we be able to re-open this? I can be the assignee."
      }
    ]
  },
  {
    "number": 8092,
    "title": "Unify field deprecation and unknown field handling behind the validation visitor",
    "created_at": "2019-08-29T20:56:06Z",
    "closed_at": "2020-06-05T14:34:21Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8092",
    "body": "Today, unknown field handling is actioned via the validation visitor but deprecated field handling is done inline. Ideally we have consistency here. This would be a nice cleanup, but isn't hugely impacting in any way beyond OCD.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8092/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "nezdolik",
        "created_at": "2020-02-05T12:23:20Z",
        "body": "@htuch don't know what is OCD, but could help with the issue :)"
      }
    ]
  },
  {
    "number": 7957,
    "title": "Prometheus: allow opting out of automatic namspacing",
    "created_at": "2019-08-18T13:27:27Z",
    "closed_at": "2020-07-10T22:11:06Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7957",
    "body": "*Title*: *Prometheus: allow opting out of automatic namespacing*\r\n\r\n*Description*:\r\nPR #2140 added automatic namspacing to all prometheus metrics emitted by envoy per prometheus best practices.\r\n\r\nA constant string `envoy_` is prepended to the prometheus metric names, with no way to override or opt out of the namespacing.\r\n\r\nAs filters add more metrics, they may be namespaced under a different prefix. For example istio would like some filter emitted metrics to be namespaced under `istio_` prefix.\r\n \r\nOptions\r\n1.  Add a convention: example: If a metrics name starts with `_` or some other marker, automatic namspacing will be disabled for that metric. \r\n2.  Add this option to admin config. \r\n    a. Add regex list to indicate metric names that would opt out of prometheus automatic namespacing.\r\n    b. Get this config to admin.cc. At present prometheus formatter receives no config.\r\n\r\n@ggreenway  @jmarantz \r\n\r\n  ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7957/comments",
    "author": "mandarjog",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-08-18T19:15:14Z",
        "body": "FWIW I would opt for (2)"
      },
      {
        "user": "mandarjog",
        "created_at": "2019-08-18T19:35:18Z",
        "body": "The challenge with admin config is that it forces us to update bootstrap.\r\nThis means any new filter injected thru wasm will need to have bootstrap updated in lockstep. It is possible though to use (2) to specify (1) and then all dynamically loaded filters continue to use that convention."
      },
      {
        "user": "mandarjog",
        "created_at": "2019-10-12T00:47:48Z",
        "body": "/cc @kyessenov "
      },
      {
        "user": "jplevyak",
        "created_at": "2020-06-29T18:13:55Z",
        "body": "@lizan we need this before upstreaming.\r\n"
      }
    ]
  },
  {
    "number": 7892,
    "title": "Segfault when referencing SDS secrets by name with default validation context",
    "created_at": "2019-08-11T20:10:29Z",
    "closed_at": "2019-09-26T23:19:31Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7892",
    "body": "Using the following TLS config for a cluster:\r\n\r\n```\r\ntls_context {\r\n  common_tls_context {\r\n    tls_params {\r\n      tls_minimum_protocol_version: TLSv1_2\r\n    }\r\n    alpn_protocols: \"h2\"\r\n    alpn_protocols: \"http/1.1\"\r\n    tls_certificate_sds_secret_configs {\r\n      name: \"s2s-certificate\"\r\n    }\r\n    combined_validation_context {\r\n      default_validation_context {\r\n        verify_subject_alt_name: \"whatever\"\r\n      }\r\n      validation_context_sds_secret_config {\r\n        name: \"s2s-ca\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nwith a static secret defined in the bootstrap as\r\n\r\n```\r\n        .addSecrets(Secret.newBuilder()\r\n            .setName(\"s2s-certificate\")\r\n            .setTlsCertificate(TlsCertificate.newBuilder()\r\n                .setCertificateChain(DataPlaneProtoUtils.fileLocation(CRT_PATH))\r\n                .setPrivateKey(DataPlaneProtoUtils.fileLocation(KEY_PATH))))\r\n        .addSecrets(Secret.newBuilder()\r\n            .setName(\"s2s-ca\")\r\n            .setValidationContext(CertificateValidationContext.newBuilder()\r\n                .setTrustedCa(DataPlaneProtoUtils.fileLocation(CA_PATH))))\r\n```\r\n\r\na crash occurs in `ContextConfigImpl::ContextConfigImpl` due to a bad dynamic cast:\r\n\r\n```\r\n    cvc_validation_callback_handle_ =\r\n        dynamic_cast<Secret::CertificateValidationContextSdsApi*>(\r\n            certificate_validation_context_provider_.get())\r\n            ->addValidationCallback(\r\n                [this](const envoy::api::v2::auth::CertificateValidationContext& dynamic_cvc) {\r\n                  getCombinedValidationContextConfig(dynamic_cvc);\r\n                });\r\n```\r\n\r\nas `certificate_validation_context_provider_` doesn't point to a `CertificateValidationContextSdsApi`:\r\n\r\n```\r\n(gdb) p *certificate_validation_context_provider_._M_ptr\r\n$4 = {_vptr$SecretProvider = 0x32f020 <vtable for Envoy::Secret::CertificateValidationContextConfigProviderImpl+16>}\r\n```\r\n\r\nFull back trace:\r\n\r\n```\r\n#0  0x000000000131a2f0 in std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*) ()\r\n#1  0x0000000000b911ac in std::__cxx11::list<Envoy::Common::CallbackManager<envoy::api::v2::auth::CertificateValidationContext const&>::CallbackHolder, std::allocator<Envoy::Common::CallbackManager<envoy::api::v2::auth::CertificateValidationContext const&>::CallbackHolder> >::_M_insert<Envoy::Common::CallbackManager<envoy::api::v2::auth::CertificateValidationContext const&>&, std::function<void (envoy::api::v2::auth::CertificateValidationContext const&)>&>(std::_List_iterator<Envoy::Common::CallbackManager<envoy::api::v2::auth::CertificateValidationContext const&>::CallbackHolder>, Envoy::Common::CallbackManager<envoy::api::v2::auth::CertificateValidationContext const&>&, std::function<void (envoy::api::v2::auth::CertificateValidationContext const&)>&) (this=0x100, __position=..., __args=..., __args=...) at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/stl_list.h:1802\r\n#2  std::__cxx11::list<Envoy::Common::CallbackManager<envoy::api::v2::auth::CertificateValidationContext const&>::CallbackHolder, std::allocator<Envoy::Common::CallbackManager<envoy::api::v2::auth::CertificateValidationContext const&>::CallbackHolder> >::emplace_back<Envoy::Common::CallbackManager<envoy::api::v2::auth::CertificateValidationContext const&>&, std::function<void (envoy::api::v2::auth::CertificateValidationContext const&)>&>(Envoy::Common::CallbackManager<envoy::api::v2::auth::CertificateValidationContext const&>&, std::function<void (envoy::api::v2::auth::CertificateValidationContext const&)>&) (this=0x100, __args=..., __args=...) at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/stl_list.h:1133\r\n#3  Envoy::Common::CallbackManager<envoy::api::v2::auth::CertificateValidationContext const&>::add(std::function<void (envoy::api::v2::auth::CertificateValidationContext const&)>) (this=0x100, callback=...)\r\n    at bazel-out/k8-opt/bin/source/common/common/_virtual_includes/callback_impl_lib/common/common/callback_impl.h:26\r\n#4  Envoy::Secret::CertificateValidationContextSdsApi::addValidationCallback(std::function<void (envoy::api::v2::auth::CertificateValidationContext const&)>) (this=0x0, callback=...) at bazel-out/k8-opt/bin/source/common/secret/_virtual_includes/sds_api_lib/common/secret/sds_api.h:163\r\n#5  0x0000000000b90255 in Envoy::Extensions::TransportSockets::Tls::ContextConfigImpl::ContextConfigImpl (this=0x258f040, vtt=<optimized out>,\r\n    config=..., default_min_protocol_version=769, default_max_protocol_version=771, default_cipher_suites=..., default_curves=...,\r\n    factory_context=...) at source/extensions/transport_sockets/tls/context_config_impl.cc:138\r\n#6  0x0000000000b91e16 in Envoy::Extensions::TransportSockets::Tls::ClientContextConfigImpl::ClientContextConfigImpl (this=0x258f040, config=...,\r\n    sigalgs=..., factory_context=...) at source/extensions/transport_sockets/tls/context_config_impl.cc:274\r\n#7  0x0000000000b89721 in Envoy::Extensions::TransportSockets::Tls::ClientContextConfigImpl::ClientContextConfigImpl (this=0x258f040, config=...,\r\n    secret_provider_context=...)\r\n    at bazel-out/k8-opt/bin/source/extensions/transport_sockets/tls/_virtual_includes/context_config_lib/extensions/transport_sockets/tls/context_config_impl.h:105\r\n#8  std::make_unique<Envoy::Extensions::TransportSockets::Tls::ClientContextConfigImpl, envoy::api::v2::auth::UpstreamTlsContext const&, Envoy::Server::Configuration::TransportSocketFactoryContext&> (__args=..., __args=...)\r\n    at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/unique_ptr.h:825\r\n#9  Envoy::Extensions::TransportSockets::Tls::UpstreamSslSocketFactory::createTransportSocketFactory (this=<optimized out>, message=..., context=...)\r\n    at source/extensions/transport_sockets/tls/config.cc:19\r\n#10 0x0000000000d72f50 in Envoy::Upstream::createTransportSocketFactory (config=..., factory_context=...)\r\n    at source/common/upstream/upstream_impl.cc:675\r\n#11 0x0000000000d7319e in Envoy::Upstream::ClusterImplBase::ClusterImplBase (this=0x220f8d0, cluster=..., runtime=..., factory_context=...,\r\n    stats_scope=..., added_via_api=<optimized out>) at source/common/upstream/upstream_impl.cc:686\r\n#12 0x0000000000d81e2f in Envoy::Upstream::BaseDynamicClusterImpl::ClusterImplBase (this=0x220f8d0)\r\n    at bazel-out/k8-opt/bin/source/common/upstream/_virtual_includes/upstream_includes/common/upstream/upstream_impl.h:799\r\n#13 Envoy::Upstream::EdsClusterImpl::EdsClusterImpl (this=0x220f8d0, cluster=..., runtime=..., factory_context=..., stats_scope=...,\r\n    added_via_api=<optimized out>) at source/common/upstream/eds.cc:14\r\n--Type <RET> for more, q to quit, c to continue without paging--\r\n#14 0x0000000000d853c3 in __gnu_cxx::new_allocator<Envoy::Upstream::EdsClusterImpl>::construct<Envoy::Upstream::EdsClusterImpl, envoy::api::v2::Cluster const&, Envoy::Runtime::Loader&, Envoy::Server::Configuration::TransportSocketFactoryContext&, std::unique_ptr<Envoy::Stats::Scope, std::default_delete<Envoy::Stats::Scope> >, bool> (this=<optimized out>, __p=0x23bf480, __args=<optimized out>, __args=<optimized out>, __args=<optimized out>,\r\n    __args=<optimized out>, __args=<optimized out>) at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/ext/new_allocator.h:136\r\n#15 std::allocator_traits<std::allocator<Envoy::Upstream::EdsClusterImpl> >::construct<Envoy::Upstream::EdsClusterImpl, envoy::api::v2::Cluster const&, Envoy::Runtime::Loader&, Envoy::Server::Configuration::TransportSocketFactoryContext&, std::unique_ptr<Envoy::Stats::Scope, std::default_delete<Envoy::Stats::Scope> >, bool> (__a=..., __p=0x23bf480, __args=<optimized out>, __args=<optimized out>, __args=<optimized out>, __args=<optimized out>,\r\n    __args=<optimized out>) at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/alloc_traits.h:475\r\n#16 std::_Sp_counted_ptr_inplace<Envoy::Upstream::EdsClusterImpl, std::allocator<Envoy::Upstream::EdsClusterImpl>, (__gnu_cxx::_Lock_policy)2>::_Sp_counted_ptr_inplace<envoy::api::v2::Cluster const&, Envoy::Runtime::Loader&, Envoy::Server::Configuration::TransportSocketFactoryContext&, std::unique_ptr<Envoy::Stats::Scope, std::default_delete<Envoy::Stats::Scope> >, bool> (this=<optimized out>, __args=<optimized out>, __args=<optimized out>,\r\n    __args=<optimized out>, __args=<optimized out>, __a=..., __args=<optimized out>)\r\n    at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr_base.h:526\r\n#17 std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<Envoy::Upstream::EdsClusterImpl, std::allocator<Envoy::Upstream::EdsClusterImpl>, envoy::api::v2::Cluster const&, Envoy::Runtime::Loader&, Envoy::Server::Configuration::TransportSocketFactoryContext&, std::unique_ptr<Envoy::Stats::Scope, std::default_delete<Envoy::Stats::Scope> >, bool> (this=<optimized out>, __a=..., __args=<optimized out>, __args=<optimized out>,\r\n    __args=<optimized out>, __args=<optimized out>, __args=<optimized out>)\r\n    at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr_base.h:637\r\n#18 std::__shared_ptr<Envoy::Upstream::EdsClusterImpl, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<Envoy::Upstream::EdsClusterImpl>, envoy::api::v2::Cluster const&, Envoy::Runtime::Loader&, Envoy::Server::Configuration::TransportSocketFactoryContext&, std::unique_ptr<Envoy::Stats::Scope, std::default_delete<Envoy::Stats::Scope> >, bool> (this=<optimized out>, __a=..., __args=<optimized out>, __args=<optimized out>,\r\n    __args=<optimized out>, __args=<optimized out>, __tag=..., __args=<optimized out>)\r\n    at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr_base.h:1294\r\n#19 std::shared_ptr<Envoy::Upstream::EdsClusterImpl>::shared_ptr<std::allocator<Envoy::Upstream::EdsClusterImpl>, envoy::api::v2::Cluster const&, Envoy::Runtime::Loader&, Envoy::Server::Configuration::TransportSocketFactoryContext&, std::unique_ptr<Envoy::Stats::Scope, std::default_delete<Envoy::Stats::Scope> >, bool> (this=<optimized out>, __a=..., __args=<optimized out>, __args=<optimized out>, __args=<optimized out>, __args=<optimized out>,\r\n    __tag=..., __args=<optimized out>) at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr.h:344\r\n#20 std::allocate_shared<Envoy::Upstream::EdsClusterImpl, std::allocator<Envoy::Upstream::EdsClusterImpl>, envoy::api::v2::Cluster const&, Envoy::Runtime::Loader&, Envoy::Server::Configuration::TransportSocketFactoryContext&, std::unique_ptr<Envoy::Stats::Scope, std::default_delete<Envoy::Stats::Scope> >, bool> (__a=..., __args=<optimized out>, __args=<optimized out>, __args=<optimized out>, __args=<optimized out>, __args=<optimized out>)\r\n    at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr.h:690\r\n#21 std::make_shared<Envoy::Upstream::EdsClusterImpl, envoy::api::v2::Cluster const&, Envoy::Runtime::Loader&, Envoy::Server::Configuration::TransportSocketFactoryContext&, std::unique_ptr<Envoy::Stats::Scope, std::default_delete<Envoy::Stats::Scope> >, bool> (__args=<optimized out>,\r\n    __args=<optimized out>, __args=<optimized out>, __args=<optimized out>, __args=<optimized out>)\r\n    at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr.h:706\r\n#22 Envoy::Upstream::EdsClusterFactory::createClusterImpl (this=<optimized out>, cluster=..., context=..., socket_factory_context=...,\r\n    stats_scope=...) at source/common/upstream/eds.cc:270\r\n#23 0x0000000000d9019f in Envoy::Upstream::ClusterFactoryImplBase::create (this=0x1397050 <Envoy::Upstream::EdsClusterFactory_registered>,\r\n    cluster=..., context=...) at source/common/upstream/cluster_factory_impl.cc:103\r\n--Type <RET> for more, q to quit, c to continue without paging--\r\n#24 0x0000000000d8f434 in Envoy::Upstream::ClusterFactoryImplBase::create (cluster=..., cluster_manager=..., stats=..., tls=..., dns_resolver=...,\r\n    ssl_context_manager=..., runtime=..., random=..., dispatcher=..., log_manager=..., local_info=..., admin=..., singleton_manager=...,\r\n    outlier_event_logger=..., added_via_api=<optimized out>, validation_visitor=..., api=...) at source/common/upstream/cluster_factory_impl.cc:68\r\n#25 0x0000000000c4bc12 in Envoy::Upstream::ProdClusterManagerFactory::clusterFromProto (this=<optimized out>, cluster=..., cm=...,\r\n    outlier_event_logger=..., added_via_api=<optimized out>) at source/common/upstream/cluster_manager_impl.cc:1267\r\n#26 0x0000000000c405e1 in Envoy::Upstream::ClusterManagerImpl::loadCluster (this=0x22fcf00, cluster=..., version_info=...,\r\n    added_via_api=<optimized out>, cluster_map=...) at source/common/upstream/cluster_manager_impl.cc:584\r\n#27 0x0000000000c4283f in Envoy::Upstream::ClusterManagerImpl::addOrUpdateCluster (this=0x22fcf00, cluster=..., version_info=...)\r\n    at source/common/upstream/cluster_manager_impl.cc:486\r\n#28 0x0000000000c5644c in Envoy::Upstream::CdsApiImpl::onConfigUpdate (this=<optimized out>, added_resources=..., removed_resources=...,\r\n    system_version_info=...) at source/common/upstream/cds_api_impl.cc:76\r\n#29 0x0000000000c55e9e in Envoy::Upstream::CdsApiImpl::onConfigUpdate (this=0x239dc00, resources=..., version_info=...)\r\n    at source/common/upstream/cds_api_impl.cc:53\r\n#30 0x0000000000da130a in Envoy::Config::GrpcMuxSubscriptionImpl::onConfigUpdate (this=0x239f050, resources=..., version_info=...)\r\n    at source/common/config/grpc_mux_subscription_impl.cc:56\r\n#31 0x0000000000d9e596 in Envoy::Config::GrpcMuxImpl::onDiscoveryResponse (this=0x231b900, message=...) at source/common/config/grpc_mux_impl.cc:174\r\n#32 0x0000000000d9f253 in Envoy::Grpc::AsyncStreamCallbacks<envoy::api::v2::DiscoveryResponse>::onReceiveMessageRaw (this=0x231b910, response=...)\r\n    at bazel-out/k8-opt/bin/source/common/grpc/_virtual_includes/typed_async_client_lib/common/grpc/typed_async_client.h:89\r\n#33 0x0000000000dc53d7 in Envoy::Grpc::AsyncStreamImpl::onData (this=0x2254750, data=..., end_stream=<optimized out>)\r\n    at source/common/grpc/async_client_impl.cc:138\r\n#34 0x0000000000dc9df4 in Envoy::Http::AsyncStreamImpl::encodeData (this=0x23b8800, data=..., end_stream=<optimized out>)\r\n    at source/common/http/async_client_impl.cc:101\r\n#35 0x0000000000e3c127 in Envoy::Http::Http2::ConnectionImpl::onFrameReceived (this=0x239f838, frame=0x22533a0)\r\n    at source/common/http/http2/codec_impl.cc:490\r\n#36 0x0000000000e407cc in Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_8::operator()(nghttp2_session*, nghttp2_frame const*, void*) const (this=<optimized out>, frame=0x100, user_data=0x23bf480) at source/common/http/http2/codec_impl.cc:799\r\n#37 Envoy::Http::Http2::ConnectionImpl::Http2Callbacks::Http2Callbacks()::$_8::__invoke(nghttp2_session*, nghttp2_frame const*, void*) (frame=0x100,\r\n    user_data=0x23bf480) at source/common/http/http2/codec_impl.cc:798\r\n#38 0x0000000000e497cf in session_call_on_frame_received (session=<optimized out>, frame=<optimized out>)\r\n    at /build/tmp/_bazel_bazel/b570b5ccd0454dc9af9f65ab1833764d/execroot/envoy/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:3295\r\n#39 nghttp2_session_on_data_received (session=0x2253200, frame=0x22533a0)\r\n    at /build/tmp/_bazel_bazel/b570b5ccd0454dc9af9f65ab1833764d/execroot/envoy/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:4966\r\n#40 0x0000000000e4b4a7 in session_process_data_frame (session=<optimized out>)\r\n    at /build/tmp/_bazel_bazel/b570b5ccd0454dc9af9f65ab1833764d/execroot/envoy/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:4986\r\n#41 nghttp2_session_mem_recv (session=<optimized out>, in=0x22bc8a0 \"\\300\\357%\\002\", inlen=<optimized out>)\r\n    at /build/tmp/_bazel_bazel/b570b5ccd0454dc9af9f65ab1833764d/execroot/envoy/external/com_github_nghttp2_nghttp2/lib/nghttp2_session.c:6607\r\n#42 0x0000000000e3ba76 in Envoy::Http::Http2::ConnectionImpl::dispatch (this=0x239f838, data=...) at source/common/http/http2/codec_impl.cc:350\r\n#43 0x0000000000db9b96 in Envoy::Http::CodecClient::onData (this=0x239f7a0, data=...) at source/common/http/codec_client.cc:116\r\n#44 0x0000000000dba62d in Envoy::Http::CodecClient::CodecReadFilter::onData (this=<optimized out>, data=...)\r\n--Type <RET> for more, q to quit, c to continue without paging--\r\n    at bazel-out/k8-opt/bin/source/common/http/_virtual_includes/codec_client_lib/common/http/codec_client.h:172\r\n#45 0x0000000000c17611 in Envoy::Network::FilterManagerImpl::onContinueReading (this=0x235a928, filter=<optimized out>, buffer_source=...)\r\n    at source/common/network/filter_manager_impl.cc:65\r\n#46 0x0000000000c13fcc in Envoy::Network::ConnectionImpl::onRead (this=<optimized out>, read_buffer_size=<optimized out>)\r\n    at source/common/network/connection_impl.cc:269\r\n#47 Envoy::Network::ConnectionImpl::onReadReady (this=0x235a900) at source/common/network/connection_impl.cc:513\r\n#48 0x0000000000c13a91 in Envoy::Network::ConnectionImpl::onFileEvent (this=0x235a900, events=<optimized out>)\r\n    at source/common/network/connection_impl.cc:489\r\n#49 0x0000000000c0e735 in std::function<void (unsigned int)>::operator()(unsigned int) const (this=0x23bf480, __args=3)\r\n    at /usr/lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/std_function.h:706\r\n#50 Envoy::Event::FileEventImpl::assignEvents(unsigned int)::$_0::operator()(int, short, void*) const (this=<optimized out>, what=<optimized out>,\r\n    arg=0x3) at source/common/event/file_event_impl.cc:65\r\n#51 Envoy::Event::FileEventImpl::assignEvents(unsigned int)::$_0::__invoke(int, short, void*) (what=<optimized out>, arg=0x3)\r\n    at source/common/event/file_event_impl.cc:49\r\n#52 0x0000000000fbe39d in event_process_active_single_queue ()\r\n#53 0x0000000000fbc950 in event_base_loop ()\r\n```\r\n\r\nTraces taken from 3d6d6fa67530c2915a863ea775996f813b7a6e3b, but this also reproduces on latest master (I didn't check to see if the back trace changed).",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7892/comments",
    "author": "snowp",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-08-11T20:19:07Z",
        "body": "@JimmyCYJ "
      },
      {
        "user": "mattklein123",
        "created_at": "2019-08-12T18:55:13Z",
        "body": "cc @lizan @PiotrSikora also"
      },
      {
        "user": "snowp",
        "created_at": "2019-08-13T01:12:55Z",
        "body": "Poked around the code a bit and think the fix might be to just lift up the `addValidationCallback` up the class hierarchy to avoid the dynamic downcast and provide noop implementations for the other subclasses. This seems to be the pattern used for the other callback related function."
      }
    ]
  },
  {
    "number": 7860,
    "title": "xds: omit node in subsequent discovery requests in the gRPC call",
    "created_at": "2019-08-07T23:57:56Z",
    "closed_at": "2019-08-13T22:48:13Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7860",
    "body": "xDS requests include a node proto with an arbitrarily large metadata field. This causes a problem if this metadata is a large JSON since it is sent for every EDS request. The proposal is to omit `node` field in a gRPC stream after the first request. Most gRPC servers can save some state per stream, so they can store the node in the local scope. In fact, this optimization is already applied for ALS and metrics services, so it seems like a gap in xDS.\r\n\r\ncc @htuch\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7860/comments",
    "author": "kyessenov",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-08-08T03:06:25Z",
        "body": "Yes, this should be done if we're not already. I thought we were already doing this."
      },
      {
        "user": "kyessenov",
        "created_at": "2019-08-08T03:22:18Z",
        "body": "I'm pretty sure it's not happening right now. Taking a cursory look at go-control-plane, I see that it does not cache Node from the first request, but it still works. More importantly, this is under-specified in xDS specification."
      }
    ]
  },
  {
    "number": 7713,
    "title": "Allow Envoy::OptionsImpl to take a Bootstrap proto object directly",
    "created_at": "2019-07-24T22:16:16Z",
    "closed_at": "2019-07-27T13:52:06Z",
    "labels": [
      "enhancement",
      "design proposal",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7713",
    "body": "*Description*:\r\n\r\nI'd like to add a method to `Envoy::OptionsImpl` to specify a Bootstrap proto directly as opposed to using the `setConfigYaml`/`setConfigPath` fields, i.e. add something like:\r\n\r\n```c\r\nvoid setConfigProto(const envoy::config::bootstrap::v2::Bootstrap& config_proto)\r\n```\r\n\r\nThis would allow clients to directly specify a Bootstrap proto object without having to deal with yaml serialization and would simplify our configuration process since we construct the Bootstrap proto programmatically as a protocol buffer.\r\n\r\nAnother option might be to have Envoy::OptionsImpl take a text proto, e.g. `configTextProto`.\r\n\r\nWhile `configPath` does allow one to provide a text proto via the command line, I'd prefer a route that avoids the file system if possible.\r\n\r\nDo any of these changes sound reasonable?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7713/comments",
    "author": "fcfort",
    "comments": [
      {
        "user": "dio",
        "created_at": "2019-07-24T22:28:55Z",
        "body": "@alyssawilk @htuch what do you think?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-07-24T23:58:59Z",
        "body": "This seems fine to me."
      },
      {
        "user": "htuch",
        "created_at": "2019-07-25T14:35:53Z",
        "body": "Please do the direct typed proto, this is a great idea. I would skip text proto, unless there is a product need for this, since it is not a stable format and we're trying to encourage folks to stick to YAML as the canonical text format in Envoy."
      },
      {
        "user": "fcfort",
        "created_at": "2019-07-25T14:40:38Z",
        "body": "SG, I will send a PR soon."
      },
      {
        "user": "stevenzzzz",
        "created_at": "2019-07-25T19:26:33Z",
        "body": "+1, please expose the bootstrap protobuf from OptionsImpl as well, as in our user story, there are components which read from bootstrap config. "
      }
    ]
  },
  {
    "number": 7706,
    "title": "HeaderMap: simplify implementation and reduce unsafe implementation features",
    "created_at": "2019-07-24T09:09:00Z",
    "closed_at": "2020-09-17T01:58:32Z",
    "labels": [
      "area/security",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7706",
    "body": "We know that HeaderMapImpl (header_map_impl.cc) has been a historical source of multiple security and correctness issues. This is largely because of its complex implementation driven by performance concerns (O(1), dynamic and inline headers) as well as the use of manual memory allocation, memcpy, C string manipulation etc. Opening this issue to track any structural wins we can achieve here. CC @PiotrSikora @mattklein123 ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7706/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-09-17T01:58:32Z",
        "body": "I think all of the known fixes have been done or are in progress, so closing. We can open new/specific issues if necessary."
      }
    ]
  },
  {
    "number": 7579,
    "title": "HCM: buffered data ignored when onData never returns StopIteration",
    "created_at": "2019-07-15T14:10:49Z",
    "closed_at": "2020-03-18T19:08:33Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7579",
    "body": "If any `encodeData` callbacks call `addEncodedData` and no StopIteration* return values are returned, the buffered data will never be read, as we never call `commonContinue` due to the `iteration_state_` check in `commonHandleAfterDataCallback`:\r\n\r\n```\r\n  if (status == FilterDataStatus::Continue) {\r\n    if (iteration_state_ == IterationState::StopSingleIteration) {\r\n      commonHandleBufferData(provided_data);\r\n      commonContinue();\r\n      return false;\r\n    } else {\r\n      ASSERT(headers_continued_);\r\n    }\r\n```\r\n\r\nThis becomes problematic because the following code misbehaves when there's only one `encodeData` callback:\r\n\r\n```\r\n  Http::FilterDataStatus encodeData(Buffer::Instance& data, bool end_stream) {\r\n    encoder_callbacks_->addEncodedData(data, false);\r\n\r\n    if (end_stream) {\r\n      encoder_callbacks_->modifyEncodingBuffer([](auto& buffer) {\r\n        // do something with the buffer\r\n      });\r\n      return Http::FilterDataStatus::Continue;\r\n    }\r\n\r\n    return Http::FilterDataStatus::StopIterationNoBuffer;\r\n  }\r\n};\r\n```\r\n\r\nSince the first call is has `end_stream = true`, we add data to buffer but since we return `Continue`, the buffer is never passed to the next filter.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7579/comments",
    "author": "snowp",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-07-15T15:29:10Z",
        "body": "Thinking about a solution here: maybe we should track whether `add*Data` has been called and enter the `commonContinue` block if that's the case even if `iteration_state_ != IterationState::StopSingleIteration`?\r\n\r\ncc @alyssawilk @mattklein123 "
      },
      {
        "user": "mattklein123",
        "created_at": "2019-07-15T16:48:45Z",
        "body": "cc @soya3129 who has probably been thinking about this code the most recently. I'm guessing she will have some ideas."
      },
      {
        "user": "soya3129",
        "created_at": "2019-07-16T09:24:48Z",
        "body": "> Thinking about a solution here: maybe we should track whether `add*Data` has been called and enter the `commonContinue` block if that's the case even if `iteration_state_ != IterationState::StopSingleIteration`?\r\n> \r\n> cc @alyssawilk @mattklein123\r\n\r\nIf we call commonContinue() when `iteration_state_ != IterationState::StopSingleIteration`, are we going to send headers and not buffered data, etc to the following filters twice? Because returning continue will keep iterating through the following filters, and commonContinue() will do the same(?).\r\n\r\nI could be wrong. But I thought buffering data implies we need to StopIteration? Otherwise, we might just send data to the next filter? What is the scenario where we need to buffer but don't need to wait? Thanks!!\r\n\r\n"
      },
      {
        "user": "snowp",
        "created_at": "2019-07-16T17:43:55Z",
        "body": "I think sending the data through directly is a valid solution too, it's definitely  better than just dropping the data completely. \r\n\r\nThe trade off between the two becomes an API question imo: should the filter author be responsible for tracking whether they are buffering multiple frames? or can they use the addData callback to collect all the data into the buffer regardless of the # of frames? \r\n\r\nAnother thing to consider is what the semantics of calling addData multiple times:\r\n```\r\nDataStatus decodeData(Buffer::Instance& data, bool end_stream) {\r\n  decoder_callbacks_->addDecodedData(Buffer::OwnedImpl(\"data1\"));\r\n  decoder_callbacks_->addDecodedData(Buffer::OwnedImpl(\"data2\"));\r\n  decoder_callbacks_->addDecodedData(Buffer::OwnedImpl(\"data3\"));\r\n  return Continue;\r\n}\r\n```\r\n\r\nshould this pass through the data to the subsequent filters 3 times? or should it be buffered together with the original data as a single callback?\r\n\r\nI'm think I'm leaning towards the buffering solution and emptying the buffer on a Continue (without repeating data/headers/etc. that was already sent), wdyt @soya3129 "
      },
      {
        "user": "soya3129",
        "created_at": "2019-07-17T02:19:38Z",
        "body": "The buffering solution on a Continue sounds good as long as we don't repeat, :)"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-03-18T19:08:33Z",
        "body": "I think this was fixed actually."
      }
    ]
  },
  {
    "number": 7548,
    "title": "flip all deprecated config fatal in example configs test",
    "created_at": "2019-07-11T20:10:59Z",
    "closed_at": "2019-08-28T15:04:25Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7548",
    "body": "It's annoying to update example configs when flipping things fatal as part of release - we should make it so folks update the configs when they mark config deprecated.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7548/comments",
    "author": "alyssawilk",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-07-11T20:23:06Z",
        "body": "Big +1"
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-07-17T19:27:05Z",
        "body": "Huh, so for flipping things on, we can run setAllFeaturesAllowed, which iterates through the vector and flips things.  Unfortunately for config we don't have an equivalent list of deprecated-but-not-fatal config.\r\n\r\nOffhand we could\r\n1) once per test runner grep for deprecated fields and generate the list\r\n2) always pre-generate the list of deprecated-but-not-fatal fields, format-verify it is up to date, and have a setAllDeprecatedFeaturesFatal function we can run in unit tests\r\n3) have a compile option for \"default all deprecated options fatal\" and run it on one of our builds\r\n\r\n2 gives us more flexibility in doing this per-test, but 3 allows folks to capture their own locally deprecated proto fields.  I lean a bit towards 3, just because I like the debug build being cutting-edge latest-and-greatest, and eventually having all features true by default and all deprecated features false\r\n\r\nAnyone have other ideas or preferences before I start doing work?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-07-22T21:14:59Z",
        "body": "(3) sounds good to me."
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-08-22T20:29:07Z",
        "body": "Grr, not closed.\r\nLooking into coverage, this only works where we have an actual runtime.  More work to be done, including the new tests."
      }
    ]
  },
  {
    "number": 7358,
    "title": "Propagate downstream timeout to upstream through multiple Envoys",
    "created_at": "2019-06-21T20:36:06Z",
    "closed_at": "2019-10-08T02:48:24Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7358",
    "body": "When running Envoy on both egress and ingress, the client will provide a timeout header to the egress Envoy, which will propagate the expected upstream timeout in `x-envoy-expected-rq-timeout-ms`. The upstream Envoy will not read this header, so it will resolve a new timeout value that is set as the expected timeout for the upstream service. This means that the deadline expected by the egress Envoy is ignored in favor of the ingress Envoy, resulting in the upstream service having an incorrect view of the actual deadline.\r\n\r\nIt seems like either \r\n\r\n1) inserting `x-envoy-rq-timeout-ms` with the expected timeout on egress\r\n2) parsing `x-envoy-expected-rq-timeout-ms` as the deadline on the ingress side\r\n\r\nwould solve the issue (likely guarded by a config flag).\r\n\r\nThoughts?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7358/comments",
    "author": "snowp",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-06-21T21:33:49Z",
        "body": "Yeah agreed I think either would solve. I don't have a super strong opinion on which one. \r\n\r\nThis does bring up the general topic of deadline propagation which we haven't really tackled yet in a holistic way. I've thought at some point we may also want to stick the deadline in trace context baggage since this would also propagate through app calls, though that is a larger problem than you are trying to solve here."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2019-06-25T10:23:27Z",
        "body": "+1. It would be very useful.  Inserting `x-envoy-rq-timeout-ms` on the ingress side may be good idea. That might require users to set `use_remote_address` to ensure that it is not sanitized?\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-07-25T10:25:14Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "nezdolik",
        "created_at": "2019-08-01T15:18:17Z",
        "body": "@snowp would like to help out with this."
      },
      {
        "user": "snowp",
        "created_at": "2019-08-01T17:29:07Z",
        "body": "@nezdolik Great, I'll assign this one to you. Feel free to ping me with any questions"
      }
    ]
  },
  {
    "number": 7325,
    "title": "Evaluate accesslog filters in Envoy rather than accesslog implementations",
    "created_at": "2019-06-19T19:59:47Z",
    "closed_at": "2019-07-16T23:51:42Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7325",
    "body": "(Cleanup proposal.)\r\n\r\nCurrently, access log filters are manually evaluated in each implementation. Filters are instantiated and passed to the AccessLog::Instance, and then each AccessLog::Instance's `log()` implementation has logic of `if (filter_ && filter_.evaluate(...)) then log()`. Instead, I propose we move that logic up to some sort of public `bool evaluate_filter(...)` function in the AccessLog interface, and wrap all of Envoy's AccessLog::Instance::log() calls in a conditional.\r\n\r\ncc: @ambuc @eziskind @AndresGuedez ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7325/comments",
    "author": "auni53",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-06-19T20:33:46Z",
        "body": "+1 would love to see this happen, agreed the current situation is a lot of copied code."
      },
      {
        "user": "auni53",
        "created_at": "2019-06-20T14:45:41Z",
        "body": "Do you like my proposed design? Or is there a cleaner way to automatically call that evaluate_filter() automatically on Instance::log() calls. Alternative idea, I'll add a submitLog() function, implemented in the base class, that does the filter check and then calls log(). Thereby keep the logic contained and tested within AccessLog while still having a better implementation/inheritance pattern.\r\n\r\nAs well, as far as I'm aware this change will have no performance benefits, but make the code better. Correct assessment?"
      },
      {
        "user": "auni53",
        "created_at": "2019-06-20T18:07:38Z",
        "body": "Ah, revised question. This log() function offered by the AccessLog seems to be available to all filters. I had considered making this change slyly such that custom implementations of an AccessLog don't have to change anything, but on the other hand if I change the log() call that the HCM makes, should I worry about other consumers' filter implementations relying on calling an accesslog log()?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-06-21T00:04:57Z",
        "body": "@auni53 I think it's probably fine to change the log interface if it makes it better. I would have to sit down and look at all the code in detail to page it back in to have a real opinion, but my general feeling is that there are a bunch of ways you can de-dup the code and I don't feel too strongly about any one method. Feel free to put up a PR and we can discuss if you like one way over the others."
      }
    ]
  },
  {
    "number": 7314,
    "title": "buffer: Switch from the libevent impl of Envoy::Buffer to Brian Pane's implementation",
    "created_at": "2019-06-18T16:24:15Z",
    "closed_at": "2019-08-14T18:45:13Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7314",
    "body": "The codebase has two alternate implementations of Envoy::Buffer. The legacy one is derived from libevent's buffer implementation, and has a ton of mileage on it, but also numerous known fuzzing issues.\r\n\r\nA new one was created by @brian-pane which is off by default. There has been some fuzzing work done on it and it looks clean.\r\n\r\nWe should use this issue to collect experiences canarying the new buffer implementation, by adding command-line option:\r\n```\r\n   --use-libevent-buffers false\r\n```\r\n\r\nPotential canaryiers include: @moderation and @mattklein123 .\r\n\r\nOnce done we can (a) switch the default to use the new buffer implementation, and then later (b) drop the libevent implementation from the codebase, enabling some simplification.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7314/comments",
    "author": "jmarantz",
    "comments": [
      {
        "user": "derekargueta",
        "created_at": "2019-06-18T18:55:28Z",
        "body": "I'm interested in canarying the new implementation at Pinterest"
      },
      {
        "user": "jmarantz",
        "created_at": "2019-06-18T18:59:23Z",
        "body": "CC @htuch @asraa for any more detail available on fuzzing of the new buffers.\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-06-18T19:01:19Z",
        "body": "I will give this a smoke test at Lyft in the next few days."
      },
      {
        "user": "moderation",
        "created_at": "2019-06-18T20:19:02Z",
        "body": "I've been running the new buffer implementation in my home network and at work for a couple of weeks. The testing is pretty low volume and low complexity. No Lua, no hot restart etc. But we have tested http, h2, grpc, tcp, rate limiting, external auth etc. at work. No issues thus far."
      },
      {
        "user": "asraa",
        "created_at": "2019-06-19T16:53:42Z",
        "body": "The  buffer_fuzz_test uses the new buffer implementation, and there haven't been any issues reported so far. If there's some interesting cases or things to explore, I can add them in as corpus entries and/or extend the fuzz test."
      },
      {
        "user": "jmarantz",
        "created_at": "2019-07-16T16:08:58Z",
        "body": "per discussion on conf call; will flip default, make an announcement, and add a release note.\r\n"
      },
      {
        "user": "Mythra",
        "created_at": "2019-08-09T03:53:52Z",
        "body": "Any update to making this the default? Would be nice to flip our cli flags from having to manually specify it as on."
      },
      {
        "user": "jmarantz",
        "created_at": "2019-08-09T12:19:45Z",
        "body": "Sorry about this; this slipped my mind; thanks for the reminder. Unfortunately I'm a bit overloaded right now so if you want to grab this, please do so!\r\n"
      },
      {
        "user": "Mythra",
        "created_at": "2019-08-09T13:34:36Z",
        "body": "No worries! I’ll take a crack at it later today. "
      }
    ]
  },
  {
    "number": 7203,
    "title": "extenstion / feature request - omit canary hosts during retry of a failed request",
    "created_at": "2019-06-07T12:54:08Z",
    "closed_at": "2019-06-20T21:59:00Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7203",
    "body": "*Title*: *Retry predicate that can omit canary hosts*\r\n\r\n*Description*:\r\n\r\nIn a canary deployment, failures in the canary subset can be retried on other non canary hosts that are part of the cluster.\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7203/comments",
    "author": "sriduth",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-06-10T16:57:38Z",
        "body": "This should be easy to do with a host predicate that just checks for `!host->canary()`, possibly with an accompanying retry priority plugin if this needs to be done across priorities as well."
      },
      {
        "user": "sriduth",
        "created_at": "2019-06-10T18:29:17Z",
        "body": "I'll try to implement this. "
      }
    ]
  },
  {
    "number": 7178,
    "title": "build: consistent debug symbols/copts in public images for external deps",
    "created_at": "2019-06-05T16:53:52Z",
    "closed_at": "2022-03-22T19:44:59Z",
    "labels": [
      "area/build",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7178",
    "body": "We set various options in envoy_build_system.bzl, and we should audit to figure out which options should potentially be instead set as part of bazelrc copts for different profiles. The main one that comes to mind is consistent use of `-ggdb3` for envoy and all deps. There may be others.\r\n\r\ncc @lizan @keith @jmillikin-stripe ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7178/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "keith",
        "created_at": "2019-06-05T17:05:05Z",
        "body": "Another thing we noticed when working on NDK support that is related to this is that the top level cc_binary gets some linkopts like `-lm` that are also required even if you're not using the binary target. I'm not sure how those should be handled but ideally they would be pushed down to the targets that actually need them."
      },
      {
        "user": "htuch",
        "created_at": "2019-06-05T18:30:12Z",
        "body": "As long as you push this to `.bazelrc`, `rules_foreign_cc` should be smart enough to pick up on it and propagate it out to the external deps."
      },
      {
        "user": "mattklein123",
        "created_at": "2022-03-22T19:44:59Z",
        "body": "I think with various bazel rules updates this is working now for external deps. Closing. We can open new issues as needed."
      }
    ]
  },
  {
    "number": 7158,
    "title": "Add HTTP/1.1 tests to test/common/http/htt1/conn_pool_test.cc",
    "created_at": "2019-06-04T17:27:24Z",
    "closed_at": "2019-10-28T01:47:23Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7158",
    "body": "**Issue Template**\r\n\r\n*Title*: Add HTTP/1.1 tests to test/common/http/htt1/conn_pool_test.cc\r\n\r\n*Description*:\r\nCurrently the tests in  test/common/http/htt1/conn_pool_test.cc are for HTTP/1.0 because the protocol_ variable is not set because the parser_ is not run.  We should have tests which cover HTTP/1.1 as well.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7158/comments",
    "author": "jplevyak",
    "comments": [
      {
        "user": "derekargueta",
        "created_at": "2019-10-27T02:47:36Z",
        "body": "It seems the referenced PR (#7124) fixed this by setting the default protocol to HTTP/1.1 for all tests except when overridden for HTTP/1.0-specific tests?"
      }
    ]
  },
  {
    "number": 7130,
    "title": "Filesystem subscription assert failure, inotify_fd_ negative",
    "created_at": "2019-05-31T21:36:19Z",
    "closed_at": "2019-12-04T19:09:44Z",
    "labels": [
      "enhancement",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7130",
    "body": "**Issue Template**\r\n\r\n*Title*: Filesystem subscription assert failure, inotify_fd_ negative\r\n\r\n*Description*:\r\nCreate a large config using filesystem based XDS, seems to crash every time with `assert failure: inotify_fd_ >= 0`\r\n\r\nRunning `envoyproxy/envoy-dev:829b905ca0fdc85233c3969247e53a62a52ac627` on Kubernetes\r\n\r\nI have 226 different eds files and 14 different rds files\r\n\r\n*Config*:\r\n>Include the config used to configure Envoy.\r\n\r\n```\r\n    node:\r\n      metadata:\r\n        NODE_UID: node.default\r\n        NODE_NAMESPACE: default\r\n      id: node\r\n      cluster: envoy\r\n    admin:\r\n      access_log_path: /dev/stdout\r\n      address:\r\n        socket_address:\r\n          address: 0.0.0.0\r\n          port_value: 15000\r\n\r\n    dynamic_resources:\r\n      lds_config:\r\n        path: /etc/config/config/lds.yaml\r\n      cds_config:\r\n        path:  /etc/config/config/cds.yaml\r\n```\r\nI can provide all the xDS config if needed, just let me know. Its about 2mb which is why I didn't include it.\r\n\r\n*Call Stack*:\r\n```\r\n[2019-05-31 21:28:03.840][1][critical][assert] [external/envoy/source/common/filesystem/inotify/watcher_impl.cc:27] assert failure: inotify_fd_ >= 0.\r\n[2019-05-31 21:28:03.841][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:81] Caught Aborted, suspect faulting address 0x1\r\n[2019-05-31 21:28:03.841][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:69] Backtrace (use tools/stack_decode.py to get line numbers):\r\n[2019-05-31 21:28:03.841][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #0: __restore_rt [0x7fded743c390]\r\n[2019-05-31 21:28:03.844][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #1: Envoy::Event::DispatcherImpl::createFilesystemWatcher() [0x8d4188]\r\n[2019-05-31 21:28:03.847][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #2: Envoy::Config::FilesystemSubscriptionImpl::FilesystemSubscriptionImpl() [0x8b34f8]\r\n[2019-05-31 21:28:03.850][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #3: Envoy::Config::SubscriptionFactory::subscriptionFromConfigSource() [0x8b12ad]\r\n[2019-05-31 21:28:03.853][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #4: Envoy::Upstream::EdsClusterImpl::EdsClusterImpl() [0xa13d65]\r\n[2019-05-31 21:28:03.856][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #5: Envoy::Upstream::EdsClusterFactory::createClusterImpl() [0xa15d9a]\r\n[2019-05-31 21:28:03.859][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #6: Envoy::Upstream::ClusterFactoryImplBase::create() [0xa2678c]\r\n[2019-05-31 21:28:03.862][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #7: Envoy::Upstream::ClusterFactoryImplBase::create() [0xa25a69]\r\n[2019-05-31 21:28:03.865][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #8: Envoy::Upstream::ProdClusterManagerFactory::clusterFromProto() [0x90f779]\r\n[2019-05-31 21:28:03.868][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #9: Envoy::Upstream::ClusterManagerImpl::loadCluster() [0x903f8d]\r\n[2019-05-31 21:28:03.871][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #10: Envoy::Upstream::ClusterManagerImpl::addOrUpdateCluster() [0x905d21]\r\n[2019-05-31 21:28:03.874][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #11: Envoy::Upstream::CdsApiImpl::onConfigUpdate() [0x919f5c]\r\n[2019-05-31 21:28:03.877][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #12: Envoy::Upstream::CdsApiImpl::onConfigUpdate() [0x91998f]\r\n[2019-05-31 21:28:03.880][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #13: Envoy::Config::FilesystemSubscriptionImpl::refresh() [0x8b37c6]\r\n[2019-05-31 21:28:03.883][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #14: Envoy::Upstream::CdsApiImpl::initialize() [0x91ac21]\r\n[2019-05-31 21:28:03.886][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #15: Envoy::Upstream::ClusterManagerInitHelper::maybeFinishInitialize() [0x901a54]\r\n[2019-05-31 21:28:03.889][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #16: Envoy::Upstream::ClusterManagerImpl::ClusterManagerImpl() [0x902a06]\r\n[2019-05-31 21:28:03.892][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #17: Envoy::Upstream::ProdClusterManagerFactory::clusterManagerFromProto() [0x90efd4]\r\n[2019-05-31 21:28:03.895][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #18: Envoy::Server::Configuration::MainImpl::initialize() [0x8ae448]\r\n[2019-05-31 21:28:03.898][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #19: Envoy::Server::InstanceImpl::initialize() [0x88550f]\r\n[2019-05-31 21:28:03.901][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #20: Envoy::Server::InstanceImpl::InstanceImpl() [0x88224c]\r\n[2019-05-31 21:28:03.904][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #21: std::make_unique<>() [0x54921f]\r\n[2019-05-31 21:28:03.907][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #22: Envoy::MainCommonBase::MainCommonBase() [0x548d05]\r\n[2019-05-31 21:28:03.909][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #23: Envoy::MainCommon::MainCommon() [0x5498b8]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #24: main [0x548543]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #25: [0x7fded6d78830]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:81] Caught Segmentation fault, suspect faulting address 0x0\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:69] Backtrace (use tools/stack_decode.py to get line numbers):\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #0: __restore_rt [0x7fded743c390]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #1: Envoy::Event::DispatcherImpl::createFilesystemWatcher() [0x8d4188]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #2: Envoy::Config::FilesystemSubscriptionImpl::FilesystemSubscriptionImpl() [0x8b34f8]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #3: Envoy::Config::SubscriptionFactory::subscriptionFromConfigSource() [0x8b12ad]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #4: Envoy::Upstream::EdsClusterImpl::EdsClusterImpl() [0xa13d65]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #5: Envoy::Upstream::EdsClusterFactory::createClusterImpl() [0xa15d9a]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #6: Envoy::Upstream::ClusterFactoryImplBase::create() [0xa2678c]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #7: Envoy::Upstream::ClusterFactoryImplBase::create() [0xa25a69]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #8: Envoy::Upstream::ProdClusterManagerFactory::clusterFromProto() [0x90f779]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #9: Envoy::Upstream::ClusterManagerImpl::loadCluster() [0x903f8d]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #10: Envoy::Upstream::ClusterManagerImpl::addOrUpdateCluster() [0x905d21]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #11: Envoy::Upstream::CdsApiImpl::onConfigUpdate() [0x919f5c]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #12: Envoy::Upstream::CdsApiImpl::onConfigUpdate() [0x91998f]\r\n[2019-05-31 21:28:03.912][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #13: Envoy::Config::FilesystemSubscriptionImpl::refresh() [0x8b37c6]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #14: Envoy::Upstream::CdsApiImpl::initialize() [0x91ac21]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #15: Envoy::Upstream::ClusterManagerInitHelper::maybeFinishInitialize() [0x901a54]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #16: Envoy::Upstream::ClusterManagerImpl::ClusterManagerImpl() [0x902a06]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #17: Envoy::Upstream::ProdClusterManagerFactory::clusterManagerFromProto() [0x90efd4]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #18: Envoy::Server::Configuration::MainImpl::initialize() [0x8ae448]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #19: Envoy::Server::InstanceImpl::initialize() [0x88550f]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #20: Envoy::Server::InstanceImpl::InstanceImpl() [0x88224c]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #21: std::make_unique<>() [0x54921f]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #22: Envoy::MainCommonBase::MainCommonBase() [0x548d05]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #23: Envoy::MainCommon::MainCommon() [0x5498b8]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #24: main [0x548543]\r\n[2019-05-31 21:28:03.913][1][critical][backtrace] [bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #25: [0x7fded6d78830]\r\n```\r\n\r\ncc @silentdai ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7130/comments",
    "author": "howardjohn",
    "comments": [
      {
        "user": "howardjohn",
        "created_at": "2019-06-01T00:06:25Z",
        "body": "This was fixed by setting `sysctl -w user.max_inotify_watches=524288`, so feel free to close this unless there is interest in improving the error message, as it isn't immediately obvious what the issue is"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-06-01T02:02:41Z",
        "body": "I would probably just make the RELEASE_ASSERT error message describe which sysctl might need tweaking. Marking help wanted but PRs appreciated."
      },
      {
        "user": "sriduth",
        "created_at": "2019-07-01T05:56:41Z",
        "body": "Hello! if no one has picked this up / finished this yet, I can work on this."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-07-01T13:27:54Z",
        "body": "@sriduth go for it!"
      }
    ]
  },
  {
    "number": 6736,
    "title": "//test/integration/... requires compiling all extensions",
    "created_at": "2019-04-28T16:43:20Z",
    "closed_at": "2020-06-25T23:22:06Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6736",
    "body": "I have noticed that when testing `//test/integration/...` it compiles all extensions. I think this is probably because there is some server test in there that also compiles all extensions, but I'm not sure.\r\n\r\nOptimally, we should be able to run these tests without having to compile any extra extensions beyond potentially core ones like router, HCM, etc. This is useful on platforms that won't support all extensions.\r\n\r\ncc @junr03 ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6736/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "yanavlasov",
        "created_at": "2020-06-24T02:48:22Z",
        "body": "The only test that needs to compile all extensions is `//test/integration:run_envoy_test`. This test seems to test that envoy process fails with bogus command line parameters. This should work with envoy without any extensions at all, I think. Unless I'm missing something the solution is to create another target that builds the server without any extensions.\r\n\r\n```\r\n$ bazel query \"rdeps(//test/integration/..., //source/exe:envoy-static)\"\r\n//test/integration:run_envoy_test\r\n//source/exe:envoy-static\r\nLoading: 0 packages loaded\r\n$\r\n```"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-06-24T15:06:50Z",
        "body": "@yanavlasov I think there are a couple of others like the hot restart test that use the full binary. But yeah, the solution is to build the binary twice, one the real version and one the light version. Note that @jmarantz already has a custom binary going for the hot restart test."
      }
    ]
  },
  {
    "number": 6716,
    "title": "http: allow server header to remain unmodified",
    "created_at": "2019-04-26T03:51:24Z",
    "closed_at": "2019-08-28T17:40:19Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6716",
    "body": "Right now there is no option to allow the server header to be passed through unmodified. This may be useful in various cases, but especially when running Envoy as a client library.\r\n\r\ncc @goaway @junr03 ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6716/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "bobvanderlinden",
        "created_at": "2019-08-12T09:34:30Z",
        "body": "Is the intention here that casing of headers is also being preserved?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-08-22T18:22:09Z",
        "body": "@alyssawilk I think I saw a comment go by that said you are working on this so assigning over to you."
      }
    ]
  },
  {
    "number": 6640,
    "title": "Add `injectDataToFilterChain(data, end_stream)` methods to NetworkFilter callbacks",
    "created_at": "2019-04-18T17:12:48Z",
    "closed_at": "2019-05-09T20:54:55Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6640",
    "body": "*Title*: Similarly to HTTP use case, add `injectDataToFilterChain(data, end_stream)` methods to NetworkFilter callbacks\r\n\r\n*Description*:\r\n\r\nRecently, new methods have been added to `StreamFilterCallbacks` in order to support low bandwidth simulation (see #6242):\r\n\r\n* `StreamDecoderFilterCallbacks::injectDecodedDataToFilterChain(data, end_stream)`\r\n* `StreamEncoderFilterCallbacks::injectEncodedDataToFilterChain(data, end_stream)`\r\n\r\nIt would be useful to have methods with the same semantics in the context of NetworkFilters as well.\r\n\r\nExample use cases include:\r\n\r\n* low bandwidth simulation at L4 (see #5942)\r\n* piping request/response data through an out-of-process filter (see #5209)\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6640/comments",
    "author": "yskopets",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-04-18T17:36:45Z",
        "body": "@yskopets thanks for opening, I just checked and yes, you are correct, we will need these new APIs to do similar things as what we are doing for HTTP. Conveniently, the implementation should be much easier. Will mark help wanted."
      },
      {
        "user": "yskopets",
        "created_at": "2019-04-18T18:07:32Z",
        "body": "@mattklein123 Great! I will try to implement it"
      }
    ]
  },
  {
    "number": 6557,
    "title": "Envoy crashes with \"assert failure: !response_headers_.\"",
    "created_at": "2019-04-11T09:50:35Z",
    "closed_at": "2019-07-23T02:58:42Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6557",
    "body": "*Title*: Envoy crashes ocasionally while making a http health check\r\n\r\n*Description*:\r\nEnvoy crashes once in a few hours. Server is configured dynamically via xDS with ~300 virtual hosts, some of them have http health checks. There was no traffic to Envoy; only http checks were issued periodically.\r\n\r\nGit version: cfc514546bc0284536893cca5fa43d7128edcd35\r\n\r\n*Repro steps*:\r\nI do not have a way of reproducing crashes. I hope stack trace will be sufficient to pinpoint this problem.\r\n\r\nHttp health check example:\r\n```\r\nhealth_checks {\r\n  timeout {\r\n    seconds: 1\r\n  }\r\n  interval {\r\n    seconds: 10\r\n  }\r\n  unhealthy_threshold {\r\n    value: 3\r\n  }\r\n  healthy_threshold {\r\n    value: 1\r\n  }\r\n  http_health_check {\r\n    path: \"/\"\r\n    expected_statuses {\r\n      start: 200\r\n      end: 300\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\n*Call Stack*:\r\n```\r\n[2019-04-10 20:24:33.755][10928][critical][assert] [source/common/upstream/health_checker_impl.cc:166] assert failure: !response_headers_.\r\n[2019-04-10 20:24:33.758][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:81] Caught Aborted, suspect faulting address 0x3e800002ab0\r\n[2019-04-10 20:24:33.758][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:69] Backtrace (use tools/stack_decode.py to get line numbers):\r\n[2019-04-10 20:24:33.797][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #0: Envoy::SignalAction::sigHandler() [0x5614cc9eec60]\r\n[2019-04-10 20:24:33.797][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #1: __restore_rt [0x7f201082e730]\r\n[2019-04-10 20:24:33.806][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #2: Envoy::Http::StreamDecoderWrapper::decodeHeaders() [0x5614cc3619fd]\r\n[2019-04-10 20:24:33.816][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #3: Envoy::Http::Http1::ClientConnectionImpl::onHeadersComplete() [0x5614cc4ace53]\r\n[2019-04-10 20:24:33.825][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #4: Envoy::Http::Http1::ConnectionImpl::onHeadersCompleteBase() [0x5614cc4aab5a]\r\n[2019-04-10 20:24:33.835][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #5: Envoy::Http::Http1::ConnectionImpl::{lambda()#5}::operator()() [0x5614cc4a9610]\r\n[2019-04-10 20:24:33.844][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #6: Envoy::Http::Http1::ConnectionImpl::{lambda()#5}::_FUN() [0x5614cc4a962f]\r\n[2019-04-10 20:24:33.853][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #7: http_parser_execute [0x5614cc7608ca]\r\n[2019-04-10 20:24:33.862][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #8: Envoy::Http::Http1::ConnectionImpl::dispatchSlice() [0x5614cc4aa5df]\r\n[2019-04-10 20:24:33.871][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #9: Envoy::Http::Http1::ConnectionImpl::dispatch() [0x5614cc4aa47f]\r\n[2019-04-10 20:24:33.880][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #10: Envoy::Http::CodecClient::onData() [0x5614cc3f1ca6]\r\n[2019-04-10 20:24:33.889][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #11: Envoy::Http::CodecClient::CodecReadFilter::onData() [0x5614cc3f272c]\r\n[2019-04-10 20:24:33.898][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #12: Envoy::Network::FilterManagerImpl::onContinueReading() [0x5614cc1440ed]\r\n[2019-04-10 20:24:33.908][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #13: Envoy::Network::FilterManagerImpl::onRead() [0x5614cc144204]\r\n[2019-04-10 20:24:33.917][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #14: Envoy::Network::ConnectionImpl::onRead() [0x5614cc13b16e]\r\n[2019-04-10 20:24:33.925][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #15: Envoy::Network::ConnectionImpl::onReadReady() [0x5614cc13c935]\r\n[2019-04-10 20:24:33.934][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #16: Envoy::Network::ConnectionImpl::onFileEvent() [0x5614cc13c6d3]\r\n[2019-04-10 20:24:33.943][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #17: Envoy::Network::ConnectionImpl::ConnectionImpl()::{lambda()#3}::operator()() [0x5614cc138eb9]\r\n[2019-04-10 20:24:33.953][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #18: std::_Function_handler<>::_M_invoke() [0x5614cc13e7cf]\r\n[2019-04-10 20:24:33.962][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #19: std::function<>::operator()() [0x5614cc132f38]\r\n[2019-04-10 20:24:33.971][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #20: Envoy::Event::FileEventImpl::assignEvents()::{lambda()#1}::operator()() [0x5614cc132b94]\r\n[2019-04-10 20:24:33.980][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #21: Envoy::Event::FileEventImpl::assignEvents()::{lambda()#1}::_FUN() [0x5614cc132c12]\r\n[2019-04-10 20:24:33.989][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #22: event_process_active_single_queue.isra.33 [0x5614cc73d963]\r\n[2019-04-10 20:24:33.998][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #23: event_base_loop [0x5614cc73eb8f]\r\n[2019-04-10 20:24:34.008][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #24: Envoy::Event::LibeventScheduler::run() [0x5614cc150c79]\r\n[2019-04-10 20:24:34.017][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #25: Envoy::Event::DispatcherImpl::run() [0x5614cc12e19e]\r\n[2019-04-10 20:24:34.026][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #26: Envoy::Server::InstanceImpl::run() [0x5614cc07e199]\r\n[2019-04-10 20:24:34.035][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #27: Envoy::MainCommonBase::run() [0x5614cb9a05ca]\r\n[2019-04-10 20:24:34.043][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #28: Envoy::MainCommon::run() [0x5614cb9873b6]\r\n[2019-04-10 20:24:34.052][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #29: main [0x5614cb985cc4]\r\n[2019-04-10 20:24:34.052][10928][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #30: [0x7f20104fc09b]\r\n```\r\n\r\nI have also other stack trace from older version of Envoy, but with debug logs enabled:\r\n```\r\n[2019-04-10 11:00:43.978][28897][debug][connection] [source/common/network/connection_impl.cc:644] [C2913] connecting to 10.32.204.142:8080\r\n[2019-04-10 11:00:43.978][28897][debug][connection] [source/common/network/connection_impl.cc:653] [C2913] connection in progress\r\n[2019-04-10 11:00:43.978][28897][debug][connection] [source/common/network/connection_impl.cc:517] [C2913] connected\r\n[2019-04-10 11:00:43.978][28897][debug][client] [source/common/http/codec_client.cc:64] [C2913] connected\r\n[2019-04-10 11:00:43.990][28897][critical][assert] [source/common/upstream/health_checker_impl.cc:166] assert failure: !response_headers_.\r\n[2019-04-10 11:00:43.994][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:81] Caught Aborted, suspect faulting address 0x3e8000070e1\r\n[2019-04-10 11:00:43.994][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:69] Backtrace (use tools/stack_decode.py to get line numbers):\r\n[2019-04-10 11:00:44.025][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #0: Envoy::SignalAction::sigHandler() [0x561faba52e06]\r\n[2019-04-10 11:00:44.025][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #1: __restore_rt [0x7faee2343730]\r\n[2019-04-10 11:00:44.034][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #2: Envoy::Http::StreamDecoderWrapper::decodeHeaders() [0x561fab3c365d]\r\n[2019-04-10 11:00:44.043][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #3: Envoy::Http::Http1::ClientConnectionImpl::onHeadersComplete() [0x561fab511967]\r\n[2019-04-10 11:00:44.052][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #4: Envoy::Http::Http1::ConnectionImpl::onHeadersCompleteBase() [0x561fab50f66e]\r\n[2019-04-10 11:00:44.061][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #5: Envoy::Http::Http1::ConnectionImpl::{lambda()#5}::operator()() [0x561fab50e202]\r\n[2019-04-10 11:00:44.070][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #6: Envoy::Http::Http1::ConnectionImpl::{lambda()#5}::_FUN() [0x561fab50e221]\r\n[2019-04-10 11:00:44.079][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #7: http_parser_execute [0x561fab7c54da]\r\n[2019-04-10 11:00:44.088][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #8: Envoy::Http::Http1::ConnectionImpl::dispatchSlice() [0x561fab50f1d1]\r\n[2019-04-10 11:00:44.096][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #9: Envoy::Http::Http1::ConnectionImpl::dispatch() [0x561fab50f071]\r\n[2019-04-10 11:00:44.106][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #10: Envoy::Http::CodecClient::onData() [0x561fab45a1b6]\r\n[2019-04-10 11:00:44.114][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #11: Envoy::Http::CodecClient::CodecReadFilter::onData() [0x561fab45ac3c]\r\n[2019-04-10 11:00:44.123][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #12: Envoy::Network::FilterManagerImpl::onContinueReading() [0x561fab1a64bd]\r\n[2019-04-10 11:00:44.132][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #13: Envoy::Network::FilterManagerImpl::onRead() [0x561fab1a65d4]\r\n[2019-04-10 11:00:44.141][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #14: Envoy::Network::ConnectionImpl::onRead() [0x561fab19d53e]\r\n[2019-04-10 11:00:44.150][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #15: Envoy::Network::ConnectionImpl::onReadReady() [0x561fab19ed05]\r\n[2019-04-10 11:00:44.159][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #16: Envoy::Network::ConnectionImpl::onFileEvent() [0x561fab19eaa3]\r\n[2019-04-10 11:00:44.168][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #17: Envoy::Network::ConnectionImpl::ConnectionImpl()::{lambda()#3}::operator()() [0x561fab19b289]\r\n[2019-04-10 11:00:44.177][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #18: std::_Function_handler<>::_M_invoke() [0x561fab1a0b9f]\r\n[2019-04-10 11:00:44.186][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #19: std::function<>::operator()() [0x561fab19532a]\r\n[2019-04-10 11:00:44.195][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #20: Envoy::Event::FileEventImpl::assignEvents()::{lambda()#1}::operator()() [0x561fab194f86]\r\n[2019-04-10 11:00:44.204][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #21: Envoy::Event::FileEventImpl::assignEvents()::{lambda()#1}::_FUN() [0x561fab195004]\r\n[2019-04-10 11:00:44.213][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #22: event_process_active_single_queue.isra.33 [0x561fab7a2573]\r\n[2019-04-10 11:00:44.222][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #23: event_base_loop [0x561fab7a379f]\r\n[2019-04-10 11:00:44.231][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #24: Envoy::Event::LibeventScheduler::run() [0x561fab1b317d]\r\n[2019-04-10 11:00:44.240][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #25: Envoy::Event::DispatcherImpl::run() [0x561fab190590]\r\n[2019-04-10 11:00:44.249][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #26: Envoy::Server::InstanceImpl::run() [0x561fab0df567]\r\n[2019-04-10 11:00:44.258][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #27: Envoy::MainCommonBase::run() [0x561faaa2418a]\r\n[2019-04-10 11:00:44.267][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #28: Envoy::MainCommon::run() [0x561faaa0af76]\r\n[2019-04-10 11:00:44.275][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #29: main [0x561faaa09884]\r\n[2019-04-10 11:00:44.275][28897][critical][backtrace] [bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #30: [0x7faee201109b]\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6557/comments",
    "author": "bartebor",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-04-11T14:32:13Z",
        "body": "From a brief look at the code it seems to be happening because the health checker keep track of the response headers for the duration of the entire response (including headers), but it's seeing another set of headers before the first response completes.\r\n\r\nOne thought based on your settings: maybe the health checks are timing out after the headers have been received but before the response completes? Skimming through the code I don't see us cleaning up the headers in the timeout case"
      },
      {
        "user": "derekargueta",
        "created_at": "2019-05-07T23:23:04Z",
        "body": "this seems closely related to (or possibly a dupe of) #3877 "
      },
      {
        "user": "bartebor",
        "created_at": "2019-06-11T06:47:10Z",
        "body": "@derekargueta I think you are right. I have never encountered a crash using official image. Custom release builds work fine too. Only my development builds ocasionally fail. We can probably close this issue, although I do not feel comfortable with fastbuild creating differently behaving/broken executables."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-06-11T14:53:08Z",
        "body": "I know what the issue is. I will fix this. (It is actually harmless but it should be fixed so the assert doesn't fire.)"
      }
    ]
  },
  {
    "number": 6513,
    "title": "possible init regression during shutdown",
    "created_at": "2019-04-08T18:41:35Z",
    "closed_at": "2019-04-15T14:24:39Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6513",
    "body": "Seeing this crash with newly deployed init manager code:\r\n\r\n```\r\nBacktrace:\r\n#0  _M_bucket_begin (this=0x18, __bkt=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/hashtable.h:926\r\n#0  _M_bucket_begin (this=0x18, __bkt=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/hashtable.h:926\r\n#1  std::_Hashtable<std::string, std::pair<std::string const, std::unique_ptr<Envoy::Upstream::ClusterManagerImpl::ThreadLocalClusterManagerImpl::ClusterEntry, std::default_delete<Envoy::Upstream::ClusterManagerImpl::ThreadLocalClusterManagerImpl::ClusterEntry> > >, std::allocator<std::pair<std::string const, std::unique_ptr<Envoy::Upstream::ClusterManagerImpl::ThreadLocalClusterManagerImpl::ClusterEntry, std::default_delete<Envoy::Upstream::ClusterManagerImpl::ThreadLocalClusterManagerImpl::ClusterEntry> > > >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::count (this=0x18, __k=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/hashtable.h:1457\r\n#2  0x00000000006a08b4 in count (this=0x18, __x=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/unordered_map.h:938\r\n#3  operator() (this=0x606a720) at external/envoy/source/common/upstream/cluster_manager_impl.cc:514\r\n#4  std::_Function_handler<void (), Envoy::Upstream::ClusterManagerImpl::createOrUpdateThreadLocalCluster(Envoy::Upstream::ClusterManagerImpl::ClusterData&)::$_10>::_M_invoke(std::_Any_data const&) (__functor=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/std_function.h:316\r\n#5  0x0000000000600ae7 in operator() (this=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/std_function.h:706\r\n#6  Envoy::ThreadLocal::InstanceImpl::runOnAllThreads(std::function<void ()>) (this=<optimized out>, cb=...) at external/envoy/source/common/thread_local/thread_local_impl.cc:91\r\n#7  0x0000000000601a62 in Envoy::ThreadLocal::InstanceImpl::SlotImpl::runOnAllThreads(std::function<void ()>) (this=<optimized out>, cb=...) at bazel-out/k8-opt/bin/external/envoy/source/common/thread_local/_virtual_includes/thread_local_lib/common/thread_local/thread_local_impl.h:37\r\n#8  0x00000000006946ff in Envoy::Upstream::ClusterManagerImpl::createOrUpdateThreadLocalCluster (this=<optimized out>, cluster=...) at external/envoy/source/common/upstream/cluster_manager_impl.cc:509\r\n#9  0x00000000006a01be in operator() (this=0x603a9c0) at external/envoy/source/common/upstream/cluster_manager_impl.cc:497\r\n#10 std::_Function_handler<void (), Envoy::Upstream::ClusterManagerImpl::addOrUpdateCluster(envoy::api::v2::Cluster const&, std::string const&, std::function<void (std::string const&, Envoy::Upstream::ClusterManager::ClusterWarmingState)>)::$_9>::_M_invoke(std::_Any_data const&) (__functor=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/std_function.h:316\r\n#11 0x00000000006ea62a in operator() (this=0x603a9c0) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/std_function.h:706\r\n#12 Envoy::Upstream::ClusterImplBase::finishInitialization (this=0x5fb76c0) at external/envoy/source/common/upstream/upstream_impl.cc:758\r\n#13 0x00000000006ea562 in Envoy::Upstream::ClusterImplBase::onInitDone (this=0x5fb76c0) at external/envoy/source/common/upstream/upstream_impl.cc:740\r\n#14 0x00000000007349a3 in operator() (this=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/std_function.h:706\r\n#15 Envoy::Init::WatcherHandleImpl::ready (this=<optimized out>) at external/envoy/source/common/init/watcher_impl.cc:15\r\n#16 0x0000000000734200 in ready (this=<optimized out>) at external/envoy/source/common/init/manager_impl.cc:75\r\n#17 Envoy::Init::ManagerImpl::initialize (this=0x5fb76c8, watcher=...) at external/envoy/source/common/init/manager_impl.cc:46\r\n#18 0x00000000006ea3a2 in Envoy::Upstream::ClusterImplBase::onPreInitComplete (this=<optimized out>) at external/envoy/source/common/upstream/upstream_impl.cc:722\r\n#19 0x0000000000638dc6 in Envoy::Config::GrpcMuxSubscriptionImpl::onConfigUpdateFailed (this=0x5fb7e68, e=0x0) at bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_mux_subscription_lib/common/config/grpc_mux_subscription_impl.h:80\r\n#20 0x0000000000707270 in non-virtual thunk to Envoy::Config::GrpcMuxImpl::handleEstablishmentFailure() () at external/envoy/source/common/config/grpc_mux_impl.cc:204\r\n#21 0x0000000000707413 in Envoy::Config::GrpcStream<envoy::api::v2::DiscoveryRequest, envoy::api::v2::DiscoveryResponse, std::string>::onRemoteClose (this=0x5fb7d50, status=Envoy::Grpc::Status::Unavailable, message=...) at bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:104\r\n#22 0x0000000000735f97 in Envoy::Grpc::AsyncStreamImpl::onTrailers(std::unique_ptr<Envoy::Http::HeaderMap, std::default_delete<Envoy::Http::HeaderMap> >&&) (this=0x5fa7680, trailers=<unknown type in /usr/sbin/envoy, CU 0x7e81c63, DIE 0x7f3faaf>) at external/envoy/source/common/grpc/async_client_impl.cc:160\r\n#23 0x0000000000735c85 in Envoy::Grpc::AsyncStreamImpl::onHeaders(std::unique_ptr<Envoy::Http::HeaderMap, std::default_delete<Envoy::Http::HeaderMap> >&&, bool) (this=0x5fa7680, headers=<unknown type in /usr/sbin/envoy, CU 0x7e81c63, DIE 0x7f3efeb>, end_stream=true) at external/envoy/source/common/grpc/async_client_impl.cc:106\r\n#24 0x000000000073aa85 in Envoy::Http::AsyncStreamImpl::encodeHeaders(std::unique_ptr<Envoy::Http::HeaderMap, std::default_delete<Envoy::Http::HeaderMap> >&&, bool) (this=0x5fd6700, headers=<unknown type in /usr/sbin/envoy, CU 0x7f615e3, DIE 0x8050ab6>, end_stream=true) at external/envoy/source/common/http/async_client_impl.cc:96\r\n#25 0x00000000009472f6 in operator() (this=0x7ffcf927b538, __args=true, __args=true) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/std_function.h:706\r\n#26 Envoy::Http::Utility::sendLocalReply(bool, std::function<void (std::unique_ptr<Envoy::Http::HeaderMap, std::default_delete<Envoy::Http::HeaderMap> >&&, bool)>, std::function<void (Envoy::Buffer::Instance&, bool)>, bool const&, Envoy::Http::Code, absl::string_view, absl::optional<Envoy::Grpc::Status::GrpcStatus>, bool) (is_grpc=<optimized out>, encode_headers=..., encode_data=..., is_reset=<optimized out>, response_code=<optimized out>, body_text=..., grpc_status=..., is_head_request=<optimized out>) at external/envoy/source/common/http/utility.cc:327\r\n#27 0x000000000073baed in Envoy::Http::AsyncStreamImpl::sendLocalReply(Envoy::Http::Code, absl::string_view, std::function<void (Envoy::Http::HeaderMap&)>, absl::optional<Envoy::Grpc::Status::GrpcStatus>) (this=<optimized out>, code=Envoy::Http::ServiceUnavailable, body=..., modify_headers=..., grpc_status=...) at bazel-out/k8-opt/bin/external/envoy/source/common/http/_virtual_includes/async_client_lib/common/http/async_client_impl.h:324\r\n#28 0x00000000007420ce in Envoy::Router::Filter::onUpstreamAbort (this=0x5fd6740, code=<optimized out>, response_flags=<optimized out>, body=..., dropped=<optimized out>) at external/envoy/source/common/router/router.cc:597\r\n#29 0x0000000000742676 in Envoy::Router::Filter::onUpstreamReset (this=0x5fd6740, reset_reason=Envoy::Http::ConnectionTermination, transport_failure_reason=...) at external/envoy/source/common/router/router.cc:659\r\n#30 0x00000000007b4ec5 in runResetCallbacks (reason=Envoy::Http::ConnectionTermination, this=<optimized out>) at bazel-out/k8-opt/bin/external/envoy/source/common/http/_virtual_includes/codec_helper_lib/common/http/codec_helper.h:49\r\n#31 Envoy::Http::Http2::ConnectionImpl::StreamImpl::resetStream (this=0x5fd2600, reason=Envoy::Http::ConnectionTermination) at external/envoy/source/common/http/http2/codec_impl.cc:302\r\n#32 0x000000000072ad24 in Envoy::Http::CodecClient::onEvent (this=0x4001900, event=<optimized out>) at external/envoy/source/common/http/codec_client.cc:87\r\n#33 0x000000000065dafd in Envoy::Network::ConnectionImpl::raiseEvent (this=0x50f8000, event=Envoy::Network::LocalClose) at external/envoy/source/common/network/connection_impl.cc:329\r\n#34 0x000000000065d4a4 in Envoy::Network::ConnectionImpl::closeSocket (this=0x50f8000, close_type=Envoy::Network::LocalClose) at external/envoy/source/common/network/connection_impl.cc:194\r\n#35 0x000000000065d1c7 in Envoy::Network::ConnectionImpl::close (this=0x50f8000, type=Envoy::Network::NoFlush) at external/envoy/source/common/network/connection_impl.cc:110\r\n#36 0x00000000006d56c4 in Envoy::Http::Http2::ConnPoolImpl::~ConnPoolImpl (this=0x3e01780) at external/envoy/source/common/http/http2/conn_pool.cc:26\r\n#37 0x00000000006d590e in Envoy::Http::Http2::ProdConnPoolImpl::~ProdConnPoolImpl (this=0x3e01780) at bazel-out/k8-opt/bin/external/envoy/source/common/http/http2/_virtual_includes/conn_pool_lib/common/http/http2/conn_pool.h:103\r\n#38 0x000000000069e62d in operator() (__ptr=0x3e9e4758d97bcb08, this=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/unique_ptr.h:78\r\n#39 ~unique_ptr (this=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/unique_ptr.h:268\r\n#40 ~pair (this=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/stl_pair.h:193\r\n#41 destroy<std::pair<std::vector<unsigned char, std::allocator<unsigned char> >, std::unique_ptr<Envoy::Http::ConnectionPool::Instance, std::default_delete<Envoy::Http::ConnectionPool::Instance> > > > (this=<optimized out>, __p=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/ext/new_allocator.h:140\r\n#42 destroy_impl<std::allocator<std::pair<std::vector<unsigned char, std::allocator<unsigned char> > const, std::unique_ptr<Envoy::Http::ConnectionPool::Instance, std::default_delete<Envoy::Http::ConnectionPool::Instance> > > >, std::pair<std::vector<unsigned char, std::allocator<unsigned char> >, std::unique_ptr<Envoy::Http::ConnectionPool::Instance, std::default_delete<Envoy::Http::ConnectionPool::Instance> > > > (a=..., p=<optimized out>) at external/com_google_absl/absl/memory/memory.h:590\r\n#43 destroy<std::pair<std::vector<unsigned char, std::allocator<unsigned char> >, std::unique_ptr<Envoy::Http::ConnectionPool::Instance, std::default_delete<Envoy::Http::ConnectionPool::Instance> > > > (a=..., p=<optimized out>) at external/com_google_absl/absl/memory/memory.h:548\r\n#44 destroy<std::allocator<std::pair<std::vector<unsigned char, std::allocator<unsigned char> > const, std::unique_ptr<Envoy::Http::ConnectionPool::Instance, std::default_delete<Envoy::Http::ConnectionPool::Instance> > > > > (alloc=<optimized out>, slot=<optimized out>) at external/com_google_absl/absl/container/internal/container_memory.h:347\r\n#45 destroy<std::allocator<std::pair<std::vector<unsigned char, std::allocator<unsigned char> > const, std::unique_ptr<Envoy::Http::ConnectionPool::Instance, std::default_delete<Envoy::Http::ConnectionPool::Instance> > > > > (alloc=<optimized out>, slot=<optimized out>) at external/com_google_absl/absl/container/flat_hash_map.h:501\r\n#46 destroy<std::allocator<std::pair<std::vector<unsigned char, std::allocator<unsigned char> > const, std::unique_ptr<Envoy::Http::ConnectionPool::Instance, std::default_delete<Envoy::Http::ConnectionPool::Instance> > > > > (alloc=<optimized out>, slot=<optimized out>) at external/com_google_absl/absl/container/internal/hash_policy_traits.h:83\r\n#47 absl::container_internal::raw_hash_set<absl::container_internal::FlatHashMapPolicy<std::vector<unsigned char, std::allocator<unsigned char> >, std::unique_ptr<Envoy::Http::ConnectionPool::Instance, std::default_delete<Envoy::Http::ConnectionPool::Instance> > >, absl::hash_internal::Hash<std::vector<unsigned char, std::allocator<unsigned char> > >, std::equal_to<std::vector<unsigned char, std::allocator<unsigned char> > >, std::allocator<std::pair<std::vector<unsigned char, std::allocator<unsigned char> > const, std::unique_ptr<Envoy::Http::ConnectionPool::Instance, std::default_delete<Envoy::Http::ConnectionPool::Instance> > > > >::clear (this=0x402b500) at external/com_google_absl/absl/container/internal/raw_hash_set.h:1040\r\n#48 0x000000000069e2d8 in clearActivePools (this=0x402b500) at bazel-out/k8-opt/bin/external/envoy/source/common/upstream/_virtual_includes/conn_pool_map_impl_lib/common/upstream/conn_pool_map_impl.h:119\r\n#49 Envoy::Upstream::ConnPoolMap<std::vector<unsigned char, std::allocator<unsigned char> >, Envoy::Http::ConnectionPool::Instance>::~ConnPoolMap (this=0x402b500) at bazel-out/k8-opt/bin/external/envoy/source/common/upstream/_virtual_includes/conn_pool_map_impl_lib/common/upstream/conn_pool_map_impl.h:19\r\n#50 0x000000000069e09c in operator() (__ptr=0x402b500, this=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/unique_ptr.h:78\r\n#51 ~unique_ptr (this=0x330eff8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/unique_ptr.h:268\r\n#52 ~array (this=0x330eff0) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/array:94\r\n#53 ~PriorityConnPoolMap (this=0x330eff0) at bazel-out/k8-opt/bin/external/envoy/source/common/upstream/_virtual_includes/priority_conn_pool_map/common/upstream/priority_conn_pool_map.h:22\r\n#54 destroy<Envoy::Upstream::PriorityConnPoolMap<std::vector<unsigned char, std::allocator<unsigned char> >, Envoy::Http::ConnectionPool::Instance> > (this=0x330eff0, __p=0x330eff0) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/ext/new_allocator.h:140\r\n#55 destroy<Envoy::Upstream::PriorityConnPoolMap<std::vector<unsigned char, std::allocator<unsigned char> >, Envoy::Http::ConnectionPool::Instance> > (__a=..., __p=0x330eff0) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/alloc_traits.h:487\r\n#56 std::_Sp_counted_ptr_inplace<Envoy::Upstream::PriorityConnPoolMap<std::vector<unsigned char, std::allocator<unsigned char> >, Envoy::Http::ConnectionPool::Instance>, std::allocator<Envoy::Upstream::PriorityConnPoolMap<std::vector<unsigned char, std::allocator<unsigned char> >, Envoy::Http::ConnectionPool::Instance> >, (__gnu_cxx::_Lock_policy)2>::_M_dispose (this=0x330efe0) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr_base.h:535\r\n#57 0x000000000069df95 in _M_release (this=0x330efe0) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr_base.h:154\r\n#58 ~__shared_count (this=0x3dd7be0) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr_base.h:684\r\n#59 ~__shared_ptr (this=0x3dd7bd8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr_base.h:1123\r\n#60 ~ConnPoolsContainer (this=0x3dd7bd8) at bazel-out/k8-opt/bin/external/envoy/source/common/upstream/_virtual_includes/cluster_manager_lib/common/upstream/cluster_manager_impl.h:238\r\n#61 ~pair (this=0x3dd7bc8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/stl_pair.h:193\r\n#62 destroy<std::pair<std::shared_ptr<Envoy::Upstream::Host const> const, Envoy::Upstream::ClusterManagerImpl::ThreadLocalClusterManagerImpl::ConnPoolsContainer> > (this=<optimized out>, __p=0x3dd7bc8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/ext/new_allocator.h:140\r\n#63 destroy<std::pair<std::shared_ptr<Envoy::Upstream::Host const> const, Envoy::Upstream::ClusterManagerImpl::ThreadLocalClusterManagerImpl::ConnPoolsContainer> > (__a=..., __p=0x3dd7bc8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/alloc_traits.h:487\r\n#64 std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<std::shared_ptr<Envoy::Upstream::Host const> const, Envoy::Upstream::ClusterManagerImpl::ThreadLocalClusterManagerImpl::ConnPoolsContainer>, false> > >::_M_deallocate_node (this=<optimized out>, __n=0x3dd7bc0) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/hashtable_policy.h:2084\r\n#65 0x000000000069a263 in _M_deallocate_nodes (this=0x3334f60, __n=0x0) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/hashtable_policy.h:2097\r\n#66 clear (this=0x3334f60) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/hashtable.h:2032\r\n#67 clear (this=0x3334f60) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/unordered_map.h:842\r\n#68 Envoy::Upstream::ClusterManagerImpl::ThreadLocalClusterManagerImpl::~ThreadLocalClusterManagerImpl (this=0x3334f10) at external/envoy/source/common/upstream/cluster_manager_impl.cc:831\r\n#69 0x0000000000601777 in _M_release (this=0x3334f00) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr_base.h:154\r\n#70 ~__shared_count (this=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr_base.h:684\r\n#71 ~__shared_ptr (this=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr_base.h:1123\r\n#72 reset (this=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0/../../../../include/c++/7.4.0/bits/shared_ptr_base.h:1235\r\n#73 Envoy::ThreadLocal::InstanceImpl::shutdownThread (this=<optimized out>) at external/envoy/source/common/thread_local/thread_local_impl.cc:167\r\n#74 0x0000000000607f8e in Envoy::Server::InstanceImpl::terminate (this=0x2a46500) at external/envoy/source/server/server.cc:520\r\n#75 0x000000000060bfaf in Envoy::Server::InstanceImpl::run (this=0x2a46500) at external/envoy/source/server/server.cc:487\r\n#76 0x0000000000459d39 in Envoy::MainCommonBase::run (this=0x2a8e530) at external/envoy/source/exe/main_common.cc:117\r\n#77 0x0000000000458b84 in run (this=0x2a8e420) at bazel-out/k8-opt/bin/external/envoy/source/exe/_virtual_includes/envoy_main_common_lib/exe/main_common.h:88\r\n#78 main (argc=17, argv=<optimized out>) at external/envoy/source/exe/main.cc:39\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6513/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-04-08T18:42:22Z",
        "body": "cc @mergeconflict @htuch. I haven't looked at this in detail but I've seen this a couple of times since we deployed the new init manager code. It's possible it's related to other changes."
      },
      {
        "user": "mergeconflict",
        "created_at": "2019-04-08T18:44:32Z",
        "body": "I'll take a look..."
      },
      {
        "user": "mergeconflict",
        "created_at": "2019-04-09T15:38:54Z",
        "body": "Not 100% certain, but I don't think this is to do with init manager changes. The init-related bits in the stack look like:\r\n\r\n* Envoy::Upstream::ClusterImplBase::onPreInitComplete at /source/common/upstream/upstream_impl.cc:722:\r\n    ```\r\n    init_manager_.initialize(init_watcher_);\r\n    ```\r\n\r\n* Envoy::Init::ManagerImpl::initialize at /source/common/init/manager_impl.cc:46\r\n    ```\r\n    // If we have no targets, initialization trivially completes. This can happen, and is fine.\r\n    ENVOY_LOG(debug, \"{} contains no targets\", name_);\r\n    ready();\r\n    ```\r\n\r\n* (some watcher handle plumbing)\r\n\r\n* Envoy::Upstream::ClusterImplBase::onInitDone at /source/common/upstream/upstream_impl.cc:740\r\n    ```\r\n    if (pending_initialize_health_checks_ == 0) {\r\n      finishInitialization();\r\n    }\r\n    ```\r\n\r\n* Envoy::Upstream::ClusterImplBase::finishInitialization at /source/common/upstream/upstream_impl.cc:758\r\n    ```\r\n    if (snapped_callback != nullptr) {\r\n      snapped_callback();\r\n    }\r\n    ```\r\n\r\n... And at that point we're back out of the init manager, and we start getting into thread-local storage land. My guess based on the rest of the stack is that we're in some sort of funky state in which we're nominally shutting down (see frames 74 through 30-something) but also trying to do stuff with TLS (see frame 6) while handling some xDS."
      },
      {
        "user": "htuch",
        "created_at": "2019-04-09T23:45:12Z",
        "body": "I agree with @mergeconflict that this is an issue in TLS shutdown, where a stream is cancelled. We could destroy CM after its shutdown in InstanceImpl::terminate, I think that might then avoid any callbacks percolating, but it seems a pretty ad hoc way to fix the problem. Any other ideas? "
      },
      {
        "user": "mattklein123",
        "created_at": "2019-04-10T04:24:55Z",
        "body": "@htuch @mergeconflict will take a look in the next few days. "
      }
    ]
  },
  {
    "number": 6482,
    "title": "Feature request: strict header validation mode",
    "created_at": "2019-04-04T18:12:41Z",
    "closed_at": "2019-07-02T16:31:49Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6482",
    "body": "We've had several production issues caused by a client setting an invalid Envoy header, for example `\"200.0\"` as a timeout or an extra space in an `x-envoy-retry-on` header). Right now Envoy just silently ignores invalid header values. It would help avoid these situations if Envoy had a \"strict header validation\" option where it either fails every request which has an invalid Envoy header or logs an error.\r\n\r\nthoughts on whether this would be straightforward / reasonable to implement? It seems like it would be tricky to implement strict validation for every single header but even having it for just a few headers would be helpful. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6482/comments",
    "author": "julia-stripe",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-04-04T19:27:00Z",
        "body": "@julia-stripe agreed this sounds like a useful thing to add. I don't think it will be too terrible difficult to add, but it will require dealing with the case at all header sites. It might easiest to start with the headers parsed by the router and the headers parsed by the HTTP connection manager. I suppose we could also have config to enable this on a per-header basis."
      },
      {
        "user": "xyu-stripe",
        "created_at": "2019-06-18T18:04:40Z",
        "body": "@mattklein123 I'd like to work on this if it's not already being worked on.\r\n\r\nBased on your comment, would the following sound like a reasonable approach?\r\n\r\n1. Introduce a `strict_check_headers` config field to `envoy.router` configuration to enumerate headers that would be subject to strict validation. This is opt-in. Example config:\r\n  ```\r\n  http_filters:\r\n  - name: envoy.router\r\n    config:\r\n      strict_check_headers:\r\n      - x-envoy-upstream-rq-timeout-ms\r\n      - x-envoy-upstream-rq-per-try-timeout-ms\r\n      - x-envoy-retry-on\r\n```\r\n2. If a request contains a header listed in the config that has an invalid value, `Envoy::Router::Filter::decodeHeaders` rejects the request, returning HTTP status 400.\r\n3. When envoy rejects requests this way, set a new response flag `IH` to indicate the reason was because of an invalid header.\r\n4. The PR would be scoped to support the handful of headers we've observed in our own production issues (or those that are easily adjacent e.g. `x-envoy-max-retries`), and we'd validate the `strict_check_headers` field and emit a config validation error so folks won't erroneously expect some unsupported header to be strict-checked. The initial headers would be:\r\n```\r\n  x-envoy-upstream-rq-timeout-ms\r\n  x-envoy-upstream-rq-per-try-timeout-ms\r\n  x-envoy-max-retries\r\n  x-envoy-retry-on\r\n  x-envoy-retry-grpc-on\r\n```\r\n\r\nIf all of this sounds sane, I can put together a PR for review. Otherwise, would appreciate your feedback about where there may be gaps in my own understanding.\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-06-18T20:55:10Z",
        "body": "@xyu-stripe yup sounds great to me."
      }
    ]
  },
  {
    "number": 6478,
    "title": "upstream destroy stats missing from HTTP conn pools",
    "created_at": "2019-04-03T23:36:22Z",
    "closed_at": "2019-06-07T21:54:56Z",
    "labels": [
      "bug",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6478",
    "body": "Was just doing some debugging and realized that the following stats are not incremented in the HTTP connection pools, but only the TCP connection pool:\r\n\r\n```\r\nupstream_cx_destroy\r\nupstream_cx_destroy_local\r\nupstream_cx_destroy_remote\r\n```\r\n\r\nThis is confusing and we should fix. I don't recall the history here but I'm guessing these were implemented for the TCP connection pool and never ported over. I think we could easily have a shared utility function that helps us unify this logic across the pools.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6478/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "hasheddan",
        "created_at": "2019-04-08T21:14:09Z",
        "body": "I would be happy to take this one!"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-04-08T21:45:45Z",
        "body": "Please do @HashedDan! "
      }
    ]
  },
  {
    "number": 6386,
    "title": "Move DEPRECATED.md to sphinx docs",
    "created_at": "2019-03-26T19:10:39Z",
    "closed_at": "2019-04-09T15:39:04Z",
    "labels": [
      "area/docs",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6386",
    "body": "Allow ref linking as well as version snapping.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6386/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "hasheddan",
        "created_at": "2019-03-29T17:30:58Z",
        "body": "I would be happy to take this one!"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-29T20:04:33Z",
        "body": "Thanks you @HashedDan! Go for it!"
      }
    ]
  },
  {
    "number": 6350,
    "title": "upstream_rq_total is not consistent during H1 and H2 circuit breaking",
    "created_at": "2019-03-21T21:09:12Z",
    "closed_at": "2019-03-23T16:35:24Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6350",
    "body": "For the HTTP/1 connection pool, we increment upstream_rq_total even if the request ends up being circuit broken. For the HTTP/2 connection pool we do not. \r\n\r\nThis is confusing and we should fix this to be consistent, one way or the other.\r\n\r\ncc @danielhochman @junr03 ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6350/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "spenceral",
        "created_at": "2019-03-21T23:34:53Z",
        "body": "I can do this, I was just touching nearby code. I'll get @snowp to help me if I need backup."
      }
    ]
  },
  {
    "number": 5808,
    "title": "Add file `mode` to `Pipe` listeners",
    "created_at": "2019-02-01T15:32:45Z",
    "closed_at": "2019-12-17T01:07:16Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5808",
    "body": "*Title*: Add file `mode` to `Pipe` listeners\r\n\r\n*Description*:\r\nEnvoy can be configured with Unix domain socket listeners via the `Pipe` `Address` type. By default, these sockets are created such that only the file owner can read and write to them, and the typical umask of `0022` prevent access from group or world. We have a use case where we'd like to grant fine-grain access to individual `Pipe` listeners (either via group- or world-permissive configurations)\r\n\r\nIt'd be nice if `Pipe`s could be configured individually for what `mode` permissions they should be created with. This would allow users to have fine-grain permissions per-listener, rather than working around it with a broad umask change.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5808/comments",
    "author": "franklin-stripe",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-02-01T16:48:52Z",
        "body": "Sounds reasonable to me."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-03T17:23:59Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-03-10T17:29:40Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "madeddie",
        "created_at": "2019-03-29T17:34:18Z",
        "body": "This sounds like a great idea and we need it (running envoy as user envoy on a non-dockerized load)\r\n"
      },
      {
        "user": "mattrobenolt",
        "created_at": "2019-06-20T22:42:30Z",
        "body": "This would be _really_ useful to have. Or some implementation of being able to configure user/group and mode on the unix sockets. Similar to how haproxy would do it, etc.\r\n\r\nThere have also been multiple tickets for this so I didn't want to open another, but it's a shame that these are being auto closed for being stale. :( So I'm adding a louder +1 here mostly in hopes of this getting re-opened.\r\n\r\n@alyssawilk maybe? Since you're in the OWNERS.md for listeners? :)"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-06-20T23:40:19Z",
        "body": "Reopening and marking help wanted."
      },
      {
        "user": "mattrobenolt",
        "created_at": "2019-06-21T01:08:01Z",
        "body": "Thanks @mattklein123. <3"
      },
      {
        "user": "athampy",
        "created_at": "2019-09-28T06:33:08Z",
        "body": "I can pick this up"
      }
    ]
  },
  {
    "number": 5587,
    "title": "Use absl::Time for parsing, formatting and timezone conversions.",
    "created_at": "2019-01-13T05:32:03Z",
    "closed_at": "2019-02-28T20:09:54Z",
    "labels": [
      "tech debt",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5587",
    "body": "We should replace `strftime`, `strptime`, `mktime`, `gmtime`, `gmtime_r`, `localtime`, `localtime_r`, `std::get_time` and `std::put_time` with their `absl::Time` equivalents, and blacklist old functions.\r\n\r\nAlso, currently, we're using `gmtime_r` and `localtime` to convert time in relation to UTC and locally configured timezones. Instead, we should be using absolute times (i.e. `time_t`, `absl::Time`, `std::chrono::system_clock`) and explicitly pass timezones, where needed.\r\n\r\nRelated: #5582, #3064",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5587/comments",
    "author": "PiotrSikora",
    "comments": [
      {
        "user": "sprakriti",
        "created_at": "2019-01-14T03:28:18Z",
        "body": " I would like to take this up as my first issue."
      },
      {
        "user": "sprakriti",
        "created_at": "2019-01-26T05:26:39Z",
        "body": "Hi, I have gone through the `absl::Time` APIs but couldn't understand how to replace `gmtime_r` by using absolute times and explicitly pass timezones. I have tried using `absl::TimeZone` but it didn't work. Any suggestions?"
      },
      {
        "user": "PiotrSikora",
        "created_at": "2019-02-03T06:54:36Z",
        "body": "@sprakriti sorry for the delay. I think it would be helpful if you opened a PR with whatever you have right now and marked it as work-in-progress (`[WIP]`), even if the timezones don't work, because it's one of those cases when you need to try a few variants before you get desired results, and it would be easier for others to comment and/or suggest how to make things work if there is some code to look at."
      }
    ]
  },
  {
    "number": 5558,
    "title": "config: convert example, documents and tests to typed_config (Any)",
    "created_at": "2019-01-10T00:42:08Z",
    "closed_at": "2020-01-06T04:01:13Z",
    "labels": [
      "tech debt",
      "help wanted",
      "deprecation"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5558",
    "body": "Follow up of #4475.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5558/comments",
    "author": "lizan",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-01-10T00:44:23Z",
        "body": "Will working on this when I have time, feel free to take."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-12-13T18:21:05Z",
        "body": "@yanavlasov is this still on your radar? cc @derekargueta "
      },
      {
        "user": "yanavlasov",
        "created_at": "2019-12-15T02:45:16Z",
        "body": "Yes, working on it this week."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-12-15T16:51:23Z",
        "body": "Awesome thanks"
      },
      {
        "user": "mattklein123",
        "created_at": "2020-01-06T04:01:13Z",
        "body": "I think this is done."
      }
    ]
  },
  {
    "number": 5477,
    "title": "Add format fix for java proto options",
    "created_at": "2019-01-04T05:39:47Z",
    "closed_at": "2019-01-17T18:06:57Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5477",
    "body": "With #5369 the presence of some java options are required in all proto files, verified by `check_format.py`. Since these are fairly simple additions, we should add a format fix step to `check_format.py` that adds these in. \r\n\r\nThe following snippet was used to add the options to existing files and might serve as inspiration for adding them this fix to `check_format.py`.\r\n\r\n```\r\ncd api\r\nfor f in $(find . -name \"*.proto\")\r\ndo\r\n  sed -i -e '/^option\\sjava_multiple_files\\s=/d' $f\r\n  sed -i -e \"/^package\\s/a option java_multiple_files = true;\" $f\r\n  sed -i -e '/^option\\sjava_package\\s=/d' $f\r\n  cmd='grep -Po \"^package \\K.*(?=;$)\" '\"${f}\"\r\n  sed -i -e \"/^package\\s/a option java_package = \\\"io.envoyproxy.$(eval $cmd)\\\";\" $f\r\ndone\r\n``` \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5477/comments",
    "author": "snowp",
    "comments": [
      {
        "user": "jmarantz",
        "created_at": "2019-01-07T14:15:32Z",
        "body": "@snowp do you want to take this?"
      },
      {
        "user": "snowp",
        "created_at": "2019-01-07T14:20:27Z",
        "body": "Yeah i'll take this, should be easy enough"
      },
      {
        "user": "jmarantz",
        "created_at": "2019-01-07T14:22:28Z",
        "body": "Cool; I can review. Can you also add a test in check_format_test_helper.py? Thanks :)"
      }
    ]
  },
  {
    "number": 5412,
    "title": "create easier way to get a core dump locally and in CI",
    "created_at": "2018-12-24T17:03:28Z",
    "closed_at": "2019-11-08T22:24:47Z",
    "labels": [
      "area/build",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5412",
    "body": "I was trying to debug the hot restart test and get a core dump, and it took me an embarrassingly long time to figure out how to do it under bazel (the main issue was that my shell `ulimit` settings don't transfer to the test invocation I think because bazel is running the test and not my shell). It would be really nice if we could do two things:\r\n1) Have a better way to get a core dump from a test run locally. Perhaps this is a run_under thing (per @htuch) that correctly sets up ulimit and also make sets the core dump output location to something known and then sets it back, and I guess also deals with sandboxing correctly (i.e., gives you very clear instructions on how to look at the core dump with the right symbols/binary).\r\n2) Also enables this in CI somehow where we could get a core dump saved as an artifact with the binary that created it. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5412/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-11-08T22:13:00Z",
        "body": "@mattklein123 re: 1) after #8488 you can run test binary outside bazel without anything special (just invoke `bazel-bin/test/<binary>`). only general core dump configuration are needed."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-11-08T22:24:46Z",
        "body": "Cool SGTM will close. Thank you!"
      }
    ]
  },
  {
    "number": 5119,
    "title": "Add access log response flag for downstream request cancel",
    "created_at": "2018-11-26T18:43:02Z",
    "closed_at": "2018-12-04T05:00:53Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5119",
    "body": "*Title*: *Add access log response flag for downstream request cancel*\r\n\r\n*Description*:\r\nIf a downstream client cancels an HTTP request (for instance if the per try timeout is hit), server-side access logs show a \"0\" response code and no health flags. Example:\r\n\r\n```\r\n[2018-11-26T03:17:43.818Z] \"POST /foo HTTP/2\" 0 - 32 0 450 <snip>\r\n```\r\n\r\nThis is signifying that the request terminated before a response code was sent (because the client hung up), which seems like a nice case for a response flag.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5119/comments",
    "author": "mpuncel",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2018-11-26T20:17:26Z",
        "body": "@mpuncel seems reasonable, so you want to work on this?"
      }
    ]
  },
  {
    "number": 4903,
    "title": "Exclude connection setup in per try timeout",
    "created_at": "2018-10-29T22:32:54Z",
    "closed_at": "2018-11-12T06:45:52Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4903",
    "body": "We've been seeing per try timeouts trigger needlessly during boot due to per try timeout being less than the connection setup (including TLS). To get around this, we need to increase the per try timeout to the point where it becomes meaningless for some of our fast endpoints. \r\n\r\nIt would be nice to be able to specify the per try timeout for the request/response itself, not including the connection setup.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4903/comments",
    "author": "snowp",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2018-10-29T22:38:26Z",
        "body": "To exclude the connection setup I can imagine starting the timeout timer in `onPoolReady` instead of doing it when the entire downstream request has been written to `UpstreamRequest`"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-29T22:49:59Z",
        "body": "@snowp yes this seems reasonable."
      },
      {
        "user": "snowp",
        "created_at": "2018-10-29T23:03:12Z",
        "body": "Would the best approach here be to add another header to set the new timeout? \r\n\r\nI'll be working on this, this is pretty high priority for us. "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-29T23:07:50Z",
        "body": "No, I would probably just start the per-try timeout in onPoolReady instead of where it is being set now. I think that's probably fine. I don't think you need a new header. Note that this won't help with H2, since onPoolReady returns immediately IIRC."
      },
      {
        "user": "snowp",
        "created_at": "2018-10-29T23:12:12Z",
        "body": "How would one approach this for H2 then? We're primarily using H2 within the mesh"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-29T23:18:56Z",
        "body": "I can't think of anything other than changing the h2 connection pool to have logic as to whether there is a connected primary connection, and if not, having a pending request queue like we do for h1. Then you would also do the change of starting per-try-timeout in onPoolReady() while leaving the overall timeout to include everything including possible connection. IMO this makes the most sense, but is non-trivial."
      },
      {
        "user": "snowp",
        "created_at": "2018-10-29T23:24:02Z",
        "body": "I think changing how per try timeouts work + consistency between h/1.1 and h/2 would be good here, so I'll give this a go. I'll update how per try timeouts work first and then look into updating the h2 conn pool."
      },
      {
        "user": "snowp",
        "created_at": "2018-10-29T23:57:04Z",
        "body": "@mattklein123 Just to clarify: are you suggesting just modifying the existing behavior? Or introduce an option on the retry policy to specify this? I read it as just modifying the existing behavior, but that will involve straight up deleting existing tests that cover the case where the connect times out, so I wanted to check first. "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-30T00:00:20Z",
        "body": "I'm OK with just modifying the existing behavior (and release noting it) since I think what you are proposing makes more sense for the intention of the timeout, as long as the outer timeout continues to cover the entire thing. @envoyproxy/maintainers any opinions here?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-10-30T13:50:12Z",
        "body": "I think we can get away with it for now but in the long run we should probably have policy around non-breaking but behavior altering changes.  I don't want to spam envoy-announce to the point folks filter it out but we don't have a good way of engaging folks running envoy in production who might prefer the existing behavior and might want to weigh in asking for a config option or even an easy way of saying \"what has changed by default\" between hash X and hash Y since most of the relnotes are config-guarded additions rather than functional changes\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-30T15:43:37Z",
        "body": "@alyssawilk agreed. In this case, I think the new behavior is better than the old behavior in all cases, which is why I recommended that we just change it, but am happy to revisit if folks think that is not the right way to go."
      },
      {
        "user": "snowp",
        "created_at": "2018-11-12T06:45:52Z",
        "body": "Per try timeouts should now exclude connection setup for both h/1 and h/2. "
      }
    ]
  },
  {
    "number": 4899,
    "title": "AsyncClient returned by httpAsyncClientForCluster() does not follow lb_policy defined for cluster",
    "created_at": "2018-10-29T18:41:57Z",
    "closed_at": "2019-09-26T17:58:34Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4899",
    "body": "\r\n*Title*: *AsyncClient returned by httpAsyncClientForCluster() does not follow lb_policy defined for cluster*\r\n\r\n*Symptom*:\r\nLB Policy configured on a test cluster is RING_HASH, and I observed it was honoured by all calls going through HTTP Connection manager.\r\nBut any Async call sent using AsyncClient returned by httpAsyncClientForCluster() from a filter is **not following the same LB policy**. Is this the expected behavior?\r\n\r\n*Description*:\r\n\r\nHere is what is causing the problem:\r\n\r\nIn file `upstream/cluster_manager_impl.cc` the constructor of `struct ClusterEntry` is called for each cluster defined in the config when envoy starts up.\r\n\r\n In its initializer list it invokes the constructor of `Http::AsyncClientImpl` to create http_async_client_ for the cluster. http_async_client_ is returned by httpAsyncClientForCluster()  and is used for making Async calls. \r\n\r\n`Http::AsyncClientImpl` creates `AsyncStreamImpl` which creates a mock `Router::ProdFilter router_` and sets a bunch of mock objects on it. One of the mock objects is `struct RouteEntryImpl` which defines `const Router::HashPolicy* hashPolicy() const override { return nullptr; }`.\r\n\r\n*During an async call*, when `upstream/thread_aware_lb_impl.cc` chooseHost()  is called, it tries to `computeHashKey()` but in the absence a HashPolicy, it does this:\r\n\r\n`const uint64_t h = hash ? hash.value() : random_.random();`\r\n\r\nDo you think there a way to make async calls start following the lb_policy for the cluster? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4899/comments",
    "author": "shivanshu21",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-10-29T18:59:02Z",
        "body": "It does follow the LB policy, the issue is that AsyncClient/Stream doesn't correctly implement the hash policy interface. I think this should be pretty straightforward to fix by sharing the code w/ the normal router filter."
      },
      {
        "user": "shivanshu21",
        "created_at": "2018-10-29T19:00:04Z",
        "body": "I want to take this up."
      },
      {
        "user": "shivanshu21",
        "created_at": "2018-10-30T22:50:11Z",
        "body": "@mattklein123 \r\n\r\nI Looked into the code to find ways of implementing hash policy interface in AsyncClient/Stream. HashPolicy is available in route entry as seen before. But it appears like AsyncClient does not receive any information on route or route entry during construction. Route and RouteEntry are a part of StreamDecoderFilterCallbacks. AsyncClient is constructed inside ClusterManager where this information is not present. Please let me know if I am missing something here.\r\n\r\n**Here is one possibility to implement hashPolicy interface in AsyncClient/Stream** \r\n\r\nAsyncClient::send() receives an HTTP::MessagePtr which is a pointer to Protobuf::Message. In our case, it carries a header map.\r\nWe can add some headers to the HeaderMap in HTTP::MessagePtr to indicate which hash policy object should be constructed inside AsyncClient/Stream/RouteEntry (cookie/header/ip hash) and what is the value of said header or cookie name etc.\r\n\r\nIf these headers are not present, the code will continue the present behavior. Each Async Call will have to explicitly set these headers in order for the call to go through the proper hash policy.\r\n\r\nCan you recommend any other way for doing this, or is this fine?"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-30T23:29:12Z",
        "body": "> Can you recommend any other way for doing this, or is this fine?\r\n\r\nI don't think that is the way to go. Ultimately, the router filter will call back into the AsyncStream for all of the things that it needs when constructing its LBContext. You will need to plumb out the various data that it expects such that the computations look normal to the router filter. Unless I'm missing something I think this should be fairly easily doable."
      },
      {
        "user": "shivanshu21",
        "created_at": "2018-11-09T00:07:56Z",
        "body": "@mattklein123 Sorry about the delay in response.\r\n\r\nI am probably missing some link here. I am relatively new, hence the repetitive questions. \r\n\r\nThe way I see it, **there is no way to access the correct HashPolicy object from within Async client or even the aforementioned //mock router filter//** as it is not the main router filter that is used to call decodeHeaders()/decodeData() here, it is a separate filter created per async client.\r\n\r\n1. When an Async call is made using send() method, it creates an `AsyncRequestImpl` which inherits from `AsyncStreamImpl`.\r\n2. `Router::ProdFilter router_` Is a member of `AsyncStreamImpl`. The decodeHeaders() and decodeData() called to make the actual request are called on this object.\r\n3. When `AsyncStreamImpl` object is created, it calls `router_.setDecoderFilterCallbacks(*this);` This sets Route and RouteImpl inside router_ to dummy values.\r\n4. decodeHeaders/decodeData *will have no way to access HashPolicy* which is stored in the main filter. The filter they are called for is separate and has its own dummy RouteEntry.\r\n\r\nRegarding LBContext:\r\ndecodeHeaders() calls getConnPool().\r\nLBContext is sent when getConnPool() calls config_.cm_.httpConnPoolForCluster(route_entry_->clusterName(), route_entry_->priority(),protocol, this);\r\nLBContext is the \"this\" pointer to httpConnPoolForCluster() call. Again it will be the pointer to `Router::ProdFilter router_` which has dummy Route and RouteImpl. Please refer `upstream/cluster_manager_impl.cc ClusterManagerImpl::httpConnPoolForCluster`\r\n\r\n\r\nIt looks like some input param has to be added to send() to add HashPolicy right at the time the Async call is made. What am I missing?"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-11-09T02:09:55Z",
        "body": "I would need to look into things in more detail, but I think the key here is that we will need to create a dynamic route for each request that can provide the right hash values. This would be owned by the stream and then router can access it. So there will be some small refactoring needed."
      },
      {
        "user": "mandarjog",
        "created_at": "2019-05-28T04:02:56Z",
        "body": "@lizan This is the issue I was talking about. For consistent hashing to work minimally we need to specify `routeAction.HashPolicy.connection_properties.source_ip` in the current context.\r\nSince asynClient is detached from the normal routing, this could be a synthetic route."
      }
    ]
  },
  {
    "number": 4685,
    "title": "upstream: redesign per priority load calculation when all priority levels are in panic mode",
    "created_at": "2018-10-10T22:57:46Z",
    "closed_at": "2020-01-24T01:36:56Z",
    "labels": [
      "design proposal",
      "help wanted",
      "area/load balancing"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4685",
    "body": "Description:\r\nCurrently per priority traffic load distribution is calculated based on number of healthy hosts in each priority level. When there is zero healthy hosts in all priority levels, 100% of traffic goes to priority 0. \r\nThis behavior should be changed for situation when all priority levels are in panic mode. It means that there is very low number of healthy hosts in each priority, possibly none. For this scenario (all priority levels are in panic mode) load distribution algorithm should use total number of hosts in each priority, not number of healthy hosts. For example if there are 3 priorities with 5 hosts each and the number of healthy hosts is 0 (P0), 1 (P1) and 1 (P2), the load will be 34% (33% plus rounding to 100%), 33% and 33% respectively.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4685/comments",
    "author": "cpakulski",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2018-10-11T01:27:41Z",
        "body": "CC @mattklein123 @alyssawilk "
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-11-10T02:11:57Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "cpakulski",
        "created_at": "2018-11-10T20:10:32Z",
        "body": "Work in progress...."
      },
      {
        "user": "cpakulski",
        "created_at": "2018-12-20T19:21:10Z",
        "body": "DESIGN PROPOSAL:\r\nAs the number of healthy hosts in a cluster decreases, the behavior of the cluster changes. The change usually happens when the number of healthy hosts drops below a threshold. The following list describes the behavior:\r\n\r\n- **Cluster State**: Priority=0 has more than 72% (100 % divided by overprovisioning factor) hosts in healthy state:\r\n**Behavior**: 100% of the total traffic is handled by Priority=0. The rest of priority levels are not used.\r\n\r\n- **Cluster State**: Priority = 0 has less than 72% of hosts in healthy state, but there is enough hosts in other priority levels to handle the load (normalized total health across the cluster is 100%)\r\n**Behavior**: Load is distributed across priority levels. None of the levels enters panic mode.\r\n\r\n- **Cluster state**: there is not enough healthy hosts across all priority levels to handle the load (normalized total health across the cluster < 100%), but each level has enough healthy hosts to avoid entering panic mode.\r\n**Behavior**: Load is distributed across priority levels according to the number of healthy hosts in a priority in relation to the total number of healthy hosts in entire cluster.\r\n\r\n- **Cluster state**: there is not enough healthy hosts across all priority levels to handle the load and some levels entered panic mode, when the number of healthy hosts in those levels dropped below panic level (50%).\r\n**Behavior**: Load is distributed across priority levels according to the number of healthy hosts in each priority level in relation to the total number of healthy hosts in entire cluster. Priority levels in panic mode distribute the traffic to all hosts in that priority level. Note: if a priority level has zero healthy hosts, that priority level receives no traffic.\r\n\r\n- **Cluster state**: there are not enough healthy hosts across all priority levels to handle the load and ALL levels entered panic mode (when the number of healthy hosts in those levels dropped below panic level (50%)).\r\n**Behavior**: Load is distributed across priority levels according to the number of healthy hosts in each priority level in relation to the total number of healthy hosts in entire cluster. Each priority level distributes received traffic to all hosts in that priority (panic mode). Note: if a priority level has ZERO healthy hosts, that priority level receives no traffic.\r\n\r\n- **Cluster State**: There are no healthy hosts in the cluster.\r\n**Behavior**: 100% of the traffic is send to Priority=0, which is panic mode and distributes the traffic to all hosts in Priority=0.\r\n\r\nI would like to propose to change the behavior for the last two situations (when all priority levels are in the panic mode) to the following:\r\n- **Cluster State**: there is not enough healthy hosts across all priority levels to handle the load and ALL levels entered panic mode (when the number of healthy hosts in those levels dropped below panic level (50%)).\r\n**Behavior**: Load is distributed across priority levels according to the number of all hosts (healthy and not healthy) in a priority level in relation to the total number of hosts (healthy and not healthy) in the entire cluster. Each priority level distributes received traffic to all hosts in that priority level (panic mode).\r\n\r\nThe problem with current solution is that if the number of healthy hosts in a priority drops to zero that priority is excluded from load distribution. When the number of healthy hosts in a cluster is very low, most of the traffic will not be handled anyways, but there is a risk that remaining healthy hosts become overloaded. In essence I am suggesting that once all levels enter panic mode, load calculation algorithm does distribution based not on the number of healthy hosts, but based on total hosts in a priority level.\r\n\r\nAdding: @mattklein123 @alyssawilk @snowp @fredlas \r\n"
      },
      {
        "user": "fredlas",
        "created_at": "2018-12-20T20:05:15Z",
        "body": "Not that I know anything about this beyond having studied Envoy's documentation a bit, but you wrote @fredlas so I will chime in!\r\n\r\nThat makes perfect sense to me. In the existing behavior, the logic of the last case is sort of an arbitrary exception to what the logic of the second to last case would suggest. And\r\n\r\n> most of the traffic will not be handled anyways, but there is a risk that remaining healthy hosts become overloaded\r\n\r\nmakes perfect sense to me as motivation for this change."
      },
      {
        "user": "cpakulski",
        "created_at": "2018-12-20T20:15:04Z",
        "body": "@fredlas Thanks! You did great job clarifying load balancing logic in #4817 and this definitely is on the same topic."
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-01-02T21:57:01Z",
        "body": "I don't object to this change, but I counter-propose that if most hosts are unhealthy and all priorities are in panic mode, perhaps Envoy should simply fail to chose hosts for some percentage of load and return 50xs instead?  I agree overwhelming P=0 isn't optimal but at that point everything has melted down and overwhelming the lingering subset of P=1 to P=N may not be helping matters."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-01-03T20:57:41Z",
        "body": "I agree with the proposal.\r\n\r\n> I don't object to this change, but I counter-propose that if most hosts are unhealthy and all priorities are in panic mode, perhaps Envoy should simply fail to chose hosts for some percentage of load and return 50xs instead? I agree overwhelming P=0 isn't optimal but at that point everything has melted down and overwhelming the lingering subset of P=1 to P=N may not be helping matters.\r\n\r\nI like this idea, but I wonder if we should look at this as a totally different feature/issue/option which would basically be \"auto maintenance mode?\" This could then be configured in place or along size panic mode?\r\n"
      },
      {
        "user": "cpakulski",
        "created_at": "2019-01-03T21:38:47Z",
        "body": "My major concern was a situation when a priority level gets excluded from load calculation because it has zero healthy hosts while a priority next to it with let us say only 1% hosts in healthy state receives all the load.\r\n\r\nI believe that both mechanisms can co-exist. The rejection logic described by @alyssawilk would start working when all levels enter panic mode and the percentage of rejected requests would be somehow negatively related to the number of remaining healthy hosts. \r\n\r\nI am a bit lost when it comes to returning an error for rejected traffic. Load balancer is layer 3 concept and it provides connectivity to various services: HTTP based and non-http like redis. Wouldn't always returning 5xx error violate this?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-01-04T16:53:06Z",
        "body": "> I am a bit lost when it comes to returning an error for rejected traffic. Load balancer is layer 3 concept and it provides connectivity to various services: HTTP based and non-http like redis. Wouldn't always returning 5xx error violate this?\r\n\r\nDepending on the LB error/return code, we could do different things, like return a 5xx in the router. Either way, I would recommend opening a separate issue to track this."
      },
      {
        "user": "htuch",
        "created_at": "2019-01-06T23:44:40Z",
        "body": "I'd be in favor of a solution that doesn't have wild swings in load distributions across priority levels occurring as a single host goes healthy/unhealthy. It sounds like in the current situation that this can happen and @cpakulski proposed solution solves this. OTOH, anything that simplifies, e.g. 5xx load shedding, the existing (very) complicated set of behaviors might be useful. I like to think of this from a control system perspective; you don't want to have non-linear discontinuities in behavior."
      },
      {
        "user": "cpakulski",
        "created_at": "2019-01-07T18:49:11Z",
        "body": "Thanks for replies. If there are no objections, I will implement proposed solution when all levels are in panic mode and will create a new issue to design simpler mechanism to shred load in linear fashion."
      },
      {
        "user": "snowp",
        "created_at": "2019-12-12T22:09:21Z",
        "body": "@cpakulski Just checking in to see if you're still planning on working on this? Or should I unassign you?"
      },
      {
        "user": "cpakulski",
        "created_at": "2019-12-12T22:23:33Z",
        "body": "@snowp You must have used 6th sense, because I started to work on it today! I am at unit testing stage now and should create PR with a day or two."
      }
    ]
  },
  {
    "number": 4622,
    "title": "[v1.8.0 (Oct 4, 2018) deprecation] Remove features marked deprecated in #4487",
    "created_at": "2018-10-05T17:33:38Z",
    "closed_at": "2019-07-08T16:53:50Z",
    "labels": [
      "tech debt",
      "help wanted",
      "deprecation"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4622",
    "body": "#4487 (Highlight deprecation of *.deprecated_v1 fields in notice) introduced a deprecation notice for v1.8.0 (Oct 4, 2018). This issue tracks source code cleanup.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4622/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-11-04T18:13:15Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "venilnoronha",
        "created_at": "2018-11-04T18:26:07Z",
        "body": "Not stale."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-12-04T19:09:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-12-11T19:26:12Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "venilnoronha",
        "created_at": "2018-12-11T22:38:37Z",
        "body": "@mattklein123 this is already done in #4625 which was closed because it went stale."
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-07-08T16:53:50Z",
        "body": "Closing - per #6271 we can reopen once we cut the new API version."
      }
    ]
  },
  {
    "number": 4616,
    "title": "[v1.8.0 (Oct 4, 2018) deprecation] Remove features marked deprecated in #3838",
    "created_at": "2018-10-05T17:33:34Z",
    "closed_at": "2019-04-01T16:21:27Z",
    "labels": [
      "tech debt",
      "help wanted",
      "deprecation"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4616",
    "body": "#3838 (add response/request header options at route level) introduced a deprecation notice for v1.8.0 (Oct 4, 2018). This issue tracks source code cleanup.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4616/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-11-04T20:13:16Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "derekargueta",
        "created_at": "2018-11-06T22:16:37Z",
        "body": "looking at this today"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-20T16:19:03Z",
        "body": "@derekargueta any chance of getting this deprecation complete before we cut 1.10.0? Thank you!"
      },
      {
        "user": "derekargueta",
        "created_at": "2019-03-20T17:08:21Z",
        "body": "Just started picking this back up yesterday :)\r\n\r\n2 PRs on the way:\r\n1.  Convert all the router config_impl tests to the v2 API (makes testing this easier since the v1->v2 translation code doesn't really differentiate between route action and route, which is where I got lost last time, and v1 is entirely deprecated now anyways) - almost done, just a few more tests left.\r\n2. Actually remove this feature, with the first PR being very helpful in fixing the tests."
      },
      {
        "user": "derekargueta",
        "created_at": "2019-03-25T22:49:21Z",
        "body": "@mattklein123 re the new deprecation notice policy @alyssawilk sent out a few days ago, should these go to `fatal-by-default` first? Considering that this feature has been deprecated for a while (~6 months) and was removed from documentation at the same time, I'd favor just deleting but don't want to stray from the new policy.\r\n\r\nOtherwise, just waiting on #6332 to land before putting up the PR to remove this feature."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-25T23:06:41Z",
        "body": "@derekargueta I will defer to \"commander of deprecation policy\" @alyssawilk, but deleting sounds reasonable to me. :)"
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-03-26T12:57:17Z",
        "body": "+1 - good to remove.\r\nAnything with an open issue predates the changes to deprecation, so we're fine to straight up remove.\r\nThe script can't tell the difference, so both things which were ok to remove, and things subject to the new process all got listed."
      }
    ]
  },
  {
    "number": 4605,
    "title": "[v1.8.0 (Oct 4, 2018) deprecation] Remove features marked deprecated in #3675",
    "created_at": "2018-10-04T23:21:44Z",
    "closed_at": "2019-03-20T16:17:01Z",
    "labels": [
      "tech debt",
      "help wanted",
      "deprecation"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4605",
    "body": "#3675 ( ratelimit: add support for data-plane-api proto) introduced a deprecation notice for v1.8.0 (Oct 4, 2018). This issue tracks source code cleanup.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4605/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-11-04T00:00:55Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-20T16:17:01Z",
        "body": "This is already complete."
      }
    ]
  },
  {
    "number": 4485,
    "title": "Empty EDS prevents LDS and RDS requests",
    "created_at": "2018-09-20T19:31:23Z",
    "closed_at": "2018-09-23T13:55:54Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4485",
    "body": "*Description*:\r\nWhen connecting to a management server over ADS, Envoy is sending the expected CDS and EDS requests. However, if the EDS response is empty, Envoy is repeatedly requesting EDS and never moving on to LDS and RDS.\r\n\r\n*Repro steps*:\r\n1. Configure an Envoy to use ADS over gRPC\r\n2. Have the management server return a CDS with a cluster but an empty EDS response for that cluster\r\n3. Envoy will keep requesting EDS, never requesting LDS or RDS\r\n\r\n*Config*:\r\n```\r\nnode {\r\n  id: \"some_id\"\r\n  cluster: \"some_envoy_cluste\"\r\n  metadata {\r\n    <snipped>\r\n  }\r\n  locality {\r\n    sub_zone: \"some_zone\"\r\n  }\r\n}\r\ndynamic_resources {\r\n  lds_config {\r\n    ads {\r\n    }\r\n  }\r\n  cds_config {\r\n    ads {\r\n    }\r\n  }\r\n  ads_config {\r\n    api_type: GRPC\r\n    grpc_services {\r\n      google_grpc {\r\n        target_uri: \"dns:///example.com:12345\"\r\n        stat_prefix: \"stats\"\r\n        credentials_factory_name: \"com.example.creds\"\r\n        config {\r\n          fields {\r\n            key: \"creds_enabled\"\r\n            value {\r\n              bool_value: true\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\ncluster_manager {\r\n  load_stats_config {\r\n    api_type: GRPC\r\n    grpc_services {\r\n      google_grpc {\r\n        target_uri: \"dns:///example.com:12345\"\r\n        stat_prefix: \"stats\"\r\n        credentials_factory_name: \"com.example.creds\"\r\n        config {\r\n          fields {\r\n            key: \"creds_enabled\"\r\n            value {\r\n              bool_value: true\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nstats_sinks {\r\n  name: \"com.example.sink\"\r\n}\r\nadmin {\r\n  access_log_path: \"/dev/null\"\r\n  address {\r\n    socket_address {\r\n      address: \"::1\"\r\n      port_value: 43210\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n\r\n*Logs*:\r\n```\r\n[config] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources: [name: \"my_eds_cluster\"\r\ntype: EDS\r\neds_cluster_config {\r\n  eds_config {\r\n    ads {\r\n    }\r\n  }\r\n}\r\nconnect_timeout {\r\n  seconds: 10\r\n}\r\ncircuit_breakers {\r\n  thresholds {\r\n    max_connections {\r\n      value: 65536\r\n    }\r\n  }\r\n}\r\nhttp_protocol_options {\r\n}\r\noutlier_detection {\r\n  interval {\r\n    seconds: 1\r\n  }\r\n  max_ejection_percent {\r\n    value: 50\r\n  }\r\n  enforcing_consecutive_5xx {\r\n  }\r\n  consecutive_gateway_failure {\r\n    value: 3\r\n  }\r\n  enforcing_consecutive_gateway_failure {\r\n    value: 100\r\n  }\r\n}\r\nmetadata {\r\n <snipped>\r\n}\r\ncommon_lb_config {\r\n  locality_weighted_lb_config {\r\n  }\r\n}\r\n]\r\n[upstream] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"1538789483852555553\"\r\nnode {\r\n  id: \"some_id\"\r\n  cluster: \"some_envoy_cluster\"\r\n  metadata {\r\n    <snipped>\r\n  }\r\n  locality {\r\n    sub_zone: \"some_zone\"\r\n  }\r\n  build_version: \"0/1.8.0-dev/Clean/RELEASE\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"1\"\r\n[upstream] Received gRPC message for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment at version 1\r\n[upstream] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment: version_info: \"1\"\r\nnode {\r\n  id: \"some_id\"\r\n  cluster: \"some_envoy_cluste\"\r\n  metadata {\r\n    <snipped>\r\n  }\r\n  locality {\r\n    sub_zone: \"some_zone\"\r\n  }\r\n  build_version: \"0/1.8.0-dev/Clean/RELEASE\"\r\n}\r\nresource_names: \"my_eds_cluster\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\"\r\nresponse_nonce: \"1\"\r\n[upstream] Load report locality count 0\r\n[upstream] Sending LoadStatsRequest: node {\r\n  id: \"some_id\"\r\n  cluster: \"some_envoy_cluste\"\r\n  metadata {\r\n    <snipped>\r\n  }\r\n  locality {\r\n    sub_zone: \"some_zone\"\r\n  }\r\n  build_version: \"0/1.8.0-dev/Clean/RELEASE\"\r\n}\r\ncluster_stats {\r\n  cluster_name: \"my_eds_cluster\"\r\n  load_report_interval {\r\n    seconds: 10\r\n    nanos: 1604000\r\n  }\r\n}\r\n[upstream] New load report epoch: clusters: \"my_eds_cluster\"\r\nload_reporting_interval {\r\n  seconds: 10\r\n}\r\n[upstream] Received gRPC message for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment at version 2\r\n[upstream] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment: version_info: \"2\"\r\nnode {\r\n  id: \"some_id\"\r\n  cluster: \"some_envoy_cluster\"\r\n  metadata {\r\n    <snipped>\r\n  }\r\n  locality {\r\n    sub_zone: \"some_zone\"\r\n  }\r\n  build_version: \"0/1.8.0-dev/Clean/RELEASE\"\r\n}\r\nresource_names: \"my_eds_cluster\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\"\r\nresponse_nonce: \"2\"\r\n[upstream] Load report locality count 0\r\n[upstream] Sending LoadStatsRequest: node {\r\n  id: \"some_id\"\r\n  cluster: \"some_envoy_cluster\"\r\n  metadata {\r\n    <snipped>\r\n  }\r\n  locality {\r\n    sub_zone: \"some_zone\"\r\n  }\r\n  build_version: \"0/1.8.0-dev/Clean/RELEASE\"\r\n}\r\ncluster_stats {\r\n  cluster_name: \"my_eds_cluster\"\r\n  load_report_interval {\r\n    seconds: 10\r\n    nanos: 110620000\r\n  }\r\n}\r\n[upstream] Received gRPC message for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment at version 3\r\n[upstream] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment: version_info: \"3\"\r\nnode {\r\n  id: \"some_id\"\r\n  cluster: \"some_envoy_cluster\"\r\n  metadata {\r\n    <snipped>\r\n  }\r\n  locality {\r\n    sub_zone: \"some_zone\"\r\n  }\r\n  build_version: \"0/1.8.0-dev/Clean/RELEASE\"\r\n}\r\nresource_names: \"my_eds_cluster\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\"\r\nresponse_nonce: \"3\"\r\n[upstream] New load report epoch: clusters: \"my_eds_cluster\"\r\nload_reporting_interval {\r\n  seconds: 10\r\n}\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4485/comments",
    "author": "githuberto",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2018-09-20T20:18:14Z",
        "body": "@ramaraochavali I wonder if #4276 might have led to this issue? Essentially what is happening AFAICT is that cluster initialization never completes, since an empty EDS response is not sufficient to provide an empty initial update to the cluster."
      },
      {
        "user": "ramaraochavali",
        "created_at": "2018-09-21T02:27:00Z",
        "body": "@htuch I think you are right. I will revert it for now to get back the old behaviour and fix the empty stat issue later"
      }
    ]
  },
  {
    "number": 4477,
    "title": "update to clang-7",
    "created_at": "2018-09-20T06:59:04Z",
    "closed_at": "2018-09-22T21:16:02Z",
    "labels": [
      "area/build",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4477",
    "body": "*Description*:\r\nLLVM 7 is released. Consider update the build tool chain to clang-7.\r\n\r\ncc @PiotrSikora ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4477/comments",
    "author": "lizan",
    "comments": [
      {
        "user": "PiotrSikora",
        "created_at": "2018-09-20T07:12:14Z",
        "body": "I have this done and pending in my local tree already, but I'm waiting until 7.0 packages are published on `apt.llvm.org` (right now it still installs pre-release snapshots for `clang-7 llvm-7 lldb-7`) before opening a PR.\r\n\r\nFeel free to assign this to me, thanks."
      }
    ]
  },
  {
    "number": 4332,
    "title": "clean up management of durations in timer_impl",
    "created_at": "2018-09-04T13:48:45Z",
    "closed_at": "2019-01-11T02:53:14Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4332",
    "body": "*Description*:\r\n> In #4257 there was at one point a refactoring which moved TimerImpl::enableTimer(), which includes this:\r\n\r\n```\r\n      std::chrono::microseconds us = std::chrono::duration_cast<std::chrono::microseconds>(d);\r\n      timeval tv;\r\n      tv.tv_sec = us.count() / 1000000;\r\n```\r\n\r\nbringing it to the attention of @dnoe , who commented:\r\n\r\n```\r\nThe conversion factors here could be more readable by using duration cast:\r\n\r\nauto secs = std::chrono::duration_cast<std::chrono::seconds>(d);\r\nauto usecs = std::chrono::duration_cast<std::chrono::microseconds>(d - secs);\r\ntv.tv_secs = secs.count();\r\ntv.tv_usecs = usecs.count();\r\n```\r\n\r\nI didn't want to rewrite the code as I was just moving it; and then I wound up unmoving it so it really isn't part of #4257, but I'm jotting this down as tech-debt.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4332/comments",
    "author": "jmarantz",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-10-12T23:39:48Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-12-24T16:55:38Z",
        "body": "@ranchowang sure go for it"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-01-11T02:53:14Z",
        "body": "Done"
      }
    ]
  },
  {
    "number": 4274,
    "title": "Dynamically route prefix_rewrite",
    "created_at": "2018-08-28T06:23:17Z",
    "closed_at": "2019-10-23T21:16:03Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4274",
    "body": "**How to dynamically route prefix_rewrite**\r\n\r\n*Description*:\r\n>I want to redirect an incoming request to envoy to one of my services. \r\nFor example: I have two services bookings and orders. I want to route to orders/{valueGiven}/sometask when the system gets the request for bookings/{valueGiven}/sometask. I tried doing below solution. It's not working as expected.\r\n\r\n                        {\r\n                            \"match\": {\r\n                              \"regex\": \"/bookings/(.*)/sometask\"\r\n                            },\r\n                            \"route\": {\r\n                              \"prefix_rewrite\": \"/orders/$1/sometask\",\r\n                              \"cluster\": \"order_service\"\r\n                            }\r\n                          }\r\n\r\n\r\n\r\nWhen I  send request bookings/1234/sometask, system should route to orders/1234/sometask\r\n**In the logs I am getting : method=GET path=\"/orders/$1/sometask\", order not found. The expected behavior should get path with the path parameters passed.**\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4274/comments",
    "author": "Bhawna4tech",
    "comments": [
      {
        "user": "zyfjeff",
        "created_at": "2018-08-28T06:36:31Z",
        "body": "@Bhawna4tech  the variable is not currently supported, there is no way to use $1 to refer to regular matching to the content"
      },
      {
        "user": "techygarg",
        "created_at": "2018-08-28T07:59:20Z",
        "body": "@zyfjeff: Thanks for prompt response.  As we have limited knowledge with Envoy so far, do you see any other feasible way, we can achieve it."
      },
      {
        "user": "zyfjeff",
        "created_at": "2018-08-28T09:33:25Z",
        "body": "@techygarg #3977 This problem is very similar with you"
      },
      {
        "user": "Bhawna4tech",
        "created_at": "2018-08-28T10:50:05Z",
        "body": "@zyfjeff: Thanks for sharing the link. Yeah, it is same scenario. As no one has responded yet, it looks like it is not feasible currently in envoy, correct ?"
      },
      {
        "user": "dio",
        "created_at": "2018-08-28T22:53:37Z",
        "body": "It seems it is useful to have captured params to be used for subsequent `prefix_rewrite`-ing."
      },
      {
        "user": "BailyTroyer",
        "created_at": "2019-10-23T18:29:43Z",
        "body": "Has this been addressed? I am trying to do the same and have found no documentation regarding capture groups in envoy. "
      },
      {
        "user": "zuercher",
        "created_at": "2019-10-23T21:16:03Z",
        "body": "I'm closing this as a duplicate of #2092, which I believe is being worked on."
      }
    ]
  },
  {
    "number": 4174,
    "title": "Verify certificates with common name",
    "created_at": "2018-08-15T23:38:53Z",
    "closed_at": "2018-09-12T00:45:11Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4174",
    "body": "*Title*: *Envoy needs to verify certificates using CN instead of SAN*\r\n\r\n*Description*:\r\n>Customers request Istio to support TLS authentication with legacy systems using certificates with identities encoded in CN instead of SAN. The CN could be IP address or FQDN.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4174/comments",
    "author": "myidpt",
    "comments": [
      {
        "user": "PiotrSikora",
        "created_at": "2018-09-12T00:27:14Z",
        "body": "@myidpt could you elaborate on why this is needed in Envoy? If Citadel and/or other CAs produce certificates with both CN and SAN, then Envoy can verify SAN and legacy systems can verify CN.\r\n\r\nReading the linked Istio issue, it seems that the problem is that Citadel doesn't produce certificates compatible with legacy systems, not that Envoy cannot verify certificates produces by legacy CAs.\r\n\r\nNote: this is trivial to implement, but the world moved from CN to SAN, so I'd like to have a valid use case before adding this outdated verification method.\r\n\r\ncc @ggreenway "
      },
      {
        "user": "myidpt",
        "created_at": "2018-09-12T00:45:11Z",
        "body": "Yes I think the linked issue is not related to this feature. I think we can temporarily close this issue and reopen it when we need."
      }
    ]
  },
  {
    "number": 4103,
    "title": "Configurable retry codes",
    "created_at": "2018-08-10T01:07:05Z",
    "closed_at": "2018-10-09T21:49:33Z",
    "labels": [
      "design proposal",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4103",
    "body": "Internally, we have some clients that currently rely on some bespoke, legacy\r\nretry behavior, whereby an HTTP/1.1 request can be retried on a 429 response\r\ncode. Ideally, we'd like to be able to move this retry logic out of the clients\r\nand into the proxy-layer (i.e. Envoy) and still preserve the same behavior in\r\nthe event the server returns a 429.\r\n\r\nBelow are some options we'd like to pitch.\r\n\r\nThe first would involve adding a new field to the `RouteAction.RetryPolicy`\r\nproto, `retryable_codes`, that would contain one or more HTTP response codes\r\n(as integers) that are deemed retryable for the request.\r\n\r\nIn addition to this, we could create a new policy, say `\"retryable-codes\"`,\r\nwhich expects either an accompanying request header (e.g.\r\n`X-envoy-retryable-codes: 429,503`), or entry in the route configuration with\r\nthe retryable codes (as per above).\r\n\r\nAnother option, which is probably a non-starter, would be to add the specific\r\nretry policy we need, `\"retriable-429\"`, but this feels a little too specific\r\nand brittle.\r\n\r\nOpen to thoughts on how we might go about this. Thanks!\r\n\r\ncc: @mpuncel @snowp ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4103/comments",
    "author": "nicktrav",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-08-10T04:21:14Z",
        "body": "I like the idea of an option `x-envoy-retryable-codes` header an accompanying policy. Anyone else out there have an opinion on this?"
      },
      {
        "user": "snowp",
        "created_at": "2018-08-10T17:48:10Z",
        "body": "I'm a fan of the new header, it seems very flexible. Maybe also make the retry codes configurable in `RetryPolicy` so it can be configured through RDS without injecting headers?"
      },
      {
        "user": "wjessop",
        "created_at": "2018-08-16T15:33:48Z",
        "body": "I love the idea of being able to specify response codes for retry behaviour:\r\n\r\n```\r\nretry_policy:\r\n  retry_on: connect-failure\r\n  retryable_codes: 503,418\r\n  num_retries: 3\r\n```\r\n\r\nOr even just:\r\n\r\n```\r\nretry_policy:\r\n  retry_on: connect-failure,503,418\r\n  num_retries: 3\r\n```\r\n\r\nI don't fully understand the idea behind x-envoy-retryable-codes though. This was mentioned as being a request header, but it seems like the place the knowledge about wether something should be retryable or not should be the proxy or the upstream service.\r\n\r\nAs an aside, I also think getting rid of the gateway-error policy would be a good idea. IMO it's presence suggests that retrying non-503 errors is a safe thing to do."
      },
      {
        "user": "nicktrav",
        "created_at": "2018-08-21T15:59:07Z",
        "body": "I'll just chime in to mentioned we're still playing around with some ideas on this proposal and will hopefully offer up a patch soon. Feel free to assign this me for now.\r\n\r\n> This was mentioned as being a request header, but it seems like the place the knowledge about wether something should be retryable or not should be the proxy or the upstream service.\r\n\r\n@wjessop - totally agree with you _in principle_ here in that ideally retry configuration should live in the proxy layer. The reason we're proposing this is to attempt to provide feature parity between Envoy and our existing polyglot HTTP client libraries that all make extensive use of HTTP response codes to determine when to retry."
      },
      {
        "user": "wjessop",
        "created_at": "2018-08-21T18:39:31Z",
        "body": "Thanks for the reply @nicktrav. In this situation being able to have both methods of configuration would be great, though the ability to ignore re-try headers set from downstream of Envoy would be essential for us as I could see that having them enabled for public facing infra could be a DoS risk."
      },
      {
        "user": "dunjut",
        "created_at": "2018-09-04T09:40:18Z",
        "body": "I like this proposal. In our legacy systems, a custom proxy component replies http 570 indicating clients to retry another instance, when such components going into graceful upgrade process. 502 and 503 codes aren't used in this scenario because that would cause ambiguity."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-10-04T09:57:34Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 4072,
    "title": "envoy crashes when updating a cluster that's using merged updates ",
    "created_at": "2018-08-07T01:41:42Z",
    "closed_at": "2018-08-10T00:18:01Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4072",
    "body": "Steps for repro:\r\n\r\na) start with a simple cluster using the subset lb:\r\n\r\n```\r\n  ---\r\n  name: cluster-name\r\n  type: EDS\r\n  eds_cluster_config:\r\n    eds_config:\r\n      path: '.../eds.conf'\r\n  connect_timeout:\r\n    seconds: 10\r\n  lb_policy: LEAST_REQUEST\r\n  lb_subset_config:\r\n    fallback_policy: DEFAULT_SUBSET\r\n    default_subset:\r\n      stage: prod\r\n    subset_selectors:\r\n    - keys:\r\n      - v\r\n      - stage\r\n    - keys:\r\n      - stage\r\n```\r\n\r\nb) Update the cluster (new subset selector, default subset, etc). e.g.:\r\n\r\n```\r\n...\r\n    - keys:\r\n      - stage\r\n      - foo\r\n```\r\n\r\nc) crash happens\r\n\r\nInterestingly, it happens after the cluster update is apparently finished:\r\n\r\n```\r\n[2018-08-07 01:37:29.530][9][info][upstream] external/envoy/source/common/upstream/cluster_manager_impl.cc:499] add/update cluster app starting warming\r\n...\r\n[2018-08-07 01:37:30.546][9][info][upstream] external/envoy/source/common/upstream/cluster_manager_impl.cc:506] warming cluster app complete\r\n[2018-08-07 01:37:34.645][9][critical][backtrace] bazel-out/k8-opt/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:125] Caught Segmentation fault, suspect faulting address 0x2\r\n```\r\n\r\nNote that initially I thought this was triggered by adding a new subset selector, but I just checked that any update in the cluster triggers this (not sure if the update has to be related to the subset LB, I'll dig in a bit more). I'll get a stack trace and/or a test case repro later today/tomorrow. \r\n\r\ncc: @zuercher ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4072/comments",
    "author": "rgs1",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2018-08-07T21:05:45Z",
        "body": "I wasn't able to reproduce this with CDS or EDS updates via the filesystem. We also tried with GRPC-based updates. Do you have any more information?"
      },
      {
        "user": "rgs1",
        "created_at": "2018-08-07T21:09:22Z",
        "body": "@zuercher this is with CDS updates via the filesystem as well. Hmm forgot to say that we are using:\r\n\r\n```\r\n    \"common_lb_config\": {\r\n      \"update_merge_window\": \"10s\"\r\n    }\r\n```\r\n"
      },
      {
        "user": "rgs1",
        "created_at": "2018-08-07T22:36:27Z",
        "body": "@zuercher ok, seems to be related to merging updates not to the subset lb:\r\n\r\n```\r\n[2018-08-07 22:34:34.277][10][critical] Backtrace (most recent call first) from thread 0:\r\n  #1 Envoy::Upstream::ClusterManagerImpl::postThreadLocalClusterUpdate(Envoy::Upstream::Cluster const&, unsigned int, std::vector<std::shared_ptr<Envoy::Upstream::Host>, std::allocator<std::shared_ptr<Envoy::Upstream::Host> > > const&, std::vector<std::shared_ptr<Envoy::Upstream::Host>, std::allocator<std::shared_ptr<Envoy::Upstream::Host> > > const&) at cluster_manager_impl.cc:685\r\n  #2 Envoy::Upstream::ClusterManagerImpl::applyUpdates(Envoy::Upstream::Cluster const&, unsigned int, Envoy::Upstream::ClusterManagerImpl::PendingUpdates&) at cluster_manager_impl.cc:444\r\n  #3 event_process_active_single_queue at event.c:1646\r\n  #4 event_process_active at event.c:1738\r\n  #5 event_base_loop at event.c:1961\r\n  #6 Envoy::Server::InstanceImpl::run() at server.cc:413\r\n  #7 Envoy::MainCommonBase::run() at main_common.cc:83\r\n  #8 Envoy::MainCommon::run() at main_common.h:44\r\n  #9  (inlined by) main at main.cc:37\r\n  #10 \r\n  #11 ?? ??:0\r\n  #12 \r\n  #13 _start at ??:?\r\n  #14 \r\n```"
      },
      {
        "user": "rgs1",
        "created_at": "2018-08-07T22:43:16Z",
        "body": "Ah! I see the bug... PR coming up. "
      }
    ]
  },
  {
    "number": 4048,
    "title": "Expose retries with trace tags and extra response info/logs",
    "created_at": "2018-08-03T20:48:26Z",
    "closed_at": "2019-12-13T00:37:38Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4048",
    "body": "**Issue Template**\r\n\r\n*Title*: Is there a way to know retries have occurred in an egress HTTP request?\r\n\r\n*Description*:\r\n>We are using a LUA script to generate custom trace reports for each ingress/egress HTTP requests and it would be very beneficial to capture the retry events in requests and put them in report. Does envoy already provide some features for this, such as a response header to indicate retry counts? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4048/comments",
    "author": "charliezgc",
    "comments": [
      {
        "user": "ramaraochavali",
        "created_at": "2018-08-04T05:12:24Z",
        "body": "The following stats on the cluster that you are trying to hit should help you\r\nupstream_rq_retry: \r\nupstream_rq_retry_overflow: \r\nupstream_rq_retry_success: "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-08-05T18:59:42Z",
        "body": "IIRC each retry would be shown in the upstream access logs, but I don't think we explicitly indicate that it is a retry today (though that would be useful)."
      },
      {
        "user": "charliezgc",
        "created_at": "2018-08-07T18:32:39Z",
        "body": "Stats or logs won't be able to tie the retry indication to a request. We are trying to show retry activities in distributed tracing. The ideal case would be an entry in response header that says the request has been tried."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-08-07T18:57:32Z",
        "body": "@charliezgc I agree and have thought about this also. I think that we should indicate retries both as tracing tags (including the retry number) as well as possibly in the extra response info work that @junr03 is working on. Will move this over to enhancement. "
      },
      {
        "user": "mattklein123",
        "created_at": "2019-12-13T00:37:38Z",
        "body": "This was fixed. There are now retry trace tags."
      }
    ]
  },
  {
    "number": 3997,
    "title": "IpVersions/EchoIntegrationTest.AddRemoveListener/IPv6 is flaky",
    "created_at": "2018-07-31T06:40:53Z",
    "closed_at": "2018-08-30T21:45:02Z",
    "labels": [
      "help wanted",
      "area/test flakes"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3997",
    "body": "*Description*:\r\non master 028387a3b0746deaf011ea50104692dfbb8b8d2f run:\r\n`bazel test --runs_per_test=100 //test/integration:echo_integration_test`\r\n\r\nwill result 2 runs out of 100 `TIMEOUT`, with `\"-l trace\"` got 8 out of 100.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3997/comments",
    "author": "lizan",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-07-31T16:17:15Z",
        "body": "I've seen this also on my own machine."
      },
      {
        "user": "zuercher",
        "created_at": "2018-08-14T18:26:17Z",
        "body": "I poke around this a bit yesterday evening and it seems to be a race in the AddRemoveListenerTest between the RawConnectionDriver making a connect attempt and the actual listener socket being closed.\r\n\r\nIt seems like sometimes the listener socket is closed concurrently with the RawConnectionDriver's connect attempt. The RawConnectionDriver's ConnectionImpl sees a successful connection (write event triggers onWriteReady and getsockopt returns no error) and then writes the initial data. No further events occur and the RawConnectionDriver waits in Dispatcher::run until the test times out.\r\n\r\nWhen the test passes, the connect either happens before or after the socket close which either leads to an immediate connect failure or a deferred one, and in both those cases the test terminates successfully."
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-08-14T21:00:50Z",
        "body": "This was failing enough today I'd back disabling first and debugging later, if anyone is willing to own debug"
      }
    ]
  },
  {
    "number": 3994,
    "title": "Add `prepend` member functions to Buffer::Instance",
    "created_at": "2018-07-31T05:11:09Z",
    "closed_at": "2018-08-09T13:21:45Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3994",
    "body": "It would be nice to have wrappers around `evbuffer_prepend` and `evbuffer_prepend_buffer` in order to simplify prepending data to a buffer. I would image it would match the signature of the existing methods, so something like\r\n\r\n```\r\nvoid Buffer::Instance prepend(const void* data, uint64_t size) PURE;\r\nvoid Buffer::Instance prepend(const std::string& data) PURE;\r\nvoid Buffer::Instance prepend(Instance& data) PURE;\r\n```\r\n\r\nwhere the last function drains the provided buffer.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3994/comments",
    "author": "snowp",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-07-31T16:18:01Z",
        "body": "Sounds reasonable."
      },
      {
        "user": "ggreenway",
        "created_at": "2018-07-31T16:57:13Z",
        "body": "I've had to work around this a couple times (create new buffer, add \"prepend\" data, append all data from other buffer). I'm in favor of this also."
      },
      {
        "user": "athampy",
        "created_at": "2018-08-05T18:55:04Z",
        "body": "Anyone working on this? If not, I can pick it up."
      },
      {
        "user": "snowp",
        "created_at": "2018-08-06T00:03:56Z",
        "body": "I haven't had time to start working on it, so since nobody else has spoken up I'd assume nobody else is working on it. Feel free to take it"
      }
    ]
  },
  {
    "number": 3932,
    "title": "Envoy sometimes returns response body to HEAD requests",
    "created_at": "2018-07-23T18:31:04Z",
    "closed_at": "2018-08-20T19:07:04Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3932",
    "body": "*Description*:\r\nRFC 7231 says that servers should not respond with a body to HEAD requests, but Envoy does\r\n\r\n*Repro steps*:\r\n`curl -XHEAD <envoy_url>` when it has no healthy upstreams, you'll see a `upstream connect error or disconnect/reset before headers` response body.\r\n\r\nNote that using `--head` instead of `-XHEAD` in curl omits the response body but shows a `Excess found in a non pipelined read: excess = 57 url = /foo (zero-length body)` warning.\r\n\r\nThis is a minor issue but causes some surprising noise in our deploy system's logs as it performs health checks with HEAD while the service Envoy is deployed alongside is down.\r\n\r\nEnvoy version: c92a3017017dfa31241dec13dc6a1479090318d0/1.8.0-dev/Clean/RELEASE",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3932/comments",
    "author": "mpuncel",
    "comments": [
      {
        "user": "zyfjeff",
        "created_at": "2018-07-24T15:33:13Z",
        "body": "I can start this one,  mainly for the `Utility::sendLocalReply` modification"
      },
      {
        "user": "dio",
        "created_at": "2018-07-24T20:57:36Z",
        "body": "I remember had some difficulties on making \"expect true\" for `request->complete()` (on test) to work for `HEAD` requests when the body is empty."
      },
      {
        "user": "zyfjeff",
        "created_at": "2018-07-28T11:18:55Z",
        "body": "@dio code implementation and integration testing can be submitted separately?"
      },
      {
        "user": "dio",
        "created_at": "2018-07-28T11:33:37Z",
        "body": "@zyfjeff as long as the introduced changes don't break the integration tests, probably that is fine (however in this case, I think having an integration test helps you to be confident about the changes). And make sure that your changes are covered. "
      },
      {
        "user": "zyfjeff",
        "created_at": "2018-07-28T11:36:12Z",
        "body": "@dio  thanks, I'm going to make a pr"
      },
      {
        "user": "zyfjeff",
        "created_at": "2018-08-06T11:33:51Z",
        "body": "@dio \r\nWhen I was doing integration testing, it seemed that I had encountered the problem you said, response->complete() could not make \"expect true\"  #3985 \r\n\r\nDo you know how to solve it?"
      },
      {
        "user": "dio",
        "created_at": "2018-08-08T20:40:52Z",
        "body": "@zyfjeff no, not yet. Following #3985, guess you are already in the right direction and you have @mattklein123 and @alyssawilk with you 🙂 "
      }
    ]
  },
  {
    "number": 3865,
    "title": "ext_authz filter is messing up \"content_length\" header",
    "created_at": "2018-07-16T14:10:00Z",
    "closed_at": "2018-08-10T00:13:44Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3865",
    "body": "When envoy receives a request with body length of e.g. 5, the req. headers are:\r\n```\r\n[2018-07-16 15:59:23.776][22489][debug][http] source/common/http/conn_manager_impl.cc:455] [C0][S3713373574215004266] request headers complete (end_stream=false):\r\n':authority', '0.0.0.0:777'\r\n':path', '/'\r\n':method', 'POST'\r\n'user-agent', 'curl/7.52.1'\r\n'accept', '*/*'\r\n'content-length', '5'\r\n'content-type', 'application/x-www-form-urlencoded'\r\n```\r\nthen it sends this request to the ext_authz filter without body so the \"content_length\" header is supposed to be 0. But instead of 0 envoy sets it to \"0,5\" (0, original content length). This causes any http server to respond \"400 Bad request\".\r\nHere are the headers from envoy debug log: \r\n```\r\n[2018-07-16 15:59:23.776][22489][debug][router] source/common/router/router.cc:304] [C0][S18143179754686772766] router decoding headers:\r\n':method', 'POST'\r\n':path', '/'\r\n':authority', '0.0.0.0:777'\r\n':scheme', 'http'\r\n'content-length', '0,5'\r\n'x-request-id', 'f6238991-b127-4cec-aadd-0ef8a35e3508'\r\n'x-forwarded-proto', 'http'\r\n'accept', '*/*'\r\n'content-type', 'application/x-www-form-urlencoded'\r\n'user-agent', 'curl/7.52.1'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '172.17.0.1'\r\n'x-envoy-expected-rq-timeout-ms', '200'\r\n```\r\nthese are received by external auth service.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3865/comments",
    "author": "michalholecek",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-07-16T16:14:07Z",
        "body": "cc @saumoh @gsagula "
      },
      {
        "user": "gsagula",
        "created_at": "2018-07-16T18:46:11Z",
        "body": "@michalholecek  Thanks for reporting it. I will take a look at and get back to you.\r\n"
      }
    ]
  },
  {
    "number": 3819,
    "title": "improve syscall API wrappers",
    "created_at": "2018-07-09T20:24:01Z",
    "closed_at": "2020-01-23T17:10:39Z",
    "labels": [
      "tech debt",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3819",
    "body": "Per discussion in #3813 it'd be good if the doRead call didn't latch errno several levels away from where the actual syscall was performed.  We should change the APIs to return {rc, errnno} in some form to better futureproof in case the intermediate function calls add their own errno-corrupting debug logging.\r\n\r\nWe could also get really fancy and insist that errno is latched via fix_format fixes, as also discussed there.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3819/comments",
    "author": "alyssawilk",
    "comments": [
      {
        "user": "venilnoronha",
        "created_at": "2018-07-10T16:25:23Z",
        "body": "Hi @alyssawilk, I'd like to give it a try."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-07-10T17:11:50Z",
        "body": "@venilnoronha go for it!"
      },
      {
        "user": "PiotrSikora",
        "created_at": "2018-07-25T00:25:50Z",
        "body": "Erm, looking at the already merged and outstanding PRs, the errno seems to be latched one level too high (inside functions calling syscall wrappers and not in the syscall wrappers themselves).\r\n\r\nIs there any reason not to do it in the syscall wrappers and return tuple from them?"
      },
      {
        "user": "venilnoronha",
        "created_at": "2018-07-25T00:50:43Z",
        "body": "@PiotrSikora that's next in the plan of action. Briefly, I wanted to start off one level higher to keep the PRs smaller."
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-01-23T17:10:39Z",
        "body": "I think the final fix was solid, and we forgot to close this one out.  Thanks!"
      }
    ]
  },
  {
    "number": 3809,
    "title": "HttpIntegrationTest methods should time out instead of waiting forever",
    "created_at": "2018-07-09T16:31:16Z",
    "closed_at": "2019-01-11T02:54:44Z",
    "labels": [
      "enhancement",
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3809",
    "body": "The integration test framework makes it hard to send requests that might, e.g., be rejected by a decoder filter. HttpIntegrationTest::waitForNextUpstreamRequest will happily spin forever in that case. That method and similar ones should time out instead.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3809/comments",
    "author": "mkbehr",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-07-09T16:52:10Z",
        "body": "I don't think we should generically timeout in all cases, as that will introduce timing issues in tests. Feel free to add a variant that times out if it helps you."
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-07-11T20:15:37Z",
        "body": "I think a blanket change from codec_client_->waitForDisconnect(); <-- hang forever to\r\n\r\nASSERT_TRUE(codec_client_->waitForDisconnect()); <-- 5s timeout\r\nwould be a net win.  We'd have to do auditing to make sure subfunctions also exited correctly but if we used ABSL_MUST_USE_RESULT we could verify at compile time we didn't miss any.\r\n\r\nditto with other client wait functions."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-07-11T20:22:22Z",
        "body": "@alyssawilk agreed, I just don't want to rely on timeouts generically."
      },
      {
        "user": "mkbehr",
        "created_at": "2018-07-11T20:42:20Z",
        "body": "Would it work to time out by default but with an optional argument to not time out, or do you mean something else by \"generically\"?"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-07-11T20:46:51Z",
        "body": "What I'm saying is that I don't want the default behavior to be that timeouts somehow lead to tests passing. What @alyssawilk has proposed (timeout and fail) I agree it much better than hang, but that still leaves the default as timeout/fail vs. timeout and maybe pass. I think it's fine for the test writer to be able to opt-in to timeout and not fail in specific cases where that makes sense."
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-07-11T20:57:44Z",
        "body": "Do you have any concrete examples of where you think it makes more sense to time the whole test out rather than fast-fail?  Or perhaps we can give the ABSL_MUST_USE_RESULT variant a try and see if there's any corner cases where an infinite timeout makes sense."
      },
      {
        "user": "mkbehr",
        "created_at": "2018-07-11T21:00:29Z",
        "body": "ABSL_MUST_USE_RESULT will break all the existing uses for anything that returns void but it should prevent the scenario where the test ignores a timeout and passes when it should fail because the test-writer forgot to check. That scenario could still happen if a test calls something that returns a pointer like sendRequestAndWaitForResponse, stores it, and then doesn't try to dereference it in some branch, but I'd expect that to be a pretty rare case."
      },
      {
        "user": "mkbehr",
        "created_at": "2018-07-11T21:04:33Z",
        "body": "Which is to say, if I'm understanding @mattklein123 right, there's the concern that we add the interface for\r\n\r\n`ASSERT_TRUE(codec_client_->waitForDisconnect());  // 5s timeout`\r\n\r\nbut somebody accidentally writes\r\n\r\n`codec_client_->waitForDisconnect();  // Now this can time out and the test won't fail`\r\n\r\nbut if waitForDisconnect is ABSL_MUST_USE_RESULT then the second example there won't compile."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-07-11T21:05:52Z",
        "body": "Yes exactly that's my concern that I want to prevent."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-01-11T02:54:44Z",
        "body": "Done"
      }
    ]
  },
  {
    "number": 3774,
    "title": "runtime: allow additional override layers",
    "created_at": "2018-07-02T18:23:07Z",
    "closed_at": "2019-12-13T00:36:39Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3774",
    "body": "Currently, runtime allows an override based on the `service-cluster` only.\r\n\r\nThis works well except in the case that we want to set a value globally for some other logical collection of envoys.\r\n\r\nOne implementation might look like an additional list of `override` directories, and merge them in order:\r\n```\r\n{\r\n  \"symlink_root\": \"/srv/runtime/current\",\r\n  \"subdirectory\": \"envoy\",\r\n  \"override_subdirectory\": \"envoy_override\"\r\n  \"override_order\": [\r\n     \"fooservice\",\r\n     \"user-facing\",\r\n     \"tier0\",\r\n  ],\r\n}\r\n```\r\n\r\nThis would give us a merge of:\r\n```\r\n/srv/runtime/current/envoy\r\n/srv/runtime/current/envoy_override/fooservice\r\n/srv/runtime/current/envoy_override/user-facing\r\n/srv/runtime/current/envoy_override/tier0\r\n```\r\n\r\ni.e. setting the `override_order` would remove the implicit `service-cluster` override. If it's needed, specify it explicitly in the desired spot in the merge order.\r\n\r\nIt could be argued that any further override capability should be handled by the runtime deployment system, distributing merged runtime bundles based on some kind of per-service manifest. However, since we already allow an override capability, and it would be nice make it more flexible.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3774/comments",
    "author": "danielhochman",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-12-13T00:36:39Z",
        "body": "This has been implemented with the new generic layers system."
      }
    ]
  },
  {
    "number": 3750,
    "title": "auth filter headers causing unauthorised memory access",
    "created_at": "2018-06-28T17:33:19Z",
    "closed_at": "2018-07-07T22:33:22Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3750",
    "body": "When I add a http header to a request via ext_auth filter, and the header's key is longer than 15 characters, envoy turns it into something like this:\r\n**headers recieved from ext_auth filter printed via lua filter:**\r\n```\r\n:authority\t0.0.0.0:777\r\n:path\t/\r\n:method\tGET\r\nuser-agent\tcurl/7.52.1\r\naccept\t*/*\r\nx-forwarded-proto\thttp\r\nx-request-id\t2c6274e9-680d-4c86-af87-2b8fb609a7ec\r\nx\t1\r\nxx\t2\r\nxxx\t3\r\nxxxx\t4\r\nxxxxx\t5\r\nxxxxxx\t6\r\nxxxxxxx\t7\r\nxxxxxxxx\t8\r\nxxxxxxxxx\t9\r\nxxxxxxxxxx\t10\r\nxxxxxxxxxxx\t11\r\nxxxxxxxxxxxx\t12\r\nxxxxxxxxxxxxx\t13\r\nxxxxxxxxxxxxxx\t14\r\nxxxxxxxxxxxxxxx\t15\r\nxxxxxxxxxxxxxxxx\t16\r\nxxxxxxxxxxxxxxxxx\t17\r\nxxxxxxxxxxxxxxxxxx\t18\r\nxxxxxxxxxxxxxxxxxxx\t19\r\nxxxxxxxxxxxxxxxxxxxx\t20\r\nxxxxxxxxxxxxxxxxxxxxx\t21\r\nxxxxxxxxxxxxxxxxxxxxxx\t22\r\nxxxxxxxxxxxxxxxxxxxxxxx\t23\r\nxxxxxxxxxxxxxxxxxxxxxxxx\t24\r\nxxxxxxxxxxxxxxxxxxxxxxxxx\t25\r\ndate\tThu, 28 Jun 2018 17:44:29 GMT\r\nx-envoy-upstream-service-time\t0\r\n```\r\n**headers received by cluster in the envoy route:** (nginx server, receiving requests from envoy)\r\n```\r\nGET / HTTP/1.1\r\nhost: 0.0.0.0:777\r\nuser-agent: curl/7.52.1\r\naccept: */*\r\nx-forwarded-proto: http\r\nx-request-id: 2c6274e9-680d-4c86-af87-2b8fb609a7ec\r\nx: 1\r\nxx: 2\r\nxxx: 3\r\nxxxx: 4\r\nxxxxx: 5\r\nxxxxxx: 6\r\nxxxxxxx: 7\r\nxxxxxxxx: 8\r\nxxxxxxxxx: 9\r\nxxxxxxxxxx: 10\r\nxxxxxxxxxxx: 11\r\nxxxxxxxxxxxx: 12\r\nxxxxxxxxxxxxx: 13\r\nxxxxxxxxxxxxxx: 14\r\nxxxxxxxxxxxxxxx: 15\r\n�[v\u0003xxxxxxxx: 16\r\n�Kv\u0003xxxxxxxxx: 17\r\n�\u001cv\u0003xxxxxxxxxx: 18\r\n`Kv\u0003xxxxxxxxxxx: 19\r\n�\u0014v\u0003xxxxxxxxxxxx: 20\r\n�Kv\u0003xxxxxxxxxxxxx: 21\r\n�\r\n v\u0003xxxxxxxxxxxxxx: 22\r\nHv\u0003xxxxxxxxxxxxxxx: 23\r\n\r\nv\u0003xxxxxxxxxxxxxxxx: 24\r\nx\r\n�\u0003x\r\n�\u0003��t\u0003x: 25\r\ndate: Thu, 28 Jun 2018 17:44:29 GMT\r\nx-envoy-upstream-service-time: 0\r\nx-envoy-expected-rq-timeout-ms: 15000\r\ncontent-length: 0\r\n```\r\n\r\nand when the headers key is e.g. \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" (key: 47*x), envoy replaces it with this: \"cluster.web.upstream_cx_connect_ms\"\r\n\r\nI can provide more info if you want to. I am using an external auth service written in Golang, communicating via http.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3750/comments",
    "author": "michalholecek",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-06-28T21:41:43Z",
        "body": "@gsagula "
      },
      {
        "user": "gsagula",
        "created_at": "2018-06-28T22:30:27Z",
        "body": "@michalholecek Let me take a look at this issue and I get back to you."
      },
      {
        "user": "gsagula",
        "created_at": "2018-06-29T07:36:49Z",
        "body": "@michalholecek Thank you for reporting this issue. The PR above should fix it."
      }
    ]
  },
  {
    "number": 3714,
    "title": "[v1.7.0 deprecation] Remove features marked deprecated in #3001",
    "created_at": "2018-06-25T14:05:56Z",
    "closed_at": "2018-07-28T02:26:40Z",
    "labels": [
      "tech debt",
      "help wanted",
      "deprecation"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3714",
    "body": "#3001 (deprecation: Indicate that admin mutation GETs are deprecated.) introduced a deprecation notice for v1.7.0. This issue tracks source code cleanup.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3714/comments",
    "author": "alyssawilk",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2018-07-25T14:49:05Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-07-25T14:56:30Z",
        "body": "I think this one was security related so I'm going to mark it help wanted as I don't think Josh will be able to pick it up before stalebot closes it down."
      },
      {
        "user": "jmarantz",
        "created_at": "2018-07-25T16:06:01Z",
        "body": "Sorry I completely missed this due to my vacation."
      }
    ]
  },
  {
    "number": 3684,
    "title": "routing: websocket routes allow, but ignore, weighted_clusters",
    "created_at": "2018-06-21T15:07:33Z",
    "closed_at": "2019-08-20T19:29:29Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3684",
    "body": "As of envoyproxy/envoy:c755c0393d8d6fbb16a00c7f2b97b8d0d5478c03, this route:\r\n\r\n```json\r\n\"routes\":[\r\n    {\r\n        \"prefix\":\"/ws\",\r\n        \"prefix_rewrite\":\"/ws\",\r\n        \"timeout_ms\":3000,\r\n        \"use_websocket\":true,\r\n        \"weighted_clusters\":{\r\n            \"clusters\":[\r\n                {\r\n                    \"name\":\"cluster_web_basic_ambassador_sbx0\",\r\n                    \"weight\":100.0\r\n                }\r\n            ]\r\n        }\r\n    }\r\n]\r\n```\r\n\r\nis parsed fine, but will route no traffic: a reference to `/ws` simply gives a 503 with no attempt to connect upstream:\r\n\r\n```\r\n[2018-06-21 15:00:18.015][15][debug][http] source/common/http/conn_manager_impl.cc:798] [C0][S12312213233427221649] request end stream\r\n[2018-06-21 15:00:18.015][15][debug][http] source/common/http/conn_manager_impl.cc:452] [C0][S12312213233427221649] request headers complete (end_stream=true):\r\n[2018-06-21 15:00:18.015][15][debug][http] source/common/http/conn_manager_impl.cc:457] [C0][S12312213233427221649]   ':authority':'127.0.0.1:33333'\r\n[2018-06-21 15:00:18.015][15][debug][http] source/common/http/conn_manager_impl.cc:457] [C0][S12312213233427221649]   'sec-websocket-extensions':'permessage-deflate; client_max_window_bits'\r\n[2018-06-21 15:00:18.015][15][debug][http] source/common/http/conn_manager_impl.cc:457] [C0][S12312213233427221649]   'user-agent':'Python/3.6 websockets/5.0.1'\r\n[2018-06-21 15:00:18.015][15][debug][http] source/common/http/conn_manager_impl.cc:457] [C0][S12312213233427221649]   'upgrade':'websocket'\r\n[2018-06-21 15:00:18.016][15][debug][http] source/common/http/conn_manager_impl.cc:457] [C0][S12312213233427221649]   'connection':'Upgrade'\r\n[2018-06-21 15:00:18.016][15][debug][http] source/common/http/conn_manager_impl.cc:457] [C0][S12312213233427221649]   'sec-websocket-key':'YR+XVsIRUDX8TzvLBFBORg=='\r\n[2018-06-21 15:00:18.016][15][debug][http] source/common/http/conn_manager_impl.cc:457] [C0][S12312213233427221649]   'sec-websocket-version':'13'\r\n[2018-06-21 15:00:18.016][15][debug][http] source/common/http/conn_manager_impl.cc:457] [C0][S12312213233427221649]   ':path':'/ws'\r\n[2018-06-21 15:00:18.016][15][debug][http] source/common/http/conn_manager_impl.cc:457] [C0][S12312213233427221649]   ':method':'GET'\r\n[2018-06-21 15:00:18.016][15][debug][http] source/common/http/conn_manager_impl.cc:570] [C0][S12312213233427221649] found websocket connection. (end_stream=true):\r\n[2018-06-21 15:00:18.016][15][debug][filter] source/extensions/filters/network/tcp_proxy/tcp_proxy.cc:173] [C0] new tcp proxy session\r\n[2018-06-21 15:00:18.017][15][debug][http] source/common/http/conn_manager_impl.cc:972] [C0][S12312213233427221649] encoding headers via codec (end_stream=true):\r\n[2018-06-21 15:00:18.017][15][debug][http] source/common/http/conn_manager_impl.cc:976] [C0][S12312213233427221649]   ':status':'503'\r\n[2018-06-21 15:00:18.017][15][debug][http] source/common/http/conn_manager_impl.cc:976] [C0][S12312213233427221649]   'date':'Thu, 21 Jun 2018 15:00:17 GMT'\r\n[2018-06-21 15:00:18.017][15][debug][http] source/common/http/conn_manager_impl.cc:976] [C0][S12312213233427221649]   'server':'envoy'\r\nACCESS [2018-06-21T15:00:18.013Z] \"GET /ws HTTP/1.1\" 503 - 0 0 3 - \"-\" \"Python/3.6 websockets/5.0.1\" \"cb8dba3d-1ade-4f05-b587-646ade290a51\" \"127.0.0.1:33333\" \"-\"\r\n```\r\n\r\nUsing a single `cluster` instead of `weighted_clusters` works fine. Reading over the documentation, I don't know if the TCP proxy ever fully supported `weighted_clusters` or not, but I do know that at least the degenerate case used here used to function. (I can test with multiple weighted clusters later today or tomorrow.) What's the intention here?\r\n\r\nIn any case, if `weighted_clusters` simply doesn't work, presumably the config should be rejected rather than silently failing.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3684/comments",
    "author": "kflynn",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2019-08-20T19:29:29Z",
        "body": "Given the old websocket path is deprecated, and I believe this works correctly with the new path, I'm going to claim this is fixed. "
      }
    ]
  },
  {
    "number": 3621,
    "title": "Feature request: statistics on request and response body size",
    "created_at": "2018-06-13T17:28:44Z",
    "closed_at": "2020-07-21T22:33:01Z",
    "labels": [
      "enhancement",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3621",
    "body": "As far as I can tell, envoy does not output request and response body sizes in bytes as separate histogram metrics.\r\n\r\nWould be nice if it output those metrics the same way it does downstream_rq_time and upstream_rq_time.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3621/comments",
    "author": "ikatson",
    "comments": [
      {
        "user": "zyfjeff",
        "created_at": "2018-09-10T07:01:16Z",
        "body": "Is this issue  no one is working?  If yes then i can start this one"
      }
    ]
  },
  {
    "number": 3566,
    "title": "STRICT_DNS cluster with empty hosts prevents envoy initialization from completing",
    "created_at": "2018-06-07T00:11:58Z",
    "closed_at": "2020-04-10T12:21:21Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3566",
    "body": "*Description*:\r\nEnvoy does not complete startup if there is a STRICT_DNS cluster with an empty hosts array. It never gets as far as opening listeners.\r\n\r\n*Repro steps*:\r\nStart envoy configured to load the following cluster via CDS:\r\n```\r\n  - \"@type\": type.googleapis.com/envoy.api.v2.Cluster\r\n    name: \"the_cluster\"\r\n    connect_timeout: 2s\r\n    lb_policy: RANDOM\r\n    type: STRICT_DNS\r\n    hosts:  []\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3566/comments",
    "author": "ggreenway",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2018-06-07T00:13:01Z",
        "body": "The cause is that parent_onPreInitComplete() is called when the first DNS resolution completes for the cluster. But if there are no resolutions to perform, this is never called."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-06-07T04:05:10Z",
        "body": "IMO we should just treat zero hosts for a strict cluster as a config error."
      },
      {
        "user": "ggreenway",
        "created_at": "2018-06-07T16:09:09Z",
        "body": "> IMO we should just treat zero hosts for a strict cluster as a config error.\r\n\r\nOn the other hand, it's valid for static clusters to have empty hosts. I ran into this due to a machine-generated config, and I didn't notice the empty hosts.\r\n\r\nOn a related note, it was painful to debug why envoy was stuck during startup. To increase visibility/debugability, I was thinking of adding a timer and printing a log message every 5s naming which clusters are still blocking init. Thoughts?"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-06-07T16:18:14Z",
        "body": "> On the other hand, it's valid for static clusters to have empty hosts. I ran into this due to a machine-generated config, and I didn't notice the empty hosts.\r\n\r\nSorry, I don't completely follow. Shouldn't that be a config error also if it's not?\r\n\r\n> On a related note, it was painful to debug why envoy was stuck during startup. To increase visibility/debugability, I was thinking of adding a timer and printing a log message every 5s naming which clusters are still blocking init. Thoughts?\r\n\r\nSure, that sounds useful. Though, I feel like we should do it more generically as part of the init manager so we can output other blocks as well around RDS, etc.?"
      },
      {
        "user": "ggreenway",
        "created_at": "2018-06-07T16:26:02Z",
        "body": "> Sorry, I don't completely follow. Shouldn't that be a config error also if it's not?\r\n\r\nIt's not now, so that would be a breaking change to make it invalid. Also, I've found it useful to allow an empty hosts array.\r\n\r\n> Sure, that sounds useful. Though, I feel like we should do it more generically as part of the init manager so we can output other blocks as well around RDS, etc.?\r\n\r\nYes, that makes sense."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-06-07T16:29:11Z",
        "body": "> Also, I've found it useful to allow an empty hosts array.\r\n\r\nGot it. OK. Out of curiosity why? If you want to allow empty hosts that's fine, we can just fix the bug."
      },
      {
        "user": "ggreenway",
        "created_at": "2018-06-07T16:40:23Z",
        "body": "> Out of curiosity why?\r\n\r\nOne fewer conditional in generating the config. When I'm deciding which clusters to create, I don't have to consider whether they have any hosts in them. I only have to consider which hosts there are when filling in the hosts-array.\r\n\r\nIt wouldn't be hard to add the other conditional. And if we didn't already allow empty hosts for static clusters, I'd say let's just disallow it. But I like consistency."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-06-07T16:42:22Z",
        "body": "OK. I'm fine either way, we can just fix the bug."
      },
      {
        "user": "futangw",
        "created_at": "2018-10-23T13:30:16Z",
        "body": "@mattklein123 \r\nIf the CDS file without host (not empty host), the envoy would memory leak. i'm using envoy 1.6.\r\nexample cds file:\r\nversion_info: \"0\"\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.api.v2.Cluster\r\n  name: grpc1\r\n  connect_timeout: { nanos: 250000000 }\r\n  type: STRICT_DNS\r\n  lb_policy: ROUND_ROBIN\r\n\r\n- \"@type\": type.googleapis.com/envoy.api.v2.Cluster\r\n  name: grpc2\r\n  connect_timeout: { nanos: 250000000 }\r\n  type: STRICT_DNS\r\n  lb_policy: ROUND_ROBIN\r\n\r\nHere's a piece of dump of envoy process every 1 minute, the memory keeps increasing.\r\n\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n\r\nTue Oct 23 20:53:24 UTC 2018\r\nroot       603  0.1  0.4  93320 36840 ?        Sl   17:33   0:14 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 20:54:24 UTC 2018\r\nroot       603  0.1  0.4  94344 37104 ?        Sl   17:33   0:14 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 20:55:24 UTC 2018\r\nroot       603  0.1  0.4  94344 37104 ?        Sl   17:33   0:14 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 20:56:24 UTC 2018\r\nroot       603  0.1  0.4  94344 37368 ?        Sl   17:33   0:14 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 20:57:24 UTC 2018\r\nroot       603  0.1  0.4  94344 37368 ?        Sl   17:33   0:14 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 20:58:24 UTC 2018\r\nroot       603  0.1  0.4  94344 37368 ?        Sl   17:33   0:14 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 20:59:24 UTC 2018\r\nroot       603  0.1  0.4  94344 37632 ?        Sl   17:33   0:14 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 21:00:24 UTC 2018\r\nroot       603  0.1  0.4  94344 37632 ?        Sl   17:33   0:14 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 21:01:24 UTC 2018\r\nroot       603  0.1  0.4  94344 37632 ?        Sl   17:33   0:15 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 21:02:24 UTC 2018\r\nroot       603  0.1  0.4  94344 37896 ?        Sl   17:33   0:15 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 21:03:24 UTC 2018\r\nroot       603  0.1  0.4  94344 37896 ?        Sl   17:33   0:15 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 21:04:24 UTC 2018\r\nroot       603  0.1  0.4  95368 38160 ?        Sl   17:33   0:15 envoy -c tas-core.json --service-cluster test --service-node test1\r\nTue Oct 23 21:05:24 UTC 2018\r\nroot       603  0.1  0.4  95368 38160 ?        Sl   17:33   0:15 envoy -c tas-core.json --service-cluster test --service-node test1\r\n\r\n\r\nwhen I set the log level to debug, it is rolling:\r\n[2018-10-23 23:17:50.017][437][debug][file] source/common/filesystem/inotify/watcher_impl.cc:73] notification: fd: 1 mask: 2 file: nohup.out\r\n[2018-10-23 23:17:50.017][437][debug][file] source/common/filesystem/inotify/watcher_impl.cc:73] notification: fd: 1 mask: 2 file: nohup.out\r\n[2018-10-23 23:17:50.017][437][debug][file] source/common/filesystem/inotify/watcher_impl.cc:73] notification: fd: 1 mask: 2 file: nohup.out\r\n[2018-10-23 23:17:50.017][437][debug][file] source/common/filesystem/inotify/watcher_impl.cc:73] notification: fd: 1 mask: 2 file: nohup.out\r\n[2018-10-23 23:17:50.017][437][debug][file] source/common/filesystem/inotify/watcher_impl.cc:73] notification: fd: 1 mask: 2 file: nohup.out\r\n[2018-10-23 23:17:50.017][437][debug][file] source/common/filesystem/inotify/watcher_impl.cc:73] notification: fd: 1 mask: 2 file: nohup.out\r\n[2018-10-23 23:17:50.017][437][debug][file] source/common/filesystem/inotify/watcher_impl.cc:73] notification: fd: 1 mask: 2 file: nohup.out\r\n[2018-10-23 23:17:50.017][437][debug][file] source/common/filesystem/inotify/watcher_impl.cc:73] notification: fd: 1 mask: 2 file: nohup.out\r\n[2018-10-23 23:17:50.017][437][debug][file] source/common/filesystem/inotify/watcher_impl.cc:73] notification: fd: 1 mask: 2 file: nohup.out\r\n[2018-10-23 23:17:50.017][437][debug][file] source/common/filesystem/inotify/watcher_impl.cc:73] notification: fd: 1 mask: 2 file: nohup.out\r\n\r\nnote: I run the envoy with 'nohup'\r\n\r\nbtw, looks if set the 'type' to 'static' instead of 'strict_dns', there's no memory leak.\r\n"
      },
      {
        "user": "futangw",
        "created_at": "2018-10-25T02:32:30Z",
        "body": "@ggreenway @mattklein123 I had confirmed even with 'hosts: []' in cds file, the memory leak also exists."
      },
      {
        "user": "whybeyoung",
        "created_at": "2018-10-26T02:25:20Z",
        "body": "when cds return  hosts:[] , then lds will fail wihtout any infomation， envoy will not call lds api anymore....."
      },
      {
        "user": "WIgor",
        "created_at": "2018-11-19T16:09:46Z",
        "body": "I've caught almost the same problem. Empty hosts leads to partial configuration (only clusters were loaded), no listeners and routes. \r\n\r\nErrors in log about misconfiguration would be great."
      }
    ]
  },
  {
    "number": 3529,
    "title": "Address::Instance equality implemented through string compare",
    "created_at": "2018-06-03T19:54:18Z",
    "closed_at": "2018-06-12T18:39:56Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3529",
    "body": "**Issue Template**\r\n\r\n*Title*: Address::Instance equality implemented through string compare\r\n\r\n*Description*:\r\nThe currently implementations of Address::Instance (Ipv4instance, Ipv6Instance, and PipeInstance) all inherit from InstanceBase, which implements operator== as:\r\n\r\n  bool operator==(const Instance& rhs) const override { return asString() == rhs.asString(); }\r\n\r\nThis works, but is a) dependent on the derived classes having a 1-1 mapping between string representations and unique instances, and b) not a very efficient method of computing equality for Ipv4 and Ipv6 addresses.  If we implement any algorithms that need to scale and compare Instances frequently, the string compare cost could be significant.\r\n\r\nUnfortunately, the current method has the advantage of working (modulo (a) above) on all derived classes of Address::Instance, current&future, core envoy and downstream specializations as it is defined in terms of methods on Instance.  Individual derived types could use RTTI to determine if the type being compared to was a known type, and implement the comparison in a more efficient fashion if it was.  If it is desirable that RTTI be avoided, a visitor design pattern could be implemented on the Instance to do double dispatch on the types of the two objects being compared.   However, this involves a non-trivial change to the interface of the abstract base class.\r\n\r\n@htuch : I hadn't figured out when we were talking what the implications of \"We should do this upstream\" were.  Given the above paragraph, do you have any prefereces?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3529/comments",
    "author": "curiouserrandy",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2018-06-04T01:29:21Z",
        "body": "I think it would be reasonable to `dynamic_cast` to the specific type and then make use of internal representation for an efficient comparison, i.e. RTTI. Equality is actually the \"case for RTTI\" in the style guide that provides the argument in the affirmative :)"
      }
    ]
  },
  {
    "number": 3508,
    "title": "stats: when hot-restart is disabled, each stat still consumes maxNameLength() bytes for the name.",
    "created_at": "2018-05-30T22:20:07Z",
    "closed_at": "2018-07-13T18:59:31Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3508",
    "body": "*Description*:\r\n>When storing stats in shared memory for hot-restart, we preallocate max_stats * maxNameLength for string storage. But this means you have to make hard up-front decisions about how much memory you can spend.  However if we disable hot-restart, there's no compelling reason to pad each stat, so you could just set maxNameLength pretty high. If that were fixed, we wouldn't have to tune maxNameLength so carefully.\r\n\r\n@ambuc may wind up looking at this as he was the last one to dive into the not-hot-restart case.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3508/comments",
    "author": "jmarantz",
    "comments": [
      {
        "user": "jmarantz",
        "created_at": "2018-05-31T12:32:46Z",
        "body": "More thoughts on this in light of the #3506 which forced me to look at the stats code again and realize this problem:\r\n\r\nI think a solution to this problem that will improve the maintainability of the codebase is to remove the following methods & data from Stats::RawStatsData\r\n - `configure(options)`\r\n - `configureForTestsOnly(options)`\r\n - `maxNameLength()`\r\n - `maxObjNameLength()`\r\n - `nameSize()`\r\n - `static initializeAndGetMutableMaxObjNameLength()`\r\nand replace them all with:\r\n - `static calculateRequiredSize(size_t name_length)`\r\n\r\nIn the context of a hot-restart allocation, the `calculateRequiredSize` will be called with `options_->maxObjNameLength().`  In the case of a non-hot-restart allocation, it will be called with `actual_name.size()`.  Then you can remove a whole bunch of hacks in the test dealing with the static underneath initializeAndGetMutableMaxObjNameLength."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-31T19:39:13Z",
        "body": "@jmarantz big +1 on killing the statics. I hate them also."
      }
    ]
  },
  {
    "number": 3442,
    "title": "Inconsistent behavior between Base64::decode() and Hex::decode().",
    "created_at": "2018-05-19T06:41:28Z",
    "closed_at": "2018-05-23T19:41:42Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3442",
    "body": "`Base64::decode()` returns `EMPTY_STRING` when input is wrong.\r\n`Hex::decode()` throws exception when input is wrong.\r\n\r\nBoth should behave the same. Any preference which behavior is adopted?\r\n\r\nI have slight preference for returning `std::vector<uint8_t>{}` from `Hex::decode()`.\r\n\r\ncc @mattklein123 ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3442/comments",
    "author": "PiotrSikora",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-05-21T01:00:08Z",
        "body": "@PiotrSikora no real preference. I would look at the call sites and just see which type of error handling is cleaner."
      }
    ]
  },
  {
    "number": 3335,
    "title": "Ability to format the outlier event log entry",
    "created_at": "2018-05-09T22:10:41Z",
    "closed_at": "2018-05-10T17:59:19Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3335",
    "body": "*Title*: *Ability to custom format the outlier event log entry by configuration*\r\n\r\n*Description*:\r\nPlease provide a means to configure a format for outlier event log entries, much the same way of access logs. E.g.,\r\n```\r\ncluster_manager:\r\n  outlier_detection:\r\n    event_log_path: /var/log/envoy/outlier-event.log\r\n    format: [log entry format_ similar to file access log configuration]\r\n```\r\nIn our use case, we consolidate all envoy logs to a unified format and transmit to a central location for further processing. Currently we tail the outlier event log, translate it, and write out a new log in a new format. It would be great if the event log records are immediately consumable.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3335/comments",
    "author": "charliezgc",
    "comments": [
      {
        "user": "jsedgwick",
        "created_at": "2018-05-10T14:58:21Z",
        "body": "We can convert outlier logging to proto as discussed in #2028 and as implemented for active healthcheck logging in #3176. It will not be much work after that PR is merged. Would having the log consumable as proto help you? I don't know if we want to add custom formats given that the future is proto streams. WDYT @danielhochman ?"
      },
      {
        "user": "ggreenway",
        "created_at": "2018-05-10T15:53:09Z",
        "body": "> the future is proto streams\r\n\r\nI disagree with that assertion. I plan to use text-based logging exclusively."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-10T15:55:43Z",
        "body": "Correct. We will always support both text/proto, though the text JSON may be driven by protos. However, this issue is asking for fully customizable formatting like we have for access logs. This is fine from a feature perspective but someone from the community will need to contribute it."
      },
      {
        "user": "jsedgwick",
        "created_at": "2018-05-10T17:37:21Z",
        "body": "Yeah, to clarify, I was asking whether structured proto output in text based json or otherwise would help OP enough (ie by making it simpler to consume). Whether to add custom formats on top i agree with and defer to @mattklein123"
      },
      {
        "user": "charliezgc",
        "created_at": "2018-05-10T17:59:19Z",
        "body": "In that case, we may be able to code the custom format feature and make a PR in the future. We are not using proto at the moment so any new development on the logging would not be immediately useful."
      }
    ]
  },
  {
    "number": 3250,
    "title": "Internal redirect in filter chains",
    "created_at": "2018-04-27T21:56:47Z",
    "closed_at": "2019-01-21T16:58:57Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3250",
    "body": "*Title*: Support re-routing within the proxy\r\n\r\nHTTP filters are allowed to modify HTTP requests in progress. Sometimes, that leads to changing the routing decision (which is known prior to filter execution). Currently, it feels a bit ad-hoc to insert \"clear routing table\" call since it is not clear how that works with 1) multiple filters trying to clear routing tables 2) per-route config that can change mid-filter chain. I think there are three legitimate cases for this kind of behavior:\r\n\r\n1. filters change HTTP requests with no change in routing;\r\n2. filters change HTTP requests, and expect the entire chain of filters to be re-applied including the router (this is a natural expectation since it's logically two proxies, stacked together). Some checks need to be applied to prevent infinite loops. Side effects might happen during filter re-evaluation.\r\n3. filters change HTTP requests, and only the router filter gets re-applied at the end. Filter configuration is not re-evaluated using the new route. We need to clarify what happens to per-route filter configs.\r\n\r\nI think 1) and 3) are currently implementable in filters. Is there any support for case 2)? Does option 2) make more sense than 3)?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3250/comments",
    "author": "kyessenov",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-04-27T22:00:30Z",
        "body": "@kyessenov, @alyssawilk was asking me about this earlier in the week. I think she is going to implement 2. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-06-19T09:11:01Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-06-19T16:52:17Z",
        "body": "FWIW it's pretty far out on our requirements horizon. Worth keeping open in the interim though."
      },
      {
        "user": "kyessenov",
        "created_at": "2018-06-19T17:19:08Z",
        "body": "Thanks. Agree, that it's worth waiting for more use-cases from filter developers."
      },
      {
        "user": "kyessenov",
        "created_at": "2018-10-23T21:38:41Z",
        "body": "Does anyone know if there's been any progress on this issue?"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-23T22:03:43Z",
        "body": "I talked to @alyssawilk about this recently and I think we roughly sketched out an implementation. Not sure of timelines but it sounds like someone from Google might implement this."
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-10-24T15:39:56Z",
        "body": "Well my question for Matt had been if we could replace 1 with 2 (in the context of wanting custom filter chains, and custom filter chains being 'dangerous' with 1 since you'd not get the new filter chain) and the answer I got was 'probably not' since some folks probably actually want the preexisting behavior, so we'll have to document to not use (theoretical) custom filter chains with style-1 route changes.\r\n\r\nI think the IR case I'm more interested in implementing is actually getting an x-envoy-internal-redirect: scheme://new.domain/new_path from upstream, then re-sending the modified request to a new upstream.  I think the work needed to do that has a ton of overlap with 2 and I hope to have the time to tackle that in the next week or two."
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-10-29T20:56:21Z",
        "body": "Ok, first in what is I'm sure a very long series of design choices here, do we recreate the filter chain or rerun?\r\n\r\nI'm inclined to recreate.  Where I suspect we will eventually want to allow a given filter to pass information from the first pass to the second pass, I don't think folks are going to correctly make every filter redirect-aware, and so I think creating and re-running the chain \"from scratch\" (data passing tdb when/if there's a request) is going to result in far fewer (though probably not non-zero) bugs than trying to get every filter to clear sufficient state that a second pass won't be harmful."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-29T21:07:47Z",
        "body": "+1 recreate. "
      },
      {
        "user": "kyessenov",
        "created_at": "2018-11-07T21:48:36Z",
        "body": "What would be the recommendation for network filter chain re-run? I'm thinking of two HTTP connection managers on ports 80/443, and an internal redirect in 80 triggering the HTTP filter chain in 443. Should we restrict this functionality to a single HTTP connection manager?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-11-07T22:02:25Z",
        "body": "Yeah, I don't think cross-scheme redirects should be allowed, due to security concerns.  I mean I guess going from a secure to an insecure response isn't terrible, but serving an https response on an http connection is obviously unsafe and it's easier to be consistent"
      },
      {
        "user": "kyessenov",
        "created_at": "2018-11-07T22:08:29Z",
        "body": "Right, I didn't think through security implications. I'm more interested in plain http-to-http transition. Istio buckets RDS routes by capture ports, and I'm trying to find a way to jump between those RDS tables."
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-11-08T14:04:02Z",
        "body": "Ah, interesting.  We've traditionally done multi-tenant support via VIP rather than port, so can have one (or two) giant listeners and everything in the scope of that.  We have to adapt Envoy to allow per-vip matching there (see scoped rds over on #4704) and it's possible once we have that landed we could add support for some or all listeners sharing scoped rather than duplicating the scopes for all listeners.  "
      },
      {
        "user": "mattklein123",
        "created_at": "2019-01-21T16:58:57Z",
        "body": "I think this is complete so closing."
      }
    ]
  },
  {
    "number": 3222,
    "title": "Make priority/locality overprovision factor configurable",
    "created_at": "2018-04-26T13:05:01Z",
    "closed_at": "2018-11-16T18:30:03Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3222",
    "body": "*Title*: *Make priority/locality overprovision factor configurable*\r\n\r\n*Description*:\r\nCurrently the priority/locality weighting load balancer will spill over traffic from one priority or locality to another if the first choice one is partially unhealthy, according to a fixed overprovision factor of 1.4. This is an awesome feature! However, since I'm migrating from a legacy traffic shaping system (which doesn't have this feature) to Envoy, I'd like to start out with this feature disabled so it matches current expectations",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3222/comments",
    "author": "mpuncel",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-04-26T16:06:13Z",
        "body": "Agree it would be great to make this configurable. cc @htuch @alyssawilk "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-11-16T18:30:03Z",
        "body": "This is done."
      }
    ]
  },
  {
    "number": 2901,
    "title": "[v1.6.0 deprecation] Remove features marked deprecated in #2346",
    "created_at": "2018-03-26T21:23:56Z",
    "closed_at": "2019-07-08T16:54:13Z",
    "labels": [
      "tech debt",
      "help wanted",
      "deprecation"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2901",
    "body": "#2346 (Listener: Add listener filters.) introduced a deprecation notice for v1.6.0. This issue will track source code cleanup.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2901/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2018-03-26T21:26:16Z",
        "body": "@jrajahalme this one is for you. I sent you an invite to the envoyproxy org, if you can join that I can assign in the GH UI. Thanks."
      },
      {
        "user": "jrajahalme",
        "created_at": "2018-03-27T19:09:33Z",
        "body": "This depends on #1280 being implemented first."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-07-18T19:20:12Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-07-26T10:02:28Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-20T16:14:33Z",
        "body": "@htuch @PiotrSikora @jrajahalme what's the status of this one?"
      },
      {
        "user": "jrajahalme",
        "created_at": "2019-03-20T19:49:51Z",
        "body": "@mattklein123 `use_original_dst` is still in the v2 API, but marked deprecated. Depending on the status of the v1 API it can be either removed, or moved into `deperecated_v1` so that v1 API can still be supported."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-03-20T22:23:52Z",
        "body": "@jrajahalme v1 API is dead, so we can just remove it. Is the idea here that anyone using this should manually configure the listener filter? I'm mostly just trying to understand what this is tracking. :)"
      },
      {
        "user": "jrajahalme",
        "created_at": "2019-03-28T21:41:38Z",
        "body": "@mattklein123 Yes, the plan was to remove the use_original_dst flag and require that the appropriate listener filter be configured instead."
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-07-08T16:54:13Z",
        "body": "Closing - per #6271 we can reopen once we cut the new API version."
      }
    ]
  },
  {
    "number": 2881,
    "title": "--mode validate does not support cluster configs with custom dns_resolvers",
    "created_at": "2018-03-22T23:01:17Z",
    "closed_at": "2018-05-25T22:22:52Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2881",
    "body": "\r\n`--mode validate` does not support configs with custom dns_resolvers\r\n\r\n*Description*:\r\nRunning `envoy --mode validate` on a config with custom `dns_resolvers` fails with ` panic: not implemented`\r\n\r\n*Repro steps*:\r\nRun `envoy --mode validate -c ./example.yaml --v2-config-only`  with the below yaml.\r\n on `version: 7241be40a3e1cdb450a8a1feb4f468dd3e695f00/1.7.0-dev/Clean/DEBUG`\r\n\r\n*Abbreviated Config*:\r\n```\r\n---\r\nadmin:\r\n  access_log_path: /var/log/envoy/admin_access.log\r\n  address:\r\n    socket_address: { address: 127.0.0.1, port_value: 9901 }\r\n\r\nstatic_resources:\r\n  clusters:\r\n    - name: xds_service\r\n      connect_timeout: 5s\r\n      type: STRICT_DNS\r\n      lb_policy: ROUND_ROBIN\r\n      http2_protocol_options: {}\r\n      circuit_breakers: {}\r\n      outlier_detection: {}\r\n      health_checks:\r\n      - timeout: 10s\r\n        interval: 15s\r\n        unhealthy_threshold: 2\r\n        healthy_threshold: 2\r\n        grpc_health_check: { service_name: xds }\r\n      hosts: [socket_address: {address: xds.service.us-west-2.consul, port_value: 4411}]\r\n      dns_resolvers: [socket_address: {address: 127.0.0.1, port_value: 8600}] # Use consul DNS server\r\n```\r\n\r\n*Call Stack*:\r\n```\r\n$ bazel-bin/source/exe/envoy-static --mode validate -c ./example.yaml --v2-config-only\r\n[2018-03-22 15:55:32.245][252501][critical][assert] source/server/config_validation/dispatcher.cc:16] panic: not implemented\r\n[2018-03-22 15:55:32.245][252501][critical][backtrace] bazel-out/darwin-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] Caught Abort trap: 6, suspect faulting address 0x7fffd1710d42\r\n[2018-03-22 15:55:32.247][252501][critical][backtrace] bazel-out/darwin-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:87] Backtrace obj<envoy-static> thr<0>:\r\n[2018-03-22 15:55:32.247][252501][critical][backtrace] bazel-out/darwin-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:99] thr<0> obj<envoy-static                        0x0000000104568da6 _ZN8backward7details6unwindINS_14StackTraceImplINS_10system_tag10darwin_tagEE8callbackEEEmT_m>\r\n[2018-03-22 15:55:32.247][252501][critical][backtrace] bazel-out/darwin-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:105] thr<0> #0 0x104568da6:\r\n[2018-03-22 15:55:32.247][252501][critical][backtrace] bazel-out/darwin-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:99] thr<0> obj<envoy-static>\r\n[2018-03-22 15:55:32.247][252501][critical][backtrace] bazel-out/darwin-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:105] thr<0> #1 0x104568935: backward::StackTraceImpl<backward::system_tag::darwin_tag>::load_here(unsigned long) + 101\r\n[2018-03-22 15:55:32.247][252501][critical][backtrace] bazel-out/darwin-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:105] thr<0> #2 0x104568731: backward::StackTraceImpl<backward::system_tag::darwin_tag>::load_from(void*, unsigned long) + 49\r\n[2018-03-22 15:55:32.247][252501][critical][backtrace] bazel-out/darwin-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:105] thr<0> #3 0x104566cfe: Envoy::BackwardsTrace::captureFrom(void*) + 46\r\n[2018-03-22 15:55:32.247][252501][critical][backtrace] bazel-out/darwin-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:105] thr<0> #4 0x104566bbf: Envoy::SignalAction::sigHandler(int, __siginfo*, void*) + 143\r\n[2018-03-22 15:55:32.247][252501][critical][backtrace] bazel-out/darwin-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:110] end backtrace thread 0\r\nfish: 'bazel-bin/source/exe/envoy-stat…' terminated by signal SIGABRT (Abort)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2881/comments",
    "author": "wbertelsen",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2018-03-23T03:23:22Z",
        "body": "That call stack isn't useful, unfortunately. I grabbed one from linux:\r\n\r\n```\r\n[2018-03-22 20:22:00.978][6706][critical][assert] source/server/config_validation/dispatcher.cc:16] panic: not implemented\r\n[2018-03-22 20:22:00.979][6706][critical][backtrace] bazel-out/k8-fastbuild/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] Caught Aborted, suspect faulting address 0x3e800001a32\r\n[2018-03-22 20:22:00.979][6706][critical] Backtrace (most recent call first) from thread 0:\r\n  #1 ?? ??:0\r\n  #2 ?? ??:0\r\n  #3 \r\n  #4 Envoy::Event::ValidationDispatcher::createDnsResolver(std::vector<std::shared_ptr<Envoy::Network::Address::Instance const>, std::allocator<std::shared_ptr<Envoy::Network::Address::Instance const> > > const&) at ??:?\r\n  #5 Envoy::Upstream::ClusterImplBase::create(envoy::api::v2::Cluster const&, Envoy::Upstream::ClusterManager&, Envoy::Stats::Store&, Envoy::ThreadLocal::Instance&, std::shared_ptr<Envoy::Network::DnsResolver>, Envoy::Ssl::ContextManager&, Envoy::Runtime::Loader&, Envoy::Runtime::RandomGenerator&, Envoy::Event::Dispatcher&, Envoy::LocalInfo::LocalInfo const&, std::shared_ptr<Envoy::Upstream::Outlier::EventLogger>, bool) at ??:?\r\n  #6 Envoy::Upstream::ProdClusterManagerFactory::clusterFromProto(envoy::api::v2::Cluster const&, Envoy::Upstream::ClusterManager&, std::shared_ptr<Envoy::Upstream::Outlier::EventLogger>, bool) at ??:?\r\n  #7 Envoy::Upstream::ClusterManagerImpl::loadCluster(envoy::api::v2::Cluster const&, bool, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::unique_ptr<Envoy::Upstream::ClusterManagerImpl::ClusterData, std::default_delete<Envoy::Upstream::ClusterManagerImpl::ClusterData> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::unique_ptr<Envoy::Upstream::ClusterManagerImpl::ClusterData, std::default_delete<Envoy::Upstream::ClusterManagerImpl::ClusterData> > > > >&) at ??:?\r\n  #8 Envoy::Upstream::ClusterManagerImpl::ClusterManagerImpl(envoy::config::bootstrap::v2::Bootstrap const&, Envoy::Upstream::ClusterManagerFactory&, Envoy::Stats::Store&, Envoy::ThreadLocal::Instance&, Envoy::Runtime::Loader&, Envoy::Runtime::RandomGenerator&, Envoy::LocalInfo::LocalInfo const&, Envoy::AccessLog::AccessLogManager&, Envoy::Event::Dispatcher&) at ??:?\r\n  #9 Envoy::Upstream::ValidationClusterManager::ValidationClusterManager(envoy::config::bootstrap::v2::Bootstrap const&, Envoy::Upstream::ClusterManagerFactory&, Envoy::Stats::Store&, Envoy::ThreadLocal::Instance&, Envoy::Runtime::Loader&, Envoy::Runtime::RandomGenerator&, Envoy::LocalInfo::LocalInfo const&, Envoy::AccessLog::AccessLogManager&, Envoy::Event::Dispatcher&) at ??:?\r\n  #10 Envoy::Upstream::ValidationClusterManagerFactory::clusterManagerFromProto(envoy::config::bootstrap::v2::Bootstrap const&, Envoy::Stats::Store&, Envoy::ThreadLocal::Instance&, Envoy::Runtime::Loader&, Envoy::Runtime::RandomGenerator&, Envoy::LocalInfo::LocalInfo const&, Envoy::AccessLog::AccessLogManager&) at ??:?\r\n  #11 Envoy::Server::Configuration::MainImpl::initialize(envoy::config::bootstrap::v2::Bootstrap const&, Envoy::Server::Instance&, Envoy::Upstream::ClusterManagerFactory&) at ??:?\r\n  #12 Envoy::Server::ValidationInstance::initialize(Envoy::Server::Options&, std::shared_ptr<Envoy::Network::Address::Instance const>, Envoy::Server::ComponentFactory&) at ??:?\r\n  #13 Envoy::Server::ValidationInstance::ValidationInstance(Envoy::Server::Options&, std::shared_ptr<Envoy::Network::Address::Instance const>, Envoy::Stats::IsolatedStoreImpl&, Envoy::Thread::BasicLockable&, Envoy::Server::ComponentFactory&) at ??:?\r\n  #14 Envoy::Server::validateConfig(Envoy::Server::Options&, std::shared_ptr<Envoy::Network::Address::Instance const>, Envoy::Server::ComponentFactory&) at ??:?\r\n  #15 Envoy::MainCommonBase::run() at ??:?\r\n  #16 Envoy::MainCommon::run() at ??:?\r\n  #17 main at ??:?\r\n  #18 \r\n  #19 ?? ??:0\r\n  #20 \r\n  #21 _start at ??:?\r\n  #22 \r\nAborted (core dumped)\r\n```"
      },
      {
        "user": "dio",
        "created_at": "2018-03-23T09:28:48Z",
        "body": "@zuercher should it be OK to add\r\n\r\n```cpp\r\nreturn Network::DnsResolverSharedPtr{new Network::DnsResolverImpl(*this, resolvers)};\r\n```\r\n\r\ninside `Network::DnsResolverSharedPtr ValidationDispatcher::createDnsResolver`?"
      },
      {
        "user": "zuercher",
        "created_at": "2018-03-23T17:22:01Z",
        "body": "I'm not very family with this part of the code, but I think we want to prevent any resolution attempts in validation mode. Looks like you might be able to pass it an empty vector, though.\r\n\r\n```\r\nreturn Network::DnsResolverSharedPtr{new Network::DnsResolverImpl(*this, {})};\r\n```\r\n\r\nLooks like @dnoe has done some work with the DnsResolverImpl and may have a better idea. "
      }
    ]
  },
  {
    "number": 2879,
    "title": "perf: reduce the CPU usage of the HTTP access logger",
    "created_at": "2018-03-22T22:35:10Z",
    "closed_at": "2018-05-18T02:48:07Z",
    "labels": [
      "area/perf",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2879",
    "body": "*Description*:\r\nFrom a callgraph profile (`perf record -g`) of an Internet-facing Envoy instance serving HTTP traffic, I noticed that the access logging code is using a surprisingly large amount of CPU time. The logging code may be a good candidate for some performance work.\r\n\r\nHere is the relevant part of the `perf report --sort parent` output, showing the percentage of total non-idle clock cycles spent in function+children:\r\n```\r\n-   76.36%    76.36%  [other]\r\n   - 70.77% start_thread\r\n      - _ZZN5Envoy6Thread6ThreadC4ESt8functionIFvvEEENUlPvE_4_FUNES5_\r\n         - 70.72% Envoy::Server::WorkerImpl::threadRoutine\r\n            - 70.59% event_base_loop\r\n               - 66.76% event_process_active_single_queue.isra.29\r\n                  + 52.38% Envoy::Event::FileEventImpl::assignEvents(unsigned int)::{lambda(int, short, void*)#1}::_FUN\r\n                  - 8.17% Envoy::Event::DispatcherImpl::clearDeferredDeleteList\r\n                     - 7.62% Envoy::Http::ConnectionManagerImpl::ActiveStream::~ActiveStream\r\n                        - 7.31% Envoy::Http::ConnectionManagerImpl::ActiveStream::~ActiveStream\r\n                           - 4.68% Envoy::AccessLog::FileAccessLog::log\r\n                              - 4.24% Envoy::AccessLog::FormatterImpl::format[abi:cxx11]\r\n                                 - 1.72% Envoy::AccessLog::RequestHeaderFormatter::format[abi:cxx11]\r\n                                    - 1.52% Envoy::AccessLog::HeaderFormatter::format[abi:cxx11]\r\n                                         0.94% Envoy::Http::HeaderMapImpl::get\r\n                                 - 1.54% Envoy::AccessLog::RequestInfoFormatter::format[abi:cxx11]\r\n                                    - 1.02% _ZNSt17_Function_handlerIFNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKN5Envoy11RequestInfo11RequestInfoEEZNS6_9Acces\r\n                                         1.00% Envoy::AccessLogDateTimeFormatter::fromTime[abi:cxx11]\r\n                             1.03% std::__cxx11::_List_base<std::unique_ptr<Envoy::Http::ConnectionManagerImpl::ActiveStreamDecoderFilter, std::default_delete<Envoy::Http\r\n                             0.72% Envoy::Http::HeaderMapImpl::~HeaderMapImpl\r\n```\r\n\r\nThe access log format from this Envoy instance was:\r\n```\r\n\"%DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT% %START_TIME% - %REQ(:METHOD)% %REQ(X-FORWARDED-PROTO)%://%REQ(:AUTHORITY)%%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL% s%RESPONSE_CODE% %BYTES_SENT% %DURATION% - - r%REQ(X-PINTEREST-RID)% %RESP(PINTEREST-GENERATED-BY)% %REQ(REFERER)% \\\"%REQ(USER-AGENT)%\\\" - - -\\n\"\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2879/comments",
    "author": "brian-pane",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-05-18T02:48:07Z",
        "body": "Closing this as I believe @brian-pane did some perf work here. We can reopen or do additional PRs if needed."
      }
    ]
  },
  {
    "number": 2782,
    "title": "NO_TRAFFIC_INTERVAL constant causes deployments to needlessly take at least a minute",
    "created_at": "2018-03-13T05:20:12Z",
    "closed_at": "2018-03-16T03:41:11Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2782",
    "body": "We use AWS load balancer in front of envoy, which is used as a sidecar. Envoy proxies all external traffic into our local application.\r\n\r\nWhen the application comes up for the first time, envoy waits 60 seconds between healthcheck attempts because of NO_TRAFFIC_INTERVAL constant, which is used in case there's no external traffic into the cluster. \r\n\r\nHowever, there can be no external traffic at this point, as for it to come, the healthcheck needs to become healthy at least once.\r\n\r\nWhat can we optimize to workaround this? Ideally, we'd like to be able to tune the NO_TRAFFIC_INTERVAL timeout ourselves to disable this behavior.\r\n\r\nHere's the config we use, note the ```envoy.health_check``` http_filter that is the endpoint that is served to AWS load balancer.\r\n\r\n```static_resources:\r\n  clusters:\r\n    name: local_tomcat\r\n    connect_timeout: 0.1s\r\n    health_checks:\r\n    - healthy_threshold: 2\r\n      http_health_check:\r\n        path: \"/app/status\"\r\n      interval: 5s\r\n      timeout: 5s\r\n      unhealthy_threshold: 3\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 8080\r\n    lb_policy: LEAST_REQUEST\r\n    type: STATIC\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 9443\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          http_filters:\r\n            - name: envoy.health_check\r\n              config:\r\n                cluster_min_healthy_percentages:\r\n                  local_tomcat:\r\n                    value: 100\r\n                endpoint: \"/healthz\"\r\n                pass_through_mode: false\r\n            - name: envoy.router\r\n          route_config:\r\n            name: localhost_proxy\r\n            virtual_hosts:\r\n            - domains: \"*\"\r\n              name: localhost_proxy\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: local_tomcat\r\n                  timeout: 60s\r\n        \r\n      tls_context: {...}\r\n    name: localhost_proxy\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2782/comments",
    "author": "ikatson",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-03-13T15:44:45Z",
        "body": "@ikatson when Envoy initializes, it does a single HC pass, regardless of the no traffic interval, which should be able to quickly determine backend health. Can you make sure you are not seeing this behavior? If not there is a bug somewhere."
      },
      {
        "user": "ikatson",
        "created_at": "2018-03-13T17:00:46Z",
        "body": "@mattklein123 we worked around it by starting envoy sidecar ONLY when the application has come up.\r\nSo essentially, the envoy script looks smth like.\r\n\r\n```\r\nuntil application_is_up; do\r\n    sleep 1\r\ndone\r\nexec envoy\r\n```\r\n\r\nSo the issue is when the envoy starts up before the application, the first health check does not succeed, and the next one is only run 60 seconds after, which is too long\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-03-13T18:08:21Z",
        "body": "OK. The NO_TRAFFIC_INTERVAL stuff is kind of a hack. We should probably make it configurable. I'm switching this over to enhancement/help wanted."
      }
    ]
  },
  {
    "number": 2755,
    "title": "Pre-establish upstream connections (connection prefetching)",
    "created_at": "2018-03-08T00:31:50Z",
    "closed_at": "2021-01-13T21:36:20Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/http"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2755",
    "body": "*Title*: *Pre-establish upstream connections so that connections will be warmed up for future requests*\r\n\r\n*Description*:\r\nEnvoy can be deployed in use cases where establishing a new upstream connection is expensive w.r.t time (~300ms RTT). Since requests are typically queued for this duration, results in increased response times.    \r\n\r\n*Proposal*:\r\nPre-establish few upstream connections every time the number of free upstream connections falls below a specific threshold. \r\n\r\nPS: I can work on this if it is reasonable. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2755/comments",
    "author": "bmadhavan",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-03-08T01:11:53Z",
        "body": "Is this for tcp_proxy? HTTP/1 connection pools? \r\n\r\nIn general I don't have any major objection to this feature, though once at steady state I wonder how useful it really is with HTTP connection pools (tcp_proxy is a different issue). \r\n\r\nThe main issue I see with this feature is that it makes it more likely that requests will race with an upstream idle timeout."
      },
      {
        "user": "bmadhavan",
        "created_at": "2018-03-08T02:18:22Z",
        "body": "Our use case is for HTTP/1 connection pools. \r\nI agree with you that during steady state the impact of this will be much less (except for bursts). But will be quite handy during hours of the day when the requests rates either go down or starts ramping up from off peak hours.... "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-03-08T17:09:25Z",
        "body": "@bmadhavan sure SGTM. I would recommend making it configurable in the HTTP1 specific protocol options in the cluster proto and potentially defaulting to disabled."
      },
      {
        "user": "htuch",
        "created_at": "2018-03-08T17:25:36Z",
        "body": "This is also desirable for alternative transports (e.g. transports that need to negotiate sandbox permiters) that have high latency connection establishment."
      },
      {
        "user": "bmadhavan",
        "created_at": "2018-03-09T06:50:12Z",
        "body": "@mattklein123 Thank you. Sure, I will make it configurable & disabled by default. "
      },
      {
        "user": "spenceral",
        "created_at": "2020-01-31T20:25:14Z",
        "body": "I am interested in picking this up."
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-09-17T12:50:30Z",
        "body": "Oh hey, there's an issue for this.\r\nI'm not going to back-tag all the connection pool refactors, but I should have tagged #12973 and #12758 and now I know to close this off when predictive prefetch is done."
      }
    ]
  },
  {
    "number": 2649,
    "title": "//test/exe:main_common_test is broken in IPv6 only environments",
    "created_at": "2018-02-20T21:51:09Z",
    "closed_at": "2018-09-06T23:17:20Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2649",
    "body": "*Description*:\r\nThe recently added //test/exe:main_common_test does not work in IPv6 only environments, because the YAML file used by it uses 0.0.0.0 as a listening address. The test is not parameterized on IP versions in the manner of our other tests.\r\n\r\nIt can be temporarily worked around by no-oping the test when run in a non-IPv4 supporting environment, but it should be fixed properly to run in IPv4 only, IPv6 only, or dual stacked test environments.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2649/comments",
    "author": "dnoe",
    "comments": [
      {
        "user": "jmarantz",
        "created_at": "2018-02-28T19:07:59Z",
        "body": "What's a canonical test that uses a canned .yaml file with embedded port/address constants to copy?\r\n\r\nIs there one where we just pick a .yaml file based on GetParam() with the yaml file harded-coded with the appropriate-syntax ip constants?\r\n\r\nOr is the best mechanism to read a template file with symbolic IP addresses, do substitutions, and write out the yaml file as a temp so Envoy can read it?"
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-02-28T19:11:24Z",
        "body": "There's some utils for mangling but I'd suggest having your config inline, loading it up in the config helper (which already does v4/v6 conversion)\r\n\r\nConfigHelper(const Network::Address::IpVersion version,\r\n               const std::string& config = HTTP_PROXY_CONFIG);\r\n\r\nAnd spitting it out to a file\r\nTestEnvironment::writeStringToFileForTest()\r\n\r\nif at all possible!\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-06-20T04:20:08Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "jmarantz",
        "created_at": "2018-08-08T19:48:42Z",
        "body": "@hennna who may be looking at this.\r\n"
      },
      {
        "user": "akonradi",
        "created_at": "2018-09-06T18:16:16Z",
        "body": "I'll take this if nobody is working on it."
      }
    ]
  },
  {
    "number": 2412,
    "title": "Domain name matches are case-sensitive.",
    "created_at": "2018-01-19T01:58:20Z",
    "closed_at": "2018-01-30T23:47:40Z",
    "labels": [
      "bug",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2412",
    "body": "*Title*: Domain name matches are currently case-sensitive. According to RFC 2616 comparison of hostnames should be case-insensitive.\r\n\r\n*Description*:\r\nI am expecting domain name matches in envoy to be case-insensitive. \r\nBut I observe that if a client sends hostname in caps in its host header, domain match fails.  \r\n\r\n*Repro steps*:\r\ncurl -H “HOST: WWW.ABC.COM” localhost:80/internal/svc  does not work.\r\n\r\nwhile,\r\ncurl -H “HOST: www.abc.com\" localhost:80/internal/svc works.  \r\nwith the following config. \r\n\r\n*Config*:\r\n``` \"filters\": [\r\n        {\r\n          \"type\": \"read\",\r\n          \"name\": \"http_connection_manager\",\r\n          \"config\": {\r\n            \"codec_type\": \"auto\",\r\n            \"stat_prefix\": \"local_http\",\r\n            \"route_config\": {\r\n              \"virtual_hosts\": [\r\n                {\r\n                  \"name\": \"backend\",\r\n                  \"domains\": [“www.abc.com\"],\r\n                  \"routes\": [\r\n                    {\r\n                      \"prefix\": “/internal\",\r\n                      \"cluster\": “backend-service\",\r\n…….  \r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2412/comments",
    "author": "bmadhavan",
    "comments": [
      {
        "user": "bmadhavan",
        "created_at": "2018-01-19T02:09:26Z",
        "body": "I can do it. Can you assign it to me. "
      },
      {
        "user": "brian-pane",
        "created_at": "2018-01-21T03:06:49Z",
        "body": "@zuercher did you mean to assign this one to @bmadhavan?"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-01-21T20:26:19Z",
        "body": "reassigned"
      }
    ]
  },
  {
    "number": 2330,
    "title": "Macros to condition code on log level",
    "created_at": "2018-01-09T15:47:17Z",
    "closed_at": "2018-05-18T02:38:51Z",
    "labels": [
      "enhancement",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2330",
    "body": "It would be helpful to have some macros to condition code execution on log level. E.g. code that generates input variable `foo` to an `ENVOY_LOG(debug, \"{}\", foo)` should not need to execute if we're not at a log level that will emit the log message.\r\n\r\nThere isn't a huge need for this today, but it's a simple warmup exercise for new Envoy contributors.\r\n\r\n@mrice32 ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2330/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "cpakulski",
        "created_at": "2018-02-22T20:05:11Z",
        "body": "I will try to implement such macros. I am new to this project, so it will be good exercise."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-18T02:38:51Z",
        "body": "This is done."
      }
    ]
  },
  {
    "number": 2306,
    "title": "git push checker uses potentially incompatible clang-format",
    "created_at": "2018-01-03T20:14:48Z",
    "closed_at": "2020-11-27T14:14:36Z",
    "labels": [
      "area/build",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2306",
    "body": "*Description*:\r\nWhen you set up bootstrap, a pre-push check is installed which calls whatever clang-format is in the environment, which might not be compatible with the Envoy style.\r\n\r\nWe should be able to override the checking to employ docker or some other mechanism to keep the toolchain under control.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2306/comments",
    "author": "jmarantz",
    "comments": [
      {
        "user": "rishabhkumar296",
        "created_at": "2019-01-07T14:01:10Z",
        "body": "Hi, I would like to work on this issue. It seems that `clang-format-7` is used for pre-push check as mentioned in `tools/check_format.py`. Does the task involve overriding the `clang-format` version using docker or some other mechanism? "
      },
      {
        "user": "mk46",
        "created_at": "2020-11-27T08:38:17Z",
        "body": "I think we should close this now. Since #11222 clang-format-10 is used."
      }
    ]
  },
  {
    "number": 2263,
    "title": "Invalid Prometheus metric names",
    "created_at": "2017-12-22T16:21:20Z",
    "closed_at": "2018-01-04T00:35:27Z",
    "labels": [
      "bug",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2263",
    "body": "With at least one TLS cluster enabled Envoy produces invalid output on the /stats?format=prometheus endpoint, such as this one:\r\n\r\n```\r\n# TYPE envoy_cluster_ssl_ciphers_ECDHE-RSA-AES128-GCM-SHA256 counter\r\nenvoy_cluster_ssl_ciphers_ECDHE-RSA-AES128-GCM-SHA256{envoy_cluster_name=\"local_service\"} 8\r\n```\r\n\r\nDashes are not allowed. The cipher suite should be put in a label instead, e.g.\r\n```\r\n# TYPE envoy_cluster_ssl_ciphers_ECDHE-RSA-AES128-GCM-SHA256 counter\r\nenvoy_cluster_ssl_ciphers_used_total{envoy_cluster_name=\"local_service\", cipher_suite=\"ECDHE-RSA-AES128-GCM-SHA256\"} 8\r\n```\r\n\r\n*Repro steps*:\r\n- launch Envoy with config below\r\n- send at least one request to the L1 listener: `curl -I localhost:8080`\r\n- request /stats?format=prometheus on the admin listener: `curl -s localhost:5000/stats?format=prometheus | grep ciphers`\r\n\r\n*Config*:\r\n\r\n```yaml\r\nadmin:\r\n  access_log_path: /dev/null\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 5000 }\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: L1\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 8080 }\r\n    filter_chains:\r\n     - filters:\r\n       - name: envoy.http_connection_manager\r\n         config:\r\n           stat_prefix: egress_http\r\n           codec_type: AUTO\r\n           http_filters:\r\n           - name: envoy.router\r\n           route_config:\r\n             virtual_hosts:\r\n             - name: google\r\n               domains: ['*']\r\n               routes: [{ match: { prefix: / }, route: { cluster: \"google\" } }]\r\n\r\n  clusters:\r\n  - name: google\r\n    connect_timeout: 1s\r\n    type: STRICT_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: ROUND_ROBIN\r\n    tls_context: {}\r\n    hosts:\r\n    - socket_address: { address: www.google.com, port_value: 443 }\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2263/comments",
    "author": "pschultz",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-01-04T00:35:27Z",
        "body": "Fixed"
      }
    ]
  },
  {
    "number": 2184,
    "title": "Use CDS metadata to populate headers via the UPSTREAM_METADATA variable",
    "created_at": "2017-12-11T18:45:31Z",
    "closed_at": "2018-08-02T16:04:47Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2184",
    "body": "In addition to EDS endpoint metadata, there is CDS cluster metadata. The `UPSTREAM_METADATA` variable (see #2179) should check both locations for metadata, preferring EDS metadata over CDS metadata when both have matching keys. See also discussion in envoyproxy/data-plane-api#335.\r\n\r\nAt a high level, this requires plumbing the CDS metadata through from the API (probably via `Upstream::ClusterInfo` and its implementation) and then modifying the formatter in `router/header_formatter.cc`.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2184/comments",
    "author": "zuercher",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-08-02T16:04:47Z",
        "body": "I think this is done."
      }
    ]
  },
  {
    "number": 2167,
    "title": "[ASK] Envoy Cors Configuration Sample",
    "created_at": "2017-12-07T01:42:10Z",
    "closed_at": "2018-12-09T17:21:16Z",
    "labels": [
      "area/docs",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2167",
    "body": "@mattklein123  Is there any example how to set cors enabled in route config? I can't find valid example out there.\r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2167/comments",
    "author": "syukur91",
    "comments": [
      {
        "user": "codesuki",
        "created_at": "2018-02-20T00:25:32Z",
        "body": "Just found this issue by accident. \r\nMaybe I find the time to add an example in the near future. For now maybe this helps? I am using grpc-web here, too.\r\nThe CORS settings are set for all routes and on the `/serviceB` route it is specifically disabled.\r\n```\r\nlisteners:\r\n- address: tcp://0.0.0.0:443\r\n  filters:\r\n  - type: read\r\n    name: http_connection_manager\r\n    config:\r\n      access_log:\r\n      - path: /dev/stdout\r\n      codec_type: auto\r\n      stat_prefix: ingress_http\r\n      route_config:\r\n        virtual_hosts:\r\n        - name: gateway\r\n          domains:\r\n          - \"*\"\r\n          cors:\r\n            allow_origin: [\"*\"]\r\n            allow_headers: \"content-type, x-grpc-web\"\r\n            allow_methods: \"POST\"\r\n          routes:\r\n          - timeout_ms: 0\r\n            prefix: /serviceA\r\n            cluster: serviceA\r\n          - timeout_ms: 0\r\n            prefix: /serviceB\r\n            cluster: experiment\r\n            cors:\r\n              enabled: false\r\n      filters:\r\n      - type: both\r\n        name: cors\r\n        config: {}\r\n     - type: both\r\n        name: grpc_web\r\n        config: {}\r\n      - type: decoder\r\n        name: router\r\n        config: {}\r\nadmin:\r\n  access_log_path: /dev/stdout\r\n  address: tcp://0.0.0.0:8001\r\ncluster_manager:\r\n  clusters:\r\n  - name: featureA\r\n    features: http2\r\n    connect_timeout_ms: 250\r\n    type: strict_dns\r\n    lb_type: round_robin\r\n    hosts:\r\n    - url: tcp://featureA\r\n  - name: featureB\r\n    features: http2\r\n    connect_timeout_ms: 250\r\n    type: strict_dns\r\n    lb_type: round_robin\r\n    hosts:\r\n    - url: tcp://featureB\r\n```"
      },
      {
        "user": "AzharMobeen",
        "created_at": "2020-04-13T06:24:39Z",
        "body": "Is it possible to disable cors for specific method ? As you achieved for specific route/serviceB is disabled"
      }
    ]
  },
  {
    "number": 1808,
    "title": "Allow inject-able Singletons for test",
    "created_at": "2017-10-04T14:05:23Z",
    "closed_at": "2017-11-20T18:53:09Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1808",
    "body": "As disused briefly in #1802 if we need to mock out more system calls it'd probably be worthwhile instead of plumbing the OsSysCallsImpl through all relevant class hierarchies to have it be a singleton and have the Envoy Singleton class allow injection.  Then for tests we could replace the static instance with a test-only  instance which could do custom things like mock out bind calls and it would also allow testing corner cases with non-fatal syscall failure.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1808/comments",
    "author": "alyssawilk",
    "comments": [
      {
        "user": "akonradi",
        "created_at": "2017-10-13T13:42:31Z",
        "body": "It would be nice to do something similar for the static factory registries as well."
      }
    ]
  },
  {
    "number": 1796,
    "title": "Support multi-level locality aware routing",
    "created_at": "2017-10-02T22:30:11Z",
    "closed_at": "2019-09-03T15:45:32Z",
    "labels": [
      "help wanted",
      "api/v4"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1796",
    "body": "Now that Envoy is aware of hierarchical locality (region, zone, sub_zone), it may be useful to be able to perform LB in a region, zone and sub_zone aware fashion. The current implementation groups these into a single flat locality identifier, so is effectively concatenating the locality names across levels.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1796/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-08-29T17:36:30Z",
        "body": "@rshriram @mattklein123 is there a final direction we decided on taking on this? In any case, I don't see any major changes around this until v4."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-09-01T19:48:29Z",
        "body": "@htuch I haven't thought about this in a while. TBH I think we should just close this until someone asks for it explicitly but up to you."
      },
      {
        "user": "htuch",
        "created_at": "2019-09-03T15:45:32Z",
        "body": "Ack, closing it out for now, will reopen if anyone wants this."
      }
    ]
  },
  {
    "number": 1683,
    "title": "OSX Build: md5sum and /proc/cpuinfo needed",
    "created_at": "2017-09-16T07:11:04Z",
    "closed_at": "2017-09-17T17:43:35Z",
    "labels": [
      "area/build",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1683",
    "body": "Followed current build instructions for OSX but building with bazel on OSX/macos also needs a md5sum binary and a /proc/cpuinfo file.\r\n\r\ncheap fix (would be better not to grep /proc/cpuinfo or to use 'sysctl -a hw' on OSX):\r\n\r\n```\r\nbrew install md5sha1sum  \r\nsudo mkdir -p /proc\r\nsudo docker run <linux-image-you-have> cat /proc/cpuinfo  > /proc/cpuinfo\r\n```\r\n\r\nAlso OSX Sierra (10.12.x) or greater is required to get the required 8.3.3 version of xcode.  The AppStore will spin forever if you try to upgrade xcode on an earlier OSX.\r\n\r\nComplaints before cheap fix:\r\n\r\n```\r\nWARNING: /Users/*/envoy-filter-example/envoy/bazel/repositories.bzl:26:5: /private/var/tmp/_bazel*/60fe7735f74821add3f845fcade3ae2a/external/envoy_deps/./repositories.sh: line 9: md5sum: command not found\r\n\r\nWARNING: /private/var/tmp/_bazel*/729952d62f84db3f8ddad6d51f4d2c7d/external/envoy/bazel/repositories.bzl:18:5: Fetching external dependencies...\r\nExternal dependency cache directory /private/var/tmp/_bazel*/729952d62f84db3f8ddad6d51f4d2c7d/external/envoy_deps_cache_2c744dffd279d7e9e0910ce594eb4f4f\r\ngrep: /proc/cpuinfo: No such file or directory\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1683/comments",
    "author": "msample",
    "comments": [
      {
        "user": "msample",
        "created_at": "2017-09-17T02:16:19Z",
        "body": "I just reviewed my install history and I think this can be closed.   I was looking at fixing these issues and noticed that they occurred before I updated the envoy submodule in envoy-filter-example. After updating, the files with the warnings cited have OS X/Darwin specific fixes. \r\n\r\nSorry for the noise."
      },
      {
        "user": "mattklein123",
        "created_at": "2017-09-17T17:43:35Z",
        "body": "OK closing."
      }
    ]
  },
  {
    "number": 1605,
    "title": "Allow listening on UNIX sockets",
    "created_at": "2017-09-07T00:43:37Z",
    "closed_at": "2018-03-07T23:14:05Z",
    "labels": [
      "enhancement",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1605",
    "body": "Currently, creating a listener to a `unix://` (v1) or `pipe:` (v2) address will fail. The v1 codepath just crashes with a SIGSEGV, the v2 codepath fails with a somewhat opaque message about `\"\"` not being a valid IP address.\r\n\r\nThere's some conflicting comments in `listener_manager_impl.cc` about whether this feature is intended to work, so I wanted to at least record somewhere that we want it:\r\n\r\n```\r\nProdListenerComponentFactory::createListenSocket(\r\n// TODO(mattklein123): UDS support.\r\n\r\nListenerImpl::ListenerImpl(\r\n// TODO(htuch): Validate not pipe when doing v2.\r\n```\r\n\r\nThere's two specific use cases in mind:\r\n* Using sockets as intermediate layers in a multi-filter config, where one filter (say `tcp_proxy`) forwards to one or another listener. Using ports means that all layers are exposed to users with SSH access, but sockets on Linux use standard filesystem permissions and can be stuffed into a protected directory.\r\n* Providing egress to a sandboxed process (e.g. networkless Docker container) via a UNIX socket mounted into the chroot. This is super-fiddly to get working with a TCP socket because :iptables:, and unix sockets are straightforward.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1605/comments",
    "author": "jmillikin-stripe",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-09-07T00:50:52Z",
        "body": "This is not suppose to work currently. I don't think it would be very hard to get working though if you want to work on it. Likely only a few places need to be changed."
      },
      {
        "user": "jmillikin-stripe",
        "created_at": "2017-09-07T00:54:18Z",
        "body": "I have a super-hacky patch that just comments out the safety checks and it seems to work. Probably I'll have it PR-able some time next week."
      },
      {
        "user": "htuch",
        "created_at": "2017-09-07T01:48:25Z",
        "body": "The crash on v1 is basically because we don't have JSON schema guarding against this. The v2 is caveat emptor until we have proto constraints (Real Soon Now). Since @jmillikin-stripe is planning to PR, I think we can leave it as is until the PR."
      },
      {
        "user": "mattklein123",
        "created_at": "2017-09-07T03:06:15Z",
        "body": "Yeah IIRC I think I already did most of the work to make it work, even with hot restart, I just never finished it up and added integration tests. I think other than adding some tests it will be very easy to finish up."
      },
      {
        "user": "mpuncel",
        "created_at": "2018-02-20T14:08:25Z",
        "body": "+1 for this. We're looking at deploying envoy to multi-tenant machines, and we would like to set up a listener for each tenant for egress using a unix socket 1) so we don't have to worry about port conflicts and 2) so we can use filesystem permissions to control access to the listener"
      },
      {
        "user": "jnb",
        "created_at": "2018-02-26T23:13:22Z",
        "body": "Another +1 for this.  For our setup we have HAProxy listening on Unix sockets.  We're thinking about replacing HAProxy with Envoy, but need a way to get it to also listen on Unix sockets."
      },
      {
        "user": "syml",
        "created_at": "2018-03-02T00:51:42Z",
        "body": "Can hot restart work with abstract sockets? It seems there has to be a moment where the socket is unbound, and traffic would be lost then.\r\nCan we consider a first PR without hot restart support? Especially for abstract sockets, which might be a permanent incompatibility anyway."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-03-02T19:56:16Z",
        "body": "I think even abstract sockets should work as we can pass the FDs across the UDS transport like we do for everything else. All that's really needed is the name to look it up by.\r\n\r\nFor everyone that is asking for this, I'm pretty certain this basically already works. There are probably a few asserts that need to be removed, maybe a tiny amount of logic to be added, and some tests. That's it."
      }
    ]
  },
  {
    "number": 1601,
    "title": "Refactor common Subscription concerns to a base class",
    "created_at": "2017-09-06T17:21:41Z",
    "closed_at": "2019-04-19T18:31:41Z",
    "labels": [
      "tech debt",
      "help wanted",
      "api/v2"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1601",
    "body": "Each individual Subscription is managing stats/logging/version tracking. This can be shared in a common base class, e.g. `SubscriptionBaseImpl`.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1601/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-04-18T17:56:35Z",
        "body": "@fredlas have you been working on this effectively? Or is this something else? cc @htuch "
      },
      {
        "user": "fredlas",
        "created_at": "2019-04-18T18:06:59Z",
        "body": "This is from before my time, so I can't be sure, but I would maybe guess that this has already been done. Did there maybe used to be a {Cds,Eds,etc}Subscription? And that's where the (recently removed) templatization came from?"
      },
      {
        "user": "mattklein123",
        "created_at": "2019-04-18T18:21:50Z",
        "body": "@fredlas yeah I think so. I'm guessing we can close this but I'm not sure. @htuch ?"
      },
      {
        "user": "htuch",
        "created_at": "2019-04-19T18:31:41Z",
        "body": "Yep, I think this is effectively sorted by now, this goes back to early in the v2 API work."
      }
    ]
  },
  {
    "number": 1425,
    "title": "Support regex path matching in route table",
    "created_at": "2017-08-10T09:14:52Z",
    "closed_at": "2017-09-16T02:52:22Z",
    "labels": [
      "help wanted",
      "api/v2"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1425",
    "body": "The v2 API has committed to support for regex matching for route matches. This will only be enabled when regexes are specified in the route configuration (and as a result, will only imply a performance cost in this situation). This issue tracks implementation work on this.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1425/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "delisdeli",
        "created_at": "2017-09-13T22:28:13Z",
        "body": "This would be great for the following path forwarding use case:\r\n`/foo/{foo_id}/bar/{bar_id}` to `/foodle/{foo_id}/barble/{bar_id}`"
      }
    ]
  },
  {
    "number": 1407,
    "title": "Dynamically link tests",
    "created_at": "2017-08-07T14:27:20Z",
    "closed_at": "2022-03-22T19:43:11Z",
    "labels": [
      "enhancement",
      "area/build",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1407",
    "body": "As discussed in #1400, this is probably the long term way to improve both build performance and disk utilization (should be able to get ASAN from 24GB to 4GB). I think this would be a useful improvement to developer productivity and also help with the Travis CI disk and CPU situation. I would swag this as probably 1-2 days work, looking for volunteers.\r\n\r\nThe first step would be to ensure we have the right artifacts in the external deps to perform dynamic link, we build with static only flags today. Then, modify the `envoy_cc_test*` macros in `envoy_build_system.bzl` to link dynamically. Then, mostly debugging. I'll leave this ticket open for volunteers.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1407/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2022-03-22T19:43:11Z",
        "body": "We are doing this, so closing."
      }
    ]
  },
  {
    "number": 1378,
    "title": "Add support for printing stack traces when exception is unhandled",
    "created_at": "2017-08-02T20:30:40Z",
    "closed_at": "2018-04-26T18:36:34Z",
    "labels": [
      "enhancement",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1378",
    "body": "This isn't a feature that can be provided in a portable fashion, but it very useful where the compiler/runtime don't unwind the stack before calling the std::terminate_handler when dying.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1378/comments",
    "author": "jamessynge",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-08-02T21:53:34Z",
        "body": "I think this is probably as simple as just printing a backtrace inside of an std::terminate handler. cc @dnoe "
      },
      {
        "user": "dnoe",
        "created_at": "2017-08-03T13:18:54Z",
        "body": "Normally that wouldn't work but I think it will because we're using the custom pthread wrapper instead of std::thread."
      },
      {
        "user": "gumpt",
        "created_at": "2018-04-12T23:44:58Z",
        "body": "I’d be interested in taking this if it isn’t done and no-one is actively working on it."
      },
      {
        "user": "dnoe",
        "created_at": "2018-04-13T00:08:08Z",
        "body": "I don't think anyone is working on it. PRs would definitely be welcome!"
      }
    ]
  },
  {
    "number": 1364,
    "title": "Tracing headers not propagated when x-ot-span-context header is omitted",
    "created_at": "2017-08-01T00:05:49Z",
    "closed_at": "2017-09-21T19:31:39Z",
    "labels": [
      "bug",
      "area/tracing",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1364",
    "body": "Within Zipkin, there isn't a concept of an 'x-ot-span-context' header. However, the initial version of the Zipkin support within Envoy requires this header to be present for Envoy to propagate the B3 headers downstream.  The following is an example of a service making an inter-service call through Envoy.\r\n\r\nCurrent behavior: \r\n\r\n```\r\nT 172.17.0.1:37180 -> 172.17.0.3:80 [AP]\r\nGET / HTTP/1.1.\r\nHost: obfuscated:8080.\r\nx-b3-traceid: df2a47707d36857e.\r\nx-b3-spanid: 1fcdac653464b039.\r\nx-b3-parentspanid: de13127f091b8510.\r\nx-b3-sampled: 1.\r\n.\r\n\r\nT 172.17.0.3:38092 -> 172.17.0.1:80 [AP]\r\nGET / HTTP/1.1.\r\nhost: obfuscated:8080.\r\nx-b3-traceid: 0000943861e352ca.\r\nx-b3-spanid: 0000943861e352ca.\r\nx-b3-parentspanid: de13127f091b8510.\r\nx-b3-sampled: 1.\r\nx-forwarded-proto: http.\r\nx-request-id: f30826a5-e41a-99f4-a9ac-6daef36f4dd3.\r\nx-ot-span-context: 0000943861e352ca;0000943861e352ca;0000000000000000;cs.\r\n```\r\n\r\nNotice in the current behavior that the x-b3-traceid does not match the existing trace identifier.  It is dropped, and essentially a new trace is created.  Also note, the parentspanid has an erroneous value.  If Envoy decided to initiate a new and independent trace, the parent should be absent or null, and not use the parent of an unrelated trace.\r\n\r\nExpected behavior:\r\n\r\n```\r\nT 172.17.0.3:38092 -> 172.17.0.1:80 [AP]\r\nGET / HTTP/1.1.\r\nhost: obfuscated:8080.\r\nx-b3-traceid: df2a47707d36857e.\r\nx-b3-spanid: 0000943861e352ca.\r\nx-b3-parentspanid: 1fcdac653464b039.\r\nx-b3-sampled: 1.\r\nx-forwarded-proto: http.\r\nx-request-id: f30826a5-e41a-99f4-a9ac-6daef36f4dd3.\r\n```\r\n\r\nNotice in the expected behavior, x-b3-traceid remains the same as the initial request. parentspanid becomes the spanid from the upstream request, and spanid is a newly generated identifier.\r\n\r\nWithout Envoy, Trace T has one root span called X, Service A calls Service B. Service A annotates span X with CS CR.  Optional: Service B annotates span X with SR SS.\r\n\r\nWhen using Envoy, Trace T has one root span Y, and one child span Y.  Service A calls Service B via Envoy.  Service A annotates span X with CS CR.  Envoy annotates span X with SR SS.  Envoy injects span Y and annotates with CS CR.  Optional: Service B annotates span Y with SR SS.\r\n\r\n@mattklein123 @fabolive @rshriram @adriancole",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1364/comments",
    "author": "chrisleavoy",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-08-01T00:08:01Z",
        "body": "@fabolive sorry I can't assign this to you. Can you and @rshriram take a look at this when you have some cycles and propose a solution? Thank you."
      },
      {
        "user": "cstrahan",
        "created_at": "2017-08-02T22:34:09Z",
        "body": "I was about to report the same thing.\r\n\r\nTo resolve the overwritten `x-b3-traceid`, would it make sense to change the code responsible for constructing the `x-ot-span-context` such that it first checks if the incoming request is internal, and if so, use the existing `x-b3-traceid` (if present)?\r\n\r\nIf that sounds good, I could put that on my queue (but I can't give an estimate of when I could deliver at the moment)."
      },
      {
        "user": "mattklein123",
        "created_at": "2017-08-10T15:51:40Z",
        "body": "@adriancole if you have a bit of time do you think you could help advise us on what the appropriate behavior here should be? I think given that we can find someone to work on a fix."
      },
      {
        "user": "fabolive",
        "created_at": "2017-08-10T15:57:20Z",
        "body": "@mattklein123 I believe it is possible to rework the entire implementation so as to obviate the need for the `x-ot-span-context` header. I should have some cycles next week to make an assessment."
      },
      {
        "user": "codefromthecrypt",
        "created_at": "2017-08-11T05:02:29Z",
        "body": "The problem and behavior description of what should happen sounds legit to me"
      },
      {
        "user": "objectiser",
        "created_at": "2017-08-29T18:52:20Z",
        "body": "Could I suggest a slightly different fix - the `x-ot-span-context` is useful in cases where the application is responsible for propagating the context, as it is easier to just deal with a single header property.\r\n\r\nHowever this causes problems if the application itself is instrumented, as the changes to the trace context are not propagated to the egress proxy.\r\n\r\nIf both the `x-ot-span-context` and B3 headers are passed in, as now - if the application is just passing the context through, then it could just use the `x-ot-span-context` - so the egress proxy would only receive it, and decode it to obtain the constituent parts.\r\n\r\nHowever if a zipkin compatible tracer is being used by the application, it would take and build upon the B3 headers, and would eventually only pass the B3 headers to the egress proxy - thus the proxy knowing that the trace context must have been updated within the application.\r\n\r\nSo caters for simplified context passing, while still allowing the app to add its own internal tracing if required."
      },
      {
        "user": "gregdurham",
        "created_at": "2017-09-14T22:24:34Z",
        "body": "Is there any update on this issue? I seem to have run into this as well. "
      },
      {
        "user": "mattklein123",
        "created_at": "2017-09-16T16:17:48Z",
        "body": "I'm marking this issue \"help wanted.\" Someone just needs to step up and fix this. @objectiser any interest?"
      },
      {
        "user": "objectiser",
        "created_at": "2017-09-16T16:51:39Z",
        "body": "@mattklein123 will have a look early next week and let you know."
      }
    ]
  },
  {
    "number": 1297,
    "title": "Support running without an admin interface",
    "created_at": "2017-07-20T16:28:43Z",
    "closed_at": "2018-05-18T02:32:47Z",
    "labels": [
      "enhancement",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1297",
    "body": "Some of Envoy's use cases, such as reverse-proxying to a non-TLS-capable backend, don't benefit much from having an admin interface available. It's just one additional port that I need to figure out how to firewall away from unauthorized access.\r\n\r\nIt'd be nice if the admin interface were made optional. Potential ways to disable it include leaving out the `\"admin\"` section of the config, or passing some special flag.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1297/comments",
    "author": "jmillikin-stripe",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-07-20T16:31:34Z",
        "body": "It's hard for me to imagine why anyone would ever run without the admin interface. Too useful for ops. However, not opposed to making it optional if someone does the patch."
      },
      {
        "user": "tlhunter",
        "created_at": "2017-11-15T21:33:55Z",
        "body": "FWIW, if one wants to \"disable\" the admin interface, I believe the following would work:\r\n\r\n```json\r\n \"admin\": {                                       \r\n   \"access_log_path\": \"/dev/null\",                \r\n   \"address\": \"tcp://127.0.0.1:0\"                   \r\n }                                               \r\n```\r\n\r\nThis asks the OS to assign a random high port. It'll then only listen for requests originating within the machine."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-18T02:32:47Z",
        "body": "Closing as I just don't see this getting implemented. If someone wants to step up we can reopen."
      },
      {
        "user": "aceqbaceq",
        "created_at": "2023-09-10T00:23:22Z",
        "body": "tlhunter => your solution is amazing.\r\nmattklein123 = if there is smth its hard to imagine to you it doesnt mean anything"
      }
    ]
  },
  {
    "number": 1289,
    "title": "add support for weighted round robin load balancing across various weight clusters at a route level",
    "created_at": "2017-07-19T18:30:20Z",
    "closed_at": "2017-11-21T19:03:41Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1289",
    "body": "",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1289/comments",
    "author": "khgandhi",
    "comments": [
      {
        "user": "rshriram",
        "created_at": "2017-10-18T13:40:46Z",
        "body": "can you try the weighted routing again? we removed the stream ID based load balancing.. You should now get fully random load distribution across the weighted clusters, such that the net distribution at any point obeys the weights per cluster."
      },
      {
        "user": "alyssawilk",
        "created_at": "2017-11-21T19:03:41Z",
        "body": "Closing off in favor of #1285"
      }
    ]
  },
  {
    "number": 1288,
    "title": "support for weighted round robin load balancing algorithm for each cluster.",
    "created_at": "2017-07-19T18:28:44Z",
    "closed_at": "2017-11-21T19:03:38Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1288",
    "body": "",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1288/comments",
    "author": "khgandhi",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2017-11-21T19:03:38Z",
        "body": "Closing off in favor of #1285"
      }
    ]
  },
  {
    "number": 842,
    "title": "Improve build experience when consuming prebuilts outside of CI",
    "created_at": "2017-04-26T16:43:48Z",
    "closed_at": "2018-03-15T23:52:21Z",
    "labels": [
      "area/build",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/842",
    "body": "Today, we consume prebuilt external deps in CI via `ci/WORKSPACE` and `ci/prebuilt/BUILD`. Other folks who want to build Envoy are also interested in using their own versions of these libraries.\r\n\r\nWe should both document how to follow the CI prebuilt flow, make it less clunky (e.g. add some .gitignore entries so that folks don't have dirty trees) and make `ci/prebuild/BUILD` less verbose - ideally we can just specify the `--prefix` of each library (or a global `--prefix` if that's how the environment looks like on the build machine).\r\n\r\n@mattwoodyard",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/842/comments",
    "author": "htuch",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-03-15T23:52:21Z",
        "body": "I think this is basically fixed at this point so just going to go ahead and close. We can open specific issues with the current scheme as needed."
      }
    ]
  },
  {
    "number": 343,
    "title": "Allow Envoy making load balancing decisions based on request headers",
    "created_at": "2017-01-11T05:58:00Z",
    "closed_at": "2018-11-16T18:03:44Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/343",
    "body": "One use case is that internal service might want to set specific header so that Envoy will route request to canary only nodes.\r\nWe might find other use cases, make header to be `x-envoy-lb-hint` and use various types of hints in chooseHost logic.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/343/comments",
    "author": "RomanDzhabarov",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-11-16T18:03:44Z",
        "body": "This is already possible via different mechanisms. Closing in favor of more specific issues/requests."
      }
    ]
  },
  {
    "number": 232,
    "title": "perf: replace evbuffer",
    "created_at": "2016-11-17T19:05:07Z",
    "closed_at": "2018-08-01T20:33:23Z",
    "labels": [
      "area/perf",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/232",
    "body": "evbuffer has a lot of extra functionality that we don't need. It also does suboptimal things during reading (calling ioctl, etc.) that are not well tuned for using a fast allocator like tcmalloc.\r\n\r\nThis is not something we will do short term but just opening this item to track.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/232/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "brian-pane",
        "created_at": "2018-05-13T01:08:10Z",
        "body": "I volunteer to work on this, because the `ioctl` in libevent is surprisingly time-consuming."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-05-13T20:46:22Z",
        "body": "@brian-pane great, will assign this over to you. I would love to see this fixed also but have never had the time. A couple of thoughts as there are solutions that I think don't involve rewriting the entire buffer implementation if you are interested in a shortcut:\r\n- We could try for a patch to libevent itself to have an option to not do the ioctl call. Not sure if they would accept it or not, though seems reasonable to me.\r\n- We could just remove the read() and write() calls from Envoy's buffer interface. Then, we can allocate space directly using reservation, and just call readv()/writev() directly. This has the large added benefit of untangling the FD from the buffer implementation which is needed anyway from some of the FD abstraction work that is happening via the Cisco/VPP folks. \r\n\r\nSo I guess in writing this I wonder if maybe it would be a better use of your time to just do my second suggestion above? (If you completely rewrite the buffer implementation I would ask for that to be done anyway)."
      },
      {
        "user": "brian-pane",
        "created_at": "2018-05-13T22:46:27Z",
        "body": "+1 for removing the read and write calls from the buffer API. That will enable the buffer implementation and the I/O implementation to evolve separately."
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-06-28T05:05:20Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2018-07-05T05:22:31Z",
        "body": "This issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "brian-pane",
        "created_at": "2018-07-09T19:09:01Z",
        "body": "I was able to remove the `ioctl` call, but it may be a while before I have time to work on a complete replacement of evbuffer. It may make sense to return this issue to \"help wanted\" state, in case anybody else has time to work on it."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-07-09T19:15:20Z",
        "body": "TBH with the ioctol gone I'm not sure how much it matters anymore and maybe we should just close this. But I marked \"help wanted\" for now."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-08-01T20:33:23Z",
        "body": "Going to go ahead and close this as I don't think it's a priority anymore. We can revisit if it comes up again."
      }
    ]
  }
]