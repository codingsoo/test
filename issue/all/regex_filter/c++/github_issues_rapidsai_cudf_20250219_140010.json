[
  {
    "number": 16803,
    "title": "[FEA] Support datetime64[D]",
    "created_at": "2024-09-13T05:01:03Z",
    "closed_at": "2024-11-05T02:44:12Z",
    "labels": [
      "feature request",
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/16803",
    "body": "**Describe the solution you'd like**\nplease let cudf to support datetime64[D].\n\n**Describe alternatives you've considered**\nI had to convert my datetime to integer for filter and etc.\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/16803/comments",
    "author": "nghiadt22",
    "comments": [
      {
        "user": "mroeschke",
        "created_at": "2024-09-16T19:24:18Z",
        "body": "Thanks for the request. Just to understand your use case, do you regularly work with dates before 2.9e11 BC or after 2.9e11 AD? \n\nFor reference, pandas 2.0 also does not support `datetime64[D]`. The largest precision cudf supports is `datetime64[s]` which matches pandas and allows dates representable between `[2.9e11 BC, 2.9e11 AD]`"
      },
      {
        "user": "nghiadt22",
        "created_at": "2024-11-03T09:21:00Z",
        "body": "Thanks @mroeschke. I got around my problem pretty easily by converting my string dates formatted as \"yyyymmdd\" into numbers for comparison. \n\nThat's all I wanted: to compare my dates. \n\nI just thought that it'd enable more options for the user since cudf's supported data format becomes more granular.\nHowever, I understand that cudf is intertwined with pandas so this feature is only valuable when both libs work.\nSo, from your POV, is it worth it? Please let me know what you think. "
      },
      {
        "user": "mroeschke",
        "created_at": "2024-11-04T17:45:10Z",
        "body": ">  However, I understand that cudf is intertwined with pandas so this feature is only valuable when both libs work.\n\nNot necessarily; one could use cudf independently of pandas and cudf _could_ support day resolution datetime type. But a day resolution type is functionally equivalent to other resolutions (i.e. \"compare your dates\" will work with any resolution); the only difference is that day resolution can represent a wider range of dates.\n\nSo for the team to consider implementing this feature, there needs to be a demand/use case to store dates beyond `[2.9e11 BC, 2.9e11 AD]`"
      },
      {
        "user": "nghiadt22",
        "created_at": "2024-11-05T02:43:57Z",
        "body": "> >  However, I understand that cudf is intertwined with pandas so this feature is only valuable when both libs work.\n> \n> Not necessarily; one could use cudf independently of pandas and cudf _could_ support day resolution datetime type. But a day resolution type is functionally equivalent to other resolutions (i.e. \"compare your dates\" will work with any resolution); the only difference is that day resolution can represent a wider range of dates.\n> \n> So for the team to consider implementing this feature, there needs to be a demand/use case to store dates beyond `[2.9e11 BC, 2.9e11 AD]`\n\nI see. Thank you for your effort and promptly feedback. I'll close this request."
      }
    ]
  },
  {
    "number": 15614,
    "title": "[BUG] 24.04 cuDF.pandas now errors on mixed dtype comparisons in row-wise functions (didn't in 24.02)",
    "created_at": "2024-04-29T22:45:28Z",
    "closed_at": "2024-04-29T23:16:50Z",
    "labels": [
      "question",
      "0 - Waiting on Author"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/15614",
    "body": "**Describe the bug**\r\nIn the cuDF pandas demo notebooks, we try to run `min()` on mixed dtypes.  It works in pandas, and used to work in cuDF.pandas 24.02.  It fails in 24.04.\r\n**Steps/Code to reproduce bug**\r\n```\r\n%load_ext cudf.pandas\r\nimport pandas as pd\r\n\r\nsmall_df = pd.DataFrame({'a': [0, 1, 2], 'b': [\"x\", \"y\", \"z\"]})\r\nsmall_df = pd.concat([small_df, small_df])\r\n\r\naxis = 0\r\nfor i in range(0, 2):\r\n    small_df.min(axis=axis)\r\n    axis = 1\r\n\r\ncounts = small_df.groupby(\"a\").b.count()\r\n```\r\noutputs\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:889, in _fast_slow_function_call(func, *args, **kwargs)\r\n    888 fast_args, fast_kwargs = _fast_arg(args), _fast_arg(kwargs)\r\n--> 889 result = func(*fast_args, **fast_kwargs)\r\n    890 if result is NotImplemented:\r\n    891     # try slow path\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:30, in call_operator(fn, args, kwargs)\r\n     29 def call_operator(fn, args, kwargs):\r\n---> 30     return fn(*args, **kwargs)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/frame.py:1715, in Frame.min(self, axis, skipna, numeric_only, **kwargs)\r\n   1680 \"\"\"\r\n   1681 Return the minimum of the values in the DataFrame.\r\n   1682 \r\n   (...)\r\n   1713     Parameters currently not supported are `level`, `numeric_only`.\r\n   1714 \"\"\"\r\n-> 1715 return self._reduce(\r\n   1716     \"min\",\r\n   1717     axis=axis,\r\n   1718     skipna=skipna,\r\n   1719     numeric_only=numeric_only,\r\n   1720     **kwargs,\r\n   1721 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/dataframe.py:6267, in DataFrame._reduce(self, op, axis, numeric_only, **kwargs)\r\n   6266 elif axis == 1:\r\n-> 6267     return source._apply_cupy_method_axis_1(op, **kwargs)\r\n   6268 else:\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/dataframe.py:6449, in DataFrame._apply_cupy_method_axis_1(self, method, *args, **kwargs)\r\n   6447 kwargs.pop(\"cast_to_int\", None)\r\n-> 6449 prepared, mask, common_dtype = self._prepare_for_rowwise_op(\r\n   6450     method, skipna, numeric_only\r\n   6451 )\r\n   6452 for col in prepared._data.names:\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/nvtx/nvtx.py:116, in annotate.__call__.<locals>.inner(*args, **kwargs)\r\n    115 libnvtx_push_range(self.attributes, self.domain.handle)\r\n--> 116 result = func(*args, **kwargs)\r\n    117 libnvtx_pop_range(self.domain.handle)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/core/dataframe.py:6071, in DataFrame._prepare_for_rowwise_op(self, method, skipna, numeric_only)\r\n   6066 if (\r\n   6067     not numeric_only\r\n   6068     and is_string_dtype(common_dtype)\r\n   6069     and any(not is_string_dtype(dt) for dt in filtered.dtypes)\r\n   6070 ):\r\n-> 6071     raise TypeError(\r\n   6072         f\"Cannot perform row-wise {method} across mixed-dtype columns,\"\r\n   6073         \" try type-casting all the columns to same dtype.\"\r\n   6074     )\r\n   6076 if not skipna and any(col.nullable for col in filtered._columns):\r\n\r\nTypeError: Cannot perform row-wise min across mixed-dtype columns, try type-casting all the columns to same dtype.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[2], line 11\r\n      9 axis = 0\r\n     10 for i in range(0, 2):\r\n---> 11     small_df.min(axis=axis)\r\n     12     axis = 1\r\n     14 counts = small_df.groupby(\"a\").b.count()\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:837, in _CallableProxyMixin.__call__(self, *args, **kwargs)\r\n    836 def __call__(self, *args, **kwargs) -> Any:\r\n--> 837     result, _ = _fast_slow_function_call(\r\n    838         # We cannot directly call self here because we need it to be\r\n    839         # converted into either the fast or slow object (by\r\n    840         # _fast_slow_function_call) to avoid infinite recursion.\r\n    841         # TODO: When Python 3.11 is the minimum supported Python version\r\n    842         # this can use operator.call\r\n    843         call_operator,\r\n    844         self,\r\n    845         args,\r\n    846         kwargs,\r\n    847     )\r\n    848     return result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:902, in _fast_slow_function_call(func, *args, **kwargs)\r\n    900         slow_args, slow_kwargs = _slow_arg(args), _slow_arg(kwargs)\r\n    901         with disable_module_accelerator():\r\n--> 902             result = func(*slow_args, **slow_kwargs)\r\n    903 return _maybe_wrap_result(result, func, *args, **kwargs), fast\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:30, in call_operator(fn, args, kwargs)\r\n     29 def call_operator(fn, args, kwargs):\r\n---> 30     return fn(*args, **kwargs)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11630, in DataFrame.min(self, axis, skipna, numeric_only, **kwargs)\r\n  11622 @doc(make_doc(\"min\", ndim=2))\r\n  11623 def min(\r\n  11624     self,\r\n   (...)\r\n  11628     **kwargs,\r\n  11629 ):\r\n> 11630     result = super().min(axis, skipna, numeric_only, **kwargs)\r\n  11631     if isinstance(result, Series):\r\n  11632         result = result.__finalize__(self, method=\"min\")\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:12385, in NDFrame.min(self, axis, skipna, numeric_only, **kwargs)\r\n  12378 def min(\r\n  12379     self,\r\n  12380     axis: Axis | None = 0,\r\n   (...)\r\n  12383     **kwargs,\r\n  12384 ):\r\n> 12385     return self._stat_function(\r\n  12386         \"min\",\r\n  12387         nanops.nanmin,\r\n  12388         axis,\r\n  12389         skipna,\r\n  12390         numeric_only,\r\n  12391         **kwargs,\r\n  12392     )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:12374, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\r\n  12370 nv.validate_func(name, (), kwargs)\r\n  12372 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\r\n> 12374 return self._reduce(\r\n  12375     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\r\n  12376 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11549, in DataFrame._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\r\n  11545     df = df.T\r\n  11547 # After possibly _get_data and transposing, we are now in the\r\n  11548 #  simple case where we can use BlockManager.reduce\r\n> 11549 res = df._mgr.reduce(blk_func)\r\n  11550 out = df._constructor_from_mgr(res, axes=res.axes).iloc[0]\r\n  11551 if out_dtype is not None and out.dtype != \"boolean\":\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:1500, in BlockManager.reduce(self, func)\r\n   1498 res_blocks: list[Block] = []\r\n   1499 for blk in self.blocks:\r\n-> 1500     nbs = blk.reduce(func)\r\n   1501     res_blocks.extend(nbs)\r\n   1503 index = Index([None])  # placeholder\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/internals/blocks.py:404, in Block.reduce(self, func)\r\n    398 @final\r\n    399 def reduce(self, func) -> list[Block]:\r\n    400     # We will apply the function and reshape the result into a single-row\r\n    401     #  Block with the same mgr_locs; squeezing will be done at a higher level\r\n    402     assert self.ndim == 2\r\n--> 404     result = func(self.values)\r\n    406     if self.values.ndim == 1:\r\n    407         res_values = result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11468, in DataFrame._reduce.<locals>.blk_func(values, axis)\r\n  11466         return np.array([result])\r\n  11467 else:\r\n> 11468     return op(values, axis=axis, skipna=skipna, **kwds)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:147, in bottleneck_switch.__call__.<locals>.f(values, axis, skipna, **kwds)\r\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n    146 else:\r\n--> 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n    149 return result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:404, in _datetimelike_compat.<locals>.new_func(values, axis, skipna, mask, **kwargs)\r\n    401 if datetimelike and mask is None:\r\n    402     mask = isna(values)\r\n--> 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\r\n    406 if datetimelike:\r\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:1098, in _nanminmax.<locals>.reduction(values, axis, skipna, mask)\r\n   1093     return _na_for_min_count(values, axis)\r\n   1095 values, mask = _get_values(\r\n   1096     values, skipna, fill_value_typ=fill_value_typ, mask=mask\r\n   1097 )\r\n-> 1098 result = getattr(values, meth)(axis)\r\n   1099 result = _maybe_null_out(result, axis, mask, values.shape)\r\n   1100 return result\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/numpy/core/_methods.py:45, in _amin(a, axis, out, keepdims, initial, where)\r\n     43 def _amin(a, axis=None, out=None, keepdims=False,\r\n     44           initial=_NoValue, where=True):\r\n---> 45     return umr_minimum(a, axis, None, out, keepdims, initial, where)\r\n\r\nTypeError: '<=' not supported between instances of 'int' and 'str'\r\n```\r\n\r\nit used to output a warning:\r\n```\r\n/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py:5971: UserWarning: Row-wise operations currently only support int, float and bool dtypes. Non numeric columns are ignored.\r\n  warnings.warn(msg)\r\n```\r\nand then worked:\r\n```\r\na\r\n0    2\r\n1    2\r\n2    2\r\nName: b, dtype: int64\r\n```\r\n\r\n**Expected behavior**\r\n```\r\n>>> counts\r\na\r\n0    2\r\n1    2\r\n2    2\r\nName: b, dtype: int64\r\n```\r\nwhich is what I get in pandas and 24.02 cuDF.pandas\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: [Docker]\r\n - Method of cuDF install: [Docker]\r\n   - If method of install is [Docker], provide `docker pull` & `docker run` commands used:  \r\n   - for 24.02: `docker run --user root --gpus all --rm -it --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8888:8888 -p 8787:8787 -p 8786:8786 rapidsai/notebooks:24.02-cuda11.8-py3.10 jupyter-lab --notebook-dir=/home/rapids/notebooks --ip=0.0.0.0 --no-browser --NotebookApp.token='' --NotebookApp.allow_origin='*' --allow-root`\r\n   - For 24.04:  `docker run --user root --gpus all --rm -it --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8888:8888 -p 8787:8787 -p 8786:8786 rapidsai/notebooks:24.04-cuda11.8-py3.10 jupyter-lab --notebook-dir=/home/rapids/notebooks --ip=0.0.0.0 --no-browser --NotebookApp.token='' --NotebookApp.allow_origin='*' --allow-root`\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/15614/comments",
    "author": "taureandyernv",
    "comments": [
      {
        "user": "beckernick",
        "created_at": "2024-04-29T23:00:26Z",
        "body": "Pandas 2 enforces `numeric_only=False` rather than the prior behavior that filtered down to numeric if things failed.\r\n\r\nSo I believe this is now the expected behavior and we should update our notebooks. @galipremsagar @mroeschke , is that your understanding?"
      },
      {
        "user": "galipremsagar",
        "created_at": "2024-04-29T23:08:40Z",
        "body": "That's right @beckernick, @taureandyernv can you verify if cudf-24.04 matches upto pandas-2.x? Here is what I get for `pandas-2.x`:\r\n\r\n```ipython\r\nIn [1]: import pandas as pd\r\n   ...: \r\n   ...: small_df = pd.DataFrame({'a': [0, 1, 2], 'b': [\"x\", \"y\", \"z\"]})\r\n   ...: small_df = pd.concat([small_df, small_df])\r\n   ...: \r\n   ...: axis = 0\r\n   ...: for i in range(0, 2):\r\n   ...:     small_df.min(axis=axis)\r\n   ...:     axis = 1\r\n   ...: \r\n   ...: counts = small_df.groupby(\"a\").b.count()\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[1], line 8\r\n      6 axis = 0\r\n      7 for i in range(0, 2):\r\n----> 8     small_df.min(axis=axis)\r\n      9     axis = 1\r\n     11 counts = small_df.groupby(\"a\").b.count()\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/frame.py:11643, in DataFrame.min(self, axis, skipna, numeric_only, **kwargs)\r\n  11635 @doc(make_doc(\"min\", ndim=2))\r\n  11636 def min(\r\n  11637     self,\r\n   (...)\r\n  11641     **kwargs,\r\n  11642 ):\r\n> 11643     result = super().min(axis, skipna, numeric_only, **kwargs)\r\n  11644     if isinstance(result, Series):\r\n  11645         result = result.__finalize__(self, method=\"min\")\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/generic.py:12388, in NDFrame.min(self, axis, skipna, numeric_only, **kwargs)\r\n  12381 def min(\r\n  12382     self,\r\n  12383     axis: Axis | None = 0,\r\n   (...)\r\n  12386     **kwargs,\r\n  12387 ):\r\n> 12388     return self._stat_function(\r\n  12389         \"min\",\r\n  12390         nanops.nanmin,\r\n  12391         axis,\r\n  12392         skipna,\r\n  12393         numeric_only,\r\n  12394         **kwargs,\r\n  12395     )\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/generic.py:12377, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\r\n  12373 nv.validate_func(name, (), kwargs)\r\n  12375 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\r\n> 12377 return self._reduce(\r\n  12378     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\r\n  12379 )\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/frame.py:11562, in DataFrame._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\r\n  11558     df = df.T\r\n  11560 # After possibly _get_data and transposing, we are now in the\r\n  11561 #  simple case where we can use BlockManager.reduce\r\n> 11562 res = df._mgr.reduce(blk_func)\r\n  11563 out = df._constructor_from_mgr(res, axes=res.axes).iloc[0]\r\n  11564 if out_dtype is not None and out.dtype != \"boolean\":\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/internals/managers.py:1500, in BlockManager.reduce(self, func)\r\n   1498 res_blocks: list[Block] = []\r\n   1499 for blk in self.blocks:\r\n-> 1500     nbs = blk.reduce(func)\r\n   1501     res_blocks.extend(nbs)\r\n   1503 index = Index([None])  # placeholder\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/internals/blocks.py:404, in Block.reduce(self, func)\r\n    398 @final\r\n    399 def reduce(self, func) -> list[Block]:\r\n    400     # We will apply the function and reshape the result into a single-row\r\n    401     #  Block with the same mgr_locs; squeezing will be done at a higher level\r\n    402     assert self.ndim == 2\r\n--> 404     result = func(self.values)\r\n    406     if self.values.ndim == 1:\r\n    407         res_values = result\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/frame.py:11481, in DataFrame._reduce.<locals>.blk_func(values, axis)\r\n  11479         return np.array([result])\r\n  11480 else:\r\n> 11481     return op(values, axis=axis, skipna=skipna, **kwds)\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/nanops.py:147, in bottleneck_switch.__call__.<locals>.f(values, axis, skipna, **kwds)\r\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n    146 else:\r\n--> 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\r\n    149 return result\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/nanops.py:404, in _datetimelike_compat.<locals>.new_func(values, axis, skipna, mask, **kwargs)\r\n    401 if datetimelike and mask is None:\r\n    402     mask = isna(values)\r\n--> 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\r\n    406 if datetimelike:\r\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/pandas/core/nanops.py:1098, in _nanminmax.<locals>.reduction(values, axis, skipna, mask)\r\n   1093     return _na_for_min_count(values, axis)\r\n   1095 values, mask = _get_values(\r\n   1096     values, skipna, fill_value_typ=fill_value_typ, mask=mask\r\n   1097 )\r\n-> 1098 result = getattr(values, meth)(axis)\r\n   1099 result = _maybe_null_out(result, axis, mask, values.shape)\r\n   1100 return result\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.11/site-packages/numpy/core/_methods.py:45, in _amin(a, axis, out, keepdims, initial, where)\r\n     43 def _amin(a, axis=None, out=None, keepdims=False,\r\n     44           initial=_NoValue, where=True):\r\n---> 45     return umr_minimum(a, axis, None, out, keepdims, initial, where)\r\n\r\nTypeError: '<=' not supported between instances of 'int' and 'str'\r\n```"
      },
      {
        "user": "taureandyernv",
        "created_at": "2024-04-29T23:16:50Z",
        "body": "I verified.  We'll update the notebooks accordingly.  Thanks at @beckernick and @galipremsagar "
      }
    ]
  },
  {
    "number": 15532,
    "title": "[QST] Any magic fixes for str.edit_distance_matrix with dask_cudf across partitions?",
    "created_at": "2024-04-15T04:50:34Z",
    "closed_at": "2024-04-17T18:45:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/15532",
    "body": "I can generate sequences of str:\r\n\r\n```\r\ndef generate_sequence(length):\r\n    return ''.join(random.choices(AMINO_ACIDS, k=length))\r\n\r\nstart = time.time()\r\n\r\nAMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\r\nLENGTH = 12\r\nMATRIX_SIZE = 1_000\r\nN_PART = 1\r\n\r\n# Generate sequences\r\nsequences = [generate_sequence(LENGTH) for _ in range(MATRIX_SIZE)]\r\n\r\nsequences[:10]\r\n```\r\n\r\nthen convert to df and then to ddf:\r\n\r\n```\r\ndf = cudf.DataFrame({'sequence': sequences})\r\nddf = dask_cudf.from_cudf(df, npartitions=N_PART) \r\n```\r\n\r\nand then use libcudf methods to rapidly calculate the Levenshtein_distance matrix on GPU:\r\n\r\n```\r\nfinal = ddf.map_partitions(lambda x: x['sequence'].str.edit_distance_matrix(), meta=('', cudf.Series))\r\nfinal = final.compute()\r\n```\r\n\r\nI get the expected [n x n] matrix (although technically its a `cudf.Series` of lists). Amazing and fast!\r\n\r\nHowever, at ~100,000 or so rows, I get the expected max string CUDA error. When I bump up the n_partitions >1, the resulting output post-compute() `cudf.Series` has the correct len for dim0 (n_rows) but the width of matrix aka `len(final.iloc[0])` is truncated to a single partition such that the effective the matrix output is: [MATRIX_SIZE, MATRIX_SIZE//N_PART]. Thus, it looks like the edit_distance_matrix calculation is not propagated 'for free' for both dims in a ddf context using dask_cudf. Shucks!\r\n\r\nAny tricks to get this to work?\r\n\r\nThx!",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/15532/comments",
    "author": "dangraysf",
    "comments": [
      {
        "user": "rjzamora",
        "created_at": "2024-04-16T22:30:07Z",
        "body": "Hi @dangraysf - Thanks for filing an issue!\r\n\r\nThe `edit_distance_matrix` method in cudf currently operates on the entire string column at once. When you convert to a `dask.dataframe.DataFrame` object (using `dask_cudf`), you are partitioning your data into a collection of distinct `cudf.DataFrame` objects. Using `ddf.map_partitions(func, ...)` tells dask/dask_cudf that `func` can be **independently** mapped across the partitions in an \"embarrassingly parallel\" fashion. Unfortunately, the logic needed to perform an `edit_distance_matrix` calculation across the global collection is **not** embarrassingly parallel at all (In fact, it requires that each partition be compared to every other partition).\r\n\r\nI don't personally have much expertise in string processing, but my sense is that the all-to-all nature of `edit_distance_matrix` will make it tricky to scale out. If you only want to compare every row to a smaller number of `target` strings (using `edit_distance(target=...)`, then the problem becomes much easier."
      },
      {
        "user": "dangraysf",
        "created_at": "2024-04-17T12:27:14Z",
        "body": "Great insight! My workaround is the following:\r\n```\r\nAMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\r\nLENGTH = 12\r\nMATRIX_SIZE = 250_000\r\n\r\n# Initialize Dask client\r\ncluster = LocalCUDACluster()\r\nclient = Client(cluster)\r\nprint(client.dashboard_link)\r\n\r\n# Generate sequences\r\nsequences = [generate_sequence(LENGTH) for _ in range(MATRIX_SIZE)]\r\n\r\ndf = cudf.DataFrame({'sequence': sequences})\r\nddf = dask_cudf.from_cudf(df, npartitions=4) \r\n\r\noutputs = []\r\n\r\nfor idx, s in tqdm(enumerate(sequences)):\r\n    output = ddf['sequence'].map_partitions(lambda x: x.str.edit_distance(s), meta=(idx, 'int')).compute()\r\n    outputs.append(pd.DataFrame({idx:output.to_dict()}))\r\n    \r\n    # Flush CUDA memory\r\n    mempool = cp.get_default_memory_pool()\r\n    mempool.free_all_blocks()\r\n```\r\n\r\nExactly as you suggest -- edit_distance down the row is all goodand across partitions.\r\n\r\nThis is still much more performant than a vanilla edit_distance matrix using the the standard C implementations (rapidfuzz) and so RAPIDs cuDF + nvedit remains very good for this simple biologist without resorting to CPU-based HPC -- thank you for this package!! \r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "rjzamora",
        "created_at": "2024-04-17T18:45:22Z",
        "body": "Sounds good! Thanks again for the discussion @dangraysf \r\n\r\nI'll close this issue for now, but feel free to follow-up if you have other questions/concerns."
      }
    ]
  },
  {
    "number": 15246,
    "title": "[QST] Returning from multi-thread. TypeError: a bytes-like object is required, not 'dict'",
    "created_at": "2024-03-07T07:19:32Z",
    "closed_at": "2024-06-06T16:19:30Z",
    "labels": [
      "question",
      "doc"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/15246",
    "body": "When running my code with `cudf`, I got `TypeError: a bytes-like object is required, not 'dict'` in the multi-thread returning part.\r\n1. Running the code without `-m cudf.pandas` option is *fine*.\r\n2. It's *okay* if each multi-thread branch returns merely a scalar.\r\n3. Program **CRUSHES** if a multi-thread branch returns a dataframe.\r\n\r\nThis is the code message:\r\n```\r\nconcurrent.futures.process._RemoteTraceback:\r\n'''\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.9/concurrent/futures/process.py\", line 387, in wait_result_broken_or_wakeup\r\n    result_item = result_reader.recv()\r\n  File \"/usr/lib64/python3.9/multiprocessing/connection.py\", line 255, in recv\r\n    return _ForkingPickler.loads(buf.getbuffer())\r\n  File \"/usr/local/lib64/python3.9/site-packages/cudf/pandas/fast_slow_proxy.py\", line 742, in __setstate__\r\n    unpickled_wrapped_obj = pickle.loads(state)\r\nTypeError: a bytes-like object is required, not 'dict'\r\n'''\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib64/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib64/python3.9/site-packages/cudf/pandas/__main__.py\", line 91, in <module>\r\n    main()\r\n  File \"/usr/local/lib64/python3.9/site-packages/cudf/pandas/__main__.py\", line 87, in main\r\n    runpy.run_path(args.args[0], run_name=\"__main__\")\r\n  File \"/usr/lib64/python3.9/runpy.py\", line 288, in run_path\r\n    return _run_module_code(code, init_globals, run_name,\r\n  File \"/usr/lib64/python3.9/runpy.py\", line 97, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n  File \"/usr/lib64/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"clean_header.py\", line 48, in <module>\r\n    main()\r\n  File \"clean_header.py\", line 45, in main\r\n    my_func()\r\n  File \"clean_header.py\", line 39, in my_func\r\n    for obj in r:\r\n  File \"/usr/lib64/python3.9/concurrent/futures/process.py\", line 562, in _chain_from_iterable_of_lists\r\n    for element in iterable:\r\n  File \"/usr/lib64/python3.9/concurrent/futures/_base.py\", line 609, in result_iterator\r\n    yield fs.pop().result()\r\n  File \"/usr/lib64/python3.9/concurrent/futures/_base.py\", line 439, in result\r\n    return self.__get_result()\r\n  File \"/usr/lib64/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\r\n    raise self._exception\r\nconcurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.\r\n```\r\n\r\n\r\nHere is my code.\r\n```\r\nfrom datetime import datetime, timedelta, date\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom random import randint\r\nimport swifter\r\nimport json, sys, os\r\nfrom cudf.pandas.module_accelerator import disable_module_accelerator\r\n\r\nfrom functools import partial\r\nfrom concurrent.futures import ProcessPoolExecutor as Pool\r\nfrom multiprocessing import set_start_method\r\n\r\n\r\ndef data_generation(nRows: int):\r\n################## unimportant, for reproducing purpose ###################\r\n# This function generates the dataframe obj, which has 5 columns, and the data are sorted by WorkingDay and Minute ascendingly\r\n    my_df = pd.DataFrame(data={'WorkingDay': ['2019-01-02', '2018-01-02', '2019-05-02', '2020-01-02', '2021-01-02'], 'name': ['albert', 'alex', 'alice', 'ben', 'bob'], 'Minute': ['09:00:00', '09:20:00', '08:00:00', '07:00:00', '09:30:00'], 'aaa': np.random.rand(5), 'bbb': np.    random.rand(5)})\r\n    my_df = pd.concat([my_df for i in range(int(nRows/5))], axis=0)\r\n    my_df['WorkingDay'] = my_df['WorkingDay'].map(lambda x: (date(randint(2010,2020), randint(1,4), randint(1,5))).strftime('%Y-%m-%d'))\r\n    my_df['Minute'] = np.random.permutation(my_df['Minute'].values)\r\n    my_df = my_df.sort_values(by=['WorkingDay', 'Minute'], inplace=False).reset_index(drop=True,inplace=False)\r\n    return my_df\r\n\r\ndef my_func_single(branchIndex: int):\r\n    my_df = data_generation(20-5*branchIndex)\r\n# data generated\r\n#############################################################################\r\n    # The multi-thread return is problematic\r\n#############################################################################\r\n    #return my_df.shape[0]\r\n    return my_df\r\n\r\n\r\ndef my_func():\r\n    set_start_method('spawn')\r\n    my_func_partial = partial(my_func_single)\r\n    with Pool(max_workers=2) as pool:\r\n        r = pool.map(my_func_partial, range(4))\r\n    for obj in r:\r\n        #print('df has length: {}.'.format(obj))\r\n        print('df has length: {}.'.format(obj.shape[0]))\r\n\r\ndef main():\r\n    print('-------------------- program starts -----------------------')\r\n    my_func()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nRelevant dependencies:\r\n```\r\ncuda-python==12.4.0\r\ncudf-cu12==24.4.0a516\r\ncugraph-cu12==24.4.0a69\r\ncuml-cu12==24.4.0a37\r\ndask==2024.1.1\r\ndask-cuda==24.4.0a11\r\ndask-cudf-cu12==24.4.0a516\r\npylibcugraph-cu12==24.4.0a69\r\npylibraft-cu12==24.4.0a70\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/15246/comments",
    "author": "blue-cat-whale",
    "comments": [
      {
        "user": "blue-cat-whale",
        "created_at": "2024-03-07T07:39:36Z",
        "body": "I tried another parallel mechanism and a similar error appers.\r\n\r\nThe new code:\r\n```\r\ndef my_func():\r\n  num_cores = 2\r\n  inputs = range(4)\r\n  results = Parallel(n_jobs=num_cores)(delayed(my_func_single)(i) for i in inputs)\r\n  for obj in results:   \r\n    print('df has length: {}.'.format(obj.shape[0]))\r\n\r\ndef main():\r\n  print('-------------------- program starts -----------------------')\r\n  my_func()  \r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\n\r\nThe error message:\r\n```\r\njoblib.externals.loky.process_executor._RemoteTraceback:\r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib64/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 661, in wait_result_broken_or_wakeup\r\n    result_item = result_reader.recv()\r\n  File \"/usr/lib64/python3.9/multiprocessing/connection.py\", line 255, in recv\r\n    return _ForkingPickler.loads(buf.getbuffer())\r\n  File \"/usr/local/lib64/python3.9/site-packages/cudf/pandas/fast_slow_proxy.py\", line 742, in __setstate__\r\n    unpickled_wrapped_obj = pickle.loads(state)\r\nTypeError: a bytes-like object is required, not 'dict'\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib64/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib64/python3.9/site-packages/cudf/pandas/__main__.py\", line 91, in <module>\r\n    main()\r\n  File \"/usr/local/lib64/python3.9/site-packages/cudf/pandas/__main__.py\", line 87, in main\r\n    runpy.run_path(args.args[0], run_name=\"__main__\")\r\n  File \"/usr/lib64/python3.9/runpy.py\", line 288, in run_path\r\n    return _run_module_code(code, init_globals, run_name,\r\n  File \"/usr/lib64/python3.9/runpy.py\", line 97, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n  File \"/usr/lib64/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"clean_header.py\", line 49, in <module>\r\n    main()\r\n  File \"clean_header.py\", line 45, in main\r\n    my_func()\r\n  File \"clean_header.py\", line 38, in my_func\r\n    results = Parallel(n_jobs=num_cores)(delayed(my_func_single)(i) for i in inputs)\r\n  File \"/usr/local/lib64/python3.9/site-packages/joblib/parallel.py\", line 1952, in __call__\r\n    return output if self.return_generator else list(output)\r\n  File \"/usr/local/lib64/python3.9/site-packages/joblib/parallel.py\", line 1595, in _get_outputs\r\n    yield from self._retrieve()\r\n  File \"/usr/local/lib64/python3.9/site-packages/joblib/parallel.py\", line 1699, in _retrieve\r\n    self._raise_error_fast()\r\n  File \"/usr/local/lib64/python3.9/site-packages/joblib/parallel.py\", line 1734, in _raise_error_fast\r\n    error_job.get_result(self.timeout)\r\n  File \"/usr/local/lib64/python3.9/site-packages/joblib/parallel.py\", line 736, in get_result\r\n    return self._return_or_raise()\r\n  File \"/usr/local/lib64/python3.9/site-packages/joblib/parallel.py\", line 754, in _return_or_raise\r\n    raise self._result\r\njoblib.externals.loky.process_executor.BrokenProcessPool: A result has failed to un-serialize. Please ensure that the objects returned by the function are always picklable.\r\n```"
      },
      {
        "user": "blue-cat-whale",
        "created_at": "2024-03-07T09:48:56Z",
        "body": "I use `return my_df.values, list(my_df.index)` to bypass this problem."
      },
      {
        "user": "vyasr",
        "created_at": "2024-03-07T18:19:57Z",
        "body": "I'm glad you were able to get the issue resolved in your case! That said, it does look like you're highlighting a real issue with using cudf.pandas objects in multiprocessing, so I'm going to reopen this issue for now. Here's a MWE for future investigation indicating that it's also sensitive to how the process is created. Since fork works while spawn does not, we're probably relying on some implicit state being preserved that is lost when a new process is spawned.\r\n\r\n```\r\n# Works correctly for `import cudf as pd`\r\nimport pandas as pd\r\n\r\nfrom concurrent.futures import ProcessPoolExecutor as Pool\r\nfrom multiprocessing import set_start_method\r\n\r\n\r\ndef f(i: int):\r\n    return pd.DataFrame({'a': [i]})\r\n\r\n\r\ndef main():\r\n    for method in ['fork', 'spawn', 'forkserver']:\r\n        set_start_method(method, force=True)\r\n        with Pool(max_workers=2) as pool:\r\n            r = pool.map(f, range(4))\r\n        try:\r\n            list(r)\r\n        except Exception as e:\r\n            print(f'{type(e).__name__}: {method}')\r\n        else:\r\n            print(f'Succeeded: {method}')\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```"
      },
      {
        "user": "wence-",
        "created_at": "2024-03-11T09:52:58Z",
        "body": "This problem exhibits because when using `spawn` or `forkserver`, the new python process that is started by `multiprocessing` does not have the custom cudf.pandas metapath finder installed. Hence, the import of pandas as `import pandas as pd` fetches the real (unwrapped) pandas module, rather than the wrapped (cudf.pandas) module.\r\n\r\nConsider:\r\n\r\n```python\r\nimport sys\r\nfrom concurrent.futures import ProcessPoolExecutor as Pool\r\nfrom multiprocessing import set_start_method\r\n\r\ndef f():\r\n    print(sys.meta_path)\r\n\r\ndef main():\r\n    for method in ['fork', 'spawn', 'forkserver']:\r\n        print(method)\r\n        set_start_method(method, force=True)\r\n        with Pool(max_workers=1) as pool:\r\n            result = pool.submit(f).result()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nWhen run with `python -m cudf.pandas bug.py`:\r\n```\r\nfork\r\n[ModuleAccelerator(fast=cudf, slow=pandas), <_distutils_hack.DistutilsMetaFinder object at 0x76f18b8991e0>, <_rmm_editable.ScikitBuildRedirectingFinder object at 0x76f18ba67fa0>, <_cudf_kafka_editable.ScikitBuildRedirectingFinder object at 0x76f18ba64700>, <_cudf_editable.ScikitBuildRedirectingFinder object at 0x76f18bb2b3d0>, <class '_frozen_importlib.BuiltinImporter'>, <class '_frozen_importlib.FrozenImporter'>, <class '_frozen_importlib_external.PathFinder'>, <six._SixMetaPathImporter object at 0x76f04651b4c0>]\r\n ^^^^^^^^^^^^^^^^^^^ Good!\r\nspawn\r\n[<_distutils_hack.DistutilsMetaFinder object at 0x78af5ec412d0>, <_rmm_editable.ScikitBuildRedirectingFinder object at 0x78af5ec405b0>, <_cudf_kafka_editable.ScikitBuildRedirectingFinder object at 0x78af5ee0c7f0>, <_cudf_editable.ScikitBuildRedirectingFinder object at 0x78af5eed74c0>, <class '_frozen_importlib.BuiltinImporter'>, <class '_frozen_importlib.FrozenImporter'>, <class '_frozen_importlib_external.PathFinder'>]\r\n ^ BAD!\r\nforkserver\r\n[<_distutils_hack.DistutilsMetaFinder object at 0x7c5cd58e92a0>, <_rmm_editable.ScikitBuildRedirectingFinder object at 0x7c5cd58e8580>, <_cudf_kafka_editable.ScikitBuildRedirectingFinder object at 0x7c5cd58a47c0>, <_cudf_editable.ScikitBuildRedirectingFinder object at 0x7c5cd596f490>, <class '_frozen_importlib.BuiltinImporter'>, <class '_frozen_importlib.FrozenImporter'>, <class '_frozen_importlib_external.PathFinder'>]\r\n```\r\n\r\nThe way one can work around this is to use the functional interface to cudf.pandas and install manually at the start of the file. Note that this must be done before an import of pandas. So:\r\n\r\n```\r\nimport cudf.pandas\r\ncudf.pandas.install()\r\n\r\nimport pandas as pd\r\nfrom concurrent.futures import ProcessPoolExecutor as Pool\r\nfrom multiprocessing import set_start_method\r\n\r\n\r\ndef f(i: int):\r\n    return pd.DataFrame({'a': [i]})\r\n\r\n\r\ndef main():\r\n    for method in ['fork', 'spawn', 'forkserver']:\r\n        set_start_method(method, force=True)\r\n        with Pool(max_workers=2) as pool:\r\n            r = pool.map(f, range(4))\r\n        try:\r\n            list(r)\r\n        except Exception as e:\r\n            print(f'{type(e).__name__}: {method}')\r\n        else:\r\n            print(f'Succeeded: {method}')\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nWill work for all three cases."
      },
      {
        "user": "wence-",
        "created_at": "2024-03-11T09:53:12Z",
        "body": "We should probably add this as a known limitation in the FAQ."
      }
    ]
  },
  {
    "number": 14500,
    "title": "[QST] cudf.pandas prefer using CPU over GPU in some cases",
    "created_at": "2023-11-27T18:14:30Z",
    "closed_at": "2025-01-31T00:35:56Z",
    "labels": [
      "question",
      "0 - Waiting on Author",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/14500",
    "body": "Hi,\r\nI'm trying to move from a basic pandas to cudf.pandas and I faced with the issue. It's not clear how cudf decides to use **CPU** or **GPU** in calculations.\r\nHere is the example when I have a dataframe with around 280kk rows and 9 columns.\r\nThe steps:\r\n1) I perform `.groupby.sum() `for the original df. I takes too much time and the profiler show that all calculations were on **CPU** not GPU.\r\n2) I cut df like `[:100000000]` so that there are 100kk rows left.\r\n3)  I perform `.groupby.sum() `for the modified df and... it takes 0.1 sec and the profiler says **GPU** was using for that.\r\n\r\nSo, here is some question.\r\n- what's the reason that 100kk df is being calculated on GPU and 280kk df on CPU? Hard to belive that the size is the reason.\r\n- If not the size then what's the criteria for that?\r\n\r\nThanks in advance.\r\np.s. I also tried `.sort_values()` and there were the same.\r\n\r\n```\r\nCOM_ORDER_LINE.shape\r\n(284125143, 9)\r\n```\r\n```\r\nCOM_ORDER_LINE.head()\r\n\r\nCODE | ORDER_CODE | VERSION_CODE | ID_WARE | QTY_ORDERED | CATALOG_PRICE | PRICE | TO_PAY | DISCOUNT_TOTAL\r\n10000006215177 | 10000006215175 | 10000006215176 | 1.787585e+11 | 1 | 3799.0 | 2659.0 | 2659.0 | 1140.0\r\n10000006215189 | 10000006215187 | 10000006215188 | 1.736505e+11 | 1 | 9999.0 | 6999.0 | 6999.0 | 3000.0\r\n10000006215364 | 10000006215362 | 10000006215363 | 1.736709e+11 | 1 | 1399.0 | 980.0 | 980.0 | 419.0\r\n```\r\n```\r\n%%cudf.pandas.profile\r\ndf=COM_ORDER_LINE.groupby(['ID_WARE'])['PRICE'].sum()\r\n```\r\n\r\n\r\n```\r\nTotal time elapsed: 31.764 seconds                                    \r\n                                          0 GPU function calls in 0.000 seconds                                   \r\n                                          3 CPU function calls in 23.186 seconds                                  \r\n                                                                                                                  \r\n                                                          Stats                                                   \r\n                                                                                                                  \r\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\r\n┃ Function                     ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃\r\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\r\n│ DataFrame.groupby            │ 0          │ 0.000       │ 0.000       │ 1          │ 2.929       │ 2.929       │\r\n│ DataFrameGroupBy.__getitem__ │ 0          │ 0.000       │ 0.000       │ 1          │ 2.915       │ 2.915       │\r\n│ SeriesGroupBy.sum            │ 0          │ 0.000       │ 0.000       │ 1          │ 17.341      │ 17.341      │\r\n└──────────────────────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘\r\n\r\nNot all pandas operations ran on the GPU. The following functions required CPU fallback:\r\n\r\n- DataFrame.groupby\r\n- DataFrameGroupBy.__getitem__\r\n- SeriesGroupBy.sum\r\n```\r\n\r\n```\r\nCOM_ORDER_LINE_100KK = COM_ORDER_LINE[:100000000]\r\nCOM_ORDER_LINE_100KK.shape\r\n(100000000, 9)\r\n```\r\n\r\n```\r\n%%cudf.pandas.profile\r\ndf=COM_ORDER_LINE_100KK.groupby(['ID_WARE'])['PRICE'].sum()\r\n```\r\n\r\n```\r\nTotal time elapsed: 0.109 seconds                                     \r\n                                          3 GPU function calls in 0.082 seconds                                   \r\n                                          0 CPU function calls in 0.000 seconds                                   \r\n                                                                                                                  \r\n                                                          Stats                                                   \r\n                                                                                                                  \r\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\r\n┃ Function                     ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃\r\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\r\n│ DataFrame.groupby            │ 1          │ 0.000       │ 0.000       │ 0          │ 0.000       │ 0.000       │\r\n│ DataFrameGroupBy.__getitem__ │ 1          │ 0.001       │ 0.001       │ 0          │ 0.000       │ 0.000       │\r\n│ SeriesGroupBy.sum            │ 1          │ 0.081       │ 0.081       │ 0          │ 0.000       │ 0.000       │\r\n└──────────────────────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/14500/comments",
    "author": "kolfild26",
    "comments": [
      {
        "user": "bdice",
        "created_at": "2023-11-27T18:20:23Z",
        "body": "I suspect this is hitting an out-of-memory error on the GPU and falling back to the CPU. What GPU are you using?\r\n\r\nIf your columns are int64/float64 types, (284 million rows * 9 columns * 8 bytes per element) gives about 20 GB of memory consumption for the data alone, before the intermediate storage needed for the groupby computation and results."
      },
      {
        "user": "kolfild26",
        "created_at": "2023-11-27T22:05:59Z",
        "body": "```\r\nCODE                int64\r\nORDER_CODE          int64\r\nVERSION_CODE        int64\r\nID_WARE           float64\r\nQTY_ORDERED         int64\r\nCATALOG_PRICE     float64\r\nPRICE             float64\r\nTO_PAY            float64\r\nDISCOUNT_TOTAL    float64\r\n```\r\nI played with the size, at some point it starts falling `DataFrame.groupby` back  the CPU and then `SeriesGroupBy.sum` too.\r\nBut the point it starts falling is around 110.000.000 that corresponds to ~7.4Gb.\r\nMy GPU is **Tesla V100-PCIE-32GB**\r\n\r\n                                                                                                                  \r\n```                                                                                                    \r\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\r\n┃ Function                     ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃\r\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\r\n│ DataFrame.groupby            │ 1          │ 0.000       │ 0.000       │ 0          │ 0.000       │ 0.000       │\r\n│ DataFrameGroupBy.__getitem__ │ 1          │ 0.001       │ 0.001       │ 0          │ 0.000       │ 0.000       │\r\n│ SeriesGroupBy.sum            │ 0          │ 0.000       │ 0.000       │ 1          │ 20.493      │ 20.493      │\r\n└──────────────────────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘\r\n```\r\n```\r\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\r\n┃ Function                     ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃\r\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\r\n│ DataFrame.groupby            │ 0          │ 0.000       │ 0.000       │ 1          │ 0.681       │ 0.681       │\r\n│ DataFrameGroupBy.__getitem__ │ 0          │ 0.000       │ 0.000       │ 1          │ 0.677       │ 0.677       │\r\n│ SeriesGroupBy.sum            │ 0          │ 0.000       │ 0.000       │ 1          │ 7.586       │ 7.586       │\r\n└──────────────────────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘\r\n```\r\n"
      },
      {
        "user": "shwina",
        "created_at": "2023-11-27T22:17:35Z",
        "body": "Thanks for exploring!\r\n\r\nJust curious, does it help to filter out just the `ID` and `PRICE` columns _before_ the groupby? If so, we might be missing out on some optimizations internally and that would be a bug:\r\n\r\n```python\r\ndf = df[[\"ID\", \"TOTAL\"]]\r\nresult = df.groupby(\"ID\").sum()\r\n```\r\n\r\nAt the same time, you can try turning cuDF's spilling on to spill unused data:\r\n\r\n```\r\nCUDF_SPILL=1 python -m cudf.pandas ...\r\n```\r\n"
      },
      {
        "user": "kolfild26",
        "created_at": "2023-11-28T21:11:59Z",
        "body": "@shwina\r\nThanks fro your updates. My answers are below.\r\n\r\n1️⃣ \r\n> Just curious, does it help to filter out just the ID and PRICE columns before the groupby? If so, we might be missing out on some optimizations internally and that would be a bug:\r\n> \r\n> > df = df[[\"ID\", \"TOTAL\"]]\r\n> > result = df.groupby(\"ID\").sum()\r\n\r\nYes, that's what I see now. Filtering out two columns before the groupby fixes all. The groupby is again on the GPU.\r\n\r\n```\r\n%%cudf.pandas.profile\r\ndf=COM_ORDER_LINE.groupby(['ID_WARE'])['PRICE'].sum()\r\n                                                                                                                  \r\n                                            Total time elapsed: 27.720 seconds                                    \r\n                                          0 GPU function calls in 0.000 seconds                                   \r\n                                          3 CPU function calls in 20.844 seconds                                  \r\n                                                                                                                                                                                                                                        \r\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\r\n┃ Function                     ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃\r\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\r\n│ DataFrame.groupby            │ 0          │ 0.000       │ 0.000       │ 1          │ 2.359       │ 2.359       │\r\n│ DataFrameGroupBy.__getitem__ │ 0          │ 0.000       │ 0.000       │ 1          │ 2.334       │ 2.334       │\r\n│ SeriesGroupBy.sum            │ 0          │ 0.000       │ 0.000       │ 1          │ 16.152      │ 16.152      │\r\n└──────────────────────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘\r\n```\r\n```\r\nCOM_ORDER_LINE_tmp=COM_ORDER_LINE[['ID_WARE', 'PRICE']]\r\n```\r\n```\r\n%%cudf.pandas.profile\r\ndf=COM_ORDER_LINE_tmp.groupby(['ID_WARE'])['PRICE'].sum()\r\n                                            Total time elapsed: 0.358 seconds                                     \r\n                                          3 GPU function calls in 0.329 seconds                                   \r\n                                          0 CPU function calls in 0.000 seconds                                   \r\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\r\n┃ Function                     ┃ GPU ncalls ┃ GPU cumtime ┃ GPU percall ┃ CPU ncalls ┃ CPU cumtime ┃ CPU percall ┃\r\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\r\n│ DataFrame.groupby            │ 1          │ 0.001       │ 0.001       │ 0          │ 0.000       │ 0.000       │\r\n│ DataFrameGroupBy.__getitem__ │ 1          │ 0.001       │ 0.001       │ 0          │ 0.000       │ 0.000       │\r\n│ SeriesGroupBy.sum            │ 1          │ 0.327       │ 0.327       │ 0          │ 0.000       │ 0.000       │\r\n└──────────────────────────────┴────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘\r\n```\r\n\r\n2️⃣ \r\n```\r\nAt the same time, you can try turning cuDF's spilling on to spill unused data:\r\n```\r\n\r\nI also noticed that when I restart the machine, the first try after the restart is successfully being calculated on the GPU. Only once. Next run is falling back to the CPU again.\r\nSo, it looks like that the gpu memory really needs to be cleaned from unused data.\r\n\r\n```\r\nCUDF_SPILL=1 python -m cudf.pandas ...\r\n```\r\nIs there any way to switch on this option in the jupyter notebook? "
      },
      {
        "user": "shwina",
        "created_at": "2023-11-28T21:42:51Z",
        "body": "> Filtering out two columns before the groupby fixes all.\r\n\r\nThanks! We'll investigate whether we can optimize things so that you don't have to do this filter, and report back here.\r\n\r\n> I also noticed that when I restart the machine, the first try after the restart is successfully being calculated on the GPU. Only once. Next run is falling back to the CPU again.\r\n\r\nAh, interesting. There are a few possibilities then:\r\n\r\n- We have a memory leak (this would be bad)\r\n- Some Python objects are caught in reference cycles and haven't been cleared. You can try running `gc.collect()` to release the memory associated with those objects and see if that helps with memory usage.\r\n\r\nCan you try the following:\r\n\r\n```Python\r\nimport gc\r\n\r\n# run groupby-sum for the first time\r\ngc.collect()\r\n# run groupby-sum for the second time\r\n```\r\n\r\nand let us know if that works?"
      },
      {
        "user": "shwina",
        "created_at": "2023-11-28T21:54:00Z",
        "body": "> Is there any way to switch on this option in the jupyter notebook?\r\n\r\nYes -- you can run jupyter notebook with:\r\n\r\n```\r\nCUDF_SPILL=1 jupyter notebook ...\r\n```"
      },
      {
        "user": "kolfild26",
        "created_at": "2023-11-30T00:58:51Z",
        "body": "`gc.collect()` doesn't change anything. \r\nNo need to fully restart(as I mentioned above) but just to recreate the conda env with rapids and cuda like:\r\n`conda create -n rapids-23.10 -c rapidsai -c conda-forge -c nvidia rapids=23.10 python=3.10 cuda-version=12.0`\r\nand the groupby() works out several times and then starts falling back to the CPU. The exact number of times before the falling is unpredictable, it might be 1 (as I said earlier) or 2-3, around.\r\nI've also tried the `sort_values()` and the `join() `operations. The same picture in there. There is a limit of the df size below which it's all on the GPU and above which it's  going to the CPU. The only thing that varies is the size. Each of that three operations has its own limit in my case. For the groupby and sort_values it's around ~100kk. For the join (_inner_ for instance) is to be ~10kk.\r\n\r\n> CUDF_SPILL=1\r\n\r\nI tried this and **periodically** I catch the warning:\r\n```\r\n[WARNING] RMM allocation of 2.12GiB bytes failed, spill-on-demand couldn't find any device memory to spill:\r\n<SpillManager spill_on_demand=True device_memory_limit=N/A | 7.38GiB spilled | 22.28GiB (100%) unspilled (unspillable)>\r\ntraceback\r\n```\r\nAnd, unfortunatelly _spilling_ doesn't help here too, no matter with or w/o this warning."
      },
      {
        "user": "kolfild26",
        "created_at": "2023-11-30T01:03:31Z",
        "body": "`7.38GiB spilled | 22.28GiB (100%) unspilled (unspillable)`\r\n🤔  might this be the cause? Looks like it says that only 7,4Gb from 32Gb were available for that operation."
      },
      {
        "user": "shwina",
        "created_at": "2023-11-30T01:22:45Z",
        "body": "Thanks for looking into it!\r\n\r\n> There is a limit of the df size below which it's all on the GPU and above which it's going to the CPU. The only thing that varies is the size. Each of that three operations has its own limit in my case. For the groupby and sort_values it's around ~100kk. For the join (inner for instance) is to be ~10kk.\r\n\r\nYeah this variability makes sense. The amount of intermediate memory required by a `join` operation can be very different from that required by `groupby` or `sort`.\r\n\r\nOn the `groupby` front, it does sound like we can optimize things so that you don't have to do a filter of the columns before `groupby`. \r\n\r\nI think you're essentially running up to the limitation that operations on larger data require more than the available GPU memory. While spilling can _sometimes_ help with that, it doesn't seem to in this particular situation. So the operations end up executing on CPU.\r\n\r\n--- \r\n\r\nTaking a step back, are you able to share what your workflow looks like? Perhaps we can provide more useful/specific suggestions if we can see the whole code. "
      },
      {
        "user": "kolfild26",
        "created_at": "2023-12-01T19:08:53Z",
        "body": "> are you able to share what your workflow looks like? Perhaps we can provide more useful/specific suggestions if we can see the whole code.\r\n\r\nI just started learning the cudf from exploring its boundaries. So, didn't apply it for the actual pipeline yet. But will definitely do this!\r\nWill raise an issue if I have any questions. Or I will update this one if it's relevant.\r\nThanks for your help.\r\n"
      },
      {
        "user": "vyasr",
        "created_at": "2025-01-31T00:35:56Z",
        "body": "I'm going to close this since the above discussion seems mostly resolved, but feel free to reopen and add more information if needed."
      }
    ]
  },
  {
    "number": 14395,
    "title": "[QST]use cudf.concat slower than pandas.concat",
    "created_at": "2023-11-10T10:37:20Z",
    "closed_at": "2023-11-10T14:15:25Z",
    "labels": [
      "question",
      "Performance"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/14395",
    "body": "** cudf.concat slower than pandas.concat**\r\n## here is my code:\r\n```\r\nimport os, time, pandas as pd, numpy as np\r\nimport cudf\r\nfrom tqdm import tqdm\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES']='1'\r\ndef pd_concat_test(df):\r\n    st = 10\r\n    tdf = df[:st].copy()\r\n    n = len(df) - st\r\n    ta = time.time()\r\n    for i in tqdm(range(st, len(df))):\r\n        tdf = pd.concat([tdf, df[i:i+1]])\r\n    tb = time.time()\r\n\r\n    print(f'pd concat {n} times cost {tb-ta :.2f} s.')\r\n\r\n\r\ndef cupd_concat_test(cdf):\r\n    st = 10\r\n    tdf = cdf[:st].copy()\r\n    n = len(cdf) - st\r\n    ta = time.time()\r\n    for i in tqdm(range(st, len(cdf))):\r\n        tdf = cudf.concat([tdf, cdf[i:i+1]])\r\n    tb = time.time()\r\n\r\n    print(f'cudf concat {n} times cost {tb-ta :.2f} s.')\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    \r\n    in_csv = 'target.csv'\r\n    \r\n    df = pd.read_csv(in_csv)\r\n    cdf = cudf.read_csv(in_csv)\r\n    print(df.head(5))\r\n    cupd_concat_test(cdf)\r\n    pd_concat_test(df)\r\n```\r\n## here's output:\r\n```\r\n       timestamp     open     high      low    close    volume  quote_volume\r\n0  1577836800000  7189.43  7190.52  7170.15  7171.55  2449.049   17576407.75\r\n1  1577840400000  7171.55  7225.00  7171.10  7210.24  3865.038   27838016.40\r\n2  1577844000000  7210.24  7239.30  7206.46  7237.99  3228.365   23324787.16\r\n3  1577847600000  7237.41  7239.74  7215.00  7221.65  2513.307   18161803.91\r\n4  1577851200000  7221.65  7225.41  7211.22  7213.86  1176.666    8493621.94\r\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33398/33398 [01:01<00:00, 542.44it/s]\r\ncudf concat 33398 times cost 61.57 s.\r\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33398/33398 [00:14<00:00, 2279.08it/s]\r\npd concat 33398 times cost 14.65 s.\r\n```\r\n## here's my env: \r\n* python3.10.4\r\n*  nvcc -V\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Thu_Nov_18_09:45:30_PST_2021\r\nCuda compilation tools, release 11.5, V11.5.119\r\nBuild cuda_11.5.r11.5/compiler.30672275_0\r\n```\r\n* pip list |grep cu\r\n```\r\ncubinlinker-cu11          0.3.0.post1\r\ncucim                     23.10.0\r\ncuda-python               11.8.3\r\ncudf-cu11                 23.10.1\r\ncugraph-cu11              23.10.0\r\ncuml-cu11                 23.10.0\r\ncuproj-cu11               23.10.0\r\ncupy-cuda11x              12.2.0\r\ncuspatial-cu11            23.10.0\r\ncuxfilter-cu11            23.10.0\r\ndask-cuda                 23.10.0\r\ndask-cudf-cu11            23.10.1\r\ndocutils                  0.20\r\nexecuting                 1.2.0\r\nptxcompiler-cu11          0.7.0.post1\r\npylibcugraph-cu11         23.10.0\r\npylibraft-cu11            23.10.0\r\nraft-dask-cu11            23.10.0\r\nrmm-cu11                  23.10.0\r\ntorch                     1.12.0+cu113\r\nucx-py-cu11               0.34.0\r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/14395/comments",
    "author": "haoran1062",
    "comments": [
      {
        "user": "shwina",
        "created_at": "2023-11-10T11:44:14Z",
        "body": "Hi @haoran1062 -- thank you for reporting! Please let me know if the below answers your question:\r\n\r\n## Why is it slow?\r\n\r\nGPUs are generally faster because they operate on data in parallel. If you have very small operations (e.g., involving a single row), there is little or no parallelism that the GPU can take advantage of. \r\n\r\n## Operate on larger chunks of data to see the benefit from GPUs\r\n\r\nEach `concat` operation in your example appends a single row to a dataframe. In general, you will not see the benefit of the GPU for very small operations like these. You may even see some slowdown for very small operations compared to the CPU.\r\n\r\nLet's modify the example to use `concat` with larger chunks. In the snippet below, each concat appends a dataframe of size `10_000` to ultimately produce a dataframe of size `10_000_000`. The speedup from using the GPU should be more obvious:\r\n\r\n```\r\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 999/999 [00:02<00:00, 441.12it/s]\r\ncudf concat 999 times cost 2.27 s.\r\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 999/999 [00:34<00:00, 28.90it/s]\r\npd concat 999 times cost 34.57 s.\r\n```\r\n\r\nThe code:\r\n\r\n```python\r\nimport os, time, pandas as pd, numpy as np\r\nimport cudf\r\nfrom tqdm import tqdm\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES']='1'\r\ndef pd_concat_test(df):\r\n    chunk_size = 10_000\r\n    tdf = df[:chunk_size].copy()\r\n    n = len(df) - chunk_size\r\n    ta = time.time()\r\n    rng = range(chunk_size, len(df), chunk_size)\r\n    for i, chunk_start in enumerate(tqdm(rng)):\r\n        chunk_end = chunk_start + chunk_size        \r\n        tdf = pd.concat([tdf, df[chunk_start:chunk_end]])\r\n    tb = time.time()\r\n\r\n    print(f'pd concat {i+1} times cost {tb-ta :.2f} s.')\r\n    return tdf\r\n\r\n\r\ndef cupd_concat_test(cdf):\r\n    chunk_size = 10_000\r\n    tdf = cdf[:chunk_size].copy()\r\n    n = len(cdf) - chunk_size\r\n    ta = time.time()\r\n    rng = range(chunk_size, len(cdf), chunk_size)\r\n    for i, chunk_start in enumerate(tqdm(rng)):\r\n        chunk_end = chunk_start + chunk_size\r\n        tdf = cudf.concat([tdf, cdf[chunk_start:chunk_end]])\r\n    tb = time.time()\r\n\r\n    print(f'cudf concat {i+1} times cost {tb-ta :.2f} s.')\r\n    return tdf\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    cdf = cudf.datasets.randomdata(10_000_000)\r\n    df = cdf.to_pandas()\r\n    print(\"Heads:\")\r\n    print(cdf.head())\r\n    print(df.head())\r\n\r\n    print(\"Tails:\")\r\n    print(cdf.tail())\r\n    print(df.tail())\r\n    cdf = cupd_concat_test(cdf)\r\n    df = pd_concat_test(df)\r\n    print(\"Heads:\")\r\n    print(cdf.head())\r\n    print(df.head())\r\n\r\n    print(\"Tails:\")\r\n    print(cdf.tail())\r\n    print(df.tail())\r\n\r\n```\r\n\r\n"
      },
      {
        "user": "haoran1062",
        "created_at": "2023-11-10T12:13:17Z",
        "body": "@shwina thanks for your answer. I tested it according to your suggestions and the results are really good. \r\n```\r\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1282/1282 [00:17<00:00, 71.86it/s]\r\ncudf concat 1282 times cost 17.84 s.\r\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1282/1282 [02:49<00:00,  7.58it/s]\r\npd concat 1282 times cost 169.24 s.\r\n```\r\nbut I do really have that question Do you have any suitable suggestions how to speed up add columns one by one with high frequency operation"
      },
      {
        "user": "shwina",
        "created_at": "2023-11-10T12:17:23Z",
        "body": "Thanks! I'll close this issue out, but please feel free to reopen if you have any further questions. "
      },
      {
        "user": "haoran1062",
        "created_at": "2023-11-10T12:17:59Z",
        "body": "@shwina Do you have any suitable suggestions about how to speed up add rows one by one with high frequency operation？"
      },
      {
        "user": "shwina",
        "created_at": "2023-11-10T12:20:58Z",
        "body": "Sorry, I missed your question at the end.\r\n\r\n> Do you have any suitable suggestions how to speed up add columns one by one with high frequency operation？\r\n\r\nDo you mean rows, and not columns?\r\n\r\nCan you provide a bit more information about your use case: where is the data coming from? "
      },
      {
        "user": "haoran1062",
        "created_at": "2023-11-10T12:28:13Z",
        "body": "> Sorry, I missed your question at the end.\r\n> \r\n> > Do you have any suitable suggestions how to speed up add columns one by one with high frequency operation？\r\n> \r\n> Do you mean rows, and not columns?\r\n> \r\n> Can you provide a bit more information about your use case: where is the data coming from?\r\n\r\nmy bad, add one rows one by one .\r\nthe data is market data, which have timestamp, open, high, low, close, volume, and other data.\r\n"
      },
      {
        "user": "shwina",
        "created_at": "2023-11-10T14:01:19Z",
        "body": "Thanks! I think the way to do this is to collect your data in batches before appending it to the DataFrame. Here's how I would do it at a high level:\r\n\r\n```python\r\nimport cudf\r\nimport numpy as np\r\n\r\n\r\ndef producer():\r\n    for i in range(1_000_000):\r\n        ts = np.datetime64(\"now\") + np.timedelta64(i, \"s\")\r\n        yield (ts, np.random.rand(), np.random.rand(), np.random.rand(), np.random.rand(), np.random.rand())\r\n\r\n        \r\nif __name__ == \"__main__\":\r\n    batch_size = 100_000\r\n    \r\n    df = cudf.DataFrame()\r\n\r\n    records = np.recarray(batch_size, dtype=[(\"ts\", \"datetime64[ms]\"), (\"a\", \"float64\"), (\"b\", \"float64\"), (\"c\", \"float64\"), (\"d\", \"float64\"), (\"e\", \"float64\")])\r\n\r\n    for i, record in enumerate(producer()):\r\n        print(i)\r\n        # add the record to the batch\r\n        records[i % batch_size] = record\r\n        if i > 0 and (i % batch_size == 0):\r\n            # add the records to the DataFrame\r\n            df = cudf.concat([df, cudf.DataFrame.from_records(records)])\r\n    print(df.head())\r\n        \r\n```\r\n\r\nPlease forgive any minor mistakes there might be in the code above."
      },
      {
        "user": "shwina",
        "created_at": "2023-11-10T14:06:27Z",
        "body": "Note that the code above actually won't be much faster than pandas, since most of the time is spent populating the records rather than on any pandas operations."
      },
      {
        "user": "haoran1062",
        "created_at": "2023-11-10T14:15:26Z",
        "body": "> Note that the code above actually won't be much faster than pandas, since most of the time is spent populating the records rather than on any pandas operations.\r\n\r\nThanks a lot! I'll try your suggestion and thanks again for your great project! have a nice day~"
      }
    ]
  },
  {
    "number": 13651,
    "title": "[QST] cufio-drv:705 running in compatible mode",
    "created_at": "2023-07-03T00:01:08Z",
    "closed_at": "2024-05-15T19:20:48Z",
    "labels": [
      "question",
      "0 - Waiting on Author"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/13651",
    "body": "When reading a large file, the cufile.log will show \"cufio-drv:705 running in compatible mode\".",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/13651/comments",
    "author": "qzc438",
    "comments": [
      {
        "user": "GregoryKimball",
        "created_at": "2023-07-10T21:57:04Z",
        "body": "Hello @qzc438, thank you for trying out GDS support in cudf. Would you please clarify - are you interested in what \"compatible mode\" means in this case? Is GDS working well for your use case?"
      },
      {
        "user": "qzc438",
        "created_at": "2023-07-11T07:23:59Z",
        "body": "Hi Gregory, this is my question. I did not know why cufio went into a compatible mode. Could you please indicate the potential reasons for this?"
      },
      {
        "user": "GregoryKimball",
        "created_at": "2023-07-22T20:11:09Z",
        "body": "Please excuse the delay. It sounds like the cudf build you are using was not built with cuFile. Comparing with my workstation, I see `cufio-drv:705 running in compatible mode` in `cufile.log` when I'm running on a build that has not been setup with cuFile. Do you see an error when you set the environment variable `LIBCUDF_CUFILE_POLICY=ALWAYS`?"
      },
      {
        "user": "vyasr",
        "created_at": "2024-05-15T19:20:48Z",
        "body": "Closing as stale. Feel free to reopen if needed."
      }
    ]
  },
  {
    "number": 13007,
    "title": "[QST] How much memory Parquet Writer requires?",
    "created_at": "2023-03-24T10:18:43Z",
    "closed_at": "2023-09-13T05:32:53Z",
    "labels": [
      "question",
      "0 - Waiting on Author"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/13007",
    "body": "Hello all,\r\n\r\nI have a large DataFrame which occupies around 2Gb in memory, but when I try to write it to disk using `cudf.DataFrame.to_parquet` or `dask_cudf.DataFrame.to_parquet`, it raises `MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/rapids/include/rmm/mr/device/cuda_memory_resource.hpp` even though I still have 20Gb free in the GPU. \r\n\r\nHow much memory does the parquet writer consumes? Is this expected behavior? Also, is there a work around?\r\n\r\nThank you\r\n\r\nDataFrame metadata:\r\n- shape: (94080000, 7)\r\n- memory_usage sum: 2126208004\r\n\r\nError traceback:\r\n```python\r\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\r\n│ /workspaces/calcifer/cli.py:58 in my_app                                                         │\r\n│                                                                                                  │\r\n│   57 │   try:                                                                                    │\r\n│ ❱ 58 │   │   metrics = Pipeline(cfg).run()                                                       │\r\n│   59 │   except Exception:                                                                       │\r\n│                                                                                                  │\r\n│ /workspaces/calcifer/calcifer/fit.py:152 in run                                                  │\r\n│                                                                                                  │\r\n│   151 │   │   if self.cfg.PIPELINE.train:                                                        │\r\n│ ❱ 152 │   │   │   results = self.fit(trainer, model, data)                                       │\r\n│   153                                                                                            │\r\n│                                                                                                  │\r\n│ /workspaces/calcifer/calcifer/fit.py:100 in fit                                                  │\r\n│                                                                                                  │\r\n│    99 │   │   ckpt_path = None if self.cfg.PIPELINE.force_rerun else \"last\"                      │\r\n│ ❱ 100 │   │   trainer.fit(model, datamodule=data, ckpt_path=ckpt_path)                           │\r\n│   101 │   │   return self.get_min_monitored_metric(trainer)                                      │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:520 in  │\r\n│ fit                                                                                              │\r\n│                                                                                                  │\r\n│    519 │   │   self.strategy._lightning_module = model                                           │\r\n│ ❱  520 │   │   call._call_and_handle_interrupt(                                                  │\r\n│    521 │   │   │   self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule,  │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44 in      │\r\n│ _call_and_handle_interrupt                                                                       │\r\n│                                                                                                  │\r\n│    43 │   │   else:                                                                              │\r\n│ ❱  44 │   │   │   return trainer_fn(*args, **kwargs)                                             │\r\n│    45                                                                                            │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:559 in  │\r\n│ _fit_impl                                                                                        │\r\n│                                                                                                  │\r\n│    558 │   │   )                                                                                 │\r\n│ ❱  559 │   │   self._run(model, ckpt_path=ckpt_path)                                             │\r\n│    560                                                                                           │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:944 in  │\r\n│ _run                                                                                             │\r\n│                                                                                                  │\r\n│    943 │   │   if self.state.fn == TrainerFn.FITTING:                                            │\r\n│ ❱  944 │   │   │   call._call_callback_hooks(self, \"on_fit_end\")                                 │\r\n│    945 │   │   │   call._call_lightning_module_hook(self, \"on_fit_end\")                          │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:190 in     │\r\n│ _call_callback_hooks                                                                             │\r\n│                                                                                                  │\r\n│   189 │   │   │   with trainer.profiler.profile(f\"[Callback]{callback.state_key}.{hook_name}\")   │\r\n│ ❱ 190 │   │   │   │   fn(trainer, trainer.lightning_module, *args, **kwargs)                     │\r\n│   191                                                                                            │\r\n│                                                                                                  │\r\n│ /workspaces/calcifer/calcifer/history/callback.py:77 in on_fit_end                               │\r\n│                                                                                                  │\r\n│   76 │   │   │   if isinstance(attr, History):                                                   │\r\n│ ❱ 77 │   │   │   │   self.log_history_artifact(attr)                                             │\r\n│   78                                                                                             │\r\n│                                                                                                  │\r\n│ /workspaces/calcifer/calcifer/history/callback.py:83 in log_history_artifact                     │\r\n│                                                                                                  │\r\n│   82 │   │   breakpoint()                                                                        │\r\n│ ❱ 83 │   │   self.log_artifact(                                                                  │\r\n│   84 │   │   │   df,                                                                             │\r\n│                                                                                                  │\r\n│ /workspaces/calcifer/calcifer/abstract/callback.py:23 in log_artifact                            │\r\n│                                                                                                  │\r\n│   22 │   def log_artifact(self, *args, **kwargs) -> None:                                        │\r\n│ ❱ 23 │   │   return self._helper.log_artifact(*args, **kwargs)                                   │\r\n│   24                                                                                             │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/site-packages/lightning_utilities/core/rank_zero.py:27 in  │\r\n│ wrapped_fn                                                                                       │\r\n│                                                                                                  │\r\n│    26 │   │   if rank == 0:                                                                      │\r\n│ ❱  27 │   │   │   return fn(*args, **kwargs)                                                     │\r\n│    28 │   │   return None                                                                        │\r\n│                                                                                                  │\r\n│ /workspaces/calcifer/calcifer/abstract/logger_helper.py:72 in log_artifact                       │\r\n│                                                                                                  │\r\n│    71 │   │                                                                                      │\r\n│ ❱  72 │   │   saver(data, filepath)                                                              │\r\n│    73 │   │   logger.info(f\"saved {artifact_type} to {str(filepath)}\")                           │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/site-packages/cudf/core/dataframe.py:6303 in to_parquet    │\r\n│                                                                                                  │\r\n│   6302 │   │                                                                                     │\r\n│ ❱ 6303 │   │   return parquet.to_parquet(                                                        │\r\n│   6304 │   │   │   self,                                                                         │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/contextlib.py:79 in inner                                  │\r\n│                                                                                                  │\r\n│    78 │   │   │   with self._recreate_cm():                                                      │\r\n│ ❱  79 │   │   │   │   return func(*args, **kwds)                                                 │\r\n│    80 │   │   return inner                                                                       │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/site-packages/cudf/io/parquet.py:738 in to_parquet         │\r\n│                                                                                                  │\r\n│    737 │   │   )                                                                                 │\r\n│ ❱  738 │   │   return _write_parquet(                                                            │\r\n│    739 │   │   │   df,                                                                           │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/contextlib.py:79 in inner                                  │\r\n│                                                                                                  │\r\n│    78 │   │   │   with self._recreate_cm():                                                      │\r\n│ ❱  79 │   │   │   │   return func(*args, **kwds)                                                 │\r\n│    80 │   │   return inner                                                                       │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/site-packages/cudf/io/parquet.py:102 in _write_parquet     │\r\n│                                                                                                  │\r\n│    101 │   else:                                                                                 │\r\n│ ❱  102 │   │   write_parquet_res = libparquet.write_parquet(                                     │\r\n│    103 │   │   │   df, filepaths_or_buffers=paths_or_bufs, **common_args                         │\r\n│                                                                                                  │\r\n│ /opt/conda/envs/rapids/lib/python3.10/contextlib.py:79 in inner                                  │\r\n│                                                                                                  │\r\n│    78 │   │   │   with self._recreate_cm():                                                      │\r\n│ ❱  79 │   │   │   │   return func(*args, **kwds)                                                 │\r\n│    80 │   │   return inner                                                                       │\r\n│                                                                                                  │\r\n│ in cudf._lib.parquet.write_parquet:429                                                           │\r\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\r\nMemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/rapids/include/rmm/mr/device/cuda_memory_resource.hpp\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/13007/comments",
    "author": "Pedrexus",
    "comments": [
      {
        "user": "Pedrexus",
        "created_at": "2023-03-28T06:09:27Z",
        "body": "By the way, I kept trying, and it seems the limit is around 1.3Gb. \r\n\r\nI am using an RTX 4090 24Gb, what means I have to keep the dataframe at 5% the total available memory. Is this a bug? Is there any work around?\r\n\r\nAlso, for now, my solution was simply to convert it to_pandas and use the pandas parquet writer."
      },
      {
        "user": "GregoryKimball",
        "created_at": "2023-06-05T18:28:20Z",
        "body": "Thank you @Pedrexus for raising this issue. I agree with your suggestion that these OOM errors seem to be happening too early. Would you please share a bit more about your data - dtypes, cardinality, etc? I would love to have a data generator repro that shows a similar issue. \r\n\r\nI notice that your workflow appears to be using pytorch as well - are you using separate or shared memory pools between pytorch and cudf?  \r\n\r\nYou might also want to try using the `partition_cols` argument to limit memory usage."
      }
    ]
  },
  {
    "number": 12980,
    "title": "[QST] What's the cudf overhead for small dataset?",
    "created_at": "2023-03-20T23:29:11Z",
    "closed_at": "2023-03-23T17:45:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/12980",
    "body": "**What is your question?**\r\n\r\n```\r\nimport cudf\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf1 = pd.DataFrame()\r\ndim = 1000\r\ndf1[\"A\"] = np.random.randint(0, dim, dim)\r\n\r\ndf1_cu = cudf.from_pandas(df1)\r\n\r\n%%time\r\ndf1_cu[\"A\"].sum()\r\n600 µs\r\n\r\n%%time\r\ndf1[\"A\"].sum()\r\n200 µs\r\n```\r\n\r\ncudf seems to have some  overhead for small datasets. Where does it come from? It should not from data transfer as \r\ndf1_cu = cudf.from_pandas(df1) has transferred the data.",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/12980/comments",
    "author": "zachary62",
    "comments": [
      {
        "user": "bdice",
        "created_at": "2023-03-20T23:46:36Z",
        "body": "@zachary62 I can’t answer this definitively right now, but I would point out that there is still device-host data transfer: the result of the sum must be copied back to the host, incurring a device (or stream) synchronization."
      },
      {
        "user": "zachary62",
        "created_at": "2023-03-20T23:59:36Z",
        "body": "For a task of a large number (e.g., 100000) sum, group-by, max queries, but over small datasets (<10000 rows), is there any way to use cudf for speedup? These queries are independent, and can we exploit inter-query parallelism?"
      },
      {
        "user": "shwina",
        "created_at": "2023-03-23T14:16:57Z",
        "body": "> For a task of a large number (e.g., 100000) sum, group-by, max queries, but over small datasets (<10000 rows), is there any way to use cudf for speedup? These queries are independent, and can we exploit inter-query parallelism?\r\n\r\nOne way would be to leverage groupby.\r\n\r\nSay, for example you have 100 small datasets of 10_000 rows each:\r\n\r\n```python\r\ndfs = [cudf.datasets.randomdata(10_000) for i in range(100)]  # 100 dataframes of 10_000 rows each\r\n```\r\n\r\nYou could compute for example the `min` and `max` of each dataframe as follows:\r\n\r\n```python\r\ndf_stats = [df.agg(['max', 'min']) for df in dfs]\r\nprint(\"\\n\".join(map(str, df_stats[:5])))  # print the first 5 results\r\n         id         x         y\r\nmax  1141.0  0.999934  0.999911\r\nmin   867.0 -0.999895 -0.999854\r\n         id         x         y\r\nmax  1118.0  0.999983  0.999700\r\nmin   890.0 -0.999549 -0.999927\r\n         id         x         y\r\nmax  1104.0  0.999812  0.999611\r\nmin   887.0 -0.999343 -0.999895\r\n         id         x         y\r\nmax  1129.0  0.999822  0.999234\r\nmin   880.0 -0.999846 -0.999479\r\n         id         x         y\r\nmax  1120.0  0.998873  0.999985\r\nmin   884.0 -0.999894 -0.999906\r\n```\r\n\r\nThis is quite slow:\r\n\r\n```python\r\n%%timeit\r\ndf_stats = [df.agg(['max', 'min']) for df in dfs]\r\n\r\n316 ms ± 5.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nAnother option would be to use a groupby to compute the `max` and `min` in a single operation. Here's a trick for doing that:\r\n\r\n```python\r\nimport cupy as cp\r\n\r\ndfs_concatenated = cudf.concat(dfs)\r\ngroups = cp.repeat(cp.arange(100), 10_000)\r\ndf_stats = dfs_concatenated.groupby(groups, sort=True).agg(['max', 'min'])\r\nprint(df_stats.head(5))\r\n     id              x                   y          \r\n    max  min       max       min       max       min\r\n0  1141  867  0.999934 -0.999895  0.999911 -0.999854\r\n1  1118  890  0.999983 -0.999549  0.999700 -0.999927\r\n2  1104  887  0.999812 -0.999343  0.999611 -0.999895\r\n3  1129  880  0.999822 -0.999846  0.999234 -0.999479\r\n4  1120  884  0.998873 -0.999894  0.999985 -0.999906\r\n```\r\n\r\nThis is faster:\r\n\r\n```python\r\n%%timeit\r\ngroups = cp.repeat(cp.arange(100), 10_000)\r\ndf_stats = cudf.concat(dfs).groupby(groups, sort=True).agg(['max', 'min'])\r\n\r\n21.4 ms ± 1.54 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\r\n```\r\n"
      },
      {
        "user": "zachary62",
        "created_at": "2023-03-23T17:45:46Z",
        "body": "That's smart! Thank you!"
      }
    ]
  },
  {
    "number": 12640,
    "title": "Build source code from 22.12",
    "created_at": "2023-01-29T12:23:56Z",
    "closed_at": "2023-04-02T22:16:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/12640",
    "body": "log print:\r\n/home/cudf/cpp/src/io/utilities/parsing_utils.cuh(240): error #20014-D: calling a __host__ function from a __host__ __device__ function is not allowed\r\n          detected during:\r\n            instantiation of \"std::optional<T> cudf::io::parse_numeric<T,base>(const char *, const char *, const cudf::io::parse_options_view &) [with T=__nv_bool, base=10]\"\r\nHello,hope for your reply",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/12640/comments",
    "author": "newjavaer",
    "comments": [
      {
        "user": "newjavaer",
        "created_at": "2023-02-01T06:41:38Z",
        "body": "Hi,I have solved the above problem . But when  I build the python cudf .\r\nThere is log pring .hope for you reply\r\n/home/cudf/python/cudf/cudf/_lib/aggregation.pyx:8:0: 'libcpp/utility/move.pxd' not found\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n        return AggregationKind(self.c_obj.get()[0].kind).name\r\n\r\n    @classmethod\r\n    def sum(cls):\r\n        cdef RollingAggregation agg = cls()\r\n        agg.c_obj = move(\r\n                       ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/aggregation.pyx:105:24: 'move' is not a constant, variable or function identifier\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n\r\n    @classmethod\r\n    def sum(cls):\r\n        cdef RollingAggregation agg = cls()\r\n        agg.c_obj = move(\r\n            libcudf_aggregation.make_sum_aggregation[rolling_aggregation]())\r\n                                                                        ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/aggregation.pyx:106:73: Cannot convert 'unique_ptr[rolling_aggregation]' to Python object\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n        return AggregationKind(self.c_obj.get()[0].kind).name\r\n\r\n    @classmethod\r\n    def sum(cls):\r\n        cdef RollingAggregation agg = cls()\r\n        agg.c_obj = move(\r\n                       ^\r\n------------------------------------------------------------"
      },
      {
        "user": "vyasr",
        "created_at": "2023-02-08T17:48:41Z",
        "body": "Hi @newjavaer, what version of Cython do you have? It should be at least 0.29.17 to support `move` (but would recommend pulling the latest patch version of Cython 0.29.X just in case)."
      },
      {
        "user": "newjavaer",
        "created_at": "2023-02-09T09:39:59Z",
        "body": "Thank you for your reply. @vyasr  I update my Cython. But there still have other erro.\r\nFAILED: cudf/_lib/datetime.cxx /home/cudf/python/cudf/_skbuild/linux-x86_64-3.8/cmake-build/cudf/_lib/datetime.cxx\r\ncd /home/cudf/python/cudf/_skbuild/linux-x86_64-3.8/cmake-build/cudf/_lib && /root/anaconda3/envs/cudf_dev/bin/cython --cplus --include-dir /root/anaconda3/envs/cudf_dev/include/python3.8 -3 --directive binding=True,embedsignature=True,always_allow_keywords=True /home/cudf/python/cudf/cudf/_lib/datetime.pyx --output-file /home/cudf/python/cudf/_skbuild/linux-x86_64-3.8/cmake-build/cudf/_lib/datetime.cxx\r\nwarning: /home/cudf/python/cudf/cudf/_lib/cpp/types.pxd:51:8: 'UNEQUAL' redeclared\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n\r\nfrom libcpp.memory cimport unique_ptr\r\nfrom libcpp.utility cimport move\r\n\r\ncimport cudf._lib.cpp.datetime as libcudf_datetime\r\nfrom cudf._lib.column cimport Column\r\n^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:9:0: 'cudf/_lib/column/Column.pxd' not found\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\nfrom cudf._lib.cpp.types cimport size_type\r\nfrom cudf._lib.scalar cimport DeviceScalar\r\n\r\n\r\n@acquire_spill_lock()\r\ndef add_months(Column col, Column months):\r\n              ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:18:15: 'Column' is not a type identifier\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\nfrom cudf._lib.cpp.types cimport size_type\r\nfrom cudf._lib.scalar cimport DeviceScalar\r\n\r\n\r\n@acquire_spill_lock()\r\ndef add_months(Column col, Column months):\r\n                          ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:18:27: 'Column' is not a type identifier\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n\r\n    return Column.from_unique_ptr(move(c_result))\r\n\r\n\r\n@acquire_spill_lock()\r\ndef extract_datetime_component(Column col, object field):\r\n                              ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:36:31: 'Column' is not a type identifier\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n        raise ValueError(f\"Invalid resolution: '{freq}'\")\r\n    return freq_val\r\n\r\n\r\n@acquire_spill_lock()\r\ndef ceil_datetime(Column col, object freq):\r\n                 ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:107:18: 'Column' is not a type identifier\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n    result = Column.from_unique_ptr(move(c_result))\r\n    return result\r\n\r\n\r\n@acquire_spill_lock()\r\ndef floor_datetime(Column col, object freq):\r\n                  ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:121:19: 'Column' is not a type identifier\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n    result = Column.from_unique_ptr(move(c_result))\r\n    return result\r\n\r\n\r\n@acquire_spill_lock()\r\ndef round_datetime(Column col, object freq):\r\n                  ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:135:19: 'Column' is not a type identifier\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n    result = Column.from_unique_ptr(move(c_result))\r\n    return result\r\n\r\n\r\n@acquire_spill_lock()\r\ndef is_leap_year(Column col):\r\n                ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:149:17: 'Column' is not a type identifier\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n        ))\r\n    return Column.from_unique_ptr(move(c_result))\r\n\r\n\r\n@acquire_spill_lock()\r\ndef extract_quarter(Column col):\r\n                   ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:179:20: 'Column' is not a type identifier\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n\r\n    return Column.from_unique_ptr(move(c_result))\r\n\r\n\r\n@acquire_spill_lock()\r\ndef days_in_month(Column col):\r\n                 ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:194:18: 'Column' is not a type identifier\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n\r\n    return Column.from_unique_ptr(move(c_result))\r\n\r\n\r\n@acquire_spill_lock()\r\ndef last_day_of_month(Column col):\r\n                     ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:207:22: 'Column' is not a type identifier\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n\r\n@acquire_spill_lock()\r\ndef add_months(Column col, Column months):\r\n    # months must be int16 dtype\r\n    cdef unique_ptr[column] c_result\r\n    cdef column_view col_view = col.view()\r\n                                       ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:21:40: Cannot convert Python object to 'column_view'\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n@acquire_spill_lock()\r\ndef add_months(Column col, Column months):\r\n    # months must be int16 dtype\r\n    cdef unique_ptr[column] c_result\r\n    cdef column_view col_view = col.view()\r\n    cdef column_view months_view = months.view()\r\n                                             ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:22:46: Cannot convert Python object to 'column_view'\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n                col_view,\r\n                months_view\r\n            )\r\n        )\r\n\r\n    return Column.from_unique_ptr(move(c_result))\r\n                ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:32:17: cimported module has no attribute 'from_unique_ptr'\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n        elif field == \"day_of_year\":\r\n            c_result = move(libcudf_datetime.day_of_year(col_view))\r\n        else:\r\n            raise ValueError(f\"Invalid datetime field: '{field}'\")\r\n\r\n    result = Column.from_unique_ptr(move(c_result))\r\n                  ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:73:19: cimported module has no attribute 'from_unique_ptr'\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n        elif field == \"day_of_year\":\r\n            c_result = move(libcudf_datetime.day_of_year(col_view))\r\n        else:\r\n            raise ValueError(f\"Invalid datetime field: '{field}'\")\r\n\r\n    result = Column.from_unique_ptr(move(c_result))\r\n                  ^\r\n------------------------------------------------------------\r\n\r\n/home/cudf/python/cudf/cudf/_lib/datetime.pyx:73:19: Compiler crash in AnalyseExpressionsTransform\r\n\r\nModuleNode.body = StatListNode(datetime.pyx:3:0)\r\nStatListNode.stats[9] = StatListNode(datetime.pyx:36:0)\r\nStatListNode.stats[0] = DefNode(datetime.pyx:36:0,\r\n    is_cyfunction = True,\r\n    modifiers = [...]/0,\r\n    name = 'extract_datetime_component',\r\n    np_args_idx = [...]/0,\r\n    num_required_args = 2,\r\n    outer_attrs = [...]/2,\r\n    py_wrapper_required = True,\r\n    reqd_kw_flags_cname = '0',\r\n    used = True)\r\nFile 'ExprNodes.py', line 5369, in infer_type: SimpleCallNode(datetime.pyx:73:35,\r\n    result_is_used = True,\r\n    use_managed_ref = True)\r\nFile 'ExprNodes.py', line 6852, in infer_type: AttributeNode(datetime.pyx:73:19,\r\n    attribute = 'from_unique_ptr',\r\n    is_attribute = 1,\r\n    needs_none_check = True,\r\n    result_is_used = True,\r\n    use_managed_ref = True)\r\n\r\nCompiler crash traceback from this point on:\r\n  File \"/root/anaconda3/envs/cudf_dev/lib/python3.8/site-packages/Cython/Compiler/ExprNodes.py\", line 6852, in infer_type\r\n    if node.entry.type and node.entry.type.is_cfunction:\r\nAttributeError: 'NoneType' object has no attribute 'type'\r\n"
      },
      {
        "user": "GregoryKimball",
        "created_at": "2023-04-02T22:16:47Z",
        "body": "Thank you @newjavaer for reporting this. For now I'd like to close this issue. Please let us know if you have any issues building with the latest branch! Thank you for your help."
      }
    ]
  },
  {
    "number": 12450,
    "title": "Does cudf support compression?",
    "created_at": "2022-12-29T12:24:53Z",
    "closed_at": "2023-04-02T22:24:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/12450",
    "body": "The arrow table contains multiple string columns. When cudf is used, transmission is affected due to the PICE bandwidth. Does cudf support compression to reduce transmission overhead? Or is there any other way to solve?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/12450/comments",
    "author": "mjshare",
    "comments": [
      {
        "user": "beckernick",
        "created_at": "2023-01-03T16:04:37Z",
        "body": "cuDF readers support a variety of compression formats. Could you provide a more concrete example of what you're trying to do? Are you trying to take an Arrow table in memory and convert to a cuDF DataFrame?"
      },
      {
        "user": "mjshare",
        "created_at": "2023-01-04T09:31:17Z",
        "body": "> cuDF readers support a variety of compression formats. Could you provide a more concrete example of what you're trying to do? Are you trying to take an Arrow table in memory and convert to a cuDF DataFrame?\r\n\r\n\r\n\r\n//Yes. Our scenario is as follows: Use the Arrow format, convert the arrow to cudf, and use the GPU for calculation. Currently, the arrow contains multiple columns of character strings, which causes the GPU transmission bottleneck. One solution is to compress the arrow, transmit it to the GPU, and then convert it to cudf. Do you have any other suggestions on this issue?\r\n@beckernick \r\n"
      },
      {
        "user": "GregoryKimball",
        "created_at": "2023-01-06T18:50:24Z",
        "body": "Thank you @mjshare for your comments. You might try writing a compressed parquet file to a host buffer and then reading that with `cudf.read_parquet`. Even an uncompressed parquet file dramatically reduces data size for some inputs with dictionary encoding. \r\n> causes the GPU transmission bottleneck\r\nWhat is the throughput (GB/s) you are observing? "
      },
      {
        "user": "GregoryKimball",
        "created_at": "2023-04-02T22:24:35Z",
        "body": "Thank you @mjshare for raising this use case. Please let us know if you would like to continue the discussion."
      }
    ]
  },
  {
    "number": 12437,
    "title": "Aggregation Unique supports the list format.",
    "created_at": "2022-12-26T08:04:09Z",
    "closed_at": "2023-04-03T17:27:28Z",
    "labels": [
      "feature request",
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/12437",
    "body": "Aggregation Unique supports the list format. Does the community have a plan?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/12437/comments",
    "author": "mjshare",
    "comments": [
      {
        "user": "beckernick",
        "created_at": "2023-01-03T16:15:24Z",
        "body": "Could you provide a little bit more information?\r\n\r\nAre you working in C++ or Python? Are you looking for a way to get the unique elements per list, the unique lists across a column, or something else?"
      },
      {
        "user": "GregoryKimball",
        "created_at": "2023-04-03T17:27:28Z",
        "body": "Hello @mjshare thank you for your message. I believe the issue here is that we don't support `groupby` aggregations on List columns yet in cuDF-python.\r\n\r\nWe support `unique` on List columns:\r\n```\r\n>>> df = cudf.DataFrame({'a':[[1],[2],[1]], 'b':[1,2,3]})\r\n>>> df['a'].unique()\r\n0    [1]\r\n1    [2]\r\nName: a, dtype: list\r\n```\r\nhowever, `groupby` throws a `NotImplementedError`\r\n```\r\n>>> df.groupby('a').min()\r\nTraceback (most recent call last):\r\n...\r\nNotImplementedError: Unsupported column type passed to create an Index: <class 'cudf.core.column.lists.ListColumn'>\r\n```\r\n\r\nThis issue appears to already be covered by #8039. For now I'll close this issue in favor of #8039 and the related #12037."
      }
    ]
  },
  {
    "number": 12430,
    "title": "[QST] apply_rows on string-based columns?",
    "created_at": "2022-12-22T13:02:48Z",
    "closed_at": "2023-01-04T13:55:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/12430",
    "body": "**Does cudf support apply_rows on string-based columns ?**\r\n\r\nI am trying to use cudf for faster processing of chemical structures to calculate some properties based on a toolkit called RDKit. For each row, which is a molecule denoted as SMILES string, the code converts SMILES into a molecule object and calculates some basic properties. Here's how my code looks:\r\n\r\n```\r\nimport cudf\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom rdkit import Chem\r\nfrom rdkit.Chem import Descriptors\r\nimport time\r\n\r\ndef calc_desc(smiles, desc_dict):\r\n        m = Chem.MolFromSmiles(smiles)\r\n        if m is not None:\r\n                desc_dict['MOLWT'] = Descriptors.MolWt(m) # molecular weight\r\n                desc_dict['TPSA'] = Descriptors.TPSA(m) # topological polar surface area\r\n                desc_dict['LOGP'] = Descriptors.MolLogP(m) # octanol-water partition coefficient\r\n        return desc_dict\r\n\r\nstart = time.time()\r\n\r\ndf = pd.read_csv('virtual_lib_100k.csv', names=['SMILES'])\r\n# desc_list = ['MOLWT', 'TPSA', 'LOGP']\r\n# df[desc_list] = df.SMILES.apply(lambda x: pd.Series(calc_desc(x)))\r\n# df.to_csv('virtual_lib_100k_desc_pandas.csv', index=False)\r\ncu_df = cudf.from_pandas(df)\r\ncu_df = cu_df.apply_rows(calc_desc, incols=['SMILES'], outcols=dict(desc_dict=np.float64), kwargs=dict())\r\ncu_df.to_csv('virtual_lib_100k_desc_rapids.csv', index=False)\r\n\r\nend = time.time()\r\nprint(f\"Time taken: {(end-start)} seconds\")\r\n```\r\nI was hoping to process millions of rows to see if I can get this done faster than using Pandas, but I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"rdkit_rapids.py\", line 30, in <module>\r\n    cu_df = cu_df.apply_rows(calc_desc, incols=['SMILES'], outcols=dict(desc_dict=np.float64), kwargs=dict())\r\n  File \"/home/ubuntu/anaconda3/envs/rdkit_2020_06/lib/python3.6/site-packages/cudf/core/dataframe.py\", line 3578, in apply_rows\r\n    cache_key=cache_key,\r\n  File \"/home/ubuntu/anaconda3/envs/rdkit_2020_06/lib/python3.6/site-packages/cudf/utils/applyutils.py\", line 86, in apply_rows\r\n    return applyrows.run(df)\r\n  File \"/home/ubuntu/anaconda3/envs/rdkit_2020_06/lib/python3.6/site-packages/cudf/utils/applyutils.py\", line 158, in run\r\n    inputs = {k: df[k]._column.data_array_view for k in self.incols}\r\n  File \"/home/ubuntu/anaconda3/envs/rdkit_2020_06/lib/python3.6/site-packages/cudf/utils/applyutils.py\", line 158, in <dictcomp>\r\n    inputs = {k: df[k]._column.data_array_view for k in self.incols}\r\n  File \"/home/ubuntu/anaconda3/envs/rdkit_2020_06/lib/python3.6/site-packages/cudf/core/column/column.py\", line 84, in data_array_view\r\n    raise ValueError(\"Cannot get an array view of a StringColumn\")\r\nValueError: Cannot get an array view of a StringColumn\r\n```\r\n\r\nSo, I am wondering if cudf supports operations on string based columns. I see some discussion in #5646 but was not sure if it was answered completely. The commented lines in above code is the pandas equivalent code that achieves what I want.\r\n\r\nThank you,\r\nVishal",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/12430/comments",
    "author": "iwwwish",
    "comments": [
      {
        "user": "shwina",
        "created_at": "2023-01-03T16:35:03Z",
        "body": "Unfortunately, the `.apply()` function in cuDF is somewhat limited compared to Pandas. In particular, we don't support using \"external\" libraries (in this case, `rdkit`, inside UDFs). We simply have no way to generate CUDA kernels from totally arbitrary Python functions like these.\r\n\r\nThat being said, I'm curious if it isn't possible to leverage RAPIDS/cuDF to do this kind of operation on the GPU. Given a SMILES string, how trivial or non-trivial is it to write a function like `MolWt()` that will compute the molecular weight corresponding to that string? If that function is composed of relatively simple operations, it may be possible to parallelize on the GPU. "
      },
      {
        "user": "iwwwish",
        "created_at": "2023-01-03T18:55:43Z",
        "body": "Hi Ashwin! Thanks for explaining the limitation. To answer your question on how trivial it is to write a function like MolWt(): it should be pretty straightforward if it is a simple property like Molecular Weight but I will have to compute a few complex descriptors that are already well defined in libraries like RDKit. So I am not sure if it is worth re-writing them. I'll explore further to see if I can find ways to parallelize my task using the RDkit library."
      },
      {
        "user": "shwina",
        "created_at": "2023-01-04T13:55:39Z",
        "body": "Yes, unfortunately it will need rewriting of the descriptor functions. \r\n\r\nI'm going to go ahead and close this issue as \"answered\", but if you're interested in exploring this further down the road, please feel free to reopen this issue!"
      }
    ]
  },
  {
    "number": 12348,
    "title": "Is host memory used during cudf.serialize?",
    "created_at": "2022-12-09T02:44:20Z",
    "closed_at": "2023-01-05T02:02:08Z",
    "labels": [
      "question",
      "0 - Waiting on Author"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/12348",
    "body": "cudf description\r\n\r\nclass Serializable:\r\n    \"\"\"A serializable object composed of device memory buffers.\r\n\r\n    This base class defines a standard serialization protocol for objects\r\n    encapsulating device memory buffers. Serialization proceeds by copying\r\n    device data onto the host, then returning it along with suitable metadata\r\n    for reconstruction of the object. Deserialization performs the reverse\r\n    process, copying the serialized data from the host to new device buffers.\r\n    Subclasses must define the abstract methods :meth:`~.serialize` and\r\n    :meth:`~.deserialize`. The former defines the conversion of the object\r\n    into a representative collection of metadata and data buffers, while the\r\n    latter converts back from that representation into an equivalent object.\r\n    \"\"\"",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/12348/comments",
    "author": "mjshare",
    "comments": [
      {
        "user": "wence-",
        "created_at": "2022-12-16T14:58:39Z",
        "body": "tl;dr: no, unless you are using it through the pickle interface.\r\n\r\nIf you call `serialize()` on a cudf object, you won't get host memory being used (this is used in dask/distributed to ensure that in a multi-GPU setting we do direct GPU-to-GPU transfer of the data). The metadata \"header\" that describes the object that has been serialized does use host memory (it's just a normal python dictionary).\r\n\r\nThrough the serialization interface, cudf also supports the Python's pickle protocol (so you can do `pickle.dump` and `pickle.load` with cudf objects). In \"out of band\" protocol5 mode, this doesn't serialize the device buffers \"in-band\" but it does need to bring them to host, because we have to deliver `memoryview` objects which are wrappers around host pointers."
      },
      {
        "user": "vyasr",
        "created_at": "2023-01-05T02:02:08Z",
        "body": "I'm going to close this as answered, feel free to reopen if there are further questions though."
      }
    ]
  },
  {
    "number": 11722,
    "title": "Write csv without scientific notation[QST]",
    "created_at": "2022-09-20T15:01:51Z",
    "closed_at": "2022-09-26T17:06:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/11722",
    "body": "**What is your question?**\r\nHello, I met a problem to write data into csv file with a scientific notation. Do we have a workaround to remove the scientific notation?  Appreciate it if anyone has an idea about it.\r\n```python\r\n>>> import cudf\r\n>>> df = cudf.DataFrame({'a':[0.0000009]})\r\n>>> df.to_csv('test.csv', float_format='%.15f')\r\n\r\n$ cat test.csv\r\n\t\ta\r\n0\t9.0e-07 \r\n\r\n#expected behavior\r\n$ cat test.csv\r\n\t\ta\r\n0\t0.0000009\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/11722/comments",
    "author": "MikeChenfu",
    "comments": [
      {
        "user": "davidwendt",
        "created_at": "2022-09-20T20:35:28Z",
        "body": "Sorry, there is no workaround for this.\r\nScientific notation is used for numbers less then 0.0001 or greater than 1,000,000."
      },
      {
        "user": "MikeChenfu",
        "created_at": "2022-09-26T17:04:10Z",
        "body": "Thanks @davidwendt . It was working to convert it to decimal and write it as csv."
      },
      {
        "user": "davidwendt",
        "created_at": "2022-09-26T17:05:44Z",
        "body": "Ah. That is a good idea.\r\nCan we close this then?"
      },
      {
        "user": "MikeChenfu",
        "created_at": "2022-09-26T17:06:26Z",
        "body": "Yes, thanks again!"
      }
    ]
  },
  {
    "number": 11219,
    "title": "[QST] Disabling  decimal128 support",
    "created_at": "2022-07-07T21:28:34Z",
    "closed_at": "2022-08-29T17:43:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/11219",
    "body": "Hi cudf team,\r\n\r\nI am building from source cudf on pcc64le (Summit cluster)  using CUDA 11.4 and driver 450.80.02\r\n\r\n```\r\n...\r\n[ 11%] Building CXX object CMakeFiles/cudf.dir/src/aggregation/result_cache.cpp.o\r\n[ 11%] Building CXX object CMakeFiles/cudf.dir/src/ast/expression_parser.cpp.o\r\n/sw/summit/ums/gen119/nvrapids/src/nvrapids_v22.06.00_src/cudf_v22.06.00/cpp/include/cudf/utilities/type_dispatcher.hpp(522): error: \"numeric::decimal128\" contains a 128-bit integer, which is not supported in device code\r\n          detected during instantiation of \"decltype(auto) cudf::type_dispatcher(cudf::data_type, Functor, Ts &&...) [with IdTypeMap=cudf::id_to_type_impl, Functor=cudf::detail::unary_relationally_comparable_functor, Ts=<>]\" \r\n/sw/summit/ums/gen119/nvrapids/src/nvrapids_v22.06.00_src/cudf_v22.06.00/cpp/include/cudf/utilities/traits.hpp(149): here\r\n...\r\n```\r\n\r\nIs there way to disable decimal128 support ? \r\n\r\nFrom what I checked so far decimal128 support started in CUDA 11.5 so I would expect support should be disabled when finding older CUDA version, right ?\r\n\r\nThanks,\r\n\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/11219/comments",
    "author": "benjha",
    "comments": [
      {
        "user": "quasiben",
        "created_at": "2022-08-02T15:01:27Z",
        "body": "I dont' think we can disable decimal 128 support.  Instead of building on 11.4, you could build with 11.5 -> 11.7 then rely on CEC for CUDA 11.0->11.4 backwards compatibility.  This is how we are getting older CUDA Driver/Toolkit while we build conda packages in a 11.7 environment"
      },
      {
        "user": "benjha",
        "created_at": "2022-08-08T17:35:01Z",
        "body": "Thanks,\r\n\r\nI recompiled using the  CUDA 11.5 module available on Summit\r\n"
      }
    ]
  },
  {
    "number": 11117,
    "title": "[QST] building libcudf with Anaconda3-2022.05",
    "created_at": "2022-06-16T20:35:03Z",
    "closed_at": "2022-07-19T20:51:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/11117",
    "body": "Has anyone tried building libcudf with the lastest anaconda3?  When I try the tests explode on TYPED_TEST tests.  There will be much spew like \r\n```\r\nIn file included from /home/seidl2/src/cudf/cpp/tests/column/bit_cast_test.cpp:23:\r\n/home/seidl2/src/cudf/cpp/include/cudf_test/column_wrapper.hpp: In instantiation of 'rmm::device_buffer cudf::test::detail::make_elements(InputIterator, InputIterator) [with ElementTo = testing::Types<signed char, short int, int, long int, unsigned char, short unsigned int, unsigned int, long unsigned int, bool, float, double, cuda::std::__4::chrono::time_point<cuda::std::__4::chrono::system_clock, cuda::std::__4::chrono::duration<int, cuda::std::__4::ratio<86400, 1> > >, cuda::std::__4::chrono::time_point<cuda::std::__4::chrono::system_clock, cuda::std::__4::chrono::duration<long int, cuda::std::__4::ratio<1, 1> > >, cuda::std::__4::chrono::time_point<cuda::std::__4::chrono::system_clock, cuda::std::__4::chrono::duration<long int, cuda::std::__4::ratio<1, 1000> > >, cuda::std::__4::chrono::time_point<cuda::std::__4::chrono::system_clock, cuda::std::__4::chrono::duration<long int, cuda::std::__4::ratio<1, 1000000> > >, cuda::std::__4::chrono::time_point<cuda::std::__4::chrono::system_clock, cuda::std::__4::chrono::duration<long int, cuda::std::__4::ratio<1, 1000000000> > >, cuda::std::__4::chrono::duration<int, cuda::std::__4::ratio<86400, 1> >, cuda::std::__4::chrono::duration<long int, cuda::std::__4::ratio<1, 1> >, cuda::std::__4::chrono::duration<long int, cuda::std::__4::ratio<1, 1000> >, cuda::std::__4::chrono::duration<long int, cuda::std::__4::ratio<1, 1000000> >, cuda::std::__4::chrono::duration<long int, cuda::std::__4::ratio<1, 1000000000> >, numeric::fixed_point<int, numeric::Radix::BASE_10>, numeric::fixed_point<long int, numeric::Radix::BASE_10>, numeric::fixed_point<__int128, numeric::Radix::BASE_10> >; ElementFrom = int; InputIterator = thrust::counting_iterator<int>; std::enable_if_t<(! is_fixed_point<T>())>* <anonymous> = 0]':\r\n...\r\n/home/seidl2/src/cudf/cpp/include/cudf_test/column_wrapper.hpp:167:48: error: static assertion failed: Unexpected non-fixed width type.\r\n  167 |   static_assert(cudf::is_fixed_width<ElementTo>(), \"Unexpected non-fixed width type.\");\r\n      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~\r\n```\r\nIf I downgrade to 2021.11 the build works.  It seems the later anaconda includes some version of the gtest headers which don't work as expected.  \r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/11117/comments",
    "author": "etseidl",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-16T21:02:55Z",
        "body": "This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days."
      }
    ]
  },
  {
    "number": 11110,
    "title": "ppc64le - Could NOT find cuFile (missing: cuFile_LIBRARY cuFileRDMA_LIBRARY cuFile_INCLUDE_DIR)",
    "created_at": "2022-06-15T03:32:05Z",
    "closed_at": "2022-08-01T01:09:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/11110",
    "body": "I am building cudf 21.10 on ppc64le (POWER) architecture with cuda 11.2 enabled. While building libcudf tests using the command \"./build.sh libcudf tests\" I see the following errors. Can someone quickly help on this? \r\n\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: CMakeFiles/MULTIBYTE_SPLIT_TEST.dir/io/text/multibyte_split_test.cpp.o: in function `MultibyteSplitTest_LargeInput_Test::TestBody()':\r\nmultibyte_split_test.cpp:(.text+0x5138): undefined reference to `cudf::io::text::multibyte_split(cudf::io::text::data_chunk_source const&, std::string const&, rmm::mr::device_memory_resource*)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: multibyte_split_test.cpp:(.text+0x5174): undefined reference to `testing::ScopedTrace::PushTrace(char const*, int, std::string)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: CMakeFiles/MULTIBYTE_SPLIT_TEST.dir/io/text/multibyte_split_test.cpp.o: in function `MultibyteSplitTest_NondeterministicMatching_Test::TestBody()':\r\nmultibyte_split_test.cpp:(.text+0x56cc): undefined reference to `cudf::io::text::multibyte_split(cudf::io::text::data_chunk_source const&, std::string const&, rmm::mr::device_memory_resource*)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: multibyte_split_test.cpp:(.text+0x5704): undefined reference to `testing::ScopedTrace::PushTrace(char const*, int, std::string)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: CMakeFiles/MULTIBYTE_SPLIT_TEST.dir/io/text/multibyte_split_test.cpp.o: in function `MultibyteSplitTest_DelimiterAtEnd_Test::TestBody()':\r\nmultibyte_split_test.cpp:(.text+0x5b18): undefined reference to `cudf::io::text::multibyte_split(cudf::io::text::data_chunk_source const&, std::string const&, rmm::mr::device_memory_resource*)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: multibyte_split_test.cpp:(.text+0x5b54): undefined reference to `testing::ScopedTrace::PushTrace(char const*, int, std::string)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: CMakeFiles/MULTIBYTE_SPLIT_TEST.dir/io/text/multibyte_split_test.cpp.o: in function `MultibyteSplitTest_OverlappingMatchErasure_Test::TestBody()':\r\nmultibyte_split_test.cpp:(.text+0x5f70): undefined reference to `cudf::io::text::multibyte_split(cudf::io::text::data_chunk_source const&, std::string const&, rmm::mr::device_memory_resource*)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: CMakeFiles/MULTIBYTE_SPLIT_TEST.dir/io/text/multibyte_split_test.cpp.o: in function `MultibyteSplitTest_HandpickedInput_Test::TestBody()':\r\nmultibyte_split_test.cpp:(.text+0x6574): undefined reference to `cudf::io::text::multibyte_split(cudf::io::text::data_chunk_source const&, std::string const&, rmm::mr::device_memory_resource*)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: multibyte_split_test.cpp:(.text+0x65b8): undefined reference to `testing::ScopedTrace::PushTrace(char const*, int, std::string)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: CMakeFiles/MULTIBYTE_SPLIT_TEST.dir/io/text/multibyte_split_test.cpp.o:(.data.rel.ro._ZTIN4cudf4test11BaseFixtureE[_ZTIN4cudf4test11BaseFixtureE]+0x10): undefined reference to `typeinfo for testing::Test'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudftestutil.a(column_utilities.cu.o): in function `cudf::test::(anonymous namespace)::get_nested_type_str[abi:cxx11](cudf::column_view const&) [clone .localalias]':\r\ntmpxft_002a5d19_00000000-6_column_utilities.cudafe1.cpp:(.text+0x343c): undefined reference to `cudf::jit::get_type_name[abi:cxx11](cudf::data_type)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: tmpxft_002a5d19_00000000-6_column_utilities.cudafe1.cpp:(.text+0x35b4): undefined reference to `cudf::jit::get_type_name[abi:cxx11](cudf::data_type)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: tmpxft_002a5d19_00000000-6_column_utilities.cudafe1.cpp:(.text+0x3974): undefined reference to `cudf::jit::get_type_name[abi:cxx11](cudf::data_type)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `std::__cxx11::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream()@GLIBCXX_3.4.26'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `std::filesystem::create_directories(std::filesystem::path const&)@GLIBCXX_3.4.26'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `std::filesystem::path::_M_split_cmpts()@GLIBCXX_3.4.26'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `cudf::io::detail::cufile_input::read_async(unsigned long, unsigned long, unsigned char*, rmm::cuda_stream_view)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `cudf::io::detail::parquet::writer::close(std::string const&)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `cudf::string_scalar::string_scalar(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, rmm::cuda_stream_view, rmm::mr::device_memory_resource*)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `cudf::io::table_input_metadata::table_input_metadata(cudf::table_view const&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `cudf::jit::ptx_parser::ptx_parser(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::set<int, std::less<int>, std::allocator<int> > const&)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream()@GLIBCXX_3.4.26'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `std::filesystem::path::operator/=(std::filesystem::path const&)@GLIBCXX_3.4.26'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `std::filesystem::path::_M_append(std::basic_string_view<char, std::char_traits<char> >)@GLIBCXX_3.4.26'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `cudf::io::build_timezone_transition_table(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, rmm::cuda_stream_view)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `cudf::jit::parse_single_function_cuda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `cudf::jit::ptx_parser::parse[abi:cxx11]()'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `std::filesystem::path::_List::_List(std::filesystem::path::_List const&)@GLIBCXX_3.4.26'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `cudf::string_scalar::to_string[abi:cxx11](rmm::cuda_stream_view) const'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `std::filesystem::path::_List::_Impl_deleter::operator()(std::filesystem::path::_List::_Impl*) const@GLIBCXX_3.4.26'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `cudf::io::convert_string_to_dtype(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/opt/rh/gcc-toolset-10/root/usr/bin/ld: ../libcudf.so: undefined reference to `std::filesystem::path::_List::_List()@GLIBCXX_3.4.26'\r\ncollect2: error: ld returned 1 exit status\r\ngmake[2]: *** [tests/CMakeFiles/MULTIBYTE_SPLIT_TEST.dir/build.make:107: gtests/MULTIBYTE_SPLIT_TEST] Error 1\r\ngmake[1]: *** [CMakeFiles/Makefile2:2245: tests/CMakeFiles/MULTIBYTE_SPLIT_TEST.dir/all] Error 2\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/11110/comments",
    "author": "rnukala1",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2022-07-29T04:21:22Z",
        "body": "This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days."
      },
      {
        "user": "GregoryKimball",
        "created_at": "2022-08-01T01:09:16Z",
        "body": "After live debugging with @rnukala1 from Rocket Software, we resolved the build issue.\r\n\r\nWhen building libcudf, CMake searches for a version of cuFile installed locally on the machine, and if found enables\r\nthe logic. The associated symbol `cudf::io::detail::cufile_input::read_async(unsigned long, unsigned long, unsigned char*, rmm::cuda_stream_view)` maps to a virtual function with no implementation no matter the state of GDS. The solution to the problem is to add an implementation for this virtual function to that the symbol can be resolved.\r\n\r\nto the file `src/io/utilities/file_io_utilities.hpp`, around line 262. Add:\r\n```\r\nstd::future<size_t> read_async(size_t offset,\r\n                               size_t size,\r\n                               uint8_t* dst,\r\n                               rmm::cuda_stream_view stream) override\r\n{\r\n  CUDF_FAIL(\"Only used to compile without cufile library, should not be called\");\r\n}\r\n```"
      }
    ]
  },
  {
    "number": 10799,
    "title": "[ENH]: serialization schema cleanup",
    "created_at": "2022-05-05T16:45:28Z",
    "closed_at": "2024-09-30T11:39:51Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/10799",
    "body": "Followup from #10784. Hyphens and underscores are used inconsistently when separating names in metadata keys in `serialize`; go through and standardise on one choice (hyphens seem more popular).",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/10799/comments",
    "author": "wence-",
    "comments": [
      {
        "user": "wence-",
        "created_at": "2022-05-09T16:46:18Z",
        "body": "Going through and doing the minimum thing to add `frame_count` slots to all serializable objects, a further thought occurred which is that as well as lack of consistency in key names, there's also a lack of consistency in the metadata schema.\r\n\r\nSome of the metadata slots (in particular `type-serialized`) are picked to align with what dask names things; as far as I can tell this is not the case for others, and doesn't really need to be, but cc @jakirkham whom `git-annotate` suggests might know.\r\n\r\nSometimes, properties of nested objects are copied in to the parent header, and sometimes not, I think it makes sense to clean up and have a model of:\r\n```\r\n{\"type-serialized\": my-type,\r\n  \"properties\": {attribute: value, nested_object: nested_stuff},\r\n  \"frame-count\": number_of_frames_consumed_in_deserialization}\r\n```\r\n\r\nPerhaps something like this was considered and rejected?\r\n\r\nA much larger change would be to set up all of the serializable objects as `dataclass`-like things with constructors that just set attributes, then the schema for serialization is completely clear and lots of this code can be removed. A downside here is that serialization is not as extensible in an ad-hoc manner, and I am not sure that all cudf classes can get away with default simple constructors."
      },
      {
        "user": "bdice",
        "created_at": "2022-05-09T16:57:10Z",
        "body": "A small consideration that might tip the balance: using underscores makes it map more directly to valid Python identifiers, e.g.\r\n```python\r\n# Using underscores in the keys:\r\nframe_count = metadata[\"frame_count\"]\r\ntype_serialized = metadata[\"type_serialized\"]\r\n# vs. mapping dashes to underscores:\r\nframe_count = metadata[\"frame-count\"]\r\ntype_serialized = metadata[\"type-serialized\"]\r\n```"
      },
      {
        "user": "vyasr",
        "created_at": "2022-05-09T17:30:30Z",
        "body": "I would definitely welcome more insight from a Dask expert. Some thoughts and questions:\r\n- Are you proposing that nested headers are always copied into their parents? Does that mean that we always duplicate data?\r\n- How would nesting work when nested objects that aren't buffers actually return frames? In particular, our `CategoricalDtype` serializes into both header information and a frame consisting of the categories (just curious if you've considered how this impacts your proposal, because I haven't).\r\n- You're right that simple `dataclass` constructors won't work for most of our classes like `DataFrame` or `Series`. Those must be constructible from a wide range of objects to match what pandas supports.\r\n\r\nIt might help to consider whether we could change our classes so that only `Frame` and `BaseIndex` are `Serializable`. `ColumnBase` would still need its current implementation, but we could simplify dtype and `Buffer` logic. We shouldn't need non-API classes to conform to the `dask.distributed` API, and it leads to some incongruities:\r\n- dtypes include \"frames\" when serializing even though they don't actually have frames of data. The exception is categoricals, which are tricky because they are the only dtype that stores data on device (the categories). We would probably need a special case to support those.\r\n- Buffers ultimately just insert \"self\" into the frames, so every Column could really just insert its underlying buffer into the frames list and call that good. `Buffer` is an implementation detail of cudf and shouldn't need to conform to the dask API. We really just use the CAI protocol to save and load them anyway. That would remove one additional `serialize` implementation."
      },
      {
        "user": "jakirkham",
        "created_at": "2022-05-09T18:34:27Z",
        "body": "Dask uses `-`s\r\n\r\nNot sure I follow what else is being proposed here"
      },
      {
        "user": "vyasr",
        "created_at": "2022-05-09T21:04:08Z",
        "body": "@jakirkham I think the two main questions for you are:\r\n- Does it matters at all for cudf to use the same field names as dask (such as type-serialized)? We think not, but would like confirmation.\r\n- What objects actually need to support the serialization protocols? Is it just DataFrame,Series, and all the types of Indexes? i.e. Is there any need for `cudf.Buffer` or each dtype to support the protocols? Of course, we need a way to serialize that information, i.e. we need to know whether a Series contains strings or ints, but here we're just talking about necessary APIs for each type of object. "
      },
      {
        "user": "jakirkham",
        "created_at": "2022-05-09T23:48:58Z",
        "body": "Yes `\"type-serialized\"` matters. It is a special field in Dask\r\n\r\nWhen adding support for cuDF serialization, we found all sorts of objects went over the wire. Any we missed supporting surfaced as errors in benchmarks. So we added them all\r\n\r\nI think what I'm missing is what we are trying to fix here"
      },
      {
        "user": "wence-",
        "created_at": "2022-05-10T08:43:11Z",
        "body": "> I would definitely welcome more insight from a Dask expert. Some thoughts and questions:\r\n> \r\n> * Are you proposing that nested headers are always copied into their parents? Does that mean that we always duplicate data?\r\n\r\nOn the contrary. I'm proposing:\r\n```python\r\nheader = {\"properties\": {}}\r\nframes = []\r\nsub_obj = self.child # object we're serializing has a child to be serialized\r\nsub_header, sub_frames = sub_obj.serialize()\r\nheader[\"properties\"][\"child\"] = sub_header\r\nframes.extend(sub_frames)\r\n```\r\n\r\nAt the moment, depending on the particular object, in serialization, sometimes this is done, sometimes some information is carried redundantly in the `header` itself. Moreover, if adding a new slot to the \"top-level\" header key space, one has to read (or know) non-local code to know whether there are any reserved keys. For example `Serializable.device_serialize` (which calls the class-implemented `serialize`) overwrites `\"type-serialized\"`, `\"is-cuda\"`, and `\"lengths\"`, `Serializable.host_serialize` (called via pickle) additionally overwrites `\"writeable\"`.\r\n\r\n> * How would nesting work when nested objects that aren't buffers actually return frames? In particular, our `CategoricalDtype` serializes into both header information and a frame consisting of the categories (just curious if you've considered how this impacts your proposal, because I haven't).\r\n\r\nThis is not problematic. Deserialization takes a (nested) metadata descriptor and a list of frames and returns a deserialized object and a (partially) consumed list of frames. So a helper function:\r\n\r\n```python\r\ndef unpack(header, frames):\r\n    typ = pickle.loads(header[\"type-serialized\"])\r\n    count = header[\"frame_count\"]\r\n    obj = typ.deserialize(header, frames[:count])\r\n    return obj, frames[count:]\r\n```\r\n\r\nworks to unfold the part of a nested definition. So suppose we were deserializing a column with a categorical dtype:\r\n```python\r\ndtype_header = header[\"properties\"][\"dtype\"]\r\ndtype, frames = unpack(dtype_header, frames)\r\n# continue with deserialization of other properties\r\n```\r\n\r\n> * You're right that simple `dataclass` constructors won't work for most of our classes like `DataFrame` or `Series`. Those must be constructible from a wide range of objects to match what pandas supports.\r\n\r\nOne way to square that circle (though it is a big API-breaking change) is to split the munging of data for `__init__` into a free function. That is the API offers:\r\n```\r\ndef DataFrame(args):\r\n    # munge args\r\n    processed_args = ...(args)\r\n    return impl.DataFrame(processed_args)\r\n```\r\n\r\n~It may well not be worth it, however.~\r\n\r\nEDIT: that's not possible due to API constraints (as pointed out below by @shwina).\r\n\r\n> It might help to consider whether we could change our classes so that only `Frame` and `BaseIndex` are `Serializable`. `ColumnBase` would still need its current implementation, but we could simplify dtype and `Buffer` logic. We shouldn't need non-API classes to conform to the `dask.distributed` API, and it leads to some incongruities:\r\n> \r\n> * dtypes include \"frames\" when serializing even though they don't actually have frames of data. The exception is categoricals, which are tricky because they are the only dtype that stores data on device (the categories). We would probably need a special case to support those.\r\n\r\nThe advantage of everything supporting the same interface is you don't need to do any special-casing. You just recurse calling serialize until the base case is hit. If you don't have this then any dtype-carrying object that needs to be serialized has to `if isinstance(dtype, CategoricalDtype)` I think.\r\n\r\n> * Buffers ultimately just insert \"self\" into the frames, so every Column could really just insert its underlying buffer into the frames list and call that good. `Buffer` is an implementation detail of cudf and shouldn't need to conform to the dask API. We really just use the CAI protocol to save and load them anyway. That would remove one additional `serialize` implementation.\r\n\r\nI think this would work, since the wire format is to effectively send all the frames out of band and the reconstruct on the other end. The column metadata can include enough information to rebuild/validate the buffer.\r\n\r\n\r\n\r\n"
      },
      {
        "user": "wence-",
        "created_at": "2022-05-10T09:30:12Z",
        "body": "> I think what I'm missing is what we are trying to fix here\r\n\r\nInitially, I was adding support for serialization that was missing on struct columns (that was #10784). As part of that, the schema for the metadata headers seemed a bit inconsistent. So I am looking at if it is worth investing effort in cleaning that up a bit."
      },
      {
        "user": "shwina",
        "created_at": "2022-05-11T10:34:50Z",
        "body": "Just a drive-by comment here: `DataFrame` needs to be a type (as does `Index`). Unfortunately, we cannot make those into factory free-functions.\r\n"
      },
      {
        "user": "wence-",
        "created_at": "2022-05-11T12:28:44Z",
        "body": "> Just a drive-by comment here: `DataFrame` needs to be a type (as does `Index`). Unfortunately, we cannot make those into factory free-functions.\r\n\r\nI'm guessing because code basically relies on `isinstance(foo, cudf.DataFrame)` and the like?"
      },
      {
        "user": "shwina",
        "created_at": "2022-05-11T14:30:30Z",
        "body": "Yes -- and also there are classmethods defined on `cudf.DataFrame` that are in the public API; e.g., `cudf.DataFrame.from_pandas()`.\r\n\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-06-10T15:03:39Z",
        "body": "This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-08T16:03:12Z",
        "body": "This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed."
      },
      {
        "user": "wence-",
        "created_at": "2022-11-28T19:10:06Z",
        "body": "I think anything we do here will need to be in tandem with proposed serialisation changes in dask/distributed that are being contemplated. So I'll revisit this then."
      },
      {
        "user": "vyasr",
        "created_at": "2024-05-17T14:21:33Z",
        "body": "@wence- has anything changed in dask since the last comment to move the needle here?"
      },
      {
        "user": "wence-",
        "created_at": "2024-07-31T17:34:53Z",
        "body": "I'm a bit out of the loop. I think that they moved to allowing pickle/unpickle. But that doesn't fundamentally change things (since that works with gpu-backed data but necessitates a device-host transfer)."
      },
      {
        "user": "wence-",
        "created_at": "2024-09-30T11:39:51Z",
        "body": "I _think_ this is probably not worth it for the code churn, FWIW."
      }
    ]
  },
  {
    "number": 10517,
    "title": "cudaErrorMemoryAllocation out of memory",
    "created_at": "2022-03-26T16:45:43Z",
    "closed_at": "2022-07-15T16:38:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/10517",
    "body": "**Describe the bug**\r\nMemoryError: std::bad_alloc: out_of_memory: CUDA error at: /usr/local/miniconda3/envs/rapids-22.02/include/rmm/mr/device/cuda_memory_resource.hpp\r\n\r\nEven if it reads an empty file, it  will also report the error!\r\n\r\n\r\n\r\n\r\n**Expected behavior**\r\n```python \r\ndf = cudf.Series([1,2,3,None,4])\r\n```\r\n\r\n**Environment overview **\r\n - Environment location: Cloud\r\n - Method of cuDF install: conda\r\n   \r\n\r\n**Environment details:**\r\n- i use the flow command to create a conda virtual env:\r\n```python \r\nconda create -n rapids-22.02 -c rapidsai -c nvidia -c conda-forge rapids=22.02 python=3.8 cudatoolkit=11.4 dask-sql\r\n```\r\n\r\nthen i use the virtual env to make jupyter kernel\r\n\r\ni hope someone can solve it.\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/10517/comments",
    "author": "faker2081",
    "comments": [
      {
        "user": "beckernick",
        "created_at": "2022-04-05T20:23:49Z",
        "body": "Hi @faker2081 , could you please provide more information about the system on which you are running? One option would be to clone this repository and run the `print_env.sh` script."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-05-05T21:03:05Z",
        "body": "This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days."
      }
    ]
  },
  {
    "number": 10036,
    "title": "ModuleNotFoundError: No module named 'dask.dataframe.dispatch'",
    "created_at": "2022-01-13T17:51:35Z",
    "closed_at": "2022-03-30T19:05:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/10036",
    "body": "Hello,\r\n\r\nI installed Rapids via:\r\n\r\nconda clean --all\r\n\r\nmodule load Python/3.7.4-GCCcore-8.3.0\r\nmodule load CUDA/11.0.2-GCC-9.3.0 \r\n\r\nconda create -n rapids-21.12 -c rapidsai -c nvidia -c conda-forge rapids=21.12 python=3.7 cudatoolkit=11.0 dask-sql\r\n\r\nI use Rapids via:\r\n\r\nconda activate rapids-21.12\r\n\r\n(rapids-21.12) anamaria@gpu-2-0.saber:~ $ python\r\nPython 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import dask_cudf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/anamaria/.conda/envs/rapids-21.12/lib/python3.7/site-packages/dask_cudf/__init__.py\", line 6, in <module>\r\n    from . import backends\r\n  File \"/home/anamaria/.conda/envs/rapids-21.12/lib/python3.7/site-packages/dask_cudf/backends.py\", line 11, in <module>\r\n    from dask.dataframe.dispatch import (\r\nModuleNotFoundError: No module named 'dask.dataframe.dispatch'\r\n>>> import dask; print(dask.__file__)\r\n/home/anamaria/.local/lib/python3.7/site-packages/dask/__init__.py\r\n>>> quit()\r\n(rapids-21.12) anamaria@gpu-2-0.saber:~ $ conda install dask distributed \r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: done\r\n\r\n# All requested packages already installed.\r\n\r\n(rapids-21.12) anamaria@gpu-2-0.saber:~ $ python\r\nPython 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import dask_cudf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/anamaria/.conda/envs/rapids-21.12/lib/python3.7/site-packages/dask_cudf/__init__.py\", line 6, in <module>\r\n    from . import backends\r\n  File \"/home/anamaria/.conda/envs/rapids-21.12/lib/python3.7/site-packages/dask_cudf/backends.py\", line 11, in <module>\r\n    from dask.dataframe.dispatch import (\r\nModuleNotFoundError: No module named 'dask.dataframe.dispatch'\r\n\r\n\r\nPlease advise how to resolve this issue?\r\n\r\nThanks\r\nAna",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/10036/comments",
    "author": "anamariaUIC",
    "comments": [
      {
        "user": "galipremsagar",
        "created_at": "2022-01-14T15:25:34Z",
        "body": "@anamariaUIC Can you share the output of `conda list`?\r\n\r\nI'm not able to get the errors you have mentioned:\r\n```python\r\npgali@dt07:/nvme/0/pgali/cudf$ python\r\nPython 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import cudf\r\ncu>>> cudf.__version__\r\n'21.12.02'\r\n>>> import dask_cudf\r\n>>> dask_cudf.__version__\r\n'21.12.02'\r\n```"
      },
      {
        "user": "anamariaUIC",
        "created_at": "2022-01-14T15:58:41Z",
        "body": "@galipremsagar dask-cuda  is there, please see bellow:\r\n\r\n(base) anamaria@login-2[SABER]: ~ $ ssh gpu-2-0.saber\r\nLast login: Thu Jan 13 16:26:22 2022 from login-2.saber.ib\r\n\r\nconda activate rapids-21.12\r\nexport PATH=/home/anamaria/.conda/envs/rapids-21.12/bin:$PATH(base) anamaria@gpu-2-0.saber:~ $ \r\n(base) anamaria@gpu-2-0.saber:~ $ conda activate rapids-21.12\r\n(rapids-21.12) anamaria@gpu-2-0.saber:~ $ export PATH=/home/anamaria/.conda/envs/rapids-21.12/bin:$PATH\r\n(rapids-21.12) anamaria@gpu-2-0.saber:~ $ conda list\r\n# packages in environment at /home/anamaria/.conda/envs/rapids-21.12:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                 conda_forge    conda-forge\r\n_openmp_mutex             4.5                       1_gnu    conda-forge\r\nabseil-cpp                20210324.2           h9c3ff4c_0    conda-forge\r\naiohttp                   3.8.1            py37h5e8e339_0    conda-forge\r\naiosignal                 1.2.0              pyhd8ed1ab_0    conda-forge\r\nalsa-lib                  1.2.3                h516909a_0    conda-forge\r\nanyio                     3.4.0            py37h89c1867_0    conda-forge\r\nappdirs                   1.4.4              pyh9f0ad1d_0    conda-forge\r\nargcomplete               2.0.0              pyhd8ed1ab_0    conda-forge\r\nargon2-cffi               21.3.0             pyhd8ed1ab_0    conda-forge\r\nargon2-cffi-bindings      21.2.0           py37h5e8e339_1    conda-forge\r\narrow-cpp                 5.0.0           py37h6ab10f6_20_cuda    conda-forge\r\narrow-cpp-proc            3.0.0                      cuda    conda-forge\r\nasgiref                   3.4.1              pyhd8ed1ab_0    conda-forge\r\nasync-timeout             4.0.2              pyhd8ed1ab_0    conda-forge\r\nasync_generator           1.10                       py_0    conda-forge\r\nasynctest                 0.13.0                     py_0    conda-forge\r\nattrs                     21.4.0             pyhd8ed1ab_0    conda-forge\r\naws-c-auth                0.6.8                hadad3cd_1    conda-forge\r\naws-c-cal                 0.5.12               h70efedd_7    conda-forge\r\naws-c-common              0.6.17               h7f98852_0    conda-forge\r\naws-c-compression         0.2.14               h7c7754b_7    conda-forge\r\naws-c-event-stream        0.2.7               hd2be095_32    conda-forge\r\naws-c-http                0.6.10               h416565a_3    conda-forge\r\naws-c-io                  0.10.14              he836878_0    conda-forge\r\naws-c-mqtt                0.7.10               h885097b_0    conda-forge\r\naws-c-s3                  0.1.29               h8d70ed6_0    conda-forge\r\naws-c-sdkutils            0.1.1                h7c7754b_4    conda-forge\r\naws-checksums             0.1.12               h7c7754b_6    conda-forge\r\naws-crt-cpp               0.17.10              h6ab17b9_5    conda-forge\r\naws-sdk-cpp               1.9.160              h36ff4c5_0    conda-forge\r\nbackcall                  0.2.0              pyh9f0ad1d_0    conda-forge\r\nbackports                 1.0                        py_2    conda-forge\r\nbackports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    conda-forge\r\nbackports.zoneinfo        0.2.1            py37h5e8e339_4    conda-forge\r\nbleach                    4.1.0              pyhd8ed1ab_0    conda-forge\r\nblosc                     1.21.0               h9c3ff4c_0    conda-forge\r\nbokeh                     2.4.0            py37h89c1867_0    conda-forge\r\nboost                     1.74.0           py37h796e4cb_4    conda-forge\r\nboost-cpp                 1.74.0               h312852a_4    conda-forge\r\nbrotli                    1.0.9                h7f98852_6    conda-forge\r\nbrotli-bin                1.0.9                h7f98852_6    conda-forge\r\nbrotlipy                  0.7.0           py37h5e8e339_1003    conda-forge\r\nbrunsli                   0.1                  h9c3ff4c_0    conda-forge\r\nbzip2                     1.0.8                h7f98852_4    conda-forge\r\nc-ares                    1.18.1               h7f98852_0    conda-forge\r\nc-blosc2                  2.0.4                h5f21a17_1    conda-forge\r\nca-certificates           2021.10.8            ha878542_0    conda-forge\r\ncachetools                5.0.0              pyhd8ed1ab_0    conda-forge\r\ncairo                     1.16.0            h6cf1ce9_1008    conda-forge\r\ncertifi                   2021.10.8        py37h89c1867_1    conda-forge\r\ncffi                      1.15.0           py37h036bc23_0    conda-forge\r\ncfitsio                   3.470                hb418390_7    conda-forge\r\ncharls                    2.2.0                h9c3ff4c_0    conda-forge\r\ncharset-normalizer        2.0.9              pyhd8ed1ab_0    conda-forge\r\nclick                     8.0.3            py37h89c1867_1    conda-forge\r\nclick-plugins             1.1.1                      py_0    conda-forge\r\ncligj                     0.7.2              pyhd8ed1ab_1    conda-forge\r\ncloudpickle               2.0.0              pyhd8ed1ab_0    conda-forge\r\ncolorama                  0.4.4              pyh9f0ad1d_0    conda-forge\r\ncolorcet                  3.0.0              pyhd8ed1ab_0    conda-forge\r\ncryptography              36.0.1           py37hf1a17b8_0    conda-forge\r\ncucim                     21.12.00        cuda_11_py37_g6d1f082_0    rapidsai\r\ncudatoolkit               11.0.221             h6bb024c_0    nvidia\r\ncudf                      21.12.02        cuda_11_py37_g06540b9b37_0    rapidsai\r\ncudf_kafka                21.12.02        py37_g06540b9b37_0    rapidsai\r\ncugraph                   21.12.00        cuda11_py37_g3a43e9d0_0    rapidsai\r\ncuml                      21.12.00        cuda11_py37_g04c4927f3_0    rapidsai\r\ncupy                      9.6.0            py37h580ddce_0    conda-forge\r\ncurl                      7.81.0               h2574ce0_0    conda-forge\r\ncusignal                  21.12.00        py37_g2bf865c_0    rapidsai\r\ncuspatial                 21.12.00        py37_gab6748f_0    rapidsai\r\ncustreamz                 21.12.02        py37_g06540b9b37_0    rapidsai\r\ncuxfilter                 21.12.00        py37_g2e0fb5a_0    rapidsai\r\ncycler                    0.11.0             pyhd8ed1ab_0    conda-forge\r\ncyrus-sasl                2.1.27               h230043b_5    conda-forge\r\ncytoolz                   0.11.2           py37h5e8e339_1    conda-forge\r\ndask                      2021.11.2          pyhd8ed1ab_0    conda-forge\r\ndask-core                 2021.11.2          pyhd8ed1ab_0    conda-forge\r\ndask-cuda                 21.12.00                 py37_0    rapidsai\r\ndask-cudf                 21.12.02        cuda_11_py37_g06540b9b37_0    rapidsai\r\ndask-sql                  2021.12.0        py37h89c1867_0    conda-forge\r\ndatashader                0.11.1             pyh9f0ad1d_0    conda-forge\r\ndatashape                 0.5.4                      py_1    conda-forge\r\ndebugpy                   1.5.1            py37hcd2ae1e_0    conda-forge\r\ndecorator                 5.1.1              pyhd8ed1ab_0    conda-forge\r\ndefusedxml                0.7.1              pyhd8ed1ab_0    conda-forge\r\ndistributed               2021.11.2        py37h89c1867_0    conda-forge\r\ndlpack                    0.5                  h9c3ff4c_0    conda-forge\r\nentrypoints               0.3             pyhd8ed1ab_1003    conda-forge\r\nexpat                     2.4.2                h9c3ff4c_0    conda-forge\r\nfaiss-proc                1.0.0                      cuda    rapidsai\r\nfastapi                   0.70.1             pyhd8ed1ab_0    conda-forge\r\nfastavro                  1.4.8            py37h5e8e339_0    conda-forge\r\nfastrlock                 0.8              py37hcd2ae1e_1    conda-forge\r\nfiona                     1.8.20           py37hb7e2723_2    conda-forge\r\nflit-core                 3.6.0              pyhd8ed1ab_0    conda-forge\r\nfont-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge\r\nfont-ttf-inconsolata      3.000                h77eed37_0    conda-forge\r\nfont-ttf-source-code-pro  2.038                h77eed37_0    conda-forge\r\nfont-ttf-ubuntu           0.83                 hab24e00_0    conda-forge\r\nfontconfig                2.13.1            hba837de_1005    conda-forge\r\nfonts-conda-ecosystem     1                             0    conda-forge\r\nfonts-conda-forge         1                             0    conda-forge\r\nfonttools                 4.28.5           py37h5e8e339_0    conda-forge\r\nfreetype                  2.10.4               h0708190_1    conda-forge\r\nfreexl                    1.0.6                h7f98852_0    conda-forge\r\nfrozenlist                1.2.0            py37h5e8e339_1    conda-forge\r\nfsspec                    2021.11.1          pyhd8ed1ab_0    conda-forge\r\ngdal                      3.3.2            py37hd5a0ba4_3    conda-forge\r\ngeopandas                 0.9.0              pyhd8ed1ab_1    conda-forge\r\ngeopandas-base            0.9.0              pyhd8ed1ab_1    conda-forge\r\ngeos                      3.9.1                h9c3ff4c_2    conda-forge\r\ngeotiff                   1.7.0                h08e826d_2    conda-forge\r\ngettext                   0.19.8.1          h73d1719_1008    conda-forge\r\ngflags                    2.2.2             he1b5a44_1004    conda-forge\r\ngiflib                    5.2.1                h36c2ea0_2    conda-forge\r\nglog                      0.5.0                h48cff8f_0    conda-forge\r\ngraphite2                 1.3.13            h58526e2_1001    conda-forge\r\ngrpc-cpp                  1.42.0               ha1441d3_1    conda-forge\r\nh11                       0.12.0             pyhd8ed1ab_0    conda-forge\r\nharfbuzz                  2.9.1                h83ec7ef_1    conda-forge\r\nhdf4                      4.2.15               h10796ff_3    conda-forge\r\nhdf5                      1.12.1          nompi_h2750804_103    conda-forge\r\nheapdict                  1.0.1                      py_0    conda-forge\r\nicu                       68.2                 h9c3ff4c_0    conda-forge\r\nidna                      3.1                pyhd3deb0d_0    conda-forge\r\nimagecodecs               2021.8.26        py37hfe5a812_1    conda-forge\r\nimageio                   2.13.5             pyh239f2a4_0    conda-forge\r\nimportlib-metadata        4.10.0           py37h89c1867_0    conda-forge\r\nimportlib_metadata        4.10.0               hd8ed1ab_0    conda-forge\r\nimportlib_resources       5.4.0              pyhd8ed1ab_0    conda-forge\r\nipykernel                 6.6.1            py37h6531663_0    conda-forge\r\nipython                   7.31.0           py37h89c1867_0    conda-forge\r\nipython_genutils          0.2.0                      py_1    conda-forge\r\nipywidgets                7.6.5              pyhd8ed1ab_0    conda-forge\r\njbig                      2.1               h7f98852_2003    conda-forge\r\njedi                      0.18.1           py37h89c1867_0    conda-forge\r\njinja2                    3.0.3              pyhd8ed1ab_0    conda-forge\r\njoblib                    1.1.0              pyhd8ed1ab_0    conda-forge\r\njpeg                      9d                   h36c2ea0_0    conda-forge\r\njpype1                    1.3.0            py37h2527ec5_2    conda-forge\r\njson-c                    0.15                 h98cffda_0    conda-forge\r\njsonschema                4.3.3              pyhd8ed1ab_0    conda-forge\r\njupyter-server-proxy      3.2.0              pyhd8ed1ab_0    conda-forge\r\njupyter_client            7.1.0              pyhd8ed1ab_0    conda-forge\r\njupyter_core              4.9.1            py37h89c1867_1    conda-forge\r\njupyter_server            1.13.1             pyhd8ed1ab_0    conda-forge\r\njupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge\r\njupyterlab_widgets        1.0.2              pyhd8ed1ab_0    conda-forge\r\njxrlib                    1.1                  h7f98852_2    conda-forge\r\nkealib                    1.4.14               h87e4c3c_3    conda-forge\r\nkiwisolver                1.3.2            py37h2527ec5_1    conda-forge\r\nkrb5                      1.19.2               hcc1bbae_3    conda-forge\r\nlcms2                     2.12                 hddcbb42_0    conda-forge\r\nld_impl_linux-64          2.36.1               hea4e1c9_2    conda-forge\r\nlerc                      3.0                  h9c3ff4c_0    conda-forge\r\nlibaec                    1.0.6                h9c3ff4c_0    conda-forge\r\nlibblas                   3.9.0           12_linux64_openblas    conda-forge\r\nlibbrotlicommon           1.0.9                h7f98852_6    conda-forge\r\nlibbrotlidec              1.0.9                h7f98852_6    conda-forge\r\nlibbrotlienc              1.0.9                h7f98852_6    conda-forge\r\nlibcblas                  3.9.0           12_linux64_openblas    conda-forge\r\nlibcucim                  21.12.00        cuda11_g6d1f082_0    rapidsai\r\nlibcudf                   21.12.02        cuda11_g06540b9b37_0    rapidsai\r\nlibcudf_kafka             21.12.02          g06540b9b37_0    rapidsai\r\nlibcugraph                21.12.00        cuda11_g3a43e9d0_0    rapidsai\r\nlibcuml                   21.12.00        cuda11_g04c4927f3_0    rapidsai\r\nlibcumlprims              21.12.00        cuda11_g0a7f19f_0    nvidia\r\nlibcurl                   7.81.0               h2574ce0_0    conda-forge\r\nlibcusolver               11.3.2.107           hc875929_0    nvidia\r\nlibcuspatial              21.12.00        cuda11_gab6748f_0    rapidsai\r\nlibdap4                   3.20.6               hd7c4107_2    conda-forge\r\nlibdeflate                1.8                  h7f98852_0    conda-forge\r\nlibedit                   3.1.20191231         he28a2e2_2    conda-forge\r\nlibev                     4.33                 h516909a_1    conda-forge\r\nlibevent                  2.1.10               h9b69904_4    conda-forge\r\nlibfaiss                  1.7.0           cuda110h8045045_8_cuda    conda-forge\r\nlibffi                    3.4.2                h7f98852_5    conda-forge\r\nlibgcc-ng                 11.2.0              h1d223b6_11    conda-forge\r\nlibgcrypt                 1.9.4                h7f98852_0    conda-forge\r\nlibgdal                   3.3.2                h6acdded_3    conda-forge\r\nlibgfortran-ng            11.2.0              h69a702a_11    conda-forge\r\nlibgfortran5              11.2.0              h5c6108e_11    conda-forge\r\nlibglib                   2.70.2               h174f98d_1    conda-forge\r\nlibgomp                   11.2.0              h1d223b6_11    conda-forge\r\nlibgpg-error              1.42                 h9c3ff4c_0    conda-forge\r\nlibgsasl                  1.10.0               h5b4c23d_0    conda-forge\r\nlibhwloc                  2.3.0                h5e5b7d1_1    conda-forge\r\nlibiconv                  1.16                 h516909a_0    conda-forge\r\nlibkml                    1.3.0             h238a007_1014    conda-forge\r\nliblapack                 3.9.0           12_linux64_openblas    conda-forge\r\nlibllvm11                 11.1.0               hf817b99_2    conda-forge\r\nlibnetcdf                 4.8.1           nompi_hb3fd0d9_101    conda-forge\r\nlibnghttp2                1.43.0               h812cca2_1    conda-forge\r\nlibnsl                    2.0.0                h7f98852_0    conda-forge\r\nlibntlm                   1.4               h7f98852_1002    conda-forge\r\nlibopenblas               0.3.18          pthreads_h8fe5266_0    conda-forge\r\nlibpng                    1.6.37               h21135ba_2    conda-forge\r\nlibpq                     13.5                 hd57d9b9_1    conda-forge\r\nlibprotobuf               3.19.2               h780b84a_0    conda-forge\r\nlibrdkafka                1.6.1                hc49e61c_1    conda-forge\r\nlibrmm                    21.12.00        cuda11_g957ad04_0    rapidsai\r\nlibrttopo                 1.1.0                h1185371_6    conda-forge\r\nlibsodium                 1.0.18               h36c2ea0_1    conda-forge\r\nlibspatialindex           1.9.3                h9c3ff4c_4    conda-forge\r\nlibspatialite             5.0.1                h5cf074c_8    conda-forge\r\nlibssh2                   1.10.0               ha56f1ee_2    conda-forge\r\nlibstdcxx-ng              11.2.0              he4da1e4_11    conda-forge\r\nlibthrift                 0.15.0               he6d91bd_1    conda-forge\r\nlibtiff                   4.3.0                h6f004c6_2    conda-forge\r\nlibutf8proc               2.7.0                h7f98852_0    conda-forge\r\nlibuuid                   2.32.1            h7f98852_1000    conda-forge\r\nlibuv                     1.42.0               h7f98852_0    conda-forge\r\nlibwebp                   1.2.1                h3452ae3_0    conda-forge\r\nlibwebp-base              1.2.1                h7f98852_0    conda-forge\r\nlibxcb                    1.13              h7f98852_1004    conda-forge\r\nlibxgboost                1.5.0dev.rapidsai21.12      cuda11.0_0    rapidsai\r\nlibxml2                   2.9.12               h72842e0_0    conda-forge\r\nlibzip                    1.8.0                h4de3113_1    conda-forge\r\nlibzlib                   1.2.11            h36c2ea0_1013    conda-forge\r\nlibzopfli                 1.0.3                h9c3ff4c_0    conda-forge\r\nllvmlite                  0.37.0           py37h9d7f4d0_1    conda-forge\r\nlocket                    0.2.0                      py_2    conda-forge\r\nlz4-c                     1.9.3                h9c3ff4c_1    conda-forge\r\nmapclassify               2.4.3              pyhd8ed1ab_0    conda-forge\r\nmarkdown                  3.3.6              pyhd8ed1ab_0    conda-forge\r\nmarkupsafe                2.0.1            py37h5e8e339_1    conda-forge\r\nmatplotlib-base           3.5.1            py37h1058ff1_0    conda-forge\r\nmatplotlib-inline         0.1.3              pyhd8ed1ab_0    conda-forge\r\nmistune                   0.8.4           py37h5e8e339_1005    conda-forge\r\nmsgpack-python            1.0.3            py37h2527ec5_0    conda-forge\r\nmultidict                 5.2.0            py37h5e8e339_1    conda-forge\r\nmultipledispatch          0.6.0                      py_0    conda-forge\r\nmunch                     2.5.0                      py_0    conda-forge\r\nmunkres                   1.1.4              pyh9f0ad1d_0    conda-forge\r\nnbclient                  0.5.9              pyhd8ed1ab_0    conda-forge\r\nnbconvert                 6.4.0            py37h89c1867_0    conda-forge\r\nnbformat                  5.1.3              pyhd8ed1ab_0    conda-forge\r\nnccl                      2.11.4.1             h96e36e3_0    conda-forge\r\nncurses                   6.2                  h58526e2_4    conda-forge\r\nnest-asyncio              1.5.4              pyhd8ed1ab_0    conda-forge\r\nnetworkx                  2.6.3              pyhd8ed1ab_1    conda-forge\r\nnodejs                    14.17.4              h92b4a50_0    conda-forge\r\nnotebook                  6.4.6              pyha770c72_0    conda-forge\r\nnspr                      4.32                 h9c3ff4c_1    conda-forge\r\nnss                       3.73                 hb5efdd6_0    conda-forge\r\nnumba                     0.54.1           py37h2d894fd_0    conda-forge\r\nnumpy                     1.20.3           py37h038b26d_1    conda-forge\r\nnvtx                      0.2.3            py37h5e8e339_1    conda-forge\r\nolefile                   0.46               pyh9f0ad1d_1    conda-forge\r\nopenjdk                   11.0.9.1             h5cc2fde_1    conda-forge\r\nopenjpeg                  2.4.0                hb52868f_1    conda-forge\r\nopenssl                   1.1.1l               h7f98852_0    conda-forge\r\norc                       1.7.1                h1be678f_1    conda-forge\r\npackaging                 21.3               pyhd8ed1ab_0    conda-forge\r\npandas                    1.3.5            py37he8f5f7f_0    conda-forge\r\npandoc                    2.16.2               h7f98852_0    conda-forge\r\npandocfilters             1.5.0              pyhd8ed1ab_0    conda-forge\r\npanel                     0.12.4             pyhd8ed1ab_0    conda-forge\r\nparam                     1.12.0             pyh6c4a22f_0    conda-forge\r\nparquet-cpp               1.5.1                         2    conda-forge\r\nparso                     0.8.3              pyhd8ed1ab_0    conda-forge\r\npartd                     1.2.0              pyhd8ed1ab_0    conda-forge\r\npcre                      8.45                 h9c3ff4c_0    conda-forge\r\npexpect                   4.8.0              pyh9f0ad1d_2    conda-forge\r\npickle5                   0.0.12           py37h5e8e339_0    conda-forge\r\npickleshare               0.7.5                   py_1003    conda-forge\r\npillow                    8.4.0            py37h0f21c89_0    conda-forge\r\npip                       21.3.1             pyhd8ed1ab_0    conda-forge\r\npixman                    0.40.0               h36c2ea0_0    conda-forge\r\npooch                     1.5.2              pyhd8ed1ab_0    conda-forge\r\npoppler                   21.09.0              ha39eefc_3    conda-forge\r\npoppler-data              0.4.11               hd8ed1ab_0    conda-forge\r\npostgresql                13.5                 h2510834_1    conda-forge\r\nproj                      8.1.0                h277dcde_1    conda-forge\r\nprometheus_client         0.12.0             pyhd8ed1ab_0    conda-forge\r\nprompt-toolkit            3.0.24             pyha770c72_0    conda-forge\r\nprotobuf                  3.19.2           py37hcd2ae1e_0    conda-forge\r\npsutil                    5.9.0            py37h5e8e339_0    conda-forge\r\npthread-stubs             0.4               h36c2ea0_1001    conda-forge\r\nptxcompiler               0.2.0            py37h81e21aa_0    rapidsai\r\nptyprocess                0.7.0              pyhd3deb0d_0    conda-forge\r\npy-xgboost                1.5.0dev.rapidsai21.12  cuda11.0py37_0    rapidsai\r\npyarrow                   5.0.0           py37h63cede7_20_cuda    conda-forge\r\npycparser                 2.21               pyhd8ed1ab_0    conda-forge\r\npyct                      0.4.6                      py_0    conda-forge\r\npyct-core                 0.4.6                      py_0    conda-forge\r\npydantic                  1.9.0            py37h5e8e339_0    conda-forge\r\npydeck                    0.5.0              pyh9f0ad1d_0    conda-forge\r\npyee                      8.1.0              pyh9f0ad1d_0    conda-forge\r\npygments                  2.11.1             pyhd8ed1ab_0    conda-forge\r\npynvml                    11.4.1             pyhd8ed1ab_0    conda-forge\r\npyopenssl                 21.0.0             pyhd8ed1ab_0    conda-forge\r\npyparsing                 3.0.6              pyhd8ed1ab_0    conda-forge\r\npyppeteer                 0.2.6              pyhd8ed1ab_0    conda-forge\r\npyproj                    3.1.0            py37hdf91f24_4    conda-forge\r\npyrsistent                0.18.0           py37h5e8e339_0    conda-forge\r\npysocks                   1.7.1            py37h89c1867_4    conda-forge\r\npython                    3.7.12          hb7a2778_100_cpython    conda-forge\r\npython-confluent-kafka    1.6.0            py37h5e8e339_1    conda-forge\r\npython-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\r\npython-tzdata             2021.5             pyhd8ed1ab_0    conda-forge\r\npython_abi                3.7                     2_cp37m    conda-forge\r\npytz                      2021.3             pyhd8ed1ab_0    conda-forge\r\npytz-deprecation-shim     0.1.0.post0      py37h89c1867_1    conda-forge\r\npyviz_comms               2.1.0              pyhd8ed1ab_0    conda-forge\r\npywavelets                1.2.0            py37hb1e94ed_1    conda-forge\r\npyyaml                    6.0              py37h5e8e339_3    conda-forge\r\npyzmq                     22.3.0           py37h336d617_1    conda-forge\r\nrapids                    21.12.00        cuda11.0_py37_gc46440c_94    rapidsai\r\nrapids-xgboost            21.12.00        cuda11.0_py37_gc46440c_94    rapidsai\r\nre2                       2021.11.01           h9c3ff4c_0    conda-forge\r\nreadline                  8.1                  h46c0cb4_0    conda-forge\r\nrequests                  2.27.0             pyhd8ed1ab_0    conda-forge\r\nrmm                       21.12.00        cuda11_py37_g957ad04_0_no_cma    rapidsai\r\nrtree                     0.9.7            py37h0b55af0_3    conda-forge\r\ns2n                       1.3.0                h9b69904_0    conda-forge\r\nscikit-image              0.18.1           py37hdc94413_0    conda-forge\r\nscikit-learn              1.0.2            py37hf9e9bfc_0    conda-forge\r\nscipy                     1.7.3            py37hf2a6cf1_0    conda-forge\r\nsend2trash                1.8.0              pyhd8ed1ab_0    conda-forge\r\nsetuptools                60.2.0           py37h89c1867_0    conda-forge\r\nshapely                   1.8.0            py37h48c49eb_0    conda-forge\r\nsimpervisor               0.4                pyhd8ed1ab_0    conda-forge\r\nsix                       1.16.0             pyh6c4a22f_0    conda-forge\r\nsnappy                    1.1.8                he1b5a44_3    conda-forge\r\nsniffio                   1.2.0            py37h89c1867_2    conda-forge\r\nsortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge\r\nspdlog                    1.8.5                h4bd325d_0    conda-forge\r\nsqlite                    3.37.0               h9cd32fc_0    conda-forge\r\nstarlette                 0.16.0             pyhd8ed1ab_0    conda-forge\r\nstreamz                   0.6.3              pyh6c4a22f_0    conda-forge\r\ntabulate                  0.8.9              pyhd8ed1ab_0    conda-forge\r\ntblib                     1.7.0              pyhd8ed1ab_0    conda-forge\r\nterminado                 0.12.1           py37h89c1867_1    conda-forge\r\ntestpath                  0.5.0              pyhd8ed1ab_0    conda-forge\r\nthreadpoolctl             3.0.0              pyh8a188c0_0    conda-forge\r\ntifffile                  2021.11.2          pyhd8ed1ab_0    conda-forge\r\ntiledb                    2.3.4                he87e0bf_0    conda-forge\r\ntk                        8.6.11               h27826a3_1    conda-forge\r\ntoolz                     0.11.2             pyhd8ed1ab_0    conda-forge\r\ntornado                   6.1              py37h5e8e339_2    conda-forge\r\ntqdm                      4.62.3             pyhd8ed1ab_0    conda-forge\r\ntraitlets                 5.1.1              pyhd8ed1ab_0    conda-forge\r\ntreelite                  2.1.0            py37h4b3d254_0    conda-forge\r\ntreelite-runtime          2.1.0                    pypi_0    pypi\r\ntyping-extensions         4.0.1                hd8ed1ab_0    conda-forge\r\ntyping_extensions         4.0.1              pyha770c72_0    conda-forge\r\ntzcode                    2021e                h7f98852_0    conda-forge\r\ntzdata                    2021e                he74cb21_0    conda-forge\r\ntzlocal                   4.1              py37h89c1867_1    conda-forge\r\nucx                       1.11.2+gef2bbcf      cuda11.0_0    rapidsai\r\nucx-proc                  1.0.0                       gpu    rapidsai\r\nucx-py                    0.23.0          py37_gef2bbcf_0    rapidsai\r\nunicodedata2              14.0.0           py37h5e8e339_0    conda-forge\r\nurllib3                   1.26.7             pyhd8ed1ab_0    conda-forge\r\nuvicorn                   0.16.0           py37h89c1867_0    conda-forge\r\nwcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge\r\nwebencodings              0.5.1                      py_1    conda-forge\r\nwebsocket-client          1.2.3              pyhd8ed1ab_0    conda-forge\r\nwebsockets                9.1              py37h5e8e339_0    conda-forge\r\nwheel                     0.37.1             pyhd8ed1ab_0    conda-forge\r\nwidgetsnbextension        3.5.2            py37h89c1867_1    conda-forge\r\nxarray                    0.20.2             pyhd8ed1ab_0    conda-forge\r\nxerces-c                  3.2.3                h9d8b166_3    conda-forge\r\nxgboost                   1.5.0dev.rapidsai21.12  cuda11.0py37_0    rapidsai\r\nxorg-fixesproto           5.0               h7f98852_1002    conda-forge\r\nxorg-inputproto           2.3.2             h7f98852_1002    conda-forge\r\nxorg-kbproto              1.0.7             h7f98852_1002    conda-forge\r\nxorg-libice               1.0.10               h7f98852_0    conda-forge\r\nxorg-libsm                1.2.3             hd9c2040_1000    conda-forge\r\nxorg-libx11               1.7.2                h7f98852_0    conda-forge\r\nxorg-libxau               1.0.9                h7f98852_0    conda-forge\r\nxorg-libxdmcp             1.1.3                h7f98852_0    conda-forge\r\nxorg-libxext              1.3.4                h7f98852_1    conda-forge\r\nxorg-libxfixes            5.0.3             h7f98852_1004    conda-forge\r\nxorg-libxi                1.7.10               h7f98852_0    conda-forge\r\nxorg-libxrender           0.9.10            h7f98852_1003    conda-forge\r\nxorg-libxtst              1.2.3             h7f98852_1002    conda-forge\r\nxorg-recordproto          1.14.2            h7f98852_1002    conda-forge\r\nxorg-renderproto          0.11.1            h7f98852_1002    conda-forge\r\nxorg-xextproto            7.3.0             h7f98852_1002    conda-forge\r\nxorg-xproto               7.0.31            h7f98852_1007    conda-forge\r\nxz                        5.2.5                h516909a_1    conda-forge\r\nyaml                      0.2.5                h7f98852_2    conda-forge\r\nyarl                      1.7.2            py37h5e8e339_1    conda-forge\r\nzeromq                    4.3.4                h9c3ff4c_1    conda-forge\r\nzfp                       0.5.5                h9c3ff4c_8    conda-forge\r\nzict                      2.0.0                      py_0    conda-forge\r\nzipp                      3.6.0              pyhd8ed1ab_0    conda-forge\r\nzlib                      1.2.11            h36c2ea0_1013    conda-forge\r\nzstd                      1.5.1                ha95c52a_0    conda-forge\r\n(rapids-21.12) anamaria@gpu-2-0.saber:~ $ \r\n"
      },
      {
        "user": "anamariaUIC",
        "created_at": "2022-01-17T21:19:30Z",
        "body": "@galipremsagar additional related question: in order to use DASK do I need to have NVLink on my GPUs?"
      },
      {
        "user": "anamariaUIC",
        "created_at": "2022-01-25T20:25:21Z",
        "body": "@galipremsagar are there are any updates on these questions?"
      },
      {
        "user": "beckernick",
        "created_at": "2022-01-25T20:45:56Z",
        "body": "Using Dask does not require NVLink.\r\n\r\nYour conda environment appears to have Dask (`dask 2021.11.2 pyhd8ed1ab_0 conda-forge`). Is it possible your environment is not correctly set up on your system (do other libraries work), or that you may be picking up a different Dask from somewhere else on the system?"
      },
      {
        "user": "anamariaUIC",
        "created_at": "2022-01-27T21:41:34Z",
        "body": "@beckernick I can import  dask.dataframe but not  dask.dataframe.dispatch. please see the following:\r\n\r\n(rapids-21.12) anamaria@gpu-2-0.saber:~ $ python\r\nPython 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import dask\r\n>>> import dask.dataframe.dispatch\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'dask.dataframe.dispatch'\r\n>>> import dask.dataframe\r\n>>> import dask.dataframe.dispatch\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'dask.dataframe.dispatch'\r\n\r\n(rapids-21.12) anamaria@gpu-2-0.saber:~ $ env\r\nPROJ_LIB=/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids-21.12/share/proj\r\nCUDA_PATH=/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids-21.12\r\nMANPATH=/software/linux-el7-x86_64/tools/lmod/lmod/share/man::\r\nXDG_SESSION_ID=3727\r\nJAVA_LD_LIBRARY_PATH=/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids-21.12/lib/server\r\n_ModuleTable003_=dmljZXMvaG9kLy5sb2NhbC9lYXN5YnVpbGQvbW9kdWxlcy9hbGw6L3NvZnR3YXJlL2xpbnV4LWVsNy14ODZfNjQvdG9vbHMvbG1vZC9sbW9kL21vZHVsZWZpbGVzL0NvcmUiLH0=\r\nHOSTNAME=gpu-2-0.saber\r\nSELINUX_ROLE_REQUESTED=\r\nTERM=xterm-256color\r\nSHELL=/bin/bash\r\nHISTSIZE=1000\r\nMODULEPATH_ROOT=/software/linux-el7-x86_64/tools/modulefiles\r\nSSH_CLIENT=192.168.15.4 57388 22\r\nCONDA_SHLVL=1\r\nCONDA_PROMPT_MODIFIER=(rapids-21.12) \r\nSELINUX_USE_CURRENT_RANGE=\r\nLMOD_PKG=/software/linux-el7-x86_64/tools/lmod/lmod\r\nQTDIR=/usr/lib64/qt-3.3\r\nLMOD_VERSION=7.7\r\nQTINC=/usr/lib64/qt-3.3/include\r\nSSH_TTY=/dev/pts/1\r\n__LMOD_REF_COUNT_LOADEDMODULES=apps/anaconda3:1\r\nQT_GRAPHICSSYSTEM_CHECKED=1\r\nUSER=anamaria\r\nLMOD_sys=Linux\r\nLS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:\r\nCONDA_EXE=/software/linux-el7-x86_64/apps/anaconda3/anaconda3/bin/conda\r\n__LMOD_REF_COUNT__LMFILES_=/software/linux-el7-x86_64/modules/apps/anaconda3:1\r\n_CE_CONDA=\r\nCPL_ZIP_ENCODING=UTF-8\r\n_ModuleTable001_=X01vZHVsZVRhYmxlXz17WyJNVHZlcnNpb24iXT0zLFsiY19yZWJ1aWxkVGltZSJdPWZhbHNlLFsiY19zaG9ydFRpbWUiXT1mYWxzZSxkZXB0aFQ9e30sZmFtaWx5PXt9LG1UPXtbImFwcHMvYW5hY29uZGEzIl09e1siZm4iXT0iL3NvZnR3YXJlL2xpbnV4LWVsNy14ODZfNjQvbW9kdWxlcy9hcHBzL2FuYWNvbmRhMyIsWyJmdWxsTmFtZSJdPSJhcHBzL2FuYWNvbmRhMyIsWyJsb2FkT3JkZXIiXT0xLHByb3BUPXt9LFsic3RhY2tEZXB0aCJdPTAsWyJzdGF0dXMiXT0iYWN0aXZlIixbInVzZXJOYW1lIl09ImFwcHMvYW5hY29uZGEzIix9LH0sbXBhdGhBPXsiL3NvZnR3YXJlL2xpbnV4LWVsNy14ODZfNjQvdG9vbHMvRWFzeUJ1aWxkLTQuMS4wL21vZHVsZXMvYWxsIiwiL3NvZnR3\r\nMAIL=/var/spool/mail/anamaria\r\nPATH=/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids-21.12/bin:/software/linux-el7-x86_64/apps/anaconda3/anaconda3/bin:/software/linux-el7-x86_64/apps/anaconda3/anaconda3/condabin:/usr/local/bin:/usr/lib64/qt-3.3/bin:/usr/bin:/usr/local/sbin:/usr/sbin\r\nPROJ_NETWORK=ON\r\nCONDA_PREFIX=/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids-21.12\r\nLMOD_SETTARG_CMD=:\r\nPWD=/home/anamaria\r\n_LMFILES_=/software/linux-el7-x86_64/modules/apps/anaconda3\r\nJAVA_HOME=/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids-21.12\r\nLANG=en_US.UTF-8\r\nMODULEPATH=/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/modules/all:/software/linux-el7-x86_64/modules:/software/linux-el7-nvidia/modules:/software/linux-el7-x86_64/services/hod/.local/easybuild/modules/all:/software/linux-el7-x86_64/tools/lmod/lmod/modulefiles/Core\r\n_ModuleTable_Sz_=3\r\nLOADEDMODULES=apps/anaconda3\r\nKDEDIRS=/usr\r\nSELINUX_LEVEL_REQUESTED=\r\nGDAL_DATA=/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids-21.12/share/gdal\r\nLMOD_CMD=/software/linux-el7-x86_64/tools/lmod/lmod/libexec/lmod\r\n_CE_M=\r\nSSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass\r\nHISTCONTROL=ignoredups\r\nSHLVL=1\r\nHOME=/home/anamaria\r\n__LMOD_REF_COUNT_PATH=/software/linux-el7-x86_64/apps/anaconda3/anaconda3/bin:1;/software/linux-el7-x86_64/apps/anaconda3/anaconda3/condabin:1;/usr/local/bin:2;/usr/lib64/qt-3.3/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1\r\n_ModuleTable002_=YXJlL2xpbnV4LWVsNy14ODZfNjQvbW9kdWxlcyIsIi9zb2Z0d2FyZS9saW51eC1lbDctbnZpZGlhL21vZHVsZXMiLCIvc29mdHdhcmUvbGludXgtZWw3LXg4Nl82NC9zZXJ2aWNlcy9ob2QvLmxvY2FsL2Vhc3lidWlsZC9tb2R1bGVzL2FsbCIsIi9zb2Z0d2FyZS9saW51eC1lbDcteDg2XzY0L3Rvb2xzL2xtb2QvbG1vZC9tb2R1bGVmaWxlcy9Db3JlIix9LFsic3lzdGVtQmFzZU1QQVRIIl09Ii9zb2Z0d2FyZS9saW51eC1lbDcteDg2XzY0L3Rvb2xzL0Vhc3lCdWlsZC00LjEuMC9tb2R1bGVzL2FsbDovc29mdHdhcmUvbGludXgtZWw3LXg4Nl82NC9tb2R1bGVzOi9zb2Z0d2FyZS9saW51eC1lbDctbnZpZGlhL21vZHVsZXM6L3NvZnR3YXJlL2xpbnV4LWVsNy14ODZfNjQvc2Vy\r\nBASH_ENV=/software/linux-el7-x86_64/tools/lmod/lmod/init/bash\r\nCONDA_PYTHON_EXE=/software/linux-el7-x86_64/apps/anaconda3/anaconda3/bin/python\r\nLOGNAME=anamaria\r\nQTLIB=/usr/lib64/qt-3.3/lib\r\nXDG_DATA_DIRS=/home/anamaria/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share\r\nSSH_CONNECTION=192.168.15.4 57388 192.168.12.36 22\r\nMODULESHOME=/software/linux-el7-x86_64/tools/lmod/lmod\r\nJAVA_HOME_CONDA_BACKUP=/software/linux-el7-x86_64/compilers/jdk-1.8.0_131\r\nCONDA_DEFAULT_ENV=rapids-21.12\r\nLESSOPEN=||/usr/bin/lesspipe.sh %s\r\nLMOD_FULL_SETTARG_SUPPORT=no\r\nXDG_RUNTIME_DIR=/run/user/554479\r\nQT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins\r\nJAVA_LD_LIBRARY_PATH_BACKUP=\r\nLMOD_DIR=/software/linux-el7-x86_64/tools/lmod/lmod/libexec\r\nBASH_FUNC_module()=() {  eval $($LMOD_CMD bash \"$@\") && eval $(${LMOD_SETTARG_CMD:-:} -s sh)\r\n}\r\nBASH_FUNC_ml()=() {  eval $($LMOD_DIR/ml_cmd \"$@\")\r\n}\r\n_=/usr/bin/env\r\n"
      },
      {
        "user": "anamariaUIC",
        "created_at": "2022-02-11T23:11:32Z",
        "body": "Hello,\r\n\r\nCan someone please advise me on this:\r\nimport dask_cudf\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n/tmp/ipykernel_59808/2855451223.py in <module>\r\n----> 1 import dask_cudf\r\n\r\n/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids/lib/python3.7/site-packages/dask_cudf/__init__.py in <module>\r\n      4 from cudf._version import get_versions\r\n      5 \r\n----> 6 from . import backends\r\n      7 from .core import DataFrame, Series, concat, from_cudf, from_dask_dataframe\r\n      8 from .groupby import groupby_agg\r\n\r\n/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids/lib/python3.7/site-packages/dask_cudf/backends.py in <module>\r\n      9 \r\n     10 from dask.dataframe.core import get_parallel_type, meta_nonempty\r\n---> 11 from dask.dataframe.dispatch import (\r\n     12     categorical_dtype_dispatch,\r\n     13     concat_dispatch,\r\n\r\nModuleNotFoundError: No module named 'dask.dataframe.dispatch'\r\n\r\n"
      },
      {
        "user": "anamariaUIC",
        "created_at": "2022-02-11T23:25:22Z",
        "body": "This is what I have in .bashrc\r\n\r\n```\r\n# !! Contents within this block are managed by 'conda init' !!\r\n__conda_setup=\"$('/software/linux-el7-x86_64/apps/anaconda3/anaconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\r\nif [ $? -eq 0 ]; then\r\n    eval \"$__conda_setup\"\r\nelse\r\n    if [ -f \"/software/linux-el7-x86_64/apps/anaconda3/anaconda3/etc/profile.d/conda.sh\" ]; then\r\n        . \"/software/linux-el7-x86_64/apps/anaconda3/anaconda3/etc/profile.d/conda.sh\"\r\n    else\r\n        export PATH=\"/software/linux-el7-x86_64/apps/anaconda3/anaconda3/bin:$PATH\"\r\n    fi\r\nfi\r\nunset __conda_setup\r\n```\r\n"
      },
      {
        "user": "anamariaUIC",
        "created_at": "2022-02-12T00:51:04Z",
        "body": "I am submitting this as well, if it is of any help to debug this:\r\n\r\n`(rapids) anamaria@gpu-2-0.saber:~ $ python\r\nPython 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import os\r\n>>> os.environ\r\nenviron({'PROJ_LIB': '/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids/share/proj', 'CUDA_PATH': '/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids', 'MANPATH': '/software/linux-el7-x86_64/tools/lmod/lmod/share/man::', 'XDG_SESSION_ID': '4116', 'JAVA_LD_LIBRARY_PATH': '/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids/lib/server', '_ModuleTable003_': 'dmljZXMvaG9kLy5sb2NhbC9lYXN5YnVpbGQvbW9kdWxlcy9hbGw6L3NvZnR3YXJlL2xpbnV4LWVsNy14ODZfNjQvdG9vbHMvbG1vZC9sbW9kL21vZHVsZWZpbGVzL0NvcmUiLH0=', 'HOSTNAME': 'gpu-2-0.saber', 'SELINUX_ROLE_REQUESTED': '', 'TERM': 'xterm-256color', 'SHELL': '/bin/bash', 'HISTSIZE': '1000', 'MODULEPATH_ROOT': '/software/linux-el7-x86_64/tools/modulefiles', 'SSH_CLIENT': '192.168.15.4 58410 22', 'CONDA_SHLVL': '1', 'CONDA_PROMPT_MODIFIER': '(rapids) ', 'SELINUX_USE_CURRENT_RANGE': '', 'LMOD_PKG': '/software/linux-el7-x86_64/tools/lmod/lmod', 'QTDIR': '/usr/lib64/qt-3.3', 'LMOD_VERSION': '7.7', 'QTINC': '/usr/lib64/qt-3.3/include', 'SSH_TTY': '/dev/pts/1', '__LMOD_REF_COUNT_LOADEDMODULES': 'apps/anaconda3:1', 'QT_GRAPHICSSYSTEM_CHECKED': '1', 'USER': 'anamaria', 'LMOD_sys': 'Linux', 'LS_COLORS': 'rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:', 'CONDA_EXE': '/software/linux-el7-x86_64/apps/anaconda3/anaconda3/bin/conda', '__LMOD_REF_COUNT__LMFILES_': '/software/linux-el7-x86_64/modules/apps/anaconda3:1', '_CE_CONDA': '', 'CPL_ZIP_ENCODING': 'UTF-8', '_ModuleTable001_': 'X01vZHVsZVRhYmxlXz17WyJNVHZlcnNpb24iXT0zLFsiY19yZWJ1aWxkVGltZSJdPWZhbHNlLFsiY19zaG9ydFRpbWUiXT1mYWxzZSxkZXB0aFQ9e30sZmFtaWx5PXt9LG1UPXtbImFwcHMvYW5hY29uZGEzIl09e1siZm4iXT0iL3NvZnR3YXJlL2xpbnV4LWVsNy14ODZfNjQvbW9kdWxlcy9hcHBzL2FuYWNvbmRhMyIsWyJmdWxsTmFtZSJdPSJhcHBzL2FuYWNvbmRhMyIsWyJsb2FkT3JkZXIiXT0xLHByb3BUPXt9LFsic3RhY2tEZXB0aCJdPTAsWyJzdGF0dXMiXT0iYWN0aXZlIixbInVzZXJOYW1lIl09ImFwcHMvYW5hY29uZGEzIix9LH0sbXBhdGhBPXsiL3NvZnR3YXJlL2xpbnV4LWVsNy14ODZfNjQvdG9vbHMvRWFzeUJ1aWxkLTQuMS4wL21vZHVsZXMvYWxsIiwiL3NvZnR3', 'MAIL': '/var/spool/mail/anamaria', 'PATH': '/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids/bin:/software/linux-el7-x86_64/apps/anaconda3/anaconda3/bin:/software/linux-el7-x86_64/apps/anaconda3/anaconda3/condabin:/usr/local/bin:/usr/lib64/qt-3.3/bin:/usr/bin:/usr/local/sbin:/usr/sbin', 'PROJ_NETWORK': 'ON', 'CONDA_PREFIX': '/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids', 'LMOD_SETTARG_CMD': ':', 'PWD': '/home/anamaria', 'JAVA_HOME': '/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids', '_LMFILES_': '/software/linux-el7-x86_64/modules/apps/anaconda3', 'LANG': 'en_US.UTF-8', 'MODULEPATH': '/software/linux-el7-x86_64/tools/EasyBuild-4.1.0/modules/all:/software/linux-el7-x86_64/modules:/software/linux-el7-nvidia/modules:/software/linux-el7-x86_64/services/hod/.local/easybuild/modules/all:/software/linux-el7-x86_64/tools/lmod/lmod/modulefiles/Core', '_ModuleTable_Sz_': '3', 'LOADEDMODULES': 'apps/anaconda3', 'KDEDIRS': '/usr', 'SELINUX_LEVEL_REQUESTED': '', 'GDAL_DATA': '/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids/share/gdal', 'LMOD_CMD': '/software/linux-el7-x86_64/tools/lmod/lmod/libexec/lmod', '_CE_M': '', 'SSH_ASKPASS': '/usr/libexec/openssh/gnome-ssh-askpass', 'HISTCONTROL': 'ignoredups', 'SHLVL': '1', 'HOME': '/home/anamaria', '__LMOD_REF_COUNT_PATH': '/software/linux-el7-x86_64/apps/anaconda3/anaconda3/bin:1;/software/linux-el7-x86_64/apps/anaconda3/anaconda3/condabin:1;/usr/local/bin:2;/usr/lib64/qt-3.3/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1', '_ModuleTable002_': 'YXJlL2xpbnV4LWVsNy14ODZfNjQvbW9kdWxlcyIsIi9zb2Z0d2FyZS9saW51eC1lbDctbnZpZGlhL21vZHVsZXMiLCIvc29mdHdhcmUvbGludXgtZWw3LXg4Nl82NC9zZXJ2aWNlcy9ob2QvLmxvY2FsL2Vhc3lidWlsZC9tb2R1bGVzL2FsbCIsIi9zb2Z0d2FyZS9saW51eC1lbDcteDg2XzY0L3Rvb2xzL2xtb2QvbG1vZC9tb2R1bGVmaWxlcy9Db3JlIix9LFsic3lzdGVtQmFzZU1QQVRIIl09Ii9zb2Z0d2FyZS9saW51eC1lbDcteDg2XzY0L3Rvb2xzL0Vhc3lCdWlsZC00LjEuMC9tb2R1bGVzL2FsbDovc29mdHdhcmUvbGludXgtZWw3LXg4Nl82NC9tb2R1bGVzOi9zb2Z0d2FyZS9saW51eC1lbDctbnZpZGlhL21vZHVsZXM6L3NvZnR3YXJlL2xpbnV4LWVsNy14ODZfNjQvc2Vy', 'BASH_ENV': '/software/linux-el7-x86_64/tools/lmod/lmod/init/bash', 'CONDA_PYTHON_EXE': '/software/linux-el7-x86_64/apps/anaconda3/anaconda3/bin/python', 'LOGNAME': 'anamaria', 'QTLIB': '/usr/lib64/qt-3.3/lib', 'XDG_DATA_DIRS': '/home/anamaria/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share', 'SSH_CONNECTION': '192.168.15.4 58410 192.168.12.36 22', 'MODULESHOME': '/software/linux-el7-x86_64/tools/lmod/lmod', 'JAVA_HOME_CONDA_BACKUP': '', 'CONDA_DEFAULT_ENV': 'rapids', 'LESSOPEN': '||/usr/bin/lesspipe.sh %s', 'LMOD_FULL_SETTARG_SUPPORT': 'no', 'XDG_RUNTIME_DIR': '/run/user/554479', 'QT_PLUGIN_PATH': '/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins', 'JAVA_LD_LIBRARY_PATH_BACKUP': '', 'LMOD_DIR': '/software/linux-el7-x86_64/tools/lmod/lmod/libexec', 'BASH_FUNC_module()': '() {  eval $($LMOD_CMD bash \"$@\") && eval $(${LMOD_SETTARG_CMD:-:} -s sh)\\n}', 'BASH_FUNC_ml()': '() {  eval $($LMOD_DIR/ml_cmd \"$@\")\\n}', '_': '/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids/bin/python'})\r\n>>> \r\n`"
      },
      {
        "user": "anamariaUIC",
        "created_at": "2022-02-12T01:16:49Z",
        "body": "Ok, I resolved this and here is solution for all people who might be facing the same issue:\r\n\r\nI obtained these lines from a person on the same cluster for whom importing dask_cudf did work:\r\n\r\n```\r\n>>> import dask\r\n>>> print(dask.__file__)\r\n/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids/lib/python3.7/site-packages/dask/__init__.py\r\n```\r\n\r\nThis is what I was getting:\r\n\r\n```\r\n>>> import dask\r\n>>> print(dask.__file__)\r\n/home/anamaria/.local/lib/python3.7/site-packages/dask/__init__.py\r\n```\r\n\r\nMy issue was resolved by putting in my .bashrc:\r\n\r\n`export PYTHONPATH=$PYTHONPATH:\"/software/linux-el7-x86_64/apps/anaconda3/anaconda3/envs/rapids/lib/python3.7/site-packages”`"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-03-15T23:03:29Z",
        "body": "This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days."
      }
    ]
  },
  {
    "number": 9780,
    "title": "[QST] cuDF Memorycpy Problems",
    "created_at": "2021-11-26T08:21:56Z",
    "closed_at": "2022-04-06T11:04:39Z",
    "labels": [
      "question",
      "libcudf"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/9780",
    "body": "**What is your question?**\r\nHello every master,here is another for the cuDF:\r\nRecently ,I've been using and reading the codes of cuDF.\r\nThe function **from_arrow()** is a wonderful work\r\nIn the from_arrow.cu I saw that **return detail::from_arrow(input_table, rmm::cuda_stream_default, mr);**\r\nSo I wonder if the cuDF doing the Memcpy with only one stream as rmm::cuda_stream_default Or rmm::cuda_stream_default is a parallel way of loading data?\r\nIs there any other way to accelerate the Data Loading process?\r\nThx for your replies!!!",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/9780/comments",
    "author": "EdisonZiXing",
    "comments": [
      {
        "user": "harrism",
        "created_at": "2021-11-29T20:53:18Z",
        "body": "> `return detail::from_arrow(input_table, rmm::cuda_stream_default, mr);` \r\n\r\nAll libcudf public APIs have a corresponding `detail` API that takes a stream. Since CUDA streams are not exposed in the public API, we pass `rmm::cuda_stream_default` from the public to the detail API. That's what the above line is doing. One day, we plan to expose streams in the public APIs, but we have not done that. `cuda_stream_default` is just CUDA's default stream. If per-thread default stream is enabled, this will be the per-thread default stream. So one could call `from_arrow` on different data from multiple threads and get parallelism. However, there is nothing inherently parallel about `cuda_stream_default`.\r\n\r\n> Is there any other way to accelerate the Data Loading process?\r\n\r\nI think for us to look at this we should ask you about your specific problem: do you have a workflow where `from_arrow` is a significant bottleneck that needs to be accelerated? What specifically do you mean by `Data Loading`? From file? From Arrow? Something else?"
      },
      {
        "user": "EdisonZiXing",
        "created_at": "2021-12-03T08:39:19Z",
        "body": "> > `return detail::from_arrow(input_table, rmm::cuda_stream_default, mr);`\r\n> \r\n> All libcudf public APIs have a corresponding `detail` API that takes a stream. Since CUDA streams are not exposed in the public API, we pass `rmm::cuda_stream_default` from the public to the detail API. That's what the above line is doing. One day, we plan to expose streams in the public APIs, but we have not done that. `cuda_stream_default` is just CUDA's default stream. If per-thread default stream is enabled, this will be the per-thread default stream. So one could call `from_arrow` on different data from multiple threads and get parallelism. However, there is nothing inherently parallel about `cuda_stream_default`.\r\n> \r\n> > Is there any other way to accelerate the Data Loading process?\r\n> \r\n> I think for us to look at this we should ask you about your specific problem: do you have a workflow where `from_arrow` is a significant bottleneck that needs to be accelerated? What specifically do you mean by `Data Loading`? From file? From Arrow? Something else?\r\n\r\n@harrism \r\nDear Harrism:\r\nThank you for the instruction about Q1 and I really looking forward to seeing cuda_stream_per_thread coming out!!!\r\nI've seen cuda_stream_per_thread in the code and now I knew that this part of function is still working on.\r\nAbout Q2,I should have clearly explained my  question and your infer is right:Our data is loaded in host in-mem,so we do have a workflow where from_arrow is a significant bottleneck that needs to be accelerated.I was trying to pull multi processes by manual  and doing from_arrow(),seems like the speed of transmission from Host to Device become higher, 14GB/s at peak but still not meeting the 32GB/s of Tesla-v100 PCIE .But we still want to accelerate this part so that we could lower the cost of data transmitting.As a member of cuDF and the expert of this area,do you have any suggests about the using ways of cuDF that we could using the extreme performance of the host(cpu/in-mem) and device or any other plans?\r\nThanks again for your patient and advice,Harrism!"
      },
      {
        "user": "harrism",
        "created_at": "2021-12-06T21:36:09Z",
        "body": "I don't think we can reach peak PCI-E rates with data in host without registering (pinning) the host memory. I think we would need a test program to look at your specific use case."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-05T22:03:01Z",
        "body": "This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-05T23:03:04Z",
        "body": "This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed."
      },
      {
        "user": "shwina",
        "created_at": "2022-04-06T11:04:39Z",
        "body": "Going to mark this one closed, but please feel free to reopen if necessary."
      }
    ]
  },
  {
    "number": 9622,
    "title": "[BUG] Failed to import Python cudf in the recent nightly releases",
    "created_at": "2021-11-08T04:58:03Z",
    "closed_at": "2021-11-19T02:44:19Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/9622",
    "body": "**Describe the bug**\r\nFailed to import the cudf python module due to the error below in the latest nightly release (11-07).\r\n\r\n```\r\nPython 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import cudf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/liangcail/miniconda3/envs/cudf-python-rt/lib/python3.8/site-packages/cudf/__init__.py\", line 4, in <module>\r\n    validate_setup()\r\n  File \"/home/liangcail/miniconda3/envs/cudf-python-rt/lib/python3.8/site-packages/cudf/utils/gpu_utils.py\", line 18, in validate_setup\r\n    from rmm._cuda.gpu import (\r\n  File \"/home/liangcail/miniconda3/envs/cudf-python-rt/lib/python3.8/site-packages/rmm/__init__.py\", line 16, in <module>\r\n    from rmm import mr\r\n  File \"/home/liangcail/miniconda3/envs/cudf-python-rt/lib/python3.8/site-packages/rmm/mr.py\", line 14, in <module>\r\n    from rmm._lib.memory_resource import (\r\n  File \"/home/liangcail/miniconda3/envs/cudf-python-rt/lib/python3.8/site-packages/rmm/_lib/__init__.py\", line 15, in <module>\r\n    from .device_buffer import DeviceBuffer\r\n  File \"rmm/_lib/device_buffer.pyx\", line 1, in init rmm._lib.device_buffer\r\nImportError: /home/liangcail/miniconda3/envs/cudf-python-rt/lib/python3.8/site-packages/rmm/_lib/memory_resource.cpython-38-x86_64-linux-gnu.so: undefined symbol: cudaMemPoolCreate, version libcudart.so.11.0\r\n>>> \r\n```\r\n\r\n**Steps/Code to reproduce bug**\r\nInstall the latest nightly version of Python cudf .\r\n  `conda install -c rapidsai-nightly -c nvidia -c numba -c conda-forge cudf python=3.8 cudatoolkit=11.0`,\r\nThen open a Python shell and execute `import cudf`.\r\n\r\n**Expected behavior**\r\nThe `import` statement should not fail.\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location:  Bare-metal\r\n - Method of cuDF install: [conda]\r\n\r\n\r\n ***conda packages***\r\n```\r\n     /home/liangcail/miniconda3/condabin/conda\r\n     # packages in environment at /home/liangcail/miniconda3/envs/cudf-python-rt:\r\n     #\r\n     # Name                    Version                   Build  Channel\r\n     _libgcc_mutex             0.1                 conda_forge    conda-forge\r\n     _openmp_mutex             4.5                       1_gnu    conda-forge\r\n     abseil-cpp                20210324.2           h9c3ff4c_0    conda-forge\r\n     arrow-cpp                 5.0.0           py38hd93b8a5_0_cuda    conda-forge\r\n     arrow-cpp-proc            3.0.0                      cuda    conda-forge\r\n     attrs                     21.2.0             pyhd8ed1ab_0    conda-forge\r\n     aws-c-cal                 0.5.11               h95a6274_0    conda-forge\r\n     aws-c-common              0.6.2                h7f98852_0    conda-forge\r\n     aws-c-event-stream        0.2.7               h3541f99_13    conda-forge\r\n     aws-c-io                  0.10.5               hfb6a706_0    conda-forge\r\n     aws-checksums             0.1.11               ha31a3da_7    conda-forge\r\n     aws-sdk-cpp               1.8.186              hb4091e7_3    conda-forge\r\n     bzip2                     1.0.8                h7f98852_4    conda-forge\r\n     c-ares                    1.17.1               h7f98852_1    conda-forge\r\n     ca-certificates           2021.10.8            ha878542_0    conda-forge\r\n     cachetools                4.2.2              pyhd8ed1ab_0    conda-forge\r\n     certifi                   2021.10.8        py38h578d9bd_1    conda-forge\r\n     cudatoolkit               11.0.221             h6bb024c_0    nvidia\r\n     cudf                      21.12.00a211105 cuda_11.0_py38_g4cba672c17_224    rapidsai-nightly\r\n     cudnn                     8.0.0                cuda11.0_0    nvidia\r\n     cupy                      8.0.0            py38hb7c6141_0    rapidsai-nightly\r\n     dlpack                    0.5                  h9c3ff4c_0    conda-forge\r\n     fastavro                  1.4.4            py38h497a2fe_0    conda-forge\r\n     fastrlock                 0.6              py38h709712a_1    conda-forge\r\n     fsspec                    2021.7.0           pyhd8ed1ab_0    conda-forge\r\n     gflags                    2.2.2             he1b5a44_1004    conda-forge\r\n     glog                      0.5.0                h48cff8f_0    conda-forge\r\n     grpc-cpp                  1.38.1               h36ce80c_0    conda-forge\r\n     importlib-metadata        4.6.1            py38h578d9bd_0    conda-forge\r\n     importlib_metadata        4.6.1                hd8ed1ab_0    conda-forge\r\n     iniconfig                 1.1.1              pyh9f0ad1d_0    conda-forge\r\n     krb5                      1.19.1               hcc1bbae_0    conda-forge\r\n     ld_impl_linux-64          2.36.1               hea4e1c9_1    conda-forge\r\n     libblas                   3.9.0                9_openblas    conda-forge\r\n     libbrotlicommon           1.0.9                h7f98852_5    conda-forge\r\n     libbrotlidec              1.0.9                h7f98852_5    conda-forge\r\n     libbrotlienc              1.0.9                h7f98852_5    conda-forge\r\n     libcblas                  3.9.0                9_openblas    conda-forge\r\n     libcudf                   21.12.00a211107 cuda11.0_g4cba672c17_224    rapidsai-nightly\r\n     libcurl                   7.77.0               h2574ce0_0    conda-forge\r\n     libedit                   3.1.20191231         he28a2e2_2    conda-forge\r\n     libev                     4.33                 h516909a_1    conda-forge\r\n     libevent                  2.1.10               hcdb4288_3    conda-forge\r\n     libffi                    3.3                  h58526e2_2    conda-forge\r\n     libgcc-ng                 9.3.0               h2828fa1_19    conda-forge\r\n     libgfortran-ng            9.3.0               hff62375_19    conda-forge\r\n     libgfortran5              9.3.0               hff62375_19    conda-forge\r\n     libgomp                   9.3.0               h2828fa1_19    conda-forge\r\n     liblapack                 3.9.0                9_openblas    conda-forge\r\n     libllvm10                 10.0.1               he513fc3_3    conda-forge\r\n     libnghttp2                1.43.0               h812cca2_0    conda-forge\r\n     libopenblas               0.3.15          pthreads_h8fe5266_1    conda-forge\r\n     libprotobuf               3.16.0               h780b84a_0    conda-forge\r\n     librmm                    21.12.00a211107 cuda11_g76ae622_20_no_cma    rapidsai-nightly\r\n     libssh2                   1.9.0                ha56f1ee_6    conda-forge\r\n     libstdcxx-ng              9.3.0               h6de172a_19    conda-forge\r\n     libthrift                 0.14.2               he6d91bd_1    conda-forge\r\n     libutf8proc               2.6.1                h7f98852_0    conda-forge\r\n     llvmlite                  0.37.0           py38he1b5a44_0    numba\r\n     lz4-c                     1.9.3                h9c3ff4c_0    conda-forge\r\n     more-itertools            8.8.0              pyhd8ed1ab_0    conda-forge\r\n     nccl                      2.7.8.1            h4962215_100    nvidia\r\n     ncurses                   6.2                  h58526e2_4    conda-forge\r\n     numba                     0.54.1          np1.11py3.8hc13618b_g39aef3deb_0    numba\r\n     numpy                     1.20.3           py38h9894fe3_1    conda-forge\r\n     nvtx                      0.2.3            py38h497a2fe_0    conda-forge\r\n     openssl                   1.1.1k               h7f98852_0    conda-forge\r\n     orc                       1.6.9                h58a87f1_0    conda-forge\r\n     packaging                 21.0               pyhd8ed1ab_0    conda-forge\r\n     pandas                    1.3.1            py38h1abd341_0    conda-forge\r\n     parquet-cpp               1.5.1                         2    conda-forge\r\n     pip                       21.1.3             pyhd8ed1ab_0    conda-forge\r\n     pluggy                    1.0.0            py38h578d9bd_2    conda-forge\r\n     protobuf                  3.16.0           py38h709712a_0    conda-forge\r\n     py                        1.10.0             pyhd3deb0d_0    conda-forge\r\n     pyarrow                   5.0.0           py38hed47224_0_cuda    conda-forge\r\n     pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge\r\n     pytest                    6.2.5            py38h578d9bd_1    conda-forge\r\n     python                    3.8.10          h49503c6_1_cpython    conda-forge\r\n     python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\r\n     python_abi                3.8                      2_cp38    conda-forge\r\n     pytz                      2021.1             pyhd8ed1ab_0    conda-forge\r\n     re2                       2021.06.01           h9c3ff4c_0    conda-forge\r\n     readline                  8.1                  h46c0cb4_0    conda-forge\r\n     rmm                       21.12.00a211107 cuda_11_py38_g76ae622_20    rapidsai-nightly\r\n     s2n                       1.0.10               h9b69904_0    conda-forge\r\n     setuptools                58.5.3           py38h578d9bd_0    conda-forge\r\n     six                       1.16.0             pyh6c4a22f_0    conda-forge\r\n     snappy                    1.1.8                he1b5a44_3    conda-forge\r\n     spdlog                    1.8.5                h4bd325d_0    conda-forge\r\n     sqlite                    3.36.0               h9cd32fc_0    conda-forge\r\n     sre_yield                 1.2              py38h32f6830_2    conda-forge\r\n     tk                        8.6.10               h21135ba_1    conda-forge\r\n     toml                      0.10.2             pyhd8ed1ab_0    conda-forge\r\n     typing_extensions         3.10.0.0           pyha770c72_0    conda-forge\r\n     wheel                     0.36.2             pyhd3deb0d_0    conda-forge\r\n     xz                        5.2.5                h516909a_1    conda-forge\r\n     zipp                      3.5.0              pyhd8ed1ab_0    conda-forge\r\n     zlib                      1.2.11            h516909a_1010    conda-forge\r\n     zstd                      1.5.0                ha95c52a_0    conda-forge\r\n\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/9622/comments",
    "author": "firestarman",
    "comments": [
      {
        "user": "beckernick",
        "created_at": "2021-11-08T15:13:52Z",
        "body": "@firestarman could you try with a more recent nightly or provide a reproducible example with system information?\r\n\r\nI am unable to reproduce this issue with the current nightly installed with the traditional install of `mamba create -n cudf-21.12 -c rapidsai-nightly -c nvidia -c conda-forge cudf=21.12 python=3.8 cudatoolkit=11.0` (note that this nightly is at a more recent commit of libcudf and interestingly with different versions of rmm/numba as well).\r\n\r\n```\r\nconda list | grep \"rapids\\|numba\"\r\ncudf                      21.12.00a211105 cuda_11.0_py38_g4cba672c17_224    rapidsai-nightly\r\ncupy                      8.0.0            py38hb7c6141_0    rapidsai-nightly\r\nlibcudf                   21.12.00a211108 cuda11.0_g4cba672c17_224    rapidsai-nightly\r\nlibrmm                    21.12.00a211104 cuda11.0_gef1981d_17    rapidsai-nightly\r\nnumba                     0.53.1           py38h8b71fd7_1    conda-forge\r\nrmm                       21.12.00a211104 cuda_11.0_py38_gef1981d_17    rapidsai-nightly```\r\n"
      },
      {
        "user": "pxLi",
        "created_at": "2021-11-09T06:26:39Z",
        "body": "Thanks! ~~libcudf 211105-211107 builds have the issue, newer libcudf like 21.12.00a211108/21.12.00a211109 works fine~~\r\n\r\n~~close this issue~~"
      },
      {
        "user": "pxLi",
        "created_at": "2021-11-09T09:02:59Z",
        "body": "looks like this is all about timing\r\n\r\n`mamba create -n cudf-21.12 -c rapidsai-nightly -c nvidia -c conda-forge cudf=21.12 python=3.8 cudatoolkit=11.0`, this `g3280be232d_233` build just failed again w/ rmm 211109\r\n```\r\ncudf                      21.12.00a211109 cuda_11.0_py38_g3280be232d_233    rapidsai-nightly\r\nlibcudf                   21.12.00a211109 cuda11.0_g3280be232d_233    rapidsai-nightly\r\nlibrmm                    21.12.00a211109 cuda11_g728a117_21_no_cma    rapidsai-nightly\r\nrmm                       21.12.00a211109 cuda_11_py38_g728a117_21    rapidsai-nightly\r\n```\r\n\r\n```\r\n>>> import cudf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/conda/lib/python3.8/site-packages/cudf/__init__.py\", line 4, in <module>\r\n    validate_setup()\r\n  File \"/opt/conda/lib/python3.8/site-packages/cudf/utils/gpu_utils.py\", line 18, in validate_setup\r\n    from rmm._cuda.gpu import (\r\n  File \"/opt/conda/lib/python3.8/site-packages/rmm/__init__.py\", line 16, in <module>\r\n    from rmm import mr\r\n  File \"/opt/conda/lib/python3.8/site-packages/rmm/mr.py\", line 14, in <module>\r\n    from rmm._lib.memory_resource import (\r\n  File \"/opt/conda/lib/python3.8/site-packages/rmm/_lib/__init__.py\", line 15, in <module>\r\n    from .device_buffer import DeviceBuffer\r\n  File \"rmm/_lib/device_buffer.pyx\", line 1, in init rmm._lib.device_buffer\r\nImportError: /opt/conda/lib/python3.8/site-packages/rmm/_lib/memory_resource.cpython-38-x86_64-linux-gnu.so: symbol cudaMemPoolCreate, version libcudart.so.11.0 not defined in file libcudart.so.11.0 with link time reference\r\n```\r\n\r\nif we are lucky \r\nto get **rmm&librmm** **cuda11.0**_xxx, everything will be fine\r\nbut if  **rmm&librmm** **cuda11**_xxx, we will fail importing\r\n\r\ncan you help fix the non-deterministic dependencies here? thanks~"
      },
      {
        "user": "galipremsagar",
        "created_at": "2021-11-09T12:43:55Z",
        "body": "I ran into a similar issue:\r\n```\r\npgali@dt07:/nvme/0/pgali/cudf$ python\r\nPython 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:59:51) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import cudf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/nvme/0/pgali/envs/cudf110/lib/python3.8/site-packages/cudf/__init__.py\", line 4, in <module>\r\n    validate_setup()\r\n  File \"/nvme/0/pgali/envs/cudf110/lib/python3.8/site-packages/cudf/utils/gpu_utils.py\", line 18, in validate_setup\r\n    from rmm._cuda.gpu import (\r\n  File \"/nvme/0/pgali/envs/cudf110/lib/python3.8/site-packages/rmm/__init__.py\", line 16, in <module>\r\n    from rmm import mr\r\n  File \"/nvme/0/pgali/envs/cudf110/lib/python3.8/site-packages/rmm/mr.py\", line 14, in <module>\r\n    from rmm._lib.memory_resource import (\r\n  File \"/nvme/0/pgali/envs/cudf110/lib/python3.8/site-packages/rmm/_lib/__init__.py\", line 15, in <module>\r\n    from .device_buffer import DeviceBuffer\r\n  File \"rmm/_lib/device_buffer.pyx\", line 1, in init rmm._lib.device_buffer\r\nImportError: /nvme/0/pgali/envs/cudf110/lib/python3.8/site-packages/rmm/_lib/memory_resource.cpython-38-x86_64-linux-gnu.so: symbol cudaMemPoolCreate version libcudart.so.11.0 not defined in file libcudart.so.11.0 with link time reference\r\n>>> exit()\r\npgali@dt07:/nvme/0/pgali/cudf$ conda list | grep \"librmm\"\r\nlibrmm                    21.12.00a211109 cuda11_g728a117_21_no_cma    rapidsai-nightly\r\npgali@dt07:/nvme/0/pgali/cudf$ conda list \r\n# packages in environment at /nvme/0/pgali/envs/cudf110:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                 conda_forge    conda-forge\r\n_openmp_mutex             4.5                       1_gnu    conda-forge\r\nabseil-cpp                20210324.2           h9c3ff4c_0    conda-forge\r\narrow-cpp                 5.0.0           py38hb22f866_11_cuda    conda-forge\r\narrow-cpp-proc            3.0.0                      cuda    conda-forge\r\naws-c-auth                0.6.4                h66ccdbb_3    conda-forge\r\naws-c-cal                 0.5.12               hf5774b0_2    conda-forge\r\naws-c-common              0.6.11               h7f98852_0    conda-forge\r\naws-c-compression         0.2.14               hcbdb60a_2    conda-forge\r\naws-c-event-stream        0.2.7               hb3c4c74_24    conda-forge\r\naws-c-http                0.6.6                h0021954_1    conda-forge\r\naws-c-io                  0.10.9               h14fba84_3    conda-forge\r\naws-c-mqtt                0.7.8                h8df633b_2    conda-forge\r\naws-c-s3                  0.1.27               h8e2754f_1    conda-forge\r\naws-checksums             0.1.12               hcbdb60a_1    conda-forge\r\naws-crt-cpp               0.17.1               hf793bd7_3    conda-forge\r\naws-sdk-cpp               1.9.120              hab07a0e_0    conda-forge\r\nbzip2                     1.0.8                h7f98852_4    conda-forge\r\nc-ares                    1.18.1               h7f98852_0    conda-forge\r\nca-certificates           2021.10.8            ha878542_0    conda-forge\r\ncachetools                4.2.4              pyhd8ed1ab_0    conda-forge\r\ncudatoolkit               11.0.221             h6bb024c_0    nvidia\r\ncudf                      21.12.00a211109 cuda_11.0_py38_g3280be232d_233    rapidsai-nightly\r\ncudnn                     8.0.0                cuda11.0_0    nvidia\r\ncupy                      8.0.0            py38hb7c6141_0    rapidsai-nightly\r\ndlpack                    0.5                  h9c3ff4c_0    conda-forge\r\nfastavro                  1.4.7            py38h497a2fe_1    conda-forge\r\nfastrlock                 0.8              py38h709712a_1    conda-forge\r\nfsspec                    2021.11.0          pyhd8ed1ab_0    conda-forge\r\ngflags                    2.2.2             he1b5a44_1004    conda-forge\r\nglog                      0.5.0                h48cff8f_0    conda-forge\r\ngrpc-cpp                  1.40.0               h05f19cf_2    conda-forge\r\nkrb5                      1.19.2               hcc1bbae_3    conda-forge\r\nld_impl_linux-64          2.36.1               hea4e1c9_2    conda-forge\r\nlibblas                   3.9.0           12_linux64_openblas    conda-forge\r\nlibbrotlicommon           1.0.9                h7f98852_6    conda-forge\r\nlibbrotlidec              1.0.9                h7f98852_6    conda-forge\r\nlibbrotlienc              1.0.9                h7f98852_6    conda-forge\r\nlibcblas                  3.9.0           12_linux64_openblas    conda-forge\r\nlibcudf                   21.12.00a211109 cuda11.0_g3280be232d_233    rapidsai-nightly\r\nlibcurl                   7.79.1               h2574ce0_1    conda-forge\r\nlibedit                   3.1.20191231         he28a2e2_2    conda-forge\r\nlibev                     4.33                 h516909a_1    conda-forge\r\nlibevent                  2.1.10               h9b69904_4    conda-forge\r\nlibffi                    3.4.2                h9c3ff4c_4    conda-forge\r\nlibgcc-ng                 11.2.0              h1d223b6_11    conda-forge\r\nlibgfortran-ng            11.2.0              h69a702a_11    conda-forge\r\nlibgfortran5              11.2.0              h5c6108e_11    conda-forge\r\nlibgomp                   11.2.0              h1d223b6_11    conda-forge\r\nliblapack                 3.9.0           12_linux64_openblas    conda-forge\r\nlibllvm10                 10.0.1               he513fc3_3    conda-forge\r\nlibnghttp2                1.43.0               h812cca2_1    conda-forge\r\nlibnsl                    2.0.0                h7f98852_0    conda-forge\r\nlibopenblas               0.3.18          pthreads_h8fe5266_0    conda-forge\r\nlibprotobuf               3.18.1               h780b84a_0    conda-forge\r\nlibrmm                    21.12.00a211109 cuda11_g728a117_21_no_cma    rapidsai-nightly\r\nlibssh2                   1.10.0               ha56f1ee_2    conda-forge\r\nlibstdcxx-ng              11.2.0              he4da1e4_11    conda-forge\r\nlibthrift                 0.15.0               he6d91bd_1    conda-forge\r\nlibutf8proc               2.6.1                h7f98852_0    conda-forge\r\nlibzlib                   1.2.11            h36c2ea0_1013    conda-forge\r\nllvmlite                  0.36.0           py38h4630a5e_0    conda-forge\r\nlz4-c                     1.9.3                h9c3ff4c_1    conda-forge\r\nnccl                      2.7.8.1            h4962215_100    nvidia\r\nncurses                   6.2                  h58526e2_4    conda-forge\r\nnumba                     0.53.1           py38h8b71fd7_1    conda-forge\r\nnumpy                     1.21.4           py38he2449b9_0    conda-forge\r\nnvtx                      0.2.3            py38h497a2fe_0    conda-forge\r\nopenssl                   1.1.1l               h7f98852_0    conda-forge\r\norc                       1.7.1                h68e2c4e_0    conda-forge\r\npackaging                 21.0               pyhd8ed1ab_0    conda-forge\r\npandas                    1.3.4            py38h43a58ef_1    conda-forge\r\nparquet-cpp               1.5.1                         2    conda-forge\r\npip                       21.3.1             pyhd8ed1ab_0    conda-forge\r\nprotobuf                  3.18.1           py38h709712a_0    conda-forge\r\npyarrow                   5.0.0           py38hb90f683_11_cuda    conda-forge\r\npyparsing                 3.0.5              pyhd8ed1ab_0    conda-forge\r\npython                    3.8.12          hb7a2778_2_cpython    conda-forge\r\npython-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\r\npython_abi                3.8                      2_cp38    conda-forge\r\npytz                      2021.3             pyhd8ed1ab_0    conda-forge\r\nre2                       2021.09.01           h9c3ff4c_0    conda-forge\r\nreadline                  8.1                  h46c0cb4_0    conda-forge\r\nrmm                       21.12.00a211109 cuda_11_py38_g728a117_21    rapidsai-nightly\r\ns2n                       1.1.1                h9b69904_0    conda-forge\r\nsetuptools                58.5.3           py38h578d9bd_0    conda-forge\r\nsix                       1.16.0             pyh6c4a22f_0    conda-forge\r\nsnappy                    1.1.8                he1b5a44_3    conda-forge\r\nspdlog                    1.8.5                h4bd325d_0    conda-forge\r\nsqlite                    3.36.0               h9cd32fc_2    conda-forge\r\ntk                        8.6.11               h27826a3_1    conda-forge\r\ntyping_extensions         3.10.0.2           pyha770c72_0    conda-forge\r\nwheel                     0.37.0             pyhd8ed1ab_1    conda-forge\r\nxz                        5.2.5                h516909a_1    conda-forge\r\nzlib                      1.2.11            h36c2ea0_1013    conda-forge\r\nzstd                      1.5.0                ha95c52a_0    conda-forge\r\n```"
      },
      {
        "user": "galipremsagar",
        "created_at": "2021-11-09T12:57:50Z",
        "body": "Note this doesn't happen on 11.2, only happens on 11.0."
      },
      {
        "user": "pxLi",
        "created_at": "2021-11-19T02:44:19Z",
        "body": "not seeing this for several days, close it"
      }
    ]
  },
  {
    "number": 9401,
    "title": "missing: cuFile_LIBRARY cuFileRDMA_LIBRARY cuFile_INCLUDE_DIR",
    "created_at": "2021-10-07T19:45:20Z",
    "closed_at": "2022-07-25T16:56:35Z",
    "labels": [
      "question",
      "CMake"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/9401",
    "body": "Hi,\r\n\r\nI am trying to build the C++ library from source using cuda 11.2 / gcc 9.4 / Ubuntu 18.04 / Driver Version: 470.74. However, it seems that a dependency is missing. In particular, I get the following error:\r\n\r\nCould NOT find cuFile (missing: cuFile_LIBRARY cuFileRDMA_LIBRARY cuFile_INCLUDE_DIR) \r\n\r\nIs there a way to obtain this library for Cuda 11.2? Cuda 11.4 seems to include GDS, but I havent managed to find this library for Cuda 11.2. Any pointer here is appreciated. \r\n\r\nBest,\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/9401/comments",
    "author": "pedronahum",
    "comments": [
      {
        "user": "beckernick",
        "created_at": "2021-10-08T16:53:36Z",
        "body": "Hi @pedronahum , thanks for filing the issue. We've made some updates in 21.10 that should prevent this in the general case. Could you share the full error?"
      },
      {
        "user": "pedronahum",
        "created_at": "2021-10-09T13:08:46Z",
        "body": "Hi @beckernick. Deeply sorry, yesterday evening I upgraded my distribution to Ubuntu 20.04 and cannot reproduce the error.\r\n\r\nfyi, it seems that aws-sdk-cpp is required, as the build will fail if a header file (ie, aws/core/Aws.h) from this library is not found. After installing this sdk, I managed to build the cudf library. \r\n\r\nBest, \r\n\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-15T21:02:55Z",
        "body": "This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days."
      },
      {
        "user": "Atharex",
        "created_at": "2021-12-09T21:18:50Z",
        "body": "seeing a similar error with a centos build with version 21.12. \r\n\r\nWhat is this cuFile? Does it have to be externally installed/built before cudf?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-01-08T22:04:58Z",
        "body": "This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-04-08T23:03:04Z",
        "body": "This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed."
      },
      {
        "user": "vyasr",
        "created_at": "2022-07-25T16:56:35Z",
        "body": "@Atharex cuFile is a part of the CTK since CUDA 11.4. If you have a sufficiently new CTK it will be present. Building libcudf now requires CUDA>=11.5, so if you can build the latest versions of cudf you should automatically have cuFile available."
      }
    ]
  },
  {
    "number": 8563,
    "title": "[QST] error: could not find git for clone of jitify-populate",
    "created_at": "2021-06-19T05:56:36Z",
    "closed_at": "2022-07-15T01:50:07Z",
    "labels": [
      "question",
      "0 - Waiting on Author",
      "CMake"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/8563",
    "body": "**What is your question?**\r\nI am building the project with the command below.\r\n`sudo bash ./build.sh --ptds`\r\nBelow is the order in which I solved the error....\r\nThe last jitify-populate error could not be resolved...\r\n### first error\r\nBuilding for the architecture of the GPU in the system...\r\n-- The CUDA compiler identification is NVIDIA 11.2.67\r\n-- Detecting CUDA compiler ABI info\r\n-- Detecting CUDA compiler ABI info - done\r\n-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\r\n-- Detecting CUDA compile features\r\n-- Detecting CUDA compile features - done\r\n-- CUDF: Auto detection of gpu-archs: 61\r\n-- CUDF: Building CUDF for GPU architectures: 61-real\r\nCMake Error at /snap/cmake/882/share/cmake-3.20/Modules/FindPackageHandleStandardArgs.cmake:230 (message):\r\n  Could NOT find ZLIB (missing: ZLIB_LIBRARY ZLIB_INCLUDE_DIR)\r\nCall Stack (most recent call first):\r\n  /snap/cmake/882/share/cmake-3.20/Modules/FindPackageHandleStandardArgs.cmake:594 (_FPHSA_FAILURE_MESSAGE)\r\n  /snap/cmake/882/share/cmake-3.20/Modules/FindZLIB.cmake:120 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)\r\n  CMakeLists.txt:121 (find_package)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/home/rmfkdehd/cudf-branch-21.08/cpp/build/CMakeFiles/CMakeOutput.log\".\r\nSee also \"/home/rmfkdehd/cudf-branch-21.08/cpp/build/CMakeFiles/CMakeError.log\".\r\n### solution\r\nsudo apt-get install zlib1g-dev\r\n### Second Error\r\nBuilding for the architecture of the GPU in the system...\r\n-- CUDF: Auto detection of gpu-archs: 61\r\n-- CUDF: Building CUDF for GPU architectures: 61-real\r\n-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \r\n-- CPM: adding package jitify@2.0.0 (cudf_0.19)\r\nCMake Error at /snap/cmake/882/share/cmake-3.20/Modules/ExternalProject.cmake:2633 (message):\r\n  error: could not find git for clone of jitify-populate\r\nCall Stack (most recent call first):\r\n  /snap/cmake/882/share/cmake-3.20/Modules/ExternalProject.cmake:3683 (_ep_add_download_command)\r\n  CMakeLists.txt:22 (ExternalProject_Add)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/home/rmfkdehd/cudf-branch-21.08/cpp/build/_deps/jitify-subbuild/CMakeFiles/CMakeOutput.log\".\r\n\r\nCMake Error at /snap/cmake/882/share/cmake-3.20/Modules/FetchContent.cmake:1000 (message):\r\n  CMake step for jitify failed: 1\r\nCall Stack (most recent call first):\r\n  /snap/cmake/882/share/cmake-3.20/Modules/FetchContent.cmake:1141:EVAL:2 (__FetchContent_directPopulate)\r\n  /snap/cmake/882/share/cmake-3.20/Modules/FetchContent.cmake:1141 (cmake_language)\r\n  build/cmake/CPM_7644c3a40fc7889f8dee53ce21e85dc390b883dc.cmake:753 (FetchContent_Populate)\r\n  build/cmake/CPM_7644c3a40fc7889f8dee53ce21e85dc390b883dc.cmake:581 (cpm_fetch_package)\r\n  build/cmake/CPM_7644c3a40fc7889f8dee53ce21e85dc390b883dc.cmake:258 (CPMAddPackage)\r\n  cmake/thirdparty/CUDF_GetJitify.cmake:20 (CPMFindPackage)\r\n  cmake/thirdparty/CUDF_GetJitify.cmake:29 (find_and_configure_jitify)\r\n  CMakeLists.txt:127 (include)\r\nHow To Fix jitify-populate Errors?......",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/8563/comments",
    "author": "MickeyDragon",
    "comments": [
      {
        "user": "harrism",
        "created_at": "2021-06-22T00:54:20Z",
        "body": "Do you have `git` installed?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-15T21:03:35Z",
        "body": "This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed."
      },
      {
        "user": "vyasr",
        "created_at": "2022-07-15T01:50:07Z",
        "body": "I'm going to close this for now as likely caused by either a bad connection or broken git installation, but please reopen if needed."
      }
    ]
  },
  {
    "number": 8134,
    "title": "[QST] cudf column for strings",
    "created_at": "2021-05-01T22:33:05Z",
    "closed_at": "2022-07-15T01:07:32Z",
    "labels": [
      "question",
      "Python",
      "dask"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/8134",
    "body": "I've got a `dask_cudf` dataframe named \"ddf\" that has a column named \"name\" which consists of strings that are 8 characters long.  When I use `map_partitions()` to apply some quadtree filtering with `cuspatial`, I get an error meassage stating:\r\n```\r\nRuntimeError: cuDF failure at: ../include/cudf/strings/detail/gather.cuh:148: total size of output strings is too large for a cudf column\r\n```\r\nIf I drop the \"name\" column, this error does not occur.  I should be able to use a left join to reinstate the \"name\" column after quadtree filtering, but when I add the following code to do so, I get the same error.  How can I fix or workaround this issue?\r\n```\r\nunique_cudf = (ddf.loc[:, [\"num_id\", \"name\"]]\r\n                .map_partitions(lambda cudf: cudf.head(1))\r\n                .reset_index()\r\n                .drop(\"index\", axis=1)\r\n                .compute())\r\nddf = ddf.drop(\"name\", axis=1)\r\nddf = ddf.map_partitions(lambda cudf: my_quadtree_filter())\r\nddf = ddf.merge(unique_cudf, how=\"left\", on=\"num_id\")\r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/8134/comments",
    "author": "buckeye17",
    "comments": [
      {
        "user": "beckernick",
        "created_at": "2021-05-06T21:26:08Z",
        "body": "When the error presents, are you calling `compute`, `persist`, or something else?\r\n\r\nIn this context, the error means an individual partition has a column with too many total characters across all strings in the column. It's hard to be more concrete without additional details. Does this still occur if you increase the number of partitions during either the read stage or by repartitioning your dataframe in memory?"
      },
      {
        "user": "buckeye17",
        "created_at": "2021-05-11T03:52:13Z",
        "body": "@beckernick Thanks for your follow-up.  Initially I worked around this issue as follows (write the ddf produced by `cuspatial` to disk, then read it back in and carry on).  I then continued modifying my geospatial algorithm for other reasons and later discovered that this issue somehow resolved itself.  I no longer need the write/read hack.  Unfortaunately, I have no idea what I did to fix it.\r\n\r\nFor context, none of my partitions were \"large\" when I was encountering this error.  Each partition had < 50,000 rows.  Even when I replaced the individual strings with \"a\" in every instance, I still got the same error.  Somehow `cuspatial` wasn't happy with having a string column in the dataframe, even though it was unused by `cuspatial`.\r\n```\r\nunique_cudf = (ddf.loc[:, [\"num_id\", \"name\"]]\r\n                .map_partitions(lambda cudf: cudf.head(1))\r\n                .reset_index()\r\n                .drop(\"index\", axis=1)\r\n                .compute())\r\nddf = ddf.drop(\"name\", axis=1)\r\nddf = ddf.map_partitions(lambda cudf: my_quadtree_filter())\r\nddf.to_parquet(\"temp.parquet\")\r\nddf = dask_cudf.read_parquet(\"temp.parquet\")\r\nddf = ddf.merge(unique_cudf, how=\"left\", on=\"num_id\")\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-11-15T21:04:26Z",
        "body": "This issue has been labeled `inactive-90d` due to no recent activity in the past 90 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed."
      },
      {
        "user": "vyasr",
        "created_at": "2022-07-15T01:07:32Z",
        "body": "Since this issue vanished and we have no repro of the original error case, and since we know the typical cause of this error (column with too many characters), I'm going to close this issue as resolved. Please feel free to reopen if the issue recurs."
      }
    ]
  },
  {
    "number": 8019,
    "title": "How to \"concatenate\" rows into 1 list with groupby",
    "created_at": "2021-04-21T16:48:29Z",
    "closed_at": "2021-04-22T12:48:23Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/8019",
    "body": "I am trying to concatenate multiple rows into one single list after a _groupby_.\r\n\r\nWith _Pandas_, I can do this:\r\n\r\n```\r\ndf = pd.DataFrame({'A': [1,1,2,2,2,2,3],'B':['a','b','c','d','e','f','g']})\r\ndf = df.groupby('A')['B'].apply(list)\r\n\r\nA\r\n-------------------\r\n1          [a, b]\r\n2    [c, d, e, f]\r\n3             [g]\r\n```\r\n\r\nIs there any equivalent solutions using _cudf_?\r\n\r\nI tried the following without success\r\n\r\n```\r\ngdf = gdf.groupby('A')['B'].apply(list)\r\n\r\n**TypeError: Series object is not iterable. Consider using `.to_arrow()`, `.to_pandas()` or `.values_host` if you wish to iterate over the values.**\r\n```\r\n\r\n```\r\ngdf = gdf.groupby('A')['B'].apply(lambda x : list(x))\r\n\r\n**TypeError: Series object is not iterable. Consider using `.to_arrow()`, `.to_pandas()` or `.values_host` if you wish to iterate over the values.**\r\n```\r\n```\r\ngdf = gdf.groupby('A')['B'].apply(lambda x : x.to_arrow())\r\n\r\n**TypeError: cannot concatenate object of type <class 'pyarrow.lib.StringArray'>**\r\n```\r\n\r\n```\r\ngdf = gdf.groupby('A').agg({'B': lambda x: list(x)})\r\n\r\n**TypeError: 'type' object is not iterable**\r\n```\r\n\r\nAny suggestions? Thanks!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/8019/comments",
    "author": "gfiameni",
    "comments": [
      {
        "user": "beckernick",
        "created_at": "2021-04-21T17:07:01Z",
        "body": "You can use the `agg` API. `df.groupby('A')['B'].agg(list)` or `df.groupby('A').agg({\"B\":list})`"
      },
      {
        "user": "gfiameni",
        "created_at": "2021-04-22T12:48:23Z",
        "body": "Thanks! It works perfectly."
      }
    ]
  },
  {
    "number": 7991,
    "title": "[QST] Queston about row number limit in cuDF dataframe",
    "created_at": "2021-04-19T05:13:54Z",
    "closed_at": "2021-04-19T17:25:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/7991",
    "body": "Dear cuDF developers,\r\n\r\nI am using Dask to load in some parquet data as a dask-cudf dataframe. When I use `.compute()` to convert from dask-cudf back to cuDF (I need some functions that aren't supported in dask-cudf) I am encountering this error:\r\n\r\n```\r\nRuntimeError: cuDF failure at: ../src/copying/concatenate.cu:365: Total number of concatenated rows exceeds size_type range\r\n```\r\n\r\nMy dataframe has 27 million rows which seems... large but maybe still reasonable? What is the row limit? Is there any way I can increase this limit? \r\n\r\nIf I can provide more info please let me know.\r\n\r\nThank you very much,\r\nLaurie",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/7991/comments",
    "author": "lastephey",
    "comments": [
      {
        "user": "davidwendt",
        "created_at": "2021-04-19T12:09:44Z",
        "body": "The row limit for a cudf column/dataframe is 2 billion (2,147,483,647)."
      },
      {
        "user": "beckernick",
        "created_at": "2021-04-19T13:06:00Z",
        "body": "> When I use .compute() to convert from dask-cudf back to cuDF (I need some functions that aren't supported in dask-cudf) I am encountering this error:\r\n\r\nWould you be able to share which operations aren't supported in dask-cudf?\r\n\r\nIn the meantime, could you stay in Dask land but use the cudf operations with `ddf.map_partitions(custom_func)` to operate independently on each individual DataFrame partition?"
      },
      {
        "user": "lastephey",
        "created_at": "2021-04-19T16:32:19Z",
        "body": "Thank you for your quick responses, David and Nick.\r\n\r\nSure, a few of the operations I'm using in cuDF:\r\n* `cudf.melt`\r\n* `cudf.to_datetime`\r\n* `cudf.drop_duplicates`\r\n\r\nThank you for the suggestion-- I can try the map partitions approach and report back. \r\n\r\nI am wondering why I hit this row limit when I am well under 2 billion rows. Does it sound like a possible bug? If so I am happy to file a report.\r\n\r\n"
      },
      {
        "user": "beckernick",
        "created_at": "2021-04-19T17:12:19Z",
        "body": "Thanks Laurie!\r\n\r\nThis likely came up due to non numeric columns in the table. For example, the ~2 billion MAX(int32) limit on string columns presents based on the number of individual characters, rather than rows.\r\n\r\n\r\n> Sure, a few of the operations I'm using in cuDF:\r\n> cudf.melt\r\n> cudf.to_datetime\r\n> cudf.drop_duplicates\r\n\r\ndrop_duplicates and melt should be available as `ddf.drop_duplicates()` and `ddf.melt()`. For `to_datetime`, could you intead explicitly cast the column with `ddf[col].astype(\"datetime64[ms]\")`? If anything of these aren't working, please let us know!"
      },
      {
        "user": "lastephey",
        "created_at": "2021-04-19T17:25:49Z",
        "body": "Thanks Nick. Ok I see, the row limit makes sense.\r\n\r\nThanks for the pointer about melt and drop_duplicates. I see now I was trying to use them incorrectly, like:\r\n\r\n```\r\ndask_cudf.melt(ddf)\r\n```\r\ninstead of\r\n\r\n```\r\nddf.melt()\r\n```\r\n\r\nI think I should be able to make this work within Dask using your suggestions. I'll close this, thank you very much for your help!"
      }
    ]
  },
  {
    "number": 7824,
    "title": "[QST]running cudf of cpu",
    "created_at": "2021-04-01T21:29:11Z",
    "closed_at": "2021-04-01T22:04:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/7824",
    "body": "I am developing a ML pipeline on my macbook pro laptop to be deployed on a server which has NVDIA servers. There is not ssh, neither interactive terminal to work with the server. The challenge is I wish I could make my code in my laptop, debug and make sure it works and the send it out to the server. However, I seems I can't have CUDF running on my laptop due to the hardware.\r\n\r\nIn TensorFlow using a parameter I can set the hardware to be cpu or gpu and TF will take care of the rest. This is very handy since one can develop the code with cpu but deploy on gpu. \r\n\r\nI wonder if there is any recommendation in such a setup - where I can keep developing the pipeline with CUDF on my laptop but be able simple to run it on GPU ?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/7824/comments",
    "author": "naarkhoo",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2021-04-01T21:35:22Z",
        "body": "@naarkhoo the general guidance we give here is if you're using `cuDF` directly to use pandas instead of `cuDF` and if you're using `dask-cudf` to use `dask.dataframe` instead. As of now, our stance is that it's out of scope to add a CPU implementation for cuDF."
      },
      {
        "user": "naarkhoo",
        "created_at": "2021-04-01T21:39:49Z",
        "body": "I understand. The code is in Pandas and works perfectly on my laptop but extremely slow (given the data size). I need to translate it to CUDF - but as it is now, not possible to debug/troubleshoot easily. "
      },
      {
        "user": "kkraus14",
        "created_at": "2021-04-01T22:04:49Z",
        "body": "Closing this issue as the question has been answered."
      }
    ]
  },
  {
    "number": 7682,
    "title": "[QST] if possible & how to build the python package of cudf for K40 GPU (kepler) by myself?",
    "created_at": "2021-03-23T15:04:37Z",
    "closed_at": "2021-03-23T16:11:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/7682",
    "body": "I knew: cudf is required for Pascal architecture or better(compute capabilty>6.0). \r\n\r\nbut I only have a tesla K40m GPU (Kepler) in a normal PC (24GB + i5 CPU), which can run ubuntu 20.04 or 18.04. whatverer...\r\n\r\nMy budgt is very limited. I had 40GB data in mysql DB and have already run my python programs in win10. but the running speed became slower and slower. So it seem to a good start to use a GPU for the beter performance. I just bought a K40m GPU 2 weeks before. \r\n\r\nI met the problem that 'no kernel image is not available for the execution in the device' and read some posts about it.\r\n\r\nSo, my question is \r\nFor this GPU, can I build the special python package by myself by following the build instruction and the addtional make features like\r\n    \"nvcc -arch=sm_35 -gencode=arch=compute_35, code=sm_35\" when seting CUDA_NVCC_FLAGS\r\n    or some other features\r\n\r\nThanks in advance!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/7682/comments",
    "author": "helai78",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2021-03-23T15:15:21Z",
        "body": "@helai78 the problem is that we take advantage of some device features in libcudf that don't exist on architectures older than Pascal where trying to compile for something like the Kepler architecture won't work unfortunately."
      },
      {
        "user": "helai78",
        "created_at": "2021-03-23T15:30:09Z",
        "body": "Thank you for your immediate reply!\r\nI can understood that there is a dilemma between the better performace (the better device features) and the applicability.\r\nCould I just use the basic cudf features like:  \r\n1. io features: read_csv, read_sql, read_sql to_sql \r\n2. the basic floating points calculation capability \r\n3. some advance features: like sum, mean, and rolling\r\nwhile not using the device features in libcudf of Pascal?\r\n\r\nA little impovement of GPU utilization should be better for me than using CPU.\r\nDoes it make sense?  "
      },
      {
        "user": "kkraus14",
        "created_at": "2021-03-23T16:10:28Z",
        "body": "Specifically for aggregations in point 3, we use device atomics that are not available in architectures earlier than Pascal. I'm not sure about other features, but in development we prioritize building the most efficient implementations which often requires using newer features not supported in older architectures.\r\n\r\nYou're more than welcome to give building with Kepler architecture a shot, but we make no guarantees and won't provide support for doing so."
      }
    ]
  },
  {
    "number": 7596,
    "title": "[QST] How to Install/Build CuDf using PyArrow 3.0.0?",
    "created_at": "2021-03-15T02:18:24Z",
    "closed_at": "2021-03-26T21:04:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/7596",
    "body": "I'm trying to use the newer version of Apache Arrow (3.0.0), because some libraries depend on it (vaex 4.0.0). And it would allow sharing data between Cudf and Vaex, and possible more libraries.\r\n\r\nIs there any possible way to build it, or install it with arrow 3?\r\n\r\nThanks in advance",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/7596/comments",
    "author": "ziongh",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2021-03-26T21:04:12Z",
        "body": "Closing as duplicate of #7224"
      }
    ]
  },
  {
    "number": 7521,
    "title": "[FEA] enable iteration over cudf series",
    "created_at": "2021-03-05T20:20:43Z",
    "closed_at": "2021-03-05T23:27:26Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/7521",
    "body": "Hi cuDF developers,\r\n\r\n**Is your feature request related to a problem? Please describe.**\r\nI am not sure if this is a feature request or bug, but I would like to be able to iterate over a cuDF series. I am using cuDF 0.19.\r\n\r\nHere is a sample that demonstrates what I mean:\r\n\r\n```\r\nstephey@cgpu19:~$ python\r\nPython 3.8.5 (default, Sep  4 2020, 07:30:14) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import dask\r\n>>> import dask_cudf\r\n>>> import cudf\r\n>>> df = dask.datasets.timeseries()\r\n>>> df_id_cpu = df['id']\r\n>>> cpu_loop = [i for i in df_id_cpu]\r\n>>> df_gpu = df.map_partitions(cudf.from_pandas)\r\n>>> df_id_gpu = df_gpu['id']\r\n>>> gpu_loop = [i for i in df_id_gpu]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<stdin>\", line 1, in <listcomp>\r\n  File \"/app/miniconda3/lib/python3.8/site-packages/dask/dataframe/core.py\", line 3055, in __iter__\r\n    for row in s:\r\n  File \"/app/miniconda3/lib/python3.8/site-packages/cudf/core/series.py\", line 877, in __iter__\r\n    cudf.utils.utils.raise_iteration_error(obj=self)\r\n  File \"/app/miniconda3/lib/python3.8/site-packages/cudf/utils/utils.py\", line 388, in raise_iteration_error\r\n    raise TypeError(\r\nTypeError: Series object is not iterable. Consider using `.to_arrow()`, `.to_pandas()` or `.values_host` if you wish to iterate over the values.\r\n\r\n```\r\n\r\n**Describe the solution you'd like**\r\nI would like to be able to iterate over a cudf series the same way I can in pandas:\r\n\r\n```\r\n>>> df_gpu = df.map_partitions(cudf.from_pandas)\r\n>>> df_id_gpu = df_gpu['id']\r\n>>> gpu_loop = [i for i in df_id_gpu]\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nOne option is to convert from a cudf series to a cupy array, although this is much slower in my application than using `to_pandas()`.\r\n\r\n**Additional context**\r\nUsing `.to_pandas()` is a workable option, although I'm trying to keep all my work on the GPU if possible. \r\n\r\nPlease let me know if I can answer questions or provide any other information.\r\n\r\nThank you very much,\r\nLaurie\r\n\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/7521/comments",
    "author": "lastephey",
    "comments": [
      {
        "user": "galipremsagar",
        "created_at": "2021-03-05T22:08:51Z",
        "body": "> **Additional context**\r\n> Using `.to_pandas()` is a workable option, although I'm trying to keep all my work on the GPU if possible.\r\n\r\nIn your example above why do you want to be iterating over a GPU-backed series object? Also, we see that you are trying to convert a `Series` object to a list for which the best way to do it is via `to_pandas().tolist()`. The reason iterating over a GPU-backed series object is disabled is because it is quite slower when compared to iterating over a CPU-backed series object. \r\n\r\nMaybe it would be helpful for us to help you with your next operation if you can provide what you want to do with the iterable?"
      },
      {
        "user": "lastephey",
        "created_at": "2021-03-05T22:45:36Z",
        "body": "Thank you for the information-- this is helpful. Now I understand why iterating in this way would be slow/disabled, but then it might be nice to have a more informative error message. \r\n\r\nI'm trying to do something like:\r\n\r\n```\r\nimage = cp.zeros((10,10))\r\nfor i, j in zip(df_id_gpu, df_id_gpu):\r\n    image[i, j] += 1.0\r\n```\r\n\r\nwhere I use the values in the cudf series `df_id_gpu` to populate an image. \r\n\r\nWhat's the best strategy in this situation? Thank you. "
      },
      {
        "user": "galipremsagar",
        "created_at": "2021-03-05T23:00:48Z",
        "body": "You can achieve that operation by keeping all the data on GPU memory:\r\n\r\n```python\r\n>>> import cudf\r\n>>> import cupy as cp\r\n>>> df_id_gpu = cudf.Series([0, 1, 2, 3, 4, 5])\r\n>>> image = cp.zeros((10,10))\r\n>>> df_id_gpu\r\n0    0\r\n1    1\r\n2    2\r\n3    3\r\n4    4\r\n5    5\r\ndtype: int64\r\n>>> image\r\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\r\n>>> df_id_gpu.values\r\narray([0, 1, 2, 3, 4, 5])\r\n>>> image[df_id_gpu.values, df_id_gpu.values] += 1\r\n>>> image\r\narray([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\r\n```"
      },
      {
        "user": "lastephey",
        "created_at": "2021-03-05T23:27:26Z",
        "body": "Thank you very much for this solution-- this strategy is exactly what I wanted to know. \r\n\r\nI appreciate your time. I'll go ahead and close. "
      },
      {
        "user": "mohandeepzumen",
        "created_at": "2022-06-07T05:25:24Z",
        "body": "I'm doing a different operation where i have a spell check method and each of the word in the cell has to go through this spellCheck method. I'm not able to iterate through every cell. Tried doing .topandas().tolist(), but the performance remains the same. Is there a way i can iterate through each cell in a series?"
      },
      {
        "user": "shwina",
        "created_at": "2022-06-07T15:51:36Z",
        "body": "@mohandeepzumen can you show us what your `spellcheck` method looks like?"
      },
      {
        "user": "mohandeepzumen",
        "created_at": "2022-06-10T09:38:35Z",
        "body": "def spellCheck(word,model):\r\n    if word in model.words:\r\n        pass\r\n    else:\r\n        w = model.get_nearest_neighbors(word, k=1)[0][1]\r\n\r\nthe model here refers to a fasttext model trained on a corpus of text."
      }
    ]
  },
  {
    "number": 7481,
    "title": "[QST]problems with dask_cudf custom aggregation",
    "created_at": "2021-03-02T04:40:00Z",
    "closed_at": "2021-04-04T09:19:17Z",
    "labels": [
      "question",
      "Python",
      "dask"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/7481",
    "body": "**What is your question?**\r\nHi there,\r\n\r\nI'm trying to do a string-join aggregation in dask_cudf groupby dataframe. The input dataframe looks like below:\r\n`documents_categories.compute()`\r\n\r\ndocument_id | kv\r\n-- | --\r\n1595802 | 1611:0.92\r\n1595802 | 1610:0.07\r\n1524246 | 1807:0.92\r\n1524246 | 1608:0.07\r\n\r\n`documents_categories.dtypes`\r\n\r\n> document_id     int64\r\n> kv             object\r\n> dtype: object\r\n\r\nThe expected string-joined result should be:\r\ndocument_id | kv\r\n-- | --\r\n1595802 | 1611:0.92;1610:0.07\r\n1524246 | 1807:0.92;1608:0.07\r\n\r\nI have tried the following codes and other several methods, but still can't get this function running successfully. I'm not a expert in dask_cudf, any suggestions? Thanks!\r\n\r\n```\r\ncustom_join = dask.dataframe.Aggregation(\"custom_join\", lambda x: x.str.join(\";\"), lambda y: y.str.join(\";\"))\r\ndocuments_categories.groupby('document_id').agg({\"kv\": custom_join})\r\n```\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/utils.py in raise_on_meta_error(funcname, udf)\r\n    179     try:\r\n--> 180         yield\r\n    181     except Exception as e:\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py in _emulate(func, *args, **kwargs)\r\n   5315     with raise_on_meta_error(funcname(func), udf=kwargs.pop(\"udf\", False)):\r\n-> 5316         return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n   5317 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in _groupby_apply_funcs(df, *index, **kwargs)\r\n    920     for result_column, func, func_kwargs in funcs:\r\n--> 921         r = func(grouped, **func_kwargs)\r\n    922 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in _apply_func_to_column(df_like, column, func)\r\n    966 \r\n--> 967     return func(df_like[column])\r\n    968 \r\n\r\n<ipython-input-45-5dd27ef25785> in <lambda>(x)\r\n----> 1 custom_join = dask.dataframe.Aggregation(\"custom_join\", lambda x: x.str.join(\";\"), lambda y: y.str.join(\";\"))\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/groupby/groupby.py in __getattribute__(self, key)\r\n     62         try:\r\n---> 63             return super().__getattribute__(key)\r\n     64         except AttributeError:\r\n\r\nAttributeError: 'SeriesGroupBy' object has no attribute 'str'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-46-31b5ac92e045> in <module>\r\n----> 1 documents_categories.groupby('document_id').agg({\"kv\": custom_join})\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in agg(self, arg, split_every, split_out)\r\n   1846     @derived_from(pd.core.groupby.DataFrameGroupBy)\r\n   1847     def agg(self, arg, split_every=None, split_out=1):\r\n-> 1848         return self.aggregate(arg, split_every=split_every, split_out=split_out)\r\n   1849 \r\n   1850 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask_cudf/groupby.py in aggregate(self, arg, split_every, split_out)\r\n     81 \r\n     82         return super().aggregate(\r\n---> 83             arg, split_every=split_every, split_out=split_out\r\n     84         )\r\n     85 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in aggregate(self, arg, split_every, split_out)\r\n   1842             return self.size()\r\n   1843 \r\n-> 1844         return super().aggregate(arg, split_every=split_every, split_out=split_out)\r\n   1845 \r\n   1846     @derived_from(pd.core.groupby.DataFrameGroupBy)\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py in aggregate(self, arg, split_every, split_out)\r\n   1622             split_out=split_out,\r\n   1623             split_out_setup=split_out_on_index,\r\n-> 1624             sort=self.sort,\r\n   1625         )\r\n   1626 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py in apply_concat_apply(args, chunk, aggregate, combine, meta, token, chunk_kwargs, aggregate_kwargs, combine_kwargs, split_every, split_out, split_out_setup, split_out_setup_kwargs, sort, ignore_index, **kwargs)\r\n   5267 \r\n   5268     if meta is no_default:\r\n-> 5269         meta_chunk = _emulate(chunk, *args, udf=True, **chunk_kwargs)\r\n   5270         meta = _emulate(\r\n   5271             aggregate, _concat([meta_chunk], ignore_index), udf=True, **aggregate_kwargs\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py in _emulate(func, *args, **kwargs)\r\n   5314     \"\"\"\r\n   5315     with raise_on_meta_error(funcname(func), udf=kwargs.pop(\"udf\", False)):\r\n-> 5316         return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n   5317 \r\n   5318 \r\n\r\n/opt/conda/envs/rapids/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)\r\n    128                 value = type()\r\n    129             try:\r\n--> 130                 self.gen.throw(type, value, traceback)\r\n    131             except StopIteration as exc:\r\n    132                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/utils.py in raise_on_meta_error(funcname, udf)\r\n    199         )\r\n    200         msg = msg.format(\" in `{0}`\".format(funcname) if funcname else \"\", repr(e), tb)\r\n--> 201         raise ValueError(msg) from e\r\n    202 \r\n    203 \r\n\r\nValueError: Metadata inference failed in `_groupby_apply_funcs`.\r\n\r\nYou have supplied a custom function and Dask is unable to \r\ndetermine the type of output that that function returns. \r\n\r\nTo resolve this please provide a meta= keyword.\r\nThe docstring of the Dask function you ran should have more information.\r\n\r\nOriginal error is below:\r\n------------------------\r\nAttributeError(\"'SeriesGroupBy' object has no attribute 'str'\")\r\n\r\nTraceback:\r\n---------\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/utils.py\", line 180, in raise_on_meta_error\r\n    yield\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/core.py\", line 5316, in _emulate\r\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py\", line 921, in _groupby_apply_funcs\r\n    r = func(grouped, **func_kwargs)\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/groupby.py\", line 967, in _apply_func_to_column\r\n    return func(df_like[column])\r\n  File \"<ipython-input-45-5dd27ef25785>\", line 1, in <lambda>\r\n    custom_join = dask.dataframe.Aggregation(\"custom_join\", lambda x: x.str.join(\";\"), lambda y: y.str.join(\";\"))\r\n  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/core/groupby/groupby.py\", line 63, in __getattribute__\r\n    return super().__getattribute__(key)\r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/7481/comments",
    "author": "yuanqingz",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-01T05:11:48Z",
        "body": "This issue has been labeled `inactive-30d` due to no recent activity in the past 30 days. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be labeled `inactive-90d` if there is no activity in the next 60 days."
      },
      {
        "user": "beckernick",
        "created_at": "2021-04-01T14:02:14Z",
        "body": "@cocorosiekz we've recently implemented collect list. It looks like it's not cleanly working with Dask (I'll file an issue), but perhaps the following would work for you?\r\n\r\n```python\r\nimport cudf\r\nimport dask_cudf\r\nfrom io import StringIO\r\n​\r\n​\r\ndata = \"\"\"document_id   kv\r\n1595802 1611:0.92\r\n1595802 1610:0.07\r\n1524246 1807:0.92\r\n1524246 1608:0.07\"\"\"\r\n​\r\ndf = cudf.read_csv(StringIO(data), sep=\"\\t\")\r\nddf = dask_cudf.from_cudf(df, 2)\r\n​\r\n​\r\ndef collect_list_agg(df):\r\n    return df.groupby(\"document_id\").agg({\"kv\": list})\r\n​\r\n# ensure every row of a given key is in the same partition\r\npartitioned = ddf.shuffle(on=[\"document_id\"])\r\n​\r\n# run a within-partition cudf groupby collect list\r\nprint(partitioned.map_partitions(collect_list_agg).compute())\r\n                                 kv\r\ndocument_id                        \r\n1595802      [1611:0.92, 1610:0.07]\r\n1524246      [1807:0.92, 1608:0.07]\r\n```\r\n\r\n\r\n\r\n"
      },
      {
        "user": "yuanqingz",
        "created_at": "2021-04-04T09:19:13Z",
        "body": "Thanks @beckernick ! The shuffle-then-map-partitions way works for me. But it would be great to use groupby-collect-list to solve this. I think we can close this issue."
      }
    ]
  },
  {
    "number": 7281,
    "title": "[QST] Is it better to use shared memory in regex engine implementation?",
    "created_at": "2021-02-02T16:16:49Z",
    "closed_at": "2021-02-09T23:13:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/7281",
    "body": "Hi,\r\n\r\nI appreciate a lot your effort in releasing the RAPIDS suite. I have a question about the performance choice when you implement the regex engine during my reading the source code. It seems the current implementation uses thread-local memory and does not use shared memory during processing regular expression. Besides, some other APIs implementation that uses thrust to parallelize or transform does not use shared memory, either. Does that mean thread-local memory performance is close to shared memory in latest microarchitectures?\r\n\r\nThank you.\r\n\r\nBest Regards,\r\nKun",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/7281/comments",
    "author": "K-Wu",
    "comments": [
      {
        "user": "harrism",
        "created_at": "2021-02-09T01:48:13Z",
        "body": "@davidwendt have you thought about this?"
      },
      {
        "user": "davidwendt",
        "created_at": "2021-02-09T11:15:53Z",
        "body": "I've been thinking about it. There are a few things I certainly like to try. But I want to create some strings benchmarks #5698 before trying to experiment with performance improvements. The next one on my list does include an API that uses regex.\r\n\r\nSo I don't really have an answer here right now."
      },
      {
        "user": "harrism",
        "created_at": "2021-02-09T23:13:48Z",
        "body": "@K-Wu I'm going to close this since it's a bit open-ended.  Our regex implementation can certainly be improved, and likely will in the future."
      },
      {
        "user": "K-Wu",
        "created_at": "2021-02-09T23:17:31Z",
        "body": "@harrism @davidwendt Sure. Thank you for the answers and examining this issue!"
      }
    ]
  },
  {
    "number": 7008,
    "title": "[QST] Can we improve performance of Parquet file scans with large string columns?",
    "created_at": "2020-12-15T01:14:01Z",
    "closed_at": "2021-03-23T10:18:53Z",
    "labels": [
      "question",
      "libcudf",
      "cuIO",
      "Performance"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/7008",
    "body": "**What is your question?**\r\n\r\nWe are seeing poor performance in Spark on GPU for Parquet file scans where the files contain columns of large strings. Performance is considerably slower than Spark running on CPU in this case.\r\n\r\nIn order to reproduce this issue in cuDF, I wrote a data generator and benchmark in Python and the source code is included below.\r\n\r\nTwo files are generated - `small-strings.parquet` and `large-strings.parquet`.\r\n\r\nBoth files have two columns where:\r\n\r\n- c0 is a 32 byte string\r\n- c1 is also a string but with different sizes between the two files (32 bytes vs 4480 bytes)\r\n\r\nThere files are generated with different row counts to ensure that the overall volume of data is the same (although the final file size does vary due to compression differences).\r\n\r\nI am seeing a 3x difference in file scan performance between these two files, with `large-strings.parquet` being slower.\r\n\r\ncuDF is ~35x faster than Pandas for the small-string case but \"only\" 1.4x faster for the large-string case\r\n\r\n## output from running datagen.py\r\n\r\n```\r\nGenerating 224000 rows x 4480 bytes; total size = 1003520000 (large-strings.parquet)\r\nGenerating 15680000 rows x 64 bytes; total size = 1003520000 (small-strings.parquet)\r\n```\r\n\r\n## file sizes\r\n\r\n```\r\n998472247 large-strings.parquet\r\n619507382 small-strings.parquet\r\n```\r\n\r\n## benchmark results\r\n\r\n```\r\nRead /tmp/small-strings.parquet in 2.294814109802246\r\nRead /tmp/small-strings.parquet in 0.26930928230285645\r\nRead /tmp/small-strings.parquet in 0.2762606143951416\r\nRead /tmp/small-strings.parquet in 0.2749598026275635\r\nRead /tmp/small-strings.parquet in 0.285076379776001\r\nRead /tmp/small-strings.parquet in 0.27698278427124023\r\n```\r\n\r\n```\r\nRead /tmp/large-strings.parquet in 2.949741840362549\r\nRead /tmp/large-strings.parquet in 0.9218177795410156\r\nRead /tmp/large-strings.parquet in 0.9389686584472656\r\nRead /tmp/large-strings.parquet in 0.9387798309326172\r\nRead /tmp/large-strings.parquet in 0.9365167617797852\r\nRead /tmp/large-strings.parquet in 0.9358391761779785\r\n```\r\n\r\n## datagen.py\r\n\r\n```python\r\nimport cudf as pd\r\nimport random\r\nimport string\r\n\r\n# filename = '/tmp/large-strings.parquet'\r\n# rows = 224000\r\n# str_len = 4448\r\n\r\nfilename = '/tmp/small-strings.parquet'\r\nrows = 224000 * 70\r\nstr_len = 32\r\n\r\nrow_size = 32 + str_len\r\ntotal_bytes = rows * row_size\r\n\r\nprint(\"Generating {} rows x {} bytes; total size = {}\".format(rows, row_size, total_bytes))\r\nc0 = []\r\nc1 = []\r\nfor i in range(0, rows):\r\n    str = \"{:32d}\".format(i)\r\n    random_string = ''.join(random.choices(string.ascii_letters, k=str_len))\r\n    c0.append(str)\r\n    c1.append(random_string)\r\n\r\nc0_series = pd.Series(c0)\r\nc1_series = pd.Series(c1)\r\ndf = pd.DataFrame({ 'c0': c0_series, 'c1': c1_series })\r\ndf.to_parquet(filename)\r\n```\r\n\r\n## bench.py\r\n\r\n```python\r\nimport cudf as pd\r\nimport time\r\n\r\n# filename = \"/tmp/small-strings.parquet\"\r\nfilename = \"/tmp/large-strings.parquet\"\r\n\r\nfor i in range(0, 6):\r\n    start = time.time()\r\n    df = pd.read_parquet(filename)\r\n    end = time.time()\r\n    print(\"Read {} in {}\".format(filename, end - start))\r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/7008/comments",
    "author": "andygrove",
    "comments": [
      {
        "user": "OlivierNV",
        "created_at": "2020-12-29T06:46:20Z",
        "body": "[yes]"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-02-16T20:20:00Z",
        "body": "This issue has been marked stale due to no recent activity in the past 30d. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be marked rotten if there is no activity in the next 60d."
      },
      {
        "user": "devavret",
        "created_at": "2021-03-12T21:17:16Z",
        "body": "Possibly be fixed by #7576 . Need to confirm if that conversion is indeed the bottleneck."
      },
      {
        "user": "devavret",
        "created_at": "2021-03-19T17:39:49Z",
        "body": "#7576 has been merged. @andygrove Can you check if the issue persists?"
      },
      {
        "user": "andygrove",
        "created_at": "2021-03-22T22:51:58Z",
        "body": "Thanks for the ping @devavret. I asked @viadea if he could verify the fix."
      },
      {
        "user": "viadea",
        "created_at": "2021-03-23T04:26:03Z",
        "body": "Used Andy's tool on 0.19-nightly cudf:\r\n```\r\n$  python bench.py\r\nRead /tmp/small-strings.parquet in 1.44537353515625\r\nRead /tmp/small-strings.parquet in 0.2352135181427002\r\nRead /tmp/small-strings.parquet in 0.2478954792022705\r\nRead /tmp/small-strings.parquet in 0.24788236618041992\r\nRead /tmp/small-strings.parquet in 0.24469709396362305\r\nRead /tmp/small-strings.parquet in 0.24921441078186035\r\nRead /tmp/large-strings.parquet in 0.23854398727416992\r\nRead /tmp/large-strings.parquet in 0.235579252243042\r\nRead /tmp/large-strings.parquet in 0.23622751235961914\r\nRead /tmp/large-strings.parquet in 0.2348644733428955\r\nRead /tmp/large-strings.parquet in 0.23695826530456543\r\nRead /tmp/large-strings.parquet in 0.2357475757598877\r\n```\r\n\r\nThey are the same now."
      },
      {
        "user": "devavret",
        "created_at": "2021-03-23T10:18:53Z",
        "body": "Closed by #7576 "
      }
    ]
  },
  {
    "number": 6941,
    "title": "[QST]",
    "created_at": "2020-12-08T15:14:11Z",
    "closed_at": "2020-12-09T22:58:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/6941",
    "body": "**What is your question?**\r\nPlease share the code for data analytics (correlations and regressions) and visualization (scatter and bar) for data frame in blazingsql",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/6941/comments",
    "author": "Elias254",
    "comments": [
      {
        "user": "taureandyernv",
        "created_at": "2020-12-08T17:49:24Z",
        "body": "Please go to app.blazingsql.com.  this will open one of their free GPU instances on jupyterhub.  Go to `Welcome_to_BlazingSQL_Notebooks/intro_notebooks/` and that has much of what you're looking for.  You can test and retest your ideas right there as well.  "
      },
      {
        "user": "Elias254",
        "created_at": "2020-12-12T08:57:03Z",
        "body": "> Please go to app.blazingsql.com. this will open one of their free GPU instances on jupyterhub. Go to `Welcome_to_BlazingSQL_Notebooks/intro_notebooks/` and that has much of what you're looking for. You can test and retest your ideas right there as well.\r\n\r\nThanks a lot"
      }
    ]
  },
  {
    "number": 6849,
    "title": "[QST] Arrow Usage with CuDF",
    "created_at": "2020-11-25T20:51:43Z",
    "closed_at": "2020-12-03T06:23:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/6849",
    "body": "Does CuDF support mapping data from a PyArrow table directly or does it work with ChunkArrays or Arrays in Arrow? \r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/6849/comments",
    "author": "vibhatha",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-12-03T06:23:51Z",
        "body": "Hi @vibhatha cuDF works entirely in GPU memory, so you can't memmap directly as you normally would with CPU memory. We follow the same memory layout as Apache Arrow with the exceptions of using a byte per boolean (as opposed to a bit for Arrow), and support smaller bitwidth decimals (currently decimal32 and decimal64).\r\n\r\nIn cuDF all columns use contiguous memory under the hood, so we don't directly support ChunkedArrays like Arrow has. Generally, we defer to an upstream framework like BlazingSQL, Dask, or Spark to use cuDF APIs and do this chunking automatically."
      },
      {
        "user": "vkhodygo",
        "created_at": "2024-09-12T13:08:24Z",
        "body": "@kkraus14 I've been meaning to ask this for quite some time: what stands behind the choice to use bytes to store bools? As someone who works with big data full of indicator columns switching to PyArrow backend in Pandas helped immensely with memory consumption. It also allows to move from larger than memory datasets to something manageable as well as to reduce the code complexity. "
      }
    ]
  },
  {
    "number": 6841,
    "title": "Showing cudf content or printing .head() of a cudf is extremely slow ",
    "created_at": "2020-11-24T09:04:11Z",
    "closed_at": "2021-01-11T23:36:09Z",
    "labels": [
      "invalid",
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/6841",
    "body": "**What is your question?**\r\nI am using rapids docker, and on jupyter notebook whenever I try to see content of a cudf, or the frist couple of rows, this takes a really long time. Possibly doesnt even execute?\r\n\r\nRegards,\r\nDavor",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/6841/comments",
    "author": "DKorman",
    "comments": [
      {
        "user": "galipremsagar",
        "created_at": "2020-11-24T14:46:25Z",
        "body": "Can you share the dataframe and code where you are seeing long execution times? "
      },
      {
        "user": "kkraus14",
        "created_at": "2021-01-11T23:36:09Z",
        "body": "No answer, closing."
      }
    ]
  },
  {
    "number": 6816,
    "title": "[QST] Should cudf support an incoming schema for parquet writing and possibly other formats",
    "created_at": "2020-11-20T03:08:56Z",
    "closed_at": "2021-03-19T14:28:46Z",
    "labels": [
      "question",
      "libcudf",
      "cuIO",
      "Spark"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/6816",
    "body": "**What is your question?**\r\nIt seems like there are options sneaking in to the writing for parquet such as int96 timestamps and decimal precision that indicate that the data alone is not enough to know how to write a parquet file. Cudf will probably never support all the types that are in parquet and has no real need. The software using cudf could have knowledge about the desired parquet schema that cudf has no real interest in knowing and know way of guessing properly. In those cases, it seems that it would be nice for cudf to have opinionated defaults, but support an incoming schema of how to write the data. If no schema is given, it writes data as it wants, but if the schema exists then cudf would attempt to write the data in this way. Obviously, there can be invalid schemas passed in, but I think it's completely fine to error if the schema isn't possible. Further, I think there are times when coercion is desired and times when it isn't. Something like converting decimals to floats on write could be desired or an error. I think the default should be error, but a boolean should exist in the write options to allow coercion of types where possible.\r\n\r\nWhat are the thoughts on something like this?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/6816/comments",
    "author": "hyperbolic2346",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-11-20T18:16:47Z",
        "body": "cc @vuule @devavret who have been looking at parquet pretty heavily and may have some opinions here"
      },
      {
        "user": "devavret",
        "created_at": "2020-11-20T18:40:43Z",
        "body": "Passing in a schema means you primarily want coercion. Apart from int96, are there other types that we want to write that don't have a cuDF equivalent? i.e. cannot be achieved with a `cuDF::cast` before writing. "
      },
      {
        "user": "hyperbolic2346",
        "created_at": "2020-11-20T19:14:23Z",
        "body": "I'll let @revans2 and @jlowe chime in wish specifics, but int96 and decimal precision come immediately to mind. I think they were talking about an issue with structs or lists as well."
      },
      {
        "user": "kkraus14",
        "created_at": "2020-11-20T19:26:15Z",
        "body": "I think being able to specify that a column is a `Map` column is likely another situation since libcudf current has no `Map` support."
      },
      {
        "user": "jlowe",
        "created_at": "2020-11-20T19:33:05Z",
        "body": "> int96 and decimal precision come immediately to mind. I think they were talking about an issue with structs or lists as well.\r\n\r\nSpark needs to be able to specify the precision of the decimal being written (we can't assume the max precision for DECIMAL64 is appropriate, for example).  And @kkraus14 is correct, Spark needs to specify that a Parquet map type is being written rather than a list-of-struct-of-pairs column as cudf sees it."
      },
      {
        "user": "revans2",
        "created_at": "2020-11-20T19:44:58Z",
        "body": "Yes, we also have a situation with storing the Binary type.  We don't support reading it yet, but we need to be able to say is this a list of bytes that is not nullable or is it binary."
      },
      {
        "user": "devavret",
        "created_at": "2020-11-20T20:23:52Z",
        "body": "> we also have a situation with storing the Binary type\r\n\r\nWhat's the cudf column equivalent of binary type? What do you use/plan to use to write this?"
      },
      {
        "user": "jlowe",
        "created_at": "2020-11-20T23:32:59Z",
        "body": "> What's the cudf column equivalent of binary type?\r\n\r\nLIST of non-nullable INT8 or UINT8.  It could technically be implemented as a STRING as well, since cudf strings don't rely on string terminator characters.\r\n\r\n> What do you use/plan to use to write this?\r\n\r\nlibcudf already supports a `byte_cast` function that returns a LIST of UINT8 which is one way these columns could be created.  Another case would be loading a column from Parquet that's encoded as a binary column and preserving that column's schema when written to another Parquet file.\r\n"
      },
      {
        "user": "hyperbolic2346",
        "created_at": "2020-11-30T22:48:58Z",
        "body": "I added a feature request for Apache Arrow's schema for parquet writing. This isn't the only possible approach, but content specific to that option should be routed there. This discussion is more an over-arching \"do we need this\" more than a \"how would we do this\", which the feature request covers."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-02-16T20:20:37Z",
        "body": "This issue has been marked stale due to no recent activity in the past 30d. Please close this issue if no further response or action is needed. Otherwise, please respond with a comment indicating any updates or changes to the original issue and/or confirm this issue still needs to be addressed. This issue will be marked rotten if there is no activity in the next 60d."
      },
      {
        "user": "devavret",
        "created_at": "2021-03-16T23:06:05Z",
        "body": "As per the meeting today, the input metadata added in #7461 satisfies most of the spark use cases. I'll add a closes tag to this in that PR. Feel free to edit #7461 to remove the tag if you disagree."
      }
    ]
  },
  {
    "number": 6422,
    "title": "[QST] cuDF way to do group by e.g. sum TD",
    "created_at": "2020-10-05T14:43:44Z",
    "closed_at": "2020-10-12T18:38:16Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/6422",
    "body": "Looking to do some speedy time series crunching. Is there a way to do a lagged summing to date on a time series in cuDF concisely? I have some stuff up and running in R but wanting to move it over to Python, specifically cuDF! \r\n\r\ne.g. \r\nperson A scores 1, 2, 3 on Mon, Tues, Wed\r\nthen\r\nperson B scores 4, 5, 6 on Mon, Tues, Wed\r\n\r\nThen using whatever function gives `TD_col` 0, 1, 2 for person A and 0, 4, 9",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/6422/comments",
    "author": "Lion-Mod",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-10-05T14:47:06Z",
        "body": "@Lion-Mod do you happen to have Pandas code of what you're looking to accomplish? I'm unclear on your description where example code would be very helpful."
      },
      {
        "user": "Lion-Mod",
        "created_at": "2020-10-05T15:58:27Z",
        "body": "Sorry @kkraus14, I realised I was way too unspecific/unclear.\r\n\r\nMy current idea.\r\n\r\n1. `Sort the cuDF dataframe by group then by datetime`\r\n(`a` is the group column, another column not shown `b` is to be summed to date)\r\n\r\n2. `s['cumsum'] = s.groupby(\"a\").cumsum()`\r\n\r\n3. `s[\"cumsum_TD\"] = s.groupby(\"a\")[\"cumsum\"].shift(1, fill_value = 0)`\r\n\r\nThis seems to work but was curious if there is something out there to make it work quicker/more concise code.\r\n\r\n\r\n"
      },
      {
        "user": "devavret",
        "created_at": "2020-10-12T18:38:16Z",
        "body": "What you're asking for then is groupby scan operations. This would be a duplicate of #1298"
      }
    ]
  },
  {
    "number": 6126,
    "title": "[QST] OOM of cuDF (much smaller than GPU memory)",
    "created_at": "2020-08-31T06:32:22Z",
    "closed_at": "2020-08-31T14:49:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/6126",
    "body": "**What is your question?**\r\n```df = cudf.read_csv('/mypath/train-noIndex.csv')```\r\nI am just reading 11.5 GB files into GPU memory. (V100 32GB). No other operations. \r\nHowever, it leads to OOM (11.5 GB is much smaller than 32 GB). Are there some settings with RMM that may help to solve it ?\r\nAnd why it happened?\r\n**Error:**\r\n```MemoryError: std::bad_alloc: CUDA error at: ../include/rmm/mr/device/cuda_memory_resource.hpp:68: cudaErrorMemoryAllocation out of memory```\r\n\r\n**Env:**\r\nDocker: ```rapidsai/rapidsai:0.15-cuda11.0-runtime-ubuntu18.04-py3.8```\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/6126/comments",
    "author": "PerkzZheng",
    "comments": [
      {
        "user": "jlowe",
        "created_at": "2020-08-31T14:31:50Z",
        "body": "Does the CSV file contain a lot of fields that are encoded much more efficiently in the text file than they would be once loaded into GPU memory?  For example, take the following CSV file snippet containing long and double values:\r\n```\r\nlongs,doubles,morelongs\r\n0,1,2\r\n3,4,5\r\n1,2,3\r\n0,0,0\r\n[...]\r\n```\r\n\r\nEach field in this input only takes 2 bytes to encode in the CSV file, but each will be loaded into the GPU as a 64-bit value (INT64 or FLOAT64).  That's a memory increase of 4x, which means an 11.5GB file would need much more than 32GB of data to hold the result, ignoring any overhead of the parsing operation itself.\r\n\r\nSpeaking of parsing overhead, I would expect the libcudf parser may need at least ~2x the memory during parsing as it holds both the input text data and the parse result output buffers simultaneously."
      },
      {
        "user": "PerkzZheng",
        "created_at": "2020-08-31T14:44:41Z",
        "body": "@jlowe Thanks for the reply! it really helped a lot, and I will try to read as text files. "
      }
    ]
  },
  {
    "number": 6070,
    "title": "[BUG] .str.stod() no longer works on a String column",
    "created_at": "2020-08-22T02:44:33Z",
    "closed_at": "2020-08-24T03:14:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/6070",
    "body": "**Describe the bug**\r\nI used `.stod()` to convert a string column to a decimal. However, this has stopped working on nightly 0.15.\r\n\r\n**Steps/Code to reproduce bug**\r\nMinimal example:\r\n\r\n```\r\ndf = cudf.DataFrame([['0.01'], ['0.02']], columns=['string_column'])\r\ndf['string_column'].str.stod()\r\n```\r\n\r\n**Expected behavior**\r\nA columns converted to decimal type.\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: Docker\r\n - Method of cuDF install: Docker\r\n   - If method of install is [Docker], provide `docker pull` & `docker run` commands used\r\n\r\n   - pull: `docker pull rapidsai/rapidsai-nightly:cuda10.2-runtime-ubuntu18.04-py3.7`\r\n   - run: \r\n\r\n```\r\ndocker run --gpus all -it -p 8888:8888 -p 8787:8787 -p 8786:8786 --name kdd_rapids \\\r\n\trapidsai/rapidsai-nightly:cuda10.2-runtime-ubuntu18.04-py3.7\r\n```\r\n\r\n**Environment details**\r\nPlease run and paste the output of the `cudf/print_env.sh` script here, to gather any other relevant environment details\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/6070/comments",
    "author": "drabastomek",
    "comments": [
      {
        "user": "galipremsagar",
        "created_at": "2020-08-22T17:55:17Z",
        "body": "This behavior was changed with integration of `nvstrings` into `cudf`. We have removed `stod` access via `.str` StringMethods, but instead you can attain the same type-cast by doing `.astype`:\r\n\r\n\r\n```python\r\n>>> import cudf\r\n>>> df = cudf.DataFrame([['0.01'], ['0.02']], columns=['string_column'])\r\n>>> df['string_column'].astype('float64')\r\n0    0.01\r\n1    0.02\r\nName: string_column, dtype: float64\r\n```\r\n\r\nLet us know if this helps?"
      },
      {
        "user": "argenisleon",
        "created_at": "2020-08-22T18:41:31Z",
        "body": "@drabastomek  maybe this could help\r\n```python\r\ndf = cudf.DataFrame([['0.01'], ['0.02']], columns=['string_column'])\r\ncudf.Series(cudf.core.column.string.str_cast.stod(df[\"string_column\"]._column))\r\n```"
      },
      {
        "user": "drabastomek",
        "created_at": "2020-08-23T01:27:03Z",
        "body": "Thanks all! I used the casting and it worked fine! I just had the code that was using `.stod()` before and couldn't find it anymore. Any plans on bring it back or the `.stod()` will no longer be included in strings functions?"
      },
      {
        "user": "galipremsagar",
        "created_at": "2020-08-24T02:56:14Z",
        "body": "`.stod` was not exposed via `.str.stod` as we recommend to use `astype` API because, though underlying we call the identical code-path(`stod`) but we have added additional validation if all the string values to be type-casted are capable/valid of being type-casted to float(in this case). "
      },
      {
        "user": "drabastomek",
        "created_at": "2020-08-24T03:14:12Z",
        "body": "Makes sense! Thanks! Closing this bug."
      }
    ]
  },
  {
    "number": 5848,
    "title": "[QST] Prewarm kernels / JIT",
    "created_at": "2020-08-04T22:30:17Z",
    "closed_at": "2020-08-27T18:48:34Z",
    "labels": [
      "question",
      "numba",
      "libcudf",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/5848",
    "body": "**What is your question?**\r\n\r\nIs there a recommended way to precompile cudf kernels, ideally on app start / module import, such as a decorator?\r\n\r\nThe root issue is we have many small methods, and as not every task triggers all of them, it's a bit tricky to make sure there are no surprise runtime compilation happening, which kills interactivity and trips all sorts of QoS alerts & timeouts.  We currently work around by writing a `def warm()` method in  RAPIDS-using module that calls various methods to trigger compilation. We were going to look at adding some sort of `@warm_rapids` decorator, but thought to check here first for any guidance on if/where/how.",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/5848/comments",
    "author": "lmeyerov",
    "comments": [
      {
        "user": "harrism",
        "created_at": "2020-08-05T06:03:23Z",
        "body": "We are investigating eliminating JIT for binary operations. If we do, then JIT will only be used for UDFs, which by their nature would require user-defined warming.\r\n\r\nIf there are other cases where you are experiencing poor JIT performance, please provide specifics."
      },
      {
        "user": "lmeyerov",
        "created_at": "2020-08-05T21:44:00Z",
        "body": "Binops + stdlib (`.merge()`, ...) + UDFs  are indeed top of mind, and in roughly that order, for JIT warming\r\n\r\nIf broadening scope to JIT / interpreter overhead in general, less clear to me is ~deforestation. In the fn `def(df, y, z): df.merge(..) + y + z`, I can imagine fusing partial `f(a, b, c) => a + b + c` , and potentially going even deeper with some of the more popular std ops like `merge`. Likewise, both for first run and for re-runs. (We're software, so heavy reuse.) I already filed a separate thread on async support, which might give breathing room here as well. \r\n\r\n\r\nEdit: Pragmatically... sounds like, \"For the foreseeable future, keep writing your own `warmup()`\". Should we do that just for UDFs, or for non-UDF use too?"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-08-05T22:01:04Z",
        "body": "There's no JITing happening in standard operations, so nothing to warm there. There's a few usages of cupy that have some JITing under the hood that we're already working to remove to something that isn't JIT based.\r\n\r\nBiggest offender right now is definitely binops (especially with scalars), which will move to an AST based implementation instead of the current JIT based approach in the future which will remove the JIT overhead.\r\n\r\nUDFs are a black box to us where you'll likely have to pre-warm yourself for the foreseeable future."
      },
      {
        "user": "kkraus14",
        "created_at": "2020-08-27T18:48:34Z",
        "body": "Closing as this has been answered. Additionally we have removed more usage of cupy in #5974 to further avoid some JITing."
      }
    ]
  },
  {
    "number": 5847,
    "title": "[QST] Setting include search path for jitify/nvrtc with cuDF built from source",
    "created_at": "2020-08-04T22:05:32Z",
    "closed_at": "2020-08-27T18:49:13Z",
    "labels": [
      "question",
      "libcudf"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/5847",
    "body": "**What is your question?**\r\n\r\nI'm trying to build cuDF from source. The build seems to have succeeded fine (I can import cudf; I can create and inspect dataframes). However, when I try to do any operation that requires JIT compiling a kernel, I run into compilation errors that seem to indicate that nvrtc/jitify can't find the libcudacxx (particularly simt) headers.\r\n\r\nRunning\r\n```python\r\nimport cudf\r\n\r\ns1 = cudf.Series([1,2,3,None,4])\r\ns2 = cudf.Series([1,2,3,None,4])\r\nprint(s1)\r\nprint(s1 + s2)\r\nprint(s1 * s2)\r\n```\r\nfails with\r\n```\r\n0       1\r\n1       2\r\n2       3\r\n3    null\r\n4       4\r\ndtype: int64\r\n---------------------------------------------------\r\n--- JIT compile log for  ---\r\n---------------------------------------------------\r\ncudf/wrappers/timestamps.hpp(35): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(35): error: expected a \";\"\r\n\r\ncudf/wrappers/timestamps.hpp(38): error: not a class or struct name\r\n\r\ncudf/wrappers/timestamps.hpp(64): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(64): error: too many arguments for class template \"cudf::detail::timestamp\"\r\n\r\ncudf/wrappers/timestamps.hpp(64): error: expected a \";\"\r\n\r\ncudf/wrappers/timestamps.hpp(69): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(69): error: too many arguments for class template \"cudf::detail::timestamp\"\r\n\r\ncudf/wrappers/timestamps.hpp(69): error: expected a \";\"\r\n\r\ncudf/wrappers/timestamps.hpp(74): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(74): error: too many arguments for class template \"cudf::detail::timestamp\"\r\n\r\ncudf/wrappers/timestamps.hpp(74): error: expected a \";\"\r\n\r\ncudf/wrappers/timestamps.hpp(79): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(79): error: too many arguments for class template \"cudf::detail::timestamp\"\r\n\r\ncudf/wrappers/timestamps.hpp(79): error: expected a \";\"\r\n\r\ncudf/wrappers/timestamps.hpp(84): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(84): error: too many arguments for class template \"cudf::detail::timestamp\"\r\n\r\ncudf/wrappers/timestamps.hpp(84): error: expected a \";\"\r\n\r\ncudf/wrappers/timestamps.hpp(86): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(87): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(88): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(89): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(90): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(118): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(118): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(118): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(119): error: class \"std::numeric_limits<cudf::timestamp_D>\" has already been defined\r\n\r\ncudf/wrappers/timestamps.hpp(119): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(119): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(119): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(120): error: class \"std::numeric_limits<cudf::timestamp_D>\" has already been defined\r\n\r\ncudf/wrappers/timestamps.hpp(120): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(120): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(120): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(121): error: class \"std::numeric_limits<cudf::timestamp_D>\" has already been defined\r\n\r\ncudf/wrappers/timestamps.hpp(121): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(121): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(121): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(122): error: class \"std::numeric_limits<cudf::timestamp_D>\" has already been defined\r\n\r\ncudf/wrappers/timestamps.hpp(122): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(122): error: name followed by \"::\" must be a class or namespace name\r\n\r\ncudf/wrappers/timestamps.hpp(122): error: name followed by \"::\" must be a class or namespace name\r\n\r\ntraits.h(9): error: name followed by \"::\" must be a class or namespace name\r\n\r\ntraits.h(9): error: the global scope has no \"value\"\r\n\r\ntraits.h(13): error: name followed by \"::\" must be a class or namespace name\r\n\r\ntraits.h(13): error: the global scope has no \"value\"\r\n\r\ntraits.h(22): error: name followed by \"::\" must be a class or namespace name\r\n\r\ntraits.h(22): error: expected a \";\"\r\n\r\noperation.h(7): error: name followed by \"::\" must be a class or namespace name\r\n\r\noperation.h(99): error: common_type is not a template\r\n\r\ntype_traits(9): error: class \"__jitify_type_traits_ns::enable_if<<error-constant>, void>\" has no member \"type\"\r\n          detected during instantiation of type \"__jitify_type_traits_ns::enable_if_t<<error-constant>, void>\" \r\noperation.h(99): here\r\n\r\noperation.h(108): error: common_type is not a template\r\n\r\ntype_traits(9): error: class \"__jitify_type_traits_ns::enable_if<false, void>\" has no member \"type\"\r\n          detected during instantiation of type \"__jitify_type_traits_ns::enable_if_t<false, void>\" \r\noperation.h(108): here\r\n\r\noperation.h(109): error: invalid redeclaration of member function template \"TypeOut Mod::operate<TypeOut,TypeLhs,TypeRhs,<unnamed>>(TypeLhs, TypeRhs)\"\r\n(100): here\r\n\r\noperation.h(116): error: common_type is not a template\r\n\r\noperation.h(117): error: invalid redeclaration of member function template \"TypeOut Mod::operate<TypeOut,TypeLhs,TypeRhs,<unnamed>>(TypeLhs, TypeRhs)\"\r\n(100): here\r\n\r\noperation.h(126): error: common_type is not a template\r\n\r\noperation.h(135): error: common_type is not a template\r\n\r\noperation.h(136): error: invalid redeclaration of member function template \"TypeOut RMod::operate<TypeOut,TypeLhs,TypeRhs,<unnamed>>(TypeLhs, TypeRhs)\"\r\n(127): here\r\n\r\noperation.h(143): error: common_type is not a template\r\n\r\noperation.h(144): error: invalid redeclaration of member function template \"TypeOut RMod::operate<TypeOut,TypeLhs,TypeRhs,<unnamed>>(TypeLhs, TypeRhs)\"\r\n(127): here\r\n\r\noperation.h(465): error: name followed by \"::\" must be a class or namespace name\r\n\r\noperation.h(465): error: too many arguments for variable template \"is_integral_v\"\r\n\r\noperation.h(465): error: expected an expression\r\n\r\noperation.h(478): error: name followed by \"::\" must be a class or namespace name\r\n\r\noperation.h(478): error: too many arguments for variable template \"is_integral_v\"\r\n\r\noperation.h(478): error: expected an expression\r\n\r\noperation.h(479): error: invalid redeclaration of member function template \"TypeOut PMod::operate<TypeOut,TypeLhs,TypeRhs,<unnamed>>(TypeLhs, TypeRhs)\"\r\n(466): here\r\n\r\n68 errors detected in this compilation.\r\n\r\n---------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"cudf_test.py\", line 6, in <module>\r\n    print(s1 + s2)\r\n  File \".../python3.6/cudf/core/series.py\", line 1022, in __add__\r\n    return self._binaryop(other, \"add\")\r\n  File \".../python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \".../python3.6/cudf/core/series.py\", line 1002, in _binaryop\r\n    outcol = lhs._column.binary_operator(fn, rhs, reflect=reflect)\r\n  File \".../python3.6/cudf/core/column/numerical.py\", line 95, in binary_operator\r\n    lhs=self, rhs=rhs, op=binop, out_dtype=out_dtype, reflect=reflect\r\n  File \".../python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \".../python3.6/cudf/core/column/numerical.py\", line 434, in _numeric_column_binop\r\n    out = libcudf.binaryop.binaryop(lhs, rhs, op, out_dtype)\r\n  File \"cudf/_lib/binaryop.pyx\", line 202, in cudf._lib.binaryop.binaryop\r\n  File \"cudf/_lib/binaryop.pyx\", line 106, in cudf._lib.binaryop.binaryop_v_v\r\nRuntimeError: Runtime compilation failed\r\n```\r\n\r\nstrace seems to indicate that nvrtc/jitify is only looking for simt/chrono.h in the current working directory and /usr/include. Is this expected behavior, or have I made a mistake during the build?\r\n\r\nIs there an environment variable that I can provide to change the search path, or some location in either cudf or jitify that I can patch to accommodate other search paths?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/5847/comments",
    "author": "aadamson",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-08-05T22:12:53Z",
        "body": "@aadamson How did you build / install libcudf from source? IIRC we vendor the `libcudacxx` / `libcxx` bits into the libcudf includes via cmake and install them alongside the libcudf includes and expect to find them there."
      },
      {
        "user": "kkraus14",
        "created_at": "2020-08-27T18:49:13Z",
        "body": "Closing as no response."
      }
    ]
  },
  {
    "number": 5830,
    "title": "install error[QST]",
    "created_at": "2020-08-03T12:26:41Z",
    "closed_at": "2020-08-06T05:05:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/5830",
    "body": "Dear developer,\r\nThanks for developing nice tool. I would like to install cudf. But when I tried to install cudf with conda, I got following error.\r\ncations were found to be incompatible with your CUDA driver:\r\n\r\n  - feature:/linux-64::__cuda==11.0=0\r\n  - feature:|@/linux-64::__cuda==11.0=0\r\n\r\nYour installed CUDA driver is: 11.0\r\n\r\nMy cuder driver version is 450 and nvidia-smi shows cuda version is 11.0. But I installed condatoolkit version 10.1.\r\nSo I think actual cuda version of my env is cuda10.1.\r\nAre there any way to install cudf without downgrading nvidia-drive version?\r\nAny comments a/o suggestions will be greatly appreciated.\r\nThanks in advance.\r\n\r\nTaka",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/5830/comments",
    "author": "iwatobipen",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-08-03T14:53:27Z",
        "body": "@iwatobipen those messages related to `__cuda` are a bug in conda and are typically innocuous. Any chance you could share the full output of your conda install/create command to help troubleshoot?"
      },
      {
        "user": "iwatobipen",
        "created_at": "2020-08-03T22:47:59Z",
        "body": "@kkraus14 Thanks for your prompt reply. Here is a full output when I tried to install cudf.\r\n\r\n\r\n$ conda install -c rapidsai cudf=0.13\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: - \r\nFound conflicts! Looking for incompatible packages.\r\nThis can take several minutes.  Press CTRL-C to abort.\r\nfailed                                                                                                                         \r\n\r\nUnsatisfiableError: The following specifications were found to be incompatible with each other:\r\n\r\nOutput in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your CUDA driver:\r\n\r\n  - feature:/linux-64::__cuda==11.0=0\r\n  - feature:|@/linux-64::__cuda==11.0=0\r\n\r\nYour installed CUDA driver is: 11.0\r\n\r\n\r\nAnd list of conda package which has 'cuda' in their name.\r\n\r\n\r\n$ conda list | grep cuda\r\ncudatoolkit               10.1.243             h6bb024c_0    nvidia\r\ncudatoolkit-dev           10.1.243             h516909a_3    conda-forge\r\ncudnn                     7.6.5                cuda10.1_0  \r\nopenmm                    7.4.2           py37_cuda101_rc_1    omnia\r\n\r\n\r\nThanks"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-08-03T23:57:53Z",
        "body": "Can you dump the full output of `conda list` here?\r\n\r\nDo you have a `.condarc` file that specifies other channels already? If so could you post your channels here as well?"
      },
      {
        "user": "iwatobipen",
        "created_at": "2020-08-04T00:38:16Z",
        "body": "Here is the full list of my env and I don't have a .condarc file now.\r\nThanks\r\n\r\n$ conda list\r\n# packages in environment at /home/iwatobipen/miniconda3/envs/chemoinfo:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main  \r\n_py-xgboost-mutex         2.0                       cpu_0  \r\nabsl-py                   0.9.0                    py37_0  \r\nalembic                   1.4.2                      py_0  \r\namberlite                 16.0                     pypi_0    pypi\r\nambertools                17.0                     pypi_0    pypi\r\nambit                     0.3                  h137fa24_1    psi4\r\nappdirs                   1.4.3            py37h28b3542_0  \r\nase                       3.19.2                   pypi_0    pypi\r\nasn1crypto                1.3.0                    py37_1  \r\nattrs                     19.3.0                     py_0  \r\nautograd                  1.3                        py_0    conda-forge\r\nautograd-gamma            0.4.1                      py_0    conda-forge\r\nbackcall                  0.2.0                      py_0  \r\nbcrypt                    3.1.7            py37h7b6447c_1  \r\nblack                     19.10b0                    py_0  \r\nblas                      1.0                         mkl  \r\nbleach                    3.1.5                      py_0  \r\nblosc                     1.19.0               hd408876_0  \r\nbokeh                     2.1.1                    py37_0  \r\nboost-cpp                 1.68.0            h11c811c_1000    conda-forge\r\nbrotlipy                  0.7.0           py37h7b6447c_1000  \r\nbzip2                     1.0.8                h7b6447c_0  \r\nca-certificates           2020.6.24                     0  \r\ncairo                     1.14.12              h8948797_3  \r\ncatch2                    2.11.2               hc9558a2_0    conda-forge\r\ncertifi                   2020.6.20                py37_0  \r\ncffi                      1.14.0           py37he30daa8_1  \r\nchardet                   3.0.4                 py37_1003  \r\nchemps2                   1.8.9                h8c3debe_0    psi4\r\nclang                     10.0.1          default_hde54327_0    conda-forge\r\nclang-tools               10.0.1          default_hde54327_0    conda-forge\r\nclangdev                  10.0.1          default_hde54327_0    conda-forge\r\nclangxx                   10.0.1          default_hde54327_0    conda-forge\r\nclick                     7.1.2                      py_0  \r\ncliff                     3.3.0                      py_0    conda-forge\r\ncloudpickle               1.5.0                      py_0  \r\ncmaes                     0.6.0              pyhbc3b93e_0    conda-forge\r\ncmd2                      0.9.22                   py37_0    conda-forge\r\ncolorama                  0.4.3                      py_0  \r\ncolorlog                  4.2.1                    py37_0  \r\nconfigparser              5.0.0                      py_0  \r\ncryptography              2.9.2            py37h1ba5d50_0  \r\ncudatoolkit               10.1.243             h6bb024c_0    nvidia\r\ncudatoolkit-dev           10.1.243             h516909a_3    conda-forge\r\ncudnn                     7.6.5                cuda10.1_0  \r\ncupy                      7.7.0            py37h0632833_0    conda-forge\r\ncurl                      7.69.1               hbc83047_0  \r\ncycler                    0.10.0                   py37_0  \r\ncython                    0.29.21          py37he6710b0_0  \r\ncytoolz                   0.10.1           py37h7b6447c_0  \r\ndask                      2.20.0                     py_0  \r\ndask-core                 2.20.0                     py_0  \r\ndatabricks-cli            0.9.1                      py_0    conda-forge\r\ndbus                      1.13.16              hb2f20db_0  \r\ndecorator                 4.4.2                      py_0  \r\ndeepdiff                  3.3.0                    py37_1    psi4\r\ndefusedxml                0.6.0                      py_0  \r\ndgl-cu101                 0.4.3.post2              pypi_0    pypi\r\ndgllife                   0.2.4                    pypi_0    pypi\r\ndistributed               2.20.0                   py37_0  \r\ndkh                       1.2                  h173d85e_2    psi4\r\ndocker-py                 4.2.2                    py37_0  \r\ndocker-pycreds            0.4.0                      py_0  \r\nentrypoints               0.3                      py37_0  \r\nexpat                     2.2.9                he6710b0_2  \r\nfastcache                 1.1.0            py37h7b6447c_0  \r\nfastrlock                 0.4              py37he6710b0_0  \r\nfftw3f                    3.3.4                         2    omnia\r\nflake8                    3.8.3                      py_0  \r\nflask                     1.1.2                      py_0  \r\nfontconfig                2.13.0               h9420a91_0  \r\nfpsim2                    0.2.3           py37_1_g29b1e09    efelix\r\nfreetype                  2.10.2               he06d7ca_0    conda-forge\r\nfsspec                    0.7.4                      py_0  \r\nfuture                    0.18.2                   py37_1  \r\ngau2grid                  1.3.1                h035aef0_0    psi4\r\ngdma                      2.2.6                h0e1e685_6    psi4\r\ngitdb                     4.0.5                      py_0  \r\ngitpython                 3.1.3                      py_1  \r\nglib                      2.65.0               h3eb4bd4_0  \r\ngoogledrivedownloader     0.4                      pypi_0    pypi\r\ngorilla                   0.3.0                      py_0    conda-forge\r\ngst-plugins-base          1.14.0               hbbd80ab_1  \r\ngstreamer                 1.14.0               hb31296c_0  \r\ngunicorn                  20.0.4                   py37_0  \r\nh5py                      2.10.0                   pypi_0    pypi\r\nhdf4                      4.2.13               h3ca952b_2  \r\nhdf5                      1.10.2               hba1933b_1  \r\nheapdict                  1.0.1                      py_0  \r\nhyperopt                  0.2.4                    pypi_0    pypi\r\nicu                       58.2                 he6710b0_3  \r\nidna                      2.10                       py_0  \r\nimportlib-metadata        1.7.0                    py37_0  \r\nimportlib_metadata        1.7.0                         0  \r\nintel-openmp              2020.1                      217  \r\nipykernel                 5.3.4            py37h5ca1d4c_0  \r\nipython                   7.16.1           py37h5ca1d4c_0  \r\nipython_genutils          0.2.0                    py37_0  \r\nipywidgets                7.5.1                      py_0  \r\nisodate                   0.6.0                    pypi_0    pypi\r\nisort                     5.0.9                    py37_0  \r\nitsdangerous              1.1.0                    py37_0  \r\njedi                      0.17.1                   py37_0  \r\njinja2                    2.11.2                     py_0  \r\njoblib                    0.16.0                     py_0  \r\njpeg                      9b                   h024ee3a_2  \r\njsonpickle                1.4.1                      py_0  \r\njsonschema                3.2.0                    py37_1  \r\njupyter                   1.0.0                      py_2    conda-forge\r\njupyter_client            6.1.6                      py_0  \r\njupyter_console           6.1.0                      py_0  \r\njupyter_core              4.6.3                    py37_0  \r\nkiwisolver                1.2.0            py37hfd86e86_0  \r\nkrb5                      1.17.1               h173b8e3_0  \r\nlcms2                     2.11                 h396b838_0  \r\nld_impl_linux-64          2.33.1               h53a641e_7  \r\nlibboost                  1.67.0               h46d08c1_4  \r\nlibclang                  10.0.1          default_hde54327_0    conda-forge\r\nlibclang-cpp              10.0.1          default_hde54327_0    conda-forge\r\nlibclang-cpp10            10.0.1          default_hde54327_0    conda-forge\r\nlibcurl                   7.69.1               h20c2e04_0  \r\nlibedit                   3.1.20191231         h14c3975_1  \r\nlibffi                    3.3                  he6710b0_2  \r\nlibgcc-ng                 9.1.0                hdf63c60_0  \r\nlibgfortran-ng            7.3.0                hdf63c60_0  \r\nlibiconv                  1.15              h516909a_1006    conda-forge\r\nlibint                    1.2.1                hb4a4fd4_6    psi4\r\nlibllvm10                 10.0.1               he513fc3_0    conda-forge\r\nlibnetcdf                 4.4.1.1             hfc65e7b_11    conda-forge\r\nlibpng                    1.6.37               hed695b0_1    conda-forge\r\nlibpq                     12.2                 h20c2e04_0  \r\nlibprotobuf               3.12.3               hd408876_0  \r\nlibsodium                 1.0.18               h7b6447c_0  \r\nlibssh2                   1.9.0                h1ba5d50_1  \r\nlibstdcxx-ng              9.1.0                hdf63c60_0  \r\nlibtiff                   4.1.0                h2733197_1  \r\nlibuuid                   1.0.3                h1bed415_2  \r\nlibxc                     4.3.4                h7b6447c_0    psi4\r\nlibxcb                    1.14                 h7b6447c_0  \r\nlibxgboost                1.1.1                he1b5a44_0    conda-forge\r\nlibxml2                   2.9.10               he19cac6_1  \r\nlibxslt                   1.1.34               hc22bd24_0  \r\nlifelines                 0.25.0                     py_0    conda-forge\r\nlightgbm                  2.3.0            py37he6710b0_0  \r\nllvm-tools                10.0.1               he513fc3_0    conda-forge\r\nllvmdev                   10.0.1               he513fc3_0    conda-forge\r\nllvmlite                  0.33.0                   pypi_0    pypi\r\nlocket                    0.2.0                    py37_1  \r\nlz4-c                     1.9.2                he6710b0_1  \r\nlzo                       2.10                 h7b6447c_2  \r\nmako                      1.1.3                      py_0  \r\nmarkupsafe                1.1.1            py37h14c3975_1  \r\nmatplotlib                3.3.0                         1    conda-forge\r\nmatplotlib-base           3.3.0            py37hd478181_1    conda-forge\r\nmccabe                    0.6.1                    py37_1  \r\nmesalib                   18.3.1               h590aaf7_0    conda-forge\r\nmistune                   0.8.4           py37h14c3975_1001  \r\nmkl                       2020.1                      217  \r\nmkl-service               2.3.0            py37he904b0f_0  \r\nmkl_fft                   1.1.0            py37h23d657b_0  \r\nmkl_random                1.1.1            py37h0573a6f_0  \r\nml-metrics                0.1.4                    pypi_0    pypi\r\nmlflow                    1.2.0                      py_1    conda-forge\r\nmmpbsa-py                 16.0                     pypi_0    pypi\r\nmongodb                   4.0.3                h597af5e_0  \r\nmongoengine               0.20.0           py37hc8dfbb8_2    conda-forge\r\nmore-itertools            8.4.0                      py_0  \r\nmsgpack-c                 3.2.0                hc5b1762_0    conda-forge\r\nmsgpack-python            1.0.0            py37hfd86e86_1  \r\nmypy_extensions           0.4.3                    py37_0  \r\nnbconvert                 5.6.1                    py37_1  \r\nnbformat                  5.0.7                      py_0  \r\nnccl                      2.7.8.1              h51cf6c1_0    conda-forge\r\nncurses                   6.2                  he6710b0_1  \r\nnetworkx                  2.4                        py_1  \r\nngboost                   0.2.1              pyh9f0ad1d_0    conda-forge\r\nnotebook                  6.0.3                    py37_0  \r\nnumba                     0.50.1                   pypi_0    pypi\r\nnumexpr                   2.7.1            py37h423224d_0  \r\nnumpy                     1.19.1           py37hbc911f0_0  \r\nnumpy-base                1.19.1           py37hfa32c7d_0  \r\nolefile                   0.46                     py37_0  \r\nopenforcefield            0.7.1+45.g6426b42a          pypi_0    pypi\r\nopenforcefields           1.2.0                    py37_0    omnia\r\nopenmm                    7.4.2           py37_cuda101_rc_1    omnia\r\nopenssl                   1.1.1g               h7b6447c_0  \r\nopenvr                    1.0.17               h6bb024c_1    schrodinger\r\nopt-einsum                3.0.0                      py_0    conda-forge\r\noptuna                    2.0.0                      py_0    conda-forge\r\npackaging                 20.4                       py_0  \r\npackmol-memgen            1.0.5rc0                 pypi_0    pypi\r\npandas                    1.0.5            py37h0573a6f_0  \r\npandoc                    2.10                          0  \r\npandocfilters             1.4.2                    py37_1  \r\nparmed                    3.2.0                    pypi_0    pypi\r\nparso                     0.7.0                      py_0  \r\npartd                     1.1.0                      py_0  \r\npathspec                  0.7.0                      py_0  \r\npatsy                     0.5.1                    py37_0  \r\npbr                       5.4.5                      py_0  \r\npcmsolver                 1.2.1            py37h142c950_0    psi4\r\npcre                      8.44                 he6710b0_0  \r\npdb4amber                 1.7.dev0                 pypi_0    pypi\r\npexpect                   4.8.0                    py37_1  \r\npickleshare               0.7.5                 py37_1001  \r\npillow                    7.2.0            py37hb39fc2d_0  \r\npint                      0.10                       py_0    psi4\r\npip                       20.1.1                   py37_1  \r\npixman                    0.40.0               h7b6447c_0  \r\nplotly                    4.8.2                      py_0  \r\npluggy                    0.13.1                   py37_0  \r\npmw                       2.0.1           py37hc8dfbb8_1002    conda-forge\r\npostgresql                12.2                 h20c2e04_0  \r\nprettytable               0.7.2                      py_3    conda-forge\r\nprometheus_client         0.8.0                      py_0  \r\nprompt-toolkit            3.0.5                      py_0  \r\nprompt_toolkit            3.0.5                         0  \r\nprotobuf                  3.12.3           py37he6710b0_0  \r\npsi4                      1.3.2+ecbda83    py37h31b3128_0    psi4\r\npsutil                    5.7.0            py37h7b6447c_0  \r\npsycopg2                  2.8.5            py37hb09aad4_1    conda-forge\r\nptyprocess                0.6.0                    py37_0  \r\npy                        1.9.0                      py_0  \r\npy-boost                  1.67.0           py37h04863e7_4  \r\npy-cpuinfo                7.0.0                      py_0  \r\npy-xgboost                1.1.1            py37hc8dfbb8_0    conda-forge\r\npy3dmol                   0.8.0                      py_0    conda-forge\r\npychembldb                0.4.1                     dev_0    <develop>\r\npycodestyle               2.6.0                      py_0  \r\npycparser                 2.20                       py_2  \r\npydantic                  1.5.1            py37h7b6447c_0  \r\npyflakes                  2.2.0                      py_0  \r\npygments                  2.6.1                      py_0  \r\npymol                     2.5.0a0                  pypi_0    pypi\r\npymongo                   3.9.0            py37he6710b0_0  \r\npyopenssl                 19.1.0                     py_1  \r\npyparsing                 2.4.7                      py_0  \r\npyperclip                 1.8.0              pyh9f0ad1d_0    conda-forge\r\npyqt                      5.9.2            py37h05f1152_2  \r\npyrsistent                0.16.0           py37h7b6447c_0  \r\npyside2                   5.9.0a1          py37h4dc837a_0    conda-forge\r\npysocks                   1.7.1                    py37_1  \r\npytables                  3.4.4            py37ha205bf6_0  \r\npytest                    5.4.3                    py37_0  \r\npython                    3.7.7                hcff3b4d_5  \r\npython-dateutil           2.8.1                      py_0  \r\npython-editor             1.0.4                      py_0  \r\npython_abi                3.7                     1_cp37m    conda-forge\r\npytraj                    2.0.5                    pypi_0    pypi\r\npytz                      2020.1                     py_0  \r\npyyaml                    5.3.1            py37h7b6447c_1  \r\npyzmq                     19.0.1           py37he6710b0_1  \r\nqcelemental               0.4.2                      py_0    psi4\r\nqcengine                  0.8.2                      py_0    conda-forge\r\nqcfractal                 0.7.2                      py_0    conda-forge\r\nqcportal                  0.7.2                      py_0    conda-forge\r\nqt                        5.9.7                h5867ecd_1  \r\nqtconsole                 4.7.5                      py_0  \r\nqtpy                      1.9.0                      py_0  \r\nquerystring_parser        1.2.4                      py_0    conda-forge\r\nrazi                      0.0.0                    pypi_0    pypi\r\nrdflib                    5.0.0                    pypi_0    pypi\r\nrdkit                     2020.03.3.0      py37hc20afe1_1    rdkit\r\nrdkit-postgresql          2020.03.3.0          h8ea0133_0    rdkit\r\nreadline                  8.0                  h7b6447c_0  \r\nregex                     2020.6.8         py37h7b6447c_0  \r\nrequests                  2.24.0                     py_0  \r\nresp                      0.8.1              pyha93d1a2_0    psi4\r\nretrying                  1.3.3                    py37_2  \r\nsander                    16.0                     pypi_0    pypi\r\nscikit-learn              0.23.1           py37h423224d_0  \r\nscipy                     1.5.0            py37h0b6359f_0  \r\nseaborn                   0.10.1                        1    conda-forge\r\nseaborn-base              0.10.1                     py_1    conda-forge\r\nsend2trash                1.5.0                    py37_0  \r\nsetuptools                49.2.0                   py37_0  \r\nsimint                    0.7                  h642920c_1    psi4\r\nsimplejson                3.17.0           py37h7b6447c_0  \r\nsip                       4.19.8           py37hf484d3e_0  \r\nsix                       1.15.0                     py_0  \r\nsmmap                     3.0.2                      py_0  \r\nsnappy                    1.1.8                he6710b0_0  \r\nsortedcontainers          2.2.2                      py_0  \r\nsqlalchemy                1.3.18           py37h8f50634_0    conda-forge\r\nsqlite                    3.32.3               h62c20be_0  \r\nsqlparse                  0.3.1                      py_0  \r\nstatsmodels               0.11.1           py37h7b6447c_0  \r\nstevedore                 3.2.0            py37hc8dfbb8_0    conda-forge\r\ntabulate                  0.8.3                    py37_0  \r\ntblib                     1.6.0                      py_0  \r\nterminado                 0.8.3                    py37_0  \r\ntestpath                  0.4.4                      py_0  \r\nthreadpoolctl             2.1.0              pyh5ca1d4c_0  \r\ntk                        8.6.10               hbc83047_0  \r\ntoml                      0.10.1                     py_0  \r\ntoolz                     0.10.0                     py_0  \r\ntorch                     1.5.0+cu101              pypi_0    pypi\r\ntorch-cluster             1.5.6                    pypi_0    pypi\r\ntorch-geometric           1.6.0                    pypi_0    pypi\r\ntorch-scatter             2.0.5                    pypi_0    pypi\r\ntorch-sparse              0.6.6                    pypi_0    pypi\r\ntorch-spline-conv         1.2.0                    pypi_0    pypi\r\ntorchvision               0.6.0+cu101              pypi_0    pypi\r\ntornado                   6.0.4            py37h7b6447c_1  \r\ntqdm                      4.48.0                   pypi_0    pypi\r\ntraitlets                 4.3.3                    py37_0  \r\ntyped-ast                 1.4.1            py37h7b6447c_0  \r\ntyping_extensions         3.7.4.2                    py_0  \r\nurllib3                   1.25.9                     py_0  \r\nwcwidth                   0.2.5                      py_0  \r\nwebencodings              0.5.1                    py37_1  \r\nwebsocket-client          0.57.0                   py37_1  \r\nwerkzeug                  1.0.1                      py_0  \r\nwheel                     0.34.2                   py37_0  \r\nwidgetsnbextension        3.5.1                    py37_0  \r\nxfeat                     0.1.0                     dev_0    <develop>\r\nxgboost                   1.1.1            py37h3340039_0    conda-forge\r\nxz                        5.2.5                h7b6447c_0  \r\nyaml                      0.2.5                h7b6447c_0  \r\nzeromq                    4.3.2                he6710b0_2  \r\nzict                      2.0.0                      py_0  \r\nzipp                      3.1.0                      py_0  \r\nzlib                      1.2.11               h7b6447c_3  \r\nzstd                      1.4.5                h9ceee32_0  \r\n\r\n"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-08-04T01:13:48Z",
        "body": "Could you try changing your install command to `conda install -c rapidsai -c nvidia -c conda-forge -c defaults cudf=0.14`?\r\n\r\nFrom the looks of your environment what stands out most to me is you have `pandas 1.0.5` while v0.14 of RAPIDS requires `pandas 0.25.3` as well as having a bunch of pip packages installed which can play havoc on solving / finding dependencies properly.\r\n\r\nIf using a new environment is an option I'd strongly suggest taking that route and using: `conda create -c rapidsai -c nvidia -c conda-forge -c defaults cudf=0.14`"
      },
      {
        "user": "iwatobipen",
        "created_at": "2020-08-05T04:18:58Z",
        "body": "Sorry for my late reply. I tried to your suggested command but it didn't work. On the other side when I created a new environment, I could install cudf without any problems.\r\n\r\n"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-08-05T05:52:14Z",
        "body": "Got it. Looks like there's conflicts in your current environment and given the number of packages installed in it this isn't unexpected.\r\n\r\nDoes creating a new environment work for you or do you need it in this existing environment?"
      },
      {
        "user": "iwatobipen",
        "created_at": "2020-08-05T06:57:06Z",
        "body": "Hopefully, I would like to install cudf in the existing environment but I got following error (it was picked up cudf related).\r\nBut new env is acceptable if it is difficult to solve the conflict.\r\nPackage pyarrow conflicts for:\r\ncudf=0.14 -> pyarrow=0.15.0\r\nqcportal -> pyarrow[version='>=0.13.0']\r\nqcfractal -> qcfractal-core[version='>=0.13.1,<0.13.2.0a0'] -> pyarrow[version='>=0.13.0']\r\n\r\nThanks.\r\n"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-08-05T14:15:52Z",
        "body": "Is there any additional conflicts being shown? That looks solvable with pyarrow 0.15.\r\n"
      },
      {
        "user": "iwatobipen",
        "created_at": "2020-08-06T05:05:03Z",
        "body": "Hi, I tried to remove some packages and install cudf in my env and found that new version of rdkit  202003.03 and rdkit-postgresql cause the issue. When I uninstalled rdkit-postgresql and downgrade rdkit version to 2018, installation was succeeded. \r\nAnd I would like to close the issue.\r\nThank you for taking your time.\r\n"
      },
      {
        "user": "aniruddhakal",
        "created_at": "2020-10-17T15:25:58Z",
        "body": "> Could you try changing your install command to `conda install -c rapidsai -c nvidia -c conda-forge -c defaults cudf=0.14`?\r\n> \r\n> From the looks of your environment what stands out most to me is you have `pandas 1.0.5` while v0.14 of RAPIDS requires `pandas 0.25.3` as well as having a bunch of pip packages installed which can play havoc on solving / finding dependencies properly.\r\n> \r\n> If using a new environment is an option I'd strongly suggest taking that route and using: `conda create -c rapidsai -c nvidia -c conda-forge -c defaults cudf=0.14`\r\n\r\nworked like a charm for me,\r\nThanks @kkraus14 "
      }
    ]
  },
  {
    "number": 5798,
    "title": "[QST] Choose specific GPU to use with Dask-cuDF",
    "created_at": "2020-07-29T00:36:18Z",
    "closed_at": "2020-07-29T13:37:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/5798",
    "body": "In a machine with multiple GPUs is it possible to select which GPU to use for Dask-cuDF? Thank you!\r\n\r\nRelated question: #3428 (for cuDF)",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/5798/comments",
    "author": "jmkim",
    "comments": [
      {
        "user": "beckernick",
        "created_at": "2020-07-29T13:37:34Z",
        "body": "Yes! The answer in your linked issue also applies to Dask-cuDF.\r\n\r\nIf you're using a LocalCUDACluster, you can pass `CUDA_VISIBLE_DEVICES=...` to set specific GPUs. If you're using the command line to launch dask-cuda-workers, you can set `CUDA_VISIBLE_DEVICES` as an environment variable to dictate which GPUs get workers.\r\n\r\nClosing this for now, but please feel free to open again if you have any more questions regarding how to do this."
      }
    ]
  },
  {
    "number": 5711,
    "title": "[QST] Is there dask.compute() equivalent function for dask_cudf dataframes?",
    "created_at": "2020-07-17T00:24:20Z",
    "closed_at": "2020-07-20T21:42:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/5711",
    "body": "**What is your question?**\r\nI am wondering if there is a function to which I can pass a dictionary with values as `dask_cudf` dataframes and it can compute and return those. I believe you can do that for `dask` dataframes when passed it to `dask.compute()`.  \r\n\r\nDo let me know if there is some understanding flaw here. \r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/5711/comments",
    "author": "think-high",
    "comments": [
      {
        "user": "Salonijain27",
        "created_at": "2020-07-17T22:30:04Z",
        "body": "`dask_cudf` has a `.compute()` function.\r\n\r\n```\r\nimport dask_cudf\r\nimport cudf\r\n​\r\ndf = cudf.datasets.randomdata(10)\r\nddf = dask_cudf.from_cudf(df, 2)\r\nprint(\" ddf : \\n\", ddf.compute())\r\nprint(\" ddf.sum() : \\n\", ddf.sum().compute())\r\n```\r\n"
      },
      {
        "user": "think-high",
        "created_at": "2020-07-20T20:36:37Z",
        "body": "Thanks, I was thinking something like `dask.compute()` instead of something like `dask.DataFrame.compute()` so that I can pass on multiple-dataframes to be computed through that method. But I figured out that actually `dask.compute()` can be used with `dask_cudf`dataframes as well so I am sorted now. \r\n\r\nThanks for the help!! Appreciate it 👍  :) :) "
      }
    ]
  },
  {
    "number": 5638,
    "title": "[QST] performance CPU VS GPU on String, DateTime, Bool  data type",
    "created_at": "2020-07-04T16:10:28Z",
    "closed_at": "2020-07-20T01:24:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/5638",
    "body": "**What is your question?**\r\nIs there any example to show the performance  CPU vs GPU on String, DateTime, Bool data type on CUDF ? i would like to try them. \r\nThanks ",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/5638/comments",
    "author": "liuandy-good",
    "comments": [
      {
        "user": "harrism",
        "created_at": "2020-07-20T01:24:50Z",
        "body": "This is an extremely open-ended question. What operations do you want to perform on those data types, and what data sizes are you interested in? A better approach would be to take your desired workflow code, and make the changes necessary to use cuDF instead of Pandas (assuming your existing code is in Pandas).\r\n\r\nIf you have specific questions, please don't hesitate to reopen or file a new issue, but I'm going to close for now."
      }
    ]
  },
  {
    "number": 5631,
    "title": "[QST] Run Dask client process on a machine without a GPU",
    "created_at": "2020-07-02T23:20:29Z",
    "closed_at": "2020-07-03T02:05:57Z",
    "labels": [
      "question",
      "dask"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/5631",
    "body": "I currently has a Dask SSH cluster  where the scheduler is an IMAC (no nvidia GPU) and the workers are two machines with nvidia GPUS each. \r\nI would connect a DASK SSH cluster as follows:\r\n```\r\nfrom dask.distributed import Client, SSHCluster\r\ncluster = SSHCluster(\r\n        [\"localhost\", \"192.168.1.119\", \"192.168.1.191\"],\r\n        connect_options={\"known_hosts\": None,\"username\": \"vinhdiesal\"},\r\n        worker_options={\"nthreads\": 100},\r\n        scheduler_options={\"port\": 0, \"dashboard_address\": \":8797\"},\r\n        worker_module= 'dask_cuda.dask_cuda_worker'\r\n)\r\nclient = Client(cluster)\r\nclient\r\nfrom dask.distributed import Client, SSHCluster\r\ncluster = SSHCluster(\r\n        [\"localhost\", \"192.168.1.119\", \"192.168.1.191\"],\r\n        connect_options={\"known_hosts\": None,\"username\": \"vinhdiesal\"},\r\n        worker_options={\"nthreads\": 100},\r\n        scheduler_options={\"port\": 0, \"dashboard_address\": \":8797\"},\r\n        worker_module= 'dask_cuda.dask_cuda_worker'\r\n)\r\nclient = Client(cluster)\r\nclient\r\n```\r\nThen I try to import cudf, but it gets the following error: \r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-426582e71336> in <module>\r\n      2 import numpy as np\r\n      3 import pandas as pd\r\n----> 4 import cudf\r\n      5 import dask_cudf\r\n\r\n~/anaconda3/envs/dask-rapids/lib/python3.7/site-packages/cudf/__init__.py in <module>\r\n      3 from cudf.utils.gpu_utils import validate_setup  # isort:skip\r\n      4 \r\n----> 5 validate_setup(check_dask=False)\r\n      6 \r\n      7 import cupy\r\n\r\n~/anaconda3/envs/dask-rapids/lib/python3.7/site-packages/cudf/utils/gpu_utils.py in validate_setup(check_dask)\r\n      8         return\r\n      9 \r\n---> 10     from cudf._cuda.gpu import (\r\n     11         getDeviceCount,\r\n     12         driverGetVersion,\r\n\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n```\r\nWhen I import cudf on my worker machines its fine. \r\nIs my setup valid? Does downloading a cuda package on a non-nvidia machine make sense?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/5631/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-07-02T23:33:30Z",
        "body": "> Is my setup valid? Does downloading a cuda package on a non-nvidia machine make sense?\r\n\r\nCurrently the way that dask works is that the client machine and the scheduler both end up needing to run a small amount of code similar to the workers in order to understand the expected output metadata. In the case of cuDF this means that they need to be able to run code that requires a GPU as well as the driver and cuda toolkit."
      },
      {
        "user": "ghost",
        "created_at": "2020-07-03T02:05:57Z",
        "body": "Unfortunately my setup will work for Dask and dask-ml/dask-xgboost, but won't work with rapids. In order for it to work with the RAPIDS package, I changed my setup so instead of the scheduler being the imac, the schedule is the same machine as one of the workers with nvidia GPUS and it works. \r\nI can close. Thanks."
      }
    ]
  },
  {
    "number": 5445,
    "title": "[QST] Lots of package conflicts when installing cuDF on Ubuntu machine",
    "created_at": "2020-06-11T04:30:03Z",
    "closed_at": "2020-08-27T18:56:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/5445",
    "body": "I am trying to install the RAPIDS cuDF package on my Ubuntu machine using conda.\r\n\r\nRunning the command\r\n\r\n    conda install -c nvidia -c rapidsai -c numba -c conda-forge -c defaults cudf\r\n\r\nor\r\n\r\n    conda install -c rapidsai -c nvidia -c numba -c conda-forge cudf=0.13 python=3.7 cudatoolkit=10.2\r\n\r\nboth gives the following messages\r\n\r\n```\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: | \r\nFound conflicts! Looking for incompatible packages.\r\nThis can take several minutes.  Press CTRL-C to abort.\r\n```\r\n\r\nThis took a really long time, before attempting to estimate conflicts 172 packages \r\n\r\n\r\n> Examining conflict for dask-core pickleshare backcall python_abi osqp mkl_random cloudpickle cvxpy-base psutil libtiff fastparquet python-snappy pillow fsspec pexpect pytz et_xmlfile heapdict wcwidth qt traitlets mkl-service pyqt libxml2 setuptools numba cython beautifulsoup4 multiprocess cycler sip pyyaml fastcache fontconfig pyopenssl cudf cx_oracle prompt_toolkit gstreamer numexpr msgpack-python wrapt markupsafe llvmlite future sortedcontainers urllib3 numpy-base jedi distributed pycparser protobuf pytables dill hmmlearn mysql-connector-python locket chardet xz ipython_genutils joblib gst-plugins-base matplotlib sqlalchemy python jdcal xlrd mock click matplotlib-base peewee ptyprocess zstd openpyxl jinja2 ipython tblib decorator wheel nose pysocks cytoolz partd numpy scikit-learn pyparsing h5py tornado python-dateutil pygments ecos soupsieve cffi blosc parso ppft mkl_fft dask kiwisolver cvxopt patsy zict cryptography pytorch ninja pox typing_extensions pathos packaging multitasking statsmodels certifi bottleneck requests yfinance bokeh pymysql pip sExamining conflict for dask-core pickleshare backcall python_abi osqp mkl_random cloudpickle cvxpy-base psutil libtiff fastparquet python-snappy pillow fsspec pexpect pytz et_xmlfile heapdict wcwidth qt traitlets mkl-service pyqt libxml2 setuptools numba cython beautifulsoup4 multiprocess cycler sip pyyaml fastcache fontconfig pyopenssl cudf cx_oracle prompt_toolkit gstreamer numexpr msgpack-python wrapt markupsafe llvmlite future sortedcontainers urllib3 numpy-base jedi distributed pycparser protobuf pytables dill hmmlearn mysql-connector-python locket chardet xz ipython_genutils joblib gst-plugins-base matplotlib sqlalchemy python jdcal xlrd mock click matplotlib-base peewee ptyprocess zstd openpyxl jinja2 ipython tblib decorator wheel nose pysocks cytoolz partd numpy scikit-learn pyparsing h5py tornado python-dateutil pygments ecos soupsieve cffi blosc parso ppft mkl_fft dask kiwisolver cvxopt patsy zict cryptography pytorch ninja pox typing_extensions pathos packaging multitasking statsmodels certifi bottleneck requests yfinance bokeh pymysql pip sExamining conflict for dask-core pickleshare backcall python_abi osqp mkl_random cloudpickle cvxpy-base psutil libtiff libpng fastparquet python-snappy pillow freetype fsspec pexpect pytz et_xmlfile heapdict wcwidth qt glib traitlets mkl-service pyqt libxml2 setuptools numba cython beautifulsoup4 multiprocess cycler sip pyyaml fastcache fontconfig pyopenssl cudf cx_oracle prompt_toolkit gstreamer numexpr msgpack-python wrapt markupsafe llvmlite future sortedcontainers urllib3 numpy-base jedi distributed pycparser protobuf pytables dill hmmlearn mysql-connector-python locket chardet ipython_genutils joblib gst-plugins-base matplotlib sqlalchemy python jdcal xlrd mock click matplotlib-base peewee ptyprocess zstd openpyxl jinja2 ipython tblib decorator wheel nose pysocks cytoolz partd libprotobuf numpy scikit-learn pyparsing h5py tornado dbus python-dateutil pygments ecos soupsieve cffi blosc parso ppft mkl_fft zlib dask kiwisolver cvxopt hdf5 patsy zict cryptography pytorch ninja pox typing_extensions pathos packaging multitasking statsmodels certifi botExamining conflict for dask-core pickleshare backcall python_abi osqp mkl_random cloudpickle cvxpy-base psutil libtiff libpng fastparquet python-snappy pillow freetype fsspec pexpect pytz et_xmlfile heapdict wcwidth qt glib traitlets mkl-service pyqt libxml2 setuptools numba cython beautifulsoup4 multiprocess cycler sip pyyaml fastcache fontconfig pyopenssl cudf cx_oracle prompt_toolkit gstreamer numexpr msgpack-python wrapt markupsafe llvmlite future sortedcontainers urllib3 numpy-base jedi distributed pycparser protobuf pytables dill hmmlearn mysql-connector-python locket chardet ipython_genutils joblib gst-plugins-base matplotlib sqlalchemy python jdcal xlrd mock click matplotlib-base peewee ptyprocess zstd openpyxl jinja2 ipython tblib decorator wheel nose pysocks cytoolz partd libprotobuf numpy scikit-learn pyparsing h5py tornado dbus python-dateutil pygments ecos soupsieve cffi blosc parso ppft mkl_fft zlib dask kiwisolver cvxopt hdf5 patsy zict cryptography pytorch ninja pox typing_extensions pathos packaging multitasking statsmodels certifi botExamining conflict for dask-core pickleshare backcall python_abi osqp mkl_random cloudpickle cvxpy-base psutil fastparquet python-snappy pillow fsspec pexpect pytz et_xmlfile heapdict wcwidth qt traitlets mkl-service pyqt numba setuptools cython beautifulsoup4 multiprocess cycler sip pyyaml fastcache pyopenssl cudf cx_oracle prompt_toolkit numexpr msgpack-python wrapt markupsafe llvmlite future sortedcontainers urllib3 numpy-base jedi distributed pycparser protobuf pytables dill hmmlearn mysql-connector-python locket chardet ipython_genutils joblib matplotlib sqlalchemy python sqlite jdcal xlrd mock click matplotlib-base peewee ptyprocess openpyxl jinja2 ipython tblib decorator wheel nose pysocks cytoolz partd numpy scikit-learn pyparsing h5py tornado python-dateutil pygments ecos soupsieve cffi parso ppft mkl_fft dask kiwisolver cvxopt patsy zict cryptography pytorch ninja pox typing_extensions pathos packaging multitasking statsmodels certifi bottleneck requests yfinance bokeh pymysql pip scipy cvxpy arch scs toolz pandas idna six olefile thrift:   2Examining conflict for dask-core pickleshare backcall python_abi osqp mkl_random cloudpickle cvxpy-base psutil fastparquet python-snappy pillow fsspec pexpect pytz et_xmlfile heapdict wcwidth qt traitlets mkl-service pyqt numba setuptools cython beautifulsoup4 multiprocess cycler sip pyyaml fastcache pyopenssl cudf cx_oracle prompt_toolkit numexpr msgpack-python wrapt markupsafe llvmlite future sortedcontainers urllib3 numpy-base jedi distributed pycparser protobuf pytables dill hmmlearn mysql-connector-python locket chardet ipython_genutils joblib matplotlib sqlalchemy python sqlite jdcal xlrd mock click matplotlib-base peewee ptyprocess openpyxl jinja2 ipython tblib decorator wheel nose pysocks cytoolz partd numpy scikit-learn pyparsing h5py tornado python-dateutil pygments ecos soupsieve cffi parso ppft mkl_fft dask kiwisolver cvxopt patsy zict cryptography pytorch ninja pox typing_extensions pathos packaging multitasking statsmodels certifi bottleneck requests yfinance bokeh pymysql pip scipy cvxpy arch scs toolz pandas idna six olefile thrift:   2Examining conflict for dask-core pickleshare backcall python_abi osqp mkl_random cloudpickle cvxpy-base psutil fastparquet python-snappy pillow fsspec pexpect pytz et_xmlfile heapdict wcwidth qt traitlets mkl-service pyqt numba setuptools cython beautifulsoup4 multiprocess cycler sip pyyaml readline fastcache pyopenssl cudf cx_oracle prompt_toolkit numexpr msgpack-python wrapt markupsafe llvmlite future sortedcontainers urllib3 numpy-base jedi distributed pycparser protobuf pytables dill hmmlearn mysql-connector-python locket chardet ipython_genutils joblib matplotlib sqlalchemy python jdcal xlrd mock click matplotlib-base peewee ptyprocess openpyxl jinja2 ipython tblib decorator wheel nose pysocks cytoolz partd numpy scikit-learn pyparsing h5py tornado python-dateutil pygments ecos soupsieve cffi parso ppft mkl_fft dask kiwisolver cvxopt patsy zict cryptography pytorch ninja pox typing_extensions pathos packaging multitasking statsmodels certifi bottleneck requests yfinance bokeh pymysql pip scipy cvxpy arch scs toolz pandas idna six olefile thrift:  Examining conflict for dask-core pickleshare backcall python_abi osqp mkl_random cloudpickle cvxpy-base psutil fastparquet python-snappy pillow fsspec pexpect pytz et_xmlfile heapdict wcwidth qt traitlets mkl-service pyqt numba setuptools cython beautifulsoup4 multiprocess cycler sip pyyaml readline fastcache pyopenssl cudf cx_oracle prompt_toolkit numexpr msgpack-python wrapt markupsafe llvmlite future sortedcontainers urllib3 numpy-base jedi distributed pycparser protobuf pytables dill hmmlearn mysql-connector-python locket chardet ipython_genutils joblib matplotlib sqlalchemy python jdcal xlrd mock click matplotlib-base peewee ptyprocess openpyxl jinja2 ipython tblib decorator wheel nose pysocks cytoolz partd numpy scikit-learn pyparsing h5py tornado python-dateutil pygments ecos soupsieve cffi parso ppft mkl_fft dask kiwisolver cvxopt patsy zict cryptography pytorch ninja pox typing_extensions pathos packaging multitasking statsmodels certifi bottleneck requests yfinance bokeh pymysql pip scipy cvxpy arch scs toolz pandas idna six olefile thrift:  Examining conflict for dask-core pickleshare backcall python_abi openssl osqp mkl_random cloudpickle cvxpy-base psutil fastparquet python-snappy pillow fsspec pexpect pytz et_xmlfile heapdict wcwidth qt traitlets mkl-service pyqt numba setuptools cython beautifulsoup4 multiprocess cycler sip pyyaml fastcache pyopenssl cudf cx_oracle prompt_toolkit numexpr msgpack-python wrapt markupsafe llvmlite future sortedcontainers urllib3 numpy-base jedi distributed pycparser protobuf pytables dill hmmlearn mysql-connector-python locket chardet ipython_genutils joblib matplotlib sqlalchemy python jdcal xlrd mock click matplotlib-base peewee ptyprocess openpyxl jinja2 ipython tblib decorator wheel nose pysocks cytoolz partd numpy scikit-learn pyparsing h5py tornado python-dateutil pygments ecos soupsieve cffi parso ppft mkl_fft mysql-connector-c dask kiwisolver cvxopt patsy zict cryptography pytorch ninja pox typing_extensions pathos packaging multitasking statsmodels certifi bottleneck requests yfinance bokeh pymysql pip scipy cvxpy arch scs toolz pandas idna six Examining conflict for dask-core pickleshare backcall python_abi openssl osqp mkl_random cloudpickle cvxpy-base psutil fastparquet python-snappy pillow fsspec pexpect pytz et_xmlfile heapdict wcwidth qt traitlets mkl-service pyqt numba setuptools cython beautifulsoup4 multiprocess cycler sip pyyaml fastcache pyopenssl cudf cx_oracle prompt_toolkit numexpr msgpack-python wrapt markupsafe llvmlite future sortedcontainers urllib3 numpy-base jedi distributed pycparser protobuf pytables dill hmmlearn mysql-connector-python locket chardet ipython_genutils joblib matplotlib sqlalchemy python jdcal xlrd mock click matplotlib-base peewee ptyprocess openpyxl jinja2 ipython tblib decorator wheel nose pysocks cytoolz partd numpy scikit-learn pyparsing h5py tornado python-dateutil pygments ecos soupsieve cffi parso ppft mkl_fft mysql-connector-c dask kiwisolver cvxopt patsy zict cryptography pytorch ninja pox typing_extensions pathos packaging multitasking statsmodels certifi bottleneck requests yfinance bokeh pymysql pip scipy cvxpy arch scs toolz pandas idna six olefile thrift:\r\n\r\nand finding lots of conflicts, such as the following  \r\n\r\n```\r\nPackage typing_extensions conflicts for:\r\nanaconda/linux-64::bokeh==2.0.2=py37_0 -> typing_extensions[version='>=3.7.4']\r\ndefaults/noarch::dask==2.9.2=py_0 -> bokeh[version='>=1.0.0'] -> typing_extensions[version='>=3.7.4']\r\n\r\nPackage cryptography-vectors conflicts for:\r\ndefaults/linux-64::pymysql==0.9.3=py37_0 -> cryptography -> cryptography-vectors[version='2.3.*|2.3.1.*']\r\nanaconda/linux-64::urllib3==1.25.8=py37_0 -> cryptography[version='>=1.3.4'] -> cryptography-vectors[version='2.3.*|2.3.1.*']\r\n\r\nPackage hdf5 conflicts for:\r\nanaconda/linux-64::pytables==3.5.2=py37h71ec239_1 -> hdf5[version='>=1.10.4,<1.10.5.0a0']\r\nanaconda/linux-64::h5py==2.9.0=py37h7918eee_0 -> hdf5[version='>=1.10.4,<1.10.5.0a0']The following specifications were found to be incompatible with your CUDA driver:\r\n\r\n  - feature:/linux-64::__cuda==10.2=0\r\n  - feature:|@/linux-64::__cuda==10.2=0\r\n\r\nYour installed CUDA driver is: 10.2\r\n```\r\n\r\n`conda list cudf` shows that cuDF is still not installed. \r\n\r\nIs there a better `conda` command to run to install cuDF?\r\n\r\n**System Environment**\r\n\r\n- Nvidia driver 440.33.01 \r\n- CUDA Version 10.2.89\r\n- cudatoolkit 10.2.89 \r\n- Python 3.7.4\r\n- conda 4.8.3\r\n- Ubuntu 18.04\r\n- Nvidia 2080 Ti\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/5445/comments",
    "author": "athenawisdoms",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-06-11T04:51:15Z",
        "body": "@athenawisdoms could you dump the output of `conda list` here? Alternatively, if creating a new environment is an option I'd suggest taking that route and doing:\r\n\r\n```\r\nconda create --name rapids -c rapidsai -c nvidia -c conda-forge -c defaults python=3.7 cudatoolkit=10.2 cudf=0.14\r\n```\r\n\r\nNOTE: 0.14 just released so upgraded cudf to 0.14 in the command, but feel free to roll it back if needed."
      },
      {
        "user": "athenawisdoms",
        "created_at": "2020-06-11T14:24:19Z",
        "body": "@kkraus14 Thanks your suggestion to createa a new environment works!\r\n\r\nHowever, I'll still like to install cudf to my existing environment. Here's the `conda list` output as requested. Wonder if you can identify the problem...\r\n\r\n```\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main    conda-forge\r\narch                      4.9.1           np116py37h14c3975_0    bashtage\r\nbackcall                  0.1.0                    py37_0    anaconda\r\nbeautifulsoup4            4.8.2                    py37_0    anaconda\r\nblas                      1.0                         mkl  \r\nblosc                     1.16.3               hd408876_0    anaconda\r\nbokeh                     2.0.2                    py37_0    anaconda\r\nbottleneck                1.3.1            py37hdd07704_0    anaconda\r\nbzip2                     1.0.8                h7b6447c_0    anaconda\r\nca-certificates           2020.4.5.2           hecda079_0    conda-forge\r\ncertifi                   2020.4.5.2       py37hc8dfbb8_0    conda-forge\r\ncffi                      1.14.0           py37h2e261b9_0    anaconda\r\nchardet                   3.0.4                 py37_1003    anaconda\r\nclick                     7.0                      py37_0    anaconda\r\ncloudpickle               1.4.1                      py_0    anaconda\r\ncryptography              2.9.2            py37h1ba5d50_0    anaconda\r\ncudatoolkit               10.2.89              hfd86e86_1    anaconda\r\ncvxopt                    1.2.0            py37hfa32c7d_0    anaconda\r\ncvxpy                     1.0.24           py37he1b5a44_0    conda-forge\r\ncvxpy-base                1.0.24           py37he1b5a44_0    conda-forge\r\ncx_oracle                 7.0.0            py37h7b6447c_0    anaconda\r\ncycler                    0.10.0                   py37_0    anaconda\r\ncython                    0.29.13          py37he6710b0_0    anaconda\r\ncytoolz                   0.10.1           py37h7b6447c_0    anaconda\r\ndask                      2.9.2                      py_0  \r\ndask-core                 2.9.2                      py_0  \r\ndbus                      1.13.6               h746ee38_0    anaconda\r\ndecorator                 4.4.0                    py37_1    anaconda\r\ndill                      0.3.1.1                  py37_0    anaconda\r\ndistributed               2.16.0                   py37_0    anaconda\r\necos                      2.0.7           py37h3010b51_1000    conda-forge\r\net_xmlfile                1.0.1                    py37_0    anaconda\r\nexpat                     2.2.6                he6710b0_0    anaconda\r\nfastcache                 1.1.0            py37h516909a_0    conda-forge\r\nfastparquet               0.4.0            py37h03ebfcd_0    conda-forge\r\nfontconfig                2.13.0               h9420a91_0    anaconda\r\nfreetype                  2.9.1                h8a8886c_1    anaconda\r\nfsspec                    0.7.1                      py_0    anaconda\r\nfuture                    0.17.1                py37_1000    conda-forge\r\nglib                      2.56.2               hd408876_0    anaconda\r\nglpk                      4.65                 h3ceedfd_2    anaconda\r\ngmp                       6.1.2                h6c8ec71_1  \r\ngsl                       2.4                  h14c3975_4    anaconda\r\ngst-plugins-base          1.14.0               hbbd80ab_1    anaconda\r\ngstreamer                 1.14.0               hb453b48_1    anaconda\r\nh5py                      2.9.0            py37h7918eee_0    anaconda\r\nhdf5                      1.10.4               hb1b8bf9_0    anaconda\r\nheapdict                  1.0.1                      py_0    anaconda\r\nhmmlearn                  0.2.3            py37hc1659b7_1    conda-forge\r\nicu                       58.2                 h9c2bf20_1  \r\nidna                      2.8                      py37_0    anaconda\r\nintel-openmp              2019.4                      243    anaconda\r\nipython                   7.7.0            py37h39e3cac_0    anaconda\r\nipython_genutils          0.2.0                    py37_0    anaconda\r\njdcal                     1.4.1                      py_0    conda-forge\r\njedi                      0.15.1                   py37_0    conda-forge\r\njinja2                    2.11.2                     py_0    anaconda\r\njoblib                    0.13.2                   py37_0    anaconda\r\njpeg                      9b                   h024ee3a_2  \r\nkiwisolver                1.1.0            py37he6710b0_0    anaconda\r\nlibedit                   3.1.20181209         hc058e9b_0    anaconda\r\nlibffi                    3.2.1                hd88cf55_4  \r\nlibgcc-ng                 9.1.0                hdf63c60_0    anaconda\r\nlibgfortran-ng            7.3.0                hdf63c60_0    anaconda\r\nlibpng                    1.6.37               hbc83047_0    anaconda\r\nlibprotobuf               3.6.0                hdbcaa40_0    anaconda\r\nlibstdcxx-ng              9.1.0                hdf63c60_0    anaconda\r\nlibtiff                   4.1.0                h2733197_0    anaconda\r\nlibuuid                   1.0.3                h1bed415_2    anaconda\r\nlibxcb                    1.13                 h1bed415_1    anaconda\r\nlibxml2                   2.9.9                hea5a465_1    anaconda\r\nllvmlite                  0.32.1           py37hd408876_0    anaconda\r\nlocket                    0.2.0                    py37_1    anaconda\r\nlz4-c                     1.8.1.2              h14c3975_0    anaconda\r\nlzo                       2.10                 h49e0be7_2  \r\nmarkupsafe                1.1.1            py37h7b6447c_0    anaconda\r\nmatplotlib                3.1.1            py37h5429711_0    anaconda\r\nmatplotlib-base           3.1.3            py37hef1b27d_0  \r\nmetis                     5.1.0                hf484d3e_4    anaconda\r\nmkl                       2019.4                      243    anaconda\r\nmkl-service               2.0.2            py37h7b6447c_0    anaconda\r\nmkl_fft                   1.0.12           py37ha843d7b_0    anaconda\r\nmkl_random                1.0.2            py37hd81dba3_0    anaconda\r\nmock                      3.0.5                    py37_0    conda-forge\r\nmsgpack-python            1.0.0            py37hfd86e86_1    anaconda\r\nmultiprocess              0.70.9           py37h516909a_0    conda-forge\r\nmultitasking              0.0.9                      py_0    ranaroussi\r\nmysql-connector-c         6.1.11               h597af5e_0  \r\nmysql-connector-python    8.0.18           py37h9c95fcb_1    anaconda\r\nncurses                   6.1                  he6710b0_1    anaconda\r\nninja                     1.9.0            py37hfd86e86_0    anaconda\r\nnose                      1.3.7                    py37_2    conda-forge\r\nnumba                     0.49.1           py37h0573a6f_0    anaconda\r\nnumexpr                   2.7.0            py37h9e4a6bb_0    anaconda\r\nnumpy                     1.16.4           py37h7e9f1db_0    anaconda\r\nnumpy-base                1.16.4           py37hde5b4d6_0    anaconda\r\nolefile                   0.46                     py37_0    anaconda\r\nopenblas                  0.3.3             h9ac9557_1001    conda-forge\r\nopenpyxl                  2.6.2                      py_0    conda-forge\r\nopenssl                   1.1.1g               h516909a_0    conda-forge\r\nosqp                      0.5.0            py37hb3f55d8_0    conda-forge\r\npackaging                 20.3                       py_0    anaconda\r\npandas                    1.0.3            py37h0573a6f_0    anaconda\r\nparso                     0.5.1                      py_0    conda-forge\r\npartd                     1.1.0                      py_0    anaconda\r\npathos                    0.2.5                      py_0    conda-forge\r\npatsy                     0.5.1                    py37_0    anaconda\r\npcre                      8.43                 he6710b0_0    anaconda\r\npeewee                    3.9.2            py37h6b74fdf_0    conda-forge\r\npexpect                   4.7.0                    py37_0    conda-forge\r\npickleshare               0.7.5                    py37_0    anaconda\r\npillow                    7.1.2            py37hb39fc2d_0    anaconda\r\npip                       19.1.1                   py37_0    conda-forge\r\npox                       0.2.7                      py_0    conda-forge\r\nppft                      1.6.6.1                  py37_0    conda-forge\r\nprompt_toolkit            2.0.9                    py37_0    anaconda\r\nprotobuf                  3.6.0            py37hf484d3e_0    anaconda\r\npsutil                    5.7.0            py37h7b6447c_0    anaconda\r\nptyprocess                0.6.0                    py37_0    conda-forge\r\npycparser                 2.20                       py_0    anaconda\r\npygments                  2.4.2                      py_0    conda-forge\r\npymysql                   0.9.3                    py37_0  \r\npyopenssl                 19.1.0                   py37_0    anaconda\r\npyparsing                 2.4.2                      py_0    conda-forge\r\npyqt                      5.9.2            py37h05f1152_2    anaconda\r\npysocks                   1.7.1                    py37_0    anaconda\r\npytables                  3.5.2            py37h71ec239_1    anaconda\r\npython                    3.7.3                h0371630_0    anaconda\r\npython-dateutil           2.8.0                    py37_0    anaconda\r\npython-snappy             0.5.4            py37he6710b0_0  \r\npython_abi                3.7                     1_cp37m    conda-forge\r\npytorch                   1.5.0           py3.7_cuda10.2.89_cudnn7.6.5_0    pytorch\r\npytz                      2019.1                     py_0    conda-forge\r\npyyaml                    5.3.1            py37h7b6447c_0    anaconda\r\nqt                        5.9.7                h5867ecd_1    anaconda\r\nrarfile                   3.1                      pypi_0    pypi\r\nreadline                  7.0                  h7b6447c_5    anaconda\r\nrequests                  2.22.0                   py37_1  \r\nscikit-learn              0.22.1           py37hd81dba3_0  \r\nscipy                     1.4.1            py37h0b6359f_0  \r\nscs                       2.0.2           py37h0290663_1000    conda-forge\r\nsetuptools                41.0.1                   py37_0    conda-forge\r\nsip                       4.19.8           py37hf484d3e_0    anaconda\r\nsix                       1.12.0                   py37_0    anaconda\r\nsnappy                    1.1.7                hbae5bb6_3  \r\nsortedcontainers          2.1.0                    py37_0    anaconda\r\nsoupsieve                 2.0.1                      py_0    anaconda\r\nsqlalchemy                1.3.7            py37h7b6447c_0    anaconda\r\nsqlite                    3.28.0               h7b6447c_0    anaconda\r\nstatsmodels               0.10.1           py37hdd07704_0    anaconda\r\nsuitesparse               5.2.0                h9e4a6bb_0    anaconda\r\ntbb                       2019.4               hfd86e86_0    anaconda\r\ntblib                     1.6.0                      py_0    anaconda\r\nthrift                    0.11.0           py37hf484d3e_0  \r\ntk                        8.6.8                hbc83047_0    anaconda\r\ntoolz                     0.10.0                     py_0    anaconda\r\ntornado                   6.0.3            py37h7b6447c_0    anaconda\r\ntraitlets                 4.3.2                    py37_0    conda-forge\r\ntyping_extensions         3.7.4.1                  py37_0    anaconda\r\nunrar                     0.4                      pypi_0    pypi\r\nurllib3                   1.25.8                   py37_0    anaconda\r\nwcwidth                   0.1.7                    py37_0    anaconda\r\nwheel                     0.33.4                   py37_0    conda-forge\r\nwrapt                     1.11.2           py37h7b6447c_0  \r\nxlrd                      1.2.0                    py37_0    anaconda\r\nxz                        5.2.4                h14c3975_4    anaconda\r\nyaml                      0.1.7                h96e3832_1    anaconda\r\nyfinance                  0.1.54                     py_0    ranaroussi\r\nzict                      2.0.0                      py_0    anaconda\r\nzlib                      1.2.11               h7b6447c_3    anaconda\r\nzstd                      1.3.7                h0b5b093_0    anaconda\r\n```"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-06-11T17:25:07Z",
        "body": "The things that stick out to me from your environment are `pandas`, `dask`, `dask-core`, and `distributed`.\r\n\r\nWe currently require `pandas >=0.25.3,<1.0.0a0`, `dask >=2.15.0`, `dask-core >=2.15.0`, `distributed >=2.15.0`.\r\n\r\nThere's also likely a lot of conflicts coming from trying to move from defaults to conda-forge in your environment where you could try:\r\n```\r\nconda install -c nvidia -c rapidsai -c numba -c defaults -c conda-forge \"cudf\" \"pandas\" \"dask\" \"distributed\"\r\n```"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-08-27T18:56:15Z",
        "body": "Closing as answered."
      }
    ]
  },
  {
    "number": 5296,
    "title": "[QST] Illegal Memory Access",
    "created_at": "2020-05-27T10:25:35Z",
    "closed_at": "2020-08-10T03:38:26Z",
    "labels": [
      "invalid",
      "question",
      "libcudf"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/5296",
    "body": "Whenever I try to either join or filter the dataframe,I m getting runtime error 700.\nThe detailed error description  is:\nRuntimeError:CUDA error encountered ar: /conda/conda-bld/libcudf_1580806686173/work/cpp/src/join/legacy/join_compute_apu.h:357:700 cudaErrorIllegalAddress an illegal memory address was encountered \n\nThe code worked fine on cudf 0.10,but post upgrade to 0.12,started getting this error.\nThe data size is 4GB.\n\nSpecification:\nGPU:Volta V100\nGPU memory:32GB\n\n\nRequest help to resolve the issue.\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/5296/comments",
    "author": "datascientistpur",
    "comments": [
      {
        "user": "jrhemstad",
        "created_at": "2020-05-27T12:40:51Z",
        "body": "0.12 is quite old. Your error is being thrown from code that no longer exists. I suggest trying with the latest 0.14 or 0.15. "
      },
      {
        "user": "datascientistpur",
        "created_at": "2020-05-27T13:53:18Z",
        "body": "@jrhemstad, wanted to know if 0.13 works,since 0.14 is in nightly mode?"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-05-27T14:56:23Z",
        "body": "@datascientistpur can you try with 0.13 and report back? There's no reproducer for us to try with."
      },
      {
        "user": "harrism",
        "created_at": "2020-08-10T03:38:26Z",
        "body": "Closing due to no response. Please reopen if this is still an issue."
      }
    ]
  },
  {
    "number": 5082,
    "title": "Kernel crashing",
    "created_at": "2020-05-02T09:58:28Z",
    "closed_at": "2020-05-27T10:18:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/5082",
    "body": "I am processing a huge file with strings(file size approx 5.12GB,rows 208 million).Whenever I try to concatenate strings the memory utilization shoots to 32GB and kernel crashes.\n\nRequest help in processing the same.\n\nCUDF version:0.10.0\nCUDA version:10.1\nGPU:Tesla V100",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/5082/comments",
    "author": "datascientistpur",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-05-05T15:22:57Z",
        "body": "@datascientistpur It sounds like you're running out of memory. Could you update to cudf 0.13 which is the latest release? The error messaging should be much better where we can help troubleshoot."
      }
    ]
  },
  {
    "number": 4952,
    "title": "[QST]ffi.error: symbol 'rmmInitialize' not found in library ???",
    "created_at": "2020-04-20T13:54:18Z",
    "closed_at": "2020-04-22T10:17:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4952",
    "body": "when i run this command :\r\n(rapids) ehsan@tensor:~/Documents$ python3.7 -c \"import cudf\"\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ehsan/.local/lib/python3.7/site-packages/librmm_cffi/wrapper.py\", line 37, in __getattr__\r\n    return self._cached[name]\r\nKeyError: 'rmmInitialize'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/ehsan/.local/lib/python3.7/site-packages/cudf/__init__.py\", line 3, in <module>\r\n    from cudf import dataframe\r\n  File \"/home/ehsan/.local/lib/python3.7/site-packages/cudf/dataframe/__init__.py\", line 1, in <module>\r\n    from cudf.dataframe import (buffer, dataframe, series,\r\n  File \"/home/ehsan/.local/lib/python3.7/site-packages/cudf/dataframe/buffer.py\", line 3, in <module>\r\n    from librmm_cffi import librmm as rmm\r\n  File \"/home/ehsan/.local/lib/python3.7/site-packages/librmm_cffi/__init__.py\", line 54, in <module>\r\n    librmm.initialize()\r\n  File \"/home/ehsan/.local/lib/python3.7/site-packages/librmm_cffi/wrapper.py\", line 90, in initialize\r\n    return self.rmmInitialize(opts)\r\n  File \"/home/ehsan/.local/lib/python3.7/site-packages/librmm_cffi/wrapper.py\", line 39, in __getattr__\r\n    fn = getattr(self._api, name)\r\nffi.error: symbol 'rmmInitialize' not found in library '/home/ehsan/anaconda3/envs/rapids/lib/librmm.so': /home/ehsan/anaconda3/envs/rapids/lib/librmm.so: undefined symbol: rmmInitialize\r\n#---------------------------------------------------------------------------------------------------------------------------\r\ni install rapidsai with :\r\nconda install -c rapidsai -c nvidia -c conda-forge -c defaults rapids=0.13 python=3.7 cudatoolkit=10.2\r\n#---------------------------------------------------------------------------------------------------------------------------\r\n#/usr/local/cuda-10.2/\r\n\r\nexport PATH=$PATH:/usr/local/cuda-10.2/bin\r\nexport CUDADIR=/usr/local/cuda-10.2/\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib:\r\n\r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.48.02    Driver Version: 440.48.02    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   45C    P8    16W / 280W |    748MiB / 11175MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1464      G   /usr/lib/xorg/Xorg                            26MiB |\r\n|    0      2715      G   /usr/lib/xorg/Xorg                           186MiB |\r\n|    0      2931      G   /usr/bin/gnome-shell                         267MiB |\r\n|    0      3177      G   gnome-ring                                     2MiB |\r\n|    0      3880      G   ...rogram/Idea-IU-193.6015.39/jbr/bin/java     3MiB |\r\n|    0     19084      G   ...n/Program/pycharm-2019.3.2/jbr/bin/java     3MiB |\r\n|    0     25162      G   ...AAAAAAAAAAAAAAgAAAAAAAAA --shared-files   114MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n#\r\n# Name                    Version                   Build  Channel\r\nlibnvstrings              0.13.0               cuda10.2_0    rapidsai\r\nnvstrings                 0.13.0                   py37_0    rapidsai\r\n(rapids) ehsan@tensor:~/Documents$ conda list cuda\r\n# packages in environment at /home/ehsan/anaconda3/envs/rapids:\r\n#\r\n# Name                    Version                   Build  Channel\r\ncudatoolkit               10.2.89              h6bb024c_0    nvidia\r\ndask-cuda                 0.13.0                   py37_0    rapidsai\r\n(rapids) ehsan@tensor:~/Documents$ \r\n\r\nand os : \r\nUbuntu 19.10\r\n\r\n<pre># packages in environment at /home/ehsan/anaconda3/envs/rapids:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                 conda_forge    conda-forge\r\n_openmp_mutex             4.5                      1_llvm    conda-forge\r\naiohttp                   3.6.2            py37h516909a_0    conda-forge\r\nappdirs                   1.4.3                      py_1    conda-forge\r\narrow-cpp                 0.15.0           py37h090bef1_2    conda-forge\r\nasync-timeout             3.0.1                   py_1000    conda-forge\r\nattrs                     19.3.0                     py_0    conda-forge\r\nbackcall                  0.1.0                      py_0    conda-forge\r\nbleach                    3.1.4              pyh9f0ad1d_0    conda-forge\r\nbokeh                     1.4.0            py37hc8dfbb8_1    conda-forge\r\nboost                     1.70.0           py37h9de70de_1    conda-forge\r\nboost-cpp                 1.70.0               h8e57a91_2    conda-forge\r\nbrotli                    1.0.7             he1b5a44_1001    conda-forge\r\nbrotlipy                  0.7.0           py37h8f50634_1000    conda-forge\r\nbzip2                     1.0.8                h516909a_2    conda-forge\r\nc-ares                    1.15.0            h516909a_1001    conda-forge\r\nca-certificates           2020.4.5.1           hecc5488_0    conda-forge\r\ncairo                     1.16.0            hcf35c78_1003    conda-forge\r\ncertifi                   2020.4.5.1       py37hc8dfbb8_0    conda-forge\r\ncffi                      1.14.0           py37hd463f26_0    conda-forge\r\ncfitsio                   3.470                hb60a0a2_2    conda-forge\r\nchardet                   3.0.4           py37hc8dfbb8_1006    conda-forge\r\nclick                     7.1.1              pyh8c360ce_0    conda-forge\r\nclick-plugins             1.1.1                      py_0    conda-forge\r\ncligj                     0.5.0                      py_0    conda-forge\r\ncloudpickle               1.3.0                      py_0    conda-forge\r\ncolorcet                  2.0.1                      py_0    conda-forge\r\ncryptography              2.8              py37hb09aad4_2    conda-forge\r\ncudatoolkit               10.2.89              h6bb024c_0    nvidia\r\ncudf                      0.13.0                   py37_0    rapidsai\r\ncudnn                     7.6.5                cuda10.2_0  \r\ncugraph                   0.13.0                   py37_0    rapidsai\r\ncuml                      0.13.0          cuda10.2_py37_0    rapidsai\r\ncupy                      7.3.0            py37h940342b_0    conda-forge\r\ncurl                      7.69.1               h33f0ec9_0    conda-forge\r\ncusignal                  0.13.0                   py37_0    rapidsai\r\ncuspatial                 0.13.0                   py37_0    rapidsai\r\ncuxfilter                 0.13.0                   py37_0    rapidsai\r\ncycler                    0.10.0                     py_2    conda-forge\r\ncytoolz                   0.10.1           py37h516909a_0    conda-forge\r\ndask                      2.14.0                     py_0    conda-forge\r\ndask-core                 2.14.0                     py_0    conda-forge\r\ndask-cuda                 0.13.0                   py37_0    rapidsai\r\ndask-cudf                 0.13.0                   py37_0    rapidsai\r\ndask-xgboost              0.2.0.dev28      cuda10.2py36_0    rapidsai\r\ndatashader                0.10.0                     py_0    conda-forge\r\ndatashape                 0.5.4                      py_1    conda-forge\r\ndecorator                 4.4.2                      py_0    conda-forge\r\ndefusedxml                0.6.0                      py_0    conda-forge\r\ndistributed               2.14.0           py37hc8dfbb8_0    conda-forge\r\ndlpack                    0.2                  he1b5a44_1    conda-forge\r\ndouble-conversion         3.1.5                he1b5a44_2    conda-forge\r\nentrypoints               0.3             py37hc8dfbb8_1001    conda-forge\r\nexpat                     2.2.9                he1b5a44_2    conda-forge\r\nfastavro                  0.23.2           py37h8f50634_0    conda-forge\r\nfastrlock                 0.4             py37h3340039_1001    conda-forge\r\nfiona                     1.8.9.post2      py37hdff7cfa_0    conda-forge\r\nfontconfig                2.13.1            h86ecdb6_1001    conda-forge\r\nfreetype                  2.10.1               he06d7ca_0    conda-forge\r\nfreexl                    1.0.5             h14c3975_1002    conda-forge\r\nfsspec                    0.6.3                      py_0    conda-forge\r\ngdal                      2.4.4            py37h5f563d9_0    conda-forge\r\ngeopandas                 0.7.0                      py_1    conda-forge\r\ngeos                      3.8.0                he1b5a44_1    conda-forge\r\ngeotiff                   1.5.1                h38872f0_8    conda-forge\r\ngettext                   0.19.8.1          hc5be6a0_1002    conda-forge\r\ngflags                    2.2.2             he1b5a44_1002    conda-forge\r\ngiflib                    5.1.7                h516909a_1    conda-forge\r\nglib                      2.64.2               h6f030ca_0    conda-forge\r\nglog                      0.4.0                h49b9bf7_3    conda-forge\r\ngrpc-cpp                  1.23.0               h18db393_0    conda-forge\r\nhdf4                      4.2.13            hf30be14_1003    conda-forge\r\nhdf5                      1.10.5          nompi_h3c11f04_1104    conda-forge\r\nheapdict                  1.0.1                      py_0    conda-forge\r\nicu                       64.2                 he1b5a44_1    conda-forge\r\nidna                      2.9                        py_1    conda-forge\r\nimageio                   2.8.0                      py_0    conda-forge\r\nimportlib-metadata        1.6.0            py37hc8dfbb8_0    conda-forge\r\nimportlib_metadata        1.6.0                         0    conda-forge\r\nipykernel                 5.2.0            py37h43977f1_1    conda-forge\r\nipython                   7.13.0           py37hc8dfbb8_2    conda-forge\r\nipython_genutils          0.2.0                      py_1    conda-forge\r\njedi                      0.17.0           py37hc8dfbb8_0    conda-forge\r\njinja2                    2.11.2             pyh9f0ad1d_0    conda-forge\r\njoblib                    0.14.1                     py_0    conda-forge\r\njpeg                      9c                h14c3975_1001    conda-forge\r\njson-c                    0.13.1            h14c3975_1001    conda-forge\r\njsonschema                3.2.0            py37hc8dfbb8_1    conda-forge\r\njupyter-server-proxy      1.3.2                      py_0    conda-forge\r\njupyter_client            6.1.3                      py_0    conda-forge\r\njupyter_core              4.6.3            py37hc8dfbb8_1    conda-forge\r\nkealib                    1.4.13               hec59c27_0    conda-forge\r\nkiwisolver                1.2.0            py37h99015e2_0    conda-forge\r\nkrb5                      1.17.1               h2fd8d38_0    conda-forge\r\nld_impl_linux-64          2.34                 h53a641e_0    conda-forge\r\nlibblas                   3.8.0               16_openblas    conda-forge\r\nlibcblas                  3.8.0               16_openblas    conda-forge\r\nlibcudf                   0.13.0               cuda10.2_0    rapidsai\r\nlibcugraph                0.13.0               cuda10.2_0    rapidsai\r\nlibcuml                   0.13.0               cuda10.2_0    rapidsai\r\nlibcumlprims              0.13.0               cuda10.2_0    nvidia\r\nlibcurl                   7.69.1               hf7181ac_0    conda-forge\r\nlibcuspatial              0.13.0               cuda10.2_0    rapidsai\r\nlibdap4                   3.20.4               hd3bb157_0    conda-forge\r\nlibedit                   3.1.20170329      hf8c457e_1001    conda-forge\r\nlibevent                  2.1.10               h72c5cf5_0    conda-forge\r\nlibffi                    3.2.1             he1b5a44_1007    conda-forge\r\nlibgcc-ng                 9.2.0                h24d8f2e_2    conda-forge\r\nlibgdal                   2.4.4                h2b6fda6_0    conda-forge\r\nlibgfortran-ng            7.3.0                hdf63c60_5    conda-forge\r\nlibhwloc                  2.1.0                h3c4fd83_0    conda-forge\r\nlibiconv                  1.15              h516909a_1006    conda-forge\r\nlibkml                    1.3.0             h4fcabce_1010    conda-forge\r\nliblapack                 3.8.0               16_openblas    conda-forge\r\nlibllvm8                  8.0.1                hc9558a2_0    conda-forge\r\nlibnetcdf                 4.7.3           nompi_h9f9fd6a_101    conda-forge\r\nlibnvstrings              0.13.0               cuda10.2_0    rapidsai\r\nlibopenblas               0.3.9                h5ec1e0e_0    conda-forge\r\nlibpng                    1.6.37               hed695b0_1    conda-forge\r\nlibpq                     12.2                 h5513abc_1    conda-forge\r\nlibprotobuf               3.8.0                h8b12597_0    conda-forge\r\nlibrmm                    0.13.0               cuda10.2_0    rapidsai\r\nlibsodium                 1.0.17               h516909a_0    conda-forge\r\nlibspatialindex           1.9.3                he1b5a44_3    conda-forge\r\nlibspatialite             4.3.0a            ha48a99a_1034    conda-forge\r\nlibssh2                   1.8.2                h22169c7_2    conda-forge\r\nlibstdcxx-ng              9.2.0                hdf63c60_2    conda-forge\r\nlibtiff                   4.1.0                hfc65ed5_0    conda-forge\r\nlibuuid                   2.32.1            h14c3975_1000    conda-forge\r\nlibxcb                    1.13              h14c3975_1002    conda-forge\r\nlibxgboost                1.0.2dev.rapidsai0.13      cuda10.2_6    rapidsai\r\nlibxml2                   2.9.10               hee79883_0    conda-forge\r\nllvm-openmp               10.0.0               hc9558a2_0    conda-forge\r\nllvmlite                  0.31.0           py37h5202443_1    conda-forge\r\nlocket                    0.2.0                      py_2    conda-forge\r\nlz4-c                     1.8.3             he1b5a44_1001    conda-forge\r\nmarkdown                  3.2.1                      py_0    conda-forge\r\nmarkupsafe                1.1.1            py37h8f50634_1    conda-forge\r\nmatplotlib-base           3.2.1            py37h30547a4_0    conda-forge\r\nmistune                   0.8.4           py37h8f50634_1001    conda-forge\r\nmsgpack-python            1.0.0            py37h99015e2_1    conda-forge\r\nmultidict                 4.7.5            py37h516909a_0    conda-forge\r\nmultipledispatch          0.6.0                      py_0    conda-forge\r\nmunch                     2.5.0                      py_0    conda-forge\r\nnbconvert                 5.6.1            py37hc8dfbb8_1    conda-forge\r\nnbformat                  5.0.6                      py_0    conda-forge\r\nnccl                      2.5.7.1              hc6a2c23_0    conda-forge\r\nncurses                   6.1               hf484d3e_1002    conda-forge\r\nnetworkx                  2.4                        py_1    conda-forge\r\nnotebook                  6.0.3                    py37_0    conda-forge\r\nnumba                     0.48.0           py37hb3f55d8_0    conda-forge\r\nnumpy                     1.18.1           py37h8960a57_1    conda-forge\r\nnvstrings                 0.13.0                   py37_0    rapidsai\r\nolefile                   0.46                       py_0    conda-forge\r\nopenjpeg                  2.3.1                h981e76c_3    conda-forge\r\nopenssl                   1.1.1f               h516909a_0    conda-forge\r\npackaging                 20.1                       py_0    conda-forge\r\npandas                    0.25.3           py37hb3f55d8_0    conda-forge\r\npandoc                    2.9.2.1                       0    conda-forge\r\npandocfilters             1.4.2                      py_1    conda-forge\r\npanel                     0.6.4                         0    conda-forge\r\nparam                     1.9.3                      py_0    conda-forge\r\nparquet-cpp               1.5.1                         2    conda-forge\r\nparso                     0.7.0              pyh9f0ad1d_0    conda-forge\r\npartd                     1.1.0                      py_0    conda-forge\r\npcre                      8.44                 he1b5a44_0    conda-forge\r\npexpect                   4.8.0            py37hc8dfbb8_1    conda-forge\r\npickleshare               0.7.5           py37hc8dfbb8_1001    conda-forge\r\npillow                    7.1.1            py37h718be6c_0    conda-forge\r\npip                       20.0.2                     py_2    conda-forge\r\npixman                    0.38.0            h516909a_1003    conda-forge\r\npoppler                   0.67.0               h14e79db_8    conda-forge\r\npoppler-data              0.4.9                         1    conda-forge\r\npostgresql                12.2                 h8573dbc_1    conda-forge\r\nproj                      6.3.0                hc80f0dc_0    conda-forge\r\nprometheus_client         0.7.1                      py_0    conda-forge\r\nprompt-toolkit            3.0.5                      py_0    conda-forge\r\npsutil                    5.7.0            py37h8f50634_1    conda-forge\r\npthread-stubs             0.4               h14c3975_1001    conda-forge\r\nptyprocess                0.6.0                   py_1001    conda-forge\r\npy-xgboost                1.0.2dev.rapidsai0.13  cuda10.2py37_6    rapidsai\r\npyarrow                   0.15.0           py37h8b68381_1    conda-forge\r\npycparser                 2.20                       py_0    conda-forge\r\npyct                      0.4.6                      py_0    conda-forge\r\npyct-core                 0.4.6                      py_0    conda-forge\r\npyee                      7.0.1                      py_0    conda-forge\r\npygments                  2.6.1                      py_0    conda-forge\r\npynvml                    8.0.4                      py_0    conda-forge\r\npyopenssl                 19.1.0                     py_1    conda-forge\r\npyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge\r\npyppeteer                 0.0.25                     py_1    conda-forge\r\npyproj                    2.5.0            py37h8ff28aa_0    conda-forge\r\npyrsistent                0.16.0           py37h8f50634_0    conda-forge\r\npysocks                   1.7.1            py37hc8dfbb8_1    conda-forge\r\npython                    3.7.6           h8356626_5_cpython    conda-forge\r\npython-dateutil           2.8.1                      py_0    conda-forge\r\npython_abi                3.7                     1_cp37m    conda-forge\r\npytz                      2019.3                     py_0    conda-forge\r\npyviz_comms               0.7.4              pyh8c360ce_0    conda-forge\r\npywavelets                1.1.1            py37h03ebfcd_1    conda-forge\r\npyyaml                    5.3.1            py37h8f50634_0    conda-forge\r\npyzmq                     19.0.0           py37hac76be4_1    conda-forge\r\nrapids                    0.13.0          cuda10.2_py37_0    rapidsai\r\nrapids-xgboost            0.13.0          cuda10.2_py37_0    rapidsai\r\nre2                       2020.04.01           he1b5a44_0    conda-forge\r\nreadline                  8.0                  hf8c457e_0    conda-forge\r\nrequests                  2.23.0             pyh8c360ce_2    conda-forge\r\nrmm                       0.13.0                   py37_0    rapidsai\r\nrtree                     0.9.4            py37h8526d28_1    conda-forge\r\nscikit-image              0.16.2           py37hb3f55d8_0    conda-forge\r\nscikit-learn              0.22.2.post1     py37hcdab131_0    conda-forge\r\nscipy                     1.4.1            py37ha3d9a3c_3    conda-forge\r\nsend2trash                1.5.0                      py_0    conda-forge\r\nsetuptools                46.1.3           py37hc8dfbb8_0    conda-forge\r\nshapely                   1.7.0            py37hb106bac_1    conda-forge\r\nsimpervisor               0.3                        py_1    conda-forge\r\nsix                       1.14.0                     py_1    conda-forge\r\nsnappy                    1.1.8                he1b5a44_1    conda-forge\r\nsortedcontainers          2.1.0                      py_0    conda-forge\r\nsqlite                    3.30.1               hcee41ef_0    conda-forge\r\ntblib                     1.6.0                      py_0    conda-forge\r\nterminado                 0.8.3            py37hc8dfbb8_1    conda-forge\r\ntestpath                  0.4.4                      py_0    conda-forge\r\nthrift-cpp                0.12.0            hf3afdfd_1004    conda-forge\r\ntk                        8.6.10               hed695b0_0    conda-forge\r\ntoolz                     0.10.0                     py_0    conda-forge\r\ntornado                   6.0.4            py37h8f50634_1    conda-forge\r\ntqdm                      4.45.0             pyh9f0ad1d_0    conda-forge\r\ntraitlets                 4.3.3            py37hc8dfbb8_1    conda-forge\r\ntzcode                    2019a             h516909a_1002    conda-forge\r\nucx                       1.7.0+g9d06c3a       cuda10.2_0    rapidsai\r\nucx-py                    0.13.0+g9d06c3a          py37_0    rapidsai\r\nuriparser                 0.9.3                he1b5a44_1    conda-forge\r\nurllib3                   1.25.9                     py_0    conda-forge\r\nwcwidth                   0.1.9              pyh9f0ad1d_0    conda-forge\r\nwebencodings              0.5.1                      py_1    conda-forge\r\nwebsockets                8.1              py37h8f50634_1    conda-forge\r\nwheel                     0.34.2                     py_1    conda-forge\r\nxarray                    0.15.1                     py_0    conda-forge\r\nxerces-c                  3.2.2             h8412b87_1004    conda-forge\r\nxgboost                   1.0.2dev.rapidsai0.13  cuda10.2py37_6    rapidsai\r\nxorg-kbproto              1.0.7             h14c3975_1002    conda-forge\r\nxorg-libice               1.0.10               h516909a_0    conda-forge\r\nxorg-libsm                1.2.3             h84519dc_1000    conda-forge\r\nxorg-libx11               1.6.9                h516909a_0    conda-forge\r\nxorg-libxau               1.0.9                h14c3975_0    conda-forge\r\nxorg-libxdmcp             1.1.3                h516909a_0    conda-forge\r\nxorg-libxext              1.3.4                h516909a_0    conda-forge\r\nxorg-libxrender           0.9.10            h516909a_1002    conda-forge\r\nxorg-renderproto          0.11.1            h14c3975_1002    conda-forge\r\nxorg-xextproto            7.3.0             h14c3975_1002    conda-forge\r\nxorg-xproto               7.0.31            h14c3975_1007    conda-forge\r\nxz                        5.2.5                h516909a_0    conda-forge\r\nyaml                      0.2.4                h516909a_0    conda-forge\r\nyarl                      1.3.0           py37h516909a_1000    conda-forge\r\nzeromq                    4.3.2                he1b5a44_2    conda-forge\r\nzict                      2.0.0                      py_0    conda-forge\r\nzipp                      3.1.0                      py_0    conda-forge\r\nzlib                      1.2.11            h516909a_1006    conda-forge\r\nzstd                      1.4.3                h3b9ef0a_0    conda-forge\r\n</pre>\r\n\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4952/comments",
    "author": "ehsanshah",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-04-20T15:47:12Z",
        "body": "@ehsanshah `librmm_cffi` is a remnant of an old version of the `rmm` python library that's likely left on your system somewhere.\r\n\r\nCould you run a `pip list | grep rmm` and see if there's a `librmm_cffi` or `rmm_cffi` or something similar package installed that you can remove?"
      },
      {
        "user": "ehsanshah",
        "created_at": "2020-04-21T06:58:01Z",
        "body": "@kkraus14 i run command : pip list | grep rmm and pip list | grep librmm_cffi and pip list | grep rmm_cffi \r\nbut not found any package  \r\nnote: i run in virtualenv 'rapids' \r\nthis is pip list\r\n<pre>Package                 Version        \r\n----------------------- ---------------\r\nappdirs                 1.4.3          \r\napturl                  0.5.2          \r\nasn1crypto              0.24.0         \r\nastroid                 2.3.3          \r\nattrs                   19.3.0         \r\nAutomat                 20.2.0         \r\nbcrypt                  3.1.6          \r\nbeautifulsoup4          4.8.0          \r\nblinker                 1.4            \r\nboto                    2.49.0         \r\nBrlapi                  0.6.7          \r\ncertifi                 2018.8.24      \r\ncffi                    1.14.0         \r\nchardet                 3.0.4          \r\nClick                   7.0            \r\ncolorama                0.4.3          \r\ncommand-not-found       0.3            \r\nconstantly              15.1.0         \r\ncryptography            2.6.1          \r\ncssselect               1.1.0          \r\ncudf                    0.6.1          \r\ncupshelpers             1.0            \r\nCython                  0.29.16        \r\ndbus-python             1.2.12         \r\ndecorator               4.3.0          \r\ndefer                   1.0.6          \r\ndistlib                 0.3.0          \r\ndistro                  1.3.0          \r\ndistro-info             0.21ubuntu4    \r\ndnspython               1.16.0         \r\nduplicity               0.8.4          \r\nentrypoints             0.3            \r\nfasteners               0.12.0         \r\nfilelock                3.0.12         \r\nfuture                  0.16.0         \r\nhtml5lib                1.0.1          \r\nHTMLParser              0.0.2          \r\nhttplib2                0.11.3         \r\nhyperlink               19.0.0         \r\nidna                    2.6            \r\nimportlib-metadata      1.5.0          \r\nincremental             17.5.0         \r\nipython                 5.8.0          \r\nipython-genutils        0.2.0          \r\nisort                   4.3.21         \r\nkeyring                 18.0.1         \r\nkeyrings.alt            3.1.1          \r\nlanguage-selector       0.1            \r\nlaunchpadlib            1.10.7         \r\nlazr.restfulclient      0.14.2         \r\nlazr.uri                1.0.3          \r\nlazy-object-proxy       1.4.3          \r\nllvmlite                0.31.0         \r\nlockfile                0.12.2         \r\nlouis                   3.10.0         \r\nlxml                    4.4.1          \r\nmacaroonbakery          1.2.3          \r\nMako                    1.0.7          \r\nMarkupSafe              1.1.0          \r\nmccabe                  0.6.1          \r\nmonotonic               1.5            \r\nmysql-connector-python  8.0.19         \r\nnetifaces               0.10.4         \r\nnumba                   0.40.0         \r\nnumpy                   1.18.2         \r\nnvstrings-cuda92        0.3.0.post1    \r\noauthlib                2.1.0          \r\nolefile                 0.46           \r\npandas                  1.0.3          \r\nparamiko                2.6.0          \r\nparsel                  1.5.2          \r\npexpect                 4.6.0          \r\npickleshare             0.7.5          \r\nPillow                  6.1.0          \r\npip                     18.1           \r\nprompt-toolkit          1.0.15         \r\nProtego                 0.1.16         \r\nprotobuf                3.6.1          \r\npyarrow                 0.12.1         \r\npyasn1                  0.4.8          \r\npyasn1-modules          0.2.8          \r\npycairo                 1.16.2         \r\npycparser               2.19           \r\npycrypto                2.6.1          \r\npycups                  1.9.73         \r\nPyDispatcher            2.0.5          \r\nPygments                2.3.1          \r\nPyGObject               3.34.0         \r\nPyHamcrest              2.0.0          \r\nPyJWT                   1.7.0          \r\npylint                  2.4.4          \r\npymacaroons             0.13.0         \r\npymongo                 3.10.1         \r\nPyNaCl                  1.3.0          \r\npyOpenSSL               19.1.0         \r\npyRFC3339               1.1            \r\npython-apt              1.9.0+ubuntu1.3\r\npython-dateutil         2.7.3          \r\npython-debian           0.1.36         \r\npytz                    2019.2         \r\npyxdg                   0.25           \r\nPyYAML                  5.1.2          \r\nqueuelib                1.5.0          \r\nrequests                2.21.0         \r\nrequests-unixsocket     0.1.5          \r\nScrapy                  1.8.0          \r\nscreen-resolution-extra 0.0.0          \r\nSecretStorage           2.3.1          \r\nselenium                3.141.0        \r\nservice-identity        18.1.0         \r\nsetuptools              41.1.0         \r\nsimplegeneric           0.8.1          \r\nsimplejson              3.16.0         \r\nsix                     1.14.0         \r\nsoupsieve               1.9.2          \r\nssh-import-id           5.7            \r\nsystem-service          0.3            \r\nsystemd-python          234            \r\ntraitlets               4.3.2          \r\nTwisted                 19.10.0        \r\ntyped-ast               1.4.1          \r\nubuntu-advantage-tools  19.5           \r\nubuntu-drivers-common   0.0.0          \r\nufw                     0.36           \r\nunattended-upgrades     0.1            \r\nurllib3                 1.24.1         \r\nusb-creator             0.3.7          \r\nvboxapi                 1.0            \r\nvirtualenv              20.0.5         \r\nw3lib                   1.21.0         \r\nwadllib                 1.3.3          \r\nwcwidth                 0.1.7          \r\nwebencodings            0.5.1          \r\nwheel                   0.32.3         \r\nwrapt                   1.11.2         \r\nxkit                    0.0.0          \r\nyoutube-dl              2020.1.24      \r\nzipp                    3.0.0          \r\nzope.interface          4.7.1   </pre>\r\n"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-04-21T14:47:33Z",
        "body": "Is that the same environment as your conda environment above? If so you have a ton of conflicting and old versions of things, i.e. `cudf 0.6.1`. Based on this I think your best bet is to blow away the environment and start fresh with a new environment."
      },
      {
        "user": "ehsanshah",
        "created_at": "2020-04-22T10:17:46Z",
        "body": "@kkraus14 thanks I'm uninstall annaconda and remove all library and install again "
      }
    ]
  },
  {
    "number": 4893,
    "title": "[QST] I need a \"reduce\" operation",
    "created_at": "2020-04-14T09:46:42Z",
    "closed_at": "2020-04-15T14:22:30Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4893",
    "body": "Hi!\r\n\r\nI have grouped my cudf stream and now I need to reduce values by unique values.\r\n For example, My stream has:\r\na   b\r\n1  1\r\n1  2\r\n2  3\r\n2  4\r\n\r\nI need to get stream with have unique value of column 'a' and result of some function from column 'b'\r\na  f(a)\r\n1  10\r\n2  20\r\n\r\nHow I can do it? Thanks for advice!",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4893/comments",
    "author": "schernolyas",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-04-14T18:31:34Z",
        "body": "@schernolyas it sounds like you're doing a groupby. Can you give more information about what `f(a)` looks like? Do built in groupby aggregations not suffice for your use case?"
      },
      {
        "user": "schernolyas",
        "created_at": "2020-04-14T18:52:58Z",
        "body": "Hi @kkraus14 !\r\nDo you mean that I can decrease count of rows by groupby? I tried groupby without success."
      },
      {
        "user": "kkraus14",
        "created_at": "2020-04-14T18:55:56Z",
        "body": "@schernolyas Yes, in your above example, you could do something like `df.groupby(['a']).sum()` which would return you two rows of `[3, 7]` which is the sum of the `b` column."
      },
      {
        "user": "schernolyas",
        "created_at": "2020-04-15T07:12:17Z",
        "body": "Hi @kkraus14 !\r\n\r\nThank you very much for your comments. "
      },
      {
        "user": "kkraus14",
        "created_at": "2020-04-15T14:22:30Z",
        "body": "@schernolyas my pleasure, I'm going to close this as it seems your question is resolved. If you have any additional questions feel free to open a new issue."
      }
    ]
  },
  {
    "number": 4892,
    "title": "[QST][Java] first call of 'ColumnVector.from...(...) ' is too slow ?",
    "created_at": "2020-04-14T03:52:57Z",
    "closed_at": "2020-04-15T00:29:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4892",
    "body": "> I use the java interface ’ColumnVector.fromLongs‘ to generate the Table. and at the same time, i want to know what time it is spent , code like this:\r\n--------------\r\n                val currentColumnVecotr: Array[ColumnVector] = new Array[ColumnVector](3)\r\n                val s1 = System.currentTimeMillis\r\n                currentColumnVecotr(0) = ColumnVector.fromBoxedInts(10, 2, 30, 40)\r\n                val s2 = System.currentTimeMillis\r\n                currentColumnVecotr(1) = ColumnVector.fromBoxedLongs(1L, 2L, 3L, 4L)\r\n                currentColumnVecotr(2) = ColumnVector.fromBoxedBooleans(true, false, false, true)\r\n                val retTable : Table = new Table(currentColumnVecotr: _*)\r\n                val s3 = System.currentTimeMillis\r\n                System.out.println(fisrt call ColumnVector.from... time is \" + (t2 - t1))\r\n                System.out.println(generate table time is \" + (t3 - t1))\r\n---------------\r\nfinally, i get the result :\r\n**first call ColumnVecotr.from...  time is 1599ms\r\ngenerate table time is 1601ms**\r\n\r\nquetion: \r\n1.my first question is that why the first call of ColumnVector.from... cost 1599ms, but the follow code is cost few  ? I doubt that I have not add the cudaThreadSynchronize() to avoid the Asynchrony of CUDA API ？if so , is there some good method to profile  java performance.\r\n2. I want to know if it is normal to this time cost (1601ms) ,after all  I just generate 3 columns ",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4892/comments",
    "author": "chenrui17",
    "comments": [
      {
        "user": "revans2",
        "created_at": "2020-04-14T12:54:43Z",
        "body": "There is often a lot of startup time with Cuda.  When Cuda first interacts with a GPU it needs to set up a number of things, both in the kernel and on the GPU, including sending the precompiled kernels to the GPU.  In the case of cuDF that can be over 500MB of kernels that are sent.  That will hopefully drop a lot once we remove the legacy API support and the corresponding kernels, but it will not all go away.\r\n\r\nIf it is any consolation when I run the same program, with a few edits to make it compile it takes me 4.1 seconds to do the first column vector call.  This is a known issue both in the java side and on the python side.\r\n\r\n@kkraus14 do you know if there is anything being done to address this in the C++ code or that python is doing that the java API could adopt?\r\n"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-04-14T15:58:07Z",
        "body": "> @kkraus14 do you know if there is anything being done to address this in the C++ code or that python is doing that the java API could adopt?\r\n\r\nWe're not doing anything special on the C++ side where I think this is just standard CUDA startup overheads. In addition to context creation and copying over the device code, driver API initialization also occurs which is a non-trivial cost as well.\r\n\r\nWe validate the GPU setup (GPU architecture, driver version, toolkit version, etc.) on import which does the driver initialization, so we only need to initialize the context (and implicitly copy device code) on first column creation, but it's still costly."
      },
      {
        "user": "revans2",
        "created_at": "2020-04-14T17:45:06Z",
        "body": "@rui911013 does that answer your question?"
      },
      {
        "user": "chenrui17",
        "created_at": "2020-04-15T00:29:01Z",
        "body": "> There is often a lot of startup time with Cuda. When Cuda first interacts with a GPU it needs to set up a number of things, both in the kernel and on the GPU, including sending the precompiled kernels to the GPU. In the case of cuDF that can be over 500MB of kernels that are sent. That will hopefully drop a lot once we remove the legacy API support and the corresponding kernels, but it will not all go away.\r\n> \r\n> If it is any consolation when I run the same program, with a few edits to make it compile it takes me 4.1 seconds to do the first column vector call. This is a known issue both in the java side and on the python side.\r\n> \r\n> @kkraus14 do you know if there is anything being done to address this in the C++ code or that python is doing that the java API could adopt?\r\n\r\n\r\n\r\n> @rui911013 does that answer your question?\r\n\r\nyes，thank you and @kkraus14 \r\n"
      }
    ]
  },
  {
    "number": 4874,
    "title": "[QST] ai.rapids:cudf:cuda10 for CUDA 10.2",
    "created_at": "2020-04-10T10:44:23Z",
    "closed_at": "2020-04-14T09:09:59Z",
    "labels": [
      "question",
      "Java"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4874",
    "body": "Hi!\r\nI need artifact\r\n`<dependency>\r\n        <groupId>ai.rapids</groupId>\r\n        <artifactId>cudf</artifactId>\r\n        <classifier>cuda10-1</classifier>\r\n        <version>0.9.2</version>\r\n    </dependency>`\r\nfor CUDA 10.2. How I can build it?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4874/comments",
    "author": "schernolyas",
    "comments": [
      {
        "user": "schernolyas",
        "created_at": "2020-04-10T10:45:57Z",
        "body": "Or ... can someone build and push the artifact to central maven repository? Please ..."
      },
      {
        "user": "jlowe",
        "created_at": "2020-04-10T14:05:54Z",
        "body": "`<classifier>cuda10-1</classifier>` indicates it is built for CUDA 10.1.  The cudf 0.9.2 release does not support CUDA 10.2.  CUDA 10.2 is supported starting with 0.13.\r\n\r\nNote that multiple CUDA runtime versions can be installed on a system because the kernel driver is guaranteed to be backwards compatible.   Therefore the 10.2-compatible driver also supports running applications using the 10.1 runtime.  The easiest solution may be to install the CUDA 10.1 runtime libraries on the system somewhere and update `LD_LIBRARY_PATH` if necessary so the application can find the 10.1 libraries."
      },
      {
        "user": "schernolyas",
        "created_at": "2020-04-14T09:09:59Z",
        "body": "Hi @jlowe !\r\nThank you very much for the comment."
      }
    ]
  },
  {
    "number": 4810,
    "title": "[QST] cuDF Query is too slow",
    "created_at": "2020-04-06T08:37:42Z",
    "closed_at": "2020-04-07T03:54:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4810",
    "body": "Hi All,\r\n\r\nI recently ported a script to use cuDF dataframes in place of pandas dataframes. On comparing the time taken by API calls, I found panda APIs to be doing much better. Honestly, I am confused by seeing such results. I am not sure if I have done it correctly, as this is my first time using cuDF. For your reference:\r\n\r\n1. temp = cudf_df.query('Id == @i')\r\nThe Id column is int64 type, dataframe contains around 12 rows.\r\nThis query took:  **0:00:01.262744**\r\n\r\n\r\n2. temp=pandas_df[pandas_df['Id']==i]  \r\nThe Id column is int64 type, dataframe contains data identical to cudf dataframe.\r\nThis query took:  **0:00:00.006388**\r\n\r\n**System Configuration**\r\ncuDF package version: cudf-cuda100  0.6.1 \r\nNVIDIA CUDA toolkit version:  CUDA Version 10.0.130 with CUDA Patch Version 10.0.130.1\r\nGPU Name: Titan RTX\r\nOperating System: Linux\r\nArchitecture: x86_64\r\nDistribution: Ubuntu\r\nVersion: 18.04.3 \r\n\r\ncudf query API is almost 200 times slower. Could someone please point me in the right direction? \r\nThanks in advance!\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4810/comments",
    "author": "GoyalMansi",
    "comments": [
      {
        "user": "harrism",
        "created_at": "2020-04-06T10:19:30Z",
        "body": "thanks for your question! You probably need thousands of rows minimum to see a speedup. For small data frames, the overhead of launching a kernel on the GPU will be larger than the overhead of calling a function on the CPU. 12 rows is miniscule."
      },
      {
        "user": "GoyalMansi",
        "created_at": "2020-04-07T03:54:54Z",
        "body": "Thank you, that does make a lot of sense. Maybe, I need to find a better use case for cuDF then. Thanks Mark for the reply!"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-04-07T04:01:58Z",
        "body": "@GoyalMansi additionally it looks like you installed an old version of cuDF via pip as opposed to the latest version from conda. Would highly recommend installing from conda if/when you try cuDF again!"
      }
    ]
  },
  {
    "number": 4791,
    "title": "[DOC] Document supported NVIDIA GPUs and CUDA versions for cuDF",
    "created_at": "2020-04-03T13:24:47Z",
    "closed_at": "2020-04-21T06:02:40Z",
    "labels": [
      "bug",
      "question",
      "doc"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4791",
    "body": "I realize RAPIDS overall (especially model fitting, etc.) requires a certain level (or above) of NVIDIA GPU chips -- but curious what the cuDF-specific requirements are? Just via trial and error, I already noticed it seems to work on a broader set of chips than RAPIDS overall.\r\n\r\nIs there a compatibility matrix available specifically for cuDF and which chips are supported?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4791/comments",
    "author": "saifrahmed",
    "comments": [
      {
        "user": "jrhemstad",
        "created_at": "2020-04-03T13:49:37Z",
        "body": "We currently support CUDA 10 and newer and compute capability 6.0 and newer (Pascal+). "
      },
      {
        "user": "harrism",
        "created_at": "2020-04-06T01:18:11Z",
        "body": "Seems this is not documented anywhere  I can find. Let's change this into a doc bug."
      }
    ]
  },
  {
    "number": 4605,
    "title": "[QST] Arrow 0.16 support",
    "created_at": "2020-03-19T16:40:12Z",
    "closed_at": "2020-05-29T16:48:30Z",
    "labels": [
      "question",
      "CMake"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4605",
    "body": "**What is your question?**\r\nHi all - is there an ETA for Arrow 0.16 support in cudf/RAPIDs?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4605/comments",
    "author": "niviksha",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-03-31T15:52:40Z",
        "body": "Hi @niviksha, We are exploring moving to Arrow 0.16, but we're hitting non-trivial nvcc issues related to Arrow headers in the process. We're trying to work through them but things may take a bit of time unfortunately."
      },
      {
        "user": "kkraus14",
        "created_at": "2020-05-29T16:48:30Z",
        "body": "We will be skipping Arrow 0.16 and moving to Arrow 0.17.1 based on fixes that were merged for nvcc compatibility. Closing this and opening a new issue to upgrade to Arrow 0.17.1."
      },
      {
        "user": "niviksha",
        "created_at": "2020-06-10T18:54:19Z",
        "body": "is this move to arrow 0.17 going to happen in rapids 0.14?"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-06-10T19:00:23Z",
        "body": "> is this move to arrow 0.17 going to happen in rapids 0.14?\r\n\r\nNo it will not. It will happen in RAPIDS 0.15. You can follow the PR here: #5408 "
      }
    ]
  },
  {
    "number": 4426,
    "title": "[QST]how to use libcudf directly？",
    "created_at": "2020-03-11T14:09:35Z",
    "closed_at": "2020-03-13T04:27:37Z",
    "labels": [
      "question",
      "libcudf"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4426",
    "body": "hi everybody.I am just new to here.Can I ask you a simple question?I wanna use libcudf directly,how should I do that?Now I download cuda10.2 and libcudf-0.13.0a200216-cuda10.2_1863.tar.bz2.In cmakelist I wrote something like this:\r\n`include_directories(../libcudf/include)\r\ntarget_link_libraries(helloworld libcudf.so)\r\ntarget_link_libraries(helloworld libcudftestutil.a)`\r\n\r\nIs that correct? \r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4426/comments",
    "author": "scirocc",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-03-13T04:27:37Z",
        "body": "@scirocc you shouldn't need to link against `libcudftestutil.a` unless you're looking for our testing utilities. Just linking against `libcudf.so` should get you all of the functionality you're looking for."
      }
    ]
  },
  {
    "number": 4378,
    "title": "[QST]convert boolean value into float32",
    "created_at": "2020-03-09T13:31:36Z",
    "closed_at": "2020-03-13T04:28:41Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4378",
    "body": "**What is your question?**\r\nHi, is there a good method e.g. apply lambda in pandas, but now in cudf to convert a boolean value into float32 in a DataFrame?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4378/comments",
    "author": "dominicshanshan",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-03-13T04:28:41Z",
        "body": "@dominicshanshan you should be able to just do `df['my_boolean_column'].as_type('float32')` which will be much more efficient then using an apply UDF."
      }
    ]
  },
  {
    "number": 4356,
    "title": "[QST] Optimal Parquet size for CUDF dataframe.",
    "created_at": "2020-03-06T15:50:24Z",
    "closed_at": "2020-03-11T01:31:54Z",
    "labels": [
      "question",
      "cuIO"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/4356",
    "body": "**What is your question?**\r\n\r\nFor huge data like 60GB, what will be best approach to create the parquet file?\r\n\r\nShould be have multiple parquet file or single parquet file?\r\n\r\nIn which scenario, cudf will read faster? \r\nAlso, what should be max size of parquet to get best performance of CUDF dataframe and query?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/4356/comments",
    "author": "raviy0807",
    "comments": [
      {
        "user": "OlivierNV",
        "created_at": "2020-03-11T01:03:07Z",
        "body": "For single gpu, it shouldn't matter much (assuming you can iteratively read a subset of the large file in each read using the min_row/num_rows or rowgroup apis, and/or iteratively write the file using the recently-added chunked write APIs).\r\nFor multiple GPUs, it is preferable to distribute separate files to different GPUs, but you generally want each parquet file to be at least 500MB to 1GB per read_parquet() call in order to get maximum throughput."
      },
      {
        "user": "kkraus14",
        "created_at": "2020-03-11T01:31:54Z",
        "body": "Thanks for answering @OlivierNV! Closing"
      }
    ]
  },
  {
    "number": 3631,
    "title": "[QST]Why cuDF is not able to run ",
    "created_at": "2019-12-17T23:25:59Z",
    "closed_at": "2019-12-19T20:58:41Z",
    "labels": [
      "question",
      "Needs Triage"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/3631",
    "body": "I installed cuDF with anaconda during cugraph installation but my kernel is not able to produce any output. \r\n\r\nMy code is below.\r\n\r\nimport cugraph\r\nimport cudf\r\nimport pandas as pd\r\nprint(\"hi\")\r\nM = cudf.read_csv('karate.csv', delimiter=' ', dtype=['int32', 'int32', 'float32'], header=None)\r\nsources = cudf.Series(M['0'])\r\ndestinations = cudf.Series(M['1'])\r\nG = cugraph.Graph()\r\nG.add_edge_list(sources, destinations, None)\r\nparts, modularity_score = cugraph.louvain(G)\r\nprint(modularity_score)\r\n\r\nbut i am not getting output for the cudf.read_csv. i tried in jupyter notebook and it is not working. so now what should i do?\r\n\r\nwhen i tried to execute some functions like \r\n\r\ns = cudf.Series([1,2,3,None,4])\r\nprint(s)\r\n but i got the error like below\r\n\r\nERROR:Call to cuMemcpyHtoD results in CUDA_ERROR_INVALID_VALUE\r\n\r\nplease help me out from this situation.\r\nThank you so muich.\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/3631/comments",
    "author": "vrushang25",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-12-17T23:27:24Z",
        "body": "@vrushang25 what do you mean you're not getting output for the `cudf.read_csv`? Is `M` not a `cudf.DataFrame` after the call in your example above?"
      },
      {
        "user": "vrushang25",
        "created_at": "2019-12-17T23:33:25Z",
        "body": "I am not getting why kernel is not executing. when i tried to execute some functions like\r\n\r\ns = cudf.Series([1,2,3,None,4])\r\nprint(s)\r\nbut i got the error like below\r\n\r\nERROR:Call to cuMemcpyHtoD results in CUDA_ERROR_INVALID_VALUE\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n~/anaconda3/lib/python3.7/site-packages/cudf/dataframe/columnops.py in as_column(arbitrary, nan_as_null, dtype)\r\n    410         try:\r\n--> 411             data = as_column(memoryview(arbitrary))\r\n    412         except TypeError:\r\n\r\nTypeError: memoryview: a bytes-like object is required, not 'list'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nCudaAPIError                              Traceback (most recent call last)\r\n<ipython-input-6-d9ded96cafd5> in <module>\r\n      1 df = cudf.DataFrame([('a', list(range(20))),\r\n      2 ('b', list(reversed(range(20)))),\r\n----> 3 ('c', list(range(20)))])\r\n      4 print(df)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/cudf/dataframe/dataframe.py in __init__(self, name_series, index)\r\n    108                 name_series = name_series.items()\r\n    109             for k, series in name_series:\r\n--> 110                 self.add_column(k, series, forceindex=index is not None)\r\n    111 \r\n    112     def serialize(self, serialize):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/cudf/dataframe/dataframe.py in add_column(self, name, data, forceindex)\r\n    810         if isinstance(data, GeneratorType):\r\n    811             data = Series(data)\r\n--> 812         series = self._prepare_series_for_add(data, forceindex=forceindex)\r\n    813         series.name = name\r\n    814         self._cols[name] = series\r\n\r\n~/anaconda3/lib/python3.7/site-packages/cudf/dataframe/dataframe.py in _prepare_series_for_add(self, col, forceindex)\r\n    781         The prepared Series object.\r\n    782         \"\"\"\r\n--> 783         self._sanitize_columns(col)\r\n    784         col = self._sanitize_values(col)\r\n    785 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/cudf/dataframe/dataframe.py in _sanitize_columns(self, col)\r\n    734            col values\r\n    735         \"\"\"\r\n--> 736         series = Series(col)\r\n    737         if len(self) == 0 and len(self.columns) > 0 and len(series) > 0:\r\n    738             ind = series.index\r\n\r\n~/anaconda3/lib/python3.7/site-packages/cudf/dataframe/series.py in __init__(self, data, index, name, nan_as_null, dtype)\r\n     79         if not isinstance(data, columnops.TypedColumnBase):\r\n     80             data = columnops.as_column(data, nan_as_null=nan_as_null,\r\n---> 81                                        dtype=dtype)\r\n     82 \r\n     83         if index is not None and not isinstance(index, Index):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/cudf/dataframe/columnops.py in as_column(arbitrary, nan_as_null, dtype)\r\n    424                 data = as_column(\r\n    425                     pa.array(arbitrary, type=pa_type, from_pandas=nan_as_null),\r\n--> 426                     nan_as_null=nan_as_null\r\n    427                 )\r\n    428             except (pa.ArrowInvalid, pa.ArrowTypeError, TypeError):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/cudf/dataframe/columnops.py in as_column(arbitrary, nan_as_null, dtype)\r\n    355             )\r\n    356         else:\r\n--> 357             pamask, padata = buffers_from_pyarrow(arbitrary)\r\n    358             data = numerical.NumericalColumn(\r\n    359                 data=padata,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/cudf/utils/utils.py in buffers_from_pyarrow(pa_arr, dtype)\r\n    152     if buffers[0]:\r\n    153         mask_dev_array = make_mask(len(pa_arr))\r\n--> 154         arrow_dev_array = rmm.to_device(np.array(buffers[0]).view('int8'))\r\n    155         copy_array(arrow_dev_array, mask_dev_array)\r\n    156         pamask = Buffer(\r\n\r\n~/anaconda3/lib/python3.7/site-packages/librmm_cffi/wrapper.py in to_device(self, ary, stream, copy, to)\r\n    239         if to is None:\r\n    240             to = self.device_array_like(ary, stream=stream)\r\n--> 241             to.copy_to_device(ary, stream=stream)\r\n    242             return to\r\n    243         if copy:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/numba/cuda/cudadrv/devices.py in _require_cuda_context(*args, **kws)\r\n    223     def _require_cuda_context(*args, **kws):\r\n    224         with _runtime.ensure_context():\r\n--> 225             return fn(*args, **kws)\r\n    226 \r\n    227     return _require_cuda_context\r\n\r\n~/anaconda3/lib/python3.7/site-packages/numba/cuda/cudadrv/devicearray.py in copy_to_device(self, ary, stream)\r\n    196                 copy=not ary_core.flags['WRITEABLE'])\r\n    197             check_array_compatibility(self_core, ary_core)\r\n--> 198             _driver.host_to_device(self, ary_core, self.alloc_size, stream=stream)\r\n    199 \r\n    200     @devices.require_context\r\n\r\n~/anaconda3/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py in host_to_device(dst, src, size, stream)\r\n   1894         fn = driver.cuMemcpyHtoD\r\n   1895 \r\n-> 1896     fn(device_pointer(dst), host_pointer(src, readonly=True), size, *varargs)\r\n   1897 \r\n   1898 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py in safe_cuda_api_call(*args)\r\n    292             _logger.debug('call driver api: %s', libfn.__name__)\r\n    293             retcode = libfn(*args)\r\n--> 294             self._check_error(fname, retcode)\r\n    295         return safe_cuda_api_call\r\n    296 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py in _check_error(self, fname, retcode)\r\n    327                     _logger.critical(msg, _getpid(), self.pid)\r\n    328                     raise CudaDriverError(\"CUDA initialized before forking\")\r\n--> 329             raise CudaAPIError(retcode, msg)\r\n    330 \r\n    331     def get_device(self, devnum=0):\r\n\r\nCudaAPIError: [1] Call to cuMemcpyHtoD results in CUDA_ERROR_INVALID_VALUE"
      },
      {
        "user": "vrushang25",
        "created_at": "2019-12-19T19:46:25Z",
        "body": "> @vrushang25 what do you mean you're not getting output for the `cudf.read_csv`? Is `M` not a `cudf.DataFrame` after the call in your example above?\r\n\r\nCan you please help me ? is it reason due to the improper installation in anaconda? my versino is 0.6.1+0.gbeb4ef3.dirty\r\n\r\nThank you."
      },
      {
        "user": "vrushang25",
        "created_at": "2019-12-19T20:58:41Z",
        "body": "I solved this problem by just updating my nvidia drivers now it's working very well. \r\n\r\nThank you so much for the support"
      }
    ]
  },
  {
    "number": 3630,
    "title": "[QST] What should be the default delimiters for string splitting functions?",
    "created_at": "2019-12-17T20:23:13Z",
    "closed_at": "2020-05-29T16:46:50Z",
    "labels": [
      "question",
      "strings"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/3630",
    "body": "Various string functions involving splitting strings. There are some subtle differences in the default settings across these functions. We should make sure that we're aligned on the proper defaults per function (which may intentionally vary).\r\n\r\nE.g., should the carriage return and newline delimiters be considered part of the default whitespace delimiter for tokenize (docs indicate whitespace is the default), but not token count.\r\n\r\n```python\r\nimport nvtext\r\nimport nvstrings\r\ns = 'hel\\nlo\\rthe re'\r\ns = nvstrings.to_device(s)\r\nprint(nvtext.tokenize(s, delimiter=' '))\r\nprint(nvtext.tokenize(s))\r\n['hel\\nlo\\rthe', 're']\r\n['hel', 'lo', 'the', 're']\r\n```\r\n\r\n```python\r\nimport nvtext\r\nimport nvstrings\r\ns = 'hel\\nlo\\rthe re'\r\ns = nvstrings.to_device(s)\r\nprint(nvtext.token_count(s, delimiter=' '))\r\nprint(nvtext.token_count(s))\r\n[2]\r\n[2]\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/3630/comments",
    "author": "beckernick",
    "comments": [
      {
        "user": "davidwendt",
        "created_at": "2019-12-19T04:34:10Z",
        "body": "This may be related to #2801 "
      },
      {
        "user": "harrism",
        "created_at": "2020-02-11T03:27:29Z",
        "body": "@beckernick @davidwendt is this discussion still relevant? I see that in #4035 @davidwendt has gone with any white space as the default delimiter."
      },
      {
        "user": "davidwendt",
        "created_at": "2020-02-11T14:07:22Z",
        "body": "I believe this was addressed in #2801 / #3704 and that #4035 will honor the default delimiter as whitespace.\r\nI will leave it up to @beckernick to close this if he is satisfied."
      }
    ]
  },
  {
    "number": 3568,
    "title": "[QST] Usage of `np.frombuffer`",
    "created_at": "2019-12-10T05:51:19Z",
    "closed_at": "2020-01-21T22:58:14Z",
    "labels": [
      "question",
      "Needs Triage"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/3568",
    "body": "**What is your question?**\r\n\r\nCurrently there are several calls to `np.frombuffer`. Is there a reason for this?\r\n\r\nThis is somewhat interesting (to me) as `np.frombuffer` is less friendly to use compared to alternatives like `np.asarray`. The former (`np.frombuffer`) loses information about the data (like type, shape, striding, and so on). As a result users need to feed this information back in (like setting type in the call to `np.frombuffer` or calling `.reshape(...)`). On the other hand the latter (`np.asarray`) preserves all this information. It also works nicely with `memoryview`s. So a typical thing one might do is call `np.asarray(memoryview(...))` on some object to get a NumPy view onto the underlying data. Given this, would it make sense to switch to `np.asarray` or are there other reasons not to?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/3568/comments",
    "author": "jakirkham",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-01-21T22:58:14Z",
        "body": "This is currently only used in tests after recent refactoring. Closing this."
      }
    ]
  },
  {
    "number": 3367,
    "title": "[BUG]cudf-0.10 parse csv file error",
    "created_at": "2019-11-13T07:02:12Z",
    "closed_at": "2020-05-29T16:44:51Z",
    "labels": [
      "question",
      "libcudf"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/3367",
    "body": "I use cudf-0.10 to parse the CSV file. \r\ncode ：\r\nauto &&df = cudf::read_csv(args);\r\n\r\nbut not use：\r\nrmmInitialize();   rmmFinalize();\r\n\r\nOccasionally, the following errors are thrown，and it's hard to reproduce：\r\nterminate called after throwing an instance of 'thrust::system::system_error'\r\nwhat(): parallel_for failed: invalid argument\r\n\r\nHow to solve it?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/3367/comments",
    "author": "guoxiangzhou",
    "comments": [
      {
        "user": "guoxiangzhou",
        "created_at": "2019-11-13T11:19:06Z",
        "body": "now，I modify code as follow，not ok：\r\n\r\n```\r\nrmmInitialize(nullptr);\r\n\r\nauto &&df = cudf::read_csv(args);\r\n.......\r\ndf.destroy();\r\n\r\nrmmFinalize();\r\n```"
      },
      {
        "user": "guoxiangzhou",
        "created_at": "2019-11-14T02:36:01Z",
        "body": "anybody here?"
      },
      {
        "user": "guoxiangzhou",
        "created_at": "2019-11-14T10:16:37Z",
        "body": "i mistake，don't use &&df"
      },
      {
        "user": "guoxiangzhou",
        "created_at": "2019-11-15T07:36:59Z",
        "body": "The problem remains"
      },
      {
        "user": "kkraus14",
        "created_at": "2020-05-29T16:44:51Z",
        "body": "Closing as this is stale. If this is still an issue with the latest 0.14 please open a new issue."
      }
    ]
  },
  {
    "number": 3315,
    "title": "[QST]why apply_rows works so different compare to the apply in pandas dataframe",
    "created_at": "2019-11-07T09:10:28Z",
    "closed_at": "2020-01-14T05:59:15Z",
    "labels": [
      "question",
      "Needs Triage"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/3315",
    "body": "The way to call apply_rows method is so bad! why not make is the same as pandas dataframe apply?\r\nfor example:\r\n>>> in1 = df['in1']\r\n>>> in2 = df['in2']\r\n>>> in3 = df['in3']\r\n>>> def kernel(in1, in2, in3, out1, out2, kwarg1, kwarg2):\r\n...     for i, (x, y, z) in enumerate(zip(in1, in2, in3)):\r\n...         out1[i] = kwarg2 * x - kwarg1 * y\r\n...         out2[i] = y - kwarg1 * z\r\n\r\nCall ``.apply_rows`` with the name of the input columns, the name and\r\ndtype of the output columns, and, optionally, a dict of extra\r\narguments.\r\n\r\n>>> df.apply_rows(kernel,\r\n...               incols=['in1', 'in2', 'in3'],\r\n...               outcols=dict(out1=np.float64, out2=np.float64),\r\n...               kwargs=dict(kwarg1=3, kwarg2=4))\r\n\r\nThe for loop in kernel function looks so redundancy and feels uncomfortable！\r\n\r\nIt's looks like we should use kernel function like this: kernel(df.in1, df.in2, df.in3, df.out1, df.out2, kwarg1, kwarg2) but not the appy_rows!\r\n\r\nwhy do not make it the same as pandas dataframe apply!",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/3315/comments",
    "author": "naturesphere",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-01-14T05:59:08Z",
        "body": "@naturesphere we are due for a large refactor of UDFs and apply functions. That being said, because we execute vectorized on the GPU via the Numba library as opposed to via Python iteration, we can only support what Numba can compile. This unfortunately limits what functionality we can support in UDFs but makes our UDFs orders of magnitude faster than Pandas."
      }
    ]
  },
  {
    "number": 2958,
    "title": "cudf flatten API [QST]",
    "created_at": "2019-10-03T20:42:50Z",
    "closed_at": "2019-12-12T20:52:24Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/2958",
    "body": "**What is your question?**\r\nDoes CUDF  support an api to flat the dataframe like pd.values.flatten() ? Thanks you.\r\n```python\r\nIn [40]: df_matchColor1                                       \r\nOut[40]: \r\n      color  key_index\r\n0   #ffffd9          0\r\n1   #7fcdbb          1\r\n2   #41b6c4          2\r\n3   #1d91c0          3\r\n4   #ffffd9          4\r\n5   #7fcdbb          5\r\n6   #41b6c4          6\r\n7   #1d91c0          7\r\n8   #ffffd9          8\r\n9   #7fcdbb          9\r\n10  #41b6c4         10\r\n11  #1d91c0         11\r\n\r\nIn [53]: df_matchColor1.to_pandas().values.flatten()                                 \r\nOut[53]: \r\narray(['#ffffd9', 0, '#7fcdbb', 1, '#41b6c4', 2, '#1d91c0', 3, '#ffffd9',\r\n       4, '#7fcdbb', 5, '#41b6c4', 6, '#1d91c0', 7, '#ffffd9', 8,\r\n       '#7fcdbb', 9, '#41b6c4', 10, '#1d91c0', 11], dtype=object)\r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/2958/comments",
    "author": "MikeChenfu",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-10-10T22:30:10Z",
        "body": "@MikeChenfu in your use case you're utilizing Pandas/Numpy's ability to fall back to using Python objects to created a mixed dtype array. This is extremely detrimental to performance and won't be supported in cuDF for mixed dtypes.\r\n\r\nOtherwise, you should be able to use the same code as `df.values.flatten()` that will go from cuDF --> cuPy --> flattened cuPy array."
      },
      {
        "user": "MikeChenfu",
        "created_at": "2019-10-14T15:43:22Z",
        "body": "Thanks @kkraus14. It works right now. Thanks for your kind help.:)   "
      },
      {
        "user": "Mirror-HHf",
        "created_at": "2019-11-01T13:38:53Z",
        "body": "Hello, I saw that you were learning NSGA2 algorithm before, could you give me a contact information opportunity to communicate with you?"
      },
      {
        "user": "randerzander",
        "created_at": "2019-12-12T20:52:24Z",
        "body": "I believe @MikeChenfu solved his problem another way"
      }
    ]
  },
  {
    "number": 2880,
    "title": "Does Dask Distributed or dask_cudf supports Apache Arrow Flight batch stream for IO?",
    "created_at": "2019-09-27T12:54:34Z",
    "closed_at": "2019-10-25T12:50:47Z",
    "labels": [
      "question",
      "Python",
      "dask"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/2880",
    "body": "**What is your question?**\r\nI am sharing my data in arrow format through arrow flight as stream; and need to consume it from dask_cudf in GPU.\r\n\r\nWhat i tried \r\n - read arrow batch stream as a table.\r\n - Create cudf\r\n - pass it to dask_cudf.\r\n \r\nThis works for small data, but reading full stream as a single arrow table into cudf may cause out of memory error.\r\n\r\nIs there any better way out of the box in dask distributed or in dask_cudf to read/write to Arrow Flight Buffer?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/2880/comments",
    "author": "vnkesarwani",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-10-10T22:50:25Z",
        "body": "@vnkesarwani is there a reason you couldn't read the Arrow batch stream directly on a worker so that you don't have to do the local --> distributed data movement? I.E. using delayed function(s) to read the Arrow flight source on the workers and create the arrow tables and pass to cudf in an embarassingly parallel manner? Once you have the cudf DataFrames created on the workers it's a simple `dask_cudf.from_delayed(...)` call (zero-op) to create a dask dataframe from them."
      },
      {
        "user": "vnkesarwani",
        "created_at": "2019-10-25T12:50:47Z",
        "body": "Thanks @kkraus14. \r\n\r\nI am able to run in distributed mode."
      }
    ]
  },
  {
    "number": 2871,
    "title": "[QST] Out of Memory when counting number of rows using multiple parquet fles",
    "created_at": "2019-09-26T05:58:12Z",
    "closed_at": "2019-09-27T20:50:12Z",
    "labels": [
      "question",
      "Needs Triage"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/2871",
    "body": "I have 500 parquet files. Each parquet file is SNAPPY compressed and is around 200MB of size. Parquet consists of three columns. Each column is a string.\r\n\r\nI am trying to find the number of rows using the following code:\r\n\r\n```python\r\ncluster = LocalCUDACluster(ip=sched_ip, n_workers=num_of_gpus)\r\nclient = Client(cluster)\r\ndf = dask_cudf.read_parquet(path_to_local, columns=['col1','col2'])\r\nrows, cols = df.shape\r\nnum_rows = rows.compute()\r\n```\r\n\r\nIt throws a Runtime Exception: ```Exception: RuntimeError('parallel_for failed: out of memory')```\r\n\r\nI am using an EC2 instance (p3.8xlarge) with following configuration:\r\n1) **RAM**: 244GB\r\n2) **vCPU**: 32\r\n3) **GPU RAM**: 64GB\r\n4) **GPUs**: 4 Tesla V100\r\n\r\nIs this expected behaviour? Or are there any workarounds?\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/2871/comments",
    "author": "chochim",
    "comments": [
      {
        "user": "OlivierNV",
        "created_at": "2019-09-27T00:40:59Z",
        "body": "read_parquet_metadata() is probably what you want (no need to actually read column data)"
      },
      {
        "user": "chochim",
        "created_at": "2019-09-27T20:50:12Z",
        "body": "That makes sense. Thanks"
      }
    ]
  },
  {
    "number": 2478,
    "title": "[QST] Memory Reporting",
    "created_at": "2019-08-06T21:18:00Z",
    "closed_at": "2020-05-29T16:37:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/2478",
    "body": "I am seeing strange behavior with cudf memory reporting. \r\n\r\nIf I create a dataframe with 2**26 ints I should expect ~.5GB to be allocated on the GPU + some amount for cuda context.  Instead, I see 1.8GB:\r\n\r\n```python\r\nIn [1]: import cudf\r\n\r\nIn [2]: from numba import cuda\r\n\r\nIn [3]: free_before = cuda.current_context().get_memory_info()[0]\r\n\r\nIn [4]: cdf = cudf.DataFrame({'a': range(2**26)})\r\n\r\nIn [5]: free_after = cuda.current_context().get_memory_info()[0]\r\n\r\nIn [6]: free_before - free_after\r\nOut[6]: 1541406720\r\n\r\nIn [7]: cdf.__sizeof__()\r\nOut[7]: 536870944\r\n```\r\n\r\nHowever, on subsequent constructions the \"appropriate\" amount of memory is allocated:\r\n\r\n```python\r\nIn [17]: free_before = cuda.current_context().get_memory_info()[0]\r\n\r\nIn [18]: cdf3 = cudf.DataFrame({'a': range(2**26)})\r\nfree_after =\r\nIn [19]: free_after = cuda.current_context().get_memory_info()[0]\r\n\r\nIn [20]: free_before - free_after\r\nOut[20]: 536870912\r\n```\r\n\r\nIs rmm building a larger cuda context than expected ?  Would this at all impact serialization in that the full context is not being serialized ?  I am looking at this through a UCX debugging lens as well (grasping at straws for why things aren't working)\r\n\r\ncc @kkraus14 @pentschev ",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/2478/comments",
    "author": "quasiben",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-08-15T22:50:26Z",
        "body": "@quasiben there's a few things happening here:\r\nFirst, the CUDA context is created on the dataframe call as opposed to on import (will change in the future) and takes ~500MB. We use numba to create this and you can recreate the behavior using the following:\r\n```python\r\nfrom numba import cuda\r\nctx = cuda.current_context()\r\nfree, total = ctx..get_memory_info()\r\nprint(total - free)\r\n# 332005376 for me\r\n```\r\n\r\nThen you can run your cudf operation afterwards and you should see the roughly ~500MB consumed:\r\n```\r\nimport cudf\r\ncdf = cudf.DataFrame({'a': range(2**26)})\r\nfree, total = ctx..get_memory_info()\r\nprint(total - free)\r\n# 1005191168 for me\r\n```\r\n\r\nIf there's additional processes running on the same GPU it would affect the results as numba will be creating the driver API as opposed to the runtime API in this situation as far as I know."
      },
      {
        "user": "kkraus14",
        "created_at": "2019-08-15T22:52:18Z",
        "body": "Additionally, I've seen the CUDA context vary in size but I'm not sure exactly what the semantics related to it are."
      },
      {
        "user": "kkraus14",
        "created_at": "2020-05-29T16:37:22Z",
        "body": "Closing as this isn't an issue."
      }
    ]
  },
  {
    "number": 2375,
    "title": "[QST]dataframe rolling regression",
    "created_at": "2019-07-23T04:45:10Z",
    "closed_at": "2020-05-29T16:41:47Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/2375",
    "body": "**What is your question?**\r\nI'm thinking if there is a good way to replace my existing CPU-based DF rolling regression code by cudf.\r\nMy code is as below. I'm trying to think the apply_* methods as the possible replacement. However, seems those methods are targeting to the way that each GPU thread handle each row. My goal is how to let the GPU thread handle a frame. Can anyone candle a light of this?\r\n\r\n# function def\r\n``` python\r\ndef best_fit2(dfr):\r\n    y = dfr.Px.values\r\n    x = dfr.time_tick.values.reshape(-1,1)\r\n    last_seq = dfr.seq.tail(1).values[0]\r\n    linear = sklearn.LinearRegression()\r\n    linear.fit(x,y)\r\n    p_slope, p_intercept = linear.coef_[0], linear.intercept_\r\n    p_angle = np.rad2deg(np.arctan2(price_slope, 1))    \r\n    yy = dfr.o_delta.values\r\n    linear.fit(x,yy)\r\n    o_slope, oi_intercept = linear.coef_[0], linear.intercept_\r\n    o_angle = np.rad2deg(np.arctan2(oi_slope, 1))\r\n    return(last_seq, p_slope, p_angle, p_intercept, o_slope, o_angle, o_intercept)\r\n\r\n# code\r\ndef RegressionRoll(df, win):\r\n    end = df.shape[0]+1\r\n    win = win\r\n    rng = np.arange(start = win, stop = end, step = 1)\r\n    \r\n    // Subset and store dataframes\r\n    frames = {}\r\n    n = 1\r\n\r\n    //Rolling slice the dataframe, and keep them each in a dict\r\n    for i in rng:\r\n        df_temp = df.iloc[:i].tail(win)\r\n        newname = 'df' + str(n)\r\n        frames.update({newname: df_temp})\r\n        n += 1\r\n    print(f'n: {n}')\r\n    \r\n    // distribute the invidual slice to cpu-core for furher processing\r\n    num_cores = multiprocessing.cpu_count() - 5\r\n    inputs = tqdm(list(frames.values()))\r\n    processed_list = Parallel(n_jobs=num_cores)(delayed(best_fit2)(i) for i in inputs)\r\n    \r\n   return processed_list\r\n```\r\n# This is the caller\r\n``` python\r\nrtn = RegressionRoll(df=df_obs, win=120);\r\ndf_res = pd.DataFrame(rtn, columns =['seq', 'price_slope', 'price_angle', 'price_intercept','oi_slope', 'oi_angle', 'oi_intercept'])\r\ndf_res.head(30)\r\n````\r\n# result\r\nseq     |      p_slope | p_angle     | p_intercept     | o_slope     | o_angle       | o_intercept\r\n15119 | 0.0065768 | 0.3768201 | 2139.0836777 | 0.2529342 | 14.1943636 | -32461.5495868\r\n15120 | 0.0059744 | 0.3423023 | 2139.1153581 | 0.2799500 | 15.6395897 | -32462.9903581\r\n15121 | 0.0053754 | 0.3079832 | 2139.1468320 | 0.3068269 | 17.0574202 | -32464.4228650\r\n15122 | 0.0049865 | 0.2857006 | 2139.1699725 | 0.3343913 | 18.4894856 | -32465.8796143\r\n15123 | 0.0045975 | 0.2634179 | 2139.1931129 | 0.3626293 | 19.9321308 | -32467.3597796\r\n15124 | 0.0040020 | 0.2292973 | 2139.2243802 | 0.3907007 | 21.3406244 | -32468.8300275\r\n15125 | 0.0034100 | 0.1953755 | 2139.2554408 | 0.4194319 | 22.7547314 | -32470.3228650\r\n15126 | 0.0028214 | 0.1616525 | 2139.2862948 | 0.4479825 | 24.1315445 | -32471.8049587\r\n15127 | 0.0022363 | 0.1281284 | 2139.3169421 | 0.4654282 | 24.9585945 | -32472.4096419\r\n15128 | 0.0016546 | 0.0948032 | 2139.3473829 | 0.4825127 | 25.7578983 | -32472.9928375\r\n15129 | 0.0010765 | 0.0616768 | 2139.3776171 | 0.4992361 | 26.5300240 | -32473.5545455\r\n15130 | 0.0002917 | 0.0167124 | 2139.4243113 | 0.5155983 | 27.2755551 | -32474.0947658\r\n15131 | 0.0001302 | 0.0074609 | 2139.4380854 | 0.5332523 | 28.0688725 | -32474.678512\r\n\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/2375/comments",
    "author": "richwu",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-05-29T16:41:47Z",
        "body": "Stale closing."
      }
    ]
  },
  {
    "number": 1972,
    "title": "dask_cudf.read_csv memory usage in AWS Sagemaker",
    "created_at": "2019-06-10T16:09:22Z",
    "closed_at": "2019-09-30T17:11:59Z",
    "labels": [
      "feature request",
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1972",
    "body": "Are there any plans to work with AWS to enable RAPIDS to be available out of the box within Sagemaker?\r\n\r\nIt would be useful to be able to start a Sagemaker instance with GPUs and then use RAPIDS the same way you can already use Dask for example.",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1972/comments",
    "author": "ivenzor",
    "comments": [
      {
        "user": "quasiben",
        "created_at": "2019-06-10T18:02:20Z",
        "body": "I believe when you launch sagemaker it comes with Anaconda installed so you should be able to install the rapids packages through conda:\r\n\r\n```\r\nconda install -c defaults -c nvidia -c rapidsai -c pytorch -c numba -c conda-forge \\\r\ncudf=0.7 cuml=0.7 python=3.6 cudatoolkit=9.2\r\n```"
      },
      {
        "user": "nicolasburtey",
        "created_at": "2019-06-11T15:37:19Z",
        "body": "Just tried it. I can confirm this works."
      },
      {
        "user": "ivenzor",
        "created_at": "2019-06-11T16:28:59Z",
        "body": "Many thanks for your quick reply.  I will test this myself today."
      },
      {
        "user": "nicolasburtey",
        "created_at": "2019-06-13T04:38:22Z",
        "body": "Actually, that doesn't work. I have cudaErrorNoKernelImageForDevice error.\r\nI guess this is because the driver is set at Driver Version: 418.40.04\r\nNot sure if there is an easy way to update the driver on Sagemaker."
      },
      {
        "user": "ivenzor",
        "created_at": "2019-06-17T20:59:40Z",
        "body": "As far as I know the driver version requirement for Cuda 10 is 418.40.04, so it should work with the driver version you mentioned.\r\n\r\nI did some testing and it's working for me after conda install.\r\n\r\n_import cudf\r\ndf = cudf.DataFrame()\r\ndf['key'] = [0, 1, 2, 3, 4]\r\ndf['val'] = [float(i + 10) for i in range(5)]  # insert column\r\nprint(df)\r\n\r\n   key   val\r\n0    0  10.0\r\n1    1  11.0\r\n2    2  12.0\r\n3    3  13.0\r\n4    4  14.0_\r\n\r\nI have some issues to read csv files from s3 and also the pandas encoding parameter is missing. But I will do more testing tomorrow to try to solve these issues."
      },
      {
        "user": "ivenzor",
        "created_at": "2019-06-17T21:07:35Z",
        "body": "Something is wrong with cuml:\r\n\r\nterminate called after throwing an instance of 'MLCommon::Exception'\r\n  what():  Exception occured! file=/conda/envs/gdf/conda-bld/libcuml_1557509586353/work/cuML/external/ml-prims/src/linalg/cutlass_wrappers.h line=380: FAIL: call='cudaPeekAtLastError()'. Reason:no kernel image is available for execution on the device"
      },
      {
        "user": "ivenzor",
        "created_at": "2019-06-18T14:30:52Z",
        "body": "Perhaps this issue is due to Cuda 10.1 being used by Sagemaker instead Cuda 10.0?"
      },
      {
        "user": "jrhemstad",
        "created_at": "2019-06-18T15:31:45Z",
        "body": "> Perhaps this issue is due to Cuda 10.1 being used by Sagemaker instead Cuda 10.0?\r\n\r\nWhat GPU are you running on? \r\n\r\n```\r\n Reason:no kernel image is available for execution on the device\r\n```\r\n\r\nThis error means you're trying to run on a GPU that doesn't support the architecture the binary was compiled for. "
      },
      {
        "user": "ivenzor",
        "created_at": "2019-06-18T15:51:48Z",
        "body": "K80"
      },
      {
        "user": "ivenzor",
        "created_at": "2019-06-18T16:36:50Z",
        "body": "I tested again with a V100 and it seems it's working now.\r\n\r\n_import cuml\r\ndf_float = cudf.DataFrame()\r\ndf_float['0'] = [1.0, 2.0, 5.0]\r\ndf_float['1'] = [4.0, 2.0, 1.0]\r\ndf_float['2'] = [4.0, 2.0, 1.0]\r\ndbscan_float = cuml.DBSCAN(eps=1.0, min_samples=1)\r\ndbscan_float.fit(df_float)\r\nprint(dbscan_float.labels_)\r\n\r\n0    0\r\n1    1\r\n2    2\r\ndtype: int32_\r\n\r\n"
      },
      {
        "user": "jrhemstad",
        "created_at": "2019-06-18T16:52:57Z",
        "body": "A K80 is a Kepler GPU. The oldest architecture RAPIDS supports is Pascal. "
      },
      {
        "user": "ivenzor",
        "created_at": "2019-06-18T16:56:45Z",
        "body": "Thanks @jrhemstad, I saw the >=Pascal requirement only after I tested again with a V100 :)\r\n\r\nAltough the \"no kernel image is available for execution on the device\" error is solved, I still can't read a csv file on s3. It works when using pandas read_csv, but cudf.read_csv fails with the following exception:\r\nFileNotFoundError\r\n\r\nIs there any special procedure to read csv files on s3?\r\n\r\n"
      },
      {
        "user": "ivenzor",
        "created_at": "2019-06-19T14:34:02Z",
        "body": "I will try with dask_cudf.read_csv "
      },
      {
        "user": "ivenzor",
        "created_at": "2019-06-28T21:57:33Z",
        "body": "Currently I can use dask_cudf.read_csv to read s3 files.\r\n\r\nHowever if I specify the dtypes parameter no matter what type I specify for the columns I always got:\r\nGDFError: b'GDF_UNSUPPORTED_DTYPE'"
      },
      {
        "user": "ivenzor",
        "created_at": "2019-07-10T16:19:31Z",
        "body": "I'm currently using dask_cudf to read some gzip files using something similar to this:\r\n\r\ndata=dask_cudf.read_csv(list_of_files, compression='gzip',header=None, sep='~', names=[...], dtype=[...])\r\n\r\nThe gzip files are about 150MB each, and about 2.5GB each in non-compressed form. \r\nHowever, if I read more than 2 files I got a memory error while using a 16GB Tesla V100 GPU.\r\n\r\nMaybe I'm missing something, but even in raw format, the files are way less than 16GB. And it's my understanding that inside the GPU they are in a columnar apache arrow format and in that case the data should be way smaller.\r\n\r\nIt's not clear to me why I'm facing a memory issue in this scenario."
      }
    ]
  },
  {
    "number": 1836,
    "title": "[QST] Does the Parquet_reader support List columns",
    "created_at": "2019-05-23T11:08:59Z",
    "closed_at": "2019-07-03T23:35:29Z",
    "labels": [
      "question",
      "cuIO"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1836",
    "body": "I'm trying to use the cuDF project to decode parquet files. I've been testing with various simple files to see the capabilities of the library. It seems to me that there are issues with decoding Parquet files that have LIST columns. E.g a simple file with 5 rows of 4 columns one of which is a list/array column should look something like (Apache Spark or PyArrow output):\r\n```\r\n+-------+------+--------------------+---------+\r\n|byteCol|intCol|primArray           |longCol  |\r\n+-------+------+--------------------+---------+\r\n|8      |320   |[10, 11, 12]        |73763636 |\r\n|12     |640   |[13, 14, 15, 16]    |173763636|\r\n|16     |1280  |[17, 18]            |273763636|\r\n|18     |2560  |[19, 20, 21, 22, 23]|373763636|\r\n|20     |5120  |[24, 25]            |473763636|\r\n+-------+------+--------------------+---------+\r\n```\r\nhowever the output from libcudf (using cudf python wrapper in the Docker image) looks like below \r\n```\r\n   byteCol  intCol  primArray.list.element    longCol\r\n0        8     320                      10   73763636\r\n1       12     640                          173763636\r\n2       16    1280                      11  273763636\r\n3       18    2560                          373763636\r\n4       20    5120                          473763636\r\n```\r\nRunning my own C++ code example with the debug turned on in parquet_reader.cu gives the following output. \r\n```\r\n[+] Metadata:\r\n version = 1\r\n created_by = \"parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)\"\r\n schema (7 entries):\r\n  [0] type=0, name=\"spark_schema\", num_children=4, rep_type=0, max_def_lvl=0, max_rep_lvl=0\r\n  [1] type=1, name=\"byteCol\", num_children=0, rep_type=0, max_def_lvl=0, max_rep_lvl=0\r\n  [2] type=1, name=\"intCol\", num_children=0, rep_type=0, max_def_lvl=0, max_rep_lvl=0\r\n  [3] type=0, name=\"primArray\", num_children=1, rep_type=1, max_def_lvl=1, max_rep_lvl=0\r\n  [4] type=0, name=\"list\", num_children=1, rep_type=2, max_def_lvl=2, max_rep_lvl=1\r\n  [5] type=1, name=\"element\", num_children=0, rep_type=0, max_def_lvl=2, max_rep_lvl=1\r\n  [6] type=2, name=\"longCol\", num_children=0, rep_type=0, max_def_lvl=0, max_rep_lvl=0\r\n num rows = 5\r\n num row groups = 1\r\n num columns = 4\r\n[+] Selected row groups: 1\r\n[+] Selected columns: 1\r\n[+] Selected skip_rows: 0 num_rows: 5\r\n  0: name=primArray.list.element size=5 type=3 data=0 valid=0\r\n[+] Column Chunk Description\r\n  2: primArray.list.element start_row=0, num_rows=5, codec=1, num_values=16\r\n     total_compressed_size=127 total_uncompressed_size=124\r\n     schema_idx=5, type=1, type_width=0, max_def_level=2, max_rep_level=1\r\n     data_page_offset=138, index_page_offset=0, dict_page_offset=0\r\n[+] Chunk Information\r\n  0: comp_data=140096592413184, comp_size=127, num_values=16\r\n     start_row=0 num_rows=5 max_def_level=2 max_rep_level=1\r\n     data_type=1 def_level_bits=2 rep_level_bits=1\r\n     num_data_pages=1 num_dict_pages=0 max_num_pages=0\r\n[+] Page Header Information\r\n  0: comp_size=80, uncomp_size=77, num_values=16, chunk_row=0, num_rows=16\r\n     chunk_idx=0, flags=0, encoding=0, def_level=3 rep_level=3, valid_count=0\r\n[+] Compression\r\n Total compressed size: 77\r\n Number of compressed pages: 1\r\n  gzip:    0 \r\n  snappy: 1\r\n[+] Page Data Information\r\n  0: valid_count=2/5\r\nValue : 10 , valid: true\r\nValue : 27268353 , valid: false\r\nValue : 11 , valid: true\r\nValue : 352720128 , valid: false\r\nValue : 68688902 , valid: false\r\n``` \r\nLooking at the code it seems the metadata for the column is decoded correctly (sees there should be 16 values) but the data gets decoded as just 5 values.  Am I missing something here or is this a bug??",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1836/comments",
    "author": "darylparker",
    "comments": [
      {
        "user": "ayushdg",
        "created_at": "2019-05-23T17:03:19Z",
        "body": "The io team might be able to comment on whether the metadata decoding logic is a bug or not. \r\nBut as of now my understanding is that cudf does not support nested columns and does not support the type of a column to be a list. Supported types currently include numeric types, strings, bools, datetime and categorical."
      },
      {
        "user": "OlivierNV",
        "created_at": "2019-05-23T20:32:51Z",
        "body": "@ayushdg is correct: lists, maps (any type with multiple values per record) are not currently supported (this is visible in the metadata dump as max_rep_lvl != 0)."
      },
      {
        "user": "darylparker",
        "created_at": "2019-05-24T08:07:24Z",
        "body": "Thanks for the clarification. Are there any plans for adding support for these types of repeated columns? I didn't find anything specific in the issues/features. Has anyone already started looking at this?"
      },
      {
        "user": "randerzander",
        "created_at": "2019-05-24T16:12:52Z",
        "body": "Hi @darylparker can you provide some more details about what you want to do with nested columns?\r\n\r\nWe're discussing how to approach this problem, but it wont be for a few releases.\r\n\r\nDo you need to operate on individual values in the nested list, or just be able to read them from files generated by an upstream process?\r\n\r\nSupposing you were able to read them into a delimited string, would that be a useful stopgap for you? If so, and you needed to access individual elements, you could use string split functions to do so."
      }
    ]
  },
  {
    "number": 1832,
    "title": "Error while trying to find the linear regression for a given data set",
    "created_at": "2019-05-23T06:35:28Z",
    "closed_at": "2019-05-30T14:56:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1832",
    "body": "I am pretty new to using the RAPIDS library. I am trying to find the class of breast cancer(malignant or benign) using linear regression for a dataset that I got from the internet. \r\n\r\ndfcu = cudf.read_csv('breast-cancer-wisconsin.data.txt', delimiter=',',names = ['id', 'clump_thickness' , 'unif_cell_size' , 'unif_cell_shape' , 'marg_adhesion', 'single_eputh_cell_size' , 'bare_nuclei', 'bland_chrom', 'norm_nucleoli', 'mitoses', 'class'])\r\n\r\nXcu = dfcu.drop(['class'],1)\r\nycu = dfcu['class']\r\n\r\nXcu_train, Xcu_test, ycu_train, ycu_test = sklearn.model_selection.train_test_split(Xcu,ycu,test_size=0.2)\r\n\r\n\r\ncuols = cuLinearRegression(fit_intercept=True,\r\n                  normalize=True,\r\n                  algorithm='eig')\r\n\r\n_cuols.fit(Xcu_train, ycu_train)_\r\n\r\n**THE ERROR I GET WHILE TRYING TO RUN THE ABOVE LINE:**\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-108-469c8d9812ee> in <module>()\r\n----> 1 cuols.fit(Xcu_train, ycu_train)\r\n\r\ncuml/linear_model/linear_regression.pyx in cuml.linear_model.linear_regression.LinearRegression.fit()\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/site-packages/cudf/dataframe/column.py in to_gpu_array(self, fillna)\r\n    288         output size could be smaller.\r\n    289         \"\"\"\r\n--> 290         return self.to_dense_buffer(fillna=fillna).to_gpu_array()\r\n    291 \r\n    292     def to_array(self, fillna=None):\r\n\r\nAttributeError: 'nvstrings' object has no attribute 'to_gpu_array'\r\n\r\n\r\nAny idea on where I might be going wrong?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1832/comments",
    "author": "see-k-er",
    "comments": [
      {
        "user": "thomcom",
        "created_at": "2019-05-23T14:39:27Z",
        "body": "It looks like you need to convert the string columns in your dataset to numerical or categorical columns - I suspect the issue here is that `nvstrings` can't have a linear regression computed on it. That doesn't make sense. :)"
      },
      {
        "user": "thomcom",
        "created_at": "2019-05-23T14:39:57Z",
        "body": "This is a great question to post to StackOverflow, try there?"
      },
      {
        "user": "see-k-er",
        "created_at": "2019-05-24T04:24:11Z",
        "body": "Thanks for your input. I'll try it out :)"
      },
      {
        "user": "kkraus14",
        "created_at": "2019-05-30T14:56:19Z",
        "body": "Going to close this issue as it's been triaged that nvstrings can't be used for linear regression."
      }
    ]
  },
  {
    "number": 1775,
    "title": "[DOC] Monitoring and debugging tool for rapids.io ",
    "created_at": "2019-05-17T13:24:16Z",
    "closed_at": "2019-07-03T23:27:44Z",
    "labels": [
      "question",
      "doc"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1775",
    "body": "\r\n\r\n## Report needed documentation\r\n\r\n**Monitoring and debugging tool for rapids.io**\r\nHow do you monitor and debug cudf? \r\n\r\n1- What function call is executing @present in GPU.\r\n2- Time taken for data load, execution time, memory allocated/used, time cudf library loading in GPU or context creation.\r\n3- Is there any gpu job monitoring tool which can provide this kind of details?\r\n4- Which tool is suited for running cudf ETL in distributed mode.\r\n5- How do we Accumulate resultant data in distributed mode and return.\r\n6- is there any tool which can convert sql to cudf dataframe directly?\r\n\r\nI am exploring these options to create an end to end flow. Let me know if any of you can give a pointer here.",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1775/comments",
    "author": "vnkesarwani",
    "comments": [
      {
        "user": "vnkesarwani",
        "created_at": "2019-06-19T14:10:54Z",
        "body": "Any Comment on that."
      },
      {
        "user": "kkraus14",
        "created_at": "2019-07-03T23:27:44Z",
        "body": "Responded on the google group request so closing this and will continue discussion there."
      }
    ]
  },
  {
    "number": 1429,
    "title": "[QST]How to display more rows by setting?",
    "created_at": "2019-04-15T09:25:44Z",
    "closed_at": "2019-07-03T23:12:21Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1429",
    "body": "**What is your question?**\r\nI tried set_options as pandas to show more rows?\r\nBut it's not working. Then searching some setting function to do it.\r\n`from cudf import settings\r\nsettings.set_options(nrows=50)`\r\nHow should set it?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1429/comments",
    "author": "yoyitowang",
    "comments": [
      {
        "user": "j-ieong",
        "created_at": "2019-04-15T17:55:45Z",
        "body": "cuDF accesses the print options under a `formatting` dict.\r\n\r\nIn the call to `set_options`, pass a `formatting` dict with the desired key-value pairs. Additionally, that call returns the settings object in the local context so your print should be within its scope. Ex:\r\n\r\n```python\r\nfrom cudf.settings import set_options\r\n\r\ndef test_print():\r\n    df = ...\r\n    with set_options(formatting={'nrows': 50}):\r\n        print(df)\r\n```"
      }
    ]
  },
  {
    "number": 1406,
    "title": "[QST] Regarding the usage of CMAKE_CXX11_ABI flag in cudf/cpp/CMakeLists.txt ",
    "created_at": "2019-04-11T12:46:24Z",
    "closed_at": "2019-04-11T18:10:33Z",
    "labels": [
      "question",
      "CMake"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1406",
    "body": "**What is your question?**\r\n1. I can see that  `CMAKE_CXX11_ABI` flag  set by default without explicitly setting it up for `GNU compiler`. \r\n[ without setting `-DCMAKE_CXX11_ABI` in `CMAKE_COMMON_VARIABLES` ] \r\n \r\n``` \r\nif(CMAKE_COMPILER_IS_GNUCXX)\r\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\r\n\r\n    option(CMAKE_CXX11_ABI \"Enable the GLIBCXX11 ABI\" ON)\r\n    if(CMAKE_CXX11_ABI)\r\n        message(STATUS \"CUDF: Enabling the GLIBCXX11 ABI\")\r\n```\r\nBuild log snippet: \r\n``` \r\n-- CUDF: Enabling the GLIBCXX11 ABI\r\n   \r\n-std=c++14 -x cu -c $SRC_DIR/cpp/src/comms/ipc/ipc.cu -o CMakeFiles/cudf.dir/src/comms/ipc/ipc.cu.o\r\n```\r\nDoes that mean for a default case of `GNU compiler` cuDF supports `CXX11_ABI`  and `CMAKE_CXX_STANDARD` and `CMAKE_CUDA_STANDARD` are set to `14` ? \r\n\r\n2.  `set(CMAKE_CXX_STANDARD 14) set(CMAKE_CUDA_STANDARD 14)` are these the mandatory requirement for latest `cuDF` version to build. ? \r\n\r\nThanks in advance for your response ! ",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1406/comments",
    "author": "pradghos",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-04-11T17:46:48Z",
        "body": "@pradghos Yes, the default case for `GNU compiler` is to use the new ABI, denoted by `CXX11_ABI`. A user can change back to the old ABI by passing `-DCMAKE_CXX11_ABI=OFF` if they want.\r\n\r\nThe library uses a bunch of C++14 features, so `STANDARD 14` is required and is not an option."
      },
      {
        "user": "pradghos",
        "created_at": "2019-04-11T18:07:23Z",
        "body": "@kkraus14 Thanks for your response ! "
      }
    ]
  },
  {
    "number": 1299,
    "title": "[FEA] Ensure that we can get a columns libgdf dtype from python",
    "created_at": "2019-03-26T15:58:42Z",
    "closed_at": "2019-07-03T22:02:39Z",
    "labels": [
      "feature request",
      "question",
      "proposal"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1299",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nI wish the python side of CUDF had a representation of libcudf dtype before it gets sent over to  libcudf. If you are someone like us where you provide users interaction from the python side but may send over a message to cudf not through cython but rather over the network. So we generate ipc handles for gpu data, put the handles and the rest of the column metadata into flatbuffers and ship that around. Right now the dtype is a pandas dtype before it goes to libcudf and gets converted to libcudf dtype when its going in.\r\n\r\n**Describe the solution you'd like**\r\nI want to know the libgdf dtype of a column at all points in its lifecycle regardless of wheter we are looking at it in python or in c++. Because we need to keep compatibility with pandas I would recommend having a panda_dtype and libgdf_dtype so people can appropriately choose the one they are using.\r\n\r\n**Describe alternatives you've considered**\r\nWe are currently adding hacky logic that basically checks to see if something is a string column and if so setting its type to GDF_STRING on serialization.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1299/comments",
    "author": "felipeblazing",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-03-27T00:28:40Z",
        "body": "@felipeblazing there's a dictionary in `cudf.bindings.cudf_cpp.dtypes` that you can use to convert from the Pandas dtype to a libcudf dtype. If we exposed a convenience function to handle that dictionary lookup for you and return the proper enum would that suffice for your needs?"
      },
      {
        "user": "kkraus14",
        "created_at": "2019-07-03T22:02:39Z",
        "body": "Not an issue."
      }
    ]
  },
  {
    "number": 1121,
    "title": "[QST] How to port this piece of code",
    "created_at": "2019-03-06T12:36:45Z",
    "closed_at": "2019-07-03T22:51:17Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1121",
    "body": "**What is your question?**\r\nHi,\r\n\r\nI am porting a few notebooks to RAPIDS, and I have not been able to find a workaround for the following code templates.\r\n\r\nExample 1:\r\n```\r\nfile_agg = file_df_tmp.groupby(['user','timestep'])\\\r\n                        .agg(OrderedDict([\\\r\n                                         ('date',min),\\\r\n                                         ('activity','size'),\\\r\n                                         ('pc',lambda x:list(x)),\\\r\n                                         ('id',lambda x:list(x)),\\\r\n                                         ('content',lambda x: ' '.join(str(x)))\\\r\n                                         ]))\\\r\n                        .reset_index()\r\nfile_agg.columns = ['user','timestep','file_min_date','file_count','file_pcs','file_ids','file_content']\r\n\r\n```\r\nExample 2:\r\n```\r\nemail_pagg = email_pdf.groupby('user')\\\r\n     .agg(OrderedDict([\\\r\n             ('pc',lambda x:list(x)),\\\r\n              ]))\\\r\n      .reset_index()\r\n```\r\n\r\nThanks!\r\nMiguel\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1121/comments",
    "author": "miguelusque",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-03-06T15:19:00Z",
        "body": "@miguelangel Unfortunately it looks like this is using nested types which aren't supported yet. In particular these three aggregations are expecting to work on some form of iterable:\r\n```\r\n('pc',lambda x:list(x)),\\\r\n('id',lambda x:list(x)),\\\r\n('content',lambda x: ' '.join(str(x)))\\\r\n```"
      },
      {
        "user": "randerzander",
        "created_at": "2019-03-12T00:43:56Z",
        "body": "@miguelangel were you able to come up with a suitable workaround using the upcoming string support, or other?\r\n\r\nWe don't plan to support nested types for awhile."
      },
      {
        "user": "miguelusque",
        "created_at": "2019-03-12T16:22:41Z",
        "body": "Hi @randerzander,\r\n\r\nI managed to create a container based on 0.6 branch, and merged @kkraus14 `fea-ext-string-support` work.\r\n\r\nIt let me continue on those areas of the code where string support was needed. Unfortunately, I still need to find a workaround for that part of code.\r\n\r\nI will keep you posted on my progress. \r\n\r\nRegards,\r\nMiguel"
      }
    ]
  },
  {
    "number": 1082,
    "title": "[QST] pip build for cuda 10.1",
    "created_at": "2019-03-04T20:58:19Z",
    "closed_at": "2019-07-03T22:46:52Z",
    "labels": [
      "question",
      "CMake"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1082",
    "body": "Just two questions:\r\n1) is cuDF compatible with CUDA 10.1 already?\r\n2) what are the plans to build and publish corresponding pip packages?\r\nJust installed CUDA 10.1 and tried to use cudf-cuda100 and it does not work because of libcudart100.so is missing. \r\nIt will be nice to know if it's better to build from source or  wait for a pip package. Thanks.",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1082/comments",
    "author": "svelts",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-03-04T21:00:04Z",
        "body": "Hi @svelts, we are planning on testing and bug squashing CUDA 10.1 related issues over the next few days before releasing 0.6. If you could try to build from source and report any issues you run into that would be extremely helpful!"
      },
      {
        "user": "kkraus14",
        "created_at": "2019-07-03T22:46:52Z",
        "body": "No longer supporting pip for the time being, closing this."
      }
    ]
  },
  {
    "number": 1078,
    "title": "[QST] cudf styling",
    "created_at": "2019-03-04T01:46:30Z",
    "closed_at": "2019-07-03T22:47:36Z",
    "labels": [
      "question",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1078",
    "body": "I am currently working on a PR which adds some functionality and I want to have nice docstrings/documentation along with the features.  What should the *ideal* import and variable name be for cudf.  Essentially, I would like a style guide and try to adhere to that guide as best as possible. Variations I have seen thus for imports/names\r\n\r\n## Imports:\r\n- import cudf\r\n- import cudf as gd\r\n- from cudf import DataFrame\r\n- from cudf import Series\r\n\r\n## Variable names\r\n- df = cudf.DataFrame(...)\r\n- gdf = cudf.DataFrame(...)\r\n- cdf = cudf.DataFrame(...)\r\n- my_gdf = cudf.DataFrame(...)\r\n- pdf = cudf.DataFrame(...)\r\n- sr = cudf.Series(...)\r\n\r\nIt would be nice to build consensus around one import style and one variable assignment style when creating DataFrames and/or Series.  This would be a benefit not only to developers but also to users when they read consistent docs/docstring/notebooks/etc. \r\n\r\nHowever, the above demonstrates that there are currently many held styles.  How should this be decided?  And to be clear, once a decision has been made I would like to reformat (at least) the python facing cudf docstrings and eventually tests\r\n\r\ncc @thomcom @kkraus14 @mrocklin ",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1078/comments",
    "author": "quasiben",
    "comments": [
      {
        "user": "mrocklin",
        "created_at": "2019-03-04T02:20:42Z",
        "body": "### Imports\r\n\r\nI like \r\n    \r\n    import cudf\r\n\r\nBecause it's fully explicit, there is only one name, and it's short.\r\n\r\nSecondarily I understand the motivation for \r\n\r\n    import cudf as gd\r\n\r\nBecause it looks like `import pandas as pd` and `import dask.dataframe as dd` (I think that this is @kkraus14 's preference and am happy to defer to him) .  \r\n\r\nPersonally I don't think that it's necessary to establish a `*d` convention like this.  I also think that if we can avoid injecting an alias then we should do so.  This helps users who see `cudf` code only rarely because they won't necessarily know that `gd` means `cudf`.  We made this choice also for `dask`, where we encourage people to use functions like `dask.compute` and `dask.delayed` rather than something like `dk.compute`.  There is a cost to aliases, especially when people unfamiliar with libraries review code.  *Very* popular libraries like `numpy` and `pandas` can get away with it, but I think that most libraries should avoid it.\r\n\r\n    from cudf import DataFrame\r\n    from cudf import Series\r\n\r\nI'm fully against these.  They make it unclear *which* DataFrame/Series class we're using within code.\r\n\r\n### Variable Names\r\n\r\nI like using `df` when there is only one kind of dataframe around.  I think I've seen `pdf`/`gdf` most commonly when there are both Pandas and cuDF dataframes around.  I could imagine replacing `gdf` with `cdf` for \"CUDA DataFrame\", but don't have a strong preference."
      },
      {
        "user": "thomcom",
        "created_at": "2019-03-09T00:00:41Z",
        "body": "I approve of\r\n```\r\nimport cudf\r\ngdf = cudf.DataFrame(...)\r\n```\r\nfor now. :)"
      },
      {
        "user": "randerzander",
        "created_at": "2019-04-04T17:26:01Z",
        "body": "It sounds like we've settled on:\r\n\r\n1. `import cudf`\r\n2. It's fine to use `df` as DataFrame variable names.\r\n3. When using both pandas and cudf DataFrames in related examples, distinguishing type with `pdf` and `gdf` is a good idea.\r\n\r\nI'd like to eventually add these suggestions to dev docs/wiki pages.\r\n\r\nAny other comments or suggestions?"
      },
      {
        "user": "kkraus14",
        "created_at": "2019-04-04T17:38:24Z",
        "body": "@randerzander I 110% agree with @mrocklin that we should avoid:\r\n```\r\nfrom cudf import DataFrame\r\nfrom cudf import Series\r\nfrom cudf import ${YOUR_CLASS_HERE}\r\n```\r\nBecause it becomes completely ambiguous."
      },
      {
        "user": "thomcom",
        "created_at": "2019-04-04T19:16:25Z",
        "body": "Is it really that ambiguous? How often do you think we should expect to be using pandas objects and cudf objects intermixed in our own library code?\r\n\r\nHow do you feel about using `from cudf import DataFrame` as a local import to avoid circular dependencies?"
      },
      {
        "user": "randerzander",
        "created_at": "2019-04-05T19:25:00Z",
        "body": "I think @kkraus14 meant in example and notebook code, where it is certainly common to mix cudf and Pandas code."
      },
      {
        "user": "kkraus14",
        "created_at": "2019-07-03T22:47:36Z",
        "body": "Settled on `import cudf`"
      }
    ]
  },
  {
    "number": 1011,
    "title": "[QST] Does rapidsai's cudf helps in loading only DataFrame kind data or any kind of data?",
    "created_at": "2019-02-20T17:54:49Z",
    "closed_at": "2019-02-20T19:20:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/1011",
    "body": "Does rapidsai's cudf helps in loading only DataFrame kind data or any kind of data?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/1011/comments",
    "author": "asispatra",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-02-20T18:01:55Z",
        "body": "@asispatra currently the IO capabilities are for loading tabular data into a columnar data structure, but we have plans to separate this out into a separate cuIO library that will have more flexibility. Is there a specific data format or use case you have in mind?"
      },
      {
        "user": "asispatra",
        "created_at": "2019-02-20T19:09:07Z",
        "body": "@kkraus14, Alexnet on Tensorflow usages TensorRecords for training and inference. In this kind of scenario I have seen that the data loading and pre-processing takes longer time than the GPU computation. \r\n\r\nCan I use CuDF to optimize the data loading and pre-processing time here?\r\n\r\nThanks."
      },
      {
        "user": "kkraus14",
        "created_at": "2019-02-20T19:18:15Z",
        "body": "We don't currently support TFRecords as an input format, but we'll add it to our list to explore, thanks!"
      },
      {
        "user": "asispatra",
        "created_at": "2019-02-20T19:20:11Z",
        "body": "Thanks @kkraus14 for the quick reply. "
      }
    ]
  },
  {
    "number": 742,
    "title": "[QST]docker build failed with dockerfile",
    "created_at": "2019-01-22T09:06:54Z",
    "closed_at": "2019-04-29T01:18:02Z",
    "labels": [
      "invalid",
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/742",
    "body": "**What is your question?**\r\n\r\nStep 22/26 : RUN source activate cudf &&     mkdir -p /cudf/cpp/build &&     cd /cudf/cpp/build &&     cmake .. -DCMAKE_INSTALL_PREFIX=${CONDA_PREFIX} &&     make -j install &&     make python_cffi &&     make install_python\r\n ---> Running in fc2619bea379\r\n-- The C compiler identification is GNU 5.4.0\r\n-- The CXX compiler identification is GNU 5.4.0\r\n-- The CUDA compiler identification is unknown\r\n-- Check for working C compiler: /usr/bin/gcc-5\r\n-- Check for working C compiler: /usr/bin/gcc-5 -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/g++-5\r\n-- Check for working CXX compiler: /usr/bin/g++-5 -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc\r\n-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc -- broken\r\nCMake Error at /conda/envs/cudf/share/cmake-3.13/Modules/CMakeTestCUDACompiler.cmake:46 (message):\r\n  The CUDA compiler\r\n\r\n    \"/usr/local/cuda/bin/nvcc\"\r\n\r\n  is not able to compile a simple test program.\r\n\r\n  It fails with the following output:\r\n\r\n    Change Dir: /cudf/cpp/build/CMakeFiles/CMakeTmp\r\n\r\n    Run Build Command:\"/usr/bin/make\" \"cmTC_49543/fast\"\r\n    /usr/bin/make -f CMakeFiles/cmTC_49543.dir/build.make CMakeFiles/cmTC_49543.dir/build\r\n    make[1]: Entering directory '/cudf/cpp/build/CMakeFiles/CMakeTmp'\r\n    Building CUDA object CMakeFiles/cmTC_49543.dir/main.cu.o\r\n    /usr/local/cuda/bin/nvcc     -x cu -c /cudf/cpp/build/CMakeFiles/CMakeTmp/main.cu -o CMakeFiles/cmTC_49543.dir/main.cu.o\r\n    In file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_config.h:179:0,\r\n                     from /usr/local/cuda/bin/../targets/x86_64-linux/include/host_config.h:50,\r\n                     from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:78,\r\n                     from <command-line>:0:\r\n    /usr/include/features.h:367:25: fatal error: sys/cdefs.h: No such file or directory\r\n    compilation terminated.\r\n    CMakeFiles/cmTC_49543.dir/build.make:65: recipe for target 'CMakeFiles/cmTC_49543.dir/main.cu.o' failed\r\n    make[1]: *** [CMakeFiles/cmTC_49543.dir/main.cu.o] Error 1\r\n    make[1]: Leaving directory '/cudf/cpp/build/CMakeFiles/CMakeTmp'\r\n    Makefile:121: recipe for target 'cmTC_49543/fast' failed\r\n    make: *** [cmTC_49543/fast] Error 2\r\n\r\n  CMake will not be able to correctly generate this project.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:18 (project)",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/742/comments",
    "author": "senseb",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2019-02-13T16:50:00Z",
        "body": "@senseb Are you still having this issue? This looks to be an issue with nvidia-docker or docker as opposed to our library as it's erroring in just detecting a usable nvcc."
      },
      {
        "user": "harrism",
        "created_at": "2019-04-29T01:18:02Z",
        "body": "No response, closing."
      }
    ]
  },
  {
    "number": 638,
    "title": "Spark-based multi-GPU planned? [QST]",
    "created_at": "2019-01-08T18:54:30Z",
    "closed_at": "2019-07-03T22:12:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/638",
    "body": "Do you plan to support multi-GPU and multi-node outside of dask? ",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/638/comments",
    "author": "emily-potyraj",
    "comments": [
      {
        "user": "datametrician",
        "created_at": "2019-01-08T18:55:09Z",
        "body": "@anfeng would you take this one, please?"
      },
      {
        "user": "anfeng",
        "created_at": "2019-01-08T23:00:56Z",
        "body": "@em-watkins We will enable Spark apps to leverage cuDF. Please share your use case on multi-GPU and multi-node."
      }
    ]
  },
  {
    "number": 339,
    "title": "Garbage collection - freeing up GPU memory",
    "created_at": "2018-11-06T14:29:18Z",
    "closed_at": "2018-11-10T15:23:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/339",
    "body": "This may be a question about how Rapids CUDF handles garbage collection. I've been seeing strange results.\r\n\r\nI have a multistage process where I need to apply a multistep operations on a dataframe, so creates intermediate dataframes that I no longer need after the stage is complete.\r\n\r\nSo after I start with nvidia-smi reporting 579MiB in use\r\n\r\nSo I create a DataFrame gdf1 randomly filled and now have 4361MiB in use\r\n\r\nI then apply an apply_rows function creates an additional four columns\r\n\r\ngdf2 = gdf1.applyrows(.....\r\n\r\nIt then shows 10595MiB in use\r\n\r\nIf I then use the command: def gdf1, it stays the same and the RAM isn't freed.\r\n\r\nIf I then use the command: del gdf2, it goes down to 6509MiB\r\n\r\nIf I then recreate gdf1 as before, it shows 10085MiB in use\r\n\r\nand recreate gdf2 with apply_rows again it shows 10355MiB in use\r\n\r\nbut then when I delete gdf2 'del gdf2' then gdf1 'del gdf1', in that order, it drops to 2453MiB in use.\r\n\r\nIt would appear that it is ok if the deletions are done in reverse order of creation, but either overwriting, by reassignment, or deleting in any other order, doesn't free up all the GPU memory.\r\n\r\nif I follow steps as above to recreate gdf1, it shows  4361MiB\r\n\r\nbut then if I reassign the apply_rows:\r\n\r\ngdf1 = gdf1.applyrows(.....\r\n\r\nit shows 10595MiB in use again, but the original gdf1 is lost but it's memory hasn't been freed.\r\n\r\nif I then delete gdf1 'del gdf1', then it shows 6509MiB, so not all the RAM is being used\r\n\r\nI am trying to understand how memory allocation of DataFrames works, so I can make more efficient use of GPU RAM.\r\n\r\nP.S. Now I have found I can call Numba JIT CUDA functions, within an apply_rows kernel, I can possibly do all the steps without the need for intermediate DataFrames\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/339/comments",
    "author": "MurrayData",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2018-11-06T14:37:32Z",
        "body": "@harrism Does RMM handle the garbage collection by default, or with the standard backend does Numba still handle garbage collection?"
      },
      {
        "user": "harrism",
        "created_at": "2018-11-06T19:52:41Z",
        "body": "Numba handles the garbage collection of dataframes in all cases. From the looks of it @MurrayData is using the current default (no pool allocator) but even with the pool allocator RMM just creates a finalizer which is passed to Numba."
      },
      {
        "user": "harrism",
        "created_at": "2018-11-06T19:53:40Z",
        "body": "@MurrayData are you running out of memory at any point?"
      },
      {
        "user": "MurrayData",
        "created_at": "2018-11-06T22:13:36Z",
        "body": "@harrism yes I have, several times, when reassigning result to dataframe of same name. I am now bejng careful how I use and release memory. I am using the Docker container. Restarting the kernel does clewr the RAM."
      },
      {
        "user": "kkraus14",
        "created_at": "2018-11-06T22:14:36Z",
        "body": "@MurrayData do you have a snippet that I could use to reproduce the issue to investigate?"
      },
      {
        "user": "MurrayData",
        "created_at": "2018-11-10T15:20:36Z",
        "body": "I think it was the way I was coding it. I've paid more attention, following your advice, and haven't had the issue since."
      },
      {
        "user": "kkraus14",
        "created_at": "2018-11-10T15:23:12Z",
        "body": "@MurrayData Glad to hear it, going to close this issue but feel free to reopen or open a new issue if you run into the problem."
      }
    ]
  },
  {
    "number": 307,
    "title": "E2E.ipynb questions.",
    "created_at": "2018-10-30T20:59:01Z",
    "closed_at": "2018-12-29T18:26:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/307",
    "body": "-  In the code to construct the delayed DMatrix calls there is the following line:\r\n\r\n```\r\ngpu_dfs = [(gpu_df[['delinquency_12']], gpu_df[delayed(list)(gpu_df.columns.difference(['delinquency_12']))]) \\\r\n               for gpu_df in gpu_dfs]\r\n```\r\n\r\nWhat is **gpu_df.columns.difference** doing?\r\n\r\n-  How do I view the actual DMatrix referenced in gpu_df?\r\n\r\n```\r\ntype(gpu_train_dfs[0])\r\ndask.delayed.Delayed\r\n```\r\n\r\nBut\r\n\r\n```\r\ngpu_train_dfs[0].compute()\r\nValueError: ctypes objects containing pointers cannot be pickled\r\n```\r\nSo, how do I get at the data from a delayed function?\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/307/comments",
    "author": "sfleisch",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2018-10-30T21:36:06Z",
        "body": "```delayed(list)(gpu_df.columns.difference(['delinquency_12'])```\r\nThis chunk of code gets the list of columns in the dataframe other than `delinquency_12`.\r\n\r\nUnfortunately there's currently no easy way to move a DMatrix object from the remote workers to the local process. Someone would need to build a custom serializer for DMatrix objects that handles the ctypes pointers."
      }
    ]
  },
  {
    "number": 279,
    "title": "Multi GPU",
    "created_at": "2018-10-10T14:46:31Z",
    "closed_at": "2018-10-22T23:59:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/279",
    "body": "If I'm creating multiple GPU dataframes. Is there a way to distribute them to different devices?",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/279/comments",
    "author": "Denisevi4",
    "comments": [
      {
        "user": "scopatz",
        "created_at": "2018-10-10T14:56:09Z",
        "body": "I believe that this is what dask_gdf is for, @Denisevi4"
      },
      {
        "user": "Denisevi4",
        "created_at": "2018-10-10T17:36:25Z",
        "body": "Thanks. It seems dask_gdf distributes each dataframe. It has its own interface that appears not compatible with what I'm doing. \r\n\r\nFor now I'm interested in say putting entire GPU dataframe into a GPU with a given gpu_id. Right now everything is going to gpu_id=0"
      },
      {
        "user": "kkraus14",
        "created_at": "2018-10-11T14:59:29Z",
        "body": "@Denisevi4 You can use dask_gdf for doing that, but you should start each worker with a different GPU mapped to gpu 0. I.E. if you wanted to use two GPUs you could start the dask workers as follows:\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1 dask-worker --nprocs=1 --nthreads=1\r\nCUDA_VISIBLE_DEVICES=1,0 dask-worker --nprocs=1 --nthreads=1\r\n```\r\n\r\nWe expose the additional devices to allow for cross-GPU communication using CUDA IPC and use 1 thread and 1 process per worker to avoid multiple tasks from executing on a single GPU at once."
      }
    ]
  },
  {
    "number": 223,
    "title": "One_hot_encoding directly on categorical columns interface",
    "created_at": "2018-08-24T19:45:28Z",
    "closed_at": "2020-05-29T16:42:04Z",
    "labels": [
      "question",
      "proposal",
      "Python"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/223",
    "body": "I wonder if it's useful to expose one hot encoding directly on categorical columns. The DataFrame method `one_hot_encoding` (which is a wrapper to the Series method) operates only on integer and float type columns, and as a result requires multiple steps to go from a raw categorical column to a dataframe with the categorical column one hot encoded.\r\n\r\nAdditionally, the documentation in `dataframe.one_hot_encoding` is excellent (shown below), but implies we still need to use a pandas dataframe to get the underlying category codes successfully one hot encode a pygdf dataframe. \r\n\r\n```\r\n    Examples\r\n    -------\r\n    >>> import pandas as pd\r\n    >>> from pygdf.dataframe import DataFrame as gdf\r\n    \r\n    >>> pet_owner = [1, 2, 3, 4, 5]\r\n    >>> pet_type = ['fish', 'dog', 'fish', 'bird', 'fish']\r\n    \r\n    >>> df = pd.DataFrame({'pet_owner': pet_owner, 'pet_type': pet_type})\r\n    >>> df.pet_type = df.pet_type.astype('category')\r\n    \r\n    Create a column with numerically encoded category values\r\n    >>> df['pet_codes'] = df.pet_type.cat.codes\r\n    >>> my_gdf = gdf.from_pandas(df)\r\n    \r\n    Create the list of category codes to use in the encoding\r\n    >>> codes = my_gdf.pet_codes.unique()\r\n    >>> enc_gdf = my_gdf.one_hot_encoding('pet_codes', 'pet_dummy', codes)\r\n    >>> enc_gdf.head()\r\n      pet_owner pet_type pet_codes pet_dummy_0 pet_dummy_1 pet_dummy_2\r\n      0         1     fish         2         0.0         0.0         1.0\r\n      1         2      dog         1         0.0         1.0         0.0\r\n      2         3     fish         2         0.0         0.0         1.0\r\n      3         4     bird         0         1.0         0.0         0.0\r\n      4         5     fish         2         0.0         0.0         1.0\r\n```\r\n\r\nIn the example it's perfect, but in some workflows I might have already created `my_gdf` and done various tasks before one-hot encoding; I wouldn't want to refer back to the original pandas dataframe, especially if my processing had altered the observations.\r\n\r\nI could do this by leveraging the `series.cat.codes` property for categorical columns. With that, I can then write a wrapper function to this method so I could operate directly on the categorical column, (brief example shown below).\r\n\r\n```\r\nimport math\r\nfrom string import ascii_lowercase\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom pygdf import Series, DataFrame\r\n\r\n\r\nNUM_ELEMENTS = 100000\r\n\r\n# create categorical series\r\nnp.random.seed(12)\r\npd_cat = pd.Categorical(\r\n    pd.Series(\r\n        np.random.choice(list(ascii_lowercase[:5]), NUM_ELEMENTS),\r\n        dtype='category'\r\n        )\r\n    )\r\n\r\n# gdf\r\ngdf = DataFrame()\r\ngdf['cat_col'] = Series.from_categorical(pd_cat)\r\ngdf['float_col'] = pd.Series(np.random.sample(NUM_ELEMENTS))\r\n\r\n\r\ndef ohe_categorical_interface(gpu_gdf, categorical_col, out_col_prefix):\r\n    gpu_gdf['cat_codes'] = gpu_gdf[categorical_col].cat.codes\r\n    one_hot_gdf = gpu_gdf.one_hot_encoding('cat_codes', out_col_prefix, gpu_gdf['cat_codes'].unique())\r\n#     del one_hot_gdf['cat_codes']\r\n    return one_hot_gdf\r\n\r\n\r\nout_df = ohe_categorical_interface(gdf, 'cat_col', 'cat_col_dummy')\r\nprint(out_df)\r\n   cat_col           float_col cat_codes cat_col_dummy_0 cat_col_dummy_1 cat_col_dummy_2 cat_col_dummy_3 cat_col_dummy_4\r\n 0       d  0.5744208534945319         3             0.0             0.0             0.0             1.0             0.0\r\n 1       d  0.7599272909708823         3             0.0             0.0             0.0             1.0             0.0\r\n 2       b  0.8512863304999798         1             0.0             1.0             0.0             0.0             0.0\r\n 3       c  0.9728014169562701         2             0.0             0.0             1.0             0.0             0.0\r\n 4       d  0.9677282236871539         3             0.0             0.0             0.0             1.0             0.0\r\n 5       d   0.884428828921966         3             0.0             0.0             0.0             1.0             0.0\r\n 6       e  0.4106131546544277         4             0.0             0.0             0.0             0.0             1.0\r\n 7       a 0.39469291429556175         0             1.0             0.0             0.0             0.0             0.0\r\n 8       b 0.24534859309103318         1             0.0             1.0             0.0             0.0             0.0\r\n 9       e  0.6742388338831965         4             0.0             0.0             0.0             0.0             1.0\r\n[99990 more rows]\r\n\r\n```\r\n\r\nDoes it potentially make sense to tweak  `Series.one_hot_encoding` to allow for categorical columns in such a way? An interface allowing direct one-hot encoding on categorical variables might make the transition from pandas even easier.\r\n",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/223/comments",
    "author": "beckernick",
    "comments": [
      {
        "user": "kkraus14",
        "created_at": "2020-05-29T16:42:03Z",
        "body": "Stale, closing."
      }
    ]
  },
  {
    "number": 148,
    "title": "Series.cat.set_categories is replacing invalid categories with None instead of NaN ",
    "created_at": "2018-07-12T19:21:18Z",
    "closed_at": "2018-12-12T20:04:14Z",
    "labels": [
      "question",
      "Needs Triage"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/148",
    "body": "```\r\nimport pandas as pd\r\nfrom pygdf.dataframe import DataFrame as gdf\r\n\r\npet_owner = [1, 2, 3, 4, 5]\r\npet_type = ['fish', 'dog', 'fish', 'bird', 'fish']\r\n\r\ndf = pd.DataFrame({'pet_owner': pet_owner, 'pet_type': pet_type})\r\ndf.pet_type = df.pet_type.astype('category')\r\n\r\nmy_gdf = gdf.from_pandas(df)\r\nmy_gdf['pet_type_mod'] = my_gdf.pet_type.cat.set_categories(['dog', 'bird'])\r\nmy_gdf.head()\r\n```\r\n```\r\n  pet_owner pet_type pet_type_mod\r\n0         1     fish             \r\n1         2      dog          dog\r\n2         3     fish             \r\n3         4     bird         bird\r\n4         5     fish \r\n```\r\nmy_gdf.pet_type_mod[0] == None is True, meaning fillna wont replace missing values as expected:\r\n\r\n```\r\nmy_gdf.pet_type_mod.fillna('dummy')\r\n```\r\n```\r\n0     \r\n1  dog\r\n2     \r\n3 bird\r\n4   \r\n```\r\nExpected Output:\r\n```\r\n0 dummy  \r\n1 dog\r\n2 dummy   \r\n3 bird\r\n4 dummy \r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/148/comments",
    "author": "randerzander",
    "comments": [
      {
        "user": "mike-wendt",
        "created_at": "2018-08-06T18:06:12Z",
        "body": "@randerzander are you able to retest with the current code?"
      },
      {
        "user": "kkraus14",
        "created_at": "2018-12-12T20:04:14Z",
        "body": "I believe this was due to issues with null handling that are now resolved. Closing and will reopen if it reappears."
      }
    ]
  },
  {
    "number": 146,
    "title": "Series.astype doesn't support 'category' type ",
    "created_at": "2018-07-12T15:41:44Z",
    "closed_at": "2018-12-10T21:11:28Z",
    "labels": [
      "question",
      "Needs Triage"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/146",
    "body": "```\r\nfrom pygdf.dataframe import DataFrame\r\n\r\ndf = DataFrame()\r\ndf['pet_owners'] = [1, 2, 3, 4, 5]\r\ndf['pet_type'] = ['fish', 'dog', 'fish', 'bird', 'fish']\r\ndf.pet_type.astype('category')\r\n```\r\n\r\nResult:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-33-07ac1c35a147> in <module>()\r\n      4 df['pet_owners'] = [1, 2, 3, 4, 5]\r\n      5 df['pet_type'] = ['fish', 'dog', 'fish', 'bird', 'fish']\r\n----> 6 df.pet_type.astype('category')\r\n\r\n/conda/envs/gdf/lib/python3.6/site-packages/pygdf-0.1.0a2+168.g78a5ca8-py3.6.egg/pygdf/series.py in astype(self, dtype)\r\n    448         If the dtype is not changed, ``self`` is returned.\r\n    449         \"\"\"\r\n--> 450         if dtype == self.dtype:\r\n    451             return self\r\n    452 \r\n\r\nTypeError: data type \"category\" not understood\r\n```\r\n\r\nOne workaround is to create a Pandas df with the correct dtypes:\r\n```\r\nimport pandas as pd\r\nfrom pygdf.dataframe import DataFrame as gdf\r\n\r\npet_owner = [1, 2, 3, 4, 5]\r\npet_type = ['fish', 'dog', 'fish', 'bird', 'fish']\r\n\r\ndf = pd.DataFrame(data = {'pet_owner': pet_owner, 'pet_type': pet_type})\r\ndf.pet_type = df.pet_type.astype('category')\r\nmy_gdf = gdf.from_pandas(df)\r\nmy_gdf.dtypes\r\n```\r\nOutput\r\n```\r\npet_owner       int64\r\npet_type     category\r\ndtype: object\r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/146/comments",
    "author": "randerzander",
    "comments": [
      {
        "user": "mike-wendt",
        "created_at": "2018-08-06T18:06:03Z",
        "body": "@randerzander are you able to retest with the current code?"
      },
      {
        "user": "kkraus14",
        "created_at": "2018-12-10T21:11:28Z",
        "body": "Given `object` dtypes aren't supported and there's now nvstrings going to close this as stale."
      }
    ]
  },
  {
    "number": 145,
    "title": "Series.value_counts should return a descending list of value frequencies ",
    "created_at": "2018-07-11T22:07:41Z",
    "closed_at": "2018-08-18T23:00:57Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/145",
    "body": "```\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(data = {'num_col': [1, 2, 3, 4], 'cat_col': ['a', 'b', 'b', 'b']})\r\ndf.cat_col = df.cat_col.astype('category')\r\ndf['cat_codes'] = df.cat_col.cat.codes.astype('int64')\r\n\r\nfrom pygdf.dataframe import DataFrame as gdf\r\nmy_gdf = gdf.from_pandas(df)\r\nmy_gdf.cat_codes.value_counts()\r\n```\r\n\r\nExpected output:\r\n```\r\n1    3\r\n0    1\r\n```\r\n\r\nActual output:\r\n```\r\n0    1\r\n1    3\r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/145/comments",
    "author": "randerzander",
    "comments": [
      {
        "user": "mike-wendt",
        "created_at": "2018-08-06T14:21:42Z",
        "body": "@randerzander does #153 merge fix this issue?"
      },
      {
        "user": "kkraus14",
        "created_at": "2018-08-18T23:00:57Z",
        "body": "Fixed via #153."
      }
    ]
  },
  {
    "number": 144,
    "title": "Series.value_counts throws AssertionError with type int8 ",
    "created_at": "2018-07-11T22:01:26Z",
    "closed_at": "2018-08-18T23:00:31Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/144",
    "body": "```\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(data = {'num_col': [1, 2], 'cat_col': ['a', 'b']})\r\ndf.cat_col = df.cat_col.astype('category')\r\ndf['cat_codes'] = df.cat_col.cat.codes\r\n# Work around -- cast the codes column to a type that works\r\n#df['cat_codes'] = df.cat_col.cat.codes.astype('int64')\r\n\r\nfrom pygdf.dataframe import DataFrame as gdf\r\nmy_gdf = gdf.from_pandas(df)\r\nmy_gdf.cat_codes.value_counts()\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-60-2feb35565298> in <module>()\r\n----> 1 my_gdf.cat_codes.value_counts()\r\n\r\n/conda/envs/gdf/lib/python3.6/site-packages/pygdf-0.1.0a2+168.g78a5ca8-py3.6.egg/pygdf/series.py in value_counts(self, type)\r\n    735         if self.null_count == len(self):\r\n    736             return 0\r\n--> 737         vals, cnts = self._column.value_counts(type=type)\r\n    738         res = Series(cnts, index=GenericIndex(vals))\r\n    739         return res\r\n\r\n/conda/envs/gdf/lib/python3.6/site-packages/pygdf-0.1.0a2+168.g78a5ca8-py3.6.egg/pygdf/numerical.py in value_counts(self, type)\r\n    167         out2 = cudautils.value_count(segs, len(sortedvals))\r\n    168         out_vals = self.replace(data=Buffer(out1), mask=None)\r\n--> 169         out_counts = self.replace(data=Buffer(out2), mask=None)\r\n    170         return out_vals, out_counts\r\n    171 \r\n\r\n/conda/envs/gdf/lib/python3.6/site-packages/pygdf-0.1.0a2+168.g78a5ca8-py3.6.egg/pygdf/column.py in replace(self, **kwargs)\r\n    282         if 'mask' in kwargs and 'null_count' not in kwargs:\r\n    283             del params['null_count']\r\n--> 284         return type(self)(**params)\r\n    285 \r\n    286     def view(self, newcls, **kwargs):\r\n\r\n/conda/envs/gdf/lib/python3.6/site-packages/pygdf-0.1.0a2+168.g78a5ca8-py3.6.egg/pygdf/numerical.py in __init__(self, **kwargs)\r\n     60         \"\"\"\r\n     61         super(NumericalColumn, self).__init__(**kwargs)\r\n---> 62         assert self._dtype == self._data.dtype\r\n     63 \r\n     64     def serialize(self, serialize):\r\n\r\nAssertionError: \r\n```",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/144/comments",
    "author": "randerzander",
    "comments": [
      {
        "user": "mike-wendt",
        "created_at": "2018-08-06T14:21:08Z",
        "body": "@randerzander has this been fixed with the #153 merge?"
      },
      {
        "user": "dantegd",
        "created_at": "2018-08-18T22:58:06Z",
        "body": "Yes, this was solved with #153 merge, just as issue 145"
      },
      {
        "user": "kkraus14",
        "created_at": "2018-08-18T23:00:31Z",
        "body": "Great, closing."
      }
    ]
  },
  {
    "number": 78,
    "title": "Category comparison is not checking that the categories are equivalent ",
    "created_at": "2017-09-07T22:32:19Z",
    "closed_at": "2018-12-10T21:12:54Z",
    "labels": [
      "question",
      "Needs Triage"
    ],
    "url": "https://github.com/rapidsai/cudf/issues/78",
    "body": "",
    "comments_url": "https://api.github.com/repos/rapidsai/cudf/issues/78/comments",
    "author": "sklam",
    "comments": [
      {
        "user": "mike-wendt",
        "created_at": "2018-08-06T14:25:45Z",
        "body": "@sklam any idea if this is still an issue?"
      },
      {
        "user": "kkraus14",
        "created_at": "2018-12-10T21:12:54Z",
        "body": "This should be resolved in latest code."
      }
    ]
  }
]