[
  {
    "number": 4082,
    "title": "joblib and faiss (interrupted by signal 11:SIGSEGV)",
    "created_at": "2024-12-10T09:04:10Z",
    "closed_at": "2024-12-11T19:26:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/4082",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 --> macOS 15.1.1\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->faiss-cpu-1.9.0.post1\r\njoblib version: 1.4.2\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> pip install faiss-cpu\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n```\r\nimport faiss\r\nfrom joblib import load\r\nload('model.joblib')\r\n```\r\n**error:Process finished with exit code 139 (interrupted by signal 11:SIGSEGV)**\r\nThis error occurs when I want to load a model\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/4082/comments",
    "author": "yuanke2001",
    "comments": [
      {
        "user": "ramilbakhshyiev",
        "created_at": "2024-12-11T19:26:17Z",
        "body": "Hello @yuanke2001, pip is not currently a supported method of installation. I am resolving this with 3 ideas for you to try:\r\n1. Try to load without importing faiss to see if it works.\r\n2. Try to import faiss after joblib\r\n3. Try installing faiss  from conda."
      }
    ]
  },
  {
    "number": 3983,
    "title": "How to reconstruct() from an IVF index use add_with_ids",
    "created_at": "2024-10-23T10:30:20Z",
    "closed_at": "2024-10-23T17:52:57Z",
    "labels": [
      "question",
      "autoclose"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3983",
    "body": "I got the error `DirectMap.cpp:82: direct map not initialized` when `reconstruct()` from an IVF index.\r\n\r\n```\r\nimport faiss\r\nquantizer = faiss.IndexFlatL2(dim)\r\nindex = faiss.IndexIVFFlat(quantizer, dim, nb)\r\nindex.train(train_vectors)\r\n\r\nindex.add_with_ids(db_vectors, idx)\r\nindex.make_direct_map(False) # use nomap type\r\nitem0 = index.reconstruct(0)\r\nprint(\"Reconstructed item 0: \", item0)\r\n```\r\nIs there any solution for this scenario?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3983/comments",
    "author": "czpmango",
    "comments": [
      {
        "user": "bshethmeta",
        "created_at": "2024-10-23T17:16:31Z",
        "body": "Can you help me understand why setting make_direct_map to True will not work for your usecase?"
      }
    ]
  },
  {
    "number": 3820,
    "title": "Refine Layer not support `add_with_ids`",
    "created_at": "2024-09-03T08:08:11Z",
    "closed_at": "2024-09-09T20:14:20Z",
    "labels": [
      "question",
      "autoclose"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3820",
    "body": "# Summary\r\n\r\nI want to use refine layer to re-order the coarse-results for more accurate output, but I find refine-layer is seemingly not support add_with_ids, I'm a little confused because the raw id should be stored in coarse-grained sorting model, the refine layer just rerank the coarse-result. How can I modify my parameters to use a refine-layer that support `add_with_ids` for improve the accuracy? Do I have to use `IDMap` in refine-layer? I think it's cracy to memory.\r\n\r\n# Platform\r\n\r\nOS: CentOS\r\n\r\nFaiss version: faiss 1.5\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\nHere is my index_type parameter: `IVF20480,PQ192x4fs,Refine(PQ96x8)`\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3820/comments",
    "author": "Tickdack",
    "comments": [
      {
        "user": "Tickdack",
        "created_at": "2024-09-04T03:26:28Z",
        "body": "I read the source Cpp file, and I find the class `IndexRefine`, I find `IndexRefine` not support `add_with_ids`, but I want to use `fastscan` index(Refine is necessary), what the general  scheme to use it if i want to get our own assigned id when `search`ï¼Ÿthank you."
      },
      {
        "user": "mdouze",
        "created_at": "2024-09-04T05:55:40Z",
        "body": "The `IndexRefine` does not support `add_with_ids` because the ids need to be sequential indices for the refinement index, which is most often an `IndexFlatCodes`. \r\nIf you need `add_with_ids`, please wrap the index in an `IndexIDMap`."
      },
      {
        "user": "Tickdack",
        "created_at": "2024-09-04T06:34:42Z",
        "body": "> The `IndexRefine` does not support `add_with_ids` because the ids need to be sequential indices for the refinement index, which is most often an `IndexFlatCodes`. If you need `add_with_ids`, please wrap the index in an `IndexIDMap`.\r\n\r\nThank you sir, it means I should use \"IDMap,IVF20480,PQ192X4fsr,Refine(PQ96x8)\" as the index factory key, is that right?"
      },
      {
        "user": "Tickdack",
        "created_at": "2024-09-04T09:17:55Z",
        "body": "> The `IndexRefine` does not support `add_with_ids` because the ids need to be sequential indices for the refinement index, which is most often an `IndexFlatCodes`. If you need `add_with_ids`, please wrap the index in an `IndexIDMap`.\r\n\r\nread the function `index_factory` and cpp files, i find `refine` also not support `remove_ids`, `remove_ids` on the index generated by parameter \"IDMap,IVF20480,PQ192X4fsr,Refine(PQ96x8)\" is not work, too. i think `fastscann` is a potential index to our project, how can i use it with high-performance accuracy and inner store and remove our assigned ids?"
      },
      {
        "user": "mdouze",
        "created_at": "2024-09-04T12:22:08Z",
        "body": "Right, supporting `revove_ids` would be an enhancement. Making a new issue for this. "
      }
    ]
  },
  {
    "number": 3648,
    "title": "Can FAISS index be integrated with a vector database?",
    "created_at": "2024-07-18T10:36:10Z",
    "closed_at": "2024-07-18T17:05:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3648",
    "body": "# Summary\r\n\r\nI understand that what FAISS offers is an index which can be stored and loaded. I would like to know if this can be integrated with any of the vector databases out there.\r\n Perhaps such a thing is not currently possible, I would also appreciate it if someone could point me towards a databases that uses a similar indexing algorithm.\r\n\r\n# Platform\r\n\r\nOS: \r\n\r\nFaiss version: faiss-cpu 1.8.0.post1\r\n\r\nInstalled from: pypi\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3648/comments",
    "author": "blacksmithop",
    "comments": [
      {
        "user": "ramilbakhshyiev",
        "created_at": "2024-07-18T17:05:32Z",
        "body": "Hi @blacksmithop, it is a very broad question but a general answer is yes, it is possible. There many databases implemented with FAISS, Milvus and OpenSearch are some good examples. Moving this to a discussion and closing. Please let me know if you have any additional questions."
      }
    ]
  },
  {
    "number": 3486,
    "title": "Is there a way to verbose search function?",
    "created_at": "2024-05-29T08:05:23Z",
    "closed_at": "2024-07-01T00:49:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3486",
    "body": "Applying search on large dataset on CPU takes a long time. Is there any way of showing remaining time for the search? That would be valuable feature.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3486/comments",
    "author": "zivlak",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2024-05-29T14:15:10Z",
        "body": "Are you using large query batches? If yes you can set index.verbose to true or break the computation in smaller batches."
      }
    ]
  },
  {
    "number": 3453,
    "title": "Soft K-Means clustering",
    "created_at": "2024-05-17T12:50:17Z",
    "closed_at": "2024-07-01T00:47:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3453",
    "body": "Is there a way for K-Means to return NxK, the probabilities that each point belongs to cluster K?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3453/comments",
    "author": "joangog",
    "comments": [
      {
        "user": "hammad7",
        "created_at": "2024-05-22T19:02:35Z",
        "body": "@joangog , \r\nTo get the NxK matrix, you can actually use softmax on top of the distances returned by kmeans.index.search() . Here is the working code for the same.\r\n\r\n```\r\ndef softmax(x):\r\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\r\n    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\r\n    return e_x / e_x.sum(axis=1, keepdims=True)\r\n\r\n\r\ndef soft_prob(kmeans,data):\r\n    centroids = kmeans.centroids\r\n    \r\n    assert centroids.ndim == 2, \"Centroids must be a 2D array\"\r\n    assert data.ndim == 2, \"Data must be a 2D array\"\r\n\r\n    # calculate the distance between each data point and each centroid\r\n    # distances = np.linalg.norm(data[:, np.newaxis, :] - centroids[np.newaxis, :, :], axis=2)\r\n\r\n    distances, assignments = kmeans.index.search(data, len(centroids))\r\n\r\n    # calculate the probability of each data point belonging to each cluster\r\n    probabilities = softmax(-distances)\r\n\r\n    for i in range(len(probabilities)):\r\n        # Get the current row of assignments\r\n        current_assignments = assignments[i]\r\n        \r\n        # Sort the current row of _ based on the assignments\r\n        sorted_row = [probabilities[i][j] for j in np.argsort(current_assignments)]\r\n        \r\n        # Update the current row of probabilities with the sorted values\r\n        probabilities[i] = sorted_row \r\n```\r\n\r\n**Usage**:\r\n```\r\n# Generate dummy data\r\nd = 10\r\nn = 100\r\nk = 5\r\nnp.random.seed(1234)\r\nx = np.random.random((n, d)).astype('float32')\r\n\r\n# # Perform k-means clustering\r\nkmeans = faiss.Kmeans(d, k, niter=25)\r\nkmeans.train(x)\r\n\r\nprint(soft_prob(kmeans,x))\r\n```\r\n\r\n**Output**:\r\n[[0.17769898 0.22218941 0.1265448  0.29491347 0.17865327]\r\n [0.31340864 0.17389828 0.18536964 0.18793808 0.13938534]\r\n [0.12620465 0.1935667  0.19170085 0.19852394 0.29000384]\r\n ...\r\n ...\r\n [0.20580962 0.2837541  0.12138043 0.17246573 0.21659008]\r\n [0.15004209 0.16144404 0.2073633  0.18762918 0.29352143]\r\n [0.27158892 0.2264123  0.17823231 0.09824435 0.22552218]]"
      }
    ]
  },
  {
    "number": 3400,
    "title": "Remove lapack dependency?",
    "created_at": "2024-04-30T10:33:21Z",
    "closed_at": "2024-05-04T13:45:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3400",
    "body": "Seems lapack is not used anymore? We could remove it.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3400/comments",
    "author": "chasingegg",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2024-04-30T15:40:23Z",
        "body": "dsyev and dgesvd are used"
      }
    ]
  },
  {
    "number": 3194,
    "title": "What indexes does the faiss database use by default? Do you have any visual management tools for the faiss database?",
    "created_at": "2024-01-07T13:04:28Z",
    "closed_at": "2024-03-18T13:58:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3194",
    "body": "\r\nWhat indexes does the faiss database use by default? Do you have any visual management tools for the faiss database?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3194/comments",
    "author": "qianbaidu1266",
    "comments": [
      {
        "user": "algoriddle",
        "created_at": "2024-01-08T14:34:36Z",
        "body": "What do you mean 'by default'? The client code must specify the index type during index construction. \r\n\r\nFor the second question, Faiss is not a full-fledged database, it 'only' provides the core functionality for vector search. As such, there are no visual management tools."
      }
    ]
  },
  {
    "number": 3187,
    "title": "Clarification Needed: Differences Between Faiss IndexFlatIP Search Results and Cosine Similarity",
    "created_at": "2024-01-04T02:10:23Z",
    "closed_at": "2024-07-01T00:07:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3187",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 --> Ubuntu20\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a --> lastest\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? -->  sourec build\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [v] CPU\r\n- [v] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [v] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\nI am reaching out with a query regarding some inconsistencies I've encountered while using Faiss for indexing and search operations. My primary concern revolves around the discrepancy between results obtained from a search function on an IndexFlatIP index and those calculated using `cosine similarity` (both scipy and scikit-learn) on the same dataset. **Values â€‹â€‹from faiss search have a higher matching rate**\r\n\r\nSpecifically, there are occasional mismatches between the distance values (D) from cosine_similarity(np.expand_dims(face_embedding, axis=0), index_np) and those obtained from index.search. While these discrepancies aren't constant, they are noticeable in certain instances.\r\n\r\nI am curious about how Faiss handles distance calculations and whether there is any additional preprocessing applied to feature vectors post L2-normalization within Faiss. Any clarification or additional information on this matter would be immensely helpful.\r\n\r\nYour insights and experiences could greatly assist in enhancing my understanding and in finding a resolution to these inconsistent results. Thank you in advance for your time and assistance.\r\n\r\nBest regards,\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3187/comments",
    "author": "YoungjaeDev",
    "comments": [
      {
        "user": "algoriddle",
        "created_at": "2024-01-08T14:37:52Z",
        "body": "Faiss does not L2 normalize either the query, not the database vectors. Have you done this normalization on the vectors yourself before adding them to the index and querying?"
      },
      {
        "user": "YoungjaeDev",
        "created_at": "2024-01-08T14:45:24Z",
        "body": "> Faiss does not L2 normalize either the query, not the database vectors. Have you done this normalization on the vectors yourself before adding them to the index and querying?\r\n\r\nYes, I have indeed applied L2 normalization to the vectors before adding them to the index and querying. \r\nMy confusion arises from the fact that while some results from the Faiss search align perfectly with those obtained from my own inner product calculations, there are occasional discrepancies. This led me to wonder if Faiss implements any additional steps or calculations that might account for these differences.\r\n\r\nAny insights into this would be greatly appreciated."
      },
      {
        "user": "mlomeli1",
        "created_at": "2024-06-19T14:57:41Z",
        "body": "hi @YoungjaeDev, as @algoriddle mentioned, you need to normalise all vectors before constructing the index and use the inner product metric. Could you provide a toy example where you reproduce the issue so this is actionable?"
      }
    ]
  },
  {
    "number": 3183,
    "title": "Search quality decreases significantly in case of a rapidly growing faiss index",
    "created_at": "2023-12-28T13:53:51Z",
    "closed_at": "2024-07-01T00:03:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3183",
    "body": "# Summary\r\nHi! \r\nIn my project I have to deal with a rapidly growing collection of objects (the indexation process is ongoing), which are indexed into faiss index. To be more accurate, let's say that the training set (initial collection) of ~5-7 million objects is ~100 times smaller than the potential size of the whole collection (after the indexation ends). According the guidelines, for this use case I should use something like OPQ32_128,IVF262144,PQ32. But it turns out that the search quality rapidly falls as the index grows. For example, for 10 million collection recall would be 85%, and for 52 million it is 69%. Seems a bit too fast for me)\r\nI also discovered that only about 60% of all centroids are non-empty, which means that the index is quite unbalanced. Is it likely to be some kind of issue with the faiss build I use?\r\nWould really appreciate any advice on index choice and hyperparameter tuning or any other suggestions. \r\n\r\nFaiss version: 1.7.3\r\n\r\nInstalled from: conda\r\n\r\nRunning on:\r\n CPU\r\n\r\nInterface: \r\nPython\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3183/comments",
    "author": "mkapry",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2024-01-02T18:57:41Z",
        "body": "what is your accuracy measure? \r\nis there a distribution drift when increasing the index size? it is not a good sign that 60% centroids are non-empty.\r\n\r\nNormally, the 1-recall@1 should *improve* for fixed nprobe and nlist when the index size increases.\r\n\r\n"
      },
      {
        "user": "mkapry",
        "created_at": "2024-01-03T11:49:14Z",
        "body": "@mdouze, thanks for the fast answer)\r\nThe accuracy measure I use is Recall@2048, which is recall for top 2048 candidates. The metric is calculated on a fixed test set.\r\nAbout the distribution drift: I can't tell for sure (as I receive data from different sources), but it shouldn't be any significant drift.\r\nThe question is if it is supposed to be so or it's likely to be something wrong with the index. "
      },
      {
        "user": "mdouze",
        "created_at": "2024-03-20T01:36:55Z",
        "body": "Is ground-truth brute force search ground truth or some application ground-truth?"
      }
    ]
  },
  {
    "number": 3176,
    "title": "QUESTION: Is it possible to do Kernel PCA with faiss?",
    "created_at": "2023-12-21T17:03:38Z",
    "closed_at": "2024-06-19T14:49:23Z",
    "labels": [
      "question",
      "out-of-scope"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3176",
    "body": "In short: Is it possible to do Kernel PCA in faiss?\r\nI.e. we have `faiss.PCAMatrix`, is there an analogous way to kernel PCA?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3176/comments",
    "author": "dominiquegarmier",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-12-26T07:19:02Z",
        "body": "It is not implemented. Could you use that as a pre-processing prior to calling Faiss? "
      },
      {
        "user": "dominiquegarmier",
        "created_at": "2023-12-27T13:56:28Z",
        "body": "Well im unsure if this would be possible, as applying the kernel to my data would already blow up the memory.\r\n(edit: unless we would approximate the feature map with one between finite dimensions? perhaps using mercers theorem. Then we could apply the feature map + regular PCA)\r\n\r\nI also dont really know how faiss's PCA is implemented. I assume that it's not doing a full SVD?\r\nHowever im unaware of any way of computing a kernel PCA without computing the kernel matrix and the SVD of it."
      }
    ]
  },
  {
    "number": 3096,
    "title": "Is it possible to (efficiently) query different subsets of the database vectors for different query vectors?",
    "created_at": "2023-10-17T07:48:18Z",
    "closed_at": "2024-06-30T23:50:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3096",
    "body": "I would like to calculate the similarities between the embedding of a paper and the embeddings of its cited references, and I have a lot of them (~30 million).\r\n\r\nBy using multiple GPUs, I was able to calculate pair-wise similarities between all papers, but I could only save the top K most similar results. The problem is that the cited references of a paper are not always the most similar papers in terms of semantic distance. Even though I have calculated all pair-wise similarities, I cannot obtain my desired results.\r\n\r\nTherefore, I am wondering if is it possible to (efficiently) query different subsets of the database vectors for different query vectors. Or maybe there is a smarter way to achieve my goal?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3096/comments",
    "author": "tomleung1996",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-10-24T10:11:02Z",
        "body": "This looks like a filtered search problem. The answer depends on what the filtering criterion would be. If the dataset is clustered then you can build one index per clustrer. "
      }
    ]
  },
  {
    "number": 3005,
    "title": "Is there a way to find the m cluster_ids of each original data after building an index?",
    "created_at": "2023-08-15T10:13:08Z",
    "closed_at": "2024-06-30T22:46:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3005",
    "body": "When using IndexIVFPQ to build an index, kmeans algorithm is used to generate m cluster_ids ranging from 0 to 255 for each original data.\r\nAfter using IndexIVFPQ to build an index, is there a convenient way to find the m cluster_ids of each original data?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3005/comments",
    "author": "estella778",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-08-16T10:29:51Z",
        "body": "It is unclear whether you want:\r\n\r\n- the cluster ids from the inverted file, which you get with `index.quantizer.search(x, 1)[1]`\r\n\r\n- the PQ codes, that you can get with `index.pq.compute_codes(x)`\r\n"
      }
    ]
  },
  {
    "number": 2994,
    "title": "Extracting and using the PQ from a full IndexIVF",
    "created_at": "2023-08-08T17:03:40Z",
    "closed_at": "2024-06-30T22:43:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2994",
    "body": "# Summary\r\n\r\nI'm trying to extract the pre-precessing part (PQ) of my FAISS Index so that I can return the approximate vectors as part of my API response.\r\n\r\nSpecifically, I am not able to extract the Product Quantizer from the Index.\r\n\r\n# Reproduction instructions\r\n\r\n```python\r\nimport numpy as np\r\nimport faiss\r\n\r\nd = 1024\r\nindex_name = 'OPQ64_1280,IVF512_HNSW32,PQ64x8'\r\nindex = faiss.index_factory(d, index_name, faiss.METRIC_INNER_PRODUCT)\r\nembeddings = np.random.normal(size=(1000,d)).astype(np.float32)\r\nindex.train(embeddings)\r\n\r\n# Now, I want to apply just the PQ encoder to my query:\r\nquery = np.random.normal(size=(1,d)).astype(np.float32)\r\nquantizer = index.quantizer # This is still a IndexIVF class (!)\r\n\r\n# ERROR HERE\r\nencoded_vec = quantizer.compute_code(query) ## This doesn't work\r\n\r\n# And compare against another encoded vector I can fetch from the index\r\nindex = faiss.extract_index_ivf(index)\r\nindex.set_direct_map_type(faiss.DirectMap.Array)\r\nindex.reconstruct(1).shape # returns the encoded vector at position = 1 (d=1280)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2994/comments",
    "author": "guillaumeguy",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-08-09T06:36:13Z",
        "body": "`quantizer = index.quantizer` will not work because your index is an `IndexPreTransform`.\r\nUse `quantizer = index.extract_index_ivf(index).quantizer`. \r\n\r\nThe encoded representation of the query can be computed with \r\n`index.sa_decode(index.sa_encode(query))`. \r\n\r\n"
      },
      {
        "user": "guillaumeguy",
        "created_at": "2023-08-09T15:18:15Z",
        "body": "This is super helpful. I was able to create my `encoded_query` but it's not comparable with the `index_reconstruct` (due to the OPQ putting my vector in d=1280?)\r\n\r\n```python\r\nencoded_query = index.sa_decode(index.sa_encode(query)) # of dim 1024\r\n\r\n# Now, insert query into index and inspect how it's encoded\r\nn = index.ntotal\r\nindex.add(query) \r\nindex2 = faiss.extract_index_ivf(index)\r\nindex2.set_direct_map_type(faiss.DirectMap.Array)\r\nencoded_query_recon = index2.reconstruct(n) # of dim 1280 (makes sense, OPQ outputs d=1280)\r\n\r\n# Presumably `encoded_query` and `encoded_query_recon` should be very close but they have diff dimensions here.\r\n```\r\n\r\nPS: In your example, I think you meant `quantizer = **faiss**.extract_index_ivf(index).quantizer` instead of `quantizer = index.extract_index_ivf(index).quantizer`"
      },
      {
        "user": "guillaumeguy",
        "created_at": "2023-08-10T00:34:53Z",
        "body": "I think I figured it out ! One needs to apply the OPQ before the encode / decode step.\r\n\r\nHere is the code snippet:\r\n```python\r\n# Extract the OPQ matrix\r\nmat = faiss.downcast_VectorTransform(index.chain.at(0))\r\n\r\n# Apply it to the query\r\nrot_query = mat.apply(query)\r\n\r\n# Now, apply PQ\r\nind2 = faiss.extract_index_ivf(index)\r\nencoded_query2 = ind2.sa_decode(ind2.sa_encode(rot_query))\r\n\r\n# You can check whether the encoding is correct by adding to the index and checking how it's represented with `reconstruct`\r\nindex.add(query)\r\nindex2 = faiss.extract_index_ivf(index)\r\nindex2.set_direct_map_type(faiss.DirectMap.Array) # Is this necessary? Not sure\r\nencoded_query = index2.reconstruct(index.ntotal-1)\r\n\r\nassert np.allclose(encoded_query2, encoded_query) \r\n\r\n``` \r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 2989,
    "title": "Runnning Faiss on high dimensional dataset",
    "created_at": "2023-08-06T14:08:13Z",
    "closed_at": "2023-08-08T07:37:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2989",
    "body": "Hi,\r\nI am trying to run Faiss on a very high dimensional dataset (>40000). On the other hand the number of samples is relatively contained (in between 10k and 100k). I was testing different indexes implementations but when I perform the search they all return very wrong values of distances. Is it possible that I am incurring in some kind of max limit for the dimension of the feature space?\r\n\r\nThanks a lot.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2989/comments",
    "author": "nash169",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-08-07T12:31:45Z",
        "body": "Faiss indexes are typically used for vector sized in the 100-1000 range. \r\nIf the number of dimensions d is larger than index size n, then you can PCA down the vector size to n without loss of accuracy. "
      },
      {
        "user": "nash169",
        "created_at": "2023-08-08T07:37:17Z",
        "body": "Thanks a lot for the quick answer! I'll try that."
      }
    ]
  },
  {
    "number": 2971,
    "title": "Why is the recall rate of IP lower than L2 for IVFPQ ?",
    "created_at": "2023-07-24T08:27:58Z",
    "closed_at": "2024-06-30T22:37:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2971",
    "body": "Why is the recall rate of IP lower than L2 for IVFPQ ?\r\n\r\nlower about 5~30% for top1 and top10\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2971/comments",
    "author": "shiwanghua",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-07-29T06:20:43Z",
        "body": "It depends on the data distribution. But indeed it is typical that IP is harder to index than L2. This is because the quantizers are trained with k-means that guarantees to minimize the quantization error only for squared L2. "
      },
      {
        "user": "shiwanghua",
        "created_at": "2023-07-29T07:20:58Z",
        "body": "> It depends on the data distribution. But indeed it is typical that IP is harder to index than L2. This is because the quantizers are trained with k-means that guarantees to minimize the quantization error only for squared L2.\r\n\r\nBut I used HNSWFlatIP as the k-means quantizers for IVF train and PQ train."
      }
    ]
  },
  {
    "number": 2956,
    "title": "A software engineering design question ",
    "created_at": "2023-07-17T13:01:43Z",
    "closed_at": "2023-08-03T09:33:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2956",
    "body": "Why is the `fvec_madd` function declared in `utils.h` but not implemented in `utils.cpp` ?\r\n\r\nInstead, it's implemented in distances_simd.cpp with both avx and sse version.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2956/comments",
    "author": "shiwanghua",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-07-24T07:23:40Z",
        "body": "There is not always a strict correspondence between .cpp and .h files. "
      }
    ]
  },
  {
    "number": 2953,
    "title": "IndexFlatL2 multithread is slower than single thread",
    "created_at": "2023-07-14T09:33:48Z",
    "closed_at": "2024-06-30T22:34:20Z",
    "labels": [
      "question",
      "Performance"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2953",
    "body": "python faiss-cpu 1.7.4 installed with pip3.x\r\nMultithread performance is pool on my 32-processor machine\r\n\r\nmodel name\t: Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz\r\n************ nthread= 1\r\n*********** nq= 100\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=1.393 ms (Â± 0.1564)\r\nsearch k= 10 t=2.679 ms (Â± 0.0422)\r\nsearch k=100 t=6.473 ms (Â± 0.4788)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=11.656 ms (Â± 23.1539)\r\nsearch k= 10 t=3.664 ms (Â± 0.4651)\r\nsearch k=100 t=6.653 ms (Â± 0.6943)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=4.447 ms (Â± 0.4957)\r\nsearch k= 10 t=4.460 ms (Â± 0.0903)\r\nsearch k=100 t=8.210 ms (Â± 0.8620)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=7.682 ms (Â± 1.1851)\r\nsearch k= 10 t=8.133 ms (Â± 1.1031)\r\nsearch k=100 t=10.987 ms (Â± 1.5985)\r\nrestab=\r\n 1.39302\t2.67902\t6.4728\r\n11.6563\t3.66396\t6.65313\r\n4.44698\t4.45956\t8.20962\r\n7.68209\t8.13305\t10.9866\r\n*********** nq= 10000\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.080 s (Â± 0.0044)\r\nsearch k= 10 t=0.257 s (Â± 0.0085)\r\nsearch k=100 t=0.564 s (Â± 0.0193)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.259 s (Â± 0.0097)\r\nsearch k= 10 t=0.321 s (Â± 0.0092)\r\nsearch k=100 t=0.635 s (Â± 0.0237)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.368 s (Â± 0.0306)\r\nsearch k= 10 t=0.410 s (Â± 0.0379)\r\nsearch k=100 t=0.681 s (Â± 0.0412)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.599 s (Â± 0.0144)\r\nsearch k= 10 t=0.645 s (Â± 0.0107)\r\nsearch k=100 t=0.921 s (Â± 0.0569)\r\nrestab=\r\n 0.0801447\t0.257458\t0.56392\r\n0.259316\t0.321337\t0.635152\r\n0.368472\t0.410237\t0.680965\r\n0.599093\t0.644711\t0.921228\r\n************ nthread= 32\r\n*********** nq= 100\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=12.850 ms (Â± 7.3587)\r\nsearch k= 10 t=326.201 ms (Â± 9.8362)\r\nsearch k=100 t=331.151 ms (Â± 16.7528)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=181.012 ms (Â± 20.5017)\r\nsearch k= 10 t=325.893 ms (Â± 12.7326)\r\nsearch k=100 t=325.874 ms (Â± 24.1845)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=181.696 ms (Â± 14.6625)\r\nsearch k= 10 t=329.945 ms (Â± 17.0235)\r\nsearch k=100 t=329.392 ms (Â± 14.8352)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=176.828 ms (Â± 9.2367)\r\nsearch k= 10 t=326.336 ms (Â± 16.2117)\r\nsearch k=100 t=325.248 ms (Â± 13.9408)\r\nrestab=\r\n 12.8498\t326.201\t331.151\r\n181.012\t325.893\t325.874\r\n181.696\t329.945\t329.392\r\n176.828\t326.336\t325.248\r\n*********** nq= 10000\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.027 s (Â± 0.0119)\r\nsearch k= 10 t=0.980 s (Â± 0.0149)\r\nsearch k=100 t=1.029 s (Â± 0.0168)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.524 s (Â± 0.0138)\r\nsearch k= 10 t=0.986 s (Â± 0.0122)\r\nsearch k=100 t=1.066 s (Â± 0.0379)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.572 s (Â± 0.0328)\r\nsearch k= 10 t=0.999 s (Â± 0.0171)\r\nsearch k=100 t=1.090 s (Â± 0.0780)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.721 s (Â± 0.0103)\r\nsearch k= 10 t=1.059 s (Â± 0.0262)\r\nsearch k=100 t=1.147 s (Â± 0.0235)\r\nrestab=\r\n 0.0267251\t0.979833\t1.02869\r\n0.523988\t0.985733\t1.0658\r\n0.571997\t0.999151\t1.09039\r\n0.721175\t1.05897\t1.14676\r\n\r\n# Reproduction instructions\r\n\r\nbench_index_flat.py \r\nI modified faiss.cvar.distance_compute_min_k_reservoir from 5 to 100",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2953/comments",
    "author": "RongchunYao",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-07-24T07:20:39Z",
        "body": "Please install Faiss with conda to make sure that the proper MKL version is installed. \r\nOn intel, we sometimes observe worse MKL perf with nthread = nb cores. Please try 16 threads"
      },
      {
        "user": "RongchunYao",
        "created_at": "2023-07-24T12:28:27Z",
        "body": "> \r\nIt tried out that nthread = nb cores/2 works good for me on another server which has 16 amd processors (both training and query). Thank you so much && I wonder why the performance is bad  with nthread = nb cores :-)"
      },
      {
        "user": "alexanderguzhva",
        "created_at": "2023-07-24T16:41:19Z",
        "body": "@RongchunYao the performance is likely bad because of the hyper-threading. As you know, typically the hyper-threading is about having two virtual CPU cores sharing the same compute resources of a single real core. And such a sharing is not efficient for linear-algebra ops within Faiss. So, by specifying \"nthread = nb codes / 2\" you make sure that there's no fight among two virtual CPU cores.\r\nHope it helps. \r\n"
      },
      {
        "user": "RongchunYao",
        "created_at": "2023-07-25T02:19:01Z",
        "body": "> @RongchunYao the performance is likely bad because of the hyper-threading. As you know, typically the hyper-threading is about having two virtual CPU cores sharing the same compute resources of a single real core. And such a sharing is not efficient for linear-algebra ops within Faiss. So, by specifying \"nthread = nb codes / 2\" you make sure that there's no fight among two virtual CPU cores. Hope it helps.\r\n\r\nThank you!"
      },
      {
        "user": "RongchunYao",
        "created_at": "2023-11-30T15:30:14Z",
        "body": "> @RongchunYao the performance is likely bad because of the hyper-threading. As you know, typically the hyper-threading is about having two virtual CPU cores sharing the same compute resources of a single real core. And such a sharing is not efficient for linear-algebra ops within Faiss. So, by specifying \"nthread = nb codes / 2\" you make sure that there's no fight among two virtual CPU cores. Hope it helps.\r\n\r\nHi, I recently run faiss with openblas that compiled with omp, and I set the omp thread to 32. I run the jobs in batch on some computing platform, most machines gain great acceleration, but some machine runs very slow (each machine has similar\r\n workload). What's stranger is that part of the slow machine has a high cpu utilization ( same as normal machine ).\r\n\r\nI wonder the potential reasons, could the tasks submited to the machine by other users be a great influence factor?\r\nLooking forward to your reply."
      }
    ]
  },
  {
    "number": 2951,
    "title": "Can we do clustering by using inner product",
    "created_at": "2023-07-14T03:52:36Z",
    "closed_at": "2023-08-03T08:28:34Z",
    "labels": [
      "duplicate",
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2951",
    "body": "Do we have any materials to know if it could be done well?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2951/comments",
    "author": "chasingegg",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-08-03T08:28:34Z",
        "body": "see #2363 "
      }
    ]
  },
  {
    "number": 2921,
    "title": "What is a good training index size for CPA?",
    "created_at": "2023-06-21T07:36:17Z",
    "closed_at": "2023-08-11T14:13:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2921",
    "body": "Assuming I've reduced D from 1,536 to 192, and I've got 5,000 vectors I need to train. What is a good general rule of thumb for the size of my training data to make it as accurate as possible, while still preserving maximum amount of memory while using the index?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2921/comments",
    "author": "polterguy",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-06-23T12:17:44Z",
        "body": "5x to 10x the dimension us usually fine to train a PCA. "
      },
      {
        "user": "polterguy",
        "created_at": "2023-06-23T14:22:09Z",
        "body": "Thank you. Is this the original dimension or the new dimension. Say I'm using OpenAI, it's got 1,536 dimensions. Then I reduce to 384. Should I have 5 to 10 x 384 or 1,536 ...?"
      },
      {
        "user": "mdouze",
        "created_at": "2023-06-26T07:02:40Z",
        "body": "say 5*1636 to be on the safe side "
      },
      {
        "user": "polterguy",
        "created_at": "2023-06-30T04:10:19Z",
        "body": "Thx Matthijs, what (roughly) quality loss should I expect from something like that? Do you know? I know obviously everything here is guesstimates, but I presume you're better positioned to create a qualified guesstimate ..."
      },
      {
        "user": "mdouze",
        "created_at": "2023-06-30T06:54:39Z",
        "body": "It depends on the data distribution, so needs to be tested experimentally. "
      }
    ]
  },
  {
    "number": 2903,
    "title": "Performance issue of scalar quantizer when vector is not aligned",
    "created_at": "2023-06-07T18:54:29Z",
    "closed_at": "2023-06-20T15:07:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2903",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\nAFAIK, when using scalar quantizers, avx is not enabled when vector dim is not divisible by 8, which causes performance issues when vectors are not aligned. Is there any plan to support avx on unaligned vectors? ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2903/comments",
    "author": "hhy3",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-06-08T13:58:16Z",
        "body": "Probably not. You may want to round up the vector sizes to a multiple of 8. "
      }
    ]
  },
  {
    "number": 2829,
    "title": "GpuIndexBinaryIVF implementation",
    "created_at": "2023-04-23T18:51:27Z",
    "closed_at": "2024-06-30T21:45:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2829",
    "body": "# Summary\r\n\r\nHello, \r\nI want to contribute to the project by implementing a `GpuIndexBinaryIVF` index. I wanted to get some directions if possible on the  index implementation. As I understand it, Faiss already has a GpuIndexIVFFlat which is limited to  `METRIC_L2` and `METRIC_INNER_PRODUCT` so to implement a binary version I could check the vector dimension to be a multiple of 8, change search distance to be GPU `binary distance`, and use same interface as `GpuIndexBinaryIVF`  ? ðŸ¤” \r\n\r\nAlso looking at the CPU implementation `IndexBinaryIVF` I saw that it inherits from `IndexBinary` and not the  `IndexIVF` | `IndexIVFInterface`, is there a reason for this separation in implementation? \r\n\r\nThank you all for your help, \r\nInterface: \r\n- [X] C++\r\n- [X] Python\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2829/comments",
    "author": "AmineDiro",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-04-24T07:45:12Z",
        "body": "Thanks for your interest in contributing to Faiss. \r\n`IndexBinaryIVF` does not inherit from `IndexIVF` because the coarse quantizer is an `IndexBinary`.\r\n"
      }
    ]
  },
  {
    "number": 2792,
    "title": "Reconstructing all vectors with Arbitrary ID mapping",
    "created_at": "2023-03-27T02:30:36Z",
    "closed_at": "2024-06-30T21:34:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2792",
    "body": "# Summary\r\n\r\nHow do I reconstruct all vectors from an Index with ID mapping enabled? The IDs are non-contiguous arbitrary integers in my case, and calling `reconstruct_n(0, index.ntotal)` throws a Fatal Python Error which I assume is because faiss is reconstructing the vectors based on my non-contiguous ID mapping.\r\n\r\nIf I understand this correctly, I should be able to get pass the ID maps and call `reconstruct_n` directly on the Index, which I assume still uses incremental IDs starting at 0.\r\n\r\nI'm aware that I can always loop through the IDs and call `reconstruct` on each item, but I believe there must be a better way?\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2792/comments",
    "author": "Isaac-the-Man",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-03-27T15:42:04Z",
        "body": "Please use `reconstruct_batch` with the ids you want to reconstruct. "
      },
      {
        "user": "Isaac-the-Man",
        "created_at": "2023-03-28T09:10:07Z",
        "body": "Thanks for the quick response, `reconstruct_batch` works perfectly for me! \r\n\r\nI'd still like to know if there is any way to bypass ID Mapping and call all the `reconstruct_x` methods directly on the default incremental ID?"
      }
    ]
  },
  {
    "number": 2766,
    "title": "FAISS test TTI1M return r1@100=0",
    "created_at": "2023-03-16T11:25:31Z",
    "closed_at": "2024-06-27T23:41:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2766",
    "body": "# Summary\r\n\r\nI try to do some benchmark on TTI1M dataset, but I get all recall1@100 to be 0\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: conda latest\r\n\r\nInstalled from: conda\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\nThis file can reproduce:\r\n```\r\nimport os                                                                                                                                                                       \r\nimport time\r\nimport numpy as np\r\nimport pdb \r\n\r\nimport faiss\r\nfrom faiss.contrib import inspect_tools\r\nfrom datasets import load_tti1M, evaluate \r\n\r\nxb, xq, xt, gt = load_tti1M()\r\nnq, d = xq.shape\r\nres = faiss.StandardGpuResources()\r\n\r\nindex = faiss.index_factory(d, \"IVF4096,PQ8\", faiss.METRIC_INNER_PRODUCT)\r\nco = faiss.GpuClonerOptions()\r\nco.useFloat16 = True\r\n\r\nindex = faiss.index_cpu_to_gpu(res, 0, index, co) \r\nprint(\"train\")\r\nindex.train(xb)\r\nprint(\"add vectors to index\")\r\nindex.add(xb)\r\nprint(\"benchmark\")\r\n\r\nfor lnprobe in range(10):\r\n    nprobe = 1 << lnprobe\r\n    index.setNumProbes(nprobe)\r\n    t, r = evaluate(index, xq, gt, 100)\r\n\r\n    print(r[100])\r\n```\r\nThe `dataset.py`:\r\n```\r\nfrom __future__ import print_function\r\nimport sys \r\nimport time\r\nimport numpy as np\r\n\r\n\r\npath = \"/path/to/TTI1M/\"\r\n\r\ndef read_fbin(filename, start_idx=0, chunk_size=None):\r\n    \"\"\" Read *.fbin file that contains float32 vectors\r\n    Args:\r\n        :param filename (str): path to *.fbin file\r\n        :param start_idx (int): start reading vectors from this index\r\n        :param chunk_size (int): number of vectors to read. \r\n                                 If None, read all vectors\r\n    Returns:\r\n        Array of float32 vectors (numpy.ndarray)\r\n    \"\"\"\r\n    with open(filename, \"rb\") as f:\r\n        nvecs, dim = np.fromfile(f, count=2, dtype=np.int32)\r\n        nvecs = (nvecs - start_idx) if chunk_size is None else chunk_size\r\n        arr = np.fromfile(f, count=nvecs * dim, dtype=np.float32, \r\n                          offset=start_idx * 4 * dim)\r\n    return arr.reshape(nvecs, dim)\r\n \r\n \r\ndef read_ibin(filename, start_idx=0, chunk_size=None):\r\n    \"\"\" Read *.ibin file that contains int32 vectors\r\n    Args:\r\n        :param filename (str): path to *.ibin file\r\n        :param start_idx (int): start reading vectors from this index\r\n        :param chunk_size (int): number of vectors to read.\r\n                                 If None, read all vectors\r\n    Returns:\r\n        Array of int32 vectors (numpy.ndarray)\r\n    \"\"\"\r\n    with open(filename, \"rb\") as f:\r\n        nvecs, dim = np.fromfile(f, count=2, dtype=np.int32)\r\n        nvecs = (nvecs - start_idx) if chunk_size is None else chunk_size\r\n        arr = np.fromfile(f, count=nvecs * dim, dtype=np.int32, \r\n                          offset=start_idx * 4 * dim)\r\n    return arr.reshape(nvecs, dim)\r\n\r\ndef load_tti1M():\r\n    print(\"Loading tti1M...\", end='', file=sys.stderr)\r\n    xt = []\r\n    xb = read_fbin(path + \"base.1M.fbin\")\r\n    xq = read_fbin(path + \"query.public.100K.fbin\", 0, 10000)\r\n    gt = read_ibin(path + \"groundtruth.public.100K.ibin\", 0, 10000)\r\n    print(\"done\", file=sys.stderr)\r\n    return xb, xq, xt, gt\r\n\r\ndef evaluate(index, xq, gt, k):\r\n    nq = xq.shape[0]\r\n    t0 = time.time()\r\n    D, I = index.search(xq, k)  # noqa: E741\r\n    t1 = time.time()\r\n    r100_1000 = 0.0\r\n    \r\n    for q in range(nq):\r\n        gt100 = gt[q][0:100]\r\n        cnt = 0\r\n        for g in I[q][0:1000]:\r\n            if g in gt100:\r\n                cnt += 1\r\n        r100_1000 += (1.0 * cnt) / (100.0)\r\n    \r\n    recalls = {}\r\n    i = 1\r\n    while i <= k:\r\n        recalls[i] = (I[:, :i] == gt[:, :1]).sum() / float(nq)\r\n        i *= 10\r\n    recalls[1000] = r100_1000 / (1.0 * nq)\r\n    return (t1 - t0) * 1000.0, recalls\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2766/comments",
    "author": "SubjectNoi",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-03-20T07:48:54Z",
        "body": "What is TTI ?\r\nPlease check your evaluation code with a brute force index first. \r\n"
      }
    ]
  },
  {
    "number": 2764,
    "title": "Differences between faiss.Kmeans class and kmeans in contrib",
    "created_at": "2023-03-15T08:46:38Z",
    "closed_at": "2024-06-27T23:38:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2764",
    "body": "# Summary\r\n\r\nI'm clustering 1B points with 10M centroids, I'm very happy to use faiss for how quick it is but because of how lengthy training the clustering is, I would like to have some checkpoints after every iteration. Is there any way to natively do that in faiss or should I break up the clustering training in python by initializing a model for every iteration and passing to the new model the centroids from the previous iteration? I noticed that later iterations of the clustering take significantly longer than the first one, is this something specific to my system or built in the C++clustering? If this is the case, breaking up the clustering per iteration wouldnâ€™t work. \r\n\r\nI've seen the kmeans function written in python in contrib in faiss 1.7.3 but because of the scale of my problem, I would like to stay in C++ to get as fast clustering as possible. \r\n\r\nThanks for any info!\r\n\r\nPS\r\nI did some tests with the python clustering, and it is almost as fast as the native C++ version, so I'll use the one in contrib for my use case. But I would still be interested in knowing why later iterations take longer than the first, aren't they all doing the same operations on the same amount of data?\r\n\r\nPPS\r\nI might have just realized that it is always a cumulative time, so it is just total elapsed time. Sorry \r\nfor the issue, but I'm still interested in understanding the differences between the native C++ clustering and python clustering.\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: 1.7.2\r\n\r\nInstalled from: from pip \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\nAny usage of the Kmeans clustering object from the faiss-gpu python package.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2764/comments",
    "author": "AntonioLopardo",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-03-20T07:45:11Z",
        "body": "The Python implementation should follow the C++ quite closely. \r\nThe C++ implementation exists because python is not available in all contexts."
      }
    ]
  },
  {
    "number": 2730,
    "title": "How to efficiently get a cluster ID given a sample ID of which sample has been added to a IVF index?",
    "created_at": "2023-02-27T19:21:52Z",
    "closed_at": "2024-06-27T23:01:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2730",
    "body": "Currently I have an ID of a vector which has been added to a IVF index and would like to get the cluster assignment. A naive way is firstly defining a mapping from the sample ID to cluster ID given the inverted list and then use the mapping to find the cluster assignment. But is there a build-in function to support fast mapping from sample ID to cluster ID without looking to the inverted list?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2730/comments",
    "author": "yzou2",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-03-06T11:42:49Z",
        "body": "There is no difference between the cluster ID and the inverted list. \r\nYou can get the cluster IDs with `index.quantizer.search(vector, 1)[1]`.\r\n\r\nIf I misunderstood the question, feel free to correct."
      }
    ]
  },
  {
    "number": 2720,
    "title": "IndexHNSWPQ how to assign a PQ centroids in python and cpp",
    "created_at": "2023-02-20T10:58:51Z",
    "closed_at": "2024-06-27T22:48:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2720",
    "body": "IndexHNSWPQ how to assign a PQ centroids in python and cpp",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2720/comments",
    "author": "bit-pku-zdf",
    "comments": [
      {
        "user": "bit-pku-zdf",
        "created_at": "2023-02-21T02:39:40Z",
        "body": "we train PQ centroids using some data, and want to assign them to HNSWPQ instead of using default trained PQï¼ŒWhich Index meets my demand? Thanks~"
      },
      {
        "user": "mdouze",
        "created_at": "2023-02-21T12:17:37Z",
        "body": "I don't understand the question. "
      }
    ]
  },
  {
    "number": 2688,
    "title": "HNSW index parameter problemï¼ŒHow to modify max_level ï¼Ÿ",
    "created_at": "2023-02-01T09:03:22Z",
    "closed_at": "2024-06-27T15:36:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2688",
    "body": "# Summary\r\n\r\nWe found that hnsw has a low recall rate on some data sets, and the difference in recall rate under different query retrievals is very obvious. \r\nWe guess that on some query retrievals, hnsw got stuck in false local neighbors.\r\n\r\n1) Is it possible to increase the randomness of HNSW by increasing the number of layers of HNSW to avoid falling into local neighbors?\r\n\r\n2) Python environment, how to modify max_level, max_level is determined in set_default_probas, is it supported to modify?\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [âœ“] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [âœ“] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2688/comments",
    "author": "myazi",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-02-13T03:07:06Z",
        "body": "`max_level` is determined automatically at build time. \r\nThe parameter that affects the exploration at construction time is `efConstruction`. It may be useful to increase that.  "
      }
    ]
  },
  {
    "number": 2617,
    "title": "Error when reading data in a stream",
    "created_at": "2022-12-12T15:43:51Z",
    "closed_at": "2024-06-24T15:13:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2617",
    "body": "# Summary\r\n\r\nI am reading a saved faiss model in a streaming fashion during containerized execution in python. I obtain a byte stream of the model and convert it into a faiss::IOReader using:\r\nf = io.BytesIO()\r\ntemp = faiss.PyCallbackIOReader(f.read)\r\ntemp = faiss.BufferedIOReader(temp)\r\ntest_nn = faiss.read_index(temp)\r\n\r\nHowever, I get a runtime error when I am reading a saved faiss model in a stream: Error in faiss::Index* faiss::read_index(faiss::IOReader*, int) at /__w/faiss-wheels/faiss-wheels/faiss/faiss/impl/index_read.cpp:414: Error: 'ret == (1)' failed: read error in : 0 != 1 (Resource temporarily unavailable).\r\n\r\nThis error does not occur if I just read the file using the filename such as:\r\ntest_nn = faiss.read_index(\"nnscorer_search_index.faiss\"),\r\nand am able to run inference with the model this way.\r\n\r\nI am not sure if I am missing a step when I am converting the byte object or is there a problem with the streamed data?\r\n\r\n# Platform\r\nOS: macOS 12.5.1\r\n\r\nFaiss version: 1.7.1\r\n\r\nInstalled from: pip \r\n\r\nFaiss compilation options: pip install faiss\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\nThe original model, \"nnscorer_search_index.faiss\" was saved using the same faiss version, 1.7.1.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2617/comments",
    "author": "Nav94",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-12-20T09:22:28Z",
        "body": "It is likely due to the reader implementation. You should make sure the read function waits until all the data is ready, possibly looping over reads from the underlying channel."
      }
    ]
  },
  {
    "number": 2520,
    "title": "Different behavior of merged IVFFlat indices when created with index_factory and when created with IndexIVFFlat function",
    "created_at": "2022-10-09T16:31:10Z",
    "closed_at": "2024-03-25T11:13:19Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2520",
    "body": "# Summary\r\nI'm trying to build different IVFFlat indices that I want to merge, the `IndexIVFFlat` function works just fine with `IndexFlatL2` as quantizer, but I want to also add specific Ids to the vectors to keep track of them once the indices are merged. So I want to use indices built with `index_factory` using `f\"IVF{n_clusters},Flat\"` as key. I verified that the indices built using `index_factory` return the same results as the ones built with the function, but once I merge them with `merge_into`, the results diverge and the merged index built with the index_factory performs much worse. \r\n\r\n# Platform\r\n\r\nOS: Ubuntu 22.04.1\r\n\r\nFaiss version: faiss-cpu 1.7.2 \r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\nI'm working with the sift1M dataset and splitting it in partitions then building the indices from the partitions and merging them, idk if this is the best way to do it, but the merging behavior is still worth figuring out.\r\n\r\n```\r\nvec_dim = 128\r\nn_clusters = 256\r\nsharded_index = None\r\nsharded_index_comp = None\r\nquantizer = faiss.IndexFlatL2(vec_dim)\r\nfor i in range(npartitions):\r\n    index_comp = faiss.index_factory(vec_dim, f\"IVF{n_clusters},Flat\")\r\n    index = faiss.IndexIVFFlat(quantizer, vec_dim, n_clusters)\r\n    index.train(partitions[i])\r\n    index.add(partitions[i])\r\n    index_comp.train(partitions[i])\r\n    index_comp.add(partitions[i])\r\n    \r\n    if None in [sharded_index, sharded_index_comp]:\r\n        sharded_index = index\r\n        sharded_index_comp = index_comp\r\n    else:\r\n        faiss.merge_into(sharded_index, index, True)\r\n        faiss.merge_into(sharded_index_comp, index_comp, True)\r\n```\r\n\r\nUsing this code `sharded_index` and `sharded_index_comp` don't return the same results and when checking its accuracy `sharded_index_comp` is much worse.\r\n\r\nAny idea for why that might be? Thanks for any help ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2520/comments",
    "author": "AntonioLopardo",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-10-09T21:48:38Z",
        "body": "The training should be the same, ie. the index should be saved after `index.train` and re-used for all partitions. Otherwise the indexes are not compatible. \r\nMaybe `merge_into` should check that. "
      },
      {
        "user": "AntonioLopardo",
        "created_at": "2022-10-10T07:57:51Z",
        "body": "Wouldn't that be the same as training only one index? In my experiment, I'm thinking that the indexes would be trained on different machines on different data and then merged, is the code above not a smart way to do that? \r\n\r\nI've tried different options for slightly different setups. For example, If I decide that I'm ok with not merging the indices but only the results I could use `merge_results_table_with` but I'm having issues with that too, I'm always getting wrong number or types of parameters errors but the signature of the function is not particularly difficult to understand just n, the number of queries, k the number of results per query, I0 the numpy array containing the ids of the top-k for each result, D0 the distances for the top-k to the query and similarly I1 and D1, keep_min and translation shouldn't matter for making the function run. Any clarification on the use of this function? \r\nI'm using the same indices trained above.\r\n\r\n`TypeError: Wrong number or type of arguments for overloaded function 'merge_result_table_with'.\r\n  Possible C/C++ prototypes are:\r\n    faiss::merge_result_table_with(size_t,size_t,int64_t *,float *,int64_t const *,float const *,bool,int64_t)\r\n    faiss::merge_result_table_with(size_t,size_t,int64_t *,float *,int64_t const *,float const *,bool)\r\n    faiss::merge_result_table_with(size_t,size_t,int64_t *,float *,int64_t const *,float const *)`"
      },
      {
        "user": "mlomeli1",
        "created_at": "2024-03-21T00:21:37Z",
        "body": "I ran the example provided (replacing it with synthetic data) and it seems that it already checks that the coarse quantisers are the same and throws and error:\r\n````\r\nimport faiss\r\nimport numpy as np\r\n\r\nvec_dim = 128\r\nn_clusters = 16\r\nn = 1000000\r\npartitions = []\r\nnpartitions = 4\r\nfor i in range(npartitions):\r\n    partitions.append(np.random.random((n // npartitions, vec_dim)).astype(\"float32\"))\r\n\r\nsharded_index = None\r\nsharded_index_comp = None\r\nquantizer = faiss.IndexFlatL2(vec_dim)\r\nfor i in range(npartitions):\r\n    index_comp = faiss.index_factory(vec_dim, f\"IVF{n_clusters},Flat\")\r\n    index = faiss.IndexIVFFlat(quantizer, vec_dim, n_clusters)\r\n    index.train(partitions[i])\r\n    index.add(partitions[i])\r\n    index_comp.train(partitions[i])\r\n    index_comp.add(partitions[i])\r\n\r\n    if None in [sharded_index, sharded_index_comp]:\r\n        sharded_index = index\r\n        sharded_index_comp = index_comp\r\n    else:\r\n        faiss.merge_into(sharded_index, index, True)\r\n        faiss.merge_into(sharded_index_comp, index_comp, True)\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_660770/1654454498.py in ?()\r\n     24         sharded_index = index\r\n     25         sharded_index_comp = index_comp\r\n     26     else:\r\n     27         faiss.merge_into(sharded_index, index, True)\r\n---> 28         faiss.merge_into(sharded_index_comp, index_comp, True)\r\n\r\n~/.conda/envs/oivf_nightly_latest/lib/python3.10/site-packages/faiss/swigfaiss_avx2.py in ?(index0, index1, shift_ids)\r\n   6470 \r\n   6471     :type shift_ids: boolean\r\n   6472     :param shift_ids:: translate the ids from index1 to index0->prev_ntotal\r\n   6473     \"\"\"\r\n-> 6474     return _swigfaiss_avx2.merge_into(index0, index1, shift_ids)\r\n\r\nRuntimeError: Error in virtual void faiss::IndexIVF::check_compatible_for_merge(const faiss::Index&) const at /home/circleci/miniconda/conda-bld/faiss-pkg_1691366670475/work/faiss/IndexIVF.cpp:1134: Error: 'v == v2' failed: coarse quantizers should be the same\r\n````\r\nI think this is what you had in mind @mdouze ? so we can close the issue."
      },
      {
        "user": "mlomeli1",
        "created_at": "2024-03-21T00:37:35Z",
        "body": "For completeness,  I am adding the code that uses `partition[0]` to train the IVFFlat indexes so they have the same coarse quantiser\r\n````\r\nimport faiss\r\nimport numpy as np\r\n\r\nvec_dim = 128\r\nn_clusters = 16\r\nn = 100000\r\npartitions = []\r\nnpartitions = 4\r\nfor i in range(npartitions):\r\n    partitions.append(np.random.random((n // npartitions, vec_dim)).astype(\"float32\"))\r\n\r\nsharded_index = None\r\nsharded_index_comp = None\r\nquantizer = faiss.IndexFlatL2(vec_dim)\r\nindex = faiss.IndexIVFFlat(quantizer, vec_dim, n_clusters)\r\nindex.train(partitions[0])\r\nindex_comp = faiss.index_factory(vec_dim, f\"IVF{n_clusters},Flat\")\r\nindex_comp.train(partitions[0])\r\nfor i in range(npartitions):\r\n    index.add(partitions[i])\r\n    index_comp.add(partitions[i])\r\n\r\n    if None in [sharded_index, sharded_index_comp]:\r\n        sharded_index = index\r\n        sharded_index_comp = index_comp\r\n    else:\r\n        faiss.merge_into(sharded_index, index, True)\r\n        faiss.merge_into(sharded_index_comp, index_comp, True)\r\n````"
      }
    ]
  },
  {
    "number": 2515,
    "title": "Get the IVF assignment in an IVFOPQ index.",
    "created_at": "2022-10-05T14:49:55Z",
    "closed_at": "2024-08-06T01:52:43Z",
    "labels": [
      "question",
      "Performance",
      "autoclose",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2515",
    "body": "# Summary\r\n\r\nI want to restore the IVF assignment, that is, which cluster does the query probe, after training an IVFOPQ index. Currently my solution is \r\n```\r\nindex = faiss.read_index(\"my_IVFOPQ_matrix\")\r\nivf_index = faiss.downcast_index(index.index)\r\nivf = faiss.downcast_index(ivf_index.quantizer)\r\n\r\nvt = faiss.downcast_VectorTransform(index.chain.at(0))\r\nnew_vt = faiss.LinearTransform(vt.d_in, vt.d_out, vt.have_bias)\r\nnew_vt.A = vt.A\r\nnew_vt.b = vt.b\r\nnew_vt.is_trained = True\r\nivf = faiss.IndexPreTransform(new_vt, ivf)\r\n\r\nquery = np.random.rand(10000, ivf.d).astype(np.float32)\r\nivf.search(query, 10)\r\n```\r\nHowever, the generated `ivf` index (actually its a Flat index prepended by an OPQ transformation) runs very slow. Why?\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Ubuntu 20.04\r\n\r\nFaiss version: faiss-gpu==1.7.2\r\n\r\nInstalled from: anaconda\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\nAs the snippet showed above.\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2515/comments",
    "author": "namespace-Pt",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-10-06T12:08:50Z",
        "body": "I don't know why it would be slow. It should be faster than running `index.sa_encode(query)`. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-30T01:52:13Z",
        "body": "This issue is stale because it has been open for 7 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-06T01:52:42Z",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ]
  },
  {
    "number": 2468,
    "title": "Is there a way to obtain codebook in product quantization index?",
    "created_at": "2022-09-14T05:13:31Z",
    "closed_at": "2022-09-28T12:56:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2468",
    "body": "# Summary\r\n\r\nI want to get the trained codebook in IVFPQ (the data of 256 centroids of M lists), is there a convenient way by using python API?\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Ubuntu 18.04 LTS\r\n\r\nFaiss version: 2cd84aa66308143d00aa0f39ccf29cbf48d243a6\r\n\r\nInstalled from: source\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2468/comments",
    "author": "SubjectNoi",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-09-14T08:03:08Z",
        "body": "please use \r\n```\r\nfaiss.contrib.inspect_tools.get_pq_centroids(index.pq)\r\n```"
      },
      {
        "user": "SubjectNoi",
        "created_at": "2022-09-15T08:21:20Z",
        "body": "> please use\r\n> \r\n> ```\r\n> faiss.contrib.inspect_tools.get_pq_centroids(index.pq)\r\n> ```\r\n\r\nThanks, I will try it"
      }
    ]
  },
  {
    "number": 2460,
    "title": "Associate a set of vectors with a single id",
    "created_at": "2022-09-07T21:38:36Z",
    "closed_at": "2022-09-28T12:58:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2460",
    "body": "During the indexing phase, is it possible to associate a set of vectors with a single id?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2460/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-09-09T08:13:54Z",
        "body": "yes, just do `add_with_ids` with the same ids."
      }
    ]
  },
  {
    "number": 2440,
    "title": "How to generate random hyperplane LSH",
    "created_at": "2022-08-31T15:23:41Z",
    "closed_at": "2022-09-28T13:02:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2440",
    "body": "# Summary\r\n\r\nI wanted to create, say 16 different set of 8 random hyperplanes to hash a target x.\r\nHowever, if I initiate a new instance of faiss.IndexLSH and faiss.IndexLSH.add(x), then do search, the same nearest neighbors are found, so obviously the 16 instances of faiss.IndexLSH used the same hashed functions.\r\nThis may be optimal for some purposes, but is there a way to generate vanilla random hyperplane hash functions?\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: macOS 12.3.1 <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: 1.7.2 <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: conda <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [X] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [X] Python\r\n\r\n# Reproduction instructions\r\n\r\n```\r\nq_vecs = np.random.normal(size=(10, 300)).astype(\"float32\")\r\nfor _ in range(16):\r\n    RP = faiss.IndexLSH(300, 8)\r\n    RP.add(x)\r\n    _, I = RP.search(q_vecs, 10)\r\n    print(I)\r\n```\r\nThen all the outputs are the same.\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2440/comments",
    "author": "samhuang17",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-09-05T13:25:27Z",
        "body": "The random seed can be changed via \r\n\r\n```\r\nRP.rrot.init(seed) \r\n```\r\n\r\nwhere seed is an integer."
      }
    ]
  },
  {
    "number": 2438,
    "title": "Strange GPU memory usage",
    "created_at": "2022-08-31T14:05:48Z",
    "closed_at": "2022-09-09T12:58:50Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2438",
    "body": "# Summary\r\n\r\nI observe a \"strange\" GPU memory usage so I would like to ask if this is expected behaviors.\r\n\r\nI use the following code to perform K-Means on my data represented by an array `X` of shape `(N, 192)`:\r\n\r\n```python\r\n    kmeans = faiss.Kmeans(X.shape[-1], K, niter=20, gpu=True, max_points_per_centroid=int(1e7))\r\n    kmeans.train(X)\r\n```\r\n\r\nwhere `K` is 1e4 or 2e4.\r\n\r\nWhen launching the training on a server with 8 GPUs:\r\n\r\n- If N = 784e4 and K = 1e4, then each GPU consumes 2385MB.\r\n- If I increase the number of clusters to K = 2e4, then GPU consumption only slightly increases: 2393MB per GPU. Is this normal?\r\n- If I increase the number of data points by ten times, i.e., N = 784e5 and K = 1e4, then only 2385MB per GPU. Why is the number of data points irrelevant?\r\n\r\nI'm asking these questions to make sure that all my data were actually used for the training.\r\n\r\nThank you very much in advance for your responses!\r\n\r\nBest regards.\r\n\r\n# Platform\r\n\r\nInstalled from: anaconda <!-- anaconda? compiled by yourself ? --> \r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2438/comments",
    "author": "netw0rkf10w",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-09-05T13:22:38Z",
        "body": "The training set is processed by batches, so the size of the training set is largely irrelevant to GPU mem usage. \r\nIf you want to be absolutely confident that data has been used for training, you can strategically insert a few NaN values in the input ;-) "
      },
      {
        "user": "netw0rkf10w",
        "created_at": "2022-09-05T13:26:04Z",
        "body": "> The training set is processed by batches, so the size of the training set is largely irrelevant to GPU mem usage.\r\n> If you want to be absolutely confident that data has been used for training, you can strategically insert a few NaN values in the input ;-)\r\n\r\nThank you very much for your reply! The NaN trick is very nice, thanks!\r\nIs there an option for increasing the batch size in this case? My GPUs have 32GB of memory, so using only 2GB is rather a pity.\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2022-09-05T16:14:18Z",
        "body": "Well the limiting factor of k-means is computation not IO or bandwidth so the 32GB will be useless in this case I'm afraid."
      },
      {
        "user": "netw0rkf10w",
        "created_at": "2022-09-09T12:58:50Z",
        "body": "Thanks, @mdouze!"
      }
    ]
  },
  {
    "number": 2414,
    "title": "How to visualize the progress of several faiss functions?",
    "created_at": "2022-08-13T13:09:53Z",
    "closed_at": "2022-09-28T13:19:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2414",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform \r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this --> \r\n\r\nOS: <!-- e.g. macOS 10.13.3 --> Linux Ubuntu 18.04 LTS\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a --> 1.7.2\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> pip install faiss-gpu\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\nHi, is there any way to visualize the progress of faiss functions e.g., search, index_gpu_to_cpu and write_index?\r\n\r\nFor example, can we use faiss with tqdm?\r\n\r\nThank you for your help!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2414/comments",
    "author": "Yb-Z",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-08-29T13:20:00Z",
        "body": "No the Faiss functions are viewed as atomic from the python side. \r\nIf you want to follow the progress of a search  search, it's simple to batch queries. "
      }
    ]
  },
  {
    "number": 2399,
    "title": "Implementation of scalar quantization",
    "created_at": "2022-07-29T03:40:25Z",
    "closed_at": "2022-08-29T10:39:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2399",
    "body": "The scalar quantization module saves 8bit data and float scalar. Is it converted to floating-point numbers  during operationï¼Œor used 8bit directly during operation",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2399/comments",
    "author": "dingtao1",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-08-29T10:39:16Z",
        "body": "It is converted to floating point because the queries are in floating point. \r\nIt would be interesting to see in what conditions this conversion could be avoided."
      }
    ]
  },
  {
    "number": 2389,
    "title": "index sync across processes",
    "created_at": "2022-07-19T06:51:57Z",
    "closed_at": "2022-09-28T13:22:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2389",
    "body": "What are the ways to sync faiss index across processes (e.g. gunicorn workers) ? ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2389/comments",
    "author": "dodler",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-07-20T09:54:30Z",
        "body": "There is nothing special in place. \r\nThe most straightforward way is to use IPC and serialize / deserialise. You can use pickle for that in python.\r\n\r\n"
      }
    ]
  },
  {
    "number": 2370,
    "title": "IndexShards ignores ids in shards",
    "created_at": "2022-06-30T12:33:28Z",
    "closed_at": "2022-07-01T18:53:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2370",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS:\r\n\r\nFaiss version: 1.7.2\r\n\r\nInstalled from:\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\nI did not expect IndexShards to ignore the ID's added to sub-indices, and I don't see how to efficiently work around this. So, I wanted to ask if this is the expected behavior, and - if so - how can I add shards with existing ID's to an IndexShards or IndexBinaryShards?\r\n\r\nI see that IndexShards has an add_with_ids, but this would require me to reconstruct an existing index's data. This would be difficult to use because I'm loading each index from disk with the IO_FLAG_MMAP to deal with memory constraints.\r\n\r\nHere is a POC of the behavior, the second assert fails, while I expected it to pass:\r\n```\r\nimport faiss\r\nimport numpy\r\n\r\n\r\ndef make_shard(dimension, data, id_0):\r\n    id_f = id_0 + data.shape[0]\r\n    print(f\"Make shard dim. {dimension} data shape {data.shape} ids {id_0}-{id_f - 1}\")\r\n    shard = faiss.IndexFlatL2(dimension)\r\n    shard_map = faiss.IndexIDMap(shard)\r\n    ids = numpy.arange(id_0, id_f)\r\n    shard_map.add_with_ids(data, numpy.arange(id_0, id_f))\r\n    return shard_map\r\n\r\n\r\ndef make_sharded_index(dimension, shards):\r\n    index_shards = faiss.IndexShards(dimension)\r\n    for i, shard in enumerate(shards):\r\n        index_shards.add_shard(shard)\r\n    return index_shards\r\n\r\n\r\ndimension = 32\r\nshard_cnt = 5\r\nshard_sz = 10\r\nkcnt = shard_sz + 1\r\nquery_row = 0\r\n\r\ndata = numpy.random.randn(shard_cnt * shard_sz, dimension).astype(numpy.float32)\r\n\r\nall_shards = [make_shard(dimension, data[i:i + shard_sz], i * shard_sz) for i in range(shard_cnt)]\r\n\r\ndata_query = data[query_row:query_row + 1]\r\n\r\nprint(f\"\\nQuery row {query_row} for each shard\")\r\nfor i, shard in enumerate(all_shards):\r\n    dists, ids = shard.search(data_query, kcnt)\r\n    print(f\"shard {i}: dist {dists[0]}\")\r\n    print(f\"shard {i}: ids {ids[0]}\\n\")\r\n\r\nprint(f\"Query row {query_row} in sharded index, in created order\")\r\nindex_shards = make_sharded_index(dimension, all_shards)\r\ndists, ids = index_shards.search(data_query, kcnt)\r\nprint(f\"shards dist {dists[0]}\")\r\nprint(f\"shards ids {ids[0]}\\n\")\r\nassert(ids[0][0] == query_row)\r\n\r\nprint(f\"Query row {query_row} in sharded index, out of order\")\r\nindex_shards = make_sharded_index(dimension, reversed(all_shards))\r\ndists, ids = index_shards.search(data_query, kcnt)\r\nprint(f\"shards rev dist {dists[0]}\")\r\nprint(f\"shards rev ids {ids[0]}\\n\")\r\nassert(ids[0][0] == query_row)\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2370/comments",
    "author": "mmaps",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-30T16:58:58Z",
        "body": "IndexShards has flag `successive_ids` to indicate whether the ids of each sub-index is relative to the last index of the previous shard. There is no way when the sub-indexes are built externally to tell if they are successive, and successive_ids is True by default. You should set is explicitly at construction time (or afterwards) with\r\n\r\n```\r\nindex_shards = IndexShards(dim, False, False) \r\n```\r\nthe first False is to indicate if search should be threaded.\r\n"
      },
      {
        "user": "mmaps",
        "created_at": "2022-07-01T18:53:39Z",
        "body": "Thanks! This fixes my issue. I had seen `successive_ids`, but didn't realize how it would affect existing ID's until I read your explanation.\r\n\r\nIf I set exaggerated (like offset +100) ID's in the sub-indexes, its more obvious that IndexShards is picking those up and not counting from 0. So, I wonder why it doesn't ignore `successive_ids` because it doesn't need to number them?"
      }
    ]
  },
  {
    "number": 2366,
    "title": "Farthest point search support further ?",
    "created_at": "2022-06-25T15:14:25Z",
    "closed_at": "2022-09-28T13:55:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2366",
    "body": "Hi!\r\n\r\nI think a high efficient farthest neighbor searching algorithm is good for some research (at least for me ^-^), that does the argmax ||Â·|| or argmin -||Â·|| instead.\r\nIs it possible this function would be supported in the further version ?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2366/comments",
    "author": "DonaldRR",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-28T00:05:51Z",
        "body": "This is not supported for L2 distance search (for Inner product it's simple to just query -x). I don't see an easy workaround for L2. "
      }
    ]
  },
  {
    "number": 2363,
    "title": "Does kmeans support INNER_PRODUCT distance?",
    "created_at": "2022-06-22T05:33:33Z",
    "closed_at": "2022-08-31T09:21:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2363",
    "body": "Hi, I want to know if the `faiss.IndexIVFFlat` index is using the kmeans method during training, can the distance calculation only use L2? Is it possible to use INNER_PRODUCT?\r\nCan I use INNER_PRODUCT as distance for kmeans?\r\n\r\nI want to implement it this way, is this correct?\r\n```\r\nquantizer = faiss.IndexFlatIP(emb_size)\r\nindex = faiss.IndexIVFFlat(quantizer, emb_size, ivf_centers_num,faiss.METRIC_INNER_PRODUCT)\r\n```\r\nAt the same time I also want to know what does `quantizer ` and `faiss.METRIC_INNER_PRODUCT` mean here?\r\n\r\nLooking forward to your replyï¼",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2363/comments",
    "author": "zhanghan9797",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-28T00:07:11Z",
        "body": "Inner product is supported.\r\nIt does not use L2 at search time, but inner product, as expected."
      },
      {
        "user": "meghbhalerao",
        "created_at": "2022-07-23T04:35:40Z",
        "body": "Hi - I am not sure this is the right place to ask this, but I would like to know the exact place in the source code where the k means clustering is happening which is using inner product as a similarity metric rather than the Euclidean L2 distance.\r\nIf inner product is being used as a similarity metric, then how are the cluster centers being calculated - since I dont think it makes sense to use the arithmetic mean as the cluster center since that is specific to the Euclidean space and distance metric. Thanks and please let me know if I am missing something.\r\n\r\n--Megh"
      },
      {
        "user": "mdouze",
        "created_at": "2022-07-26T10:33:54Z",
        "body": "Yes the arithmetic mean is used to compute centroids. \r\n\r\nIndeed the decreasing mean squared error guarantee does not hold with anything else than L2 assignment. However the inner product assignment is useful, especially in combination with the L2 normalization of the centroids after each iteration.\r\n"
      },
      {
        "user": "ghost",
        "created_at": "2022-09-23T16:45:38Z",
        "body": "Could you briefly describe how we can assign do inner product assignment in the python interface?\r\n\r\n> Yes the arithmetic mean is used to compute centroids.\r\n> \r\n> Indeed the decreasing mean squared error guarantee does not hold with anything else than L2 assignment. However the inner product assignment is useful, especially in combination with the L2 normalization of the centroids after each iteration.\r\n\r\n"
      }
    ]
  },
  {
    "number": 2361,
    "title": "Clone not supported for this type of IndexIVF",
    "created_at": "2022-06-19T19:27:41Z",
    "closed_at": "2022-06-28T16:41:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2361",
    "body": "# Summary\r\n\r\nI'm trying to move a trained composite index to a GPU, so that adding embeddings (~5.8B) to the index is faster. However, my IndexIVF cannot be cloned onto the GPU. Here's a minimal reproducing snippet:\r\n\r\n```\r\nimport faiss\r\n\r\nindex = faiss.index_factory(128, \"OPQ4_64,IVF16384_HNSW32,PQ16x4fs\")\r\nxt = faiss.rand((20000, 128))\r\nindex.train(xt)\r\n\r\nfaiss.index_cpu_to_all_gpus(index)\r\n```\r\n\r\nwhich yields:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tmp.py\", line 7, in <module>\r\n    faiss.index_cpu_to_all_gpus(index)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/__init__.py\", line 887, in index_cpu_to_all_gpus\r\n    index_gpu = index_cpu_to_gpus_list(index, co=co, gpus=None, ngpu=ngpu)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/__init__.py\", line 899, in index_cpu_to_gpus_list\r\n    index_gpu = index_cpu_to_gpu_multiple_py(res, index, co, gpus)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/__init__.py\", line 882, in index_cpu_to_gpu_multiple_py\r\n    index = index_cpu_to_gpu_multiple(vres, vdev, index, co)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/swigfaiss_avx2.py\", line 10278, in index_cpu_to_gpu_multiple\r\n    return _swigfaiss_avx2.index_cpu_to_gpu_multiple(provider, devices, index, options)\r\nRuntimeError: Error in virtual faiss::IndexIVF* faiss::Cloner::clone_IndexIVF(const faiss::IndexIVF*) at /root/miniconda3/conda-bld/faiss-pkg_1641228905850/work/faiss/clone_index.cpp:71: clone not supported for this type of IndexIVF\r\n```\r\n\r\nIs this expected behavior? The IndexIVF I'm using doesn't seem to be special. I've also tried:\r\n\r\n```\r\nindex_ivf = faiss.extract_index_ivf(index)\r\nindex_ivf = faiss.index_cpu_to_all_gpus(index_ivf)\r\n```\r\n\r\nwith similar results.\r\n\r\n# Platform\r\n\r\nOS: `Linux 53143a0863f8 5.4.0-94-generic #106~18.04.1-Ubuntu SMP Fri Jan 7 07:23:53 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux`\r\n(Docker image `nvidia/cuda:11.3.0-devel-ubuntu20.04`)\r\n\r\nFaiss version: \r\n\r\n```\r\nroot@fddb9798ebfc:/src# conda list\r\n# packages in environment at /opt/conda:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main\r\n_openmp_mutex             4.5                       1_gnu\r\nattrs                     21.4.0                   pypi_0    pypi\r\nblas                      1.0                         mkl\r\nbrotlipy                  0.7.0           py38h27cfd23_1003\r\nca-certificates           2022.4.26            h06a4308_0\r\ncertifi                   2022.5.18.1      py38h06a4308_0\r\ncffi                      1.15.0           py38hd667e15_1\r\ncharset-normalizer        2.0.4              pyhd3eb1b0_0\r\ncolorama                  0.4.4              pyhd3eb1b0_0\r\nconda                     4.13.0           py38h06a4308_0\r\nconda-content-trust       0.1.1              pyhd3eb1b0_0\r\nconda-package-handling    1.8.1            py38h7f8727e_0\r\ncryptography              37.0.1           py38h9ce1e76_0\r\ncudatoolkit               11.3.1               h2bc3f7f_2\r\neinops                    0.4.1                    pypi_0    pypi\r\nfaiss-gpu                 1.7.2           py3.8_h28a55e0_0_cuda11.3    pytorch\r\nfilelock                  3.7.1                    pypi_0    pypi\r\nfire                      0.4.0                    pypi_0    pypi\r\nhuggingface-hub           0.7.0                    pypi_0    pypi\r\nidna                      3.3                pyhd3eb1b0_0\r\nimportlib-metadata        4.11.1                   pypi_0    pypi\r\nintel-openmp              2021.4.0          h06a4308_3561\r\njsonlines                 3.0.0                    pypi_0    pypi\r\nld_impl_linux-64          2.35.1               h7274673_9\r\nlibfaiss                  1.7.2           hfc2d529_0_cuda11.3    pytorch\r\nlibffi                    3.3                  he6710b0_2\r\nlibgcc-ng                 9.3.0               h5101ec6_17\r\nlibgomp                   9.3.0               h5101ec6_17\r\nlibstdcxx-ng              9.3.0               hd4cf53a_17\r\nlibuv                     1.40.0               h7b6447c_0\r\nmkl                       2021.4.0           h06a4308_640\r\nmkl-service               2.4.0            py38h7f8727e_0\r\nmkl_fft                   1.3.1            py38hd3c417c_0\r\nmkl_random                1.2.2            py38h51133e4_0\r\nncurses                   6.3                  h7f8727e_2\r\nnumpy                     1.22.3           py38he7a7128_0\r\nnumpy-base                1.22.3           py38hf524024_0\r\nopenssl                   1.1.1o               h7f8727e_0\r\npackaging                 21.3                     pypi_0    pypi\r\npip                       21.2.4           py38h06a4308_0\r\npycosat                   0.6.3            py38h7b6447c_1\r\npycparser                 2.21               pyhd3eb1b0_0\r\npyopenssl                 22.0.0             pyhd3eb1b0_0\r\npyparsing                 3.0.9                    pypi_0    pypi\r\npysocks                   1.7.1            py38h06a4308_0\r\npython                    3.8.13               h12debd9_0\r\npytorch                   1.10.2          py3.8_cuda11.3_cudnn8.2.0_0    pytorch\r\npytorch-mutex             1.0                        cuda    pytorch\r\npyyaml                    6.0                      pypi_0    pypi\r\nreadline                  8.1.2                h7f8727e_1\r\nregex                     2022.6.2                 pypi_0    pypi\r\nrequests                  2.27.1             pyhd3eb1b0_0\r\nretro-pytorch             0.3.7                    pypi_0    pypi\r\nruamel_yaml               0.15.100         py38h27cfd23_0\r\nsentencepiece             0.1.96                   pypi_0    pypi\r\nsetuptools                61.2.0           py38h06a4308_0\r\nsix                       1.16.0             pyhd3eb1b0_1\r\nsqlite                    3.38.2               hc218d9a_0\r\ntermcolor                 1.1.0                    pypi_0    pypi\r\ntk                        8.6.11               h1ccaba5_0\r\ntokenizers                0.12.1                   pypi_0    pypi\r\ntqdm                      4.63.0             pyhd3eb1b0_0\r\ntransformers              4.20.0                   pypi_0    pypi\r\ntyping_extensions         4.1.1              pyh06a4308_0\r\ntzdata                    2022a                hda174b7_0\r\nurllib3                   1.26.8             pyhd3eb1b0_0\r\nwheel                     0.37.1             pyhd3eb1b0_0\r\nxz                        5.2.5                h7b6447c_0\r\nyaml                      0.2.5                h7b6447c_0\r\nzipp                      3.8.0                    pypi_0    pypi\r\nzlib                      1.2.12               h7f8727e_1\r\n```\r\n\r\nInstalled from: Anaconda\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [X] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [X] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2361/comments",
    "author": "mitchellgordon95",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-27T23:50:11Z",
        "body": "The index type that you build here is tuned for CPU indexing. \r\n\r\n\"OPQ4_64,IVF16384_HNSW32,PQ16x4fs\"\r\n\r\n- IVFx_HNSW is not supported (and not necessary) on GPU: use IVF16386\r\n\r\n- the \"fs\" variant of PQ is not supported on GPU. Only 8-bit PQ is supported (and more accurate anyways). \r\n\r\nSo this boils down to \"OPQ8_64,IVF16386,PQ8\"\r\n"
      },
      {
        "user": "mitchellgordon95",
        "created_at": "2022-06-28T16:41:30Z",
        "body": "Ah, thank you for replying! This answers my question so I will close the issue. \r\n\r\nI am not sure I will be able to get away with \"IVF...,PQ8\" since I have >5B vectors, but I will do some benchmarking to see what works for me.\r\n\r\n"
      }
    ]
  },
  {
    "number": 2346,
    "title": "How to use single thread when do batch search",
    "created_at": "2022-06-07T02:20:36Z",
    "closed_at": "2022-08-31T09:24:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2346",
    "body": "# Summary\r\n\r\nwe want to do ivfpq search by single thread, so we use pthread function to  bind ivfpq search on a cpu core, how to do it.\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: ubuntu 18.04\r\n\r\nFaiss version: last\r\n\r\nInstalled from: compiled \r\n\r\n\r\n\r\nRunning on:\r\n- [ Ã— ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ Ã—] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2346/comments",
    "author": "jackhouchina",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-07T09:05:31Z",
        "body": "you can call omp_set_num_threads(1) to avoid the openmp overhead. "
      },
      {
        "user": "jackhouchina",
        "created_at": "2022-06-07T09:16:24Z",
        "body": "ok,ths"
      }
    ]
  },
  {
    "number": 2343,
    "title": "Return values of  faiss.IndexFlatIP ",
    "created_at": "2022-06-01T11:02:24Z",
    "closed_at": "2022-06-07T09:30:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2343",
    "body": "# Summary\r\n```\r\nI have created a faiss IndexFlatIP index and mapped it using the below code\r\nindex = faiss.IndexIDMap(faiss.IndexFlatIP(768))\r\nids = np.array(transaction_ids)\r\nindex.add_with_ids(stored_embeddings,ids)\r\n\r\n```\r\n\r\nFaiss version: (1.7.2)\r\n\r\nInstalled from: pypi.org\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on: GPU\r\n\r\nInterface: Python\r\n\r\n# Reproduction instructions\r\nWhen I search a query on the index I get the following response:\r\n`sims, top_k = index.search(query_vector, k)`\r\n\r\n[264.67917 248.29854 241.13202 240.96162 239.8863 ] [4871818555479304801  353409970080440929   15616971026569873 15594186896145345   15624107408338433]\r\n\r\nIs the IndexFlatIP returning cosine similarity and is it ranked in descending order by default or do I have to rank them in the post processing step\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2343/comments",
    "author": "kai5gabriel",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-07T09:10:59Z",
        "body": "Indeed it is ranked by descending order, no need to reorder."
      }
    ]
  },
  {
    "number": 2342,
    "title": "How to use faiss with more threads on multi gpus?",
    "created_at": "2022-05-31T08:31:29Z",
    "closed_at": "2022-08-31T09:24:09Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2342",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\nI want to use 4 gpus and faiss_multigpu on slurm cluster environment. Here is my srun script: srun --gres=gpu:4 -n1 --ntasks-per-node=1 python xx.py. If in this way, what is the default number of threads? How can I check the number of threads?\r\nIf I want to use more threads to speed up in parallel, how to change the code or script?\r\nMany thanks!\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <cluster, slurm>\r\n\r\nFaiss version: <version 1.6.5>\r\n\r\nInstalled from: < anaconda> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n```python\r\nngpus = faiss.get_num_gpus()\r\ncpu_index = faiss.index_factory(D, \"Flat\") #use exact search\r\n\r\nco = faiss.GpuMultipleClonerOptions()\r\nco.useFloat16 = True\r\nco.usePrecomputed = False\r\nco.shard = True\r\ngpu_index = faiss.index_cpu_to_all_gpus(cpu_index, co, ngpu=ngpus)\r\ngpu_index.add(features) \r\ndists, nbrs = gpu_index.search(query, k=k)\r\n```\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2342/comments",
    "author": "glennccc",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-07T09:12:43Z",
        "body": "There is one thread per GPU and adding more threads will not speed up the search because the GPU is doing all the computations."
      },
      {
        "user": "glennccc",
        "created_at": "2022-06-07T09:30:30Z",
        "body": "> There is one thread per GPU and adding more threads will not speed up the search because the GPU is doing all the computations.\r\n\r\nWhen I search a dataset about **600000** features, it cost about **37.7642s for 1 gpus** and **33.7777s for 2 gpus**. Do you think this is reasonable?\r\n**If the codes and scripts above correct? If not, what is the best way to use?**\r\n"
      },
      {
        "user": "glennccc",
        "created_at": "2022-06-07T09:44:04Z",
        "body": "> There is one thread per GPU and adding more threads will not speed up the search because the GPU is doing all the computations.\r\n\r\nThe time-consuming of using multiple gpus is not as small as I thought. What is the matter with this?"
      },
      {
        "user": "mdouze",
        "created_at": "2022-07-08T15:43:12Z",
        "body": "> > There is one thread per GPU and adding more threads will not speed up the search because the GPU is doing all the computations.\r\n> \r\n> When I search a dataset about **600000** features, it cost about **37.7642s for 1 gpus** and **33.7777s for 2 gpus**. Do you think this is reasonable? **If the codes and scripts above correct? If not, what is the best way to use?**\r\n\r\nyes because you shard the dataset rather than replicating it (co.shards=True). Sharding is useful only if the dataset does not fit on the GPU."
      }
    ]
  },
  {
    "number": 2341,
    "title": "Is there a way to rebuild index in IVF-index after update ivf centers?",
    "created_at": "2022-05-29T08:40:34Z",
    "closed_at": "2024-07-22T22:15:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2341",
    "body": "Hi,\r\nFirst, i init a ivf index like this:\r\n```\r\nquantizer = faiss.IndexFlatIP(emb_size)\r\nindex = faiss.IndexIVFFlat(quantizer, emb_size, ivf_centers_num, faiss.METRIC_INNER_PRODUCT)\r\n```\r\nThen, I update IndexIVFFlat's centers like this:\r\n```\r\ncoarse_quantizer = faiss.downcast_index(index.quantizer)  \r\nfaiss.copy_array_to_vector(center_vecs.ravel(),coarse_quantizer.xb)\r\n```\r\nAfter that,i want to add corpus to this index and search, however this error occur:\r\n```\r\nterminate called after throwing an instance of 'faiss::FaissException'\r\n  what():  Error in faiss::IndexIVF::search_preassigned(faiss::Index::idx_t, const float*, faiss::Index::idx_t, const idx_t*, const float*, float*, faiss::Index::idx_t*, bool, const faiss::IVFSearchParameters*, faiss::IndexIVFStats*) const::<lambda(faiss::Index::idx_t, float, float*, faiss::Index::idx_t*)> at /project/faiss/faiss/IndexIVF.cpp:469: Error: 'key < (idx_t)nlist' failed: Invalid key=140461314990592 nlist=10000\r\n```\r\nHow can I create a new ivf index according my ivf_centers ? Thanks in advance.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2341/comments",
    "author": "zhanghan9797",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-05-30T09:17:01Z",
        "body": "So this is a very low level way of accessing the centroids. \r\nThe safest way of doing this is to do \r\n```\r\nindex.quantizer.reset()\r\nindex.quantizer.add(center_vecs) \r\n```\r\nalso note that \"re-training\" an index is not recommended, so this operation should happen only when the index is still empty.\r\n\r\n"
      },
      {
        "user": "zhanghan9797",
        "created_at": "2022-06-01T10:15:38Z",
        "body": "Why does the query latency become very large after updating the centers?\r\n```\r\nstart = time.perf_counter()\r\n_, dev_I = index.search(query_embeddings, 1000)\r\nend = time.perf_counter() - start\r\nprint('{:.6f}s for the total'.format(end))\r\nprint('{:.6f}s for the every'.format(end / np.array(query_embeddings).shape[0]))\r\n```"
      }
    ]
  },
  {
    "number": 2339,
    "title": "How to create an IVF index on `uint8`/`int8` data?",
    "created_at": "2022-05-24T09:15:58Z",
    "closed_at": "2024-07-22T22:14:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2339",
    "body": "I usually use the following script for generating `FAISS-IVF` indices on floating point data:\r\n```\r\nimport numpy as np\r\nimport faiss\r\n\r\nA = np.fromfile('base.bin', dtype=np.int32)\r\nshape = A[:2]\r\nA = np.fromfile('base.bin', dtype=np.float32)\r\nA = A[2:].reshape(shape)\r\nprint(shape)\r\ndim = A.shape[1]\r\nindex = faiss.IndexIVFFlat(faiss.IndexFlatL2(dim), dim, 3200, faiss.METRIC_L2)\r\nindex.train(A)\r\nindex.add(A)\r\nfaiss.write_index(index, 'my_index.index')\r\n```\r\n\r\nBut on using a dataset with `uint8`/`int8` vectors, the following script gives an error:\r\n```\r\nimport numpy as np\r\nimport faiss\r\n\r\nA = np.fromfile('base.bin', dtype=np.int32)\r\nshape = A[:2]\r\nA = np.fromfile('base.bin', dtype=np.uint8)\r\nA = A[8:].reshape(shape)\r\nprint(shape)\r\ndim = A.shape[1]\r\nindex = faiss.IndexIVFFlat(faiss.IndexFlatL2(dim), dim, 3200, faiss.METRIC_L2)\r\nindex.train(A)\r\nindex.add(A)\r\nfaiss.write_index(index, 'my_index.index')\r\n```\r\n```\r\nFile \"ivf_runner.py\", line 12, in <module>\r\n    index.train(A)\r\n  File \"/usr/local/lib/python3.6/dist-packages/faiss/__init__.py\", line 280, in replacement_train\r\n    self.train_c(n, swig_ptr(x))\r\n  File \"/usr/local/lib/python3.6/dist-packages/faiss/swigfaiss.py\", line 4610, in train\r\n    return _swigfaiss.IndexIVF_train(self, n, x)\r\nTypeError: in method 'IndexIVF_train', argument 3 of type 'float const *'\r\n```\r\n\r\nHow can I create an index with `uint8`/`int8` vector data? Thanks in advance.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2339/comments",
    "author": "ShikharJ",
    "comments": [
      {
        "user": "shiwanghua",
        "created_at": "2023-07-24T08:53:32Z",
        "body": "Now still not supported ? Is the any open source solution ?"
      },
      {
        "user": "shiwanghua",
        "created_at": "2023-07-24T08:54:36Z",
        "body": "Maybe should update the IVF file and Clustering file."
      }
    ]
  },
  {
    "number": 2337,
    "title": "SDC IVFPQ",
    "created_at": "2022-05-22T15:03:03Z",
    "closed_at": "2022-05-26T03:58:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2337",
    "body": "Hi\r\n\r\nDo you provide SDC (Symmetric distance computation)  IVFPQ?  Thanks!\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2337/comments",
    "author": "WenzhengZhang",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-05-26T01:12:04Z",
        "body": "No. I am not sure it makes sense because when using multi-probe, the query vector must be decompressed anyways to get new PQ codes."
      }
    ]
  },
  {
    "number": 2319,
    "title": "Unexpected index size after inserting elements with same ids with DirectMap.Hashtable",
    "created_at": "2022-05-05T12:38:23Z",
    "closed_at": "2024-07-22T22:12:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2319",
    "body": "# Summary\r\n\r\nI am using an `IVF` index with `DirectMap.Hashtable` to allow adding elements with arbitrary ids. After training the index, and adding 10000 elements (with a specific set of ids), checking the index size through `index.ntotal` correctly returns 10000 . However, if I add again the same set of elements **with the same ids** the index size increases to 20000.\r\n\r\nI don't know if I am missing something but what I would expect here is that if I add elements with `ids` that are currently in use in the index, those elements in the index will be replaced with the new ones. However, when I check the `index.ntotal` it returns 20000, like if the index had grown with 10000 new elements. But in addition, if I try to reconstruct the elements ranging from 10000 to 19999, a key not found exception is raised (as expected) because these `ids` do not exist.\r\n\r\nAm I missing something? Is it not the correct way to check the index size by querying `index.ntotal`?\r\n\r\nThanks\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\nOS: Windows 10\r\n\r\nFaiss version: 1.7.2\r\n\r\nInstalled from: faiss-cpu from pip\r\n\r\nFaiss compilation options: None\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n```\r\nn = 10000\r\nd = 384\r\nx = np.random.randn(n, d).astype(np.float32)\r\nids = np.arange(n, dtype=np.int64)\r\n\r\nindex = faiss.index_factory(384, f'IVF{int(2 * math.sqrt(n))},Flat', faiss.METRIC_INNER_PRODUCT)\r\nindex.set_direct_map_type(faiss.DirectMap.Hashtable)\r\nindex.train(x)\r\n\r\nprint(index.ntotal)  # 0\r\n\r\nindex.add_with_ids(x, ids)\r\nprint(index.ntotal)  # 10000\r\n\r\nindex.add_with_ids(x, ids) # what I expected here is to replace the ids with the new x\r\nprint(index.ntotal)  # 20000 ??\r\n\r\nfor i in range(2*n):\r\n    try:\r\n        index.reconstruct(i)\r\n    except Exception as error:\r\n        print(f'Error reconstructing key {i}')  # from i=10000 to 19999 throw key not found exceptions\r\n```\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2319/comments",
    "author": "javierjuan",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-05-05T17:40:00Z",
        "body": "This is expected. The index is perfectly fine with having several vectors with the same ids (they could be class labels for example). \r\nThe direct map is used only for reconstruction. If several vectors have the same id, then an arbitrary one will be returned by `reconstruct(vector_id)`. \r\n"
      },
      {
        "user": "javierjuan",
        "created_at": "2022-05-06T06:41:46Z",
        "body": "Sorry, I did not know this was the expected behavior. So, is there an index (or configuration) that allows only one vector per id, and updates the \"old\" vector if another one is added with an id that is currently in use?\r\n\r\nMoreover, right now I have a new doubt. You said that \"The direct map is used only for reconstruction\" but if I want to add and remove elements with non-sequential ids, even if I never reconstruct them, I still need DirectMap.Hashtable, is it correct?\r\n\r\nThanks"
      }
    ]
  },
  {
    "number": 2314,
    "title": "Why the max and min sample per center I set are not workingï¼Ÿ",
    "created_at": "2022-05-02T17:41:28Z",
    "closed_at": "2024-06-17T12:37:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2314",
    "body": "<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a --> 1.7.2 \r\n\r\nRunning on: GPU\r\n\r\nInterface:  Python\r\n\r\n# Summary\r\n\r\nI import faiss and use Kmeans clustering, and set the maximum and minimum number of each center, but in the actual operation process, the results obtained have many samples of their own kind, where am I wrong? , the code is as follows\r\n\r\n\r\n        clus = faiss.Clustering(d, k)\r\n        clus.verbose = False\r\n        clus.niter = 20\r\n        clus.nredo = 5\r\n        clus.seed = seed\r\n        clus.gpu = True\r\n        clus.max_points_per_centroid = 10000\r\n        clus.min_points_per_centroid = 5\r\n        res = faiss.StandardGpuResources()\r\n        cfg = faiss.GpuIndexFlatConfig()\r\n        cfg.useFloat16 = False\r\n        cfg.device = 0\r\n        index = faiss.GpuIndexFlatL2(res, d, cfg)  \r\n\r\n        clus.train(x, index)   \r\n        D, I = index.search(x, 1)\r\n        im2cluster = [int(n[0]) for n in I]\r\n\r\nI have set  'clus.max_points_per_centroid = 10000' and 'clus.min_points_per_centroid = 5', but it doesn't work.\r\nThe result of 'im2cluster' is like [1,1,1,1,1,1,1,1000,1,1,1,1,1,2000], that too many of a class contains a sample. I don't want to get results like this. \r\nIs there something wrong with my settings, or is there something wrong with my data?\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2314/comments",
    "author": "jianxiangyu",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-05-05T17:28:14Z",
        "body": "min_/max_points_per_centroid just sets the ratio of nb of training points over number of centroids. \r\nIt does not guarantee that the clusters are balanced. \r\nYou probably have many duplicates or near-duplicates in your dataset, or outliers that are far away. "
      },
      {
        "user": "jianxiangyu",
        "created_at": "2022-05-05T17:35:12Z",
        "body": "> min_/max_points_per_centroid just sets the ratio of nb of training points over number of centroids. \n> \n> It does not guarantee that the clusters are balanced. \n> \n> You probably have many duplicates or near-duplicates in your dataset, or outliers that are far away. \n\nThanks for your reply! I will check my dataset."
      }
    ]
  },
  {
    "number": 2311,
    "title": "Is there a way to change invlists in IVF index?",
    "created_at": "2022-04-28T02:13:29Z",
    "closed_at": "2022-04-29T15:54:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2311",
    "body": "Hi,\r\nHow can I change the content (codes, ids) of the invlists for an IVF index:\r\nI replaced the original IVF centroid embeddings by my own centroid embeddings (I kept the number of centroids and the database embeddings fixed). So the new centroids would form new clusters of nearest neighbors, which means the invlists need to be updated.  Could you please tell me how to update the invlists in this case?\r\nThanks!\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2311/comments",
    "author": "WenzhengZhang",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-04-28T06:36:51Z",
        "body": "In the strict sense, any change to the coordinates of a centroid could impact the assignment of all vectors stored in the index: the new centroid can \"attract\" any point into its inverted list. So normally you should re-index all the vectors. \r\n\r\nHowever, if you want to modify only the affected inverted list, then you can retrieve its current content with `faiss.contrib.inspect_tools.get_invlist`, clear the invlist with `index_ivf.invlists.resize(list_no, 0) and re-add all the vectors from there. \r\n"
      }
    ]
  },
  {
    "number": 2285,
    "title": "ProductQuantizer  compute_codes get wrong codes when nbits not 8",
    "created_at": "2022-04-04T00:18:25Z",
    "closed_at": "2022-04-04T12:31:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2285",
    "body": "    \r\n\r\n\r\n    d = 10\r\n    n = 400000\r\n    cs = 5\r\n    np.random.seed(123)\r\n    x = np.random.random(size=(n, d)).astype('float32')\r\n    testInputs=np.random.random(size=(1, d)).astype('float32')\r\n    print(testInputs)\r\n    pq = faiss.ProductQuantizer(d, cs,6)\r\n    pq.verbose=True\r\n    pq.train(x)\r\n    codes=pq.compute_codes(testInputs)\r\n    #here expect 5 code range from 0-64, but get 4 and also code number not range 0-64\r\n    print(codes.shape)\r\n  \r\n   ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2285/comments",
    "author": "jasstionzyf",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-04-04T07:51:04Z",
        "body": "This is because the codes are packed into ceil(5 * 6 / 8) = 4 bytes.  \r\nTo access the individual codes, use `BitstringReader`: \r\n\r\n```python\r\nbs = faiss.BitstringReader(faiss.swig_ptr(codes[0]), codes.shape[1])\r\nfor i in range(cs): \r\n    print(bs.read(6))  # read 6 bits at a time\r\n````\r\n\r\nAdmittedly, the `BitstringReader` API could be made more python friendly."
      },
      {
        "user": "jasstionzyf",
        "created_at": "2022-04-04T12:28:55Z",
        "body": "@mdouze   thanks very much!"
      }
    ]
  },
  {
    "number": 2266,
    "title": "How can I read the centroid vectors from the index file?",
    "created_at": "2022-03-17T00:02:22Z",
    "closed_at": "2024-06-17T11:32:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2266",
    "body": "How can I read the centroid vectors from the index file? kmeans.index is generated from the following code:\r\n\r\nncentroids = 1024\r\nniter = 20\r\nverbose = True\r\nd = x.shape[1]\r\nkmeans = faiss.Kmeans(d, ncentroids, niter=niter, verbose=verbose)\r\nkmeans.train(x)",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2266/comments",
    "author": "HossamAmer12",
    "comments": [
      {
        "user": "HossamAmer12",
        "created_at": "2022-03-17T15:33:27Z",
        "body": "This index is IndexFlatL2\r\n"
      },
      {
        "user": "KinglittleQ",
        "created_at": "2022-03-18T08:02:42Z",
        "body": "```\r\n# assume index = kmeans.index\r\nindex = faiss.read_index(\"your/index/file\")\r\ncentroids = faiss.vector_to_array(index.xb).reshape(-1, d)\r\n```"
      },
      {
        "user": "HossamAmer12",
        "created_at": "2022-03-18T10:54:58Z",
        "body": "That's what happens when I try the code snippet:\r\n'IndexFlat' object has no attribute 'xb'"
      },
      {
        "user": "HossamAmer12",
        "created_at": "2022-03-18T11:19:54Z",
        "body": "This is the correct code:\r\n```\r\nindex     = faiss.read_index(\"your/index/file\")\r\ncentroids = faiss.rev_swig_ptr(index.get_xb(), nCentroids * dim)\r\ncentroids = centroids.reshape(nCentroids, dim)\r\n```\r\n"
      },
      {
        "user": "HossamAmer12",
        "created_at": "2022-03-18T11:58:21Z",
        "body": "Want to give feedback about this --> get_xb() function name is really ambiguous to what it really does."
      },
      {
        "user": "KinglittleQ",
        "created_at": "2022-03-18T13:39:04Z",
        "body": "> This is the correct code:\r\n> \r\n> ```\r\n> index     = faiss.read_index(\"your/index/file\")\r\n> centroids = faiss.rev_swig_ptr(index.get_xb(), nCentroids * dim)\r\n> centroids = centroids.reshape(nCentroids, dim)\r\n> ```\r\n\r\nYes, you're right. I forgot that `IndexFlat.xb` has been changed to `get_xb()` since `IndexFlatCodes` was introduced."
      }
    ]
  },
  {
    "number": 2259,
    "title": "Chain an existing OPQMatrix with a new IVFPQ index",
    "created_at": "2022-03-15T08:01:49Z",
    "closed_at": "2022-03-16T03:47:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2259",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\nI have trained an IVFOPQ index and I want to migrate the OPQMatrix to the top of a new(untrained) IVFPQ index. Here is my code:\r\n```\r\nimport faiss\r\n\r\nold = faiss.index_factory(128, \"OPQ16,IVF4,PQ16\")   # suppose it is already trained, the opqmatrix is not empty\r\nnew = faiss.index_factory(128, \"IVF4,PQ16\")   # a new index that I want to prepend an OPQMatrix to\r\n\r\nvector_transform = faiss.downcast_VectorTransform(old.chain.at(0))\r\nold_opq_matrix = vector_transform.A\r\nold_opq_array = faiss.vector_to_array(old_opq_matrix)\r\n\r\nnew_opq_matrix = faiss.OPQMatrix(vector_transform.d_in, 1, vector_transform.d_out)\r\nfaiss.copy_array_to_vector(old_opq_array, new_opq_matrix.A)\r\nnew_index = faiss.IndexPreTransform(new_opq_matrix, new)\r\n```\r\nI don't think it's a good idea that we should copy the vector to a new array then copy them back. Is there a easier way to do this? I just need to chain the **old** VectorTransform and a **new** IVFPQ. \r\n\r\nI tried the following but none of them worked (throwing segmentation error when adding embeddings to the new index):\r\n```\r\nnew_index = faiss.IndexPreTransform(old.chain.at(0), new)\r\nnew_index = faiss.IndexPreTransform(faiss.downcast_VectorTransform(old.chain.at(0)), new)\r\n```\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: 1.7.1 <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: pip <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2259/comments",
    "author": "namespace-Pt",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-03-15T17:04:49Z",
        "body": "Maybe the easiest is to do \r\n```\r\nold = faiss.index_factory(128, \"OPQ16,IVF4,PQ16\")   # suppose it is already trained, the opqmatrix is not empty\r\nnew = faiss.index_factory(128, \"OPQ16,IVF4,PQ16\")   # a new index that I want to prepend an OPQMatrix to\r\n\r\n... train old opq\r\n\r\nopq_old = faiss.downcast_VectorTransform(old.chain.at(0))\r\nopq_new = faiss.downcast_VectorTransform(new.chain.at(0))\r\nopq_new.A = opq_old.A\r\nopq_new.b = opq_old.b\r\nopq_new.is_trained = opq_old.is_trained\r\n```\r\n"
      },
      {
        "user": "namespace-Pt",
        "created_at": "2022-03-16T03:47:51Z",
        "body": "Got it. Thank you."
      },
      {
        "user": "namespace-Pt",
        "created_at": "2022-03-16T03:48:39Z",
        "body": "@mdouze BTW, I wonder is there an Inner Product version of HNSWPQ?"
      },
      {
        "user": "mdouze",
        "created_at": "2022-03-31T11:25:51Z",
        "body": "no"
      }
    ]
  },
  {
    "number": 2221,
    "title": "Most efficient way to do a kNN search on an array using points from another array?",
    "created_at": "2022-02-15T20:00:33Z",
    "closed_at": "2022-04-20T16:00:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2221",
    "body": "I am using faiss for a problem and I have a question on how I could speed it up.\r\nI have two arrays, one big and the other smaller, each consisting of independent measurements. I would like to do a kNN search on the larger array using points from the smaller one. The issue for me is how to do this most efficiently, given that each row in these arrays consists of independent measurements.\r\n\r\nAn example using toy data:\r\n```python\r\n# 1000 independent measurements of 100 data points each.\r\nm, d, s = 1000, 100, 10\r\nt = np.random.random(m * d * 3).reshape(m, d, 3).astype('float32')\r\nq = np.random.random(m * s * 3).reshape(m, s, 3).astype('float32')\r\n\r\n# objective: find nearest neighbors for s:\r\nres = faiss.StandardGpuResources()\r\nif_flat = faiss.index_factory(3, 'Flat')  # always 3 dim \r\nco = faiss.GpuClonerOptions()\r\nco.useFloat16 = True\r\n\r\nfor mi in range(m):\r\n    index = faiss.index_cpu_to_gpu(res, 1, if_flat, co)\r\n    xb, xq = t[mi, :], q[mi, :]\r\n    index.train(xb)\r\n    index.add(xb)\r\n    index.search(xq, 20)\r\n```\r\nIs there a way I can avoid looping over the measurements? Or better yet, generate an index that treats all m measurements independently, i.e. confine searches for each q[mi, :] array to only the corresponding t[mi, :]?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2221/comments",
    "author": "danielpastor97",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-02-16T17:43:10Z",
        "body": "Faiss is not well optimized for low-dimensional data (like dim=3)\r\nWhat I don't get is which dimension of the problem gets large, is it m ? because in your example no dimension is large enough to even bother using a GPU. "
      },
      {
        "user": "danielpastor97",
        "created_at": "2022-02-16T19:33:29Z",
        "body": "@mdouze Thank you so much for your response!\r\n\r\nI have done benchmarks and confirmed that there are significant performance gains to using GPUs (even using the code block in the toy example I presented). Using the toy data above, the dimension that increases is `d`. Its size can be well above 10^7. And we would like to explicitly support this and allow for even larger values (10^8 or more).\r\n\r\nDespite the clear performance gains that we get, I suspect it should be possible to achieve further optimization. Perhaps by being smarter in how we generate the index? Initially, I thought we could use dynamic indexing (which would be extremely helpful since our measurements are independent, but still highly correlated), but reading through other issues here, I understand that that seems to be, understandably, not supported by `faiss`. But there may be other ways to speed things up, perhaps?\r\nOur current code may also not be the most optimal for the current problem (e.g. after setting `d=10**6`)."
      }
    ]
  },
  {
    "number": 2177,
    "title": "why SQ8 faster than float version ï¼Ÿ",
    "created_at": "2022-01-05T03:39:54Z",
    "closed_at": "2022-04-20T16:02:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2177",
    "body": "I see in  faiss/impl/scale_quantizer.cpp , to compute distance between a float query and a uint_8  code, FAISS first decode the code and then compute distance with float query and float data. Compared with the float version, this equal to  decode + float computation, so why SQ8 index is fatser than float version?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2177/comments",
    "author": "NJU-yasuo",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-01-11T09:02:39Z",
        "body": "In practical use cases, SQ8 is usually faster because the limiting factor is memory access time rather than floating-point computations. \r\n"
      }
    ]
  },
  {
    "number": 2173,
    "title": "search_centroid on GpuIndexIVFPQ",
    "created_at": "2021-12-31T14:47:52Z",
    "closed_at": "2022-01-11T10:38:57Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2173",
    "body": "# Summary\r\n\r\nI have a GPU index. I wish to perform `faiss.search_centroids()` on it, and benefit from GPU parallelism. How do to so (using Python)?\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nInstalled from: anaconda (conda-forge), also same thing on Colab.\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\nThe following code will work on a cpu index, or a GPU index if it has been converted to a CPU index (via `cpuindex = faiss.index_gpu_to_cpu(index)`).\r\n\r\n```python\r\nimport numpy as np\r\nI = np.empty(n, dtype=np.int64)\r\nI[:] = -1\r\nfaiss.search_centroid(\r\n    cpuindex,\r\n    faiss.swig_ptr(query_embs),\r\n    n,\r\n    faiss.swig_ptr(I)\r\n    )\r\n```\r\n\r\nHowever, applying it on a GPU index results in a segfault. I can provide a Colab notebook if necessary?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2173/comments",
    "author": "cmacdonald",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2021-12-31T18:25:19Z",
        "body": "The IVFLib functions such as search_centroid are unfortunately not implemented on the GPU, sorry.\r\n"
      },
      {
        "user": "cmacdonald",
        "created_at": "2022-01-01T11:37:27Z",
        "body": "So, to be clear, the best option is to extract the trained centroids from the existing trained index and put them in a new GPU FAISS index?"
      },
      {
        "user": "mdouze",
        "created_at": "2022-01-11T08:52:17Z",
        "body": "yes that should work."
      },
      {
        "user": "cmacdonald",
        "created_at": "2022-01-11T10:38:57Z",
        "body": "and that's what we did. Thanks."
      }
    ]
  },
  {
    "number": 2131,
    "title": "Force index to be deleted",
    "created_at": "2021-11-30T21:24:14Z",
    "closed_at": "2022-01-19T12:44:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2131",
    "body": "# Summary\r\n\r\nOur application is based on Faiss. We create many indices and put them into Python map with corresponding key, the application have to change the data from time to time, the way we did is recreate indices and replace them into the map with corresponding key. We found the memory usage jump in every replacement, and it will reach 3 times of data eventually. We think the reason is Python's GC won't release the indices' memory. Is the any way we can force index to release memory, we've tried to explicitly call del index, but it seems it doesn't work.\r\n\r\n# Platform\r\n\r\nLinux\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from:  anaconda\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [X] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [X] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2131/comments",
    "author": "billyean",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-12-01T18:41:41Z",
        "body": "You can try gc.collect() but usually if the index is not released, it means there is some reference to it. "
      }
    ]
  },
  {
    "number": 2096,
    "title": "IVFPQ recall decreases with increasing nprobe?",
    "created_at": "2021-10-31T17:30:43Z",
    "closed_at": "2022-01-19T13:06:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2096",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version:  faiss-gpu                 1.7.1           py3.8_h080d439_1_cuda10.2    pytorch\r\n\r\nInstalled from: Anaconda pip <!-- anaconda? compiled by yourself ? --> \r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\nI'm doing an experiment using custom (my own) centroids with the faiss IVFPQ index. I'm building an index using custom centroids using this function:\r\n```python\r\ndef build_divfpq(flat_centroids, pq_centroids, index_embeddings):\r\n    flatK, n = flat_centroids.shape\r\n    D, K, _ = pq_centroids.shape\r\n\r\n    print(\"Creating index...\")\r\n    quantizer = faiss.IndexFlatL2(n)\r\n    index = faiss.IndexIVFPQ(quantizer, n, flatK, D, round(log2(K)))\r\n    quantizer.add(flat_centroids)\r\n    index.train(flat_centroids)\r\n    faiss.copy_array_to_vector(pq_centroids.ravel(), index.pq.centroids)\r\n    index.precompute_table()\r\n\r\n    print(\"Indexing vectors...\")\r\n    index.add(index_embeddings)\r\n\r\n    return index\r\n\r\n```\r\n\r\nI'm seeing that in some instances, the recall of a query is decreasing with increasing in nprobe. What does that mean? Isn't recall supposed to increase monotonically with increasing nprobe irrespective of what the centroids are?\r\n\r\n```\r\nCalculating recalls for NPROBE = 1...\r\nrecall@100: 0.1648\r\n\r\nCalculating recalls for NPROBE = 10...\r\nrecall@100: 0.1495\r\n\r\nCalculating recalls for NPROBE = 100...\r\nrecall@100: 0.1493\r\n```\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2096/comments",
    "author": "anirudhajith",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-11-10T17:07:35Z",
        "body": "absolutely, unless there is a misalignment between the metric used for the ground truth and the IVFPQ."
      }
    ]
  },
  {
    "number": 2089,
    "title": "Is there a way to iteratively train large indices on smaller machines?",
    "created_at": "2021-10-23T03:32:32Z",
    "closed_at": "2022-01-19T11:52:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2089",
    "body": "# How to train (and update) large indices on smaller machines?\r\n\r\nHi,\r\n\r\nIs there a way to iteratively train large indices? I have lots of data (3.5M high dimensional vectors) on a smaller machine (8GB of RAM).\r\n\r\nIs there a way to iteratively train larger indices?\r\n\r\nI am using the following snippet to train the index, however, I am running out of memory.\r\n\r\nThanks!\r\n\r\n```python\r\nd = 384\r\nnlist = 50  # how many cells\r\nquantizer = faiss.IndexFlatL2(d)\r\nindex = faiss.IndexIVFFlat(quantizer, d, nlist)\r\n\r\nindex.train(tensors)\r\nindex.add(tensors)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2089/comments",
    "author": "abhinavkulkarni",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-11-10T16:26:51Z",
        "body": "Please bear in mind that it is useless to train on 3.5M vectors to get just 50 centroids. The ratio of nb of train vectors over nb centroids should be 30-1000.\r\nThis being said, there is a hard limitation on the number of training vectors that you can use: they need to fit in RAM: 3.5e6 * 384 * 4 is 5G of RAM so already a good fraction of available RAM. Then there is speed, if  the number of centroids is high, training will be very slow."
      }
    ]
  },
  {
    "number": 2083,
    "title": "nlist == nprobe does not result in same accuracy as IndexFlatIP",
    "created_at": "2021-10-19T00:18:33Z",
    "closed_at": "2021-10-21T18:51:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2083",
    "body": "Hey all,\r\n\r\nI'm attempting to use faiss's faster search, but am noticing that I'm getting worse results even when nlist is equal to nprobe (which should be equivalent to a full search).\r\n\r\nMy new code looks something like this:\r\n```\r\nquantizer = faiss.IndexFlatIP(d)\r\nfaiss_index = faiss.IndexIVFFlat(quantizer, d, nlist)\r\nassert not faiss_index.is_trained\r\nfaiss_index.train(input_faiss)\r\nassert faiss_index.is_trained\r\nfaiss_index.nprobe = nprobe\r\n```\r\nversus the original code which looked like:\r\n```\r\nfaiss_index = faiss.IndexFlatIP(d)\r\n```\r\n\r\nYet, the new code has about a 20% accuracy drop compared to the original code despite setting nlist equal to nprobe.\r\n\r\nI tested at smaller nprobe values and the accuracy does seem to drop accordingly as I manipulate the value.\r\n\r\nIs this expected? Or am I missing something?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2083/comments",
    "author": "waynchi",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-10-19T07:15:52Z",
        "body": "No this is not expected. The only thing that could explain that is if there are ties in the distances that are ordered arbitrarily. Please check that."
      },
      {
        "user": "waynchi",
        "created_at": "2021-10-19T18:42:13Z",
        "body": "The results aren't arbitrarily ordered in my results. I'm actually calculating top-n accuracy, and when using IndexIVFFlat it completely misses the correct result up to n=50. Do IndexIVFlat and IndexFlatIP expect different input shapes or formats? I'm executing both searches the same way and am wondering if I need to configure anything else."
      },
      {
        "user": "waynchi",
        "created_at": "2021-10-20T23:01:10Z",
        "body": "I've noticed a few more things: \r\n\r\n1) even at nlist = nprobe = 1, the values outputted between IndexIVFlat vs IndexFlatIP are different.\r\n2) the output value scores ordering is reversed between the two (descending fom FlatIP, and ascending from IVFlat."
      },
      {
        "user": "waynchi",
        "created_at": "2021-10-21T18:32:03Z",
        "body": "I figured it out, IVFlat defaults to using L2 distance rather than inner product (even with a FlatIP quantizer). Is there a way to configure IVFlat to use inner product instead? @mdouze "
      },
      {
        "user": "waynchi",
        "created_at": "2021-10-21T18:51:52Z",
        "body": "Alright, for anyone looking in the future, the default metric is always L2 no matter what quantizer you use. You must set the metric to inner product as such:\r\n\r\n```\r\nquantizer = faiss.IndexFlatIP(d)\r\nfaiss_index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\r\n```\r\n\r\n"
      }
    ]
  },
  {
    "number": 2079,
    "title": "Is it possible to slice index along dimension??",
    "created_at": "2021-10-12T12:23:42Z",
    "closed_at": "2022-01-19T11:50:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2079",
    "body": "I want to search for similarity when the dimension of the query vector is not equal to that of the database. \r\n\r\nFor example, let\r\n```\r\nd_db = 64                           # dimension of database\r\nd_q = 30                            # dimension of query\r\nnb = 100000                         # database size\r\nnq = 1                              # nb of query\r\n\r\nxb = np.random.random((nb, d_db)).astype('float32')\r\nxq = np.random.random((nq, d_q)).astype('float32')\r\n\r\nindex = faiss.IndexFlatL2(d_db)\r\nindex.add(xb)\r\n```\r\n\r\nIn this case, I wonder is it possible to slice the index along dimensions. I hope to make the shape of the sliced array become `(nb, d_q)`. If possible, I'd appreciate it if you tell me how to do it.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2079/comments",
    "author": "mingi3314",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-10-15T14:39:33Z",
        "body": "No this is not supported. "
      }
    ]
  },
  {
    "number": 2068,
    "title": "Sorting/combining search results across multiple indexes",
    "created_at": "2021-09-28T09:47:27Z",
    "closed_at": "2022-01-19T11:45:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2068",
    "body": "### This is not an issue\r\nHi all, \r\nI am working on a large text classification dataset using FAISS search that does not fit into the memory. Thus, I index each class of data. For each query, I sequentially perform a FAISS knn search (L2 distance) in each chunk (class) of data. I want to aggregate all the search results across all the indexes but looking at the search results, it seems that the L2 distance does not carry the same meaning across different indexes. Specifically, with query `q_i, Index_j , top_k ->D_ij, I_ij`, a small value `D_i` in index `j` does not mean that query `i` is closer to top k in index `j` than say index `j'` because the distance is normalized in each index. \r\nHas anyone faced the same issue and would be so kind to share how to compare / sort or combine the search results across different indexes in a meaningful way?\r\n\r\nThank you so much in advance!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2068/comments",
    "author": "khieu",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-10-01T08:00:20Z",
        "body": "Keep in mind that the search functions return approximate distances (except the flat indexes). \r\nThe distances estimates between different types of indexes are not necessarily comparable. \r\nTo make them comparable, you should train the index once and re-use the same trained index for all per-class indexes."
      }
    ]
  },
  {
    "number": 2061,
    "title": "Index large dataset, search on subset of variables",
    "created_at": "2021-09-23T14:50:02Z",
    "closed_at": "2021-09-28T12:10:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2061",
    "body": "Hi, \r\nI have a large dataset of N rows and p columns. \r\n\r\nfor each pair of columns I need to find the k-nn for each of the N data points. At the moment I am building a `IndexFlatL2` for each  pair of columns and extracting the k-nn. \r\nAt the moment I am doing this\r\n```\r\nN,p = 50, 150\r\n\r\ndata = np.random.random((N,p)\r\nindex = faiss.IndexFlatL2(2)\r\nindex.add(data[:,:2]) \r\nD, I = index.search(data, 3)\r\n```\r\nIs it possible to index the whole data and then search the k-nn using only a subset of the columns? something that would look like this\r\n\r\n```\r\nN,p = 50, 150\r\n\r\ndata = np.random.random((N,p)\r\nindex = faiss.INDEX(\r\nindex.add(data) \r\nD, I = index.search(data[:,:2], 3, cols = [0,1])\r\n\r\n```\r\n\r\nThanks in advance\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2061/comments",
    "author": "inti",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-09-28T07:16:52Z",
        "body": "Faiss is optimized for high-dimensional data. The performance on 2-dimensional vectors will be no better or worse than what you can code in numpy. "
      },
      {
        "user": "inti",
        "created_at": "2021-09-28T12:10:40Z",
        "body": "Thanks for your response.\r\n"
      }
    ]
  },
  {
    "number": 2058,
    "title": "Understanding which underlying lib faiss uses",
    "created_at": "2021-09-22T07:24:54Z",
    "closed_at": "2022-01-19T11:44:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2058",
    "body": "## Summary\r\nHello, I'm installing faiss-cpu package through pip and I'm wondering whether I'm missing out on some performance (given that I want to run on CPU ofc).\r\n\r\nIs it possible to know whether faiss is linked to Intel MKL or not? In general, is it possible to know what libraries faiss links to?\r\n\r\n# Platform\r\nOS: Linux $hostname 4.14.243-185.433.amzn2.x86_64 #1 SMP Mon Aug 9 05:55:52 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nFaiss version: faiss-cpu 1.7.1.post2\r\n\r\nInstalled from: `pip install faiss-cpu`\r\n\r\nFaiss compilation options: Not applicable\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2058/comments",
    "author": "mariokostelac",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-09-22T10:07:49Z",
        "body": "If you installed via conda, then you are guaranteed to use MKL. \r\nWhat platform are you running on? Please use the issue template."
      },
      {
        "user": "mariokostelac",
        "created_at": "2021-09-22T10:17:50Z",
        "body": "Hey @mdouze, sorry for not using the template, I've changed the initial comment."
      },
      {
        "user": "mariokostelac",
        "created_at": "2021-09-22T12:47:40Z",
        "body": "Here's what I did to find out what it's linked to, but thought that there might be a function within faiss package to get something similar\r\n\r\n```bash\r\n[ec2-user@hostname faiss]$ pwd\r\n/app/.heroku/python/lib/python3.6/site-packages/faiss\r\n[ec2-user@hostname faiss]$ ldd _swigfaiss.cpython-36m-x86_64-linux-gnu.so \r\n        linux-vdso.so.1 (0x00007ffe67a9f000)\r\n        libgfortran-7e18e706.so.4.0.0 => /app/.heroku/python/lib/python3.6/site-packages/faiss/./../faiss_gpu.libs/libgfortran-7e18e706.so.4.0.0 (0x00007ff48bbb6000)\r\n        librt.so.1 => /lib64/librt.so.1 (0x00007ff48b9ae000)\r\n        libstdc++.so.6 => /lib64/libstdc++.so.6 (0x00007ff48b62c000)\r\n        libm.so.6 => /lib64/libm.so.6 (0x00007ff48b2ec000)\r\n        libgomp-a34b3233.so.1.0.0 => /app/.heroku/python/lib/python3.6/site-packages/faiss/./../faiss_gpu.libs/libgomp-a34b3233.so.1.0.0 (0x00007ff48b0c2000)\r\n        libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007ff48aeac000)\r\n        libpthread.so.0 => /lib64/libpthread.so.0 (0x00007ff48ac8e000)\r\n        libc.so.6 => /lib64/libc.so.6 (0x00007ff48a8e3000)\r\n        /lib64/ld-linux-x86-64.so.2 (0x00007ff49ba41000)\r\n        libquadmath-96973f99.so.0.0.0 => /app/.heroku/python/lib/python3.6/site-packages/faiss/./../faiss_gpu.libs/libquadmath-96973f99.so.0.0.0 (0x00007ff48a6a6000)\r\n```\r\n\r\nIt's certainly not linking mkl."
      },
      {
        "user": "mdouze",
        "created_at": "2021-09-28T07:07:41Z",
        "body": "Sorry, we do not support pip for precisely that reason: if you use conda normally there is some guarantee that the libraries we link with are the correct ones."
      },
      {
        "user": "mariokostelac",
        "created_at": "2021-09-28T11:17:39Z",
        "body": "No need to apologize, it's my bad. I thought that faiss-cpu was published by somebody from your team, but it doesn't look like that."
      }
    ]
  },
  {
    "number": 2057,
    "title": "QUESTION: Can I create an IndexIVFPQ object with custom centroids?",
    "created_at": "2021-09-21T15:23:20Z",
    "closed_at": "2021-09-21T16:32:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2057",
    "body": "Hello. I'm trying to run an experiment that involves using some custom centroids (that I generate) with the IVFPQ indexing structure. Since faiss provides highly optimised infrastructure and support for IVFPQ indexing, I would like to use it to perform my experiments.\r\n\r\nIs it possible to to create an `IndexIVFPQ` object whose coarse and fine quantizer centroids are initialised to vectors I provide?\r\n\r\nHere's what I tried doing to achieve this:\r\n\r\n```python\r\nquantizer = faiss.IndexFlatL2(d)  \r\nindex = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)\r\ncustom_coarse_centroids = <a numpy array>\r\ncustom_pq_centroids = <a numpy array>\r\nquantizer.add(custom_coarse_centroids)\r\nindex.train(custom_coarse_centroids)\r\nfaiss.copy_array_to_vector(custom_pq_centroids.ravel(), index.pq.centroids)\r\n```\r\n\r\nAfter doing this, I verified by reading the corresponding centroids using `index.quantizer.reconstruct_n(0, index.nlist)` and `faiss.vector_to_array(index.pq.centroids).reshape(index.pq.M, index.pq.ksub, index.pq.dsub)` that the centroids are correctly set to what I want them to be. However, when I try to perform a query, I get nonsensical results such as negative distance estimates.\r\n\r\n```python\r\nindex.add(xb)\r\nD, I = index.search(xb[:5], k) # sanity check\r\nprint(I)\r\nprint(D)\r\n```\r\nI understand that certain distances and inner products are precomputed and stored inside an `IndexIVFPQ` object when the index is trained. Am I correct in thinking that what remains to be done to make my custom `IndexIVFPQ` object work correctly is to perform those precomputations? How can I make the `IndexIVFPQ` object carry out the relevant precomputations with the centroids I've just inserted?\r\n\r\nAlternatively, is there a better way to achieve this? My end goal is to create a queryable `IndexIVFPQ` object with my own custom centroids instead of relying on `.train()` to learn them.\r\n\r\nThanks in advance for any help you can offer!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2057/comments",
    "author": "anirudhajith",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-09-21T15:43:29Z",
        "body": "A few things to keep in mind: \r\n\r\n- by default the IVFPQ encodes the residual of the vectors wrt. the centroids they are assigned to, not the vectors themselves\r\n\r\n- the precomputed tables (used only for L2 search with residuals) are initialized after training so if you update the coarse or fine centroids after training you should call \r\n\r\n```\r\nindex.verbose = True # to see what happens\r\nindex.precompute_table()\r\n```"
      },
      {
        "user": "anirudhajith",
        "created_at": "2021-09-21T16:32:32Z",
        "body": "`index.precompute_table()` is exactly what I was looking for! It's working exactly as expected now. Thanks a lot!\r\n\r\nI'm aware of the bit about the residuals being encoded, thanks."
      }
    ]
  },
  {
    "number": 2034,
    "title": "c_api dont't export funtion fvec_renorm_L2, is there any problem if export it?",
    "created_at": "2021-09-03T04:08:00Z",
    "closed_at": "2021-10-19T21:58:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2034",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\ncore/faiss/include/faiss/utils/distances.h:line 88\r\n\r\n/* L2-renormalize a set of vector. Nothing done if the vector is 0-normed */\r\nvoid fvec_renorm_L2(size_t d, size_t nx, float* x);\r\n\r\nthis function is not being exported in c_api/utils. so is there any question?\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2034/comments",
    "author": "intech07",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-09-03T07:38:07Z",
        "body": "Please submit a PR to export."
      },
      {
        "user": "intech07",
        "created_at": "2021-09-03T16:49:25Z",
        "body": "> Please submit a PR to export.\r\n\r\nCool, I have submited a PR #2036 . And @mdouze, can you help me in reviewing it, then merge it?"
      }
    ]
  },
  {
    "number": 2012,
    "title": "The search time when parallel mode is set to 1 is the same as that set to 2",
    "created_at": "2021-08-16T07:40:30Z",
    "closed_at": "2022-01-19T10:54:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2012",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->Centos \r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->faiss1.7.0\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\nI tested faiss1.7.0/tutorial/cpp 2-ivfflat.cpp, I modified d\r\n=512, nb = 1000000, **and then set  the values of parallel_mode are 1 and  the value of parallel_mode is 2. It is found that the search time of the two is the same. I**f so,  Can't the meaning of parallel_mode (value of parallel_mode is  set 1 or 2 )be reflected?\r\n\r\nWe look forward to your reply.\r\n\r\n\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2012/comments",
    "author": "Aurevoir-68",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-08-16T08:39:29Z",
        "body": "What did you expect?"
      },
      {
        "user": "Aurevoir-68",
        "created_at": "2021-08-17T01:51:10Z",
        "body": "parallel_mode is set to different values, and their search time should be significantly different,  The search time with parallel_mode is set to 2 should be shorter than parallel_ Mode is set to the search timeof 1\r\nwhile There is no difference when parallel_ mode is set to 1 and 2 according to my  results.\r\nWhat is the reasonï¼Ÿ We look forward to your reply."
      },
      {
        "user": "mdouze",
        "created_at": "2022-01-19T10:54:43Z",
        "body": "I don't have more clues than you about this behavior."
      }
    ]
  },
  {
    "number": 2004,
    "title": "Change number of clusters of IVFIndex and re-train",
    "created_at": "2021-07-30T07:47:26Z",
    "closed_at": "2022-01-19T10:51:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2004",
    "body": "\r\nHi,\r\nI have an IVFIndex that is continuosly growing over time and at some point I want to re-train the index changing the number of clusters originally set. I don't know if changing the attribute `nlist` and training the index is enough to achieve what I want. To illustrate the problem, this is a short code snippet:\r\n```\r\ndimensions = 384\r\nnum_clusters = 500\r\nquantizer = faiss.IndexFlatIP(dimensions)\r\nindex = faiss.IndexIVFFlat(quantizer, dimensions, num_clusters, faiss.METRIC_INNER_PRODUCT)\r\nindex.set_direct_map_type(faiss.DirectMap.Hashtable)\r\n...\r\nDo a lot of stuff adding elements over time\r\nand at some point I want to change from the original 500 clusters to 1500\r\n...\r\nindex.nlist = 1500\r\nembeddings = np.random.randn([500000, 384], dtype=np.float32)\r\nindex.train(embeddings)\r\n```\r\nThe idea is to preserve the elements that were added over time, but re-train the index (perhaps only the quantizer) with a different number of clusters. Is it correct?\r\n\r\nThank you very much.\r\nJavier",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2004/comments",
    "author": "javierjuan",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-08-08T17:47:16Z",
        "body": "No you can't change the number of clusters dynamically. A new index needs to be built. Fortunately, for IndexIVFFlat, the vectors are stored uncompressed so it is possible to get the stored vectors from it with eg. `reconstruct_n`."
      }
    ]
  },
  {
    "number": 1999,
    "title": "Question about IVF accuracy",
    "created_at": "2021-07-27T09:26:50Z",
    "closed_at": "2022-01-19T10:51:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1999",
    "body": "Hi,\r\nI was a bit surprised when testing IVF indexes accuracy. Turns out when you train, add and search on the same set of data, you always get 100% accuracy. On a 1M dataset, I can go for nprobe=1, k=1, nlist = a few thousands, and that still gives me 100%! This even goes for PQ and SQ, which confuses me even further. Shouldn't the quantization introduce some error when calculating distance? Moreover, if the centroids is an average over many vectors, How did the query always land in the correct cluster on the first hit?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1999/comments",
    "author": "phosphorylation",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-07-29T08:04:47Z",
        "body": "hmmm that seems weird indeed. What code do you use?"
      },
      {
        "user": "phosphorylation",
        "created_at": "2021-07-30T06:43:40Z",
        "body": "here's the testing code, it's based on a recent master (last pulled in a month).\r\n\r\n`\r\n#include <cstdio>\r\n#include <cstdlib>\r\n#include <random>\r\n#include <iostream>\r\n\r\n#include <faiss/IndexFlat.h>\r\n#include <faiss/IndexIVFPQ.h>\r\n#include <faiss/index_io.h>\r\n#include <faiss/IndexIVFFlat.h>\r\n#include <faiss/IndexScalarQuantizer.h>\r\n#include <chrono>\r\n\r\nusing namespace std;\r\n\r\nint main() {\r\n    // dimension of the vectors to index\r\n    int d = 64;\r\n\r\n    // size of the database we plan to index\r\n    size_t nb = 100000;\r\n\r\n    // make a set of nt training vectors in the unit cube\r\n    // (could be the database)\r\n    size_t nt = 100000;\r\n\r\n    // make the index object and train it\r\n    faiss::IndexFlatL2 index_gt(d);\r\n    faiss::IndexFlatL2 coarse(d);\r\n    //faiss::IndexIVFScalarQuantizer index(&coarse,d,10000,faiss::ScalarQuantizer::QT_4bit);\r\n    faiss::IndexIVFPQ index(&coarse,d,1024,16,8);\r\n\r\n    std::mt19937 rng;\r\n    std::uniform_real_distribution<> distrib;\r\n    using namespace std::chrono;\r\n\r\n        std::vector<float> trainvecs(nt * d);\r\n        for (size_t i = 0; i < nt * d; i++) {\r\n            trainvecs[i] = distrib(rng);\r\n        }\r\n        index.verbose = true;\r\n\r\n        milliseconds ivfbegin = duration_cast< milliseconds >(system_clock::now().time_since_epoch());\r\n        index.train(nt, trainvecs.data());\r\n        milliseconds ivfend = duration_cast< milliseconds >(system_clock::now().time_since_epoch());\r\n\r\n        std::vector<float> database(nb * d);\r\n        for (size_t i = 0; i < nb * d; i++) {\r\n            database[i] = trainvecs[i];\r\n        }\r\n\r\n        index.add(nb, database.data());\r\n        index_gt.add(nb, database.data());\r\n\r\n\r\n    int nq = 10000;\r\n\r\n    { // searching the database\r\n\r\n        std::vector<float> queries(nq * d);\r\n        for (size_t i = 0; i < nq * d; i++) {\r\n            queries[i] = trainvecs[i];\r\n        }\r\n\r\n\r\n        index.nprobe = 1;\r\n        int k = 1;\r\n        std::vector<faiss::Index::idx_t> nns(k * nq);\r\n        std::vector<float> dis(k * nq);\r\n        std::vector<faiss::Index::idx_t> gt_nns(nq * k);\r\n        std::vector<float> gt_dis(nq * k);\r\n\r\n        index_gt.search(nq, queries.data(), k, gt_dis.data(), gt_nns.data());\r\n        index.search(nq, queries.data(), k, dis.data(), nns.data());\r\n\r\n        float ivf =0;\r\n\r\n        for (int q = 0; q < nq; q++) {\r\n            cout<<\"standard  result \"<<gt_nns[q*k]<<\"\\n\";\r\n            cout<<\"ivf result\"<<nns[q*k]<<\"\\n\";\r\n            for (int i = 0; i < k; i++) {\r\n                if (gt_nns[q * k] == nns[q * k+i]) {\r\n                    ivf += 1;\r\n                }\r\n            }\r\n        }\r\n        std::cout<<\"ivf acc:\"<<ivf/nq;\r\n    }\r\n}\r\n`"
      },
      {
        "user": "Zhylkaaa",
        "created_at": "2021-08-04T09:04:41Z",
        "body": "Hey, I was about to open new issue, but topic seems suitable. I am trying to get maximum accuracy from IVFFlat indexer, because I would love to integrate it into DPR for datasets that cannot fit into RAM. So I thought that using `index = faiss.index_factory(vectors.shape[1], \"IVF1,Flat\", faiss.METRIC_INNER_PRODUCT)` should work as `IndexFlatIP` because 1 centroid will give me 1 giant Voronoi cell, but I am actually getting accuracy about `0.99778`. Which is not the case for L2 metric. \r\nI know that this is a bit hacky solution and this is not the intended usage, but maybe someone can help me with this? Or should I just use dot -> L2 transform and use L2 indexer?\r\n\r\nP.S. The reason I am doing this is fact that IVF indexers can be mmaped and `IndexFlatIP` cannot (at least on my machine). \r\nfaiss version: 1.7.0\r\ninstalled via: conda \r\nOS: macOS 10.15.2"
      },
      {
        "user": "phosphorylation",
        "created_at": "2021-08-06T06:43:11Z",
        "body": "@Zhylkaaa Hi, I'm guessing the 0.99778 is due to the way you're calculating accuracy. Are you using a groundtruth table produced in L2? in that case then you shouldn't expect IP result to match the L2 table. I'd suggest feeding the same dataset and query to a IndexFlatIP and your IVFFLatIP to see if you're getting 1 acc."
      },
      {
        "user": "Zhylkaaa",
        "created_at": "2021-08-06T18:38:56Z",
        "body": "Hi @phosphorylation, sorry I've added a bit of confusion, I am taking `IndexFlatIP` as a ground truth and compare results to `faiss.index_factory(vectors.shape[1], \"IVF1,Flat\", faiss.METRIC_INNER_PRODUCT)`.\r\n\r\nYou can test this with small example:\r\n```\r\nimport faiss\r\nimport numpy as np\r\nimport os\r\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\r\n\r\nvecs = np.random.rand(10000, 768).astype('float32')\r\nflat_index = faiss.IndexFlatIP(768)\r\nflat_index.add(vecs)\r\nindex_ivf = faiss.index_factory(vecs.shape[1], \"IVF1,Flat\", faiss.METRIC_INNER_PRODUCT)\r\nindex_ivf.train(vecs)\r\nindex_ivf.add(vecs)\r\n\r\ntest_vecs = np.random.rand(1000, 768).astype('float32')\r\nflat_results = flat_index.search(test_vecs, 100)\r\nivf_results = index_ivf.search(test_vecs, 100)\r\n\r\nnp.sum(np.array(flat_results[1]) == np.array(ivf_results[1])) / len(flat_results[1])\r\n```\r\n\r\n\r\nEDIT:\r\n\r\nAside from this use case, can someone recommend a settings for 9,3M 768 dimensional vectors if I want to have >=90% accuracy with inner product metric. Or is it better to use L2?"
      },
      {
        "user": "mdouze",
        "created_at": "2021-08-16T07:09:28Z",
        "body": "Could you check if there are ties or very close distances?  The way the inner products are computed is a little different between Flat and IVF.\r\n"
      }
    ]
  },
  {
    "number": 1991,
    "title": "Error of remove ids not implemented for this type of index IVF1024,PQ64x4fs,RFlat",
    "created_at": "2021-07-20T08:56:08Z",
    "closed_at": "2022-01-19T10:46:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1991",
    "body": "# Summary\r\nHi,\r\nI'm using this setup with conda: faiss version 1.7.3 running on CPU\r\nWhen I try to run remove_ids with \"IVF1024,PQ64x4fs,RFlat\" I get this error:\r\nRuntimeError: Error in virtual size_t faiss::Index::remove_ids(const faiss::IDSelector&) at /__w/faiss-wheels/faiss-wheels/faiss/faiss/Index.cpp:43: remove_ids not implemented for this type of index\r\n\r\nDoes anyone know what can be the problem?\r\n\r\n# Platform\r\n\r\nOS: ubuntu 20.04\r\n\r\nFaiss version: 1.7.3\r\n\r\nRunning on: CPU\r\n\r\nInterface: Python\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1991/comments",
    "author": "nvlong21",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-07-29T07:50:48Z",
        "body": "as the message says, it is not implemented. "
      }
    ]
  },
  {
    "number": 1988,
    "title": "there is a problem when set the parameter parallel_mode is 2 ",
    "created_at": "2021-07-19T09:12:51Z",
    "closed_at": "2022-01-19T10:45:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1988",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->Centos8.2\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->faiss 1.6.3\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->docker\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\n\r\nWhen I run the program tutorial/cpp/2-IVFFlat.cpp, I set the parameter parallel_ mode = 2, an error occurred during searching. The error information is as followsï¼š\r\n\r\n**terminate called after throwing an instance of 'faiss::FaissException'\r\nterminate called recursiely**\r\n\r\nI set the parameter parallel_ mode = 0 or 1 ï¼Œthere is no problem. \r\n\r\nLook forward to your replyï¼ï¼ï¼\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1988/comments",
    "author": "Aurevoir-68",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-07-29T07:47:09Z",
        "body": "Could you try to run with a single Openmp thread so that the exception is propagated properly?\r\n\r\n"
      }
    ]
  },
  {
    "number": 1985,
    "title": "inconsistent query results",
    "created_at": "2021-07-16T01:53:05Z",
    "closed_at": "2022-01-19T10:45:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1985",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\nI tried two searches with the same query while different Ks. The second search uses K1 which is less than K0 used by the first search. As both use the same query and same library, I assume the result of the second search should be part of the first result(topK1 should be part of topK0). \r\n\r\nThe code is shown below. I compare the result indices and count how many pairs are same. I assume it will print 1000000(i.e., 10000 queries * top100) as the result of second query should be the first 100 columns of the result of first query. But actually I got 999892.\r\nI'm not sure if this relates to precision loss as the feature dimension is just 256 and I use the fp32 datatype. Did I use any  APIs wrong?\r\n\r\nFaiss version: 1.7.1\r\nInstalled from: anaconda\r\n\r\nFaiss compilation options: N/A\r\n\r\nRunning on:\r\n- GPU\r\n\r\nInterface: \r\n-  Python\r\n\r\n# Reproduction instructions\r\n```\r\nimport numpy as np\r\nimport faiss\r\nquery = np.random.randn(10000, 256).astype('float32')\r\nreference = np.random.randn(1000000, 256).astype('float32')\r\n\r\n\r\nindex = faiss.IndexFlatL2(query.shape[1])\r\nindex.add(reference)\r\nD, I = index.search(query, 10000)\r\n\r\nD_100, I_100 = index.search(query, 100)\r\n\r\na = np.sum(I_100 == I[:,:100])\r\nprint(a)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1985/comments",
    "author": "dongxiao92",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-07-29T07:41:12Z",
        "body": "There are probably a few ties in distances (may or may not be due to floating point roundoff errors)"
      },
      {
        "user": "dongxiao92",
        "created_at": "2021-08-02T14:04:29Z",
        "body": "@mdouze thanks for your reply. Do you have any advises on this? "
      }
    ]
  },
  {
    "number": 1984,
    "title": "Question about Faiss quantization",
    "created_at": "2021-07-15T22:51:57Z",
    "closed_at": "2022-01-19T10:44:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1984",
    "body": "Hi,\r\n\r\nCurrently I am using IndexPQ and IndexIVFPQ, and looks like when I increase the n_bits, I can get better accuracy. I am just wondering why increase the n_bits will increase the accuracy? Is it because for each sub_vec set, we can generate more centroids?\r\n\r\nAppreciate for any help or any docs! Thanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1984/comments",
    "author": "MrWWheat",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2021-07-19T20:41:41Z",
        "body": "> Is it because for each sub_vec set, we can generate more centroids?\r\n\r\nyes, there are 2^n_bits PQ centroids per each subspace generated, thus higher reproduction fidelity"
      }
    ]
  },
  {
    "number": 1973,
    "title": "Why does IndexIVFPQFastScan support only 4-bits-per-index cases?",
    "created_at": "2021-07-02T07:09:54Z",
    "closed_at": "2022-01-19T10:41:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1973",
    "body": "# Summary\r\n\r\nIn the beginning of IndexIVFPQFastScan.cpp, it checks for `FAISS_THROW_IF_NOT(nbits_per_idx == 4);`. It seems that FastScan shows better performance than normal IndexIVF search since it sorts QC with coarse list number beforehand. If this is the case, why is FastScan only applied to cases where it requires 4-bits per index? Is it also worth considering to apply this technique, sorting the queries beforehand based on coarse quantization results, to other cases, e.g., 8-bits-per-index cases, as well?\r\n\r\n# Platform\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1973/comments",
    "author": "sunhongmin225",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-07-02T21:59:30Z",
        "body": "The difference with the default PQ implementation is that the look-up tables are stored in registers, but registers are too small to host 256-entry LUTs."
      },
      {
        "user": "sunhongmin225",
        "created_at": "2021-07-03T02:15:15Z",
        "body": "I see. Thanks for your reply."
      }
    ]
  },
  {
    "number": 1971,
    "title": "Reset doesn't free GPU Memory when clustering_index is used",
    "created_at": "2021-06-30T18:15:06Z",
    "closed_at": "2021-07-21T13:18:13Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1971",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 --> Ubuntu 18.04\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a --> 1.7.1\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? -->  Anaconda\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\nI'm building a lot of faiss indexes through a loop. However, my program always stops because of a Cuda Out of Memory, even when I use reset() after each creation. The GPU memory is not released only when I'm executing this line `\r\nindex_ivf.clustering_index = clustering_index` \r\n\r\nHere is a simple code to reproduce the issue:\r\n\r\n```\r\nimport faiss\r\n\r\ndef loop_index_creation(with_clustering_index_assignment,nb_indexes=20):\r\n    for i in range(nb_indexes):\r\n        index = faiss.index_factory(128, \"OPQ16_64,IVF16384_HNSW32,PQ64\")\r\n\r\n        index_ivf = faiss.extract_index_ivf(index)\r\n\r\n        clustering_index = faiss.index_cpu_to_all_gpus(faiss.IndexFlatL2(index_ivf.d))\r\n\r\n        if with_clustering_index_assignment:\r\n            index_ivf.clustering_index = clustering_index\r\n\r\n        clustering_index.reset()\r\n\r\n\r\nprint(\"Loop 1: without assigning index_ivf.clustering_index\")\r\nloop_index_creation(with_clustering_index_assignment=False)\r\nprint(\"Loop 1 done\")\r\n\r\nprint(\"Loop 2: with assigning index_ivf.clustering_index\")\r\nloop_index_creation(with_clustering_index_assignment=True)\r\nprint(\"Loop 2 done\")\r\n```\r\n\r\nAs you'll see if you execute this code, the loop 1 has no issue, but the loop 2 never release GPU memory and will eventually throw a Cuda memory error. \r\n\r\nHow can I free the GPU memory when using  `index_ivf.clustering_index `  ?\r\n\r\nThanks in advance\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1971/comments",
    "author": "oubathAI",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-07-02T21:57:53Z",
        "body": "Do you plan to do the training inside the loop? \r\n\r\nIf no: the clustering_index is used only during training and setting it does not keep track of references, so the program will crash if train is called outside the loop.\r\n\r\nIf yes: you can just instantiate the clustering_index outside the loop and re-use it for all trainings.\r\n\r\nThe reason why it is OOMing: the index_cpu_to_all_gpus allocates a good chunk of GPU memory for the GPU resources + the python GC is a bit lazy (don't hesitate to call gc.collect() from time to time).\r\n\r\n\r\n\r\n"
      },
      {
        "user": "oubathAI",
        "created_at": "2021-07-03T00:00:06Z",
        "body": "Thanks for your reply.\r\nYes I'm plan to do the training inside the loop; the real code I have is a function creating an index and saving it ; and this function is called inside a loop. The real code looks like:\r\n```\r\ndef build_and_save_index(data,save_path):\r\n    index = faiss.index_factory(128, \"OPQ16_64,IVF16384_HNSW32,PQ64\")\r\n    index_ivf = faiss.extract_index_ivf(index)\r\n    clustering_index = faiss.index_cpu_to_all_gpus(faiss.IndexFlatL2(index_ivf.d))\r\n    index_ivf.clustering_index = clustering_index\r\n    x_train = ohlc_norm_array[sampling_indexes]\r\n    index.train(data)\r\n    index.add(data)\r\n    faiss.write_index(index, save_path)\r\n\r\n\r\nfor data,save_path in zip(data_list,save_path_list):\r\n    build_and_save_index(data,save_path)\r\n```\r\nI tried with `gc.collect()` to free the memory; but it doesn't change anything.\r\n\r\nSo you're saying that I should rewrite my code like this?\r\n```\r\nclustering_index = faiss.index_cpu_to_all_gpus(faiss.IndexFlatL2(64))\r\ndef build_and_save_index(data,save_path,clustering_index=clustering_index):\r\n    index = faiss.index_factory(128, \"OPQ16_64,IVF16384_HNSW32,PQ64\")\r\n    index_ivf = faiss.extract_index_ivf(index)\r\n    index_ivf.clustering_index = clustering_index\r\n    x_train = ohlc_norm_array[sampling_indexes]\r\n    index.train(data)\r\n    index.add(data)\r\n    faiss.write_index(index, save_path)\r\n\r\n\r\nfor data,save_path in zip(data_list,save_path_list):\r\n    build_and_save_index(data,save_path)\r\n\r\n```\r\n\r\nWhen I do it this way, there's no OOM; but my concerns is that if I use the same clustering_index would the previous training results wouldn't affect the next ones  ? since all the indexes I build are differents. \r\n"
      }
    ]
  },
  {
    "number": 1970,
    "title": "Remove from sub-index inside IndexIDMap?",
    "created_at": "2021-06-29T15:58:59Z",
    "closed_at": "2021-06-30T08:04:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1970",
    "body": "I have an `IndexFlatL2` wrapped by `IndexIDMap` / `IndexIDMap2`. Multiple inner vectors may be mapped to the same ID in the outer index (think class ID).\r\n\r\nIs there a way to remove a single vector by its original ID in the sub-index, keeping the wrapper in sync?\r\n\r\nThe only idea I could come up with so far is to remove all vectors of this class, and then re-add them, except for the one to be deleted.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1970/comments",
    "author": "kormalev",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-06-30T08:01:27Z",
        "body": "No, this is not really supported, IndexIDMap assumes the vector ids are distinct."
      }
    ]
  },
  {
    "number": 1949,
    "title": "Will searching be faster if I merge these small indexes into a bigger one?",
    "created_at": "2021-06-16T08:19:12Z",
    "closed_at": "2021-09-29T10:11:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1949",
    "body": "Total ntotal (sum of each indexes) is about 4e9. (Now there are 4 small indexes and ntotal of each one is about 1e9).\r\nIndex factory: PCAR96_IVF12288_PQ48\r\nThere are about (2e8,512) features to search every day. (512: dim number before PCA, 2e8: number of imgs)\r\nCPU of this machine: 4800%(24 cores)\r\n\r\nenvironmentï¼š\r\nDebian 4.9.144-3 (2019-02-02)\r\nfaiss-cpu==1.7.0\r\npy3.6",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1949/comments",
    "author": "splinter21",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-06-21T05:10:57Z",
        "body": "It is usually faster to search a merged index that individual indexes because the coarse quantization overhead is factorized. \r\nThe IVF size seems too small for the 1B vectors, please use at least 100k. \r\n\r\n "
      }
    ]
  },
  {
    "number": 1937,
    "title": "K-Means IP has increasing objective, but better performance - logging issue? ",
    "created_at": "2021-06-08T22:27:55Z",
    "closed_at": "2021-06-10T02:26:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1937",
    "body": "# Summary\r\n\r\nWhen running k-means with `spherical=True`, final classification results are improved compared to using an L2 distance metric when the features are unit normed. This is expected. \r\n\r\nHowever, when inspecting training loss with with `.obj` attribute, the loss increases with each iteration. I'm not sure what's causing this discrepancy. As I'm using k-means++ by initializing the centroids manually with `nredo=1` and selecting the best of multiple runs, the `.obj` attribute needs to be accurate to select the lowest loss model. \r\n\r\n\r\n# Platform\r\n\r\nRunning on:\r\n- [X] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [X] Python\r\n\r\n# Reproduction instructions\r\n\r\nRun any unit normed dataset and inspect the `.obj` attribute with `spherical=True`. It will be increasing per iteration, although the final model will perform well. \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1937/comments",
    "author": "GerardMaggiolino",
    "comments": [
      {
        "user": "GerardMaggiolino",
        "created_at": "2021-06-08T22:29:25Z",
        "body": "Alternatively, is it possible to supply multiple runs to `init_centroids` to the `.train()` function to have a set of centroids per iteration of `nredo`? \r\n\r\nIf `init_centroids` is specified, current behavior seems to be to use those centroids for all runs. "
      },
      {
        "user": "mdouze",
        "created_at": "2021-06-09T04:56:16Z",
        "body": "The objective is the sum of \"distances\" returned by the clustering index.\r\nFor an IP index the distances are actually dot products, that are better when higher, so it makes sense that the objective is increasing. \r\nNB that spherical k-means and IP search there is no clear guarantee or loss that k-means optimizes. \r\n\r\nFor the nredo / init_centroids: indeed it's a bit useless to use the combination of both.... A workaround is to run the optimization several times in an external loop."
      },
      {
        "user": "GerardMaggiolino",
        "created_at": "2021-06-10T02:26:25Z",
        "body": "@mdouze Thank you, this solves my question :) "
      }
    ]
  },
  {
    "number": 1898,
    "title": "The setting of the value of  distance_compute_blas_threshold",
    "created_at": "2021-05-19T07:13:07Z",
    "closed_at": "2022-04-27T09:10:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1898",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->centos\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->faiss 1.6.3\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n- When calculating the inner product (L2), distance_ compute_ blas_threshold is set to 20. When distance_ compute_ blas_threshold is set to 3, it also meets the expectation. So, what is the reason for setting the value of 20?\r\n\r\n    looking forward to your replyï¼ï¼ï¼\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1898/comments",
    "author": "Aurevoir-68",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-05-20T19:44:12Z",
        "body": "What do you mean with \"it also meets the expectation\"?"
      },
      {
        "user": "Aurevoir-68",
        "created_at": "2021-05-21T03:22:03Z",
        "body": "- **When set the value of distance_compute_blas_threshold is 20(default), nq increases gradually from 1 to 19, and the every search time is getting longer and longer.**\r\n\r\n- **When set the value of distance_compute_blas_threshold is 3, the search time is significantly less due to the use of Blas library acceleration;**\r\n\r\n**So why set  the value of distance_compute_blas_threshold  is 20, not any other value?**"
      },
      {
        "user": "mdouze",
        "created_at": "2022-04-27T09:10:53Z",
        "body": "This value was determined experimentally, but it is possible that other parameters also come into play. If you have more insights on the best settings depending on nb queries / nb database / d, please let us know."
      }
    ]
  },
  {
    "number": 1891,
    "title": "Adding ids with faiss.add_with_ids",
    "created_at": "2021-05-18T05:18:45Z",
    "closed_at": "2022-04-27T09:12:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1891",
    "body": "While adding the vectors in faiss, I also want to save some info under the ids for each vector.\r\nThis info can be in string format. Since faiss just supports integer to be added as ids. I was trying to encode the string in bytes, and convert it to int using `int.from_bytes(byte_value)`. The integer, I am getting from it is of length 48.  So, when, I am adding the id under `faiss.add_with_ids`, it's giving an unrecognized array type.\r\n\r\nPlease help me, how do I add the ids in this scenario.\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1891/comments",
    "author": "divyag11",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-05-19T19:39:06Z",
        "body": "This is not really supported. The Faiss ids are restricted to 63 bits. If you need more, you'll need to manage the mapping between Faiss ids and strings."
      }
    ]
  },
  {
    "number": 1870,
    "title": "IMI+OPQ recall lower than IMI alone",
    "created_at": "2021-05-07T15:17:59Z",
    "closed_at": "2024-07-18T00:40:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1870",
    "body": "Dear Faiss team,\r\n\r\nI have been trying IMI + OPQ on CPU, using PQ16. Surprisingly, it seems the recall of IMI+OPQ is even lower than IMI alone on SIFT100M dataset. Is this what you would expect or is there anything I could do wrong?\r\n\r\nThe commands I used for training and querying:\r\n```\r\npython bench_polysemous_1bn.py SIFT100M IMI2x8,PQ16 nprobe=64 nprobe=128 nprobe=256 nprobe=512\r\n\r\npython bench_polysemous_1bn.py SIFT100M OPQ16,IMI2x8,PQ16 nprobe=64 nprobe=128 nprobe=256 nprobe=512\r\n```\r\n\r\nSome recall numbers:\r\n\r\n```\r\nwithout OPQ:\r\nloading ./trained_CPU_indexes/bench_cpu_SIFT100M_IMI2x12,PQ16/SIFT100M_IMI2x12,PQ16_populated.index\r\n           \t R@1    R@10    \r\nnprobe=64 \t 0.3336 0.6466    \r\nnprobe=128 \t 0.3575 0.7211   \r\nnprobe=256 \t 0.3740 0.7784    \r\nnprobe=512 \t 0.3850 0.8201   \r\n\r\nwith OPQ:\r\nloading ./trained_CPU_indexes/bench_cpu_SIFT100M_OPQ16,IMI2x12,PQ16/SIFT100M_OPQ16,IMI2x12,PQ16_populated.index\r\n          \t R@1    R@10    \r\nnprobe=64 \t 0.3067 0.6196    \r\nnprobe=128 \t 0.3296 0.6875    \r\nnprobe=256 \t 0.3441 0.7415    \r\nnprobe=512 \t 0.3574 0.7834   \r\n```\r\n\r\nI show IMI2x12 here, but I also experienced the same situation from 2x8 to 2x14...\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1870/comments",
    "author": "WenqiJiang",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-05-10T05:25:23Z",
        "body": "Thanks for this test. \r\nIt can happen that OPQ is counter-productive with IVF and IMI because it is not directly optimizing the encoding of the PQ (it does not take into account the effect of the coarse quantizer). \r\nIt *is* possible to train it properly, I'll make a script to show how. "
      },
      {
        "user": "WenqiJiang",
        "created_at": "2021-06-29T07:09:09Z",
        "body": "> Thanks for this test.\r\n> It can happen that OPQ is counter-productive with IVF and IMI because it is not directly optimizing the encoding of the PQ (it does not take into account the effect of the coarse quantizer).\r\n> It _is_ possible to train it properly, I'll make a script to show how.\r\n\r\nThanks for the kind reply! Has that script been available? Or are there any similar scripts that I could use as a reference? Just curious about how can I do the training properly :)\r\n\r\nBest"
      }
    ]
  },
  {
    "number": 1853,
    "title": "Windows delete by IDs",
    "created_at": "2021-04-29T12:02:33Z",
    "closed_at": "2021-04-29T15:41:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1853",
    "body": "# Platform\r\nOS:\r\n- [x] Windows 10 (Error)\r\n- [x] OSX 10.15.7 (Working)\r\n\r\nFaiss version: 1.7.0\r\n\r\nInstalled from: Anaconda\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\nI have problems with `delete_ids` on Windows.\r\n```python\r\nxb = np.random.randn(10, 256)\r\nxb = xb.astype(np.float32)\r\nindex = faiss.IndexFlatL2(xb.shape[1])\r\nindex.remove_ids(np.array([0]))\r\n-------------\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\faiss\\__init__.py\", line 381, in replacement_remove_ids\r\n    sel = IDSelectorBatch(x.size, swig_ptr(x))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\faiss\\swigfaiss.py\", line 4843, in __init__\r\n    _swigfaiss.IDSelectorBatch_swiginit(self, _swigfaiss.new_IDSelectorBatch(n, indices))\r\nTypeError: in method 'new_IDSelectorBatch', argument 2 of type 'faiss::IDSelector::idx_t const *'\r\n```\r\n\r\nAlso, I've tried to use `IndexIDMap`\r\n```python\r\nindex = faiss.IndexFlatL2(xb.shape[1])\r\nindex2 = faiss.IndexIDMap(index)\r\nindex2.add_with_ids(xb, ids)\r\n--------------\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\faiss\\__init__.py\", line 212, in replacement_add_with_ids\r\n    self.add_with_ids_c(n, swig_ptr(x), swig_ptr(ids))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\faiss\\swigfaiss.py\", line 4661, in add_with_ids\r\n    return _swigfaiss.IndexIDMap_add_with_ids(self, n, x, xids)\r\nTypeError: in method 'IndexIDMap_add_with_ids', argument 4 of type 'faiss::IndexIDMapTemplate< faiss::Index >::idx_t const *'\r\n```\r\n\r\nBut have the same code samples working on OSX. How can I properly delete items from `IndexFlatL2`?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1853/comments",
    "author": "hadhoryth",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-04-29T15:08:21Z",
        "body": "please make sure it is an array of int64s\r\n```\r\nindex.remove_ids(np.array([0], dtype='int64'))\r\n```"
      },
      {
        "user": "hadhoryth",
        "created_at": "2021-04-29T15:41:51Z",
        "body": "Yes, you are right. Everything is working. Thank you."
      }
    ]
  },
  {
    "number": 1852,
    "title": "About training, history data and daily updated data.",
    "created_at": "2021-04-29T02:08:35Z",
    "closed_at": "2021-05-02T21:08:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1852",
    "body": "Running on:\r\n- [âˆš ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [âˆš ] Python\r\n\r\nFirst we add history data (about 150kk) into the index, and randomly select some to train the index.\r\nAnd then we have newly updated data everyday (about 400k/day).\r\nHow should I train the index everyday? Should I only randomly select {the daily updated data} for training, or randomly select {the daily updated data and all history data}for training?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1852/comments",
    "author": "splinter21",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2021-04-29T03:42:24Z",
        "body": "If the distribution of your data does not change, there is no need to re-train the index."
      },
      {
        "user": "splinter21",
        "created_at": "2021-04-29T08:17:18Z",
        "body": "> If the distribution of your data does not change, there is no need to re-train the index.\r\n\r\nIf there is some change of the distribution everday, and I need train the index everyday, should I sample the training data from {all history data and the daily updated data} instead of training only from the sample of {the daily updated data}?"
      },
      {
        "user": "mdouze",
        "created_at": "2021-04-29T15:06:30Z",
        "body": "We can't give a generic answer to this question: it depends by how much the data distribution changes!"
      }
    ]
  },
  {
    "number": 1843,
    "title": "View Progress During Searching",
    "created_at": "2021-04-26T13:43:19Z",
    "closed_at": "2021-04-30T08:27:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1843",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\nDoes faiss provide a method to monitor the searching progress on IndexFlat?\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Linux <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: 1.7.0 <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: anaconda <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: N/A <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\nN/A\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1843/comments",
    "author": "JerryLife",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-04-26T20:44:01Z",
        "body": "If you have several queries, the easiest would be to run them in batches of 0.1 sec each and build a progress bar over batches.\r\n"
      },
      {
        "user": "JerryLife",
        "created_at": "2021-04-27T03:37:28Z",
        "body": "Should I manually divide the batches and invoke `index.search()` or does faiss provide an interface for batching? I am concerned whether faiss has some optimization on matrix operation, in which cases the manual division could harm the performance of searching. "
      },
      {
        "user": "mdouze",
        "created_at": "2021-04-27T09:57:11Z",
        "body": "The performance penalty should not matter if the batches are large enough."
      }
    ]
  },
  {
    "number": 1811,
    "title": "IndexIVFFlat search is returning less than specified top-k closest embeddings.",
    "created_at": "2021-04-08T14:55:48Z",
    "closed_at": "2022-04-04T10:49:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1811",
    "body": "# Summary\r\n\r\nHi,\r\nI'm using an `IndexIVFFlat` Index with `IndexFlatIP` as a quantizer. After training the Index and adding my ~500k  embeddings, I'm running `index.search(query_embedding, 1000)` to receive the top-1000 embeddings according to the dot-product metric. `index.nprobe` is set to 250.\r\n\r\nThe problem I'm facing is, that I get way less than the top-1000. It depends on the query_embedding but most of the time it returns between 5-50 items only. \r\n\r\nWhy is this and how can I solve this to get exactly the top-1000 ?\r\n\r\n# Platform\r\n\r\nOS: Ubuntu 18.04LTS\r\n\r\nFaiss version: 1.7.0 release \r\n\r\nInstalled from: conda pytorch\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1811/comments",
    "author": "floschne",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-04-12T09:29:26Z",
        "body": "It is unusual to use IndexFlatIP as a quantizer but it should work. You should set the nprobe both for the toplevel index because the default nprobe is 1 (too low). This probably explains why you get fewer results. \r\nTo set the nprobe on the quantizer use \r\n\r\n```\r\nfaiss.extract_index_ivf(index.quantizer).nprobe = 123\r\n```"
      },
      {
        "user": "floschne",
        "created_at": "2021-04-15T07:52:16Z",
        "body": "Thanks for your answer! I'll give it a try and report what happened :-)\r\n\r\nWhy is using an IndexFlatIP unusual as quantizer? What quantizer would you recommend and why? "
      },
      {
        "user": "QwertyJack",
        "created_at": "2021-04-18T13:12:45Z",
        "body": "@mdouze Does the quantizer `IndexFlatIP` have a `nprobe` ?"
      },
      {
        "user": "floschne",
        "created_at": "2021-04-22T10:28:51Z",
        "body": "push :-)"
      },
      {
        "user": "wickedfoo",
        "created_at": "2021-04-22T21:00:09Z",
        "body": "> @mdouze Does the quantizer `IndexFlatIP` have a `nprobe` ?\r\n\r\nno, nprobe is only on IVF indices. IndexFlatIP is brute-force evaluation (every candidate is considered).\r\n"
      },
      {
        "user": "wickedfoo",
        "created_at": "2021-04-22T21:01:00Z",
        "body": "nprobe on IndexIVFFlat determines the number of nearest neighbors in the IndexFlatIP (the k) that are being considered though.\r\n"
      }
    ]
  },
  {
    "number": 1795,
    "title": "Change the centroid vectors for GpuIndexIVFPQ?",
    "created_at": "2021-03-30T20:28:56Z",
    "closed_at": "2021-04-09T07:10:09Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1795",
    "body": "\r\nThe wiki provides a method to change the PQ centroids with python and then write back to Faiss Index object. However, it seems not work with Gpu Index. Is there a way to write back to a GPU index object, specifically, GpuIndexIVFPQ?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1795/comments",
    "author": "jingtaozhan",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2021-03-31T00:08:27Z",
        "body": "The only way to serialize a GPU index is to convert it to/from the CPU version at present, sorry.\r\n"
      },
      {
        "user": "wickedfoo",
        "created_at": "2021-03-31T00:08:49Z",
        "body": "The centroid vectors cannot be changed natively on the GPU side, you'd have to convert to the CPU, re-encode, and then convert back.\r\n"
      },
      {
        "user": "jingtaozhan",
        "created_at": "2021-04-09T07:10:09Z",
        "body": "Thank you!"
      }
    ]
  },
  {
    "number": 1789,
    "title": "Could I train the index data  once again when I have much incremental data? ",
    "created_at": "2021-03-29T12:06:55Z",
    "closed_at": "2024-07-18T00:37:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1789",
    "body": "# Summary\r\nHi, all:\r\n    \r\nWhen I build the full amount of data, I train the data first, and then **pass add_ with_ ids** method create the  \"index\" for the data. After that, I will continue to receive new data, and add it again when the data meets a certain size.    The question is : before adding these, could I call the train function again to incrementally train the trained 'index'? And could the train function only be trained once before building an index? \r\n\r\nThank you\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\ncentos7\r\n\r\nFaiss version:  \r\n\r\nInstalled from:  compiled by yourself \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1789/comments",
    "author": "codingcai",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-03-30T17:01:19Z",
        "body": "Unfortunately there is no support for retraining after incrementally adding vectors."
      },
      {
        "user": "codingcai",
        "created_at": "2021-04-02T03:12:09Z",
        "body": "Thank you  so much @mdouze  . \r\nI have another two questions:\r\n\r\n1ï¼‰ When I build the index,  I use the name \"IVF4096,PQ4\" to build through the index_factory .  In my understanding, this index will create 4096 center points, and each vector will be compressed in PQ mode. At the same time, the quantizer is flatl2 by default. However, when I use this index to train 10W objects, on the one hand, the log prompts that there are too many 10W vectors, on the other hand, it says that training set too big. What's my opinion index.type Is there anything wrong with that? Or do I have more or less training points?\r\n\r\n********************************************************************************************************\r\n **log output**\r\nWARNING clustering 100000 points to 4096 centroids: please provide at least 159744 training points  \r\nTraining level-1 quantizer          \r\nTraining level-1 quantizer on 100000 vectors in 16D   \r\nTraining IVF residual  \r\nInput training set too big (max size is 65536), sampling 65536 / 100000 vectors  \r\ncomputing residuals  \r\ntraining 4x256 product quantizer on 65536 vectors in 16D  \r\nTraining PQ slice 0/4  \r\n***************************************************************************************************************\r\n\r\n2ï¼‰   After training data with \"IVFPQ*\" index , and I use the add_with_ids to add vectors and ids ,  I want to remove some invalids ids and vectors .  Could I remove the  specific invalid ids  so that I won't search these ids and vectors ?  I look for the wiki and see the \"remove operation\" is only supported for Flat* ?  \r\n        When I use the add_with_ids , what will happen if I use the same id with different new vectors  (how to solve the old vectors)?  When I search the result , will the old vector return ?  \r\n\r\nThank you."
      }
    ]
  },
  {
    "number": 1763,
    "title": "scikit-learn implementation vs faiss implementation",
    "created_at": "2021-03-16T22:43:12Z",
    "closed_at": "2021-03-19T08:05:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1763",
    "body": "I have never used faiss before, I heard the kmeans implementaion of faiss is faster than scikit-learn.\r\n\r\nHere's the basic code that I had used. \r\n\r\n```\r\nimport faiss\r\n\r\nclass FaissKMeans:\r\n    def __init__(self, n_clusters=200000, n_init=2, max_iter=500):\r\n        self.n_clusters = n_clusters\r\n        self.n_init = n_init\r\n        self.max_iter = max_iter\r\n        self.kmeans = None\r\n        self.cluster_centers_ = None\r\n        self.inertia_ = None\r\n\r\n    def fit(self, X):\r\n        self.kmeans = faiss.Kmeans(d=X.shape[1],\r\n                                   k=self.n_clusters,\r\n                                   niter=self.max_iter,\r\n                                   nredo=self.n_init,\r\n                                   gpu=True,\r\n                                   verbose=True)\r\n        self.kmeans.train(X.astype(np.float32))\r\n        self.cluster_centers_ = self.kmeans.centroids\r\n        self.inertia_ = self.kmeans.obj[-1]\r\n\r\n    def predict(self, X):\r\n        return self.kmeans.index.search(X.astype(np.float32), 1)[1]\r\n\r\nkmeans = FaissKMeans(n_clusters=200000, max_iter=500, n_init=2)\r\nkmeans.fit(sentence_embeddings)\r\n\r\nWARNING clustering 271425 points to 200000 centroids: please provide at least 7800000 training points\r\nClustering 271425 points in 768D to 200000 clusters, redo 2 times, 500 iterations\r\n  Preprocessing in 0.08 s\r\nOuter iteration 0 / 2\r\n\r\n Iteration 39 (1244.95 s, search 1229.47 s): objective=1.66762e+06 imbalance=1.258 nsplit=0\r\n```\r\n\r\nSo I never got such warning in scikit-learn implementation, and wanted to know will there be any issue due to this warning? \r\n\r\nI have read that kmeans implementation sub-samples the training set and to overcome this I have to use `max_points_per_centroid=271425`, do I set it as argument in `faiss.Kmeans`. Any example would help. \r\n\r\nFinally, I need some clarification about `imbalance`, what exactly an imbalance mean here?\r\n\r\nAny support is appreciated. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1763/comments",
    "author": "abhipn",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2021-03-17T00:35:08Z",
        "body": "Yes, the disadvantage is that the number of clusters being produced is about the same as the number of vectors that you are using for clusters, so it isn't really clustering the dataset by much (they will be low quality). The warning is that it wants you to use at least 7800000 / 200000 = 39 vectors per cluster being trained.\r\n\r\nImbalance is the degree to which the 200000 clusters being produced is imbalanced; i.e., the max difference between the cluster that clusters the largest number of vectors and the smallest number of ones. In this case, there is a 1.258x difference between the two. Good clusters should evenly partition the dataset as best as possible (imbalance closer to 1.0 is best)."
      }
    ]
  },
  {
    "number": 1759,
    "title": "How to assign GPU id to GpuIndexFlatL2?",
    "created_at": "2021-03-14T15:24:42Z",
    "closed_at": "2023-05-31T06:33:23Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1759",
    "body": "Hello,\r\nI want to run 4 different GpuIndexFlatL2 in parallel on 4 different GPUs. However, all indexes are being created on the one with gpu_id=0. How can I assign device or gpu id when creating index using GpuIndexFlatL2() or allocating GPU resources StandardGPUResources()?\r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1759/comments",
    "author": "shipra25jain",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2021-03-15T01:56:49Z",
        "body": "Set `device` in the `GpuIndexFlatConfig` that is passed to the constructor of `GpuIndexFlatL2` to your preferred device.\r\n"
      }
    ]
  },
  {
    "number": 1729,
    "title": "[help] Distance calculation needs improvement for my faiss index",
    "created_at": "2021-03-04T11:36:12Z",
    "closed_at": "2023-05-31T06:34:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1729",
    "body": "I tried building faiss index with template \"OPQ64_256,IVF262144,PQ32\" on 10M vector(1024 dimension), and the result distances are used to calculate some kind of probability, which seems to have bad accuracy. \r\n\r\nThe question is how could we improve the distance accuracy and keep the index size compact at the same time.\r\n\r\nThanks. \r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1729/comments",
    "author": "juncaofish",
    "comments": [
      {
        "user": "juncaofish",
        "created_at": "2021-03-05T11:09:54Z",
        "body": "I change the index factory template to \"IVF262144,PQ32\" (remove the dimension reduction step), the distance accuracy improves obviously... As a tradeoff, the index size increase from 993.9 M to 1.8 G .  Any further suggestion for optimization? "
      },
      {
        "user": "mdouze",
        "created_at": "2021-03-05T20:29:45Z",
        "body": "Actually the PQ size is the same in both cases. The dominant factor is the size of the IVF codebook (262144*1024*4 bytes). \r\nTo reduce the codebook size, you can \r\n1. compress the codebook, eg with IVF262144(SQ8) \r\n2. use IMI, eg. with IMI2x9,PQ32"
      }
    ]
  },
  {
    "number": 1672,
    "title": "The number of CPU cores affects the training and search time",
    "created_at": "2021-02-07T06:52:54Z",
    "closed_at": "2024-08-04T01:55:52Z",
    "labels": [
      "question",
      "autoclose",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1672",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: 1.6.3 <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: source<!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\nWe verify the training and search time by binding CPU cores in the docker environment. The environment is as follows:\r\n\r\nnumber of cpu: 2( 8 cores, 16threads)\r\ndocker commend:   \r\n1. docker run  --gpus \"device=0\" --cpuset-cpus=\"0-7\" \r\n2. docker run  --gpus \"device=0\" --cpuset-cpus=\"0-15\"\r\n3. docker run  --gpus \"device=0\" --cpuset-cpus=\"0-31\" \r\n\r\nWe run 3-IVFPQ.cpp in \"faiss/tutorial/cpp\".  The results are as follows:\r\n\r\n**nbï¼š200w random data\r\nnlistï¼š2048\r\nnqï¼š20 random data**\r\n\r\ncpu core | train time(s) | search time(ms)\r\n:---:|:---:|:---:\r\ncpu_0-7| 33.98| 17.42\r\ncpu_0-15|21.14|17.21\r\ncpu_0-31|63.68|36.25\r\n\r\n\r\n**Why is it that the more the number of cores, the longer the time?**\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1672/comments",
    "author": "Aurevoir-68",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-02-08T08:11:52Z",
        "body": "What does 200w mean?\r\nWhat is the number of threads used at train time, see `faiss.omp_get_num_threads()`\r\n20 queries is too few to measure meaningful timings."
      },
      {
        "user": "Aurevoir-68",
        "created_at": "2021-02-08T09:35:08Z",
        "body": "(1) 200w means 2 millions,\r\nd=512 nq=1000 m=64 nlist=2048\r\n\r\n(2) docker commend:\r\ndocker run --gpus \"device=0\" --cpuset-cpus=\"0-7\"  \r\ndocker run --gpus \"device=0\" --cpuset-cpus=\"0-15\"\r\ndocker run --gpus \"device=0\" --cpuset-cpus=\"0-31\"\r\n\r\nWhen using the above command, we print the value omp_get_num_threads() in clustering.cpp ,The results were as followsï¼š8threads,   16threads and 32threads as we expect.\r\n\r\nï¼ˆ3ï¼‰We modify the value of nq from 20 to 1000. The training time of 8 threads, 16 threads and 32 threads are as follows:\r\n      8 threadsï¼š 55s\r\n     16 threads:  32s\r\n     32 threads: 197s\r\n\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2021-02-08T23:08:07Z",
        "body": "I suspect there is something wrong with your docker install. Could you time a MKL matrix multiplication, like\r\n\r\n```\r\nimport time\r\nimport numpy as np\r\nimport faiss\r\n\r\nrs = np.random.RandomState(123)\r\nA = rs.randn(2000, 10000)\r\nB = rs.randn(10000, 2000)\r\n\r\nfor nt in 1, 2, 4, 8, 16, 24: \r\n    faiss.omp_set_num_threads(nt)\r\n    t0 = time.time()\r\n    C = A @ B     \r\n    t1 = time.time()\r\n    print(nt, t1 - t0)\r\n```\r\n\r\nOn my 24-core machine it gives \r\n```\r\n1 2.2891054153442383\r\n2 1.115039587020874\r\n4 0.6096878051757812\r\n8 0.29677295684814453\r\n16 0.30524182319641113\r\n24 0.32387566566467285\r\n```\r\n"
      },
      {
        "user": "Aurevoir-68",
        "created_at": "2021-02-23T11:21:35Z",
        "body": "@mdouze When I test the above demo in docker, the results are as follows:\r\n\r\n1 1.9535372257232666\r\n2 1.1300079822540283\r\n4 0.6262509822845459\r\n8 0.3299119472503662\r\n16 0.3111748695373535\r\n32 0.28285908699035645\r\n\r\nDoes this mean that there is no problem with the installation of docker? There may be other problems?\r\n"
      },
      {
        "user": "Aurevoir-68",
        "created_at": "2021-02-25T07:39:08Z",
        "body": "We further test the C + + programs. **The time consumption of testing different cores on the localhost is as follows:**\r\n\r\ncpu core | time\r\n:---:|:---:|\r\n1|35.4856\r\n2 |18.0447\r\n4     |9.4889 \r\n8    | 5.1875   \r\n16 |   2.9397  \r\n32   | 1.9653   \r\n\r\n**The time consumption of testing different cores on the docker container is as follows:**\r\ncpu core | time\r\n:---:|:---:|\r\n 1    | 44.2805\r\n2     |22.2557\r\n 4     |11.5767\r\n 8     |6.4056\r\n16    |3.7844\r\n32    |2.9825\r\n\r\n\r\n**Does this mean that there is no problem with the installation of docker? maybe faiss?**\r\n\r\n\r\n**Here is the c++ code  for testing:**\r\n```\r\nfloat fvec_inner_product_ref(const float * x, const float * y, size_t d)\r\n{\r\n       size_t i;\r\n       float res = 0;\r\n       for (i = 0; i < d; i++)\r\n              res += x[i] * y[i];\r\n       return res;\r\n}\r\n\r\nvoid my_inner_product(const float * x, const float * y, size_t d, size_t nq, size_t nb, float * res, int nt)\r\n{\r\n#pragma omp parallel for num_threads(nt)\r\n       for (size_t i = 0; i < nq; i++)\r\n       {\r\n              for (size_t j = 0; j < nb; j++)\r\n              {\r\n                     //cout << \"( \" << i << \" , \" << j << \")\" << \" thread num: \" << omp_get_thread_num() << endl;\r\n                     res[i * nb + j] = fvec_inner_product_ref(x + i * d, y + j * d, d);\r\n              }\r\n       }\r\n}\r\n\r\n\r\nint main()\r\n{\r\nsize_t d = 64;\r\nsize_t x_n = 20000;\r\nsize_t y_n = 40000;\r\n...\r\nfor(int nt=1;nt<64;nt *=2)\r\n{\r\n    float * res = new float[x_n*y_n];\r\n    double t0 = omp_get_wtime();\r\n    my_inner_product(x,y,d,x_n,y_n,,res,nt);\r\n    double t1 = omp_get_wtime();\r\n ...\r\n}\r\n\r\n}\r\n```"
      },
      {
        "user": "jinwenabc",
        "created_at": "2021-03-25T03:46:13Z",
        "body": "Same problem encounteredï¼Œ for IVF index trainingï¼Œ the omp programming is used in compute centroids. seems no race condition here because faiss split data into different parts, every thread taking care only one.\r\nhope some one can give an answer"
      },
      {
        "user": "PXThanhLam",
        "created_at": "2023-08-14T10:00:58Z",
        "body": "I run this script on my local docker container on mac( faiss version 1.7.2, python 3.6.8, faiss is install by pip, mac with 4 cpu cores) and the response is at follow :\r\n1 0.4522852897644043\r\n2 0.43656301498413086\r\n4 0.43779563903808594\r\n8 0.43766117095947266\r\n16 0.4380381107330322\r\n24 0.4380462169647217\r\nSeems like increasing number of threads doesnt help at all. I run the code on local machine (without docker) and the response time behaves as I expected: Increase worker decrease process time. The behavior only happen inside docker container.\r\n\r\n> I suspect there is something wrong with your docker install. Could you time a MKL matrix multiplication, like\r\n> \r\n> ```\r\n> import time\r\n> import numpy as np\r\n> import faiss\r\n> \r\n> rs = np.random.RandomState(123)\r\n> A = rs.randn(2000, 10000)\r\n> B = rs.randn(10000, 2000)\r\n> \r\n> for nt in 1, 2, 4, 8, 16, 24: \r\n>     faiss.omp_set_num_threads(nt)\r\n>     t0 = time.time()\r\n>     C = A @ B     \r\n>     t1 = time.time()\r\n>     print(nt, t1 - t0)\r\n> ```\r\n> \r\n> On my 24-core machine it gives\r\n> \r\n> ```\r\n> 1 2.2891054153442383\r\n> 2 1.115039587020874\r\n> 4 0.6096878051757812\r\n> 8 0.29677295684814453\r\n> 16 0.30524182319641113\r\n> 24 0.32387566566467285\r\n> ```\r\n\r\n"
      },
      {
        "user": "JohnTailor",
        "created_at": "2023-08-16T00:48:34Z",
        "body": "I can confirm that IndexIVFPQ search can get extremely slow (on CPU and Python ; memory is not fully utilized on my machine, I also don't use Docker)- it appears like race conditions (CPUs are stuck at max load), although since we are just searching there should be no need for locks. I need some further testing to really know the details. "
      },
      {
        "user": "mdouze",
        "created_at": "2023-08-16T12:15:10Z",
        "body": "Please install with conda not pip. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-27T01:50:41Z",
        "body": "This issue is stale because it has been open for 7 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-04T01:55:52Z",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ]
  },
  {
    "number": 1668,
    "title": "Inefficient multi-core usage",
    "created_at": "2021-02-04T11:55:58Z",
    "closed_at": "2021-10-19T21:49:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1668",
    "body": "# Summary\r\nIndex search method utilizes only around 4.5 cpus given 10+.\r\nDo I miss some configuration settings? In terms of OpenMP threads, by default it spawns a number of threads equal to the number of CPU-s. I couldn't get much better results varying this option though.\r\n\r\nNumbers:\r\n|cpus | search time|\r\n| ------------- | ------------- |\r\n|1  | 7.01 +/- 0.75|\r\n|2  | 3.35 +/- 0.28|\r\n|4  | 1.70 +/- 0.04|\r\n|6  | 1.67 +/- 0.06|\r\n|8  | 1.66 +/- 0.06|\r\n|10 | 1.66 +/- 0.06|\r\n\r\nI witnessed the same issue in AWS on 16CPU instances\r\n\r\n# Platform\r\n\r\n2,6 GHz 6-Core Intel Core i7 (12 cores in HT)\r\nPython 3.7.6\r\nfaiss-cpu=1.6.5\r\nnumpy=1.19.2\r\n\r\nInstalled from: anaconda\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n\r\nCode:\r\n```\r\nfrom time import time\r\nimport faiss\r\nimport numpy as np\r\n\r\nSEED = 42\r\nINDEX_SIZE = 1_000_000\r\nSEARCH_SIZE = 1_000\r\nVECTOR_DIM = 128\r\nN_CLOSEST = 10\r\nN_TRIES = 10\r\n\r\nnp.random.seed(SEED)\r\n\r\nvectors_index = np.random.random((INDEX_SIZE, VECTOR_DIM)).astype(np.float32)\r\nindex = faiss.IndexFlatIP(vectors_index.shape[1])\r\nindex.add(vectors_index)\r\n\r\nvectors_search = np.random.random((SEARCH_SIZE, VECTOR_DIM)).astype(np.float32)\r\ntimings = []\r\nfor _ in range(N_TRIES):\r\n    start = time()\r\n    index.search(vectors_search, N_CLOSEST)\r\n    timings.append(time() - start)\r\nprint(round(np.mean(timings), 2), \"+/-\", round(2 * np.std(timings), 2))\r\n```\r\n\r\nDocker image:\r\n```\r\nFROM continuumio/miniconda3:4.8.2\r\nRUN conda install -c pytorch faiss-cpu=1.6.5 numpy=1.19.2\r\nENV OMP_WAIT_POLICY passive\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1668/comments",
    "author": "ilivans",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-02-04T16:43:32Z",
        "body": "The speedup is not linear with the number of cpus. \r\nBTW IndexflatIP is basically just a matrix multiplication so you can probably observe the same scores with the numpy matrix mul."
      }
    ]
  },
  {
    "number": 1658,
    "title": "Does faiss support online spherical clustering with GPU?",
    "created_at": "2021-01-30T20:34:38Z",
    "closed_at": "2021-12-03T18:56:03Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1658",
    "body": "We are going to perform a spherical clustering on a large dataset which is over our gpu memory, can and how I do it with faiis?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1658/comments",
    "author": "RealNewNoob",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-02-02T09:33:54Z",
        "body": "NB that only the centroids need to fit in GPU memory, not the dataset. \r\n\r\n"
      },
      {
        "user": "RealNewNoob",
        "created_at": "2021-02-03T21:33:11Z",
        "body": "> NB that only the centroids need to fit in GPU memory, not the dataset.\r\n\r\nI read the wiki and learned how to set up the index on disk. However, I still don't know how to load huge data properly in faiss.Clustering() or faiss.Kmeans(). Is there any tutorial for this?"
      },
      {
        "user": "gasparyanartur",
        "created_at": "2023-07-31T06:44:25Z",
        "body": "Seeing as this was completed, could you mind pointing to where you found the answer to the question?"
      }
    ]
  },
  {
    "number": 1656,
    "title": "Why IndexIvfx is slower than IndexFlatL2?",
    "created_at": "2021-01-29T08:59:29Z",
    "closed_at": "2024-05-14T21:42:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1656",
    "body": "# Summary\r\nHelloï¼š\r\n\r\nWhen I make performerce test, I found IndexIVFx is slower IndexFlatL2, but when I change xq's shape to (1, 512), IndexIVFFlat is faster IndexFlatL2 about 1 times. \r\n1ã€My database shape is (3511, 512)ï¼Œtrain data is my full database\r\n2ã€xq shape is ï¼ˆ1000ï¼Œ512)\r\n3ã€IVFx nlist is 90ï¼Œ nprobe is 9\r\n\r\n\r\n\r\nCPU mode result is:\r\nFlat memory:  7248\r\nFlat add time:  0.0046727657318115234\r\nFlat search time:  0.36131834983825684\r\n----------------\r\nIVF train time:  0.5231897830963135\r\nIVF memory:  6588\r\nIVF add time:  0.04633903503417969\r\nIVF search time:  0.8941836357116699\r\nIVF accuracy with order:  0.9958\r\nIVF accuracy without order:  0.9984\r\n----------------\r\nIVFSQ train time:  0.5908169746398926\r\nIVFSQ memory:  0\r\nIVFSQ add time:  0.0765986442565918\r\nIVFSQ search time:  6.375136137008667\r\nIVFSQ accuracy with order:  0.9912\r\nIVFSQ accuracy without order:  0.9978\r\n----------------\r\n\r\nGPU modeï¼š\r\nFlat memory:  0\r\nFlat add time:  0.001638650894165039\r\nFlat search time:  0.0032868385314941406\r\n----------------\r\nIVF train time:  0.15410685539245605\r\nIVF memory:  0\r\nIVF add time:  0.006935834884643555\r\nIVF search time:  0.005383014678955078\r\nIVF accuracy with order:  0.9954\r\nIVF accuracy without order:  0.9964\r\n----------------\r\nIVFSQ train time:  0.17790675163269043\r\nIVFSQ memory:  0\r\nIVFSQ add time:  0.005623817443847656\r\nIVFSQ search time:  0.005033016204833984\r\nIVFSQ accuracy with order:  0.9896\r\nIVFSQ accuracy without order:  0.9958\r\n----------------\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS:ubuntu16.04\r\n\r\nFaiss version: 1.6.5\r\n\r\nInstalled from:  compiled\r\n\r\nFaiss compilation options:cmake -DCMAKE_CUDA_ARCHITECTURES=\"61;75\" -B build .\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [1 ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [1 ] Python\r\n\r\n# Reproduction instructions\r\n\r\nmy test code:\r\n\r\nimport os\r\nos.environ['OMP_NUM_THREADS'] = '1'\r\nimport faiss\r\nimport numpy as np\r\nimport time\r\nfrom memory_profiler import profile\r\n\r\nlimit = 1000\r\n\r\ndef indexSearch(xq, index, topk=5):\r\n    D, I = index.search(xq, topk)  # actual search\r\n    S = (3 - D) / 3\r\n    S = np.maximum(S, 0)\r\n    S = np.minimum(S, 1)\r\n    return S, I\r\n\r\n\r\ndef indexFlatL2Bench(xq, dim, topk, device='GPU'):\r\n    indexFlatL2 = faiss.IndexFlatL2(dim)\r\n    if device == 'GPU':\r\n        indexFlatL2 = getGPUIndex(indexFlatL2)\r\n    memory1 = faiss.get_mem_usage_kb()\r\n    time1 = time.time()\r\n    indexFlatL2.add(xq)\r\n    time2 = time.time()\r\n    print('Flat memory:  {0}'.format(faiss.get_mem_usage_kb() - memory1))\r\n    randomRow = np.arange(xq.shape[0])\r\n    np.random.shuffle(randomRow)\r\n    xq = xq[randomRow[:limit]]\r\n    print('Flat add time:  {0}'.format(time2 - time1))\r\n    _, benchIds = indexSearch(xq, indexFlatL2, topk)\r\n    print('Flat search time:  {0}'.format(time.time() - time2))\r\n    print('----------------')\r\n    return randomRow, benchIds\r\n\r\n\r\ndef indexIVFBench(xq, randomRow, trainData, dim, topk, benchIds, nlist=90, nprobe=9, device='GPU'):\r\n    indexIVF = faiss.IndexFlatL2(dim)\r\n    indexIVF = faiss.IndexIVFFlat(indexIVF, dim, nlist)\r\n    if device == 'GPU':\r\n        indexIVF = getGPUIndex(indexIVF)\r\n    time3 = time.time()\r\n    indexIVF.train(trainData)\r\n    memory2 = faiss.get_mem_usage_kb()\r\n    time4 = time.time()\r\n    print('IVF train time:  {0}'.format(time4 - time3))\r\n    indexIVF.add(xq)\r\n    time5 = time.time()\r\n    print('IVF memory:  {0}'.format(faiss.get_mem_usage_kb() - memory2))\r\n    print('IVF add time:  {0}'.format(time5 - time4))\r\n    indexIVF.nprobe = nprobe\r\n    xq = xq[randomRow[:limit]]\r\n    time6 = time.time()\r\n    _, IVFIds = indexSearch(xq, indexIVF, topk)\r\n    print('IVF search time:  {0}'.format(time.time() - time6))\r\n    accuracyWithOrder, accuracyWithoutOrder = getAccuracy(IVFIds, benchIds)\r\n    print('IVF accuracy with order:  {0}'.format(accuracyWithOrder))\r\n    print('IVF accuracy without order:  {0}'.format(accuracyWithoutOrder))\r\n    print('----------------')\r\n    return\r\n\r\n\r\ndef indexIVFSQBench(xq, randomRow, trainData, dim, topk, benchIds, nlist=90, nprobe=9, device='GPU'):\r\n    indexIVFSQ = faiss.IndexFlatL2(dim)\r\n    indexIVFSQ = faiss.IndexIVFScalarQuantizer(indexIVFSQ, dim, nlist, faiss.ScalarQuantizer.QT_8bit)\r\n    if device == 'GPU':\r\n        indexIVFSQ = getGPUIndex(indexIVFSQ)\r\n    time7 = time.time()\r\n    indexIVFSQ.train(trainData)\r\n    memory3 = faiss.get_mem_usage_kb()\r\n    time8 = time.time()\r\n    print('IVFSQ train time:  {0}'.format(time8 - time7))\r\n    indexIVFSQ.add(xq)\r\n    time9 = time.time()\r\n    print('IVFSQ memory:  {0}'.format(faiss.get_mem_usage_kb() - memory3))\r\n    print('IVFSQ add time:  {0}'.format(time9 - time8))\r\n    indexIVFSQ.nprobe = nprobe\r\n    xq = xq[randomRow[:limit]]\r\n    time10 = time.time()\r\n    _, IVFSQIds = indexSearch(xq, indexIVFSQ, topk)\r\n    print('IVFSQ search time:  {0}'.format(time.time() - time10))\r\n    accuracyWithOrder, accuracyWithoutOrder = getAccuracy(IVFSQIds, benchIds)\r\n    print('IVFSQ accuracy with order:  {0}'.format(accuracyWithOrder))\r\n    print('IVFSQ accuracy without order:  {0}'.format(accuracyWithoutOrder))\r\n    print('----------------')\r\n    return\r\n\r\ndef getAccuracy(ids, benchIds):\r\n    numberBench = np.sum(benchIds == benchIds)\r\n    numberIdsWithOrder = np.sum(ids == benchIds)\r\n    accuracyWithOrder = numberIdsWithOrder / numberBench\r\n    numberIdsWithoutOrder = 0\r\n    for id, benchId in zip(ids, benchIds):\r\n        for i in id:\r\n            if i in benchId:\r\n                numberIdsWithoutOrder += 1\r\n    accuracyWithoutOrder = numberIdsWithoutOrder / numberBench\r\n    return accuracyWithOrder, accuracyWithoutOrder\r\n\r\ndef getGPUIndex(index):\r\n    res = faiss.StandardGpuResources()\r\n    index = faiss.index_cpu_to_gpu(res, 1, index)  # we want to see 4 nearest neighbors\r\n    return index\r\n\r\nif __name__ == '__main__':\r\n    dim = 512\r\n    topk = 5\r\n    device = 'CPU'\r\n    xq = np.load('trainFeatures.npy')\r\n    trainData = xq\r\n    randomRow, benchIds = indexFlatL2Bench(xq=xq, dim=dim, topk=topk, device=device)\r\n    indexIVFBench(xq=xq, randomRow=randomRow, trainData=trainData, dim=dim, topk=topk, benchIds=benchIds, device=device)\r\n    indexIVFSQBench(xq=xq, randomRow=randomRow, trainData=trainData, dim=dim, topk=topk, benchIds=benchIds, device=device)\r\n   \r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1656/comments",
    "author": "ppdollhhh",
    "comments": [
      {
        "user": "ppdollhhh",
        "created_at": "2021-01-29T09:15:53Z",
        "body": "Anyone can help me?Thx"
      },
      {
        "user": "mdouze",
        "created_at": "2021-01-29T10:32:43Z",
        "body": "The database is too small for IVF to be useful."
      },
      {
        "user": "ppdollhhh",
        "created_at": "2021-01-29T12:32:04Z",
        "body": "I have try database ï¼ˆ100000ï¼Œ 512ï¼‰ï¼Œ then the result is same"
      }
    ]
  },
  {
    "number": 1632,
    "title": "ProductQuantizer does not Initialize",
    "created_at": "2021-01-18T03:00:34Z",
    "closed_at": "2021-04-02T16:53:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1632",
    "body": "# Summary\r\n# Platform\r\n\r\nOS: Linux (Virtual Machine)\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: pip\r\n\r\nRunning on:\r\n- [ ] CPU\r\n\r\nInterface: \r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\nxs,ys = train.shape\r\ncs = 4  # code size (bytes)\r\npq = faiss.ProductQuantizer(ys, cs, 8)\r\n# pq.train(train.values)\r\n\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\n/opt/conda/lib/python3.7/site-packages/faiss/swigfaiss.py in __init__(self, *args)\r\n   1561 \r\n   1562     def __init__(self, *args):\r\n-> 1563         this = _swigfaiss.new_ProductQuantizer(*args)\r\n   1564         try:\r\n   1565             self.this.append(this)\r\n\r\nRuntimeError: Error in void faiss::ProductQuantizer::set_derived_values() at ProductQuantizer.cpp:164: Error: 'd % M == 0' failed",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1632/comments",
    "author": "eladmw",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-01-18T08:31:01Z",
        "body": "The dimension of the vector (d) should be a multiple of the number of subquantizers (M)."
      }
    ]
  },
  {
    "number": 1602,
    "title": "On clustering using k-means (cosine similarity)",
    "created_at": "2021-01-04T09:24:25Z",
    "closed_at": "2021-04-02T16:47:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1602",
    "body": "Currently, we are clustering with the following code.\r\nWe are searching by L2 distance, but we want to search by cosine similarity.\r\nWill it work well if I just change faiss.GpuIndexFlatL2 to faiss.GpuIndexFlatIP?\r\n\r\ndef run_kmeans(x, nmb_clusters, verbose=False):\r\n    \"\"\"Runs kmeans on 1 GPU.\r\n    Args:\r\n        x: data\r\n        nmb_clusters (int): number of clusters\r\n    Returns:\r\n        list: ids of data in each cluster\r\n    \"\"\"\r\n    n_data, d = x.shape\r\n\r\n    # faiss implementation of k-means\r\n    clus = faiss.Clustering(d, nmb_clusters)\r\n\r\n    # Change faiss seed at each k-means so that the randomly picked\r\n    # initialization centroids do not correspond to the same feature ids\r\n    # from an epoch to another.\r\n    clus.seed = np.random.randint(1234)\r\n\r\n    clus.niter = 20\r\n    clus.max_points_per_centroid = 10000000\r\n    res = faiss.StandardGpuResources()\r\n    flat_config = faiss.GpuIndexFlatConfig()\r\n    flat_config.useFloat16 = False\r\n    flat_config.device = 0\r\n    index = faiss.GpuIndexFlatL2(res, d, flat_config)\r\n\r\n    # perform the training\r\n    clus.train(x, index)\r\n    _, I = index.search(x, 1)",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1602/comments",
    "author": "Hiiragi0107",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-01-05T16:02:26Z",
        "body": "Both should work if the training vectors are normalized."
      },
      {
        "user": "Hiiragi0107",
        "created_at": "2021-01-06T04:43:03Z",
        "body": "I see, thank you very much.\r\nI will try to normalize the input vector x with \"faiss.normalize_L2(x) \" and then \"index = faiss.GPuIndexFlatL2(res, d, flat_config)\".\r\nThanks for letting me know."
      },
      {
        "user": "FoilHao",
        "created_at": "2022-08-18T09:57:05Z",
        "body": "have u sa\r\n\r\n> I see, thank you very much. I will try to normalize the input vector x with \"faiss.normalize_L2(x) \" and then \"index = faiss.GPuIndexFlatL2(res, d, flat_config)\". Thanks for letting me know.\r\n\r\nHave u solved this? I meet 'GpuIndexFlatL2' object has no attribute 'shape' AttributeError using GPuIndexFlatL2 in Kmeans.train"
      }
    ]
  },
  {
    "number": 1569,
    "title": "Is the cosine distance normalized to 0-1 and if so how?",
    "created_at": "2020-12-10T11:45:12Z",
    "closed_at": "2020-12-15T16:46:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1569",
    "body": "I built an inner_product index with L2 normalized vectors, with the goal to search by cosine distance. The question that I have is whether this distance is in the typical -1 tot 1 range, or whether it has been normalized to 0-1, and if so - how?\r\n\r\nThanks in advance",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1569/comments",
    "author": "BramVanroy",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-12-15T16:20:44Z",
        "body": "so it's cosine similarity, which is between -1 and 1 like the normal cosine function."
      },
      {
        "user": "BramVanroy",
        "created_at": "2020-12-15T16:46:30Z",
        "body": "Alright, that is clear. Thank you."
      }
    ]
  },
  {
    "number": 1539,
    "title": "Is IMI a good index for GPU?",
    "created_at": "2020-11-22T08:53:40Z",
    "closed_at": "2020-11-23T17:35:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1539",
    "body": "# Summary\r\n\r\nHi, \r\n\r\nI have noticed that IMI is not implemented in the GPU Faiss. I am trying to guess the reason, is that because IMI's memory access pattern does not favor GPUs? Since IMI partitioned the vector set in a very fine-grained manner, it can lead to many small random memory accesses during the searching process, which is not friendly to GPU because these accesses may lead to bank conflicts on GPU global memory (unbalanced workload on each bank). I am just curious if this guess is correct.\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1539/comments",
    "author": "WenqiJiang",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-11-23T14:16:12Z",
        "body": "Yes it is. Moreover, on the GPU it is more feasible to use expensive exhaustive search for the coarse quantizer."
      },
      {
        "user": "WenqiJiang",
        "created_at": "2020-11-23T17:35:53Z",
        "body": "Thanks, great to hear that!"
      }
    ]
  },
  {
    "number": 1526,
    "title": "Can I calculate Chebyshev distance in IVFPQï¼Ÿ",
    "created_at": "2020-11-13T07:38:23Z",
    "closed_at": "2021-01-05T15:43:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1526",
    "body": "# Summary\r\nI know that  METRIC_Linf(Chebyshev distance) is only supported by IndexFlat.\r\nbut I think it's the same as METRIC_L2 ,becouse they are both special case of METRIC_Lp.\r\n\r\nso I wanna know it is possible to  modified PQ's code for supporting Chebyshev distance or other METRIC_Lpï¼Ÿ\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1526/comments",
    "author": "Law0825",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-11-13T11:21:20Z",
        "body": "It's METRIC_Linf\r\nProgramatically, PQ could compute alternative distances (like Linf) at distance compuation time, bit the training with k-means assumes that the final distance will be L2, so the approximation will not be as good in terms of distance distortion. \r\n"
      },
      {
        "user": "Law0825",
        "created_at": "2020-11-16T01:01:48Z",
        "body": "\r\n\r\n\r\n> It's METRIC_Linf\r\n> Programatically, PQ could compute alternative distances (like Linf) at distance compuation time, bit the training with k-means assumes that the final distance will be L2, so the approximation will not be as good in terms of distance distortion.\r\n\r\nso can I use K-medoids or PAM to replace k-meansï¼Ÿ"
      },
      {
        "user": "mdouze",
        "created_at": "2021-01-05T15:43:14Z",
        "body": "yes but it's not implemented in Faiss."
      }
    ]
  },
  {
    "number": 1520,
    "title": "Estimate memory usage of ivfpq",
    "created_at": "2020-11-10T11:08:27Z",
    "closed_at": "2020-11-14T07:59:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1520",
    "body": "I am estimating the memory usage of currently my index configuration, is this correct:\r\n\r\ncurrently vectors size: 114,462,080\r\n\r\nwith ivfpq settings:\r\n\r\n``` python\r\nIn [49]: i = faiss.read_index(\"/opt/data/trainedIndex/ivfpqcnn0.index\")                                                                                                                                                                 \r\n\r\nIn [50]: for j in dir(i): \r\n    ...:     a = getattr(i, j) \r\n    ...:     if j.startswith(\"__\") or str(a).startswith(\"<\"): continue \r\n    ...:     print(f\"{j}: {getattr(i, j)}\") \r\n    ...:                                                                                                                                                                                                                                \r\nby_residual: True\r\nclustering_index: None\r\ncode_size: 64\r\nd: 512\r\ndo_polysemous_training: False\r\nis_trained: True\r\nmaintain_direct_map: False\r\nmax_codes: 0\r\nmetric_arg: 0.0\r\nmetric_type: 1\r\nnlist: 4096\r\nnprobe: 1\r\nntotal: 0\r\nown_fields: True\r\nown_invlists: True\r\nparallel_mode: 0\r\npolysemous_ht: 0\r\npolysemous_training: None\r\nprecomputed_table_max_bytes: 2147483648\r\nquantizer_trains_alone: \r\nscan_table_threshold: 0\r\nuse_precomputed_table: 1\r\nverbose: False\r\n```\r\n\r\nmemory usage estimate:\r\n\r\n``` {.python}\r\nsize = 114462080\r\nmem = size * 1 * 64 / 1024 / 1024 / 1024 \r\nreturn f\"{mem:g}GB\"\r\n```\r\n\r\nGot: : 6.82247GB\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1520/comments",
    "author": "onriv",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-11-13T11:11:39Z",
        "body": "That is correct, but there are overheads like the size of the coarse quantizer and other metadata. \r\n"
      },
      {
        "user": "onriv",
        "created_at": "2020-11-14T08:25:16Z",
        "body": "> That is correct, but there are overheads like the size of the coarse quantizer and other metadata. \n> \n> \n\n@mdouze \nthanks for your answer. The coarse quantizer and other metadata like the inverted lists are quite small compared to the size of all codes right?"
      }
    ]
  },
  {
    "number": 1505,
    "title": "Clearing Cache",
    "created_at": "2020-11-05T01:55:05Z",
    "closed_at": "2020-11-05T06:12:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1505",
    "body": "Gooday all,\r\n\r\nIs there a way to clear cache after a query? (Using on-disk faiss)\r\nI noticed the ram usage started to buildup as repeated random queries are performed.\r\n\r\nI would like it to clear cache whenever the program used up more than 90% of total ram.\r\n\r\nThank you.\r\n\r\n- Stefan",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1505/comments",
    "author": "stefanjuang",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-11-05T05:41:35Z",
        "body": "Cache control is a system-level functionality. Cache can be disabled with \r\n```\r\nsync && sudo sh -c 'echo 3 >/proc/sys/vm/drop_caches'\r\n```\r\n"
      },
      {
        "user": "stefanjuang",
        "created_at": "2020-11-05T06:13:20Z",
        "body": "Thank you!"
      }
    ]
  },
  {
    "number": 1469,
    "title": "How to add a function in C++ and use it in python code in benches",
    "created_at": "2020-10-15T14:20:22Z",
    "closed_at": "2020-11-06T15:55:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1469",
    "body": "I want to add a function in C++ file and then use this function in python code in benches. I successfully compile the C++ code by 'cmake' and 'make'. But I failed to call this function in Python. Could you please tell me how to fix it? \r\nThank you",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1469/comments",
    "author": "Hap-Hugh",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-10-15T14:31:31Z",
        "body": "The function should appear in the python interface. If this is not the case, you probably forgot to install with setup.py."
      },
      {
        "user": "mdouze",
        "created_at": "2020-10-15T15:34:56Z",
        "body": "For ref, here is a one-liner I use to compile + run a test in the tests directory without installing anything: \r\n\r\n```\r\n make -C build VERBOSE=1 swigfaiss &&  (cd build/faiss/python/ ; python setup.py build ) && (pp=$PWD/build/faiss/python/build/lib; cd\r\n tests/ ; PYTHONPATH=$pp python -m unittest -v test_index )\r\n```"
      },
      {
        "user": "Hap-Hugh",
        "created_at": "2020-10-15T16:13:21Z",
        "body": "Thank you so much dear Matthijs. I run a simple test based on the new function I wrote. Fortunately, it works.\r\n\r\nThe other question is, in Link and Code bench, the code use 'import faiss'. But, if I want to use the function defined myself, I have to 'import swigfaiss'. ('import faiss' use every function that can not be modified) Is there any way that I can reconstruct this 'Link and Code' base on swigfaiss?"
      },
      {
        "user": "Hap-Hugh",
        "created_at": "2020-10-16T08:52:28Z",
        "body": "The last comment is solved. Please read the draft in mdouze's answer carefully. There is a manual wrapper, and it's really useful. So just change the python-path to build/faiss/python/build/lib and import the faiss. This will be the updated one.\r\n\r\nBy the way, do I have to run 'make clean' every time I modify the code?"
      },
      {
        "user": "mdouze",
        "created_at": "2020-10-19T16:38:01Z",
        "body": "no."
      },
      {
        "user": "mdouze",
        "created_at": "2020-11-06T15:55:04Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1463,
    "title": "Moving faiss index to multiple GPUs is stuck",
    "created_at": "2020-10-13T20:16:51Z",
    "closed_at": "2020-11-06T15:54:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1463",
    "body": "\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Ubuntu\r\n\r\nFaiss version: 1.6.3\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\n# Summary\r\n\r\nI loaded a trained FAISS index into CPU. when moving the index from CPU to multiple GPUs(8). The index is stuck indefinitely in loading state.\r\n\r\n````\r\ndef my_index_cpu_to_gpu_multiple(index, co=None, gpu_nos=None):\r\n    gpu_nos = [i for i in range(0, torch.cuda.device_count())]\r\n    resources = [faiss.StandardGpuResources() for i in gpu_nos]\r\n    \r\n    co = faiss.GpuMultipleClonerOptions()\r\n    co.usePrecomputed = False\r\n    co.shard = True\r\n    co.useFloat16 = True\r\n\r\n    vres = faiss.GpuResourcesVector()\r\n    vdev = faiss.IntVector()\r\n    if gpu_nos is None: \r\n        gpu_nos = range(len(resources))\r\n    for i, res in zip(gpu_nos, resources):\r\n        vdev.push_back(i)\r\n        vres.push_back(res)\r\n    index = faiss.index_cpu_to_gpu_multiple(vres, vdev, index, co)\r\n    index.referenced_objects = resources\r\n    return index\r\n\r\nif __name__ == \"__main__\":\r\n    faiss_model_name = 'trained.index'\r\n    knn_index_path = os.path.join('models', faiss_model_name)\r\n    knn_index_cpu = faiss.read_index(knn_index_path)\r\n    print(\"Starting to load index to GPU\")\r\n    knn_index = my_index_cpu_to_gpu_multiple(knn_index_cpu)\r\n    print(\"Done loading index to all GPU\")\r\n````\r\n\r\nGPU process using `nvidia-smi`. I see the index is loaded to all the GPUs. \r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     54640      C   python3                                     2711MiB |\r\n|    1     54640      C   python3                                     2711MiB |\r\n|    2     54640      C   python3                                     2711MiB |\r\n|    3     54640      C   python3                                     2711MiB |\r\n|    4     54640      C   python3                                     2711MiB |\r\n|    5     54640      C   python3                                     2711MiB |\r\n|    6     54640      C   python3                                     2711MiB |\r\n|    7     54640      C   python3                                     2711MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nHowever then I try this without the `main` method. It just works fine.\r\n\r\n```\r\nimport faiss\r\nimport os\r\nimport torch\r\n\r\ngpu_resources = []\r\n\r\nfor i in range(8):\r\n    res = faiss.StandardGpuResources()\r\n    gpu_resources.append(res)\r\n\r\ndef my_index_cpu_to_gpu_multiple(index, co=None, gpu_nos=None):\r\n    gpu_nos = [i for i in range(0, torch.cuda.device_count())]\r\n    resources = [faiss.StandardGpuResources() for i in gpu_nos]\r\n    \r\n    co = faiss.GpuMultipleClonerOptions()\r\n    co.usePrecomputed = False\r\n    co.shard = True\r\n    co.useFloat16 = True\r\n\r\n    vres = faiss.GpuResourcesVector()\r\n    vdev = faiss.IntVector()\r\n    if gpu_nos is None: \r\n        gpu_nos = range(len(resources))\r\n    for i, res in zip(gpu_nos, resources):\r\n        vdev.push_back(i)\r\n        vres.push_back(res)\r\n    index = faiss.index_cpu_to_gpu_multiple(vres, vdev, index, co)\r\n    index.referenced_objects = resources\r\n    return index\r\nfaiss_model_name = 'trained.index'\r\nknn_index_path = os.path.join('models', faiss_model_name)\r\nknn_index_cpu = faiss.read_index(knn_index_path)\r\nprint(\"Starting to load index to GPU\")\r\nknn_index = my_index_cpu_to_gpu_multiple(knn_index_cpu)\r\nprint(\"Done loading index to all GPU\")\r\n```\r\n\r\nThe above script works fine. \r\n\r\nIs there an issue when loading index to multiple GPU from another method call?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1463/comments",
    "author": "naveenkumarmarri",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-10-19T16:34:36Z",
        "body": "This usually means that there is a problem because one of the variables went out of scope. Have you tried using `faiss.index_cpu_to_all_gpus`, which should handle the moving properly?"
      },
      {
        "user": "naveenkumarmarri",
        "created_at": "2020-10-22T19:39:30Z",
        "body": "@mdouze\r\nThe method that you mentioned will move the index to all the available GPUs, is it possible to move the index to only few GPUs.\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2020-11-06T15:54:25Z",
        "body": "yes with `gpus=[2, 3, 4]`\r\n"
      },
      {
        "user": "stainswei",
        "created_at": "2023-08-10T08:00:30Z",
        "body": "I encountered the same quesiton but don't know how to solve it "
      }
    ]
  },
  {
    "number": 1423,
    "title": "AttributeError: 'Kmeans' object has no attribute 'iteration_stats'",
    "created_at": "2020-09-23T17:02:47Z",
    "closed_at": "2020-10-07T06:44:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1423",
    "body": "import numpy as np\r\nimport faiss\r\n\r\n\r\n#x = np.random.rand(4,2)*2\r\nx0 = np.load('vec0.npy')\r\nx1 = np.load('vec1.npy')\r\nx2 = np.load('vec2.npy')\r\nx3 = np.load('vec3.npy')\r\n\r\nx = np.concatenate((x0, x1, x2, x3), axis=0)\r\nx = x.astype(np.float32)\r\n\r\nncentroids = 256\r\nniter = 3\r\nverbose = True\r\nd = x.shape[1]\r\nkmeans = faiss.Kmeans(d, ncentroids, niter=niter, verbose=verbose, gpu=True, nredo=1, spherical=True, max_points_per_centroid=1000000000)\r\nkmeans.train(x)\r\n\r\nD, I = kmeans.index.search(x,1)\r\n\r\nprint(I)\r\nprint(kmeans.iteration_stats)\r\n\r\nOutput:\r\nClustering 3592189 points in 100D to 256 clusters, redo 1 times, 3 iterations\r\n  Preprocessing in 0.92 s\r\n  Iteration 2 (1.96 s, search 1.25 s): objective=1.08945e+07 imbalance=1.175 nsplit=0       \r\n[[179]\r\n [  3]\r\n [ 73]\r\n ...\r\n [ 64]\r\n [ 64]\r\n [ 76]]\r\nTraceback (most recent call last):\r\n  File \"faiss_kmeans.py\", line 25, in <module>\r\n    print(kmeans.iteration_stats)\r\nAttributeError: 'Kmeans' object has no attribute 'iteration_stats'\r\n\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1423/comments",
    "author": "mohitiitb",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-09-23T21:50:50Z",
        "body": "What Faiss version are you using?"
      },
      {
        "user": "mohitiitb",
        "created_at": "2020-09-24T06:11:09Z",
        "body": "```\r\n>>> import faiss\r\n>>> faiss.__version__\r\n'1.6.3'\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2020-09-28T13:15:15Z",
        "body": "This is weird, this version does have the field. Does it have a field \"obj\" ? "
      },
      {
        "user": "mdouze",
        "created_at": "2020-10-07T06:44:17Z",
        "body": "no activity, closing."
      },
      {
        "user": "nsriniva03",
        "created_at": "2021-08-17T15:53:19Z",
        "body": "> This is weird, this version does have the field. Does it have a field \"obj\" ?\r\n\r\nThis version does not have field \"obj\". I get the following error \"Clustering object has no attribute obj\"\r\n"
      }
    ]
  },
  {
    "number": 1413,
    "title": "Unknown number of classes",
    "created_at": "2020-09-17T08:08:21Z",
    "closed_at": "2020-10-07T06:41:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1413",
    "body": "I need to do some kind of clustering method but with unknown number of clusters to do unsupervised learning. Is it possible to adjust faiss in the way so it can support unknown number of clusters? If yes, then please tell me how.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1413/comments",
    "author": "memicalem",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-09-18T12:21:06Z",
        "body": "No this is not supported. "
      },
      {
        "user": "memicalem",
        "created_at": "2020-09-18T12:58:29Z",
        "body": "That would be great if possible"
      },
      {
        "user": "mdouze",
        "created_at": "2020-10-07T06:41:08Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1331,
    "title": "how to determine memory usage of PQ model?",
    "created_at": "2020-08-13T17:05:59Z",
    "closed_at": "2020-08-23T08:03:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1331",
    "body": "I have been using faiss product quantizer, can you tell me how to get the memory usage of faiss.ProductQuantization() model?\r\n\r\n@mdouze \r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1331/comments",
    "author": "SouBanerjee",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-08-14T05:40:31Z",
        "body": "the `faiss.ProductQuantizer` has a `code_size` field that gives the size of the codes in bytes."
      },
      {
        "user": "SouBanerjee",
        "created_at": "2020-08-14T09:07:54Z",
        "body": "```python\r\nimport numpy as np\r\nimport faiss\r\nimport sys\r\n\r\nnum_channels = 512\r\ntrain_data_base_init = np.random.normal(size=(50000, 1, 1, num_channels))\r\ntrain_data_base_init = train_data_base_init.astype(np.float32)\r\n\r\ntrain_data_base_init = np.reshape(train_data_base_init, (-1, num_channels))\r\nnum_samples = len(train_data_base_init)\r\n\r\ncodebook_size = 256\r\nnum_codebooks = 32\r\n\r\nnbits = int(np.log2(codebook_size))\r\npq = faiss.ProductQuantizer(num_channels, num_codebooks, nbits)\r\n\r\npq.train(train_data_base_init)\r\n\r\nprint(sys.getsizeof(train_data_base_init)) # returns memory usuage of train_data_base_init in bytes\r\n\r\n\r\nprint(pq.code_size, faiss.get_mem_usage_kb())\r\n# output: 32 417136\r\n\r\n# what is the meaning of pq.code_size ?\r\n# does pq.code_size returns the size of whole product quantization model in bytes ?\r\n# does pq.code_size returned value is similar to sys.getsizeof(train_data_base_init) ?\r\n\r\n# what is the meaning of faiss.get_mem_usage_kb() ?\r\n```\r\n\r\n@mdouze "
      },
      {
        "user": "QwertyJack",
        "created_at": "2020-08-23T07:36:32Z",
        "body": "`code_size` means *each vector* is encoded into this size of buffer (unit: Bytes).\r\n\r\nAs for `get_mem_usage_kb()`, it sums up all memory usage of current process.\r\n\r\nIn your situation, you may try  `faiss.write_ProductQuantizer(pq, 'pq_file')` and then check how much storage the file `pq_file` takes."
      }
    ]
  },
  {
    "number": 1328,
    "title": "Cannot add index to all gpus for training",
    "created_at": "2020-08-12T06:12:24Z",
    "closed_at": "2020-08-25T13:16:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1328",
    "body": "# Summary\r\n\r\nI cannot add my index to multiple gpus, \r\n\r\nI have tried using the `index_cpu_to_all_gpus`, calling `index_cpu_to_gpu_multiple_py` directly \r\n\r\nand I don't think my function arguments are wrong but it returns an error.\r\n\r\n```\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-11-a24c83291cc3> in <module>()\r\n----> 1 gidx =  faiss.index_cpu_to_all_gpus(index, co=co)\r\n\r\n/home1/greyowl/anaconda3/envs/faiss-gpu/lib/python3.6/site-packages/faiss/__init__.py in index_cpu_to_all_gpus(index, co, ngpu)\r\n    516 \r\n    517 def index_cpu_to_all_gpus(index, co=None, ngpu=-1):\r\n--> 518     index_gpu = index_cpu_to_gpus_list(index, co=co, gpus=None, ngpu=ngpu)\r\n    519     return index_gpu\r\n    520 \r\n\r\n/home1/greyowl/anaconda3/envs/faiss-gpu/lib/python3.6/site-packages/faiss/__init__.py in index_cpu_to_gpus_list(index, co, gpus, ngpu)\r\n    528         gpus = range(ngpu)\r\n    529     res = [StandardGpuResources() for _ in gpus]\r\n--> 530     index_gpu = index_cpu_to_gpu_multiple_py(res, index, co, gpus)\r\n    531     return index_gpu\r\n    532 \r\n\r\n/home1/greyowl/anaconda3/envs/faiss-gpu/lib/python3.6/site-packages/faiss/__init__.py in index_cpu_to_gpu_multiple_py(resources, index, co, gpus)\r\n    510         vdev.push_back(i)\r\n    511         vres.push_back(res)\r\n--> 512     index = index_cpu_to_gpu_multiple(vres, vdev, index, co)\r\n    513     index.referenced_objects = resources\r\n    514     return index\r\n\r\nNotImplementedError: Wrong number or type of arguments for overloaded function 'index_cpu_to_gpu_multiple'.\r\n  Possible C/C++ prototypes are:\r\n    faiss::gpu::index_cpu_to_gpu_multiple(std::vector< faiss::gpu::GpuResources * > &,std::vector< int > &,faiss::Index const *,faiss::gpu::GpuMultipleClonerOptions const *)\r\n    faiss::gpu::index_cpu_to_gpu_multiple(std::vector< faiss::gpu::GpuResources * > &,std::vector< int > &,faiss::Index const *)\r\n\r\n```\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 --> CentOS 7.6.1810\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->  1.6.3\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... --> N/A\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n```python\r\nimport faiss\r\nNUM_GPU=8\r\nd = 256\r\nindex = faiss.index_factory(d, \"OPQ64_256,IVF1048576_HNSW32,PQ64\")\r\nco = faiss.GpuClonerOptions()\r\nco.useFloat16 = True\r\ngpu_index = faiss.index_cpu_to_all_gpus(index, co=co)\r\n\r\n```\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1328/comments",
    "author": "greyowl",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-08-13T08:12:20Z",
        "body": "You probably need \r\n```\r\nco = faiss.GpuMultipleClonerOptions()\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2020-08-25T13:16:09Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1311,
    "title": "Is it suitable to add another level of index to control tail latency?",
    "created_at": "2020-08-03T14:05:12Z",
    "closed_at": "2021-01-05T15:26:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1311",
    "body": "Hi\r\nI checked one of the inverted index built by faiss, the expected list size is 1000, however some list size is smaller than 10, while some others is almost 100k size(100x expected size), this makes faiss hard to control the tail search latency. But tail latency is important for online service as we need to guarantee the SLA say all queries should finish within 10ms. Is it suitable to add another level of index for those lists whose size larger than say 10x expected size? If ok, I will try to do it.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1311/comments",
    "author": "fesun",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2020-08-04T22:14:07Z",
        "body": "By how much do the statistics of the training vectors differ from those that you are adding to the index? How many vectors are in your index, and how many do you pass at training time\r\n\r\nThe imbalance of the inverted lists in the index depends directly upon the imbalance factor seen in the k-means clustering to produce the inverted list cells in the first place.  Are you using L2 or IP? Which IVF index are you using specifically?\r\n\r\nAlso, is it possible that you have many duplicate or near-duplicate vectors?\r\n\r\n"
      },
      {
        "user": "fesun",
        "created_at": "2020-08-05T02:51:13Z",
        "body": "> By how much do the statistics of the training vectors differ from those that you are adding to the index? How many vectors are in your index, and how many do you pass at training time\r\n\r\n6 million vectors in total and all passed to train the index.\r\n\r\n\r\n> The imbalance of the inverted lists in the index depends directly upon the imbalance factor seen in the k-means clustering to produce the inverted list cells in the first place. Are you using L2 or IP? Which IVF index are you using specifically?\r\n\r\nL2 and IVFPQ.\r\n\r\n> Also, is it possible that you have many duplicate or near-duplicate vectors?\r\n\r\nThere is no duplicate. I haven't check the near-duplicate but I think it's inevitable for real-world dataset.\r\n\r\n \r\n"
      },
      {
        "user": "fesun",
        "created_at": "2020-08-06T02:41:34Z",
        "body": "@wickedfoo Any comments about this?"
      },
      {
        "user": "QwertyJack",
        "created_at": "2020-08-23T07:46:31Z",
        "body": "It'a great idea to split out the long invlists; just need some hack I think.\r\n\r\nBTW, you can also try HNSW index for just millions of vectors.\r\n\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2020-08-25T13:11:37Z",
        "body": "We don't usually see this kind of behavior because kmeans tends to balance datasets. \r\nIf you can share the data or an IVFFlat index on the data, we can take a look."
      },
      {
        "user": "QwertyJack",
        "created_at": "2020-08-26T03:03:32Z",
        "body": "> We don't usually see this kind of behavior because kmeans tends to balance datasets.\r\n> If you can share the data or an IVFFlat index on the data, we can take a look.\r\n\r\nTheoretically yes, but in practical it is really time-consuming to re-train the model."
      },
      {
        "user": "fesun",
        "created_at": "2020-08-27T15:08:03Z",
        "body": "> We don't usually see this kind of behavior because kmeans tends to balance datasets.\r\n> If you can share the data or an IVFFlat index on the data, we can take a look.\r\n\r\n@mdouze It's ok to share the data, but by how? It's almost 10GB in npy format.\r\n\r\nKmeans should balance the dataset, but maybe it fails if there are many near-duplicate data or outlier datapoints."
      },
      {
        "user": "ckald",
        "created_at": "2020-12-22T15:04:35Z",
        "body": "@wickedfoo How would you proceed in case of many (near-)duplicates? \r\nI'm experiencing possibly related issues described in #1588"
      },
      {
        "user": "mdouze",
        "created_at": "2021-01-05T15:26:57Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 1249,
    "title": "How can I use IndexIDMap with Kmeans ?",
    "created_at": "2020-06-09T13:34:41Z",
    "closed_at": "2020-06-09T17:35:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1249",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\nHow can I use `IndexIDMap` with `Kmeans` ?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1249/comments",
    "author": "diegovalenzuelaiturra",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-06-09T15:19:17Z",
        "body": "I don't understand the question. What do you want to achieve?\r\n"
      }
    ]
  },
  {
    "number": 1219,
    "title": "Range search around query point squared",
    "created_at": "2020-05-16T23:29:45Z",
    "closed_at": "2020-05-18T16:50:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1219",
    "body": "# Summary\r\nShould the radius which has to be provided for the range search be given as squared distance?\r\nIf I just generate random points and search for 10 neighbors, I get 10 squared L2 distances. If I take the square root of the largest one and do a range search around the query point I do not get the same result. I get significantly less neighbors. If I plug in the squared largest distance I get the (so the largest distance I get from the neighbor search) then I retrieve my 10 neighbors again.\r\n\r\nI am just wondering because I don't find it in the documentation:\r\n\r\n\"The method range_search returns all vectors within a radius around the query point (as opposed to the k nearest ones).\"\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1219/comments",
    "author": "wollschlager",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-05-18T06:16:10Z",
        "body": "The range search also takes a squared distance. \r\n\r\nIf you search by the max of distances found by range search (+ epsilon because comparison is strict) and not find back the same results, then it is a bug (please post a reproduction script).\r\n"
      }
    ]
  },
  {
    "number": 1200,
    "title": "Working on multiple indexes",
    "created_at": "2020-05-06T12:26:49Z",
    "closed_at": "2020-05-29T05:13:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1200",
    "body": "# This is not an issue\r\n\r\n# Summary\r\n\r\nI want to work with multiple indexes, I want search a query in all of them at the same time, collect results and put them in order. All indexes will update always, so i dont want merge them. Does faiss support these cluster indexes ? I hope I told correctly what i want to tell.\r\n\r\n# Platform\r\n\r\nOS: Ubuntu 16.04\r\n\r\nFaiss version: 1.6.1\r\n\r\n\r\n\r\nRunning on:\r\n- [X ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ X] Python\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1200/comments",
    "author": "pasa13142",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2020-05-08T04:13:46Z",
        "body": "You can query them sequentially (or in parallel from multiple threads), and merge the results that you get back from each sub-index yourself."
      },
      {
        "user": "mdouze",
        "created_at": "2020-05-29T05:13:53Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 1199,
    "title": "Question Regarding How Faiss Search Neighbors",
    "created_at": "2020-05-04T16:28:58Z",
    "closed_at": "2020-05-05T20:54:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1199",
    "body": "Hi, I have some questions about how Faiss search for neighbors:\r\n\r\n1. For HNSW, why faiss allowed searching k > ef ?\r\n2. For IndexLSH, what is the searching algorithm? Return top k data in the bucket that the query data belong to? What if k > size(bucket that query data belongs to)?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1199/comments",
    "author": "IhaveAquestionHere",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-05-04T21:35:55Z",
        "body": "1. Why not? When there are not enough search results, the missing entries are set to -1.\r\n2. no. The IndexLSH just binarizes the input vector and does exhaustive search on the binary vectors (there are no buckets)."
      },
      {
        "user": "IhaveAquestionHere",
        "created_at": "2020-05-05T14:57:44Z",
        "body": "Thank you very much for your reply! For HNSW, what will happen when the query number k is larger than ef (the dynamic list of neighbors)?"
      },
      {
        "user": "mdouze",
        "created_at": "2020-05-05T20:48:23Z",
        "body": "hen there are not enough search results, the missing entries are set to -1"
      },
      {
        "user": "IhaveAquestionHere",
        "created_at": "2020-05-05T20:54:50Z",
        "body": "Thank you very much!"
      }
    ]
  },
  {
    "number": 1198,
    "title": "Enhancement: A combined API with range_search and search",
    "created_at": "2020-05-02T19:15:21Z",
    "closed_at": "2020-05-29T05:13:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1198",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\nEnhancement: A combined API with range_search and search\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [X] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [X] C++\r\n- [X] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\nWe're currently using Faiss as our vector similarity search engine. The service is supposed to be a very low latency service(less than 10 ms). Faiss performance is excellent. But our problem is we have 2 threshold with number of vector combine with radius, also we expected the result is ordered by score like search. Currently we used [search+post filter score] or [range_search+post filter k], both of the post filter costs 10ms, can we expect a API that combine search+range_search. If you're ok with the requirement, I can contribute to the code. Let me know if it's possible\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1198/comments",
    "author": "billyean",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-05-04T08:45:00Z",
        "body": "Thanks for the suggestion, but it looks like a specific use case. \r\nIt is not normal that the post filtering takes the same amount of time as the initial search. It is likely that it is easier to spend some effort optimizing that at first. \r\nWhat is the type of index and what are the orders of magnitude of k, number of index entries and number of returned results?"
      },
      {
        "user": "mdouze",
        "created_at": "2020-05-29T05:13:46Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 1186,
    "title": "Know the index type in C++",
    "created_at": "2020-04-17T10:04:11Z",
    "closed_at": "2020-05-29T05:12:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1186",
    "body": "Is there a way to get the index type from the index pointer?\r\nI'm trying to use `reconstruct` method and need to know whether I should call `make_direct_map` before that or not.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1186/comments",
    "author": "RafailFridman",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-04-20T12:08:28Z",
        "body": "You can do `dynamic_cast<IndexIVF*>(index)` or `ivlib::extract_index_ivf`"
      },
      {
        "user": "mdouze",
        "created_at": "2020-05-29T05:12:24Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 1178,
    "title": "How to set useFloat16LookupTables for GPU IVFPQ",
    "created_at": "2020-04-08T01:47:13Z",
    "closed_at": "2020-05-29T05:10:55Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1178",
    "body": "# Summary\r\nI tried to use 8 bits per code and 64 sub-quantizers on GPU IVFPQ index.\r\nIt results in the following error:\r\n`RuntimeError: Error in void faiss::gpu::GpuIndexIVFPQ::verifySettings_() const at gpu/GpuIndexIVFPQ.cu:431: Error: 'requiredSmemSize <= getMaxSharedMemPerBlock(device_)' failed: Device 0 has 49152 bytes of shared memory, while 8 bits per code and 64 sub-quantizers requires 65536 bytes. Consider useFloat16LookupTables and/or reduce parameters`.\r\n\r\nI worry about the performance degradation coming from reducing parameters.\r\nTherefore, I want to set useFloat16LookupTables to handle this issue.\r\nHowever, I could not find an example of how to set this option.\r\nCould you elaborate on this?\r\n\r\nThanks in advance!\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Ubuntu 18.04 (nvidia - pytorch docker) <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: 1.5.3 <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n```\r\ndimension = 1024\r\nncentroids = 4096\r\ncode_size = 64\r\nnprobe = 8\r\n\r\nquantizer = faiss.IndexFlatL2(dimension)\r\nindex = faiss.IndexIVFPQ(quantizer, dimension, ncentroids, code_size, 8)\r\nres = faiss.StandardGpuResources()\r\nindex = faiss.index_cpu_to_gpu(res, 0, index)\r\nindex.nprobe = nprobe\r\n```\r\n\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1178/comments",
    "author": "gyuwankim",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-04-09T09:01:22Z",
        "body": "```python\r\nco = faiss.GpuClonerOptions()\r\nco. useFloat16LookupTables = True\r\nindex = faiss.index_cpu_to_gpu(res, 0, index, co)\r\n```"
      },
      {
        "user": "ofirpress",
        "created_at": "2020-04-11T03:36:49Z",
        "body": "Hi @mdouze - I've encountered the same error, I added the code you wrote and I still get the error:\r\n\r\n> RuntimeError: Error in void faiss::gpu::GpuIndexIVFPQ::verifySettings_() const at gpu/GpuIndexIVFPQ.cu:431: Error: 'requiredSmemSize <= getMaxSharedMemPerBlock(device_)' failed: Device 0 has 49152 bytes of shared memory, while 8 bits per code and 64 sub-quantizers requires 65536 bytes. Consider useFloat16LookupTables and/or reduce parameters`"
      },
      {
        "user": "ofirpress",
        "created_at": "2020-04-25T21:20:21Z",
        "body": "I think I figured it out. In order to use useFloat16LookupTables the command is:\r\n`co.useFloat16 = True`\r\ninstead of\r\n`co.useFloat16LookupTables = True`"
      },
      {
        "user": "mdouze",
        "created_at": "2020-05-29T05:10:55Z",
        "body": "No activity, closing."
      },
      {
        "user": "anubhav562",
        "created_at": "2022-12-07T16:31:14Z",
        "body": "### Update\r\n\r\nIf someone else is facing the same problem while training on multiple GPUs, please see the below code snippet.\r\n\r\n```\r\ndimension = 768\r\nncentroids = 131072\r\ncode_size = 64\r\n\r\nquantizer = faiss.IndexFlatL2(dimension)\r\nindex = faiss.IndexIVFPQ(quantizer, dimension, ncentroids, code_size, 8)\r\n\r\nco = faiss.GpuMultipleClonerOptions()\r\nco.useFloat16 = True\r\nindex = faiss.index_cpu_to_all_gpus(index, co)\r\n\r\nindex.nprobe = nprobe\r\n```\r\n\r\n**Summary**\r\n\r\n- Employ co.useFloat16 instead of co.useFloat16LookupTables\r\n- For training on multiple GPUs, use faiss.GpuMultipleClonerOptions() instead of faiss.GpuClonerOptions()\r\n\r\nCheers!"
      }
    ]
  },
  {
    "number": 1175,
    "title": "Precision",
    "created_at": "2020-04-04T13:07:28Z",
    "closed_at": "2020-05-29T05:10:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1175",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\nHow can i calculate the precision?\r\n\r\n```\r\ndef evaluate(index, xq, gt, k):\r\n    nq = xq.shape[0]\r\n    t0 = time.time()\r\n    D, I = index.search(xq, k)  # noqa: E741\r\n    t1 = time.time()\r\n    recalls = {}\r\n    i = 1\r\n    while i <= k:\r\n        recalls[i] = (I[:, :i] == gt[:, :1]).sum() / float(nq)\r\n        i *= 10\r\n\r\n    return (t1 - t0) * 1000.0 / nq, recalls\r\n\r\nevaluate(flat_index, xq, gt, 10000)\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1175/comments",
    "author": "shainaraza",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2020-04-06T23:44:09Z",
        "body": "You're using a flat index? That is not approximate nearest neighbor search, it is exact search, so recall is expected to be 1."
      },
      {
        "user": "shainaraza",
        "created_at": "2020-04-07T10:35:17Z",
        "body": "yes flat index, thaks"
      },
      {
        "user": "mdouze",
        "created_at": "2020-05-29T05:10:44Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 1158,
    "title": "Fix merge_into usage mistake",
    "created_at": "2020-03-23T17:50:02Z",
    "closed_at": "2020-04-08T19:45:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1158",
    "body": "# Summary\r\nDear all,\r\n\r\nI've given \"False\" to the third parameter of \"faiss.merge_into\" by mistake (I've just figured out I should have used \"True\", to properly shift feature IDs of the second index).\r\nAs a consequence, I have features mapping to the same ID by mistake.\r\n\r\nBesides rebuilding the original indices and merging them again, this time using the correct parameter values, is there any other way to fix/update the IDs within the merged index, with no index rebuilding? Let's say, shifting the IDs of all features that previously belonged to the second original index, within the new merged one? Is this possible? Is there a data structure within your data model that stores the order/time of the added indices due to merging?\r\n\r\nPlease let me know if you need any clarification.\r\n\r\nThanks in advance for your attention and for sharing FAISS with the community,\r\nDaniel.\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1158/comments",
    "author": "danielmoreira",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-03-25T08:20:41Z",
        "body": "That is hard.... because you need to be able to differentiate the ids from several adds. \r\nThere is no metadata that can be used.\r\nWhat about re-running the whole process? "
      },
      {
        "user": "danielmoreira",
        "created_at": "2020-04-08T19:45:35Z",
        "body": "Interesting, thanks for your answer.\r\nThat's what I had to do, in the end (re-running the whole process).\r\nBest, Daniel."
      }
    ]
  },
  {
    "number": 1147,
    "title": "Does faiss provide operation to find target vectors which are within L hamming distance to a query vector?",
    "created_at": "2020-03-19T04:25:31Z",
    "closed_at": "2020-04-01T12:48:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1147",
    "body": "I was wondering whether faiss provides any class or function which can find all vectors among a set of candidate vectors which are within L hamming distance to a query vector?\r\n\r\nI understand IndexBinaryFlat can be used to get the K nearest vectors for a query vector in terms of Hamming distance, but it does not seem to support the kind of query descrived above.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1147/comments",
    "author": "xwk",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-03-22T21:44:33Z",
        "body": "Epsilon-search for binary indexes will be supported in 1.6.3 in the coming weeks."
      },
      {
        "user": "mdouze",
        "created_at": "2020-04-01T12:48:53Z",
        "body": "supported in 1.6.3. Closing."
      }
    ]
  },
  {
    "number": 1138,
    "title": "Are indexes machine independent??",
    "created_at": "2020-03-11T13:08:09Z",
    "closed_at": "2020-04-01T12:48:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1138",
    "body": "May I know once the indexes are created and have been saved to a file using write_index method,can the same index file be moved to an other machine and loaded using read_index,and would it be able to produce same results??\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\nThanks in advance!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1138/comments",
    "author": "MaheshChandrra",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-03-12T22:18:38Z",
        "body": "Yes, the only constraint is that the machine should have a little-endian architecture. \r\nNote that sometimes the index format is extended. Reading old indexes with a new version is supported. Not the way round."
      },
      {
        "user": "mdouze",
        "created_at": "2020-04-01T12:48:37Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1135,
    "title": "Does it accept \"similarity\" scores?",
    "created_at": "2020-03-09T18:38:21Z",
    "closed_at": "2020-03-10T17:14:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1135",
    "body": "For some technical reasons, I do not have any vector representation for my input instances, but I have similarity values between any pair of them. Wondering if it is possible to input similarity values directly (rather than sets of vectors). \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1135/comments",
    "author": "danyaljj",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-03-10T14:19:45Z",
        "body": "It would not make much sense for Faiss to support that because given the distance matrix, it is easy to just get to top-k similarities for each row."
      }
    ]
  },
  {
    "number": 1133,
    "title": "C API Features",
    "created_at": "2020-03-09T02:43:58Z",
    "closed_at": "2020-03-09T08:53:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1133",
    "body": "Hi, I recently tried out the C API. It's really well-designed and easy to follow. I was wondering if there are plans to add more features. A few that seem to be missing are:\r\n\r\n- Binary indexes (`IndexBinaryFlat`, `IndexBinaryIVF`, `index_binary_factory`)\r\n- `IndexHNSWFlat`\r\n- `IndexPQ`, `IndexIVFPQ`, and `IndexIVFPQR`\r\n- `PCAMatrix`\r\n- `ProductQuantizer`",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1133/comments",
    "author": "ankane",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-03-09T06:48:00Z",
        "body": "The C API is not maintained by Facebook. \r\nMaybe @Enet4 could reply to these questions? "
      },
      {
        "user": "Enet4",
        "created_at": "2020-03-09T07:57:03Z",
        "body": "Thank you for reaching out. Originally, I implemented this API in order to have a better integration with my own data processing tools in my PhD. While I intend to maintain this API and the faiss-rs library for as long as I can, I am no longer using it with the same regularity, and there are no plans on my end to add the missing pieces.\r\n\r\nWith that said, if there is something you'd like to become available via this API, I would be glad to provide some guidance."
      },
      {
        "user": "ankane",
        "created_at": "2020-03-09T08:53:13Z",
        "body": "Thank you both for the response and context. It sounds like it's better to use the C++ API when possible, so will continue with that for the Ruby library."
      }
    ]
  },
  {
    "number": 1114,
    "title": "which paper describe the algorithm used in IndexLSH?",
    "created_at": "2020-02-26T10:04:30Z",
    "closed_at": "2020-02-26T10:42:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1114",
    "body": "",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1114/comments",
    "author": "dongzhen123",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-02-26T10:29:23Z",
        "body": "The closest description is \"Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval\", by Gong et al , PAMI 2012. \r\nThe default IndexLSH just does a random rotation instead of ITQ. Use \"Pad128,ITQ128,LSH\" to get ITQ."
      }
    ]
  },
  {
    "number": 1113,
    "title": "reconstruct, indexFlatL2, IVFFlat, IVFPQ, different results",
    "created_at": "2020-02-25T06:20:33Z",
    "closed_at": "2020-03-03T03:59:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1113",
    "body": "Hi, I use reconstruct() to get a vector from index, I get the right answer with indexFlatL2 and indexIVFFlat, but with indexIVFPQ, the vector I got is not the same as before when I put same index. How could I use reconstruct() for indexIVFPQ?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1113/comments",
    "author": "JiweiZh",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2020-02-25T21:49:04Z",
        "body": "IndexIVFPQ uses product quantization to lossily compress the vector, so reconstruct() is returning the quantized representation of the vector (it is \"reconstructed\" from the data present in the index). The vector you added appears to the index in the form that reconstruct() returns.\r\n\r\nIVFFlat and Flat store vector data without compression, so it makes sense that those are returning the vector that you had originally."
      },
      {
        "user": "yuyifan1991",
        "created_at": "2020-12-09T10:42:28Z",
        "body": "> Hi, I use reconstruct() to get a vector from index, I get the right answer with indexFlatL2 and indexIVFFlat, but with indexIVFPQ, the vector I got is not the same as before when I put same index. How could I use reconstruct() for indexIVFPQ?\r\n\r\nhi,Buddy. I have the problem when I use reconstruct(). Index already created by indexIVFFlat that has the hash ids. The error is :+1: RuntimeError: Error in virtual void faiss::IndexIVF::reconstruct(faiss::Index::idx_t, float*) const at IndexIVF.cpp:191: Error: 'direct_map.size() == ntotal' failed: direct map is not initialized"
      }
    ]
  },
  {
    "number": 1107,
    "title": "Explanation of IVF65536_HNSW32,PQ64 index structure",
    "created_at": "2020-02-16T12:53:31Z",
    "closed_at": "2020-02-21T09:15:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1107",
    "body": "# Summary\r\n\r\nDo I correctly understand, that with this index we firstly find $nprobe nearest InvertedFile clusters using HNSW to speed up the search process and then we compute distances to all vectors in found clusters using ProductQuantization code books, ordering results by distances and return it to user?\r\n\r\nIf I want to use HNSW at the second level (inside IVF clusters), which index structure should I use?\r\n\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1107/comments",
    "author": "sgjurano",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-02-20T10:11:24Z",
        "body": "The explanation is correct. \r\nUsing HNSW inside the IVF clusters is not implemented. It is likely that it would be less efficient than using a single HNSW on all the vectors to index."
      },
      {
        "user": "sgjurano",
        "created_at": "2020-02-21T09:15:45Z",
        "body": "Thank you for the answer."
      }
    ]
  },
  {
    "number": 1078,
    "title": "[Question ] How do we compare with mih?",
    "created_at": "2020-01-09T23:35:50Z",
    "closed_at": "2020-02-20T10:27:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1078",
    "body": "Recently we've been on the hunt for the best exact nn search algorithms. So far the big ones we wanna benchmark are y'all and MIH\r\n\r\n1. What other ones would you like us to benchmark?\r\n2. What's the best source for production code nn search benchmarks?\r\n3. Have y'all already benchmarked vs MIH?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1078/comments",
    "author": "posix4e",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2020-01-10T02:23:40Z",
        "body": "You mean binary vector exact search (e.g., binary multi-index hashing is what you mean by MIH)?\r\n\r\nI guess Faiss binary exact search (IndexBinaryFlat, GpuIndexBinaryFlat) is different than MIH, and has slightly different use cases.\r\n\r\nMIH requires a search radius and the cost of the query is dependent upon that while Faiss, which is completely brute force, does not. Faiss will of course traverse a lot more memory when doing a query (it really is brute-force) and be way slower, though it won't duplicate IDs in different buckets, the storage is just the cost of the bits and the IDs.\r\n\r\nFaiss is primarily oriented around batch queries (e.g., query the k=50 nearest neighbors for 10,000 vectors against a billion vectors in a single go) rather than query sizes of 1; e.g., for computing k-NN graphs and such things. MIH might with high probability end up touching all of the buckets when doing such a batch query and could very well be slower for large batches.\r\n\r\nFor a batch size of 1 MIH would always win I would think except in extremely small datasets.\r\n\r\nI would be curious to compare MIH against brute-force IndexBinaryFlat / GpuIndexBinaryFlat as a function of batch size (which I guess without more clever implementations would involve calling MIH query iteratively for each query in the batch) to see if there is a crossover point where batch brute-force is faster.\r\n"
      },
      {
        "user": "posix4e",
        "created_at": "2020-01-10T02:39:06Z",
        "body": "Yep binary multi-index hashing is what you mean by MIH)"
      },
      {
        "user": "mdouze",
        "created_at": "2020-01-13T13:22:10Z",
        "body": "The index most similar to multi-index hashing is `IndexBinaryIVF`.  We have not compared it with multi-index hashing because we focus on memory-efficient indexing and MIH tends to require a lot of indexes to handle large search radiuses. \r\n\r\nIt would be interesting to do some comparions though, feel free to post any findings you have here!\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2020-02-20T10:27:42Z",
        "body": "No activtiy, closing."
      }
    ]
  },
  {
    "number": 1072,
    "title": "Single Server | GpuIndexFlatL2 Write Strategy",
    "created_at": "2019-12-30T09:50:25Z",
    "closed_at": "2020-01-02T10:29:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1072",
    "body": "Running on:\r\n- [ ] CPU\r\n- [yes] GPU\r\n\r\nInterface: \r\n- [ yes] C++\r\n- [ ] Python\r\n\r\nAbout my app:\r\n1. Multi threaded http server based application\r\n2. Accepts id and vector for /add request\r\n3. Provide GpuIndexFlatL2 search functionality\r\n\r\nHowever all the adding and search is happending in memory and if the application closes or crashes the data is lost. My question is since faiss supports writing index via:\r\n\r\n```\r\n    const char *name = \"index.bin\";\r\n    faiss::write_index(faiss::gpu::index_gpu_to_cpu(index), name);\r\n```\r\n\r\nhow do i implement the most efficient index saving strategy ?\r\n\r\n1. Block all requests while index is being written to file for every new vector added\r\nThis will lead to decrease in performance.\r\n\r\n2. Periodically update the index in the background after every 10,000 new vectors\r\nif application crashes unwritten new vectors will be lost\r\n\r\n3. Other strategy ?\r\n\r\nPlease help me. I have been scratching my head for the last 2 weeks regarding this problem.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1072/comments",
    "author": "dexception",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-12-31T06:57:12Z",
        "body": "It really depends on the operating conditions. \r\nOne approach is with two indexes: one big one with most of the vectors, and one in which you add new vectors. At search time, you search in both.\r\n\r\nThen you can save every 10k adds with: \r\n\r\n1. save small index with (fast) with incremental file names\r\n\r\n2. merge small index into big one (fast, in RAM)\r\n\r\n3. clear small index.\r\n\r\nAt recover time, you then need to load the small indexes to reconstruct the big one. You could have a background job that merges the small indexes on disk.\r\n"
      },
      {
        "user": "dexception",
        "created_at": "2019-12-31T07:07:08Z",
        "body": "Thanks i think this would work without data loss in case of failure. \r\n\r\nOther question is how do you handle meta data for the vectors because when the results for distance search are achieved that might not be revelant. For example,\r\n\r\nIn our application we have clientId,categoryId for each vector and other attributes as well. So when the topK results are returned that might not be for that clientID. Is there an Index that suports adding attributes for vectors inside the index as well ?"
      },
      {
        "user": "mdouze",
        "created_at": "2020-01-02T10:26:50Z",
        "body": "No, you need to put metadata in a separate conventional table. \r\nRationale in #641\r\n"
      },
      {
        "user": "dexception",
        "created_at": "2020-01-02T10:29:51Z",
        "body": "Resolved."
      }
    ]
  },
  {
    "number": 1069,
    "title": "Any plan on python wrapper for faiss::InvertedLists::prefetch_lists",
    "created_at": "2019-12-25T13:33:17Z",
    "closed_at": "2020-01-10T07:54:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1069",
    "body": "# Summary\r\ni guess **python** can not call **faiss::InvertedLists::prefetch_lists** for now.\r\nare there any plans on adding it?\r\n\r\n# example\r\ncode:\r\n```\r\ninvlists = faiss.OnDiskInvertedLists(100, 256, \"merged_index.ivfdata\")\r\npf = np.array(range(10)).astype('int')\r\ninvlists.prefetch_lists(pf, 10)\r\n```\r\n\r\nresult:\r\n```\r\nreturn _swigfaiss.OnDiskInvertedLists_prefetch_lists(self, list_nos, nlist)\r\nTypeError: in method 'OnDiskInvertedLists_prefetch_lists', argument 2 of type 'faiss::InvertedLists::idx_t const *'\r\n```\r\n\r\n# Platform\r\n\r\nOS: macOS .\r\n\r\nRunning on:\r\n- CPU\r\n\r\nInterface: \r\n- Python\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1069/comments",
    "author": "Prymon",
    "comments": [
      {
        "user": "Prymon",
        "created_at": "2019-12-25T13:35:46Z",
        "body": "i m using trick below \r\n```\r\nindex.nprob = index.nlist\r\nindex.search(np.random.random(1,128), 1)\r\nindex.nprob = 1\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2019-12-31T06:45:54Z",
        "body": "Yes python can call it. However you have to use the low-level wrapper. \r\n```\r\ninvlists.prefetch_lists(faiss.swig_ptr(pf), 10)\r\n```"
      },
      {
        "user": "Prymon",
        "created_at": "2020-01-10T07:54:30Z",
        "body": "thanks a lot !"
      }
    ]
  },
  {
    "number": 1063,
    "title": "[Question] Searching for LEAST similar vector",
    "created_at": "2019-12-18T14:38:04Z",
    "closed_at": "2020-01-20T14:11:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1063",
    "body": "Is there a way of doing a search to find the LEAST similar instead of the most similar results?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1063/comments",
    "author": "leomrocha",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-12-18T21:22:29Z",
        "body": "If all your data lies on the surface of a hypersphere, then you could multiply your query vector by -1."
      },
      {
        "user": "mdouze",
        "created_at": "2020-01-06T14:34:50Z",
        "body": "no acivity, closing."
      }
    ]
  },
  {
    "number": 1062,
    "title": "[Question]Adding/Updating vectors in IndexIVFFlat index",
    "created_at": "2019-12-12T11:54:01Z",
    "closed_at": "2019-12-31T06:37:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1062",
    "body": "# Summary\r\n\r\nI have a question on adding/updating vectors in IndexIVFFlat index.\r\n\r\nupdate_vectors method is the only method for updating vectors in faiss index. It works only if maintain_direct_map=true.\r\n\r\nHowever, if maintain_direct_map=true, add_with_ids method throws an error. \r\n\r\nDo I understand right, that:\r\n\r\n- if maintain_direct_map=true, update_vectors method should be used for adding vectors to index\r\n \r\n- if maintain_direct_map=false, add_with_ids method should be used for updating vectors to index\r\n?\r\nAlso, what does maintain_direct_map flag means?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1062/comments",
    "author": "yana2301",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-12-13T09:17:54Z",
        "body": "maintain_direct_map means there is a mapping from ids to where they are stored in the index. \r\nSince it is an array, the ids should be sequential, so add_with_ids is not supported. "
      },
      {
        "user": "mdouze",
        "created_at": "2019-12-31T06:37:14Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 1040,
    "title": "Why IVFSQ8 is faster than IVFFlat in CPU mode?",
    "created_at": "2019-11-25T11:03:27Z",
    "closed_at": "2019-11-30T08:22:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1040",
    "body": "Hi,\r\nI'm from Milvus team.\r\nWhen I do FAISS performance testing, it confused me that IVFSQ8 is 2~3 times faster than IVFFLAT in CPU mode.\r\nFrom the source code, we can see that there is no difference between IVFFlat and IVFSQ8 distance computation except \"reconstruct_8_components()\" is called in IVFSQ8. It will be well-understood if IVFSQ8 is slower, since IVFSQ8 do more computation work.\r\nBut why we see IVFSQ8 is much faster ? \r\n\r\nDataset: sift-1b (1 billion, dimension 128)\r\nSearch mode: CPU\r\nnlist: 16384\r\nnprobe: 32\r\nnq: 1000\r\ntopk: 100\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1040/comments",
    "author": "cydrain",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-11-25T18:28:36Z",
        "body": "I would imagine this is due to 1/4x the required memory bandwidth for SQ8, most likely. IVFFlat has to load 4 bytes per vector dimension, while SQ8 loads 1 byte. The added computation is a non-issue because the original computation is likely memory bandwidth bound."
      }
    ]
  },
  {
    "number": 1036,
    "title": "To compare with any other similarly search",
    "created_at": "2019-11-22T03:45:29Z",
    "closed_at": "2019-11-25T05:20:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1036",
    "body": "# Summary\r\n\r\nThe result of using euclidean distance to do similarity search that come from any other and Faiss.\r\nIn few data the result in index is most the same however distance is different .  In a little more data the result whatever index or distance are very different.\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: ubuntu 18.4\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1036/comments",
    "author": "SGA-cai-sanru",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-11-22T04:39:56Z",
        "body": "The distances reported for L2 are distances squared. e.g.,\r\n\r\n0.7826493382453918^2 = 0.6125..."
      },
      {
        "user": "wickedfoo",
        "created_at": "2019-11-22T04:44:23Z",
        "body": "Also you are comparing two different approximate k-NN implementations (I don't know what algorithm AnnoyIndex is using), so there is no guarantee that they would return the same thing as they are both approximate. \r\n\r\nIn fact, the approximate Faiss index becomes an inefficient implementation of IndexFlat (an exact, brute force search) when nprobe == nlist, which is what you seem to be using."
      },
      {
        "user": "SGA-cai-sanru",
        "created_at": "2019-11-25T05:20:23Z",
        "body": "I have a solution for this issue. Thanks!!"
      }
    ]
  },
  {
    "number": 1034,
    "title": "Different Search Results for Different Versions of Faiss",
    "created_at": "2019-11-21T12:50:51Z",
    "closed_at": "2020-04-24T07:48:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1034",
    "body": "# Summary\r\n\r\nI have an index of type IndexHSNWFlat, and I've tried doing a search on it with two Faiss versions (1.4.0 and 1.6.0). I found that they returned slightly different results - in particular, for version 1.4.0, the search was exhaustive, while it wasn't for 1.6.0, resulting in a significant number of -1 returns. For the indices that weren't -1, there was a significant overlap between the two search results, but they were not identical. I've tried on multiple different sizes (1k, 10k, 50k) and have gotten the same behavior as shown above. I haven't been able to see why this change in behavior is happening and I was hoping someone could help explain it to me.\r\n\r\nEdit: Just as an additional note, this test was run on Ubuntu, and Faiss version 1.6.0 was loaded with AVX2 support.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1034/comments",
    "author": "benjamin-pikus-kw",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-11-22T04:46:41Z",
        "body": "I don't think the HNSW code changes substantially between the two versions. \r\nHowever, in multithreaded mode, the `IndexHNSWFlat` add function is not deterministic.\r\nTo debug this, you could either disable multithreading with `faiss.omp_set_num_threads(1)` or store an 1.4.0 index and load it in 1.6.0 to see if it is a difference with the search or add functions.\r\n"
      },
      {
        "user": "benjamin-pikus-kw",
        "created_at": "2019-12-10T18:12:44Z",
        "body": "Thanks for the answer! I tried setting the number of threads to 1, and it is now exhaustive for both indexes. However, they don't match up exactly (out of an index of size 50,000, there were 1647 mismatches between the two versions). From a quick inspection this seems to be mostly due to small pairwise flips (ex: in version 1.4 the order is 1,2,3,4,5 whereas in 1.6 it would be 1,2,4,3,5). Looking at the distances, it looks like there are small differences that are causing this (the same sample would have a distance of 0.8126916 in one version and 0.81269133 in another). Will inspect further, but is there an obvious explanation of why this could be occurring?"
      },
      {
        "user": "mdouze",
        "created_at": "2020-04-01T07:59:21Z",
        "body": "This is very weird. Distance computations in HSNW should be deterministic. "
      },
      {
        "user": "mdouze",
        "created_at": "2020-04-24T07:48:40Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 981,
    "title": "GPU indexivfpq question",
    "created_at": "2019-10-09T08:33:58Z",
    "closed_at": "2019-10-30T06:36:50Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/981",
    "body": "# Summary\r\nHi. \r\nWhy don't provide  GpuIndexPQ with gpu. if we want to use a PQ algorithm with gpu, how should we do.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/981/comments",
    "author": "isThatYue",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-10-09T19:37:37Z",
        "body": "`GpuIndexIVFPQ` with a single centroid would be a good approximation of that. "
      },
      {
        "user": "mdouze",
        "created_at": "2019-10-30T06:36:50Z",
        "body": "no activity, closing. "
      }
    ]
  },
  {
    "number": 979,
    "title": "How can I get the samples number of each centroid in python?",
    "created_at": "2019-10-09T03:35:07Z",
    "closed_at": "2019-10-30T06:36:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/979",
    "body": "I have trained an index as below setting, and added 3.5 million data in the index.\r\nd = 1x4x4x1024\r\nquantizer = faiss.index_factory(d,'PCA4978,IVF32768_HNSW64,SQ8')\r\n\r\nBut when I search a query, time is very long, about 30s. I think maybe a lot of samples were clustered into one centroid, so I want to know the samples number of each centroid.\r\nAnd is there other reasons result in  the very long search time?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/979/comments",
    "author": "fengsky401",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-10-09T19:36:30Z",
        "body": "A good thing to check is indeed whether the lists are balanced. You can call\r\n\r\nfaiss.extract_index_ivf(quantizer).display()\r\n\r\nto display some stats, or \r\n\r\nil = faiss.extract_index_ivf(quantizer).invlists\r\nlist_sizes = [il.list_size(i) for i in range(il.nlist)]\r\n\r\nto get all the list sizes"
      },
      {
        "user": "fengsky401",
        "created_at": "2019-10-10T10:57:28Z",
        "body": "> A good thing to check is indeed whether the lists are balanced. You can call\r\n> \r\n> faiss.extract_index_ivf(quantizer).display()\r\n> \r\n> to display some stats, or\r\n> \r\n> il = faiss.extract_index_ivf(quantizer).invlists\r\n> list_sizes = [il.list_size(i) for i in range(il.nlist)]\r\n> \r\n> to get all the list sizes\r\n\r\nThank you!\r\nThe second method:\r\nil = faiss.extract_index_ivf(quantizer).invlists\r\nlist_sizes = [il.list_size(i) for i in range(il.nlist)]\r\n\r\n is worked, searching a query feature in 4 million data used 66 ms, is it normal?\r\nThe index set is as below:\r\nd = 1x4x4x1024\r\nquantizer = faiss.index_factory(d,'PCA4978,IVF32768_HNSW64,SQ8')\r\n\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "fengsky401",
        "created_at": "2019-10-10T10:58:18Z",
        "body": "> A good thing to check is indeed whether the lists are balanced. You can call\r\n> \r\n> faiss.extract_index_ivf(quantizer).display()\r\n> \r\n> to display some stats, or\r\n> \r\n> il = faiss.extract_index_ivf(quantizer).invlists\r\n> list_sizes = [il.list_size(i) for i in range(il.nlist)]\r\n> \r\n> to get all the list sizes\r\n\r\nThe first method reported error:\r\nFile \"test_faiss6.py\", line 21, in <module>\r\n    faiss.extract_index_ivf(global_faiss_quantizer).display()\r\n  File \"/data/anaconda3/envs/queenie_python3/lib/python3.6/site-packages/faiss/swigfaiss_avx2.py\", line 3206, in <lambda>\r\n    __getattr__ = lambda self, name: _swig_getattr(self, IndexIVF, name)\r\n  File \"/data/anaconda3/envs/queenie_python3/lib/python3.6/site-packages/faiss/swigfaiss_avx2.py\", line 80, in _swig_getattr\r\n    raise AttributeError(\"'%s' object has no attribute '%s'\" % (class_type.__name__, name))\r\nAttributeError: 'IndexIVF' object has no attribute 'display'\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2019-10-14T11:22:07Z",
        "body": "Sorry, I meant: \r\nfaiss.extract_index_ivf(quantizer).print_stats ()"
      },
      {
        "user": "mdouze",
        "created_at": "2019-10-30T06:36:42Z",
        "body": "no activity, closing. "
      }
    ]
  },
  {
    "number": 973,
    "title": "Will use MKLML instead of full MKL impact the performance?",
    "created_at": "2019-10-01T22:28:44Z",
    "closed_at": "2019-10-30T06:35:32Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/973",
    "body": "Hello,\r\n\r\nThis is more a general question. I'm trying to use MKLML + LAPACK instead of the full MKL to reduce some runtime artifact size footprint. I'm wondering if FAISS would need the full MKL for best performance? \r\n\r\nThanks,\r\nPeter",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/973/comments",
    "author": "chenliu0831",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-10-02T15:36:23Z",
        "body": "I don't know about MKLML, but since it is based on a subset of MKL it should be fine performance wise. Maybe some comparative test would be useful, `benchs/bench_hnsw.py` (without argments) has a good coverage of Faiss operations and should give usable timinings."
      },
      {
        "user": "mdouze",
        "created_at": "2019-10-30T06:35:32Z",
        "body": "no activity, closing. "
      }
    ]
  },
  {
    "number": 955,
    "title": "Dockerfile request for benchmarking faiss-cpu",
    "created_at": "2019-09-19T20:46:43Z",
    "closed_at": "2019-09-24T22:14:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/955",
    "body": "# Summary\r\n\r\nMy team is investigating unexpected performance issue with `FLATIP`, lower dimension searching is significantly slower than higher dimension (d=1792 vs d=2025). Before we issue this with reproducible code, we want to ensure if it's not a platform or compile-related problem. (We also found that there's performance difference between different source distributions with same release version)\r\n\r\nAs faiss provides `Dockerfile` which targets gpu machines (base image is `nvidia/cuda-8.0`), can we get a official Dockerfile for benchmarking faiss-cpu?\r\n\r\nThanks!\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\nRunning on:\r\n- CPU\r\n\r\nInterface: \r\n- Python",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/955/comments",
    "author": "flrngel",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-09-20T01:18:31Z",
        "body": "What is the exact size of your problem (number of vectors in the IndexFlatIP index, and number of query vectors)?\r\n\r\nCould it be the case that the N x 2025 case no longer fits into last level cache, and performance falls off a cliff? Have you tried interpolating between 1792 and 2025 to see if there is a performance cliff?\r\n\r\n"
      },
      {
        "user": "flrngel",
        "created_at": "2019-09-20T14:18:14Z",
        "body": "HI @wickedfoo , here's our benchmark table. We didn't interpolate between 1792-2025 yet.\r\n\r\nNumber of query vector is 1. We tested 1,000 times.\r\n\r\n| DB Size | Query Iteration | Faiss Version | AWS Instance Type | Memory Usage | d=1792 | d=2025 | \r\n|---------|--------------|---------------|---------------|-------------|---------------------------|--------------------------| \r\n| 1M      | 1000         | v1.5.2        | r5.xlarge     | 14.6GB      | 0.647824494               | 0.342724457              | \r\n| 3M      | 1000         | v1.5.2        | r5.4xlarge    | 43.5GB      | 1.716049964               | 0.542412717              | \r\n| 5M      | 1000         | v1.5.2        | x1e.2xlarge   | 72.3GB      | 5.043245749               | 1.576336837              | \r\n| 7M      | 1000         | v1.5.2        | x1e.2xlarge   | 101.GB      | 7.026452808               | 2.448474893              | \r\n\r\n**Note:** \"Memory Usage\" is the result after running both `d=1702` and `d=2025` in one code."
      },
      {
        "user": "flrngel",
        "created_at": "2019-09-24T22:14:36Z",
        "body": "This is same to #918 and since benchmarks are not related to Dockerfile, I'm closing this issue and will open new issue about benchmarking."
      },
      {
        "user": "flrngel",
        "created_at": "2019-09-25T01:57:29Z",
        "body": "@wickedfoo After projection, I found that there's two way of searching, BLAS/SSE(when dimension is multiple of 4). I'm not sure when difference of SSE and BLAS, but I've got the entry point where should I look into. Thanks!"
      }
    ]
  },
  {
    "number": 946,
    "title": "Official anaconda version of faiss-cpu(1.5.3) is much slower than the pypi prebuilt faiss(1.5.3)",
    "created_at": "2019-09-12T01:45:51Z",
    "closed_at": "2019-11-07T14:20:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/946",
    "body": "# Summary\r\n\r\nI first installed the prebuilt faiss(1.5.3) use pip and found that the pypi prebuilt faiss is faster than the official anaconda version(especially during IndexPQ search stageï¼Œ3 times faster) on my PC (Intel i7-8700 CPU) with the following test script:\r\n```Python\r\nimport faiss\r\nimport numpy as np\r\n\r\ndef run_test(index: faiss.Index):\r\n    # random data\r\n    x = np.empty([n, d], dtype=np.float32)\r\n    faiss.float_rand(faiss.swig_ptr(x), n * d, 1)\r\n\r\n    index_name = index.__class__.__name__\r\n\r\n    # train\r\n    print(\"%s: indexing...\" % index_name)\r\n    start = faiss.getmillisecs()\r\n    index.train(x)\r\n    block_size = 10 * 10000\r\n    for i in range(n // block_size):\r\n        print(\"%s: adding block %d\" % (index_name, i))\r\n        i0, i1 = i * block_size, min((i + 1) * block_size, n)\r\n        index.add(x[i0:i1, :])\r\n    end = faiss.getmillisecs()\r\n    print(\"%s: build time %fms\" % (index_name, end - start))\r\n\r\n    # random query\r\n    query = np.empty([nq, d], dtype=np.float32)\r\n    faiss.float_rand(faiss.swig_ptr(query), nq * d, 2)\r\n    res = None  \r\n\r\n    # search\r\n    print(\"%s: searching...\" % index_name)\r\n    start = faiss.getmillisecs()\r\n    for i in range(n_searches):\r\n        res = index.search(query, k)\r\n    end = faiss.getmillisecs()\r\n    print(\"%s: mean search time %fms\" % (index_name, (end - start) / n_searches))\r\n\r\n    if res is not None:\r\n        print(\"Neighbor ids:\", res[1])\r\n        print(\"Distances:   \", res[0])\r\n\r\nif name == 'main':\r\n    n = 100 * 10000\r\n    d = 200\r\n    nq = 1\r\n    k = 10\r\n    M = 1\r\n    n_bits = 12\r\n    n_searches = 100\r\n\r\n    index_flat = faiss.IndexFlatL2(d)\r\n    index_pq = faiss.IndexPQ(d, M, n_bits)\r\n    index_pq.pq.verbose = True\r\n    index_pq.pq.cp.niter = 1\r\n\r\n    run_test(index_flat)\r\n    run_test(index_pq)\r\n```\r\nIs the BLAS version matter? Hope for your test and answer.\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Ubuntu16.04 (WSL)\r\n\r\nFaiss version: 1.5.3\r\n\r\nFaiss compilation options: official version\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\nPython version: 3.7",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/946/comments",
    "author": "Boreaso",
    "comments": [
      {
        "user": "Boreaso",
        "created_at": "2019-09-12T08:01:58Z",
        "body": "I debugged the c++ code and found that `pq_estimators_from_tables_generic` is the most time-consuming function with the parameter M=12, but this function does not use BLAS and SIMD optimised functions (when M=8 or 16, the performance is almost the same). What accounts for the performance difference?"
      },
      {
        "user": "Boreaso",
        "created_at": "2019-09-20T02:23:38Z",
        "body": "@beauby @mdouze Is there any solutionï¼Ÿ"
      },
      {
        "user": "beauby",
        "created_at": "2019-11-07T14:20:12Z",
        "body": "@Boreaso The codepath is optimized only when `M` is a multiple of `8`, so it is a tradeoff between memory and perfs. The (unofficial) pypi 1.5.3 likely is not the exact same version, as previous behavior was to implicitly round `M` to the next multiple of `8`."
      }
    ]
  },
  {
    "number": 926,
    "title": "On disk searching seems to use more memory than it should",
    "created_at": "2019-08-26T16:33:16Z",
    "closed_at": "2019-09-06T20:36:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/926",
    "body": "# Summary\r\n\r\nThe on-disk demo does not seem to actually be on disk. When I run stage 6, it's taking up about 550M of memory, which is roughly the same as just storing the vectors directly in memory.\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Linux\r\nFaiss version: 656368b5eda4d376177a3355673d217fa95000b6\r\nFaiss compilation options: no-cuda\r\nRunning on CPU\r\n\r\nInterface: Python\r\n\r\n# Reproduction instructions\r\nRun the ondisk demo stage 6",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/926/comments",
    "author": "triclops200",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-08-27T08:06:16Z",
        "body": "Is this RSS memory or VSZ memory (see the top command)?"
      },
      {
        "user": "triclops200",
        "created_at": "2019-08-27T14:00:05Z",
        "body": "RES memory. Virt is 4.5 GB"
      },
      {
        "user": "mdouze",
        "created_at": "2019-08-28T14:03:56Z",
        "body": "The data is on disk. What you are seeing is probably just overhead, multiplying the dataset size by 10 or 100 should clarify that."
      },
      {
        "user": "triclops200",
        "created_at": "2019-08-28T14:38:28Z",
        "body": "I multiplied the data by 10 in size and the amount of resident memory also increased by 10x. I think it's caching the file inbetween queries. Are you using an unreleased mmap for the index by any chance?"
      },
      {
        "user": "triclops200",
        "created_at": "2019-09-03T18:03:19Z",
        "body": "Well, I did some more testing with another program, and, if the index is being opened with a mmap with MAP_SHARED enabled, I'm willing to close this ticket as the RES being reported is kind-of wrong due to how linux reports mem-mapped files."
      },
      {
        "user": "mdouze",
        "created_at": "2019-09-04T07:40:49Z",
        "body": "Usually the reported amount of resident mem is reasonable on Linux. \r\nI guess the real check is to mmap an index that does not fit in RAM."
      },
      {
        "user": "triclops200",
        "created_at": "2019-09-06T20:36:30Z",
        "body": "First off, results:\r\n  larger than memory did search correctly, so it's not a bug and I can close the issue.\r\n\r\nSecondly, explaination:\r\n  Linux reports mmapped in memory pages as RES for the program, but it does not count as allocated memory at the system level. So, instead of OOMing, it will start depaging some of the file and keep the RES constant. The system reports a large cache size, though, but this isn't a problem for most usages. What this does mean is that you'll start hitting a performance cliff when the index can't fit in memory, but that's to be expected."
      }
    ]
  },
  {
    "number": 916,
    "title": "Very high matrice dimentionality makes crash the machine when indexing",
    "created_at": "2019-08-16T09:53:10Z",
    "closed_at": "2019-08-23T12:56:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/916",
    "body": "# Platform\r\n\r\nOS: Linux Ubuntu 19.04\r\nConfig machine:\r\n- 8 GPU V100\r\n- CPU 96 cores\r\n- 614 Go RAM\r\n\r\nFaiss version: 1.5.3 (commit 656368b5eda4d376177a3355673d217fa95000b6)\r\nFaiss compilation options: `./configure --with-cuda=/usr/local/cuda-10.0 --prefix=/opt/faiss --with-python=/usr/lib/python3.7/` with MKL version 2019.2.187-1\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\nHere the minimal piece of code to reproduce the issue:\r\n\r\n```\r\nimport numpy as np\r\nimport faiss\r\nimport time\r\nk = 10\r\nd = 1536\r\nnb = 30000000\r\nnq = 1\r\nnp.random.seed(1234) \r\nxb = np.random.random((nb, d)).astype('float32')\r\nxb[:, 0] += np.arange(nb) / 1000.\r\nxq = np.random.random((nq, d)).astype('float32')\r\nxq[:, 0] += np.arange(nq) / 1000.\r\n\r\nindex = faiss.IndexFlatL2(d)\r\nco = faiss.GpuMultipleClonerOptions()\r\nco.shard = True\r\nco.useFloat16 = True\r\nindex = faiss.index_cpu_to_all_gpus(index, co, ngpu=4)\r\nindex.add(xb) ##### WHERE IT CRASHS\r\nstart_time = time.process_time()\r\nD, I = index.search(xq, k)\r\nprint(time.process_time() - start_time, \" seconds\")\r\nprint(I)\r\n```\r\n\r\nWhen running this piece of code the line ```index.add()``` makes crash the machine without any error message, the machine just hangs and then restarts.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/916/comments",
    "author": "jplu",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-08-22T13:17:23Z",
        "body": "Right, it is a bit harsh that it does not report a usable error message. However, you are trying to add 11G of data at once. Could you try to add the data by slices of 1M elements?\r\n\r\n"
      },
      {
        "user": "jplu",
        "created_at": "2019-08-23T12:56:45Z",
        "body": "Cool! Thanks a lot!!! Adding by slice of 1M works perfectly."
      }
    ]
  },
  {
    "number": 905,
    "title": "IVF SQ8 index and vector norm issue",
    "created_at": "2019-08-05T10:13:51Z",
    "closed_at": "2019-08-30T11:54:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/905",
    "body": "We are going to use the vector without normalization to build IVF SQ8 Index. Would you please guide if this is OK? or We need to normalize the vector before building the index.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/905/comments",
    "author": "JinHai-CN",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-08-22T12:17:33Z",
        "body": "I don't understand the question. Are the vectors normalized or not on input to the Faiss index? Both are supported. "
      },
      {
        "user": "mdouze",
        "created_at": "2019-08-30T11:54:13Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 904,
    "title": "GPU and cpu results are different",
    "created_at": "2019-08-04T08:03:34Z",
    "closed_at": "2019-09-02T11:48:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/904",
    "body": "# Summary\r\nI have tested indexflatl2 on cpu and gpu respectively. My dataset size is 100000. The recall of the two is very different. The cpu is 0.9998 and the gpu is 0.866. But I tested it on gist1m, cpu and gpu recall are almost the same, why? Is my data set too small?\r\n\r\nmy machine:\r\nIntel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz\r\nUbuntu 18.04\r\npython3\r\ngpuï¼šTesla V100\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/904/comments",
    "author": "MrHwc",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-08-30T11:53:59Z",
        "body": "You probably have duplicate vectors in your dataset. In that case, the ordering of the vector instances will be undefined."
      },
      {
        "user": "mdouze",
        "created_at": "2019-09-02T11:48:53Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 903,
    "title": "Will IndexIVFFlat's reconstruct return the same vector as being indexed?",
    "created_at": "2019-08-01T21:38:08Z",
    "closed_at": "2019-08-25T14:59:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/903",
    "body": "I indexed two sets of embeddings separately using IndexIVFFlat, and the use the `reconstruct` method to retrieve the embeddings. \r\n\r\nTo my surprise, one of them returns the same embeddings as those indexed, but the other returns different embeddings with quite large distance to the original embeddings (15000 vectors, sum of Euclidean distance 900+). Is this normal?  ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/903/comments",
    "author": "XinliYu",
    "comments": [
      {
        "user": "XinliYu",
        "created_at": "2019-08-25T14:59:49Z",
        "body": "Confirmed it should return the same embeddings as those being indexed; any difference is wrong and we should check for bugs."
      },
      {
        "user": "yuyifan1991",
        "created_at": "2020-12-09T10:47:25Z",
        "body": "> I indexed two sets of embeddings separately using IndexIVFFlat, and the use the `reconstruct` method to retrieve the embeddings.\r\n> \r\n> To my surprise, one of them returns the same embeddings as those indexed, but the other returns different embeddings with quite large distance to the original embeddings (15000 vectors, sum of Euclidean distance 900+). Is this normal?\r\n\r\nDo you have the seuquential ids? I cannot reconstruct vectors and I using IndexIVFFlat to create the index? Could you tell me how to reconstruct? If I use _index.make_direct_map()_, error is: _RuntimeError: Error in void faiss::IndexIVF::make_direct_map(bool) at IndexIVF.cpp:159: Error: '0 <= idlist [ofs] && idlist[ofs] < ntotal' failed: direct map supported only for seuquential ids_\r\n"
      }
    ]
  },
  {
    "number": 900,
    "title": "Kmeans error",
    "created_at": "2019-07-26T12:06:03Z",
    "closed_at": "2019-07-29T00:39:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/900",
    "body": "# Summary\r\nTwo 2D tensors with the same shape lead to different kmean result. The one read from csv get an error, but the random generated one runs OK\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Unbuntu 14.04<!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: Faiss-cpu 1.5.3<!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: using conda<!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n## Code\r\n```\r\nimport numpy as np\r\nimport faiss as fs\r\n\r\nA = np.loadtxt('sift_10k.csv', dtype=float, delimiter=',')\r\n\r\nB = np.random.rand(10000, 128).astype('float32')\r\n\r\nprint A\r\nprint B\r\n\r\nprint A.shape\r\nprint B.shape\r\n\r\nncentroids = 100\r\nniter = 20\r\nverbose = True\r\nd = A.shape[1]\r\n\r\nkmeans = fs.Kmeans(d, ncentroids, niter=niter, verbose=verbose)\r\nkmeans.train(A)\r\n\r\nnp.savetxt('vocab_1k.txt',kmeans.centroids)\r\n```\r\n## Error Info\r\n```\r\n[[ 23.  53.   4. ...  18.  66.  33.]\r\n [126.  38.   0. ...  34.  30.  21.]\r\n [  0.   0.   0. ...   0.   0.  13.]\r\n ...\r\n [  5.   1.   0. ...   7.  33.  27.]\r\n [  2.  38. 135. ...   0.   0.   0.]\r\n [ 23.  11.  35. ...   0.  18.   7.]]\r\n[[0.89541894 0.00223683 0.6539429  ... 0.28040436 0.39110968 0.48791024]\r\n [0.57830787 0.5340468  0.08764375 ... 0.00290395 0.31930214 0.42608193]\r\n [0.6888714  0.49050105 0.767181   ... 0.942297   0.25581676 0.13671431]\r\n ...\r\n [0.582841   0.6721598  0.42406493 ... 0.07052245 0.55508786 0.9895143 ]\r\n [0.29442012 0.4657543  0.2024351  ... 0.4854239  0.7695257  0.37914008]\r\n [0.15035798 0.9554772  0.7352968  ... 0.37981966 0.7891361  0.15399767]]\r\n(10000, 128)\r\n(10000, 128)\r\nTraceback (most recent call last):\r\n  File \"sift_10k.py\", line 20, in <module>\r\n    kmeans.train(A)\r\n  File \"/home/dennis/anaconda2/lib/python2.7/site-packages/faiss/__init__.py\", line 626, in train\r\n    clus.train(x, self.index)\r\n  File \"/home/dennis/anaconda2/lib/python2.7/site-packages/faiss/__init__.py\", line 50, in replacement_train\r\n    self.train_c(n, swig_ptr(x), index)\r\n  File \"/home/dennis/anaconda2/lib/python2.7/site-packages/faiss/swigfaiss.py\", line 1504, in train\r\n    return _swigfaiss.Clustering_train(self, n, x, index)\r\nTypeError: in method 'Clustering_train', argument 3 of type 'float const *'\r\n```\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/900/comments",
    "author": "francescoli",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-07-26T15:52:41Z",
        "body": "Does it work if you replace\r\n```\r\nA = np.loadtxt('sift_10k.csv', dtype=float, delimiter=',')\r\n```\r\nwith\r\n```\r\nA = np.loadtxt('sift_10k.csv', dtype=np.float32, delimiter=',')\r\n```"
      },
      {
        "user": "francescoli",
        "created_at": "2019-07-29T00:39:03Z",
        "body": "Solved, Thank you!"
      }
    ]
  },
  {
    "number": 893,
    "title": "Question about PQEncoderGeneric and PQDecoderGeneric",
    "created_at": "2019-07-18T02:48:07Z",
    "closed_at": "2019-07-18T02:55:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/893",
    "body": "I want to know the algorithm behind PQEncoderGeneric and PQDecoderGeneric. Can anyone tell me about it?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/893/comments",
    "author": "Boreaso",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-07-18T02:55:57Z",
        "body": "It simply allows to write/read a sequence of `n` `k`-bit codes into `ceil((n * k) / 8)` bytes."
      }
    ]
  },
  {
    "number": 892,
    "title": "display a vector at an index",
    "created_at": "2019-07-17T10:28:21Z",
    "closed_at": "2019-07-17T16:43:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/892",
    "body": "i have index of type Index faiss.IndexIVFFlat\r\ni need to retrieve or display a vector at a particular index\r\ni am working on cpu\r\ncan anyone help me in this issue\r\nthanks in advance",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/892/comments",
    "author": "Ravikiran2611",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-07-17T16:43:02Z",
        "body": "You can call `index.reconstruct(id)`."
      },
      {
        "user": "Ravikiran2611",
        "created_at": "2019-07-18T09:50:04Z",
        "body": "thanks you so much @beauby "
      },
      {
        "user": "yuyifan1991",
        "created_at": "2020-12-09T10:52:32Z",
        "body": "> You can call `index.reconstruct(id)`.\r\n\r\nHi, when I use the _index.reconstruct(id)_ , error is: _RuntimeError: Error in virtual void faiss::IndexIVF::reconstruct(faiss::Index::idx_t, float*) const at IndexIVF.cpp:191: Error: 'direct_map.size() == ntotal' failed: direct map is not initialized_  \r\nWhen I use the _index.make_direct_map()_ , error is : _RuntimeError: Error in void faiss::IndexIVF::make_direct_map(bool) at IndexIVF.cpp:159: Error: '0 <= idlist [ofs] && idlist[ofs] < ntotal' failed: direct map supported only for seuquential ids_\r\nI have the hash ids for the index."
      }
    ]
  },
  {
    "number": 887,
    "title": "low search speed for IVFPQ with polysemous training",
    "created_at": "2019-07-09T13:48:48Z",
    "closed_at": "2019-07-15T07:59:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/887",
    "body": "# Summary\r\nI compare the query speed of ivfpq with and without polysemous training on 5000 query samples and 105000 base samples. The results show that the ivfpq with polysemous training costs almost 2 times query time compared with the one withoutpolysemous training (29.94s vs 14.75s).\r\n```\r\nnlist = 2592;\r\nm = 64;\r\nfaiss::IndexFlatL2 quantizer = new faiss::IndexFlatL2(d);\r\nfaiss::Index* index = new faiss::IndexIVFPQ(quantizers, d, nlist, m, 8);\r\n((faiss::IndexIVFPQ*)(index))->nprobe = 2048;\r\n((faiss::IndexIVFPQ*)(index))->do_polysemous_training = true; \r\n...\r\nindex->train(n, xb);\r\nindex->add(n,xb); \r\n...\r\n((faiss::IndexIVFPQ*)index)->polysemous_ht = 220; \r\nindex->search(n, xq, k, Di, Ii);\r\n```\r\nI add and search samples with batch size of 512.\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Ubuntu 14.04.6 LTS\r\n\r\nFaiss version: 656368b\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/887/comments",
    "author": "yingjianling",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-07-09T17:08:24Z",
        "body": "Why are you using nprobe = 2048 which is a significant fraction of the total number of lists (2592)? A brute-force index would be faster in this case.\r\n\r\nThe typical use case of the approximate index is where nprobe << nlist."
      },
      {
        "user": "yingjianling",
        "created_at": "2019-07-10T02:09:40Z",
        "body": "setting nprobe = 512  256 128 gives similar results. \r\nnprobe                       512     256     128     \r\nwith polysemous       8.61s   5.89s   2.64s\r\nwithout                      3.82s   2.05s   1.20s\r\n\r\nhere is the result of using 1million base samples and 5000 query samples.\r\nnlist = 8020\r\nnprobe                      512      256      128\r\nwith polysemous      9.01s    5.25s    3.22s\r\nwithout                     5.01s    2.91s    1.72s\r\n\r\nwe search 1024 nearest neighbors for each query sample."
      },
      {
        "user": "yingjianling",
        "created_at": "2019-07-11T09:45:29Z",
        "body": "I think I have solved the problem. thank your reply"
      },
      {
        "user": "beviah",
        "created_at": "2023-01-15T23:08:51Z",
        "body": "@yingjianling how did you solve the problem? "
      }
    ]
  },
  {
    "number": 884,
    "title": "AttributeError: 'IndexPreTransform' object has no attribute 'invlists'",
    "created_at": "2019-07-05T10:19:52Z",
    "closed_at": "2019-07-05T11:48:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/884",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform centOS\r\n\r\nFaiss version:  1.5.2\r\n\r\n\r\nRunning on: CPU\r\n\r\nInterface:   Python\r\n\r\n\r\n I changed the \"IVF4096,Flat\" to  'OPQ20_80,IMI2x12,PQ20'  and run demo_ondisk_ivf.py failed.\r\n\r\nerror information : \r\nAttributeError: 'IndexPreTransform' object has no attribute 'invlists'\r\n\r\nIf I use 'OPQ20_80,IMI2x12,PQ20'  how to merge the index file.\r\n\r\nThanks\r\n \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/884/comments",
    "author": "winself",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-07-05T11:48:36Z",
        "body": "You're adding an index pre-transform, so you'll need to call `index.index.invlists`."
      },
      {
        "user": "winself",
        "created_at": "2019-07-05T12:10:44Z",
        "body": "but I meet \"AttributeError: 'Index' object has no attribute 'invlists'\" "
      },
      {
        "user": "winself",
        "created_at": "2019-07-05T12:18:11Z",
        "body": "@beauby \r\nwhen i call index.index.invlists\r\n I meet \"AttributeError: 'Index' object has no attribute 'invlists'\"\r\nthanks"
      },
      {
        "user": "beauby",
        "created_at": "2019-07-05T12:55:58Z",
        "body": "`faiss.downcast_Index(index.index).invlists`"
      },
      {
        "user": "winself",
        "created_at": "2019-07-08T07:20:25Z",
        "body": "@beauby \r\nas you say , I use the faiss.downcast_Index(index.index).invlists . it works \r\nbut i meet another problem .\r\n**RuntimeError: Error in size_t faiss::OnDiskInvertedLists::merge_from(const faiss::InvertedLists\\**, int, bool) at OnDiskInvertedLists.cpp:602: Error: 'il->nlist == nlist && il->code_size == code_size' failed**\r\n\r\ni try print nlist  of  the train.index and block_x.index .they are same. \r\n\r\ncan you give some sugestion ? \r\n\r\n # my code  ( just changed the \"IVF4096,Flat\" to 'OPQ20_80,IMI2x12,PQ20') \r\n\r\n`ivfs = []\r\nfor bno in range(2):\r\n    print(\"read \" + indexdir + \"block_%d.index\" % bno)\r\n    index = faiss.read_index(indexdir + \"block_%d.index\" % bno,\r\n                             faiss.IO_FLAG_MMAP)\r\n    ivfs.append(faiss.downcast_index(index.index).invlists)\r\n    index.own_invlists = False\r\n\r\nindex = faiss.read_index(\"./data/trained.index\")\r\n\r\ninvlists = faiss.OnDiskInvertedLists(\r\n    faiss.downcast_index(index.index).nlist, faiss.downcast_index(index.index).code_size,\r\n    indexdir + \"merged_index.ivfdata\")\r\n\r\nivf_vector = faiss.InvertedListsPtrVector()\r\n\r\nfor ivf in ivfs:\r\n    ivf_vector.push_back(ivf)\r\n\r\n\r\nprint(\"merge %d inverted lists \" % ivf_vector.size())\r\nntotal = invlists.merge_from(ivf_vector.data(), ivf_vector.size())\r\n\r\nindex.ntotal = ntotal\r\nfaiss.downcast_index(index.index).replace_invlists(invlists)\r\n\r\nprint(\"write \" + indexdir + \"all.index\")\r\nfaiss.write_index(index, indexdir + \"all.index\")`\r\n"
      }
    ]
  },
  {
    "number": 881,
    "title": "Best way to search over subset of indices",
    "created_at": "2019-07-04T00:31:19Z",
    "closed_at": "2019-07-23T06:09:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/881",
    "body": "# Summary\r\n\r\nI have a question about the ideal use of FAISS for my usecase. Here's a description:\r\n\r\n1. Start with an array of on the order of 1 million vectors. These can have dimensionality ~50-3000\r\n2. Perform ~1-100 similarity searches.\r\n3. manually identify a relevant subset as identified the user. This subset can be arbitrarily small/large. \r\n4. save the indices of relevant subset, and repeat steps 2-4, this time querying over the subset, an arbitrary number of times (so we recursively are doing similarity search over smaller and smaller subsets of the original array)\r\n\r\nWe don't need exact similarity, but we might need exhaustive search, and we care about memory footprint.  \r\n\r\nIt seems to me there're 3 possible approaches to this case where the indices we want to search over is recursively filtered down to smaller and smaller subsets:\r\n\r\n1. Rebuild the index on the subset each time\r\n2. use the `remove_index` function on the existing index for all of the indices not in our filtered subset each round.\r\n3. Maintain and update a blacklist of indices not in the current subset, and filter these indices as a postprocessing step after searching with n_neighbors == size(subset)\r\n\r\nIt seems to me we'd want a flat index with quantization because we're performing few searches, and the wiki seems to recommend approach 3, because `remove_index` is O(n). Is this correct? Does the answer depend on how quickly our subsets shrink?\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/881/comments",
    "author": "siddsach",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-07-04T14:14:46Z",
        "body": "For your 3000-dimension vectors, you should probably start with a PCA. After that, you could simply use a new `IndexFlatL2` at each step."
      },
      {
        "user": "mdouze",
        "created_at": "2019-07-23T06:09:27Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 875,
    "title": "How to get the binary code in LSH?",
    "created_at": "2019-06-27T14:08:45Z",
    "closed_at": "2019-07-23T06:08:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/875",
    "body": "\r\nI use the LSH to build the Index, and I want to save the binary code of gallery dataset. So how can i get the binary code.\r\nI try to one way like this:\r\n```python\r\nwill_encode_vec = np.asarray([query_feature], \"float32\")\r\nprint will_encode_vec, np.shape(will_encode_vec), will_encode_vec[0]\r\nxt = lshIndex.apply_preprocess(1, faiss.swig_ptr(will_encode_vec))\r\ncodes = np.zeros([1 * num_bits / 8], np.uint8)\r\nfaiss.fvecs2bitvecs(xt, faiss.swig_ptr(codes), num_dimension, 1)\r\nprint codes\r\n```\r\nSo does the codes represent the binary code?\r\nLooking for your reply!\r\nThanks\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/875/comments",
    "author": "UpCoder",
    "comments": [
      {
        "user": "Enet4",
        "created_at": "2019-06-27T14:31:07Z",
        "body": "In the native API, the binary codes are in the `std::vector<uint8_t> codes` attribute. Although the Python API does not provide an idiomatic property, a standard C++ vector SWIG proxy can be converted to a NumPy array using `vector_to_array`:\r\n\r\n```python\r\nimport faiss\r\n\r\nindex = faiss.IndexLSH(8, 12)\r\nindex.add(np.random.normal(0, 1, (10, 8)).astype(np.float32))\r\nfaiss.vector_to_array(index.codes)\r\n```\r\n\r\n```none\r\narray([134,  11, 109,   6,  34,   0,  24,  13, 179,   9,  74,   8,  74,\r\n         3,  60,  10, 249,   2, 207,  13], dtype=uint8)\r\n```\r\n "
      },
      {
        "user": "UpCoder",
        "created_at": "2019-06-30T03:40:11Z",
        "body": "@Enet4 \r\nThank you for your reply.\r\nBy the way you provide, we can get the code of training features.\r\nBut how to get the binary code of the test feature as well as the query feature? "
      },
      {
        "user": "mdouze",
        "created_at": "2019-07-01T09:01:57Z",
        "body": "@UpCoder you get the codes of the added vectors (not the ones used for training). \r\nTo get the query vectors, just reset the index, add the query vectors and re-use `vector_to_array`.\r\n"
      },
      {
        "user": "UpCoder",
        "created_at": "2019-07-01T09:14:44Z",
        "body": "@mdouze Thank you\r\nis the method right? which I mentioned in the question. I write the code by the C++ search resource code."
      },
      {
        "user": "mdouze",
        "created_at": "2019-07-23T06:08:04Z",
        "body": "no activity, closing."
      },
      {
        "user": "rostandkenne",
        "created_at": "2019-08-13T17:31:47Z",
        "body": "> @UpCoder you get the codes of the added vectors (not the ones used for training).\r\n> To get the query vectors, just reset the index, add the query vectors and re-use `vector_to_array`.\r\n\r\n@mdouze Can you please provide a working example in python ?"
      }
    ]
  },
  {
    "number": 874,
    "title": "Does CUDA8.0 support?",
    "created_at": "2019-06-26T08:01:25Z",
    "closed_at": "2019-06-26T14:41:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/874",
    "body": "",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/874/comments",
    "author": "SeekPoint",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-06-26T14:38:05Z",
        "body": "What issues are you encountering?"
      },
      {
        "user": "SeekPoint",
        "created_at": "2019-06-26T14:41:54Z",
        "body": "I fixed, it support cuda8.0"
      }
    ]
  },
  {
    "number": 872,
    "title": "How to release Faiss GPU index from memory",
    "created_at": "2019-06-23T12:18:32Z",
    "closed_at": "2019-06-25T07:46:12Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/872",
    "body": "# Summary\r\nI have a Python FAISS GPU application, in which I have to load an index to the GPU multiple times (overwriting the old one). I encountered a problem since the GPU memory is not released after the Python variable has been overwritten. Is there a way to release the index from the GPU memory? \r\nAlso, is there a was to block less than 18 percent of the GPU memory in the Python API?\r\n\r\nOS: Ubuntu 18.04LTS\r\n\r\nRunning on:\r\n- [x ] GPU\r\n\r\nInterface: \r\n- [ x] Python\r\n\r\n# Reproduction instructions\r\n`main_index ` is already loaded to the GPU and I want to release it's memory and replace it by `my_index ` . \r\nI tried loading using either:\r\n```\r\nmy_index = faiss.index_cpu_to_all_gpus(my_index)\r\nmain_index = my_index \r\n\r\n```\r\nOr:\r\n```\r\nco = faiss.GpuClonerOptions()\r\nres = faiss.StandardGpuResources()\r\nmy_index = faiss.index_cpu_to_gpu(res, 0, my_index, co)\r\nmain_index = my_index \r\n```\r\nBoth methods block additional 18 percent of the GPU so I cannot overwrite the index just add additional memory on the GPU. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/872/comments",
    "author": "AmitRozner",
    "comments": [
      {
        "user": "AmitRozner",
        "created_at": "2019-06-24T05:41:08Z",
        "body": "Kind of worked around it with `main_index.reset()` before the assignment. It seems to work fine, please let me know if there is a better option. \r\nStill looking for the syntax for lowering the 18 percent GPU memory allocation."
      },
      {
        "user": "LiberiFatali",
        "created_at": "2019-06-24T08:26:08Z",
        "body": "Check       \r\n`res.noTempMemory()`\r\nand\r\n`res.setTempMemory`"
      }
    ]
  },
  {
    "number": 870,
    "title": "Support for double precision vectors ?",
    "created_at": "2019-06-21T14:48:34Z",
    "closed_at": "2019-06-25T14:34:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/870",
    "body": "Hello,\r\n\r\nI could not use the library because I have double precision vectors and all train() methods use float in their signature.\r\nI think I need to write overrides for all methods that contain floats.\r\n\r\nIs this work worth to be added in a PR ?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/870/comments",
    "author": "unmeshvrije",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-06-21T21:01:03Z",
        "body": "Just convert/round your data to single precision floating point before passing it to us.\r\n\r\nWe almost certainly won't change any of the compute to allow native double precision in the math kernels, so it's just a question of whether you do the conversion or we do, and letting you do the conversion makes the most sense to me.\r\n"
      },
      {
        "user": "unmeshvrije",
        "created_at": "2019-06-24T14:48:26Z",
        "body": "@wickedfoo , Thank you for the suggestion. Is there any technical reason why \"native double precision in the math kernels\" is not allowed ?"
      },
      {
        "user": "mdouze",
        "created_at": "2019-06-25T14:22:02Z",
        "body": "@unmeshvrije, Faiss focuses on high-performance search and most indexes are approximate. In this context, we benefit from the more compact and faster operations on float32 numbers. The added precision of float64 is of no use to Faiss."
      },
      {
        "user": "unmeshvrije",
        "created_at": "2019-06-25T14:34:02Z",
        "body": "Thank you @mdouze !"
      }
    ]
  },
  {
    "number": 867,
    "title": "Faiss use case in keep changing data",
    "created_at": "2019-06-19T16:50:10Z",
    "closed_at": "2019-07-23T06:06:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/867",
    "body": "# Summary\r\n\r\nI understand that Faiss doesn't allow concurrent add and it's not recommended that using update mode on faiss. But we're on the POC that evaluate the use case of Faiss. we have a vectors that keep changing every 5 minutes, we want to build a high performance vector query service on it. Is there any way we can use Faiss on it; one way it our service keep restart and regenerate indices but this still add latency between restart. This lead to a couple of questions, what's the data structure of indices, can we persist indices somewhere and read them from file for saving the time to recreate all indices from the beginning.\r\n\r\n\r\n# Platform\r\n\r\n\r\nOS: \r\n\r\nFaiss version: \r\n\r\nFaiss compilation options: \r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/867/comments",
    "author": "billyean",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-06-19T22:03:29Z",
        "body": "The data structure of indexes depends on the index. \r\nYou can indeed save indexes to file.\r\n\r\nAnswering your general question is difficult without knowing the specific constraints you are dealing with."
      },
      {
        "user": "billyean",
        "created_at": "2019-06-19T22:23:33Z",
        "body": "What we want to do is replace our current Solr index query with Faiss. Current solr has documents that will be changed periodically. We managed to move our vector to a mysql table, the change for the data will be changed to mysql table. We want to use Faiss to implement a high performance(lower latency than solr query and not lower throughput than solr) query. The problem for us is the data keeps changing(We have a data pipeline to write the changed data).  The service we're building need to restart when data changed, and if we need build index from scratch, it takes a couple of seconds, this is too much for us, we want to \r\n1. Lower the latency when service restart.\r\n2. Ultimate goal is a service that can run concurrent with the data pipeline"
      },
      {
        "user": "mdouze",
        "created_at": "2019-06-20T08:28:34Z",
        "body": "So you need an index type that can add / remove vectors. That could be an `IndexIVF` with a `direct_map`. \r\nFaiss does not support concurrent add / search. \r\nBut you could use two indexes that you swap periodically: one in the foreground that handles all searches, one in the background that you perform adds on. If you swap 5 minutes, changes would be taken into account with that latency."
      },
      {
        "user": "LiberiFatali",
        "created_at": "2019-06-24T08:29:50Z",
        "body": "In this case, I have tested using a lock (for example, threading.RLock()) between add / remove vectors for IndexFlat . It works fine"
      },
      {
        "user": "mdouze",
        "created_at": "2019-07-23T06:06:50Z",
        "body": "no activity. Closing."
      },
      {
        "user": "Talgin",
        "created_at": "2021-10-07T19:05:37Z",
        "body": "Hi,\r\nWe created an IndexIVF where we store vectors with ids. We used add_with_ids() method, and after inserting all the vectors (~20 million) we saved our index.\r\nNow we have new vectors (~400 000). How can we add these vectors to an existing index? \r\nWe tried to read previous index and use add_with_ids() on it, but some of the indices were changed after adding new vectors and ids. We need to keep ids and corresponding vectors of the previous index and add new ids and vectors.\r\n1. An obvious solution can be to insert previous data+new data again, but we have limited resources and time.\r\n2. What is the best way to do this?"
      }
    ]
  },
  {
    "number": 861,
    "title": "RuntimeError: Error in faiss::Index *faiss::read_index(faiss::IOReader *, int)",
    "created_at": "2019-06-13T09:53:58Z",
    "closed_at": "2019-06-20T08:37:36Z",
    "labels": [
      "question",
      "cant-repro"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/861",
    "body": "# Summary\r\n\r\nRuntimeError while trying to read index. I'm trying to use faiss with gunicorn/uwsgi. I'm currently using 3 workers and have simultaneous add and search operations. Sometimes I encounter a RuntimeError with the following trace\r\n\r\n`RuntimeError: Error in faiss::Index *faiss::read_index(faiss::IOReader *, int) at index_io.cpp:1056: Error: 'size >= 0 && size < (1L << 40)' failed`\r\n\r\nAnd sometimes the error message is: \r\n`RuntimeError: Error in faiss::Index *faiss::read_index(faiss::IOReader *, int) at index_io.cpp:892: Error: 'ret == (size)' failed: read error in index_dump: 677867 != 1312512 (Invalid argument)`\r\n\r\n# Platform\r\n\r\nOS: macOS 10.14.5\r\n\r\nFaiss version: 1.5.1\r\n\r\nFaiss compilation options: conda install faiss\r\n\r\nRunning on:\r\n- [X] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [X] Python\r\n\r\n# Reproduction instructions\r\n\r\nI'm not sure how to reproduce this consistently.\r\n\r\n#\r\nI would like to understand why this behaviour is observed. Are there any steps that I can follow to avoid them?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/861/comments",
    "author": "abmygate",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-06-13T12:38:39Z",
        "body": "Concurrent add is not supported, which might explain corrupted indexes."
      },
      {
        "user": "abmygate",
        "created_at": "2019-06-14T05:30:05Z",
        "body": "Thanks, I understand concurrent add is not supported.\r\n\r\nEach worker in gunicorn/uwsgi maintains a separate index so corruption of these indexes is not possible (as far as I understand), does it mean that I'm somehow writing to the save file (sometimes) simultaneously from two separate workers and it is corrupted?"
      },
      {
        "user": "mdouze",
        "created_at": "2019-06-14T08:28:32Z",
        "body": "These errors mean there is a data corruption in the file.\r\nNeedless to say, writing to the same file from several threads is not supported either... "
      },
      {
        "user": "husterlantern1",
        "created_at": "2020-10-10T12:33:07Z",
        "body": "RuntimeError while trying to read index.\r\nError in faiss::Index* faiss::read_index(faiss::IOReader*, int) at impl/index_read.cpp:445: Error: 'ret == (size)' failed: read error in /index_file/index_1.idx: 10954941 != 295036288 (Success).\r\nCan anyone help ? Thanks a lot!"
      },
      {
        "user": "getoar-g",
        "created_at": "2020-11-06T10:10:08Z",
        "body": "Does anyone has the solution for this problem yet?"
      },
      {
        "user": "rahulbhoyar1995",
        "created_at": "2024-03-01T19:15:33Z",
        "body": "I am also getting this error.\r\n\r\nRuntimeError: Error in faiss::Index* faiss::read_index(faiss::IOReader*, int) at /project/faiss/faiss/impl/index_read.cpp:1053: Index type 0x19ef32bc (\"\\xbc2\\xef\\x19\") not recognized\r\n\r\n\r\nThis is my code :\r\n\r\n```\r\nfrom langchain.vectorstores import FAISS\r\nfrom langchain_openai import OpenAIEmbeddings\r\nembeddings = OpenAIEmbeddings()\r\nvectordb = FAISS.load_local(VECTOR_DATABASE_PATH, embeddings)\r\n\r\n```\r\nDoes anyone knows how to resolve this ?"
      }
    ]
  },
  {
    "number": 859,
    "title": "how to guaranteed uniqueness of id in index with add_with_ids",
    "created_at": "2019-06-12T13:59:44Z",
    "closed_at": "2019-06-13T01:23:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/859",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Centos 7.5\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\nHi,\r\nI  try try to add vector with a special id into index by add_with_ids api, also I do not want to add duplicate vector(identified by id) into index. \r\nBut i find this index allow duplicate id exist, so i have to maintain an id set to decision whether exist or not. \r\nSo, my questions :\r\n1. Is there some api of index can be used to decision whether some id exist or not. \r\n2. Is there some api guaranteed uniqueness of id\r\n\r\n<pre><code>\r\nimport faiss\r\nimport numpy as np\r\n\r\nv = np.random.rand(1,128).astype('float32')\r\nindex = faiss.IndexFlatL2(128)\r\nindex = faiss.IndexIDMap(index)\r\n\r\nindex.add_with_ids(v, np.array([1001]))\r\nprint(index.ntotal) # 1\r\nindex.add_with_ids(v, np.array([1001]))\r\nprint(index.ntotal) # 2</code></pre>\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/859/comments",
    "author": "handsomefun",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-06-12T23:48:35Z",
        "body": "You would have to keep track of it yourself and enforce it. There is no requirement that the IDs are unique, in fact some use cases may desire that multiple vectors have the same identifier."
      },
      {
        "user": "handsomefun",
        "created_at": "2019-06-13T01:23:23Z",
        "body": "> You would have to keep track of it yourself and enforce it. There is no requirement that the IDs are unique, in fact some use cases may desire that multiple vectors have the same identifier.\r\n\r\nOk, thanks"
      }
    ]
  },
  {
    "number": 857,
    "title": "Can Faiss GPU index be shared between processes?",
    "created_at": "2019-06-11T12:02:02Z",
    "closed_at": "2019-06-12T06:38:40Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/857",
    "body": "# Summary\r\nCan Faiss GPU index be shared between processes? i.e. is it possible to call search on an index which is in the GPU from multiple processes?\r\n\r\nOS: Ubuntu 18.04 LTS\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x ] Python\r\n\r\n# Reproduction instructions\r\nCurrently, I call Faiss from each process using:\r\n```\r\n        index = faiss.read_index(index.index))\r\n        co = faiss.GpuClonerOptions()\r\n        res = faiss.StandardGpuResources()\r\n        index = faiss.index_cpu_to_gpu(res, 0, index , co)\r\n```\r\nThis result in a waste of GPU memory. Is it possible to search the index from other python processes as well instead of reloading it? \r\nI just search the index so it can be on the GPU the whole time.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/857/comments",
    "author": "AmitRozner",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-06-12T01:20:24Z",
        "body": "This is not possible and will not be implemented, as this requires CUDA IPC handles etc.\r\n\r\nWhy are you trying to do this? Why can't the index just be owned by a single process and you route requests to that process?\r\n"
      },
      {
        "user": "AmitRozner",
        "created_at": "2019-06-12T06:38:40Z",
        "body": "I can do that, was just wondering whether it is already implemented by any of Faiss methods. "
      }
    ]
  },
  {
    "number": 856,
    "title": "How to use the IndexLSH with python",
    "created_at": "2019-06-11T09:32:57Z",
    "closed_at": "2019-06-11T12:51:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/856",
    "body": "Hi, I want to use the LSH index. But, there is a very few information.\r\nWhen I use the code to build the Index, \r\n```python\r\n    import faiss\r\n    lshIndex = faiss.IndexLSH(num_dimension, num_bits)\r\n    print np.shape(features), len(features)\r\n    lshIndex.add(len(features), features)\r\n```\r\nIt alway occur a like this:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/mnt/cephfs_wj/vc/liangdong.tony/PycharmProjects/RetrievalCCWebVideo/LSH/main.py\", line 53, in <module>\r\n    lshIndex.train(len(features), features)\r\nTypeError: replacement_train() takes exactly 2 arguments (3 given)\r\n```\r\nIf I delete the first parameters, like this\r\n```python\r\nlshIndex.add(features)\r\n```\r\nit will occur this error:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/mnt/cephfs_wj/vc/liangdong.tony/PycharmProjects/RetrievalCCWebVideo/LSH/main.py\", line 54, in <module>\r\n    lshIndex.add(features)\r\n  File \"/data00/home/liangdong.tony/anaconda2/lib/python2.7/site-packages/faiss/__init__.py\", line 94, in replacement_add\r\n    self.add_c(n, swig_ptr(x))\r\n  File \"/data00/home/liangdong.tony/anaconda2/lib/python2.7/site-packages/faiss/swigfaiss.py\", line 2284, in add\r\n    return _swigfaiss.IndexLSH_add(self, n, x)\r\nTypeError: in method 'IndexLSH_add', argument 3 of type 'float const *'\r\n```\r\nSo, do you know how to fix it?\r\nThanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/856/comments",
    "author": "UpCoder",
    "comments": [
      {
        "user": "UpCoder",
        "created_at": "2019-06-11T09:42:41Z",
        "body": "I fixed it by this code:\r\n```python\r\n     import faiss\r\n    lshIndex = faiss.IndexLSH(num_dimension, num_bits)\r\n    print np.shape(features), len(features)\r\n    # lshIndex.add(len(features), features)\r\n    # lshIndex.add(np.ascontiguousarray(features))\r\n    input_features = np.ascontiguousarray(np.asarray(features, \"float32\"))\r\n    lshIndex.add(input_features)\r\n    print lshIndex.search(np.asarray([input_features[0]], \"float32\"), 10)\r\n```\r\nbut how to point the distance function? such as hamming distance?"
      },
      {
        "user": "beauby",
        "created_at": "2019-06-11T12:51:26Z",
        "body": "@UpCoder `IndexLSH` uses Hamming distance, there is nothing special to do."
      },
      {
        "user": "chikubee",
        "created_at": "2020-01-30T11:50:08Z",
        "body": "@beauby Is there a way to check how many elements have similar hashes. Just thinking of a condition if there are say 20 samples on \"sofware\" and 1 on \"cat\" and if the query is software relevant this will be a brute force match with 20 samples due to similar hash. Is there some way to configure nlist like in the others., for keeping consistent bucket size."
      }
    ]
  },
  {
    "number": 855,
    "title": "Confidence value to face similarity",
    "created_at": "2019-06-07T03:32:33Z",
    "closed_at": "2019-06-20T08:37:50Z",
    "labels": [
      "question",
      "out-of-scope"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/855",
    "body": "Hi,\r\n\r\nAny one knows how to convert face similarity measure (L2 distance, cosine value) in faiss into confidence value [0-1] ?\r\n\r\nThanks,",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/855/comments",
    "author": "0xhanh",
    "comments": [
      {
        "user": "0xhanh",
        "created_at": "2019-06-07T06:52:46Z",
        "body": "may be 1/(1+ d(p1, p2))"
      },
      {
        "user": "mdouze",
        "created_at": "2019-06-13T09:43:04Z",
        "body": "This is more a problem on the application side."
      }
    ]
  },
  {
    "number": 854,
    "title": "Python2 installation fail: fatal error: Python.h: No such file or directory",
    "created_at": "2019-06-05T09:31:37Z",
    "closed_at": "2019-06-05T11:38:04Z",
    "labels": [
      "question",
      "install"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/854",
    "body": "@beauby \r\nHi.\r\n\r\nI'm installing faiss for python2 (I've successfully installed faiss for python3). The same error occurs as I previously installing for python3, `fatal error: Python.h: No such file or directory`. When installing for python3, I update swig version and solved this problem. This time the directory that python -c \"import sysconfig; print(sysconfig.get_path('include'))\" returns does not exist. It returns /usr/local/include/python2.7, but it is empty and the correct directory seems to be /usr/include/python2.7, according to my python3 installation experience.\r\n\r\nWould you please show me how to solve this problem? Thanks.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/854/comments",
    "author": "WIll-Xu35",
    "comments": [
      {
        "user": "dingwoai",
        "created_at": "2019-06-05T10:24:24Z",
        "body": "here is my solution:\r\n1. open ur makefile.inc file\r\n2. \"PYTHONCFLAGS =  -I/usr/include/python2.7 -I/usr/include/python2.7\" instead of \"PYTHONCFLAGS =  -I/usr/local/include/python2.7 -I/usr/local/include/python2.7\"\r\n3. make and then make py\r\nworks for me"
      },
      {
        "user": "beauby",
        "created_at": "2019-06-05T11:38:04Z",
        "body": "@dingwoai's answer is right. There seems to be an inconsistency in the paths returned by python27 on Ubuntu."
      }
    ]
  },
  {
    "number": 850,
    "title": "HNSW support range_search?",
    "created_at": "2019-06-03T14:23:38Z",
    "closed_at": "2019-06-04T02:03:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/850",
    "body": "Hi,\r\n\r\nI am curious if HNSW supports range_search. I just know the IndexIVFFlat support range_search function.\r\n\r\nI am looking forward to your reply!\r\nThank you.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/850/comments",
    "author": "UpCoder",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-06-03T15:10:48Z",
        "body": "No range_search is not supported because the exploration strategy of nearest neighbors is tuned for knn-search. So it is not easy to add either.\r\n"
      },
      {
        "user": "UpCoder",
        "created_at": "2019-06-04T02:03:03Z",
        "body": "OK, Thanks"
      }
    ]
  },
  {
    "number": 841,
    "title": "redefine the type of idx_t ",
    "created_at": "2019-05-28T08:57:45Z",
    "closed_at": "2019-05-28T10:12:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/841",
    "body": "# Summary\r\n\r\nIn my business scenarioï¼Œ i will use the add_with_ids function to add vector to indexï¼Œ the xids is userid. but the type of xids is long and i need unsigned long.\r\nIs it possible to redefine idx_t to unsigned long and recompile the faiss?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/841/comments",
    "author": "yuxingfirst",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-05-28T10:12:33Z",
        "body": "This is not currently possible, but you can use `IndexIDMap` or maintain your own mapping."
      },
      {
        "user": "yuxingfirst",
        "created_at": "2019-05-29T01:53:08Z",
        "body": "> This is not currently possible, but you can use `IndexIDMap` or maintain your own mapping.\r\n\r\nok, I will consider other solutions"
      }
    ]
  },
  {
    "number": 840,
    "title": "faiss  whether to support distributed searchï¼Ÿ",
    "created_at": "2019-05-27T16:25:16Z",
    "closed_at": "2019-05-27T19:10:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/840",
    "body": "faiss  whether to support distributed searchï¼Ÿ\r\nThank",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/840/comments",
    "author": "AmierCheng",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-05-27T17:56:06Z",
        "body": "Faiss is a library, if you want to build a distributed service around it you will need to implement it yourself."
      },
      {
        "user": "AmierCheng",
        "created_at": "2019-05-28T02:37:49Z",
        "body": "@beauby Thanks"
      }
    ]
  },
  {
    "number": 834,
    "title": "remove is not supported for the GPUï¼Ÿ",
    "created_at": "2019-05-23T06:52:24Z",
    "closed_at": "2019-05-23T14:15:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/834",
    "body": "remove is not supported for the GPUï¼ŒWhen is the remove id operation for GPU?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/834/comments",
    "author": "AmierCheng",
    "comments": [
      {
        "user": "AmierCheng",
        "created_at": "2019-05-23T06:54:08Z",
        "body": "I want to remove id from gup, how can I do it?  Thanks."
      },
      {
        "user": "beauby",
        "created_at": "2019-05-23T14:15:09Z",
        "body": "Duplicate of #245, closing."
      }
    ]
  },
  {
    "number": 830,
    "title": "Which index has both 'indexer.add_with_ids' and 'reconstruct' method?",
    "created_at": "2019-05-20T09:39:05Z",
    "closed_at": "2019-05-29T14:42:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/830",
    "body": "As mentioned in title , which type index has the implementation of  'indexer.add_with_ids' and 'reconstruct' ?\r\n\r\nthe IndexFlatL2 has reconstruct but without add_with_ids\r\nwhile IndexHNSWFlat doesn't have reconstruct",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/830/comments",
    "author": "wenbotse",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-05-21T11:50:01Z",
        "body": "For indexes on CPU, only IVF ones have `add_with_ids`.\r\n\r\n> while IndexHNSWFlat doesn't have reconstruct\r\n\r\n`IndexHNSWFlat` should inherit `reconstruct` from `IndexHNSW`."
      },
      {
        "user": "mdouze",
        "created_at": "2019-05-22T20:27:04Z",
        "body": "Only `IndexIDMap2` has this functionality, as it requires a hash table to map the arbitrary ids to sequential ones. "
      },
      {
        "user": "mdouze",
        "created_at": "2019-05-29T14:42:41Z",
        "body": "No activity, closing."
      },
      {
        "user": "yuyifan1991",
        "created_at": "2020-12-09T10:19:48Z",
        "body": "> Only `IndexIDMap2` has this functionality, as it requires a hash table to map the arbitrary ids to sequential ones.\r\n\r\nHow to map the arbitrary ids to sequential ones? I use the IndexIVFFlat, when I created the index, I cannot reconstruct the vector. I have the hash ids ."
      }
    ]
  },
  {
    "number": 829,
    "title": "how to install faiss in windows?thanks",
    "created_at": "2019-05-19T14:14:32Z",
    "closed_at": "2019-05-19T21:11:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/829",
    "body": "i want to use faiss in windows.but i failed.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/829/comments",
    "author": "LiangWUSDU",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-05-19T14:33:13Z",
        "body": "Faiss is currently not available on Windows.\n\nOn Sun 19 May 2019 at 16:14, LiangWUSDU <notifications@github.com> wrote:\n\n> i want to use faiss in windows.but i failed.\n>\n-- \nLucas Hosseini\nlucas.hosseini@gmail.com\n"
      }
    ]
  },
  {
    "number": 828,
    "title": "Deadlock at Python search Step",
    "created_at": "2019-05-18T20:56:41Z",
    "closed_at": "2019-05-29T14:42:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/828",
    "body": "# Summary\r\n\r\nDeadlock potentially caused by OpenBLAS. I did a quick search online, it seems there is a known bug that OpenBLAS can be in deadlock state. I observe faiss hangs at the search step (e.g. below):\r\n\r\nindex = faiss.IndexFlatL2(...)\r\nknn_d, knn_i = index.search(...)\r\n\r\nDoes anyone else observe the same issue?\r\n\r\n# Platform\r\nUbuntu 14.04.5\r\n\r\nRunning on:\r\n- CPU\r\n\r\nInterface: \r\n- Python",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/828/comments",
    "author": "perfectzjf",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-05-22T20:28:23Z",
        "body": "@perfectzjf, do you observe the deadlock? \r\nCould you post the full code + makefile.inc + version of openblas?\r\nThanks"
      },
      {
        "user": "mdouze",
        "created_at": "2019-05-29T14:42:23Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 827,
    "title": "Why do index 'PQx' search on GPU but actually run on CPU?",
    "created_at": "2019-05-15T06:34:50Z",
    "closed_at": "2019-05-29T14:42:06Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/827",
    "body": "Hi, when I run index 'PQx' in GPU mode, but I find that it run on cpu actually. Please tell me the reason and whether 'PQx' running on GPU will be supported in the future. THX.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/827/comments",
    "author": "chenyihang1993",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-05-15T06:49:46Z",
        "body": "You are probably using `index_cpu_to_gpu`. That function moves to GPU only the indexes that have a GPU equivalent. The rest is copied but remains on CPU. \r\nThere is no `IndexPQ` on GPU, only the `IndexIVFPQ`. You may want to use an `IndexIVFPQ` with a single centroid, which is functionally equivalent to an `IndexPQ`."
      },
      {
        "user": "mdouze",
        "created_at": "2019-05-29T14:42:06Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 826,
    "title": "TypeError: in method 'ReconstructFromNeighbors_get_neighbor_table', argument 2 of type 'faiss::ReconstructFromNeighbors::storage_idx_t'",
    "created_at": "2019-05-14T07:34:13Z",
    "closed_at": "2019-05-14T22:53:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/826",
    "body": "# Summary\r\n\r\nwhen i run bench_link_and_code.py,  the above exception appear.\r\n```\r\nTraceback (most recent call last):\r\n  File \"bench_link_and_code.py\", line 180, in <module>\r\n    rfn, xb_full, niter=args.beta_niter)\r\n  File \"/data/scripts/imageAI/yufeiImageAI/faiss/neighbor_codec.py\", line 235, in train_beta_codebook\r\n    ngpus=0, niter=niter)\r\n  File \"/data/scripts/imageAI/yufeiImageAI/faiss/neighbor_codec.py\", line 192, in neighbors_kmeans\r\n    neighbor_table = get_neighbor_table(x_coded, Inn, pos[i])\r\n  File \"/data/scripts/imageAI/yufeiImageAI/faiss/neighbor_codec.py\", line 84, in get_neighbor_table\r\n    rfn.get_neighbor_table(i, faiss.swig_ptr(out))\r\n  File \"/data/anaconda3/lib/python3.5/site-packages/faiss/swigfaiss.py\", line 3405, in get_neighbor_table\r\n    return _swigfaiss.ReconstructFromNeighbors_get_neighbor_table(self, i, out)\r\nTypeError: in method 'ReconstructFromNeighbors_get_neighbor_table', argument 2 of type 'faiss::ReconstructFromNeighbors::storage_idx_t'\r\n```\r\n\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS:  centos\r\n\r\nFaiss version: 1.4.0\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n\r\n\r\nInterface: \r\n- [ ] Python3.5\r\n\r\n# Reproduction instructions\r\n\r\n\r\n\r\n```\r\nnohup python bench_link_and_code.py \\\r\n   --db bigann100M \\\r\n   --M0 7 \\\r\n   --indexkey OPQ40_160,HNSW32_PQ40 \\\r\n   --indexfile /data_hadoop_2/mlib_data/bigann/bigann_indexfile_100M.index \\\r\n   --beta_nsq 8   \\\r\n   --beta_centroids /data_hadoop_2/mlib_data/bigann/bigann_centroids_100M.npy \\\r\n   --neigh_recons_codes /data_hadoop_2/mlib_data/bigann/bigann_neigh_recons_codes_100M.npy \\\r\n   --k_reorder 5 --efSearch 2 \\\r\n   --add_bs 10000 \\\r\n   --searchthreads 24 > ./test_100M.out &\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/826/comments",
    "author": "jasstionzyf",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-05-14T13:25:04Z",
        "body": "Could you try replacing \r\n```\r\nrfn.get_neighbor_table(i, faiss.swig_ptr(out))\r\n``` \r\nwith \r\n```\r\nrfn.get_neighbor_table(int(i), faiss.swig_ptr(out))\r\n```\r\n"
      }
    ]
  },
  {
    "number": 825,
    "title": "CUDA error 35 ",
    "created_at": "2019-05-13T10:01:31Z",
    "closed_at": "2019-05-13T11:49:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/825",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS:  ubuntu 16.04 \r\n\r\nFaiss version: 1.5\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n-  GPU\r\n\r\nInterface: \r\n- Python\r\n\r\n# Reproduction instructions\r\n\r\nFaiss assertion 'err__ == cudaSuccess' failed in int faiss::gpu::getNumDevices() at gpu/utils/DeviceUtils.cu:32; details: CUDA error 35 CUDA driver version is insufficient for CUDA runtime version\r\nAborted (core dumped)\r\n\r\n>>> torch.version.cuda\r\n'8.0.61'\r\n>>> torch.cuda.is_available()\r\nTrue\r\n>>> torch.__version__\r\n'0.4.0'\r\n\r\n\r\nMon May 13 17:31:31 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.130                Driver Version: 384.130                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:17:00.0 Off |                  N/A |\r\n|  0%   34C    P8    10W / 280W |      2MiB / 11172MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:65:00.0  On |                  N/A |\r\n|  0%   45C    P8    13W / 280W |    563MiB / 11169MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/825/comments",
    "author": "enjoy4",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-05-13T11:49:35Z",
        "body": "The error message\r\n> `CUDA error 35 CUDA driver version is insufficient for CUDA runtime version`\r\n\r\nstates that your CUDA driver version is insufficient for CUDA 8.0 runtime. You should update your CUDA driver."
      },
      {
        "user": "enjoy4",
        "created_at": "2019-05-13T13:55:28Z",
        "body": "The original CUDA driver is suitable for cuda 8.0 according to nvidia file.  and When I update  the cuda driver to 390.87, the problem still exists. "
      },
      {
        "user": "enjoy4",
        "created_at": "2019-05-13T13:57:05Z",
        "body": "@beauby "
      },
      {
        "user": "enjoy4",
        "created_at": "2019-05-13T13:59:30Z",
        "body": "Should I update the driver to the newest one?"
      },
      {
        "user": "beauby",
        "created_at": "2019-05-13T14:01:06Z",
        "body": "Yes, please update your driver."
      }
    ]
  },
  {
    "number": 822,
    "title": "Make py -- SyntaxError: invalid syntax",
    "created_at": "2019-05-09T05:03:54Z",
    "closed_at": "2019-05-13T07:58:51Z",
    "labels": [
      "question",
      "install"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/822",
    "body": "## when I run \"make py\", the following error appears\r\n\r\n```\r\nmake[1]: Entering directory 'path_to/faiss/python'\r\npython -c++ -Doverride= -I../ -DGPU_WRAPPER -o swigfaiss.cpp swigfaiss.swig\r\n  File \"<string>\", line 1\r\n    ++\r\n     ^\r\nSyntaxError: invalid syntax\r\nMakefile:17: recipe for target 'swigfaiss.cpp' failed\r\nmake[1]: [swigfaiss.cpp] Error 1 (ignored)\r\ng++ -std=c++11 -DFINTEGER=int  -fopenmp -I/usr/local/cuda-10.0/include  -fPIC -m64 -Wno-sign-compare -g -O3 -Wall -Wextra -msse4 -mpopcnt -I \\\r\n               -I../ -c swigfaiss.cpp -o swigfaiss.o\r\ng++: error: swigfaiss.cpp: No such file or directory\r\ng++: fatal error: no input files\r\ncompilation terminated.\r\nMakefile:20: recipe for target 'swigfaiss.o' failed\r\nmake[1]: *** [swigfaiss.o] Error 1\r\nmake[1]: Leaving directory '/opt/Faiss/faiss/python'\r\nMakefile:82: recipe for target 'py' failed\r\nmake: *** [py] Error 2\r\n```\r\n# Env\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nFaiss version: up to date with 'origin/master'\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\nInterface: \r\n- [x] C++\r\n- [x] Python\r\n\r\n# Previous steps done:\r\n\r\n----\r\nswig -version\r\nSWIG Version 4.0.0\r\nCompiled with g++ [x86_64-pc-linux-gnu]\r\n---\r\n\r\n$ ./configure --with-cuda=/usr/local/cuda-10.0  --with-python=/usr/bin/python3\r\n\r\n```\r\n./configure --with-cuda=/usr/local/cuda-10.0  --with-python=/usr/bin/python3\r\nchecking for g++... g++\r\nchecking whether the C++ compiler works... yes\r\nchecking for C++ compiler default output file name... a.out\r\nchecking for suffix of executables...\r\nchecking whether we are cross compiling... no\r\nchecking for suffix of object files... o\r\nchecking whether we are using the GNU C++ compiler... yes\r\nchecking whether g++ accepts -g... yes\r\nchecking whether g++ supports C++11 features with -std=c++11... yes\r\nchecking for gcc... gcc\r\nchecking whether we are using the GNU C compiler... yes\r\nchecking whether gcc accepts -g... yes\r\nchecking for gcc option to accept ISO C89... none needed\r\nchecking how to run the C preprocessor... gcc -E\r\nchecking whether make sets $(MAKE)... yes\r\nchecking for a thread-safe mkdir -p... /bin/mkdir -p\r\nchecking for /usr/bin/python3... no\r\nchecking for Python C flags... ./configure: line 4138: -c: command not found\r\n\r\nchecking for swig... no\r\nchecking how to run the C++ preprocessor... g++ -std=c++11 -E\r\nchecking for grep that handles long lines and -e... /bin/grep\r\nchecking for egrep... /bin/grep -E\r\nchecking for ANSI C header files... yes\r\nchecking for sys/types.h... yes\r\nchecking for sys/stat.h... yes\r\nchecking for stdlib.h... yes\r\nchecking for string.h... yes\r\nchecking for memory.h... yes\r\nchecking for strings.h... yes\r\nchecking for inttypes.h... yes\r\nchecking for stdint.h... yes\r\nchecking for unistd.h... yes\r\nchecking for nvcc... /usr/local/cuda-10.0/bin/nvcc\r\nchecking cuda.h usability... yes\r\nchecking cuda.h presence... yes\r\nchecking for cuda.h... yes\r\nchecking for cublasAlloc in -lcublas... yes\r\nchecking for cudaSetDevice in -lcudart... yes\r\nchecking float.h usability... yes\r\nchecking float.h presence... yes\r\nchecking for float.h... yes\r\nchecking limits.h usability... yes\r\nchecking limits.h presence... yes\r\nchecking for limits.h... yes\r\nchecking stddef.h usability... yes\r\nchecking stddef.h presence... yes\r\nchecking for stddef.h... yes\r\nchecking for stdint.h... (cached) yes\r\nchecking for stdlib.h... (cached) yes\r\nchecking for string.h... (cached) yes\r\nchecking sys/time.h usability... yes\r\nchecking sys/time.h presence... yes\r\nchecking for sys/time.h... yes\r\nchecking for unistd.h... (cached) yes\r\nchecking for stdbool.h that conforms to C99... no\r\nchecking for _Bool... no\r\nchecking for inline... inline\r\nchecking for int32_t... yes\r\nchecking for int64_t... yes\r\nchecking for C/C++ restrict keyword... __restrict\r\nchecking for size_t... yes\r\nchecking for uint16_t... yes\r\nchecking for uint32_t... yes\r\nchecking for uint64_t... yes\r\nchecking for uint8_t... yes\r\nchecking for stdlib.h... (cached) yes\r\nchecking for GNU libc compatible malloc... yes\r\nchecking for stdlib.h... (cached) yes\r\nchecking for unistd.h... (cached) yes\r\nchecking for sys/param.h... yes\r\nchecking for getpagesize... yes\r\nchecking for working mmap... yes\r\nchecking for clock_gettime... yes\r\nchecking for floor... yes\r\nchecking for gettimeofday... yes\r\nchecking for memmove... yes\r\nchecking for memset... yes\r\nchecking for munmap... yes\r\nchecking for pow... yes\r\nchecking for sqrt... yes\r\nchecking for strerror... yes\r\nchecking for strstr... yes\r\nchecking for g++ -std=c++11 option to support OpenMP... -fopenmp\r\nchecking build system type... x86_64-pc-linux-gnu\r\nchecking host system type... x86_64-pc-linux-gnu\r\nchecking if sgemm_ is being linked in already... no\r\nchecking for sgemm_ in -lmkl_intel_lp64... no\r\nchecking for sgemm_ in -lmkl... no\r\nchecking for sgemm_ in -lopenblas... yes\r\nchecking for cheev_... yes\r\nchecking target system type... x86_64-pc-linux-gnu\r\nchecking for cpu arch... x86_64-pc-linux-gnu CPUFLAGS+=-msse4 -mpopcnt CXXFLAGS+=-m64\r\nconfigure: creating ./config.status\r\nconfig.status: creating makefile.inc\r\n```\r\n\r\n$ make\r\n$ make install\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/822/comments",
    "author": "0xhanh",
    "comments": [
      {
        "user": "Santiago810",
        "created_at": "2019-05-09T08:22:11Z",
        "body": "\r\nthe first line show some flag var are wrong\r\nthe second line show swig is not installed.\r\n\r\nI also fail when making py.\r\n```\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\nswigfaiss.swig:301: Warning 302: Identifier 'IndexShards' redefined (ignored) (Renamed from 'IndexShardsTemplate< faiss::Index >'),\r\n../IndexShards.h:79: Warning 302: previous definition of 'IndexShards'.\r\nswigfaiss.swig:302: Warning 302: Identifier 'IndexBinaryShards' redefined (ignored) (Renamed from 'IndexShardsTemplate< faiss::IndexBinary >'),\r\n../IndexShards.h:80: Warning 302: previous definition of 'IndexBinaryShards'.\r\nswigfaiss.swig:305: Warning 302: Identifier 'IndexReplicas' redefined (ignored) (Renamed from 'IndexReplicasTemplate< faiss::Index >'),\r\n../IndexReplicas.h:86: Warning 302: previous definition of 'IndexReplicas'.\r\nswigfaiss.swig:306: Warning 302: Identifier 'IndexBinaryReplicas' redefined (ignored) (Renamed from 'IndexReplicasTemplate< faiss::IndexBinary >'),\r\n../IndexReplicas.h:87: Warning 302: previous definition of 'IndexBinaryReplicas'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../IndexBinary.h:38: Warning 315: Nothing known about 'Index::idx_t'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../IndexBinary.h:38: Warning 315: Nothing known about 'Index::idx_t'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../IndexBinary.h:38: Warning 315: Nothing known about 'Index::idx_t'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../IndexBinary.h:38: Warning 315: Nothing known about 'Index::idx_t'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../IndexBinary.h:38: Warning 315: Nothing known about 'Index::idx_t'.\r\n```\r\nthis warning lead to the idx_t undefined  when compile the swigfaiss.cpp.\r\nwhen I try to explicit typedefine idx_t, it still get error about other undefine functions.Needing help"
      },
      {
        "user": "beauby",
        "created_at": "2019-05-09T10:00:44Z",
        "body": "@hanhfgia Swig does not seem to be in your path."
      },
      {
        "user": "beauby",
        "created_at": "2019-05-09T10:01:09Z",
        "body": "@Santiago810 Would you mind opening a separate issue?"
      },
      {
        "user": "0xhanh",
        "created_at": "2019-05-10T06:54:32Z",
        "body": "> @hanhfgia Swig does not seem to be in your path.\r\n\r\nThanks, reload env missed :). It's done"
      },
      {
        "user": "chenqiu01",
        "created_at": "2020-04-17T09:17:07Z",
        "body": "> > @hanhfgia Swig does not seem to be in your path.\r\n> \r\n> Thanks, reload env missed :). It's done\r\n\r\nExcuse me, What's the Path which i need to join in?"
      },
      {
        "user": "rookiezed",
        "created_at": "2022-09-27T02:06:06Z",
        "body": "> > > @hanhfgia Swig does not seem to be in your path.\r\n> > \r\n> > \r\n> > Thanks, reload env missed :). It's done\r\n> \r\n> Excuse me, What's the Path which i need to join in?\r\n\r\ntry install swig, this fix my problem"
      }
    ]
  },
  {
    "number": 820,
    "title": "How to train the mini-batch data time by time",
    "created_at": "2019-05-07T08:20:08Z",
    "closed_at": "2019-05-29T14:41:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/820",
    "body": "There is 1B vectors in my dataset. Referring to the Guidelines to choose an index, I get that I need \"...,IVF1048576_HNSW32,...\" Index and between 30 * 1048576 and 256 * 1048576 vectors for training, \r\nbut I can't read all of the train dataset to RAM. How do I train the index? Is there any training way training by mini-batch? \r\nLooking forward to your reply. THX.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/820/comments",
    "author": "chenyihang1993",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-05-15T06:53:21Z",
        "body": "No the training has to full-batch. \r\nIf 1M centroids is too many then try with 256k. It will not be very sub-optimal."
      },
      {
        "user": "mdouze",
        "created_at": "2019-05-29T14:41:51Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 819,
    "title": "Faiss assertion 'j == index->ntotal' failed in virtual long int faiss::IndexIDMap::remove_ids(const faiss::IDSelector&) at MetaIndexes.cpp:123",
    "created_at": "2019-05-06T09:26:02Z",
    "closed_at": "2019-05-29T14:41:31Z",
    "labels": [
      "question",
      "cant-repro"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/819",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- Ubuntu 16.04.6 LTS -->\r\n Ubuntu 16.04.6 LTS \r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n1.5.0\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\nremove idï¼š1265286\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/819/comments",
    "author": "AmierCheng",
    "comments": [
      {
        "user": "Enet4",
        "created_at": "2019-05-06T09:48:59Z",
        "body": "Can you provide a minimal example of code that reproduces the problem? "
      },
      {
        "user": "AmierCheng",
        "created_at": "2019-05-07T02:06:45Z",
        "body": "> Can you provide a minimal example of code that reproduces the problem?\r\n\r\n@app.route('/remove',methods=['GET'])\r\ndef remove():\r\n    \ttry:\r\n    \t\tif request.method==\"GET\":\r\n    \t\t\tiid=request.args.get('iid')\r\n\t\t\tlid=request.args.get('lid')\r\n\t\t\tdeletekey=request.args.get('appkey')\r\n\t\t\tif(deletekey!=\"1234\"):\r\n    \t\t\t\treturn jsonify({'result':4})\r\n\t\t\tdelFilePath=\"/lineindex/featuretxt/\"+iid+\"_\"+lid+\".jpg.txt\"\r\n\t\t\tif((os.path.exists(delFilePath))):\r\n    \t\t\t\tos.remove(delFilePath)\r\n\t\t\tindex2.remove_ids(faiss.IDSelectorRange(int(iid), int(iid)+2))\r\n\t\t\tredisinit.delete(iid)\r\n\t\t\treturn jsonify({'result':1})\r\n\texcept:\r\n\t\t\treturn jsonify({'result':0})"
      },
      {
        "user": "mdouze",
        "created_at": "2019-05-29T14:41:31Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 814,
    "title": "Failed on make shows \"gpu/GpuIndex.o failed\"",
    "created_at": "2019-04-30T09:09:05Z",
    "closed_at": "2019-04-30T09:12:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/814",
    "body": "# \r\nMakefile:44: recipe for target 'gpu/GpuIndex.o' failed\r\nmake: *** [gpu/GpuIndex.o] Error 1\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 --> Ubuntu 16.04\r\nGraphic Card : RTX 2080\r\nPython: 3.5\r\nCuda: 9.0, V9.0.176\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a --> 012954ebbf67b5afba8ef74c07e434852be90e44\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... --> using OpenBLAS\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [X] GPU\r\n\r\nInterface: \r\n- [X] C++\r\n- [X] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\ni use \r\n`LDFLAGS=-L/opt/OpenBLAS/lib ./configure --with-cuda=/usr/local/cuda-9.0 --with-python=/usr/include/python3.5 -with-cuda-arch=\"-gencode=arch=compute_75,code=sm_75\"\r\n`\r\n\r\nand get\r\n\r\n> checking for g++... g++\r\n> checking whether the C++ compiler works... yes\r\n> checking for C++ compiler default output file name... a.out\r\n> checking for suffix of executables... \r\n> checking whether we are cross compiling... no\r\n> checking for suffix of object files... o\r\n> checking whether we are using the GNU C++ compiler... yes\r\n> checking whether g++ accepts -g... yes\r\n> checking whether g++ supports C++11 features with -std=c++11... yes\r\n> checking for gcc... gcc\r\n> checking whether we are using the GNU C compiler... yes\r\n> checking whether gcc accepts -g... yes\r\n> checking for gcc option to accept ISO C89... none needed\r\n> checking how to run the C preprocessor... gcc -E\r\n> checking whether make sets $(MAKE)... yes\r\n> checking for a thread-safe mkdir -p... /bin/mkdir -p\r\n> checking for /usr/include/python3.5... no\r\n> checking for Python C flags... ./configure: line 4138: -c: command not found\r\n> \r\n> checking for swig... swig\r\n> checking how to run the C++ preprocessor... g++ -std=c++11 -E\r\n> checking for grep that handles long lines and -e... /bin/grep\r\n> checking for egrep... /bin/grep -E\r\n> checking for ANSI C header files... yes\r\n> checking for sys/types.h... yes\r\n> checking for sys/stat.h... yes\r\n> checking for stdlib.h... yes\r\n> checking for string.h... yes\r\n> checking for memory.h... yes\r\n> checking for strings.h... yes\r\n> checking for inttypes.h... yes\r\n> checking for stdint.h... yes\r\n> checking for unistd.h... yes\r\n> checking for nvcc... /usr/local/cuda-9.0/bin/nvcc\r\n> checking cuda.h usability... yes\r\n> checking cuda.h presence... yes\r\n> checking for cuda.h... yes\r\n> checking for cublasAlloc in -lcublas... yes\r\n> checking for cudaSetDevice in -lcudart... yes\r\n> checking float.h usability... yes\r\n> checking float.h presence... yes\r\n> checking for float.h... yes\r\n> checking limits.h usability... yes\r\n> checking limits.h presence... yes\r\n> checking for limits.h... yes\r\n> checking stddef.h usability... yes\r\n> checking stddef.h presence... yes\r\n> checking for stddef.h... yes\r\n> checking for stdint.h... (cached) yes\r\n> checking for stdlib.h... (cached) yes\r\n> checking for string.h... (cached) yes\r\n> checking sys/time.h usability... yes\r\n> checking sys/time.h presence... yes\r\n> checking for sys/time.h... yes\r\n> checking for unistd.h... (cached) yes\r\n> checking for stdbool.h that conforms to C99... yes\r\n> checking for _Bool... no\r\n> checking for inline... inline\r\n> checking for int32_t... yes\r\n> checking for int64_t... yes\r\n> checking for C/C++ restrict keyword... __restrict\r\n> checking for size_t... yes\r\n> checking for uint16_t... yes\r\n> checking for uint32_t... yes\r\n> checking for uint64_t... yes\r\n> checking for uint8_t... yes\r\n> checking for stdlib.h... (cached) yes\r\n> checking for GNU libc compatible malloc... yes\r\n> checking for stdlib.h... (cached) yes\r\n> checking for unistd.h... (cached) yes\r\n> checking for sys/param.h... yes\r\n> checking for getpagesize... yes\r\n> checking for working mmap... yes\r\n> checking for clock_gettime... yes\r\n> checking for floor... yes\r\n> checking for gettimeofday... yes\r\n> checking for memmove... yes\r\n> checking for memset... yes\r\n> checking for munmap... yes\r\n> checking for pow... yes\r\n> checking for sqrt... yes\r\n> checking for strerror... yes\r\n> checking for strstr... yes\r\n> checking for g++ -std=c++11 option to support OpenMP... -fopenmp\r\n> checking build system type... x86_64-pc-linux-gnu\r\n> checking host system type... x86_64-pc-linux-gnu\r\n> checking if sgemm_ is being linked in already... no\r\n> checking for sgemm_ in -lmkl_intel_lp64... no\r\n> checking for sgemm_ in -lmkl... no\r\n> checking for sgemm_ in -lopenblas... yes\r\n> checking for cheev_... yes\r\n> checking target system type... x86_64-pc-linux-gnu\r\n> checking for cpu arch... x86_64-pc-linux-gnu CPUFLAGS+=-msse4 -mpopcnt CXXFLAGS+=-m64\r\n> configure: creating ./config.status\r\n> config.status: creating makefile.inc\r\n\r\nbut i try to use `make` and get this error\r\n\r\n> /usr/local/cuda-9.0/bin/nvcc -I /usr/local/cuda-9.0/targets/x86_64-linux/include/ -Xcompiler -fPIC -Xcudafe --diag_suppress=unrecognized_attribute -gencode=arch=compute_75,code=sm_75 -lineinfo -ccbin g++ -std=c++11 -DFAISS_USE_FLOAT16 -g -O3 -c gpu/GpuIndex.cu -o gpu/GpuIndex.o\r\n> nvcc fatal   : Unsupported gpu architecture 'compute_75'\r\n> Makefile:44: recipe for target 'gpu/GpuIndex.o' failed\r\n> make: *** [gpu/GpuIndex.o] Error 1\r\n \r\nwhat make this happened, thank you for the help\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/814/comments",
    "author": "w11m",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-30T09:12:55Z",
        "body": "You need cuda 10 for Turing (compute_75) support."
      }
    ]
  },
  {
    "number": 808,
    "title": "The search result of IndexFlatL2 with top-k=100 is different from the first 100 search result with top-k=10000 ",
    "created_at": "2019-04-28T02:36:15Z",
    "closed_at": "2019-04-29T13:02:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/808",
    "body": "The code is\r\n```\r\nimport numpy as np\r\nimport faiss\r\n\r\nd = 64  # dimension\r\nnb = 100000  # database size\r\nnq = 10000  # nb of queries\r\nnp.random.seed(1234)  # make reproducible\r\nxb = np.random.random((nb, d)).astype('float32')\r\nxb[:, 0] += np.arange(nb) / 1000.\r\nxq = np.random.random((nq, d)).astype('float32')\r\nxq[:, 0] += np.arange(nq) / 1000.\r\n\r\nk = 100\r\n\r\nindex = faiss.IndexFlatL2(d)\r\nindex.add(xb)\r\n\r\nD, I = index.search(xq, k)\r\nD_2, I_2 = index.search(xq, k * 100)\r\n\r\nd = np.argwhere(I != I_2[:, :k])\r\nprint(len(d))\r\n```\r\nThe result is\r\n```\r\n368\r\n```\r\nThe search result of the top-k=100 and the first 100 search result of the top-k=10000 are not the same.\r\nCould you tell the cause of this problem and the solution? Thanks.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/808/comments",
    "author": "chenyihang1993",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-29T09:52:19Z",
        "body": "Most likely ties, what do you get by looking for differences in distances? (`d = np.argwhere(D != D_2[:, :k])`)"
      },
      {
        "user": "chenyihang1993",
        "created_at": "2019-04-29T10:05:22Z",
        "body": "The items in distances seem to be same. The code is \r\n```\r\nimport numpy as np\r\nimport faiss\r\n\r\nd = 64  # dimension\r\nnb = 100000  # database size\r\nnq = 10000  # nb of queries\r\nnp.random.seed(1234)  # make reproducible\r\nxb = np.random.random((nb, d)).astype('float32')\r\nxb[:, 0] += np.arange(nb) / 1000.\r\nxq = np.random.random((nq, d)).astype('float32')\r\nxq[:, 0] += np.arange(nq) / 1000.\r\n\r\nk = 100\r\n\r\nindex = faiss.IndexFlatL2(d)\r\nindex.add(xb)\r\n\r\nD, I = index.search(xq, k)\r\nD_2, I_2 = index.search(xq, k * 100)\r\n\r\nd = np.argwhere(I != I_2[:, :k])\r\nprint(len(d))\r\nd = np.argwhere(D != D_2[:, :k])\r\nprint(len(d))\r\n```\r\nThe result is\r\n```\r\n368\r\n0\r\n```"
      }
    ]
  },
  {
    "number": 807,
    "title": "Daily updated search service",
    "created_at": "2019-04-26T18:12:04Z",
    "closed_at": "2019-04-29T13:01:51Z",
    "labels": [
      "question",
      "out-of-scope"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/807",
    "body": "Hi!\r\n\r\nI'm developing a web app where I need some nearest neighbor search over ~ 100m vectors and I found faiss. As I understand, the most effective way to work with it is to store whole index in RAM. But here comes the problem -- my data should be daily updated and I don't understand how can I implement such functionality: in simple terms I have website's backend python code with variable \"index\" which stores whole index in RAM and I need some way to update it everyday (e.g. with cron). Perhaps you can give me some tip / advice about developing described feature? Thanks a lot!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/807/comments",
    "author": "s-a-nersisyan",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-29T13:01:51Z",
        "body": "It really depends on your use case, and what your constraints are: do you have several servers? can you afford downtime? It is more of a generic systems design question than a Faiss-related question."
      }
    ]
  },
  {
    "number": 805,
    "title": "About inner product and \"IVFxFlat\"",
    "created_at": "2019-04-26T01:30:02Z",
    "closed_at": "2019-04-26T09:55:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/805",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\nHi,I want to get inner products by using \"IVFxFlat\",the L2 quantizer is right? \r\n\r\n    quantizer = faiss.IndexFlatL2(d)\r\n    index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/805/comments",
    "author": "scriptboy1990",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-26T09:55:32Z",
        "body": "You should use an `IndexFlatIP` instead of an `IndexFlatL2` as quantizer."
      }
    ]
  },
  {
    "number": 804,
    "title": "How to understand the nlist parameterï¼Ÿ",
    "created_at": "2019-04-24T11:44:31Z",
    "closed_at": "2019-04-29T13:02:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/804",
    "body": "# Summary\r\nthe sample code of cpp tutorialï¼Œ like this, how to understand the nlist ?\r\n\r\n```\r\nint nlist = 100;\r\nint k = 4;\r\nint m = 8;                             // bytes per vector\r\nfaiss::IndexFlatL2 quantizer(d);       // the other index\r\nfaiss::IndexIVFPQ index(&quantizer, d, nlist, m, 8);\r\n// here we specify METRIC_L2, by default it performs inner-product search\r\nindex.train(nb, xb);\r\nindex.add(nb, xb);\r\n```\r\n\r\nRunning on:\r\n- [ ] CPU\r\n\r\nInterface: \r\n- [ ] C++\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/804/comments",
    "author": "yuxingfirst",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-24T11:49:43Z",
        "body": "All IVF index work by splitting the vectors into `nlist` clusters, according to the quantizer. During search time, only `nprobe` clusters are searched."
      },
      {
        "user": "yuxingfirst",
        "created_at": "2019-04-24T13:03:10Z",
        "body": "> All IVF index work by splitting the vectors into `nlist` clusters, according to the quantizer. During search time, only `nprobe` clusters are searched.\r\n\r\nThanks your replyï¼Œ i got that."
      }
    ]
  },
  {
    "number": 800,
    "title": "Error saving index created by IndexBinaryIVF",
    "created_at": "2019-04-22T14:30:20Z",
    "closed_at": "2019-04-23T08:11:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/800",
    "body": "my basic code:\r\nquantizer = faiss.IndexBinaryFlat(d)\r\nindex = faiss.IndexBinaryIVF(quantizer, d, nlist)\r\nindex.train(training)\r\nindex.add(db)\r\nfaiss.write_index(index,'./')\r\n\r\nError:\r\nNotImplementedError: Wrong number or type of arguments for overloaded function 'write_index'.\r\n  Possible C/C++ prototypes are:\r\n    faiss::write_index(faiss::Index const *,char const *)\r\n    faiss::write_index(faiss::Index const *,FILE *)\r\n    faiss::write_index(faiss::Index const *,faiss::IOWriter *)\r\n\r\n\r\nAnd my faiss version: v1.5.0\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/800/comments",
    "author": "yyincc",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-22T15:46:29Z",
        "body": "For binary indexes you should use the `write_index_binary` function.\r\n"
      }
    ]
  },
  {
    "number": 799,
    "title": "How can I search a tsv file instead of an h5 file? Is converting the only way?",
    "created_at": "2019-04-20T20:09:41Z",
    "closed_at": "2019-05-02T14:00:16Z",
    "labels": [
      "question",
      "out-of-scope"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/799",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/799/comments",
    "author": "JasonLLu",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-20T20:32:50Z",
        "body": "Iâ€™m not sure I understand your question, but it does not seem to be about Faiss. Could you be more specific?"
      },
      {
        "user": "JasonLLu",
        "created_at": "2019-04-20T20:59:21Z",
        "body": "Sorry, I guess my question is not directly related to Faiss. If I am trying to read a tsv file into and instance of IndexFlatL2() (as opposed to an h5 file), how would I approach that? \r\n\r\n"
      },
      {
        "user": "beauby",
        "created_at": "2019-04-23T10:09:41Z",
        "body": "Whatever the format of your data, you should make it into a numpy array with shape `(n, d)` (`n` number of vectors, `d` dimension of vectors), and then feed it to Faiss."
      },
      {
        "user": "Jorgegs102",
        "created_at": "2022-12-15T22:33:50Z",
        "body": "Good evening!\r\ndid you manage to convert a h5 model to TSV? how did you do it?\r\n\r\nThanks!"
      }
    ]
  },
  {
    "number": 798,
    "title": "How to export the result of search?",
    "created_at": "2019-04-19T08:27:59Z",
    "closed_at": "2019-05-29T14:38:20Z",
    "labels": [
      "question",
      "out-of-scope"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/798",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [X] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [X] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\n\r\nHi, We are facing the problem of exporting the searching result. We try to build the knn graph of 300M records, k = 100. Due to the memory limit of GPU and CPU, IndexShards was used for 8 GPUs.  We transformed the searching result D and I to adjacent table and exported the table to csv file for the following spark jobs. But the speed of exporting is very poor, for a sample dataset of 5M users and 100 neighbors for each user, it took 40mins for exporting the 500M rows to the disk. Is there any efficient way to do this?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/798/comments",
    "author": "Kimi-n",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-05-29T14:38:20Z",
        "body": "Using csv or any form of text to store vectors or matrices is slow. "
      }
    ]
  },
  {
    "number": 797,
    "title": "Write IndexFlatL2 in append mode",
    "created_at": "2019-04-18T13:56:29Z",
    "closed_at": "2019-04-18T14:06:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/797",
    "body": "# Summary\r\nIs it possible to write a FAISS .index file in append mode ? Assume i need to add new vectors on the fly to the existing index file on disk.\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n1.5.1\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/797/comments",
    "author": "srijiths",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-18T14:06:40Z",
        "body": "Yes, you can call `index.add(...)` multiple times."
      },
      {
        "user": "srijiths",
        "created_at": "2019-04-18T14:21:58Z",
        "body": "What i meant is like a file append mode to .index file on disk. Updated the question."
      },
      {
        "user": "beauby",
        "created_at": "2019-04-18T15:06:20Z",
        "body": "In that case no, as it stands you will have to re-write the whole index."
      },
      {
        "user": "namhong1412",
        "created_at": "2021-07-08T01:41:14Z",
        "body": "can you tell me how to append the index file to disk?"
      }
    ]
  },
  {
    "number": 794,
    "title": "Add additional attributes",
    "created_at": "2019-04-15T05:38:27Z",
    "closed_at": "2019-04-15T08:38:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/794",
    "body": "Hi ,\r\nAs far as I get faiss provide index based similarity search which will return me index and distance (I,D) .I also want to store/add UID(a simple unique number for each vector) corresponding to each vector so that I can map it back to original image .\r\nIs there any way it can be done?\r\n\r\nThanks \r\nAayush\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/794/comments",
    "author": "Aayushktyagi",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-15T08:38:17Z",
        "body": "Faiss adds vectors sequentially in the order you provide them, starting with id 0, so you can maintain a mapping between your images and Faiss internal ids."
      }
    ]
  },
  {
    "number": 792,
    "title": "undefined reference to `faiss::gpu::bruteForceKnn(faiss::gpu)",
    "created_at": "2019-04-14T15:56:29Z",
    "closed_at": "2019-04-23T10:13:42Z",
    "labels": [
      "question",
      "install"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/792",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [ yes] GPU\r\n\r\nInterface: \r\n- [yes ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\nWhen I wrote a simple code about exact Knn on GPU, the error returns â€œundefined reference to `faiss::gpu::bruteForceKnn(faiss::gpu)â€. Note that the example demo_ivfpq_indexing_gpu.cpp in gpu/test works well. \r\nCould you help me figure out whats wrong? \r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/792/comments",
    "author": "ParkWANG",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-14T17:47:49Z",
        "body": "Could you post the content of your `makefile.inc`?"
      },
      {
        "user": "ParkWANG",
        "created_at": "2019-04-15T02:00:13Z",
        "body": "> Could you post the content of your `makefile.inc`?\r\n\r\n```\r\nCXX          = g++ -std=c++11\r\nCXXCPP       = g++ -std=c++11 -E\r\n# TODO: Investigate the LAPACKE wrapper for LAPACK, which defines the correct\r\n#   type for FORTRAN integers.\r\nCPPFLAGS     = -DFINTEGER=int\r\nCXXFLAGS     = -fPIC -fopenmp -m64 -Wno-sign-compare -g -O3 -Wall -Wextra\r\nCPUFLAGS     = -msse4 -mpopcnt\r\nLDFLAGS      = -fopenmp\r\nLIBS         = -lopenblas\r\nPYTHONCFLAGS = -I/usr/include/python2.7 -I/usr/include/x86_64-linux-gnu/python2.7 -I/home/lishuai/anaconda3/lib/python3.7/site-packages/numpy/core/include\r\n\r\nNVCC         = /usr/local/cuda/bin/nvcc\r\nNVCCLDFLAGS  = -L/usr/local/cuda/lib64 \r\nNVCCLIBS     = -lcudart -lcublas -lcuda \r\nCUDAROOT     = /usr/local/cuda\r\nCUDACFLAGS   = -I/usr/local/cuda/include \r\nNVCCFLAGS    = -I $(CUDAROOT)/targets/x86_64-linux/include/ \\\r\n-Xcompiler -fPIC \\ \r\n-Xcudafe --diag_suppress=unrecognized_attribute \\\r\n-gencode arch=compute_35,code=\"compute_35\" \\\r\n-gencode arch=compute_52,code=\"compute_52\" \\\r\n-gencode arch=compute_60,code=\"compute_60\" \\\r\n-gencode arch=compute_61,code=\"compute_61\" \\\r\n-lineinfo \\ \r\n-ccbin $(CXX) -DFAISS_USE_FLOAT16\r\n    \r\nOS = $(shell uname -s)\r\n    \r\nSHAREDEXT   = so\r\nSHAREDFLAGS = -shared\r\n\r\nifeq ($(OS),Darwin)\r\n        SHAREDEXT   = dylib\r\n        SHAREDFLAGS = -dynamiclib -undefined dynamic_lookup\r\nendif\r\n\r\nMKDIR_P      = /bin/mkdir -p\r\nPYTHON       = python\r\nSWIG         =\r\n\r\nprefix      ?= /usr/local\r\nexec_prefix ?= ${prefix}\r\nlibdir       = ${exec_prefix}/lib\r\nincludedir   = ${prefix}/include\r\n```                                      \r\n\r\nThe makefile.inc is above. \r\nIt is ok when I run the cpu version, but gpu version, only the demo in gpu/test/ works. \r\nIt is something wrong about install? \r\n"
      },
      {
        "user": "beauby",
        "created_at": "2019-04-15T08:37:01Z",
        "body": "Could you show me how you are linking your *simple code about exact Knn on GPU*?"
      },
      {
        "user": "ParkWANG",
        "created_at": "2019-04-15T08:40:03Z",
        "body": "> Could you show me how you are linking your _simple code about exact Knn on GPU_?\r\nYes. The simple code is below: \r\n\r\n#include <iostream>\r\n#include <cmath>\r\n#include <cstdlib>\r\n\r\n#include <sys/time.h>\r\n\r\n#include \"../StandardGpuResources.h\"\r\n#include \"../GpuDistance.h\"\r\n\r\nint main(int argc, const char * argv[]) {\r\n    // insert code here...\r\n    std::cout << \"Hello, World!\\n\";\r\n    \r\n    int d=64;\r\n    int nb=100000;\r\n    int nq=10000;\r\n    int k=10;\r\n    float *xb=new float[d * nb];\r\n    float *xq=new float[d * nq];\r\n    \r\n    float *ot=new float[nq * k];\r\n    \r\n    \r\n    for(int i=0; i < nb; i++){\r\n        for(int j=0; j< d; j++)\r\n        {\r\n            xb[d*i +j]=drand48();\r\n        }\r\n        xb[d * i] += i/1000.;\r\n    }\r\n    \r\n    for(int i=0; i < nq; i++){\r\n        for(int j=0; j< d; j++)\r\n        {\r\n            xq[d*i +j]=drand48();\r\n        }\r\n        xq[d * i] += i/1000.;\r\n    }\r\n    \r\n    faiss::gpu::StandardGpuResources res;\r\n \r\n    faiss::Index::idx_t *outIndices;\r\n     \r\n    faiss::MetricType METRIC_L2;\r\n    //Compute\r\n    bruteForceKnn(&res, METRIC_L2, xb,nb,xq,nq,d, k, ot, outIndices);\r\n\r\n    return 0;\r\n}"
      },
      {
        "user": "beauby",
        "created_at": "2019-04-15T08:41:37Z",
        "body": "Thanks, could you show me the command line you use to compile/link this?"
      },
      {
        "user": "ParkWANG",
        "created_at": "2019-04-15T08:45:39Z",
        "body": "        \r\n\r\n> Thanks, could you show me the command line you use to compile/link this?\r\n\r\nI used Makefile (in which knn_exact_example.cpp is my main file): \r\n-include ../../makefile.inc\r\n\r\n%.o: %.cpp\r\n        $(CXX) $(CUDACFLAGS) -o $@ -c $^\r\n\r\nknn_exact_example: knn_exact_example.o ../libgpufaiss.a ../../libfaiss.a\r\n        $(CXX) $(LDFLAGS) $(NVCCLDFLAGS) -o $@ $^ $(LIBS) $(NVCCLIBS)\r\n\r\nclean:\r\n        rm -f *.o knn_exact_example\r\n\r\n.PHONY: clean\r\n~         "
      },
      {
        "user": "beauby",
        "created_at": "2019-04-15T09:03:02Z",
        "body": "This looks like an outdated version of the Faiss Makefile. Could you try\r\nwith the latest version?"
      },
      {
        "user": "ParkWANG",
        "created_at": "2019-04-15T09:04:45Z",
        "body": "> This looks like an outdated version of the Faiss Makefile. Could you try with the latest version?\r\n\r\nsure. Thank you.  "
      },
      {
        "user": "beauby",
        "created_at": "2019-04-23T10:13:42Z",
        "body": "As there is no more activity since, I am assuming this is resolved by using the latest version of Faiss, so I will close this issue. Feel free to reopen if needed."
      }
    ]
  },
  {
    "number": 790,
    "title": "Is faiss only a library and I have to implement other server function by myself?",
    "created_at": "2019-04-14T13:35:21Z",
    "closed_at": "2019-04-14T17:50:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/790",
    "body": "I just built the faiss on my server and test its performance. I love its fast speed! But when I tried to find out more information about serving such as dumping backup, distributed calculation, nothing was revealed. Does it mean that faiss is only a 'library' just like eigen. And I have to implement its 'serving' by myself, for instance, integrating with ps-lite?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/790/comments",
    "author": "Traeyee",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-14T17:50:11Z",
        "body": "Faiss is a library, so if you want to build a kNN service, you will have to handle the service part yourself."
      }
    ]
  },
  {
    "number": 759,
    "title": "Question on the cost time of searching?",
    "created_at": "2019-03-31T12:29:47Z",
    "closed_at": "2019-04-23T17:27:48Z",
    "labels": [
      "question",
      "cant-repro"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/759",
    "body": "The same faiss program ran on different machines with the same hardware configuration, but the cost time of searching is different, one is about 2ms and the other one is about 80ms. I have export OMP_WAIT_POLICY=PASSIVE before running the executable, but it does not work. Could you give me some suggestion to debug. Thank you very much!\r\n\r\nRunning on:\r\n- [ ] CPU\r\n\r\nInterface: \r\n- [ ] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/759/comments",
    "author": "mingrenbuke",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-03-31T13:50:08Z",
        "body": "Do you have a minimal example exhibiting this behavior?"
      },
      {
        "user": "beauby",
        "created_at": "2019-04-23T17:27:48Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 757,
    "title": "metric_type option",
    "created_at": "2019-03-29T09:20:14Z",
    "closed_at": "2019-03-29T10:55:25Z",
    "labels": [
      "question",
      "cant-repro"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/757",
    "body": "What does **metric_type** option mean when initializing an index? As I understand 0='IP', 1='L2', but it doesn't change when I create index with parameter _METRIC_INNER_PRODUCT_ or  _METRIC_L2_ in index_factory()",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/757/comments",
    "author": "sankovalev",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-03-29T09:36:06Z",
        "body": "Could you post the specific string you use with `index_factory()`?"
      },
      {
        "user": "sankovalev",
        "created_at": "2019-03-29T09:42:34Z",
        "body": "This code\r\n```\r\nindex_l2 = faiss.index_factory(d, \"IDMap,HNSW64\", faiss.METRIC_L2)\r\nindex_ip = faiss.index_factory(d, \"IDMap,HNSW64\", faiss.METRIC_INNER_PRODUCT)\r\n\r\nprint(index_l2.metric_type, index_ip.metric_type)\r\n```\r\noutputs **1,1**\r\n\r\n<hr>\r\nThere may be a problem in HNSW?"
      },
      {
        "user": "beauby",
        "created_at": "2019-03-29T10:46:28Z",
        "body": "HNSW does not support any other metric than L2, hence your issue."
      },
      {
        "user": "sankovalev",
        "created_at": "2019-03-29T10:55:07Z",
        "body": "I know that original HNSW only works with L2, so the question came up.\r\nSuppose it should display an error when initializing, as in the case of other indexes."
      }
    ]
  },
  {
    "number": 752,
    "title": "Is it possible to search and reconstruct when the index and the query are on the gpu?",
    "created_at": "2019-03-27T09:02:43Z",
    "closed_at": "2019-05-29T14:30:08Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/752",
    "body": "# Summary\r\nHi,\r\nIs it somehow possible to search and reconstruct a GpuIndexFlatIP with a query placed on the gpu (pytorch tensor) ? (In opposed to just searching the index)\r\n\r\nThe main motivation for this is to overcome the need to keep a copy of the index database points on the gpu.\r\n\r\n- [ ] CPU\r\n- [x ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ x] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/752/comments",
    "author": "matanatz",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-03-27T11:07:52Z",
        "body": "The best way is to short-circuit the GpuIndex structure altogether, see #725. "
      },
      {
        "user": "mdouze",
        "created_at": "2019-05-29T14:30:08Z",
        "body": "No activity, closing. \r\n"
      }
    ]
  },
  {
    "number": 742,
    "title": "How to achieve good recall for Product Quantization ",
    "created_at": "2019-03-11T11:18:09Z",
    "closed_at": "2019-05-29T14:30:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/742",
    "body": "Hi, I am using HNSW_PQ256 and get a good method for searching. Recall for 20 nearest neighbours is around 50% and the query time is fast.\r\nI am trying to build the code from scratch. However, I am wondering how the approximate distance based on product quantization can show reliable results. My concern is that the data has been compressed a lot and the distance will be affected significantly.\r\nI have tried to use Inverted index and residual but the performance is so low (less than 5%). Even though I can find a set of points located in Voronoi cells near the query point, I can not use the distance generated by PQ to get the acceptable list of nearest neighbours. So my questions are:\r\n- Do you use any other method from PQ (such as OPQ or Polysemous) in HNSW_PQ256 to retain the recall?\r\n- What is the most important factor to reach a good recall? Is that the K-means process in quantization? Should I increase the number of iteration in K-means of PQ to increase recall",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/742/comments",
    "author": "vdv1g16",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-03-11T12:25:27Z",
        "body": "To use OPQ, do OPQ256,HNSW32_PQ256 which may be more accurate. \r\nA faster option is HNSW32_SQ256, but it is not as accurate.\r\n\r\nNote that the recall depends on two parameters: \r\n- the accuracy of the encoding (Flat is no encoding -> no loss in accuracy)\r\n- the depth of the search (efSearch parameter of HNSW or nprobe for the IVF variants). You can also try to increase this parameter."
      },
      {
        "user": "mdouze",
        "created_at": "2019-05-29T14:30:28Z",
        "body": "No activity, closing. "
      }
    ]
  },
  {
    "number": 740,
    "title": "in method 'IndexBinary_range_search', argument 3 of type 'uint8_t const *'",
    "created_at": "2019-03-08T12:44:08Z",
    "closed_at": "2019-03-18T02:34:09Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/740",
    "body": "# Summary\r\n\r\nin method 'IndexBinary_range_search', argument 3 of type 'uint8_t const *'\r\n\r\n# Platform\r\n\r\n-- macOS 10.14\r\n\r\nFaiss version: 1.4.0\r\n\r\nRunning on:\r\n- [ ] CPU\r\n\r\nInterface: \r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n# program:\r\nindex = faiss.IndexBinaryFlat(64)\r\nxb = np.random.randint(low=0,high=255,size=(1000,64//8)).astype(np.uint8)\r\nindex.add(xb)\r\nxq = np.random.randint(low=0,high=255,size=(2,64//8)).astype(np.uint8)\r\nresult=[]\r\nindex.range_search(len(xq),xq,2,result)\r\n# error:\r\nreturn _swigfaiss.IndexBinary_range_search(self, n, x, radius, result)\r\n\r\nTypeError: in method 'IndexBinary_range_search', argument 3 of type 'uint8_t const *'\r\n\r\nIn python, how I give it a parameter of type 'uint8_t const *'?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/740/comments",
    "author": "TFDeeplearner",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-03-08T13:24:34Z",
        "body": "Range search is currently not implemented on binary indices."
      },
      {
        "user": "mdouze",
        "created_at": "2019-03-08T15:06:04Z",
        "body": "In case it is useful, here is a function that computes the pairwise distances between two matrices: \r\n```python\r\ndef pairwise_hamming_dis(a, b):\r\n    \"\"\" compute the pairwise Hamming distances between two matrices \"\"\"\r\n    na, d = a.shape\r\n    nb, d2 = b.shape\r\n    assert d == d2\r\n\r\n    dis = np.empty((na, nb), dtype='int32')\r\n\r\n    faiss.hammings(\r\n        faiss.swig_ptr(a), faiss.swig_ptr(b),\r\n        na, nb, d,\r\n        faiss.swig_ptr(dis)\r\n    )\r\n    return dis\r\n```\r\nThen the range search can be performed with \r\n\r\n```\r\ndis =  pairwise_hamming_dis(xq, xb)\r\nI, J = np.where(dis < threshold)\r\n```\r\nIt is not ideal but definitely faster than doing the loops in Python."
      }
    ]
  },
  {
    "number": 707,
    "title": "cudaMalloc error when calling add_with_ids.",
    "created_at": "2019-01-25T07:38:00Z",
    "closed_at": "2019-02-27T09:56:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/707",
    "body": "### Summary\r\n\r\nHi all, I was trying to build faiss index with 140 million 20-d vectors and each vector has an id of type int64.\r\nAfter loading all the data into memory, I created Index using `Index* index = index_factory(20,  \"OPQ20_20,IVF12000,PQ20\")`. Things worked fine so far. \r\nThen I called `index->add_with_ids(len_, data_vec_, ids_);` and I got \r\n\r\n> Faiss assertion 'cudaMalloc(p, size) == cudaSuccess' failed in void faiss::gpu::allocMemorySpace(faiss::gpu::MemorySpace, void**, size_t) at utils/MemorySpace.cpp:19; details: Failed to cudaMalloc 131072 bytes\r\n\r\nIt seems like a memory error. My GPU is NVIDIA Tesla P40 with memory 24GB. The memory space is not enough for raw data but it's fine for the compressed vectors with a 8 Bytes id each.\r\n\r\nBTW, I tried to convert the GPU index to CPU index using `index_gpu_to_cpu()`. And I called `add_with_ids()` using CPU index. Then the time cost of this method may be hours and maybe it's too slow? Is there anything I can do to optimize this?\r\n\r\nThanks!\r\n\r\n### Platform\r\n\r\nOS: Cent OS 7\r\nGPU: NVIDIA Tesla P40 24GB in memory\r\n\r\nFaiss version: Release 1.5.0\r\n\r\nFaiss compilation options: nothing special `-lglog` added only\r\n\r\nRunning on:\r\n- [âˆš] CPU\r\n- [âˆš] GPU\r\n\r\nInterface: \r\n- [âˆš] C++\r\n- [ ] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/707/comments",
    "author": "ParanoiaUPC",
    "comments": [
      {
        "user": "ParanoiaUPC",
        "created_at": "2019-01-28T09:26:05Z",
        "body": "any ideas? :)"
      },
      {
        "user": "mdouze",
        "created_at": "2019-02-14T10:15:07Z",
        "body": "Sorry, catching up on this. It is not normal that this fails, 140M vectors with PQ20 takes < 3 GB. \r\nDid you try adding the vectors by batches of say 1M to see where it breaks?"
      },
      {
        "user": "ParanoiaUPC",
        "created_at": "2019-02-20T03:11:46Z",
        "body": "I'll try, thanks a lot."
      },
      {
        "user": "mdouze",
        "created_at": "2019-02-27T09:56:31Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 706,
    "title": "'GPU flat' vs 'IVFFlatScan'",
    "created_at": "2019-01-25T07:34:19Z",
    "closed_at": "2019-02-14T10:12:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/706",
    "body": "ã€GPU flat uses cuBLAS GEMM for efficiency to do -2q . c, and then adds in separately ||c||^2 then performs k-selection.\r\n\r\nIVFFlatScan doesn't use GEMM, so it computes the whole thing independently.\r\n\r\nAs vectors, (a - b) . (a - b) = a.a - 2a.b = b.b = ||a||^2 - 2a . b + ||b||^2, so they compute the same thing. ã€‘\r\n\r\n@wickedfoo  Thank you very much !\r\n\r\nMy question is which of the two methods is high performance, if flat's way performance is higher, why ivfflat is not usedï¼ŸWhat scenarios do the two methods apply to?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/706/comments",
    "author": "c-captain",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-02-14T10:12:53Z",
        "body": "We use BLAS only for matrix-matrix operations, not matrix-vector or vector-vector, which is the case for the scanning code."
      }
    ]
  },
  {
    "number": 694,
    "title": "Remove shard from meta index",
    "created_at": "2019-01-16T04:23:33Z",
    "closed_at": "2019-12-02T14:01:04Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/694",
    "body": "# Summary\r\n\r\nAdd support for removing a shard from IndexShards.\r\n\r\n# Platform\r\n\r\nRunning on:\r\n- [ ] CPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\nWhile maintaining a meta_index with a bunch of shard indices, if a shard is updated with newer ids, it can be easier to remove a particular shard from the meta_index and add it back so as to minimize the downtime.\r\n\r\nAn API such as:\r\n```remove_shard(Index *)```\r\nmight be helpful. This API would swap the index to be removed with the last index in the shard_indexes vector and then remove the last element from the vector effectively reducing the size of the vector by 1.\r\n\r\nAn alternative way to accomplish the same would be to reinitialize the meta_index by reading all the shards and adding it to the meta_index.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/694/comments",
    "author": "bvarghese1",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-01-16T12:23:49Z",
        "body": "You can also just rebuild the `IndexShards` from scratch, this is an O(1) operation.  \r\nSo I will set this suggestion as low-priority. "
      },
      {
        "user": "bvarghese1",
        "created_at": "2019-01-16T17:24:48Z",
        "body": "@mdouze Yes, the \"alternative way\" refers to that where one can read all the shard indices from disk and make a corresponding call to ```add_shard(Index *)``` API. However, I am planning to run this as a search service and this might cause the entire search service to be temporarily unavailable for a brief second. The other approach might mitigate this issue by only making the relevant shard index temporarily unavailable. Hence, the suggestion. I am happy to work on this."
      },
      {
        "user": "mdouze",
        "created_at": "2019-01-16T17:51:01Z",
        "body": "You will need to freeze the search service anyway during the remove_shard. In both cases it will last less than 1 ms (not a second).\r\n"
      },
      {
        "user": "bvarghese1",
        "created_at": "2019-01-16T19:27:50Z",
        "body": "@mdouze Yes! I think its more complicated than I thought."
      },
      {
        "user": "mdouze",
        "created_at": "2019-12-02T14:01:04Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 680,
    "title": "Did something change between v1.5.0 and v1.4.0 to speed up single vector searches?",
    "created_at": "2018-12-31T21:59:50Z",
    "closed_at": "2019-01-16T03:43:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/680",
    "body": "# Summary\r\n\r\nI'm using \"Flat\" index, always doing comprehensive search, single vector at a time, CPU based search, vector dimension varies (256 or 512 or 1024), metric type varies (inner product or euclidean distance), total records about 600,000 per server (and a few hundred of those), plenty of memory and CPU.  This is pretty much the worse case for FAISS performance.\r\n\r\nFAISS version v1.4.0 \r\n\r\nDid something change between v1.4.0 and v1.5.0 that would speed up single vector searches?  I've been tasked with figuring out anything we can do to speed up FAISS searches, so if going to v1.5.0 would help that would be good to know.  It's non-trivial for me to go to 1.5.0 but if it's faster I'll make it happen.\r\n\r\nThank you very much\r\n\r\n# Platform\r\n\r\nLinux\r\n\r\nRunning on:\r\n- [ x ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ x ] C++\r\n- [ ] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/680/comments",
    "author": "gamma0577215",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-01-07T16:36:20Z",
        "body": "No I am afraid it is still the same code. \r\nThere is not much room for improvement if batching is not used because the limiting factor is memory bandwidth. "
      },
      {
        "user": "beauby",
        "created_at": "2019-01-11T16:31:34Z",
        "body": "Note that Faiss parallelizes over queries, but you could shard your index so that each shard is searched in parallel, and later merge the results."
      },
      {
        "user": "gamma0577215",
        "created_at": "2019-01-12T14:05:43Z",
        "body": "Yes, we do shard across cores and machines.\r\n\r\nIn the case of flat inner product or flat euclidean, both with exhaustive search, I wonder if this idea would help speed things up:\r\n\r\nSuppose I have a 1024-D database with N vectors and query for all entries with a 1024-D vector.  Is that equivalent to splitting the database into two 512-D based databases (DX, DY) each with N \"half vectors\" (DX with columns 0-511, DY with columns 512-1023) and querying them with two 512-D based vectors (VX, VY).  VX would query against DX and VY would query against DY, then the resulting inner products can be added together.  Mathematically it should be equivalent, technologically I don't know if it's any faster.  And it couldn't extend to other index types that don't have this additive property."
      },
      {
        "user": "mdouze",
        "created_at": "2019-01-14T09:07:18Z",
        "body": "No it is not equivalent, because the result of a search is a short-list of k nearest vectors and it is unlikely that the k nearest vectors of the first half are the same as the k nearest vectors of the second half. So there would be nothing to add.\r\nNoting that you have very large vectors and running into performance issues, I would recommend that you consider approximate search. Ask yourself: would a 10x increase in speed make a 1% drop in accuracy acceptable?"
      },
      {
        "user": "gamma0577215",
        "created_at": "2019-01-15T00:08:05Z",
        "body": "Exhaustive search, as mentioned, shouldn't have that issue, correct?  If, for some reason, an answer ID/score, didn't appear for both the first half and second half then it could be skipped with a warning.  We could also just have N/2 full size (1024-D) records in two databases and it should work about as well.\r\n\r\nWe are indeed investigating which approximate search would work best for us, as well as other performance optimizations.\r\n\r\nWe don't have a way currently to train with the same data set across servers.  For now our training set will be a large random sample per server, it's not ideal but that's what we're doing for now.\r\n\r\nThanks for your time."
      },
      {
        "user": "mdouze",
        "created_at": "2019-01-15T09:07:42Z",
        "body": "> Exhaustive search, as mentioned, shouldn't have that issue, correct? If, for some reason, an answer ID/score, didn't appear for both the first half and second half then it could be skipped with a warning. \r\n\r\n\"Exhaustive search, as mentioned, shouldn't have that issue, correct?\" -> Exhaustive search does not return the whole dataset ranked (or it could, but it would be outrageously inefficient). If you have 600k vectors per server and ask for 1000 nearest neighbors, then the expectation of intersection size between the first and the second list of results is 1000 * (1000 / 600k) = 1.6 results, ie. you would have 998 warnings and 1-2 legitimate results per query! \r\n\r\nIf the intersection is much larger than that, it means that the first 512D and the second 512D of the vectors are strongly correlated, so you would be better off using dimensionality reduction in the first place. \r\n\r\n> We could also just have N/2 full size (1024-D) records in two databases and it should work about as well.\r\n\r\nYes, that is what @beauby suggested. You can wrap the two datasets in an `IndexShards`, that will merge the results for you. "
      },
      {
        "user": "gamma0577215",
        "created_at": "2019-01-16T03:43:05Z",
        "body": "For now we score every database vector against the single query vector.  Right now we're not concerned about that being inefficient, we're more concerned about completeness.  Thanks again."
      }
    ]
  },
  {
    "number": 647,
    "title": "the problem of small symmetric pertubation",
    "created_at": "2018-11-28T06:55:06Z",
    "closed_at": "2018-11-28T22:54:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/647",
    "body": "```c++\r\n\r\n/*ã€faissã€‘ small symmetric pertubation. Much better than  */\r\n            for (size_t j = 0; j < d; j++) {\r\n                if (j % 2 == 0) {\r\n                    centroids[ci * d + j] *= 1 + EPS;\r\n                    centroids[cj * d + j] *= 1 - EPS;\r\n                } else {\r\n                    centroids[ci * d + j] *= 1 - EPS;\r\n                    centroids[cj * d + j] *= 1 + EPS;\r\n                }\r\n            }\r\n```\r\n```c++\r\n#include <iostream>\r\nint main()\r\n{\r\n        using namespace std;\r\n        float i = 1.0;\r\n        i *= 1 + 1/1024.;\r\n        i *= 1 - 1/1024.;\r\n        cout << i << endl;\r\n        float j = 1.0;\r\n        j *= 1 - 1/1024.;\r\n        j *= 1 + 1/1024.;\r\n        cout << j << endl;\r\n\r\n        return 0;\r\n}\r\n```\r\n\r\n\r\nthe result is all 0.999 for i and j. Therefore,what's the meaning of it in faiss?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/647/comments",
    "author": "quoniammm",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2018-11-28T22:54:01Z",
        "body": "Note that you are updating the same value i:\r\n```\r\ni *= 1 + 1/1024.;\r\ni *= 1 - 1/1024.;\r\n```\r\n\r\nwhereas the Faiss code does not (the index ci * d + j != cj * d + j):\r\n```\r\ncentroids[ci * d + j] *= 1 + EPS;\r\ncentroids[cj * d + j] *= 1 - EPS;\r\n```"
      }
    ]
  },
  {
    "number": 641,
    "title": "Pre processing an Index to use custom ID's and searches",
    "created_at": "2018-11-16T18:50:33Z",
    "closed_at": "2018-12-03T15:56:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/641",
    "body": "# Summary\r\n\r\nI am using IndexFlatIP and I understand that the ID of each vector will be the position at the index it assumes after being added. As a possible solution to use custom ID's, I've found about IndexIDMap, that allows to use a function called add_with_ids. This structure also has its own searching function, that implicitly searches at the original index, which makes me think that the searching can be done directly over the IndexIDMap variable and we will have the same performance.\r\n\r\nI would like to know if my understandings are correct and if there is a way to use also custom string ID's.\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/641/comments",
    "author": "carlosost",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-11-20T12:20:28Z",
        "body": "Your understanding is correct. If you you want to use string IDs, you'll have to manage the mapping from ids to strings in the calling code."
      },
      {
        "user": "Daemonyz",
        "created_at": "2018-11-27T09:52:53Z",
        "body": "> Your understanding is correct. If you you want to use string IDs, you'll have to manage the mapping from ids to strings in the calling code.\r\n\r\nHi, maybe a similar question, can we add new data with a dict-like format, that's the KEY are String type? Or the users MUST do the <String type key, Int type key> mappings by themself? @mdouze "
      },
      {
        "user": "mdouze",
        "created_at": "2018-11-27T12:12:37Z",
        "body": "Yes the users should do the mapping themselves."
      },
      {
        "user": "Daemonyz",
        "created_at": "2018-11-28T04:41:20Z",
        "body": "> Yes the users should do the mapping themselves.\r\n\r\n@mdouze , ok, got it, so does FAISS plan to support this feature(use String type as the index ids, like \"Dict\" data structure)?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-11-29T10:37:39Z",
        "body": "No. Faiss is not intended as a full-featured DBMS."
      },
      {
        "user": "beauby",
        "created_at": "2018-12-03T15:56:29Z",
        "body": "Closing, as the initial question has been addressed. Feel free to keep commenting if you have further questions."
      },
      {
        "user": "Daemonyz",
        "created_at": "2018-12-03T15:59:42Z",
        "body": "> No. Faiss is not intended as a full-featured DBMS.\r\n\r\nOK, thanks for your reply."
      },
      {
        "user": "sjakati98",
        "created_at": "2019-01-18T14:21:20Z",
        "body": "@mdouze I'm a bit new to this codebase. How would one go about handling their own mapping?"
      },
      {
        "user": "beauby",
        "created_at": "2019-01-18T16:23:21Z",
        "body": "@sjakati98 The internal ids are sequential, so you just have to maintain a map from your custom ids to internal ids. You can use `index.ntotal` in order to know the next internal id."
      },
      {
        "user": "sjakati98",
        "created_at": "2019-01-19T01:21:21Z",
        "body": "> @sjakati98 The internal ids are sequential, so you just have to maintain a map from your custom ids to internal ids. You can use `index.ntotal` in order to know the next internal id.\r\n\r\nThat seems simple enough. Thanks!"
      }
    ]
  },
  {
    "number": 639,
    "title": "Kmeans faiss distance",
    "created_at": "2018-11-15T17:18:54Z",
    "closed_at": "2018-11-28T16:15:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/639",
    "body": "Hi,\r\n\r\nWe are wondering which metric you are using to calculate the distances in kmeans algorithm. We have realized that the calculated distances are not normalized since depending on the input data the distances varies a lot. We would like to normalize the obtained distances so that the 1 value means the furthest distance. Is that possible? We could normalize it by knowing the maximum possible distance value but this seems very data dependent and it is difficult to guess it without knowing the used metric.\r\nThanks,\r\n\r\noriol\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/639/comments",
    "author": "oguitart",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-11-16T12:48:39Z",
        "body": "In the kmeans algorithm, the distance used to assign a vector to a centroid is the L2 distance (squared). \r\n\r\nHow to normalize distances is application-dependent. "
      }
    ]
  },
  {
    "number": 638,
    "title": "Dose faiss support user defined distance function?",
    "created_at": "2018-11-15T09:05:20Z",
    "closed_at": "2018-11-28T16:14:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/638",
    "body": "Dose faiss support customized distance function as (metric='pyfunc') do in sklearn.neighbors.NearestNeighbors() ?\r\nThanks a lot!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/638/comments",
    "author": "xieyi318",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-11-15T12:35:45Z",
        "body": "No, the only \"distance\" functions supported are L2 and inner product."
      },
      {
        "user": "jpetot",
        "created_at": "2021-05-21T14:18:56Z",
        "body": "hi @beauby  ,  Same question, is it still not possible to use custom distance function ? \r\nMany thanks if it would be possible ! "
      },
      {
        "user": "phoenixzw",
        "created_at": "2021-08-17T12:42:34Z",
        "body": "Hi @jpetot , did you figure out a solution to use user defined distance function? I meet the same problem.\r\nThanks!"
      },
      {
        "user": "Enet4",
        "created_at": "2021-08-17T14:08:08Z",
        "body": "This seems like a fairly frequent question, but it makes sense that this functionality is not present. Most dense vector indexing algorithms only make sense under a specific distance function. Even if Faiss enabled users to set an arbitrary distance function, the various index types and index splitting solutions available (HNSW, LSH, IVF, ...) rely on assumptions that would likely be broken by the distance function, thus returning poor search results.\r\n\r\nAt best, custom distance functions could only be supported via `IndexFlat`, but there is a cost in this form of integration, especially if they need to cross the environment boundary, as would be the case in Python. This frequent operation is not something that can be done with zero overhead for Python functions, so the computational performance would also be poor.\r\n\r\nWhen having to employ a custom metric, there are two options:\r\n\r\n- Transform the vector space so that one of the supported metrics becomes equivalent or sufficiently similar to the distance function in the original vector space.\r\n- Not use Faiss: instead keep all vectors in a contiguous ndarray and perform exhaustive k-NN manually.\r\n"
      },
      {
        "user": "dbickson",
        "created_at": "2022-12-05T18:53:07Z",
        "body": "Today I asked openAI chatGPT on how to define custom distance function at faiss and the chat bot invented the following code: \r\n```\r\n\r\n\r\n#include <faiss/IndexFlat.h>\r\n#include <algorithm>\r\n#include <cmath>\r\n\r\n// Define a custom distance metric function that takes two vectors as input\r\n// and returns the distance between them\r\nfloat custom_distance(int d, const float *x, const float *y) {\r\n    float sum = 0;\r\n    for (int i = 0; i < d; i++) {\r\n        sum += std::abs(x[i] - y[i]);\r\n    }\r\n    return sum;\r\n}\r\n\r\nint main() {\r\n    // Create a Faiss index for the nearest neighbor model\r\n    faiss::IndexFlat index(128);\r\n\r\n    // Set the custom distance metric function for the index\r\n    index.set_distance(custom_distance);\r\n\r\n    // Train the nearest neighbor model using the custom distance metric\r\n    index.train(data);\r\n\r\n    return 0;\r\n}\r\n```\r\nI think it is a very clean interface, users who asked for custom distance are not concerned about performance but are concorned of usability it would be great to support something similar even if the implementation is not a blas/ block implementation. "
      },
      {
        "user": "Enet4",
        "created_at": "2022-12-05T20:24:26Z",
        "body": "@dbickson ChatGPT is not a reliable source of code solutions, nor does it serve well as an unbiased and reasonable decision maker in terms of library and API design.\r\n\r\nWith that said, you could use whichever automated tools are at your disposal to help implement a custom vector search on nparrays. Faiss is still not quite fit for the use case."
      }
    ]
  },
  {
    "number": 637,
    "title": "Does faiss::IndexHNSW support inner_product metric?",
    "created_at": "2018-11-13T12:35:25Z",
    "closed_at": "2018-11-26T09:52:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/637",
    "body": "# Summary\r\nCurrently, I use IndexHNSWFlat and it uses IndexFlatL2 which is hard coded, how does IndexHNSW support inner_product metric? Thanks in advance!\r\n\r\nThe code i saw doesn't supply MetricType parameter.\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\na91a24e77a3e16f0336eb8b70770bdf5daa1154e\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/637/comments",
    "author": "veyrony",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-11-13T12:41:20Z",
        "body": "`IndexHNSW` does not support the inner product metric."
      },
      {
        "user": "veyrony",
        "created_at": "2018-11-13T12:53:59Z",
        "body": "Hi @beauby , thank you for your prompt reply. "
      }
    ]
  },
  {
    "number": 636,
    "title": "range search: do I need parameter 'nprobe > 1'?",
    "created_at": "2018-11-13T12:16:52Z",
    "closed_at": "2018-12-05T09:31:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/636",
    "body": "I'm doing `range search` using `faiss-cpu` python APIs under Ubuntu 18.04.\r\n\r\nAnd here is my code:\r\n\r\n```python\r\nimport numpy as np\r\nimport faiss\r\n\r\nd = 2                           # dimension\r\nnb = 10000000                     # database size\r\nnq = 90000                       # nb of queries\r\nnp.random.seed(1234)             # make reproducible\r\nxb = np.random.random((nb, d)).astype('float32')\r\nxb[:, 0] += np.arange(nb) / 1000.\r\nxq = np.random.random((nq, d)).astype('float32')\r\nxq[:, 0] += np.arange(nq) / 1000.\r\n\r\nxb[2000:2020, :] = [100, 100]\r\nxq[18, :] = [100, 100]\r\n\r\nprint('shape of database:', xb.shape)\r\nprint('shape of queries:', xq.shape)\r\n\r\nnlist = 1024\r\n\r\nquantizer = faiss.IndexFlatL2(d)\r\nindex = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)\r\n\r\nindex.train(xb)\r\nindex.add(xb)\r\n\r\nfrom datetime import datetime\r\n\r\nt1 = datetime.now()\r\nret = None\r\ndis = 0.00000001\r\nprint(\"Searching all points within range: %f\" % dis)\r\nindex.nprobe = 10 # \r\nret = index.range_search(xq, dis)\r\nt2 = datetime.now()\r\n\r\nfor e in ret:\r\n    print(e)\r\n\r\nprint(\"Time elapsed: \", (t2-t1).total_seconds())\r\n```\r\n\r\nAbout `index.nprobe` in the above example, I tried different value but resulting with the same. So I wonder:\r\n\r\n**Do I need index.nprobe > 1 when doing range search, cause bigger `nprobe` cost much more time but the same result?**\r\n\r\n**e.g.**:\r\n\r\n- nprobe = 1(default)\r\n\r\n```python\r\nshape of database: (10000000, 2)\r\nshape of queries: (90000, 2)\r\nSearching all points within range: 0.000000\r\n[ 0  0  0 ... 22 22 22]\r\n[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\r\n 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\r\n 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\r\n 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\r\n 5.1421907e-09 3.2056313e-09]\r\n[ 2000  2001  2002  2003  2004  2005  2006  2007  2008  2009  2010  2011\r\n  2012  2013  2014  2015  2016  2017  2018  2019 58795 61346]\r\nTime elapsed:  4.45379\r\n```\r\n\r\n- nprobe = 10\r\n\r\n```python\r\nshape of database: (10000000, 2)\r\nshape of queries: (90000, 2)\r\nSearching all points within range: 0.000000\r\n[ 0  0  0 ... 22 22 22]\r\n[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\r\n 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\r\n 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\r\n 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\r\n 5.1421907e-09 3.2056313e-09]\r\n[ 2000  2001  2002  2003  2004  2005  2006  2007  2008  2009  2010  2011\r\n  2012  2013  2014  2015  2016  2017  2018  2019 58795 61346]\r\nTime elapsed:  45.688296\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/636/comments",
    "author": "00001101-xt",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-11-16T12:59:27Z",
        "body": "The correct result is returned both for nprobe=1 and 10. \r\nYou can display the result with:\r\n\r\n```\r\nlim, dis, ids = index.range_search(xq, dis)\r\n\r\nfor i in range(nq):\r\n    l0, l1 = lim[i], lim[i + 1]\r\n    if l1 > l0:\r\n        print('query %d: %s %s' % (i, ids[l0:l1], dis[l0:l1]))\r\n```\r\nwhich returns\r\n```\r\nquery 18: [2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013\r\n 2014 2015 2016 2017 2018 2019] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\nquery 59108: [58795] [5.1421907e-09]\r\nquery 60782: [61346] [3.2056313e-09]\r\n```\r\nas expected."
      },
      {
        "user": "00001101-xt",
        "created_at": "2018-11-16T14:36:30Z",
        "body": "@mdouze \r\n\r\nI'm asking because as far as I'm concerned, for **range searching**, maybe the `index` one need is something like a **BVH**(not looking into how faiss did this), hence has nothing to do with `nprobe`. But here, setting `nprobe` to bigger value cost much more time, meaning much more \"cells\" are visited, strange to be.\r\n\r\nMaybe a different `index` method is better for **range search**."
      },
      {
        "user": "mdouze",
        "created_at": "2018-11-25T15:14:27Z",
        "body": "BTW, Faiss is not a good choice for 2D data. Especially for range search, a very simple structure like a grid quantizer would give the result efficiently. "
      },
      {
        "user": "mdouze",
        "created_at": "2018-12-05T09:31:39Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 634,
    "title": "How to limit the cpu utilization?",
    "created_at": "2018-11-09T09:24:04Z",
    "closed_at": "2018-11-13T05:47:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/634",
    "body": "The CPU utilization is 390% on my 4 cores cpu when searching frequently. How to limit the cpu utilization?\r\nI tried the GPU, there is no improvement.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/634/comments",
    "author": "XiaXuehai",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-11-09T15:10:31Z",
        "body": "You can limit the number of OpenMP threads using the `OMP_NUM_THREADS` env variable."
      },
      {
        "user": "quoniammm",
        "created_at": "2019-11-07T10:15:23Z",
        "body": "The CPU utilization is 450% on my 8 cores cpu when index building frequently, whether i can use the same way? @beauby "
      }
    ]
  },
  {
    "number": 632,
    "title": "IndexIVFPQ self distance in different scale",
    "created_at": "2018-11-07T03:03:05Z",
    "closed_at": "2018-11-23T09:36:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/632",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [*] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [*] Python\r\n\r\n# Reproduction instructions \r\n`d = 512                        # vector dimension  `\r\n`index = faiss.IndexFlatL2(d)   # build the index`\r\n`indexer = faiss.IndexIVFPQ(index, d, 1024, 128, 8)`\r\n`preprocessing.normalize(vectors, norm='l2') // before train and indexingï¼Œdo L2 normalize\r\n`\r\n\r\nWe have N * 512 vectors to index with IndexIVFPQ, however, when searching with the indexed vectors, the most similar ID is itself apparently, but the self distance in different scale, for example:\r\n\r\nsearch_vector_id distance result_ids\r\n0      [0.01252195 0.262466   0.27768448]  [    0 18319 33179] \r\n1      [0.12325595 0.261246   0.27768448]  [    1 87352 65232] \r\n2      [0.28974425 0.369800   0.59863836]  [    2 76546 23434] \r\n\r\nSo, **it's hard to set a threshold for distance** array to finally get the satisfied results. And how can we change the distance to similarity ?\r\n\r\nIs there any problem in that? I hope that the distance between different vectors  and themselves are in a certain scale, such as both <0.001\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/632/comments",
    "author": "lijingpeng",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-11-07T12:49:55Z",
        "body": "This represents the quantization error for vectors inside the dataset. \r\nFor vectors in denser areas of the space, the quantization error is lower because the quantization centroids are bigger and vice versa. \r\nTherefore, there is no limit to this error that is valid over the whole space. However, it is possible to recompute the exact distances once you have the nearest neighbors, by accessing the uncompressed vectors. \r\n\r\ndistance -> similarity in uncompressed space is \r\n\r\ndis = 2 - 2 * sim \r\n\r\nsince the vectors are L2-normalized. "
      },
      {
        "user": "mdouze",
        "created_at": "2018-11-23T09:36:52Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 629,
    "title": "How to read the index from the disk fastly?",
    "created_at": "2018-11-03T01:14:51Z",
    "closed_at": "2018-11-09T03:29:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/629",
    "body": "`faiss.read_index` is slow if the index file is large enough. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/629/comments",
    "author": "XiaXuehai",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-11-06T12:55:48Z",
        "body": "`IndexIVF`s can be memory-mapped instead of read from disk, load with faiss.read_index flag `IO_FLAG_MMAP|IO_FLAG_READ_ONLY`. This is efficient if you need only to do a few queries or get some stats from the index. \r\n\r\nApart from that, the index loading is as fast as the underlying storage. So faster if on SSD, faster if local disk, even faster the file is cached. "
      }
    ]
  },
  {
    "number": 627,
    "title": "The  SIFT1B(BIGANN) dataset",
    "created_at": "2018-10-29T03:35:20Z",
    "closed_at": "2018-11-20T12:22:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/627",
    "body": "# Summary\r\n\r\nWe  trained your script file (python bench_gpu_1bn.py) on the BIGANN, and the search performed well.  However, I have a question about data sets. In addition to the necessary Base set and Query set, what are the main functions of the training set(learning set)? Is it possible to train directly with Base set? \r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] Python",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/627/comments",
    "author": "ahappycutedog",
    "comments": [
      {
        "user": "ahappycutedog",
        "created_at": "2018-10-29T10:04:54Z",
        "body": "I have a vector set trained by cnn as the database set. Can I still use this dataset as the learning set to train cluster centers?"
      },
      {
        "user": "beauby",
        "created_at": "2018-10-29T10:16:50Z",
        "body": "@ahappycutedog The training set for clustering has to have the same distribution as your database, otherwise you will get unbalanced clusters."
      },
      {
        "user": "beauby",
        "created_at": "2018-10-29T10:17:34Z",
        "body": "Note that you can use your database or a fraction of it to train the clustering."
      },
      {
        "user": "ahappycutedog",
        "created_at": "2018-11-15T01:51:57Z",
        "body": "> Note that you can use your database or a fraction of it to train the clustering.\r\n\r\nThanks"
      }
    ]
  },
  {
    "number": 625,
    "title": "Bottleneck at OPQMatrix",
    "created_at": "2018-10-25T08:48:50Z",
    "closed_at": "2018-11-07T21:02:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/625",
    "body": "The PCA of vectors with large dimensionality (>8000) is very slow.\r\nWhat would be the recommended way to speed this step up?\r\nCan I use another Dimension reduction algorithm and simply feed a matrix into OPQMatrix?\r\n\r\nI have GPU resources but it appears that OPQMatrix training has to be done on CPU.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/625/comments",
    "author": "mingruimingrui",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-10-25T09:54:45Z",
        "body": "You can compute a limited number of dimensions with a PCA with scipy.sparse.linalg.eigs. For example: \r\n\r\n```python\r\nimport numpy as np\r\nimport scipy.sparse.linalg\r\nimport time\r\n\r\nx = np.random.rand(16000, 8000).astype('float32')\r\n\r\n# make non-uniform\r\nx *= 1.1 + np.sin(np.arange(8000))\r\n\r\n# compute covariance matrix\r\nxc = x - x.mean(axis=0)\r\ncov = np.dot(xc.T, xc)\r\n\r\n# compute 128 largest eigenvalues (takes 4s on my machine)\r\neigvals, eigvecs = scipy.sparse.linalg.eigs(cov, k=128, which='LM')\r\n\r\n# here is the PCA matrix\r\nPCAMatrix = eigvecs.real\r\n\r\n# transform data using matrix\r\nxt = np.dot(x, PCAMatrix)\r\n\r\n```\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2018-11-07T21:02:49Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 621,
    "title": "How use openblas support 64bit faiss index?",
    "created_at": "2018-10-18T07:50:10Z",
    "closed_at": "2018-11-07T21:02:04Z",
    "labels": [
      "question",
      "install"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/621",
    "body": "# Summary\r\nInstall 64bit faiss index\r\n# Platform\r\nUbuntu16.04\r\nFaiss version: 0.1\r\nFaiss compilation options: openblas\r\nRunning on: CPU\r\nInterface: Python\r\n# Reproduction instructions\r\n```\r\n./misc/test_blas\r\nIntentional Lapack error (appears only for 64-bit INTEGER):\r\ninfo=0000064b00000000\r\nLapack uses 32-bit integers\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/621/comments",
    "author": "nuaawzl",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-10-19T11:12:32Z",
        "body": "What is your question/what are you trying to achieve? The above simply states that in the Lapack implementation you are using, the Fortran `INTEGER` type is 32 bits."
      },
      {
        "user": "mdouze",
        "created_at": "2018-11-07T21:02:03Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 620,
    "title": "TypeError: in method 'IndexPreTransform_reconstruct', argument 2 of type 'faiss::Index::idx_t'",
    "created_at": "2018-10-18T07:47:14Z",
    "closed_at": "2018-10-22T02:39:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/620",
    "body": "I am using faiss-cpu version with python interface, when I am trying to reconstruct a vector from an idx, i meet an error below: \r\n```\r\nTypeError: in method 'IndexPreTransform_reconstruct', argument 2 of type 'faiss::Index::idx_t'\r\n```\r\n\r\nThe code I use is \r\n```\r\nfeat = np.load('feat.npy')\r\nd = 2048\r\nindex = faiss.index_factory(d, 'PCAR128,IMI2x10,SQ8')\r\nfaiss.ParameterSpace().set_index_parameter(index, 'nprobe', 100)\r\nindex.train(feat)\r\nindex.add(feat)\r\n\r\nquery_feat = np.random.rand(1, d)\r\nk = 10\r\nD, I  = index.search(query_feat, k)\r\nreconstruct_feat = index.reconstruct(I[0, 0]) # I[0, 0] is not -1\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/620/comments",
    "author": "animebing",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-10-19T11:12:57Z",
        "body": "Could you post the full stack trace?"
      },
      {
        "user": "animebing",
        "created_at": "2018-10-19T11:19:54Z",
        "body": "@beauby, below is the whole stack trace\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-101-6c9926c73508> in <module>()\r\n     30 for i in range(search_num):\r\n     31     tmp_idx = I[0, i]\r\n---> 32     tm_index.reconstruct(tmp_idx)\r\n     33     tmp_img_path = database_info_list[tmp_idx].strip('\\n').split(' ')[0]\r\n     34     tmp_img = Image.open(tmp_img_path)\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/faiss/__init__.py in replacement_reconstruct(self, key)\r\n    151     def replacement_reconstruct(self, key):\r\n    152         x = np.empty(self.d, dtype=np.float32)\r\n--> 153         self.reconstruct_c(key, swig_ptr(x))\r\n    154         return x\r\n    155 \r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/faiss/swigfaiss.py in reconstruct(self, key, recons)\r\n   1917 \r\n   1918     def reconstruct(self, key, recons):\r\n-> 1919         return _swigfaiss.IndexPreTransform_reconstruct(self, key, recons)\r\n   1920 \r\n   1921     def reconstruct_n(self, i0, ni, recons):\r\n\r\nTypeError: in method 'IndexPreTransform_reconstruct', argument 2 of type 'faiss::Index::idx_t'\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2018-10-20T16:45:59Z",
        "body": "probably a weird interaction between numpy and swig. Try casting -> \r\n\r\n```\r\nindex.reconstruct(int(I[0, 0]))\r\n```"
      },
      {
        "user": "animebing",
        "created_at": "2018-10-22T02:39:39Z",
        "body": "@mdouze, thanks for your reply, it works right now."
      },
      {
        "user": "Prymon",
        "created_at": "2019-12-25T12:28:51Z",
        "body": "try below:\r\n    query_feat = np.random.rand((1, d))\r\n\r\n    rand((a,b))    not    rand(a,b)"
      }
    ]
  },
  {
    "number": 618,
    "title": "how to get the compressed vector after PCA from the index ?",
    "created_at": "2018-10-17T05:32:21Z",
    "closed_at": "2018-11-07T21:01:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/618",
    "body": "\r\nBelow is the code I use\r\n``` \r\nfeat = np.load('feat.npy')\r\nd = 2048\r\nindex = faiss.index_factory(d, 'PCAR128,IMI2x10,SQ8')\r\nfaiss.ParameterSpace().set_index_parameter(index, 'nprobe', 100)\r\nindex.train(feat)\r\nindex.add(feat)\r\n\r\nquery_feat = np.random.rand(1, d)\r\nk = 10\r\nD, I  = index.search(query_feat, k)\r\n```\r\nI want to know:\r\n1. how to get the compressed vector after PCA from index according to `I`.\r\n2. how to get the compressed vector of `query_feat` after PCA from index.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/618/comments",
    "author": "animebing",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-10-17T09:35:34Z",
        "body": "1. the easiest way is to use index.search_and_reconstruct, which reconstructs an approximation of the vectors in addition to returning search results. \r\n2. note that query_feat is transformed by PCA but not compressed. You can apply the PCA transformation manually with `index.chain.at(0).apply_py(query_feat)` which returns a 128D vector."
      },
      {
        "user": "animebing",
        "created_at": "2018-10-18T03:52:12Z",
        "body": "@mdouze Thanks for your reply, I still have some questions about `index`\r\n\r\n- The vector after reconstruction have same dimension as the input vector, here 2048D, but not exactly same content?\r\n- the transformation by PCA is done during `search` instead of `add`? \r\n- whether the transformed vector by PCA is stored in `index`  or just the transformation matrix is stored in `index`? if the transformed vector is not stored in `index`, how is the reconstruction done?\r\n- the `search` is based on transformed vector, the vector after quantization or reconstructed vector ?\r\n\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2018-10-20T16:41:28Z",
        "body": "- the reconstruction is as good as possible. The index you use has lossy coding (PCA and scalar quantizer) so the reconstruction is lossy\r\n- the transformation is done on both the searched and the added vectors\r\n- both are stored\r\n- search based on the transformed vector (asymmetric search)"
      },
      {
        "user": "animebing",
        "created_at": "2018-10-22T03:21:58Z",
        "body": "@mdouze, thanks for your reply, I still have other questions below: \r\n- what is the API to get the vector after transformation and quantization?\r\n- where can I find the python documentation about faiss instead of c++?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-10-22T11:33:03Z",
        "body": "- `reconstruct` outputs added vectors after trans + quant\r\n- transferring the doc from c++ to py is an open problem, see #393"
      },
      {
        "user": "animebing",
        "created_at": "2018-10-26T13:01:10Z",
        "body": "@mdouze Thanks for your reply. From the wiki and documentation, I know `reconstruct(i_0, n_i)` can be used to reconstruct more than one vector, but the `ids` to be reconstructed should be sequential. If I want to use `reconstruct_n` based on the result of `search` like below, what should I do?\r\n```\r\nk = 10\r\nD, I  = index.search(query_feat, k)\r\nvectors = index.reconstruct_n(I) # this is what I what to do\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2018-10-26T14:13:49Z",
        "body": "There is also a `search_and_reconstruct` method. "
      },
      {
        "user": "animebing",
        "created_at": "2018-10-29T02:56:46Z",
        "body": "@mdouze `search_and_reconstruct` can only reconstruct the vectors of the `search` result. If I want to reconstruct some vectors given extra `idx`, what should I do?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-11-07T21:01:33Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 614,
    "title": "Cosine distance using IP",
    "created_at": "2018-10-10T13:20:11Z",
    "closed_at": "2018-10-23T05:18:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/614",
    "body": "Hello,\r\nfirst of all: great work. I like this library!\r\n\r\nSecond: I am running into problems using the IndexFlatIP for cosine similarity search.\r\nThe cosine similarity is just the dot product, if I normalize the vectors according to the l2-norm. Cosine similarity is 1, if the vectors are equal and -1 if they point in opposite direction. \r\n\r\nNow my problem/question is: \r\nHow do I get the values closest to cosine similarity=1, which would mean they are equal.\r\nI tried to search with the normalized negative vector like:\r\n`distPos, indexPos = faissPositive.search(-1.0 * findKNNToNormalized, k)`\r\nThis just gives me the negative values in DECREASING order. For example beginning with -0.58 to -0.66. This is bad, because with the negtive vector I would like to have the ones closest to -1 and it seems to break something internally. Can someone please explain this to me?\r\nIs it searching for the absolute minimum distance? Do I miss a point here?\r\n\r\nI hope you get my point, because I would like to search for the cosine distance , which is kind of the inverted cosine similarity.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/614/comments",
    "author": "Berndinio",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-10-10T14:11:34Z",
        "body": "For IndexFlatIP the values are ordered by decreasing dot product, so you don't need to multiply `findKNNToNormalized` by -1."
      },
      {
        "user": "mdouze",
        "created_at": "2018-10-23T05:18:45Z",
        "body": "No activity. Closing."
      }
    ]
  },
  {
    "number": 599,
    "title": "Question: reconstruct vectors by mapped IDs",
    "created_at": "2018-09-25T14:17:10Z",
    "closed_at": "2018-10-23T04:59:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/599",
    "body": "Hello!\r\nIs there any way to reconstruct vectors by their mapped IDs?\r\nI can not call `make_direct_map()` method - `Error: '0 <= idlist [ofs] && idlist[ofs] < ntotal' failed: direct map supported only for seuquential ids`\r\nAs well as I can not call `reconstruct()` without previously called `make_direct_map()` - `Error: 'direct_map.size() == ntotal' failed: direct map is not initialized` \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/599/comments",
    "author": "maxoodf",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-09-25T15:29:16Z",
        "body": "Are you using `add_with_ids` or `remove_ids` prior to calling `make_direct_map`?"
      },
      {
        "user": "maxoodf",
        "created_at": "2018-09-25T15:40:59Z",
        "body": "I am using `add_with_ids()`.\r\nIn case of `add()` it works as expected - I can reconstruct vectors."
      },
      {
        "user": "beauby",
        "created_at": "2018-09-27T15:00:10Z",
        "body": "In this case you can maintain a map from ids to vectors outside of Faiss, as `make_direct_map` is not supported for non-sequential ids."
      },
      {
        "user": "mdouze",
        "created_at": "2018-10-23T04:59:35Z",
        "body": "No activity. Closing.\r\n"
      }
    ]
  },
  {
    "number": 592,
    "title": "add_with_ids return index 0 ?",
    "created_at": "2018-09-17T10:13:05Z",
    "closed_at": "2018-09-20T14:19:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/592",
    "body": "```C++\r\nconst int dim = 4;\r\n\r\nint main ()\r\n{\r\n\r\ndouble t0 = elapsed();\r\n\r\n// dimension of the vectors to index\r\nint d = dim;\r\n\r\n// size of the database we plan to index\r\nsize_t nb = 5;\r\n\r\n// make a set of nt training vectors in the unit cube\r\n// (could be the database)\r\nsize_t nt = 100;\r\n\r\nsize_t nlist = 1;\r\n\r\n// make the index object and train it\r\nfaiss::IndexFlatIP coarse_quantizer (d);\r\n\r\nfaiss::IndexIVFFlat index = faiss::IndexIVFFlat(\r\n    &coarse_quantizer, d, nlist, faiss::METRIC_INNER_PRODUCT);\r\n\r\n{ // training\r\n    printf (\"[%.3f s] Generating %ld vectors in %dD for training\\n\",\r\n            elapsed() - t0, nt, d);\r\n\r\n    std::vector <float> trainvecs (nt * d);\r\n    for (size_t i = 0; i < nt * d; i++) {\r\n        trainvecs[i] = drand48();\r\n    }\r\n\r\n    printf (\"[%.3f s] Training the index\\n\",\r\n            elapsed() - t0);\r\n    index.verbose = true;\r\n\r\n    index.train (nt, trainvecs.data());\r\n}\r\n\r\n{ // I/O demo\r\n    const char *outfilename = \"/tmp/index_trained.faissindex\";\r\n    printf (\"[%.3f s] storing the pre-trained index to %s\\n\",\r\n            elapsed() - t0, outfilename);\r\n\r\n    write_index (&index, outfilename);\r\n}\r\n\r\nsize_t nq;\r\nfloat que[dim] = {0.12, 0.32, 0.33, -0.12};\r\nstd::vector<float> queries(que, que+dim);\r\nnq = 1;\r\n\r\n{ // populating the database\r\n    printf (\"[%.3f s] Building a dataset of %ld vectors to index\\n\",\r\n            elapsed() - t0, nb);\r\n\r\n    std::vector <float> database (nb * d);\r\n    for (size_t i = 0; i < nb * d; i++) {\r\n        database[i] = i/(nb * d-1.0);\r\n    }\r\n    std::vector<long> xids(nb);\r\n    xids.push_back(1354777239);\r\n    xids.push_back(1982282553);\r\n    xids.push_back(1486633334);\r\n    xids.push_back(644481400);\r\n    xids.push_back(644481302);\r\n    printf (\"[%.3f s] Adding the vectors to the index\\n\",\r\n            elapsed() - t0);\r\n\r\n    //index.add (nb, database.data());\r\n    index.add_with_ids(nb, database.data(), xids.data());\r\n\r\n    printf (\"[%.3f s] imbalance factor: %g, ntotal=%ld\\n\",\r\n            elapsed() - t0, index.imbalance_factor (), index.ntotal);\r\n\r\n    const faiss::Index::idx_t *pt = index.invlists->get_ids(nlist-1);\r\n    for (size_t i = 0; i < index.invlists->list_size(nlist-1); ++ i)\r\n    {\r\n        printf (\"%ld \", pt[i]);\r\n    }\r\n    printf (\"\\n\");\r\n}\r\n\r\n{ // searching the database\r\n    int k = 3;\r\n    printf (\"[%.3f s] Searching the %d nearest neighbors \"\r\n            \"of %ld vectors in the index\\n\",\r\n            elapsed() - t0, k, nq);\r\n\r\n    std::vector<faiss::Index::idx_t> nns (k * nq);\r\n    std::vector<float>               dis (k * nq);\r\n\r\n    index.search (nq, queries.data(), k, dis.data(), nns.data());\r\n\r\n    printf (\"[%.3f s] Query results (vector ids, then distances):\\n\",\r\n            elapsed() - t0);\r\n\r\n    for (int i = 0; i < nq; i++) {\r\n        printf (\"query %2d: \", i);\r\n        for (int j = 0; j < k; j++) {\r\n            printf (\"%7ld \", nns[j + i * k]);\r\n        }\r\n        printf (\"\\n     dis: \");\r\n        for (int j = 0; j < k; j++) {\r\n            printf (\"%7g \", dis[j + i * k]);\r\n        }\r\n        printf (\"\\n\");\r\n    }\r\n}\r\n```\r\nI met the same question with the newest version, ask for helper,thanks\r\n\r\n[0.126 s] Query results (vector ids, then distances):\r\nquery 0:   0               0                  0\r\ndis:          0.58          0.443158      0.306316",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/592/comments",
    "author": "definesun",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-09-17T14:23:37Z",
        "body": "Does the problem arise if you replace\r\n```C++\r\nfaiss::IndexIVFFlat index = faiss::IndexIVFFlat(\r\n    &coarse_quantizer, d, nlist, faiss::METRIC_INNER_PRODUCT);\r\n```\r\nwith\r\n```C++\r\nfaiss::IndexFlatIP index = faiss::IndexIVFFlat(d);\r\n```\r\n?"
      },
      {
        "user": "definesun",
        "created_at": "2018-09-18T06:47:10Z",
        "body": "> Does the problem arise if you replace\r\n> \r\n> ```c++\r\n> faiss::IndexIVFFlat index = faiss::IndexIVFFlat(\r\n>     &coarse_quantizer, d, nlist, faiss::METRIC_INNER_PRODUCT);\r\n> ```\r\n> \r\n> with\r\n> \r\n> ```c++\r\n> faiss::IndexFlatIP index = faiss::IndexIVFFlat(d);\r\n> ```\r\n> \r\n> ?\r\n\r\nthanks for your reply, but i do not know what you means, demo is :\r\n\r\nfaiss::IndexFlatIP coarse_quantizer (d);\r\nfaiss::IndexIVFFlat index = faiss::IndexIVFFlat(\r\n    &coarse_quantizer, d, nlist, faiss::METRIC_INNER_PRODUCT);"
      },
      {
        "user": "beauby",
        "created_at": "2018-09-20T14:19:32Z",
        "body": "```C++\r\nstd::vector<long> xids(nb);\r\n    xids.push_back(1354777239);\r\n    xids.push_back(1982282553);\r\n    xids.push_back(1486633334);\r\n    xids.push_back(644481400);\r\n    xids.push_back(644481302);\r\n```\r\nThis is indeed creating a vector starting with five times `0`, followed by your values. Do not add zeros in front of the vector and you shouldn't have a problem."
      }
    ]
  },
  {
    "number": 586,
    "title": "error: \"Index type 0x52517749 not supported Aborted (core dumped)\"",
    "created_at": "2018-09-12T04:46:55Z",
    "closed_at": "2018-09-22T13:25:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/586",
    "body": "when I use faiss python to read index, an error of â€œIndex type 0x52517749 not supported\r\nAborted (core dumped)â€ is occured. How to solve this problem?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/586/comments",
    "author": "5118qlx",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-09-12T05:27:33Z",
        "body": "You are probably trying to read an index saved with the current Faiss with an older version of Faiss. "
      },
      {
        "user": "beauby",
        "created_at": "2018-09-22T13:25:03Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 572,
    "title": "How to distribute multiple index into different gpus?",
    "created_at": "2018-08-29T03:24:25Z",
    "closed_at": "2018-09-22T13:23:55Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/572",
    "body": "I have got thousands of faiss indexes, and they are as large as 30GB in total.\r\nI want to distribute them in to four gpus, how can i do that?\r\n\r\nFor example, i have four index: a, b, c, d.\r\nAnd i want a index in gpu 0, b index in gpu 1, and c in gpu 2, d in gpu3.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/572/comments",
    "author": "universewill",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-08-29T09:18:20Z",
        "body": "When you create the GPU index or transfer it with `index_cpu_to_gpu` you indicate the gpu id (0..3). "
      },
      {
        "user": "beauby",
        "created_at": "2018-09-22T13:23:55Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 570,
    "title": "Can pca transform be calculated by gpu?",
    "created_at": "2018-08-24T08:33:35Z",
    "closed_at": "2018-08-27T10:07:24Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/570",
    "body": "I find the  pca_mat.apply_py() function is very cpu consuming.  \r\nIf i install the gpu version of faiss, will the apply_py() function use gpu instead of cpu to calculate vector result?\r\n\r\nI find the apply_by() fucntion will grab all cpu resource for caculation. Can i limit the cpu resource it can use? I don't want it to consume all the cpu, which causes my server down.\r\n@mdouze ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/570/comments",
    "author": "universewill",
    "comments": [
      {
        "user": "universewill",
        "created_at": "2018-08-24T08:51:51Z",
        "body": "Can i get the pca transform matrix from the faiss pca trained result? I think i can use tensorflow, pytorch or mxnet to calculate the matrix production if i can get the matrix from faiss pca trained result, which, can definitely use gpu to do the calculation."
      },
      {
        "user": "mdouze",
        "created_at": "2018-08-27T05:37:34Z",
        "body": "The pre-transform routines are not implemented on GPU because they are usually not a bottleneck in the computation. If this is the case for you, you presumably use large input vectors. \r\n\r\nTo get the PCA matrix, please do:\r\n\r\n```\r\npca = PCAMatrix(...)\r\npca.train(...)\r\nA = faiss.vector_to_array(pca.A).reshape(pca.d_out, pca.d_in)\r\nb = faiss.vector_to_array(pca.b)\r\n```\r\nThen the equivalent of applying PCA to matrix `x` is\r\n```\r\nx_out = np.dot(x, A.T) + b\r\n```\r\nwhich you can translate to your favorite GPU matrix lib."
      }
    ]
  },
  {
    "number": 563,
    "title": "bug? returned distance different for different query size n",
    "created_at": "2018-08-10T19:24:14Z",
    "closed_at": "2018-08-28T09:13:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/563",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\nx86_64\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\n\r\nFaiss version: \r\nfaiss-gpu                 1.3.0           py36_cuda8.0.61_1    pytorch\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [x] GPU\r\n\r\nboth CPU & GPU has this bug.\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\nbasically, a simple modification of tutorial/python/1-Flat.py\r\n\r\n```\r\nimport numpy as np\r\n\r\nd = 64                           # dimension\r\nnb = 100000                      # database size\r\nnq = 10000                       # nb of queries\r\nnp.random.seed(1234)             # make reproducible\r\nxb = np.random.random((nb, d)).astype('float32')\r\nxb[:, 0] += np.arange(nb) / 1000.\r\nxq = np.random.random((nq, d)).astype('float32')\r\nxq[:, 0] += np.arange(nq) / 1000.\r\n\r\nimport faiss                   # make faiss available\r\nindex = faiss.IndexFlatL2(d)   # build the index\r\nprint(index.is_trained)\r\nindex.add(xb)                  # add vectors to the index\r\nprint(index.ntotal)\r\n\r\nk = 4                          # we want to see 4 nearest neighbors\r\nn = 1024   # bug: D is different if n = 5\r\nD, I = index.search(xb[:n], k) # sanity check\r\nprint(I[:5])\r\nprint(D[:5])\r\n```\r\n\r\nif n = 5, output:\r\n```\r\nTrue\r\n100000\r\n[[  0 393 363  78]\r\n [  1 555 277 364]\r\n [  2 304 101  13]\r\n [  3 173  18 182]\r\n [  4 288 370 531]]\r\n[[0.        7.1751733 7.207629  7.2511625]\r\n [0.        6.3235645 6.684581  6.7999454]\r\n [0.        5.7964087 6.391736  7.2815123]\r\n [0.        7.2779055 7.5279865 7.6628466]\r\n [0.        6.7638035 7.2951202 7.3688145]]\r\n```\r\nall distance to self is 0.\r\n\r\nbut if n = 1024:\r\n```\r\nTrue\r\n100000\r\n[[  0 393 363  78]\r\n [  1 555 277 364]\r\n [  2 304 101  13]\r\n [  3 173  18 182]\r\n [  4 288 370 531]]\r\n[[ 1.1444092e-05  7.1751823e+00  7.2076340e+00  7.2511711e+00]\r\n [-7.6293945e-06  6.3235664e+00  6.6845779e+00  6.7999382e+00]\r\n [ 7.6293945e-06  5.7964058e+00  6.3917274e+00  7.2815170e+00]\r\n [ 7.6293945e-06  7.2779045e+00  7.5279846e+00  7.6628456e+00]\r\n [-3.8146973e-06  6.7638016e+00  7.2951088e+00  7.3688126e+00]]\r\n\r\n```\r\n\r\nWhy the distance to self is not strict 0?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/563/comments",
    "author": "mw66",
    "comments": [
      {
        "user": "mw66",
        "created_at": "2018-08-10T19:26:23Z",
        "body": "from what I read, IndexFlatL2 is exact & exhaustive search."
      },
      {
        "user": "mdouze",
        "created_at": "2018-08-16T08:30:40Z",
        "body": "With different batch sizes, different search algorithms are used (the threshold is `faiss.distance_compute_blas_threshold`). Therefore there are epsilonesque differences between the results. "
      },
      {
        "user": "mdouze",
        "created_at": "2018-08-28T09:13:40Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 556,
    "title": "Question: Query time measurement",
    "created_at": "2018-08-02T20:47:50Z",
    "closed_at": "2018-08-28T09:12:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/556",
    "body": "# Summary\r\nIn the benchmarking codes, the query time reported is usually the average query time for the whole query set. Based on this I have a couple of questions.\r\n\r\n1. Since queries are performed at one stretch, this seems to be equal to batch query time. For BIGANN case, the nq = 10K. Is it right to consider batch size = 10K? \r\n2. What should be ideal query set size in case if I am going to build my own set of queries? \r\n3. Is it possible to measure worse case latency for a query set?\r\n\r\n# Platform\r\nNA\r\n\r\nRunning on:\r\n- [X] CPU\r\n- [X] GPU\r\n\r\nInterface: \r\n- [X] C++\r\n- [X] Python\r\n\r\n# Reproduction instructions\r\n\r\nNA",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/556/comments",
    "author": "msharmavikram",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-08-16T08:02:35Z",
        "body": "1. yes\r\n2. the main constraint is that you want to have enough queries that you don't have too much statisitical jitter in measures. Eg nq=100 is too small, because a single failing vector causes a 1% change in recall\r\n3. for most index types the worst case latency is attained for badly behaved datasets (not queries), eg. when a significant fraction of vectors are identical."
      },
      {
        "user": "mdouze",
        "created_at": "2018-08-28T09:12:16Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 546,
    "title": "Search for vectors by id?",
    "created_at": "2018-07-26T07:41:02Z",
    "closed_at": "2018-07-26T10:46:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/546",
    "body": "# Summary\r\n\r\nIs it possible to search vectors by its id? \r\n\r\nI have a IMI faiss model which contains 5 million vectors with customized id. Can I search for vectors \r\nby the vector that is already in the index using it's id? \r\n\r\nThanks,\r\n\r\nBest regards,\r\nShengdong\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/546/comments",
    "author": "LuShengDong",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-07-26T10:46:21Z",
        "body": "Yes, you can use the `reconstruct()` method to retrieve a stored vector by id."
      }
    ]
  },
  {
    "number": 535,
    "title": "Enabling stats",
    "created_at": "2018-07-20T00:29:50Z",
    "closed_at": "2018-07-30T01:15:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/535",
    "body": "# Summary\r\nAs I go deeper into the codebase, I see there are some perform stats structures available in the FAISS system. Could you please let me know how to enable all the stats? Is there something like verbose mode in which I can run faiss? \r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 --> NA\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a --> NA\r\n \r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... --> NA\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->NA\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. -->  NA\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/535/comments",
    "author": "msharmavikram",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-07-20T05:56:42Z",
        "body": "At some points in search, statistics are collected and written to global variables (eg. `indexIVFPQ_stats`). The variable should be `reset()` before calling the search function and can be read afterwards (with eg. `faiss.cvar.indexIVFPQ_stats.assign_cycles`.\r\n\r\nNote however that this is undocumented and inconsistent, the stats were used during the development of Faiss. "
      }
    ]
  },
  {
    "number": 530,
    "title": "About  different dimensions vector input",
    "created_at": "2018-07-19T09:15:22Z",
    "closed_at": "2018-07-20T12:31:41Z",
    "labels": [
      "invalid",
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/530",
    "body": "In faiss, we must input  fixed dimension vector. But now, the different dimensions vectors I have got, what should I do to make the different dimensions vector also  indexed in faiss successfully.\r\n\r\nWho can help me?  Thanks a lot",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/530/comments",
    "author": "yuyifan1991",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-07-19T10:11:24Z",
        "body": "What would you like to do with vectors x and y of dimensions 10 and 20 for example?"
      },
      {
        "user": "yuyifan1991",
        "created_at": "2018-07-19T10:20:37Z",
        "body": "For example, I have extracted the Image feature,they are vectors. But these feature vectors are  in different dimensions, one for 126-d, anther one for 174-d. So how to build index? Thank you so much"
      },
      {
        "user": "mdouze",
        "created_at": "2018-07-19T10:37:13Z",
        "body": "how would you mathematically define the distance between a 126D and a 174D vector?\r\n"
      },
      {
        "user": "yuyifan1991",
        "created_at": "2018-07-19T10:57:52Z",
        "body": "I mean, whether the vector must be fixed? The d must be fixed, or the d can be  variable ?"
      },
      {
        "user": "beauby",
        "created_at": "2018-07-19T11:37:36Z",
        "body": "@yuyifan1991 If you want to work with multiple embeddings, you'll have to make one index per embedding."
      },
      {
        "user": "beauby",
        "created_at": "2018-07-20T12:31:41Z",
        "body": "Closing as the question was addressed. Feel free to keep commenting if you have further questions."
      },
      {
        "user": "nikolaydubina",
        "created_at": "2022-12-21T04:50:17Z",
        "body": "@beauby  similar question. to illustrate example:\r\n\r\n- dataset: 1024D vectors\r\n- query: vector from 144D up to 1024D, find closest 1024D vector and offset in it that has closest substring of search query dimensions\r\n\r\nIf we are to build index per each number dimensions that would be ~1000 indexes. This would take lots of RAM and nodes to host it, and network and code complexity overhead. Indexes would \"overlap\", containing similar information.\r\n\r\nSome ideas:\r\n\r\n1. only few indexes: one at 144D, one at 256D, one at 512, ..., so ~10 indexes. And allow queries only to those sizes.\r\n2. break down into smaller ones (eg, 370D = 128D + 128D + 64D + 32D) and have index for chunks then have some heuristic to of aggregate\r\n3. one index of 1024D; generate N queries by padding and filling with zeroes query vector with zeros\r\n\r\nBut would be good to have better solution and hear from experts in the area!\r\n\r\nOr some hints to where study this problem further (papers/books/etc.) would be appreciated!"
      }
    ]
  },
  {
    "number": 529,
    "title": "Running in non-Intel machine",
    "created_at": "2018-07-18T22:42:10Z",
    "closed_at": "2018-08-30T19:09:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/529",
    "body": "# Summary\r\nI am trying to run FAISS in IBM Power 8 system with P100 GPUs. I see that most of code is hardcoded to use Intel platform with SSE4 enabled. Are there any ways I could get this working without the help of SSE4? I see IndexHNSW files have SSE4 optimizations. Any means I can avoid file and get running? \r\n\r\n# Platform\r\nIBM Power 8 with Nvidia P100 machines with Cuda 8.0\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 --> Ubuntu 16.04LTS\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a --> 2dc30e14cfa76985ff8c2821c051f1725fe66afa \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... --> Not using MKL, SSE4 or Intel specific options\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\nRun it on IBM Machine. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/529/comments",
    "author": "msharmavikram",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-07-19T05:17:49Z",
        "body": "This is a good question. Sadly, by lack of a non-x64 dev platform we were dragged into writing non portable code.\r\n\r\nHowever, all code that depends on SSE/popcount has a \"slow\" version that is written in generic c++. That code must be enabled, often manually in the current version.\r\n\r\nIn IndexHNSW, the only part that depends on SSE is the one handling PQ compressed HNSW which is a corner case use of HNSW. It can be safely commented out.\r\n\r\nThe function `fvec_L2sqr` is the most useful function for HNSW without compression, it is implemented in utils.cpp and the generic version is `fvec_L2sqr_ref`.\r\n"
      },
      {
        "user": "msharmavikram",
        "created_at": "2018-07-19T20:21:56Z",
        "body": "Thanks for the information. I am going to try out these modifications that you suggested and if I find more issues, I will continue this thread. "
      },
      {
        "user": "msharmavikram",
        "created_at": "2018-07-21T23:10:50Z",
        "body": "Are there any alternatives to these two structures?  \r\nDistance2xXPQ4 and Distance2xXPQ4"
      },
      {
        "user": "mdouze",
        "created_at": "2018-08-30T19:09:37Z",
        "body": "Compilation for non-intel machines is supported in 1.4.0"
      }
    ]
  },
  {
    "number": 508,
    "title": "Does add_with_ids is idempotentï¼Ÿ",
    "created_at": "2018-07-06T03:29:12Z",
    "closed_at": "2018-07-10T09:59:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/508",
    "body": "````\r\n add_with_ids(1, feature1, id1);\r\n add_with_ids(1, feature1, id1);\r\n\r\n# then I do a query output is \r\nid1  1.000\r\nid1  1.000\r\n````\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/508/comments",
    "author": "shaocongliang",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-07-06T09:04:38Z",
        "body": "Hi\r\nThe index does not detect if the same id has been added several times."
      }
    ]
  },
  {
    "number": 498,
    "title": "Converting an on-disk IVFPQ index back to a normal IVFPQ index",
    "created_at": "2018-06-21T19:52:54Z",
    "closed_at": "2018-07-06T09:12:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/498",
    "body": "# Summary\r\n\r\nAfter merging multiple indexes using the method provided in demos/demo_ondisk_ivf.py, is there a way to convert the final index (which stores a separate on-disk file that contains the inverted lists) back into a normal IVFPQ index?\r\n\r\n# Platform\r\n\r\nOS: Ubuntu 14.04\r\n\r\nFaiss version: af564c15b5062e22d4dd62e60295322c817f8801\r\n\r\nFaiss compilation options: Default Ubuntu options\r\n\r\nRunning on :\r\n- [ x] CPU\r\n- [ x] GPU\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/498/comments",
    "author": "joelb92",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-06-25T23:17:45Z",
        "body": "Hi \r\nEverything is possible, it's just not very convenient. You can use a function like: \r\n```python\r\ndef copy_invlists(src, dst): \r\n   assert src.nlist == dst.nlist\r\n   for i in xrange(src.nlist): \r\n      n = src.list_size(i)\r\n      dst.add_entries(i, n, src.get_ids(i), src.get_codes(i))\r\n\r\nsrc = index.invlists\r\ndst = faiss.ArrayInvertedLists(src.nlist, src.code_size)\r\n\r\ncopy_invlists(src, dst)\r\n\r\n```\r\n\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2018-07-06T09:12:17Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 497,
    "title": "Can I run PCA with limited memory?",
    "created_at": "2018-06-20T08:57:53Z",
    "closed_at": "2018-06-20T15:55:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/497",
    "body": "# Summary\r\n\r\nI am working on a computer vision project. I have to work with 3 million 2048D vectors with only 16G memory. Since faiss keeps all the vectors in memory, I want to use PCA to reduce the number of dimensions to fit them into small memory. However, it seems that all the vectors have to be loaded into memory before I execute PCA, which is impossible for me.\r\n\r\nIs it possiable to run PCA with a stream of vector without loading all the vectors into memory at once?\r\n\r\nThank you.\r\n\r\nBest regards,\r\nShengdong\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on :\r\n- [x] CPU\r\n- [ ] GPU\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/497/comments",
    "author": "LuShengDong",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-06-20T15:55:29Z",
        "body": "Hi \r\nYou need only a sample of vectors to train the PCA. Then apply it by batches of say 10k vectors."
      }
    ]
  },
  {
    "number": 495,
    "title": "Nested Indexes",
    "created_at": "2018-06-19T18:32:46Z",
    "closed_at": "2018-06-20T16:34:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/495",
    "body": "# Summary\r\nI am trying to run the demo_ondisk_ivf.py, I want to try the PCA dimension reduction, I replaced this line\r\nindex = faiss.index_factory(xt.shape[1], \"IVF4096,Flat\")\r\n\r\nto \r\n\r\nindex = faiss.index_factory(xt.shape[1], \"PCAR8,IVF4096,Flat\")\r\n\r\nBut then, when in stage 5, how can I merge the images. Now the index is VectorTransform, not a IVFIndex, there's no index.invlists, how can I get index.invlists filed\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on :\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/495/comments",
    "author": "kwaibun",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-06-20T15:56:47Z",
        "body": "Hi\r\nIt is `faiss.downcast_Index(index.index).invlists`."
      },
      {
        "user": "kwaibun",
        "created_at": "2018-06-20T16:34:45Z",
        "body": "Cool, thanks!"
      }
    ]
  },
  {
    "number": 486,
    "title": "Using np.memmap() for large separate files",
    "created_at": "2018-06-12T07:28:45Z",
    "closed_at": "2018-06-13T03:45:12Z",
    "labels": [
      "question",
      "out-of-scope"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/486",
    "body": "# Summary\r\n\r\nI am working on a very large dataset (~100 million vectors of 2048 dimensions). I want to create a memmap for this data (as done in bench_polysemous_1bn.py). However, the dataset is distributed in about 100 binary files. What is the best way to do this? Do I need to combine all the binary files into one?\r\n\r\n\r\n# Platform\r\n\r\nOS: Linux 16.04\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on :\r\n- [ ] CPU\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/486/comments",
    "author": "khetanmayank",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-06-12T10:06:30Z",
        "body": "Hi \r\nEither solution is possible, mmapping 100 files is not a problem.\r\nNote that vectors of 2048 dimensions is quite large, you may want to reduce them eg. by PCA."
      },
      {
        "user": "zhouxiaoxu",
        "created_at": "2019-01-08T02:48:29Z",
        "body": "hi, @mdouze. I have 500M vectors with 1000 dimesions ,  I had try to reduce to 512 dimesions by PCA first. But the search Accuracy is low, about  50% compare to  Exhaustive searchã€‚ I think by PCA  transformï¼Œthe origin information was lost. so Are there  any other  kind of pre-processing to reduce the dimension with less information lossï¼Ÿ\r\n\r\n"
      }
    ]
  },
  {
    "number": 483,
    "title": "Faiss is optimized for batch search, but looks like during query time, the searches are done in parrel in different threads in OMP",
    "created_at": "2018-06-07T01:34:45Z",
    "closed_at": "2018-07-06T09:11:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/483",
    "body": "# Summary\r\nwhy matrix-matrix multiplication is not used in final query. I can see that knn_L2sqr_blas is implemented for IndexFlat search, and this is used to pick up piles of centroids during search. After getting the nprobes of clusters, seperate queries, say, 20 queries are conducted in vector-vector L2 distance comparison with OMP. Is there a reason for this? is blas not efficient for small blocks matrix compution?\r\n\r\n# Platform\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on :\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\n\r\n\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/483/comments",
    "author": "fishbell",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-06-08T09:54:57Z",
        "body": "Hi \r\nYou cannot map this to matrix-matrix product unless several vectors get quantized to the same centroids."
      },
      {
        "user": "fishbell",
        "created_at": "2018-06-13T01:48:23Z",
        "body": "yes I did not notice this. Thanks for your answer!"
      }
    ]
  },
  {
    "number": 475,
    "title": "It cost much more time on Approximate search when I ran bench_gpu_sift1m.py",
    "created_at": "2018-06-04T02:23:29Z",
    "closed_at": "2018-08-28T09:25:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/475",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\nOS: Ubuntu 16.04.3 LTS \r\n\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->v1.2.1\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->conda install faiss-gpu cuda90 -c pytorch # For CUDA9.0\r\n\r\n\r\nRunning on :\r\n- [ ] CPU  Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\r\n- [x] GPU TITAN X (Pascal)  12GB\r\nRAM 32GB\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\nHi!\r\nI have a gpu TITAN X (Pascal) 12GB. However ,when I ran Benchmarking script the bench_gpu_sift1m.py ,I got the Approximate search result which is quite different from the result  is shown  in Search on SIFT1M. It costed much more time on Approximate search, almost 100 times .Could you please give  some advice to cut the searching time?\r\n\r\n\r\nI had install  faiss from conda by \"conda install faiss-gpu cuda90 -c pytorch # For CUDA9.0\" .\r\n\r\nbench_gpu_sift1m.py .\r\n\r\n============ Exact search\r\nadd vectors to index\r\nwarmup\r\nbenchmark\r\n1 0\r\nk=1 0.438 s, R@1 0.9914\r\n2 1\r\nk=2 0.442 s, R@1 0.9940\r\n4 2\r\nk=4 0.442 s, R@1 0.9940\r\n8 3\r\nk=8 0.444 s, R@1 0.9940\r\n16 4\r\nk=16 0.447 s, R@1 0.9941\r\n32 5\r\nk=32 0.451 s, R@1 0.9914\r\n64 6\r\nk=64 0.461 s, R@1 0.9913\r\n128 7\r\nk=128 0.474 s, R@1 0.9941\r\n256 8\r\nk=256 0.506 s, R@1 0.9914\r\n512 9\r\nk=512 0.636 s, R@1 0.9914\r\n1024 10\r\nk=1024 1.063 s, R@1 0.9914\r\n============ Approximate search\r\ntrain\r\nWARNING clustering 100000 points to 4096 centroids: please provide at least 159744 training points\r\nadd vectors to index\r\nwarmup\r\nbenchmark\r\nnprobe=   1 4.352 s recalls=\r\n0.3901\r\n0.4312\r\n0.4312\r\nnprobe=   2 4.338 s recalls=\r\n0.5023\r\n0.5636\r\n0.5636\r\nnprobe=   4 4.348 s recalls=\r\n0.6033\r\n0.6897\r\n0.6897\r\nnprobe=   8 4.367 s recalls=\r\n0.6871\r\n0.8028\r\n0.8028\r\nnprobe=  16 4.375 s recalls=\r\n0.7519\r\n0.8940\r\n0.8940\r\nnprobe=  32 4.416 s recalls=\r\n0.7937\r\n0.9550\r\n0.9550\r\nnprobe=  64 4.488 s recalls=\r\n0.8115\r\n0.9834\r\n0.9834\r\nnprobe= 128 4.627 s recalls=\r\n0.8186\r\n0.9954\r\n0.9954\r\nnprobe= 256 4.908 s recalls=\r\n0.8209\r\n0.9994\r\n0.9994\r\nnprobe= 512 5.461 s recalls=\r\n0.8209\r\n1.0000\r\n1.0000\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/475/comments",
    "author": "bauhiniago",
    "comments": [
      {
        "user": "destino74",
        "created_at": "2018-06-10T07:53:30Z",
        "body": "The same for me. I installed from source, compiled with mkl.  I try single gpu(TITAN X) by the code \"bench_gpu_sift1m.cpp\", and it takes around 4 seconds for sifi1m dataset. How can I optimize the performance, anyone can help on this ? Thanks."
      },
      {
        "user": "destino74",
        "created_at": "2018-06-10T07:59:50Z",
        "body": "I noticed #312 may be related  to this quession"
      },
      {
        "user": "suhasjs",
        "created_at": "2018-06-21T05:51:45Z",
        "body": "Any progress on this issue? I'm facing the same 100x slowdowns on an Azure ND24 instance.\r\n- GPUs : 4x P40 (but using only gpu0 for bench)\r\n- CPUs: 2x Xeon E5-2690 v4 @ 2.6GHz, 24 cores\r\n- OS : 16.04.4 LTS\r\n- Library Version : faiss-gpu (1.2.1 - py27_cuda9.0.176_2 ), CUDA 9.0\r\n\r\nHere's the output from my run on SIFT 1M:\r\n\r\n```\r\nload data\r\nload GT\r\n============ Exact search\r\nadd vectors to index\r\nwarmup\r\nbenchmark\r\nk=1 0.548 s, R@1 0.9914\r\nk=2 0.573 s, R@1 0.9940\r\nk=4 0.574 s, R@1 0.9940\r\nk=8 0.575 s, R@1 0.9940\r\nk=16 0.585 s, R@1 0.9941\r\nk=32 0.588 s, R@1 0.9914\r\nk=64 0.595 s, R@1 0.9913\r\nk=128 0.604 s, R@1 0.9941\r\nk=256 0.596 s, R@1 0.9914\r\nk=512 0.662 s, R@1 0.9914\r\nk=1024 1.045 s, R@1 0.9914\r\n============ Approximate search\r\ntrain\r\nWARNING clustering 100000 points to 4096 centroids: please provide at least 159744 training points\r\nadd vectors to index\r\nwarmup\r\nbenchmark\r\nnprobe=   1 4.940 s recalls= 0.3901 0.4312 0.4312\r\nnprobe=   2 4.950 s recalls= 0.5023 0.5636 0.5636\r\nnprobe=   4 4.955 s recalls= 0.6033 0.6897 0.6897\r\nnprobe=   8 4.964 s recalls= 0.6871 0.8028 0.8028\r\nnprobe=  16 4.985 s recalls= 0.7519 0.8940 0.8940\r\nnprobe=  32 5.021 s recalls= 0.7937 0.9550 0.9550\r\nnprobe=  64 5.102 s recalls= 0.8115 0.9834 0.9834\r\nnprobe= 128 5.258 s recalls= 0.8186 0.9954 0.9954\r\nnprobe= 256 5.567 s recalls= 0.8209 0.9994 0.9994\r\nnprobe= 512 6.173 s recalls= 0.8209 1.0000 1.0000\r\n```\r\n"
      },
      {
        "user": "wickedfoo",
        "created_at": "2018-06-22T15:36:42Z",
        "body": "When compiling the code, did you specify the compute capability that matches the GPU that you are using for `nvcc`?\r\n"
      },
      {
        "user": "destino74",
        "created_at": "2018-06-24T10:07:36Z",
        "body": "Yes, I deteled the\r\n```\r\n-gencode arch=compute_35,code=\"compute_35\" \\ \r\n-gencode arch=compute_52,code=\"compute_52\" \\\r\n-gencode arch=compute_60,code=\"compute_60\" \\\r\n```\r\nreplace with\r\n```\r\n-gencode arch=compute_61,code=\"compute_61\" \\\r\n```\r\n\r\nMy GPU is Titan X Pascal.\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2018-08-28T09:25:45Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 472,
    "title": "Is it possible to do something like mean-shift clustering in FAISS?",
    "created_at": "2018-06-03T17:16:39Z",
    "closed_at": "2018-06-28T23:17:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/472",
    "body": "# Summary\r\n\r\nI am trying to find modes (dense clusters) in a distribution in a hyper-sphere. Mean-shift seams like a good solution for the task but it's not scalable beyond a few thousand samples. Adding a kernel (flat or Gaussian) to the k-means implementation could help perform that task within FAISS. Is there a way to do that? Any other ideas to work in my particular problem?\r\n\r\nThanks a lot!\r\n\r\n# Platform\r\n\r\nOS: Ubuntu 16.04\r\n\r\nFaiss version: conda install faiss-gpu -c pytorch\r\n\r\nRunning on :\r\n- [x] CPU\r\n- [ ] GPU\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/472/comments",
    "author": "eisenjulian",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-06-04T05:48:45Z",
        "body": "Hi, \r\nIt is not clear to me why mean shift should go into Faiss. Is it related to similarity search?\r\nWhat do you call a \"flat kernel\" ?\r\n"
      },
      {
        "user": "eisenjulian",
        "created_at": "2018-06-04T20:26:35Z",
        "body": "Hi @mdouze, thanks for the quick response! I am not saying it should go into it, it was a question. I'm curious if this is something that could be useful or fall within the scope of the library at some point in your opinion.\r\n\r\nPer the project description: \"Faiss is a library for efficient similarity search and clustering of dense vectors.\" and mean shift is another clustering algorithm. It seems that being so similar to k-means, it could be performed more efficiently by taking advantage of the quantization in Faiss. \r\n\r\nBy flat kernel, I mean just considering the points within a certain radius/bandwidth of the cluster center. "
      },
      {
        "user": "mdouze",
        "created_at": "2018-06-28T23:17:38Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 465,
    "title": "How to realign the remaining ids after remove_ids?",
    "created_at": "2018-05-31T01:50:23Z",
    "closed_at": "2018-06-28T23:31:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/465",
    "body": "After removing ids ,how to realign the remaining ids?\r\nfor example:\r\nremove before,the search result is :I=[[2,3,4,5,]]\r\nthen remove ID=0,\r\nwhat I want is I = [[1,2,3,4]]",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/465/comments",
    "author": "XiaXuehai",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-06-04T11:40:24Z",
        "body": "You could take a look at `IndexIDRemap`."
      },
      {
        "user": "XiaXuehai",
        "created_at": "2018-06-05T06:10:11Z",
        "body": "@beauby \r\nSorry,I run it like that:\r\n`index_new = faiss.IndexIDRemap(index)`\r\nbut it comes out `AttributeError: module 'faiss' has no attribute 'IndexIDRemap'`\r\nso can you tell me how to use it?"
      },
      {
        "user": "XiaXuehai",
        "created_at": "2018-06-14T09:16:47Z",
        "body": "I deal with it like thatï¼š  \r\n```\r\nindex.make_direct_map()\r\nrebuild_data = np.vstack([index.reconstruct(i) for i in range(index.ntotal) if i not in ids_to_delete]  )\r\nindex.reset()\r\nindex.add(rebuild_data)\r\n```\r\nLooks stupid."
      },
      {
        "user": "mdouze",
        "created_at": "2018-06-28T23:31:34Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 462,
    "title": "Searching time is different for c++ and python.",
    "created_at": "2018-05-29T01:47:19Z",
    "closed_at": "2018-06-12T10:15:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/462",
    "body": "I test the searching time by the file `tutorial/python/2-IVFFlat.py` and `tutorial/cpp/2-IVFFlat.cpp`. \r\nFor c++,I compiled faiss by INSTALL.md, the ide is codeblocks.For python I using the commend line installed\r\n\r\nc++:\r\n```\r\nclock_t start,stop;\r\nstart = clock();\r\nindex.search(nq, xq, k, D, I);\r\nstop = clock();\r\nprintf(\"time:%f\\n\",(double)(stop-start)/CLOCKS_PER_SEC);\r\n```\r\ntime = 0.692166 (index.nprobe = 1)\r\ntime = 1.485702 (index.nprobe = 10)\r\n\r\npython:  \r\n```\r\nt0 = time.time()\r\nD, I = index.search(xq, k)     # actual search\r\nt1 = time.time()\r\nprint(t1-t0)\r\n```\r\ntime = 0.03618192672729492 (index.nprobe = 1)\r\ntime = 0.2468726634979248 (index.nprobe = 10)\r\n\r\nWhy it is different?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/462/comments",
    "author": "XiaXuehai",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-05-29T08:24:35Z",
        "body": "The python version is just an interface to the C++ lib, so in your case it probably boils down to different compiling options."
      },
      {
        "user": "XiaXuehai",
        "created_at": "2018-05-29T10:19:42Z",
        "body": "It's so weird.I complied python interface instead conda install. It's same."
      },
      {
        "user": "beauby",
        "created_at": "2018-05-29T11:11:10Z",
        "body": "It is not the same because the flags used for compiling the C++ library were probably not the same, thus there's a good chance you're using a less performant BLAS/LAPACK implementation."
      },
      {
        "user": "XiaXuehai",
        "created_at": "2018-05-30T01:24:24Z",
        "body": "@beauby , \r\nI test it on the ohter machine.\r\n\r\n```\r\n[root@algorithm cpp]# ./2-IVFFlat \r\nUse Time:0.580000\r\nI=\r\n10049 10147 10188 10184 \r\n 9403  9750  9812 10346 \r\n10361 10184  9920 10333 \r\n 9895  9946  9335  9677 \r\n10876  9647  9756 11103 \r\nUse Time:2.030000\r\nI=\r\n10842 10827  9938 10004 \r\n 9403 10267 10880 10330 \r\n 9896 10146 10093 10361 \r\n 8603 10523 10582  9895 \r\n11460 10123 11099 10876 \r\n\r\n[root@algorithm python]# python 2-IVFFlat.py \r\nUse Time: 0.07471847534179688\r\n[[ 9900 10500  9831 10808]\r\n [11055 10812 11321 10260]\r\n [11353 10164 10719 11013]\r\n [10571 10203 10793 10952]\r\n [ 9582 10304  9622  9229]]\r\nUse Time: 0.5254294872283936\r\n[[ 9900 10500  9309  9831]\r\n [11055 10895 10812 11321]\r\n [11353 11103 10164  9787]\r\n [10571 10664 10632  9638]\r\n [ 9628  9554 10036  9582]]\r\n```\r\nyou said I'm using a less performant BLAS/LAPACK implementation, but I did not modify the makefile except the right path of openblas in my machine, please help me to fix this?\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2018-06-12T10:15:41Z",
        "body": "Please refer to documentation."
      }
    ]
  },
  {
    "number": 460,
    "title": "How to add data to index?",
    "created_at": "2018-05-25T07:17:24Z",
    "closed_at": "2018-06-04T11:41:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/460",
    "body": "I have built the index by the dataset,and stored on dask.But the dataset is changing by adding vector or deleting vectors frequently.\r\n\r\nSo does I must rebuild the index everytime or just add/delete the vector from the index built before?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/460/comments",
    "author": "XiaXuehai",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-05-25T09:04:43Z",
        "body": "You can use the `add()` and `remove_ids()` methods."
      },
      {
        "user": "XiaXuehai",
        "created_at": "2018-05-25T09:43:33Z",
        "body": "@beauby ,THX. Assume that,If I use the IVFxï¼Œand delete too many vectors from the database.the clusters built before is not correct.\r\nif num_vectors <1000,I use `IndexFlatL2`,else, use `IVFx`,` x = num_vectors /100`. I'm not sure  it is right?"
      },
      {
        "user": "beauby",
        "created_at": "2018-05-25T10:45:25Z",
        "body": "As long as the distribution of the vectors in your training set is close to that of your dataset, the clustering should be ok.\r\nRegarding the number of clusters, the right number depends on the structure of your data."
      },
      {
        "user": "beauby",
        "created_at": "2018-06-04T11:41:46Z",
        "body": "Closing as the issue is resolved. Feel free to keep commenting should you need further help. "
      },
      {
        "user": "engmubarak48",
        "created_at": "2019-07-04T13:22:03Z",
        "body": "> You can use the `add()` and `remove_ids()` methods.\r\n\r\nWhy there is no documentation of adding and removing vectors?. For example, if someone saved an index and would like to remove vectors. \r\n\r\nIf someone knows how to use these methods or has a link to look for plz share with us. "
      },
      {
        "user": "gadregayatri",
        "created_at": "2022-06-08T19:37:46Z",
        "body": "I have noticed that if I add a vector, say 'z' to already computed index with IVFx, and then  search the same vector 'z' for this updated index, it does not return 'z' to me. I even tried retraining the  index but no luck. Any idea what could I be doing so that the updated index considers newly added vectors too?"
      },
      {
        "user": "fkurushin",
        "created_at": "2024-04-27T10:42:38Z",
        "body": "@gadregayatri just tried it right now `add` method works fine for me. Keep in mind that faiss add the new vectors like stack i.e. in the end.\r\n\r\n\r\nupd: tested `remove_ids` works too =)"
      }
    ]
  },
  {
    "number": 458,
    "title": "Libgomp: Thread creation failed: Resource temporarily unavailable",
    "created_at": "2018-05-23T09:48:52Z",
    "closed_at": "2018-06-12T10:15:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/458",
    "body": "Released a faiss service with thrift, my thrift service opened 100 threads, requests more than one, it will give an error:\r\nLibgomp: Thread creation failed: Resource temporarily unavailable\r\n\r\nulimit -u 65535",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/458/comments",
    "author": "fuchao01",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-05-23T22:31:03Z",
        "body": "Hi \r\nYou may want to compile Faiss without threading if you are using thrift to do the multi-threading. OpenMP has a non-trivial overhead when a new non-openmp thread is started.\r\n"
      },
      {
        "user": "fuchao01",
        "created_at": "2018-05-25T02:53:27Z",
        "body": "@mdouze Thank you for your reply.How to compile faiss without threads"
      },
      {
        "user": "mdouze",
        "created_at": "2018-05-25T08:11:55Z",
        "body": "In `makefile.inc` in the `CFLAGS` variable replace `-fopenmp` with `-fno-openmp`. Adding `-Wno-error=unknown-pragmas` will quiet all the warnings. "
      },
      {
        "user": "fuchao01",
        "created_at": "2018-05-25T09:54:26Z",
        "body": "This really does. But there is a problem, performance is not as good as before. Can you specify the maximum number of openmp threads?"
      },
      {
        "user": "fuchao01",
        "created_at": "2018-05-25T10:05:55Z",
        "body": "faiss.omp_set_num_threads() This parameter is not set openmp open thread number?"
      },
      {
        "user": "fuchao01",
        "created_at": "2018-05-25T15:30:26Z",
        "body": "I have 200w indexed data, qps 100/s, thrift server 100 threads. The faiss flat index is used. The server is basically running at full capacity. 32-core cpu, 128g memory, load reaches 40+. Is the amount of data too large for the index?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-06-12T10:15:20Z",
        "body": "No clear question. Closing."
      }
    ]
  },
  {
    "number": 446,
    "title": "\"Dynamic\" Nearest Neighbour searches possible?",
    "created_at": "2018-05-13T20:58:44Z",
    "closed_at": "2018-05-25T10:52:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/446",
    "body": "Is it possible to use `faiss` for a task where need to get the nearest neighbour that satisfies a certain criteria?\r\n\r\nI don't know how many datapoints I would need for each input. So I would have to get a batch of `k` NNs, check if one of them satisfies the criteria and if not get the next batch. Do you expect drastic slowdowns when the size of `k` is large? Can I skip the first `n`?\r\nIt's also possible to evaluate the neighbours one by one, if that makes it easier.\r\n\r\nEDIT: also the dimensionality is quite large and the dataset doesn't fit into the RAM ðŸ˜€ ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/446/comments",
    "author": "LeanderK",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2018-05-15T17:40:13Z",
        "body": "The best way to solve this using Faiss would be to start with a very large size of k, and just query once; if the vector you are looking for is not in the results, then tough luck. Faiss is largely memory bandwidth bound, so it is best to traverse all of the memory a single time, rather than doing it many times.\r\n\r\nFor CPU, `k` is practically unlimited, but ideally it should be small enough such that the heap fits into L1/L2. For GPU, `k` is limited to 1024 or less.\r\n"
      },
      {
        "user": "beauby",
        "created_at": "2018-05-25T10:52:04Z",
        "body": "Closing this as the issue seems to be solved. Feel free to keep commenting if you have further questions."
      }
    ]
  },
  {
    "number": 445,
    "title": "Batch processing in python and/or pytorch",
    "created_at": "2018-05-13T17:54:22Z",
    "closed_at": "2018-06-12T10:12:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/445",
    "body": "Hello,\r\nThank you for this great library.\r\nI would like to ask if there exist a way to run the similarity search batch-wise in python to avoid a for loop for the number of examples in the batch.\r\nConcretely, instead of having the inputs xb and xq (of size (nb,d) and (nq,d), respectively), let us have xb (of size (bs,nb,d)) and xq (of size (bs,nq,d)), where nb is the number of batches. Does FAISS support such a batch process where for each pair of xb[i,:,:] and xq(i,:,:) (belonging to batch i), it would process independently. ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/445/comments",
    "author": "egundogdu",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-05-22T08:50:29Z",
        "body": "Yes, it is possible to do batched queries, just passing several queries at a time (in one function call)."
      },
      {
        "user": "egundogdu",
        "created_at": "2018-05-22T13:20:00Z",
        "body": "Hello,\r\nFor the xb and xq tensors (BxNxC), is there any EXAMPLE script that can make the NN calculations batch-wise? I would like to avoid the for loop in the below function:\r\n```\r\ndef NNTwoSets_FAISS(A,B,k):\r\n    xq = A.transpose(2, 1).cpu().data.numpy()\r\n    xb = B.transpose(2, 1).cpu().data.numpy()\r\n    szVec = xq.shape\r\n    IND = np.zeros((szVec[0],szVec[1],k)).astype(np.float32)\r\n    DIST = np.zeros((szVec[0],szVec[1],k)).astype(np.float32)\r\n    for bn in range(szVec[0]):\r\n        index = faiss.IndexFlatL2(3)\r\n        index.add(xb[bn,:,:])\r\n        dd, indind = index.search(xq[bn,:,:], k)\r\n        IND[bn,:,:] = indind\r\n        DIST[bn,:,:] = dd\r\n    IND = Variable(torch.from_numpy(IND), requires_grad=False).type(torch.LongTensor).cuda(A.get_device())\r\n    DIST = Variable(torch.from_numpy(DIST), requires_grad=False).cuda(A.get_device())\r\n    return IND, DIST\r\n```"
      },
      {
        "user": "egundogdu",
        "created_at": "2018-05-22T13:26:35Z",
        "body": "Concretely, I did not mean to have several queries.\r\nI mean what if I have BN number of databases so that the xb is not 2d but 3d tensor (BNxNxC), where N is the number of points in each of the datasets."
      },
      {
        "user": "beauby",
        "created_at": "2018-05-25T10:53:57Z",
        "body": "If I understand correctly, you'd like to query separate indexes in parallel? If so, it is indeed achievable using threads."
      },
      {
        "user": "mdouze",
        "created_at": "2018-06-12T10:12:09Z",
        "body": "No activity, closing."
      },
      {
        "user": "mbanani",
        "created_at": "2020-09-09T12:54:45Z",
        "body": "I have a very similar use-case where I have a batch of separate indexes (B x N x C) and a batch of queries (B x Q x C), and I would like to perform the search appropriately (preferably on GPU). Could you please provide any pointers to how to perform this? \r\n\r\n@egundogdu Were you able to find a solution that didn't involve iterating over the batch? \r\n\r\nThank you for your help. \r\n"
      }
    ]
  },
  {
    "number": 442,
    "title": "What the mean of -1  in the output of I?",
    "created_at": "2018-05-11T02:24:33Z",
    "closed_at": "2018-06-12T10:11:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/442",
    "body": "I use the factory like this:\r\n`faiss.index_factory(d, \"PCA504,IMI2x10,SQ8\")`\r\n\r\nmy database : `xb.shape=(2M,674)`,\r\nmy query vector is : `xq.shape=(1,674)`,\r\nand the output of `I` is :\r\n` [[97978 97984 97981    -1    -1    -1    -1    -1    -1    -1]]`\r\n\r\nthe first 3 ID is right.\r\nWhat the mean of `-1`?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/442/comments",
    "author": "XiaXuehai",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-05-11T13:00:03Z",
        "body": "It means there are no more results to return. \r\nPlease increase the nprobe to get more results. \r\n\r\n`ParameterSpace().set_index_parameter(index, 'nprobe', 100)`"
      },
      {
        "user": "XiaXuehai",
        "created_at": "2018-05-14T05:51:48Z",
        "body": "Can I use the `index.nprobe = 100`?but  it was no use.\r\n\r\nwhat's the difference between `index.nprobe = 100` and `ParameterSpace().set_index_parameter(index, 'nprobe', 100)`?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-06-12T10:11:54Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 440,
    "title": "Fails to add vectors to GpuIndexIVFPQ ",
    "created_at": "2018-05-09T09:27:23Z",
    "closed_at": "2018-05-17T06:49:25Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/440",
    "body": "# Summary\r\n\r\nTrying to train GpuIndexIVFPQ and add vectors to the index. Seg fault.\r\n\r\n# Platform\r\n\r\nOS: Ubuntu 16.04\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on :\r\n- [x] GPU Titan X 12GB\r\n\r\n# Reproduction instructions\r\n\r\nBriefly, I was trying to search in 50 million 128D vectors. I used GpuIndexIVFPQ(PQ8) with a GTX Titan X with 12 GB memory. It crashes when I added the vectors to the index. However, it works well when I add 40 millions vectors. I checked the nvidia-smi and find that there is enough mem to use.\r\n\r\nHere is the bt of gdb:\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x00007fffdc1e9b9c in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n(gdb) bt\r\n#0  0x00007fffdc1e9b9c in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#1  0x00007fffdc29610e in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#2  0x00007fffdc35edf9 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#3  0x00007fffdc35f9c5 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#4  0x00007fffdc297620 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#5  0x00007fffdc1b93f8 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#6  0x00007fffdc1ba910 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#7  0x00007fffdc2fa8b2 in cuMemcpyHtoDAsync_v2 ()\r\n   from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#8  0x00007ffff416e8cc in ?? ()\r\n   from /usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n#9  0x00007ffff414ab5b in ?? ()\r\n   from /usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n#10 0x00007ffff4184b08 in cudaMemcpyAsync ()\r\n   from /usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n#11 0x0000000000435609 in faiss::gpu::Tensor<float, 2, true, int, faiss::gpu::traits::DefaultPtrTraits>::copyFrom(faiss::gpu::Tensor<float, 2, true, int, faiss::gpu::traits::DefaultPtrTraits>&, CUstream_st*) ()\r\n#12 0x0000000000433a69 in faiss::gpu::DeviceTensor<float, 2, true, int, faiss::gpu::traits::DefaultPtrTraits> faiss::gpu::toDevice<float, 2>(faiss::gpu::GpuResources*, int, float*, CUstream_st*, std::initializer_list<int>) ()\r\n#13 0x000000000043de15 in faiss::gpu::GpuIndexIVFPQ::addImpl_(long, float const*, long const*) ()\r\n#14 0x000000000043a9e2 in faiss::gpu::GpuIndex::addInternal_(long, float const*, long const*) ()\r\n#15 0x000000000043a744 in faiss::gpu::GpuIndex::add_with_ids(long, float const*, long const*) ()\r\n#16 0x000000000040c90b in CwAnnTopkImpl::add_with_batch_gpu (\r\n    this=0x7fffffffc080, vec_feats=0x7ff9d3128010, feat_num=50099900, \r\n    ids=0x7ff9b52e0010) at CwAnnTopkImpl.cpp:219\r\n---Type <return> to continue, or q <return> to quit---\r\n#17 0x000000000040daed in CwAnnTopkImpl::add_with_ids_cwfeat_gpu (\r\n    this=0x7fffffffc080, vec_feats=0x7ff9d3128010, feat_num=50099900, \r\n    feat_dim=128, ids=0x7ff9b52e0010) at CwAnnTopkImpl.cpp:560\r\n#18 0x0000000000409b00 in main (argc=1, argv=0x7fffffffe118)\r\n    at test/test_cwimpl_testhitrate.cpp:143\r\n\r\n\r\nAppreciate any help.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/440/comments",
    "author": "ZhuoranLyu",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2018-05-15T17:55:39Z",
        "body": "@ZhuoranLyu can you use gdb to print out the locals and arguments to stack frame 11 above (the one with `faiss::gpu::Tensor<float, 2, true, int, faiss::gpu::traits::DefaultPtrTraits>::copyFrom(faiss::gpu::Tensor<float, 2, true, int, faiss::gpu::traits::DefaultPtrTraits>&, CUstream_st*) ()`?"
      },
      {
        "user": "wickedfoo",
        "created_at": "2018-05-15T17:56:09Z",
        "body": "similarly, the arguments and locals to `0x000000000043a9e2 in faiss::gpu::GpuIndex::addInternal_(long, float const*, long const*) ()`?\r\n\r\n"
      },
      {
        "user": "ZhuoranLyu",
        "created_at": "2018-05-17T06:23:52Z",
        "body": "@wickedfoo I try to use info args to get the arguments of certain stack frame. However, it always says no symbol table info available. Any other ways to print out the locals?\r\n"
      },
      {
        "user": "ZhuoranLyu",
        "created_at": "2018-05-17T06:49:22Z",
        "body": "figure it out. my bad."
      }
    ]
  },
  {
    "number": 439,
    "title": "Is there any function to find vector by its id?",
    "created_at": "2018-05-09T07:15:13Z",
    "closed_at": "2018-06-12T10:09:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/439",
    "body": "# Summary\r\n\r\nIs there any function provided so that I can get a vector by its id?\r\nor can I calculate the distance between two vectors by providing their ids?\r\nTHANK U!\r\n\r\n\r\n\r\nRunning on :\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/439/comments",
    "author": "xichen714",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-05-09T08:36:51Z",
        "body": "Hi \r\nPlease see if `index.reconstruct(id)` works for you. See also #374\r\n\r\n"
      },
      {
        "user": "xichen714",
        "created_at": "2018-05-10T01:49:14Z",
        "body": "Hi,\r\nI tried index.reconstruct(id), and I use the IndexFlatL2 index. It seems that this type of index does not provide the reconstruct function? so should I try to use other types of index like IndexIVFFlat?\r\nThanks!"
      },
      {
        "user": "mdouze",
        "created_at": "2018-05-11T12:56:48Z",
        "body": "What error do you get with reconstruct(id)?  "
      },
      {
        "user": "mdouze",
        "created_at": "2018-06-12T10:09:06Z",
        "body": "No activity, closing."
      },
      {
        "user": "Daemonyz",
        "created_at": "2018-12-14T02:10:07Z",
        "body": "> What error do you get with reconstruct(id)?\r\n\r\n@mdouze hi, if the id does not exist in the index, the reconstruct(id) function will throws an exception. Does faiss have another function to check if the id exists in the origin index file please?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-12-14T09:24:13Z",
        "body": "You could catch the exception then. \r\nThere is no other function to check if an id exists."
      },
      {
        "user": "mylyu",
        "created_at": "2019-01-07T12:08:17Z",
        "body": "> You could catch the exception then.\r\n> There is no other function to check if an id exists.\r\n\r\n@mdouze when I reconstruct from a id not existing in the index, it gives me \"Segmentation fault\", which leads to python interpreter crash and cannot be caught. I am using 1.4 version from conda. Any workaround to check if an id exists? Thanks."
      }
    ]
  },
  {
    "number": 436,
    "title": "Get less than k centroids using Kmeans function",
    "created_at": "2018-05-08T02:49:52Z",
    "closed_at": "2018-05-14T07:06:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/436",
    "body": "# Summary\r\n\r\nI try to use faiss.Kmeans in Python2.7 to cluster 249 data points with 512 dimensions into k=2 clusters, but I get only 1 cluster result. What cause this and how to solve it?\r\n\r\n# Platform\r\n\r\nOS: Ubuntu 16.04\r\n\r\nFaiss version: git commit, 1ae7494491ecc3f13299e217ea466030980f4a70\r\n\r\nFaiss compilation options: docker build -f Dockerfile .\r\n\r\nRunning on :\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\ncode snippet:\r\n    d = data.shape[1]\r\n    clus = faiss.Kmeans(d, 2, d, True)\r\n    clus.train(data)\r\n\r\nterminal print:\r\nClustering 249 points in 512D to 2 clusters, redo 1 times, 512 iterations\r\n    Preprocessing in 0.00s\r\n    Iteration 511 (0.11s, search 0.09s): objective=0.52617, imbalance=2.000 nsplit=1\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/436/comments",
    "author": "Ken2yLiu",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-05-09T07:59:38Z",
        "body": "Hi \r\nPlease check if the 249  points are not all identical."
      },
      {
        "user": "Ken2yLiu",
        "created_at": "2018-05-09T08:32:02Z",
        "body": "Well, I use np.unqiue() to check the 249 points, find that actually there are just 2 points. But why can't it return 2 clusters with one point each?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-05-09T08:41:31Z",
        "body": "Don't you think this is a little bit of a corner case ? :-)"
      },
      {
        "user": "Ken2yLiu",
        "created_at": "2018-05-11T07:34:26Z",
        "body": "Well, as I am building a hierarchical KMeans using faiss KMeans as backend, so I need to cover all the case that may happend. Anyway, I fix that by just checking if the unique points number is more than the k parameter, otherwise, I terminate that node in hierarchical processing. Thx!"
      }
    ]
  },
  {
    "number": 430,
    "title": "How to lock the index file in memory?",
    "created_at": "2018-05-03T03:36:14Z",
    "closed_at": "2018-05-16T09:40:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/430",
    "body": "When I build the index,and it wasted a lot of time to reload it, so how to lock the index file in memory so that I can using it fastly?\r\nI'm sorry, maybe it's not a problem about faiss.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/430/comments",
    "author": "XiaXuehai",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-05-04T11:19:33Z",
        "body": "Sorry I do not understand your question."
      },
      {
        "user": "XiaXuehai",
        "created_at": "2018-05-11T02:28:01Z",
        "body": "Sorry,I have solved the question.Please close it.@beauby"
      }
    ]
  },
  {
    "number": 428,
    "title": "Is it possible to apply faiss when each item is represented as a set of vectors",
    "created_at": "2018-04-30T14:54:45Z",
    "closed_at": "2018-06-12T10:07:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/428",
    "body": "Hi, I've read the most wiki's pages and went through the closed/opened issues and could find the answer to my question.\r\n\r\nI have images represented as descriptors. Each image is described via a set of descriptors (`N` descriptors per image). The size of a descriptor is fixed and equals to `M`. \r\n\r\nThe resulted dataset has a shape `(-1, N, M)`\r\nThe size of `N` is around 400-500 and the size of `M` is around 30, so it's not possible I guess to flatten them by having vectors of size `N * M` because it's too big.\r\n\r\nIs it possible to apply faiss for this kind of data?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/428/comments",
    "author": "Arsey",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-04-30T15:12:00Z",
        "body": "Why would it not be possible to flatten them?"
      },
      {
        "user": "Arsey",
        "created_at": "2018-04-30T15:16:11Z",
        "body": "@beauby in my case the size of vectors is at least 12 000 (N*M) and also a training set is around 1M samples. I'm afraid my hardware is not enough for it or faiis will not work. Another question is how to quantize correctly flatten descriptors? "
      },
      {
        "user": "beauby",
        "created_at": "2018-05-02T10:52:23Z",
        "body": "There is no reason that it will not work, but usually we work with embeddings that are 100-1000 dimensions."
      },
      {
        "user": "Arsey",
        "created_at": "2018-05-02T10:54:45Z",
        "body": "@beauby So should I just try it but with more RAM?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-06-12T10:07:08Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 421,
    "title": "how to dump index to disk with python ?",
    "created_at": "2018-04-26T08:08:33Z",
    "closed_at": "2018-04-27T07:48:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/421",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on :\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/421/comments",
    "author": "hbyang2",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-04-27T07:48:08Z",
        "body": "You can use `faiss.write_index(index, filename)`."
      }
    ]
  },
  {
    "number": 420,
    "title": "how to associate a vector with an id?",
    "created_at": "2018-04-26T08:07:50Z",
    "closed_at": "2018-05-11T13:30:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/420",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on :\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/420/comments",
    "author": "hbyang2",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-04-26T09:13:34Z",
        "body": "Could you be more specific?"
      },
      {
        "user": "hbyang2",
        "created_at": "2018-04-26T09:26:17Z",
        "body": "i want to accord to the return similar vector to find the idã€‚"
      },
      {
        "user": "beauby",
        "created_at": "2018-04-27T09:31:13Z",
        "body": "Are you looking for `add_with_ids()`?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-05-11T13:30:04Z",
        "body": "No activity. Closing."
      }
    ]
  },
  {
    "number": 418,
    "title": "Cannot compile custom project using faiss",
    "created_at": "2018-04-25T07:41:39Z",
    "closed_at": "2018-05-11T13:29:14Z",
    "labels": [
      "question",
      "out-of-scope"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/418",
    "body": "# Summary\r\n\r\ni use  this  file :**tutorial/cpp/1-Flat.cpp**  to use faiss in Qt5.8,but  get this error info\r\n\r\n# Platform\r\n\r\nOS: ubuntu16.04\r\n\r\nFaiss version: 1.2.1\r\n\r\nFaiss compilation options: openblas\r\n\r\nRunning on :\r\n- CPU\r\n\r\n\r\n# Reproduction instructions\r\n**error info:**\r\n```\r\n11:23:37: Running steps for project similarity_search...\r\n11:23:37: Configuration unchanged, skipping qmake step.\r\n11:23:37: Starting: \"/usr/bin/make\" \r\ng++ -fopenmp -Wl,-O1 -Wl,-rpath,/home/pa_sz_ai/Qt5.8.0/5.8/gcc_64/lib -o similarity_search main.o Heap.o Index.o IndexFlat.o utils.o   -L./-llibfaiss.a -L/home/pa_sz_ai/Qt5.8.0/5.8/gcc_64/lib -lQt5Core -lpthread \r\nIndex.o: In function `faiss::Index::remove_ids(faiss::IDSelector const&)':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/Index.cpp:49: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/Index.cpp:49: undefined reference to `typeinfo for faiss::FaissException'\r\nIndex.o: In function `faiss::Index::reconstruct(long, float*) const':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/Index.cpp:55: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/Index.cpp:55: undefined reference to `typeinfo for faiss::FaissException'\r\nIndex.o: In function `faiss::Index::range_search(long, float const*, float, faiss::RangeSearchResult*) const':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/Index.cpp:31: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/Index.cpp:31: undefined reference to `typeinfo for faiss::FaissException'\r\nIndex.o: In function `faiss::Index::add_with_ids(long, float const*, long const*)':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/Index.cpp:45: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/Index.cpp:45: undefined reference to `typeinfo for faiss::FaissException'\r\nIndex.o: In function `faiss::FaissException::~FaissException()':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/FaissException.h:20: undefined reference to `vtable for faiss::FaissException'\r\nIndex.o: In function `faiss::FaissException::~FaissException()':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/FaissException.h:20: undefined reference to `vtable for faiss::FaissException'\r\nIndexFlat.o: In function `faiss::IndexFlatL2BaseShift::search(long, float const*, long, float*, long*) const':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/IndexFlat.cpp:140: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/IndexFlat.cpp:140: undefined reference to `typeinfo for faiss::FaissException'\r\nIndexFlat.o: In function `faiss::IndexFlat1D::search(long, float const*, long, float*, long*) const':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/IndexFlat.cpp:314: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/IndexFlat.cpp:314: undefined reference to `typeinfo for faiss::FaissException'\r\nIndexFlat.o: In function `faiss::IndexRefineFlat::search(long, float const*, long, float*, long*) const':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/IndexFlat.cpp:223: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/IndexFlat.cpp:223: undefined reference to `typeinfo for faiss::FaissException'\r\nIndexFlat.o: In function `faiss::IndexRefineFlat::IndexRefineFlat(faiss::Index*)':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/IndexFlat.cpp:160: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/IndexFlat.cpp:160: undefined reference to `typeinfo for faiss::FaissException'\r\nIndexFlat.o: In function `faiss::IndexRefineFlat::add(long, float const*)':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/IndexFlat.cpp:178: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/IndexFlat.cpp:178: undefined reference to `typeinfo for faiss::FaissException'\r\nutils.o: In function `faiss::get_mem_usage_kb()':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:89: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:89: undefined reference to `typeinfo for faiss::FaissException'\r\nutils.o: In function `void faiss::range_search_sse<false>(float const*, float const*, unsigned long, unsigned long, unsigned long, float, faiss::RangeSearchResult*) [clone ._omp_fn.16]':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1232: undefined reference to `faiss::RangeSearchPartialResult::RangeSearchPartialResult(faiss::RangeSearchResult*)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1241: undefined reference to `faiss::RangeSearchPartialResult::new_result(long)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1259: undefined reference to `faiss::RangeSearchPartialResult::finalize()'\r\nutils.o: In function `faiss::RangeSearchPartialResult::~RangeSearchPartialResult()':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/AuxIndexStructures.h:139: undefined reference to `faiss::BufferList::~BufferList()'\r\nutils.o: In function `faiss::BufferList::add(long, float)':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/AuxIndexStructures.h:121: undefined reference to `faiss::BufferList::append_buffer()'\r\nutils.o: In function `void faiss::range_search_sse<true>(float const*, float const*, unsigned long, unsigned long, unsigned long, float, faiss::RangeSearchResult*) [clone ._omp_fn.15]':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1232: undefined reference to `faiss::RangeSearchPartialResult::RangeSearchPartialResult(faiss::RangeSearchResult*)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1241: undefined reference to `faiss::RangeSearchPartialResult::new_result(long)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1259: undefined reference to `faiss::RangeSearchPartialResult::finalize()'\r\nutils.o: In function `faiss::RangeSearchPartialResult::~RangeSearchPartialResult()':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/AuxIndexStructures.h:139: undefined reference to `faiss::BufferList::~BufferList()'\r\nutils.o: In function `faiss::BufferList::add(long, float)':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/AuxIndexStructures.h:121: undefined reference to `faiss::BufferList::append_buffer()'\r\nutils.o: In function `knn_inner_product_blas':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:851: undefined reference to `sgemm_'\r\nutils.o: In function `knn_L2sqr_blas<faiss::NopDistanceCorrection>':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:900: undefined reference to `sgemm_'\r\nutils.o: In function `knn_L2sqr_blas<faiss::BaseShiftDistanceCorrection>':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:900: undefined reference to `sgemm_'\r\nutils.o: In function `faiss::matrix_qr(int, int, float*)':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1331: undefined reference to `sgeqrf_'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1336: undefined reference to `sgeqrf_'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1339: undefined reference to `sorgqr_'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1324: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1324: undefined reference to `typeinfo for faiss::FaissException'\r\nutils.o: In function `faiss::pairwise_L2sqr(long, long, float const*, long, float const*, float*, long, long, long)':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1384: undefined reference to `sgemm_'\r\nutils.o: In function `faiss::bincode_hist(unsigned long, unsigned long, unsigned char const*, int*)':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1651: undefined reference to `faiss::FaissException::FaissException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char const*, char const*, int)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1651: undefined reference to `typeinfo for faiss::FaissException'\r\nutils.o: In function `range_search_blas<true>':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1154: undefined reference to `faiss::RangeSearchPartialResult::RangeSearchPartialResult(faiss::RangeSearchResult*)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1165: undefined reference to `sgemm_'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1176: undefined reference to `faiss::RangeSearchPartialResult::new_result(long)'\r\nMakefile:248: recipe for target 'similarity_search' failed\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1208: undefined reference to `faiss::RangeSearchPartialResult::set_result(bool)'\r\nutils.o: In function `faiss::RangeSearchPartialResult::~RangeSearchPartialResult()':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/AuxIndexStructures.h:139: undefined reference to `faiss::BufferList::~BufferList()'\r\nutils.o: In function `faiss::BufferList::add(long, float)':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/AuxIndexStructures.h:121: undefined reference to `faiss::BufferList::append_buffer()'\r\nutils.o: In function `range_search_blas<false>':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1154: undefined reference to `faiss::RangeSearchPartialResult::RangeSearchPartialResult(faiss::RangeSearchResult*)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1165: undefined reference to `sgemm_'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1176: undefined reference to `faiss::RangeSearchPartialResult::new_result(long)'\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/utils.cpp:1208: undefined reference to `faiss::RangeSearchPartialResult::set_result(bool)'\r\nutils.o: In function `faiss::RangeSearchPartialResult::~RangeSearchPartialResult()':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/AuxIndexStructures.h:139: undefined reference to `faiss::BufferList::~BufferList()'\r\nutils.o: In function `faiss::BufferList::add(long, float)':\r\n/home/pa_sz_ai/WorkSpace/PA_AI/similarity_search_qt/similarity_search/include/faiss/AuxIndexStructures.h:121: undefined reference to `faiss::BufferList::append_buffer()'\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [similarity_search] Error 1\r\n11:23:38: The process \"/usr/bin/make\" exited with code 2.\r\nError while building/deploying project similarity_search (kit: Desktop Qt 5.8.0 GCC 64bit)\r\nWhen executing step \"Make\"\r\n11:23:38: Elapsed time: 00:00.\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/418/comments",
    "author": "aa12356jm",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-04-25T09:09:39Z",
        "body": "Please provide the information requested in the issue template."
      },
      {
        "user": "mdouze",
        "created_at": "2018-05-11T13:29:14Z",
        "body": "No activity. Closing."
      }
    ]
  },
  {
    "number": 416,
    "title": "Question on blas",
    "created_at": "2018-04-24T10:15:35Z",
    "closed_at": "2018-04-24T11:35:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/416",
    "body": "I am new to faiss and wonder why faiss use blas while many other platforms use eigen (such as tensorflow) or even use mkl directly. Is it because blas works better in performance?\r\n\r\nThanks\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/416/comments",
    "author": "feitianxue",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-04-24T11:35:30Z",
        "body": "@feitianxue You can compile Faiss to use MKL (c.f. `makefile.inc`), or other BLAS implementations."
      }
    ]
  },
  {
    "number": 413,
    "title": "Is the distance returned using L2?",
    "created_at": "2018-04-22T09:14:23Z",
    "closed_at": "2018-04-23T10:58:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/413",
    "body": "# Summary\r\n\r\nHello guys, when the index return distance, is that the L2 distance?\r\nBecause I also build a KD-tree with scikit learn, it returns L2 distances. But the two distance is totally different.\r\n\r\n```\r\n>>> indexflatL2.search(emb2.astype('float32'), k=10)\r\n(array([[1.0788677, 1.1134999, 1.1210456, 1.1379397, 1.1737218, 1.1818781,\r\n        1.2293975, 1.2542101, 1.3202076, 1.4002445]], dtype=float32), array([[6, 5, 3, 2, 8, 7, 1, 0, 9, 4]]))\r\n>>> tree.query(emb2.astype('float32'), k=10)\r\n(array([[1.03868553, 1.05522505, 1.05879439, 1.06674252, 1.08338444,\r\n        1.08714219, 1.10878202, 1.11991525, 1.14900282, 1.18331924]]), array([[6, 5, 3, 2, 8, 7, 1, 0, 9, 4]]))\r\n```\r\n\r\nRunning on :\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\nUse sample database of (10, 128), search vector (1, 128) to test distance between Kd-tree (scikit-learn) and faiss\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/413/comments",
    "author": "hminle",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-04-23T09:36:58Z",
        "body": "Here it seems it is returning the square of the L2 distance. cc @mdouze"
      },
      {
        "user": "beauby",
        "created_at": "2018-04-23T10:58:49Z",
        "body": "So in general, our L2 indexes return the square (L2) distance."
      },
      {
        "user": "hminle",
        "created_at": "2018-04-24T02:29:16Z",
        "body": "@beauby but I calculate the distance directly, and the results is the same with kd-tree (scikit), I don't know why faiss generates the different distance? "
      },
      {
        "user": "beauby",
        "created_at": "2018-04-24T08:10:20Z",
        "body": "@hminle It is just squared. If you take the square root of the distances returned by Faiss, you will get the same value as with the kd-tree."
      }
    ]
  },
  {
    "number": 409,
    "title": "[Repetition ids]: add data into trained index",
    "created_at": "2018-04-18T09:00:26Z",
    "closed_at": "2018-04-27T09:33:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/409",
    "body": "Hi\r\n\r\nWhen I load an trained index from disk, \r\n1. add 10 vectors into it (actually, this index was empty before) \r\nquery index - the result ids is **[[9 1 2 3 4 5 6 0 8 7]]**\r\n2. remove 2 ids from this index (so index includes 8 ids)\r\n3. add 2 vectors into it again. (I found new 2 ids has come up before)\r\nquery index - the result ids is **[[9 8 2 3 4 5 6 9 8 7]]**\r\n\r\nI am not sure whether it's a bug.  If no, so how do I get just like index ids \"[[9 **10** 2 3 4 5 6 **11** 8 7]]\" rather than [[9 8 2 3 4 5 6 9 8 7]]? I think ids should be increasing from current maximum id. \r\n\r\nThanks in advance.\r\n```\r\nimport faiss\r\nimport numpy as np\r\n\r\nres = faiss.StandardGpuResources()\r\nco = faiss.GpuClonerOptions()\r\nco.useFloat16 = True\r\nco.usePrecomputed = False\r\n\r\n# index = faiss.index_factory(d, \"IVF128,PQ32\")\r\nindex_cpu = faiss.read_index('test.index')\r\nindex_gpu = faiss.index_cpu_to_gpu(res, 0, index_cpu, co)\r\n\r\n# dimension\r\nd = 128\r\n# generate base data\r\nfor i in range(1):\r\n    xb = np.random.random((10, d)).astype('float32')\r\n    index_gpu.add(xb)\r\n\r\n# one query data\r\nxq = np.random.random((1, d)).astype('float32')\r\n\r\n# search all index\r\n# search before remove ids\r\nnprobe = 128\r\nindex_gpu.setNumProbes(nprobe)\r\nD, I = index_gpu.search(xq, 10)\r\n# print(D)\r\nprint(I)\r\n# [[9 1 2 3 4 5 6 0 8 7]]\r\n\r\nindex_cpu = faiss.index_gpu_to_cpu(index_gpu)\r\nprint('start to remove ids')\r\nindex_cpu.remove_ids(np.arange(2))\r\n\r\n# add data again after remove ids\r\nindex_gpu = faiss.index_cpu_to_gpu(res, 0, index_cpu, co)\r\nindex_gpu.add(np.random.random((2, d)).astype('float32'))\r\n\r\n# search after add new data\r\nindex_gpu.setNumProbes(nprobe)\r\nD, I = index_gpu.search(xq, 10)\r\n# print(D)\r\nprint(I)\r\n# [[9 8 2 3 4 5 6 9 8 7]]\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/409/comments",
    "author": "Suntester",
    "comments": [
      {
        "user": "Suntester",
        "created_at": "2018-04-21T03:33:27Z",
        "body": "anyone can help on this ?  Thanks."
      },
      {
        "user": "FlYWMe",
        "created_at": "2018-04-23T02:33:43Z",
        "body": "The index you want to add cannot be repeated by default. \r\nYou can try another index type \"IDMap, flat\"."
      },
      {
        "user": "Suntester",
        "created_at": "2018-04-23T03:52:47Z",
        "body": "@FlYWMe thanks to your reply. \r\nI built the index using 'IVF128,PQ32' and want to keep the index structure,  and I understand you mean the index type  'IVF128,PQ32'  cause above situation is normal. Am I right ? \r\nBy the way, can index 'IVF128,PQ32' convert to index \"IDMap, flat\" ? \r\nThanks "
      },
      {
        "user": "FlYWMe",
        "created_at": "2018-04-23T04:15:08Z",
        "body": "@Suntester You're welcome, I  have not researched faiss for a long time.\r\nIn my understanding, yes, you are right. But you should not to convert the index type. Different index using different searching method, the index structure is completely different, the  'IVF128,PQ32' using  IVFADC (coarse quantizer+PQ on residuals) but the \"IDMap, flat\" using exact search method. However, only the \"IDMap, flat\" index support add_with_ids. So I suggest you add ids different from others.\r\nYou can read Faiss indexes pages in faiss WIKI  :)"
      },
      {
        "user": "Suntester",
        "created_at": "2018-04-23T05:20:11Z",
        "body": "@FlYWMe thanks\r\n\r\n@beauby @mdouze could you please help on this issue ? Thanks in advance."
      },
      {
        "user": "wickedfoo",
        "created_at": "2018-04-23T14:38:38Z",
        "body": "Remove is not currently implemented on GPU.\r\n"
      },
      {
        "user": "Suntester",
        "created_at": "2018-04-23T15:59:09Z",
        "body": "@wickedfoo Thanks for your reply.\r\neven if using the index type \"IDMap, flat\", it can support both add and remove ids operation for only CPU mode. Is it correct ?\r\n"
      },
      {
        "user": "beauby",
        "created_at": "2018-04-27T09:33:00Z",
        "body": "@Suntester Right, no index has `remove()` implemented on GPU. Closing this."
      }
    ]
  },
  {
    "number": 406,
    "title": "Got an error when test",
    "created_at": "2018-04-12T06:14:51Z",
    "closed_at": "2018-04-12T10:05:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/406",
    "body": "# Platform\r\n\r\nOS: Red Hat 4.8.5-16\r\n\r\nFaiss version: 2.1\r\n Faiss compilation options: using Openblas with compile flags\r\n\r\nRunning on :\r\n- [x] CPU\r\n\r\n\r\n# Reproduction instructions\r\n\r\nI can import faiss successfully.But I cannot run this\r\n\r\n```\r\n[root@ai-algorithm home]# python -c \"import faiss, numpy\r\nfaiss.Kmeans(10, 20).train(numpy.random.rand(1000, 10).astype('float32'))\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 2, in <module>\r\nAttributeError: module 'faiss' has no attribute 'Kmeans'\r\n```\r\n\r\n\r\nBut when I run the commend line in the ./faiss/ file  is ok.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/406/comments",
    "author": "XiaXuehai",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-04-12T10:05:54Z",
        "body": "When you do `import faiss` from a path that contains a `faiss` subdirectory, it will load that instead of the installed python module."
      },
      {
        "user": "XiaXuehai",
        "created_at": "2018-04-13T02:22:48Z",
        "body": "@beauby So,how to fix it?`python tutorial/python/3-IVFPQ.py` cannot run tooï¼"
      },
      {
        "user": "beauby",
        "created_at": "2018-04-13T08:07:30Z",
        "body": "What error are you getting?"
      }
    ]
  },
  {
    "number": 377,
    "title": "Why sift Flat's R@1 is less than 1?",
    "created_at": "2018-03-26T06:50:43Z",
    "closed_at": "2018-04-06T07:59:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/377",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\nOS: CentOS 7, on x64, with openblas-devel-0.2.20-3.el7.x86_64 installed\r\n\r\nFaiss version: 0b1f5004ecc7309a10ea0642f91b231927c1c7dc\r\n\r\nFaiss compilation options: \r\n\r\nOutput of tests/test_blas:\r\nBLAS test\r\nerrors=\r\n...\r\nIntentional Lapack error (appears only for 64-bit INTEGER):\r\ninfo=0000064b00000000\r\nLapack uses 32-bit integers\r\n\r\nRunning on :\r\n-  CPU\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\nI use the demo demo/demo_sift1M to test accuracy for several indexes. I consider \"Flat\" be the exact kNN search, so the recall should be exact 1. However it's less than 1:\r\n\r\nFlat\r\nR@1 = 0.9919\r\nR@10 = 1.0000\r\nR@100 = 1.0000\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/377/comments",
    "author": "yuzhichang",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-03-26T11:49:18Z",
        "body": "Hi, \r\nIt may be due to ties.\r\nIs this `IndexFlatL2` or `IndexFlatIP`?\r\n"
      },
      {
        "user": "yuzhichang",
        "created_at": "2018-03-26T12:45:52Z",
        "body": "It's IndexFlatL2 since demo_sift1M.cpp invokes index_factory with default metric parameter.\r\n        index = faiss::index_factory(d, index_key);\r\n\r\nI tried faiss::index_factory(d, index_key, METRIC_INNER_PRODUCT), got much lower R@1.\r\n\r\nBy the way, could you explain \"It may be due to ties.\"?\r\n"
      },
      {
        "user": "jegou",
        "created_at": "2018-03-26T13:57:35Z",
        "body": "The ground-truth is for L2, not inner product. These SIFTs descriptors are not perfectly normalized, therefore neighbors for inner product and L2 distances are not strictly equivalent. Change to METRIC_L2. \r\n\r\nTies may happen  when descriptors are exactly at the same distance, in which case the algorithm may arbitrarily choose one of the other. Yet, this should not happen that much on this dataset.  \r\n\r\nBtw, why do you say \"much lower than R@1\". R@1 = 0.9919 does not sound that different from 1. "
      },
      {
        "user": "yuzhichang",
        "created_at": "2018-03-26T14:34:39Z",
        "body": "For faiss::index_factory(d, \"Flat\", METRIC_INNER_PRODUCT)ï¼ŒR@1 is 0.8835.\r\n"
      },
      {
        "user": "jegou",
        "created_at": "2018-03-26T15:05:50Z",
        "body": "Ok, so for METRIC_INNER_PRODUCT it is expected to get R@1 that is significantly below 1, just because the ground-truth for IP is not the same as for L2.\r\n \r\nAbout the difference for L2, this could be due to ties (same distance for two descriptors), something that would need to be checked. Another possible explanation is floating-point rounding error involved in distance calculation, which can happen when using 32-bit floats with SSE/AVX like in Faiss. Similarly the code that was used to compute the ground-truth of SIFT1M could have done such rounding errors. "
      },
      {
        "user": "mdouze",
        "created_at": "2018-04-06T07:59:31Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 376,
    "title": "Access `nprobe` attribute for an `IndexPreTransform` ",
    "created_at": "2018-03-25T20:17:00Z",
    "closed_at": "2018-03-26T11:53:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/376",
    "body": "# Summary\r\n\r\nFind `nprobe` attribute for an `IndexPreTransform`, such as `OPQ64_256,IVF4096,PQ64`.\r\n\r\n# Platform\r\n\r\nOS: Linux\r\n\r\nFaiss version: 4d440b6698fcc7b08607534bc622902b52bf9c49\r\n\r\nFaiss compilation options: from pytorch/faiss-cpu\r\n\r\nRunning on :\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\n\r\nI was able to set/get `nprobe` attribute for an `IndexIVFFlat`, or `IndexIVFScalarQuantizer`, but for an index constructed through factory, or `faiss.load_index()`, such as `OPQ64_256,IVF4096,PQ64`, how can I achieve the same attribute?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/376/comments",
    "author": "terencezl",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-03-26T11:45:53Z",
        "body": "Hi \r\nYou can do:\r\n```\r\nfaiss.ParameterSpace().set_index_parameter(index, \"nprobe\", 123)\r\n```\r\nor\r\n```\r\nfaiss.downcast_index(index.index).nprobe = 123\r\n```"
      },
      {
        "user": "terencezl",
        "created_at": "2018-03-26T11:53:31Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 375,
    "title": "Running on GPU slower than CPU?",
    "created_at": "2018-03-23T04:48:37Z",
    "closed_at": "2018-03-26T14:19:36Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/375",
    "body": "# Summary\r\n\r\nI use faiss for my own dataset.\r\nFirst, I try IndexFlatL2 on cpu, it takes around 90 seconds for my dataset\r\nAnd then, I try multiple gpus by the code below, and it takes around 400 seconds for my dataset.\r\n\r\n```python\r\ncpu_index = faiss.IndexFlatL2(d)\r\n\r\ngpu_index = faiss.index_cpu_to_all_gpus(  # build the index\r\n    cpu_index\r\n)\r\n```\r\n\r\nSo, for the normal index like IndexFlat2D, how can I optimize the performance?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/375/comments",
    "author": "hminle",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-03-23T12:08:36Z",
        "body": "Hi,\r\nWhat is the number of vectors, their dimension and how are you performing the searches (by batch or one by one)?"
      },
      {
        "user": "wickedfoo",
        "created_at": "2018-03-23T15:54:21Z",
        "body": "Also, how are you timing the search on the GPU? Are you including the copy of the index to the GPUs?\r\n\r\n"
      },
      {
        "user": "hminle",
        "created_at": "2018-03-26T07:34:59Z",
        "body": "@mdouze Hi, the size of my embeddings is (23600, 128)\r\nD = 128\r\nI perform the search one by one, not by batch\r\n"
      },
      {
        "user": "hminle",
        "created_at": "2018-03-26T07:38:06Z",
        "body": "@wickedfoo I run my script on my own dataset, \r\nFirst, I run it with simple index (IndexFlat2D).\r\nAnd then I modify my code to transfer the index to the gpu, and run my script again.\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2018-03-26T12:15:44Z",
        "body": "If you run the search one by one, you cannot take advantage of the GPU because of insufficient inherent parallelism and the synchronization and memory transfer overheads. "
      },
      {
        "user": "hminle",
        "created_at": "2018-03-26T14:19:26Z",
        "body": "@mdouze Thank you a lot. I got it."
      }
    ]
  },
  {
    "number": 374,
    "title": "Error reconstructing vector: direct map not initialized",
    "created_at": "2018-03-21T16:07:49Z",
    "closed_at": "2018-04-04T14:58:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/374",
    "body": "# Summary\r\n\r\nI'm trying to get the closest vectors of a given ID that is already in the index. AFAIK Faiss does not contain this type of query. My idea was to then first request a reconstructed vector, and query based on that, but I'm failing to reconstruct the vectors.\r\n\r\n# Platform\r\n\r\nOS: Ubuntu, Python 2\r\n\r\nFaiss compilation options: OpenBlas\r\n\r\nRunning on :\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\n\r\nCalling `index.reconstruct(id)` gives\r\n\r\n    RuntimeError: Error in virtual void faiss::IndexIVF::reconstruct(faiss::Index::idx_t, float*) const at IndexIVF.cpp:304: Error: 'direct_map.size() == ntotal' failed: direct map is not initialized\r\n\r\nDo I need to do something specific to construct this map?\r\n\r\nI've tested this on indexes with these factory strings \"OPQ128_512,IMI2x12,SQ8 and \"OPQ128_512,IVF16000,SQ8\". Adding and searching work fine. Does these indexes not support reconstruction?\r\n\r\nDo you have better suggestions for querying for the neighbors of an item already in the index (only based on the ID)?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/374/comments",
    "author": "burk",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-03-21T17:05:22Z",
        "body": "Hi\r\nIMI and IVF indexes can only reconstruct if the direct map is initialized. \r\nTo initialize it, use the `make_direct_map()` method.\r\nGiven that your index is embedded in an `IndexPreTransform`, you have to do:\r\n```\r\ndowncast_index(myindex.index).make_direct_map()\r\n```"
      },
      {
        "user": "yuyifan1991",
        "created_at": "2020-12-09T11:04:31Z",
        "body": "> Hi\r\n> IMI and IVF indexes can only reconstruct if the direct map is initialized.\r\n> To initialize it, use the `make_direct_map()` method.\r\n> Given that your index is embedded in an `IndexPreTransform`, you have to do:\r\n> \r\n> ```\r\n> downcast_index(myindex.index).make_direct_map()\r\n> ```\r\nIf I use the _use the `make_direct_map()` method._ ,error happened : _RuntimeError: Error in void faiss::IndexIVF::make_direct_map(bool) at IndexIVF.cpp:159: Error: '0 <= idlist [ofs] && idlist[ofs] < ntotal' failed: direct map supported only for seuquential ids_"
      }
    ]
  },
  {
    "number": 373,
    "title": "index file copy",
    "created_at": "2018-03-21T01:58:52Z",
    "closed_at": "2018-03-24T12:49:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/373",
    "body": "One server generated index file was copied to another server and reported an error\r\n\r\nRuntimeError: Error in faiss::Index* faiss::read_index(FILE*, bool) at index_io.cpp:762: Index type 0x6c467749 not supported\r\n\r\n# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on :\r\n- [x ] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/373/comments",
    "author": "fuchao01",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-03-21T11:43:21Z",
        "body": "Hi\r\nThis is because the Faiss version used on both machines is different. The file was written by a new faiss and read by a old one (the other way round is supported)."
      }
    ]
  },
  {
    "number": 368,
    "title": "Using FAISS with Tf-Seq2Seq - Index Errors",
    "created_at": "2018-03-18T21:15:45Z",
    "closed_at": "2018-03-28T06:03:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/368",
    "body": "",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/368/comments",
    "author": "kells94",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-03-19T12:01:03Z",
        "body": "Hi \r\nFaiss supports numpy arrays, which is the case here.\r\nApparently `merged_batch_logits` does not have 128 columns. Have you checked that?\r\n\r\n"
      },
      {
        "user": "kells94",
        "created_at": "2018-03-22T11:05:30Z",
        "body": "hi , sorry , I just updated the code.  this is reproducible. I wanted to predict the value from the decoder output using Faiss. Any pointers or suggestions would be welcome ;)"
      }
    ]
  },
  {
    "number": 362,
    "title": "confused with  IVFFlat's  accuracy ",
    "created_at": "2018-03-13T03:03:39Z",
    "closed_at": "2018-03-24T04:12:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/362",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\nOS: ubuntu 16.4\r\n\r\nFaiss version: 4d440b6698fcc7b08607534bc622902b52bf9c49\r\n\r\nFaiss compilation options: default\r\n\r\nRunning on :\r\n- [  ] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\nI use the demo demo/demo_sift1M to test accuracy for different indexesï¼Œ found that the accuracy of 'IVF4096,Flat'  is lower than  any other IVF index.this is the result:\r\nIVF4096,Flat\r\nR@1 = 0.5610\r\nR@10 = 0.5636\r\nR@100 = 0.5636\r\n\r\nIVF4096,PQ8+16\r\nR@1 = 0.5327\r\nR@10 = 0.7983\r\nR@100 = 0.8028\r\n\r\nIVF4096,PQ32\r\nR@1 = 0.5700\r\nR@10 = 0.8869\r\nR@100 = 0.8940\r\n \r\nAccording to the comment, 'IVF4096,Flat'  Index stores the raw vector ,so it should has the highest accuray compared to other IVF index.\r\nthe comment is :\r\n\r\n21 /** Inverted file with stored vectors. Here the inverted file\r\n 22  * pre-selects the vectors to be searched, but they are not otherwise\r\n 23  * encoded, the code array just contains the raw float entries.\r\n 24  */\r\n 25 struct IndexIVFFlat: IndexIVF {\r\n\r\nso I'm confused! Maybe there is som misunderstanding. Could anyone explain it for me?\r\n ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/362/comments",
    "author": "yongjunjian",
    "comments": [
      {
        "user": "yongjunjian",
        "created_at": "2018-03-13T03:43:07Z",
        "body": "maybe I found out the reason . I found that autotuning set the nprobe to be a small num(2) .when  I set nprobe to be 128, the accuracy is up to 99%, but cost more time."
      },
      {
        "user": "mdouze",
        "created_at": "2018-03-13T17:09:42Z",
        "body": "Hi \r\nYou probably found the explanation. The accuracy is better for IVFFlat but for the same set of parameters."
      }
    ]
  },
  {
    "number": 359,
    "title": "Searching via spatial pyramid matching kernel",
    "created_at": "2018-03-06T07:03:57Z",
    "closed_at": "2018-03-24T04:12:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/359",
    "body": "Hi there,\r\n\r\nI would like to ask and learn the capability of FAISS on computing similarity based on spatial pyramid matching of histograms of feature vectors.\r\n\r\nAs is known, SPM provides a spatial representation of histogram based vectors. SPM generates a flat concataneted dense histogram based feature vectors. However,  it also encounters the diminishing coeffiecent factor to weight some of vector elements according to the level in which they were computed. In other words, the importance of some parts in large concatanated vectors is not uniformly distributed. At this point we use histogram intersection kernel to compute the similarity score between two feature vectors. \r\n\r\nThe problem would not exist, if we had used only finest level of features since their weights are equal to 1. In this case, we can use faiss in order to compute pure L2 distance. But, what if I also want to include coarse levels? \r\n\r\nTo sum up, is it possible to define those kinds of special similarity formulations in FAISS? What can be done to handle my specific problem? I have an idea. I may initially multiply the diminishing factor during feature vector computation. But I am not sure whether it ll be theoritically correct. It seems correct but I am not 100% sure.\r\n\r\nSo, let me reraise the main question. Is FAISS capable of defining custom similarity score computation schemes like I described above?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/359/comments",
    "author": "asfix",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-03-06T12:49:25Z",
        "body": "Hi,\r\nFaiss supports only L2 and inner product as distance functions, ie. not L1 as would be required to compute the intersection distance.\r\n \r\nIt may be possible to map the SPM distance computation to L2 by: \r\n- pre-multiplying the bins of the histogram by a factor that depends on scale\r\n- square-rooting the descriptor components to transform L2 distance into L1 on the original vectors."
      },
      {
        "user": "asfix",
        "created_at": "2018-03-06T13:36:56Z",
        "body": "Thank you so much for your answer."
      }
    ]
  },
  {
    "number": 346,
    "title": "is there a plan support integer (INT8) matrix multiplication",
    "created_at": "2018-02-23T10:28:21Z",
    "closed_at": "2018-02-26T10:09:48Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/346",
    "body": "# is there a plan support integer (INT8) matrix multiplication?\r\ncuda and cublas support integer (INT8) matrix multiplication,I see faiss support fp16,is there a plan support int8? \r\n\r\nRunning on :\r\n- [ x] CPU\r\n- [ âˆš] GPU\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/346/comments",
    "author": "RobitYadda",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-02-23T13:17:22Z",
        "body": "Hi, \r\nNo we do not plan to support int8 data directly on GPU. "
      },
      {
        "user": "RobitYadda",
        "created_at": "2018-02-24T02:01:59Z",
        "body": "Thx! \r\nCould you explain why? "
      },
      {
        "user": "mdouze",
        "created_at": "2018-02-26T10:09:48Z",
        "body": "We do not plan to extend the GPU functionality it for the coming months. \r\nWe welcome pull requests if you want to contribute."
      },
      {
        "user": "ctwd-0",
        "created_at": "2019-11-05T10:10:19Z",
        "body": "what about now "
      },
      {
        "user": "kairos03",
        "created_at": "2022-08-01T07:44:44Z",
        "body": "what about now?"
      },
      {
        "user": "mdouze",
        "created_at": "2022-08-31T09:20:37Z",
        "body": "For what type of index should it be supported?"
      }
    ]
  },
  {
    "number": 332,
    "title": "How to delete some vectors from index and then do a search without re-training. ",
    "created_at": "2018-02-07T17:13:15Z",
    "closed_at": "2018-02-21T13:26:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/332",
    "body": "Hi everyone. \r\nI want to exclude some vectors from my query after I've done training. \r\n\r\nThat is, once I train and add vectors:\r\n\r\n```\r\nindex.train(xb) \r\nindex.add(xb)\r\n``` \r\n\r\nI'd like to take off, for instance, xb[10:20] from the query\r\n  \r\n```\r\nD, I = index.search(xq, k=10)\r\n```\r\nand still return me 10 nearest neighbors\r\n\r\nThanks and \r\nMay the faiss be with you!!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/332/comments",
    "author": "ozkansafak",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2018-02-07T19:55:27Z",
        "body": "Does `vectors` include `xb`? The train vectors are not added to the database by calling train(); the database remains empty after train.\r\n\r\nIf `vectors` does include `xb`, then you have two options. First, if you only want to exclude 10 vectors as in your example with k=10, just query with k=20 and then remove any of the 10 vectors you wanted excluded if they happen to occur in the results.\r\n\r\nIf it is a substantial number of vectors that you want excluded (i.e., hundreds or thousands or millions), then you can `remove` them from the index, but that is very slow, and some indices don't support removal. Otherwise, you can build multiple indices that cover the different subsets that you wish to capture, trained on the original vectors.\r\n"
      },
      {
        "user": "ozkansafak",
        "created_at": "2018-02-07T22:58:05Z",
        "body": "I meant to say\r\n```\r\nindex.train(xb) \r\nindex.add(xb)\r\n```\r\nMy apologies for the confusion. I corrected it.\r\n\r\n--- \r\nSo I'd like to recursively search for nearest neighbors of a query vector `xq`, then get maybe 10 nearest neighbor vectors, and then do a search on all these vectors separately until I gather, say, a thousand vectors. Ideally, I'd like to be able to **remove** certain indices from `index`. \r\n\r\nRight now, I am invoking the `index.search()` function a few times. It returns duplicate vectors which I weed out. I was only wondering if there was a more clever way built into the `index` or `quantizer` object.\r\n\r\nThanks"
      },
      {
        "user": "mdouze",
        "created_at": "2018-02-08T06:30:37Z",
        "body": "Hi \r\nFaiss can only search neighbors in a fixed dataset. It is not a generic database engine, so you cannot build composite queries like in SQL. "
      },
      {
        "user": "mdouze",
        "created_at": "2018-02-21T13:26:24Z",
        "body": "No activity. Closing."
      }
    ]
  },
  {
    "number": 329,
    "title": "How to reset on index ?",
    "created_at": "2018-02-06T03:50:49Z",
    "closed_at": "2018-02-06T09:04:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/329",
    "body": "Hey , I have a question about how to reset the index. I have a index continue adding features into itself like index.add().  If the memory occupation of index is larger than 6G, I will reset the index and the ntotal should be reset to 0. Is there any useful function in faiss ? I didn't find it in tutorial... Thank you very much~",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/329/comments",
    "author": "keithbyr",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2018-02-06T04:36:12Z",
        "body": "index.reset() should clear out the contents but keep any learned parameters that it was trained on.\r\n"
      }
    ]
  },
  {
    "number": 323,
    "title": "search result issue",
    "created_at": "2018-01-29T08:46:58Z",
    "closed_at": "2018-01-29T08:58:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/323",
    "body": "Hi all, I have a very simple question. I trained a index containing features fetched from the same CNN extractor.  I need to continually add the new feature into index by index.add(new_feature).  When using index.search and obtained the nearest feature index. How can I get the nearest feature itself ?  Any attribution available for fetching the nearest feature ?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/323/comments",
    "author": "keithbyr",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-01-29T08:58:49Z",
        "body": "Hi \r\nYou can call `search_and_reconstruct` instead of just `search`. NB that for IVFPQ this will return an approximation of the stored vector because it uses a lossy codec."
      }
    ]
  },
  {
    "number": 315,
    "title": "How to convert distance values into 0-100 similarity?",
    "created_at": "2018-01-17T05:14:26Z",
    "closed_at": "2018-01-18T03:28:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/315",
    "body": "CODEï¼š\r\n>D, I = index.search(xq, k)     # actual search\r\n\r\nHow can â€˜Dâ€™ be converted to 0-100 (similarity)?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/315/comments",
    "author": "hipitt",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-01-17T10:10:17Z",
        "body": "Hi \r\nIf the vectors are L2-normalized and the index is a L2 index (the default) then you can just do \r\n\r\nsim = 100 * (4-D)\r\n\r\n"
      },
      {
        "user": "hipitt",
        "created_at": "2018-01-18T02:13:37Z",
        "body": "@mdouze copy that"
      },
      {
        "user": "jaguarproject",
        "created_at": "2018-07-28T18:33:33Z",
        "body": "@mdouze Hey Matthijs, could you help to explain what does \"4\" means in sim = 100 * (4-D)? Why not \"1\"? Thank you!"
      },
      {
        "user": "mdouze",
        "created_at": "2018-07-29T05:02:29Z",
        "body": "The squared distances between normalized vectors are between 0 (same vector) and 4 (opposite vectors), so the linear operation to convert them to a similarity [0, 100] is \r\n\r\n25 * (4 - D)\r\n\r\n(my previous formula was wrong)"
      },
      {
        "user": "gauravgund",
        "created_at": "2021-05-25T04:51:39Z",
        "body": "@mdouze : I am using faiss.IndexIDMap and I want to obtain scores to apply thresholding. So, i converted my word vectors into L2-norm and then used the operation i.e. 25*(4-D) but it is giving more similarity scores for bad values in the topk values. Ideally, it should give a higher value to top1 value but it is the other way round using the operation you suggested. Any workaround for this to convert this operation to similarity rather than dissimilarity?"
      },
      {
        "user": "tempdeltavalue",
        "created_at": "2021-12-04T12:13:53Z",
        "body": "> Hi If the vectors are L2-normalized and the index is a L2 index (the default) then you can just do\r\n> \r\n> sim = 100 * (4-D)\r\n\r\n\r\n@mdouze \r\nYou can do it if you have vectors normalised in range 0 - 4 . Does normalize_l2 normalize in this range ? \r\n\r\n(Tried to find something in swigfaiss and didn't :) )"
      },
      {
        "user": "akhilanaz",
        "created_at": "2022-10-17T12:30:34Z",
        "body": "I have used :\r\nindex_L2 = faiss.IndexFlatL2(vectormatrix.shape[1])   \r\nprint(index_L2.is_trained)\r\nfaiss.normalize_L2(vectormatrix)\r\nindex_L2.add(vectormatrix)                \r\nprint(index_L2.ntotal),\r\n\r\nwhere my vector is of shape 768,\r\nand for the nearest neighbors I am receiving a shortest distance from the range of 212, instead of zero.\r\nCan you explain why is it so."
      }
    ]
  },
  {
    "number": 309,
    "title": "Index Type after load from disk?",
    "created_at": "2018-01-15T09:31:50Z",
    "closed_at": "2018-01-23T09:03:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/309",
    "body": "Hi, I'm using IMI2_8, PQ2_16 to create an index, and then convert to gpu to training, back to cpu for wriring. And I got sereval questions:\r\n1. when using index_cpu_index_to_gpu_multiple to convert cpu index to gpu index, how can i get the origin IMI2_8, PQ2_16 type index? because this API return a faiss::Index* type index\r\n2. when load index from disk, get a faiss::Index*, how can I cast it to  IMI2_8, PQ2_16, since I didn't find a index type for IMI2_8, PQ2_16.\r\n3. how many index types can I using dynamic_cast to cast.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/309/comments",
    "author": "0DF0Arc",
    "comments": [
      {
        "user": "0DF0Arc",
        "created_at": "2018-01-15T11:20:41Z",
        "body": "one more question, how to search in a specified index? like IMI2_8, PQ2_16."
      },
      {
        "user": "wickedfoo",
        "created_at": "2018-01-16T15:49:01Z",
        "body": "IMI is not supported on the GPU.\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2018-01-16T16:02:05Z",
        "body": "Hi \r\nWhen an unsupported index is encountered during conversion, the index is just copied without moving it to GPU."
      }
    ]
  },
  {
    "number": 300,
    "title": "how to use Yfcc100M dataset",
    "created_at": "2018-01-03T01:48:40Z",
    "closed_at": "2018-02-21T13:26:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/300",
    "body": "Hi, \r\nAs you mentioned  in your paper entitled \"Billionscale similarity search with GPUs\", you use\r\nYfcc100M dataset, cloud you please let me know how we can get the same experimental setup for using Yfcc100M dataset? where can I download those four files (Base set, Learning set, Query set and Groundtruth set) ?\r\n\r\nThanks,",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/300/comments",
    "author": "Bobarshad",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-01-03T22:16:35Z",
        "body": "Hi, \r\nSorry, we did not publish features for this dataset. \r\nThe easiest to reproduce dataset is Deep1B, you may want to use that one instead. "
      },
      {
        "user": "Bobarshad",
        "created_at": "2018-01-03T23:21:21Z",
        "body": "Hi, Thanks. Where can I find those images related to Deep1B dataset? I want to have a visualize demo like you have for Yfcc100M and to see the real difference between k-nearest result vectors.   "
      },
      {
        "user": "mdouze",
        "created_at": "2018-01-09T06:46:24Z",
        "body": "Yes there are no images for Deep1B. So both datasets have their drawbacks..."
      },
      {
        "user": "mdouze",
        "created_at": "2018-02-21T13:26:47Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 284,
    "title": "Obtaining cosine similarity",
    "created_at": "2017-12-18T02:27:36Z",
    "closed_at": "2017-12-19T02:28:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/284",
    "body": "Hi.\r\nWhen we use IndexIVFFlat with METRIC_INNER_PRODUCT option, we can obtain cosine similarity. How about for IndexIVFPQ? Is there any ways to get the value?\r\nI want to use the similarity to know how similar they are",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/284/comments",
    "author": "kumon",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-12-18T16:27:56Z",
        "body": "Hi \r\nYou can set the metric_type field to METRIC_INNER_PRODUCT after the object is constructed, this should work.\r\nTwo remarks: \r\n- you can normalize the vectors before indexing and use L2 distance. Since distance_L2 = 2 - distance_inner_product the two are equivalent\r\n- IVFPQ produces approximate distances, so the estimate may be imprecise.\r\n"
      },
      {
        "user": "jegou",
        "created_at": "2017-12-18T17:39:08Z",
        "body": "Note that the strategy that Matthijs mentions would provide you very rough approximations of the inner product, because vectors quantized with PQ/IVFPQ have different norms, even if they are normalized prior to quantization. \r\n\r\nWe have not implemented the version that would provide a better estimator of the cosine directly from the quantized codes (this is possible, pull request welcome). We would do if we have an important use-case, but for normalized vectors (our typical case) we had no need of that until now, since a simple turn-around is to make the search w.r.t. L2 distances and then use the function `search_and_reconstruct` to refine the similarity estimates on the top candidates.  "
      },
      {
        "user": "kumon",
        "created_at": "2017-12-19T02:28:38Z",
        "body": "Hi.\r\nI use this library for image retrieval. I don't want to show any results if there is no similar images. For the purpose I use IndexIVFFlat with a threshold based on cosine similarity.\r\nIn order to handle more images in one server, I am considering using IndexIVFPQ. That is why I need to calculate cosine similarity to exclude not similar images from results.\r\nI will try to use the approximate value for the threshold.\r\nThank you very much."
      },
      {
        "user": "jrcavani",
        "created_at": "2022-09-08T02:25:29Z",
        "body": "@jegou I know this question and answer were provided 5 years ago but have things changed since then?\r\n\r\nIf the answer still holds, it would be search with `IndexIVFPQ` and `METRIC_INNER_PRODUCT` is less accurate as `METRIC_L2`, even for normalized vectors. Does that sound right?\r\n\r\nIn addition do the reconstructed vectors from `IndexIVFPQ` `reconstruct()` or `search_and_reconstruct()` using a `METRIC_INNER_PRODUCT` suffer from the same imprecision compared with one with `METRIC_L2`?"
      }
    ]
  },
  {
    "number": 281,
    "title": "GPU Memory when transfer cpu_index to gpu_index",
    "created_at": "2017-12-14T08:08:13Z",
    "closed_at": "2017-12-18T07:46:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/281",
    "body": "Hi, \r\nI add about 5.2 million vectors in 144 dims into cpu_index, (IVF2500, PQ48), the cput_index size is actually about 270M , when using cpu_to_gpu to transfer the index to GPU,  how can the GPU Memory Usage show a total memory about 4800MiB?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/281/comments",
    "author": "0DF0Arc",
    "comments": [
      {
        "user": "0DF0Arc",
        "created_at": "2017-12-14T08:09:47Z",
        "body": "BTW, that's the transfer code:\r\nfaiss::gpu::GpuClonerOptions* options = new faiss::gpu::GpuClonerOptions();\r\n        options->indicesOptions=faiss::gpu::INDICES_64_BIT;\r\n        options->useFloat16CoarseQuantizer = false;\r\n        options->useFloat16 = false;\r\n        options->usePrecomputed = false;\r\n        options->reserveVecs = 0;\r\n        options->storeTransposed = false;\r\n        options->verbose = true;\r\n        faiss::gpu::StandardGpuResources resources;\r\n        faiss::gpu::GpuIndexIVFPQ* index = dynamic_cast<faiss::gpu::GpuIndexIVFPQ*>   (faiss::gpu::index_cpu_to_gpu(&resources, gpu_id ,cpu_index, options));\r\n\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2017-12-15T10:07:19Z",
        "body": "Hi \r\nThere is a fixed temporary storage. You may want to tune it in the `StandardGpuResources` object. By default it is set to ~20% of the GPU memory."
      },
      {
        "user": "wickedfoo",
        "created_at": "2017-12-15T17:37:23Z",
        "body": "For good performance you shouldn't lower the temporary memory in StandardGpuResources below 1 GB of usage.\r\nFaiss performs a lot of its temporary calculations here."
      },
      {
        "user": "0DF0Arc",
        "created_at": "2017-12-18T07:46:33Z",
        "body": "OK, I'd better keep it un-changed, THX"
      }
    ]
  },
  {
    "number": 272,
    "title": "IVFFLAT is slower than IVFPQ",
    "created_at": "2017-12-01T12:28:49Z",
    "closed_at": "2017-12-04T17:44:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/272",
    "body": "I used 90 million data to create IVFFlatIndex and IVFPQIndex, but when do performance test with 100 threads or more, I find IVFPQ is much faster than IVFFLAT.  \r\nFor Example, IVFFLAT provides 160ms / 660qps  while IVFPQ provides 10ms / 5000qps. \r\nIn my Understanding, PQ is just smaller than flat in size, why can it be so much faster ?  Is there anything I missed in IVFFLAT?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/272/comments",
    "author": "David-Q",
    "comments": [
      {
        "user": "David-Q",
        "created_at": "2017-12-01T12:33:07Z",
        "body": "in addition... I use it in GPU, the nprobe is 1 and the nlist is 40000, d of the data is 32"
      },
      {
        "user": "wickedfoo",
        "created_at": "2017-12-01T14:18:42Z",
        "body": "Both indices are more or less memory bandwidth limited. For some / most? parameters, IVFFlat will be slower than IVFPQ (especially for smaller encodings), as IVFFlat has to traverse more memory, but IVFFlat will be more accurate. For larger PQ encodings, IVFFlat stands a chance at being faster."
      },
      {
        "user": "David-Q",
        "created_at": "2017-12-04T07:36:42Z",
        "body": "thx to your answer~ !"
      }
    ]
  },
  {
    "number": 259,
    "title": "The choice of the number of quantizers per subvector in IndexIVFPQ",
    "created_at": "2017-11-22T02:39:36Z",
    "closed_at": "2018-01-09T09:00:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/259",
    "body": "Hi there,\r\n\r\nI'm running IndexIVFPQ and notice that when I set nbits (the number bits to index sub-codebook) to 16 (2 bytes), FAISS would throw an assertion failure saying that nbits should be equal or less than 8. This would put a restriction on the maximal number of codewords per subvector to 256. \r\n\r\nMy understanding of PQ is that increasing nbits would significantly increase the number of codewords that can be represented without increasing memory footprint too much. This is because the total number of codewords is (2^nbits)^m (where m is the number of subvectors), the codebook size is (i.e., m * (2^nbits)), and the number of bytes per vector is (m * nbits / 8). Hence, increasing nbits will likely yield to a better memory-recall tradeoff. \r\n\r\nThe place where the exception is thrown is at the following place:\r\nError in faiss::IndexIVFPQ::IndexIVFPQ(faiss::Index*, size_t, size_t, size_t, size_t) at IndexIVFPQ.cpp:50: Error: 'nbits_per_idx <= 8' failed. \r\nAlso, in Figure 6 of the paper \"product quantization for nearest neighbor search\", K* is set to 4096, which is equivalent to nbits set to 12. I wonder if there are any particular reasons that in the current implementation nbits is limited to 8, and what I can do to increase nbits to larger values such as 16?\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/259/comments",
    "author": "minjiaz",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2017-11-22T04:12:01Z",
        "body": "The core ProductQuantizer.h/.cpp that calculates the codes supports 2 bytes per code, but both the CPU and GPU IVFPQ index which uses the ProductQuantizer do not, mainly for practical reasons. \r\n\r\nThe CPU version could be fixed, but one consideration is that at, say, nbits == 16 (65536 codes/subvector), you rapidly get into a situation where the codebook would no longer fit into L1/L2 and maybe not even the last level cache (L3), so it becomes impractical to use. The GPU code cannot support this many codes per subvector, maybe only 9/10 bits per code at most, due to shared memory limitations (48 KiB), but this would place further restrictions on how many subvectors you are using, etc.\r\n"
      },
      {
        "user": "minjiaz",
        "created_at": "2017-11-22T08:18:07Z",
        "body": "Hi wickedfoo, \r\n\r\nThank you very much for your reply! It is good to know that nbits <= 8 can be fixed on the CPU version. Can you please let me what changes roughly need to be made in order to enable nbits = 16 if the required changes are not too many? \r\n\r\nI see your point on restricting codebook size to let it fit in fast cache. But I think even with nbits == 16, there is still a large chance that the codebook can fit into L3. My understanding is that nbits == 16 means each sub-codeword index takes 16-bit (2-byte). Assume there are 65536 sub-codeword, the sub-codebook size is 2 X 65536 = 128KB. Given L3 cache is 64MB on most Intel Xeon CPU machines today, it is theoretically possible to host a codebook in L3 with an M (the number of subvectors) potentially as large as 500. However, by increasing nbits, it allows PQ to have more codewords even with some reduction on M, improving recall.  This would benefit scenarios where memory space and recall are considered more important and allow some sacrifice on execution time. \r\n\r\n"
      },
      {
        "user": "wickedfoo",
        "created_at": "2017-11-22T16:31:20Z",
        "body": "The factor \"2\" is only the size of the codes for the encoded vectors themselves, not the size of the codebook that is used to decode it.\r\n\r\nThe codebook itself is sizeof(float) x number of sub-dimensions x number of codes; i.e., the centroids of the residual for each sub-dimension. So a typical value would be more like 4 * 8 * 65536 = 2097152 bytes or more.\r\n\r\nAt query time, a distance is calculated against each IVF cluster centroid, so this data structure that is used during query is something like sizeof(float) x number of sub-vectors x number of IVF centroids, which is comparable to the above. This is something that will trash the cache as well.\r\n\r\nDepending upon the number of sub-vectors chosen, at some point an index like IVFFlat will likely perform better, as it has more regular memory access patterns and will make better use of the bandwidth provided by caches than the random accesses performed by codeword lookup. I take your point that an encoding like 4 x 8192 or 8 x 16384 could work well and is another possible operating point versus other partitioning points, but many of these points can't happen on the GPU (4 x 8192 could, but larger in either dimension can't).\r\n\r\nFor fixing the CPU side, one would have to look at IndexIVFPQ.cpp and dependencies and make sure that codes are not treated as exclusively bytes. This is not a priority of ours now.\r\n\r\n"
      },
      {
        "user": "minjiaz",
        "created_at": "2017-11-22T19:30:35Z",
        "body": "Hi Jeff,\r\n\r\nThanks for your detailed reply. That makes sense. I think the total size of the codebook is:\r\n(input vector dimension/ m) * sizeof(float) * (number of sub-dimension) * (number of sub-codeword each in sub-dimension). \r\n\r\nIt also makes sense that the pre-computation of the distance look-up table (distance between subvectors of a query and centroids of sub-dimensions) can pollute the cache.  \r\n\r\nI agree that for GPU, there is a practical limitation on the codebook size due to the limited shared memory size. However, I still think on CPU the codebook size can be a lot larger as long as it can sit in L3 cache. There are studies that have shown with larger nbit (e.g., 16), it offers a larger speed-accuracy trade-off space (as shown in table 1 in \"Accelerated Nearest Neighbor Search with Quick ADC\" ICMR'17).\r\n\r\nAnother reason for hosting the codebook in L3: In the precomputation phase, the codebook is used for distance look-up table computation. Once the precomputation phase is done, in the scanning phase each data point requires two memory loads to estimate its distance to the query: one from memory to load the PQ code (indexes to codewords), and one from decoding by checking the distance lookup table. Since there is one main memory load for each data point anyway, it is likely that the latency gain is diminishing by hosting the codebook in L2 vs. L3. \r\n\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2018-01-09T09:00:48Z",
        "body": "It seems the question is answered, closing."
      }
    ]
  },
  {
    "number": 251,
    "title": "How to use OPQ to reduce the dimensions",
    "created_at": "2017-11-13T08:24:09Z",
    "closed_at": "2017-11-22T14:09:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/251",
    "body": "I have read the paperï¼šPolysemous Codes in ECCV2016\r\n\r\nI have an questionï¼šIn the paper, you say â€œAlexandre Sablayrolles had the idea of extending the OPQ method to reduce the number of dimensionsâ€. The original OPQ can not reduce the dimensions.\r\n\r\nI want to know how to use OPQ to reduce the dimensions?\r\n\r\nThank you!\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/251/comments",
    "author": "zhaokang0826",
    "comments": [
      {
        "user": "alexandresablayrolles",
        "created_at": "2017-11-17T18:52:19Z",
        "body": "If you look at the formulation in the original paper \"Optimized Product Quantization for Approximate Nearest Neighbor Search\" (CVPR'13), it turns out you can use Algorithm 1 with a non-square R matrix (say d*p). You just need to initialize it so that R^T R  = I and you follow the steps described, as all the subsequent matrices will have the right number of dimensions."
      }
    ]
  },
  {
    "number": 248,
    "title": "Question: relationship between PCA and PQ parameters? Trying to get intuition...",
    "created_at": "2017-11-09T17:58:49Z",
    "closed_at": "2017-11-22T14:06:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/248",
    "body": "Sorry, this is not really an issue, more a theoretical question.\r\n\r\nLet's say that I have a 2048D vector (for the sake of argument, let's assume that the vector is uint8 data type) and want to reduce that representation down to 16 bytes. On one end of the scale, I could apply PCA (or other dimensionality reduction technique) to get 16 dimensions. On the other end, I could use PQ directly on the 2048D vectors and obtain 16 byte codes.\r\n\r\nThere is probably a sweet spot between those two extremes (e.g. PCA to 128D and then PQ). That sweet spot could probably be found through parameter search and testing on a validation set. \r\n\r\nWhat bothers me is I have zero intuition for what those parameters might be.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/248/comments",
    "author": "billkle1n",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-11-09T18:08:39Z",
        "body": "Hi, \r\n\r\nA practical answer: as a rule of thumb for a code of size c, first apply a OPQ transform (not PCA) to `4*c` or `8*c`, then encode with PQ. \r\n\r\n```python\r\nopq = OPQMatrix(2048, 16, 4*16) \r\npq = ProductQuantizer(4*16, 16, 8) \r\nopq.train(x)\r\nxt = opq.apply_py(x)\r\npq.train(xt)\r\ncodes = pq.compute_codes(xt)\r\n```\r\nTo decode: \r\n```python\r\nx_decoded = opq.reverse_transform(pq.decode(codes))\r\n```\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2017-11-22T14:06:38Z",
        "body": "Closing."
      }
    ]
  },
  {
    "number": 243,
    "title": "How to get the binary code of IVFPQ index?",
    "created_at": "2017-11-07T08:26:54Z",
    "closed_at": "2017-11-22T14:08:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/243",
    "body": "I want to use faiss to transform a long feature to a binary code,but the tutorials seems not have this.\r\nI find a same issue with this question,but i did not find the answer. Could someone help me? I will be very appreciated!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/243/comments",
    "author": "hengshan123",
    "comments": [
      {
        "user": "hengshan123",
        "created_at": "2017-11-07T08:32:46Z",
        "body": "I find a function:void encode (long key, const float * x, uint8_t * code) const; what is the key parameter means ? "
      },
      {
        "user": "mdouze",
        "created_at": "2017-11-07T09:02:58Z",
        "body": "Hi\r\nIVFPQ does not produce binary codes. What do you want exactly?"
      },
      {
        "user": "hengshan123",
        "created_at": "2017-11-08T09:41:07Z",
        "body": "I want compress a long long feature vector to a compact binary code. The binary code will be used as a visual word of the img. So does IndexPQ or other index can do this ?"
      }
    ]
  },
  {
    "number": 208,
    "title": "what is in the GPU global memory when i use GpuIndexIVFPQ for search?",
    "created_at": "2017-09-08T03:30:53Z",
    "closed_at": "2017-09-08T13:58:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/208",
    "body": "my test is that:  d=128  trainset10Million,  c1_centroids = 4*sqrt(nt)(adout 12000) subM=8,  32floating points ,  and i add 100Millon 128D data into the gpuivfpq-index.  my query set is 2000*128D.   query-batch=500.    Gpu is one Tesla P4 with Memory 7606MiB.    after add the 100Million*12D, i use the api  gpu_ivfpq_index.reclaimMemory() and get the result 668274176 .  but when nvidia-smi to check the GPU, GPU global memory used about 4513M.  (when it's 200Million*128D,the number is 1336548352,  definately 2times . and Gpu 6419M ) so here is my questions:\r\n1.what's the content of 668274176?     i think the index mainly has invert-list-indexs and  codes. each vector has 8 Byte index and 8 Byte PQ codes.  there's  100Million * (8+8)Byte, isn't it ?\r\n2.what 's in the GPU global memory  (use 4513M)? \r\n\r\nplease help me , thank you",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/208/comments",
    "author": "bzwqq",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2017-09-08T13:58:55Z",
        "body": "`reclaimMemory` returns the amount of memory reclaimed in bytes, not the amount of memory in use. This is done by exactly sizing lists for storage.\r\n\r\nGPU Faiss reserves ~18% of the GPU's memory for temporary calculations. This is adjustable in `StandardGpuResources`, so about 1370 MB is used for that.\r\n\r\nYou are correct, the size of the index in memory for 8 byte indices and 8 byte PQ codes is roughly N * (8 + 8).\r\n\r\nIf you have precomputed codes enabled, then there is potentially a lot of memory outstanding for that. So the memory you have in use is your list storage + precomputed codes + temp memory reservation + some other smaller, miscellaneous things.\r\n\r\n\r\n"
      },
      {
        "user": "bzwqq",
        "created_at": "2017-09-10T04:03:39Z",
        "body": "ok,thank you very much! i really appreciate it"
      }
    ]
  },
  {
    "number": 188,
    "title": "The count of search result",
    "created_at": "2017-08-28T06:35:23Z",
    "closed_at": "2017-08-28T08:59:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/188",
    "body": "` index.search (nq, queries.data(), k, dis.data(), nns.data());`\r\nI have more than one million features are in the index. I set k = 200 and the index only returned 117 results. \r\nShouldn't it return 200 results?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/188/comments",
    "author": "welfred",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-08-28T08:11:37Z",
        "body": "Hi \r\nPlease increase the `nprobe`, the default 1 means that only one data cluster is visited, which may contain too few items."
      },
      {
        "user": "welfred",
        "created_at": "2017-08-28T08:59:57Z",
        "body": "Ok, thanks!"
      }
    ]
  },
  {
    "number": 187,
    "title": "Getting decoded vector in IVFPQ index?",
    "created_at": "2017-08-26T05:46:07Z",
    "closed_at": "2017-08-28T18:52:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/187",
    "body": "I was wondering if there was a way to get a decoded vector from an IVFPQ index. For example in python:\r\n\r\n```python\r\ntest_vectors = np.random.randn(3, d).astype(np.float32)\r\nindex.add(test_vectors)\r\nsearch_vectors = np.array([test_vectors[2]])\r\nD, I = index.search(search_vectors, 5)\r\n\r\n# how to do this?\r\nindex.get_decoded_vector(I[0])\r\n# returns the decoded version of vector\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/187/comments",
    "author": "billkle1n",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-08-26T06:58:42Z",
        "body": "Hi \r\nPlase see the `reconstruct` and `reconstruct_n` methods."
      },
      {
        "user": "billkle1n",
        "created_at": "2017-08-26T19:23:51Z",
        "body": "Thanks. Does `key` refer to the vector's ID? I tried the following:\r\n\r\n```python\r\n# ...\r\nprint(index.reconstruct(2))\r\n```\r\n\r\nBut am getting this error:\r\n\r\n```python\r\n../../../../.pyenv/versions/3.6.2/envs/rise/lib/python3.6/site-packages/faiss.py:134: in replacement_reconstruct\r\n    self.reconstruct_c(key, swig_ptr(x))\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <swigfaiss.IndexIVFPQ; proxy of <Swig Object of type 'faiss::IndexIVFPQ *' at 0x10d88c810> >\r\nkey = 2\r\nrecons = <Swig Object of type 'faiss::HeapArray< faiss::CMax< float,long > >::T *' at 0x10d88c840>\r\n\r\n    def reconstruct(self, key, recons):\r\n>       return _swigfaiss.IndexIVFPQ_reconstruct(self, key, recons)\r\nE       RuntimeError: Error in virtual void faiss::IndexIVFPQ::reconstruct(idx_t, float *) const at IndexIVFPQ.cpp:304: Error: 'direct_map.size() == ntotal' failed\r\n\r\n../../../../.pyenv/versions/3.6.2/envs/rise/lib/python3.6/site-packages/swigfaiss.py:2820: RuntimeError\r\n```"
      },
      {
        "user": "billkle1n",
        "created_at": "2017-08-26T19:28:24Z",
        "body": "Printed those values:\r\n\r\n```python\r\n    print('index.ntotal =', index.ntotal)\r\n    # >>> index.ntotal = 3\r\n    print('index.direct_map.size() =', index.direct_map.size())\r\n    # >>> index.direct_map.size() = 0\r\n```\r\n\r\nSo it looks like `direct_map` is empty, whatever that is. I'm guessing it has to do with the IDMap proxy?"
      },
      {
        "user": "billkle1n",
        "created_at": "2017-08-26T19:40:41Z",
        "body": "Tried again with an IndexIDMap but different error:\r\n\r\n```python\r\nimport faiss\r\nfrom sklearn.preprocessing import normalize\r\n\r\ndef l2_normalize(v):\r\n    return normalize(v, norm='l2')\r\n\r\nd=128\r\nnlists=8\r\nM=32\r\nnbits=8\r\ncoarse_quantizer = faiss.IndexFlatL2(d)\r\nivfpq_index = faiss.IndexIVFPQ(\r\n    # coarse quantization / IVF related params\r\n    coarse_quantizer, d, nlists,\r\n    # PQ related params\r\n    M, nbits\r\n)\r\nindex = faiss.IndexIDMap(ivfpq_index)\r\n\r\ntraining_vectors = l2_normalize(\r\n    np.random.randn(266, d).astype(np.float32)\r\n)\r\n\r\ntest_vectors = l2_normalize(\r\n    np.random.randn(3, d).astype(np.float32)\r\n)\r\n\r\nindex.train(training_vectors)\r\nids = np.arange(test_vectors.shape[0])\r\nindex.add_with_ids(test_vectors, ids)\r\n\r\nprint(index.reconstruct(2))\r\n```\r\n\r\n```\r\n../../../../.pyenv/versions/3.6.2/envs/rise/lib/python3.6/site-packages/faiss.py:134: in replacement_reconstruct\r\n    self.reconstruct_c(key, swig_ptr(x))\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <swigfaiss.IndexIDMap; proxy of <Swig Object of type 'faiss::IndexIDMap *' at 0x10d0b09f0> >\r\nkey = 2\r\nrecons = <Swig Object of type 'faiss::HeapArray< faiss::CMax< float,long > >::T *' at 0x10d0b0a20>\r\n\r\n    def reconstruct(self, key, recons):\r\n>       return _swigfaiss.Index_reconstruct(self, key, recons)\r\nE       RuntimeError: Error in virtual void faiss::Index::reconstruct(idx_t, float *) const at Index.cpp:45: Can not compute reconstruct without knowing how to do so\r\n\r\n../../../../.pyenv/versions/3.6.2/envs/rise/lib/python3.6/site-packages/swigfaiss.py:1109: RuntimeError\r\n```"
      },
      {
        "user": "billkle1n",
        "created_at": "2017-08-26T19:59:58Z",
        "body": "Oh, setting `index.maintain_direct_map = True` fixed the issue. Is that because otherwise there's no way to access a vector by ID in constant time (e.g. you'd have to iterate over all the IVF lists)? And how much memory does that direct map cost? 64 bits * # of vectors?"
      },
      {
        "user": "mdouze",
        "created_at": "2017-08-28T08:10:40Z",
        "body": "Yes exactly"
      },
      {
        "user": "billkle1n",
        "created_at": "2017-08-28T18:52:39Z",
        "body": "Thanks for the quick replies ðŸ‘ "
      },
      {
        "user": "chopwoodwater",
        "created_at": "2019-01-02T12:39:12Z",
        "body": "great post, thanks for the answer."
      },
      {
        "user": "abdullahbas",
        "created_at": "2021-09-30T10:04:51Z",
        "body": "What if we have index without `index.maintain_direct_map = True ` ? In my scenario I used  'PCAR64,IVF4096(IVF512,PQ32x4fs,RFlat),SQ8' index str and now I can use reconstruct_n and search_and_reconstruct but it throws error on reconstruct. \r\n\r\n\r\n`   1914 \r\n   1915     def reconstruct(self, key, recons):\r\n-> 1916         return _swigfaiss.IndexPreTransform_reconstruct(self, key, recons)\r\n   1917 \r\n   1918     def reconstruct_n(self, i0, ni, recons):\r\n\r\nRuntimeError: Error in faiss::DirectMap::idx_t faiss::DirectMap::get(faiss::DirectMap::idx_t) const at /__w/faiss-wheels/faiss-wheels/faiss/faiss/invlists/DirectMap.cpp:78: Error: 'lo >= 0' failed: -1 entry in direct_map`\r\n\r\n\r\nI have tried `faiss.downcast_index(index.index).make_direct_map()` but nothing changed. Thanks for the help."
      }
    ]
  },
  {
    "number": 183,
    "title": "how to add new data to old index ",
    "created_at": "2017-08-18T09:32:23Z",
    "closed_at": "2017-08-21T03:38:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/183",
    "body": "Hi,\r\nI have to add some new data to my index, but I found that faiss does not support re-train in FAQ...\r\nSo if I use L2 method, could I add new data to index for search **but not rebuild** a index?\r\n\r\nthx :)",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/183/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-08-18T10:55:45Z",
        "body": "Hi, \r\nYou do not need to re-train the index to add new data to it. This is a problem only if the distribution of the new data differs from the distribution of the training data."
      },
      {
        "user": "ghost",
        "created_at": "2017-08-21T03:38:53Z",
        "body": "I appreciate you time ;D"
      },
      {
        "user": "chikubee",
        "created_at": "2020-04-17T11:35:30Z",
        "body": "@mdouze hey, I am trying to use faiss for semantic search on documents, for my use-case, editing documents, or adding fresh new data and removing data can be a common practise.,\r\nin that scenario, rebuilding the entire index on every CRUD operation can be an expensive operation.\r\nBut as you mentioned, one needs to train it only if distribution differs?\r\ncan you explain how can that be achieved?\r\nand does this hold for all index types?\r\n"
      },
      {
        "user": "wickedfoo",
        "created_at": "2020-04-21T02:11:01Z",
        "body": "@chikubee \"distribution\" here refers to the statistical distribution of the vectors in N-dimensional space. k-means clustering is frequently used in approximate Faiss indices to partition that data distribution into relatively equivalent buckets, or to minimize quantization error.\r\n\r\nIf the data you are adding to an already trained index differs in a meaningful way (significantly out of sample of the data distribution seen when training it), the recall for the newly added out-of-distribution vectors may be low. Whether this is important or not depends upon your own precision/recall metrics desired, and would require you testing these effects."
      },
      {
        "user": "bhargav-11",
        "created_at": "2024-03-12T12:46:02Z",
        "body": "```\r\n    docs = loader.load_and_split(text_splitter)\r\n    for doc in docs:\r\n        doc.metadata[\"pdf\"] = doc.metadata[\"source\"]\r\n\r\n    index = FAISS.load_local(\"embeddings/faiss_index\", embeddings, allow_dangerous_deserialization=True)\r\n\r\n    # Add the file to the index\r\n    index.add_documents(docs)\r\n```"
      }
    ]
  },
  {
    "number": 174,
    "title": "How can I set ClusteringParameters for GpuIndexIVFFlat  in python ?",
    "created_at": "2017-08-07T11:18:58Z",
    "closed_at": "2017-08-10T05:59:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/174",
    "body": "",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/174/comments",
    "author": "djy4713",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-08-07T11:34:58Z",
        "body": "Hi \r\n\r\nwith eg. `index.cp.niter = 50`"
      },
      {
        "user": "djy4713",
        "created_at": "2017-08-07T12:02:11Z",
        "body": "but on gpu edition, it can not work.   eg. GpuIndexIVFFlat object.\r\nI just modify the GpuIndexIVF.h file, change the \"cp_\" variable from projected to public and recompile, then i can work.  eg. gpu_index.cp_.niter = 50"
      },
      {
        "user": "wickedfoo",
        "created_at": "2017-08-07T20:57:20Z",
        "body": "I am changing the GPU code to expose ClusteringParameters in the same way as the CPU code, as a public member. Once the push is made, you should be able to just use `index.cp`.\r\n"
      },
      {
        "user": "djy4713",
        "created_at": "2017-08-08T07:11:11Z",
        "body": "thank you."
      },
      {
        "user": "mdouze",
        "created_at": "2017-08-09T18:22:31Z",
        "body": "Ok, the push is done in the latest Faiss version."
      },
      {
        "user": "mdouze",
        "created_at": "2017-08-10T05:59:35Z",
        "body": "Seems to solve the problem. Closing."
      }
    ]
  },
  {
    "number": 160,
    "title": "About index building with IndexFlatIP",
    "created_at": "2017-07-17T06:44:30Z",
    "closed_at": "2017-08-10T05:56:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/160",
    "body": "Hi, I want to know about the method for index constructing with IndexFlatIP.  Why it is faster than dot product calculation with numpy?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/160/comments",
    "author": "mrcsunshine",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-07-17T07:40:29Z",
        "body": "Hi\r\nThe dot product calculation is not faster. Finding n largest values is."
      },
      {
        "user": "jegou",
        "created_at": "2017-07-17T11:20:07Z",
        "body": "A big matrix multiplication needs to store many values, eventually causing cache misses, while for the index we are only interested in keeping the largest ones. Faiss avoids this problem by tiling the matrix multiplication and keeping a low temporary memory usage at minimum. "
      },
      {
        "user": "mrcsunshine",
        "created_at": "2017-07-18T03:53:08Z",
        "body": "so how to define the largest ones before the query vector comes? and how does the index which set up offline reflect the distance message ?"
      },
      {
        "user": "mdouze",
        "created_at": "2017-07-18T09:58:32Z",
        "body": "please rephrase."
      },
      {
        "user": "mdouze",
        "created_at": "2017-08-10T05:56:58Z",
        "body": "Closing."
      }
    ]
  },
  {
    "number": 151,
    "title": "how to use query server after add  feature vector index ?",
    "created_at": "2017-07-07T12:11:25Z",
    "closed_at": "2017-07-12T04:08:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/151",
    "body": "lsh = faiss.IndexLSH(d, n_bits)\r\nlsh.add(xb_tmp)\r\nI have finish to add feature vector index .\r\nIf I have added all my vectors, and then how to use only run lsh.search(q_tmp.astype('float32'), k) exclude lsh.add.\r\n\r\nseparate index server and search server.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/151/comments",
    "author": "vega110",
    "comments": [
      {
        "user": "vega110",
        "created_at": "2017-07-10T02:19:21Z",
        "body": "@mdouze can u tell me?"
      },
      {
        "user": "wickedfoo",
        "created_at": "2017-07-10T02:44:15Z",
        "body": "Sorry, what is your question? I don't understand.\r\nWhat do you mean by \"query server\"?\r\n"
      },
      {
        "user": "vega110",
        "created_at": "2017-07-10T10:08:21Z",
        "body": "After add index of image vectors in my function code by faiss  (lsh=faiss.IndexLSH(d, n_bits); lsh.add(xb_tmp)), how can I find the index of vectors.And how do I operate the specified store path. If I want to code search function in other codes(lsh = faiss.IndexLSH(d, n_bits); lsh.search(q_tmp.astype('float32'), k);) ,how query the  vector in memory?"
      },
      {
        "user": "mdouze",
        "created_at": "2017-07-10T10:27:33Z",
        "body": "Please rephrase."
      }
    ]
  },
  {
    "number": 139,
    "title": "why Spherical by default if the metric is inner_product",
    "created_at": "2017-06-19T10:40:02Z",
    "closed_at": "2017-08-10T05:55:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/139",
    "body": "in file IndexIVF.cpp line 46, why Spherical by default if the metric is inner_product?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/139/comments",
    "author": "kittenkaka",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-06-19T11:09:38Z",
        "body": "Hi \r\n\r\nWe observed that spherical assignment gives better results. This makes sense because otherwise the centroids with a higher norm inherently are assigned to more often, and unbalanced assignments are typically less efficient."
      },
      {
        "user": "kittenkaka",
        "created_at": "2017-06-19T11:49:08Z",
        "body": "Hi,\r\nBut it does't get correct result when I set spherical =true. \r\nIn 2-IVFFlat.py  I set quantizer = faiss.IndexFlatIP(d) and index = faiss.IndexIVFFlat(quantizer, d, nlist) , but the result is quite different with 1-Flat.py"
      },
      {
        "user": "mdouze",
        "created_at": "2017-08-10T05:55:49Z",
        "body": "Since the metric is different, there is no reason that the results are the same."
      }
    ]
  },
  {
    "number": 138,
    "title": "image feature is 2-Dimensional, what is ids for one picture ?",
    "created_at": "2017-06-19T01:24:48Z",
    "closed_at": "2017-06-23T02:48:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/138",
    "body": "i extract feature from image. every picture have 2-Dimensional feature, like [[0.32, 0.52 ...], [0.32, 0.52 ...], [0.32, 0.52 ...]].\r\nwhen i index for one picture. it like this\r\n\r\nxb = # xb extract from one image\r\nindex = faiss.IndexFlatL2(xb.shape[1])\r\nindex_with_ids = faiss.IndexIDMap(index)\r\nindex_with_ids.add_with_ids(xb, ids)\r\n\r\nthen, what is ids for one picture ?\r\n\r\nand, what is xb and ids for many images ? (image feature is 2-Dimensional)",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/138/comments",
    "author": "linzuk",
    "comments": [
      {
        "user": "linzuk",
        "created_at": "2017-06-19T01:24:58Z",
        "body": "does all the id for one picture xb is the same?\r\nxb = # xb extract from one image\r\nids = [image_id] * xb.shape[0] # all the id for this image is same\r\nindex = faiss.IndexFlatL2(xb.shape[1])\r\nindex_with_ids = faiss.IndexIDMap(index)\r\nindex_with_ids.add_with_ids(xb, ids)"
      },
      {
        "user": "mdouze",
        "created_at": "2017-06-19T05:04:53Z",
        "body": "Hi \r\n\r\nThe code above will not work because ids must by a numpy array of type int64 not a Python list. And indeed, if you give the same id to all vectors, you will get a single id as output.\r\n"
      }
    ]
  },
  {
    "number": 128,
    "title": "gpu indexflat fail",
    "created_at": "2017-06-06T05:01:35Z",
    "closed_at": "2017-06-21T16:43:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/128",
    "body": "Hello, I 've train to search 10 Million Vectors by GpuIndexFlat. But fail , even when I queried only one vector in index.search.\r\n\r\n~/faiss/gpu$ ./test/demo_ivfpq_indexing_gpu \r\n[4.324 s] Building a dataset of 10000000 vectors to index\r\n[36.067 s] Adding the vectors to the index\r\n[38.161 s] done\r\n[38.195 s] done\r\nWARN: increase temp memory to avoid cudaMalloc, or decrease query/add size (alloc 1280000000 B, highwater 1280000000 B)\r\nWARN: increase temp memory to avoid cudaMalloc, or decrease query/add size (alloc 1280000000 B, highwater 2560000000 B)\r\nFaiss assertion err == cudaSuccess failed in void faiss::gpu::StackDeviceMemory::Stack::returnAlloc(char*, size_t, cudaStream_t) at utils/StackDeviceMemory.cpp:136Aborted (core dumped)",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/128/comments",
    "author": "xasw1",
    "comments": [
      {
        "user": "xasw1",
        "created_at": "2017-06-06T06:56:09Z",
        "body": "Should all vectors in index.add load in Gpu when I performing gpu search?"
      },
      {
        "user": "wickedfoo",
        "created_at": "2017-06-06T16:16:44Z",
        "body": "What is the dimensionality of your vectors?\r\n\r\nWe also might have a fix for this locally that we haven't pushed to github yet."
      },
      {
        "user": "wickedfoo",
        "created_at": "2017-06-06T17:56:45Z",
        "body": "Are you using GpuIndexIVFPQ or GpuIndexFlat?\r\n\r\nIf GpuIndexIVFPQ, in the meantime, you can try disabling precomputed codes (`setPrecomputedCodes(false)`) since you appear to be running out of memory."
      },
      {
        "user": "xasw1",
        "created_at": "2017-06-07T00:16:13Z",
        "body": "Hi, I'm using GpuIndexFlat(INNER_PRODUCT) with the dimension = 256 and My Gpu is nvidia Titan X."
      },
      {
        "user": "xasw1",
        "created_at": "2017-06-07T00:19:16Z",
        "body": "By the way , When I used GpuIndexIVFFlat with nt = 1,000,000 , database = 50,000,000 , nlist = 28284,  I received different problem:\r\n\r\nTraining IVF quantizer on 1000000 vectors in 256D\r\nWARNING clustering 1000000 points to 28284 centroids: please provide at least 1103076 training points\r\nClustering 1000000 points in 256D to 28284 clusters, redo 1 times, 10 iterations\r\n  Preprocessing in 0.25 s\r\n  Iteration 0 (71.66 s, search 71.52 s): objective=7.60792e+06 imbalance=16.259   Iteration 1 (144.12 s, search 143.90 s): objective=8.12524e+06 imbalance=2.999  Iteration 2 (216.85 s, search 216.56 s): objective=8.13144e+06 imbalance=2.557  Iteration 3 (289.57 s, search 289.22 s): objective=8.13492e+06 imbalance=2.391  Iteration 4 (362.31 s, search 361.88 s): objective=8.13722e+06 imbalance=2.306  Iteration 5 (435.44 s, search 434.94 s): objective=8.13878e+06 imbalance=2.253  Iteration 6 (508.27 s, search 507.70 s): objective=8.14026e+06 imbalance=2.217  Iteration 7 (581.38 s, search 580.74 s): objective=8.14126e+06 imbalance=2.189  Iteration 8 (654.35 s, search 653.64 s): objective=8.14207e+06 imbalance=2.169  Iteration 9 (727.65 s, search 726.85 s): objective=8.14278e+06 imbalance=2.152 nsplit=0       \r\n[734.751 s] storing the pre-trained index to /tmp/index_trained.faissindex\r\n[734.883 s] Building a dataset of 50000000 vectors to index\r\n[895.238 s] Adding the vectors to the index\r\nterminate called after throwing an instance of 'thrust::system::system_error'\r\n  what():  function_attributes(): after cudaFuncGetAttributes: an illegal memory access was encountered\r\nAborted (core dumped)"
      },
      {
        "user": "wickedfoo",
        "created_at": "2017-06-07T00:37:24Z",
        "body": "For GpuIndexFlat, 10 million vectors * 256 * sizeof(float) = 10.24 GB of memory. Faiss will reserve some fraction (1-2 GB) of GPU memory up front for temporary space, so you will run out of memory in this case (your card has 12 GB I believe).\r\n\r\nFor the GpuIndexIVFPQ case, we have a fix for add() internally that we'll be updating the repo with sometime soon.\r\n"
      },
      {
        "user": "xasw1",
        "created_at": "2017-06-07T01:04:26Z",
        "body": "Can I store Vectors on Cpu but search by Gpu?"
      },
      {
        "user": "wickedfoo",
        "created_at": "2017-06-07T01:26:43Z",
        "body": "With unified memory you could, but that defeats the point of using the GPU, as you are limited by the speed of the interconnect between the CPU and GPU (e.g., PCIe or NVLINK). Doing that would run much more slowly than just using the CPU for everything.\r\n"
      },
      {
        "user": "xasw1",
        "created_at": "2017-06-07T02:46:08Z",
        "body": "So it means that if my database have more than 100Millions vectors (256D) , I need more than 10 nvidia cards to store them on Gpu , otherwise,  I can't query on Gpu?"
      },
      {
        "user": "wickedfoo",
        "created_at": "2017-06-07T03:55:25Z",
        "body": "You can store that many vectors (and more, up to a billion or so) on a single GPU, you just can't use GpuIndexFlat or GpuIndexIVFFlat. You'd have to compress the data using GpuIndexIVFPQ."
      },
      {
        "user": "xasw1",
        "created_at": "2017-06-08T03:53:50Z",
        "body": "New question. I found that 10M vectors could be added by GpuIndexIVFFlat and I've just found that when I add 8.25Million vectors to GpuIndexFlat , nvidia-smi showed it spent only 6000M+ memory. And strangely, it used the same memory size on Gpu when adding 5Million vectors and 8Million vectors. But when I add 8.5Million vectors, nvidia-smi shows it spent 2397M memory which is workspace I guess. I think it means adding not success. So why 1.5M vectors couldn't be added, when there still 5000+M free video memory?"
      },
      {
        "user": "mdouze",
        "created_at": "2017-06-21T16:43:52Z",
        "body": "nvidia-smi seems to report inaccurate memory usage sometimes. \r\nSee @wickedfoo's comment above on memory usage."
      }
    ]
  },
  {
    "number": 122,
    "title": "Where is the ideal directory to install FAISS in Ubuntu 16.04?",
    "created_at": "2017-05-24T20:56:36Z",
    "closed_at": "2017-06-19T11:12:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/122",
    "body": "1. Where is the ideal directory to install FAISS in Ubuntu 16.04?\r\n2. Where should the libfaiss.a be located for g++ compiler?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/122/comments",
    "author": "zoe-cheung",
    "comments": [
      {
        "user": "zoe-cheung",
        "created_at": "2017-05-25T05:56:45Z",
        "body": "Need to improve because for a cpp script in faiss directory with \r\n\r\n#include <cmath>\r\n#include <cstdio>\r\n#include <cstdlib>\r\n#include <cassert>\r\n#include <cstring>\r\n#include <sys/types.h>\r\n#include <sys/stat.h>\r\n#include <unistd.h>\r\n#include <sys/time.h>\r\n#include <faiss/AutoTune.h>\r\n#include <faiss/IndexFlat.h>\r\n\r\nneed to be compile with a lengthy g++ command: \r\n\r\nz@z-VirtualBox:~/faiss$ **g++ -o demo_v3 -I.. -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=NULL -Doverride= -fopenmp demo_v3.cpp libfaiss.a -g -fPIC  -fopenmp -Wl,--no-as-needed -L/opt/intel/compilers_and_libraries/linux/mkl//lib/intel64   -lmkl_intel_ilp64  -lmkl_core -lmkl_gnu_thread -ldl -lpthread**\r\n\r\n\r\n\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2017-05-31T11:17:49Z",
        "body": "Hi \r\n\r\nI don't understand the question."
      }
    ]
  },
  {
    "number": 111,
    "title": "Using indexPQ on disk?",
    "created_at": "2017-05-07T14:35:32Z",
    "closed_at": "2017-05-31T14:00:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/111",
    "body": "Hi, I've tried to look through the wiki and API docs to find an answer to this question but its still not very clear to me.  \r\n\r\nI'd like to generate a very large PQ index (tens/hundreds of millions of large vectors).  The original vectors would definitely not fit in memory (I currently use HDF5) and even the PQ index may not.  So I'd like to work with the PQ index \"on disk\" -- as with HDF5.  I'd like to be able to add vectors to the index on disk and also retrieve vectors based on their IDs not just via a search, all without reading the whole index into memory.  Is this currently possible?\r\n\r\nThe project description suggests that it should be possible to work with datasets that are larger than RAM but I don't see how to have an on-disk PQ index.  Even if its not directly possible would their be a way to hack it by using HDF5 to store the codes and then reconstruct partial PQ indices in memory?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/111/comments",
    "author": "funnydevnull",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-05-07T20:05:49Z",
        "body": "Hi \r\n\r\nAll Faiss indexes work only in RAM, except IndexIVFPQCompact that can optionally be memory-mapped from disk. The Faiss indexes are just not optimized properly to work with disk based storage. \r\n\r\nThis being said, there are two answers to your question: \r\n\r\n- for search, `IndexPQ` is orders of magnitude slower than indexes that perform non-exhaustive search like `IndexIVFPQ`, especially for billion-scale datasets, so I am not sure what a proper use case would be for it.\r\n\r\n- for id-based access, the easiest way to store a huge set of PQ encoded vectors would be to use a `ProductQuantizer` object's compute_codes and decode functions to access a memory-mapped file on disk (probably hdf5 offers memory mapping as well). It does not make much sense to use `IndexPQ`.\r\n"
      },
      {
        "user": "funnydevnull",
        "created_at": "2017-05-07T21:10:08Z",
        "body": "Many thanks for a response.  I was primarily thinking about indexing rather than search so I was looking at IndexPQ rather than IndexIVFPQ.  I think your second option might work but I'm also wondering if there isn't another way for my use case.\r\n\r\nI would like to train a ProductQuantizer, then pull out its centroids and store them on disk (this seems to be possible).  I would then encode any vectors I want, store them on disk (in a compressed form), using HDF5 and then just decode them on the fly when I need them using the PQ object (which I need to be able to reconstruct from on-disk data).\r\n\r\nUPDATE: I had some questions below regarding how to do this using ProductQuantizer.get_centroids() but I figured out that the correct approach is to just use faiss.write_ProductQuantizer.  I removed the rest of this question because I think its now clear how to proceed (and I don't think the question added much).  I will test this approach and update my response in case its useful for anyone in the future.\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2017-05-31T14:00:54Z",
        "body": "Closing this issue. Please re-open if there is news on this front."
      }
    ]
  },
  {
    "number": 61,
    "title": "how to search with cosine similarity",
    "created_at": "2017-04-01T03:26:47Z",
    "closed_at": "2017-04-03T06:10:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/61",
    "body": "hi, I only see two choices for searching:  METRIC_INNER_PRODUCT, METRIC_L2. how can I search with cosine similarity?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/61/comments",
    "author": "YaaoTu",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-04-03T06:10:26Z",
        "body": "Hi \r\n\r\nPlease L2-normalize the vectors before adding and searching, then search with METRIC_INNER_PRODUCT."
      },
      {
        "user": "yhpku",
        "created_at": "2017-04-13T13:38:38Z",
        "body": "@mdouze ï¼ŒI have the same question. But how to search with METRIC_INNER_PRODUCT? can you give example and code? thanks."
      },
      {
        "user": "billkle1n",
        "created_at": "2017-08-29T03:15:07Z",
        "body": "@yhpku something like that:\r\n\r\n```python\r\nfrom faiss import normalize_L2\r\n\r\n# ...\r\n\r\nindex.train(normalize_L2(training_vectors))\r\nindex.add(normalize_L2(index_vectors))\r\nindex.search(normalize_L2(search_vectors), 5)\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2017-08-29T09:01:00Z",
        "body": "The metric inner product flag is set when the index is built."
      },
      {
        "user": "HoiM",
        "created_at": "2018-08-24T05:52:09Z",
        "body": "@billkle1n \r\nIt seems that faiss.normalize_L2() doesn't have a return value. It normalizes the matrix in place. So instead of\r\n `index.train(normalize_L2(training_vectors))`, \r\nit should be\r\n`normalize_L2(training_vectors)`\r\n`index.train(training_vectors)`"
      },
      {
        "user": "13293824182",
        "created_at": "2019-01-05T17:08:42Z",
        "body": "I have a question, when i try normalize_L2(dest_array_one) , i get the error:\r\n File \"<stdin>\", line 1, in <module>\r\n  File \"/root/anaconda3/envs/faiss/lib/python2.7/site-packages/faiss/__init__.py\", line 523, in normalize_L2\r\n    fvec_renorm_L2(x.shape[1], x.shape[0], swig_ptr(x))\r\nTypeError: in method 'fvec_renorm_L2', argument 3 of type 'float *' "
      },
      {
        "user": "artmoskvin",
        "created_at": "2020-02-10T14:58:22Z",
        "body": "@13293824182 make sure your array is of type float32"
      },
      {
        "user": "pranavnijampurkar33",
        "created_at": "2020-10-22T15:10:05Z",
        "body": "Just adding example if noob like me came here to find how to calculate the Cosine similarity from scratch\r\n\r\nimport faiss\r\ndataSetI = [.1, .2, .3]\r\ndataSetII = [.4, .5, .6]\r\n#dataSetII = [.1, .2, .3]\r\n\r\nx = np.array([dataSetI]).astype(np.float32)\r\nq = np.array([dataSetII]).astype(np.float32)\r\nindex = faiss.index_factory(3, \"Flat\", faiss.METRIC_INNER_PRODUCT)\r\nindex.ntotal\r\nfaiss.normalize_L2(x)\r\nindex.add(x)\r\nfaiss.normalize_L2(q)\r\ndistance, index = index.search(q, 5)\r\nprint('Distance by FAISS:{}'.format(distance))\r\n\r\n#To Tally the results check the cosine similarity of the following example\r\n\r\nfrom scipy import spatial\r\n\r\n\r\nresult = 1 - spatial.distance.cosine(dataSetI, dataSetII)\r\nprint('Distance by FAISS:{}'.format(result))"
      },
      {
        "user": "pzdkn",
        "created_at": "2023-09-12T12:50:34Z",
        "body": "do I need to use this function `faiss.normalize_L2` or can I use a numpy function for normalization ?\r\nwhat's the advantage of using it over writing it yourself?"
      }
    ]
  },
  {
    "number": 3511,
    "title": "SearchParametersIVF does not seem to work with IndexHNSWFlat",
    "created_at": "2024-06-13T14:27:12Z",
    "closed_at": "2024-07-01T00:59:40Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3511",
    "body": "I use IndexHNSWFlat to search a small dataset (1024 entries) of dimension 64 and it works fine. But when I try to use the SearchParametersIVF option to restrict some of the searches I get the following error:\r\n\r\n`---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_3079933/2324586828.py in <module>\r\n----> 1 M_b1 = calc_faiss_mask_causal_blocks(query_data, key_data, k=65, blocks=1, mode='graph')\r\n      2 evaluate_mask(query_data, key_data, M_b1, k=65)\r\n      3 # print(np.sum(Mf, axis=1))\r\n      4 # print(Mf[0,:])\r\n\r\n~/cs-gpt/notebooks/lsh_aux.py in calc_faiss_mask_causal_blocks(Q, K, k, blocks, mode)\r\n    378             D, I = index.search(Q[query_block_range,:], k=k-1, params=faiss.SearchParametersIVF(sel=id_selector, nprobe=3))\r\n    379         elif mode == 'graph':\r\n--> 380             D, I = index.search(Q[query_block_range,:], k=k-1, params=faiss.SearchParametersIVF(sel=id_selector))\r\n    381 \r\n    382         # print(I)\r\n\r\n~/.local/lib/python3.9/site-packages/faiss/class_wrappers.py in replacement_search(self, x, k, params, D, I)\r\n    341             assert I.shape == (n, k)\r\n    342 \r\n--> 343         self.search_c(n, swig_ptr(x), k, swig_ptr(D), swig_ptr(I), params)\r\n    344         return D, I\r\n    345 \r\n\r\n~/.local/lib/python3.9/site-packages/faiss/swigfaiss_avx2.py in search(self, n, x, k, distances, labels, params)\r\n   6424     def search(self, n, x, k, distances, labels, params=None):\r\n   6425         r\"\"\" entry point for search\"\"\"\r\n-> 6426         return _swigfaiss_avx2.IndexHNSW_search(self, n, x, k, distances, labels, params)\r\n   6427 \r\n   6428     def reconstruct(self, key, recons):\r\n\r\nRuntimeError: Error in virtual void faiss::IndexHNSW::search(faiss::idx_t, const float*, faiss::idx_t, float*, faiss::idx_t*, const faiss::SearchParameters*) const at /project/faiss/faiss/IndexHNSW.cpp:297: Error: 'params' failed: params type invalid`\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3511/comments",
    "author": "eigenvectorBazuz",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2024-06-17T12:22:05Z",
        "body": "As the name indicates, you should use `SearchParametersHNSW`, that also has an IDSelector field."
      }
    ]
  },
  {
    "number": 3378,
    "title": "Cannot debug similarity search",
    "created_at": "2024-04-20T13:06:07Z",
    "closed_at": "2024-05-24T17:31:56Z",
    "labels": [
      "help wanted",
      "out-of-scope"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3378",
    "body": "am trying to build a similarity search in python, cannot debug the function: \r\n\r\ndef perform_similarity_search(query_text, index, embeddings, top_k=5):\r\n    \"\"\"Perform similarity search in the FAISS index for a given query text.\"\"\"\r\n    # Use the embeddings object to embed the query_text into a vector.\r\n    # Ensure the text is passed as a list and the result is accessed correctly.\r\n    query_vector = embeddings.encode([query_text])\r\n    # Reshape the query_vector for compatibility with FAISS search method if necessary.\r\n    # FAISS expects the query vector to be a 2D array.\r\n    if len(query_vector.shape) == 1:\r\n        query_vector = query_vector.reshape(1, -1)\r\n        \r\n    # Search the index using the reshaped query_vector.\r\n    distances, indices = index.search(query_vector, top_k)  # Search the index for the top_k closest vectors\r\n    return distances, indices\r\n\r\n\r\ndef run_indexing_pipeline():\r\n    documents = fetch_documents(documents_dir)\r\n    text_chunks = divide_documents_into_text_chunks(documents)\r\n    embeddings_model = prepare_embeddings()\r\n    faiss_index = build_and_store_faiss_index(text_chunks, embeddings_model, faiss_db_path)\r\n    \r\n\r\n    # Example query for testing purposes\r\n    query = \"Enter some example text here\"\r\n    distances, indices = perform_similarity_search(query, faiss_index, embeddings_model)\r\n    print(\"Distances:\", distances)\r\n    print(\"Indices:\", indices)\r\n\r\ndef perform_similarity_search(query_text, index, embeddings, top_k=5):\r\n    \"\"\"Perform similarity search in the FAISS index for a given query text.\"\"\"\r\n    # Use the embeddings object to embed the query_text into a vector.\r\n    # Ensure the text is passed as a list and the result is accessed correctly.\r\n    query_vector = embeddings.encode([query_text])\r\n    # Reshape the query_vector for compatibility with FAISS search method if necessary.\r\n    # FAISS expects the query vector to be a 2D array.\r\n    if len(query_vector.shape) == 1:\r\n        query_vector = query_vector.reshape(1, -1)\r\n        \r\n    # Search the index using the reshaped query_vector.\r\n    distances, indices = index.search(query_vector, top_k)  # Search the index for the top_k closest vectors\r\n    return distances, indices\r\nERRORS: \r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/new_d.py\", line 61, in <module>\r\n    run_indexing_pipeline()\r\n  File \"/home/ubuntu/new_d.py\", line 56, in run_indexing_pipeline\r\n    distances, indices = perform_similarity_search(query, faiss_index, embeddings_model)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/new_d.py\", line 36, in perform_similarity_search\r\n    query_vector = embeddings.encode([query_text])\r\n\r\nThis is error being shown, pls let me know how I can correct it\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3378/comments",
    "author": "ssdidis",
    "comments": [
      {
        "user": "mlomeli1",
        "created_at": "2024-04-24T09:17:20Z",
        "body": "it looks like in this pipeline, the function build_and_store_faiss_index() is a wrapper that calls the faiss library. However, the rest of the functions are either user-defined or come from some other library - can't really tell because your code is not reproducible. Your error says you have a problem in embeddings.encode([query_text]) which is probably not using faiss since the core faiss does not support embedding text @ssdidis so this is out of scope for us. "
      }
    ]
  },
  {
    "number": 3339,
    "title": "HNSW not support remove_ids? \"IDMap2,HNSW32,Flat\" also not support remove_ids?",
    "created_at": "2024-04-02T09:27:36Z",
    "closed_at": "2024-07-01T00:38:30Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3339",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3339/comments",
    "author": "lgbest123",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2024-04-05T22:03:05Z",
        "body": "No, HNSW does not support remove_ids. \r\nIt is non-trivial to implement because the graph has to be re-linked after some nodes are removed."
      }
    ]
  },
  {
    "number": 3333,
    "title": "Can I retrieve points within each cluster from spherical k-means clustering?",
    "created_at": "2024-03-29T13:23:24Z",
    "closed_at": "2024-05-24T17:34:25Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3333",
    "body": "# Platform\r\n\r\n\r\nOS: ubuntu 22.04\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: source\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\nI'm considering using spherical k-means clustering for my project. I'm curious if there's a way to obtain a list, preferably in a 2D format, that contains all points within each cluster. For instance, assuming a k value of 2 and my vectors are (1, 1), (1, 2), (1, 3), (-1, -2), and (-3, -4), I'm looking for a method that returns the points in clusters similar to the following structure:\r\n\r\nCentroid point a: [(1, 1), (1, 2), (1, 3)]\r\nCentroid point b: [(-1, -2), (-3, -4)]\r\nIs there a straightforward way to achieve this? Any guidance or suggestions would be greatly appreciated.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3333/comments",
    "author": "KimMinSang96",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2024-04-05T22:00:35Z",
        "body": "you can use `kmeans.assign(points)`"
      }
    ]
  },
  {
    "number": 3321,
    "title": "call ivf search coredump  through multithreads",
    "created_at": "2024-03-27T09:12:54Z",
    "closed_at": "2024-07-01T00:36:04Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3321",
    "body": "# Summary\r\n\r\nmy program call ivf's search coredump through  multithreads. if the parameter **nprobe** is the same in all requests, problem disappeared. single thread has not the problem.  \r\n\r\n# Platform\r\ncentos 7 x86_64\r\n\r\nFaiss version: 1.70\r\n\r\nInstalled from: compile source code\r\n\r\nRunning on:\r\n-CPU\r\n\r\nInterface: \r\n- C++\r\n\r\n# error information:\r\n\r\nterminate called after throwing an instance of 'faiss::FaissException'\r\n what():  Error in faiss::IndexIVF::search_preassigned(faiss::Index::idx_t, const float*, faiss::Index::idx_t, const idx_t*, const float*, float*, faiss::Index::idx_t*, bool, const faiss::IVFSearchParameters*, faiss::IndexIVFStats*) const::__lambda4 at /data1/minisearch/upload/zhixue3/git/vs/third-64/faiss/faiss/IndexIVF.cpp:459: Error: 'key < (idx_t)nlist' failed: Invalid key=4537784286748968700 nlist=20480\r\n\r\nI suspect this issue is related to OpenMP.\r\n\r\nHow should this issue be resolved?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3321/comments",
    "author": "shaozhixue",
    "comments": [
      {
        "user": "alexanderguzhva",
        "created_at": "2024-03-28T15:00:17Z",
        "body": "Do I get it correctly that you'd like to run `index.search()` using multiple threads, and search thread wants to use its own `nprobe`? If so, then you need to pass up `const faiss::IVFSearchParameters*`  parameter, which allows to configure `nprobe` for each search request\r\n"
      },
      {
        "user": "shaozhixue",
        "created_at": "2024-04-01T23:30:16Z",
        "body": "> Do I get it correctly that you'd like to run `index.search()` using multiple threads, and search thread wants to use its own `nprobe`? If so, then you need to pass up `const faiss::IVFSearchParameters*` parameter, which allows to configure `nprobe` for each search request\r\n\r\nThank you for your reply, I will try it right away"
      }
    ]
  },
  {
    "number": 3198,
    "title": "Unable to write IndexFlatIP in faiss-gpu",
    "created_at": "2024-01-10T19:02:42Z",
    "closed_at": "2024-06-19T14:48:41Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3198",
    "body": "# Summary\r\n\r\nI am using Faiss to retrieve similar products. My embedding size is 1024. I tried faiss-cpu but it was too slow. Hence, I am trying faiss-gpu. However, in my experiments, I am unable to write an IndexFlatIP index. I was able to use write_index() in faiss-cpu.\r\n\r\n# Platform\r\n\r\nOS: Ubuntu 20.04.5 LTS\r\n\r\nFaiss version: faiss-gpu: 1.7.2\r\n\r\nInstalled from: pip\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n## Code\r\n\r\n```import pandas as pd\r\nimport numpy as np\r\nimport faiss\r\n\r\ndef create_dummy_embeddings():\r\n    # Create dummy embeddings\r\n    embeddings = np.random.rand(100000, 1024).astype('float32')\r\n    print('Dummy embeddings created:', embeddings.shape)\r\n    return embeddings\r\n\r\ndef create_index(embeddings):\r\n    # Create index\r\n    EMBEDDING_SIZE = 1024\r\n    print('Embedding size:', EMBEDDING_SIZE)\r\n\r\n    # GPU\r\n    res = faiss.StandardGpuResources()  # use a single GPU\r\n    ngpus = faiss.get_num_gpus()\r\n    print(\"Number of GPUs:\", ngpus)\r\n    cpu_index = faiss.IndexFlatIP(EMBEDDING_SIZE)\r\n    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\r\n    gpu_index.add(embeddings)\r\n    print('Number of vectors in index:', gpu_index.ntotal)\r\n\r\n    faiss.write_index(gpu_index, 'faiss_index_dummy.index')\r\n    print('Index saved to file.')\r\n\r\n\r\ndef load_index():\r\n    index = faiss.read_index('faiss_index_dummy.index')\r\n    print('Index loaded from file.')\r\n    print('Number of vectors in index:', index.ntotal)\r\n\r\n\r\ndef main():\r\n    embeddings = create_dummy_embeddings()\r\n    create_index(embeddings)\r\n    load_index()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n    \r\n## Error\r\n`RuntimeError: Error in void faiss::write_index(const faiss::Index*, faiss::IOWriter*) at /project/faiss/faiss/impl/index_write.cpp:590: don't know how to serialize this type of index`\r\n\r\nHoping to get this fixed! Thank you!\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3198/comments",
    "author": "gajghatenv",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2024-01-11T15:50:21Z",
        "body": "GPU indexes cannot be stored directly. Please convert to CPU first. "
      },
      {
        "user": "gajghatenv",
        "created_at": "2024-01-11T16:14:56Z",
        "body": "So basically:\r\n\r\n1. Create a cpu_index\r\n2. Convert it to a gpu_index\r\n3. Add vectors to the gpu_index\r\n4. Convert gpu_index back to cpu_index\r\n5. Write cpu_index\r\n\r\nAm I correct? \r\n\r\nAnd then when trying to use the index:\r\n1. Read in the cpu_index\r\n2. Convert to gpu_index\r\n3. Perform searches\r\n\r\nDid I get this right?"
      }
    ]
  },
  {
    "number": 3151,
    "title": "IVFFlat is slower than FlatIP on cpu",
    "created_at": "2023-11-29T11:20:42Z",
    "closed_at": "2024-06-19T14:50:38Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3151",
    "body": "# Summary\r\nfaiss.IndexIVFFlat is slower than faiss.IndexFlatIP, I dont know why , the numpy installed like \"pip install intel-numpy\" faiss installed like \"pip install faiss-cpu\", whatever windows or linux , always slow\r\n\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n```python\r\nimport faiss\r\nimport numpy as np\r\nimport time\r\n\r\nfeat = np.random.rand(200000,256).astype(np.float32)\r\nfaiss.normalize_L2(feat)\r\n\r\nindex = faiss.IndexFlatIP(256)\r\n\r\nt1 = time.time()\r\nindex.add(feat)\r\n\r\nD, I = index.search(feat, 80)\r\nI = I[5]\r\nI.sort()\r\nprint(I)\r\nprint(time.time() - t1)\r\n\r\n\r\nnlist = 100\r\nk = 80\r\nquantizer = faiss.IndexFlatIP(256)  # the other index\r\nindex = faiss.IndexIVFFlat(quantizer, 256, nlist, faiss.METRIC_INNER_PRODUCT)\r\n# here we specify METRIC_L2, by default it performs inner-product search\r\nt1 = time.time()\r\nassert not index.is_trained\r\nindex.train(feat)\r\nassert index.is_trained\r\n\r\nt2 = time.time()\r\nprint(\"train time\", t2 - t1)\r\nindex.add(feat)                  # add may be a bit slower as well\r\nprint(\"add time\", time.time() - t2)\r\nindex.nprobe = 10\r\nD, I2 = index.search(feat, k)     # actual search\r\nI2 = I2[5]\r\nI2.sort()\r\nprint(I2)                  # neighbors of the 5 last queries\r\nprint(time.time() - t2)\r\n``` \r\n\r\nthe result like this\r\n[     5   1894   2280   5111  10577  11089  14732  18807  18967  21015\r\n  23812  24122  28390  34554  35481  35825  36582  36676  38201  41180\r\n  43978  45719  52503  53086  60008  64889  66976  70646  72942  73486\r\n  74631  75932  79380  79633  80082  86244  90109  93231  94581  94660\r\n 100069 101279 106608 108983 109923 110811 111534 114150 115507 120026\r\n 121706 121856 122207 128322 130886 132046 132558 132770 135639 136214\r\n 138723 139387 140430 143404 143824 147467 150482 154372 159113 161653\r\n 162230 163741 164281 171253 175471 181143 188320 190826 196939 198481]\r\n68.06165814399719\r\ntrain time 0.2234022617340088\r\nadd time 0.1904909610748291\r\n[     5    649   2003   2592   5275   5722   6303   8940  14659  14732\r\n  16775  17021  18792  18967  21015  23812  26365  27189  27495  28345\r\n  28839  30134  31050  34804  37239  37717  38332  41180  47334  50602\r\n  51536  52664  53286  55272  58175  58839  61951  68532  70991  74631\r\n  75405  75932  83016  88473  91417  92814  97242 100885 110811 111843\r\n 115925 116086 117007 120136 121706 125411 125960 127650 139682 140102\r\n 147988 154360 159113 160612 163741 164281 165284 181111 181976 186438\r\n 188130 188616 190336 191513 192460 192993 193155 193458 197498 199241]\r\n222.23961925506592\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3151/comments",
    "author": "devilztt",
    "comments": [
      {
        "user": "EFLql",
        "created_at": "2023-11-29T11:28:58Z",
        "body": "I also encountered the same question, please take a look of it, thanks. @mdouze @devilztt @tomdyson @ot @lsb @benfred "
      },
      {
        "user": "mdouze",
        "created_at": "2023-12-07T08:50:59Z",
        "body": "things to look at: \r\n- 10/256 is a significant fraction of the inverted lists, try reduce nprobe\r\n- the inverted lists may be very unbalanced. Check index.invlists.imbalance_factor()\r\n- install via conda, pip fetches non standard packages.\r\n"
      },
      {
        "user": "devilztt",
        "created_at": "2023-12-12T11:31:33Z",
        "body": "> Reduce nprobe can make it quick a little, but still slow, if set nprobe = 1, the acc is too low. Why is setting it to 10 so slow, my nlist = 100 .\r\n> The data is random, but even when I use real data, the results are similar. I think it has nothing to do with the proportion of the data. And I print index.invlists.imbalance_factor() = 1.06509302.\r\n> I have tried using Conda to install it, but it is also the same problem, that is, using C++as an example, compiling with the addition of the mkl library is the same\r\n> I want to know if you have tried this before. Currently, as long as I use a CPU platform, the results are the same. As long as I use a math library, IVFFlat is much slower than FlatL2.\r\n\r\n1. Reduce nprobe can make it quick a little, but still slow, if set nprobe = 1, the acc is too low. Why is setting it to 10 so slow, my nlist = 100 .\r\n2. The data is random, but even when I use real data, the results are similar. I think it has nothing to do with the proportion of the data. And I print index.invlists.imbalance_factor() = 1.06509302.\r\n3. I have tried using Conda to install it, but it is also the same problem, that is, using C++as an example, compiling with the addition of the mkl library is the same\r\n\r\nI want to know if you have tried this before. Currently, as long as I use a CPU platform, the results are the same. As long as I use a math library, IVFFlat is much slower than FlatL2."
      }
    ]
  },
  {
    "number": 3124,
    "title": "Storing metadata along with embedding and filtering support + mmap",
    "created_at": "2023-11-02T22:24:33Z",
    "closed_at": "2024-06-30T23:56:12Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3124",
    "body": "QQ : Does faiss ivf variants support storing metadata along with embeddings and support filtering based on this metadata ? \r\nI do see id based filtering , curios if getting eligible list of ids from some sort of inverted or other index are also being supported or natively supported by some ann algoithms\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3124/comments",
    "author": "patelprateek",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-11-06T11:13:38Z",
        "body": "no only int64 is supported as ids. "
      }
    ]
  },
  {
    "number": 3119,
    "title": "User-Defined Metrics",
    "created_at": "2023-11-02T02:08:23Z",
    "closed_at": "2024-06-30T23:54:45Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3119",
    "body": "Is it possible to include User-Defined Metrics?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3119/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-11-02T16:27:53Z",
        "body": "Yes if you code them in C++."
      }
    ]
  },
  {
    "number": 3111,
    "title": "HNSW as post index for IVF",
    "created_at": "2023-10-23T13:12:45Z",
    "closed_at": "2024-03-18T18:03:28Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3111",
    "body": "# Summary\r\n\r\nI want to create a HNSW index with IVF, but index factory can't parse \"IVF5000_HNSW16,HNSW\"\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Ubuntu\r\n\r\nFaiss version: 1.7.4\r\n\r\nInstalled from: pip\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n```python\r\nimport faiss\r\n\r\nfaiss.index_factory(16, \"IVF5000_HNSW16,HNSW\") # returns Error: 'index_ivf' failed: could not parse code description HNSW in IVF5000_HNSW16,HNSW\r\n```\r\nI've also tried \"IVF5000_HNSW16,HNSW,Flat\", but faced the same problem.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3111/comments",
    "author": "kimihailv",
    "comments": [
      {
        "user": "shiwanghua",
        "created_at": "2023-10-24T02:19:17Z",
        "body": "How about 'HNSWFlat' ?"
      },
      {
        "user": "mdouze",
        "created_at": "2023-10-24T10:24:44Z",
        "body": "I don't understand what you want to do. Have 2 HSNW indexes?\r\nThe supported syntax is \"IVF5000_HNSW16,Flat\""
      },
      {
        "user": "kimihailv",
        "created_at": "2023-10-24T10:44:07Z",
        "body": "I want to use IVF with HNSW as coarse quantizer and HNSW as index for post-verification. Maybe I don't understand the idea of IVF:\r\n\r\n1) Train Kmeans\r\n2) Find the nearest cluster for a query vector using coarse quantizer (HNSW in my case because k is large).\r\n3) Find the neighbours in the nearest cluster from 2) using HNSW"
      },
      {
        "user": "mdouze",
        "created_at": "2024-03-18T18:03:28Z",
        "body": "Sorry for not answering. This is not supported in Faiss (although this method is used in some research works)."
      }
    ]
  },
  {
    "number": 3069,
    "title": "(Help Needed) TypeError: in method 'IndexIDMap_search', argument 4 of type 'faiss::IndexIDMapTemplate< faiss::Index >::idx_t'",
    "created_at": "2023-09-19T07:32:34Z",
    "closed_at": "2023-11-22T20:36:28Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3069",
    "body": "# Summary\r\nI am using IndexIVFFlat followed by IndexIDMap to add the ids. It runs fine on the same platform and databricks notebook but when I try to use this in a script to log the same index in mlflow and load the index from mlflow, it throws type error\r\n\r\n\r\nFaiss version:  faiss-gpu     1.7.2\r\n\r\nInstalled from: !pip install faiss-gpu\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n - GPU\r\n\r\nInterface: \r\n- Python\r\n\r\n# Reproduction instructions\r\n\r\n#preparing  embeddings\r\nfaiss_embedding_dict has embeddings along with id column\r\nembeddings_list=list()\r\nids_list=list()\r\nfor key, value in faiss_embedding_dict.items():\r\n    embedding = np.array(value.get(\"embedding\")[0], dtype=np.float32)  # Convert to NumPy array\r\n    embeddings_list.append(embedding)\r\n    ids_list.append(key)\r\n# Stack the embeddings along a new axis to form a 2D array\r\nall_embeddings = np.stack(embeddings_list)\r\n# Convert the IDs to a numpy array\r\nids = np.array(ids_list)\r\n\r\ncount=all_embeddings.shape[0]\r\ndimensions=all_embeddings.shape[1] #512\r\n\r\ndef create_ivfflatindex():\r\n  # Define the number of clusters for ivf\r\n  nlist = math.ceil(4 * math.sqrt(count))\r\n\r\n  quantizer = faiss.IndexFlatL2(dimensions)\r\n  print(dimensions,nlist)\r\n  index = faiss.IndexIVFFlat(quantizer,dimensions,nlist,faiss.METRIC_INNER_PRODUCT)\r\n\r\n  # Add the data to the index\r\n  index.train(all_embeddings)\r\n  index = faiss.IndexIDMap(index)\r\n  index.add_with_ids(all_embeddings, ids)\r\n  return index\r\n\r\n#create the index and save them\r\nindex_ivfflat = create_ivfflatindex()\r\nfaiss.write_index(index_ivfflat, index_path)\r\n\r\n#create the query embedding\r\nquery_embedding = model.get_text_features(**processor(\"black\",padding=True, return_tensors=\"pt\").to(device))\r\nquery_embedding_array = query_embedding.detach().cpu().numpy()\r\n\r\n#read the index and search\r\ndbfs_ivfflat_index = faiss.read_index(index_path)\r\n distances,ids = dbfs_ivfflat_index.search(query_embedding_array, num_of_results) \r\n\r\n**Error log:**\r\nTypeError                                 Traceback (most recent call last)\r\n<command-1787498931992782> in <cell line: 2>()\r\n      1 query=pd.DataFrame({\"query\":[\"fre@ky fucer\"],\"num_results\":150,\"ui_flag\":1})\r\n----> 2 predicted_image_list=loaded_model.predict(query)\r\n      3 print(predicted_image_list)\r\n\r\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py in predict(self, data)\r\n    411         if input_schema is not None:\r\n    412             data = _enforce_schema(data, input_schema)\r\n--> 413         return self._predict_fn(data)\r\n    414 \r\n    415     @experimental\r\n\r\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/mlflow/pyfunc/model.py in predict(self, model_input)\r\n    303 \r\n    304     def predict(self, model_input):\r\n--> 305         return self.python_model.predict(self.context, model_input)\r\n\r\n<command-1787498931992778> in predict(self, context, model_input)\r\n     52         # Perform your inference using the loaded models and other resources\r\n     53         # You can access the loaded models and resources using self.img_model, self.text_model, self.annoy_index, etc.\r\n---> 54         return self.index_search(model_input)\r\n     55 \r\n     56     def index_search(self, model_input):\r\n\r\n<command-1787498931992778> in index_search(self, model_input)\r\n    121 \r\n    122       #We use faiss ivf flat index to find the num_results\r\n--> 123       distances,index_ids = self.dbfs_ivfflat_index.search(query_embedding_array, num_results)\r\n    124       distances=distances[0]\r\n    125       index_ids=index_ids[0]\r\n\r\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/faiss/__init__.py in replacement_search(self, x, k, D, I)\r\n    320             assert I.shape == (n, k)\r\n    321 \r\n--> 322         self.search_c(n, swig_ptr(x), k, swig_ptr(D), swig_ptr(I))\r\n    323         return D, I\r\n    324 \r\n\r\n/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/faiss/swigfaiss.py in search(self, n, x, k, distances, labels)\r\n   8402 \r\n   8403     def search(self, n, x, k, distances, labels):\r\n-> 8404         return _swigfaiss.IndexIDMap_search(self, n, x, k, distances, labels)\r\n   8405 \r\n   8406     def train(self, n, x):\r\n\r\nTypeError: in method 'IndexIDMap_search', argument 4 of type 'faiss::IndexIDMapTemplate< faiss::Index >::idx_t'\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3069/comments",
    "author": "priyankaiiit14",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-09-27T14:17:42Z",
        "body": "make sure num_of_results is an int "
      }
    ]
  },
  {
    "number": 3010,
    "title": "How does the similarity search calculate the distance or score with multiple query vectors?",
    "created_at": "2023-08-17T09:22:24Z",
    "closed_at": "2024-06-30T23:03:48Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/3010",
    "body": "When we select the similarity search function, such as IndexFlatL2, IndexHNSWFlat. How does it calculate the distance or score with multiple query vectors? \r\n\r\nIf I have two query vectors, using cosine similarity as similarity search.\r\n\r\nquery vector 1:\r\nrank1: database A image\r\nrank2: database C image\r\nrank3: database B image\r\n\r\nquery vector 2:\r\nrank1: database A image\r\nrank2: database B image\r\nrank3: database D image\r\n\r\nHow does it combine the two query vector and get the final rank? ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/3010/comments",
    "author": "LionLion888",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-08-18T12:31:56Z",
        "body": "The results are ranked independently. If you want to combine several query vectors, you have to do it yourself. "
      }
    ]
  },
  {
    "number": 2906,
    "title": "INDEX NOT LOADING FROM DISK",
    "created_at": "2023-06-08T19:14:05Z",
    "closed_at": "2024-06-30T21:55:44Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2906",
    "body": "# Summary\r\n\r\nI created a faiss index and train it as follows:\r\nfaiss::IndexHNSWFlat* generate_index(int m = 128, int ef_search = 250, int ef_construction = 64){\r\n\tfaiss::IndexHNSWFlat* index = new faiss::IndexHNSWFlat(512, m, faiss::METRIC_INNER_PRODUCT);\r\n\tindex->hnsw.efSearch = ef_search;\r\n\tindex->hnsw.efConstruction = ef_construction;\r\n\r\n\treturn index;\r\n}\r\nfaiss::write_index(index, faiss_index_path.c_str());\r\nstd::cout<<\"SAVED INDEX TO \"<<faiss_index_path<<std::endl;\r\n\r\nNow when I try to load the index and use it as \r\nfaiss::IndexHNSWFlat* index;\r\nfaiss::Index *loaded = faiss::read_index(faiss_index_path.c_str());\r\nindex = static_cast<faiss::IndexHNSWFlat*>(loaded);\r\nstd::vector<float> embed_vec(embed.begin<float>(), embed.end<float>());\r\nstd::vector<long int> labels(5);\r\nstd::vector<float> distances(5);\r\nindex->search(1, embed_vec.data(), 5, distances.data(), labels.data());\r\nwhere embed_vec is a 512 dimensional embedding, i get the following output\r\npure virtual method called\r\nterminate called without an active exception\r\nAborted (core dumped)\r\n\r\nInstalled from: built from source.\r\n\r\nFaiss compilation options:using mkl\r\n\r\nRunning on:\r\n- CPU\r\nInterface: \r\n- C++\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2906/comments",
    "author": "Sha-x2-nk",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-06-12T07:03:55Z",
        "body": "please don't use `static_cast`, always use `dynamic_cast`."
      },
      {
        "user": "Sha-x2-nk",
        "created_at": "2023-06-12T07:09:04Z",
        "body": "hey there. I also tried with dynamic_cast and it is still not working. \r\nright now I am using index->search only without casting it to HNSWFlat index\r\n"
      }
    ]
  },
  {
    "number": 2894,
    "title": "TypeError: in method 'IndexFlat_range_search', argument 4 of type 'float'",
    "created_at": "2023-06-05T18:34:02Z",
    "closed_at": "2023-06-06T17:24:48Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2894",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\nI have been using the `range_search` functionality with great success within the Python interpreter. However, when I attempt to call it through a bash interface, I get prompted the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/path_to_script/test_faiss_cmd.py\", line 24, in <module>\r\n    lim, D, I = idx.range_search(X, thresh=r)\r\n  File \"/home/sebastiaan/miniconda3/envs/knn_tcr/lib/python3.9/site-packages/faiss/__init__.py\", line 492, in replacement_range_search\r\n    self.range_search_c(n, swig_ptr(x), thresh, res)\r\n  File \"/home/sebastiaan/miniconda3/envs/knn_tcr/lib/python3.9/site-packages/faiss/swigfaiss_avx2.py\", line 1631, in range_search\r\n    return _swigfaiss_avx2.IndexFlat_range_search(self, n, x, radius, result)\r\nTypeError: in method 'IndexFlat_range_search', argument 4 of type 'float'\r\n```\r\nRunning the exact same code in a Python interpreter does not produce the error, it only occurs from a command line interface.\r\n\r\n# Platform\r\n\r\nOS: Ubuntu 20.04.5 LTS\r\n\r\nFaiss version: faiss 1.7.2 py39h44b29b8_3_cpu conda-forge\r\n\r\nInstalled from: anaconda \r\n\r\nFaiss compilation options: /\r\n\r\nRunning on:\r\n- [X] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [X] Python\r\n\r\n# Reproduction instructions\r\n\r\n```python\r\nimport faiss\r\n\r\n# Generate random input array of shape (n, d)\r\nn = 500\r\nd = 272python3 test_faiss_cmd.py --n_vecs 100 --n_dims 272 --radius 50\r\nvecs = np.random.rand(n,d).astype(\"float32\")\r\n\r\n# Build Flat Index\r\nidx = faiss.IndexFlatL2(272)\r\nidx.train(vecs)\r\nidx.add(vecs)\r\n\r\n# Search Flat Index\r\nr = 24\r\nX = np.random.rand(1,d).astype(\"float32\")\r\nlim, D, I = idx.range_search(X, thresh=r)\r\n```\r\n\r\nThis example runs perfectly in a Python interpreter. However, in the following situation, this script fails and prompts the error that was mentioned previously.\r\n\r\n`argparse` script (test_faiss_cmd.py):\r\n\r\n```python\r\nimport faiss\r\nimport numpy as np\r\nimport argparse\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--n_vecs', type=int)\r\nparser.add_argument('--n_dims', type=int)\r\nparser.add_argument('--radius')\r\nargs = parser.parse_args()\r\n\r\n# Generate random input array of shape (n, d)\r\nn = args.n_vecs\r\nd = args.n_dims\r\nvecs = np.random.rand(n,d).astype(\"float32\")\r\n\r\n# Build Flat Index\r\nidx = faiss.IndexFlatL2(args.n_dims)\r\nidx.train(vecs)\r\nidx.add(vecs)\r\n\r\n# Search Flat Index\r\nr = args.radius\r\nX = np.random.rand(1,d).astype(\"float32\")\r\nlim, D, I = idx.range_search(X, thresh=r)\r\n```\r\nCommand line:\r\n`python3 test_faiss_cmd.py --n_vecs 100 --n_dims 272 --radius 50`\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2894/comments",
    "author": "svalkiers",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-06-06T09:12:15Z",
        "body": "radius is a string......"
      },
      {
        "user": "svalkiers",
        "created_at": "2023-06-06T17:24:48Z",
        "body": "Wow, I can't believe I did not realize this. Issue solved."
      }
    ]
  },
  {
    "number": 2869,
    "title": "Can I obtain the completion time of index construction when I create an index or add data to the index, such as HNSW index",
    "created_at": "2023-05-22T08:20:01Z",
    "closed_at": "2023-05-24T15:48:12Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2869",
    "body": "Running on:\r\n- [âœ“] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [âœ“] Python\r\n\r\n**problem:**\r\nWhen I create an index or add data to an index, I want to obtain the completion time of index construction, especially for HNSW. Is there a way to obtain it, or do you have any suggestions\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2869/comments",
    "author": "ly7354820",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-05-23T14:19:47Z",
        "body": "I don't understand the question. Just see the difference between `time.time()` before and after the call no? "
      },
      {
        "user": "ly7354820",
        "created_at": "2023-05-23T15:07:16Z",
        "body": "> I don't understand the question. Just see the difference between `time.time()` before and after the call no?\r\n\r\nSorry, I didn't describe it clearly enough.\r\nIn my database, I want to add a new field called index_time.This time is the timestamp of the operations that occur when I create an index or add data to the index,can I get these data?"
      },
      {
        "user": "mlomeli1",
        "created_at": "2023-05-24T15:48:12Z",
        "body": "I echo Matthijs's suggestion to use the `time` library to obtain it before and after calling `index.add()` and adding it to your database."
      }
    ]
  },
  {
    "number": 2754,
    "title": "Add my own index class using python",
    "created_at": "2023-03-10T08:48:18Z",
    "closed_at": "2023-07-14T09:55:28Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2754",
    "body": "Hi, I want to add a new index class to faiss by reusing some faiss interfaces or  inheritting from faiss class in Python. So Does have any interface or demo to help me.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2754/comments",
    "author": "bit-pku-zdf",
    "comments": [
      {
        "user": "bit-pku-zdf",
        "created_at": "2023-03-10T08:49:35Z",
        "body": "Maybe like :\r\nClass MyIndex(HNSWPQ):\r\n     def __init__(self):\r\n          xxx\r\n     def train(self):\r\n        xxx\r\n      def add(self, x):\r\n       xxx\r\n\r\n\r\n"
      },
      {
        "user": "bit-pku-zdf",
        "created_at": "2023-03-10T08:50:22Z",
        "body": "Or I can only realize in C++ï¼Œ and to modify faiss code and recompile to python.\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2023-03-10T15:23:13Z",
        "body": "I would advise to not trying to do that because the wrapping is very complex. \r\nYou are probably better off having the Faiss index as a field of your `MyIndex` object. "
      }
    ]
  },
  {
    "number": 2700,
    "title": "What's the accurate rate of faiss?",
    "created_at": "2023-02-08T03:18:51Z",
    "closed_at": "2024-06-27T15:43:49Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2700",
    "body": "I had a try with IndexIVFFlat, but accurate rate is so low(only 20%), why? my code is as follows.\r\n\r\n#########accurate rate testing############\r\n\r\nimport faiss\r\nimport numpy as np\r\nimport pandas as pd\r\ndimension = 512                           # dimension\r\nnb = 100000                      # database size\r\nncluster = 316\r\nnprobe = 20\r\ntopk = 10\r\nnp.random.seed(123)             # make reproducible\r\nxb = np.random.random((nb, dimension)).astype('float32')\r\n#xb[:, 10] += np.arange(nb) / 1000.\r\nfaiss.normalize_L2(xb)\r\nnq = 1000\r\nxq = np.random.random((nq, dimension)).astype('float32')\r\n#xq[:, 0] += np.arange(nq) / 1000.\r\n\r\n\r\nquantizer = faiss.IndexFlatIP(dimension)  # for cluster\r\nindex = faiss.IndexIVFFlat(quantizer, dimension, ncluster, faiss.METRIC_INNER_PRODUCT)\r\nassert not index.is_trained\r\nindex.train(xb)\r\nassert index.is_trained\r\nindex.nprobe = nprobe\r\nindex.add(xb)\r\nD,I=index.search(xq, topk)\r\n\r\n\r\nindex1 = faiss.IndexFlatL2(dimension)\r\nindex1.add(xb)\r\nstart = time.time()\r\nD_stardard,I_stardard=index1.search(xq, topk)\r\n\r\n\r\nsum=0.0\r\nfor i in range(I.shape[0]):\r\n    sum = sum + 1.0*len(set(I[i]).intersection(set(I_stardard[i])))/topk\r\nacc_rate = sum/I.shape[0]\r\n\r\n#########accurate rate testing############",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2700/comments",
    "author": "Fortrus",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-02-13T03:13:23Z",
        "body": "Indexing uniform random data is inefficient with any faiss index because they exploit clusters in the vector distribution. "
      }
    ]
  },
  {
    "number": 2610,
    "title": "need IndexFlatIP support float16",
    "created_at": "2022-12-07T04:53:11Z",
    "closed_at": "2024-06-24T15:07:16Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2610",
    "body": "# Summary\r\n\r\nneed `IndexFlatIP` support `float16`\r\n\r\nwhen the number of vector is very very large, such as `1e10`. Use `IndexFlatIP` of `float32` is too expensive, maybe `float16` is much fastter",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2610/comments",
    "author": "yangsp5",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-12-07T09:06:53Z",
        "body": "You can use `IndexFlatScalarQuantizer` with `QT_fp16`.\r\n"
      },
      {
        "user": "yangsp5",
        "created_at": "2022-12-13T01:04:48Z",
        "body": "How to use `IndexFlatScalarQuantizer` with `QT_fp16`,\r\n\r\n`index = faiss.IndexFlatScalarQuantizer(emb_size, faiss.ScalarQuantizer.QT_fp16)` got wrong"
      },
      {
        "user": "yunjiangster",
        "created_at": "2022-12-22T06:54:12Z",
        "body": "@yangsp5 I think @mdouze meant `IndexScalarQuantizer`, but that still takes in float32 index vectors. I found the function `knn_gpu` that computes k nearest neighbors without building an index first, and it seems to support float16 input directly."
      }
    ]
  },
  {
    "number": 2607,
    "title": "What does the IVF index training process actually do?",
    "created_at": "2022-12-05T21:36:15Z",
    "closed_at": "2024-01-06T18:53:33Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2607",
    "body": "I can't seem to find any documentation about what the training process does for the IVF?  It looks like in some examples you need to train with the data ahead of time before adding?  Does that mean I can't initialize an empty index, and dynamically add items?  \r\n\r\nI have a very dynamic database where items will be added and removed sporadically.  I'm not sure which index type is best suited for my needs",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2607/comments",
    "author": "ahndroo",
    "comments": [
      {
        "user": "anubhav562",
        "created_at": "2022-12-12T16:27:48Z",
        "body": "Hi @ahndroo!\r\n\r\nFirst you create an empty index. Then you load the data and train the IVF index, the training process internally initiates training of K means clustering, which results in the formation of k centroids (k = the number of inverted lists that you want in the IVF). Once it is trained you need to add the vectors to the index. \r\n\r\nThink of it in this way: Before adding the vectors, you are creating k buckets. While adding vectors you assign each vector to the closest bucket (based on euclidean distance). So even if you have more data flowing-in in the future, you can simply keep on adding them to the trained index.\r\nNote: If you feel that your data distribution changes, you can re-train the index once in a while.  "
      }
    ]
  },
  {
    "number": 2545,
    "title": "[GPU] Adding new vectors to loaded index doesn't work on GPU",
    "created_at": "2022-10-21T14:38:50Z",
    "closed_at": "2024-08-06T01:52:41Z",
    "labels": [
      "help wanted",
      "GPU",
      "autoclose",
      "stale"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2545",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: CentOS (Fair Cluster) <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\nLatest version obtain by `conda install faiss-gpu` (1.7.2)\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\n```\r\nimport faiss\r\nimport numpy as np\r\n\r\nindex = faiss.read_index(\"index.faiss\")  # non empty faiss IVF index, specifically OPQ64_256,IVF262144,PQ64 in my case\r\n\r\ngpu_options = faiss.GpuMultipleClonerOptions()\r\ngpu_options.useFloat16 =True\r\n\r\nchunk = np.array(np.ones((n, d)), dtype=np.float32, copy=True, order=\"C\", ndmin=1,)\r\nindex_gpu = faiss.index_cpu_to_gpus_list(index, co=gpu_options, gpus=None)\r\nindex_gpu.add(chunk)\r\n```\r\n\r\nResults in \r\n```\r\nFaiss assertion 'indices->numVecs == oldNumVecs' failed in int faiss::gpu::IVFBase::addVectors(faiss::gpu::Tensor<float, 2, true>&, faiss::gpu::Tensor<long int, 1, true>&) at /home/conda/feedstock_root/build_artifacts/faiss-split_1644327811086/work/faiss/gpu/impl/IVFBase.cu:581\r\n```\r\n\r\nWhile, if I load a trained but empty index of the exact same type, the `add` will work.\r\nIf I try to do the same thing but on CPU, it will also work.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2545/comments",
    "author": "MarcSzafraniec",
    "comments": [
      {
        "user": "umbra-scientia",
        "created_at": "2022-10-28T17:14:30Z",
        "body": "We also observe this behavior in faiss-gpu 1.7.2.\r\n(using python 3.8 and cuda 11.3 installed via `conda -c pytorch`)\r\n\r\n```python\r\nindex = faiss.read_index(filename)\r\nindex = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), gpu_id, index)\r\nindex.add(...)\r\n```\r\nThis code works correctly if `filename` points to a Flat index, but fails with the previously mentioned assert if an IVF index is specified."
      },
      {
        "user": "mdouze",
        "created_at": "2022-10-30T09:03:59Z",
        "body": "please use `add_with_ids`."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-30T01:52:12Z",
        "body": "This issue is stale because it has been open for 7 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-06T01:52:41Z",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale."
      }
    ]
  },
  {
    "number": 2531,
    "title": "Using faiss to compute hashes of embeddings ",
    "created_at": "2022-10-13T23:52:44Z",
    "closed_at": "2024-06-17T19:39:18Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2531",
    "body": "Hello,\r\n\r\nI am trying to figure out if any of the internal structures of faiss indices (say an ivf pq or a lsh) could be used to compute hashes of embeddings with the property that they should be equal if similar enough in cosine distance\r\n\r\nI am aware this is not a normal use of faiss, but I am wondering if this is something that has been tried before.\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2531/comments",
    "author": "rom1504",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-10-16T22:28:23Z",
        "body": "With hashes you mean binary vectors? \r\nThen can use IndexLSH and sa_encode to get the vectors."
      },
      {
        "user": "rom1504",
        "created_at": "2022-10-17T11:14:52Z",
        "body": "I'm trying to find a function f such that for an embedding e of size let's say 1024 floats i have for most e1, e2 in the space of my embeddings :\r\nh1 = f(e1)\r\nh2 = f(e2)\r\nSuch that if sim(e1, e2) > threshold then h1 = h2 (and if sim(e1,e2) < threshold then h1 != h2)\r\nWith h preferably encoded as a small amount of bytes\r\n\r\nOne use case of such a function f would be to perform efficient deduplication of items represented by embeddings.\r\n\r\nI think it would be possible to directly train a neural net to be f. But I'm wondering if using the quantization techniques implemented in faiss could be also a good technique.\r\n\r\nMaybe the encodings produced by IndexLSH could work. Maybe ones produced by PQ index could be helpful too.\r\n\r\nIf you're aware of any previous use of faiss for this kind of use case / or have any idea if that makes sense to try, I'd be interested."
      },
      {
        "user": "mdouze",
        "created_at": "2022-10-17T14:57:51Z",
        "body": "You can't have the strict similarity conditions above, otherwise perfect hashing would be solved ;-) \r\nBut if you have a sufficiently small PQ or LSH it may work sufficiently well. "
      }
    ]
  },
  {
    "number": 2469,
    "title": "Cosine similarity is too small",
    "created_at": "2022-09-14T10:30:39Z",
    "closed_at": "2022-09-15T12:20:51Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2469",
    "body": "# Summary\r\nHi! I want to get cosine similarity for vectors. I expect, that found vectors dist will be close to 1 (smth like 0.99), but I get 0.1.\r\nHere is the code and output. Ids are right, but dist is small.\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Windows 11\r\n\r\nFaiss version: 1.7.2\r\n\r\nInstalled from: pip\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [v] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [V] Python\r\n\r\n# Reproduction instructions\r\n\r\nimport numpy as np\r\nimport faiss\r\nfrom faiss import normalize_L2\r\ndim = 512  # dimension\r\nnb = 65536  # size of dataset\r\nnp.random.seed(228)\r\nvectors = np.random.random((nb, dim)).astype('float32')\r\nquery = vectors[:5]\r\nids = np.array(range(0, nb)).astype(np.int64)\r\nM = 64\r\nD = M * 4\r\nclusters = 4096  # ~16*math.sqrt(nb)\r\nvector_size = D * 4 + M * 2 * 4\r\ntotal_size_gb = round(vector_size*nb/(1024**3), 2)\r\nfactory = f\"IDMap,OPQ{M}_{D},IVF{clusters}_HNSW32,PQ{M}\"\r\nprint(f\"factory: {factory}, {vector_size} bytes per vector, {total_size_gb} gb total\")\r\nfaiss.omp_set_num_threads(10)\r\nindex = faiss.index_factory(dim, factory, faiss.METRIC_INNER_PRODUCT)\r\nnormalize_L2(vectors)\r\nindex.train(vectors)\r\nprint(f'Index trained')\r\nindex.add_with_ids(vectors, ids)\r\nprint(f'{index.ntotal} vectors have been added to index')\r\nk = 1\r\nnprobe = 1\r\nnormalize_L2(query)\r\nindex.nprobe = nprobe\r\ndist, idx = index.search(query, k)\r\nprint(idx)\r\nprint(dist)\r\n\r\n\r\nOUTPUT:\r\nfactory: IDMap,OPQ64_256,IVF4096_HNSW32,PQ64, 1536 bytes per vector, 0.09 gb total\r\nIndex trained\r\n65536 vectors have been added to index\r\n[[0]\r\n [1]\r\n [2]\r\n [3]\r\n [4]]\r\n[[0.11132257]\r\n [0.13959643]\r\n [0.13129388]\r\n [0.12439864]\r\n [0.1243098 ]]\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2469/comments",
    "author": "jump155",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-09-15T11:36:25Z",
        "body": "This is normal as the distances are approximate. If you increase the M or use SQ compression, the accuracy will improve."
      },
      {
        "user": "jump155",
        "created_at": "2022-09-15T12:20:47Z",
        "body": "Thank you"
      }
    ]
  },
  {
    "number": 2383,
    "title": "too many values to unpack (expected 2)",
    "created_at": "2022-07-15T01:07:49Z",
    "closed_at": "2022-09-28T13:51:07Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2383",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\nubuntu 22.04\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n1.7.2\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\npip3 install faiss-gpu\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [O ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ O] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n-```\r\n--------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-04eca7795502> in <module>\r\n      1 model = Vit()\r\n      2 mobile_model = Mobile()\r\n----> 3 D,I,vectorImages,vetorAnaswers = model.analysis(query_image_paths,answer_image_paths)\r\n      4 mobile_model.analysis(query_image_paths,answer_image_paths)\r\n      5 \r\n\r\n<ipython-input-4-0c0c122af3ce> in analysis(self, query_image_paths, answer_image_paths)\r\n     47 \r\n     48         index = faiss.IndexFlatL2(query_vectors.shape[-1])  # build the index\r\n---> 49         index.add(np.stack(answer_vectors))\r\n     50         distance, indices = index.search(query_vectors,  len(answer_image_paths))\r\n     51         #print(distance)\r\n\r\n/usr/local/lib/python3.6/dist-packages/faiss/__init__.py in replacement_add(self, x)\r\n    211         \"\"\"\r\n    212 \r\n--> 213         n, d = x.shape\r\n    214         assert d == self.d\r\n    215         self.add_c(n, swig_ptr(x))\r\n```\r\n\r\nValueError: too many values to unpack (expected 2)\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2383/comments",
    "author": "SlowMonk",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-07-20T09:49:33Z",
        "body": "please make sure you pass in a 2D matrix"
      }
    ]
  },
  {
    "number": 2381,
    "title": "IVF1024 PQ8 train a indexï¼Œplease help me sove this problem!",
    "created_at": "2022-07-12T02:14:29Z",
    "closed_at": "2022-09-28T13:51:24Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2381",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->Ubuntu20.04\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->1.7.2\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> Anaconda\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\n\r\nabout six milion vector, After training, index is 1.2m in size\r\nAfter, add another vector to index, eight milion vector, index is 146.3m in size\r\nIs this normalï¼Ÿ\r\nThere is no PCA and other operations on the vectorï¼ŒIs my action correct?\r\nMy date is very very largeï¼Œneed add more vector to index\r\nIs there any good wayï¼Ÿ Thanksï¼ï¼",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2381/comments",
    "author": "black-zyp",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-07-20T09:48:49Z",
        "body": "what is the vector dimension?"
      },
      {
        "user": "black-zyp",
        "created_at": "2022-07-25T02:04:58Z",
        "body": "> what is the vector dimension?\r\n\r\nThanksï¼I understandï¼"
      }
    ]
  },
  {
    "number": 2377,
    "title": "Getting Cosine similarity different for \"Flat\" & \"HNSW32Flat\" Indexes",
    "created_at": "2022-07-07T05:45:32Z",
    "closed_at": "2024-07-24T18:25:52Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2377",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: linux <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\nHello,\r\n\r\nI am trying to find the cosine similarity with HNSW.\r\nBut the cosine similarity found to be incorrect below is the code and comparison of \"Flat\", \"HNSW\" & \"scipy\"\r\n```\r\nimport faiss\r\nemb1 = np.fromfile(\"emb1.raw\", dtype=np.float32)\r\nemb2 = np.fromfile(\"emb2.raw\", dtype=np.float32)\r\n```\r\nScipy code & result\r\n\r\n```\r\nfrom scipy import spatial\r\nresult = 1 - spatial.distance.cosine(emb1, emb2)\r\nprint('Cosine Similarity by scipy:{}'.format(result))\r\n```\r\nResult:\r\n`Cosine Similarity by scipy::0.991761326789856`\r\n\r\nIndexFlatL2/Flat code & result\r\n```\r\nxb = np.expand_dims(emb1,axis=0)\r\nxq = np.expand_dims(emb2,axis=0)\r\n\r\nindex = faiss.index_factory(128, \"Flat\", faiss.METRIC_INNER_PRODUCT)\r\nindex.ntotal\r\nfaiss.normalize_L2(xb)\r\nindex.add(xb)\r\nfaiss.normalize_L2(xq)\r\ndistance, index = index.search(xq, 1)\r\nprint('[FAISS] Cosine Similarity by Flat:{}'.format(distance))\r\n```\r\nResult:\r\n`[FAISS] Cosine Similarity by Flat:[[0.9917611]]`\r\n\r\nIndexHNSWFlat/HNSW32Flat code & result\r\n\r\n```\r\nxb = np.expand_dims(emb1,axis=0)\r\nxq = np.expand_dims(emb2,axis=0)\r\n\r\nindex = faiss.index_factory(128, \"HNSW32Flat\", faiss.METRIC_INNER_PRODUCT)\r\nindex.ntotal\r\nfaiss.normalize_L2(xb)\r\nindex.add(xb)\r\nfaiss.normalize_L2(xq)\r\ndistance, index = index.search(xq, 1)\r\nprint('[FAISS] Cosine Similarity by HNSW32Flat:{}'.format(distance))\r\n```\r\nResult:\r\n`[FAISS] Cosine Similarity by HNSW32Flat:[[0.01647742]]`\r\n\r\n**The results of Scipy & Flat are matching.\r\nWhereas the result is incorrect for HNSW.\r\nVerified the results using C++ & Python API's**",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2377/comments",
    "author": "Kapil-23",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-07-08T08:39:07Z",
        "body": "This is with an old version of Faiss, HNSW32Flat is not a valid index_factory string, it should be HNSW32,Flat. \r\nIn addition, the faiss.METRIC_INNER_PRODUCT is not taken into account, so it computes L2 distances. \r\nThis is fine, it just requires to do the translation to cosine similarity: \r\n\r\n2 - 2 * 0.9917611 = 0.0164778"
      },
      {
        "user": "Kapil-23",
        "created_at": "2022-07-08T10:33:53Z",
        "body": "@mdouze Thanks for your reply !!!\r\n\r\nYes the faiss python version that was installed was (1.5.3) after upgrading to 1.7.2 the issue resolved. \r\nUpdated the api \r\n`faiss.index_factory(128, \"HNSW32,Flat\", faiss.METRIC_INNER_PRODUCT)`\r\nCorrect Result : `0.9917613`\r\n\r\n**Note : Results are direct from API (Not used: 2 - 2 * 0.9917611 = 0.0164778)**\r\n\r\nWith respect to C++ I am facing the same issue of incorrect results (i.e getting Euclidean distance) instead of cosine similarity.\r\nI am using the following code.\r\nFaiss compiled from repo : latest version\r\n```\r\nfaiss::IndexHNSWFlat index(128,64);\r\nindex.metric_type = faiss::METRIC_INNER_PRODUCT;\r\n\r\nnormalize(xb)\r\nindex.add(xb)\r\nnormalize(xq)\r\n\r\nindex.search(...)\r\n```\r\nResult: `-0.0164774` \r\n"
      }
    ]
  },
  {
    "number": 2365,
    "title": "Search Knn With One Piece Of Data  Optimization",
    "created_at": "2022-06-23T11:00:54Z",
    "closed_at": "2022-06-28T01:16:34Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2365",
    "body": "# Summary\r\n\r\nI want to speed up when building knn with one piece of data .\r\nIn theory, the optimal implementation requires only half the amount of computation of the existing implementation.\r\nSo I want to ask if there is any other way to speed up the construction of knn.\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:è¿™æ˜¯æˆ‘ç›®å‰çš„å®žçŽ°ï¼Œæƒ³è¯·é—®ä¸€ä¸‹æœ‰æ²¡æœ‰æ›´å¥½çš„æ–¹æ³•\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\nThis is my current implementation, would like to ask if there is a better way\r\n\r\nres = faiss.StandardGpuResources()\r\nindex = faiss.GpuIndexFlatIp(res,dim)\r\nindex.add(feat)\r\nsims,nbrs = index.search(feat,k=k)\r\n\r\n\r\nI try to get close to optimal speed by splitting to reduce the number of alignments,just like\r\n\r\nres = faiss.StandardGpuResources()\r\nfeat = np.split(feat,2)\r\na = feat[0]\r\nb = feat[1]\r\nindex1 = faiss.GpuIndexFlatIp(res,dim)\r\nindex2 = faiss.GpuIndexFlatIp(res,dim)\r\nindex1.add(a)\r\nindex2.add(b)\r\nsims1,nbrs1 = index.search(a,k=k)\r\nsims2,nbrs2 = index.search(b,k=k)\r\nsims3,nbrs3 = index.search(b,k=k)\r\n\r\nThe number of alignments is reduced by a*b but there is a problem in organizing the results\r\n\r\nIn theory, a*b only needs to be compared once, which can reduce the comparison of a*b. However, because the returned topk has only one b to a\r\n\r\nSo I want to ask if there is another way to write it?\r\n\r\nThanks\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2365/comments",
    "author": "suwen-ux",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-27T23:56:02Z",
        "body": "you probably want to split the search over sub-datasets. For this you can just do\r\n\r\nindex = faiss.index_cpu_to_all_gpus(faiss.IndexFlatIP(dim))\r\n\r\nwhich will use all GPUs by default"
      },
      {
        "user": "suwen-ux",
        "created_at": "2022-06-28T01:16:30Z",
        "body": "Thanks"
      }
    ]
  },
  {
    "number": 2359,
    "title": "Correct pronunciation",
    "created_at": "2022-06-17T10:46:51Z",
    "closed_at": "2022-06-17T12:08:47Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2359",
    "body": "Sorry for such a stupid question. I was not able to find this information anywhere.\r\n\r\nWhat is the right  way to pronounce `faiss`?\r\n\r\n1. Like the word `facebook`\r\n2. Like the word `feisty`\r\n\r\n\r\nThanks in advance",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2359/comments",
    "author": "jankrepl",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-17T11:58:48Z",
        "body": "there is no agreed upon pronunciation ;-) "
      },
      {
        "user": "jankrepl",
        "created_at": "2022-06-17T12:08:47Z",
        "body": "> there is no agreed upon pronunciation ;-)\r\n\r\nOK then! Thank you!"
      }
    ]
  },
  {
    "number": 2349,
    "title": "Discrepancy in the results of IndexFlat and IndexFlatIP",
    "created_at": "2022-06-09T18:33:08Z",
    "closed_at": "2024-06-17T14:48:24Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2349",
    "body": "# Summary\r\nI have created an flat index wrapped with IndexIDMap so that I can add custom Ids to it. The code which I have used to create is attached below\r\n\r\n```\r\ncc_index = faiss.IndexIDMap(faiss.IndexFlatIP(model.get_dimension()))\r\n#conversion required for the integer type\r\nids = np.array(transaction_ids)\r\nids = np.asarray(ids.astype('int64'))\r\nemb = np.array(address_embedding)\r\nemb = np.asarray(emb.astype('float32'))\r\ncc_index.add_with_ids(emb,ids)\r\n```\r\n\r\nRunning on:\r\n- [x] CPU\r\n\r\nInterface: \r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\nI see that IndexFlat has two subclasses IndexFlatIp and IndexFlatL2. However when the index is created using IndexFlatIP, I am seeing the rank order of results is wrong. But if I choose IndexFlat instead of the IndexFlatIP I see the results ranked correctly in the top_k. I calculated the cosine similarity using python code and the same ranking order I am able to find in IndexFlat.\r\nWhat is causing the discrepancy in the results rank order?\r\ncc_index = faiss.IndexIDMap(faiss.IndexFlatIP(model.get_dimension()))\r\nvs\r\ncc_index = faiss.IndexIDMap(faiss.IndexFlat(model.get_dimension()))\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2349/comments",
    "author": "kai5gabriel",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-13T07:54:20Z",
        "body": "For `faiss.IndexFlat` the default distance measure is L2. To get IP just do\r\n\r\n`faiss.IndexFlat(model.get_dimension(), faiss.METRIC_INNER_PRODUCT)`\r\n"
      }
    ]
  },
  {
    "number": 2309,
    "title": "Similarity Search within the dataset",
    "created_at": "2022-04-26T16:28:15Z",
    "closed_at": "2024-07-22T21:56:26Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2309",
    "body": "Hello,\r\nI am new to the field of similarity search and have a quick question.\r\nIs there a way/suggestion to obtain buckets containing similar vectors (Let's say with indexLSH) based on some distance criteria within the dataset instead of passing a query vector and doing an exhaustive search?\r\n\r\nThanks! \r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2309/comments",
    "author": "Vikasdubey0551",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-04-27T08:35:56Z",
        "body": "Do you want to cluster the dataset?"
      },
      {
        "user": "Vikasdubey0551",
        "created_at": "2022-04-27T09:55:12Z",
        "body": "@mdouze  Thanks for the reply. Yeah. It definitely sounds like I need some clustering techniques like DBSCAN. However, they are slow and memory intensive so far in my attempts. The speed of faiss and GPU support is really attractive. I am not necessarily looking for extremely accurate results, Just more in line or perhaps a little better than Regular LSH. Any suggestions in that regard?"
      },
      {
        "user": "qmpzzpmq",
        "created_at": "2023-10-10T09:45:10Z",
        "body": "Hi, @Vikasdubey0551 did you find anymethod to do with DBSCAN?"
      }
    ]
  },
  {
    "number": 2278,
    "title": "range_search",
    "created_at": "2022-03-26T12:59:36Z",
    "closed_at": "2024-06-17T12:13:41Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2278",
    "body": "Hi, I need to get all the tie lists, so I get the k=1 neighbor, add a small value to the distance of this neighbor (0.000001) and then do a range_search to get all vectors that are in the same distance. \r\n\r\nFor range_search how does IndexIVFFlat perform? it is useful ? or does not matter if I am using IndexFlatL2 or IndexIVFFlat? \r\n\r\nAny suggestion on how do I scale this better? \r\n\r\nthanks\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2278/comments",
    "author": "anapaulaappel",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-03-31T10:53:40Z",
        "body": "your application context should dictate if you need IP or L2 search."
      }
    ]
  },
  {
    "number": 2183,
    "title": "Clarification regarding nbits parameters in PQ indices",
    "created_at": "2022-01-09T11:03:48Z",
    "closed_at": "2022-04-20T16:02:30Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2183",
    "body": "Hi,\r\n\r\nI need some clarification regarding `nbits` parameter in PQ indices, such as `IndexIVFPQ`.\r\n\r\n`IndexIVFPQ` is initialized as follows in Python:\r\n\r\n```python\r\nd = 1024\t\t# 1024-sized vectors\r\nnlist = 100\t\t# 100 coarse divisions of data\r\nM = 8\t\t\t# number of subquantizers (what does this exactly mean? subvectors?)\r\nnbits = 8\t\t# number of bits needed to represent codes learnt for each subvector group?\r\n\r\ncoarse_quantizer = faiss.IndexFlatL2(d)\r\nindex = faiss.IndexIVFPQ(coarse_quantizer, d, nlist, M, nbits)\r\n                                    # 8 specifies that each sub-vector is encoded as 8 bits\r\n```\r\n\r\nAs far as I understand, first, the index uses a coarse quantizer to divide data into `nlist` (100) regions (or Voronoi cells).\r\n\r\nThen, for each region, the `1024` dimensional vector is subdivided into 8 subvectors of size `1014/M= 128` floats.\r\n\r\nEach subvector group (`M` such groups in total) are kNN-ed into `2^nbits = 256` centroids.\r\n\r\nIs this understanding correct?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2183/comments",
    "author": "abhinavkulkarni",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-01-11T09:08:21Z",
        "body": "yes"
      },
      {
        "user": "claeyzre",
        "created_at": "2022-02-17T15:31:52Z",
        "body": "Following the question you said that\r\n> Each subvector group (M such groups in total) are kNN-ed into 2^nbits = 256 centroids.\r\n\r\nSo if I understand correctly there is a total of nlist PQ trainings, one for each region ? \r\nSo far my understanding was that eventhough nlist is equal to X, there is only one PQ training done on the entire training set."
      }
    ]
  },
  {
    "number": 2122,
    "title": "Same dimension but different search time",
    "created_at": "2021-11-25T15:49:47Z",
    "closed_at": "2022-01-19T12:47:14Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2122",
    "body": "Dear authors,\r\n\r\nThanks for your library for fast searching :)\r\n\r\nWhen I used **faiss** to select 1 nearest sample from 2000 vectors (256 d), I found different data (different query, different references) led to much different runtime (even 10 times difference). Is this normal? What caused this issues?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2122/comments",
    "author": "BIT-MJY",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-11-29T07:54:15Z",
        "body": "If you do a single experiment at such a small scale it is probably just timing jitter.\r\n"
      }
    ]
  },
  {
    "number": 2110,
    "title": "what's the relationship between IndexHNSW and impl/HNSW?",
    "created_at": "2021-11-16T06:45:45Z",
    "closed_at": "2022-01-19T12:51:36Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2110",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\nI want to use indexHNSW but i can not find where to set efconstruction.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2110/comments",
    "author": "fgheng",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-11-17T10:19:51Z",
        "body": "it is in `index.hnsw.efConstruction`\r\n\r\n"
      }
    ]
  },
  {
    "number": 2109,
    "title": "How can I update FAISS index that on disk ?",
    "created_at": "2021-11-14T22:37:18Z",
    "closed_at": "2021-11-17T20:01:25Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2109",
    "body": "# Summary\r\nQ1\r\nI am trying to update my blocks on disk with respect to the index that is currently running on the script. I am capable to add new blocks but I have doubts on does FAISS saves all of the index to the new block or does it save only newly added data to the new block?  \r\nQ2\r\nIs there any way to save FAISS index (loaded from the disk /from blocks) after adding new data without creating new blocks?  \r\n\r\n\r\nFaiss version: <faiss-gpu=1.7.1>\r\n\r\nInstalled from: <pip> \r\n\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2109/comments",
    "author": "abdullahbas",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-11-15T08:43:25Z",
        "body": "What are blocks? Are you referring to an OnDisk index built from several indexes?"
      },
      {
        "user": "abdullahbas",
        "created_at": "2021-11-15T09:21:36Z",
        "body": "Yes. We have data that does not fit in RAM. Hence, we created several indexes and then save them on the disk. Lastly merged them on one index. What should we do if we want to update our index on disk with newly added data? Should I save it as new? "
      },
      {
        "user": "mdouze",
        "created_at": "2021-11-17T10:42:13Z",
        "body": "It is not practical to add vectors to an OnDisk index. I would suggest that you keep an in-RAM index for the additional vectors and merge the results from the static OnDisk index and the in-RAM index."
      },
      {
        "user": "abdullahbas",
        "created_at": "2021-11-17T20:01:25Z",
        "body": "Ok, I will update my pipe like that. Thanks for the help and FAISS. "
      },
      {
        "user": "gustavz",
        "created_at": "2024-06-05T12:26:42Z",
        "body": ">  It is not practical to add vectors to an OnDisk index. I would suggest that you keep an in-RAM index for the additional vectors and merge the results from the static OnDisk index and the in-RAM index.\r\n\r\n@mdouze is this still the preferred approach or are there now supported ways to update OnDisk indexes?\r\nI assume it is building new OnDisk indexes and then merging?\r\n"
      }
    ]
  },
  {
    "number": 2087,
    "title": "how to save the trained model for inference?",
    "created_at": "2021-10-20T20:26:27Z",
    "closed_at": "2022-01-19T11:51:20Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2087",
    "body": "# Summary\r\n\r\nI am using the following sample to train a kmean, it works perfectly fine.\r\nMy question is how I can use the train model without retraining it over and over for inference time?\r\nis there a way to save it somewhere and reload it for testing? I checked the tutorial but could not get any answer for my question.\r\n\r\n``` python\r\n\r\ndef run_kmeans(x, nmb_clusters,gpu_ids=None, verbose=True):\r\n    \r\n    \"\"\"Runs kmeans on 1 GPU.\r\n    Args:\r\n        x: data\r\n        nmb_clusters (int): number of clusters\r\n    Returns:\r\n        list: ids of data in each cluster\r\n    \"\"\"\r\n    n_data, d = x.shape\r\n\r\n    print('FAISS: using GPUs {}'.format(gpu_ids))\r\n    print('We have {} samples and dimention is for each is {}'.format(n_data,d))\r\n\r\n    # faiss implementation of k-means\r\n    clus = faiss.Clustering(d, nmb_clusters)\r\n\r\n    # Change faiss seed at each k-means so that the randomly picked\r\n    # initialization centroids do not correspond to the same feature ids\r\n    # from an epoch to another.\r\n    clus.seed = np.random.randint(1234)\r\n\r\n    clus.niter = 20\r\n    # nredo.nredo = 10\r\n    clus.max_points_per_centroid = 10000000\r\n    res = faiss.StandardGpuResources()\r\n    flat_config = faiss.GpuIndexFlatConfig()\r\n    flat_config.useFloat16 = False\r\n    flat_config.device = gpu_ids\r\n    index = faiss.GpuIndexFlatL2(res, d, flat_config)\r\n\r\n    # perform the training\r\n    clus.train(x, index)\r\n\r\n    dists, pred_labels = index.search(x, 1) # # we want to see 1 nearest neighbors\r\n    # pred_labels = pred_labels.squeeze()\r\n\r\n    # nmi = normalized_mutual_info_score(labels, pred_labels)\r\n\r\n    # print(\"NMI: {}\".format(nmi))\r\n    stats = clus.iteration_stats\r\n    losses = np.array([\r\n        stats.at(i).obj for i in range(stats.size())\r\n    ])\r\n    if verbose:\r\n        print('k-means loss evolution: {0}'.format(losses))\r\n\r\n    return [int(n[0]) for n in pred_labels], losses[-1] \r\n\r\n```\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x ] Python\r\n\r\n\r\n \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2087/comments",
    "author": "seyeeet",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-11-10T16:21:22Z",
        "body": "You can save the centroids, stored in `clus.centroids`, then add them back to the GPU index object. \r\n"
      },
      {
        "user": "seyeeet",
        "created_at": "2021-11-10T17:59:18Z",
        "body": "Thank you for your help. can you show me in a small snippet how it would be possible please?"
      },
      {
        "user": "mdouze",
        "created_at": "2022-01-19T11:51:20Z",
        "body": "`np.save(\"x\", clus.centrdoids)`"
      },
      {
        "user": "vinit13792",
        "created_at": "2022-07-29T12:20:27Z",
        "body": "I didn't understand this code, is there a sample snippet where I can check how to run my faiss model on inference mode for deployment? "
      },
      {
        "user": "rohitdwivedula",
        "created_at": "2022-08-16T19:36:05Z",
        "body": "@vinit13792 I assume you want to: (1) save a trained KMeans model to file (by saving the centroids), and (2) load the model later and assign clusters to data points.\r\n\r\nFor step 1, you do something that looks like this: \r\n\r\n```python3\r\nkmeans = faiss.Kmeans(inpuit_dim, num_clusters, verbose=True, gpu=True, niter=300, nredo=10, seed=42)\r\nkmeans.train(data)\r\n\r\n# save centroids to file\r\nwith open(\"centroids.npy\", 'wb') as f:\r\n  np.save(f, kmeans.centroids)\r\n```\r\n\r\nTo load the centroids and use them, do: \r\n```python3\r\ncentroids = np.load(\"centroids.npy\")\r\nkmeans = faiss.Kmeans(inpuit_dim, num_clusters, verbose=True, gpu=True, niter=0, nredo=0, seed=42)\r\nkmeans.train(data, init_centroids=centroids) # this ensures that kmeans.index is created\r\nassert np.sum(kmeans.centroids - centroids) == 0, \"centroids are not the same\" # sanity check\r\ncluster_distances, cluster_indices = kmeans.assign(new_data)\r\n```"
      },
      {
        "user": "sahyagiri",
        "created_at": "2023-08-22T10:02:43Z",
        "body": "From where do we get \"logits\" variable initialized? "
      },
      {
        "user": "rohitdwivedula",
        "created_at": "2023-08-22T16:56:41Z",
        "body": "@sahyagiri - logits is just the data. I was clustering the outputs of a language model (which is why that variable was called logits). I updated my previous reply also, just now."
      },
      {
        "user": "joangog",
        "created_at": "2024-05-18T09:44:53Z",
        "body": "> @vinit13792 I assume you want to: (1) save a trained KMeans model to file (by saving the centroids), and (2) load the model later and assign clusters to data points.\r\n> \r\n> For step 1, you do something that looks like this:\r\n> \r\n> ```python\r\n> kmeans = faiss.Kmeans(inpuit_dim, num_clusters, verbose=True, gpu=True, niter=300, nredo=10, seed=42)\r\n> kmeans.train(data)\r\n> \r\n> # save centroids to file\r\n> with open(\"centroids.npy\", 'wb') as f:\r\n>   np.save(f, kmeans.centroids)\r\n> ```\r\n> \r\n> To load the centroids and use them, do:\r\n> \r\n> ```python\r\n> centroids = np.load(\"centroids.npy\")\r\n> kmeans = faiss.Kmeans(inpuit_dim, num_clusters, verbose=True, gpu=True, niter=0, nredo=0, seed=42)\r\n> kmeans.train(data, init_centroids=centroids) # this ensures that kmeans.index is created\r\n> assert np.sum(kmeans.centroids - centroids) == 0, \"centroids are not the same\" # sanity check\r\n> cluster_distances, cluster_indices = kmeans.assign(new_data)\r\n> ```\r\n\r\n\r\n\r\nDoesn't that force us to retrain again? "
      }
    ]
  },
  {
    "number": 2015,
    "title": "Generate indices for 30Million vectors (out of memory)",
    "created_at": "2021-08-18T06:25:46Z",
    "closed_at": "2021-09-08T13:29:05Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2015",
    "body": "Any suggestions on how I could generate indices for 30Million vectors? \r\nI am running my code on Spark. Running on driver caused memory problem when I read in the vectors all together.\r\nCan I run in smaller batches and generate IndexIVF for each vector, and then aggregate the indices to find closest ones to a query vector?\r\nThank you!\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2015/comments",
    "author": "maggiex",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-08-23T07:51:07Z",
        "body": "Of course batch adding is supported. \r\nYou mean aggregate the indices or aggregate the search results? "
      },
      {
        "user": "mdouze",
        "created_at": "2021-09-08T13:29:05Z",
        "body": "no activity, closing."
      },
      {
        "user": "chexki",
        "created_at": "2022-05-24T06:24:08Z",
        "body": "Re-opening, as it's a quite good question, In fact i'm also solving similar to such issue.\r\n\r\nHi @mdouze ,\r\nI think aggregate search results would increase the total time of execution !\r\n\r\nSo, aggregating the indices is right option, but I'm wondering if that can be done with limited memory and high volume of records ?"
      }
    ]
  },
  {
    "number": 1968,
    "title": "search based two vector",
    "created_at": "2021-06-29T06:43:34Z",
    "closed_at": "2022-01-19T10:40:57Z",
    "labels": [
      "help wanted",
      "out-of-scope"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1968",
    "body": "# Summary\r\n\r\nif I have a question vector q, and all documents have each two vectors d1 , d2 , can faiss do calculate score based\r\n```\r\nmax(q dot d1 , q dot d2)\r\n````\r\nthank you\r\n\r\n\r\n# Platform\r\n\r\n\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1968/comments",
    "author": "hongyuntw",
    "comments": [
      {
        "user": "KinglittleQ",
        "created_at": "2021-07-03T02:14:22Z",
        "body": "You could create two indexes for `q dot d1` and `q dot d2` respectively and merge the results at the end."
      }
    ]
  },
  {
    "number": 1964,
    "title": "ValueError: not enough values to unpack (expected 2, got 1) for index.add_with_ids(encoded_data, ids)",
    "created_at": "2021-06-28T05:20:58Z",
    "closed_at": "2022-01-19T10:40:03Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1964",
    "body": "**ValueError: not enough values to unpack (expected 2, got 1) for index.add_with_ids(encoded_data, ids)**\r\n\r\nI'm trying to encode the data with the help of below data and code\r\n\r\nAnd the code is below : \r\n\r\n```\r\nencoded_data = model.encode(str(df.Content.tolist()))\r\nencoded_data = np.asarray(encoded_data.astype('float32'))\r\nindex = faiss.IndexIDMap(faiss.IndexFlatIP(768))\r\nids = np.array(range(0, len(df)))\r\nids = np.asarray(ids.astype('int64'))\r\nindex.add_with_ids(encoded_data, ids)\r\n```\r\n\r\nThe error is with the line of code **index.add_with_ids(encoded_data, ids)**\r\n\r\nThe error it's returning is :\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-32-791ec30482ee> in <module>\r\n----> 1 index.add_with_ids(encoded_data, ids)\r\n\r\n~\\t5\\lib\\site-packages\\faiss\\__init__.py in replacement_add_with_ids(self, x, ids)\r\n    229             in result lists to mean \"not found\" so it's better to not use it as an id.\r\n    230         \"\"\"\r\n--> 231         n, d = x.shape\r\n    232         assert d == self.d\r\n    233 \r\n\r\nValueError: not enough values to unpack (expected 2, got 1)\r\n```\r\n\r\n**When i'm trying to add index.add_with_ids(encoded_data, ids), it's returning error like ValueError: not enough values to unpack (expected 2, got 1)**",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1964/comments",
    "author": "nithinreddyy",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-06-30T07:57:10Z",
        "body": "please make sure `encoded_data` is a 2-dimensional array."
      }
    ]
  },
  {
    "number": 1955,
    "title": "make -C build -j faiss",
    "created_at": "2021-06-18T06:57:42Z",
    "closed_at": "2022-01-19T10:38:57Z",
    "labels": [
      "help wanted",
      "install"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1955",
    "body": "when I use \"make -C build -j faiss\",there is a problem:\r\nmake: Entering directory '/home/lishengwen/faiss/build'\r\nmake: *** No rule to make target 'faiss'.  Stop.\r\nmake: Leaving directory '/home/lishengwen/faiss/build'\r\nI have use execute â€œcmake -B build .â€  and directory \"build\" is empty.\r\nThough the step2 is wrong, but I have the same result as showing when I execute:\r\n\r\nimport numpy as np\r\nd = 64                           # dimension\r\nnb = 100000                      # database size\r\nnq = 10000                       # nb of queries\r\nnp.random.seed(1234)             # make reproducible\r\nxb = np.random.random((nb, d)).astype('float32')\r\nxb[:, 0] += np.arange(nb) / 1000.\r\nxq = np.random.random((nq, d)).astype('float32')\r\nxq[:, 0] += np.arange(nq) / 1000.\r\n\r\nimport faiss                   # make faiss available\r\nindex = faiss.IndexFlatL2(d)   # build the index\r\nprint(index.is_trained)\r\nindex.add(xb)                  # add vectors to the index\r\nprint(index.ntotal)\r\n\r\nk = 4                          # we want to see 4 nearest neighbors\r\nD, I = index.search(xb[:5], k) # sanity check\r\nprint(I)\r\nprint(D)\r\nD, I = index.search(xq, k)     # actual search\r\nprint(I[:5])                   # neighbors of the 5 first queries\r\nprint(I[-5:])                  # neighbors of the 5 last queries\r\n\r\nI don't know if I am successful.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1955/comments",
    "author": "monsters-s",
    "comments": [
      {
        "user": "r00tk1ts",
        "created_at": "2021-07-20T08:16:19Z",
        "body": "I encountered the same problem after executing \"cmake3 -B build . -DFAISS_ENABLE_GPU=OFF -DFAISS_ENABLE_PYTHON=OFF -DBLA_VENDOR=Intel10_64_dyn -DMKL_LIBRARIES=/opt/intel/mkl/lib/intel64_lin/*.so -DCMAKE_BUILD_TYPE=Release -DFAISS_OPT_LEVEL=avx2\". The source tag is v1.7.1.\r\n\r\nBut when I reset the repository to initial status and redo the operations, the problem was solved automatically."
      }
    ]
  },
  {
    "number": 1871,
    "title": "Find vectors furthest away / most distant from query",
    "created_at": "2021-05-07T16:32:26Z",
    "closed_at": "2021-05-10T07:12:04Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1871",
    "body": "# Summary\r\nI would like to find the vector furthest away from the query vector. How can I do that? I tried `index.search(query, k=-1)`, but it fails as k must be positive.\r\n\r\n```python\r\ndimension = 16\r\nquery_vectors = np.random.random((5, dimension)).astype('f')\r\ndata = np.random.random((100, dimension)).astype('f')\r\n\r\n# how to do it without faiss\r\ndistance_matrix = sklearn.metrics.pairwise_distances(query_vectors, data)\r\nindizes_most_distant = np.argmax(distance_matrix, axis=1)\r\n\r\n# expected with faiss\r\nfaiss_index = faiss.IndexFlatL2(dimension)\r\nfaiss_index.add(data)\r\nindizes_most_distant = faiss_index.search(query_vectors, k=-1)[1][:, 0]\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1871/comments",
    "author": "MalteEbner",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-05-10T05:28:22Z",
        "body": "This is not implemented in Faiss. \r\nAs a special case, if you have normalized vectors, it is possible to find the most distant one by querying the negated vector."
      },
      {
        "user": "MalteEbner",
        "created_at": "2021-05-10T07:12:04Z",
        "body": "@mdouze \r\nThank you for your quick answer! Unfortunately, my vectors aren't normalised, thus I will have to stick with the current solution without Faiss."
      }
    ]
  },
  {
    "number": 1861,
    "title": "How to use Fiass with SIFT descriptors ?",
    "created_at": "2021-05-03T20:06:18Z",
    "closed_at": "2022-04-27T09:17:39Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1861",
    "body": "# Summary\r\n\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\nHi,\r\nAfter performing SIFT on an image the output I am looking to store is the descriptors information. Each image has different no. of descriptors  and each descriptor has 128 elements.\r\ngiven 2 images in my dataset. the output i get is (1, no. of descriptors in image1, 128) ,( 1, no. of descriptors in image2, 128) \r\nconverting this set of information to array gives a shape of (2,) and \"fiass throws error requires two positional arguments but provided one\". in case I take equal no. of descriptors say 10000 and pass the data it throws the error \"too many values to unpack\" at the index.train(vectors) code line as the array is in shape (2, 10000, 128)  \r\nPlease let me know how to handle the data from SIFT to perform feature matching using FIASS.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1861/comments",
    "author": "sruthi1014",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-05-05T21:33:14Z",
        "body": "Vectors to index or to search should be in a 2D array of size (nb vectors, nb dimensions)."
      }
    ]
  },
  {
    "number": 1796,
    "title": "IndexPQ.cpp:226: Error: 'is_trained' failed",
    "created_at": "2021-03-30T21:33:43Z",
    "closed_at": "2022-04-04T10:54:51Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1796",
    "body": "# Summary\r\n\r\nError when search for docs\r\n\r\n# Platform\r\n\r\nOS: Linux\r\n\r\nFaiss version: \r\n- faiss-cpu              1.7.0\r\n- faiss-gpu              1.7.0\r\n\r\nInstalled from: pip3 install faiss-cpu; pip3 install faiss-gpu\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [ âˆš] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [âˆš ] Python\r\n\r\n# Reproduction instructions\r\n\r\nI've trained the index: \r\n`index = index_factory(lang_model.config.hidden_size, args.faiss_factory)`\r\n`  if not index.is_trained:`\r\n`     sentence_embeddings = np.concatenate(temp_emb, 0)`\r\n`      ids = np.concatenate(temp_ids, 0)`\r\n`      index.train(sentence_embeddings)`\r\n`      ids = np.ascontiguousarray(ids)`\r\n`      index.add_with_ids(sentence_embeddings, ids)`\r\n`  else:`\r\n`      # normalize the embeddings`\r\n `     ids = np.ascontiguousarray(ids.cpu().numpy())`\r\n `     index.add_with_ids(sentence_embeddings, ids)`\r\n\r\nWhen I search for doc using question embedding:\r\n` question_embedding = self.model(**question_tokens)[0].cpu()`\r\n` question_embedding = np.ascontiguousarray(question_embedding[:, 0, :].numpy())`\r\n` scores, ids = self.index.search(question_embedding, k=top_k)`\r\n\r\nit complained like :\r\n`File \".../retrieval/classes.py\", line 50, in retrieve`\r\n` scores, ids = self.index.search(question_embedding, k=top_k)`\r\n` File \".../ENV/lib/python3.6/site-packages/faiss/__init__.py\", line 144, in replacement_search`\r\n`    self.search_c(n, swig_ptr(x), k, swig_ptr(D), swig_ptr(I))`\r\n`  File \".../ENV/lib/python3.6/site-packages/faiss/swigfaiss.py\", line 4832, in search`\r\n`   return _swigfaiss.IndexIDMap_search(self, n, x, k, distances, labels)`\r\n`RuntimeError: Error in virtual void faiss::IndexPQ::search(faiss::Index::idx_t, const float*, faiss::Index::idx_t, float*, ``faiss::Index::idx_t*) const at /__w/faiss-wheels/faiss-wheels/faiss/faiss/IndexPQ.cpp:226: Error: 'is_trained' failed`",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1796/comments",
    "author": "yanchao-yu",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-04-02T16:40:45Z",
        "body": "Probably an error on your side. Is index = self.index ?"
      }
    ]
  },
  {
    "number": 1705,
    "title": "Indexing the feature vector list of unequal shapes",
    "created_at": "2021-02-23T22:31:38Z",
    "closed_at": "2021-02-24T14:56:17Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1705",
    "body": "I have extracted embeddings from images and each vectors are of different shapes. so I have added all the individual vector to a list `descriptors`.\r\n\r\nFinally I use the following code to index the descriptors,\r\n\r\n```\r\ndef create_index(features, index_file_name):\r\n    d = features.shape[1]\r\n    index_model = faiss.IndexFlatIP(d)\r\n    index_model.train(features)\r\n    index_model.add(features)\r\n    faiss.write_index(index_model, index_file_name)\r\n\r\n```\r\nwhereas `features` is a list of features with embeddings of varied shape. Unfortunately the list element has no shape which then throws an error message as follow,\r\n\r\n\r\n`\r\n    d = features.shape[1]\r\nAttributeError: 'list' object has no attribute 'shape'\r\n`\r\nHow can I index list of unequal feature vectors?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1705/comments",
    "author": "Zumbalamambo",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-02-24T14:08:39Z",
        "body": "You can't because it is not possible to compute a distance between vectors of different sizes."
      },
      {
        "user": "Zumbalamambo",
        "created_at": "2021-02-24T14:56:17Z",
        "body": "thank you"
      }
    ]
  },
  {
    "number": 1692,
    "title": "Batch wise index creation and search for data won't find memory  ?  ",
    "created_at": "2021-02-17T03:25:49Z",
    "closed_at": "2023-05-31T06:39:48Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1692",
    "body": "# Summary\r\n\r\nHow to create index of very big data while loading data in batches ? My goal is to get K-NN for all the data points in dataset. \r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Linux amd64\r\n\r\nFaiss version: 1.6.5\r\n\r\nInstalled from: anaconda \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\nThis is the snippet. How should I create index here ?  \r\n**Can't  share the data :(** \r\n\r\n```python\r\nimport faiss\r\nimprot pandas\r\n\r\nquantizer = faiss.IndexFlatL2(64) \r\nindex = faiss.IndexIVFPQ(quantizer, 64, 1024, 8, 8)\r\n\r\ndf = pd.read_csv(\"Very big file won't fit into the memory\", chunksize=10000)\r\n\r\nfor df_batch in df:\r\n    \r\n    batch_data = df.values ## just getting numpy  array of df \r\n\r\n    # is this step right ? will it update the index ? \r\n    index.train(batch_data)\r\n    index.add(batch_data)\r\n\r\n# will I have index on all the data once for-loop is complete ? \r\n\r\nall_knn = []\r\nfor df_batch in df:\r\n\r\n     batch_data = df.values ## just getting numpy  array of df \r\n     all_knn += index.search(batch_data, k=15)\r\n\r\n```\r\n\r\n**Task:**\r\nI have 100M data points with 100+  features. I want to find K-NN value for each of these 10M points.\r\n\r\n**Questions:**\r\n* How should I efficiently **create index** for 100M points.. preferable batch wise (I have memory constraint whole data won't fit into the memory) ? \r\n\r\n* How should I efficiently **search** for K-NN in  index for all 100M points ? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1692/comments",
    "author": "naveen-panwar",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-02-18T17:00:29Z",
        "body": "You should train once and call add on each of the batches.\r\n\r\nFrom the second loop I assume you want to search the set of vectors among themselves. "
      },
      {
        "user": "naveen-panwar",
        "created_at": "2021-02-18T17:16:27Z",
        "body": "Thanks @mdouze for information.. \r\n> You should train once and call add on each of the batches.\r\n\r\nI am assuming it won't harm the indexing.. As I am **only training on 1 batch(10K data points) and adding rest (9M 90K data points)** \r\n\r\n\r\n\r\n\r\n> From the second loop I assume you want to search the set of vectors among themselve\r\n\r\nNo, I want to search for k nearest neighbours of data points of one batches but **neighbours should be from entire data**.. \r\n\r\nJust wanted to confirm again.. `index.search(batch_data, k=15)` **will give me neighbours from all dataset not only from this batch.**\r\n\r\n```python\r\nall_knn = []\r\nfor df_batch in df:\r\n\r\n     batch_data = df.values ## just getting numpy  array of df \r\n     batch_knn = index.search(batch_data, k=15)\r\n     all_knn += batch_knn\r\n````"
      }
    ]
  },
  {
    "number": 1561,
    "title": "FAISS indexes in docker container",
    "created_at": "2020-12-09T08:20:32Z",
    "closed_at": "2021-01-05T15:51:38Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1561",
    "body": "# Summary\r\nFor using FAISS in docker containers, we need to use a mechanism to persist the index and enable sharing (read only) with another container and also transfer it to other containers.\r\nWe can use write_index() and read_index() to save and load the index. Are there special considerations (usage/performance...) for doing this when using docker volumes?\r\nAlso, when the items are being dynamically added to the index (after initial creation), what would be a good/recommended way for updating the index and persistence.\r\n\r\n# Platform\r\nDocker (Linux container on Windows host)\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1561/comments",
    "author": "saurabhkumar",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-12-09T13:28:24Z",
        "body": "There is no special consideration, but a written index cannot be updated: it has to be re-written fully."
      },
      {
        "user": "saurabhkumar",
        "created_at": "2020-12-09T14:25:05Z",
        "body": "@mdouze \r\nPlease help me correct my understanding about saving, loading and updating the index:\r\nIf I have an IndexFlatIP index in memory, I could save it to disk with `faiss.write_index(index, filename)`.\r\nThe applications could then exit.\r\nWhen the application restarts, I can do `index = faiss.read_index('filename')`\r\nThe whole index data (vectors) does not have to be loaded in RAM in this case. Search uses vectors on the disk.\r\nWhen I add new vectors using  `index.add(vectors)`, followed by `index.search(search_vector, num_of_results)`, it would use the newly added vectors? \r\n\r\nOr is there a different sequence to be followed  with\r\n1. Create new index for new vectors and save to disk (different filename)\r\n2. Then use merge_ondisk() to merge it with the earlier index\r\n3. Save the merged index again with write_index()\r\n4. Load this index using read_index() and perform search\r\n\r\nIt will be very helpful if I could get some suggestions on this."
      },
      {
        "user": "mdouze",
        "created_at": "2020-12-09T14:28:20Z",
        "body": "`index = faiss.read_index('filename')` loads all data in RAM."
      },
      {
        "user": "saurabhkumar",
        "created_at": "2020-12-09T14:39:29Z",
        "body": "@mdouze \r\nThanks a lot for this clarification. I understood the demo_ondisk_ivf.py wrong.\r\nIs there another way to have an index on disk to search when the index size is very big and is using up almost all the RAM (even though the search would be slower).  (I do not want to use compression using quantizer for this particular case)."
      },
      {
        "user": "Veyronl",
        "created_at": "2020-12-17T12:01:50Z",
        "body": "@saurabhkumar How do you read 0.1Billion data from the disk and use FAiss to build the index,like np.memmap?\r\nPlease help me"
      },
      {
        "user": "mdouze",
        "created_at": "2021-01-05T15:51:38Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1556,
    "title": "[Question] Database support & euclidean",
    "created_at": "2020-12-04T16:00:18Z",
    "closed_at": "2020-12-04T19:22:18Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1556",
    "body": "# Summary\r\n\r\nHi, i want to make Reverse image search engine, I already have numpy.nd but i have no idea how to do the search on database (the postgresql CUBE max dim are only 100), how do I implement reverse image search engine with FAISS and the database, thankyou\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1556/comments",
    "author": "batara666",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2020-12-04T19:22:18Z",
        "body": "Faiss is a library that can be used as a component for an image search engine and can be interfaced with a database, but implementing image search and interfacing with a database such as PostgresSQL is outside the scope of Faiss library itself and is not something we can provide support on, sorry. You could try other members of the community, perhaps in the FB group."
      }
    ]
  },
  {
    "number": 1551,
    "title": "Selection of GPU and data quantity in IndexIVFPQ experiment",
    "created_at": "2020-12-03T06:39:17Z",
    "closed_at": "2021-01-05T15:50:18Z",
    "labels": [
      "help wanted",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1551",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\n\r\n\r\nThe experiment was carried out in NVIDIA dockerï¼Œnb = 500w, d = 512, There are three graphics cardsï¼š\r\n\r\n0. 1080(8G)\r\n1. 2080ti(11G)\r\n2. 1080(8G)\r\n\r\nI encountered the following problems in the ivfpq experiment:\r\n\r\n(1) **When running a program with multiple GPUs, the difference between **faiss::gpu::index_ cpu_ to_ gpu_multiple** method and **faiss::gpu::GpuiIndexIVFPQ**?**\r\n\r\n(2) **Can **faiss::gpu::GpuiIndexIVFPQ** method control the number of graphics cards and which one ?(I know the method of **faiss:: GPU:: index_ cpu_to_gpu_ Multiple** can control the number of GPUs)**\r\n\r\n(3) When I use nb = 4000000, d = 512,there is no problem.  **When I modify nb = 5000000, an error is reported:\r\n\r\n**_terminate called after throwing an instance of 'std::bad_alloc'_ \r\n what():    std::bad_alloc\r\n Aborted(core dumped)** \r\n\r\n **In addition, the graphics card uses one 2080ti by default. Shouldn't it be 1080 in order?**\r\n\r\n```\r\nfaiss::IndexFlatL2 quantizer(d);\r\nfaiss::IndexIVFPQ cpu_index(&quantizer,d,nlist,m,8);\r\nfaiss::Index * gpu_index=faiss::gpu::index_cpu_to_gpu_multiple(res,devs,&cpu_index);\r\n```\r\n\r\n```\r\nfaiss::gpu::GpuIndexIVFPQConfig config;\r\nconfig.device = 1;\r\nfaiss::gpu::GpuIndexIVFPQ index(&res,d,nlist,m,8,faiss::METRIC_L2,config);\r\n```\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1551/comments",
    "author": "Aurevoir-68",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2020-12-04T19:19:55Z",
        "body": "1. index_cpu_to_gpu_multiple is just a wrapper around calling the GpuIndexIVFPQ constructor. In C++ I'd probably prefer calling the constructor yourself, though the multiple part wraps the indices in an IndexShards or IndexReplicas object, which dedicates a separate thread to each GPU. In order to effectively use multiple GPUs you'll want a separate CPU thread for each index on each GPU.\r\n\r\n2. The GpuIndexIVFPQConfig object contains a field `device` that says which GPU a particular index will use.\r\n\r\n3. yes. Note that 5000000 x 512 x sizeof(float) is 10.24 GB, which is likely exceeding the memory available/remaining on your GPU.\r\n\r\n4. 2080ti vs 1080, you are specifying `config.device = 1`. Nvidia specifies devices numbered from 0. You yourself have the 2080ti device as device #1 above, so don't see what the issue is here?\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2021-01-05T15:50:18Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1347,
    "title": "how to save IndexBinaryFlat on disk",
    "created_at": "2020-08-21T07:51:38Z",
    "closed_at": "2020-08-23T06:33:59Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1347",
    "body": "when I try to save index by `write_index()`\r\nI meet some problem\r\n\r\n> Traceback (most recent call last):\r\n>   File \"hamming.py\", line 50, in <module>\r\n>     main()\r\n>   File \"hamming.py\", line 43, in main\r\n>     faiss.write_index(index, \"./index_BinaryIVF_Hamming.index\")\r\n> NotImplementedError: Wrong number or type of arguments for overloaded function 'write_index'.\r\n>   Possible C/C++ prototypes are:\r\n>     faiss::write_index(faiss::Index const *,char const *)\r\n>     faiss::write_index(faiss::Index const *,FILE *)\r\n>     faiss::write_index(faiss::Index const *,faiss::IOWriter *)\r\n\r\nhere is my code\r\n```\r\nindex = faiss.IndexBinaryFlat(d)\r\n    index.add(data)\r\n    faiss.write_index(index, \"./index_BinaryIVF_Hamming.index\")\r\n\r\n```\r\nfaiss-cpuï¼š1.6.3\r\npythonï¼š7.5\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1347/comments",
    "author": "0ZhangJc0",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-08-23T05:41:55Z",
        "body": "To save binary indexes, use `faiss.write_index_binary`."
      },
      {
        "user": "0ZhangJc0",
        "created_at": "2020-08-23T06:33:47Z",
        "body": "> To save binary indexes, use `faiss.write_index_binary`.\r\n\r\nthank you~"
      }
    ]
  },
  {
    "number": 1310,
    "title": "is there any method/metrics to evaluate model?",
    "created_at": "2020-08-03T03:44:43Z",
    "closed_at": "2020-08-25T13:09:58Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1310",
    "body": "# Summary\r\nI want to check difference between models  that trained by random data, real data or any other datas? so how to measure a model, I know recall@1, recall@10 so far.\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n```ubuntu 18.04 4.15.0-43-generic```\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n```1.5.3```\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n```./configure --without-cuda && make -j && make install ```\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [x] Python",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1310/comments",
    "author": "bigtiger90",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-08-10T19:33:14Z",
        "body": "Please clarify the question."
      },
      {
        "user": "mdouze",
        "created_at": "2020-08-25T13:09:58Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1263,
    "title": "why isn't there a HNSWx,SQy index type",
    "created_at": "2020-06-23T15:42:34Z",
    "closed_at": "2020-07-30T07:21:03Z",
    "labels": [
      "duplicate",
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1263",
    "body": "Thanks a lot for your super nice work. \r\nI realized there is no HNSWx,SQy index type. I am wondering why?  \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1263/comments",
    "author": "wjtan99",
    "comments": [
      {
        "user": "orrorcol",
        "created_at": "2020-06-24T02:07:35Z",
        "body": "You can write HNSWx_SQy"
      },
      {
        "user": "mdouze",
        "created_at": "2020-06-24T07:51:47Z",
        "body": "Will be supported in next Faiss release. See also #1213"
      },
      {
        "user": "wjtan99",
        "created_at": "2020-06-29T15:02:50Z",
        "body": "Thanks for your answer.  I tested it HNSWx_SQy, and IVF4096_HNSWx,SQy. The index file sizes of HNSWx_SQy are all the same, but the sizes of IVF4096_HNSWx,SQy reduce proportionally for different y, compared with IVF4096_HNSWx,Flat. I do not understand why the index file sizes of HNSWx_SQy for different y's are all the same.  Can you help explain?  "
      },
      {
        "user": "mdouze",
        "created_at": "2020-07-30T07:21:03Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1232,
    "title": "Why IndexPreTransform?",
    "created_at": "2020-05-27T09:56:12Z",
    "closed_at": "2020-05-29T02:10:44Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1232",
    "body": "This code, the assertion will fail if I do not tranform `index` to `IndexPreTransform*` fisst, why is this?\r\n``` c++\r\n      std::string pat(\"OPQ32,IVF1000_HNSW32,PQ32+32\");\r\n      auto index = faiss::index_factory(dim, pat.c_str(), metric_type);\r\n      auto cpu_index = dynamic_cast<faiss::IndexPreTransform*>(index.get());\r\n      auto index_ivf = dynamic_cast<faiss::IndexIVF*>(cpu_index->index);\r\n      assert(index_ivf != nullptr);\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1232/comments",
    "author": "orrorcol",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-05-28T22:19:46Z",
        "body": "It's clear if you don't use auto:\r\n\r\n``` c++\r\n      std::string pat(\"OPQ32,IVF1000_HNSW32,PQ32+32\");\r\n      faiss::Index* index = faiss::index_factory(dim, pat.c_str(), metric_type);\r\n      faiss::IndexPreTransform* cpu_index = dynamic_cast<faiss::IndexPreTransform*>(index.get());\r\n      faiss::IndexIVF* index_ivf = dynamic_cast<faiss::IndexIVF*>(cpu_index->index);\r\n      assert(index_ivf != nullptr);\r\n```\r\n\r\nIndex does not have an index field while IndexPreTransform does."
      }
    ]
  },
  {
    "number": 1220,
    "title": "How to use add_one() and build() ? Like simpleneighbors ",
    "created_at": "2020-05-18T05:47:20Z",
    "closed_at": "2020-06-10T14:35:07Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1220",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\nI want to add single data first, then build the index\r\n\r\nsomething like this:\r\n```python\r\nfor batch in batches:\r\n    encoding = encode(batch)\r\n    for en in encoding:\r\n          index.add_one(en)\r\nindex.build()\r\n```\r\n\r\n# Platform\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1220/comments",
    "author": "ilham-bintang",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-05-18T06:19:15Z",
        "body": "I assume the equivalent Faiss is: \r\n```python\r\nindex = ... # some index\r\nindex.train(xt) # train on some training data\r\nfor batch in batches:\r\n    encoding = encode(batch)\r\n    index.add(encoding)\r\n```"
      },
      {
        "user": "ilham-bintang",
        "created_at": "2020-05-18T06:33:44Z",
        "body": "Hi, thank you for the prompt response\r\n\r\nAnyway, what is the `xt` ?\r\n\r\nin my case, the `encode()` is a function that encodes text to vector, and stores the vector the faiss"
      },
      {
        "user": "ilham-bintang",
        "created_at": "2020-05-18T06:38:12Z",
        "body": "To give a more context, here the implementation using simpleneighbour.\r\n\r\nThis code implement minibatch for texts encoding.\r\nI am quite new with faiss, I didn't know what type of index that support `add_single()`. I did not find that function to add single value to index. I have using `index.add()` but it will add last data\r\n\r\nThanks in advance\r\n\r\n\r\n```python\r\nbatch_size = 100\r\n\r\nencodings = model.signatures['response_encoder'](\r\n  input=tf.constant([sentences[0][0]]),\r\n  context=tf.constant([sentences[0][1]]))\r\nindex = simpleneighbors.SimpleNeighbors(\r\n    len(encodings['outputs'][0]), metric='angular')\r\n\r\nprint('Computing embeddings for %s sentences' % len(sentences))\r\nslices = zip(*(iter(sentences),) * batch_size)\r\nnum_batches = int(len(sentences) / batch_size)\r\nfor s in tqdm(slices, total=num_batches):\r\n  response_batch = list([r for r, c in s])\r\n  context_batch = list([c for r, c in s])\r\n  encodings = model.signatures['response_encoder'](\r\n    input=tf.constant(response_batch),\r\n    context=tf.constant(context_batch)\r\n  )\r\n  for batch_index, batch in enumerate(response_batch):\r\n    index.add_one(batch, encodings['outputs'][batch_index])\r\n\r\nindex.build()\r\nprint('simpleneighbors index for %s sentences built.' % len(sentences))\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2020-06-10T14:35:07Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1125,
    "title": "What's the difference between these two implementations?",
    "created_at": "2020-03-02T07:29:50Z",
    "closed_at": "2020-04-01T12:45:08Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1125",
    "body": "dim=128\r\nm=128\r\nn_bits=8\r\n\r\n1. pq=faiss.IndexPQ(dim,m,n_bits)\r\n\r\n2. sq=faiss.IndexScalarQuantizer(dim,faiss.METRIC_L2)\r\n\r\nmy understanding  that when dim=m , each vector(dim=128) will be divide into 128 subvectors .\r\nAnd each subvector  is a scalar  then the first implementation be equivalent to the second. Is this right?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1125/comments",
    "author": "dongzhen123",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2020-03-03T17:20:33Z",
        "body": "They are similar, but still different.\r\n\r\nPQ would use k-means to learn 256 separate scalar quantization values, whereas IndexScalarQuantizer uses QT_8bit by default, which learns an appropriate minimum value and scale only, then uses linear interpolation for each of the 256 quantization levels.\r\n\r\nPQ at m == dim would be highly inefficient of course."
      },
      {
        "user": "mdouze",
        "created_at": "2020-04-01T12:45:08Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1124,
    "title": "How to use IMI(inverted multi index) in faiss?",
    "created_at": "2020-03-02T07:19:28Z",
    "closed_at": "2020-04-01T12:44:50Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1124",
    "body": "What's the difference from this implementation:\r\n\r\nq = faiss.IndexPQ(d, m, nbits)\r\nrq = faiss.IndexRefineFlat(q)",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1124/comments",
    "author": "dongzhen123",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-04-01T12:44:50Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1119,
    "title": "Regarding the IndexFlatIP",
    "created_at": "2020-02-28T14:03:05Z",
    "closed_at": "2020-04-01T12:43:41Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1119",
    "body": "# Summary\r\n\r\nHi ,May I please know how can I get Cosine similarities not Cosine Distances while searching for similar documents. I've used IndexFlatIP as indexes,as it gives inner product.\r\n\r\n`distances, indices = index.search(query_vectors, k)\r\n`\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1119/comments",
    "author": "MaheshChandrra",
    "comments": [
      {
        "user": "MaheshChandrra",
        "created_at": "2020-03-09T10:17:37Z",
        "body": "When I try to do a search I'm getting be below values:\r\n```\r\nresults = index.search(query_vector, 10)\r\nprint(results)#prints distances and similar ids\r\n\r\n(array([[267.5353 , 234.20415, 227.57852, 226.83115, 225.78455, 220.038  ,\r\n         218.0101 , 217.20752, 217.03021, 215.2745 , 215.01762, 214.11276,\r\n         213.06128, 212.98251, 212.56494, 210.98376, 210.3661 , 209.87708,\r\n         209.74539, 209.55539]], dtype=float32),\r\n array([[  3205711,   5535941,   5639730,   5572735,   5803736,   5819228,\r\n           5692490,   2974726,  11847732,   3104495,   2989770,   5845608,\r\n           3132981, 127403668, 127401208,   5728888,   5799607,   5799609,\r\n           5669756,   5579338]]))\r\n\r\n```\r\nCan someone please help me in understanding the distances which I received in the above list(distances,id's),how do I get Cosine similarity in the range or 0 to 1.\r\n\r\n"
      },
      {
        "user": "EvilPort2",
        "created_at": "2020-03-09T13:11:06Z",
        "body": "You need to normalize your query vectors and the search space vectors. Something like this should do.\r\n\r\n```python\r\nnum_vectors = 1000000\r\nvector_dim = 1024\r\nvectors = np.random.rand(num_vectors, vector_dim)\r\n\r\n#sample index code\r\nquantizer = faiss.IndexFlatIP(1024)\r\nindex = faiss.IndexIVFFlat(quantizer, vector_dim, int(np.sqrt(num_vectors)), faiss.METRIC_INNER_PRODUCT)\r\ntrain_vectors = vectors[:int(num_vectors/2)].copy()\r\nfaiss.normalize_L2(train_vectors)\r\nindex.train(train_vectors)\r\nfaiss.normalize_L2(vectors)\r\nindex.add(vectors)\r\n#index creation done\r\n\r\n#let's search\r\nquery_vector = np.random.rand(10, 1024)\r\nfaiss.normalize_L2(query_vector)\r\nD, I = index.search(query_vector, 100)\r\n\r\nprint(D)\r\n```\r\n\r\nPlease note:- <b>faiss.normalize_L2() changes the input vector itself. No copy is created. Hence there it returns None.</b> In case you want to use the original vector you need to create a copy of it by yourself before calling faiss.normalize_L2().\r\nHope this helps."
      },
      {
        "user": "MaheshChandrra",
        "created_at": "2020-03-09T14:19:04Z",
        "body": "Hi EvilPort2,Thanks for  the quick response,may I please know why are we doing index.train for the first half corpus and then adding the complete corpus,is there any possible way of normalizing all the vectors at once without doing a train??\r\n\r\nThanks in advance."
      },
      {
        "user": "EvilPort2",
        "created_at": "2020-03-10T05:33:24Z",
        "body": "I am not exactly sure as to what algorithm IndexIVFFlat uses underneath. But as far as I know, it uses something called KD tree for doing approximate search (@mdouze feel free to correct me). In a KD tree you first create some k clusters using the points in the corpus i.e the vector search space. The **training is done for this clustering** to happen. Now to search a vector you see which of the k clusters is nearest to the query vector by measuring the distance between the query and the cluster centroid. The cluster which is nearest to the query vector is now searched for the top nearest points hence reducing the search space. I have chosen k = square_root(number of vectors in the corpus). \r\nWhen your vector search space is huge and you don't have enough RAM you can take a part of the corpus and train. Ideally you should train with all the vectors and not half of them like I have shown. Hence the ideal code should be something like this.\r\n```python\r\nfaiss.normalize_L2(vectors)\r\nindex.train(vectors)\r\nindex.add(vectors)\r\n```"
      },
      {
        "user": "EvilPort2",
        "created_at": "2020-03-10T07:09:02Z",
        "body": "Also, just a small note. Since you want cosine similarity, it will range from -1 to +1. "
      },
      {
        "user": "MaheshChandrra",
        "created_at": "2020-03-11T05:00:31Z",
        "body": "My bad, forgot about negative similarity,Thanks for addressing.\r\nOne last query does faiss work well in creating indexes on a corpus of 6M embeddings?\r\n\r\nThanks for the quick response and the fix @EvilPort2 , got it fixed."
      },
      {
        "user": "mdouze",
        "created_at": "2020-04-01T12:43:41Z",
        "body": "no activity, closing."
      },
      {
        "user": "ucasiggcas",
        "created_at": "2020-05-31T05:19:40Z",
        "body": "> You need to normalize your query vectors and the search space vectors. Something like this should do.\r\n> \r\n> ```python\r\n> num_vectors = 1000000\r\n> vector_dim = 1024\r\n> vectors = np.random.rand(num_vectors, vector_dim)\r\n> \r\n> #sample index code\r\n> quantizer = faiss.IndexFlatIP(1024)\r\n> index = faiss.IndexIVFFlat(quantizer, vector_dim, int(np.sqrt(num_vectors)), faiss.METRIC_INNER_PRODUCT)\r\n> train_vectors = vectors[:int(num_vectors/2)].copy()\r\n> faiss.normalize_L2(train_vectors)\r\n> index.train(train_vectors)\r\n> faiss.normalize_L2(vectors)\r\n> index.add(vectors)\r\n> #index creation done\r\n> \r\n> #let's search\r\n> query_vector = np.random.rand(10, 1024)\r\n> faiss.normalize_L2(query_vector)\r\n> D, I = index.search(query_vector, 100)\r\n> \r\n> print(D)\r\n> ```\r\n> \r\n> Please note:- faiss.normalize_L2() changes the input vector itself. No copy is created. Hence there it returns None. In case you want to use the original vector you need to create a copy of it by yourself before calling faiss.normalize_L2().\r\n> Hope this helps.\r\n\r\nhi,dear\r\nhave tried the codes,but\r\n```\r\nTraceback (most recent call last):\r\n  File \"faiss_method_.py\", line 266, in <module>\r\n    faiss.normalize_L2(train_vectors)\r\n  File \"/home/xulm1/anaconda3/lib/python3.7/site-packages/faiss/__init__.py\", line 674, in normalize_L2\r\n    fvec_renorm_L2(x.shape[1], x.shape[0], swig_ptr(x))\r\n  File \"/home/xulm1/anaconda3/lib/python3.7/site-packages/faiss/swigfaiss.py\", line 886, in fvec_renorm_L2\r\n    return _swigfaiss.fvec_renorm_L2(d, nx, x)\r\nTypeError: in method 'fvec_renorm_L2', argument 3 of type 'float *'\r\n```\r\nSO could you pls help me?\r\nthx\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2020-05-31T20:45:14Z",
        "body": "train_vectors should be of dtype float32"
      },
      {
        "user": "EvilPort2",
        "created_at": "2020-05-31T21:31:53Z",
        "body": "> My bad, forgot about negative similarity,Thanks for addressing.\r\n> One last query does faiss work well in creating indexes on a corpus of 6M embeddings?\r\n> \r\n> Thanks for the quick response and the fix @EvilPort2 , got it fixed.\r\n\r\nFaiss is awesome for searching in a huge number of vectors. I think the search time will vary on your vector size and also the type of index you use. I think for 6M vectors you can either go for IVFFlat or HNSW index type. Or you can take a mixture of the both (which I don't know how it works) called IVF65536_HNSW32."
      }
    ]
  },
  {
    "number": 1074,
    "title": "Seed id value for Index to help with merge_from ",
    "created_at": "2020-01-02T12:14:23Z",
    "closed_at": "2020-02-20T10:27:00Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1074",
    "body": "OS: Ubuntu 16.04\r\nFaiss version: 1.6.1\r\n\r\nRunning on:\r\n- [.] CPU\r\n\r\nInterface: \r\n- [.] Python\r\n\r\nI have a 5000 top limit for large index and app creates a .smallindex file with the same basename and when it reaches 5000 both files are merged. My queries:\r\n\r\n1. Large index has ids from 0-4999. So will .smallIndex file. After merge_from is called how are these ids merged ? Do they start from 5000 and go on like 5001, 5002 .... \r\n\r\n2. Is there a seed value that i can specify while i create a .smallIndex so that it automatically managed ?\r\n\r\n3. Is the following code correct for merging indexes ?\r\nlargeIndex.merge_from(smallIndex, smallIndex.ntotal)\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1074/comments",
    "author": "jamessmith90",
    "comments": [
      {
        "user": "jamessmith90",
        "created_at": "2020-01-06T06:55:22Z",
        "body": "@mdouze Waiting for your reply.."
      },
      {
        "user": "jamessmith90",
        "created_at": "2020-01-14T08:18:15Z",
        "body": "@mdouze \r\nMy birthday is on 24th April. Please reply by then."
      },
      {
        "user": "mdouze",
        "created_at": "2020-01-20T14:17:40Z",
        "body": "The \"help wanted\" tag is for basic questions that I expect the community to reply. "
      },
      {
        "user": "uday60",
        "created_at": "2020-01-30T08:17:42Z",
        "body": "@jamessmith90 \r\nThe solution to store these ids separately.\r\n\r\n```\r\nclass IndexBlock:\r\n    def __init__(self, filename, index, blockId):\r\n        self.filename = filename\r\n        self.index = index\r\n        self.blockId = blockId\r\n\r\nclass Result:\r\n    def __init__(self, blockId,faissId):\r\n        self.blockId = blockId\r\n        self.faissId = faissId\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2020-02-20T10:27:00Z",
        "body": "No activity. Closing."
      }
    ]
  },
  {
    "number": 1001,
    "title": "IndexIVFFlat on 2M embeddings from FaceNet is giving poor results",
    "created_at": "2019-10-23T16:53:21Z",
    "closed_at": "2019-10-23T21:31:22Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1001",
    "body": "# Summary\r\nI am using embeddings computed from the popular FaceNet model. I have calculate about 2.5M embeddings in d=512 and am looking at performance of the `IndexIVFFlat` compared to the simple `Flat` index. Even with large `k` I see flat results in the recall\r\n\r\nRunning on:\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n```\r\nxb = np.ascontiguousarray(X[::2][:2*1000*1000])\r\nxq = np.ascontiguousarray(X[1::2][:10*1000])\r\nd = xq.shape[1]\r\n\r\n# compute gt\r\nflat_index = faiss.index_factory(d, \"Flat\")\r\nres = faiss.StandardGpuResources()\r\nindex = faiss.index_cpu_to_gpu(res, 0, flat_index, None)\r\nflat_index.train(xb)\r\nflat_index.add(xb)\r\nD, gt = flat_index.search(xq, k)\r\n\r\n# try an approximate method\r\nindex = faiss.index_factory(d, \"IVF<n_centroids>,Flat\")\r\nres = faiss.StandardGpuResources()\r\nindex = faiss.index_cpu_to_gpu(res, 0, index, None)\r\nindex.train(xb)\r\nindex.add(xb)\r\n\r\ndef evaluate(index, xq, gt, k):\r\n    nq = xq.shape[0]\r\n    t0 = time.time()\r\n    D, I = index.search(xq, k)  # noqa: E741\r\n    t1 = time.time()\r\n    recalls = {}\r\n    i = 1\r\n    while i <= k:\r\n        recalls[i] = (I[:, :i] == gt[:, :1]).sum() / float(nq)\r\n        i *= 10\r\n\r\n    return (t1 - t0) * 1000.0 / nq, recalls\r\n\r\nevaluate(flat_index, xq, gt, 1000)\r\n>>\r\n(2.1849388122558593, \r\n {1: 0.99850000000000005, \r\n  10: 1.0, \r\n  100: 1.0, \r\n  1000: 1.0})\r\n\r\nevaluate(index, xq, gt, 1000)\r\n\r\n>>\r\n(0.038869810104370114,\r\n {1: 0.35210000000000002,\r\n  10: 0.35289999999999999,\r\n  100: 0.35289999999999999,\r\n  1000: 0.35299999999999998})\r\n```\r\nNotice how the recall is not increasing as k increases.\r\n\r\nI have tried many ,<n_centroids>, between  4096 to 20000 and I do not see any improvement. \r\n\r\n### Questions:\r\n1. Is it possible that the data distribution is not conducive to this method? \r\n\r\n2. Am I possibly splitting my query and training set incorrectly?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1001/comments",
    "author": "ljstrnadiii",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-10-23T20:12:05Z",
        "body": "You are only looking in a single IVF list, as `nprobe` is by default 1.\r\n\r\nIncrease `nprobe` rather than `k`.\r\n"
      },
      {
        "user": "ljstrnadiii",
        "created_at": "2019-10-23T21:31:22Z",
        "body": "of course, merci beaucoup!\r\n\r\nI did want to ask about the typical strategy to split your datasets. In some examples I have noticed that you build an xb, xt, xq dataset: one for training, one for adding and the last for query (equivalent to a test set). I am not sure what is the typical split for this field. Do you usually train on xt, add [xt, xb] (or does xb already contain xt?) to the index, and search with xt? It is hard to tell how you have constructed your memmap files. What proportion of the whole dataset is xq, xt and xb typically?\r\n\r\nthanks for such a killer project!"
      }
    ]
  },
  {
    "number": 974,
    "title": "Index with integer math",
    "created_at": "2019-10-02T08:28:11Z",
    "closed_at": "2019-10-30T06:36:20Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/974",
    "body": "Hi, \r\nIs there any example that I can build an index then do query search where the math doing the index creation is in integer type (or even lower precision, 8bit, etc.) ?\r\n\r\nThanks in advance",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/974/comments",
    "author": "jimd6776",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-10-02T14:48:17Z",
        "body": "Could you expand on what you mean by \"the math doing the index creation\"?"
      },
      {
        "user": "jimd6776",
        "created_at": "2019-10-07T11:28:09Z",
        "body": "I mean if the matrix multiplications (.i.e. GEMM) during the index creation can be done using integers and not floats."
      },
      {
        "user": "mdouze",
        "created_at": "2019-10-30T06:36:20Z",
        "body": "no activity, closing. "
      }
    ]
  },
  {
    "number": 949,
    "title": "Train \"OPQ64_128,IVF1048576_HNSW32,Flat\" too long time and seem only work in CPU",
    "created_at": "2019-09-13T09:32:03Z",
    "closed_at": "2019-10-30T06:48:47Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/949",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Linux\r\n\r\nFaiss version: 1.5.2\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x ] GPU: 2 GTX 2080 with 12 GB RAM\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\n\r\nHello, I'm working on faiss with 600M vectors database and have got an issue:\r\n\r\nWhen I training faiss with \"OPQ64_128,IVF1048576_HNSW32,Flat\", I found that GPU just work at the very first process, and after that, my program do not run on GPU but run only on 1 CPU. And it's take me too long to train it. (for now is 15 hours and still nothing happened). \r\nCan you tell me what's wrong with my code and usually how long it will take to train such this program?\r\n\r\nHere is my code:\r\n\r\n```\r\nimport faiss\r\nimport numpy as np\r\n\r\nxt = np.load('url_features_full/0.npy').astype('float32')   # 50M of vectors\r\n\r\nprint (xt.shape)\r\nindex = faiss.index_factory(xt.shape[1], \"OPQ64_128,IVF1048576_HNSW32,Flat\")\r\nd = 64\r\nindex_ivf = faiss.extract_index_ivf(index)\r\nclustering_index = faiss.index_cpu_to_all_gpus(faiss.IndexFlatL2(d))\r\nindex_ivf.clustering_index = clustering_index\r\nindex.train(xt)\r\n\r\nprint(\"write \" + tmpdir + \"trained.index\")\r\nfaiss.write_index(index, tmpdir + \"trained.index\")\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/949/comments",
    "author": "hamhochoi",
    "comments": [
      {
        "user": "hamhochoi",
        "created_at": "2019-09-13T09:59:04Z",
        "body": "I tried \"IVF65536_HNSW32,Flat\" and it works on GPU as expected. So I assume that the problem is \"OPQ64_128\" part. But as describe in document: ```note: the OPQ transform is done on CPU, but it is not performance critical```.  "
      },
      {
        "user": "mdouze",
        "created_at": "2019-09-15T23:04:52Z",
        "body": "You can try making the training more verbose with `ParameterSpace().set_index_parameter(index, \"verbose\", 1)`.\r\nThe OPQ64_128 is not supposed to be so slow, but in any case you don't need it because you use a Flat encoding (OPQ is useful for PQ only). If you need to reduce the dimensionality it is better to use `PCARxxx`."
      },
      {
        "user": "hamhochoi",
        "created_at": "2019-09-18T09:57:40Z",
        "body": "1. I actually tried with PCAR but the result was the same. GPUs just load at very first time when my program run and then only 1 CPU is loaded. Do you see any error in my code above?\r\n\r\n2. By the way, why I couldn't reproduce the result from the code in the tutorial? \r\n```\r\nindex = faiss.IndexFlatL2(xb.shape[1]) \r\nids = np.arange(xb.shape[0])\r\nindex2 = faiss.IndexIDMap(index)\r\nindex2.add_with_ids(xb, ids) # works, the vectors are stored in the underlying index\r\n```\r\nThis causes an error:\r\n```\r\nTypeError: in method 'IndexIDMap_add_with_ids', argument 3 of type 'faiss::IndexIDMapTemplate< faiss::Index >::component_t const *'\r\n```\r\nAnd when I use 'IVF65536_HNSW32,Flat' (use or don't use ```IndexIDMap```), I face the problem:\r\n```\r\nTypeError: in method 'IndexIVFFlat_add_with_ids', argument 4 of type 'faiss::Index::idx_t const *'\r\n``` \r\nwhen don't use ```IndexIDMap`` and \r\n```\r\nTypeError: in method 'IndexIDMap_add_with_ids', argument 4 of type 'faiss::IndexIDMapTemplate< faiss::Index >::idx_t const *'\r\n```\r\nwhen use ```IndexIDMap``.\r\n\r\nMy code is:\r\n```\r\nindex = faiss.index_factory(xt.shape[1], \"IVF65536_HNSW32,Flat\")\r\nindex_ivf = faiss.extract_index_ivf(index)\r\nclustering_index = faiss.index_cpu_to_all_gpus(faiss.IndexFlatL2(d))\r\nindex_ivf.clustering_index = clustering_index\r\n\r\nindex = faiss.IndexIDMap(index)\r\nindex.add_with_ids(xt, ids)\r\nindex.train(xt)\r\n```\r\nP/S: Another weird thing is somehow, I can run the code in the tutorial as I mention above one time correctly, but then I cannot reproduce the result?\r\n\r\nUPDATE: It's turn out the error in 2. because of wrong type of xt and ids. The correct ```dtype``` is ```float32 (for xt)``` and ```int64 (for ids)```.\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2019-10-30T06:48:47Z",
        "body": "no activity, closing."
      },
      {
        "user": "Srinath-Raja",
        "created_at": "2020-04-18T19:24:11Z",
        "body": "Hi sir,\r\nI wanted to know that how training of the indexes in faiss takes palce.\r\nCan you please share details on this.\r\n"
      }
    ]
  },
  {
    "number": 482,
    "title": "Question: What to do if the index does not fit in the GPU memory",
    "created_at": "2018-06-06T23:38:35Z",
    "closed_at": "2018-06-15T20:38:33Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/482",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\nOS: <!-- e.g. macOS 10.13.3 --> Ubuntu 16.04\r\n\r\nRunning on :\r\n- [ ] CPU\r\n- [X] GPU\r\n\r\n# Reproduction instructions - NA\r\n\r\nI am trying to run a very large dataset that cannot fit in the GPU memory. So, in this case, do you suggest by default to use unified memory and not use cudaMemcpyAsync? Or does the FAISS internally support split schemes?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/482/comments",
    "author": "msharmavikram",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-06-08T09:52:44Z",
        "body": "Hi \r\nYour options are: \r\n- use more compression on GPU (move to IVFPQ rather than Flat)\r\n- move to CPU\r\n- use non-uniform memory.\r\nI would try the two first options first, as they are easier to test."
      }
    ]
  },
  {
    "number": 403,
    "title": "compile wrong !",
    "created_at": "2018-04-11T10:31:04Z",
    "closed_at": "2018-04-11T10:32:45Z",
    "labels": [
      "help wanted",
      "cant-repro"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/403",
    "body": "# Platform\r\n\r\nOS: Red Hat 4.8.5-16 \r\n\r\nFaiss version:  2.1\r\nFaiss compilation options: using Openblas with compile flags\r\n\r\nRunning on :\r\n- [ ] CPU\r\n\r\n# Reproduction instructions\r\n\r\n```\r\n [root@ai-algorithm faiss]# make tests/test_blas\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -fopenmp -std=c++11 tests/test_blas.cpp -o tests/test_blas -g -fPIC  -fopenmp /usr/lib64/libopenblas.so.0 -DFINTEGER=int\r\n/usr/bin/ld: warning: libgfortran.so.4, needed by /usr/lib64/libopenblas.so.0, not found (try using -rpath or -rpath-link)\r\n/usr/lib64/libopenblas.so.0: undefined reference to `_gfortran_compare_string@GFORTRAN_7'\r\n/usr/lib64/libopenblas.so.0: undefined reference to `_gfortran_etime@GFORTRAN_7'\r\n/usr/lib64/libopenblas.so.0: undefined reference to `_gfortran_concat_string@GFORTRAN_7'\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [tests/test_blas] Error 1\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/403/comments",
    "author": "XiaXuehai",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-04-11T10:32:45Z",
        "body": "Closing this for lack of information. Feel free to reopen with relevant information."
      }
    ]
  },
  {
    "number": 325,
    "title": "Can't install faiss",
    "created_at": "2018-02-02T03:20:59Z",
    "closed_at": "2018-02-21T13:29:38Z",
    "labels": [
      "help wanted",
      "install"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/325",
    "body": "make tests/test_blas\r\n\r\nin the response to the command given above, I get the following output.\r\ng++ -fPIC -m64 -Wall -g -O3 -mavx -msse4 -mpopcnt -fopenmp -Wno-sign-compare -std=c++11 -fopenmp tests/test_blas.cpp -o tests/test_blas -g -fPIC  -fopenmp /usr/lib64/libopenblas.so.0 -DFINTEGER=int\r\n/usr/bin/ld: warning: libgfortran.so.4, needed by /usr/lib64/libopenblas.so.0, not found (try using -rpath or -rpath-link)\r\n/usr/lib64/libopenblas.so.0: undefined reference to `_gfortran_compare_string@GFORTRAN_7'\r\n/usr/lib64/libopenblas.so.0: undefined reference to `_gfortran_etime@GFORTRAN_7'\r\n/usr/lib64/libopenblas.so.0: undefined reference to `_gfortran_concat_string@GFORTRAN_7'\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [tests/test_blas] Error 1\r\n\r\nCan anyone help me to install faiss? \r\nï¼ˆLinux version 3.10.0-514.26.2.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-11) (GCC) )\r\nGNU Fortran (GCC) 8.0.1 20180202 (experimental) [trunk revision 257324]\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.ï¼‰\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/325/comments",
    "author": "Jingjinganhao",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-02-06T09:05:50Z",
        "body": "Hi \r\nThis looks like an openblas install problem. "
      },
      {
        "user": "mdouze",
        "created_at": "2018-02-21T13:29:38Z",
        "body": "No activity, closing."
      },
      {
        "user": "hihei",
        "created_at": "2018-05-02T02:55:28Z",
        "body": "i meet this problem too in the CentOS7 and i install the openblas by conda[python3.6 version]"
      }
    ]
  },
  {
    "number": 298,
    "title": "GPU Version Compile Error with using MKL",
    "created_at": "2017-12-29T11:33:54Z",
    "closed_at": "2018-02-21T13:27:36Z",
    "labels": [
      "help wanted",
      "install"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/298",
    "body": "Here comes an Error when I try to compile GPU version of faiss with using MKL as BLAS tool:\r\n\r\nOS: CentOS 7\r\nCUDA Version: 9.1\r\nMKL Version: 2017-4\r\n\r\nin faiss/gpu \r\nexecuted `make` then I finally got an error below:\r\n\r\n/usr/local/cuda//bin/nvcc  -I /usr/local/cuda//targets/x86_64-linux/include/ -Xcompiler -fPIC -Xcudafe --diag_suppress=unrecognized_attribute -gencode arch=compute_35,code=\"compute_35\" -gencode arch=compute_52,code=\"compute_52\" -gencode arch=compute_60,code=\"compute_60\" --std c++11 -lineinfo -ccbin /usr/bin/g++ -DFAISS_USE_FLOAT16 -o test/demo_ivfpq_indexing_gpu test/demo_ivfpq_indexing_gpu.cpp libgpufaiss.a ../libfaiss.a -Xcompiler -fopenmp -lcublas \\\r\n-Xlinker -Wl,--no-as-needed -L/data/user/software/intel/compilers_and_libraries/linux/mkl//lib/intel64   -lmkl_intel_ilp64 -lmkl_core -lmkl_gnu_thread -ldl -lpthread\r\n/bin/ld: unrecognized option '-Wl'\r\n/bin/ld: use the --help option for usage information\r\ncollect2: error: ld returned 1 exit status\r\n\r\n\r\nThe makefile.inc is set as:\r\n\r\nCC=/usr/bin/g++\r\n\r\nCFLAGS=-fPIC -m64 -Wall -g -O3 -mavx -msse4 -mpopcnt -fopenmp -Wno-sign-compare -std=c++11 -fopenmp\r\nLDFLAGS=-g -fPIC  -fopenmp\r\n\r\nSHAREDEXT=so\r\nSHAREDFLAGS=-shared\r\nFAISSSHAREDFLAGS=-shared\r\n\r\n\r\nMKLROOT=/data/user/software/intel/compilers_and_libraries/linux/mkl/\r\nBLASLDFLAGS=-Wl,--no-as-needed -L$(MKLROOT)/lib/intel64   -lmkl_intel_ilp64 \\\r\n    -lmkl_core -lmkl_gnu_thread -ldl -lpthread\r\n\r\n\r\nSWIGEXEC=swig\r\n\r\nPYTHONCFLAGS=-I/data/anaconda3/envs/idp/include/ -I/data/anaconda3/envs/idp/lib/python3.6/site-packages/numpy/core/include/ -I/data/anaconda3/envs/idp/include/python3.6m\r\n\r\n\r\nCC11=/usr/bin/g++\r\n\r\nCUDAROOT=/usr/local/cuda/\r\n\r\nCUDACFLAGS=-I$(CUDAROOT)/include\r\n\r\nNVCC=$(CUDAROOT)/bin/nvcc\r\n\r\nNVCCFLAGS= $(CUDAFLAGS) \\\r\n   -I $(CUDAROOT)/targets/x86_64-linux/include/ \\\r\n   -Xcompiler -fPIC \\\r\n   -Xcudafe --diag_suppress=unrecognized_attribute \\\r\n   -gencode arch=compute_35,code=\"compute_35\" \\\r\n   -gencode arch=compute_52,code=\"compute_52\" \\\r\n   -gencode arch=compute_60,code=\"compute_60\" \\\r\n   --std c++11 -lineinfo \\\r\n   -ccbin $(CC11) -DFAISS_USE_FLOAT16\r\n\r\nBLASLDFLAGSNVCC=-Xlinker $(BLASLDFLAGS)\r\n\r\nBLASLDFLAGSSONVCC=-Xlinker  $(BLASLDFLAGS)\r\n\r\n\r\nIt seems like that the link command has some problems. Is there any method to fix it?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/298/comments",
    "author": "popfido",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-01-03T22:18:22Z",
        "body": "Hi \r\nI think -Xlinker and -Wl are redundant. You may want to just remove -Wl, "
      },
      {
        "user": "popfido",
        "created_at": "2018-01-08T06:04:40Z",
        "body": "@mdouze You mean remove -Xlinker and -WI from makefile.inc or just this generated compiling command?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-01-09T06:56:37Z",
        "body": "I guess you have to move the flags around a bit until it works..."
      },
      {
        "user": "popfido",
        "created_at": "2018-01-09T08:47:33Z",
        "body": "@mdouze remove -WI, from makefile.inc will cause error below\r\n\r\n/usr/local/cuda//bin/nvcc  -I /usr/local/cuda//targets/x86_64-linux/include/ -Xcompiler -fPIC -Xcudafe --diag_suppress=unrecognized_attribute -gencode arch=compute_35,code=\"compute_35\" -gencode arch=compute_52,code=\"compute_52\" -gencode arch=compute_60,code=\"compute_60\" --std c++11 -lineinfo -ccbin /usr/bin/g++ -DFAISS_USE_FLOAT16 -o test/demo_ivfpq_indexing_gpu test/demo_ivfpq_indexing_gpu.cpp libgpufaiss.a ../libfaiss.a -Xcompiler -fopenmp -lcublas \\\r\n-Xlinker --no-as-needed -L/data/user/software/intel/compilers_and_libraries/linux/mkl//lib/intel64   -lmkl_intel_ilp64 -lmkl_core -lmkl_gnu_thread -ldl -lpthread\r\n/bin/ld: libgpufaiss.a(BlockSelectHalf1.o): unrecognized relocation (0x2a) in section `.text'\r\n/bin/ld: final link failed: Bad value\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [test/demo_ivfpq_indexing_gpu] Error 1\r\n\r\n  "
      },
      {
        "user": "insikk",
        "created_at": "2018-01-25T07:43:16Z",
        "body": "@popfido I succeed compiling by remove -Wl from makefile.inc. \r\nI got the same error. \r\nI removed -Wl from makefile.inc. That is \r\n```\r\nBLASLDFLAGS=--no-as-needed -L$(MKLROOT)/lib/intel64   -lmkl_intel_ilp64 \\\r\n-lmkl_core -lmkl_gnu_thread -ldl -lpthread\r\n```\r\nthen continue `make` in `./gpu/`.\r\n\r\n/usr/local/cuda-8.0//bin/nvcc  -I /usr/local/cuda-8.0//targets/x86_64-linux/include/ -Xcompiler -fPIC -Xcudafe --diag_suppress=unrecognized_attribute -gencode arch=compute_35,code=\"compute_35\" -gencode arch=compute_52,code=\"compute_52\" -gencode arch=compute_60,code=\"compute_60\" --std c++11 -lineinfo -ccbin g++ -DFAISS_USE_FLOAT16 -o test/demo_ivfpq_indexing_gpu test/demo_ivfpq_indexing_gpu.cpp libgpufaiss.a ../libfaiss.a -Xcompiler -fopenmp -lcublas \\\r\n-Xlinker --no-as-needed -L/opt/intel/compilers_and_libraries/linux/mkl//lib/intel64   -lmkl_intel_ilp64 -lmkl_core -lmkl_gnu_thread -ldl -lpthread\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2018-02-21T13:27:36Z",
        "body": "No activity, closing."
      }
    ]
  },
  {
    "number": 218,
    "title": "Can this do region based search instead of nearest neighbour?",
    "created_at": "2017-09-28T01:42:39Z",
    "closed_at": "2017-11-22T14:12:21Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/218",
    "body": "Region based search refers to all points within say a rectangle or sphere.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/218/comments",
    "author": "yxchng",
    "comments": [
      {
        "user": "bkj",
        "created_at": "2017-09-28T15:30:50Z",
        "body": "Some of the indices can -- see `range_search`"
      }
    ]
  },
  {
    "number": 217,
    "title": "Faiss for Face-Identification",
    "created_at": "2017-09-27T02:08:09Z",
    "closed_at": "2017-10-18T01:50:28Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/217",
    "body": "Hi,\r\nI'm trying to apply faiss to my face identification approach, but don't have a very clear idea how to do it. \r\nLike I get 5000 feature vectors of 1000 identities for train, and 100 identities for query. What I am trying to do is use the 5000 feature vectors to build the index and save this index, when do the query, load the index and use 100 identities to query. \r\n\r\nIs that a correct idea? and if i expand the feature vectors number of 1000 identities to 10000, do I need to re-train the index? also if i expand the identities to 2000 with feature vectors number to 10000, do I need to re-train the index?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/217/comments",
    "author": "0DF0Arc",
    "comments": [
      {
        "user": "bkj",
        "created_at": "2017-09-28T15:32:39Z",
        "body": "For that number of observations, you can probably use the `IndexFlatL2`, roughly like:\r\n```\r\ndim = # ... dimension of your  features ... \r\nindex = faiss.IndexFlatL2(dim)\r\nindex.add(index_feats)\r\ndinstances, neighbors = index.search(query_feats, 16)\r\n```"
      },
      {
        "user": "azmathmoosa",
        "created_at": "2018-03-28T10:42:47Z",
        "body": "Lets say, there are 3 persons A, B and C with faces in a 100 images.  Their embeddings are generated from a neural network that is ideally supposed to project same faces within 128D hypersphere of radius 0.6.  However, it doesn't necessarily do so.  After running the network over these 100 images, we have 100 embeddings  clustered around 3 major points with most of them staying within the radius of 0.6 except some.\r\n\r\nIf one trains an SVM on this, it is possible to map these outlier embeddings to one of the 3 face classes.  \r\nHowever, how can one achieve something similar in FAISS?  Is there a provision of this sort?\r\nFAISS returns the closest vectors to a given input vector.  What if that closest vector is actually an anomaly and belonging to a different face?\r\n"
      }
    ]
  },
  {
    "number": 173,
    "title": "undefined symbol GPU python",
    "created_at": "2017-08-07T10:38:21Z",
    "closed_at": "2017-09-08T11:23:37Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/173",
    "body": "when i compile the gpu python part. i do it success.  but when i try to test it with\" import _swigfaiss_gpu\", it output the error info :\"/opt/intel****/intel64/libmkl_gnu_thread.so: undefined symbol: mkl_sparse_z_block_mv_a_rowmajor_i8\".\r\n\r\nthen,  i find the libmkl_gnu_thread.so, and do:  nm -s libmkl_gnu_thread.so | grep \"mkl_sparse_z_block_mv_a_*\".  it do have the symbol \"mkl_sparse_z_block_mv_a_rowmajor_i8\".\r\n\r\nI don't known  why ,  can anybody help me ?  thanks anyway!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/173/comments",
    "author": "bzwqq",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-08-07T11:34:05Z",
        "body": "Hi \r\n\r\nThis is yet another MKL link problem..."
      },
      {
        "user": "bzwqq",
        "created_at": "2017-08-08T11:14:05Z",
        "body": "now, i have solve this problem.\r\n i delete all mkl-versions in my machine, and completely reinstall the lastest mkl version.  and then  binggo now. \r\ni think the reason is  there's some old mkl-versions before, and that cause  the  link error."
      },
      {
        "user": "mdouze",
        "created_at": "2017-09-08T11:23:37Z",
        "body": "closing."
      }
    ]
  },
  {
    "number": 124,
    "title": "Error compiling with GCC 6.3",
    "created_at": "2017-05-31T06:15:03Z",
    "closed_at": "2017-06-19T11:15:00Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/124",
    "body": "```\r\n$ make\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=NULL -Doverride= -fopenmp -c hamming.cpp -o hamming.o  \r\n<command-line>:0:9: error: â€˜NULLâ€™ was not declared in this scope\r\n<command-line>:0:9: error: â€˜NULLâ€™ was not declared in this scope\r\nmake: *** [hamming.o] Error 1\r\n\r\n$ g++ -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=g++\r\nCOLLECT_LTO_WRAPPER=/camp/apps/eb/software/GCCcore/6.3.0/libexec/gcc/x86_64-pc-linux-gnu/6.3.0/lto-wrapper\r\nTarget: x86_64-pc-linux-gnu\r\nConfigured with: ../configure --enable-languages=c,c++,fortran --enable-lto --enable-checking=release --disable-multilib --enable-shared=yes --enable-static=yes --enable-threads=posix --enable-gold=default --enable-plugins --enable-ld --with-plugin-ld=ld.gold --prefix=/camp/apps/eb/software/GCCcore/6.3.0 --with-local-prefix=/camp/apps/eb/software/GCCcore/6.3.0 --enable-bootstrap --with-isl=/camp/apps/eb/build/GCCcore/6.3.0/dummy-/gcc-6.3.0/stage2_stuff\r\nThread model: posix\r\ngcc version 6.3.0 (GCC) \r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/124/comments",
    "author": "verdurin",
    "comments": [
      {
        "user": "xasw1",
        "created_at": "2017-05-31T07:48:06Z",
        "body": "hello, try to change 'NULL' to 'nullptr'"
      },
      {
        "user": "verdurin",
        "created_at": "2017-05-31T08:16:02Z",
        "body": "Thanks, that fixes the compilation problem, but then there's a link error:\r\n```\r\n$ make\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c hamming.cpp -o hamming.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c utils.cpp -o utils.o  -DFINTEGER=long\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c IndexFlat.cpp -o IndexFlat.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c IndexIVF.cpp -o IndexIVF.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c IndexLSH.cpp -o IndexLSH.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c IndexPQ.cpp -o IndexPQ.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c IndexIVFPQ.cpp -o IndexIVFPQ.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c Clustering.cpp -o Clustering.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c Heap.cpp -o Heap.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c VectorTransform.cpp -o VectorTransform.o  -DFINTEGER=long\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c index_io.cpp -o index_io.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c PolysemousTraining.cpp -o PolysemousTraining.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c MetaIndexes.cpp -o MetaIndexes.o\r\nMetaIndexes.cpp:199:6: warning: â€˜void faiss::{anonymous}::translate_labels(long int, faiss::{anonymous}::idx_t*, long int)â€™ defined but not used [-Wunused-function]\r\n void translate_labels (long n, idx_t *labels, long translation)\r\n      ^~~~~~~~~~~~~~~~\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c Index.cpp -o Index.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c ProductQuantizer.cpp -o ProductQuantizer.o  -DFINTEGER=long\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c AutoTune.cpp -o AutoTune.o\r\ng++ -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp -c AuxIndexStructures.cpp -o AuxIndexStructures.o\r\nar r libfaiss.a hamming.o utils.o IndexFlat.o IndexIVF.o IndexLSH.o IndexPQ.o IndexIVFPQ.o Clustering.o Heap.o VectorTransform.o index_io.o PolysemousTraining.o MetaIndexes.o Index.o ProductQuantizer.o AutoTune.o AuxIndexStructures.o\r\nar: creating libfaiss.a\r\ng++ -o tests/demo_ivfpq_indexing -fPIC -m64 -Wall -g -O3  -msse4 -mpopcnt -fopenmp -Wno-sign-compare -Dnullptr=nullptr -Doverride= -fopenmp tests/demo_ivfpq_indexing.cpp libfaiss.a -g -fPIC  -fopenmp -Wl,--no-as-needed -L/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/lib/intel64   -lmkl_intel_ilp64 -lmkl_core -lmkl_gnu_thread -ldl -lpthread\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'strdup'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'isspace'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'strchr'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'free'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'vsnprintf'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'getuid'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'qsort'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'mprotect'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'sched_yield'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'strtol'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to '__cxa_atexit'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'strtod'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'fopen64'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'malloc'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'strtoul'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'ceil'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'vfprintf'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'syscall'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'realloc'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to '__ctype_b_loc'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'getenv'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_core.so: error: undefined reference to 'calloc'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_intel_ilp64.so: error: undefined reference to 'exit'\r\n/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64/libmkl_gnu_thread.so: error: undefined reference to 'strcasecmp'\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [tests/demo_ivfpq_indexing] Error 1\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2017-05-31T11:23:10Z",
        "body": "Hi\r\n\r\nThanks to @xasw1 for the answer. You can also leave the -Dnullptr=NULL option out altogether.\r\n\r\nAbout the undefined reference errors, they are about very basic libc functions, that are required by mkl. It may be that libmkl_core.so relies on another libc. Could you check if you can compile simple MKL programs with gcc 6?"
      },
      {
        "user": "verdurin",
        "created_at": "2017-05-31T13:38:12Z",
        "body": "I'm able to compile and run some of the `mkl` examples at least, with these linking options:\r\n```\r\n-L\"/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/lib/intel64\" -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core \\\r\n -L\"/camp/apps/eb/intel-2017a/software/imkl/2017.1.132-iimpi-2017a/mkl/../compiler/lib/intel64\" -liomp5 -lpthread  -ldl \\\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2017-05-31T13:55:41Z",
        "body": "Ok, thanks, \r\n\r\nMaybe try using the same linking flags then?\r\n\r\n"
      },
      {
        "user": "verdurin",
        "created_at": "2017-05-31T14:15:33Z",
        "body": "If anything that seems to cause even more linker errors...\r\n\r\nSigh."
      },
      {
        "user": "mdouze",
        "created_at": "2017-06-19T11:15:00Z",
        "body": "I still think this is an MKL related error, sorry we cannot debug that further. \r\nClosing."
      }
    ]
  },
  {
    "number": 98,
    "title": "How to search similar image in the index of 10 millions of CNN features ",
    "created_at": "2017-04-29T15:18:48Z",
    "closed_at": "2017-05-04T09:31:13Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/98",
    "body": "I generated 10 million of CNN features for lots of images (512 dimensions)ï¼Œand created indexes in the Faiss, but when I load indexes and search one similar image,  it is out of memory by Python.\r\n\r\nIs there any one have encountered such a problem? Could you give me some sample codes?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/98/comments",
    "author": "mengk007",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-05-01T20:35:21Z",
        "body": "Hi\r\n\r\nPlease be more specific. Is it with a GPU index? What type of index did you use?\r\n"
      },
      {
        "user": "mengk007",
        "created_at": "2017-05-02T14:43:11Z",
        "body": "CPU only, we are going to apply GPU env. IVFFlat index.\r\nThank you for your reply!"
      },
      {
        "user": "mdouze",
        "created_at": "2017-05-04T09:31:13Z",
        "body": "Closing this issue. Re-open if you have more questions."
      }
    ]
  }
]