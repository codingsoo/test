[
  {
    "number": 2506,
    "title": "Medusa max_draft_len overhead impact",
    "created_at": "2024-11-26T22:50:45Z",
    "closed_at": "2024-12-03T08:49:20Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2506",
    "body": "### System Info\n\nCPU architecture: x86_64\nGPU: 8 NVIDIA H200\nLibraries\nTensorRT-LLM: v0.14.0\nCUDA: 12.4\nNVIDIA driver version: 550.127.05\n\n### Setup Info\n\nI'm attempting to use Medusa with TensorRT-LLM to accelerate inference of a fine-tuned Llama 3.1 70B model originally in FP16 precision. To achieve this, I first converted the model to FP8 precision and built it using the following commands:\n\n```\nquantize.py --model_dir=<FINE-TUNED MODEL DIR> --dtype=float16 --tp_size=1 --output_dir=<QUANTIZED MODEL DIR> --qformat=fp8 --kv_cache_dtype=fp8 --calib_dataset=<CALIB DATASET> --calib_size=512 --batch_size=8 --calib_max_seq_length=1024\n\ntrtllm-build --checkpoint_dir=<QUANTIZED MODEL DIR> --max_beam_width=1 --max_seq_len=131072 --max_input_len=130560 --max_num_tokens=32768 --max_batch_size=8 --context_fmha=enable --output_dir=<OUT DIR> --use_fp8_context_fmha=disable\n```\n\nI used this FP8 model to distill a dataset and then trained 3 Medusa heads. When evaluated on a validation dataset, the Medusa heads achieved the following token prediction accuracies wrt the tokens generated by the original FP16 fine-tuned model:\n```\nTopK=0\n> Head 0 Accuracy=0.6837761270606081\n> Head 1 Accuracy=0.32617484167971394\n> Head 2 Accuracy=0.1807497640902462\n\nTopK=4\n> Head 0 Accuracy=0.8547368890673448\n> Head 1 Accuracy=0.5451475708643937\n> Head 2 Accuracy=0.35749212612899667\n```\nThese results indicate that the Medusa heads are correctly predicting tokens.\n\nNext, I built an FP8 model with Medusa heads and set `max_draft_len=1`:\n```\nquantize.py --model_dir=<FINE-TUNED MODEL DIR> --dtype=float16 --tp_size=1 --output_dir=<QUANTIZED MODEL DIR> --qformat=fp8 --kv_cache_dtype=fp8 --calib_dataset=<CALIB DATASET> --calib_size=512 --batch_size=8 --calib_max_seq_length=1024 --max_draft_len=1 --num_medusa_heads=3 --num_medusa_layers=1 --medusa_model_dir=<MEDUSA MODEL DIR>\n\ntrtllm-build --checkpoint_dir=<QUANTIZED MODEL DIR> --max_beam_width=1 --max_seq_len=131072 --max_input_len=130560 --max_num_tokens=32768 --max_batch_size=8 --context_fmha=enable --output_dir=<OUT DIR> --use_fp8_context_fmha=disable --speculative_decoding_mode=medusa --max_draft_len=1\n```\n\nRunning this model built with Medusa and a comparable model built without Medusa in a framework that utilizes TensorRT-LLM's implementation of inflight batching, I observed the following inference p99 latencies:\n- FP8 model without Medusa: 2.526s\n- FP8 model with Medusa and `medusa_choices=\"[[0]]\"`: 2.271s\n\n\nI'm adding the `medusa_choices` in the code as follows:\n```\ndecoding_config = trtllm.DecodingConfig()\nif medusa_choices is not None:\n    decoding_config.medusa_choices = ast.literal_eval(medusa_choices)\n\nexecutor_config = trtllm.ExecutorConfig(\n    max_beam_width=max_beam_width,\n    max_batch_size=max_batch_size,\n    max_num_tokens=max_num_tokens,\n    batching_type=trtllm.BatchingType.INFLIGHT,\n    scheduler_config=trtllm.SchedulerConfig(trtllm.CapacitySchedulerPolicy.GUARANTEED_NO_EVICT),\n    kv_cache_config=kv_cache_config,\n    decoding_config=decoding_config,\n    enable_chunked_context=enable_chunked_context,\n    gpu_weights_percent=1\n)\n\nsession = trtllm.Executor(model_path, trtllm.ModelType.DECODER_ONLY, executor_config)\n```\n\nand creating the `trtllm.Request` like so:\n```\nimport tensorrt_llm.bindings.executor as trtllm\n\ntokens = self.tokenizer.encode(prompt, add_special_tokens=True,\n                               max_length=self.config.build_config.max_input_len, truncation=True)\n\noutput_config = trtllm.OutputConfig()\noutput_config.exclude_input_from_output = True\n\nsampling_conf = trtllm.SamplingConfig(\n    temperature=1 if self.medusa else 0.1,\n    top_k=1 if self.medusa else 50,\n    top_p=0.9,\n    random_seed=self.seed,\n    beam_width=1 if self.medusa else self.max_beam_width\n)\n\ntrt_request = trtllm.Request(\n    input_token_ids = tokens,\n    max_new_tokens = self.max_output_len,\n    pad_id = self.tokenizer.pad_token_id,\n    end_id = self.tokenizer.eos_token_id,\n    streaming = True,\n    sampling_config = sampling_conf,\n    output_config = output_config\n) \n```\n\n### Questions\n\n1. When I build a similar engine with `max_draft_len=17` and run it with the same `medusa_choices=\"[[0]]\"`, I notice a clear increase in inference latency (p99 of 2.918s). Is this expected behavior due to the increased `max_draft_len`, even though I'm specifying to use only topk 0 of the first head?\n2. Do you have any benchmarks that demonstrate the overhead introduced by increasing the Medusa choice tree size (and the `max_draft_len` with it)?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2506/comments",
    "author": "ValeGian",
    "comments": [
      {
        "user": "rakib-hasan",
        "created_at": "2024-12-02T22:16:58Z",
        "body": "Hi @ValeGian \nFor (1), I think it is expected for the following reason. (It is a matter of compile-time-known vs runtime-known dimension)\nFor `max_draft_len=1`, TRT could choose some kernel where there is no need for any overhead (e.g. loop). \nvs\nFor `max_draft_len=17` and running with 1 medusa choice, TRT will still need to build an engine that is valid for all draft lengths from 1 to 17. So, it could choose a different kernel with an extra loop and an entirely different optimization strategy that is optimal for all values from 1 to 17, not just 1 as in the previous case. Adding that flexibility and balanced performance across all possible shapes, it can cost some performance.\n\nFor (2), unfortunately, we do not have any benchmarks yet that demonstrate the impact of these parameters. Maybe we can add it in the near future."
      },
      {
        "user": "ValeGian",
        "created_at": "2024-12-03T08:49:20Z",
        "body": "Thank you for the answer! Closing the question"
      }
    ]
  },
  {
    "number": 2501,
    "title": "How to visualize network?",
    "created_at": "2024-11-26T08:21:46Z",
    "closed_at": "2024-12-06T08:17:43Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2501",
    "body": "Is there any tool or option to visualize trt engine of LLMs? I believe TREx doen't support LLM and also `trtllm-buil --visualize_network` doesn't work.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2501/comments",
    "author": "youki-sada",
    "comments": [
      {
        "user": "wili-65535",
        "created_at": "2024-12-03T00:54:40Z",
        "body": "There are several way to visualize the network.\n1. Using `trtllm-build --log_level=verbose`, you can get detailed layer information from the output log by searching \"Engine Layer Information:\".\n\n2. Using `trtllm-build --visualize_network`, but what is the exact error information you meet? Is something like \"str object has no attribute of 'name'?\"\n    That is a known issue, we will fix it in a later commit. Now we can fix it temporarily by this:\n        1. Find the file `network.py` in the installation directory of tensorrt_llm, for me as an example, \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/network.py\"\n        2. Find the function \"to_onnx\"\n        3. Add one line `path = Path(path)` after the line `trt_network = self.trt_network`.\n        4. Go back to use `trtllm-build --visualize_network`\n\n3. `benchmarks/python/benchmark.py` can print engine information per layer by using parameter `--dump_layer_info`.\n        "
      },
      {
        "user": "youki-sada",
        "created_at": "2024-12-03T02:51:51Z",
        "body": "@wili-65535 \nThank you. As for solution1 and 2, I could get layer information according to your procedure. However, I prefer graph visualization by `--visualize_network` so that I can understand TRT computational graph easily.\n> 2. Using trtllm-build --visualize_network, but what is the exact error information you meet? Is something like \"str object has no attribute of 'name'?\"\n\n`trtllm-build --checkpoint trt_models/llama3.2-1b-hf_fp16 --output_dir /trt_engines/tllm_llama3.2_1b_inst_fp16 --gemm_plugin auto --max_input_len 2048 --max_batch_size 1 --visualize_network` didn't provide onnx file or svg one.\n\n```\n$ ls /trt_engines/tllm_llama3.2_1b_inst_fp16\nconfig.json  rank0.engine\n```\n\n```\n[TensorRT-LLM] TensorRT-LLM version: 0.14.0\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set bert_attention_plugin to auto.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set gpt_attention_plugin to auto.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set gemm_plugin to auto.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set fp8_rowwise_gemm_plugin to None.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set nccl_plugin to auto.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set lookup_plugin to None.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set lora_plugin to None.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set moe_plugin to auto.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set low_latency_gemm_plugin to None.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set context_fmha to True.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set bert_context_fmha_fp32_acc to False.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set remove_input_padding to True.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set reduce_fusion to False.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set enable_xqa to True.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set tokens_per_block to 64.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set multiple_profiles to False.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set paged_state to True.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set streamingllm to False.\n[12/03/2024-02:35:55] [TRT-LLM] [I] Set use_fused_mlp to True.\n[12/03/2024-02:35:55] [TRT-LLM] [W] Implicitly setting LLaMAConfig.producer = {'name': 'modelopt', 'version': '0.19.0'}\n[12/03/2024-02:35:55] [TRT-LLM] [W] Implicitly setting LLaMAConfig.bias = False\n[12/03/2024-02:35:55] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rotary_pct = 1.0\n[12/03/2024-02:35:55] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rank = 0\n[12/03/2024-02:35:55] [TRT-LLM] [W] Implicitly setting LLaMAConfig.decoder = llama\n[12/03/2024-02:35:55] [TRT-LLM] [W] Implicitly setting LLaMAConfig.rmsnorm = True\n[12/03/2024-02:35:55] [TRT-LLM] [W] Implicitly setting LLaMAConfig.lm_head_bias = False\n[12/03/2024-02:35:56] [TRT-LLM] [I] Compute capability: (8, 6)\n[12/03/2024-02:35:56] [TRT-LLM] [I] SM count: 82\n[12/03/2024-02:35:56] [TRT-LLM] [I] SM clock: 2100 MHz\n[12/03/2024-02:35:56] [TRT-LLM] [I] int4 TFLOPS: 705\n[12/03/2024-02:35:56] [TRT-LLM] [I] int8 TFLOPS: 352\n[12/03/2024-02:35:56] [TRT-LLM] [I] fp8 TFLOPS: 0\n[12/03/2024-02:35:56] [TRT-LLM] [I] float16 TFLOPS: 176\n[12/03/2024-02:35:56] [TRT-LLM] [I] bfloat16 TFLOPS: 176\n[12/03/2024-02:35:56] [TRT-LLM] [I] float32 TFLOPS: 88\n[12/03/2024-02:35:56] [TRT-LLM] [I] Total Memory: 24 GiB\n[12/03/2024-02:35:56] [TRT-LLM] [I] Memory clock: 9751 MHz\n[12/03/2024-02:35:56] [TRT-LLM] [I] Memory bus width: 384\n[12/03/2024-02:35:56] [TRT-LLM] [I] Memory bandwidth: 936 GB/s\n[12/03/2024-02:35:56] [TRT-LLM] [I] NVLink is active: False\n[12/03/2024-02:35:56] [TRT-LLM] [I] PCIe speed: 2500 Mbps\n[12/03/2024-02:35:56] [TRT-LLM] [I] PCIe link width: 16\n[12/03/2024-02:35:56] [TRT-LLM] [I] PCIe bandwidth: 5 GB/s\n[12/03/2024-02:35:56] [TRT-LLM] [I] Set dtype to float16.\n[12/03/2024-02:35:56] [TRT-LLM] [I] Set paged_kv_cache to True.\n[12/03/2024-02:35:56] [TRT-LLM] [W] Overriding paged_state to False\n[12/03/2024-02:35:56] [TRT-LLM] [I] Set paged_state to False.\n[12/03/2024-02:35:56] [TRT-LLM] [W] max_seq_len is scaled to 4194304 by rotary scaling 32.0\n[12/03/2024-02:35:56] [TRT-LLM] [I] max_seq_len is not specified, using deduced value 4194304\n[12/03/2024-02:35:56] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width.\n\n[12/03/2024-02:35:56] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored\n[12/03/2024-02:36:06] [TRT] [I] [MemUsageChange] Init CUDA: CPU +15, GPU +0, now: CPU 1245, GPU 263 (MiB)\n[12/03/2024-02:36:09] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +2133, GPU +396, now: CPU 3533, GPU 659 (MiB)\n[12/03/2024-02:36:09] [TRT-LLM] [I] Set nccl_plugin to None.\n[12/03/2024-02:36:09] [TRT-LLM] [E] Failed to import graphviz, please install graphviz to enable Network.to_dot()\n[12/03/2024-02:36:09] [TRT-LLM] [I] Total time of constructing network from module object 13.18428659439087 seconds\n[12/03/2024-02:36:09] [TRT-LLM] [I] Total optimization profiles added: 1\n[12/03/2024-02:36:09] [TRT-LLM] [I] Total time to initialize the weights in network Unnamed Network 0: 00:00:00\n[12/03/2024-02:36:09] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n[12/03/2024-02:36:09] [TRT] [W] Unused Input: position_ids\n[12/03/2024-02:36:09] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n[12/03/2024-02:36:09] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n[12/03/2024-02:36:09] [TRT] [I] Compiler backend is used during engine build.\n[12/03/2024-02:36:13] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n[12/03/2024-02:36:13] [TRT] [I] Detected 16 inputs and 1 output network tensors.\n[12/03/2024-02:36:30] [TRT] [I] Total Host Persistent Memory: 46400 bytes\n[12/03/2024-02:36:30] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[12/03/2024-02:36:30] [TRT] [I] Max Scratch Memory: 67141632 bytes\n[12/03/2024-02:36:30] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 256 steps to complete.\n[12/03/2024-02:36:30] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 19.9799ms to assign 17 blocks to 256 nodes requiring 469768704 bytes.\n[12/03/2024-02:36:30] [TRT] [I] Total Activation Memory: 469768192 bytes\n[12/03/2024-02:36:35] [TRT] [I] Total Weights Memory: 3030520448 bytes\n[12/03/2024-02:36:35] [TRT] [I] Compiler backend is used during engine execution.\n[12/03/2024-02:36:35] [TRT] [I] Engine generation completed in 25.5894 seconds.\n[12/03/2024-02:36:35] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 32 MiB, GPU 2891 MiB\n[12/03/2024-02:36:37] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 11314 MiB\n[12/03/2024-02:36:37] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:28\n[12/03/2024-02:36:37] [TRT] [I] Serialized 27 bytes of code generator cache.\n[12/03/2024-02:36:37] [TRT] [I] Serialized 138954 bytes of compilation cache.\n[12/03/2024-02:36:37] [TRT] [I] Serialized 9 timing cache entries\n[12/03/2024-02:36:37] [TRT-LLM] [I] Timing cache serialized to model.cache\n[12/03/2024-02:36:37] [TRT-LLM] [I] Build phase peak memory: 11316.69 MB, children: 25.25 MB\n[12/03/2024-02:36:37] [TRT-LLM] [I] Serializing engine to /trt_engines/tllm_llama3.2_1b_inst_fp16/rank0.engine...\n[12/03/2024-02:36:40] [TRT-LLM] [I] Engine serialized. Total time: 00:00:02\n```"
      },
      {
        "user": "wili-65535",
        "created_at": "2024-12-03T02:59:22Z",
        "body": "Could you find a file \"rank0.onnx\" somewhere after running the command above, it might not be in the directory of the output engine."
      },
      {
        "user": "youki-sada",
        "created_at": "2024-12-03T03:05:20Z",
        "body": "No I couldn't find it by below commands. \n> $ find . -name rank0.onnx\n> $ find /trt_engines -name rank0.onnx"
      },
      {
        "user": "wili-65535",
        "created_at": "2024-12-03T03:20:25Z",
        "body": "How about ` find / -name rank0.onnx`? The path of the output file was hard-code in current tensorrt_llm and I'm not sure where it should be if not building from source code."
      },
      {
        "user": "youki-sada",
        "created_at": "2024-12-03T03:45:09Z",
        "body": "I couldn't still find it.\n>find / -name 'rank0.onnx' 2>/dev/null"
      },
      {
        "user": "youki-sada",
        "created_at": "2024-12-06T08:17:43Z",
        "body": "I successfully visualized my LLMs by using `trtexec` and NVIDIA Nsight Deep Learning Designer. @wili-65535 Thank you for your kind support."
      }
    ]
  },
  {
    "number": 2345,
    "title": "Status of TensorRT-LLM Eagle Implementation",
    "created_at": "2024-10-16T20:39:08Z",
    "closed_at": "2024-10-18T14:35:51Z",
    "labels": [
      "question",
      "triaged",
      "not a bug"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2345",
    "body": "@puneeshkhanna @ethnzhng @kaiyux \n\nHello.\n\nI notice there has been some commits related to Eagle Speculative decoding in the latest version of TensorRT-LLM.\n\nHowever, there is no runtime code for Py Runtime or CPP runtime that I can see (in generation.py etc).\n\nIs this speculative decoding mode planned for support soon? When I look at the comments in the model.py for EAGLE, it still looks like it is quite far away.\n\nTip: in TensorRT framework, for the context phase you may have to use Conditional Branches due to the way the EAGLE algorithm works differently between context and generation phase (we have implement successfully eagle algorithm in tensorrtllm framework independently)\n\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2345/comments",
    "author": "avianion",
    "comments": [
      {
        "user": "laikhtewari",
        "created_at": "2024-10-18T14:35:51Z",
        "body": "Thanks for the question, @avianion ! Development on eagle is in progress and will be made available in an upcoming release"
      }
    ]
  },
  {
    "number": 2328,
    "title": "GptManager vs Executorl: Why using an Executor instead of a GptManager the release version?",
    "created_at": "2024-10-14T07:31:42Z",
    "closed_at": "2024-10-14T10:19:21Z",
    "labels": [
      "question",
      "triaged",
      "not a bug"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2328",
    "body": "Why using an Executor instead of a GptManager the release version?  ",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2328/comments",
    "author": "Guangjun-A",
    "comments": [
      {
        "user": "Superjomn",
        "created_at": "2024-10-14T08:25:36Z",
        "body": "This may not be sufficient to answer your question, the GptManager is deprecated, and the Executor API is the official API now, it is available both in C++ API and Python API."
      },
      {
        "user": "Guangjun-A",
        "created_at": "2024-10-14T08:51:44Z",
        "body": "> This may not be sufficient to answer your question, the GptManager is deprecated, and the Executor API is the official API now, it is available both in C++ API and Python API.\n\nSO, what reasons was deprecated for GptManager?"
      },
      {
        "user": "MartinMarciniszyn",
        "created_at": "2024-10-14T10:19:21Z",
        "body": "`Executor` API is a refined and generalized version of `GptManager`. It adds another layer of abstraction. To avoid duplicating APIs, the `GptManager` API is not going to be supported beyond 0.14."
      }
    ]
  },
  {
    "number": 2326,
    "title": "Are there any ways to get QK scores from attention?",
    "created_at": "2024-10-14T04:20:10Z",
    "closed_at": "2024-10-15T13:42:18Z",
    "labels": [
      "question",
      "triaged",
      "not a bug"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2326",
    "body": "Hi!\n\nI'm wondering if there is a way to get attention scores from the engine in TRT-LLM? Are there any plans to make it available?\n\nThank you!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2326/comments",
    "author": "ttim",
    "comments": [
      {
        "user": "syuoni",
        "created_at": "2024-10-15T05:04:19Z",
        "body": "Hi @ttim , TRT-LLM does not support returning attention scores.\n\nIn addition, TRT-LLM by default uses fused MHA, which avoids writing the attention scores to global memory. This improves computation efficiency and reduces memory consumption. Returning attention scores significantly damages these features. Currently, we don't have any plans for returning attention scores."
      },
      {
        "user": "ttim",
        "created_at": "2024-10-15T13:42:18Z",
        "body": "@syuoni got it, thanks for the reply!"
      }
    ]
  },
  {
    "number": 2322,
    "title": "C++ Executor Leader Mode",
    "created_at": "2024-10-12T08:56:12Z",
    "closed_at": "2024-10-16T06:15:44Z",
    "labels": [
      "question",
      "triaged",
      "not a bug"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2322",
    "body": "Is it possible to have one process with multiple Executors with Leader mode inside if i have one gpu device? \n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2322/comments",
    "author": "Guangjun-A",
    "comments": [
      {
        "user": "pcastonguay",
        "created_at": "2024-10-15T14:44:59Z",
        "body": "It is possible, but not officially supported. You need to be careful when setting the `KvCacheConfig` `maxTokens` or `freeGpuMemoryFraction` and there could be performance implications when sharing a single gpu between multiple executor instances."
      },
      {
        "user": "Guangjun-A",
        "created_at": "2024-10-16T06:15:44Z",
        "body": "> It is possible, but not officially supported. You need to be careful when setting the `KvCacheConfig` `maxTokens` or `freeGpuMemoryFraction` and there could be performance implications when sharing a single gpu between multiple executor instances.\n\nget it, thanks."
      }
    ]
  },
  {
    "number": 2312,
    "title": "question about flased multi head attention in trtllm-build",
    "created_at": "2024-10-10T10:01:59Z",
    "closed_at": "2024-12-03T02:09:44Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2312",
    "body": "when using trtllm-build, we can use fused attention in LLMs.\nI wonder TensorRT-LLM down fmha data precision automatically.\nI want to use TensorRT-LLM without data type downprecision for accuary.\n ",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2312/comments",
    "author": "yoon5862",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2024-11-16T02:04:58Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-12-03T02:09:43Z",
        "body": "This issue was closed because it has been stalled for 15 days with no activity."
      }
    ]
  },
  {
    "number": 2308,
    "title": "CPU Inference",
    "created_at": "2024-10-09T17:46:17Z",
    "closed_at": "2024-10-16T06:38:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2308",
    "body": "Could TensorRT-LLM use only CPU for inference?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2308/comments",
    "author": "JocelynPanPan",
    "comments": [
      {
        "user": "nv-guomingz",
        "created_at": "2024-10-11T15:37:08Z",
        "body": "No"
      }
    ]
  },
  {
    "number": 2252,
    "title": "Question regarding Executor(BufferView, ...) constructor",
    "created_at": "2024-09-24T14:41:00Z",
    "closed_at": "2024-09-30T11:17:38Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2252",
    "body": "Hi everyone, I have a question regarding `Executer(BufferView engineBuffer,...)` constructor -- could you please confirm that my code should be responsible for the correct lifetime of the object I pass into the constructor? Or is it being copied somewhere inside the constructor? Thanks in advance",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2252/comments",
    "author": "muscowite",
    "comments": [
      {
        "user": "lfr-0531",
        "created_at": "2024-09-30T07:58:04Z",
        "body": "@Funatiq could you help take a look at this question?"
      },
      {
        "user": "Funatiq",
        "created_at": "2024-09-30T11:17:38Z",
        "body": "Hi @muscowite !\nThe engine in deserialized in the constructor and the `engineBuffer` can be released after that."
      }
    ]
  },
  {
    "number": 2174,
    "title": "When will the batch_manager and executor of the cpp code be open sourced?",
    "created_at": "2024-08-31T13:01:27Z",
    "closed_at": "2024-10-08T03:24:28Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2174",
    "body": "When will the batch_manager and executor of the cpp code be open sourced?\n\n\ncode:\n\nTensorRT-LLM\\cpp\\tensorrt_llm\\batch_manager\\x86_64-linux-gnu\nTensorRT-LLM\\cpp\\tensorrt_llm\\executor\\x86_64-linux-gnu\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2174/comments",
    "author": "w066650",
    "comments": [
      {
        "user": "lfr-0531",
        "created_at": "2024-09-04T04:20:33Z",
        "body": "We don't have a plan for now."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-10-05T02:02:04Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      }
    ]
  },
  {
    "number": 2102,
    "title": "How to add a new quantization method?",
    "created_at": "2024-08-09T05:05:26Z",
    "closed_at": "2024-11-14T01:20:24Z",
    "labels": [
      "question",
      "triaged",
      "Low Precision"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2102",
    "body": "I have developed a new KV cache quantization scheme. I am now interested in testing its performance within TensorRT-LLM.\r\n\r\nI'm new to this project, so I am trying to understand the current implementation of quantization. I see that INT8 KV cache quantization is already supported. Could you please guide me to the relevant parts of the codebase where this is implemented? Specifically: Where can I find the implementation of the INT8 KV cache quantization? And are there any tests or examples that demonstrate its usage?\r\n\r\nAlso, I noticed the QuantMode definition in `cpp/include/tensorrt_llm/common/quantization.h` and the quantization kernels in `cpp/tensorrt_llm/kernels/quantization.cu`. How are these CUDA kernels integrated with the Python bindings?\r\n\r\nAny pointers or documentation that can help me get started with integrating and testing my quantization scheme would be greatly appreciated.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2102/comments",
    "author": "Davids048",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2024-09-09T02:00:41Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "Tracin",
        "created_at": "2024-09-11T09:02:13Z",
        "body": "I am afraid those part is not fully open-sourced."
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-14T01:20:24Z",
        "body": "@Davids048 please re-open this issue if you have further questions"
      }
    ]
  },
  {
    "number": 2055,
    "title": "can i add images embedding to llm input? How can i do it？",
    "created_at": "2024-07-30T15:13:09Z",
    "closed_at": "2024-11-14T02:38:55Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2055",
    "body": "such as，i want to use a Visual Pretrained Language Models to take the image embedding,and add it to llm input to get the output",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2055/comments",
    "author": "Onwaydbh",
    "comments": [
      {
        "user": "Popsicle0-0",
        "created_at": "2024-08-05T07:33:42Z",
        "body": "Same problem "
      },
      {
        "user": "amukkara",
        "created_at": "2024-09-04T20:36:23Z",
        "body": "We support several popular multimodal models in `examples/multimodal/`.\n\nFor these models, we pass image embedding input to LLM via `prompt_table` argument (this extends the embedding table of LLM) and modify `input_ids` with indices into `prompt_table`.\n\nYou can check `tensorrt_llm/runtime/multimodal_model_runner.py` for how this mechanism is used for different models.\n\n "
      },
      {
        "user": "Onwaydbh",
        "created_at": "2024-09-04T20:36:53Z",
        "body": "您发给我的信件已收到"
      },
      {
        "user": "Onwaydbh",
        "created_at": "2024-11-14T02:39:25Z",
        "body": "您发给我的信件已收到"
      }
    ]
  },
  {
    "number": 2051,
    "title": "Is the executor part of the code closed source?",
    "created_at": "2024-07-30T10:32:09Z",
    "closed_at": "2024-08-05T08:25:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2051",
    "body": "While reading through the code, I tried to find the implementation of the executor but couldn't find it after searching for a while. However, I did find several static libraries in the cpp/tensorrt_llm/executor directory. I'd like to know if this part is closed source?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2051/comments",
    "author": "vonchenplus",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-08-05T08:25:32Z",
        "body": "Yes, these codes are closed. "
      }
    ]
  },
  {
    "number": 2034,
    "title": "done",
    "created_at": "2024-07-26T10:03:43Z",
    "closed_at": "2024-09-05T02:00:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2034",
    "body": null,
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2034/comments",
    "author": "xiangxinhello",
    "comments": [
      {
        "user": "vonchenplus",
        "created_at": "2024-07-30T10:50:37Z",
        "body": "The default value of the kv_cache_free_gpu_memory_fraction parameter is 0.9. This means that the entire runner will use approximately 90% of the GPU memory, with a portion allocated for the normal operation of the user model and another portion reserved for the kv cache."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-09-05T01:59:03Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      }
    ]
  },
  {
    "number": 2026,
    "title": "Wasteful computations in cross attention?",
    "created_at": "2024-07-25T12:53:51Z",
    "closed_at": "2024-11-14T01:50:45Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2026",
    "body": "As far as i understand, when using cross_attention we first compute `qkv = self.qkv(hidden_states)`, and then `cross_qkv = self.qkv(encoder_output)`. But later only `q` from `qkv` is used, and only `kv` from `cross_qkv` is used.\r\n\r\nSeems like wasteful computation. Perhaps, there should be two matrices: q and kv.\r\n(This will also improve quantization a bit, since there will be separate scales)\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2026/comments",
    "author": "thefacetakt",
    "comments": [
      {
        "user": "QiJune",
        "created_at": "2024-07-26T02:26:38Z",
        "body": "@symphonylyh  Could you please take a look? Thanks"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-26T01:56:36Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "symphonylyh",
        "created_at": "2024-08-26T06:54:26Z",
        "body": "@thefacetakt correct. this is to be more consistent with the fused QKV gemm used in all models, but indeed will bring some redundant computation.\r\n\r\nHowever, I think it won't be very significant in the entire computation:\r\n\r\n1. 1st qkv gemm on hidden states: this is arguably redundant. But since each time the length of hidden states is 1, the cost of this redundant computation might not be huge, i.e. [1, H] * [H, 3X] vs [1, H] * [H, X] \r\n2. 2nd qkv gemm on encoder output: this would be less important, as this gemm is done only once during the entire run. After that, the data is saved in cross kv cache and no long need to use encoder output.\r\n\r\nDo you agree?\r\n\r\nWe can still investigate on (1), to see whether it's critical enough. Otherwise, maybe keeping consistent with other models with <1% slow down is acceptable."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-09-26T02:02:19Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-14T01:51:05Z",
        "body": "Hi @thefacetakt  please reopen it if needed."
      }
    ]
  },
  {
    "number": 1996,
    "title": "Question: node_sharding_weight / edge_resharding_weight",
    "created_at": "2024-07-22T03:48:56Z",
    "closed_at": "2024-07-23T00:43:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1996",
    "body": "Hi, I am trying to understand the underlying code for auto_parallel. \r\n\r\nthe solver.py inside auto_parallel uses node_sharding_weight and edge_resharding_weight to add weight coefficients to \r\neach node communication cost and edge resharding costs. \r\n\r\nthe node_sharding_weight and node_resharding_weight gets incremented as follows: \r\n```       \r\nfor layer_name in layer_mapping.values():\r\n   node = self.get_node(layer_name)\r\n   node.sharding_weight += 1\r\n   node.resharding_weight += 1\r\n```\r\n\r\nMay I ask the purpose of applying these weights? \r\n\r\nI believe that such work is important and appreciate your help and the works. \r\nThanks :)\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1996/comments",
    "author": "saeyoonoh",
    "comments": [
      {
        "user": "yuxianq",
        "created_at": "2024-07-22T08:55:56Z",
        "body": "@saeyoonoh Because we simplify the graph before passing it to the solver (see tensorrt_llm/auto_parallel/simplifier.py). LLM usually contains a lot of repeated blocks, we can simplify the model by elimating most blocks (e.g. from N blocks to 2 blocks) to reduce computation cost of the solver. To correctly estimate the cost of each node/edge, we assign a `sharding_weight` for each of them to make the total cost equal to the original graph before simplified. For example, if the simplified graph contains 2 blocks, the `sharding_weight` of nodes/edges in one block should be 1, and those in another block should be N-1, which simulates the original N-block case. The `layer_mapping` records the mapping from the elimated layers to the reserved layers."
      },
      {
        "user": "saeyoonoh",
        "created_at": "2024-07-23T00:43:07Z",
        "body": "Thanks for the detailed answer. It helps me a lot."
      }
    ]
  },
  {
    "number": 1990,
    "title": "Performance issues with TP and PP settings",
    "created_at": "2024-07-20T00:40:51Z",
    "closed_at": "2024-09-22T02:05:39Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1990",
    "body": "When I use 4 A10s and use PCIe to connect, I can only set TP=4 and PP=1 when I accelerate inference. If I use tp=2 and pp=2, the inference speed decreases significantly.This is an anti-intuitive phenomenon, generally speaking, when using the PCIe bus, the use of PP can reduce the communication consumption, and the model segmentation based on the Decoder structure is very neat, and its computational complexity should be consistent. It was observed that only the first GPU occupancy could reach 95%, and the subsequent GPU occupancy was very low. Ask if there may be some parameter setting issues, or if there are other possibilities",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1990/comments",
    "author": "luoyang1999",
    "comments": [
      {
        "user": "jinyangyuan-nvidia",
        "created_at": "2024-07-22T00:56:50Z",
        "body": "Could you provide the commit you are using?"
      },
      {
        "user": "luoyang1999",
        "created_at": "2024-07-22T01:57:40Z",
        "body": "> Could you provide the commit you are using?\r\n\r\nI use the release version tags: [v0.9.0] with commit ID: [250d9c2],  Is the new version updated with features related to parallel computing?"
      },
      {
        "user": "jinyangyuan-nvidia",
        "created_at": "2024-07-22T02:20:13Z",
        "body": "Yes, there used to be a performance issue of PP + IFB (v0.9.0 has this issue). The issue has been fixed since the commit `a96cccafcf6365c128f004f779160951f8c0801c` in the main branch. Can you try to use the latest main branch and see whether the problem can be solved?"
      },
      {
        "user": "luoyang1999",
        "created_at": "2024-07-22T03:05:13Z",
        "body": "Thank you, I will try testing on the latest main branch and provide you with the test results later\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-22T01:56:36Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-09-22T02:05:39Z",
        "body": "This issue was closed because it has been stalled for 15 days with no activity."
      }
    ]
  },
  {
    "number": 1949,
    "title": "Does tensorrt-llm support blip2 with fp8 quantization??",
    "created_at": "2024-07-15T11:24:13Z",
    "closed_at": "2024-08-30T01:58:48Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1949",
    "body": "I wonder if tensorrt-llm supports blip2 with fp8 quantization? Thanks!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1949/comments",
    "author": "SVT-Yang",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-15T01:50:39Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-30T01:58:48Z",
        "body": "This issue was closed because it has been stalled for 15 days with no activity."
      }
    ]
  },
  {
    "number": 1907,
    "title": "PromptTuning can not work with block_reuse",
    "created_at": "2024-07-06T12:09:15Z",
    "closed_at": "2024-08-05T01:31:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1907",
    "body": "Hi, i found that when i use prompttuning, the block_reuse seems not work. \r\n\r\n> cuda version : 12.2\r\n> TRT-LLM version 0.9.0\r\n> deivice: A100\r\n> precision: FP16\r\n\r\n\r\nFor Yi-6B with 512 input tokens and 1 output tokens and batch size 32.\r\n\r\n* For model without prompt tuning\r\n  * disable block_reuse: 0.99iter/s\r\n  * enable block_reuse: **3.00iter/s**\r\n* for model with prompt tuning\r\n  * disable block_reuse: 0.99iter/s\r\n  * enable block_reuse: 0.99iter/s\r\n\r\nIt seems they can not work simultaneously, could you please help to have a look? Thanks!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1907/comments",
    "author": "littletomatodonkey",
    "comments": [
      {
        "user": "QiJune",
        "created_at": "2024-07-15T08:08:34Z",
        "body": "Yes, it's expected. The prompt tuning can not work with block_reuse now."
      }
    ]
  },
  {
    "number": 1903,
    "title": "How to use LoRA with rank 1024+?",
    "created_at": "2024-07-05T11:50:49Z",
    "closed_at": "2024-08-05T01:34:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1903",
    "body": "### System Info\n\nHello, I'm trying to apply LoRA and getting the following error. Does anyone know if there is a way to run this?\r\n\r\n[TensorRT-LLM][ERROR] Assertion failed: Invalid low_rank (1024). low_rank must be smaller than mMaxLowRank (64)\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n.\n\n### Expected behavior\n\nI expect to be able to use LoRA with a rank of 1024 or higher without encountering any errors.\n\n### actual behavior\n\nWhen I attempt to use LoRA with a rank of 1024, I receive an error stating that the low_rank must be smaller than mMaxLowRank (64).\n\n### additional notes\n\n- I have experimented with lower ranks, and they work without issues.\r\n- It seems the maximum allowable rank is hardcoded to 64.\r\n- Increasing the rank beyond 64 consistently results in the mentioned error.\r\n- I am using the latest version of TensorRT-LLM.\r\n- Any guidance on how to adjust the maximum allowable rank or a workaround to achieve higher ranks would be appreciated.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1903/comments",
    "author": "NextNextDev",
    "comments": [
      {
        "user": "QiJune",
        "created_at": "2024-07-08T00:49:13Z",
        "body": "@byshiue Could you please have a look? Thanks"
      },
      {
        "user": "robmsmt",
        "created_at": "2024-07-11T20:31:14Z",
        "body": "When you do trtllm-build you can set `--max_lora_rank=256`. I have used this, worth try setting to 1024.\r\n"
      },
      {
        "user": "byshiue",
        "created_at": "2024-07-17T07:59:52Z",
        "body": "@robmsmt 's comment is correct. If you don't setup the `max_lora_rank` during building engine, the default would be 64. "
      }
    ]
  },
  {
    "number": 1894,
    "title": "[Quesetion]Recommendation and Future Maintenance of gptSession for Inference in C++ Runtime: Flexibility Concerns with Executor",
    "created_at": "2024-07-04T09:36:03Z",
    "closed_at": "2024-07-05T20:32:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1894",
    "body": "Is it recommended to use GPTSession for inference in the C++ runtime? Will this part continue to be maintained and exposed to users in the future? Although using the executor is convenient, its flexibility is too limited. For example, methods like split-wise cannot be further developed for testing purposes.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1894/comments",
    "author": "Yoh-Z",
    "comments": [
      {
        "user": "QiJune",
        "created_at": "2024-07-05T01:48:46Z",
        "body": "@Shixiaowei02 could you please take a look? Thanks"
      },
      {
        "user": "MartinMarciniszyn",
        "created_at": "2024-07-05T20:32:02Z",
        "body": "`GptSession` is deprecated and will not be further maintained. Please submit feature requests for Executor API in case of any missing flexibility."
      }
    ]
  },
  {
    "number": 1871,
    "title": "How to get or calculate First token latency and second / next token latency in python runtime and cpp runtime benchmarking ?",
    "created_at": "2024-06-30T18:09:37Z",
    "closed_at": "2024-08-17T01:52:56Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1871",
    "body": "I am doing benchmarking in python runtime and in cpp runtime. But there are no first token and second token latency values.\r\n\r\nPlease help me with the process to calculate them.\r\n\r\nTensorRT-LLM version : v0.10.0\r\n\r\nGPU : Nvidia L40S\r\n \r\nsample output of cpp runtime :\r\n\r\n/app/tensorrt_llm/benchmarks/cpp/gptSessionBenchmark --engine_dir ./engines/llama_7b/fp16/fp16-bs32-beam1-normalusecase/ --warm_up 1 --batch_size 1 --num_runs 10 --input_output_len 1024,128 --beam_width 1 \r\n                                    \r\nBenchmarking done. Iteration: 10, duration: 25.61 sec.\r\nLatencies: [2561.47, 2561.20, 2560.63, 2562.01, 2557.57, 2562.35, 2562.13, 2562.02, 2562.01, 2562.08]\r\n[BENCHMARK] batch_size 1 input_length 1024 output_length 128 latency(ms) 2561.35 tokensPerSec 49.97 generation_time(ms) 2492.30 generationTokensPerSec 51.36 gpu_peak_mem(gb) 44.75\r\n \r\nsample output of python runtime : \r\n\r\n[BENCHMARK] model_name gpt_350m world_size 1 num_heads 16 num_kv_heads 16 num_layers 24 hidden_size 1024 vocab_size 51200 precision float16 batch_size 1 input_length 60 output_length 20 gpu_peak_mem(gb) 4.2 build_time(s) 25.67 tokens_per_sec 483.54 percentile95(ms) 41.537 percentile99(ms) 42.102 latency(ms) 41.362 compute_cap sm80",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1871/comments",
    "author": "GunturuSandeep",
    "comments": [
      {
        "user": "QiJune",
        "created_at": "2024-07-01T02:57:45Z",
        "body": "@kaiyux  Could you please take a look? Thanks"
      },
      {
        "user": "GunturuSandeep",
        "created_at": "2024-07-01T05:02:18Z",
        "body": "Thanks @QiJune , for the response. @kaiyux could you help me out here. I just need the process how to calculate first token and second token latency in python and c++ runtime."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-01T01:58:53Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-17T01:52:56Z",
        "body": "This issue was closed because it has been stalled for 15 days with no activity."
      },
      {
        "user": "sbaby171",
        "created_at": "2025-01-30T01:59:35Z",
        "body": "It is a shame this when stale - vLLM benchmarking tools provides TTFT and TBOT latencies. \n\nCan `trtllm-bench` somehow support this? "
      }
    ]
  },
  {
    "number": 1867,
    "title": "Do the tensorrt_llm support for the parameter like past_key_values?",
    "created_at": "2024-06-29T07:24:24Z",
    "closed_at": "2024-08-15T01:50:43Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1867",
    "body": "Please if any similar parameter like huggingface transformer past_key_values is supported in tensorrt_llm? \r\nSo that It will be possible to calculate the kv cache in advance, then pass it to ModelRunner.generate() or ModelRunnerCpp.generate(), It will speed up the decode.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1867/comments",
    "author": "GooVincent",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-31T01:46:39Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-15T01:50:42Z",
        "body": "This issue was closed because it has been stalled for 15 days with no activity."
      }
    ]
  },
  {
    "number": 1866,
    "title": "Run inference / do a forward pass on certain segments of LLM only during inference",
    "created_at": "2024-06-28T13:45:43Z",
    "closed_at": "2024-08-15T01:50:44Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1866",
    "body": "Many research papers add an additional lm_head or decoder_layer to an LLM.\r\n\r\nWhat is the process in the C++ or pytorch runtime to selectively run a forward pass on inference only on a single layer or head of the model, as is common for example in Medusa decoding?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1866/comments",
    "author": "avianion",
    "comments": [
      {
        "user": "QiJune",
        "created_at": "2024-07-01T01:12:34Z",
        "body": "Hi @avianion , could you please give some reference codes(in PyTorch) to show the forward pass? Thanks"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-31T01:46:40Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-08-15T01:50:43Z",
        "body": "This issue was closed because it has been stalled for 15 days with no activity."
      }
    ]
  },
  {
    "number": 1858,
    "title": "Llama 3 70B XQA Support",
    "created_at": "2024-06-27T20:07:07Z",
    "closed_at": "2024-06-28T22:11:56Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1858",
    "body": "Can someone confirm whether or not Llama 3 70B or LLama 3 8B supports XQA?\r\n\r\n\"Another optimization for MQA/GQA in generation phase called XQA optimization. It is still experimental feature and support limited configurations. LLAMA2 70B is one model that it supports.\"\r\n\r\nThe docs state this, but it doesn't mention llama 3 \r\n\r\nIf not, when will you support this?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1858/comments",
    "author": "avianion",
    "comments": [
      {
        "user": "nv-guomingz",
        "created_at": "2024-06-28T08:54:01Z",
        "body": "Yes, LLAMA3 supports XQA optimization."
      }
    ]
  },
  {
    "number": 1810,
    "title": "Is it \"INT8 or FP8\" with \"--use_weight_only --weight_only_precision int8 --qformat fp8\"",
    "created_at": "2024-06-19T15:04:19Z",
    "closed_at": "2024-12-04T10:25:41Z",
    "labels": [
      "bug",
      "question",
      "Low Precision",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1810",
    "body": "### System Info\n\nGPU - A10\r\n\r\n\r\n\n\n### Who can help?\n\n@Tracin \n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nHi there, \r\n\r\nWith \"--use_weight_only --weight_only_precision int8 --qformat fp8\", will the quantization be in INT8 or FP8? \n\n### Expected behavior\n\nFP8 dtype when printing out weights.\n\n### actual behavior\n\nThe weights are in torch.int8.\n\n### additional notes\n\nN/A",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1810/comments",
    "author": "aiiAtelier",
    "comments": [
      {
        "user": "nv-guomingz",
        "created_at": "2024-06-19T15:54:27Z",
        "body": "May I know your full cmd with above parameters？"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-20T01:51:44Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      }
    ]
  },
  {
    "number": 1807,
    "title": "cluster key option not working?",
    "created_at": "2024-06-19T07:36:21Z",
    "closed_at": "2024-11-14T02:23:44Z",
    "labels": [
      "question",
      "triaged",
      "stale",
      "waiting for feedback"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1807",
    "body": "Hi,\r\nI tried the `--cluster-key` option with trtllm-build.\r\nI did the conversion with A100-80gb-sxm, then tried to deploy it on L4 after converting using the L4 option and it failed when starting up the tritonserver.\r\n\r\nWhen I'm trying to deploy it on a100-40gb-sxm with its relevant option it does start-up work  but I'm getting:\r\n ```[TensorRT-LLM][WARNING] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.```\r\n\r\nIn my config.json there appears to be the cluster_key I converted with.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1807/comments",
    "author": "tonylek",
    "comments": [
      {
        "user": "yuxianq",
        "created_at": "2024-06-19T09:58:58Z",
        "body": "`--cluster_key` is used with `--auto_parallel N`, it can provide cluster info to determine auto parallel's sharding strategy. In other word, `--cluster_key` does not help to cross-build engine to different type of GPUs, which is unsupported in TRT-LLM. Since you does not use `--auto_parallel N`, `--cluster_key` should make no effect to the build process. I will update the help message to avoid confusion when users does not use `--auto_parallel N`."
      },
      {
        "user": "tonylek",
        "created_at": "2024-06-19T13:29:22Z",
        "body": "any chance it will be supported in the future?"
      },
      {
        "user": "yuxianq",
        "created_at": "2024-06-19T14:37:53Z",
        "body": "The cross-build feature is not planned. Please build and deploy on the same type of GPU."
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-06-25T10:36:24Z",
        "body": "@tonylek  could we close this ticket now？"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-26T01:53:47Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-14T02:23:44Z",
        "body": "Please reopen it if needed."
      }
    ]
  },
  {
    "number": 1778,
    "title": "`Parameter transformer.layers.N.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method`",
    "created_at": "2024-06-13T13:06:39Z",
    "closed_at": "2024-06-13T16:07:01Z",
    "labels": [
      "bug",
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1778",
    "body": "### System Info\r\n\r\nWhile trying to debug poor quality of outputs from TRT LLM for Llama3 70b tp=4 (compared to vLLM and HF), I ran into the following message when building bfloat16 engine.\r\n\r\n```\r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network\r\n```\r\n\r\n(repeated for each layer)\r\n\r\nIs this message harmless?\r\n\r\nThe commands I run:\r\n\r\n```sh\r\npython convert_checkpoint.py \\\r\n--model_dir /workspace/llama3-70b \\\r\n--output_dir /workspace/llama3-70b-bf16-tp4 \\\r\n--dtype bfloat16 \\\r\n--tp_size 4\r\n\r\ntrtllm-build \\\r\n--checkpoint_dir /workspace/llama3-70b-bf16-tp4 \\\r\n--output_dir /workspace/llama3-70b-bf16-tp4-engine \\\r\n--gpt_attention_plugin bfloat16 \\\r\n--gemm_plugin bfloat16 \\\r\n--use_custom_all_reduce disable \\\r\n--max_num_tokens 32768 \\\r\n--max_batch_size 48 \\\r\n--max_input_len 8192 \\\r\n--max_output_len 4096\r\n```\r\n\r\nThe full logs:\r\n\r\n```\r\n[TensorRT-LLM] TensorRT-LLM version: 0.11.0.dev2024060400                                                                                                                                                                                                                                              \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set bert_attention_plugin to auto.                                                                                                                                                                                                                                 \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.                                                                                                                                                                                                                              \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gemm_plugin to bfloat16.                                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.                                                                                                                                                                                                                                    \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set nccl_plugin to auto.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set lookup_plugin to None.                                                                                                                                                                                                                                         \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set lora_plugin to None.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set moe_plugin to auto.                                                                                                                                                                                                                                            \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.                                                                                                                                                                                                                                   \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set context_fmha to True.                                                                                                                                                                                                                                          \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set paged_kv_cache to True.                                                                                                                                                                                                                                        \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set remove_input_padding to True.                                                                                                                                                                                                                                  \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_custom_all_reduce to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set multi_block_mode to False.                                                                                                                                                                                                                                     \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set enable_xqa to True.                                                                                                                                                                                                                                            \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set tokens_per_block to 64.                                                                                                                                                                                                                                        \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_paged_context_fmha to False.                                                                                                                                                                                                                               \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_fp8_context_fmha to False.                                                                                                                                                                                                                                 \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set multiple_profiles to False.                                                                                                                                                                                                                                    \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set paged_state to True.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set streamingllm to False.                                                                                                                                                                                                                                         \r\n[06/13/2024-13:01:14] [TRT-LLM] [W] Specifying a `max_num_tokens` larger than 16384 is usually not recommended, we do not expect perf gain with that and too large `max_num_tokens` could possibly exceed the TensorRT tensor volume, causing runtime errors. Got `max_num_tokens` = 32768             \r\n[06/13/2024-13:01:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.mup_width_multiplier = 1.0                                                                                                                                                                                                          \r\n[06/13/2024-13:01:15] [TRT-LLM] [I] Set dtype to bfloat16.                                                                                                                                                                                                                                             \r\n[06/13/2024-13:01:15] [TRT] [I] [MemUsageChange] Init CUDA: CPU +17, GPU +0, now: CPU 160, GPU 528 (MiB)                                                                                                                                                                                               \r\n[06/13/2024-13:01:19] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4312, GPU +1150, now: CPU 4607, GPU 1678 (MiB)                                                                                                                                                                      \r\n[06/13/2024-13:01:19] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.                                                                                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [W] allreduce algorithm is selected automatically during execution now. use_custom_all_reduce will be deprecated in future releases.\r\n[06/13/2024-13:01:19] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.                                                                                                                                                              [138/782]\r\n[06/13/2024-13:01:19] [TRT-LLM] [W] allreduce algorithm is selected automatically during execution now. use_custom_all_reduce will be deprecated in future releases.                                                                                                                                   \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Set nccl_plugin to bfloat16.                                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Set use_custom_all_reduce to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                         \r\n```\r\n\r\n### Who can help?\r\n\r\n@byshiue \r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n4xH100 SXM\r\n\r\n### Expected behavior\r\n\r\nUnsure, maybe the message is harmless\r\n\r\n### actual behavior\r\n\r\nN/A\r\n\r\n### additional notes\r\n\r\nN/A",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1778/comments",
    "author": "DreamGenX",
    "comments": [
      {
        "user": "nv-guomingz",
        "created_at": "2024-06-13T14:11:46Z",
        "body": "This is a known issue introduced by new feature weightless engine, it's just a warning message and harmless. Please ignore it and we'll fix it in the coming release."
      },
      {
        "user": "DreamGenX",
        "created_at": "2024-06-13T15:32:50Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 1748,
    "title": "‘cudaStream_t’ has not been declared when building tensorrt_llm v0.10.0",
    "created_at": "2024-06-06T06:09:39Z",
    "closed_at": "2024-06-06T06:29:02Z",
    "labels": [
      "question",
      "not a bug"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1748",
    "body": "### System Info\n\nCPU: INTEL\r\nGPU Name: A100-SXM4-80GB\r\nTensorRT-LLM: tag v0.10.0\r\nContainer Used: No\r\nDriver Version: 535.54.03\r\nCUDA Version: 12.1\r\nOS: Ubuntu 22.04\r\nOthers: tensorrt==10.0.1.16, pytorch==2.2.0+cu121, python==3.10.12\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\npython scripts/build_wheel.py --trt_root=/usr/local/tensorrt --clean --install --cuda_architectures=\"80-real\"\n\n### Expected behavior\n\nWorks fine\n\n### actual behavior\n\nxxx/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.h:48:37: error: ‘cudaStream_t’ has not been declared\r\n   48 | void invokeRGLRU(lruParams& params, cudaStream_t stream);\r\n      |                                     ^~~~~~~~~~~~\r\nxxx/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.h:51:43: error: ‘cudaStream_t’ has not been declared\r\n   51 | void invokeRGLRUUpdate(lruParams& params, cudaStream_t stream);\r\n      |                                           ^~~~~~~~~~~~\n\n### additional notes\n\nNo",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1748/comments",
    "author": "zhangts20",
    "comments": [
      {
        "user": "hijkzzz",
        "created_at": "2024-06-06T06:12:26Z",
        "body": "Env issue\r\nTry this container: nvcr.io/nvidia/tritonserver:24.05-trtllm-python-py3"
      },
      {
        "user": "zhangts20",
        "created_at": "2024-06-06T06:22:10Z",
        "body": "@hijkzzz Can you share the solution? I want to install tensorrt_llm in my own docker env, thanks"
      },
      {
        "user": "hijkzzz",
        "created_at": "2024-06-06T06:29:35Z",
        "body": "> @hijkzzz Can you share the solution? I want to install tensorrt_llm in my own docker env, thanks\r\n\r\nYou could build the container that come FROM this container\r\nOr use pip install tensorrt_llm==xxx in your container."
      },
      {
        "user": "zhangts20",
        "created_at": "2024-06-06T06:46:14Z",
        "body": "@hijkzzz thanks! I have installed it successfully using pip install tensorrt_llm==xxx"
      },
      {
        "user": "yorickvP",
        "created_at": "2024-06-12T13:19:32Z",
        "body": "In case anyone else wants to build from source, add `#include <cuda_runtime.h>` to `cpp/tensorrt_llm/kernels/lruKernel.h`"
      }
    ]
  },
  {
    "number": 1731,
    "title": "Enabling w4a16 and 2:4 sparsity.",
    "created_at": "2024-06-05T08:39:40Z",
    "closed_at": "2024-06-05T12:41:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1731",
    "body": "I found that performance which enabling w4a16 and 2：4 sparsity is similar to enabling w4a16 alone.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1731/comments",
    "author": "jianyuheng",
    "comments": [
      {
        "user": "hijkzzz",
        "created_at": "2024-06-05T12:41:39Z",
        "body": "from Chong (TRT-LLM engineer): Recently, I have been testing the sparse performance of TRT-LLM.\r\nIndeed, both BF16 and FP8 have acceleration ratios within 5% (GPT3-843M)."
      },
      {
        "user": "youki-sada",
        "created_at": "2024-10-28T05:04:11Z",
        "body": "@hijkzzz @jianyuheng Do you have any information for reduction of VRAM footprint?"
      }
    ]
  },
  {
    "number": 1730,
    "title": "Warning: Function too large, generated debug information may not be accurate.",
    "created_at": "2024-06-05T08:28:44Z",
    "closed_at": "2024-12-04T10:25:34Z",
    "labels": [
      "question",
      "need more info",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1730",
    "body": "### System Info\r\n\r\nGPU:A100\r\nMem: 1007G\r\nTRTLLM: v0.9.0\r\nG++\\NVCC\r\n\r\nWhen I build TensorRT_LLM, Warning: Function too large, generated debug information may not be accurate.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1730/comments",
    "author": "nanmi",
    "comments": [
      {
        "user": "5cat",
        "created_at": "2024-06-06T08:28:27Z",
        "body": "Hello! May I ask if you can share the steps to preproduce the steps? Do you mean building the model or building tensorrt_llm source code?\r\nIf you could add the logs as well that would be helpful"
      },
      {
        "user": "nanmi",
        "created_at": "2024-06-11T02:25:30Z",
        "body": "> Hello! May I ask if you can share the steps to preproduce the steps? Do you mean building the model or building tensorrt_llm source code? If you could add the logs as well that would be helpful\r\n\r\nThank you for your reply. My question is about the compilation process of TensorRT_LLM source code. The compilation method is the basic build_wheel.py application."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-12T01:52:44Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-14T06:57:50Z",
        "body": "Hi @5cat do u still have further issue or question now? If not, we'll close it soon.\n"
      }
    ]
  },
  {
    "number": 1729,
    "title": "How to open inflight batching in TensorRT_LLM in v0.9.0",
    "created_at": "2024-06-05T02:26:51Z",
    "closed_at": "2024-06-05T10:51:33Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1729",
    "body": "### System Info\n\nRTX-8*4090\n\n### Who can help?\n\n@kaiyux @ncomly-nvidia @jun\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nI want to test the inflight batching feature, how to open it in build engine. \"--use_inflight_batching\" has been removed in trtllm-build in v0.9.0. \n\n### Expected behavior\n\nInflight batching is opened correctly.\n\n### actual behavior\n\nI am not sure how to open it in v0.9.0.\n\n### additional notes\n\nI want to test the inflight batching feature, how to open it in build engine. \"--use_inflight_batching\" has been removed in trtllm-build in v0.9.0. ",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1729/comments",
    "author": "Godlovecui",
    "comments": [
      {
        "user": "hijkzzz",
        "created_at": "2024-06-05T04:48:57Z",
        "body": "1. build engine with --max_batch_size > 1\r\n\r\n3. use the flag `=trtllm.BatchingType.INFLIGHT`\r\n```\r\n trtllm.ExecutorConfig( \r\n            1, \r\n            parallel_config=trt_parallel_config, \r\n            normalize_log_probs=False, \r\n            batching_type=trtllm.BatchingType.INFLIGHT, \r\n            scheduler_config=trt_scheduler_config, \r\n        ), \r\n```\r\n\r\nHere is an example for tensorrt_llm==0.10.0.dev2024050700\r\n```\r\nimport argparse \r\nimport logging \r\nimport time \r\nfrom datetime import datetime, timedelta \r\nfrom pathlib import Path \r\nfrom threading import Thread \r\n \r\nimport tensorrt_llm \r\nimport tensorrt_llm.bindings.executor as trtllm \r\nfrom transformers import PreTrainedTokenizerFast \r\n \r\nlogger = logging.getLogger(__name__) \r\n \r\n \r\ndef tensorrt_llm_executor_worker_path() -> str: \r\n    worker_path = Path(tensorrt_llm.__file__).parent / 'bin' / 'executorWorker' \r\n    if not worker_path.exists(): \r\n        raise Exception(\"TensorRT-LLM executor worker not found\") \r\n    return str(worker_path) \r\n \r\n \r\ndef get_trt_parallel_config(): \r\n    world_size = 2 \r\n    if world_size > 1: \r\n        executor_worker_path = tensorrt_llm_executor_worker_path() \r\n        orchestrator_config = trtllm.OrchestratorConfig(True, executor_worker_path) \r\n        return trtllm.ParallelConfig( \r\n            trtllm.CommunicationType.MPI, \r\n            trtllm.CommunicationMode.ORCHESTRATOR, \r\n            orchestrator_config=orchestrator_config, \r\n            # TODO:BIS fix device_ids \r\n            device_ids=[0, 1], \r\n        ) \r\n    else: \r\n        return trtllm.ParallelConfig(trtllm.CommunicationType.MPI, trtllm.CommunicationMode.LEADER) \r\n \r\n \r\ndef create_executor(model_path: str) -> trtllm.Executor: \r\n    trt_parallel_config = get_trt_parallel_config() \r\n    trt_scheduler_config = trtllm.SchedulerConfig(trtllm.CapacitySchedulerPolicy.GUARANTEED_NO_EVICT) \r\n \r\n    return trtllm.Executor( \r\n        Path(model_path), \r\n        trtllm.ModelType.DECODER_ONLY, \r\n        trtllm.ExecutorConfig( \r\n            1, \r\n            parallel_config=trt_parallel_config, \r\n            normalize_log_probs=False, \r\n            batching_type=trtllm.BatchingType.INFLIGHT, \r\n            scheduler_config=trt_scheduler_config, \r\n        ), \r\n    ) \r\n \r\n \r\ndef create_request(input_ids, output_len, eos_id: int, sample_params): \r\n    output_config = trtllm.OutputConfig(exclude_input_from_output=True) \r\n    ## This seems to somewhat resolve the issue \r\n    # sampling_config = trtllm.SamplingConfig(beam_width=1, frequency_penalty=1.0) \r\n    request = trtllm.Request( \r\n        input_token_ids=input_ids, \r\n        max_new_tokens=output_len, \r\n        streaming=True, \r\n        output_config=output_config, \r\n        end_id=eos_id, \r\n        sampling_config=sample_params, \r\n    ) \r\n    return request \r\n \r\n \r\ntrt_id = None \r\n \r\n \r\ndef main(): \r\n    default_prompt = \"You have been tasked with designing a solar-powered water heating system for a residential building. Describe the key components and considerations you would include in your design. Design a five-step workflow.!\" \r\n    # default_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\nYou have been tasked with designing a solar-powered water heating system for a residential building. Describe the key components and considerations you would include in your design. Design a five-step workflow.!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" \r\n    parser = argparse.ArgumentParser() \r\n    parser.add_argument(\"--model_path\", required=False, default=\"./tmp/llama3-8b-tp2-engine\") \r\n    parser.add_argument(\"--tokenizer_path\", required=False, default=\"/home/scratch.trt_llm_data/llm-models/llama-models-v3/llama-v3-8b-instruct-hf/\") \r\n    parser.add_argument(\"--prompt\", required=False, default=default_prompt) \r\n \r\n    args = parser.parse_args() \r\n \r\n    tokenizer = PreTrainedTokenizerFast.from_pretrained(args.tokenizer_path) \r\n    executor = create_executor(args.model_path) \r\n    prompt = args.prompt \r\n    prompt_ids = tokenizer.encode(prompt) \r\n    print(prompt_ids) \r\n \r\n    def do_decode(sampling_config): \r\n        output_ids = [] \r\n        finished = False \r\n        req = create_request(prompt_ids, 150, tokenizer.eos_token_id, sampling_config) \r\n        _ = executor.enqueue_request(req) \r\n        while not finished: \r\n            responses = executor.await_responses(timeout=timedelta(seconds=1)) \r\n            for r in responses: \r\n                if r.has_error(): \r\n                    raise RuntimeError(r.error_msg) \r\n                result = r.result \r\n                output_ids.extend(result.output_token_ids[0]) \r\n                if result.is_final: \r\n                    finished = True \r\n        return tokenizer.decode(output_ids) \r\n \r\n\r\n    print(do_decode(trtllm.SamplingConfig(beam_width=1, top_k=1, random_seed=1234)))     \r\n    print(\"===================================\") \r\n \r\n    executor.shutdown() \r\n \r\n \r\nif __name__ == \"__main__\": \r\n    main() \r\n```"
      },
      {
        "user": "Godlovecui",
        "created_at": "2024-06-05T06:11:26Z",
        "body": "> 1. build engine with --max_batch_size > 1\r\n> 2. use the flag `=trtllm.BatchingType.INFLIGHT`\r\n> \r\n> ```\r\n>  trtllm.ExecutorConfig( \r\n>             1, \r\n>             parallel_config=trt_parallel_config, \r\n>             normalize_log_probs=False, \r\n>             batching_type=trtllm.BatchingType.INFLIGHT, \r\n>             scheduler_config=trt_scheduler_config, \r\n>         ), \r\n> ```\r\n> \r\n> Here is an example for tensorrt_llm==0.10.0.dev2024050700\r\n> \r\n> ```\r\n> import argparse \r\n> import logging \r\n> import time \r\n> from datetime import datetime, timedelta \r\n> from pathlib import Path \r\n> from threading import Thread \r\n>  \r\n> import tensorrt_llm \r\n> import tensorrt_llm.bindings.executor as trtllm \r\n> from transformers import PreTrainedTokenizerFast \r\n>  \r\n> logger = logging.getLogger(__name__) \r\n>  \r\n>  \r\n> def tensorrt_llm_executor_worker_path() -> str: \r\n>     worker_path = Path(tensorrt_llm.__file__).parent / 'bin' / 'executorWorker' \r\n>     if not worker_path.exists(): \r\n>         raise Exception(\"TensorRT-LLM executor worker not found\") \r\n>     return str(worker_path) \r\n>  \r\n>  \r\n> def get_trt_parallel_config(): \r\n>     world_size = 2 \r\n>     if world_size > 1: \r\n>         executor_worker_path = tensorrt_llm_executor_worker_path() \r\n>         orchestrator_config = trtllm.OrchestratorConfig(True, executor_worker_path) \r\n>         return trtllm.ParallelConfig( \r\n>             trtllm.CommunicationType.MPI, \r\n>             trtllm.CommunicationMode.ORCHESTRATOR, \r\n>             orchestrator_config=orchestrator_config, \r\n>             # TODO:BIS fix device_ids \r\n>             device_ids=[0, 1], \r\n>         ) \r\n>     else: \r\n>         return trtllm.ParallelConfig(trtllm.CommunicationType.MPI, trtllm.CommunicationMode.LEADER) \r\n>  \r\n>  \r\n> def create_executor(model_path: str) -> trtllm.Executor: \r\n>     trt_parallel_config = get_trt_parallel_config() \r\n>     trt_scheduler_config = trtllm.SchedulerConfig(trtllm.CapacitySchedulerPolicy.GUARANTEED_NO_EVICT) \r\n>  \r\n>     return trtllm.Executor( \r\n>         Path(model_path), \r\n>         trtllm.ModelType.DECODER_ONLY, \r\n>         trtllm.ExecutorConfig( \r\n>             1, \r\n>             parallel_config=trt_parallel_config, \r\n>             normalize_log_probs=False, \r\n>             batching_type=trtllm.BatchingType.INFLIGHT, \r\n>             scheduler_config=trt_scheduler_config, \r\n>         ), \r\n>     ) \r\n>  \r\n>  \r\n> def create_request(input_ids, output_len, eos_id: int, sample_params): \r\n>     output_config = trtllm.OutputConfig(exclude_input_from_output=True) \r\n>     ## This seems to somewhat resolve the issue \r\n>     # sampling_config = trtllm.SamplingConfig(beam_width=1, frequency_penalty=1.0) \r\n>     request = trtllm.Request( \r\n>         input_token_ids=input_ids, \r\n>         max_new_tokens=output_len, \r\n>         streaming=True, \r\n>         output_config=output_config, \r\n>         end_id=eos_id, \r\n>         sampling_config=sample_params, \r\n>     ) \r\n>     return request \r\n>  \r\n>  \r\n> trt_id = None \r\n>  \r\n>  \r\n> def main(): \r\n>     default_prompt = \"You have been tasked with designing a solar-powered water heating system for a residential building. Describe the key components and considerations you would include in your design. Design a five-step workflow.!\" \r\n>     # default_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\nYou have been tasked with designing a solar-powered water heating system for a residential building. Describe the key components and considerations you would include in your design. Design a five-step workflow.!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" \r\n>     parser = argparse.ArgumentParser() \r\n>     parser.add_argument(\"--model_path\", required=False, default=\"./tmp/llama3-8b-tp2-engine\") \r\n>     parser.add_argument(\"--tokenizer_path\", required=False, default=\"/home/scratch.trt_llm_data/llm-models/llama-models-v3/llama-v3-8b-instruct-hf/\") \r\n>     parser.add_argument(\"--prompt\", required=False, default=default_prompt) \r\n>  \r\n>     args = parser.parse_args() \r\n>  \r\n>     tokenizer = PreTrainedTokenizerFast.from_pretrained(args.tokenizer_path) \r\n>     executor = create_executor(args.model_path) \r\n>     prompt = args.prompt \r\n>     prompt_ids = tokenizer.encode(prompt) \r\n>     print(prompt_ids) \r\n>  \r\n>     def do_decode(sampling_config): \r\n>         output_ids = [] \r\n>         finished = False \r\n>         req = create_request(prompt_ids, 150, tokenizer.eos_token_id, sampling_config) \r\n>         _ = executor.enqueue_request(req) \r\n>         while not finished: \r\n>             responses = executor.await_responses(timeout=timedelta(seconds=1)) \r\n>             for r in responses: \r\n>                 if r.has_error(): \r\n>                     raise RuntimeError(r.error_msg) \r\n>                 result = r.result \r\n>                 output_ids.extend(result.output_token_ids[0]) \r\n>                 if result.is_final: \r\n>                     finished = True \r\n>         return tokenizer.decode(output_ids) \r\n>  \r\n> \r\n>     print(do_decode(trtllm.SamplingConfig(beam_width=1, top_k=1, random_seed=1234)))     \r\n>     print(\"===================================\") \r\n>  \r\n>     executor.shutdown() \r\n>  \r\n>  \r\n> if __name__ == \"__main__\": \r\n>     main() \r\n> ```\r\n\r\nCan you tell me the path of this file? I can not find it.\r\nThere are two steps, the first step is \"python convert_checkpoint.py xxx\", the second step is \"trtllm-build xxx\". Then to develop it in Triton server.\r\nWhich step would I need to modify this file?\r\nThank you~\r\n@hijkzzz "
      },
      {
        "user": "hijkzzz",
        "created_at": "2024-06-05T06:50:10Z",
        "body": "> > 1. build engine with --max_batch_size > 1\r\n> > 2. use the flag `=trtllm.BatchingType.INFLIGHT`\r\n> > \r\n> > ```\r\n> >  trtllm.ExecutorConfig( \r\n> >             1, \r\n> >             parallel_config=trt_parallel_config, \r\n> >             normalize_log_probs=False, \r\n> >             batching_type=trtllm.BatchingType.INFLIGHT, \r\n> >             scheduler_config=trt_scheduler_config, \r\n> >         ), \r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > Here is an example for tensorrt_llm==0.10.0.dev2024050700\r\n> > ```\r\n> > import argparse \r\n> > import logging \r\n> > import time \r\n> > from datetime import datetime, timedelta \r\n> > from pathlib import Path \r\n> > from threading import Thread \r\n> >  \r\n> > import tensorrt_llm \r\n> > import tensorrt_llm.bindings.executor as trtllm \r\n> > from transformers import PreTrainedTokenizerFast \r\n> >  \r\n> > logger = logging.getLogger(__name__) \r\n> >  \r\n> >  \r\n> > def tensorrt_llm_executor_worker_path() -> str: \r\n> >     worker_path = Path(tensorrt_llm.__file__).parent / 'bin' / 'executorWorker' \r\n> >     if not worker_path.exists(): \r\n> >         raise Exception(\"TensorRT-LLM executor worker not found\") \r\n> >     return str(worker_path) \r\n> >  \r\n> >  \r\n> > def get_trt_parallel_config(): \r\n> >     world_size = 2 \r\n> >     if world_size > 1: \r\n> >         executor_worker_path = tensorrt_llm_executor_worker_path() \r\n> >         orchestrator_config = trtllm.OrchestratorConfig(True, executor_worker_path) \r\n> >         return trtllm.ParallelConfig( \r\n> >             trtllm.CommunicationType.MPI, \r\n> >             trtllm.CommunicationMode.ORCHESTRATOR, \r\n> >             orchestrator_config=orchestrator_config, \r\n> >             # TODO:BIS fix device_ids \r\n> >             device_ids=[0, 1], \r\n> >         ) \r\n> >     else: \r\n> >         return trtllm.ParallelConfig(trtllm.CommunicationType.MPI, trtllm.CommunicationMode.LEADER) \r\n> >  \r\n> >  \r\n> > def create_executor(model_path: str) -> trtllm.Executor: \r\n> >     trt_parallel_config = get_trt_parallel_config() \r\n> >     trt_scheduler_config = trtllm.SchedulerConfig(trtllm.CapacitySchedulerPolicy.GUARANTEED_NO_EVICT) \r\n> >  \r\n> >     return trtllm.Executor( \r\n> >         Path(model_path), \r\n> >         trtllm.ModelType.DECODER_ONLY, \r\n> >         trtllm.ExecutorConfig( \r\n> >             1, \r\n> >             parallel_config=trt_parallel_config, \r\n> >             normalize_log_probs=False, \r\n> >             batching_type=trtllm.BatchingType.INFLIGHT, \r\n> >             scheduler_config=trt_scheduler_config, \r\n> >         ), \r\n> >     ) \r\n> >  \r\n> >  \r\n> > def create_request(input_ids, output_len, eos_id: int, sample_params): \r\n> >     output_config = trtllm.OutputConfig(exclude_input_from_output=True) \r\n> >     ## This seems to somewhat resolve the issue \r\n> >     # sampling_config = trtllm.SamplingConfig(beam_width=1, frequency_penalty=1.0) \r\n> >     request = trtllm.Request( \r\n> >         input_token_ids=input_ids, \r\n> >         max_new_tokens=output_len, \r\n> >         streaming=True, \r\n> >         output_config=output_config, \r\n> >         end_id=eos_id, \r\n> >         sampling_config=sample_params, \r\n> >     ) \r\n> >     return request \r\n> >  \r\n> >  \r\n> > trt_id = None \r\n> >  \r\n> >  \r\n> > def main(): \r\n> >     default_prompt = \"You have been tasked with designing a solar-powered water heating system for a residential building. Describe the key components and considerations you would include in your design. Design a five-step workflow.!\" \r\n> >     # default_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\nYou have been tasked with designing a solar-powered water heating system for a residential building. Describe the key components and considerations you would include in your design. Design a five-step workflow.!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\" \r\n> >     parser = argparse.ArgumentParser() \r\n> >     parser.add_argument(\"--model_path\", required=False, default=\"./tmp/llama3-8b-tp2-engine\") \r\n> >     parser.add_argument(\"--tokenizer_path\", required=False, default=\"/home/scratch.trt_llm_data/llm-models/llama-models-v3/llama-v3-8b-instruct-hf/\") \r\n> >     parser.add_argument(\"--prompt\", required=False, default=default_prompt) \r\n> >  \r\n> >     args = parser.parse_args() \r\n> >  \r\n> >     tokenizer = PreTrainedTokenizerFast.from_pretrained(args.tokenizer_path) \r\n> >     executor = create_executor(args.model_path) \r\n> >     prompt = args.prompt \r\n> >     prompt_ids = tokenizer.encode(prompt) \r\n> >     print(prompt_ids) \r\n> >  \r\n> >     def do_decode(sampling_config): \r\n> >         output_ids = [] \r\n> >         finished = False \r\n> >         req = create_request(prompt_ids, 150, tokenizer.eos_token_id, sampling_config) \r\n> >         _ = executor.enqueue_request(req) \r\n> >         while not finished: \r\n> >             responses = executor.await_responses(timeout=timedelta(seconds=1)) \r\n> >             for r in responses: \r\n> >                 if r.has_error(): \r\n> >                     raise RuntimeError(r.error_msg) \r\n> >                 result = r.result \r\n> >                 output_ids.extend(result.output_token_ids[0]) \r\n> >                 if result.is_final: \r\n> >                     finished = True \r\n> >         return tokenizer.decode(output_ids) \r\n> >  \r\n> > \r\n> >     print(do_decode(trtllm.SamplingConfig(beam_width=1, top_k=1, random_seed=1234)))     \r\n> >     print(\"===================================\") \r\n> >  \r\n> >     executor.shutdown() \r\n> >  \r\n> >  \r\n> > if __name__ == \"__main__\": \r\n> >     main() \r\n> > ```\r\n> \r\n> Can you tell me the path of this file? I can not find it. There are two steps, the first step is \"python convert_checkpoint.py xxx\", the second step is \"trtllm-build xxx\". Then to develop it in Triton server. Which step would I need to modify this file? Thank you~ @hijkzzz\r\n\r\nyou only need to compile the engine with --max_batch_size > 1."
      }
    ]
  },
  {
    "number": 1715,
    "title": "After deployment, each request exception generates a core.xxxx file ",
    "created_at": "2024-06-03T08:27:32Z",
    "closed_at": "2024-12-04T10:25:33Z",
    "labels": [
      "bug",
      "question",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1715",
    "body": "### System Info\r\n\r\nGPU: NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.3  \r\n\r\n### Who can help?\r\n\r\n@Pzzzzz5142 @fjosw @ami\r\n\r\n### Information\r\n\r\n- [x] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [x] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nStart the service in this directory：sherpa/triton/whisper \r\n\r\n### Expected behavior\r\n\r\nreport bug without core.xxxx files\r\n\r\n### actual behavior\r\n\r\ngenerate too many core.xxxx files, each file is 2.4G. \r\nIf the number of abnormal requests increases, the disk may explode easily.\r\n\r\n### additional notes\r\n\r\nIs there any setting that can prevent the generation of core.XXXX files?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1715/comments",
    "author": "taorui-plus",
    "comments": [
      {
        "user": "nv-guomingz",
        "created_at": "2024-06-03T08:31:59Z",
        "body": "Hi @Shixiaowei02  could u please add some comments here?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-07-18T01:53:32Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-14T07:02:10Z",
        "body": "Hi @taorui-plus do u still have further issue or question now? If not, we'll close it soon.\n"
      }
    ]
  },
  {
    "number": 1691,
    "title": "How does use_paged_context_fmha work?",
    "created_at": "2024-05-28T13:05:25Z",
    "closed_at": "2024-06-07T08:48:52Z",
    "labels": [
      "question",
      "triaged",
      "waiting for feedback"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1691",
    "body": "### System Info\n\nnone\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nnone\n\n### Expected behavior\n\nnone\n\n### actual behavior\n\nnone\n\n### additional notes\n\nWhen I use use_paged_context_fmha to build the engine and deploy it with enable_kv_cache_reuse, even as the QPS increases, the latency does not significantly increase. Why is this happening?\r\nI understand that this can reduce the time consumption of a single request, but I don’t understand why the time consumption of concurrent requests does not increase significantly. Can you provide me with some information?\r\nThank you !\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1691/comments",
    "author": "zhangyu68",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-05-29T01:39:58Z",
        "body": "When you enable the kv cache reuse, TRT-LLM can skip the context phase (prefill phase) to generate kv cache if it receives some cached inputs. So, it could reduce the latency of context phase if the concurrent requests have same inputs. \r\n\r\nIf the concurrent requests use different inputs, it might only because you still not fully use GPU. And the GPU utilization increases when you increase the concurrency. \r\n\r\nIt requires more information to investigate and analyze which is the reason. "
      }
    ]
  },
  {
    "number": 1663,
    "title": "Is there a roadmap to support TRT10.1?",
    "created_at": "2024-05-24T09:05:38Z",
    "closed_at": "2024-11-14T03:50:59Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1663",
    "body": "### System Info\n\n A100\n\n### Who can help?\n\n@n\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nNone\n\n### Expected behavior\n\nSupport TRT10.1\n\n### actual behavior\n\nNewest release depend on trt9.3\n\n### additional notes\n\nNo",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1663/comments",
    "author": "KANGRuipeng",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2024-06-29T01:51:01Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-14T03:50:59Z",
        "body": "Hi @KANGRuipeng we'll update to trt 10.6 in coming 0.15 release\nFeel free to reopen this ticket if needed."
      }
    ]
  },
  {
    "number": 1643,
    "title": "Question regarding RowLinear and ColumnLinear",
    "created_at": "2024-05-21T18:29:54Z",
    "closed_at": "2024-05-23T09:03:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1643",
    "body": "Hi, I would like to know when to use `RowLinear` and `ColumnLinear`. I see it used in conjuction in `mlp.py` and `attention.py` and I'm finding it difficult to know what's the efficient or correct way to identify the correct one to choose when defining custom `Linear` layers.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1643/comments",
    "author": "Ashwin-Ramesh2607",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-05-23T09:03:53Z",
        "body": "The ideas are same to Megatron. `RowLinear` means that the we partition the weights on different GPUs by rows (input feature) when we use TP. `ColumnLinear` menas that we partition the weights on differet GPUs by columns (output feature) when we use TP. "
      }
    ]
  },
  {
    "number": 1631,
    "title": "In the process of quantizing fp8, how to switch the form of quantization, E4M3 or E5M2?",
    "created_at": "2024-05-20T02:46:09Z",
    "closed_at": "2024-05-21T08:17:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1631",
    "body": "### System Info\n\n11\n\n### Who can help?\n\n11\n\n### Information\n\n- [X] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\n11\n\n### Expected behavior\n\n11\n\n### actual behavior\n\n11\n\n### additional notes\n\n11",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1631/comments",
    "author": "fuxuelinwudi",
    "comments": [
      {
        "user": "fuxuelinwudi",
        "created_at": "2024-05-21T03:25:58Z",
        "body": "please answer my question, thx"
      },
      {
        "user": "byshiue",
        "created_at": "2024-05-21T08:17:51Z",
        "body": "In inference, we only use the `e4m3` format and there is not option. "
      }
    ]
  },
  {
    "number": 1623,
    "title": "Increase chunk size while streaming",
    "created_at": "2024-05-17T13:22:34Z",
    "closed_at": "2024-12-04T10:19:19Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1623",
    "body": "Is it possible to increase the amount of tokens sent per chunk during the streaming process and how to do so?\r\n\r\n\r\nThis could also be with triton-inference-server",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1623/comments",
    "author": "avianion",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-05-21T08:26:45Z",
        "body": "I am a little confused your question. Do you want to get more tokens each time in streaming? (Since you use chunk size, I want to make sure it is not related to the `chunked-context` feature)."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-06-21T01:50:53Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      }
    ]
  },
  {
    "number": 1607,
    "title": "Question: when Running Whisper TRT Max 90% GPU Utilization but 20% Tensor Core Utilization ",
    "created_at": "2024-05-15T09:45:52Z",
    "closed_at": "2024-12-04T10:19:57Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1607",
    "body": "I have requestion why is GPU utilization is around 90% but Tensor Core Utilization is less than 25%. When I am running Whisper TRT on A10G card?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1607/comments",
    "author": "shashikr2",
    "comments": [
      {
        "user": "symphonylyh",
        "created_at": "2024-05-22T07:18:10Z",
        "body": "Hi @shashikr2 , do you have some details on how you measured these? And especially at what phase you're observing this, the audio encoder phase, or the decoder phase?"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-06-22T01:50:19Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-14T06:43:25Z",
        "body": "Hi @shashikr2 do u still have further issue or question now? If not, we'll close it soon.\n"
      }
    ]
  },
  {
    "number": 1605,
    "title": "Question- language model onnx files location",
    "created_at": "2024-05-15T07:54:10Z",
    "closed_at": "2024-05-21T08:49:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1605",
    "body": "Hi, I had a general question\r\nFor LLM models eg: llama, when we build the engine from the .safetensors and the json file created in `convert_checkpoint.py` , while building the TRT engine in `tensorrt_llm/commands/build.py`  is the engine directly created from the safetensors or is it converted to some intermediate format, eg : onnx. If so where is this (eg: onnx file) created?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1605/comments",
    "author": "apbose",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-05-21T08:49:28Z",
        "body": "The engine is created from the safetensors. "
      }
    ]
  },
  {
    "number": 1600,
    "title": "Deployment of Pruned Models",
    "created_at": "2024-05-14T12:35:37Z",
    "closed_at": "2024-12-04T10:19:57Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1600",
    "body": "Hi there,\r\n\r\nI just want to ask that for the pruned model, how can we deploy it using TensorRT-LLM? Since the qkv dimensions in each layer are different, the model is stored using torch.save rather than save_pretrained. So I'm a little confused about how to use TensorRT-LLM with this model? Could you please give me some tips or advice?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1600/comments",
    "author": "qianjyM",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-05-15T05:19:43Z",
        "body": "It is not supported to use different dimension in each layer. If you want to run your model, you could implement a new model based on existing model, and set different shape for each layer. It might also affect other parts like the checkpoint converter. "
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-14T06:41:34Z",
        "body": "Hi @qianjyM do u still have further issue or question now? If not, we'll close it soon.\n"
      }
    ]
  },
  {
    "number": 1581,
    "title": "How to build int4_gptq on Mixtral 8x7b",
    "created_at": "2024-05-12T03:20:04Z",
    "closed_at": "2024-06-03T09:06:27Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1581",
    "body": "I use following code to generate the checkpoint:\r\n\r\n```\r\nset -e\r\n\r\nexport MODEL_DIR=/mnt/memory\r\nexport MODEL_NAME=Mixtral-8x7B-Instruct-v0.1\r\nexport LD_LIBRARY_PATH=/usr/local/tensorrt/lib:$LD_LIBRARY_PATH\r\nexport PATH=/usr/local/tensorrt/bin:$PATH\r\nexport PRECISION=int4_gptq_a16\r\nexport QUANTIZE=int4_gptq\r\nexport DTYPE=bfloat16\r\nexport PYTHONPATH=/app/tensorrt-llm:$PYTHONPATH\r\n\r\n\r\npython ../llama/convert_checkpoint.py \\\r\n    --model_dir $MODEL_DIR/${MODEL_NAME} \\\r\n    --output_dir $MODEL_DIR/tmp/trt_models/${MODEL_NAME}/$PRECISION/1-gpu \\\r\n    --dtype $DTYPE \\\r\n    --use_weight_only \\\r\n    --weight_only_precision $QUANTIZE \r\n```\r\n\r\nget error:\r\n```\r\n[TensorRT-LLM] TensorRT-LLM version: 0.10.0.dev2024050700\r\n0.10.0.dev2024050700\r\nTraceback (most recent call last):\r\n  File \"/app/tensorrt-llm/examples/llama/../llama/convert_checkpoint.py\", line 466, in <module>\r\n    main()\r\n  File \"/app/tensorrt-llm/examples/llama/../llama/convert_checkpoint.py\", line 445, in main\r\n    assert args.modelopt_quant_ckpt_path is not None\r\n```\r\nIt looks the convert_checkpoint.py require a parameter for modelopt_quant_ckpt_path. How to generate modelopt_quant_ckpt_path?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1581/comments",
    "author": "gloritygithub11",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-05-15T03:08:43Z",
        "body": "Thank you for the report. GPT-Q is not supported in MoE model. "
      },
      {
        "user": "gloritygithub11",
        "created_at": "2024-05-15T03:52:34Z",
        "body": "Thanks @byshiue for the response. Will it be supported at sometime in future?"
      },
      {
        "user": "byshiue",
        "created_at": "2024-05-17T08:43:51Z",
        "body": "We are working on the feature. We will update here if the feature is supported. "
      }
    ]
  },
  {
    "number": 1577,
    "title": "How to set the initial kv cache length?",
    "created_at": "2024-05-11T09:38:34Z",
    "closed_at": "2024-05-23T07:40:48Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1577",
    "body": "I want to test an example: the initial kv cache length is 2048, and LLM iterate 2048 times, so  the output_tokens=2048, but the initial kv cache  length is 2048, and the final kv cache length is 4096(2048+2048).\r\n\r\nif I run:\r\n```\r\nFT_NVTX=ON /opt/nvidia/nsight-systems/2024.2.1/bin/nsys profile mpirun  -n 8 --allow-run-as-root --oversubscribe ./cpp/build/benchmarks/gptSessionBenchmark --engine_dir ./benchmarks/cpp/temp/engine_out_builddocker_tp8/ --warm_up 1 --batch_size \"64\" --duration 0 --num_runs 1 --input_output_len \"1,2048\"\r\n```\r\nthe initial kv cache length is 1, not 2048. \r\nSo, how to set the initial kv cache length?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1577/comments",
    "author": "liminn",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-05-14T07:12:19Z",
        "body": "You should set `--input_output_len \"2048,2048\"`. "
      },
      {
        "user": "liminn",
        "created_at": "2024-05-17T09:56:58Z",
        "body": "Sorry, I may not have expressed my meaning clearly.\r\nIf I set `-- input_output_len \"2048,2048\"`, then I understand that it includes two part time:\r\n- part 1: one Prefill inference time (input sequence length is 2048, initial kv cache length is 0)\r\n- part 2: **2047 Decoding iteration inference times (input sequence length is actually 1, initial kv cache length is 2048)**, right?\r\n\r\nHowever, I only want to test the inference time of part 2, so how can I set it?"
      },
      {
        "user": "byshiue",
        "created_at": "2024-05-23T07:38:34Z",
        "body": "There is no way to measure that directly. You could use nsys to measure the whole workflow, and calculate the time of part 2 manually. "
      },
      {
        "user": "liminn",
        "created_at": "2024-05-23T07:40:48Z",
        "body": "ok, thanks"
      }
    ]
  },
  {
    "number": 1565,
    "title": "[Quantization] Long latency for generating first token",
    "created_at": "2024-05-09T04:51:45Z",
    "closed_at": "2024-12-04T10:19:52Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1565",
    "body": "## Environment\r\n- RTX8000 GPU\r\n- TensorRT-LLM v0.9.0\r\n## Model\r\n- LLaVA v1.5 7B (LLaMA2 7B)\r\n- fp16 and int8/int4 weight quantization\r\n- batchsize = 16\r\n## Script\r\n- official `examples/multimodal/run.py`\r\n## Who can help?\r\n@Tracin\r\n\r\n## Question\r\nWe measured execution speed for generating a first token and tokens coming after the first one.\r\nCompared with fp16 latency, int8 and int4 latency are about +25% long for the first token. Is it due to casting time of int4/8 to fp16? It is slow than I expected.\r\n\r\nLatency [ms/step]\r\noutput token# | fp16 | int8 | int4\r\n-- | -- | -- | --\r\n1st | 1981.4 | 2687.7 | 2609.2\r\n2nd~last token | 34.6 | 23.3 | 18.0\r\nAvg. total| 4922.4 | 4668.2 | 4139.2\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1565/comments",
    "author": "youki-sada",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-05-10T08:45:33Z",
        "body": "Yes. It is caused by casting. For long context case, it is expected that int8/int4 would be slower than fp16. "
      },
      {
        "user": "youki-sada",
        "created_at": "2024-05-13T10:05:04Z",
        "body": "@byshiue Thank you for your reply. For generating 1st output token, TC utilization (TENSO) of LLaMA w/ int4 WQ is lower than fp16 and also general CNN models. I assume int4/int8 implementation should be utilized weight reuse and casting cost should be much lower.\r\n\r\n## 1st generation (int4 weight)\r\n```\r\n$ dcgmi dmon -i 0 -e 1002,1003,1004,1005 -d 200\r\n#Entity   SMACT        SMOCC        TENSO        DRAMA\r\nID\r\nGPU 0     0.996        0.274        0.622        0.661\r\nGPU 0     0.995        0.284        0.571        0.669\r\nGPU 0     0.996        0.274        0.623        0.658\r\nGPU 0     0.995        0.284        0.572        0.667\r\nGPU 0     0.986        0.277        0.614        0.651\r\n```\r\n## 2nd token generation (int4 weight)\r\n```\r\n$ dcgmi dmon -i 0 -e 1002,1003,1004,1005 -d 200\r\n#Entity   SMACT        SMOCC        TENSO        DRAMA\r\nID\r\nGPU 0     0.912        0.600        0.175        0.775\r\nGPU 0     0.910        0.601        0.174        0.775\r\nGPU 0     0.912        0.603        0.172        0.776\r\nGPU 0     0.916        0.607        0.173        0.777\r\nGPU 0     0.916        0.607        0.171        0.777\r\n```\r\n\r\n## 1st token generation (fp16 weight)\r\n```\r\n#Entity   SMACT        SMOCC        TENSO        DRAMA\r\nID\r\nGPU 0     0.991        0.347        0.656        0.399\r\nGPU 0     0.992        0.357        0.623        0.404\r\nGPU 0     0.992        0.339        0.667        0.400\r\nGPU 0     0.990        0.345        0.659        0.397\r\nGPU 0     0.991        0.355        0.629        0.403\r\n```\r\n## 2nd token generation (fp16 weight)\r\n```\r\n#Entity   SMACT        SMOCC        TENSO        DRAMA\r\nID\r\nGPU 0     0.929        0.312        0.063        0.820\r\nGPU 0     0.929        0.315        0.063        0.821\r\nGPU 0     0.930        0.316        0.063        0.822\r\nGPU 0     0.929        0.316        0.063        0.821\r\nGPU 0     0.928        0.318        0.063        0.820\r\n```"
      },
      {
        "user": "byshiue",
        "created_at": "2024-05-17T08:05:59Z",
        "body": "> For generating 1st output token, TC utilization (TENSO) of LLaMA w/ int4 WQ is lower than fp16 and also general CNN models. I assume int4/int8 implementation should be utilized weight reuse and casting cost should be much lower.\r\n\r\nI don't get the point about the \"general CNN model\". What does it mean? \r\n\r\nAlso, what's the meaning of \"utilized weight reuse\"? "
      },
      {
        "user": "youki-sada",
        "created_at": "2024-05-17T08:22:21Z",
        "body": "> what's the meaning of \"utilized weight reuse\"?\r\n\r\nI meant computational intensity is high in first token inference. Thus, I assume DRAMA of the int4 first inference should be reduced and TENSO should be around 65% as well as fp16. But there's only 10% reduction in memory bandwidth and I think it cannot be explained by casting cost only. Maybe I need to check the CUDA implementation for further discussion."
      },
      {
        "user": "Void1024",
        "created_at": "2024-05-24T07:01:53Z",
        "body": "> Thus, I assume DRAMA of the int4 first inference should be reduced and TENSO should be around 65% as well as fp16. But there's only 10% reduction in memory bandwidth and I think it cannot be explained by casting cost only.\r\n \r\nHi, I'm a little confused. Do you mean that compared to subsequent inference, w4a16's first token will have a higher memory throughput drop than 10%? Or do you think the compute throughput of first token is lower than 65%, which is not in line with expectations?"
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-14T03:35:30Z",
        "body": "Hi @Void1024 do u still have further issue or question now? If not, we'll close it soon.\n"
      },
      {
        "user": "Void1024",
        "created_at": "2024-11-15T06:33:37Z",
        "body": ">do u still have further issue or question now?\nThanks, no question."
      }
    ]
  },
  {
    "number": 1553,
    "title": "AttributeError: 'NoneType' object has no attribute 'name'",
    "created_at": "2024-05-07T11:31:34Z",
    "closed_at": "2024-12-04T10:19:51Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1553",
    "body": "I want to test `OneShot AllReduce` and `TwoShot AllReduce` separately, so I have modified the following code:\r\n```\r\nclass LLaMAModel(Module):\r\n\r\n    def __init__(self, config: PretrainedConfig) -> None:\r\n        super().__init__()\r\n        init_all_reduce_helper()\r\n\r\n        self.mapping = config.mapping\r\n        if self.mapping.is_first_pp_rank():\r\n            self.vocab_embedding = Embedding(config.vocab_size,\r\n                                             config.hidden_size,\r\n                                             dtype=config.dtype)\r\n\r\n        self.layers = DecoderLayerList(LLaMADecoderLayer, config)\r\n\r\n        if self.mapping.is_last_pp_rank():\r\n            self.ln_f = RmsNorm(normalized_shape=config.hidden_size,\r\n                                eps=config.norm_epsilon,\r\n                                dtype=config.dtype)\r\n       \r\n        self.tp_group=config.mapping.tp_group\r\n        \r\n    def forward(\r\n            self,\r\n            input_ids,\r\n            position_ids=None,\r\n            use_cache=False,\r\n            attention_mask=None,\r\n            medusa_position_offsets=None,  # For Medusa support\r\n            medusa_packed_mask=None,  # For Medusa support\r\n            kv_cache_params=None,\r\n            attention_params=None,\r\n            hidden_states=None,\r\n            prompt_embedding_table: Optional[Tensor] = None,\r\n            prompt_tasks: Optional[Tensor] = None,\r\n            prompt_vocab_size: Optional[Tensor] = None,\r\n            lora_params=None):\r\n\r\n        ptuning_args = [\r\n            prompt_embedding_table, prompt_tasks, prompt_vocab_size\r\n        ] if prompt_embedding_table is not None else []\r\n\r\n        if self.mapping.is_first_pp_rank():\r\n            hidden_states = self.vocab_embedding(input_ids, *ptuning_args)\r\n        else:\r\n            hidden_states = recv(hidden_states, self.mapping.prev_pp_rank())\r\n\r\n        ### Test OneShot_AllReduce and TwoShot_AllReduce\r\n        x = Tensor(name='input_ids', dtype=trt.float16, shape=[64, 8192])\r\n        torch.cuda.synchronize()\r\n        for i in range(10):\r\n             #hidden_states = allreduce(x, self.tp_group, strategy=1)\r\n             hidden_states = allreduce(x, self.tp_group, strategy=2)\r\n        torch.cuda.synchronize()\r\n\r\n        return hidden_states\r\n```\r\n\r\nwhen I buid:\r\n```\r\n[05/07/2024-11:22:02] [TRT] [E] 3: [network.cpp::addInput::1973] Error Code 3: API Usage Error (Parameter check failed at: optimizer/api/network.cpp::addInput::1973, condition: inName != knownInput->getName() )\r\nconcurrent.futures.process._RemoteTraceback: \r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\r\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 315, in build_and_save\r\n    engine = build_model(build_config,\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 308, in build_model\r\n    return build(model, build_config)\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/builder.py\", line 732, in build\r\n    model(**inputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/module.py\", line 40, in __call__\r\n    output = self.forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/modeling_utils.py\", line 634, in forward\r\n    hidden_states = self.transformer.forward(**kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/llama/model.py\", line 191, in forward\r\n    x = Tensor(name='input_ids', dtype=trt.float16, shape=[64, 8192])\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/functional.py\", line 226, in __init__\r\n    self.name = name\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/functional.py\", line 248, in name\r\n    self.trt_tensor.name = name\r\nAttributeError: 'NoneType' object has no attribute 'name'\r\n```\r\n\r\nThe reason for the error is due to this line of code:`x = Tensor(name='input_ids', dtype=trt.float16, shape=[64, 8192])` \r\nWhy is this error reported?\r\nIs there any way for me to create a tensorrt tensor for any shape during the `forward` process?\r\n\r\n---------------------------------------\r\nIn addition, I also tried to modify the shape of existing tensor `hidden states` , but none of them were successful. Is there any way to modify the shape of an existing tensor during the `forawrd` inference process?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1553/comments",
    "author": "liminn",
    "comments": [
      {
        "user": "nv-guomingz",
        "created_at": "2024-06-06T14:00:01Z",
        "body": "Hi @QiJune  would u please take a look this question?"
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-14T03:32:19Z",
        "body": "Hi @liminn do u still have further issue or question now? If not, we'll close it soon.\n"
      }
    ]
  },
  {
    "number": 1523,
    "title": "can trtllm-build process on cpu? ",
    "created_at": "2024-04-29T09:02:41Z",
    "closed_at": "2024-04-30T03:43:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1523",
    "body": "### System Info\n\nNVIDIA A800 40G\n\n### Who can help?\n\n@byshiue \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\ncan trtllm-build process on cpu? like parameter load_model_on_cpu in convert_checkpoint.py\n\n### Expected behavior\n\nnone\n\n### actual behavior\n\nnone\n\n### additional notes\n\nnone",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1523/comments",
    "author": "thend-wk",
    "comments": [
      {
        "user": "hijkzzz",
        "created_at": "2024-04-29T14:04:27Z",
        "body": "trtllm-build requires NVIDIA GPUs to timing the kernel perf."
      },
      {
        "user": "thend-wk",
        "created_at": "2024-04-30T01:27:55Z",
        "body": "> trtllm-build requires NVIDIA GPUs to timing the kernel perf.\r\n\r\ni get it, thanks"
      }
    ]
  },
  {
    "number": 1517,
    "title": "Is there any feature related to GPT-like models that can be applied to BERT-like models?",
    "created_at": "2024-04-29T02:42:28Z",
    "closed_at": "2024-11-14T07:25:09Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1517",
    "body": "Is there any fesature related to GPT-like models that can be applied to BERT-like models?\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1517/comments",
    "author": "zhangxin81",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-04-30T03:32:27Z",
        "body": "They have some common optimization idea, like fusing the multi head attention kernel, quantizing the model to int8 or fp8. "
      },
      {
        "user": "Ashwin-Ramesh2607",
        "created_at": "2024-05-16T17:46:37Z",
        "body": "hI @byshiue, a related question. Does `BertAttentionPlugin` also use FlashAttention2 that `GptAttention` uses? "
      },
      {
        "user": "byshiue",
        "created_at": "2024-05-17T07:28:07Z",
        "body": "Yes. "
      }
    ]
  },
  {
    "number": 1503,
    "title": "Is Mistral-7B v0.2 supported?",
    "created_at": "2024-04-26T09:20:53Z",
    "closed_at": "2024-06-03T11:47:52Z",
    "labels": [
      "question",
      "triaged",
      "feature request"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1503",
    "body": "### System Info\n\nA100 GPUs (40GB)\n\n### Who can help?\n\n@byshiue\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nn/a\n\n### Expected behavior\n\nn/a\n\n### actual behavior\n\nn/a\n\n### additional notes\n\nn/a",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1503/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "nv-guomingz",
        "created_at": "2024-06-03T11:47:49Z",
        "body": "Yes, the tensorrt-llm already supported the  Mistral-7B v0.2 even we don't clarify it.\r\nYou may have a try with latest code."
      }
    ]
  },
  {
    "number": 1491,
    "title": "Does Mistral can run on different GPUs(A100 & A5500) ?",
    "created_at": "2024-04-23T13:09:24Z",
    "closed_at": "2024-11-14T08:04:50Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1491",
    "body": "I follow the instruction and can run Mistral with llama on GPU A100 or A5500\r\nbut I don't know how to set tp_size=2 / workers=2 to get the convert_checkpoint config\r\nI try 2 GPUs on 1 device or 2 device(with 1 GPU), both are failed\r\nIf any suggestion, please let me know. Thanks for the reply.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1491/comments",
    "author": "thj08",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-04-24T06:46:28Z",
        "body": "You could try referring the guide of standard llama or other model. To run TP, you should add `--tp_size 2` during converting the ckpt. If you still encounter error, please share your scripts and the full log by following the issue template. "
      },
      {
        "user": "thj08",
        "created_at": "2024-04-24T12:19:12Z",
        "body": "### System Info\r\n- CPU architecture - x86_64\r\n- GPU properties\r\n   - GPU name: NVIDIA A100 memory size: 40G\r\n   - GPU name: NVIDIA A5500 memory size: 24G\r\n- Libraries\r\n  - TensorRT-LLM branch or tag: v0.10.0\r\n  - TensorRT-LLM commit: dev2024041600\r\n  - Container used: yes\r\n- NVIDIA driver version: 550.54.15\r\n- OS: Ubuntu 22.04\r\n\r\n### Who can help?\r\n@byshiue \r\n\r\n### Reproduction\r\npython3 convert_checkpoint.py --model_dir ./mistral --tp_size 2\r\ntrtllm-build --checkpoint_dir ./tllm_checkpoint --output_dir /tmp/fp16 --gemm_plugin float16 --max_input_len 32256\r\npython3 ../run.py --max_output_len=50 --tokenizer_dir ./mistral/ --engine_dir=/tmp/fp16 --max_attention_window_size=4096\r\n\r\n### Expected behavior\r\nrun success\r\n\r\n### actual behavior\r\n[TensorRT-LLM] TensorRT-LLM version: 0.10.0.dev2024041600\r\n[TensorRT-LLM] TensorRT-LLM version: 0.10.0.dev2024041600\r\n[TensorRT-LLM][INFO] Engine version 0.10.0.dev2024041600 found in the config file, assuming engine(s) built by new builder API.\r\n[TensorRT-LLM][INFO] Engine version 0.10.0.dev2024041600 found in the config file, assuming engine(s) built by new builder API.\r\n[TensorRT-LLM][WARNING] [json.exception.type_error.302] type must be string, but is null\r\n[TensorRT-LLM][WARNING] Optional value for parameter quant_algo will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.type_error.302] type must be string, but is null\r\n[TensorRT-LLM][WARNING] Optional value for parameter kv_cache_quant_algo will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found\r\n[TensorRT-LLM][WARNING] [json.exception.type_error.302] type must be string, but is null\r\n[TensorRT-LLM][WARNING] Optional value for parameter quant_algo will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.type_error.302] type must be string, but is null\r\n[TensorRT-LLM][WARNING] Optional value for parameter kv_cache_quant_algo will not be set.\r\n[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found\r\n[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found\r\n[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found\r\n[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.\r\n[TensorRT-LLM][INFO] MPI size: 2, rank: 0\r\n[TensorRT-LLM][INFO] MPI size: 2, rank: 1\r\nTraceback (most recent call last):\r\n  File \"/tmp/k8s/TensorRTLLM/examples/llama/../run.py\", line 564, in <module>\r\n    main(args)\r\n  File \"/tmp/k8s/TensorRTLLM/examples/llama/../run.py\", line 413, in main\r\n    runner = runner_cls.from_dir(**runner_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/runtime/model_runner_cpp.py\", line 184, in from_dir\r\n    session = GptSession(config=session_config,\r\nRuntimeError: [TensorRT-LLM][ERROR] CUDA runtime error in cudaSetDevice(device): invalid device ordinal (/home/jenkins/agent/workspace/LLM/main/L0_MergeRequest/tensorrt_llm/cpp/tensorrt_llm/runtime/utils/sessionUtils.cpp:34)\r\n1       0x7215a5417031 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/libtensorrt_llm.so(+0x68c031) [0x7215a5417031]\r\n2       0x7215a6e972d7 tensorrt_llm::runtime::GptSession::GptSession(tensorrt_llm::runtime::GptSession::Config const&, tensorrt_llm::runtime::GptModelConfig const&, tensorrt_llm::runtime::WorldConfig const&, void const*, unsigned long, std::shared_ptr<nvinfer1::ILogger>) + 487\r\n3       0x721617805a95 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/bindings.cpython-310-x86_64-linux-gnu.so(+0xb8a95) [0x721617805a95]\r\n4       0x7216177b5549 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/bindings.cpython-310-x86_64-linux-gnu.so(+0x68549) [0x7216177b5549]\r\n5       0x721617798737 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/bindings.cpython-310-x86_64-linux-gnu.so(+0x4b737) [0x721617798737]\r\n6       0x5644bebab10e python3(+0x15a10e) [0x5644bebab10e]\r\n7       0x5644beba1a7b _PyObject_MakeTpCall + 603\r\n8       0x5644bebb9acb python3(+0x168acb) [0x5644bebb9acb]\r\n9       0x5644bebba635 _PyObject_Call + 277\r\n10      0x5644bebb6087 python3(+0x165087) [0x5644bebb6087]\r\n11      0x5644beba1e2b python3(+0x150e2b) [0x5644beba1e2b]\r\n12      0x721617797da9 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/bindings.cpython-310-x86_64-linux-gnu.so(+0x4ada9) [0x721617797da9]\r\n13      0x5644beba1a7b _PyObject_MakeTpCall + 603\r\n14      0x5644beb9b150 _PyEval_EvalFrameDefault + 30112\r\n15      0x5644bebb97f1 python3(+0x1687f1) [0x5644bebb97f1]\r\n16      0x5644bebba492 PyObject_Call + 290\r\n17      0x5644beb965d7 _PyEval_EvalFrameDefault + 10791\r\n18      0x5644bebab9fc _PyFunction_Vectorcall + 124\r\n19      0x5644beb9426d _PyEval_EvalFrameDefault + 1725\r\n20      0x5644beb909c6 python3(+0x13f9c6) [0x5644beb909c6]\r\n21      0x5644bec86256 PyEval_EvalCode + 134\r\n22      0x5644becb1108 python3(+0x260108) [0x5644becb1108]\r\n23      0x5644becaa9cb python3(+0x2599cb) [0x5644becaa9cb]\r\n24      0x5644becb0e55 python3(+0x25fe55) [0x5644becb0e55]\r\n25      0x5644becb0338 _PyRun_SimpleFileObject + 424\r\n26      0x5644becaff83 _PyRun_AnyFileObject + 67\r\n27      0x5644beca2a5e Py_RunMain + 702\r\n28      0x5644bec7902d Py_BytesMain + 45\r\n29      0x72178dce8d90 /lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x72178dce8d90]\r\n30      0x72178dce8e40 __libc_start_main + 128\r\n31      0x5644bec78f25 _start + 37\r\n[TensorRT-LLM][INFO] Loaded engine size: 7050 MiB\r\n[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 7210, GPU 7504 (MiB)\r\n[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +2, GPU +10, now: CPU 7212, GPU 7514 (MiB)\r\n[TensorRT-LLM][WARNING] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.2\r\n\r\n### additional notes\r\nnone"
      },
      {
        "user": "byshiue",
        "created_at": "2024-04-25T00:12:33Z",
        "body": "You cannot run on different GPUs by tp."
      },
      {
        "user": "thj08",
        "created_at": "2024-04-25T03:45:27Z",
        "body": "Thank for the reply.\r\nIs there any way can run on different GPUs？\r\ncontainer recognize different GPUs as the same (ex. nvidia.com/gpu), is it possibe to run on different GPUs in container？"
      },
      {
        "user": "thj08",
        "created_at": "2024-04-25T05:18:16Z",
        "body": "Is pp_size also?\r\nI set pp_size=2, it doesn't show any error message, but no more information\r\n\r\n[TensorRT-LLM] TensorRT-LLM version: 0.10.0.dev2024041600\r\n[TensorRT-LLM] TensorRT-LLM version: 0.10.0.dev2024041600\r\n[TensorRT-LLM][INFO] Engine version 0.10.0.dev2024041600 found in the config file, assuming engine(s) built by new builder API.\r\n[TensorRT-LLM][WARNING] [json.exception.type_error.302] type must be string, but is null\r\n[TensorRT-LLM][WARNING] Optional value for parameter quant_algo will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.type_error.302] type must be string, but is null\r\n[TensorRT-LLM][WARNING] Optional value for parameter kv_cache_quant_algo will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found\r\n[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found\r\n[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.\r\n[TensorRT-LLM][INFO] MPI size: 2, rank: 0\r\n[TensorRT-LLM][INFO] Engine version 0.10.0.dev2024041600 found in the config file, assuming engine(s) built by new builder API.\r\n[TensorRT-LLM][WARNING] [json.exception.type_error.302] type must be string, but is null\r\n[TensorRT-LLM][WARNING] Optional value for parameter quant_algo will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.type_error.302] type must be string, but is null\r\n[TensorRT-LLM][WARNING] Optional value for parameter kv_cache_quant_algo will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found\r\n[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.\r\n[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found\r\n[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.\r\n[TensorRT-LLM][INFO] MPI size: 2, rank: 1\r\n[TensorRT-LLM][INFO] Loaded engine size: 6923 MiB\r\n[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 7082, GPU 7502 (MiB)\r\n[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 7083, GPU 7512 (MiB)\r\n[TensorRT-LLM][WARNING] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.2\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO Bootstrap : Using eth0:10.44.0.1<0>\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO NET/Plugin: Loaded coll plugin SHARP (v6)\r\n \r\nTraceback (most recent call last):\r\n  File \"/tmp/k8s/TensorRTLLM/examples/llama/../run.py\", line 564, in <module>\r\n    main(args)\r\n  File \"/tmp/k8s/TensorRTLLM/examples/llama/../run.py\", line 413, in main\r\n    runner = runner_cls.from_dir(**runner_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/runtime/model_runner_cpp.py\", line 184, in from_dir\r\n    session = GptSession(config=session_config,\r\nRuntimeError: [TensorRT-LLM][ERROR] CUDA runtime error in cudaSetDevice(device): invalid device ordinal (/home/jenkins/agent/workspace/LLM/main/L0_MergeRequest/tensorrt_llm/cpp/tensorrt_llm/runtime/utils/sessionUtils.cpp:34)\r\n1       0x704289817031 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/libtensorrt_llm.so(+0x68c031) [0x704289817031]\r\n2       0x70428b2972d7 tensorrt_llm::runtime::GptSession::GptSession(tensorrt_llm::runtime::GptSession::Config const&, tensorrt_llm::runtime::GptModelConfig const&, tensorrt_llm::runtime::WorldConfig const&, void const*, unsigned long, std::shared_ptr<nvinfer1::ILogger>) + 487\r\n3       0x7042fbc05a95 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/bindings.cpython-310-x86_64-linux-gnu.so(+0xb8a95) [0x7042fbc05a95]\r\n4       0x7042fbbb5549 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/bindings.cpython-310-x86_64-linux-gnu.so(+0x68549) [0x7042fbbb5549]\r\n5       0x7042fbb98737 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/bindings.cpython-310-x86_64-linux-gnu.so(+0x4b737) [0x7042fbb98737]\r\n6       0x57e82a1eb10e python3(+0x15a10e) [0x57e82a1eb10e]\r\n7       0x57e82a1e1a7b _PyObject_MakeTpCall + 603\r\n8       0x57e82a1f9acb python3(+0x168acb) [0x57e82a1f9acb]\r\n9       0x57e82a1fa635 _PyObject_Call + 277\r\n10      0x57e82a1f6087 python3(+0x165087) [0x57e82a1f6087]\r\n11      0x57e82a1e1e2b python3(+0x150e2b) [0x57e82a1e1e2b]\r\n12      0x7042fbb97da9 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/bindings.cpython-310-x86_64-linux-gnu.so(+0x4ada9) [0x7042fbb97da9]\r\n13      0x57e82a1e1a7b _PyObject_MakeTpCall + 603\r\n14      0x57e82a1db150 _PyEval_EvalFrameDefault + 30112\r\n15      0x57e82a1f97f1 python3(+0x1687f1) [0x57e82a1f97f1]\r\n16      0x57e82a1fa492 PyObject_Call + 290\r\n17      0x57e82a1d65d7 _PyEval_EvalFrameDefault + 10791\r\n18      0x57e82a1eb9fc _PyFunction_Vectorcall + 124\r\n19      0x57e82a1d426d _PyEval_EvalFrameDefault + 1725\r\n20      0x57e82a1d09c6 python3(+0x13f9c6) [0x57e82a1d09c6]\r\n21      0x57e82a2c6256 PyEval_EvalCode + 134\r\n22      0x57e82a2f1108 python3(+0x260108) [0x57e82a2f1108]\r\n23      0x57e82a2ea9cb python3(+0x2599cb) [0x57e82a2ea9cb]\r\n24      0x57e82a2f0e55 python3(+0x25fe55) [0x57e82a2f0e55]\r\n25      0x57e82a2f0338 _PyRun_SimpleFileObject + 424\r\n26      0x57e82a2eff83 _PyRun_AnyFileObject + 67\r\n27      0x57e82a2e2a5e Py_RunMain + 702\r\n28      0x57e82a2b902d Py_BytesMain + 45\r\n29      0x70449f0afd90 /lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x70449f0afd90]\r\n30      0x70449f0afe40 __libc_start_main + 128\r\n31      0x57e82a2b8f25 _start + 37\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO cudaDriverVersion 12040\r\nNCCL version 2.19.3+cuda12.0\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO P2P plugin IBext\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO NET/IB : No device found.\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO NET/IB : No device found.\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO NET/Socket : Using [0]eth0:10.44.0.1<0>\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO Using non-device net plugin version 0\r\ntrt-deployment-6fc4ffd45-pb29j:1499:1499 [0] NCCL INFO Using network Socket"
      },
      {
        "user": "byshiue",
        "created_at": "2024-04-26T05:49:46Z",
        "body": "No. For TP or PP, it must be using same GPUs now. "
      }
    ]
  },
  {
    "number": 1484,
    "title": "How to convert functional.Tensor to numpy or a list of Tensor ?",
    "created_at": "2024-04-22T12:16:20Z",
    "closed_at": "2024-11-14T08:15:48Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1484",
    "body": "Due to upgrading the version of trtllm from 0.8.0 to 0.9.0, I originally obtained the \"host_max_attention_window_sizes\" as a List in models/generation_mixin.py, but now it is a Tensor, so in order to meet my original interface, I must convert the Tensor to a List or numpy, I can find the function about that, is there any function I can use ?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1484/comments",
    "author": "lzcchl",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-04-24T02:09:08Z",
        "body": "In 0.8.0, although `host_max_attention_window_sizes` is a list, but the list still store the TensorRT-LLM tensors instead of numpy or torch tensor. How do you use these tensors at the time? \r\n\r\nAlso, as a input tensor in TensorRT-LLM, this means that we will pass inputs in `generation.py` by torch tensor, so you should be able find them in the `generation.py` (in python runtime). "
      }
    ]
  },
  {
    "number": 1410,
    "title": "What is the meaning for the benchmark output `tokens_per_sec` and `generation_tokens_per_second`? ",
    "created_at": "2024-04-07T07:43:18Z",
    "closed_at": "2024-04-10T09:11:21Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1410",
    "body": "I run benchmark like this:\r\n```\r\nmpirun -n 2 --allow-run-as-root python benchmark.py \\\r\n    -m llama_13b \\\r\n    --mode plugin \\\r\n    --batch_size \"1;8;16\" \\\r\n    --input_output_len \"710,190\" \\\r\n    --max_input_len 750 --max_output_len 200\r\n```\r\nI got this:\r\n```\r\n[BENCHMARK] model_name llama_13b world_size 2 num_heads 40 num_kv_heads 40 num_layers 40 hidden_size 5120 vocab_size 32000 precision float16 batch_size 1 input_length 710 output_length 190 gpu_peak_mem(gb) 0.0 build_time(s) 116.39 tokens_per_sec 43.09 percentile95(ms) 5120.208 percentile99(ms) 5120.208 latency(ms) 4409.816 compute_cap sm80 quantization QuantMode.0 generation_time(ms) 3751.546 total_generated_tokens 189.0 generation_tokens_per_second 50.379\r\n```\r\n\r\nI see there are two token per sec numbers, which is correct? and what is the meaning for each of them?\r\n\r\nI can't find any documentation mentioning that.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1410/comments",
    "author": "sleepwalker2017",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-04-09T07:57:54Z",
        "body": "`tokens_per_sec` means the throughput of end to end inference. It is computed by `generated_tokens / total_latency`. In your case, it is computed by `190 / 4409.816 * 1000 = 43.09`.\r\n\r\n`generation_tokens_per_second` only consider the generation. It means the thorughput during generation and computed by `generated_tokens / generation_time`. In your case, it is computed by  `189 / 3751.546 * 1000 = 50.379`. "
      },
      {
        "user": "sleepwalker2017",
        "created_at": "2024-04-09T08:02:32Z",
        "body": "> `tokens_per_sec` means the throughput of end to end inference. It is computed by `generated_tokens / total_latency`. In your case, it is computed by `190 / 4409.816 * 1000 = 43.09`.\r\n> \r\n> `generation_tokens_per_second` only consider the generation. It means the thorughput during generation and computed by `generated_tokens / generation_time`. In your case, it is computed by `189 / 3751.546 * 1000 = 50.379`.\r\n\r\nGot it, the lower number includes the prefill stage."
      },
      {
        "user": "YiandLi",
        "created_at": "2024-04-26T07:55:08Z",
        "body": "what about  `gpu_peak_mem` mean ? It is 0 in my case.\r\n"
      },
      {
        "user": "byshiue",
        "created_at": "2024-05-09T07:05:06Z",
        "body": "Could you take a try on latest main branch? "
      }
    ]
  },
  {
    "number": 1409,
    "title": "Could this repo support for cuda11.8? my device couldn't install with cuda12.1",
    "created_at": "2024-04-07T07:08:37Z",
    "closed_at": "2024-06-01T01:52:42Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1409",
    "body": "### System Info\n\nA100 80G, UBUNTU 20.04 LTS \n\n### Who can help?\n\n1\n\n### Information\n\n- [X] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\n1\n\n### Expected behavior\n\n1\n\n### actual behavior\n\n1\n\n### additional notes\n\n1",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1409/comments",
    "author": "fuxuelinwudi",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-04-09T07:50:27Z",
        "body": "It might work, but we don't verify on that and we cannot provide any guarantee. "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-05-17T01:50:00Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-06-01T01:52:42Z",
        "body": "This issue was closed because it has been stalled for 15 days with no activity."
      }
    ]
  },
  {
    "number": 1390,
    "title": "[Question] Question about Memory Usage in TRT-LLM",
    "created_at": "2024-04-02T03:44:17Z",
    "closed_at": "2024-04-07T06:25:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1390",
    "body": "Hi, \r\n\r\nRecently, I quantized Llama model with FP8 precision, I have a question about memory usage when running model: \r\n\r\nWhat dtype of Layernorm use to do computation, FP8, FP16 or FP32?\r\nWhat dtype of token embedding or ROPE embedding use to do computation, FP8, FP16, FP32?\r\n\r\nLook forward you reply, Thanks.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1390/comments",
    "author": "1649759610",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-04-07T06:25:30Z",
        "body": "The computing of layer-norm and ROPE are on FP16 or FP32, depending on the op types and some flags (like `context_fmha_fp32_acc`). "
      }
    ]
  },
  {
    "number": 1319,
    "title": "Are there any way to use FP8 weights, FP16 activations quantization",
    "created_at": "2024-03-19T15:28:54Z",
    "closed_at": "2024-06-07T09:55:39Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1319",
    "body": "### System Info\n\n- H100\r\n- Llama 70B model\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Quantize model weights with FP8 format but not activations\n\n### Expected behavior\n\n1. Activations kept in FP16, but weights in FP8\n\n### actual behavior\n\n1. Only possible to quantize both weights and activations to FP8\n\n### additional notes\n\nMaybe there is a way to enable it but I didn't find it",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1319/comments",
    "author": "ttim",
    "comments": [
      {
        "user": "Tracin",
        "created_at": "2024-03-20T07:03:07Z",
        "body": "I think you can modify code like removing QDQ pattern for activation and use original FP8 checkpoint to build a model."
      },
      {
        "user": "ttim",
        "created_at": "2024-04-09T19:46:05Z",
        "body": "@Tracin Currently I'm using `LLaMAForCausalLM.quantize` method and `trtllm-build` script. What exactly original checkpoint would mean in this case? Is it possible to build engine this way (FP8 weights only quantization basically)? Thank you!"
      },
      {
        "user": "Tracin",
        "created_at": "2024-04-11T08:15:51Z",
        "body": "@ttim Sorry, I think we don't have FP8 x FP16 kernels right now, so you cannot do that."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-06-07T01:52:06Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      }
    ]
  },
  {
    "number": 1317,
    "title": "[New Feature]Does W8A8 currently support mixed precision inference?",
    "created_at": "2024-03-19T09:49:08Z",
    "closed_at": "2024-06-03T10:48:57Z",
    "labels": [
      "question",
      "triaged",
      "Low Precision"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1317",
    "body": "Currently SQ encounters accuracy and diff rate issues\r\n\r\nIs there any good solution?\r\n\r\nDoes TRT_LLM currently support mixed precision? FP16 is used in the Linear layer with high quantization sensitivity, and INT8 is used in others.\r\n\r\nThank you~",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1317/comments",
    "author": "wjj19950828",
    "comments": [
      {
        "user": "wjj19950828",
        "created_at": "2024-03-21T05:46:16Z",
        "body": "@Tracin Do you have some suggestions?\r\n\r\nThanks~"
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-06-03T10:48:57Z",
        "body": "No, we don't support mixed-precision at current stage."
      }
    ]
  },
  {
    "number": 1276,
    "title": "Does lora support other llm besides llama? ",
    "created_at": "2024-03-12T08:14:16Z",
    "closed_at": "2024-11-15T09:29:34Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1276",
    "body": "I saw v0.8.0 support lora. But i just found example of llama. Does lora support other llm? Should we adapt it ourself?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1276/comments",
    "author": "zhaocc1106",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-03-14T06:53:26Z",
        "body": "Currently, TensorRT-LLM only supports LoRA on GPT and LLaMA. "
      },
      {
        "user": "zhaocc1106",
        "created_at": "2024-03-14T14:34:54Z",
        "body": "> Currently, TensorRT-LLM only supports LoRA on GPT and LLaMA.\r\n\r\nAny plan for chatglm?"
      },
      {
        "user": "zzykira",
        "created_at": "2024-03-18T08:53:22Z",
        "body": "> > Currently, TensorRT-LLM only supports LoRA on GPT and LLaMA.\r\n> \r\n> Any plan for chatglm?\r\n\r\nyou can try to merge lora layer to base layer all together, and apply the chatglm conversion. Theoretically, both lora A and lora B are Linear layer, the output of lora is Wa * WbT * x，and the base layer W_base * x + B_base. Therefore, the output is (W_base + Wa * WbT) * x + B_base. You can convert the original W_base to (W_base + Wa * WbT) before applying tensorRT-llm. We tried this way and got similar answer with the model with lora."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-06-11T01:51:58Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      },
      {
        "user": "tongjinle123",
        "created_at": "2024-09-04T07:05:23Z",
        "body": "what about chatglm with multi lora ? "
      },
      {
        "user": "tongjinle123",
        "created_at": "2024-09-04T07:06:44Z",
        "body": "any suggestion about customizing the chatglm/model.py to support lora ? "
      },
      {
        "user": "byshiue",
        "created_at": "2024-11-15T09:29:34Z",
        "body": "LoRA is optimized and it is a general feature now. Please take a try and feel free to open bug if you encounter any issue. "
      }
    ]
  },
  {
    "number": 1269,
    "title": "How do I turn on in-flight batching for inference engines based on LLaMA models in the 0.8.0 release?",
    "created_at": "2024-03-11T18:49:51Z",
    "closed_at": "2024-03-19T15:45:16Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1269",
    "body": "### System Info\n\nN/A\n\n### Who can help?\n\n@juney-nvidia  @ncomly-nvidia \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nHow do I turn on in-flight batching for inference engines based on LLaMA models in the 0.8.0 release?\n\n### Expected behavior\n\nN/A\n\n### actual behavior\n\nN/A\n\n### additional notes\n\nN/A",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1269/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-03-12T03:49:56Z",
        "body": "It should be enabled by default. Do you encounter any issue? "
      },
      {
        "user": "ghost",
        "created_at": "2024-03-14T21:09:39Z",
        "body": "Sorry I meant to ask, *How do I turn **off** in-flight batching?"
      },
      {
        "user": "byshiue",
        "created_at": "2024-03-19T06:36:10Z",
        "body": "You can disable `paged_kv_cache` by `--paged_kv_cache disable` during building engine."
      }
    ]
  },
  {
    "number": 1178,
    "title": "int16 support",
    "created_at": "2024-02-28T06:10:21Z",
    "closed_at": "2024-12-04T10:22:52Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1178",
    "body": "### System Info\n\nLinux centos\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nThx for your excellent work\r\nI wonder Is tensorRT-llm support int 16 quantization\r\n\n\n### Expected behavior\n\nnone\n\n### actual behavior\n\nnone\n\n### additional notes\n\nnone",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1178/comments",
    "author": "dongxuemin666",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-02-29T02:40:13Z",
        "body": "Could you explain the motivation or target to support INT16? From computing view, INT16 should have same peak ops as float16 and bfloat16. Why do we need to use INT16 instread of using float16 or bfloat16? "
      },
      {
        "user": "hello-11",
        "created_at": "2024-11-15T10:19:35Z",
        "body": "@dongxuemin666 Do you still have the question? If not, we will close it soon. "
      }
    ]
  },
  {
    "number": 1114,
    "title": "Does AMMO support QAT training process?",
    "created_at": "2024-02-20T13:41:22Z",
    "closed_at": "2024-11-15T17:38:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1114",
    "body": "as the title, thanks~",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1114/comments",
    "author": "littletomatodonkey",
    "comments": [
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-15T12:25:31Z",
        "body": "+ @laikhtewari for vis"
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-15T17:38:47Z",
        "body": "yes, it does now."
      }
    ]
  },
  {
    "number": 1058,
    "title": "What is the role of TensorRT in TensorRT-LLM?",
    "created_at": "2024-02-06T18:52:18Z",
    "closed_at": "2024-02-09T02:35:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1058",
    "body": "Hi, I am new to TensorRT, but I have experience of using FasterTransformer. \r\n\r\n As I understand, TensorRT-LLM inherits all the features in FasterTransformer, however, in FasterTransformer, TensorRT is not required to run those models. Thus, I am curious about why TensorRT is involved here? And how does it participate in compiling the model?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1058/comments",
    "author": "YJHMITWEB",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-02-09T02:35:18Z",
        "body": "TensorRT-LLM is based on the TensorRT to leverage the optimizer of TensorRT, this is helpful for scalability. At the same time, we use some plugins to support the latest features in short term and enhance the performance, like what we do in FasterTransformer. "
      },
      {
        "user": "YJHMITWEB",
        "created_at": "2024-02-12T18:22:01Z",
        "body": "@byshiue Thanks!"
      }
    ]
  },
  {
    "number": 986,
    "title": "Layernorm plugin to be deprecated",
    "created_at": "2024-01-27T07:44:52Z",
    "closed_at": "2024-03-22T14:42:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/986",
    "body": "Hi, this is continuation of #925 . I had a typo in the question and the Issue couldn't reopen so I am raising another issue.\r\n\r\nI noticed that there is warning of deprecating layernorm plugin. \r\n1. So it means that TRTLLM won't be using the layernorm kernel? \r\n2. If yes, then **which** layernorm kernel will be used in the model instead?\r\n\r\n@Shixiaowei02 @byshiue",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/986/comments",
    "author": "ekagra-ranjan",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-01-30T06:38:45Z",
        "body": "TensorRT-LLM will let TensorRT handle the layernorm kernel in the future. "
      },
      {
        "user": "litaotju",
        "created_at": "2024-03-22T14:42:00Z",
        "body": "Closing this one. Since already answered, TRT-LLM will always try to find the best kernels to be used, whether to be plugin or TRT native kernels."
      }
    ]
  },
  {
    "number": 930,
    "title": "what is the different between the original tensorrt  api and tensorrt llm?",
    "created_at": "2024-01-22T07:21:39Z",
    "closed_at": "2024-01-22T09:39:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/930",
    "body": "trt-llm is better?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/930/comments",
    "author": "henbucuoshanghai",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-01-22T09:39:27Z",
        "body": "TensorRT-LLM is based on TensorRT and add many features/functions about large language model.\r\n\r\nSo, if you want to run large langugae model inference, TensorRT-LLM is suggested. "
      },
      {
        "user": "henbucuoshanghai",
        "created_at": "2024-01-22T11:22:35Z",
        "body": "stable diffusion？SDXL is not supported in 2024 year？I have to do it myself? this repo can be helpful?"
      }
    ]
  },
  {
    "number": 911,
    "title": "Support for number of results per context",
    "created_at": "2024-01-18T08:17:56Z",
    "closed_at": "2024-11-15T20:55:10Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/911",
    "body": "Hi TRT LLM Team, \r\n\r\nA general question about the requests inference API, \r\nIs there an option to get several results per context ? [OAI API \"n\" equivalent]\r\nI couldn't find it in the documentation. If there's no support at the moment, Is it on the team's roadmap?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/911/comments",
    "author": "mzusman",
    "comments": [
      {
        "user": "avshalomman",
        "created_at": "2024-01-18T08:46:50Z",
        "body": "Joining @mzusman's question☝️"
      },
      {
        "user": "byshiue",
        "created_at": "2024-01-19T08:24:34Z",
        "body": "You could use beam search or pass same inputs and set different random seeds for them under sampling. "
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-15T17:23:13Z",
        "body": "hi do u still have further issue or question now? If not, we'll close it soon.\n"
      }
    ]
  },
  {
    "number": 906,
    "title": "Does gptSessionBenchmark benchmark tool support prompt text as input?",
    "created_at": "2024-01-18T02:32:46Z",
    "closed_at": "2024-01-18T06:07:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/906",
    "body": "Usage:\r\n  TensorRT-LLM C++ Runtime Benchmark [OPTION...]\r\n\r\n  -h, --help                    Print usage\r\n  -m, --model arg               Model name specified for engines. (default:\r\n                                gpt_350m)\r\n      --engine_dir arg          Directory that store the engines.\r\n      --batch_size arg          Specify batch size(s) you want to\r\n                                benchmark. Multiple batch sizes can be\r\n                                separated by \";\", example: \"1;8;64\".\r\n                                (default: 8)\r\n      --beam_width arg          Specify beam width you want to benchmark.\r\n                                (default: 1)\r\n      --input_output_len arg    Specify input-output length(s) you want to\r\n                                benchmark. Multiple input lengths can be\r\n                                separated by \";\", example: \"60,20;128,20\".\r\n                                (default: 128,20)\r\n      --log_level arg           Choose log level between\r\n                                verbose/info/warning/error/internal_error.\r\n                                (default: error)\r\n      --warm_up arg             Specify warm up iterations before benchmark\r\n                                starts. (default: 2)\r\n      --num_runs arg            Minimal number of iterations to run during\r\n                                benchmarking. (default: 10)\r\n      --duration arg            Minimal duration of iterations to measure\r\n                                in seconds. (default: 60)\r\n      --ctx_micro_batch_size arg\r\n                                Batch size for context phase.\r\n      --gen_micro_batch_size arg\r\n                                Batch size for generation phase.\r\n      --max_attention_window arg\r\n                                Max kv cache length per sequence.\r\n      --max_tokens_in_paged_kvcache arg\r\n                                Max tokens in paged K-V Cache.\r\n      --kv_cache_free_gpu_mem_fraction arg\r\n                                K-V Cache Free Gpu Mem Fraction.\r\n      --enable_cuda_graph       Execute GPT session with CUDA graph.\r\n      --print_all_logits        Print all context and generation logits.\r\n\n```[tasklist]\n### Tasks\n```\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/906/comments",
    "author": "Fred-cell",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-01-18T06:07:21Z",
        "body": "It is not supported. `gptSessionBenchmark` does not contain the tokenizer. It will use random input ids as inputs."
      }
    ]
  },
  {
    "number": 888,
    "title": "use TensorRT to accelerate the Reward Model based LLM",
    "created_at": "2024-01-16T03:52:09Z",
    "closed_at": "2024-12-04T10:23:32Z",
    "labels": [
      "question",
      "not a bug",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/888",
    "body": "If I want to use tensorRT to accelerate the Reward Model based on Bloom-560m, how should I modify the code?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/888/comments",
    "author": "XuJing1022",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-01-17T01:52:53Z",
        "body": "The question is too abstract. You could try running Bloom-560m by TRT-LLM first, and replace your original Bloom-560m model by TRT-LLM engine. "
      },
      {
        "user": "taoyun951753",
        "created_at": "2024-11-01T03:29:37Z",
        "body": "@XuJing1022  I am also very puzzled about this. Has this problem been solved？"
      }
    ]
  },
  {
    "number": 781,
    "title": "unable to launch tensorrt-llm image",
    "created_at": "2023-12-31T01:22:09Z",
    "closed_at": "2024-12-04T10:17:58Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/781",
    "body": "I USED THE FOLLOWING COMMAD TO BUILD THE IMAGE.\r\n\r\nmake -C release_build \r\nOnce this image is built,\r\nI added  CMD to launch jupyter lab on top of above built image.\r\n\r\nimage build successfully on mac , even i provided the ---platform during build but when i am running it , gave below error.\r\n\r\n\r\nexec /opt/nvidia/nvidia_entrypoint.sh: exec format error",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/781/comments",
    "author": "riyaj8888",
    "comments": [
      {
        "user": "juney-nvidia",
        "created_at": "2024-01-01T11:44:18Z",
        "body": "@riyaj8888 \r\nTo confirm, so you are trying to build and start the docker on a Mac machine, correct?\r\n\r\nJune"
      },
      {
        "user": "hello-11",
        "created_at": "2024-11-18T02:39:05Z",
        "body": "@riyaj8888 Do you still have the problem? If not, we will close it soon. "
      }
    ]
  },
  {
    "number": 775,
    "title": "run.py failed for gpt ",
    "created_at": "2023-12-29T10:53:21Z",
    "closed_at": "2024-12-04T10:13:37Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/775",
    "body": "during Running  run.py for gpt2-medium i am getting following error.\r\n\r\n\r\nRuntimeError: [TensorRT-LLM][ERROR] CUDA runtime error in cub::DeviceSegmentedRadixSort::SortPairsDescending(nullptr, cub_temp_storage_size, log_probs, (T*) nullptr, id_vals, (int*) nullptr, vocab_size * batch_size, batch_size, begin_offset_buf, offset_buf + 1, 0, sizeof(T) * 8, stream): invalid device function (/code/tekit/cpp/tensorrt_llm/kernels/samplingTopPKernels.cu:1077)",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/775/comments",
    "author": "riyaj8888",
    "comments": [
      {
        "user": "juney-nvidia",
        "created_at": "2023-12-30T13:01:05Z",
        "body": "@riyaj8888 \r\n\r\nHi, can you share the concrete steps of reproducing this error?\r\n\r\nI have suspects that there might be some issues with your installation/build process of TensorRT-LLM, after seeing more concrete steps of reproducing it, it is easier for us to provide help.\r\n\r\nJune"
      },
      {
        "user": "riyaj8888",
        "created_at": "2024-01-02T07:20:40Z",
        "body": "My run.py unable to find the config.json .\r\nWhen does it gets created?\r\nDuring build or conversion of ckpts?\r\n\r\n"
      },
      {
        "user": "riyaj8888",
        "created_at": "2024-01-02T07:24:13Z",
        "body": ":/app/tensorrt_llm/examples/gpt$ python3 ../run.py --max_output_len=450 --no_add_special_tokens --engine_dir engine.outputs\r\n[01/02/2024-06:45:01] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml>=11.5.0 to get accurate memory usage\r\nTraceback (most recent call last):\r\nFile \"/app/tensorrt_llm/examples/gpt/../run.py\", line 390, in\r\nmain(args)\r\nFile \"/app/tensorrt_llm/examples/gpt/../run.py\", line 276, in main\r\nmodel_name = read_model_name(args.engine_dir)\r\nFile \"/app/tensorrt_llm/examples/utils.py\", line 54, in read_model_name\r\nengine_version = tensorrt_llm.builder.get_engine_version(engine_dir)\r\nFile \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/builder.py\", line 524, in get_engine_version\r\nwith open(config_path, 'r') as f:"
      },
      {
        "user": "hello-11",
        "created_at": "2024-11-18T02:42:07Z",
        "body": "@riyaj8888 Do you still have the problem? If not, we will close it soon."
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-12-04T10:13:37Z",
        "body": "Feel free to reopen it if needed."
      }
    ]
  },
  {
    "number": 774,
    "title": "convert chatglm2-6b failed",
    "created_at": "2023-12-29T10:51:13Z",
    "closed_at": "2024-12-04T10:15:01Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/774",
    "body": "when I convert chatglm2-6b with A10, there is error as below:\r\nTraceback (most recent call last):\r\n  File \"/code/tensorrt-llm/tensorrt-llm/TensorRT-LLM/examples/chatglm/build.py\", line 895, in <module>\r\n    run_build()\r\n  File \"/code/tensorrt-llm/tensorrt-llm/TensorRT-LLM/examples/chatglm/build.py\", line 887, in run_build\r\n    build(0, args)\r\n  File \"/code/tensorrt-llm/tensorrt-llm/TensorRT-LLM/examples/chatglm/build.py\", line 827, in build\r\n    engine = build_rank_engine(\r\n  File \"/code/tensorrt-llm/tensorrt-llm/TensorRT-LLM/examples/chatglm/build.py\", line 592, in build_rank_engine\r\n    profiler.print_memory_usage(f'Rank {rank} Engine build starts')\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/profiler.py\", line 269, in print_memory_usage\r\n    _default_memory_monitor.print_memory_usage(tag=tag, unit=unit)\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/profiler.py\", line 226, in print_memory_usage\r\n    alloc_device_mem, _, _ = self.device_memory_info(device=device)\r\n  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/profiler.py\", line 175, in device_memory_info\r\n    handle = pynvml.nvmlDeviceGetHandleByIndex(index)\r\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 1651, in nvmlDeviceGetHandleByIndex\r\n    c_index = c_uint(index)\r\nTypeError: 'NoneType' object cannot be interpreted as an integer\r\n\r\nscript is : python build.py -m chatglm2_6b --output_dir /trtModel/",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/774/comments",
    "author": "Fred-cell",
    "comments": [
      {
        "user": "juney-nvidia",
        "created_at": "2023-12-30T13:05:13Z",
        "body": "@Fred-cell \r\nHi,\r\n\r\nWhich commit ID are you using with hitting this error?\r\nAnd how do you install TensorRT-LLM into your environment? \r\n\r\nThanks\r\nJune"
      },
      {
        "user": "hello-11",
        "created_at": "2024-11-18T03:06:19Z",
        "body": "@Fred-cell Do you still have the problem? If not, we will close it soon. "
      }
    ]
  },
  {
    "number": 685,
    "title": "Is GPTQ or AWQ supported on V100?",
    "created_at": "2023-12-18T02:40:26Z",
    "closed_at": "2023-12-18T02:56:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/685",
    "body": "when I run AWQ on V100, it reports error\r\n```\r\n  what():  [TensorRT-LLM][ERROR] Assertion failed: No valid weight only groupwise GEMM tactic(It is usually caused by the failure to execute all candidate configurations of the CUTLASS kernel, please pay attention to the warning information when building the engine.) (/data/TRT-LLM-0.6/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.cpp:378)\r\n```\r\n\r\nwhen run GPTQ it reports:\r\n```\r\nterminate called after throwing an instance of 'tensorrt_llm::common::TllmException'\r\n  what():  [TensorRT-LLM][ERROR] Assertion failed: No valid SQ GEMM tactic (/data/TensorRT-LLM/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.cpp:329\r\n)\r\n1       0x7f7cc5ba2e0b /data/TensorRT-LLM/cpp/build/tensorrt_llm/plugins/libnvinfer_plugin_tensorrt_llm.so.9(+0x35e0b) [0x7f7cc5ba2e0b]\r\n2       0x7f7cc5ba5592 /data/TensorRT-LLM/cpp/build/tensorrt_llm/plugins/libnvinfer_plugin_tensorrt_llm.so.9(+0x38592) [0x7f7cc5ba5592]\r\n3       0x7f7c80d88fc9 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x10aefc9) [0x7f7c80d88fc9]\r\n4       0x7f7c80d4be04 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x1071e04) [0x7f7c80d4be04]\r\n5       0x7f7c80d4d9a0 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x10739a0) [0x7f7c80d4d9a0]\r\n6       0x56374ceade73 benchmarks/gptManagerBenchmark(+0x75e73) [0x56374ceade73]\r\n7       0x56374ceb82b2 benchmarks/gptManagerBenchmark(+0x802b2) [0x56374ceb82b2]\r\n8       0x56374ceb89c2 benchmarks/gptManagerBenchmark(+0x809c2) [0x56374ceb89c2]\r\n9       0x56374cea1a91 benchmarks/gptManagerBenchmark(+0x69a91) [0x56374cea1a91]\r\n10      0x56374cea2bda benchmarks/gptManagerBenchmark(+0x6abda) [0x56374cea2bda]\r\n11      0x7f7cc58f4253 /lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253) [0x7f7cc58f4253]\r\n12      0x7f7cc5664b43 /lib/x86_64-linux-gnu/libc.so.6(+0x94b43) [0x7f7cc5664b43]\r\n13      0x7f7cc56f5bb4 clone + 68\r\n```",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/685/comments",
    "author": "sleepwalker2017",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2023-12-18T02:56:35Z",
        "body": "GPT and AWQ are not supported on V100. "
      }
    ]
  },
  {
    "number": 683,
    "title": "Question regarding using 8 bit gptq quantized models",
    "created_at": "2023-12-17T21:42:14Z",
    "closed_at": "2023-12-18T02:46:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/683",
    "body": "I built the quantized weights using 8 bit GPTQ ( python llama.py ./tmp/llama/7B/ c4 --wbits 8 --true-sequential --groupsize 128 --save_safetensors ./llama-7b-4bit-gs128.safetensors )\r\n\r\n\r\nNow, in order to build engines from checkpoints, I need to use the tags --use_weight_only --weight_only_precision XXXX. I am required to choose from 'int8', 'int4', 'int4_awq', 'int4_gptq' . Can you let me know if int8 is the right option to use ? What is the difference between using int4 int4_gptq int4_awq ? Does the absence of int8_gptq mean that using 8 bit gptq will lead to poor results ?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/683/comments",
    "author": "AbhinavDutta",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2023-12-18T02:46:50Z",
        "body": "For int8, we only support pure int8 only because the accuracy is good enough in our experiement. You couldn't run int8 gptq model by int8 directly. \r\n\r\nint4, int4_gptq and int4_awq are different quantization methods. "
      }
    ]
  },
  {
    "number": 583,
    "title": "Assertion failed: d == a + length (/app/tensorrt_llm/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp:320",
    "created_at": "2023-12-06T05:44:25Z",
    "closed_at": "2023-12-16T15:46:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/583",
    "body": "[TensorRT-LLM][INFO] Loaded engine size: 15448 MiB\r\n[TensorRT-LLM][ERROR] tensorrt_llm::common::TllmException: [TensorRT-LLM][ERROR] Assertion failed: d == a + length (/app/tensorrt_llm/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp:320)\r\n1       0x7f6ce803be0b /opt/tritonserver/backends/tensorrtllm/libnvinfer_plugin_tensorrt_llm.so.9(+0x35e0b) [0x7f6ce803be0b]\r\n2       0x7f6ce809746c tensorrt_llm::plugins::GPTAttentionPluginCommon::GPTAttentionPluginCommon(void const*, unsigned long) + 908\r\n3       0x7f6ce80a9b2d tensorrt_llm::plugins::GPTAttentionPlugin::GPTAttentionPlugin(void const*, unsigned long) + 13\r\n4       0x7f6ce80a9b72 tensorrt_llm::plugins::GPTAttentionPluginCreator::deserializePlugin(char const*, void const*, unsigned long) + 50\r\n5       0x7f6d1ff13c36 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x10a0c36) [0x7f6d1ff13c36]\r\n6       0x7f6d1ff22a8e /usr/local/tensorrt/lib/libnvinfer.so.9(+0x10afa8e) [0x7f6d1ff22a8e]\r\n7       0x7f6d1fead737 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x103a737) [0x7f6d1fead737]\r\n8       0x7f6d1feab81e /usr/local/tensorrt/lib/libnvinfer.so.9(+0x103881e) [0x7f6d1feab81e]\r\n9       0x7f6d1fec252b /usr/local/tensorrt/lib/libnvinfer.so.9(+0x104f52b) [0x7f6d1fec252b]\r\n10      0x7f6d1fec4fa2 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x1051fa2) [0x7f6d1fec4fa2]\r\n11      0x7f6d1fec537c /usr/local/tensorrt/lib/libnvinfer.so.9(+0x105237c) [0x7f6d1fec537c]\r\n12      0x7f6d1fef7051 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x1084051) [0x7f6d1fef7051]\r\n13      0x7f6d1fef7e17 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x1084e17) [0x7f6d1fef7e17]\r\n14      0x7f6d2e8d5594 /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0xed594) [0x7f6d2e8d5594]\r\n15      0x7f6d2e84ef4e /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x66f4e) [0x7f6d2e84ef4e]\r\n16      0x7f6d2e83ec0c /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x56c0c) [0x7f6d2e83ec0c]\r\n17      0x7f6d2e8395f5 /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x515f5) [0x7f6d2e8395f5]\r\n18      0x7f6d2e8374db /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x4f4db) [0x7f6d2e8374db]\r\n19      0x7f6d2e81b182 /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x33182) [0x7f6d2e81b182]\r\n20      0x7f6d2e81b235 TRITONBACKEND_ModelInstanceInitialize + 101\r\n21      0x7f6e2319aa86 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x1a4a86) [0x7f6e2319aa86]\r\n22      0x7f6e2319bcc6 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x1a5cc6) [0x7f6e2319bcc6]\r\n23      0x7f6e2317ec15 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x188c15) [0x7f6e2317ec15]\r\n24      0x7f6e2317f256 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x189256) [0x7f6e2317f256]\r\n25      0x7f6e2318b27d /opt/tritonserver/bin/../lib/libtritonserver.so(+0x19527d) [0x7f6e2318b27d]\r\n26      0x7f6e227f9ee8 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x99ee8) [0x7f6e227f9ee8]\r\n27      0x7f6e2317597b /opt/tritonserver/bin/../lib/libtritonserver.so(+0x17f97b) [0x7f6e2317597b]\r\n28      0x7f6e23185695 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x18f695) [0x7f6e23185695]\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/583/comments",
    "author": "zhaoxjmail",
    "comments": [
      {
        "user": "moseshu",
        "created_at": "2023-12-06T12:37:40Z",
        "body": "> [TensorRT-LLM][INFO] Loaded engine size: 15448 MiB [TensorRT-LLM][ERROR] tensorrt_llm::common::TllmException: [TensorRT-LLM][ERROR] Assertion failed: d == a + length (/app/tensorrt_llm/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp:320) 1 0x7f6ce803be0b /opt/tritonserver/backends/tensorrtllm/libnvinfer_plugin_tensorrt_llm.so.9(+0x35e0b) [0x7f6ce803be0b] 2 0x7f6ce809746c tensorrt_llm::plugins::GPTAttentionPluginCommon::GPTAttentionPluginCommon(void const*, unsigned long) + 908 3 0x7f6ce80a9b2d tensorrt_llm::plugins::GPTAttentionPlugin::GPTAttentionPlugin(void const*, unsigned long) + 13 4 0x7f6ce80a9b72 tensorrt_llm::plugins::GPTAttentionPluginCreator::deserializePlugin(char const*, void const*, unsigned long) + 50 5 0x7f6d1ff13c36 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x10a0c36) [0x7f6d1ff13c36] 6 0x7f6d1ff22a8e /usr/local/tensorrt/lib/libnvinfer.so.9(+0x10afa8e) [0x7f6d1ff22a8e] 7 0x7f6d1fead737 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x103a737) [0x7f6d1fead737] 8 0x7f6d1feab81e /usr/local/tensorrt/lib/libnvinfer.so.9(+0x103881e) [0x7f6d1feab81e] 9 0x7f6d1fec252b /usr/local/tensorrt/lib/libnvinfer.so.9(+0x104f52b) [0x7f6d1fec252b] 10 0x7f6d1fec4fa2 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x1051fa2) [0x7f6d1fec4fa2] 11 0x7f6d1fec537c /usr/local/tensorrt/lib/libnvinfer.so.9(+0x105237c) [0x7f6d1fec537c] 12 0x7f6d1fef7051 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x1084051) [0x7f6d1fef7051] 13 0x7f6d1fef7e17 /usr/local/tensorrt/lib/libnvinfer.so.9(+0x1084e17) [0x7f6d1fef7e17] 14 0x7f6d2e8d5594 /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0xed594) [0x7f6d2e8d5594] 15 0x7f6d2e84ef4e /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x66f4e) [0x7f6d2e84ef4e] 16 0x7f6d2e83ec0c /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x56c0c) [0x7f6d2e83ec0c] 17 0x7f6d2e8395f5 /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x515f5) [0x7f6d2e8395f5] 18 0x7f6d2e8374db /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x4f4db) [0x7f6d2e8374db] 19 0x7f6d2e81b182 /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so(+0x33182) [0x7f6d2e81b182] 20 0x7f6d2e81b235 TRITONBACKEND_ModelInstanceInitialize + 101 21 0x7f6e2319aa86 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x1a4a86) [0x7f6e2319aa86] 22 0x7f6e2319bcc6 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x1a5cc6) [0x7f6e2319bcc6] 23 0x7f6e2317ec15 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x188c15) [0x7f6e2317ec15] 24 0x7f6e2317f256 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x189256) [0x7f6e2317f256] 25 0x7f6e2318b27d /opt/tritonserver/bin/../lib/libtritonserver.so(+0x19527d) [0x7f6e2318b27d] 26 0x7f6e227f9ee8 /usr/lib/x86_64-linux-gnu/libc.so.6(+0x99ee8) [0x7f6e227f9ee8] 27 0x7f6e2317597b /opt/tritonserver/bin/../lib/libtritonserver.so(+0x17f97b) [0x7f6e2317597b] 28 0x7f6e23185695 /opt/tritonserver/bin/../lib/libtritonserver.so(+0x18f695) [0x7f6e23185695]\r\n\r\nchange your version to v0.6.1 ,don't use main branch."
      },
      {
        "user": "byshiue",
        "created_at": "2023-12-07T02:06:43Z",
        "body": "It is often because you build the engine on one commit, and run on another commit, and the parameters of the plugin are different. \r\n\r\nPlease try rebuilding the engine. "
      },
      {
        "user": "byshiue",
        "created_at": "2023-12-16T15:46:55Z",
        "body": "Close this bug because another same issue is resolved. Feel free to ask here if you still have question/issue. We will reopen this issue."
      },
      {
        "user": "wangqy1216",
        "created_at": "2024-04-26T08:08:47Z",
        "body": "> It is often because you build the engine on one commit, and run on another commit, and the parameters of the plugin are different.\r\n> \r\n> Please try rebuilding the engine.\r\n\r\nHow do I know which commit I am running on?"
      },
      {
        "user": "byshiue",
        "created_at": "2024-05-09T07:01:30Z",
        "body": "> > It is often because you build the engine on one commit, and run on another commit, and the parameters of the plugin are different.\r\n> > Please try rebuilding the engine.\r\n> \r\n> How do I know which commit I am running on?\r\n\r\nIn the older version of TensorRT, we don't have way to check that. In newer TensorRT-LLM, you could see the TRT LLM version when you run TRT-LLM. "
      }
    ]
  },
  {
    "number": 581,
    "title": "Best way to deploy/test LLM models on TensorRT-LLM for production",
    "created_at": "2023-12-06T04:50:10Z",
    "closed_at": "2024-12-04T10:16:27Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/581",
    "body": "Hello,\r\nI am using a fine tuned open source LLM and it works great in the Docker after following the instructions to build TensorRT-LLM. \r\n\r\nHowever, after building the wheel install package I am not able to install it on Ubuntu VM, fails at TensorRT installation step.\r\n\r\nCan someone please help with the following:\r\n- What is the recommended way to deploy using TensorRT-LLM?\r\n- If its using Docker/container are there any instructions available to build custom image(with custom model serving logic) on top the one provided by TensorRT-LLM?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/581/comments",
    "author": "amir1m",
    "comments": [
      {
        "user": "juney-nvidia",
        "created_at": "2023-12-07T11:33:37Z",
        "body": "@amir1m \r\n\r\nHi,\r\n\r\nWhich Ubuntu version are you using to install TensorRT-LLM pip package.\r\nAnd recording to the _\"recommended way to deploy using TensorRT-LLM\"_, I want to understand more of this request. Do you mean our suggestion as to how to install TensorRT-LLM in different environments? Or how to set up a LLM service based on TensorRT-LLM?  \r\n\r\n_\"If its using Docker/container are there any instructions available to build custom image(with custom model serving logic) on top the one provided by TensorRT-LLM\"_\r\nSince there are already provided docker file in the github, I think you can just use them as the starting point to add your own docker customization logic. \r\n\r\nHoping this can be helpful to you.\r\n\r\nThanks\r\nJune\r\n\r\n\r\n"
      },
      {
        "user": "amir1m",
        "created_at": "2023-12-07T12:34:09Z",
        "body": "Hi @juney-nvidia ,\r\nI think we need to rebuild the Docker image if we need any more python packages, custom inference code and rebuild it. I was using incorrect command to re-build it. \r\n\r\nIs there a way to run it on Ubuntu 22.04.3 LTS?\r\n\r\nThanks for your response!\r\n\r\nThanks."
      },
      {
        "user": "hello-11",
        "created_at": "2024-11-18T04:37:32Z",
        "body": "@amir1m Do you still have the problem? If not, we will close it soon. "
      }
    ]
  },
  {
    "number": 578,
    "title": "Does TenserRT-LLM support infer on A10 or 4090？",
    "created_at": "2023-12-06T03:21:29Z",
    "closed_at": "2023-12-07T02:30:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/578",
    "body": "just as the title",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/578/comments",
    "author": "wanghao07456",
    "comments": [
      {
        "user": "shiqingzhangCSU",
        "created_at": "2023-12-06T09:23:09Z",
        "body": "A10 is support."
      },
      {
        "user": "byshiue",
        "created_at": "2023-12-07T02:30:49Z",
        "body": "A10 is supported. 4090 should be supported, but we don't verify. "
      }
    ]
  },
  {
    "number": 570,
    "title": "How to run batch inference for llama2",
    "created_at": "2023-12-05T19:48:12Z",
    "closed_at": "2023-12-07T02:37:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/570",
    "body": "Hi team, could you help me in figuring how could I run batch inference for llama2 in python?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/570/comments",
    "author": "tiny-dentist",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2023-12-07T02:37:09Z",
        "body": "You could set `--batch_size` > 1 in summarize.py "
      }
    ]
  },
  {
    "number": 567,
    "title": "Using yi-34B llamafied model",
    "created_at": "2023-12-05T14:39:13Z",
    "closed_at": "2023-12-07T03:01:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/567",
    "body": "can i use example/llama code to use it on yi-34b llamafied model?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/567/comments",
    "author": "MrD005",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2023-12-07T03:01:07Z",
        "body": "Yes. Please let us know if you encoutner any issue. "
      }
    ]
  },
  {
    "number": 563,
    "title": "How to distribute inference using multi-nodes via MPI?",
    "created_at": "2023-12-05T07:27:42Z",
    "closed_at": "2023-12-16T14:47:13Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/563",
    "body": "Hi! How do I need to set up the MPI commands and cluster environment configuration for distributed inference use among multiple nodes?\r\n\r\nFor example, I want to implement pipeline and/or tensor parallelism on two machines with only one gpu each.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/563/comments",
    "author": "delsiiin",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2023-12-07T02:47:50Z",
        "body": "It should be same to single node. "
      },
      {
        "user": "Jeffwan",
        "created_at": "2024-04-05T18:00:00Z",
        "body": "@byshiue for the multi node inference. could you explains whether TP or PP used? Is there a way to use TP and PP together? "
      },
      {
        "user": "byshiue",
        "created_at": "2024-04-07T09:33:21Z",
        "body": "It depends on your hardwares, network bandwidth, model size and problem size. So, I cannot give a simple answer. \r\n\r\nThe high level ideas are: \r\n1. TP requires more communication. So, it requires higher network bandwidht. \r\n2. PP requires larger batch size to separate into micro batches. \r\n\r\nSome extreme case:\r\n1. If you use nvlink and the batch size is 1, TP should be better than PP.\r\n2. If you use pcie and the batch size is 1024, input length is 512, output length is 10, PP should be better than PP. "
      },
      {
        "user": "Jeffwan",
        "created_at": "2024-04-07T17:30:52Z",
        "body": "@byshiue Thanks for the reply. I do have two follow up questions. \r\n\r\n1. Let's say if I have a scenarios requires TP=2, PP=4, the model needs to be split into 8 GPUs. I only have nodes with 4 * A100-40G GPUs. In that case, I see TensorRT-LLM uses MPI for communication. I am new to TensorRT, just curious whether the GPU on the same node (TP part) will communicate with nvLink. and GPUs cross node will fall back to MPI (PP part) cpu communication automatically? \r\n\r\n\r\n2. Why not use `torch.distributed` package to manage the distributed inference? I kind of feel the orchestration is much easier in that case comparing to use MPI directly? `mpi` can be the `torch.distributed` backend as well. "
      },
      {
        "user": "byshiue",
        "created_at": "2024-04-10T09:33:20Z",
        "body": "1. It depends on your hardware. Across nodes, the data can go through infiniband, Roce or pcie. But they called by nccl directly, we don't go through mpi. \r\n2. We don't want the dependency of torch. "
      }
    ]
  },
  {
    "number": 548,
    "title": "Memory utilization",
    "created_at": "2023-12-04T11:21:06Z",
    "closed_at": "2024-11-18T04:40:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/548",
    "body": "model: Llama-2-13b-hf\r\nGPU: A100 40g\r\ncuda version: 12.2\r\nwhen I build  the engine, I use \"cuda_engine.device_memory_size\" found the memory of activation is about 208M. The engine memory utilization is 24825M. kv cache takes up about 3200M. But when I check the total memory usage with the command \"nvidia-smi\", I find that the memory usage is 28791M. What takes up the rest of the 500M of cuda memory? How do I calculate them?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/548/comments",
    "author": "sitabulaixizawaluduo",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2023-12-04T13:19:21Z",
        "body": "Have you checked the memory pool of pytorch? \r\nAlso, pytorch may require some memory to put the workspace and handles. "
      },
      {
        "user": "sitabulaixizawaluduo",
        "created_at": "2023-12-04T13:21:41Z",
        "body": "> Have you checked the memory pool of pytorch? Also, pytorch may require some memory to put the workspace and handles.\r\n\r\nI used c++ runtime, and I found the rest of cuda memory is related to tensor_parallel"
      },
      {
        "user": "byshiue",
        "created_at": "2023-12-05T08:56:26Z",
        "body": "Have you got the answer? Or do you still investigate the reason? "
      },
      {
        "user": "sitabulaixizawaluduo",
        "created_at": "2023-12-05T09:51:44Z",
        "body": "> Have you got the answer? Or do you still investigate the reason?\r\n\r\nI have no idea to find what occupy the memory? I can not see all code which is related to allocate buffer."
      }
    ]
  },
  {
    "number": 529,
    "title": "Is GatedMLP the same as using swiglu as the ACT2FN of a normal MLP? ",
    "created_at": "2023-12-01T22:41:15Z",
    "closed_at": "2023-12-05T00:12:03Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/529",
    "body": "If so, why are there two equivalent implementations instead of merging to one? It seems a bit confusing if they are identical. ",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/529/comments",
    "author": "chenho74",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2023-12-04T13:22:11Z",
        "body": "ACT2FN is only the activation, while GatedMLP is MLP layer. They are different level. "
      },
      {
        "user": "symphonylyh",
        "created_at": "2023-12-05T00:12:03Z",
        "body": "@chenho74 , the mechanism is also different. For normal MLP you usually don't use swiglu. When you use swiglu, MLP will automatically be GatedMLP and the activation func becomes silu.\r\n\r\nNormal MLP only have one projection layer. If you apply swiglu in normal MLP, you're doing gated activation on the output after the FC projection. Whereas in GatedMLP, you have FC and Gate projection layers, you're doing projection on the same hidden_states and then apply element-wise gate operation between them"
      }
    ]
  },
  {
    "number": 511,
    "title": "Why --max_num_tokens  must be associated with enable_context_fmha?",
    "created_at": "2023-11-30T09:02:09Z",
    "closed_at": "2023-11-30T11:05:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/511",
    "body": "I'm using V100 which is Volta architecture. On this platform, --enable_context_fmha is not supported.\r\n\r\nI want to change the max_num_tokens to generate longer sequences. How can I set this?\r\n\r\n```python\r\n   if args.max_num_tokens is not None:\r\n        assert args.enable_context_fmha\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/511/comments",
    "author": "sleepwalker2017",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2023-11-30T11:05:38Z",
        "body": "`max_num_tokens` is only used in fused mha case to determine the workspace size. For unfused case, the workspace size is determined by batch size and context_length directly. "
      },
      {
        "user": "sleepwalker2017",
        "created_at": "2023-11-30T11:08:09Z",
        "body": "> `max_num_tokens` is only used in fused mha case to determine the workspace size. For unfused case, the workspace size is determined by batch size and context_length directly.\r\n\r\nHi, what I'm confused is : How to enable longer sequence generation on V100.\r\n\r\nHow can I disable the following info:\r\n\r\n```\r\n[TensorRT-LLM][WARNING] Number of requested output tokens (513) exceeds maximum output length (512). Number of requested output tokens is changed to 512\r\n```"
      },
      {
        "user": "byshiue",
        "created_at": "2023-12-05T08:57:53Z",
        "body": "This should be related to the input length you setup when you build engine. "
      }
    ]
  },
  {
    "number": 354,
    "title": "inflight_batching",
    "created_at": "2023-11-11T06:04:24Z",
    "closed_at": "2023-12-11T09:30:46Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/354",
    "body": "1. where the source code about infight_batching?\r\n2. What's the difference between the batch mannager and infight_batching. \r\n3. Will you open source about batch mannager?\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/354/comments",
    "author": "wm901115nwpu",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2023-11-13T08:55:24Z",
        "body": "1/2. inflight_batching is put in batch_manager and it is part of batch_manager. \r\n3. We cannot claim anything. \r\n"
      },
      {
        "user": "byshiue",
        "created_at": "2023-12-11T09:30:46Z",
        "body": "Close this bug because the issue is resolved. Feel free to ask here if you still have question/issue, we will reopen the issue.\r\n"
      }
    ]
  },
  {
    "number": 342,
    "title": "How can I build it without docker container?",
    "created_at": "2023-11-10T02:49:54Z",
    "closed_at": "2023-11-23T10:57:08Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/342",
    "body": "I can't install docker on my server, is there any way I can built the project without docker container?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/342/comments",
    "author": "sunchen-1",
    "comments": [
      {
        "user": "ThinkPadRiver",
        "created_at": "2023-11-10T03:44:45Z",
        "body": "of course you can，follow dockerfile.multi，Install dependencies in your server。in my server i build success without docker container"
      },
      {
        "user": "Shixiaowei02",
        "created_at": "2023-11-23T10:57:08Z",
        "body": "Thanks. I'm closing that issue. Feel free to reopen if needed."
      }
    ]
  },
  {
    "number": 331,
    "title": "Build Sucess，But build model error",
    "created_at": "2023-11-09T07:46:27Z",
    "closed_at": "2023-12-11T09:43:29Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/331",
    "body": "In my case，build tensorrtt-llm source code success，but my image runs in a cloud cluster, and the cluster GPU driver is version 470.82. When compiling the model, it prompts that the driver version is too low. I would like to ask, is there any way to make TensorRT-LLM use this version of the driver?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/331/comments",
    "author": "ThinkPadRiver",
    "comments": [
      {
        "user": "juney-nvidia",
        "created_at": "2023-11-09T14:33:55Z",
        "body": "Sorry, but we don't have plan to support lower version of driver due to the limitation of bandwidth.\r\n\r\nJune"
      },
      {
        "user": "byshiue",
        "created_at": "2023-12-11T09:43:30Z",
        "body": "Close this bug because the question is answered. Feel free to ask here if you still have question/issue, we will reopen the issue.\r\n"
      }
    ]
  },
  {
    "number": 308,
    "title": "Will batch manager open source?",
    "created_at": "2023-11-07T14:23:37Z",
    "closed_at": "2023-11-17T02:30:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/308",
    "body": null,
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/308/comments",
    "author": "beginlner",
    "comments": [
      {
        "user": "Shixiaowei02",
        "created_at": "2023-11-08T03:44:42Z",
        "body": "Hello, we have no open source plans at the moment."
      }
    ]
  },
  {
    "number": 284,
    "title": "Quantization preference hierarchy?",
    "created_at": "2023-11-05T05:12:40Z",
    "closed_at": "2023-12-11T09:01:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/284",
    "body": "Hi - wanted to ask a question. I think most folks are familiar with GPTQ & AWQ and relative speeds & quality losses, but int8 weight only (and variants of int8/int4 including with/without smoothquant) as well as fp8 I understand less about and see less in practice.\r\n\r\nWould like to verify what I think is true which is fp8 is the best when supported and is the closest to fp16, doubles throughput and halves memory, while other options like int8 with smoothquant/weight only are lesser preferred and only good if fp8 is not available, and even less so is GPTQ/AWQ which introduce much more significant artifacts and quality loss for reduced memory. Is this true always/most of the time? The benchmarks for Llama especially are for fp8 which suggests to me it's the recommended/standard way to serve them with no real loss of quality from fp16 to fp8?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/284/comments",
    "author": "0xymoro",
    "comments": [
      {
        "user": "0xymoro",
        "created_at": "2023-11-05T06:59:37Z",
        "body": "Also as a note, it seems like a lot of great optimizations like flash attention and in flight batching aren't enabled by default & users have to dig a bit. Is this intentional? For example the code to quantize & build a 70b llama2:\r\n\r\n\r\npython quantize.py --model_dir ./tmp/llama/70B \\\r\n                   --dtype float16 \\\r\n                   --qformat fp8 \\\r\n                   --export_path ./quantized_fp8 \\\r\n                   --calib_size 512 \\\r\n\r\npython build.py --model_dir ./tmp/llama/70B \\\r\n                --quantized_fp8_model_path ./quantized_fp8/llama_tp1_rank0.npz \\\r\n                --dtype float16 \\\r\n                --use_gpt_attention_plugin float16 \\\r\n                --use_gemm_plugin float16 \\\r\n                --output_dir ./tmp/llama/70B/trt_engines/fp8/2-gpu/ \\\r\n                --remove_input_padding \\\r\n                --enable_fp8 \\\r\n                --fp8_kv_cache \\\r\n                --world_size 2 \\\r\n                --tp_size 2\r\n\r\n\r\nIt doesn't seem to be building a production-ready one, which would rather be:\r\n\r\npython build.py --model_dir ./tmp/llama/70B \\\r\n                --quantized_fp8_model_path ./quantized_fp8/llama_tp1_rank0.npz \\\r\n                --dtype float16 \\\r\n                --use_gpt_attention_plugin float16 \\\r\n                --use_gemm_plugin float16 \\\r\n                --output_dir ./tmp/llama/70B/trt_engines/fp8/2-gpu/ \\\r\n                --remove_input_padding \\\r\n                --use_inflight_batching \\    <-----------------------------here\r\n                --paged_kv_cache \\ <-----------------------------here \r\n                --enable_context_fmha \\ <-----------------------------here\r\n                --strongly_typed \\ <-----------------------------here\r\n                --enable_fp8 \\\r\n                --fp8_kv_cache \\\r\n                --world_size 2 \\\r\n                --tp_size 2\r\n\r\nIs there a reason why the example fp8 build doesn't include these 4 I added? Are there drawbacks, since flash attention seems critical for memory constrained environments that would best use fp8?"
      },
      {
        "user": "byshiue",
        "created_at": "2023-12-11T09:01:26Z",
        "body": "For different quantization methods, they have different cons and pros, so users should consider which one is better for their use case. \r\n\r\nFor fp8 example, that's only because it is an example, so we only provide the simplest script to run fp8 example successfully. "
      }
    ]
  },
  {
    "number": 272,
    "title": "Does weight only quantization of llama in tensorrt-llm change maximum allowed input length?",
    "created_at": "2023-11-03T12:25:28Z",
    "closed_at": "2023-11-24T09:10:11Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/272",
    "body": "Got the error:\r\n`ValueError: Couldn't assign past_key_value_0 with shape torch.Size([1, 2, 32, 2747, 128]), engine supports [min, opt, max] = [(1, 2, 32, 0, 128), (4, 2, 32, 1280, 128), (8, 2, 32, 2560, 128)]`\r\n\r\nWhen inputting a text with token length around 2236, and max_output_length = 512. Even though llama's max output length is 4096. ",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/272/comments",
    "author": "Bhuvanesh09",
    "comments": [
      {
        "user": "juney-nvidia",
        "created_at": "2023-11-03T13:58:14Z",
        "body": "@Bhuvanesh09 \r\nHi, \r\n\r\nCan you share the concrete steps to reproduce the errors?\r\nWe will firstly reproduce it in our environment.\r\n\r\nThanks\r\nJune"
      },
      {
        "user": "Lzhang-hub",
        "created_at": "2023-11-04T08:50:57Z",
        "body": "\r\n@Bhuvanesh09 I have same error, and I chang the  `max_input_len` args in the `build.py` from 1024 to 4096, it solved."
      },
      {
        "user": "byshiue",
        "created_at": "2023-11-04T14:32:00Z",
        "body": "@Bhuvanesh09 You should set the correct `max_input_len` and `max_output_len` during building engine. "
      }
    ]
  },
  {
    "number": 268,
    "title": "How to confirm the paged attention is enabled?",
    "created_at": "2023-11-03T08:38:19Z",
    "closed_at": "2024-11-18T06:27:51Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/268",
    "body": "I build the engine using `--paged_kv_cache`, how can I check it's effect?\r\n\r\nUsing `./benchmarks/gptSessionBenchmark`, I can't see any difference.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/268/comments",
    "author": "sleepwalker2017",
    "comments": [
      {
        "user": "juney-nvidia",
        "created_at": "2023-11-03T14:13:17Z",
        "body": "When `paged_kv_cache` is enabled, you are expected to see some logging message like\r\n\r\n> [TensorRT-LLM][INFO] Using xxx tokens in paged KV cache.\r\n\r\nJune"
      },
      {
        "user": "sleepwalker2017",
        "created_at": "2023-11-06T02:22:18Z",
        "body": "> When `paged_kv_cache` is enabled, you are expected to see some logging message like\r\n> \r\n> > [TensorRT-LLM][INFO] Using xxx tokens in paged KV cache.\r\n> \r\n> June\r\n\r\nWhat is the effect of paged attention? For larger batches? I don't know how to see the effect of this feature. Because the memory usage of TRT-LLM is always higher than it really needs."
      },
      {
        "user": "byshiue",
        "created_at": "2023-12-11T09:17:26Z",
        "body": "You could try running inflight batching example because paged attention is necessary in inflight batching. It is hard to see obvious difference in standard workflow (run a request, wait until it finishes, and run a new request)"
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-18T06:27:51Z",
        "body": "Hi @sleepwalker2017 please feel free to reopen this ticket if needed."
      }
    ]
  },
  {
    "number": 223,
    "title": "APIs of forward multiple tokens given kv cache?",
    "created_at": "2023-10-31T18:26:42Z",
    "closed_at": "2024-12-04T10:16:06Z",
    "labels": [
      "question",
      "triaged",
      "stale"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/223",
    "body": "As an example, we can generate from partial kv values, for example:\r\n```\r\nkey: [k1, k2, k3]\r\nvalue: [v1, v2, v3]\r\n```\r\nif it's the regular generation phase, given the new input token t4, we can get q4, and we can get the attention score by `mha([k1, k2, k3], [v1, v2, v3], q4)`.\r\n\r\nI'm wondering in TensorRT-LLM, is it possible to forward on multiple tokens in a single forward pass? Specifically, the new input tokens are a list of tokens `[t4, t5, t6]` and we get the attention score (and eventually logits) by `mha([k1, k2, k3], [v1, v2, v3], [q4, q5, q6])`.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/223/comments",
    "author": "LiuXiaoxuanPKU",
    "comments": [
      {
        "user": "juney-nvidia",
        "created_at": "2023-11-01T11:45:22Z",
        "body": "@LiuXiaoxuanPKU  thanks for bringing this interesting thought to us. Let me confirm my understanding, so you are asking in TensorRT-LLM whether it is possible to generate multiple output tokens in a single decoder generation step, right? \r\n\r\nThanks\r\nJune"
      },
      {
        "user": "LiuXiaoxuanPKU",
        "created_at": "2023-11-01T16:27:58Z",
        "body": "Thanks for the reply. Yeah, maybe more precisely, generate the logits for multiple tokens in a single decoder generation step. Still the example above, it will still only generate one token `t7`, but it will generate the logits of `[t4, t5, t6]` at the same time. "
      },
      {
        "user": "juney-nvidia",
        "created_at": "2023-11-02T11:57:56Z",
        "body": "Interesting idea, currently TensorRT-LLM doesn't directly support such scenario, although I don't see any fundamental blockers of implementing this in TensorRT-LLM since both the attention logics and the generation logics can be customized with enough flexibility.  May I know your concrete thoughts as to the multi logits generation process in a single decoder generation step?  For example, \r\n\r\n```\r\npast_key: [k1, k2, k3]\r\npast_val: [v1, v2, v3]\r\nthe 1st generated token:\r\n   t4\r\nthe second generation step:       \r\n   logits generation for t5\r\n      mha([k1, k2, k3], [v1, v2, v3], q4)\r\n   logits generation for t6\r\n      mha([k1, k2, k3], [v1, v2, v3], (q4, q5))\r\n   logits for t7 and the generated token t7\r\n      mha([k1, k2, k3], [v1, v2, v3], (q4, q5, q6))\r\n```\r\nFor sure the above multi-steps for t5/t6/t7 can be merged as a single step if the dependency can be eliminated with some modeling considerations. \r\n\r\nIs this one of the possible way of implementing what you ask here? \r\n\r\nAnd out of curiosity, it looks that in this way more computations are used with only generating a single token, may I know the consideration of doing this? In case it may relate to some of your ongoing research idea, pls ignore my question to make you feel comfortable. Also, if you have interest to try with TensorRT-LLM for some research work, we are happy to have a dedicated discussion with you to see what we may help. \r\n\r\nJune "
      },
      {
        "user": "LiuXiaoxuanPKU",
        "created_at": "2023-11-02T19:15:52Z",
        "body": "1. Yes, exactly. \r\n\r\n2. It would be very helpful if the tensorRT-LLM team could help and guide the implementation. I could not find your email, please email me (xiaoxuan_liu@berkeley.edu) to set up a dedicated discussion. Appreciate it.\r\n\r\nLily"
      },
      {
        "user": "juney-nvidia",
        "created_at": "2023-11-07T13:50:37Z",
        "body": "> 1. Yes, exactly.\r\n> 2. It would be very helpful if the tensorRT-LLM team could help and guide the implementation. I could not find your email, please email me ([xiaoxuan_liu@berkeley.edu](mailto:xiaoxuan_liu@berkeley.edu)) to set up a dedicated discussion. Appreciate it.\r\n> \r\n> Lily\r\n\r\nSure, already sent out an email to you and pls check it.\r\n\r\nJune"
      },
      {
        "user": "0xbitches",
        "created_at": "2023-11-12T10:32:45Z",
        "body": "This would be quite interesting to our serving architecture as well. Looks like the issue has been triaged, is an update coming soon?"
      },
      {
        "user": "juney-nvidia",
        "created_at": "2023-12-11T14:50:48Z",
        "body": "@0xbitches \r\n\r\nThere was some discussions with @LiuXiaoxuanPKU , although no further feedback from her recently:)\r\n\r\nGoing back to your question, there is already Python binding for TRT-LLM C++ runtime, so you can have a try with current TRT-LLM(based on the main branch) workflow to see whether it is flexible enough to add the features you want. If you feel that there is inflexibility preventing you from adding new features, pls let us know.\r\nThanks\r\nJune"
      },
      {
        "user": "nv-guomingz",
        "created_at": "2024-11-18T15:14:29Z",
        "body": "do u still have further issue or question now? If not, we'll close it soon.\n"
      }
    ]
  },
  {
    "number": 204,
    "title": "Alternative way to specify max capacity for paged KV cache models",
    "created_at": "2023-10-31T00:35:29Z",
    "closed_at": "2023-10-31T00:56:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/204",
    "body": "It seem `engine.device_memory_size` is decided the same way between paged/non-paged KV cache models, using some combinations of batch size, max sequence length etc. This is less ideal given the high variance of input sizes. Is it possible to specify this in some alternative ways like max # of blocks?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/204/comments",
    "author": "yunfeng-scale",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2023-10-31T00:56:56Z",
        "body": "Hi, @yunfeng-scale . Thank you for the report. The workspace of TensorRT engine is pre-allocated during initialization, so it is hard to apply the technique of page kv cache to these buffer for TensorRT. We know memory consumption is a very important topic and we are investigating more way to optimize the memory usage. \r\n\r\nClose this bug. Feel free to open the bug if needed. "
      }
    ]
  },
  {
    "number": 70,
    "title": "How to build Tensorrt-LLM without docker?",
    "created_at": "2023-10-23T07:56:29Z",
    "closed_at": "2023-10-24T13:09:22Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/70",
    "body": null,
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/70/comments",
    "author": "beginlner",
    "comments": [
      {
        "user": "juney-nvidia",
        "created_at": "2023-10-23T08:36:09Z",
        "body": "In theory, as long as you have the required dependency(CUDA, Python, TRT, etc) properly set up, you can build TensorRT-LLM without docker. However, it is still suggested to build TensorRT-LLM with the docker-based approach to reduce the potential tricky environment issue. \r\n\r\nIn our daily development, we mainly build TensorRT-LLM inside docker. \r\n\r\nJune"
      }
    ]
  },
  {
    "number": 1429,
    "title": "Does medusa model here support bs>1?",
    "created_at": "2024-04-09T08:39:59Z",
    "closed_at": "2024-06-07T09:56:35Z",
    "labels": [
      "help wanted",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1429",
    "body": "when I try to infer prompts with bs=2,  the following error raised:\r\n\r\n[04/09/2024-16:36:11] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (an illegal memory access was encountered)\r\n\r\nAs far as i know, it seems to be an index error, which maybe related to the batch size.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1429/comments",
    "author": "boxiaowave",
    "comments": [
      {
        "user": "juney-nvidia",
        "created_at": "2024-04-13T12:15:42Z",
        "body": "@boxiaowave \r\n\r\nCan you share the concrete steps reproducing the issue?\r\n\r\nThanks\r\nJune"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-05-15T01:51:11Z",
        "body": "This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 15 days.\""
      }
    ]
  }
]