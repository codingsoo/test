[
  {
    "number": 20124,
    "title": "Why does the progress bar not show the total steps when using iterable dataset?",
    "created_at": "2024-07-24T11:26:15Z",
    "closed_at": "2024-07-26T23:36:12Z",
    "labels": [
      "question",
      "ver: 2.2.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/20124",
    "body": "### Bug description\n\nThe parameters to Trainer:\r\n```\r\nmax_epochs=-1, max_steps=200000\r\n```\r\nThe progress bar does not show the total iters and the estimated remaining time is also nonsense.\r\n\r\n```\r\nEpoch 1: |                                                             | 43/? [00:26<00:00 ****\r\n```\n\n### What version are you seeing the problem on?\n\nmaster\n\n### How to reproduce the bug\n\n```python\nThe version is 2.3.3\n```\n\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/20124/comments",
    "author": "hiyyg",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-07-26T17:37:56Z",
        "body": "@hiyyg When using iterable datasets, the length can't be determined and therefore the progress bar can't take a total. This is very intentional and not a bug. \r\n\r\nAlso note that when you set:\r\n\r\n```\r\nmax_steps=200000\r\n```\r\n\r\nthat doesn't mean the progress bar can show \"200000\" as the total, because the training in Lightning is epoch based. So 20000 just refers to the total steps taken in the entire training across epochs. \r\n\r\nLet me know if you have any further questions."
      },
      {
        "user": "hiyyg",
        "created_at": "2024-07-26T23:36:12Z",
        "body": "Thanks. I ended up using my own iteration-based training loop."
      },
      {
        "user": "TopCoder2K",
        "created_at": "2025-02-06T13:40:33Z",
        "body": "> When using iterable datasets, the length can't be determined and therefore the progress bar can't take a total. This is very intentional and not a bug.\n\nBut this is really inconvenient since I have to calculate the remaining time by myself... I anyway have to know the total number of steps in one epoch as I run validation more often than once per epoch. @awaelchli, what do you think about adding a special parameter into `Trainer.__init__()` for `IterableDataset`s? Something like `steps_count_per_epoch` or `steps_in_one_epoch`?"
      }
    ]
  },
  {
    "number": 20107,
    "title": "TypeError: on_train_batch_start() takes 3 positional arguments but 4 were given",
    "created_at": "2024-07-20T03:28:39Z",
    "closed_at": "2024-07-20T09:00:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/20107",
    "body": "a solved problem:\r\n\r\nHi, I just started using Lightning and I've been running into this bug lately.\r\n\r\nHere's my code:\r\n\r\n```\r\nfrom typing import Type, Mapping, Callable, Optional, Union, Tuple, List\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom torch import Tensor\r\nfrom torch_geometric.data.storage import recursive_apply\r\nfrom torchmetrics import Metric\r\n\r\nfrom tsl.predictors import Predictor\r\n\r\nclass Imputer(Predictor):\r\n    ...\r\n    def on_train_batch_start(self, batch, batch_idx: int,\r\n                                 unused: Optional[int] = 0) -> None:\r\n        r\"\"\"For every training batch, randomly mask out value with probability\r\n        :math:`p = \\texttt{self.whiten\\_prob}`. Then, whiten missing values in\r\n         :obj:`batch.input.x`\"\"\"\r\n        super(Imputer, self).on_train_batch_start(batch, batch_idx, unused)\r\n        # randomly mask out value with probability p = whiten_prob\r\n        batch.original_mask = mask = batch.input.mask\r\n        p = self.whiten_prob\r\n        if isinstance(p, Tensor):\r\n            p_size = [mask.size(0)] + [1] * (mask.ndim - 1)\r\n            p = p[torch.randint(len(p), p_size)].to(device=mask.device)\r\n        whiten_mask = torch.rand(mask.size(), device=mask.device) > p\r\n        batch.input.mask = mask & whiten_mask\r\n        # whiten missing values\r\n        if 'x' in batch.input:\r\n            batch.input.x = batch.input.x * batch.input.mask\r\n    ...\r\n```\r\nHere's my traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./experiments/run_imputation.py\", line 313, in <module>\r\n    run_experiment(args)\r\n  File \"./experiments/run_imputation.py\", line 286, in run_experiment\r\n    trainer.fit(imputer,\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 543, in fit\r\n    call._call_and_handle_interrupt(\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 579, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _run\r\n    results = self._run_stage()\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1030, in _run_stage\r\n    self.fit_loop.run()\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\r\n    self.advance()\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\r\n    self.epoch_loop.run(self._data_fetcher)\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 140, in run\r\n    self.advance(data_fetcher)\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 234, in advance\r\n    response = call._call_lightning_module_hook(trainer, \"on_train_batch_start\", batch, batch_idx)\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 159, in _call_lightning_module_hook\r\n    output = fn(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/tsl/imputers/imputer.py\", line 134, in on_train_batch_start\r\n    super(Imputer, self).on_train_batch_start(batch, batch_idx, unused)\r\nTypeError: on_train_batch_start() takes 3 positional arguments but 4 were given\r\n\r\n```\r\n\r\nI asked ChatGPT to solve this problem, and it suggested changing:\r\n\r\n\r\ndef on_train_batch_start(self, batch, batch_idx: int, \r\n                         unused: Optional[int] = 0) -> None:\r\nto:\r\n\r\n\r\ndef on_train_batch_start(self, batch, batch_idx: int, dataloader_idx=None,\r\n                         unused: Optional[int] = 0) -> None:\r\nBut this caused a new issue. Here's the new traceback:\r\n\r\nTraceback (most recent call last):\r\n  File \"./experiments/run_imputation.py\", line 313, in <module>\r\n    run_experiment(args)\r\n  File \"./experiments/run_imputation.py\", line 286, in run_experiment\r\n    trainer.fit(imputer,\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 543, in fit\r\n    call._call_and_handle_interrupt(\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 579, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _run\r\n    results = self._run_stage()\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1030, in _run_stage\r\n    self.fit_loop.run()\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\r\n    self.advance()\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\r\n    self.epoch_loop.run(self._data_fetcher)\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 140, in run\r\n    self.advance(data_fetcher)\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 234, in advance\r\n    response = call._call_lightning_module_hook(trainer, \"on_train_batch_start\", batch, batch_idx)\r\n  File \"/root/anaconda3/envs/spin/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 159, in _call_lightning_module_hook\r\n    output = fn(*args, **kwargs)\r\nTypeError: on_train_batch_start() missing 1 required positional argument: 'dataloader_idx'\r\nI'm confused! Could you please help me resolve this issue? Thank you!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.2\r\n\r\n### How to reproduce the bug\r\n\r\nOh! I solved it!\r\nThe super(Imputer, self).on_train_batch_start(batch, batch_idx, unused)\r\nwas calling the parent class's function instead of the one defined in the subclass.\r\nBy tracing the inheritance hierarchy, I found that the on_train_batch_start function in the parent class's parent class's parent is defined as follows:\r\ndef on_train_batch_start(self, batch: Any, batch_idx: int) -> Optional[int]:\r\nThis means I just needed to remove the unused parameter in the call to resolve the issue.\r\n\r\n\r\n\r\n### Error messages and logs\r\n\r\n```\r\n# Error messages and logs here please\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/20107/comments",
    "author": "jkkjjj",
    "comments": [
      {
        "user": "jkkjjj",
        "created_at": "2024-07-20T03:28:52Z",
        "body": "Python: 3.12.4\r\nPyTorch: 2.3.1+cu121\r\nPyTorch Lightning: 2.3.3"
      },
      {
        "user": "jkkjjj",
        "created_at": "2024-07-20T07:41:03Z",
        "body": "Oh! I solved it! \r\nThe `super(Imputer, self).on_train_batch_start(batch, batch_idx, unused)` \r\nwas calling the parent class's function instead of the one defined in the subclass.\r\nBy tracing the inheritance hierarchy, I found that the `on_train_batch_start` function in the parent class's parent class's parent is defined as follows:\r\n`def on_train_batch_start(self, batch: Any, batch_idx: int) -> Optional[int]:`\r\nThis means I just needed to remove the `unused` parameter in the call to resolve the issue.\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2024-07-20T09:00:53Z",
        "body": "Yes exactly, the extra `unused` parameter was the problem. You can just remove it. This was probably put there as a placeholder from an older version of Lightning. "
      }
    ]
  },
  {
    "number": 20101,
    "title": "pl.TrainResult not found in 2.3.3",
    "created_at": "2024-07-18T18:07:26Z",
    "closed_at": "2024-07-18T21:48:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/20101",
    "body": "### Bug description\n\nDocumentation I'm looking at says to use `pl.TrainResult`, but I get an attribute error: module 'pytorch_lightning' has no attribute 'TrainResult'. Other issues I see on here seem to specify that I should be using 0.90 or above, which is satisfied since I'm on 2.3.3. Has TrainResult been removed?\n\n### What version are you seeing the problem on?\n\nv2.2, master\n\n### How to reproduce the bug\n\n```python\nimport pytorch_lightning as pl\r\nprint(pl.__version__)\r\npl.TrainResult\n```\n\n\n### Error messages and logs\n\n```\r\n2.3.3\r\nTraceback (most recent call last):\r\n  File \"/local/home/mkulshr/test.py\", line 3, in <module>\r\n    pl.TrainResult\r\nAttributeError: module 'pytorch_lightning' has no attribute 'TrainResult'\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.3.3\r\n#- PyTorch Version (e.g., 2.0): 2.3.0\r\n#- Python version (e.g., 3.9): 3.11.9\r\n#- OS (e.g., Linux): Linux\r\n#- CUDA/cuDNN version: 12.1\r\n#- GPU models and configuration: 8 Tesla V100-SXM2-16GBs\r\n#- How you installed Lightning(`conda`, `pip`, source): conda\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/20101/comments",
    "author": "manavkulshrestha",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-07-18T21:48:25Z",
        "body": "There aren't any references to `pl.TrainResult` in the docs of recent version of Lightning. If you are reading the docs on our official page, you will see a version number in the top left. This must match the version you have installed. It's been over 2 years since `pl.TrainResult` has been removed. This was before Lightning was even officially stable (1.0). I strongly recommend to upgrade the code."
      }
    ]
  },
  {
    "number": 20086,
    "title": "module statistics has no attribute mean",
    "created_at": "2024-07-15T07:18:01Z",
    "closed_at": "2024-07-15T11:51:42Z",
    "labels": [
      "question",
      "ver: 2.2.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/20086",
    "body": "### Bug description\r\n\r\nWhen updating to the new PyTorch-lightning version 2.3.3 and using the MlFlowLogger as logger arg in the trainer I‘m getting the error trace (see error section)\r\n\r\n\r\n### What version are you seeing the problem on?\r\n\r\nmaster\r\n\r\n### How to reproduce the bug\r\n\r\n_No response_\r\n\r\n### Error messages and logs\r\n\r\n```\r\ntrain.py 9 <module>\r\nimport mlflow.pytorch\r\n \r\n__init__.py 1190 <module>\r\nfrom mlflow.pytorch._lightning_autolog import MlflowModelCheckpointCallback  # noqa: F401\r\n \r\n_lightning_autolog.py 24 <module>\r\nimport pytorch_lightning as pl\r\n \r\n__init__.py 27 <module>\r\nfrom pytorch_lightning.callbacks import Callback  # noqa: E402\r\n \r\n__init__.py 29 <module>\r\nfrom pytorch_lightning.callbacks.pruning import ModelPruning\r\n \r\npruning.py 32 <module>\r\nfrom pytorch_lightning.core.module import LightningModule\r\n \r\n__init__.py 16 <module>\r\nfrom pytorch_lightning.core.module import LightningModule\r\n \r\nmodule.py 62 <module>\r\nfrom pytorch_lightning.loggers import Logger\r\n \r\n__init__.py 14 <module>\r\nfrom pytorch_lightning.loggers.comet import CometLogger\r\n \r\ncomet.py 30 <module>\r\nfrom pytorch_lightning.loggers.logger import Logger, rank_zero_experiment\r\n \r\nlogger.py 103 <module>\r\ndefault_func: Callable[[Sequence[float]], float] = statistics.mean,\r\n \r\nAttributeError:\r\nmodule 'statistics' has no attribute 'mean'\r\n\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.3.3\r\n#- PyTorch Version (e.g., 2.0): 2.3.1\r\n#- Python version (e.g., 3.9): 3.11\r\n#- OS (e.g., Linux): MacOS Sonoma 14.5\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source): pip\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/20086/comments",
    "author": "FabianKuon",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-07-15T10:28:22Z",
        "body": "Hey @FabianKuon \r\nThe statistics module is a standard module in Python and it definitely has a mean function. Could you please check that you don't have a different statistics module in the python path? For example, if you have a `statistics.py` module in your code, it would cause a collision with the standard library package. In this case, please rename or delete it. "
      },
      {
        "user": "FabianKuon",
        "created_at": "2024-07-15T11:51:42Z",
        "body": "@awaelchli thank you very much for your quick response. You‘re right I do have a statistics.py file in my repo. Renaming that solved the issue. \r\n\r\nOnce again thank you very much "
      },
      {
        "user": "awaelchli",
        "created_at": "2024-07-15T14:04:16Z",
        "body": "Perfect!"
      }
    ]
  },
  {
    "number": 20020,
    "title": "Teardown trying to copy \"meta\" tensors",
    "created_at": "2024-06-27T08:19:33Z",
    "closed_at": "2024-06-27T21:48:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/20020",
    "body": "### Bug description\n\nI've got a model template that I'm using with torch.vmap, and to use it I need to store a meta model in my lightning module. However lightning keeps trying to copy the meta model to devices / during teardown etc... This results in an error for the meta tensor, since it does not have a copy method. Any good way to work around this?\r\n\r\nThis code generates my MLP's template, and during any lightning method that copies things, it dies when ```self.base_model.copy()``` or ```self.base_model.to(\"device\")``` is called.\r\n\r\n```python\r\nself.base_model = copy.deepcopy(self.mlp)\r\nself.base_model.to('meta')\r\n```\n\n### What version are you seeing the problem on?\n\nv2.2\n\n### How to reproduce the bug\n\n```python\nRun any lightning model with a meta model as a lightning property\r\n\r\n\r\nself.base_model = copy.deepcopy(self.mlp)\r\nself.base_model.to('meta')\n```\n\n\n### Error messages and logs\n\n```\r\nNotImplementedError: Cannot copy out of meta tensor; no data!\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.2\r\n#- Lightning App Version (e.g., 0.5.2): \r\n#- PyTorch Version (e.g., 2.0): 2.2\r\n#- Python version (e.g., 3.9): 3.10\r\n#- OS (e.g., Linux): Windows\r\n#- CUDA/cuDNN version: 12.1\r\n#- GPU models and configuration: 3090\r\n#- How you installed Lightning(`conda`, `pip`, source): conda\r\n#- Running environment of LightningApp (e.g. local, cloud): local\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/20020/comments",
    "author": "kvndhrty",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-06-27T18:35:26Z",
        "body": "Hey @kvndhrty \r\nI think a pretty easy way to work around this is to not register your meta-template model as a submodule. You can easily do that by packing it into a list:\r\n```py\r\ndef __init__(self):\r\n    super().__init__()\r\n    with torch.device(\"meta\"):\r\n        self._template_model = [TemplateModel()]\r\n        \r\n    # then access it like so in your other code: \r\n    self._template_model[0]\r\n    \r\n    # ... or write a getter to return you the template model without indexing\r\n```\r\n\r\nI think that the assumption Lightning makes about your model not being on the meta device after training is a reasonable one. Even so before training, since eventually Lightning moves the model to GPU before training. I think it would become quite complex if we had to add logic to ignore such submodules on the meta-device. More so, it would be error-prone, because meta-device initialization is needed for large model training.\r\nSo I would like to suggest we don't treat this as a bug.\r\n\r\nOne other thing you could do is ask yourself whether it is even necessary to have your template model as an attribute at all. Since the creation on meta-device is basically free, you could also just do that on-the-fly whenever you need that. Get the properties you need and store them somewhere. Then you don't need to keep that template model around.\r\n    "
      },
      {
        "user": "kvndhrty",
        "created_at": "2024-06-27T21:48:58Z",
        "body": "@awaelchli I think that is entirely reasonable, I'll pack my module into a list for now. The small re-factor required to init the meta module each time isn't something I'll do this week, but maybe in the near future.\r\n\r\nThank you for the quick response! "
      }
    ]
  },
  {
    "number": 19873,
    "title": "AttributeError: module 'pytorch_lightning.callbacks' has no attribute 'ProgressBarBase'. Did you mean: 'ProgressBar'?",
    "created_at": "2024-05-16T02:10:17Z",
    "closed_at": "2024-06-22T18:42:28Z",
    "labels": [
      "question",
      "progress bar: tqdm"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19873",
    "body": "### Bug description\n\nI'm trying to add a custom progress bar (basically the intention is to suppress progress bar display completely), but the following error is thrown:\r\n```\r\n2024-05-16 10:02:14 - [worker tcp://127.0.0.1:40955] - marten.data.etl - ERROR - module 'pytorch_lightning.callbacks' has no attribute 'ProgressBarBase'\r\nTraceback (most recent call last):\r\n  File \"/home/jx/git/marten/src/marten/models/worker_func.py\", line 381, in log_metrics_for_hyper_params\r\n    _, metrics = train(\r\n                 ^^^^^^\r\n  File \"/home/jx/git/marten/src/marten/models/worker_func.py\", line 298, in train\r\n    for attempt in Retrying(\r\n  File \"/home/jx/.pyenv/versions/3.12.2/envs/venv_3.12.2/lib/python3.12/site-packages/tenacity/__init__.py\", line 347, in __iter__\r\n    do = self.iter(retry_state=retry_state)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jx/.pyenv/versions/3.12.2/envs/venv_3.12.2/lib/python3.12/site-packages/tenacity/__init__.py\", line 314, in iter\r\n    return fut.result()\r\n           ^^^^^^^^^^^^\r\n  File \"/home/jx/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\r\n    return self.__get_result()\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jx/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\r\n    raise self._exception\r\n  File \"/home/jx/git/marten/src/marten/models/worker_func.py\", line 305, in train\r\n    m, metrics = _try_fitting(df,\r\n                 ^^^^^^^^^^^^^^^^\r\n  File \"/home/jx/git/marten/src/marten/models/worker_func.py\", line 252, in _try_fitting\r\n    metrics = m.fit(\r\n              ^^^^^^\r\n  File \"/home/jx/.pyenv/versions/3.12.2/envs/venv_3.12.2/lib/python3.12/site-packages/neuralprophet/forecaster.py\", line 1085, in fit\r\n    metrics_df = self._train(\r\n                 ^^^^^^^^^^^^\r\n  File \"/home/jx/.pyenv/versions/3.12.2/envs/venv_3.12.2/lib/python3.12/site-packages/neuralprophet/forecaster.py\", line 2757, in _train\r\n    self.trainer, checkpoint_callback = utils.configure_trainer(\r\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jx/.pyenv/versions/3.12.2/envs/venv_3.12.2/lib/python3.12/site-packages/neuralprophet/utils.py\", line 961, in configure_trainer\r\n    and any(isinstance(callback, pl.callbacks.ProgressBarBase) for callback in config[\"callbacks\"])\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jx/.pyenv/versions/3.12.2/envs/venv_3.12.2/lib/python3.12/site-packages/neuralprophet/utils.py\", line 961, in <genexpr>\r\n    and any(isinstance(callback, pl.callbacks.ProgressBarBase) for callback in config[\"callbacks\"])\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: module 'pytorch_lightning.callbacks' has no attribute 'ProgressBarBase'. Did you mean: 'ProgressBar'?\r\n```\r\n\n\n### What version are you seeing the problem on?\n\nv2.2\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n_No response_\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19873/comments",
    "author": "carusyte",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-06-22T18:42:28Z",
        "body": "@carusyte From Lightning 1.9 to 2.0, this base class changed the name. It's just called `ProgressBar` now."
      }
    ]
  },
  {
    "number": 19862,
    "title": "Add dog has an error: FileNotFoundError:",
    "created_at": "2024-05-13T11:23:29Z",
    "closed_at": "2024-08-03T19:52:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19862",
    "body": "### Bug description\n\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:\\codes2\\GaussianEditor\\launch.py\", line 252, in <module>\r\n    main(args, extras)\r\n  File \"D:\\codes2\\GaussianEditor\\launch.py\", line 195, in main\r\n    trainer.fit(system, datamodule=dm, ckpt_path=cfg.resume)\r\n  File \"D:\\softwares\\anaconda3\\envs\\gsEditor\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 544, in fit\r\n    call._call_and_handle_interrupt(\r\n  File \"D:\\softwares\\anaconda3\\envs\\gsEditor\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"D:\\softwares\\anaconda3\\envs\\gsEditor\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 581, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"D:\\softwares\\anaconda3\\envs\\gsEditor\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 969, in _run\r\n    call._call_lightning_module_hook(self, \"on_fit_start\")\r\n  File \"D:\\softwares\\anaconda3\\envs\\gsEditor\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py\", line 157, in _call_lightning_module_hook\r\n    output = fn(*args, **kwargs)\r\n  File \"D:\\softwares\\anaconda3\\envs\\gsEditor\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"D:\\codes2\\GaussianEditor\\threestudio\\systems\\GassuianEditorAdd.py\", line 121, in on_fit_start\r\n    p1 = subprocess.Popen(\r\n  File \"D:\\softwares\\anaconda3\\envs\\gsEditor\\lib\\subprocess.py\", line 951, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"D:\\softwares\\anaconda3\\envs\\gsEditor\\lib\\subprocess.py\", line 1438, in _execute_child\r\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\r\nFileNotFoundError: [WinError 2] 系统找不到指定的文件。\r\n\r\n```\r\nwe had an error : FileNotFoundError: [WinError 2] 系统找不到指定的文件。But it doesn't show which file .\r\nCan you please take a look?\r\n\n\n### What version are you seeing the problem on?\n\nmaster\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19862/comments",
    "author": "BaiYuanxi-dev",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2024-05-13T13:43:50Z",
        "body": "Your issue happens in code that doesn't come from Lightning:\r\n\r\n```python\r\n  File \"D:\\codes2\\GaussianEditor\\threestudio\\systems\\GassuianEditorAdd.py\", line 121, in on_fit_start\r\n```\r\n\r\nThere you will find the `on_fit_start` hook defined. This is what is running a subprocess call.\r\n\r\nYou can try debugging that."
      }
    ]
  },
  {
    "number": 19580,
    "title": "An invalid dataloader was passed to `Trainer.predict(dataloaders=...)`",
    "created_at": "2024-03-06T02:24:06Z",
    "closed_at": "2024-03-08T23:18:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19580",
    "body": "### Bug description\r\n\r\nI am trying to predict on one dataset and I have defined the datamodule. However, when I call the tester.predict(...) function, it keeps producing error:\r\n\r\n```\r\nTypeError: An invalid dataloader was passed to `Trainer.predict(dataloaders=...)`. Found <utils.CustomDatamodule object at 0x7f8bb66feb60>.\r\n```\r\n\r\nWouldn't just pass in one of datamodule or dataloaders? Why do I keep getting dataloader errors after passing in datamodule?\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.2\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nimport lightning.pytorch as pl\r\n\r\ntester = pl.Trainer(devices=config.experiment.device_num, inference_mode=True)\r\ndata_module = CustomDatamodule(config.task, data_root_dir, tokenizer)\r\npredictions = tester.predict(\r\n    model=experiment,\r\n    datamodule=data_module,\r\n    return_predictions=True,\r\n    ckpt_path=config.model.ckpt_path if not config.model.load_model_state_dict else None\r\n)\r\n```\r\n\r\nMy datamodule\r\n\r\n```\r\nclass CustomDatamodule(pl.LightningDataModule):\r\n\r\n    def __init__(self, cfg, root_dir, tokenizer):\r\n        super().__init__()\r\n        self.cfg = cfg\r\n        self.root_dir = root_dir\r\n        self.tokenizer = tokenizer\r\n        self.prepare_data_per_node = True\r\n        self.dataset_kwargs = {\r\n            \"max_seq_length\": self.cfg.dataset.max_seq_length,\r\n            \"cluster_batch\": self.cfg.dataset.cluster_batch,            \r\n        }\r\n        self.dataset_kwargs.update(self.cfg.other_cfgs)\r\n    \r\n    ...\r\n    \r\n    def predict_dataloader(self) -> EVAL_DATALOADERS:\r\n        return DataLoader(\r\n            self.test_dataset, \r\n            batch_size=1, \r\n            num_workers=self.cfg.dataset.nworkers, \r\n            pin_memory=self.cfg.dataset.pin_memory, \r\n            drop_last=False, \r\n            shuffle=False,\r\n        )\r\n```\r\n\r\n### Error messages and logs\r\n\r\n```\r\nTypeError: An invalid dataloader was passed to `Trainer.predict(dataloaders=...)`. Found <utils.CustomDatamodule object at 0x7f8bb66feb60>.\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n\r\n#- PyTorch Lightning Version (e.g., 2.2.0.post0):\r\n\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19580/comments",
    "author": "ZetangForward",
    "comments": [
      {
        "user": "ZetangForward",
        "created_at": "2024-03-06T03:14:18Z",
        "body": "BTW, I try to debug in data_connect.py (line 330)\r\n```\r\ndef _request_dataloader(data_source: _DataLoaderSource) -> Union[TRAIN_DATALOADERS, EVAL_DATALOADERS]:\r\n    \"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\r\n\r\n    Returns:\r\n        The requested dataloader\r\n\r\n    \"\"\"\r\n    with _replace_dunder_methods(DataLoader, \"dataset\"), _replace_dunder_methods(BatchSampler):\r\n        # under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\r\n        # attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\r\n        # Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\r\n        # methods so that the re-instantiated object is as close to the original as possible.\r\n        return data_source.dataloader()\r\n\r\n```\r\n\r\n\r\n, and I find\r\n\r\n```\r\n(Pdb) data_source.dataloader()\r\n<bound method CustomDatamodule.predict_dataloader of <utils.CustomDatamodule object at 0x7ff400da7c40>>\r\n(Pdb) isinstance(data_source.dataloader(), DataLoader)\r\nFalse\r\n```\r\n\r\nI also check the ```_DataLoaderSource``` in ```def setup_data(self) -> None:``` and the ``self._data_source`` is :\r\n```\r\n_DataLoaderSource(instance=<utils.CustomDatamodule object at 0x7f99f1c73d60>, name='predict_dataloader')\r\n```\r\n\r\n\r\nI am confused about why predict_dataloader() is an instance(bound method) of Datamodule. Shouldn't it return the instance of ```DataLoader()```?\r\n\r\nCan you provide some suggestions for me? \r\n@vanpelt @kashif @ozen @ashwinb "
      },
      {
        "user": "awaelchli",
        "created_at": "2024-03-06T12:49:43Z",
        "body": "@ZetangForward You are probably mixing imports. Check all your lightning imports. Use\r\n```py\r\nimport lightning\r\n```\r\neverywhere, or use\r\n\r\n```py\r\nimport pytorch_lightning\r\n```\r\neverywhere, but don't mix them."
      },
      {
        "user": "awaelchli",
        "created_at": "2024-03-08T23:18:47Z",
        "body": "\r\nI am confident this was the issue. Please let me know if it worked.\r\n\r\n"
      },
      {
        "user": "ZetangForward",
        "created_at": "2024-03-09T01:52:14Z",
        "body": "> I am confident this was the issue. Please let me know if it worked.\r\n\r\nHi， thanks for your suggestion. I will check it today."
      }
    ]
  },
  {
    "number": 19539,
    "title": "The prefetch_batches in _PrefetchDataFetcher always is 1",
    "created_at": "2024-02-27T11:09:09Z",
    "closed_at": "2024-02-27T15:45:06Z",
    "labels": [
      "question",
      "data handling",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19539",
    "body": "### Bug description\n\nI set prefetch_factors in pytorch dataloader, but the __next__ still costs a lot of time. My _DataFetcher is _PrefetchDataFetcher, the printed prefetch_factors never changed. \r\n\r\n_PrefetchDataFetcher was returned in  return _PrefetchDataFetcher() but the prefetch_factors was unsetted\n\n### What version are you seeing the problem on?\n\nv2.1\n\n### How to reproduce the bug\n\n```python\nchange the data_factor in dataloader and print prefetch_batches in _PrefetchDataFetcher\n```\n\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19539/comments",
    "author": "AnnaTrainingG",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-02-27T15:45:06Z",
        "body": "@niuliling123 `prefetch_batches` in Lightning internals is not the same as the prefetch factor for the DataLoader, they are independent things that work differently. If you set `DataLoader(prefetch_factor=x)`, it will be used. You can check that by printing \r\n\r\n```py\r\nprint(self.trainer.train_dataloader.prefetch_factor)\r\n```\r\n\r\nin your `training_step` for example."
      }
    ]
  },
  {
    "number": 19476,
    "title": "Loss not getting printed in when manual optimization is turned on",
    "created_at": "2024-02-15T05:34:55Z",
    "closed_at": "2024-02-16T05:23:02Z",
    "labels": [
      "question",
      "logging",
      "progress bar: tqdm",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19476",
    "body": "### Bug description\r\n\r\nI ported an simple toy example in pytorch lightning and found that when manual optimization is turned on (to implement a custom optimizer), the loss doesn't get printed. Not sure if I am missing to set some something.\r\n\r\nAny help is appreciated.\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.1\r\n\r\n### How to reproduce the bug\r\nBaseline toy example (works fine and prints loss correctly):\r\n```python\r\n\r\nimport pytorch_lightning as pl\r\nimport numpy as np\r\nimport torch\r\nfrom torch.nn import MSELoss\r\nfrom torch.optim import Adam\r\nfrom ptadamw import AdamW\r\nfrom torch.utils.data import DataLoader, Dataset\r\nimport torch.nn as nn\r\n\r\n\r\nclass SimpleDataset(Dataset):\r\n    def __init__(self):\r\n        X = np.arange(10000)\r\n        y = X * 2\r\n        X = [[_] for _ in X]\r\n        y = [[_] for _ in y]\r\n        self.X = torch.Tensor(X)\r\n        self.y = torch.Tensor(y)\r\n\r\n    def __len__(self):\r\n        return len(self.y)\r\n\r\n    def __getitem__(self, idx):\r\n        return {\"X\": self.X[idx], \"y\": self.y[idx]}\r\n\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fc = nn.Linear(1, 1)\r\n        self.criterion = MSELoss()\r\n\r\n    def forward(self, inputs_id, labels=None):\r\n        outputs = self.fc(inputs_id)\r\n        loss = 0\r\n        if labels is not None:\r\n            loss = self.criterion(outputs, labels)\r\n        return loss, outputs\r\n\r\n    def train_dataloader(self):\r\n        dataset = SimpleDataset()\r\n        return DataLoader(dataset, batch_size=1000)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        input_ids = batch[\"X\"]\r\n        labels = batch[\"y\"]\r\n        loss, outputs = self(input_ids, labels)\r\n        return {\"loss\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = Adam(self.parameters())\r\n        return optimizer\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = MyModel()\r\n    trainer = pl.Trainer(max_epochs=100, gpus=1)\r\n    trainer.fit(model)\r\n\r\n    X = torch.Tensor([[1.0], [51.0], [89.0]])\r\n    _, y = model(X)\r\n    print(y)\r\n\r\n```\r\n\r\n\r\nPorted toy example with manual optimization (runs without any errors but no loss being printed in the progress bar):\r\n```\r\nimport pytorch_lightning as pl\r\nimport numpy as np\r\nimport torch\r\nfrom torch.nn import MSELoss\r\nfrom torch.optim import Adam\r\nfrom sam import SAM\r\nfrom ptadamw import AdamW\r\nfrom torch.utils.data import DataLoader, Dataset\r\nimport torch.nn as nn\r\n\r\n\r\nclass SimpleDataset(Dataset):\r\n    def __init__(self):\r\n        X = np.arange(10000)\r\n        y = X * 2\r\n        X = [[_] for _ in X]\r\n        y = [[_] for _ in y]\r\n        self.X = torch.Tensor(X)\r\n        self.y = torch.Tensor(y)\r\n\r\n    def __len__(self):\r\n        return len(self.y)\r\n\r\n    def __getitem__(self, idx):\r\n        return {\"X\": self.X[idx], \"y\": self.y[idx]}\r\n\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fc = nn.Linear(1, 1)\r\n        self.criterion = MSELoss()\r\n        self.automatic_optimization = False\r\n\r\n    def forward(self, inputs_id, labels=None):\r\n        outputs = self.fc(inputs_id)\r\n        loss = 0\r\n        if labels is not None:\r\n            loss = self.criterion(outputs, labels)\r\n        return loss, outputs\r\n\r\n    def train_dataloader(self):\r\n        dataset = SimpleDataset()\r\n        return DataLoader(dataset, batch_size=1000)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        optimizer = self.optimizers()\r\n\r\n        # first forward-backward pass\r\n        loss_1 = self.compute_loss(batch)\r\n        self.manual_backward(loss_1)\r\n        optimizer.first_step(zero_grad=True)\r\n\r\n        # second forward-backward pass\r\n        loss_2 = self.compute_loss(batch)\r\n        self.manual_backward(loss_2)\r\n        optimizer.second_step(zero_grad=True)\r\n\r\n        return {\"loss\": loss_1}\r\n\r\n    def compute_loss(self, batch):\r\n        input_ids = batch[\"X\"]\r\n        labels = batch[\"y\"]\r\n        loss, outputs = self(input_ids, labels)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        base_optimizer = Adam\r\n        optimizer = SAM(model.parameters(), base_optimizer, lr=0.01, rho=0.05)\r\n        return optimizer\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = MyModel()\r\n    trainer = pl.Trainer(max_epochs=100, gpus=1)\r\n    trainer.fit(model)\r\n\r\n    X = torch.Tensor([[1.0], [51.0], [89.0]])\r\n    _, y = model(X)\r\n    print(y)\r\n\r\n```\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\nBaseline code: time, it/s, loss and v_num fields are printed correctly\r\n```\r\nEpoch 99: 100%|█████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 203.64it/s, loss=4.16e+07, v_num=32]\r\n`Trainer.fit` stopped: `max_epochs=100` reached.\r\nEpoch 99: 100%|█████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 197.27it/s, loss=4.16e+07, v_num=32]\r\ntensor([[ 1.9360],\r\n        [46.3258],\r\n        [80.0620]], grad_fn=<AddmmBackward0>)\r\n\r\n```\r\n\r\nPorted code: (loss is not being printed)\r\n```\r\nEpoch 99: 100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 180.96it/s, v_num=33]\r\ntensor([[  2.8524],\r\n        [102.8746],\r\n        [178.8915]], grad_fn=<AddmmBackward0>)\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_\n\ncc @carmocca @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19476/comments",
    "author": "ParamsRaman",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-02-16T05:23:02Z",
        "body": "Hey @ParamsRaman \r\nThanks for reporting. At first this may look like a bug, and you may have been using an older version of Lightning. But as of version 2.0, we changed the progress bar to not automatically log the loss there. More details here: #16192\r\n\r\nIf you want to have it there, you will need to explicitly log it via `self.log(\"loss\", loss, prog_bar=True)`. \r\nClosing this issue as it is the expected behavior (for both automatic and manual). "
      }
    ]
  },
  {
    "number": 19337,
    "title": "ONNX export doesn't work with BFloat16",
    "created_at": "2024-01-24T12:55:42Z",
    "closed_at": "2024-01-26T07:09:10Z",
    "labels": [
      "question",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19337",
    "body": "### Bug description\r\n\r\nTrying to export a model with BFloat16 weights breaks.\r\nUsually, lightning transfers the inputs to the correct device, and i thought, the correct dtype. That might not be the case.\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.1\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nimport os\r\nimport torch\r\nimport lightning.pytorch as pl\r\n\r\nclass RandomDataset(torch.utils.data.Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\nclass CallExport(pl.callbacks.Callback):\r\n    def on_validation_end(self, trainer, pl_module: BoringModel):\r\n        if trainer.is_global_zero:\r\n            print(\"Running ONNX export\")\r\n            x = torch.randn(10, 32)\r\n            pl_module.to_onnx('/tmp/model.onnx', input_sample=(x,))\r\n        trainer.strategy.barrier()\r\n\r\ndef run():\r\n    train_data  = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data    = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = pl.Trainer(\r\n        accelerator=\"gpu\",\r\n        devices=1,\r\n        precision=\"bf16-true\",\r\n        callbacks=[CallExport()],\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        limit_test_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### Error messages and logs\r\n\r\n```\r\nRuntimeError: mat1 and mat2 must have the same dtype, but got Float and BFloat16\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA RTX A6000\r\n        - available:         True\r\n        - version:           12.1\r\n* Lightning:\r\n        - lightning:         2.1.3\r\n        - lightning-utilities: 0.10.0\r\n        - pytorch-lightning: 2.1.2\r\n        - recurrent-memory-transformer-pytorch: 0.5.5\r\n        - torch:             2.1.2\r\n        - torchaudio:        2.1.2\r\n        - torchmetrics:      1.2.0\r\n        - torchvision:       0.16.2\r\n* Packages:\r\n        - absl-py:           2.0.0\r\n        - aiofiles:          23.2.1\r\n        - aiohttp:           3.9.1\r\n        - aiosignal:         1.3.1\r\n        - argparse:          1.4.0\r\n        - async-timeout:     4.0.3\r\n        - attrs:             23.1.0\r\n        - cachetools:        5.3.2\r\n        - certifi:           2023.11.17\r\n        - charset-normalizer: 3.3.2\r\n        - coloredlogs:       15.0.1\r\n        - contourpy:         1.2.0\r\n        - cycler:            0.12.1\r\n        - datasets:          2.16.1\r\n        - dill:              0.3.7\r\n        - einops:            0.7.0\r\n        - filelock:          3.13.1\r\n        - flashlight:        0.1.1\r\n        - flashlight-text:   0.0.4\r\n        - flatbuffers:       23.5.26\r\n        - fonttools:         4.45.1\r\n        - frozenlist:        1.4.0\r\n        - fsspec:            2023.10.0\r\n        - future:            0.18.3\r\n        - google-auth:       2.23.4\r\n        - google-auth-oauthlib: 1.1.0\r\n        - grpcio:            1.59.3\r\n        - httptools:         0.6.1\r\n        - huggingface-hub:   0.20.1\r\n        - humanfriendly:     10.0\r\n        - idna:              3.6\r\n        - jaxtyping:         0.2.23\r\n        - jinja2:            3.1.2\r\n        - kiwisolver:        1.4.5\r\n        - lightning:         2.1.3\r\n        - lightning-utilities: 0.10.0\r\n        - llvmlite:          0.41.1\r\n        - mako:              1.3.0\r\n        - markdown:          3.5.1\r\n        - markdown-it-py:    3.0.0\r\n        - markupsafe:        2.1.3\r\n        - matplotlib:        3.8.2\r\n        - mdurl:             0.1.2\r\n        - mpmath:            1.3.0\r\n        - multidict:         6.0.4\r\n        - multiprocess:      0.70.15\r\n        - networkx:          3.2.1\r\n        - numba:             0.58.1\r\n        - numpy:             1.26.2\r\n        - nvidia-cublas-cu12: 12.1.3.1\r\n        - nvidia-cuda-cupti-cu12: 12.1.105\r\n        - nvidia-cuda-nvrtc-cu12: 12.1.105\r\n        - nvidia-cuda-runtime-cu12: 12.1.105\r\n        - nvidia-cudnn-cu12: 8.9.2.26\r\n        - nvidia-cufft-cu12: 11.0.2.54\r\n        - nvidia-curand-cu12: 10.3.2.106\r\n        - nvidia-cusolver-cu12: 11.4.5.107\r\n        - nvidia-cusparse-cu12: 12.1.0.106\r\n        - nvidia-nccl-cu12:  2.18.1\r\n        - nvidia-nvjitlink-cu12: 12.3.101\r\n        - nvidia-nvtx-cu12:  12.1.105\r\n        - oauthlib:          3.2.2\r\n        - onnx:              1.15.0\r\n        - onnxruntime:       1.16.3\r\n        - onnxruntime-gpu:   1.16.3\r\n        - onnxscript:        0.1.0.dev20231213\r\n        - onnxsim:           0.4.35\r\n        - packaging:         23.2\r\n        - pandas:            2.1.4\r\n        - pillow:            10.1.0\r\n        - pip:               22.0.2\r\n        - pipe:              2.0\r\n        - protobuf:          4.23.4\r\n        - pyarrow:           14.0.2\r\n        - pyarrow-hotfix:    0.6\r\n        - pyasn1:            0.5.1\r\n        - pyasn1-modules:    0.3.0\r\n        - pybombs:           2.3.5\r\n        - pygments:          2.17.2\r\n        - pyparsing:         3.1.1\r\n        - pyqt5:             5.15.10\r\n        - pyqt5-qt5:         5.15.2\r\n        - pyqt5-sip:         12.13.0\r\n        - pyqtgraph:         0.13.3\r\n        - python-dateutil:   2.8.2\r\n        - pytorch-lightning: 2.1.2\r\n        - pytz:              2023.3.post1\r\n        - pyyaml:            6.0.1\r\n        - recurrent-memory-transformer-pytorch: 0.5.5\r\n        - requests:          2.31.0\r\n        - requests-oauthlib: 1.3.1\r\n        - rich:              13.7.0\r\n        - rsa:               4.9\r\n        - ruamel.yaml:       0.18.5\r\n        - ruamel.yaml.clib:  0.2.8\r\n        - sanic:             0.7.0\r\n        - scipy:             1.11.4\r\n        - setuptools:        59.6.0\r\n        - six:               1.16.0\r\n        - sympy:             1.12\r\n        - tensorboard:       2.15.1\r\n        - tensorboard-data-server: 0.7.2\r\n        - torch:             2.1.2\r\n        - torchaudio:        2.1.2\r\n        - torchmetrics:      1.2.0\r\n        - torchvision:       0.16.2\r\n        - tqdm:              4.66.1\r\n        - triton:            2.1.0\r\n        - typeguard:         2.13.3\r\n        - typing-extensions: 4.8.0\r\n        - tzdata:            2023.4\r\n        - uhd:               4.6.0\r\n        - ujson:             5.9.0\r\n        - urllib3:           2.1.0\r\n        - uvloop:            0.19.0\r\n        - websockets:        12.0\r\n        - werkzeug:          3.0.1\r\n        - x-transformers:    1.27.9\r\n        - xxhash:            3.4.1\r\n        - yarl:              1.9.3\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.10.12\r\n        - release:           6.2.0-33-generic\r\n        - version:           #33~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Sep  7 10:33:52 UTC 2\r\n\r\n</details>\r\n\r\n### More info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19337/comments",
    "author": "pfeatherstone",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-01-26T02:44:39Z",
        "body": "@pfeatherstone This is because the input is float32, yet the model weights are in float16. To fix this, you need to create the input with the right precision:\r\n\r\n```py\r\nx = torch.randn(10, 32, dtype=torch.bfloat16)\r\n```\r\n\r\nI don't think this can be handled automatically by Lightning.\r\n"
      }
    ]
  },
  {
    "number": 19252,
    "title": "lighting.pytorch IS NOT pytorch_lightning",
    "created_at": "2024-01-09T11:29:48Z",
    "closed_at": "2024-01-11T11:26:39Z",
    "labels": [
      "question",
      "ver: 2.0.x",
      "package"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19252",
    "body": "### Bug description\n\nDocumentation mentions that `pytorch_lightning` is now `lightning`, and that the old API can be found at `lightning_pytorch`.\r\n\r\nAnd yet, if you do:\r\n\r\n```python\r\nimport lightning.pytorch as pl\r\nfrom pytorch_lightning import LightningModule\r\n\r\nclass MyModel(LightningModule):\r\n    pass\r\n\r\nmodel = MyModel()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(model)\r\n```\r\nthis still won't work, you will get:\r\n\r\n```\r\nTypeError: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `LightningModule`\r\n```\r\n\r\nbecause the `Trainer` was imported from `lightning.pytorch`, while the model is a `LightningModule` from `pytorch_lightning`. Are they the same or not?\r\n\r\nWhy can't we \"mix\" them?\r\n\r\nThis is sometimes necessary when working with other packages where some rely on `lightning` or `lightning.pytorch`, and other rely on `pytorch_lightning`.\n\n### What version are you seeing the problem on?\n\nv2.0\n\n### How to reproduce the bug\n\n```python\nimport lightning.pytorch as pl\r\nfrom pytorch_lightning import LightningModule\r\n\r\nclass MyModel(LightningModule):\r\n    pass\r\n\r\nmodel = MyModel()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(model)\n```\n\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19252/comments",
    "author": "svnv-svsv-jm",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-01-09T12:54:51Z",
        "body": "Hi @svnv-svsv-jm \r\n\r\n> because the Trainer was imported from lightning.pytorch, while the model is a LightningModule from pytorch_lightning. Are they the same or not?\r\n> \r\n> Why can't we \"mix\" them?\r\n\r\nThey are two different packages, with the same source code and the only difference is the imports. But Python doesn't know that, so you can't mix them together in the same source code. If you import LightningModule from one package, but the Trainer from the other one, it won't work. They are not meant to be interchangeable. \r\n\r\n> This is sometimes necessary when working with other packages where some rely on lightning or lightning.pytorch, and other rely on pytorch_lightning.\r\n\r\nIt's not possible to mix them. You will have to rewrite the code so that all of it is only from one package.\r\n\r\n\r\n\r\n"
      },
      {
        "user": "svnv-svsv-jm",
        "created_at": "2024-01-10T10:33:14Z",
        "body": "Thanks a lot for your reply!\r\n\r\nSo it is literally source code duplication?\r\n\r\nI thought that in the `lightning.pytorch` module, you would just import `pytorch_lightning` so that just allows the import of the same source code `pytorch_lightning` from two places, but it actually is duplicated code! Why maintain two exact replicas?\r\n\r\nJust curious at this point."
      },
      {
        "user": "awaelchli",
        "created_at": "2024-01-10T10:41:16Z",
        "body": "We don't maintain two copies of the code. We maintain lightning (see the content in GitHub here) and generate the pytorch_lightning package automatically. \r\n\r\nThat there are two packages is a consequence of the decision to rename the package from pytorch-lightning to lightning. "
      },
      {
        "user": "svnv-svsv-jm",
        "created_at": "2024-01-11T11:26:39Z",
        "body": "Ok, thanks lot!"
      },
      {
        "user": "awaelchli",
        "created_at": "2024-01-11T12:05:40Z",
        "body": "Thanks and you're welcome. Sorry for the inconvenience. Over time, more users will switch to the new package name and these questions and struggles will hopefully disappear in history. "
      }
    ]
  },
  {
    "number": 19218,
    "title": "load_from_checkpoint not working at all",
    "created_at": "2023-12-29T07:11:44Z",
    "closed_at": "2024-01-04T13:32:21Z",
    "labels": [
      "question",
      "waiting on author",
      "ver: 1.9.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19218",
    "body": "### Bug description\n\nI want to use my trained model to predict something, so in my own predict module, i use load_from_checkpoint to load a ckpt to my model\r\n\r\n` new_model = DiFFdisenSystem(hparams)`\r\n` model = copy.deepcopy(new_model)`\r\n` new_model.load_from_checkpoint('ckpts/12.27-freq-self-coord/last.ckpt')`\r\n\r\nand to verify whether or not the model has already loaded the ckpt, i use\r\n`for (name, param), (new_name, new_param) in zip(model.named_parameters(), new_model.named_parameters()):`\r\n          `  if torch.equal(param, new_param):`\r\n              `        print(f\"Parameters in {name} are the same.\")`\r\n         `  else:`\r\n              `        print(f\"Parameters in {name} are different.\")`\r\nand the printing output turns out each parameter is the SAME!\n\n### What version are you seeing the problem on?\n\nv1.9\n\n### How to reproduce the bug\n\n```python\nclass DiFFdisenSystem(LightningModule):\r\n\r\n    def __init__(\r\n        self,\r\n        hparams,\r\n    ):\r\n        super().__init__()\r\n        self.save_hyperparameters(hparams)\r\n\r\n        self.model = BeatGANsAutoencConfig(\r\n            enc_out_channels = self.hparams.enc_out_channels,\r\n            enc_attn_resolutions = self.hparams.enc_attn_resolutions,\r\n            enc_pool = self.hparams.enc_pool,\r\n            enc_num_res_block = self.hparams.enc_num_res_block,\r\n            enc_channel_mult = self.hparams.enc_channel_mult,\r\n            enc_grad_checkpoint = self.hparams.enc_grad_checkpoint,\r\n            latent_net_conf = None,\r\n        ).make_model()\r\n        \r\n        # self.model = ToyModel()\r\n        \r\n        self.tr_noise_scheduler = DDIMScheduler.from_pretrained(self.hparams.pretrained_model_name_or_path, subfolder='scheduler')\r\n        self.noise_scheduler = DDIMScheduler.from_pretrained(self.hparams.pretrained_model_name_or_path, subfolder='scheduler')\r\n        self.vae = AutoencoderKL.from_pretrained(self.hparams.pretrained_model_name_or_path, subfolder='vae')\r\n        self.scale_factor = 0.18125\r\n        \r\n        self.vae_channel = 4\r\n        self.vae_out_dim = 16               \r\n        \r\n        self.reduce_channel = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=1)\r\n        \r\n        freeze_params(self.vae.parameters())\n```\n\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19218/comments",
    "author": "YixuannnL",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-12-29T16:02:48Z",
        "body": "Hi\r\n\r\nYou need to use\r\n\r\n```py\r\nnew_model = DiFFdisenSystem.load_from_checkpoint('ckpts/12.27-freq-self-coord/last.ckpt', hparams=hparams)\r\n```\r\n(and delete the deepcopy call)\r\n\r\nIn recent versions of Lightning, the previous way would error now and inform the user of the correct usage. "
      },
      {
        "user": "YixuannnL",
        "created_at": "2023-12-29T16:03:10Z",
        "body": "您好，我已经收到您的邮件。我将尽快给您回复。谢谢！"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-12-30T23:41:45Z",
        "body": "@YixuannnL Could you give this a try and let me know if that resolves your issue?"
      },
      {
        "user": "awaelchli",
        "created_at": "2024-01-04T13:32:21Z",
        "body": "@YixuannnL I didn't get a reply from you, but I'm confident that my previous answer will resolve the issue with the code that you use to load and verify. "
      }
    ]
  },
  {
    "number": 19189,
    "title": "last.ckpt is a soft link of best.ckpt",
    "created_at": "2023-12-20T08:59:56Z",
    "closed_at": "2023-12-21T16:38:50Z",
    "labels": [
      "question",
      "callback: model checkpoint",
      "ver: 2.1.x",
      "ver: 2.2.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19189",
    "body": "### Bug description\n\nWhen I save the last.ckpt, the last.ckpt is symlink of the best.ckpt. I have already set model_checkpoint.save_last=True.\n\n### What version are you seeing the problem on?\n\nv2.1, master\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @carmocca @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19189/comments",
    "author": "Jinbo-Hu",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-12-20T22:55:52Z",
        "body": "The was a new feature in 2.1, not a bug. You can find some discussion here: #18995\r\nSorry if it came unexpected."
      },
      {
        "user": "Jinbo-Hu",
        "created_at": "2023-12-21T07:03:16Z",
        "body": "But how do I save the latest or newest epoch? I just upgraded the version from 2.0.4 to 2.1.3."
      },
      {
        "user": "Jinbo-Hu",
        "created_at": "2023-12-21T07:14:08Z",
        "body": "> But how do I save the latest or newest epoch? I just upgraded the version from 2.0.4 to 2.1.3.\r\n\r\nautomatically"
      },
      {
        "user": "Eleven1Liu",
        "created_at": "2024-01-06T16:12:10Z",
        "body": "In my case, when I set `model_checkpoint.save_last=True`, the last checkpoint links to `best_model.ckpt`.\r\nHowever, `best_model.ckpt` is different from `last.ckpt`. In my understanding, `best_model.ckpt` is the best model you get by validation metric. `last.ckpt` is the model of the last epoch. They are not the same.\r\n\r\nThe second error we got was when we use \r\n```\r\ntrainer.fit(model, train_loader)\r\n```\r\nwithout `val_loader` (training for a fixed number of epochs without validation)\r\n\r\nWe got OSError: [Error 40] Too many levels of symbolic links: '...' because `last.ckpt` is linked to `last.ckpt`.\r\nI think this PR is helpful for us.\r\n\r\nOr should we wait for the latest release of the following code:\r\n```\r\nModelCheckpoint(save_last='copy')\r\n```\r\nWe've upgraded our lightning version to 2.1.3 by pip install but still got the symbolic link version.\r\nThanks!\r\n"
      },
      {
        "user": "bfs18",
        "created_at": "2024-02-09T10:07:13Z",
        "body": "It is note fixed in 2.1.4"
      }
    ]
  },
  {
    "number": 19162,
    "title": "WandbLogger causes Segmentation Fault",
    "created_at": "2023-12-14T17:45:06Z",
    "closed_at": "2023-12-15T12:51:32Z",
    "labels": [
      "question",
      "3rd party",
      "logger: wandb",
      "ver: 2.1.x",
      "repro needed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19162",
    "body": "### Bug description\n\nsimply initialization of a `WandbLogger` instance results in Segmentation fault. This happens in any context. \r\n\r\nhere a very basic example:\r\n```python\r\nfrom lightning.pytorch.loggers import WandbLogger\r\nimport time\r\n\r\nlogger = WandbLogger()\r\n\r\n# wait for logger to be initialized\r\nwhile not isinstance(logger.experiment.name, str):\r\n    print('waiting for logger to be initialized...')\r\n    # wait .1 seconds\r\n    time.sleep(.1)\r\n    continue\r\n```\r\n\r\nresults in\r\n```\r\nSegmentation fault (core dumped)\r\n```\n\n### What version are you seeing the problem on?\n\nv2.1\n\n### How to reproduce the bug\n\n```python\nfrom lightning.pytorch.loggers import WandbLogger\r\nimport time\r\n\r\nlogger = WandbLogger()\r\n\r\n# wait for logger to be initialized\r\nwhile not isinstance(logger.experiment.name, str):\r\n    print('waiting for logger to be initialized...')\r\n    # wait .1 seconds\r\n    time.sleep(.1)\r\n    continue\n```\n\n\n### Error messages and logs\n\n```\r\nSegmentation fault (core dumped)\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce RTX 2080 Ti\r\n                - NVIDIA GeForce RTX 2080 Ti\r\n                - NVIDIA GeForce RTX 2080 Ti\r\n                - NVIDIA GeForce RTX 2080 Ti\r\n                - NVIDIA GeForce RTX 2080 Ti\r\n                - NVIDIA GeForce RTX 2080 Ti\r\n                - NVIDIA GeForce RTX 2080 Ti\r\n                - NVIDIA GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           11.7\r\n* Lightning:\r\n        - lightning:         2.1.2\r\n        - lightning-cloud:   0.5.39\r\n        - lightning-utilities: 0.10.0\r\n        - pytorch-lightning: 2.1.2\r\n        - pytorch-transformers: 1.2.0\r\n        - torch:             1.13.1\r\n        - torchaudio:        0.13.1\r\n        - torchmetrics:      1.2.1\r\n        - torchsummary:      1.5.1\r\n        - torchvision:       0.14.1\r\n* Packages:\r\n        - absl-py:           2.0.0\r\n        - accelerate:        0.23.0\r\n        - aiofiles:          22.1.0\r\n        - aiohttp:           3.8.5\r\n        - aiosignal:         1.3.1\r\n        - aiosqlite:         0.19.0\r\n        - annotated-types:   0.6.0\r\n        - antlr4-python3-runtime: 4.9.3\r\n        - anyio:             3.7.1\r\n        - aplus:             0.11.0\r\n        - appdirs:           1.4.4\r\n        - argon2-cffi:       23.1.0\r\n        - argon2-cffi-bindings: 21.2.0\r\n        - argparse:          1.4.0\r\n        - arrow:             1.3.0\r\n        - astropy:           6.0.0\r\n        - astropy-iers-data: 0.2023.12.11.0.31.11\r\n        - asttokens:         2.4.1\r\n        - async-timeout:     4.0.3\r\n        - attrs:             23.1.0\r\n        - babel:             2.14.0\r\n        - backoff:           2.2.1\r\n        - batchgenerators:   0.24\r\n        - beautifulsoup4:    4.12.2\r\n        - bitsandbytes:      0.41.2.post2\r\n        - blake3:            0.3.3\r\n        - bleach:            6.1.0\r\n        - blessed:           1.19.1\r\n        - bokeh:             3.3.2\r\n        - boto3:             1.28.57\r\n        - botocore:          1.31.57\r\n        - bottleneck:        1.3.7\r\n        - bqplot:            0.12.42\r\n        - branca:            0.7.0\r\n        - brotli:            1.0.9\r\n        - build:             1.0.3\r\n        - cachecontrol:      0.13.1\r\n        - cached-property:   1.5.2\r\n        - cachetools:        5.3.1\r\n        - certifi:           2023.11.17\r\n        - cffi:              1.16.0\r\n        - charset-normalizer: 3.3.2\r\n        - cleo:              2.1.0\r\n        - click:             8.1.7\r\n        - cloudpickle:       3.0.0\r\n        - colorama:          0.4.6\r\n        - coloredlogs:       15.0.1\r\n        - comm:              0.1.4\r\n        - contourpy:         1.2.0\r\n        - cramjam:           2.7.0\r\n        - crashtest:         0.4.1\r\n        - croniter:          1.4.1\r\n        - cryptography:      41.0.7\r\n        - cycler:            0.12.1\r\n        - cytoolz:           0.12.2\r\n        - dask:              2023.12.0\r\n        - dask-jobqueue:     0.8.2\r\n        - datasets:          2.14.5\r\n        - dateutils:         0.6.12\r\n        - debugpy:           1.8.0\r\n        - decorator:         5.1.1\r\n        - deepdiff:          6.7.1\r\n        - defusedxml:        0.7.1\r\n        - dicom2nifti:       2.4.7\r\n        - dill:              0.3.7\r\n        - distlib:           0.3.8\r\n        - distributed:       2023.12.0\r\n        - docker-pycreds:    0.4.0\r\n        - docstring-parser:  0.15\r\n        - dulwich:           0.21.7\r\n        - einops:            0.6.1\r\n        - entrypoints:       0.4\r\n        - exceptiongroup:    1.2.0\r\n        - executing:         2.0.1\r\n        - fastapi:           0.105.0\r\n        - fastcore:          1.5.27\r\n        - fastjsonschema:    2.19.0\r\n        - fastparquet:       2023.4.0\r\n        - filelock:          3.13.1\r\n        - fonttools:         4.46.0\r\n        - fqdn:              1.5.1\r\n        - frozendict:        2.3.10\r\n        - frozenlist:        1.4.0\r\n        - fsspec:            2023.12.2\r\n        - future:            0.18.2\r\n        - gitdb:             4.0.10\r\n        - gitpython:         3.1.37\r\n        - google-auth:       2.23.3\r\n        - google-auth-oauthlib: 1.0.0\r\n        - graphviz:          0.20.1\r\n        - grpcio:            1.59.0\r\n        - h11:               0.14.0\r\n        - h5py:              3.7.0\r\n        - hiddenlayer:       0.3\r\n        - huggingface-hub:   0.16.4\r\n        - humanfriendly:     10.0\r\n        - hydra-core:        1.3.2\r\n        - idna:              3.6\r\n        - imagecodecs:       2023.1.23\r\n        - imageio:           2.23.0\r\n        - importlib-metadata: 7.0.0\r\n        - importlib-resources: 6.1.1\r\n        - inquirer:          3.1.4\r\n        - installer:         0.7.0\r\n        - ipydatawidgets:    4.3.2\r\n        - ipykernel:         6.26.0\r\n        - ipyleaflet:        0.18.0\r\n        - ipympl:            0.9.3\r\n        - ipython:           8.18.1\r\n        - ipython-genutils:  0.2.0\r\n        - ipyvolume:         0.6.3\r\n        - ipyvue:            1.10.1\r\n        - ipyvuetify:        1.8.10\r\n        - ipywebrtc:         0.6.0\r\n        - ipywidgets:        8.1.1\r\n        - isoduration:       20.11.0\r\n        - itsdangerous:      2.1.2\r\n        - jaraco.classes:    3.3.0\r\n        - jedi:              0.19.1\r\n        - jeepney:           0.8.0\r\n        - jinja2:            3.1.2\r\n        - jmespath:          1.0.1\r\n        - joblib:            1.3.2\r\n        - json5:             0.9.14\r\n        - jsonargparse:      4.27.0\r\n        - jsonpointer:       2.4\r\n        - jsonschema:        4.20.0\r\n        - jsonschema-specifications: 2023.11.2\r\n        - jupyter-client:    7.4.9\r\n        - jupyter-core:      5.5.0\r\n        - jupyter-events:    0.9.0\r\n        - jupyter-server:    2.12.1\r\n        - jupyter-server-fileid: 0.9.0\r\n        - jupyter-server-terminals: 0.5.0\r\n        - jupyter-server-ydoc: 0.8.0\r\n        - jupyter-ydoc:      0.2.4\r\n        - jupyterlab:        3.6.3\r\n        - jupyterlab-pygments: 0.3.0\r\n        - jupyterlab-server: 2.25.2\r\n        - jupyterlab-widgets: 3.0.9\r\n        - keyring:           24.3.0\r\n        - kiwisolver:        1.4.5\r\n        - lazy-loader:       0.3\r\n        - lightning:         2.1.2\r\n        - lightning-cloud:   0.5.39\r\n        - lightning-utilities: 0.10.0\r\n        - linecache2:        1.0.0\r\n        - llvmlite:          0.41.1\r\n        - locket:            1.0.0\r\n        - lovely-numpy:      0.2.8\r\n        - lovely-tensors:    0.1.14\r\n        - lz4:               4.3.2\r\n        - markdown:          3.5\r\n        - markdown-it-py:    3.0.0\r\n        - markupsafe:        2.1.3\r\n        - matplotlib:        3.6.2\r\n        - matplotlib-inline: 0.1.6\r\n        - mdurl:             0.1.0\r\n        - medpy:             0.4.0\r\n        - mistune:           3.0.2\r\n        - mkl-fft:           1.3.8\r\n        - mkl-random:        1.2.4\r\n        - mkl-service:       2.4.0\r\n        - more-itertools:    10.1.0\r\n        - mpmath:            1.3.0\r\n        - msgpack:           1.0.7\r\n        - multidict:         6.0.4\r\n        - multiprocess:      0.70.15\r\n        - munkres:           1.1.4\r\n        - nbclassic:         1.0.0\r\n        - nbclient:          0.8.0\r\n        - nbconvert:         7.12.0\r\n        - nbformat:          5.9.2\r\n        - nest-asyncio:      1.5.8\r\n        - networkx:          2.8.8\r\n        - nibabel:           4.0.2\r\n        - nnunet:            1.7.0\r\n        - notebook:          6.5.6\r\n        - notebook-shim:     0.2.3\r\n        - numba:             0.58.1\r\n        - numexpr:           2.8.7\r\n        - numpy:             1.26.2\r\n        - oauthlib:          3.2.2\r\n        - omegaconf:         2.3.0\r\n        - optimum:           1.13.2\r\n        - ordered-set:       4.1.0\r\n        - orjson:            3.9.10\r\n        - overrides:         7.4.0\r\n        - packaging:         23.2\r\n        - pandas:            1.5.2\r\n        - pandocfilters:     1.5.0\r\n        - parso:             0.8.3\r\n        - partd:             1.4.1\r\n        - pathtools:         0.1.2\r\n        - pexpect:           4.8.0\r\n        - pickleshare:       0.7.5\r\n        - pillow:            10.0.1\r\n        - pip:               23.3.1\r\n        - pkginfo:           1.9.6\r\n        - pkgutil-resolve-name: 1.3.10\r\n        - platformdirs:      3.11.0\r\n        - ply:               3.11\r\n        - poetry:            1.7.1\r\n        - poetry-core:       1.8.1\r\n        - poetry-plugin-export: 1.6.0\r\n        - progressbar2:      4.2.0\r\n        - prometheus-client: 0.19.0\r\n        - prompt-toolkit:    3.0.42\r\n        - protobuf:          3.20.2\r\n        - psutil:            5.9.5\r\n        - ptyprocess:        0.7.0\r\n        - pure-eval:         0.2.2\r\n        - py-cpuinfo:        9.0.0\r\n        - pyarrow:           11.0.0\r\n        - pyarrow-hotfix:    0.6\r\n        - pyasn1:            0.5.0\r\n        - pyasn1-modules:    0.3.0\r\n        - pycparser:         2.21\r\n        - pydantic:          2.1.1\r\n        - pydantic-core:     2.4.0\r\n        - pydicom:           2.3.1\r\n        - pyerfa:            2.0.1.1\r\n        - pygments:          2.17.2\r\n        - pyjwt:             2.8.0\r\n        - pynndescent:       0.5.11\r\n        - pyparsing:         3.1.1\r\n        - pyproject-hooks:   1.0.0\r\n        - pyqt5:             5.15.10\r\n        - pyqt5-sip:         12.13.0\r\n        - pysocks:           1.7.1\r\n        - python-dateutil:   2.8.2\r\n        - python-editor:     1.0.4\r\n        - python-gdcm:       3.0.20\r\n        - python-json-logger: 2.0.7\r\n        - python-multipart:  0.0.6\r\n        - python-utils:      3.8.1\r\n        - pythreejs:         2.4.2\r\n        - pytorch-lightning: 2.1.2\r\n        - pytorch-transformers: 1.2.0\r\n        - pytz:              2023.3.post1\r\n        - pywavelets:        1.4.1\r\n        - pyyaml:            6.0.1\r\n        - pyzmq:             24.0.1\r\n        - rapidfuzz:         3.5.2\r\n        - readchar:          4.0.5.dev0\r\n        - referencing:       0.32.0\r\n        - regex:             2023.8.8\r\n        - requests:          2.31.0\r\n        - requests-oauthlib: 1.3.1\r\n        - requests-toolbelt: 1.0.0\r\n        - rfc3339-validator: 0.1.4\r\n        - rfc3986-validator: 0.1.1\r\n        - rich:              13.7.0\r\n        - rpds-py:           0.13.2\r\n        - rsa:               4.9\r\n        - s3transfer:        0.7.0\r\n        - sacremoses:        0.0.53\r\n        - safetensors:       0.3.3\r\n        - scikit-image:      0.19.3\r\n        - scikit-learn:      1.2.0\r\n        - scipy:             1.11.4\r\n        - seaborn:           0.12.2\r\n        - secretstorage:     3.3.3\r\n        - send2trash:        1.8.2\r\n        - sentencepiece:     0.1.99\r\n        - sentry-sdk:        1.32.0\r\n        - setproctitle:      1.3.3\r\n        - setuptools:        68.2.2\r\n        - shellingham:       1.5.4\r\n        - simpleitk:         2.2.1\r\n        - sip:               6.7.12\r\n        - six:               1.16.0\r\n        - sklearn:           0.0.post1\r\n        - smmap:             5.0.1\r\n        - sniffio:           1.3.0\r\n        - sortedcontainers:  2.4.0\r\n        - soupsieve:         2.5\r\n        - stack-data:        0.6.2\r\n        - starlette:         0.27.0\r\n        - starsessions:      1.3.0\r\n        - sympy:             1.12\r\n        - tables:            3.8.0\r\n        - tabulate:          0.9.0\r\n        - tblib:             3.0.0\r\n        - tensorboard:       2.14.1\r\n        - tensorboard-data-server: 0.7.1\r\n        - tensorboardx:      2.6.2.2\r\n        - terminado:         0.18.0\r\n        - threadpoolctl:     3.1.0\r\n        - tifffile:          2022.10.10\r\n        - tinycss2:          1.2.1\r\n        - tokenizer:         3.4.3\r\n        - tokenizers:        0.13.3\r\n        - tomli:             2.0.1\r\n        - tomlkit:           0.12.3\r\n        - toolz:             0.12.0\r\n        - torch:             1.13.1\r\n        - torchaudio:        0.13.1\r\n        - torchmetrics:      1.2.1\r\n        - torchsummary:      1.5.1\r\n        - torchvision:       0.14.1\r\n        - tornado:           6.3.3\r\n        - tqdm:              4.64.1\r\n        - traceback2:        1.4.0\r\n        - traitlets:         5.14.0\r\n        - traittypes:        0.2.1\r\n        - transformers:      4.27.4\r\n        - trove-classifiers: 2023.11.29\r\n        - types-python-dateutil: 2.8.19.14\r\n        - typeshed-client:   2.4.0\r\n        - typing-extensions: 4.9.0\r\n        - typing-utils:      0.1.0\r\n        - tzdata:            2023.3\r\n        - umap-learn:        0.5.4\r\n        - unicodedata2:      15.1.0\r\n        - unittest2:         1.1.0\r\n        - uri-template:      1.3.0\r\n        - urllib3:           2.1.0\r\n        - uvicorn:           0.24.0.post1\r\n        - vaex-astro:        0.9.3\r\n        - vaex-core:         4.16.1\r\n        - vaex-hdf5:         0.14.1\r\n        - vaex-jupyter:      0.8.1\r\n        - vaex-ml:           0.18.1\r\n        - vaex-server:       0.8.1\r\n        - vaex-viz:          0.5.4\r\n        - virtualenv:        20.25.0\r\n        - wandb:             0.16.1\r\n        - wcwidth:           0.2.12\r\n        - webcolors:         1.13\r\n        - webencodings:      0.5.1\r\n        - websocket-client:  1.7.0\r\n        - websockets:        12.0\r\n        - werkzeug:          3.0.0\r\n        - wheel:             0.42.0\r\n        - widgetsnbextension: 4.0.9\r\n        - xarray:            2023.12.0\r\n        - xxhash:            3.3.0\r\n        - xyzservices:       2023.10.0\r\n        - y-py:              0.5.9\r\n        - yarl:              1.9.2\r\n        - ypy-websocket:     0.8.2\r\n        - zict:              3.0.0\r\n        - zipp:              3.17.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.10.13\r\n        - release:           5.4.0-167-generic\r\n        - version:           #184-Ubuntu SMP Tue Oct 31 09:21:49 UTC 2023\r\n\r\n</details>\n\n### More info\n\n_No response_\n\ncc @awaelchli @morganmcg1 @borisdayma @scottire @parambharat",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19162/comments",
    "author": "DucoG",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-12-14T23:29:50Z",
        "body": "@DucoG Does simply running\r\n\r\n```py\r\nimport wandb\r\n\r\nwandb.init()\r\n```\r\n\r\ncause a segfault too?"
      },
      {
        "user": "morganmcg1",
        "created_at": "2023-12-15T12:42:53Z",
        "body": "Following"
      },
      {
        "user": "morganmcg1",
        "created_at": "2023-12-15T12:46:26Z",
        "body": "Tested the code above in colab and it worked ok, asked for wandb auth, put in my api key and the while loop exited as expected.\r\n\r\nwandb 0.16.1\r\nlightning 2.1.2"
      },
      {
        "user": "DucoG",
        "created_at": "2023-12-15T12:51:32Z",
        "body": "@awaelchli \r\n\r\nFull reinstall of wandb solved the issue. Wandb probably got corrupted.\r\n\r\nThanks!"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-12-15T13:00:41Z",
        "body": "Cool, thank you for getting back @DucoG and @morganmcg1 for testing.\r\nCheers"
      }
    ]
  },
  {
    "number": 19119,
    "title": "StagedFinetuning.finetune_function() missing 1 required positional argument: 'opt_idx'",
    "created_at": "2023-12-06T10:50:14Z",
    "closed_at": "2023-12-10T00:26:53Z",
    "labels": [
      "question",
      "waiting on author",
      "callback: finetuning",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19119",
    "body": "### Bug description\r\n\r\nFinetunning with a pytorch_lightning Trainer does not work now.\r\nThe call to `self.finetune_function()` should pass the `opt_idx` as the last parameter.\r\nLine 313 in `pytorch_lightning/callbacks/finetuning.py`.\r\nAfter that fix, finetuning works again.\r\n\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.1\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nRunning a baseline training run.\r\nI am using a `StagedFinetuning` finetuning object, not sure if that is related.\r\nUsing training with a simple data loader: num_workers=1, batch_size=1\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\nMain error message:\r\n```StagedFinetuning.finetune_function() missing 1 required positional argument: 'opt_idx'```\r\n\r\nDetailed trace:\r\n```\r\n  File \"/home/rob/projects/model-training/model_training/training/train.py\", line 83, in main\r\n    trainer.fit(model, dl_train, val_dataloaders=[dl_val])\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 544, in fit\r\n    call._call_and_handle_interrupt(\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 989, in _run\r\n    results = self._run_stage()\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1035, in _run_stage\r\n    self.fit_loop.run()\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 201, in run\r\n    self.on_advance_start()\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 341, in on_advance_start\r\n    call._call_callback_hooks(trainer, \"on_train_epoch_start\")\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 208, in _call_callback_hooks\r\n    fn(trainer, trainer.lightning_module, *args, **kwargs)\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/callbacks/finetuning.py\", line 313, in on_train_epoch_start\r\n    self.finetune_function(pl_module, trainer.current_epoch, optimizer)\r\nTypeError: StagedFinetuning.finetune_function() missing 1 required positional argument: 'opt_idx'\r\n```\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\nOnly installed the \"pytorch-lightning\" pip package, not \"lightning\".\r\n\r\n```\r\n#- Lightning Component:  Trainer\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.1.2\r\n#- Lightning App Version (e.g., 0.5.2): NA\r\n#- PyTorch Version (e.g., 2.0): 2.1.1\r\n#- Python version (e.g., 3.9): 3.10\r\n#- OS (e.g., Linux): Linux, Ubuntu 22.04, kernel 6.2.0-37-generic #38~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n#- CUDA/cuDNN version: 12.2\r\n#- GPU models and configuration: Nvidia GTX 1050 Ti 4 GiB\r\n#- How you installed Lightning(`conda`, `pip`, source): poetry (pip install)\r\n#- Running environment of LightningApp (e.g. local, cloud): local\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19119/comments",
    "author": "robwijnhoven",
    "comments": [
      {
        "user": "robwijnhoven",
        "created_at": "2023-12-06T11:02:57Z",
        "body": "Downgraded pytorch-lightning to 2.1.1 to be in sync with the pytorch version, but no effect there.\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-12-06T15:05:55Z",
        "body": "@robwijnhoven You can remove `opt_idx` from your `finetune_function` definition. If this doesn't work, please share your entire `StagedFinetuning` implementation. Thanks"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-12-08T12:30:21Z",
        "body": "@robwijnhoven Did it work?"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-12-10T00:26:53Z",
        "body": "I didn't get a reply, but I'm confident that my answer will solve the issue, so I'm closing the issue. "
      },
      {
        "user": "robwijnhoven",
        "created_at": "2023-12-11T12:33:24Z",
        "body": "Yes it works, thanks for the quick reply! My bad in the end :+1: \r\nSOLVED!"
      }
    ]
  },
  {
    "number": 18975,
    "title": "Training a simple XOR network yields incorrect, undeterministic behaviour",
    "created_at": "2023-11-09T10:17:57Z",
    "closed_at": "2023-11-10T12:10:01Z",
    "labels": [
      "question",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18975",
    "body": "### Bug description\n\nHi, I am trying to train a simple DNN to solve the XOR problem. This can be trivially solved with a pure torch implementation. I cannot replicate the same simple model in lightning. Instead the trained model oscillates between different states, never managing to correctly produce XOR.\n\n### What version are you seeing the problem on?\n\nv2.1\n\n### How to reproduce the bug\n\n```python\n# import libraries\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\n\r\nclass XOR(nn.Module):\r\n    def __init__(self):\r\n        super(XOR, self).__init__()\r\n        self.linear_sigmoid_stack = nn.Sequential(\r\n            nn.Linear(2, 2),\r\n            nn.Sigmoid(),\r\n            nn.Linear(2, 1)\r\n        )\r\n\r\n    def forward(self, x):\r\n        return self.linear_sigmoid_stack(x)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    # create data\r\n    Xs = torch.Tensor([[0., 0.],\r\n                       [0., 1.],\r\n                       [1., 0.],\r\n                       [1., 1.]])\r\n\r\n    y = torch.Tensor([0., 1., 1., 0.]).reshape(Xs.shape[0], 1)\r\n\r\n    xor_network = XOR()\r\n\r\n    epochs = 1000\r\n    mseloss = nn.MSELoss()\r\n    optimizer = torch.optim.Adam(xor_network.parameters(), lr=0.03)\r\n    all_losses = []\r\n    current_loss = 0\r\n    plot_every = 50\r\n\r\n    for epoch in range(epochs):\r\n\r\n        # input training example and return the prediction\r\n        yhat = xor_network.forward(Xs)\r\n\r\n        # calculate MSE loss\r\n        loss = mseloss(yhat, y)\r\n\r\n        # backpropogate through the loss gradiants\r\n        loss.backward()\r\n\r\n        # update model weights\r\n        optimizer.step()\r\n\r\n        # remove current gradients for next iteration\r\n        optimizer.zero_grad()\r\n\r\n        # append to loss\r\n        current_loss += loss\r\n        if epoch % plot_every == 0:\r\n            all_losses.append(current_loss / plot_every)\r\n            current_loss = 0\r\n\r\n        # print progress\r\n        if epoch % 500 == 0:\r\n            print(f'Epoch: {epoch} completed')\r\n```\r\n\r\nI tried to use Lightning to simplify away the boilerplate code like so:\r\n```\r\nimport torch\r\nfrom torch import nn\r\nimport torch.nn.functional as F\r\nimport lightning as L\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\n\r\n\r\nclass XORNetwork(L.LightningModule):\r\n    def __init__(self):\r\n        super(XORNetwork, self).__init__()\r\n        self.linear_sigmoid_stack = nn.Sequential(\r\n            nn.Linear(2, 2),\r\n            nn.Sigmoid(),\r\n            nn.Linear(2, 1)\r\n        )\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defines the train loop.\r\n        x, y = batch\r\n        yhat = self.forward(x)\r\n        loss = F.mse_loss(yhat, y)\r\n        return loss\r\n\r\n    def forward(self, x):\r\n        return self.linear_sigmoid_stack(x)\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    X = torch.Tensor([[0., 0.], [0., 1.], [1., 0], [1., 1]])\r\n    labels = torch.Tensor([0., 1., 1., 0])\r\n    dataset = TensorDataset(X, labels)\r\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\r\n\r\n    xor_network = XORNetwork()\r\n\r\n    # train model\r\n    trainer = L.Trainer(max_epochs=500, accelerator=\"cpu\")\r\n    trainer.fit(model=xor_network, train_dataloaders=dataloader)\r\n\r\n    xor_network.eval()\r\n    with torch.no_grad():\r\n        test_output = xor_network(X)\r\n        print(test_output.round())\r\n```\n```\n\n\n### Error messages and logs\n\n_No response_\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n\t- GPU:               None\r\n\t- available:         False\r\n\t- version:           None\r\n* Lightning:\r\n\t- lightning:         2.1.1\r\n\t- lightning-utilities: 0.9.0\r\n\t- pytorch-lightning: 2.1.1\r\n\t- torch:             2.1.0\r\n\t- torchmetrics:      1.2.0\r\n* Packages:\r\n\t- aiohttp:           3.8.6\r\n\t- aiosignal:         1.3.1\r\n\t- async-timeout:     4.0.3\r\n\t- attrs:             23.1.0\r\n\t- certifi:           2023.7.22\r\n\t- charset-normalizer: 3.3.2\r\n\t- filelock:          3.13.1\r\n\t- frozenlist:        1.4.0\r\n\t- fsspec:            2023.10.0\r\n\t- idna:              3.4\r\n\t- jinja2:            3.1.2\r\n\t- lightning:         2.1.1\r\n\t- lightning-utilities: 0.9.0\r\n\t- markupsafe:        2.1.3\r\n\t- mpmath:            1.3.0\r\n\t- multidict:         6.0.4\r\n\t- networkx:          3.2.1\r\n\t- numpy:             1.26.1\r\n\t- packaging:         23.2\r\n\t- pip:               22.3.1\r\n\t- pytorch-lightning: 2.1.1\r\n\t- pyyaml:            6.0.1\r\n\t- requests:          2.31.0\r\n\t- setuptools:        65.5.1\r\n\t- sympy:             1.12\r\n\t- torch:             2.1.0\r\n\t- torchmetrics:      1.2.0\r\n\t- tqdm:              4.66.1\r\n\t- typing-extensions: 4.8.0\r\n\t- urllib3:           2.0.7\r\n\t- wheel:             0.38.4\r\n\t- yarl:              1.9.2\r\n* System:\r\n\t- OS:                Darwin\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         arm\r\n\t- python:            3.10.13\r\n\t- release:           23.0.0\r\n\t- version:           Darwin Kernel Version 23.0.0: Fri Sep 15 14:41:34 PDT 2023; root:xnu-10002.1.13~1/RELEASE_ARM64_T8103\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18975/comments",
    "author": "Fohlen",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-11-09T13:59:33Z",
        "body": "@Fohlen In your Lightning code,\r\n\r\n1. You didn't choose the same learning rate. Make it 0.03 in both cases.\r\n2. You didn't run for the same number of epochs. Make it 1000 in both cases.\r\n\r\nAnd in the raw PyTorch code you are missing the test code:\r\n\r\n```py\r\n    xor_network.eval()\r\n    with torch.no_grad():\r\n        test_output = xor_network(Xs)\r\n        print(test_output.round())\r\n```\r\n\r\nTo make both of them the same, the hyperparameters need to be the same of course. Can you try again? I get the correct predictions (i.e. 0 1 1 0) after these fixes. \r\n\r\nIn addition, to make it fully deterministic you can set the seed \r\n\r\n```py\r\nL.seed_everything(0)\r\n```"
      },
      {
        "user": "Fohlen",
        "created_at": "2023-11-10T12:10:01Z",
        "body": "Hi @awaelchli, this indeed produces the correct result. I can get the code to converge correctly within 100 epochs or less with pure Torch, any idea why that wouldn't be the case with lightning?"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-11-10T12:32:37Z",
        "body": "> I can get the code to converge correctly within 100 epochs or less with pure Torch\r\n\r\nThe code that you posted can't actually converge in 100 epochs. Please share what you changed to make that possible. "
      },
      {
        "user": "Fohlen",
        "created_at": "2023-11-10T17:07:55Z",
        "body": "Sorry for the imprecise wording. After some experimentation with epochs I could produce the correct result at `epoch=250` (not convergence). However, this appears to be extremely sensitive to the seed one uses when training. I find this interesting. According to the Deep Learning book, the correct weights should be learned with a single pass of this network. However, this behaviour is not lightning-specific. Thanks for your help, I will keep on digging in torch to find out the reason for this behaviour 👍 "
      }
    ]
  },
  {
    "number": 18941,
    "title": "SkipResumeTrainingValidationLoop._should_check_val_fx() takes 1 positional argument but 2 were given",
    "created_at": "2023-11-03T22:29:23Z",
    "closed_at": "2023-11-03T23:01:48Z",
    "labels": [
      "question",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18941",
    "body": "### Bug description\n\nI got the error when starting the Nemo Vits training.\r\n\n\n### What version are you seeing the problem on?\n\nv2.1\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\nError executing job with overrides: ['model.sample_rate=16000', 'train_dataset=datasets/dev_clean.json', 'validation_datasets=datasets/test_clean.json', 'phoneme_dict_path=conf/ipa_cmudict-0.7b_nv23.01.txt', 'heteronyms_path=conf/heteronyms-052722', 'trainer.max_epochs=4', 'trainer.accelerator=gpu', 'trainer.check_val_every_n_epoch=1', 'trainer.devices=1']\r\nTraceback (most recent call last):\r\n  File \"/home/sergey/vits.py\", line 32, in <module>\r\n    main()  # noqa pylint: disable=no-value-for-parameter\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/nemo/core/config/hydra_runner.py\", line 126, in wrapper\r\n    _run_hydra(\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\r\n    _run_app(\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 457, in _run_app\r\n    run_and_report(\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 223, in run_and_report\r\n    raise ex\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 220, in run_and_report\r\n    return func()\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 458, in <lambda>\r\n    lambda: hydra.run(\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/hydra/_internal/hydra.py\", line 132, in run\r\n    _ = ret.return_value\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/hydra/core/utils.py\", line 260, in return_value\r\n    raise self._return_value\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/hydra/core/utils.py\", line 186, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File \"/home/sergey/vits.py\", line 28, in main\r\n    trainer.fit(model)\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 545, in fit\r\n    call._call_and_handle_interrupt(\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 43, in _call_and_handle_interrupt\r\n    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 102, in launch\r\n    return function(*args, **kwargs)\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 581, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 990, in _run\r\n    results = self._run_stage()\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1036, in _run_stage\r\n    self.fit_loop.run()\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 202, in run\r\n    self.advance()\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 359, in advance\r\n    self.epoch_loop.run(self._data_fetcher)\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 137, in run\r\n    self.on_advance_end(data_fetcher)\r\n  File \"/home/sergey/.local/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 274, in on_advance_end\r\n    should_check_val = self._should_check_val_fx(data_fetcher)\r\nTypeError: SkipResumeTrainingValidationLoop._should_check_val_fx() takes 1 positional argument but 2 were given\r\nEpoch 0:   1%|          | 1/84 [00:06<08:22,  0.17it/s, v_num=9-43, train_step_timing in s=3.740]         \r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - Tesla T4\r\n        - available:         True\r\n        - version:           12.1\r\n* Lightning:\r\n        - lightning-utilities: 0.9.0\r\n        - pytorch-lightning: 2.1.0\r\n        - torch:             2.1.0\r\n        - torchaudio:        2.1.0\r\n        - torchmetrics:      1.2.0\r\n        - torchvision:       0.16.0\r\n* Packages:\r\n        - absl-py:           2.0.0\r\n        - aiohttp:           3.8.6\r\n        - aiosignal:         1.3.1\r\n        - alabaster:         0.7.13\r\n        - aniso8601:         9.0.1\r\n        - antlr4-python3-runtime: 4.9.3\r\n        - appdirs:           1.4.4\r\n        - asciitree:         0.3.3\r\n        - asttokens:         2.4.1\r\n        - async-timeout:     4.0.3\r\n        - attrdict:          2.0.1\r\n        - attrs:             23.1.0\r\n        - audioread:         3.0.1\r\n        - babel:             2.13.1\r\n        - beautifulsoup4:    4.12.2\r\n        - black:             19.10b0\r\n        - boto3:             1.28.78\r\n        - botocore:          1.31.78\r\n        - braceexpand:       0.1.7\r\n        - cachetools:        5.3.2\r\n        - cdifflib:          1.2.6\r\n        - certifi:           2023.7.22\r\n        - cffi:              1.16.0\r\n        - charset-normalizer: 3.3.2\r\n        - click:             8.0.2\r\n        - colorama:          0.4.6\r\n        - comm:              0.1.4\r\n        - contourpy:         1.2.0\r\n        - cycler:            0.12.1\r\n        - cython:            3.0.5\r\n        - datasets:          2.14.6\r\n        - decorator:         5.1.1\r\n        - dill:              0.3.7\r\n        - distance:          0.1.3\r\n        - docker-pycreds:    0.4.0\r\n        - docopt:            0.6.2\r\n        - docutils:          0.20.1\r\n        - editdistance:      0.6.2\r\n        - einops:            0.7.0\r\n        - exceptiongroup:    1.1.3\r\n        - executing:         2.0.1\r\n        - faiss-cpu:         1.7.4\r\n        - fasteners:         0.19\r\n        - fasttext:          0.9.2\r\n        - filelock:          3.13.1\r\n        - flask:             2.2.5\r\n        - flask-restful:     0.3.10\r\n        - fonttools:         4.44.0\r\n        - frozenlist:        1.4.0\r\n        - fsspec:            2023.10.0\r\n        - ftfy:              6.1.1\r\n        - g2p-en:            2.1.0\r\n        - gdown:             4.7.1\r\n        - gitdb:             4.0.11\r\n        - gitpython:         3.1.40\r\n        - google-auth:       2.23.4\r\n        - google-auth-oauthlib: 1.1.0\r\n        - grpcio:            1.59.2\r\n        - h5py:              3.10.0\r\n        - huggingface-hub:   0.18.0\r\n        - hydra-core:        1.3.2\r\n        - idna:              3.4\r\n        - ijson:             3.2.3\r\n        - imagesize:         1.4.1\r\n        - inflect:           7.0.0\r\n        - iniconfig:         2.0.0\r\n        - ipython:           8.17.2\r\n        - ipywidgets:        8.1.1\r\n        - isort:             5.12.0\r\n        - itsdangerous:      2.1.2\r\n        - jedi:              0.19.1\r\n        - jieba:             0.42.1\r\n        - jinja2:            3.1.2\r\n        - jiwer:             2.5.2\r\n        - jmespath:          1.0.1\r\n        - joblib:            1.3.2\r\n        - jupyterlab-widgets: 3.0.9\r\n        - kaldi-python-io:   1.2.2\r\n        - kaldiio:           2.18.0\r\n        - kiwisolver:        1.4.5\r\n        - kornia:            0.7.0\r\n        - latexcodec:        2.0.1\r\n        - lazy-loader:       0.3\r\n        - levenshtein:       0.22.0\r\n        - librosa:           0.10.1\r\n        - lightning-utilities: 0.9.0\r\n        - llvmlite:          0.41.1\r\n        - loguru:            0.7.2\r\n        - lxml:              4.9.3\r\n        - markdown:          3.5.1\r\n        - markdown-it-py:    3.0.0\r\n        - markdown2:         2.4.10\r\n        - markupsafe:        2.1.3\r\n        - marshmallow:       3.20.1\r\n        - matplotlib:        3.8.1\r\n        - matplotlib-inline: 0.1.6\r\n        - mdurl:             0.1.2\r\n        - megatron-core:     0.3.0\r\n        - mpmath:            1.3.0\r\n        - msgpack:           1.0.7\r\n        - multidict:         6.0.4\r\n        - multiprocess:      0.70.15\r\n        - nemo-text-processing: 0.2.2rc0\r\n        - nemo-toolkit:      1.21.0rc0\r\n        - networkx:          3.2.1\r\n        - nltk:              3.8.1\r\n        - numba:             0.58.1\r\n        - numcodecs:         0.12.1\r\n        - numpy:             1.23.5\r\n        - nvidia-cublas-cu12: 12.1.3.1\r\n        - nvidia-cuda-cupti-cu12: 12.1.105\r\n        - nvidia-cuda-nvrtc-cu12: 12.1.105\r\n        - nvidia-cuda-runtime-cu12: 12.1.105\r\n        - nvidia-cudnn-cu12: 8.9.2.26\r\n        - nvidia-cufft-cu12: 11.0.2.54\r\n        - nvidia-curand-cu12: 10.3.2.106\r\n        - nvidia-cusolver-cu12: 11.4.5.107\r\n        - nvidia-cusparse-cu12: 12.1.0.106\r\n        - nvidia-nccl-cu12:  2.18.1\r\n        - nvidia-nvjitlink-cu12: 12.3.52\r\n        - nvidia-nvtx-cu12:  12.1.105\r\n        - oauthlib:          3.2.2\r\n        - omegaconf:         2.3.0\r\n        - onnx:              1.15.0\r\n        - opencc:            1.1.6\r\n        - packaging:         23.2\r\n        - pandas:            2.1.2\r\n        - pangu:             4.0.6.1\r\n        - parameterized:     0.9.0\r\n        - parso:             0.8.3\r\n        - pathspec:          0.11.2\r\n        - pathtools:         0.1.2\r\n        - pexpect:           4.8.0\r\n        - pillow:            10.1.0\r\n        - pip:               23.3.1\r\n        - plac:              1.4.1\r\n        - platformdirs:      3.11.0\r\n        - pluggy:            1.3.0\r\n        - pooch:             1.8.0\r\n        - portalocker:       2.8.2\r\n        - progress:          1.6\r\n        - prompt-toolkit:    3.0.39\r\n        - protobuf:          4.23.4\r\n        - psutil:            5.9.6\r\n        - ptyprocess:        0.7.0\r\n        - pure-eval:         0.2.2\r\n        - pyannote.core:     5.0.0\r\n        - pyannote.database: 5.0.1\r\n        - pyannote.metrics:  3.2.1\r\n        - pyarrow:           14.0.0\r\n        - pyasn1:            0.5.0\r\n        - pyasn1-modules:    0.3.0\r\n        - pybind11:          2.11.1\r\n        - pybtex:            0.24.0\r\n        - pybtex-docutils:   1.0.3\r\n        - pycparser:         2.21\r\n        - pydantic:          1.10.13\r\n        - pydub:             0.25.1\r\n        - pygments:          2.16.1\r\n        - pynini:            2.1.5\r\n        - pyparsing:         3.1.1\r\n        - pypinyin:          0.49.0\r\n        - pypinyin-dict:     0.6.0\r\n        - pysocks:           1.7.1\r\n        - pytest:            7.4.3\r\n        - pytest-runner:     6.0.0\r\n        - python-dateutil:   2.8.2\r\n        - pytorch-lightning: 2.1.0\r\n        - pytz:              2023.3.post1\r\n        - pyyaml:            6.0.1\r\n        - rapidfuzz:         2.13.7\r\n        - regex:             2023.10.3\r\n        - requests:          2.31.0\r\n        - requests-oauthlib: 1.3.1\r\n        - rich:              13.6.0\r\n        - rouge-score:       0.1.2\r\n        - rsa:               4.9\r\n        - ruamel.yaml:       0.18.5\r\n        - ruamel.yaml.clib:  0.2.8\r\n        - s3transfer:        0.7.0\r\n        - sacrebleu:         2.3.1\r\n        - sacremoses:        0.1.1\r\n        - safetensors:       0.4.0\r\n        - scikit-learn:      1.3.2\r\n        - scipy:             1.11.3\r\n        - sentence-transformers: 2.2.2\r\n        - sentencepiece:     0.1.99\r\n        - sentry-sdk:        1.34.0\r\n        - setproctitle:      1.3.3\r\n        - setuptools:        68.2.2\r\n        - shellingham:       1.5.4\r\n        - six:               1.16.0\r\n        - smmap:             5.0.1\r\n        - snowballstemmer:   2.2.0\r\n        - sortedcontainers:  2.4.0\r\n        - soundfile:         0.12.1\r\n        - soupsieve:         2.5\r\n        - sox:               1.4.1\r\n        - soxr:              0.3.7\r\n        - sphinx:            7.2.6\r\n        - sphinxcontrib-applehelp: 1.0.7\r\n        - sphinxcontrib-bibtex: 2.6.1\r\n        - sphinxcontrib-devhelp: 1.0.5\r\n        - sphinxcontrib-htmlhelp: 2.0.4\r\n        - sphinxcontrib-jsmath: 1.0.1\r\n        - sphinxcontrib-qthelp: 1.0.6\r\n        - sphinxcontrib-serializinghtml: 1.1.9\r\n        - stack-data:        0.6.3\r\n        - sympy:             1.12\r\n        - tabulate:          0.9.0\r\n        - tensorboard:       2.15.1\r\n        - tensorboard-data-server: 0.7.2\r\n        - tensorstore:       0.1.45\r\n        - termcolor:         2.3.0\r\n        - text-unidecode:    1.3\r\n        - textdistance:      4.6.0\r\n        - texterrors:        0.4.4\r\n        - threadpoolctl:     3.2.0\r\n        - tokenizers:        0.13.3\r\n        - toml:              0.10.2\r\n        - tomli:             2.0.1\r\n        - torch:             2.1.0\r\n        - torchaudio:        2.1.0\r\n        - torchmetrics:      1.2.0\r\n        - torchvision:       0.16.0\r\n        - tqdm:              4.66.1\r\n        - traitlets:         5.13.0\r\n        - transformers:      4.33.3\r\n        - triton:            2.1.0\r\n        - typed-ast:         1.5.5\r\n        - typer:             0.9.0\r\n        - typing-extensions: 4.8.0\r\n        - tzdata:            2023.3\r\n        - urllib3:           2.0.7\r\n        - wandb:             0.15.12\r\n        - wcwidth:           0.2.9\r\n        - webdataset:        0.1.62\r\n        - werkzeug:          3.0.1\r\n        - wget:              3.2\r\n        - wheel:             0.41.3\r\n        - widgetsnbextension: 4.0.9\r\n        - wrapt:             1.15.0\r\n        - xxhash:            3.4.1\r\n        - yarl:              1.9.2\r\n        - youtokentome:      1.0.6\r\n        - zarr:              2.16.1\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         \r\n        - python:            3.10.0\r\n        - release:           4.19.0-25-cloud-amd64\r\n        - version:           #1 SMP Debian 4.19.289-2 (2023-08-08)\r\n\r\n</details>\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18941/comments",
    "author": "ganymedenet",
    "comments": [
      {
        "user": "ganymedenet",
        "created_at": "2023-11-03T22:53:52Z",
        "body": "solved. I missed the Nemo's config requirement - `pytorch-lightning<=2.0.7,>=2.0. `\r\nMaybe someone will encounter the same issue. Thanks"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-11-03T23:01:48Z",
        "body": "NeMo is in the process of upgrading to 2.1. So this will be resolved eventually on their side."
      }
    ]
  },
  {
    "number": 18926,
    "title": "TPU FSDP support for Fabric requires major refactoring of training script(s)",
    "created_at": "2023-11-02T15:52:11Z",
    "closed_at": "2023-11-02T20:47:13Z",
    "labels": [
      "question",
      "strategy: xla"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18926",
    "body": "### Outline & Motivation\n\nLooking into porting a Fabric training script that works with FSDP on GPUs to TPUs. The script is written with the Fabric API, and I was hoping that minimal changes would support running training with FSDP on TPUs. I've resolved a few problems and can now get to the point where Fabric complains about:\r\n\r\n`TypeError: To use the `XLAFSDPStrategy` strategy, `.launch()` needs to be called with a function that contains the code to launch in processes.`\r\n\r\nFor FSDP on NCCL/CUDA, the script is started with torchrun, which creates the different processes, so the code is not organized in a function I could give to the fabric.launch() method. Refactoring is painful because of many variables defined in local scope, some of them which cannot be pickled. \r\n\r\nI assume that it is quite common to use fabric.launch() without function arguments across the fabric user base (I had not realized the method accepted a function after reading many tutorials and docs).\n\n### Pitch\n\nWould it be possible to support the launch method with no argument for FSDP TPU when the training script has already been started in multiple processes, or does XLA training require additional coordination between the processes that only a fabric process spawn can support?\n\n### Additional context\n\n_No response_\n\ncc @carmocca @JackCaoG @Liyang90 @gkroiz @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18926/comments",
    "author": "fabienGenhealth",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2023-11-02T20:47:13Z",
        "body": "This restriction would be lifted with #18335. But it's unavoidable until then because XLA requires spawning processes.\r\nI'll go ahead and close this, but feel free to track #18335 if rewriting into a function is a no-go for you."
      }
    ]
  },
  {
    "number": 18890,
    "title": "ModuleNotFoundError: No module named 'lightning' in lightning container image",
    "created_at": "2023-10-30T07:58:12Z",
    "closed_at": "2023-11-04T14:29:38Z",
    "labels": [
      "question",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18890",
    "body": "### Bug description\n\n`lightning.pytorch` module is installed in lightning container images as pytorch_lightning, thus it is not compatible with the documentation. \r\n\r\nIn order for import to work in a container image, the import should be of the following form:\r\n\r\n```python\r\nfrom lightning.pytorch.loggers import CSVLogger\r\n```\r\n\r\nWhile the documentation states:\r\n\r\n```python\r\nfrom pytorch_lightning.loggers import CSVLogger\r\n```\n\n### What version are you seeing the problem on?\n\nv2.1\n\n### How to reproduce the bug\n\n```python\ndocker run -it --rm pytorchlightning/pytorch_lightning:latest-py3.10-torch2.0-cuda12.0.1\r\npython -c \"from lightning.pytorch.loggers import CSVLogger\"\n```\n\n\n### Error messages and logs\n\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'lightning'\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n        - GPU:               None\r\n        - available:         False\r\n        - version:           11.7\r\n* Lightning:\r\n        - lightning-utilities: 0.9.0\r\n        - pytorch-lightning: 2.1.0rc0\r\n        - torch:             2.0.1\r\n        - torchmetrics:      1.0.3\r\n* Packages:\r\n        - absl-py:           2.0.0\r\n        - aiohttp:           3.8.5\r\n        - aiosignal:         1.3.1\r\n        - antlr4-python3-runtime: 4.9.3\r\n        - anyio:             3.7.1\r\n        - async-timeout:     4.0.3\r\n        - attrs:             23.1.0\r\n        - cachetools:        5.3.1\r\n        - certifi:           2019.11.28\r\n        - chardet:           3.0.4\r\n        - charset-normalizer: 3.3.0\r\n        - click:             8.1.7\r\n        - cloudpickle:       2.2.1\r\n        - cmake:             3.27.6\r\n        - coloredlogs:       15.0.1\r\n        - contourpy:         1.1.1\r\n        - coverage:          7.3.1\r\n        - cycler:            0.12.0\r\n        - dbus-python:       1.2.16\r\n        - deepspeed:         0.9.3\r\n        - docstring-parser:  0.15\r\n        - exceptiongroup:    1.1.3\r\n        - fastapi:           0.103.2\r\n        - filelock:          3.12.4\r\n        - flatbuffers:       23.5.26\r\n        - fonttools:         4.43.0\r\n        - frozenlist:        1.4.0\r\n        - fsspec:            2023.9.2\r\n        - google-auth:       2.23.2\r\n        - google-auth-oauthlib: 1.0.0\r\n        - grpcio:            1.59.0\r\n        - h11:               0.14.0\r\n        - hjson:             3.1.0\r\n        - humanfriendly:     10.0\r\n        - hydra-core:        1.3.2\r\n        - idna:              2.8\r\n        - importlib-resources: 6.1.0\r\n        - iniconfig:         2.0.0\r\n        - jinja2:            3.1.2\r\n        - joblib:            1.3.2\r\n        - jsonargparse:      4.25.0\r\n        - kiwisolver:        1.4.5\r\n        - lightning-utilities: 0.9.0\r\n        - lit:               17.0.2\r\n        - markdown:          3.4.4\r\n        - markdown-it-py:    3.0.0\r\n        - markupsafe:        2.1.3\r\n        - matplotlib:        3.7.3\r\n        - mdurl:             0.1.2\r\n        - mpmath:            1.3.0\r\n        - multidict:         6.0.4\r\n        - networkx:          3.1\r\n        - ninja:             1.11.1\r\n        - numpy:             1.26.0\r\n        - nvidia-cublas-cu11: 11.10.3.66\r\n        - nvidia-cuda-cupti-cu11: 11.7.101\r\n        - nvidia-cuda-nvrtc-cu11: 11.7.99\r\n        - nvidia-cuda-runtime-cu11: 11.7.99\r\n        - nvidia-cudnn-cu11: 8.5.0.96\r\n        - nvidia-cufft-cu11: 10.9.0.58\r\n        - nvidia-curand-cu11: 10.2.10.91\r\n        - nvidia-cusolver-cu11: 11.4.0.1\r\n        - nvidia-cusparse-cu11: 11.7.4.91\r\n        - nvidia-nccl-cu11:  2.14.3\r\n        - nvidia-nvtx-cu11:  11.7.91\r\n        - oauthlib:          3.2.2\r\n        - omegaconf:         2.3.0\r\n        - onnx:              1.14.1\r\n        - onnxruntime:       1.16.0\r\n        - packaging:         23.1\r\n        - pandas:            2.1.1\r\n        - pillow:            10.0.1\r\n        - pip:               23.2.1\r\n        - pluggy:            1.3.0\r\n        - protobuf:          4.24.3\r\n        - psutil:            5.9.5\r\n        - py-cpuinfo:        9.0.0\r\n        - pyasn1:            0.5.0\r\n        - pyasn1-modules:    0.3.0\r\n        - pydantic:          1.10.13\r\n        - pygments:          2.16.1\r\n        - pygobject:         3.36.0\r\n        - pyparsing:         3.1.1\r\n        - pytest:            7.4.0\r\n        - pytest-cov:        4.1.0\r\n        - pytest-random-order: 1.1.0\r\n        - pytest-rerunfailures: 12.0\r\n        - pytest-timeout:    2.1.0\r\n        - python-apt:        2.0.1+ubuntu0.20.4.1\r\n        - python-dateutil:   2.8.2\r\n        - pytorch-lightning: 2.1.0rc0\r\n        - pytz:              2023.3.post1\r\n        - pyyaml:            6.0.1\r\n        - requests:          2.22.0\r\n        - requests-oauthlib: 1.3.1\r\n        - requests-unixsocket: 0.2.0\r\n        - rich:              13.5.3\r\n        - rsa:               4.9\r\n        - scikit-learn:      1.3.1\r\n        - scipy:             1.11.3\r\n        - setuptools:        59.5.0\r\n        - six:               1.14.0\r\n        - sniffio:           1.3.0\r\n        - starlette:         0.27.0\r\n        - sympy:             1.12\r\n        - tensorboard:       2.14.1\r\n        - tensorboard-data-server: 0.7.1\r\n        - tensorboardx:      2.6.2.2\r\n        - threadpoolctl:     3.2.0\r\n        - tomli:             2.0.1\r\n        - torch:             2.0.1\r\n        - torchmetrics:      1.0.3\r\n        - tqdm:              4.66.1\r\n        - triton:            2.0.0\r\n        - typeshed-client:   2.4.0\r\n        - typing-extensions: 4.7.1\r\n        - tzdata:            2023.3\r\n        - urllib3:           1.25.8\r\n        - uvicorn:           0.23.2\r\n        - werkzeug:          3.0.0\r\n        - wget:              3.2\r\n        - wheel:             0.41.2\r\n        - yarl:              1.9.2\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         x86_64\r\n        - python:            3.10.13\r\n        - release:           5.3.0-28-generic\r\n        - version:           #30~18.04.1-Ubuntu SMP Fri Jan 17 06:14:09 UTC 2020\r\n\r\n</details>\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18890/comments",
    "author": "GuyPozner",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-10-30T13:56:30Z",
        "body": "Hey @GuyPozner \r\n\r\nTo use the `import lightning.x.y` import, you need to install the lightning package: `pip install lightning`\r\nIf you want to use the `import pytorch_lightning.x.y` style, you need to install `pip install pytorch-lightning`\r\n\r\nYou can't mix and match them. Our documentation exclusively uses the new package imports with lightning. That's the new and recommended way. "
      },
      {
        "user": "GuyPozner",
        "created_at": "2023-10-30T14:19:38Z",
        "body": "Hi @awaelchli,\r\nThanks for the fast response, I understood that. What I am thinking, is that if this is the recommended way to import, it should work as expected with the containers images that gets released. what do you think? have you reproduced the issue?"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-10-30T14:20:31Z",
        "body": "Sorry, which container images are you talking about?"
      },
      {
        "user": "GuyPozner",
        "created_at": "2023-10-31T07:05:18Z",
        "body": "this container image: pytorchlightning/pytorch_lightning:latest-py3.10-torch2.0-cuda12.0.1\r\n\r\nBut I think that this is true for every container image.\r\n\r\nThanks for the fix :)"
      }
    ]
  },
  {
    "number": 18589,
    "title": "Can't seem to change distributed backend to gloo on Windows",
    "created_at": "2023-09-19T14:08:12Z",
    "closed_at": "2023-09-19T18:28:28Z",
    "labels": [
      "question",
      "strategy: ddp",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18589",
    "body": "### Bug description\r\n\r\nI am trying to run a training module with CUDA using PyTorch Lightning, but Lightning keeps trying to use NCCL. I have tried every solution I have found online, from specifying it in the code to prepending `PL_TORCH_DISTRIBUTED_BACKEND=gloo` to the laucnh command in the terminal, but Lightning still seems to try to use NCCL. I have verified that gloo is available for use in my system. Any help would be greatly appreciated.\r\n\r\n### What version are you seeing the problem on?\r\n\r\nmaster\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nos.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\"\r\nmy_data = MyDataModule(args...)\r\nmy_model = MyModel(args...)\r\ntrainer = Trainer()\r\ntrainer.fit(my_model, my_data.train_dataloader, my_data.val_dataloader)\r\n\r\n# also pops up when running PL_TORCH_DISTRIBUTED_BACKEND=gloo python train.py\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\n```\r\n$ PL_TORCH_DISTRIBUTED_BACKEND=gloo python train.py\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\r\n  warnings.warn(\"No audio backend is available.\")\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarnin\r\ng: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the\r\nML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip\r\n install lightning[extra]` or one of them to enable TensorBoard support by default\r\n  warning_cache.warn(\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:72: PossibleUserWarning: `max_epochs` was not set.\r\nSetting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\r\n  rank_zero_warn(\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:69: UserWarning: You passed in a `v\r\nal_dataloader` but have no `validation_step`. Skipping val loop.\r\n  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\r\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\r\n  warnings.warn(\"No audio backend is available.\")\r\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\ntrain.py 62 <module>\r\ntrainer.fit(my_model, my_data.train_dataloader, my_data.val_dataloader)\r\n\r\ntrainer.py 532 fit\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\ntrain.py 62 <module>\r\ntrainer.fit(my_model, my_data.train_dataloader, my_data.val_dataloader)\r\n\r\ntrainer.py 532 fit\r\ncall._call_and_handle_interrupt(\r\n\r\ncall.py 42 _call_and_handle_interrupt\r\ncall._call_and_handle_interrupt(\r\n\r\ncall.py 42 _call_and_handle_interrupt\r\nreturn trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n\r\nsubprocess_script.py 93 launch\r\nreturn trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n\r\nreturn function(*args, **kwargs)\r\nsubprocess_script.py 93 launch\r\n\r\ntrainer.py 571 _fit_impl\r\nself._run(model, ckpt_path=ckpt_path)\r\n\r\ntrainer.py 938 _run\r\nself.strategy.setup_environment()\r\n\r\nddp.py 143 setup_environment\r\nreturn function(*args, **kwargs)\r\n\r\ntrainer.py 571 _fit_impl\r\nself._run(model, ckpt_path=ckpt_path)\r\n\r\ntrainer.py 938 _run\r\nself.strategy.setup_environment()\r\nself.setup_distributed()\r\n\r\n\r\nddp.py 143 setup_environment\r\nddp.py 191 setup_distributed\r\n_init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)\r\n\r\ndistributed.py 258 _init_dist_connection\r\nself.setup_distributed()\r\n\r\ntorch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\r\nddp.py 191 setup_distributed\r\n\r\ndistributed_c10d.py 907 init_process_group\r\n_init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)\r\n\r\ndistributed.py 258 _init_dist_connection\r\ntorch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\r\n\r\ndistributed_c10d.py 907 init_process_group\r\ndefault_pg = _new_process_group_helper(\r\n\r\ndistributed_c10d.py 1013 _new_process_group_helper\r\nraise RuntimeError(\"Distributed package doesn't have NCCL \" \"built in\")\r\n\r\nRuntimeError:\r\nDistributed package doesn't have NCCL built in\r\ndefault_pg = _new_process_group_helper(\r\n\r\ndistributed_c10d.py 1013 _new_process_group_helper\r\nraise RuntimeError(\"Distributed package doesn't have NCCL \" \"built in\")\r\n\r\nRuntimeError:\r\nDistributed package doesn't have NCCL built in\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA TITAN X (Pascal)\r\n                - NVIDIA GeForce GTX 970\r\n        - available:         True\r\n        - version:           11.8\r\n* Lightning:\r\n        - lightning:         2.0.8\r\n        - lightning-cloud:   0.5.37\r\n        - lightning-utilities: 0.9.0\r\n        - pytorch-lightning: 2.0.8\r\n        - pytorchvideo:      0.1.5\r\n        - torch:             2.0.1\r\n        - torchaudio:        2.0.2\r\n        - torchmetrics:      1.1.1\r\n        - torchvision:       0.15.2\r\n* Packages:\r\n        - aiofiles:          22.1.0\r\n        - aiohttp:           3.8.5\r\n        - aiosignal:         1.3.1\r\n        - aiosqlite:         0.18.0\r\n        - annotated-types:   0.5.0\r\n        - ansicon:           1.89.0\r\n        - anyio:             3.5.0\r\n        - argon2-cffi:       21.3.0\r\n        - argon2-cffi-bindings: 21.2.0\r\n        - arrow:             1.2.3\r\n        - asttokens:         2.0.5\r\n        - async-timeout:     4.0.3\r\n        - attrs:             22.1.0\r\n        - av:                10.0.0\r\n        - babel:             2.11.0\r\n        - backcall:          0.2.0\r\n        - backoff:           2.2.1\r\n        - beautifulsoup4:    4.12.2\r\n        - bleach:            4.1.0\r\n        - blessed:           1.20.0\r\n        - boto3:             1.28.42\r\n        - botocore:          1.31.42\r\n        - bottleneck:        1.3.5\r\n        - brotlipy:          0.7.0\r\n        - certifi:           2023.7.22\r\n        - cffi:              1.15.1\r\n        - charset-normalizer: 2.0.4\r\n        - click:             8.1.7\r\n        - colorama:          0.4.6\r\n        - comm:              0.1.2\r\n        - contourpy:         1.0.5\r\n        - croniter:          1.4.1\r\n        - cryptography:      41.0.2\r\n        - cycler:            0.11.0\r\n        - dateutils:         0.6.12\r\n        - debugpy:           1.6.7\r\n        - decorator:         5.1.1\r\n        - deepdiff:          6.4.1\r\n        - deeplake:          3.6.23\r\n        - defusedxml:        0.7.1\r\n        - dill:              0.3.7\r\n        - einops:            0.6.1\r\n        - entrypoints:       0.4\r\n        - executing:         0.8.3\r\n        - fastapi:           0.103.1\r\n        - fastjsonschema:    2.16.2\r\n        - filelock:          3.12.3\r\n        - fonttools:         4.25.0\r\n        - frozenlist:        1.4.0\r\n        - fsspec:            2023.9.0\r\n        - fvcore:            0.1.5.post20221221\r\n        - h11:               0.14.0\r\n        - huggingface-hub:   0.17.1\r\n        - humbug:            0.3.2\r\n        - idna:              3.4\r\n        - inquirer:          3.1.3\r\n        - iopath:            0.1.10\r\n        - ipykernel:         6.25.0\r\n        - ipython:           8.12.2\r\n        - ipython-genutils:  0.2.0\r\n        - ipywidgets:        8.0.4\r\n        - itsdangerous:      2.1.2\r\n        - jedi:              0.18.1\r\n        - jinja2:            3.1.2\r\n        - jinxed:            1.2.0\r\n        - jmespath:          1.0.1\r\n        - joblib:            1.2.0\r\n        - json5:             0.9.6\r\n        - jsonschema:        4.17.3\r\n        - jupyter:           1.0.0\r\n        - jupyter-client:    7.4.9\r\n        - jupyter-console:   6.6.3\r\n        - jupyter-core:      5.3.0\r\n        - jupyter-events:    0.6.3\r\n        - jupyter-server:    1.23.4\r\n        - jupyter-server-fileid: 0.9.0\r\n        - jupyter-server-ydoc: 0.8.0\r\n        - jupyter-ydoc:      0.2.4\r\n        - jupyterlab:        3.6.3\r\n        - jupyterlab-pygments: 0.1.2\r\n        - jupyterlab-server: 2.22.0\r\n        - jupyterlab-widgets: 3.0.5\r\n        - kiwisolver:        1.4.4\r\n        - lightning:         2.0.8\r\n        - lightning-cloud:   0.5.37\r\n        - lightning-utilities: 0.9.0\r\n        - llvmlite:          0.40.0\r\n        - lxml:              4.9.2\r\n        - markdown-it-py:    3.0.0\r\n        - markupsafe:        2.1.1\r\n        - matplotlib:        3.7.2\r\n        - matplotlib-inline: 0.1.6\r\n        - mdurl:             0.1.2\r\n        - mistune:           0.8.4\r\n        - mkl-fft:           1.3.6\r\n        - mkl-random:        1.2.2\r\n        - mkl-service:       2.4.0\r\n        - mpmath:            1.3.0\r\n        - multidict:         6.0.4\r\n        - multiprocess:      0.70.15\r\n        - munkres:           1.1.4\r\n        - nbclassic:         0.5.5\r\n        - nbclient:          0.5.13\r\n        - nbconvert:         6.5.4\r\n        - nbformat:          5.7.0\r\n        - nest-asyncio:      1.5.6\r\n        - networkx:          3.1\r\n        - notebook:          6.5.4\r\n        - notebook-shim:     0.2.2\r\n        - numba:             0.57.0\r\n        - numcodecs:         0.11.0\r\n        - numexpr:           2.8.4\r\n        - numpy:             1.24.3\r\n        - ordered-set:       4.1.0\r\n        - packaging:         23.1\r\n        - pandas:            2.0.3\r\n        - pandocfilters:     1.5.0\r\n        - parameterized:     0.9.0\r\n        - parso:             0.8.3\r\n        - pathos:            0.3.1\r\n        - pickleshare:       0.7.5\r\n        - pillow:            9.4.0\r\n        - pip:               23.2.1\r\n        - platformdirs:      3.10.0\r\n        - ply:               3.11\r\n        - portalocker:       2.7.0\r\n        - pox:               0.3.3\r\n        - ppft:              1.7.6.7\r\n        - pretty-errors:     1.2.25\r\n        - prometheus-client: 0.14.1\r\n        - prompt-toolkit:    3.0.36\r\n        - psutil:            5.9.0\r\n        - pure-eval:         0.2.2\r\n        - pycparser:         2.21\r\n        - pydantic:          2.1.1\r\n        - pydantic-core:     2.4.0\r\n        - pygments:          2.15.1\r\n        - pyjwt:             2.8.0\r\n        - pyopenssl:         23.2.0\r\n        - pyparsing:         3.0.9\r\n        - pyqt5:             5.15.7\r\n        - pyqt5-sip:         12.11.0\r\n        - pyrsistent:        0.18.0\r\n        - pysocks:           1.7.1\r\n        - python-dateutil:   2.8.2\r\n        - python-editor:     1.0.4\r\n        - python-json-logger: 2.0.7\r\n        - python-multipart:  0.0.6\r\n        - pytorch-lightning: 2.0.8\r\n        - pytorchvideo:      0.1.5\r\n        - pytz:              2022.7\r\n        - pywin32:           305.1\r\n        - pywinpty:          2.0.10\r\n        - pyyaml:            6.0\r\n        - pyzmq:             23.2.0\r\n        - qtconsole:         5.4.2\r\n        - qtpy:              2.2.0\r\n        - readchar:          4.0.5\r\n        - regex:             2023.8.8\r\n        - requests:          2.31.0\r\n        - rfc3339-validator: 0.1.4\r\n        - rfc3986-validator: 0.1.1\r\n        - rich:              13.5.2\r\n        - s3transfer:        0.6.2\r\n        - safetensors:       0.3.3\r\n        - scikit-learn:      1.2.2\r\n        - scipy:             1.11.1\r\n        - send2trash:        1.8.0\r\n        - setuptools:        68.0.0\r\n        - sip:               6.6.2\r\n        - six:               1.16.0\r\n        - sniffio:           1.2.0\r\n        - soupsieve:         2.4\r\n        - stack-data:        0.2.0\r\n        - starlette:         0.27.0\r\n        - starsessions:      1.3.0\r\n        - sympy:             1.11.1\r\n        - tabulate:          0.9.0\r\n        - termcolor:         2.3.0\r\n        - terminado:         0.17.1\r\n        - threadpoolctl:     2.2.0\r\n        - tinycss2:          1.2.1\r\n        - tokenizers:        0.13.3\r\n        - toml:              0.10.2\r\n        - torch:             2.0.1\r\n        - torchaudio:        2.0.2\r\n        - torchmetrics:      1.1.1\r\n        - torchvision:       0.15.2\r\n        - tornado:           6.3.2\r\n        - tqdm:              4.65.0\r\n        - traitlets:         5.7.1\r\n        - transformers:      4.33.1\r\n        - typing-extensions: 4.7.1\r\n        - tzdata:            2023.3\r\n        - urllib3:           1.26.16\r\n        - uvicorn:           0.23.2\r\n        - wcwidth:           0.2.5\r\n        - webencodings:      0.5.1\r\n        - websocket-client:  0.58.0\r\n        - websockets:        11.0.3\r\n        - wheel:             0.38.4\r\n        - widgetsnbextension: 4.0.5\r\n        - win-inet-pton:     1.1.0\r\n        - y-py:              0.5.9\r\n        - yacs:              0.1.8\r\n        - yarl:              1.9.2\r\n        - ypy-websocket:     0.8.2\r\n* System:\r\n        - OS:                Windows\r\n        - architecture:\r\n                - 64bit\r\n                - WindowsPE\r\n        - processor:         Intel64 Family 6 Model 85 Stepping 4, GenuineIntel\r\n        - python:            3.11.4\r\n        - release:           10\r\n        - version:           10.0.19041\r\n\r\n</details>\n\ncc @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18589/comments",
    "author": "amansingh427",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-09-19T17:27:00Z",
        "body": "Hey @amansingh427 \r\nIn the latest Lightning versions, the backend can no longer be set through the environment variable `PL_TORCH_DISTRIBUTED_BACKEND`. You can set it like so:\r\n\r\n```py\r\nfrom lightning.pytorch.strategies import DDPStrategy\r\n\r\ntrainer = Trainer(strategy=DDPStrategy(process_group_backend=\"gloo\"), ...)\r\n```"
      },
      {
        "user": "amansingh427",
        "created_at": "2023-09-19T18:28:29Z",
        "body": "Fixed. Thanks!"
      }
    ]
  },
  {
    "number": 18457,
    "title": "Example on \"how to have untrainable parameters in the pl.LightningModule with DDP\" or \"example showing multiple resnets inside a DDP pl.LightningModule\"?",
    "created_at": "2023-09-01T17:47:03Z",
    "closed_at": "2023-09-14T18:36:07Z",
    "labels": [
      "question",
      "strategy: ddp",
      "ver: 2.0.x",
      "ver: 1.6.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18457",
    "body": "### Bug description\n\nI am trying to have an ensemble learning network with fixed pre-trained backbones that aren't trainable and whose extracted features are getting passed into another learnable model. All of this happens inside the pl.LightningModule. I am trying to train via DDP.\r\n\r\nI am looking for examples on \"how to have untrainable parameters in the pl.LightningModule with DDP\" or \"example showing multiple resents inside a DDP pl.LightningModule\"?\n\n### What version are you seeing the problem on?\n\nv1.6, v2.0\n\n### How to reproduce the bug\n\n```python\nI am looking for an example code.\n```\n\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\nRuntimeError: It looks like your LightningModule has parameters that were not used in producing the loss returned by training_step. If this is intentional, you must enable the detection of unused parameters in DDP, either by setting the string value `strategy='ddp_find_unused_parameters_true'` or by setting the flag in the strategy with `strategy=DDPStrategy(find_unused_parameters=True)`.\r\n,\n\n### Environment\n\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- NVIDIA RTX A6000\r\n\t\t- NVIDIA RTX A6000\r\n\t\t- NVIDIA RTX A6000\r\n\t\t- NVIDIA RTX A6000\r\n\t\t- NVIDIA RTX A6000\r\n\t\t- NVIDIA RTX A6000\r\n\t\t- NVIDIA RTX A6000\r\n\t\t- NVIDIA RTX A6000\r\n\t- available:         True\r\n\t- version:           11.6\r\n* Lightning:\r\n\t- facenet-pytorch:   2.5.3\r\n\t- lightning-utilities: 0.9.0\r\n\t- pytorch-lightning: 2.0.2\r\n\t- torch:             1.13.1\r\n\t- torchaudio:        0.13.1\r\n\t- torchmetrics:      0.11.4\r\n\t- torchvision:       0.14.1\r\n* Packages:\r\n\t- absl-py:           1.4.0\r\n\t- aiohttp:           3.8.5\r\n\t- aiosignal:         1.3.1\r\n\t- antlr4-python3-runtime: 4.9.3\r\n\t- appdirs:           1.4.4\r\n\t- astroid:           2.15.6\r\n\t- astunparse:        1.6.3\r\n\t- async-timeout:     4.0.2\r\n\t- attrs:             23.1.0\r\n\t- bottleneck:        1.3.5\r\n\t- brotlipy:          0.7.0\r\n\t- cachetools:        5.3.1\r\n\t- certifi:           2022.12.7\r\n\t- cffi:              1.15.1\r\n\t- charset-normalizer: 2.0.4\r\n\t- click:             8.1.6\r\n\t- cloudpickle:       2.0.0\r\n\t- continuum:         1.2.7\r\n\t- contourpy:         1.0.5\r\n\t- cryptography:      38.0.4\r\n\t- cycler:            0.11.0\r\n\t- cytoolz:           0.12.0\r\n\t- dask:              2022.7.0\r\n\t- datasets:          2.14.4\r\n\t- dill:              0.3.7\r\n\t- dlutils:           0.0.12\r\n\t- dm-tree:           0.1.8\r\n\t- docker-pycreds:    0.4.0\r\n\t- dodgy:             0.2.1\r\n\t- dominate:          2.7.0\r\n\t- einops:            0.6.1\r\n\t- exceptiongroup:    1.1.3\r\n\t- facenet-pytorch:   2.5.3\r\n\t- filelock:          3.12.2\r\n\t- flake8:            5.0.4\r\n\t- flake8-polyfill:   1.0.2\r\n\t- flit-core:         3.6.0\r\n\t- fonttools:         4.25.0\r\n\t- frozenlist:        1.4.0\r\n\t- fsspec:            2022.11.0\r\n\t- gast:              0.5.4\r\n\t- gitdb:             4.0.10\r\n\t- gitpython:         3.1.32\r\n\t- google-auth:       2.22.0\r\n\t- google-auth-oauthlib: 1.0.0\r\n\t- grpcio:            1.56.2\r\n\t- h5py:              3.9.0\r\n\t- huggingface-hub:   0.16.4\r\n\t- hydra-core:        1.3.2\r\n\t- idna:              3.4\r\n\t- imagecodecs:       2021.8.26\r\n\t- imagehash:         4.3.1\r\n\t- imageio:           2.26.0\r\n\t- iniconfig:         2.0.0\r\n\t- isort:             5.12.0\r\n\t- joblib:            1.1.1\r\n\t- kiwisolver:        1.4.4\r\n\t- lazy-object-proxy: 1.9.0\r\n\t- lightning-utilities: 0.9.0\r\n\t- locket:            1.0.0\r\n\t- markdown:          3.4.4\r\n\t- markupsafe:        2.1.3\r\n\t- matplotlib:        3.6.2\r\n\t- mccabe:            0.7.0\r\n\t- mkl-fft:           1.3.1\r\n\t- mkl-random:        1.2.2\r\n\t- mkl-service:       2.4.0\r\n\t- multidict:         6.0.4\r\n\t- multiprocess:      0.70.15\r\n\t- munkres:           1.1.4\r\n\t- mypy:              1.5.1\r\n\t- mypy-extensions:   1.0.0\r\n\t- networkx:          2.8.4\r\n\t- ninja:             1.11.1\r\n\t- numexpr:           2.8.4\r\n\t- numpy:             1.23.5\r\n\t- nvidia-dali-cuda110: 1.29.0\r\n\t- oauthlib:          3.2.2\r\n\t- omegaconf:         2.3.0\r\n\t- opencv-python:     4.7.0.68\r\n\t- packaging:         22.0\r\n\t- pandas:            1.5.2\r\n\t- partd:             1.2.0\r\n\t- pathtools:         0.1.2\r\n\t- pep8-naming:       0.10.0\r\n\t- pillow:            9.3.0\r\n\t- pip:               23.0.1\r\n\t- platformdirs:      3.10.0\r\n\t- pluggy:            1.3.0\r\n\t- ply:               3.11\r\n\t- pooch:             1.4.0\r\n\t- prospector:        1.10.2\r\n\t- protobuf:          4.23.4\r\n\t- psutil:            5.9.5\r\n\t- pyarrow:           13.0.0\r\n\t- pyasn1:            0.5.0\r\n\t- pyasn1-modules:    0.3.0\r\n\t- pycodestyle:       2.9.1\r\n\t- pycparser:         2.21\r\n\t- pydeprecate:       0.3.2\r\n\t- pydocstyle:        6.3.0\r\n\t- pyeer:             0.5.5\r\n\t- pyflakes:          2.5.0\r\n\t- pylint:            2.17.5\r\n\t- pylint-celery:     0.3\r\n\t- pylint-django:     2.5.3\r\n\t- pylint-flask:      0.6\r\n\t- pylint-plugin-utils: 0.7\r\n\t- pyopenssl:         22.0.0\r\n\t- pyparsing:         3.0.9\r\n\t- pyqt5-sip:         12.11.0\r\n\t- pysocks:           1.7.1\r\n\t- pytest:            7.4.0\r\n\t- pytest-mock:       3.11.1\r\n\t- python-dateutil:   2.8.2\r\n\t- pytorch-lightning: 2.0.2\r\n\t- pytz:              2022.7\r\n\t- pywavelets:        1.4.1\r\n\t- pyyaml:            6.0\r\n\t- requests:          2.28.1\r\n\t- requests-oauthlib: 1.3.1\r\n\t- requirements-detector: 1.2.2\r\n\t- rsa:               4.9\r\n\t- safetensors:       0.3.1\r\n\t- scikit-image:      0.19.3\r\n\t- scikit-learn:      1.2.1\r\n\t- scipy:             1.10.0\r\n\t- semver:            3.0.1\r\n\t- sentry-sdk:        1.29.2\r\n\t- setoptconf-tmp:    0.3.1\r\n\t- setproctitle:      1.3.2\r\n\t- setuptools:        65.6.3\r\n\t- sip:               6.6.2\r\n\t- six:               1.16.0\r\n\t- smmap:             5.0.0\r\n\t- snowballstemmer:   2.2.0\r\n\t- solo-learn:        1.0.6\r\n\t- tensorboard:       2.13.0\r\n\t- tensorboard-data-server: 0.7.1\r\n\t- threadpoolctl:     2.2.0\r\n\t- tifffile:          2021.7.2\r\n\t- timm:              0.9.2\r\n\t- toml:              0.10.2\r\n\t- tomli:             2.0.1\r\n\t- tomlkit:           0.12.1\r\n\t- toolz:             0.12.0\r\n\t- torch:             1.13.1\r\n\t- torchaudio:        0.13.1\r\n\t- torchmetrics:      0.11.4\r\n\t- torchvision:       0.14.1\r\n\t- tornado:           6.2\r\n\t- tqdm:              4.64.1\r\n\t- typing-extensions: 4.4.0\r\n\t- urllib3:           1.26.14\r\n\t- wandb:             0.15.9\r\n\t- werkzeug:          2.3.6\r\n\t- wheel:             0.37.1\r\n\t- wrapt:             1.15.0\r\n\t- xxhash:            3.3.0\r\n\t- yacs:              0.1.6\r\n\t- yarl:              1.9.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.10.9\r\n\t- release:           5.15.0-82-generic\r\n\t- version:           #91-Ubuntu SMP Mon Aug 14 14:14:14 UTC 2023\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18457/comments",
    "author": "LightingMc",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-09-08T17:16:01Z",
        "body": "@LightingMc Did you disable grad on these models/modules that you don't use? Here is a trivial example that explains this:\r\n\r\n\r\n```py\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom lightning.pytorch import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n        # this layer 2 is \"unused\"\r\n        self.layer2 = torch.nn.Linear(32, 2)\r\n        # IMPORTANT: Without this, parameters get recorded by DDP and you get the error\r\n        # self.layer2.requires_grad_(False)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self.layer(batch).sum()  #  + self.layer2(batch).sum()\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        # note: we are passing in all params, including the one that is not used\r\n        return torch.optim.SGD(self.parameters(), lr=0.1)\r\n\r\n\r\ntrain_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\nmodel = BoringModel()\r\ntrainer = Trainer(\r\n    default_root_dir=os.getcwd(),\r\n    max_epochs=1,\r\n    enable_model_summary=False,\r\n    accelerator=\"cpu\",\r\n    devices=2,\r\n)\r\ntrainer.fit(model, train_data)\r\n\r\n\r\n\r\n```\r\n\r\nI can update the error message to include the remark that you should disable grad on modules that you don't use for producing the loss.\t"
      },
      {
        "user": "LightingMc",
        "created_at": "2023-09-08T17:48:48Z",
        "body": "Hey, i was able to fix my issue. Not really sure why but defining the model like this was giving me errors wrt unused parameters.:\r\n\r\n```py\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n```\r\n\r\n\r\nHowever, if I defined my models like below I stopped getting the error.\r\n\r\n```py\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super(BoringModel,self).__init__()\r\n```"
      }
    ]
  },
  {
    "number": 18378,
    "title": "Fabric cannot launch with specified gpu indices",
    "created_at": "2023-08-24T00:21:36Z",
    "closed_at": "2023-08-24T12:48:57Z",
    "labels": [
      "question",
      "3rd party",
      "fabric",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18378",
    "body": "### Bug description\n\nHi,\r\n\r\nI launch fabric with specified gpu indices, and get RuntineError messges as follows.\r\nHow can I solve this issue? Thanks.\r\n\r\nSoftware version:\r\n```\r\ndeepspeed 0.10.0\r\nlightning 2.0.7\r\npytorch 2.0.1\r\npython 3.10.9\r\n```\r\n\r\nThe code looks like:\r\n```python\r\nimport lightning as L\r\nfabric = L.Fabric(accelerator=\"cuda\", devices=\"0,1,4,5\", strategy='deepspeed')\r\nfabric.launch()\r\n```\r\n\r\nThe error messges and logs is:\r\n```\r\n[2023-08-24 07:56:20,780] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/test_fab.py\", line 45, in <module>\r\n    fabric.launch()\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 687, in launch\r\n    return self._strategy.launcher.launch(function, *args, **kwargs)\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/launchers/subprocess_script.py\", line 92, in launch\r\n    return function(*args, **kwargs)\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 776, in _run_with_setup\r\n    self._strategy.setup_environment()\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/ddp.py\", line 113, in setup_environment\r\n    self._setup_distributed()\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/deepspeed.py\", line 579, in _setup_distributed\r\n    _validate_device_index_selection(self.parallel_devices)\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/deepspeed.py\", line 821, in _validate_device_index_selection\r\n    raise RuntimeError(\r\nRuntimeError: The selected device indices [0, 1, 4, 5] don't match the local rank values of processes. If you need to select GPUs at a specific index, set the `CUDA_VISIBLE_DEVICES` environment variable instead. For example: `CUDA_VISIBLE_DEVICES=0,1,4,5`.\r\n```\r\n\n\n### What version are you seeing the problem on?\n\nv2.0\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n_No response_\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9): \r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @carmocca @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18378/comments",
    "author": "seraphzl",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-08-24T01:28:38Z",
        "body": "@seraphzl This is expected, and is not supported by DeepSpeed. The error comes directly from Fabric and is informing you about the limitation. In addition, it suggests how you can select the devices via the environment variable. If you read the error message carefully:\r\n\r\n```\r\nRuntimeError: The selected device indices [0, 1, 4, 5] don't match the local rank values of processes. If you need to select GPUs at a specific index, set the `CUDA_VISIBLE_DEVICES` environment variable instead. For example: `CUDA_VISIBLE_DEVICES=0,1,4,5`.\r\n```\r\n\r\nSo basically you remove the setting for `devices=[0, 1, 4, 5]` and launch with `CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py`. Please let me know if that works.\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-08-24T12:48:58Z",
        "body": "@seraphzl I'm closing the issue but if you have any further questions, don't hesitate to ask. "
      },
      {
        "user": "seraphzl",
        "created_at": "2023-08-24T13:55:21Z",
        "body": "> @seraphzl This is expected, and is not supported by DeepSpeed. The error comes directly from Fabric and is informing you about the limitation. In addition, it suggests how you can select the devices via the environment variable. If you read the error message carefully:\r\n> \r\n> ```\r\n> RuntimeError: The selected device indices [0, 1, 4, 5] don't match the local rank values of processes. If you need to select GPUs at a specific index, set the `CUDA_VISIBLE_DEVICES` environment variable instead. For example: `CUDA_VISIBLE_DEVICES=0,1,4,5`.\r\n> ```\r\n> \r\n> So basically you remove the setting for `devices=[0, 1, 4, 5]` and launch with `CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py`. Please let me know if that works.\r\n\r\n@awaelchli Using the CUDA_VISIBLE_DEVICES environment setting to launch the training works. Thank you for reply."
      }
    ]
  },
  {
    "number": 18170,
    "title": "Basic ProgressBar does not work",
    "created_at": "2023-07-26T21:40:27Z",
    "closed_at": "2023-07-27T11:20:36Z",
    "labels": [
      "question",
      "progress bar: tqdm",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18170",
    "body": "### Bug description\r\n\r\nVersion of `pytorch_lightning==2.0.6`, `tqdm==4.65.0`\r\nI want to display the training progress of my models and the basic ProgressBar from pytorch_lightning.callbacks does not work (nothing shows up).\r\nHowever, when I switched to RichProgressBar, the rich progress bar shows up.\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.0\r\n\r\n### How to reproduce the bug\r\n\r\n`python\r\npytorch_lightning.callbacks import ProgressBar` does not show up anything.\r\n\r\n`from pytorch_lightning.callbacks import RichProgressBar` can show the training progress.\r\n\r\n\r\n### Error messages and logs\r\n\r\n\r\nNothing shows up for ```ProgressBar```.\r\n\r\n### Environment\r\n`pytorch_lightning==2.0.6`\r\n`tqdm==4.65.0`\r\n\r\n### More info\r\nRunning things in a Linux environment, with an A40 GPU.\n\ncc @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18170/comments",
    "author": "dnaihao",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-07-26T22:22:25Z",
        "body": "Hi @dnaihao \r\n\r\nYou don't need to import the progress bar if you want to use tqdm. It gets enabled by default if you just run the Trainer :)\r\nThe reason why you don't see anything is because you accidentally imported the base class, but you probably wanted to import `pytorch_lightning.callbacks import TQDMProgressBar`. (But again, it is the default, so it's not necessary technically). \r\n\r\nLet me know if that resolves your problem :)"
      },
      {
        "user": "dnaihao",
        "created_at": "2023-07-27T01:30:52Z",
        "body": "Thanks for the explanation. \r\nBut for the previous versions of Pytorch Lightening (probably around 1.6.X), I imported ProgressBar and it worked out perfectly fine.\r\nBut sure, I am using the TODMProgressBar now and it works fine."
      },
      {
        "user": "awaelchli",
        "created_at": "2023-07-27T11:20:36Z",
        "body": "Yes, you are right, in previous versions prior to 2.0, ProgressBar was the tqdm-version. Later, the name of the base class was changed from ProgressBarBase to just ProgressBar #17058. Apologies if this caused confusion!"
      }
    ]
  },
  {
    "number": 18146,
    "title": "ImportError: cannot import name 'ProgressBarBase' from 'pytorch_lightning.callbacks.progress'",
    "created_at": "2023-07-24T13:40:05Z",
    "closed_at": "2023-07-31T13:36:49Z",
    "labels": [
      "question",
      "docs",
      "ver: 1.6.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18146",
    "body": "### Bug description\n\nHello, when I try to train a model, I failed to import 'ProgressBarBase', the message shows below:\r\n`Traceback (most recent call last):\r\n  File \"D:\\LoFTR-master\\train.py\", line 8, in <module>\r\n    import pytorch_lightning as pl\r\n  File \"C:\\Users\\Administrator\\.conda\\envs\\patch\\lib\\site-packages\\pytorch_lightning\\__init__.py\", line 21, in <module>\r\n    from pytorch_lightning.callbacks import Callback  # noqa: E402\r\n  File \"C:\\Users\\Administrator\\.conda\\envs\\patch\\lib\\site-packages\\pytorch_lightning\\callbacks\\__init__.py\", line 23, in <module>\r\n    from pytorch_lightning.callbacks.progress import ProgressBar, ProgressBarBase\r\nImportError: cannot import name 'ProgressBarBase' from 'pytorch_lightning.callbacks.progress' (C:\\Users\\Administrator\\.conda\\envs\\patch\\lib\\site-packages\\pytorch_lightning\\callbacks\\progress\\__init__.py)\r\n`\r\nI tried both update to the newest version and to 1.3.5 version,none of them works.\r\nWhich really confused me for a long time, please help me, tks.\n\n### What version are you seeing the problem on?\n\nv1.6, v2.0\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @borda",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18146/comments",
    "author": "fu-hanghang",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-07-27T13:31:02Z",
        "body": "Hey @fu-hanghang \r\nThe ProgressBarBase got renamed to ProgressBar in Lightning 2.0 #11908 \r\n\r\nThe correct import is:\r\n```py\r\nfrom pytorch_lightning.callbacks.progress import ProgressBar \r\n```"
      }
    ]
  },
  {
    "number": 18135,
    "title": "_PrefetchDataFetcher ignores prefetch_factor",
    "created_at": "2023-07-21T21:34:48Z",
    "closed_at": "2023-07-24T15:11:19Z",
    "labels": [
      "question",
      "data handling",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18135",
    "body": "### Bug description\r\n\r\nNo matter what prefetch_factor is set for the `DataLoader` in a `LightningDataModule` wrapper, when the `_PrefetchDataFetcher` is initialized, the value is always reset to 1.\r\n\r\n\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.0\r\n\n\ncc @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18135/comments",
    "author": "botcs",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-07-23T18:56:39Z",
        "body": "@botcs The prefetching done in the trainer is independent of the prefetching in the DataLoader. The trainer prefetches 1 batch just to know in advance whether the data loader is exhausted or not, that's all. But of course, you can set any value for `DataLoader(prefetch_factor=N)` and this will be handled by PyTorch. Let me know if you have any questions."
      },
      {
        "user": "botcs",
        "created_at": "2023-07-24T15:11:19Z",
        "body": "A fair! thanks for the explanation :)"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-07-24T15:26:24Z",
        "body": "You are welcome of course, happy to help :)"
      }
    ]
  },
  {
    "number": 18066,
    "title": "Trainer wraps the weights of the model in extra initialized classes.",
    "created_at": "2023-07-12T22:37:34Z",
    "closed_at": "2023-08-26T12:56:32Z",
    "labels": [
      "question",
      "waiting on author",
      "checkpointing",
      "ver: 1.7.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18066",
    "body": "### Bug description\n\nI have a PL model. which is defined as:\r\n```python\r\nclass PL_Model(pl.LightningModule):\r\n    def __init__(self, opt):\r\n        super().__init__() \r\n        self.opt = opt\r\n        self.model = AwesomeModel(self.opt)\r\n        self.prior_function= ComputePrior(config = self.opt)\r\n  def forward(...):\r\n....#usual PL logic\r\n```\r\nI am using ModelCheckpoint callback to monitor the `val/avg_iou` metric to save the entire checkpoint and not just the weights.\r\n\r\nHowever, while loading the checkpoints for testing, in a seperate script( not after calling `.fit()`), it loads the keys in a nested manner, such as:\r\n\r\n```python\r\n{\"self.model.{conv/fc/ blah blah blah}, \r\nself.prior_function.model.{conv/fc/ blah blah blah}, }\r\n```\r\nthe same thing but this time the model is nested within another unnecessary key, which doesn't have any learnable parameters.\r\nThis throws an error while loading the model checkpoints.\r\nI can set `strict=False` to solve this, but I don't understand why this should happen.\r\nEven though, ComputePrior is `nn.Module` class, it should have its parameters sepearate, or everything should be nested inside \n\n### What version are you seeing the problem on?\n\nv1.7\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\nMissing keys\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 1.7.5\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0): 1.12.1\r\n#- Python version (e.g., 3.9): 3.8\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version: 11.3\r\n#- GPU models and configuration: 1\r\n#- How you installed Lightning(`conda`, `pip`, source): pip\r\n#- Running environment of LightningApp (e.g. local, cloud): local\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18066/comments",
    "author": "sinAshish",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-07-16T23:28:12Z",
        "body": "@sinAshish I'm sorry I don't quite understand your explanation. How did you end up with this?\r\n\r\n```\r\n{\"self.model.{conv/fc/ blah blah blah}, \r\nself.prior_function.model.{conv/fc/ blah blah blah}, }\r\n```\r\n\r\nThis doesn't look anything like a state dict of a model. Would you mind please sharing a concise, runnable code example?\r\n\r\nFYI, in PyTorch, it is normal that if you have a module nested under another module, you get the weights with a dotted name like `\"module.submodule.subsubmodule.weight\"` etc. This is indeed how it is supposed to be."
      },
      {
        "user": "sinAshish",
        "created_at": "2023-07-21T18:07:47Z",
        "body": "the keys should be like:\r\n`\"<lightning_model_name>.<module1>.submodule.weight\"`\r\n\r\nhowever, in my case:\r\napart from `\"<lightning_model_name>.<module1>.submodule.weight\"`, there are other keys:\r\n`\"<lightning_model_name>.<module2>.<module1>.submodule.weight\"`\r\n\r\neven though `module2` and `moduleq` are both initialised in the `___init__()` of PL module."
      },
      {
        "user": "awaelchli",
        "created_at": "2023-07-30T13:56:32Z",
        "body": "@sinAshish I'm sorry, but I still don't understand what you are describing. This does not at all look like a Lightning checkpoint. Maybe you have implemented some custom checkpointing? Please explain the problem you are facing with an accurate description and an example code that runs, or I'll will close the issue due to insufficient information.\r\n\r\nHere is a code snippet that demonstrates how the keys are named in the checkpoint file:\r\n\r\n```py\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom lightning.pytorch import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model = torch.nn.Linear(32, 2)\r\n        self.prior_function = torch.nn.Sequential(torch.nn.Linear(5, 5), torch.nn.Linear(5, 2))\r\n\r\n    def forward(self, x):\r\n        return self.model(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    model = BoringModel()\r\n    trainer = Trainer(max_steps=1)\r\n    trainer.fit(model, train_dataloaders=train_data)\r\n\r\n    checkpoint = torch.load(trainer.checkpoint_callback.best_model_path)\r\n    print(list(checkpoint[\"state_dict\"].keys()))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n\r\n```\r\n\r\nThis prints:\r\n\r\n```\r\n['model.weight', 'model.bias', 'prior_function.0.weight', 'prior_function.0.bias', 'prior_function.1.weight', 'prior_function.1.bias']\r\n```\r\nAs you can see, the state-dict contains the dotted module names of all submodules, named after their attributes in the LightningModule. Please take this example code and modify it to your use case to show what the issue is you are facing. Only if you provide us with an accurate description are we able to help, or otherwise we are just spending valuable time guessing the unknown. Thank you!"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-08-02T21:53:18Z",
        "body": "Hey @sinAshish \r\nDid my answer help, or were you able to resolve the problem?"
      },
      {
        "user": "sinAshish",
        "created_at": "2023-08-07T20:30:33Z",
        "body": "@awaelchli \r\ni understand your answer, and that's how the model keys are strored. \r\nsince I am a bit busy with a  deadline, i'll provide the model checkpoint and a minimal example in the format your suggested in a few days."
      },
      {
        "user": "sinAshish",
        "created_at": "2023-08-07T20:31:10Z",
        "body": "meanwhile, I just turn off strict matching of keys to get past this issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-08-26T12:56:32Z",
        "body": "I'll close the ticket now, but if we find evidence that there is an issue with how Lightning is handling this (through a reproducible example) we can certainly reopen and look into it again. "
      }
    ]
  },
  {
    "number": 17989,
    "title": "Will using Lightning be faster than using PyTorch?",
    "created_at": "2023-07-05T03:05:35Z",
    "closed_at": "2023-07-17T00:23:20Z",
    "labels": [
      "question",
      "docs",
      "performance"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17989",
    "body": "### 📚 Documentation\n\nWill using Lightning be faster than using Pytorch?\n\ncc @borda",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17989/comments",
    "author": "QinHsiu",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-07-05T20:30:44Z",
        "body": "@QinHsiu In theory, Lightning can never be faster than PyTorch because it is running on PyTorch entirely. When comparing apples to apples, they will (and should) result in the same exactly. \r\n\r\nOf course, Lightning has a bunch of features that can be enabled with a single line of code change (e.g. enabling mixed precision in Trainer) that would take you a lot of effort otherwise in adding it to your PyTorch code. In this sense, it is much faster to iterate and try different settings, optimizations, and scale models than in raw PyTorch, simply because you don't have to put up with writing and maintaining boilerplate code.\r\n\r\nI hope this comes across well in our tutorials and code examples. "
      }
    ]
  },
  {
    "number": 17855,
    "title": "No module named 'lightening' even after module shows as installed",
    "created_at": "2023-06-17T02:23:19Z",
    "closed_at": "2023-06-17T14:23:56Z",
    "labels": [
      "question",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17855",
    "body": "### Bug description\n\nI am on Python 3.10.12 environment (OS 13.x), this also occurred in a 3.10.9 venv. \r\n\r\nClear jupyter notebook kernel: \r\n\r\n```\r\n!pip3 install lightning\r\n!pip3 list\r\n\r\n```\r\nshows: \r\n\r\n```\r\ntorch               2.0.1\r\ntorchmetrics        0.11.4\r\njupyter_core        5.3.1\r\nlightning           2.0.3\r\nlightning-cloud     0.5.36\r\nlightning-utilities 0.8.0\r\n```\r\n\r\nThen I do: \r\n\r\n`import lightening as L`\r\n\r\n(I am trying to avoid `pytorch_lightening` as this clashes with `pytorch-forecasting` module I want to use)\r\n\r\n```\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nCell In[11], line 6\r\n----> 6 import lightening as L\r\n\r\nModuleNotFoundError: No module named 'lightening'\r\n```\r\n\r\nI am a bit at a loss... Python 3.10 should be supported. The module lists as installed, but the import does not work. \r\n\n\n### What version are you seeing the problem on?\n\nv2.0\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n_No response_\n\n### Environment\n\n_No response_\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17855/comments",
    "author": "dorienh",
    "comments": [
      {
        "user": "dorienh",
        "created_at": "2023-06-17T02:55:01Z",
        "body": "Well this in embarrassing. Lightening is spelled without -e lightning...  \r\n\r\n`import lightning as L`\r\n"
      },
      {
        "user": "lightningforever",
        "created_at": "2023-06-17T10:41:04Z",
        "body": "Hi @dorienh \r\nDid fixing the spelling error resolve your import issue?"
      },
      {
        "user": "dorienh",
        "created_at": "2023-06-17T12:12:38Z",
        "body": "Yes."
      }
    ]
  },
  {
    "number": 17637,
    "title": "Loading a model from torch.hub leads to a pickle related error",
    "created_at": "2023-05-15T20:30:52Z",
    "closed_at": "2023-10-29T02:47:40Z",
    "labels": [
      "question",
      "won't fix",
      "3rd party",
      "ver: 1.9.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17637",
    "body": "### Bug description\n\nAs an input to my model, I am using the output of the MiDaS monocular depth output. For this, I initialize it as such in the constructor in my model:\r\n`self.mono_depth = torch.hub.load(\"intel-isl/MiDaS\", self.model_type).eval()`\r\n\r\nI then use it in the dataloader as such to generate the mono-depth prediction:\r\n```\r\nwith torch.no_grad():\r\n    mono_depth_out = self.mono_depth(img)\r\n    mono_depth_out = torch.nn.functional.interpolate(mono_depth_out.unsqueeze(1), size=img.shape[-2:], mode=\"bicubic\", align_corners=False,).squeeze(0)\r\n```\r\n\r\nFor the trainer, I initialize it as such:\r\n```\r\ncheckpoint_callback = ModelCheckpoint(\r\n        dirpath=\"/home/obagoren/log\", save_last=True\r\n    )\r\n    trainer = pl.Trainer(max_steps=2e5, callbacks=[checkpoint_callback], gpus=[0, 1])\r\n    trainer.fit(model)\r\n```\r\nIn the `trainer.fit(model)` line, I get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/obagoren/miniconda3/envs/vair_env/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"/home/obagoren/miniconda3/envs/vair_env/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nModuleNotFoundError: No module named 'midas'\r\n```\r\n\r\nFor the data loader, I am using 64 workers and a batch size of 16. \r\n\r\nI am lost as to how to resolve this issue, as when I try to debug, the debugger does not step into `trainer.fit(model)`. I was wondering if anyone has come across this issue, and whether there is a working solution to it. Thank you!\n\n### What version are you seeing the problem on?\n\nv1.9\n\n### How to reproduce the bug\n\n```python\nclass Model(pl.LightningModule):\r\n    def __init__(self,....):\r\n        self.model_type = model_type\r\n        print('Mono depth loaded')\r\n        self.mono_depth = torch.hub.load(\"intel-isl/MiDaS\", self.model_type).eval()\r\n\r\n    ....\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        with torch.no_grad():\r\n        with torch.no_grad():\r\n            mono_depth_out = self.mono_depth(img)\r\n            mono_depth_out = torch.nn.functional.interpolate(mono_depth_out.unsqueeze(1), size=img.shape[-2:], mode=\"bicubic\", align_corners=False,).squeeze(0)\r\n\r\n        ....\r\n        return loss\r\n\r\n\r\n             \r\n        mono_depth =\n```\n\n\n### Error messages and logs\n\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/obagoren/miniconda3/envs/vair_env/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"/home/obagoren/miniconda3/envs/vair_env/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nModuleNotFoundError: No module named 'midas'\r\n```\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- NVIDIA RTX A6000\r\n\t\t- NVIDIA RTX A6000\r\n\t- available:         True\r\n\t- version:           11.7\r\n* Lightning:\r\n\t- lightning-utilities: 0.8.0\r\n\t- pytorch-lightning: 1.9.3\r\n\t- torch:             2.0.0\r\n\t- torchaudio:        2.0.0\r\n\t- torchmetrics:      0.11.4\r\n\t- torchvision:       0.15.0\r\n* Packages:\r\n\t- accelerate:        0.18.0\r\n\t- addict:            2.4.0\r\n\t- aiohttp:           3.8.4\r\n\t- aiosignal:         1.3.1\r\n\t- appdirs:           1.4.4\r\n\t- asttokens:         2.2.1\r\n\t- async-timeout:     4.0.2\r\n\t- attrs:             23.1.0\r\n\t- backcall:          0.2.0\r\n\t- backports.functools-lru-cache: 1.6.4\r\n\t- blinker:           1.6.2\r\n\t- brotlipy:          0.7.0\r\n\t- certifi:           2023.5.7\r\n\t- cffi:              1.15.1\r\n\t- charset-normalizer: 2.0.4\r\n\t- click:             8.1.3\r\n\t- cloudpickle:       2.0.0\r\n\t- colorama:          0.4.6\r\n\t- configargparse:    1.5.3\r\n\t- contourpy:         1.0.7\r\n\t- cryptography:      39.0.1\r\n\t- cycler:            0.11.0\r\n\t- cytoolz:           0.12.0\r\n\t- dash:              2.9.3\r\n\t- dash-core-components: 2.0.0\r\n\t- dash-html-components: 2.0.0\r\n\t- dash-table:        5.0.0\r\n\t- dask:              2022.7.0\r\n\t- datasets:          2.11.0\r\n\t- debugpy:           1.5.1\r\n\t- decorator:         5.1.1\r\n\t- diffusers:         0.15.1\r\n\t- dill:              0.3.6\r\n\t- docker-pycreds:    0.4.0\r\n\t- einops:            0.6.1\r\n\t- entrypoints:       0.4\r\n\t- executing:         1.2.0\r\n\t- fastjsonschema:    2.16.3\r\n\t- filelock:          3.9.0\r\n\t- flask:             2.3.2\r\n\t- fonttools:         4.39.3\r\n\t- frozenlist:        1.3.3\r\n\t- fsspec:            2022.11.0\r\n\t- gitdb:             4.0.10\r\n\t- gitpython:         3.1.31\r\n\t- gmpy2:             2.1.2\r\n\t- h5py:              3.8.0\r\n\t- huggingface-hub:   0.13.4\r\n\t- idna:              3.4\r\n\t- imagecodecs:       2021.8.26\r\n\t- imageio:           2.19.3\r\n\t- importlib-metadata: 6.6.0\r\n\t- ipykernel:         6.15.0\r\n\t- ipython:           8.12.0\r\n\t- ipywidgets:        8.0.6\r\n\t- itsdangerous:      2.1.2\r\n\t- jedi:              0.18.2\r\n\t- jinja2:            3.1.2\r\n\t- joblib:            1.2.0\r\n\t- jsonschema:        4.17.3\r\n\t- jupyter-client:    7.3.4\r\n\t- jupyter-core:      4.12.0\r\n\t- jupyterlab-widgets: 3.0.7\r\n\t- kiwisolver:        1.4.4\r\n\t- kornia:            0.6.12\r\n\t- lightning-utilities: 0.8.0\r\n\t- locket:            1.0.0\r\n\t- markupsafe:        2.1.1\r\n\t- matplotlib:        3.7.1\r\n\t- matplotlib-inline: 0.1.6\r\n\t- mkl-fft:           1.3.1\r\n\t- mkl-random:        1.2.2\r\n\t- mkl-service:       2.4.0\r\n\t- mpmath:            1.2.1\r\n\t- multidict:         6.0.4\r\n\t- multiprocess:      0.70.14\r\n\t- nbformat:          5.7.0\r\n\t- nest-asyncio:      1.5.6\r\n\t- networkx:          2.8.4\r\n\t- numpy:             1.23.5\r\n\t- open3d:            0.17.0\r\n\t- packaging:         23.1\r\n\t- pandas:            2.0.0\r\n\t- parso:             0.8.3\r\n\t- partd:             1.2.0\r\n\t- pathtools:         0.1.2\r\n\t- pexpect:           4.8.0\r\n\t- pickleshare:       0.7.5\r\n\t- pillow:            9.4.0\r\n\t- pip:               23.0.1\r\n\t- plotly:            5.14.1\r\n\t- pooch:             1.4.0\r\n\t- prompt-toolkit:    3.0.38\r\n\t- protobuf:          4.22.3\r\n\t- psutil:            5.9.0\r\n\t- ptyprocess:        0.7.0\r\n\t- pure-eval:         0.2.2\r\n\t- pyarrow:           11.0.0\r\n\t- pycparser:         2.21\r\n\t- pygments:          2.15.1\r\n\t- pykernel:          0.1.6\r\n\t- pyopenssl:         23.0.0\r\n\t- pyparsing:         3.0.9\r\n\t- pyquaternion:      0.9.9\r\n\t- pyrsistent:        0.19.3\r\n\t- pysocks:           1.7.1\r\n\t- python-dateutil:   2.8.2\r\n\t- pytorch-lightning: 1.9.3\r\n\t- pytz:              2023.3\r\n\t- pywavelets:        1.4.1\r\n\t- pyyaml:            6.0\r\n\t- pyzmq:             23.2.0\r\n\t- regex:             2023.3.23\r\n\t- requests:          2.28.1\r\n\t- responses:         0.18.0\r\n\t- scikit-image:      0.19.3\r\n\t- scikit-learn:      1.2.2\r\n\t- scipy:             1.10.0\r\n\t- sentry-sdk:        1.20.0\r\n\t- setproctitle:      1.3.2\r\n\t- setuptools:        66.0.0\r\n\t- six:               1.16.0\r\n\t- smmap:             5.0.0\r\n\t- stack-data:        0.6.2\r\n\t- sympy:             1.11.1\r\n\t- tenacity:          8.2.2\r\n\t- threadpoolctl:     3.1.0\r\n\t- tifffile:          2021.7.2\r\n\t- timm:              0.6.13\r\n\t- tinycudann:        1.7\r\n\t- toolz:             0.12.0\r\n\t- torch:             2.0.0\r\n\t- torchaudio:        2.0.0\r\n\t- torchmetrics:      0.11.4\r\n\t- torchvision:       0.15.0\r\n\t- tornado:           6.1\r\n\t- tqdm:              4.65.0\r\n\t- traitlets:         5.9.0\r\n\t- triton:            2.0.0\r\n\t- typing-extensions: 4.5.0\r\n\t- tzdata:            2023.3\r\n\t- urllib3:           1.26.15\r\n\t- wandb:             0.15.0\r\n\t- wcwidth:           0.2.6\r\n\t- werkzeug:          2.3.4\r\n\t- wheel:             0.38.4\r\n\t- widgetsnbextension: 4.0.7\r\n\t- xxhash:            3.2.0\r\n\t- yarl:              1.9.1\r\n\t- zipp:              3.15.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.10.11\r\n\t- release:           5.15.0-71-generic\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17637/comments",
    "author": "onurbagoren",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2023-06-18T06:23:17Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-10-29T02:47:40Z",
        "body": "This is purely a PyTorch limitation due to spawning worker processes for the DataLoader. Closing the issue since it is unrelated to Lightning. Here is a hint at what to do:\r\n\r\n```py\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n        # Don't do this:\r\n        # self.model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\r\n        # Do this:\r\n        self.model = None\r\n\r\n    def __getitem__(self, index):\r\n        # Do this:\r\n        # load model on first batch (this is inside worker process)\r\n        if self.model is None:\r\n            self.model = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\r\n\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2, num_workers=2)\r\n    for _ in train_data:\r\n        pass\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n\r\n```"
      }
    ]
  },
  {
    "number": 17578,
    "title": "PyTorch Lightning >= 2.0.0 causes multiple iterations over IterableDataset",
    "created_at": "2023-05-05T10:38:07Z",
    "closed_at": "2023-09-17T15:07:48Z",
    "labels": [
      "question",
      "data handling",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17578",
    "body": "### Bug description\r\n\r\nAfter upgrading from PyTorch Lightning 1.9.5 to 2.0.0, I have noticed an issue with my code that uses IterableDataset. In version 1.9.5, it worked correctly, and the output was as follows:\r\n\r\n```\r\nBegin iterating for worker 0\r\nBegin iterating for worker 1\r\nBegin iterating for worker 2\r\nTraining End\r\n```\r\n\r\nHowever, since version 2.0.0, the output looks like this:\r\n\r\n```\r\nBegin iterating for worker 0\r\nBegin iterating for worker 1\r\nBegin iterating for worker 0\r\nBegin iterating for worker 1\r\nBegin iterating for worker 0\r\nBegin iterating for worker 1\r\nBegin iterating for worker 0\r\nBegin iterating for worker 1\r\nBegin iterating for worker 0\r\nBegin iterating for worker 1\r\nBegin iterating for worker 1\r\nBegin iterating for worker 0\r\nBegin iterating for worker 1\r\nBegin iterating for worker 0\r\nBegin iterating for worker 2\r\nBegin iterating for worker 2\r\nBegin iterating for worker 2\r\nBegin iterating for worker 0\r\nBegin iterating for worker 1\r\nBegin iterating for worker 2\r\nTraining End\r\nTraining End\r\nTraining End\r\nTraining End\r\n```\r\n\r\nThe issue is that since 2.0.0, the dataset is being iterated over several times\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.0\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nimport pytorch_lightning as pl\r\nfrom torch.utils.data import DataLoader\r\nimport numpy as np\r\nfrom torch.utils.data import IterableDataset\r\n\r\n\r\nclass MyDataset(IterableDataset):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.data = np.random.rand(1000, 2)\r\n        self.worker_id = 0\r\n        self.num_workers = 1\r\n\r\n    def __iter__(self):\r\n        print(f\"Begin iterating for worker {self.worker_id}\")\r\n        worker_data = [self.data[i] for i in range(self.worker_id, len(self.data), self.num_workers)]\r\n        for item in worker_data:\r\n            input_data = torch.tensor(item[0], dtype=torch.float32).view(1, -1)\r\n            output_data = torch.tensor(item[1], dtype=torch.float32).view(1, -1)\r\n            yield input_data, output_data\r\n\r\n\r\ndef worker_init_fn(worker_id):\r\n    worker_info = torch.utils.data.get_worker_info()\r\n    dataset = worker_info.dataset\r\n    dataset.worker_id = worker_id\r\n    dataset.num_workers = worker_info.num_workers\r\n\r\n\r\nclass SimpleNet(pl.LightningModule):\r\n    def __init__(self):\r\n        super(SimpleNet, self).__init__()\r\n        self.layer = nn.Sequential(nn.Linear(1, 10), nn.ReLU(), nn.Linear(10, 1))\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.layer(x)\r\n        loss = nn.MSELoss()(y_hat, y)\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n\r\n    def on_train_epoch_end(self) -> None:\r\n        print(\"Training End\")\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.001)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = SimpleNet()\r\n\r\n    train_dataset = MyDataset()\r\n    train_dataloader = DataLoader(train_dataset, batch_size=32, num_workers=3, worker_init_fn=worker_init_fn)\r\n\r\n\r\n    trainer = pl.Trainer(max_epochs=1, num_sanity_val_steps=0, enable_progress_bar=False)\r\n    trainer.fit(model, train_dataloader)\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\npytorch-lightning: 2.0.2\r\ntorch:             2.0.0\r\ntorchmetrics:      0.11.4\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_\n\ncc @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17578/comments",
    "author": "mdaniluk",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-09-17T15:07:48Z",
        "body": "The change from Lightning 1.9 to 2.0 was that by default, the Trainer would automatically select all the available GPUs on the system. It looks like you probably have 4 GPUs on the system, and since you didn't specify `Trainer(devices=1)` in 2.0 the trainer will launch 4 processes, one on each GPU. \r\n\r\nSo to fix this, you can set `devices=1` to get the same output as in 1.9.x. Note that if you want to use multi-GPU with an iterable dataset, you'd have to add a DistributedSampler inside your iterable dataset (or shard your data in a differnt way). \r\n\r\nI'm closing the issue since the behavior looks as expected to me. Let me know if you have any questions. "
      }
    ]
  },
  {
    "number": 17543,
    "title": "Resuming training gives different model result / weights",
    "created_at": "2023-05-02T08:37:01Z",
    "closed_at": "2024-02-01T02:11:21Z",
    "labels": [
      "question",
      "won't fix",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17543",
    "body": "### Bug description\r\n\r\n### Expected behavior\r\n\r\nSaving a checkpoint and resuming from that checkpoint in Pytorch Lightning (2.0.2) should give the same model in the end (weights, states, loss).\r\n\r\n### Actual behavior\r\n\r\nSaving a checkpoint and resuming from that checkpoint in Pytorch Lightning (2.0.2) gives different training result in the end (checked via MD5, loss etc). This brakes continual learning.\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2_0\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nimport os\r\nimport warnings\r\n\r\nwarnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\r\n\r\nimport copy\r\nfrom pathlib import Path\r\nimport warnings\r\nimport tempfile\r\nimport json, hashlib\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport torch\r\nimport random\r\nimport lightning.pytorch  as pl\r\n\r\nfrom pytorch_forecasting import TimeSeriesDataSet, NHiTS\r\n\r\nimport logging\r\nlogging.getLogger(\"lightning.pytorch.accelerators.cuda\").setLevel(logging.WARNING)\r\n\r\n##################### Eval\r\n\r\nclass NHitsRunner():\r\n\r\n\tdef __init__(self):\r\n\r\n\t\tself.batch_size = 256 \r\n\t\tself.reset_seed()\r\n\t\tself.create_datasets()\r\n\t\tself.create_model()\r\n\t\tself.path = None\r\n\r\n\tdef create_model(self):\r\n\r\n\t\t# Dataset\r\n\r\n\t\tself.nhits_model = NHiTS.from_dataset(\r\n\t\t\tself.test_datasets[0],\r\n\t\t\tlearning_rate=0.1,\r\n\t\t\thidden_size=16,\r\n\t\t\toptimizer='adam'\r\n\t\t)\r\n\r\n\t\tself.trainer = pl.Trainer(\r\n\t\t\tdevices=1, accelerator=\"gpu\",\r\n\t\t\tmax_epochs=1,\r\n\t\t\tgradient_clip_val=0.1,\r\n\t\t\tenable_model_summary=False,\r\n\t\t\tenable_progress_bar=True,\r\n\t\t\tenable_checkpointing=False,\r\n\t\t\tlogger=False,\r\n\t\t)\r\n\r\n\tdef create_dataset(self, frame):\r\n\r\n\t\tself.max_prediction_length = 10\r\n\t\tself.max_encoder_length = 24\r\n\t\ttraining_cutoff = frame[\"time_idx\"].max() - self.max_prediction_length\r\n\r\n\t\treturn TimeSeriesDataSet(\r\n\t\t\tframe[lambda x: x.time_idx <= training_cutoff],\r\n\t\t\ttime_idx=\"time_idx\",\r\n\t\t\ttarget=\"price\",\r\n\t\t\tgroup_ids=[\"group_id\"],\r\n\t\t\tmax_encoder_length=self.max_encoder_length,\r\n\t\t\tmax_prediction_length=self.max_prediction_length,\r\n\t\t\tstatic_categoricals=[],\r\n\t\t\ttime_varying_known_reals=[],\r\n\t\t\ttime_varying_unknown_categoricals=[],\r\n\t\t\ttime_varying_unknown_reals=[\r\n\t\t\t\t\"price\",\r\n\t\t\t\t\"volume\",\r\n\t\t\t],\r\n\t\t\ttarget_normalizer=None,\r\n\t\t)\r\n\r\n\tdef train_epoch(self, set_id):\r\n\r\n\t\ttrain_dataloader = self.test_datasets[set_id].to_dataloader(train=True, batch_size=self.batch_size, num_workers=0)\r\n\r\n\t\tif self.path != None:\r\n\r\n\t\t\tprint('Consumed trainer checkpoint')\r\n\r\n\t\t\tself.trainer.fit(\r\n\t\t\t\tself.nhits_model,\r\n\t\t\t\tckpt_path=self.path,\r\n\t\t\t\ttrain_dataloaders=train_dataloader,\r\n\t\t\t)\r\n\r\n\t\t\tos.remove(self.path)\r\n\r\n\t\t\tself.path = None\r\n\r\n\t\telse:\r\n\t\t\t\r\n\t\t\tself.trainer.fit(\r\n\t\t\t\tself.nhits_model,\r\n\t\t\t\ttrain_dataloaders=train_dataloader,\r\n\t\t\t)\r\n\r\n\t\tself.trainer.fit_loop.max_epochs += int(1)\r\n\r\n\tdef reset_seed(self):\r\n\r\n\t\tpl.seed_everything(0)\r\n\t\ttorch.set_float32_matmul_precision('medium')\r\n\r\n\tdef create_frame(self):\r\n\r\n\t\t# Create sine wave\r\n\t\tdataframe_size = 128 * 64\r\n\t\ttest_dataframe = pd.DataFrame({'time_idx': np.arange(0, dataframe_size, 1, dtype=int)})\r\n\t\ttest_dataframe = test_dataframe.assign(price=lambda x: np.sin(x.time_idx/3)*100)\r\n\t\ttest_dataframe = test_dataframe.assign(volume=lambda x: np.sin(x.time_idx/2)*200)\r\n\t\ttest_dataframe['group_id'] = 'group_a'\r\n\t\treturn test_dataframe\r\n\r\n\tdef create_datasets(self):\r\n\r\n\t\tself.test_frames = []\r\n\t\tself.test_datasets = []\r\n\r\n\t\tfor i in range(3):\r\n\r\n\t\t\tnew_frame = self.create_frame()\r\n\r\n\t\t\tself.test_frames.append(new_frame)\r\n\r\n\t\t\tself.test_datasets.append(self.create_dataset(new_frame))\r\n\r\n\tdef convert_dict(self, d, flat_dict):\r\n\t\tfor k, v in d.items():\r\n\t\t\tif isinstance(v, dict):\r\n\t\t\t\tself.convert_dict(v, flat_dict)\r\n\t\t\telse:\r\n\t\t\t\tif isinstance(v, torch.Tensor):\r\n\t\t\t\t\tv = v.cpu().detach().numpy().tolist()\r\n\t\t\t\tflat_dict[k] = v\r\n\r\n\tdef dict_md5(self):\r\n\r\n\t\t_dict = self.nhits_model.state_dict()\r\n\t\t\r\n\t\tflat_dict = {}\r\n\t\tself.convert_dict(_dict, flat_dict)\r\n\r\n\t\tdata_md5 = hashlib.md5(json.dumps(flat_dict, sort_keys=True).encode('utf-8')).hexdigest()\r\n\r\n\t\treturn data_md5\r\n\r\n\tdef test_save_load_trainer_checkpoint(self):\r\n\r\n\t\tprint(\"Saving trainer checkpoint\")\r\n\r\n\t\t# Save trainer file\r\n\r\n\t\tmodel_filename = \"test_checkpoint_forecast.ckpt\"\r\n\t\ttempdir = tempfile.gettempdir()\r\n\t\tfile_name = tempdir+\"/\"+model_filename\r\n\t\tself.trainer.save_checkpoint(file_name)\r\n\r\n\t\tself.path = file_name\r\n\r\n#### Test running WITHOUT saving and loading \r\n\r\nrunner_1 = NHitsRunner()\r\n\r\nfor i in range(3):\r\n\r\n\trunner_1.train_epoch(i)\r\n\r\nrun1_md5 = runner_1.dict_md5()\r\n\r\n#### Test running WITHOUt saving and loading \r\n\r\nrunner_2 = NHitsRunner()\r\n\r\nfor i in range(3):\r\n\r\n\tif i == 2:\r\n\t\trunner_2.test_save_load_trainer_checkpoint()\r\n\r\n\trunner_2.train_epoch(i)\r\n\r\nrun2_md5 = runner_2.dict_md5()\r\n\r\n#### Compare\r\n\r\nif run1_md5 != run2_md5:\r\n\r\n\tprint('Models md5 differs {} != {}'.format(run1_md5, run2_md5))\r\n\r\nelse:\r\n\r\n\tprint('Models md5 equals {} == {}'.format(run1_md5, run2_md5))\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\n```\r\nGlobal seed set to 0\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nEpoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 17.52it/s, train_loss_step=0.737, train_loss_epoch=1.700]`Trainer.fit` stopped: `max_epochs=1` reached.\r\nEpoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 17.51it/s, train_loss_step=0.737, train_loss_epoch=1.700]\r\nEpoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 17.75it/s, train_loss_step=0.648, train_loss_epoch=1.150]`Trainer.fit` stopped: `max_epochs=2` reached.\r\nEpoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 17.74it/s, train_loss_step=0.648, train_loss_epoch=1.150]\r\nEpoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 18.43it/s, train_loss_step=0.598, train_loss_epoch=1.440]`Trainer.fit` stopped: `max_epochs=3` reached.\r\nEpoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 18.41it/s, train_loss_step=0.598, train_loss_epoch=1.440]\r\nGlobal seed set to 0\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nEpoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 17.57it/s, train_loss_step=0.737, train_loss_epoch=1.700]`Trainer.fit` stopped: `max_epochs=1` reached.\r\nEpoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 17.55it/s, train_loss_step=0.737, train_loss_epoch=1.700]\r\nEpoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 18.70it/s, train_loss_step=0.648, train_loss_epoch=1.150]`Trainer.fit` stopped: `max_epochs=2` reached.\r\nEpoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 18.69it/s, train_loss_step=0.648, train_loss_epoch=1.150]\r\nSaving trainer checkpoint\r\nConsumed trainer checkpoint\r\nRestoring states from the checkpoint path at /tmp/test_checkpoint_forecast.ckpt\r\nRestored all states from the checkpoint at /tmp/test_checkpoint_forecast.ckpt\r\nEpoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 18.91it/s, train_loss_step=0.342, train_loss_epoch=0.470]`Trainer.fit` stopped: `max_epochs=3` reached.\r\nEpoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████| 31/31 [00:01<00:00, 18.90it/s, train_loss_step=0.342, train_loss_epoch=0.470]\r\nModels md5 differs 0d6210c3c296c7acfccac4a4b27e1a16 != 25eee25731cd9d21e9991afa47a90b82\r\n\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n- PyTorch-Forecasting version: 1.0.0 (based on LightningModule)\r\n- PyTorch version: 2.0.0\r\n- PyTorch lightning: 2.0.2 \r\n- Python version: 3.8.10\r\n- Operating System: Ubuntu 20.04\r\n- CUDA 12.1\r\n- Running environment: Local\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\nIn the example code I create 3 data frames and train on each, one epoch at the time. This will render a md5 sum. Rerunning the code will render the same md5 sum.\r\n\r\nThe second time I run the exact same code, before the last iteration, I save a checkpoint to disk and then resume training from that checkpoint. This will render a different md5 sum. This should not be the case.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17543/comments",
    "author": "hiandersson",
    "comments": [
      {
        "user": "hiandersson",
        "created_at": "2023-05-03T12:43:26Z",
        "body": "Trying a simple example from Pytorch Lightning \"Hello world\" example with CIFAR 10 seems to work, the MD5 sum is same when resuming training.\r\nThe strange thing is that first I do Trainer.fit with the checkpoint, then `self.trainer.fit_loop.max_epochs += int(1)`, then I train again with Trainer.fit again to get another epoch. Is it suppose to be this way or do I miss something?\r\n\r\n```\r\nimport os\r\n\r\nimport lightning as L\r\nimport pandas as pd\r\nimport seaborn as sn\r\nimport torch\r\nfrom IPython.display import display\r\nfrom lightning.pytorch.loggers import CSVLogger\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchmetrics import Accuracy\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import MNIST\r\nimport hashlib, json\r\nimport logging\r\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\r\nlogging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.WARNING)\r\nlogging.getLogger(\"lightning.pytorch.accelerators.cuda\").setLevel(logging.WARNING)\r\n\r\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\r\nBATCH_SIZE = 512\r\n\r\nclass MNISTModel(L.LightningModule):\r\n\tdef __init__(self):\r\n\t\tsuper().__init__()\r\n\t\tself.l1 = torch.nn.Linear(28 * 28, 10)\r\n\r\n\tdef forward(self, x):\r\n\t\treturn torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n\tdef training_step(self, batch, batch_nb):\r\n\t\tx, y = batch\r\n\t\tloss = F.cross_entropy(self(x), y)\r\n\t\treturn loss\r\n\r\n\tdef configure_optimizers(self):\r\n\t\treturn torch.optim.Adam(self.parameters(), lr=0.02)\r\n\t\r\n\r\nclass MNISTTest():\r\n\r\n\tdef convert_dict(self, d, flat_dict):\r\n\t\tfor k, v in d.items():\r\n\t\t\tif isinstance(v, dict):\r\n\t\t\t\tself.convert_dict(v, flat_dict)\r\n\t\t\telse:\r\n\t\t\t\tif isinstance(v, torch.Tensor):\r\n\t\t\t\t\tv = v.cpu().detach().numpy().tolist()\r\n\t\t\t\tflat_dict[k] = v\r\n\r\n\tdef dict_md5(self, model):\r\n\r\n\t\t_dict = model.state_dict()\r\n\t\t\r\n\t\tflat_dict = {}\r\n\t\tself.convert_dict(_dict, flat_dict)\r\n\r\n\t\tdata_md5 = hashlib.md5(json.dumps(flat_dict, sort_keys=True).encode('utf-8')).hexdigest()\r\n\r\n\t\treturn data_md5\r\n\t\r\n\tdef train(self, resume_checkpoint):\r\n\r\n\t\t# Seed    \r\n\t\tL.seed_everything(0)\r\n\r\n\t\t# Init our model\r\n\t\tself.mnist_model = MNISTModel()\r\n\r\n\t\t# Init DataLoader from MNIST Dataset\r\n\t\ttrain_ds = MNIST(PATH_DATASETS, train=True, download=True, transform=transforms.ToTensor())\r\n\t\tself.train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE)\r\n\r\n\t\t# Initialize a trainer\r\n\t\tself.trainer = L.Trainer(\r\n\t\t\taccelerator=\"auto\",\r\n\t\t\tdevices=1,\r\n\t\t\tmax_epochs=1,\r\n\t\t\tenable_model_summary=False,\r\n\t\t\tenable_progress_bar=True,\r\n\t\t\tenable_checkpointing=False,\r\n\t\t\tlogger=False,\r\n\t\t)\r\n\r\n\t\t# Train the model :zap:\r\n\t\tif resume_checkpoint == None:\r\n\t\t\r\n\t\t\tself.trainer.fit(self.mnist_model, self.train_loader)\r\n\r\n\t\telse:\r\n\r\n\t\t\tprint('Resuming from checkpoint {}'.format(resume_checkpoint))\r\n\r\n\t\t\tself.trainer.fit(\r\n\t\t\t\tself.mnist_model, \r\n\t\t\t\tself.train_loader,\r\n\t\t\t\tckpt_path=resume_checkpoint)\r\n\r\n\tdef save(self, file_name):\r\n\r\n\t\tself.trainer.save_checkpoint(file_name)\r\n\r\n\tdef get_md5(self):\r\n\r\n\t\treturn self.dict_md5(self.mnist_model)\r\n\r\n\tdef train_again(self):\r\n\r\n\t\tself.trainer.fit_loop.max_epochs += int(1)\r\n\t\tself.trainer.fit(self.mnist_model, self.train_loader)\r\n\r\n\r\n# Test train again\r\ntest_first_run = MNISTTest()\r\ntest_first_run.train(None)\r\ntest_first_run.save('checkpoint.ckpt')\r\nprint('First run Epoch 1: {}'.format(test_first_run.get_md5()))\r\ntest_first_run.train_again()\r\nprint('First run Epoch 2: {}'.format(test_first_run.get_md5()))\r\n\r\n# Test resume from Epoch 1, training one more epoch\r\ntest_resume = MNISTTest()\r\ntest_resume.train('checkpoint.ckpt')\r\ntest_resume.train_again()\r\nprint('Train resume Epoch 2: {}'.format(test_resume.get_md5()))\r\n\r\n# Clean\r\n#os.remove('checkpoint.ckpt')\r\n```"
      },
      {
        "user": "NikolasWolke",
        "created_at": "2023-07-05T15:07:44Z",
        "body": "I did observe the same problem, that a training with resume does not lead to the same result as a training without interruption - even if `deterministic=True`.\r\n\r\nBut in addition, I see that the global step is higher for the resumed training (48 steps for the reference model without resume but 49 steps for the model with resume).\r\n\r\nReproducible example here:\r\n\r\n```python\r\n# ---\r\n# jupyter:\r\n#   jupytext:\r\n#     text_representation:\r\n#       extension: .py\r\n#       format_name: percent\r\n#       format_version: '1.3'\r\n#       jupytext_version: 1.14.7\r\n#   kernelspec:\r\n#     display_name: Python 3 (ipykernel)\r\n#     language: python\r\n#     name: python3\r\n# ---\r\n\r\n# %%\r\nfrom typing import Any\r\nfrom copy import deepcopy\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\nfrom torch.optim import AdamW, Optimizer\r\nfrom torch.optim.lr_scheduler import OneCycleLR\r\nfrom torch.utils.data import Dataset, DataLoader\r\nfrom pytorch_lightning import Trainer, LightningModule, seed_everything\r\nfrom pytorch_lightning.callbacks import Callback, ModelCheckpoint\r\n\r\n\r\n# %%\r\nclass Linear(LightningModule):\r\n    def __init__(self, inputs: int, learning_rate: float):\r\n        super().__init__()\r\n        self.linear = nn.Linear(inputs, 1)\r\n        self.learning_rate = learning_rate\r\n\r\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n        return self.linear(x)\r\n    \r\n    def training_step(self, batch: dict[str, torch.Tensor]) -> torch.Tensor:\r\n        x = batch[\"X\"]    # shape (batch_size, inputs)\r\n        y = batch[\"y\"]    # shape (batch_size,)\r\n        y_hat = self.forward(x)\r\n        \r\n        loss = F.mse_loss(y, y_hat)\r\n        return loss\r\n    \r\n    def configure_optimizers(self) -> tuple[list[Optimizer], list[dict[str, Any]]]:\r\n        optimizer = AdamW(self.parameters(), lr=self.learning_rate)\r\n        lr_scheduler = OneCycleLR(\r\n            optimizer=optimizer,\r\n            max_lr=3*self.learning_rate,\r\n            total_steps=self.trainer.estimated_stepping_batches + 1, # TODO: note that since resume does one step more, we need to increase this\r\n        )\r\n        return [optimizer], [{\"scheduler\": lr_scheduler, \"frequency\": 1, \"interval\": \"step\"}]\r\n\r\n\r\n# %%\r\nclass InterruptTraining(Callback):\r\n    def __init__(self, stop_at_step: int):\r\n        self.stop_at_step = stop_at_step\r\n        self.has_stopped = False\r\n    \r\n    def on_train_epoch_start(self, trainer: Trainer, pl_module: LightningModule) -> None:\r\n        if trainer.global_step == self.stop_at_step and not self.has_stopped:\r\n            self.has_stopped = True\r\n            raise RuntimeError(f\"Simulated training termination.\")\r\n\r\n\r\n# %%\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, inputs: int, length: int):\r\n        self.len = length\r\n        self.x = torch.randn(length, inputs)\r\n        self.y = 2 * self.x.sum(dim=1) + 3\r\n\r\n    def __getitem__(self, index: int) -> dict[str, torch.Tensor]:\r\n        return {\"X\": self.x[index], \"y\": torch.zeros(1)}\r\n\r\n    def __len__(self) -> int:\r\n        return self.len\r\n\r\n\r\n# %%\r\ndataset_size = 2**12\r\nbatch_size = 256\r\n\r\nsteps_per_epoch = int(dataset_size/batch_size)\r\nsteps_per_epoch\r\n\r\n# %%\r\nseed_everything()\r\n\r\ndataset = RandomDataset(inputs=10, length=dataset_size)\r\ndataloader = DataLoader(dataset, batch_size=batch_size)\r\n\r\n# %%\r\nmodel = Linear(inputs=10, learning_rate=1e-3)\r\nmodel_reference = deepcopy(model)\r\n\r\ntrainer = Trainer(\r\n    max_epochs=3,\r\n    callbacks=[\r\n        InterruptTraining(stop_at_step=steps_per_epoch*2), # simulate training crash after epoch 2\r\n        ModelCheckpoint(every_n_train_steps=1, save_last=True)\r\n    ],\r\n    deterministic=True,\r\n)\r\ntry:\r\n    trainer.fit(model, train_dataloaders=dataloader)\r\nexcept RuntimeError:\r\n    print(\"Simulated training error --> need to resume\")\r\n    trainer.fit(model, train_dataloaders=dataloader, ckpt_path=\"./lightning_logs/version_0/checkpoints/last.ckpt\")\r\n\r\n# %%\r\nmodel.global_step\r\n\r\n# %%\r\nmodel.state_dict()\r\n\r\n# %%\r\ntrainer = Trainer(\r\n    max_epochs=3,\r\n    callbacks=[\r\n        ModelCheckpoint(every_n_train_steps=1, save_last=True)\r\n    ],\r\n    deterministic=True,\r\n)\r\ntrainer.fit(model_reference, train_dataloaders=dataloader)\r\n\r\n# %%\r\nmodel_reference.global_step\r\n\r\n# %%\r\nmodel_reference.state_dict()\r\n\r\n# %%\r\n[torch.allclose(p, p_reference) for p, p_reference in zip(model.parameters(), model_reference.parameters())]\r\n\r\n# %%\r\n\r\n```"
      },
      {
        "user": "stale[bot]",
        "created_at": "2023-08-12T10:43:54Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-09-20T01:16:27Z",
        "body": "Hey, just wanted to comment here that full fault-tolerance would be nice, but can't be done in most cases. Lightning does and cannot provide any guaratnees of full fault-tolerance. Every time you resume from a checkpoint you start from a different random seed than what was the state before ending the previous training. This means you get different shuffling as if you had not interrupted training. But this is not an issue in practice in most cases because models are not sensitive to initialization/seed/shuffling order. If they were, it would be a different concern anyway. \r\n\r\nSo in summary, your check for the md5sum of the model state shouldn't be used to determine whether a model is trained well after resuming from a checkpoint. I suggest that you instead compute metrics on a test or training set to determine the performance of your model instead.   "
      }
    ]
  },
  {
    "number": 17427,
    "title": "`MisconfigurationException` during training multi-layer transformer",
    "created_at": "2023-04-21T11:38:48Z",
    "closed_at": "2023-05-09T09:55:36Z",
    "labels": [
      "question",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17427",
    "body": "### Bug description\n\nHello\r\n\r\nI'm training Bert based student using BertModel from Huggingface Transformers library.\r\nThe teacher is frozen, pretrained transformer, also from HF.\r\nWhen student has 1 layer and any number of attention heads (even equal the total number of heads from the teacher), everything is going smoothly.\r\nBut when I'm increasing number of layers (even for a small amount of heads), training is lasting about 1/3 epoch and then error is thrown from an automatic optimization. GPU vRAM utilization and proc is circa 90%, temperature around 80C.\r\n\r\nTraining on a Docker container based on pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime\r\nLightning version installed in the container: pytorch-lightning==2.0.1.post0\r\n\r\nStudent's config that is causing an error:\r\n```yaml\r\nhidden_size: 768\r\nnum_hidden_layers: 6\r\nnum_attention_heads: 16\r\nintermediate_size: 2048\r\nhidden_dropout_prob: 0.0\r\nhidden_act: gelu\r\n```\r\n\r\nStudent's config that is NOT causing an error:\r\n```yaml\r\nhidden_size: 768\r\nnum_hidden_layers: 1\r\nnum_attention_heads: 96\r\nintermediate_size: 2048\r\nhidden_dropout_prob: 0.0\r\nhidden_act: gelu\r\n```\r\n\r\nBelow  code snippets (I cant share whole and exact code).\r\n\r\nthe trainer:\r\n```python\r\ntrainer = Trainer(accelerator='gpu',\r\n                          precision=16, devices='auto',\r\n                          callbacks=[\r\n                              ModelCheckpoint(dirpath=f'checkpoints/{experiment_name}/{run_name}',\r\n                                              filename='{epoch}-{val_loss:.3f}',\r\n                                              every_n_epochs=1,\r\n                                              save_top_k=5, monitor='val_loss'),\r\n                              EarlyStopping(monitor=\"val_loss\", mode=\"min\",\r\n                                            patience=10)],\r\n                          max_epochs=100,\r\n                          logger=TensorBoardLogger(\"tensorboard\", name=f'{experiment_name}',\r\n                                                   version=f\"{run_name}\", log_graph=True),\r\n                          gradient_clip_val=0.5, gradient_clip_algorithm=\"value\",\r\n                          detect_anomaly=False\r\n                          )\r\n```\r\n\r\nmethods that may coincide:\r\n```python\r\n    def configure_optimizers(self) :\r\n        opt = Adam(self.parameters(), lr=0.001)\r\n        return opt\r\n\r\n    def training_step(self, batch: List[str], batch_idx: int) -> torch.Tensor:\r\n        teacher, student = self.forward(batch)\r\n        loss = self.loss_function(student, teacher)\r\n        student_cpu = student.detach().cpu()\r\n        teacher_cpu = teacher.detach().cpu()\r\n        self.train_mse.update(student_cpu, teacher_cpu)\r\n        self.train_cossim.update(student_cpu, teacher_cpu)\r\n        del student_cpu, teacher_cpu\r\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.batch_size)\r\n        return loss\r\n```\r\n\r\nMany thanks for any help or hint :)\n\n### What version are you seeing the problem on?\n\n2.0+\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\nMisconfigurationException                 Traceback (most recent call last)\r\nCell In[14], line 1\r\n----> 1 experiment.run()\r\n\r\nFile /workspace/gem/experiment_handler.py:93, in GemExperiment.run(self, **kwargs)\r\n     91 if self.cfg.train.mode == 'standard':\r\n     92     os.environ['TOKENIZERS_PARALLELISM'] = 'true'\r\n---> 93     self._run_info = run_experiment(\r\n     94         loader=self.loader,\r\n     95         model=self.model,\r\n     96         cfg=self.cfg,\r\n     97         register=self.register,\r\n     98         experiment_name=self.experiment_name,\r\n     99         tracking_uri=self.tracking_uri,\r\n    100         search_code_artifacts=self.search_code_artifacts\r\n    101     )\r\n    102 else:\r\n    103     self._run_info = self.register.train[self.cfg.train.mode](\r\n    104         loader=self.loader,\r\n    105         model=self.model,\r\n   (...)\r\n    111         **kwargs\r\n    112     )\r\n\r\nFile /workspace/building_blocks/train/lightning.py:171, in run_experiment(loader, model, cfg, register, experiment_name, tracking_uri, search_code_artifacts)\r\n    145 mlflow.log_dict(asdict(cfg), 'experiment_config.yaml')\r\n    147 trainer = Trainer(accelerator=cfg.accelerator,\r\n    148                   precision=cfg.train.precision, devices=cfg.devices,\r\n    149                   callbacks=[\r\n   (...)\r\n    168                   detect_anomaly=cfg.detect_anomaly\r\n    169                   )\r\n--> 171 trainer.fit(model, datamodule=loader)\r\n    172 val_loss = trainer.validate(model, loader, ckpt_path='best')\r\n    173 test_loss = trainer.test(model, loader, ckpt_path='best')\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:520, in Trainer.fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\r\n    518 model = _maybe_unwrap_optimized(model)\r\n    519 self.strategy._lightning_module = model\r\n--> 520 call._call_and_handle_interrupt(\r\n    521     self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n    522 )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44, in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)\r\n     42         return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n     43     else:\r\n---> 44         return trainer_fn(*args, **kwargs)\r\n     46 except _TunerExitException:\r\n     47     _call_teardown_hook(trainer)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:559, in Trainer._fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\r\n    549 self._data_connector.attach_data(\r\n    550     model, train_dataloaders=train_dataloaders, val_dataloaders=val_dataloaders, datamodule=datamodule\r\n    551 )\r\n    553 ckpt_path = self._checkpoint_connector._select_ckpt_path(\r\n    554     self.state.fn,\r\n    555     ckpt_path,\r\n    556     model_provided=True,\r\n    557     model_connected=self.lightning_module is not None,\r\n    558 )\r\n--> 559 self._run(model, ckpt_path=ckpt_path)\r\n    561 assert self.state.stopped\r\n    562 self.training = False\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:935, in Trainer._run(self, model, ckpt_path)\r\n    930 self._signal_connector.register_signal_handlers()\r\n    932 # ----------------------------\r\n    933 # RUN THE TRAINER\r\n    934 # ----------------------------\r\n--> 935 results = self._run_stage()\r\n    937 # ----------------------------\r\n    938 # POST-Training CLEAN UP\r\n    939 # ----------------------------\r\n    940 log.debug(f\"{self.__class__.__name__}: trainer tearing down\")\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:978, in Trainer._run_stage(self)\r\n    976         self._run_sanity_check()\r\n    977     with torch.autograd.set_detect_anomaly(self._detect_anomaly):\r\n--> 978         self.fit_loop.run()\r\n    979     return None\r\n    980 raise RuntimeError(f\"Unexpected state {self.state}\")\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:201, in _FitLoop.run(self)\r\n    199 try:\r\n    200     self.on_advance_start()\r\n--> 201     self.advance()\r\n    202     self.on_advance_end()\r\n    203     self._restarting = False\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:354, in _FitLoop.advance(self)\r\n    352 self._data_fetcher.setup(combined_loader)\r\n    353 with self.trainer.profiler.profile(\"run_training_epoch\"):\r\n--> 354     self.epoch_loop.run(self._data_fetcher)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:133, in _TrainingEpochLoop.run(self, data_fetcher)\r\n    131 while not self.done:\r\n    132     try:\r\n--> 133         self.advance(data_fetcher)\r\n    134         self.on_advance_end()\r\n    135         self._restarting = False\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:218, in _TrainingEpochLoop.advance(self, data_fetcher)\r\n    215 with trainer.profiler.profile(\"run_training_batch\"):\r\n    216     if trainer.lightning_module.automatic_optimization:\r\n    217         # in automatic optimization, there can only be one optimizer\r\n--> 218         batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)\r\n    219     else:\r\n    220         batch_output = self.manual_optimization.run(kwargs)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:185, in _AutomaticOptimization.run(self, optimizer, kwargs)\r\n    178         closure()\r\n    180 # ------------------------------\r\n    181 # BACKWARD PASS\r\n    182 # ------------------------------\r\n    183 # gradient update with accumulated gradients\r\n    184 else:\r\n--> 185     self._optimizer_step(kwargs.get(\"batch_idx\", 0), closure)\r\n    187 result = closure.consume_result()\r\n    188 if result.loss is None:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:261, in _AutomaticOptimization._optimizer_step(self, batch_idx, train_step_and_backward_closure)\r\n    258     self.optim_progress.optimizer.step.increment_ready()\r\n    260 # model hook\r\n--> 261 call._call_lightning_module_hook(\r\n    262     trainer,\r\n    263     \"optimizer_step\",\r\n    264     trainer.current_epoch,\r\n    265     batch_idx,\r\n    266     optimizer,\r\n    267     train_step_and_backward_closure,\r\n    268 )\r\n    270 if not should_accumulate:\r\n    271     self.optim_progress.optimizer.step.increment_completed()\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:142, in _call_lightning_module_hook(trainer, hook_name, pl_module, *args, **kwargs)\r\n    139 pl_module._current_fx_name = hook_name\r\n    141 with trainer.profiler.profile(f\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"):\r\n--> 142     output = fn(*args, **kwargs)\r\n    144 # restore current_fx when nested context\r\n    145 pl_module._current_fx_name = prev_fx_name\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1265, in LightningModule.optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure)\r\n   1226 def optimizer_step(\r\n   1227     self,\r\n   1228     epoch: int,\r\n   (...)\r\n   1231     optimizer_closure: Optional[Callable[[], Any]] = None,\r\n   1232 ) -> None:\r\n   1233     r\"\"\"\r\n   1234     Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\r\n   1235     the optimizer.\r\n   (...)\r\n   1263                     pg[\"lr\"] = lr_scale * self.learning_rate\r\n   1264     \"\"\"\r\n-> 1265     optimizer.step(closure=optimizer_closure)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:158, in LightningOptimizer.step(self, closure, **kwargs)\r\n    155     raise MisconfigurationException(\"When `optimizer.step(closure)` is called, the closure should be callable\")\r\n    157 assert self._strategy is not None\r\n--> 158 step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\r\n    160 self._on_after_step()\r\n    162 return step_output\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:224, in Strategy.optimizer_step(self, optimizer, closure, model, **kwargs)\r\n    222 # TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\r\n    223 assert isinstance(model, pl.LightningModule)\r\n--> 224 return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/amp.py:70, in MixedPrecisionPlugin.optimizer_step(self, optimizer, model, closure, **kwargs)\r\n     68 if isinstance(optimizer, LBFGS):\r\n     69     raise MisconfigurationException(\"AMP and the LBFGS optimizer are not compatible.\")\r\n---> 70 closure_result = closure()\r\n     72 if not _optimizer_handles_unscaling(optimizer):\r\n     73     # Unscaling needs to be performed here in case we are going to apply gradient clipping.\r\n     74     # Optimizers that perform unscaling in their `.step()` method are not supported (e.g., fused Adam).\r\n     75     # Note: `unscale` happens after the closure is executed, but before the `on_before_optimizer_step` hook.\r\n     76     self.scaler.unscale_(optimizer)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:140, in Closure.__call__(self, *args, **kwargs)\r\n    139 def __call__(self, *args: Any, **kwargs: Any) -> Optional[Tensor]:\r\n--> 140     self._result = self.closure(*args, **kwargs)\r\n    141     return self._result.loss\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:126, in Closure.closure(self, *args, **kwargs)\r\n    125 def closure(self, *args: Any, **kwargs: Any) -> ClosureResult:\r\n--> 126     step_output = self._step_fn()\r\n    128     if step_output.closure_loss is None:\r\n    129         self.warning_cache.warn(\"`training_step` returned `None`. If this was on purpose, ignore this warning...\")\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:311, in _AutomaticOptimization._training_step(self, kwargs)\r\n    308 training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\r\n    309 self.trainer.strategy.post_training_step()\r\n--> 311 result = self.output_result_cls.from_training_step_output(training_step_output, trainer.accumulate_grad_batches)\r\n    312 return result\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:73, in ClosureResult.from_training_step_output(cls, training_step_output, normalize)\r\n     71     closure_loss = training_step_output\r\n     72 elif training_step_output is not None:\r\n---> 73     raise MisconfigurationException(\r\n     74         \"In automatic optimization, `training_step` must return a Tensor, \"\r\n     75         \"a dict, or None (where the step will be skipped).\"\r\n     76     )\r\n     78 if closure_loss is not None:\r\n     79     # accumulate the loss. If ``accumulate_grad_batches == 1``, no effect\r\n     80     # note: avoid in-place operation `x /= y` here on purpose\r\n     81     closure_loss = closure_loss / normalize\r\n\r\nMisconfigurationException: In automatic optimization, `training_step` must return a Tensor, a dict, or None (where the step will be skipped).\r\n\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17427/comments",
    "author": "jakubgajski",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-04-21T19:43:40Z",
        "body": "The error message gives you a hint at what's wrong: \r\n\r\n> In automatic optimization, `training_step` must return a Tensor, a dict, or None (where the step will be skipped).\r\n\r\nSo it must be that your loss function doesn't return a scalar tensor:\r\n`loss = self.loss_function(student, teacher)`\r\n\r\nCheck your implementation and make sure it is always a tensor.\r\n"
      },
      {
        "user": "jakubgajski",
        "created_at": "2023-04-24T06:04:33Z",
        "body": "@awaelchli thanks for your reply. I would be very happy if it is it, but my loss is literally a sum of two nn.functional losses:\r\n```python\r\ndef cos_sim_kl_div_loss(input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\r\n    return cos_sim_loss(input, target) + kl_div_loss(input, target)\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-04-24T09:31:55Z",
        "body": "You are showing a function called cos_sim_kl_div_loss, but in your code you have\r\n`loss = self.loss_function(student, teacher)` which produces the output of training_step. So you should take a look at what this returns. "
      },
      {
        "user": "jakubgajski",
        "created_at": "2023-04-24T09:52:03Z",
        "body": "Yeah, sorry for a mental shortcut. The implementation looks like that:\r\n```python\r\nclass EncoderDistillationModule(pl.LightningModule):\r\n  ...\r\n  self.loss_fn = cos_sim_kl_div_loss   # assigned on initialization \r\n  ...\r\n  def loss_function(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\r\n          return self.loss_fn(pred, target)\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-04-29T04:49:43Z",
        "body": "@jakubgajski Have you found the place where the unsupported object is returned? Step through your training step code to find the place where you return an non-tensor. "
      },
      {
        "user": "jakubgajski",
        "created_at": "2023-04-30T19:57:44Z",
        "body": "@awaelchli Unfortunately not. Code looks just fine and works as expected for shallow transformers. But for deeper ones and high GPU load this described behaviour happens. Faulty code would undermine any attempt to train, yet I'm able to train hapilly shallow models without disruption. "
      },
      {
        "user": "awaelchli",
        "created_at": "2023-04-30T20:26:03Z",
        "body": "But what have you done to investigate? Can't you just add this\r\n\r\n```py\r\n    def training_step(self, batch, batch_idx):\r\n        ...\r\n        assert isinstance(loss, torch.Tensor)\r\n        return loss\r\n```\r\nto the end of your training step and then when it fails, backtrack where it comes from? Please understand that if there is no evidence of a bug in Lightning and there is no code to run to reproduce this, I cannot help you. \r\n"
      },
      {
        "user": "jakubgajski",
        "created_at": "2023-05-04T06:38:56Z",
        "body": "I have done my best in searching for a bug in my implementation and run numerous experiments that led to escribed conclusions, but I will try to do:\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    ...\r\n    if not isinstance(loss, torch.Tensor):\r\n        print(loss, type(loss))\r\n    return loss\r\n```\r\non Monday 8th May, as now I'm travelling.\r\n\r\nThanks for a suggestion."
      },
      {
        "user": "jakubgajski",
        "created_at": "2023-05-09T09:55:36Z",
        "body": "I have fount the root case. Some wrapper that written to protect against NaNs in loss, was returning small float instead of small float tensor when NaN occurred.\r\n\r\nThanks for discussion! "
      }
    ]
  },
  {
    "number": 17410,
    "title": "workers > 0 on DataLoader makes training unreasonably slow",
    "created_at": "2023-04-18T19:32:16Z",
    "closed_at": "2023-07-17T01:03:36Z",
    "labels": [
      "question",
      "performance",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17410",
    "body": "### Bug description\r\n\r\nSetting workers > 0 on a DataLoader slows down training by a factor of 50-100 on cpu\r\n\r\n### What version are you seeing the problem on?\r\n\r\nlightning==2.0.0\r\nlightning-cloud==0.5.32\r\nlightning-utilities==0.8.0\r\n\r\n### How to reproduce the bug\r\n\r\nI used the following script to benchmark the training speed. It's a very simple model hence I would expect the fit to be almost immediate. The script takes ~150s to run with workers.\r\n\r\n```python\r\nimport random\r\nimport time\r\n\r\nimport lightning.pytorch as pl\r\nimport torch\r\nfrom torch.utils.data import DataLoader\r\nfrom lightning.pytorch.loggers import TensorBoardLogger\r\nfrom lightning.pytorch.callbacks import TQDMProgressBar, EarlyStopping\r\n\r\n\r\nclass Model(pl.LightningModule):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.linear1 = torch.nn.Linear(1, 10)\r\n        self.f1 = torch.nn.ELU(inplace=False)\r\n        self.linear2 = torch.nn.Linear(10, 1)\r\n        self.f2 = torch.nn.ELU(inplace=False)\r\n\r\n    def forward(self, x):\r\n        x = self.linear1(x)\r\n        x = self.f1(x)\r\n        x = self.linear2(x)\r\n        x = self.f2(x)\r\n        return x\r\n\r\n    def training_step(self, train_batch, batch_idx):\r\n        # train_batch = (torch.rand(64, 324), torch.rand(64, 22))\r\n        mse_l = torch.nn.MSELoss()\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = mse_l(logits, y)\r\n        self.log('train_loss', loss)\r\n        return loss\r\n\r\n    def validation_step(self, val_batch, batch_idx):\r\n        mse_l = torch.nn.MSELoss()\r\n        x, y = val_batch\r\n        logits = self.forward(x)\r\n        loss = mse_l(logits, y)\r\n        self.log('val_loss', loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n\r\ndef f(x):\r\n    return 1 + x[0] * 2 + x[0] * x[0]\r\n\r\n\r\nif __name__ == '__main__':\r\n    if torch.backends.mps.is_available():\r\n        mps_device = torch.device(\"mps\")\r\n        x = torch.ones(1, device=mps_device)\r\n        print(x)\r\n    else:\r\n        print(\"MPS device not found.\")\r\n\r\n    size = 1000\r\n    xs = [[random.random() * 10] for x in range(0, size)]\r\n    ys = [[f(x)] for x in xs]\r\n    train_split = int(1000 * 0.8)\r\n    validation_split = int(1000 * 0.9)\r\n\r\n    data = list(zip([torch.tensor(x) for x in xs], [torch.tensor(y) for y in ys]))\r\n\r\n    train_data = data[:train_split]\r\n    validation_data = data[train_split:validation_split]\r\n    test_data = data[validation_split:]\r\n\r\n    train_loader = DataLoader(train_data, batch_size=10, num_workers=10)\r\n    validation_loader = DataLoader(validation_data, batch_size=10, num_workers=10)\r\n\r\n    # train\r\n    trainer = pl.Trainer(max_epochs=5,\r\n                         callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3),\r\n                                    TQDMProgressBar(refresh_rate=1)\r\n                                    ],\r\n                         log_every_n_steps=10\r\n                         )\r\n\r\n    model = Model()\r\n    loss_fn = torch.nn.MSELoss()\r\n    xs_test = torch.stack(list(list(zip(*test_data))[0]))\r\n    ys_test = torch.stack(list(list(zip(*test_data))[1]))\r\n\r\n    pred_bt = model(xs_test)\r\n    loss_bt = loss_fn(pred_bt, ys_test)\r\n    start_time = time.time()\r\n    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=validation_loader)\r\n    end_time = time.time()\r\n    pred_at = model(xs_test)\r\n\r\n    loss_at = loss_fn(pred_at, ys_test)\r\n\r\n    print(f\"loss before training = {loss_bt}\")\r\n    print(f\"loss after training = {loss_at}\")\r\n    print(f\"training took = {end_time -start_time}s\")\r\n\r\n```\r\n\r\nThe script takes ~150s to run with workers. By setting the workers to 0 it now takes about 1s.\r\n\r\n\r\n```\r\ntrain_loader = DataLoader(train_data, batch_size=10, num_workers=0)\r\nvalidation_loader = DataLoader(validation_data, batch_size=10, num_workers=0)\r\n```\r\n```\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n        - GPU:               None\r\n        - available:         False\r\n        - version:           None\r\n* Lightning:\r\n        - lightning:         2.0.0\r\n        - lightning-cloud:   0.5.32\r\n        - lightning-utilities: 0.8.0\r\n        - pytorch-lightning: 2.0.0\r\n        - torch:             2.0.0\r\n        - torchmetrics:      0.11.4\r\n        - torchvision:       0.15.1\r\n* Packages:\r\n        - about-time:        4.2.1\r\n        - absl-py:           1.4.0\r\n        - aiohttp:           3.8.4\r\n        - aiosignal:         1.3.1\r\n        - alive-progress:    3.1.1\r\n        - anyio:             3.6.2\r\n        - appnope:           0.1.3\r\n        - argon2-cffi:       21.3.0\r\n        - argon2-cffi-bindings: 21.2.0\r\n        - arrow:             1.2.3\r\n        - asttokens:         2.2.1\r\n        - astunparse:        1.6.3\r\n        - async-timeout:     4.0.2\r\n        - attrs:             22.2.0\r\n        - backcall:          0.2.0\r\n        - beautifulsoup4:    4.12.0\r\n        - bleach:            6.0.0\r\n        - blessed:           1.20.0\r\n        - cachetools:        5.3.0\r\n        - certifi:           2022.12.7\r\n        - cffi:              1.15.1\r\n        - charset-normalizer: 3.1.0\r\n        - click:             8.1.3\r\n        - cloudpickle:       2.2.1\r\n        - comm:              0.1.3\r\n        - contourpy:         1.0.7\r\n        - croniter:          1.3.8\r\n        - cycler:            0.11.0\r\n        - dateutils:         0.6.12\r\n        - debugpy:           1.6.6\r\n        - decorator:         5.1.1\r\n        - deepdiff:          6.3.0\r\n        - defusedxml:        0.7.1\r\n        - dnspython:         2.3.0\r\n        - elosports:         0.1.1\r\n        - email-validator:   1.3.1\r\n        - etils:             1.1.0\r\n        - executing:         1.2.0\r\n        - fastapi:           0.88.0\r\n        - fastjsonschema:    2.16.3\r\n        - filelock:          3.10.6\r\n        - flatbuffers:       23.3.3\r\n        - fonttools:         4.39.3\r\n        - fqdn:              1.5.1\r\n        - frozenlist:        1.3.3\r\n        - fsspec:            2023.3.0\r\n        - gast:              0.4.0\r\n        - google-auth:       2.16.2\r\n        - google-auth-oauthlib: 0.4.6\r\n        - google-pasta:      0.2.0\r\n        - grapheme:          0.6.0\r\n        - grpcio:            1.51.3\r\n        - gym:               0.26.2\r\n        - gym-notices:       0.0.8\r\n        - h11:               0.14.0\r\n        - h5py:              3.8.0\r\n        - httpcore:          0.16.3\r\n        - httptools:         0.5.0\r\n        - httpx:             0.23.3\r\n        - idna:              3.4\r\n        - importlib-metadata: 6.0.0\r\n        - importlib-resources: 5.12.0\r\n        - inquirer:          3.1.3\r\n        - ipykernel:         6.22.0\r\n        - ipython:           8.12.0\r\n        - ipython-genutils:  0.2.0\r\n        - ipywidgets:        8.0.6\r\n        - isoduration:       20.11.0\r\n        - itsdangerous:      2.1.2\r\n        - jax:               0.4.6\r\n        - jaxlib:            0.4.6\r\n        - jedi:              0.18.2\r\n        - jinja2:            3.1.2\r\n        - jsonpointer:       2.3\r\n        - jsonschema:        4.17.3\r\n        - jupyter:           1.0.0\r\n        - jupyter-client:    8.1.0\r\n        - jupyter-console:   6.6.3\r\n        - jupyter-core:      5.3.0\r\n        - jupyter-events:    0.6.3\r\n        - jupyter-server:    2.5.0\r\n        - jupyter-server-terminals: 0.4.4\r\n        - jupyterlab-pygments: 0.2.2\r\n        - jupyterlab-widgets: 3.0.7\r\n        - keras:             2.11.0\r\n        - keras-rl:          0.4.2\r\n        - kiwisolver:        1.4.4\r\n        - libclang:          15.0.6.1\r\n        - lightning:         2.0.0\r\n        - lightning-cloud:   0.5.32\r\n        - lightning-utilities: 0.8.0\r\n        - markdown:          3.4.1\r\n        - markdown-it-py:    2.2.0\r\n        - markupsafe:        2.1.2\r\n        - matplotlib:        3.7.1\r\n        - matplotlib-inline: 0.1.6\r\n        - mdurl:             0.1.2\r\n        - mistune:           2.0.5\r\n        - mpmath:            1.3.0\r\n        - multidict:         6.0.4\r\n        - nbclassic:         0.5.5\r\n        - nbclient:          0.7.3\r\n        - nbconvert:         7.3.0\r\n        - nbformat:          5.8.0\r\n        - nest-asyncio:      1.5.6\r\n        - networkx:          3.0\r\n        - notebook:          6.5.3\r\n        - notebook-shim:     0.2.2\r\n        - numpy:             1.24.2\r\n        - oauthlib:          3.2.2\r\n        - opt-einsum:        3.3.0\r\n        - ordered-set:       4.1.0\r\n        - orjson:            3.8.7\r\n        - packaging:         23.0\r\n        - pandas:            1.5.3\r\n        - pandocfilters:     1.5.0\r\n        - parso:             0.8.3\r\n        - pexpect:           4.8.0\r\n        - pickleshare:       0.7.5\r\n        - pillow:            9.4.0\r\n        - pip:               23.0.1\r\n        - pkgutil-resolve-name: 1.3.10\r\n        - platformdirs:      3.2.0\r\n        - poke-env:          0.5.0\r\n        - prometheus-client: 0.16.0\r\n        - prompt-toolkit:    3.0.38\r\n        - protobuf:          3.19.6\r\n        - psutil:            5.9.4\r\n        - ptyprocess:        0.7.0\r\n        - pure-eval:         0.2.2\r\n        - pyasn1:            0.4.8\r\n        - pyasn1-modules:    0.2.8\r\n        - pycparser:         2.21\r\n        - pydantic:          1.10.7\r\n        - pygments:          2.14.0\r\n        - pyjwt:             2.6.0\r\n        - pyparsing:         3.0.9\r\n        - pyrsistent:        0.19.3\r\n        - python-dateutil:   2.8.2\r\n        - python-dotenv:     1.0.0\r\n        - python-editor:     1.0.4\r\n        - python-json-logger: 2.0.7\r\n        - python-multipart:  0.0.6\r\n        - pytorch-lightning: 2.0.0\r\n        - pytz:              2023.2\r\n        - pyyaml:            6.0\r\n        - pyzmq:             25.0.2\r\n        - qtconsole:         5.4.2\r\n        - qtpy:              2.3.1\r\n        - readchar:          4.0.5\r\n        - requests:          2.28.2\r\n        - requests-oauthlib: 1.3.1\r\n        - rfc3339-validator: 0.1.4\r\n        - rfc3986:           1.5.0\r\n        - rfc3986-validator: 0.1.1\r\n        - rich:              13.3.3\r\n        - rl:                3.1\r\n        - rsa:               4.9\r\n        - scipy:             1.10.1\r\n        - seaborn:           0.12.2\r\n        - send2trash:        1.8.0\r\n        - setuptools:        49.2.1\r\n        - six:               1.16.0\r\n        - sniffio:           1.3.0\r\n        - soupsieve:         2.4\r\n        - stack-data:        0.6.2\r\n        - starlette:         0.22.0\r\n        - starsessions:      1.3.0\r\n        - sympy:             1.11.1\r\n        - tabulate:          0.9.0\r\n        - tensorboard:       2.11.2\r\n        - tensorboard-data-server: 0.6.1\r\n        - tensorboard-plugin-wit: 1.8.1\r\n        - tensorflow-estimator: 2.11.0\r\n        - tensorflow-hub:    0.13.0\r\n        - tensorflow-macos:  2.11.0\r\n        - tensorflowjs:      4.2.0\r\n        - termcolor:         2.2.0\r\n        - terminado:         0.17.1\r\n        - tf-nightly-2.0-preview: 2.0.1\r\n        - tinycss2:          1.2.1\r\n        - torch:             2.0.0\r\n        - torchmetrics:      0.11.4\r\n        - torchvision:       0.15.1\r\n        - tornado:           6.2\r\n        - tqdm:              4.65.0\r\n        - traitlets:         5.9.0\r\n        - typing-extensions: 4.5.0\r\n        - ujson:             5.7.0\r\n        - uri-template:      1.2.0\r\n        - urllib3:           1.26.15\r\n        - uvicorn:           0.21.1\r\n        - uvloop:            0.17.0\r\n        - watchfiles:        0.18.1\r\n        - wcwidth:           0.2.6\r\n        - webcolors:         1.13\r\n        - webencodings:      0.5.1\r\n        - websocket-client:  1.5.1\r\n        - websockets:        10.4\r\n        - werkzeug:          2.2.3\r\n        - wheel:             0.38.4\r\n        - widgetsnbextension: 4.0.7\r\n        - wrapt:             1.15.0\r\n        - yarl:              1.8.2\r\n        - zipp:              3.15.0\r\n* System:\r\n        - OS:                Darwin\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         arm\r\n        - python:            3.8.9\r\n        - version:           Darwin Kernel Version 21.2.0: Sun Nov 28 20:28:41 PST 2021; root:xnu-8019.61.5~1/RELEASE_ARM64_T6000\r\n\r\n</details>\r\n\r\n\r\n\n\ncc @borda",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17410/comments",
    "author": "Tripparsugo",
    "comments": [
      {
        "user": "ryan597",
        "created_at": "2023-04-18T23:32:14Z",
        "body": "This is an artifact of you having created a dataset in memory (and from how small your dataset is, it's probably sitting in the CPU cache). If you use a practical/realistic method such as loading data from a `Dataset` using the `DataLoader` class then you will see how much slower `num_workers=0` is, especially on CPU because you block the process which is actually running the model.\r\n\r\nFurther, this will get much, much worse if you actually have to do some processing on your data as you load it in.\r\n\r\nHere's an example using some of your code, but I made the model use a conv2d to simulate images.\r\n\r\n```python\r\nimport random\r\nimport time\r\n\r\nimport lightning.pytorch as pl\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom lightning.pytorch.callbacks import TQDMProgressBar, EarlyStopping\r\n\r\n\r\nclass Model(pl.LightningModule):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.conv = torch.nn.Conv2d(3, 3, 3, padding='same')\r\n\r\n    def forward(self, x):\r\n        return self.conv(x)\r\n\r\n    def training_step(self, train_batch, batch_idx):\r\n        mse_l = torch.nn.MSELoss()\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = mse_l(logits, y)\r\n        self.log('train_loss', loss)\r\n        return loss\r\n\r\n    def validation_step(self, val_batch, batch_idx):\r\n        mse_l = torch.nn.MSELoss()\r\n        x, y = val_batch\r\n        logits = self.forward(x)\r\n        loss = mse_l(logits, y)\r\n        self.log('val_loss', loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n\r\nclass Data(Dataset):\r\n    def __init__(self, length, do_processing):\r\n        super().__init__()\r\n        self.length = length\r\n        self.do_processing = do_processing\r\n    \r\n    def __len__(self):\r\n        return self.length\r\n\r\n    def __getitem__(self, index):\r\n        if self.do_processing:\r\n            time.sleep(0.01)\r\n        return [torch.rand(3, 256, 256), torch.rand(3, 256, 256)]\r\n\r\ndef test(num_workers, do_processing):\r\n    train_data = Data(3200, do_processing)\r\n    validation_data = Data(320, do_processing)\r\n\r\n    train_loader = DataLoader(train_data, batch_size=32, num_workers=num_workers)\r\n    validation_loader = DataLoader(validation_data, batch_size=32, num_workers=num_workers)\r\n\r\n    trainer = pl.Trainer(max_epochs=1,\r\n                        accelerator='cpu',\r\n                        devices=1,\r\n                        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3),\r\n                                    TQDMProgressBar(refresh_rate=1)],\r\n                        log_every_n_steps=10\r\n                        )\r\n\r\n    model = Model()\r\n\r\n    start_time = time.time()\r\n    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=validation_loader)\r\n    end_time = time.time()\r\n    print(f\"num_workers= {num_workers}\\tdo processing= {do_processing}\")\r\n    print(f\"training took = {end_time -start_time}s\\n\")\r\n\r\nif __name__ == \"__main__\":\r\n    test(0, False)\r\n    test(15, False)\r\n    test(0, True)\r\n    test(15, True)\r\n```\r\n\r\nWith this simple case for 1 epoch I got\r\n\r\n```\r\nnum_workers= 0\tdo processing= False\r\ntraining took = 6.58838677406311s\r\n\r\nnum_workers= 15\tdo processing= False\r\ntraining took = 5.423422574996948s\r\n\r\nnum_workers= 0\tdo processing= True\r\ntraining took = 48.26198148727417s\r\n\r\nnum_workers= 15\tdo processing= True\r\ntraining took = 7.059680938720703s\r\n```\r\nSo you can see the huge difference that having `num_workers=0` makes when you have to do even a tiny amount of processing of the data. This processing could be opening files, decoding images, normalizing, getting tokens in text etc.\r\n\r\nIf you're curious, here's the same tests on a GPU.\r\n\r\n```\r\nnum_workers= 0\tdo processing= False\r\ntraining took = 4.558110475540161s\r\n\r\nnum_workers= 15\tdo processing= False\r\ntraining took = 3.4102823734283447s\r\n\r\nnum_workers= 0\tdo processing= True\r\ntraining took = 46.69817566871643s\r\n\r\nnum_workers= 15\tdo processing= True\r\ntraining took = 5.927751302719116s\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-04-19T09:19:55Z",
        "body": "Thanks @ryan597 for studying the example. I agree with your analysis :) "
      }
    ]
  },
  {
    "number": 17392,
    "title": "OOM because of `deepspeed_stage_2`",
    "created_at": "2023-04-15T19:20:46Z",
    "closed_at": "2023-07-19T13:21:05Z",
    "labels": [
      "question",
      "strategy: deepspeed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17392",
    "body": "### Bug description\n\nI'm using lightning on an 8-card A100. At the beginning, my GPU memory is full, which leads to OOM during validation. But when I use deepspeed_stage_2, I get an OOM at the beginning of training. Why is this happening when all other parameters are the same?\r\nI just set the strategy to deepspeed_stage_2.\n\n### What version are you seeing the problem on?\n\n_No response_\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17392/comments",
    "author": "980202006",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2023-05-03T15:05:31Z",
        "body": "@980202006, could you pls share what lightning version you are using?"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-07-19T13:21:05Z",
        "body": "I'm closing the issue due to lack of infos. \r\nHappy to help @980202006 if you could share the version of lightning, deepspeed, and a code example of what you are trying to do. "
      }
    ]
  },
  {
    "number": 17299,
    "title": "The tensor storage device in the custom loss function is different from the model",
    "created_at": "2023-04-07T06:04:14Z",
    "closed_at": "2023-04-07T10:48:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17299",
    "body": "### Bug description\n\nI didn't find any problems at first when I tried to use a custom loss function, but when I tried to create a new tensor in the forward function of the loss function, I got an error\r\n\r\n`RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!`\n\n### What version are you seeing the problem on?\n\n2.0+\n\n### How to reproduce the bug\n\n```python\nclass AvgReturnTop10Loss(nn.Module):\r\n    def __init__(self, alpha=5):\r\n        super(AvgReturnTop10Loss, self).__init__()\r\n        self.alpha = alpha\r\n\r\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\r\n        errors = torch.pow(y_true - y_pred, 2)\r\n\r\n        top_10_percent = int(y_pred.size(0) * 0.1)\r\n        _, indices = torch.topk(y_pred, top_10_percent)\r\n\r\n        penalty_factor = torch.ones(\r\n            y_pred.size(0), dtype=torch.float32, \r\n        ) # error occur from this line\r\n\r\n        weighted_errors = penalty_factor * errors\r\n\r\n        loss = torch.mean(torch.abs(weighted_errors))\r\n        return loss\n```\n\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\nFile \"xxx/common.py\", line 203, in forward\r\n    weighted_errors = penalty_factor * errors\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce RTX 3080 Ti\r\n                - NVIDIA GeForce RTX 3080 Ti\r\n                - NVIDIA GeForce RTX 3080 Ti\r\n                - NVIDIA GeForce RTX 3080 Ti\r\n        - available:         True\r\n        - version:           11.7\r\n* Lightning:\r\n        - audtorch:          0.6.4\r\n        - lightning:         2.0.0\r\n        - lightning-cloud:   0.5.31\r\n        - lightning-utilities: 0.8.0\r\n        - pytorch-lightning: 2.0.0\r\n        - torch:             2.0.0+cu117\r\n        - torchaudio:        2.0.1+cu117\r\n        - torchmetrics:      0.11.4\r\n        - torchsort:         0.1.9\r\n        - torchvision:       0.15.1+cu117\r\n* Packages:\r\n        - absl-py:           1.4.0\r\n        - aiohttp:           3.8.4\r\n        - aiohttp-cors:      0.7.0\r\n        - aiosignal:         1.3.1\r\n        - alembic:           1.10.2\r\n        - anyio:             3.6.2\r\n        - appdirs:           1.4.4\r\n        - argon2-cffi:       21.3.0\r\n        - argon2-cffi-bindings: 21.2.0\r\n        - arrow:             1.2.3\r\n        - asttokens:         2.0.5\r\n        - astunparse:        1.6.3\r\n        - async-timeout:     4.0.2\r\n        - attrs:             22.2.0\r\n        - audeer:            1.19.0\r\n        - audiofile:         1.2.1\r\n        - audioread:         3.0.0\r\n        - audmath:           1.2.1\r\n        - audtorch:          0.6.4\r\n        - autogluon.common:  0.7.0\r\n        - autogluon.core:    0.7.0\r\n        - autogluon.eda:     0.7.0\r\n        - autogluon.features: 0.7.0\r\n        - autogluon.tabular: 0.7.0\r\n        - backcall:          0.2.0\r\n        - bandit:            1.7.5\r\n        - beautifulsoup4:    4.11.2\r\n        - black:             23.1.0\r\n        - bleach:            6.0.0\r\n        - blessed:           1.20.0\r\n        - boto3:             1.26.94\r\n        - botocore:          1.29.94\r\n        - bottle:            0.12.25\r\n        - cachetools:        5.3.0\r\n        - certifi:           2022.12.7\r\n        - cffi:              1.15.1\r\n        - charset-normalizer: 2.1.1\r\n        - click:             8.1.3\r\n        - cloudpickle:       2.2.1\r\n        - cmaes:             0.9.1\r\n        - cmake:             3.25.0\r\n        - colorful:          0.5.5\r\n        - colorlog:          6.7.0\r\n        - comm:              0.1.2\r\n        - contourpy:         1.0.7\r\n        - convertdate:       2.4.0\r\n        - croniter:          1.3.8\r\n        - cycler:            0.11.0\r\n        - dask:              2023.3.1\r\n        - dateutils:         0.6.12\r\n        - debugpy:           1.5.1\r\n        - decorator:         5.1.1\r\n        - deepdiff:          6.3.0\r\n        - defusedxml:        0.7.1\r\n        - distlib:           0.3.6\r\n        - distributed:       2023.3.1\r\n        - dnspython:         2.3.0\r\n        - docker-pycreds:    0.4.0\r\n        - email-validator:   1.3.1\r\n        - entrypoints:       0.4\r\n        - executing:         0.8.3\r\n        - fast-soft-sort:    0.1\r\n        - fastapi:           0.88.0\r\n        - fastjsonschema:    2.16.3\r\n        - featuretools:      1.23.0\r\n        - filelock:          3.9.0\r\n        - flatbuffers:       23.3.3\r\n        - fonttools:         4.39.2\r\n        - fqdn:              1.5.1\r\n        - frozenlist:        1.3.3\r\n        - fsspec:            2023.3.0\r\n        - gast:              0.4.0\r\n        - gitdb:             4.0.10\r\n        - gitpython:         3.1.31\r\n        - glances:           3.3.1.1\r\n        - google-api-core:   2.11.0\r\n        - google-auth:       2.16.2\r\n        - google-auth-oauthlib: 0.4.6\r\n        - google-pasta:      0.2.0\r\n        - googleapis-common-protos: 1.58.0\r\n        - gpustat:           1.0.0\r\n        - greenlet:          2.0.2\r\n        - grpcio:            1.51.3\r\n        - h11:               0.14.0\r\n        - h5py:              3.8.0\r\n        - heapdict:          1.0.1\r\n        - hijri-converter:   2.2.4\r\n        - holidays:          0.21.13\r\n        - html2text:         2020.1.16\r\n        - httpcore:          0.16.3\r\n        - httptools:         0.5.0\r\n        - httpx:             0.23.3\r\n        - idna:              3.4\r\n        - importlib-metadata: 6.1.0\r\n        - importlib-resources: 5.12.0\r\n        - inquirer:          3.1.3\r\n        - ipykernel:         6.19.2\r\n        - ipython:           8.10.0\r\n        - ipython-genutils:  0.2.0\r\n        - ipywidgets:        8.0.4\r\n        - isoduration:       20.11.0\r\n        - itsdangerous:      2.1.2\r\n        - jedi:              0.18.1\r\n        - jinja2:            3.1.2\r\n        - jmespath:          1.0.1\r\n        - joblib:            1.2.0\r\n        - jsonpointer:       2.3\r\n        - jsonschema:        4.17.3\r\n        - jupyter:           1.0.0\r\n        - jupyter-client:    7.4.9\r\n        - jupyter-console:   6.6.3\r\n        - jupyter-core:      5.3.0\r\n        - jupyter-events:    0.6.3\r\n        - jupyter-server:    2.5.0\r\n        - jupyter-server-terminals: 0.4.4\r\n        - jupyterlab-pygments: 0.2.2\r\n        - jupyterlab-widgets: 3.0.5\r\n        - keras:             2.11.0\r\n        - kiwisolver:        1.4.4\r\n        - korean-lunar-calendar: 0.3.1\r\n        - lazy-loader:       0.1\r\n        - libclang:          15.0.6.1\r\n        - librosa:           0.10.0.post2\r\n        - lightgbm:          3.3.5\r\n        - lightning:         2.0.0\r\n        - lightning-cloud:   0.5.31\r\n        - lightning-utilities: 0.8.0\r\n        - lit:               15.0.7\r\n        - llvmlite:          0.39.1\r\n        - locket:            1.0.0\r\n        - mako:              1.2.4\r\n        - markdown:          3.4.1\r\n        - markdown-it-py:    2.2.0\r\n        - markupsafe:        2.1.2\r\n        - matplotlib:        3.6.3\r\n        - matplotlib-inline: 0.1.6\r\n        - mdurl:             0.1.2\r\n        - missingno:         0.5.2\r\n        - mistune:           2.0.5\r\n        - modin:             0.19.0\r\n        - modin-spreadsheet: 0.1.2\r\n        - mpi4py-mpich:      3.1.2\r\n        - mpmath:            1.2.1\r\n        - msgpack:           1.0.5\r\n        - multidict:         6.0.4\r\n        - mypy-extensions:   1.0.0\r\n        - nbclassic:         0.5.3\r\n        - nbclient:          0.7.2\r\n        - nbconvert:         7.2.10\r\n        - nbformat:          5.7.3\r\n        - nest-asyncio:      1.5.6\r\n        - networkx:          2.8.8\r\n        - notebook-shim:     0.2.2\r\n        - numba:             0.56.4\r\n        - numpy:             1.23.5\r\n        - nvidia-ml-py:      11.495.46\r\n        - oauthlib:          3.2.2\r\n        - opencensus:        0.11.2\r\n        - opencensus-context: 0.1.3\r\n        - opt-einsum:        3.3.0\r\n        - optuna:            3.1.0\r\n        - optuna-dashboard:  0.9.0\r\n        - ordered-set:       4.1.0\r\n        - orjson:            3.8.7\r\n        - packaging:         23.0\r\n        - pandas:            1.5.3\r\n        - pandocfilters:     1.5.0\r\n        - parso:             0.8.3\r\n        - partd:             1.3.0\r\n        - pathspec:          0.11.1\r\n        - pathtools:         0.1.2\r\n        - pbr:               5.11.1\r\n        - pexpect:           4.8.0\r\n        - phik:              0.12.3\r\n        - pickleshare:       0.7.5\r\n        - pillow:            9.3.0\r\n        - pip:               23.0.1\r\n        - platformdirs:      2.5.2\r\n        - plumbum:           1.8.1\r\n        - pooch:             1.6.0\r\n        - prometheus-client: 0.16.0\r\n        - prompt-toolkit:    3.0.36\r\n        - protobuf:          3.19.6\r\n        - psutil:            5.9.0\r\n        - ptyprocess:        0.7.0\r\n        - pure-eval:         0.2.2\r\n        - py-spy:            0.3.14\r\n        - pyarrow:           11.0.0\r\n        - pyasn1:            0.4.8\r\n        - pyasn1-modules:    0.2.8\r\n        - pybind11:          2.10.4\r\n        - pycparser:         2.21\r\n        - pydantic:          1.10.6\r\n        - pygments:          2.14.0\r\n        - pyjwt:             2.6.0\r\n        - pymeeus:           0.5.12\r\n        - pyparsing:         3.0.9\r\n        - pypi-search:       1.2.1\r\n        - pyrsistent:        0.19.3\r\n        - python-dateutil:   2.8.2\r\n        - python-dotenv:     1.0.0\r\n        - python-editor:     1.0.4\r\n        - python-json-logger: 2.0.7\r\n        - python-multipart:  0.0.6\r\n        - pytorch-lightning: 2.0.0\r\n        - pytz:              2022.7.1\r\n        - pyyaml:            6.0\r\n        - pyzmq:             25.0.1\r\n        - qtconsole:         5.4.1\r\n        - qtpy:              2.3.0\r\n        - ray:               2.3.0\r\n        - readchar:          4.0.5\r\n        - requests:          2.28.1\r\n        - requests-oauthlib: 1.3.1\r\n        - resampy:           0.4.2\r\n        - rfc3339-validator: 0.1.4\r\n        - rfc3986:           1.5.0\r\n        - rfc3986-validator: 0.1.1\r\n        - rich:              13.3.2\r\n        - rpyc:              4.1.5\r\n        - rsa:               4.9\r\n        - s3transfer:        0.6.0\r\n        - scikit-learn:      1.2.2\r\n        - scipy:             1.10.1\r\n        - seaborn:           0.12.2\r\n        - send2trash:        1.8.0\r\n        - sentry-sdk:        1.17.0\r\n        - setproctitle:      1.3.2\r\n        - setuptools:        65.6.3\r\n        - six:               1.16.0\r\n        - smart-open:        6.3.0\r\n        - smmap:             5.0.0\r\n        - sniffio:           1.3.0\r\n        - sortedcontainers:  2.4.0\r\n        - soundfile:         0.12.1\r\n        - soupsieve:         2.4\r\n        - soxr:              0.3.4\r\n        - sqlalchemy:        2.0.7\r\n        - stack-data:        0.2.0\r\n        - starlette:         0.22.0\r\n        - starsessions:      1.3.0\r\n        - stevedore:         5.0.0\r\n        - sweetviz:          2.1.4\r\n        - sympy:             1.11.1\r\n        - tabulate:          0.9.0\r\n        - tblib:             1.7.0\r\n        - tensorboard:       2.11.2\r\n        - tensorboard-data-server: 0.6.1\r\n        - tensorboard-plugin-wit: 1.8.1\r\n        - tensorflow:        2.11.0\r\n        - tensorflow-estimator: 2.11.0\r\n        - tensorflow-io-gcs-filesystem: 0.31.0\r\n        - termcolor:         2.2.0\r\n        - terminado:         0.17.1\r\n        - threadpoolctl:     3.1.0\r\n        - tinycss2:          1.2.1\r\n        - tomli:             2.0.1\r\n        - toolz:             0.12.0\r\n        - torch:             2.0.0+cu117\r\n        - torchaudio:        2.0.1+cu117\r\n        - torchmetrics:      0.11.4\r\n        - torchsort:         0.1.9\r\n        - torchvision:       0.15.1+cu117\r\n        - tornado:           6.2\r\n        - tqdm:              4.65.0\r\n        - traitlets:         5.7.1\r\n        - triton:            2.0.0\r\n        - typing-extensions: 4.4.0\r\n        - ujson:             5.7.0\r\n        - unidist:           0.2.2\r\n        - uri-template:      1.2.0\r\n        - urllib3:           1.26.13\r\n        - uvicorn:           0.21.1\r\n        - uvloop:            0.17.0\r\n        - virtualenv:        20.21.0\r\n        - wandb:             0.14.0\r\n        - watchfiles:        0.18.1\r\n        - wcwidth:           0.2.5\r\n        - webcolors:         1.12\r\n        - webencodings:      0.5.1\r\n        - websocket-client:  1.5.1\r\n        - websockets:        10.4\r\n        - werkzeug:          2.2.3\r\n        - wheel:             0.38.4\r\n        - widgetsnbextension: 4.0.5\r\n        - woodwork:          0.22.0\r\n        - wrapt:             1.15.0\r\n        - yarl:              1.8.2\r\n        - zict:              2.2.0\r\n        - zipp:              3.15.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.9.16\r\n        - version:           #1 SMP Mon Oct 19 16:18:59 UTC 2020\r\n\r\n</details>\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17299/comments",
    "author": "ZedRover",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-04-07T10:48:39Z",
        "body": "Hey @ZedRover \r\n\r\nIn your code, replace this line of code:\r\n\r\n```py\r\n        penalty_factor = torch.ones(\r\n            y_pred.size(0), dtype=torch.float32, \r\n        ) # error occur from this line\r\n```\r\nwith\r\n\r\n```py\r\n        penalty_factor = torch.ones(\r\n            y_pred.size(0), dtype=torch.float32, device=y_pred.device\r\n        ) # error occur from this line\r\n```\r\n\r\nSince this question is unrelated to Lightning, and I am confident that my modification will solve your problem, I'm closing the issue but let me know if it doesn't work. \r\n"
      }
    ]
  },
  {
    "number": 17260,
    "title": "AttributeError: module '`pytorch_lightning.utilities.enums`' has no attribute '`_FaultTolerantMode`'",
    "created_at": "2023-04-03T08:06:48Z",
    "closed_at": "2023-07-18T13:50:57Z",
    "labels": [
      "bug",
      "question",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17260",
    "body": "### Bug description\n\nThe issue occurred when loading the previous checkpoint with `ckpt_path` (`trainer.fit(model, train_dataloaders=data_module.train_dataloader(), val_dataloaders=data_module.val_dataloader(),  ckpt_path=load_path)`) to training a model.     \r\npytorch==2.0.0   \r\nlightning=2.0.0\r\n\n\n### How to reproduce the bug\n\n```python\ndata_module = DataInterface(args)\r\nmodel = ModelInterface(args)\r\ntrainer = L.Trainer()\r\ntrainer.fit(model, train_dataloaders=data_module.train_dataloader(), val_dataloaders=data_module.val_dataloader(), ckpt_path=load_path)\n```\n\n\n### Error messages and logs\n\n```\r\nTraceback (most recent call last):\r\n  File \"/home/lchen/workspace/AbPrecision/src/main.py\", line 349, in <module>\r\n    main(args)\r\n  File \"/home/lchen/workspace/AbPrecision/src/main.py\", line 281, in main\r\n    trainer.fit(model, train_dataloaders=data_module.train_dataloader(), val_dataloaders=data_module.val_dataloader(),\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\r\n    call._call_and_handle_interrupt(\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 901, in _run\r\n    self._checkpoint_connector._restore_modules_and_callbacks(ckpt_path)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py\", line 395, in _restore_modules_and_callbacks\r\n    self.resume_start(checkpoint_path)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py\", line 82, in resume_start\r\n    loaded_checkpoint = self.trainer.strategy.load_checkpoint(checkpoint_path)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py\", line 347, in load_checkpoint\r\n    return self.checkpoint_io.load_checkpoint(checkpoint_path)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/lightning/fabric/plugins/io/torch_io.py\", line 89, in load_checkpoint\r\n    return pl_load(path, map_location=map_location)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/lightning/fabric/utilities/cloud_io.py\", line 51, in _load\r\n    return torch.load(f, map_location=map_location)  # type: ignore[arg-type]\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/torch/serialization.py\", line 809, in load\r\n    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/torch/serialization.py\", line 1172, in _load\r\n    result = unpickler.load()\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/pickle.py\", line 1213, in load\r\n    dispatch[key[0]](self)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/pickle.py\", line 1529, in load_global\r\n    klass = self.find_class(module, name)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/torch/serialization.py\", line 1165, in find_class\r\n    return super().find_class(mod_name, name)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/site-packages/pytorch_lightning/_graveyard/legacy_import_unpickler.py\", line 24, in find_class\r\n    return super().find_class(new_module, name)\r\n  File \"/home/lchen/miniconda3/envs/py3.10-torch2.0/lib/python3.10/pickle.py\", line 1584, in find_class\r\n    return getattr(sys.modules[module], name)\r\nAttributeError: module 'pytorch_lightning.utilities.enums' has no attribute '_FaultTolerantMode'\r\n```\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n- PyTorch Lightning Version (2.0.0):\r\n- PyTorch Version (2.0.0):\r\n- Python version (3.10):\r\n- OS (Ubuntu22.04):\r\n- CUDA/cuDNN version: cuda11.7\r\n- GPU: RTX4090\r\n- How you installed Lightning(``pip`)\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17260/comments",
    "author": "lingyun-hifibio",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2023-04-03T13:29:25Z",
        "body": "Correct, this private/protected attribute was removed in 2.0.0\r\n\r\ncould you pls share full example to reproduce/trace the issue"
      },
      {
        "user": "philippjh",
        "created_at": "2023-05-04T17:43:22Z",
        "body": "For me, this error occured during loading an older model, which has been trained with earlier version of both `torch` and `pytorch-lightning` (pre 2.0 releases)."
      },
      {
        "user": "YaduKini",
        "created_at": "2023-06-01T14:48:15Z",
        "body": "I have the same issue. has anyone found a fix for this?. Any help would be greatly appreciated. Thanks. "
      },
      {
        "user": "awaelchli",
        "created_at": "2023-07-17T00:51:00Z",
        "body": "@philippjh or @YaduKini Would you be so kind to send me the checkpoint file that you're trying to load, so I can inspect it and validate a fix? Thanks"
      }
    ]
  },
  {
    "number": 17099,
    "title": "`pytorch-lightning` is also installed when `pip install lightning`",
    "created_at": "2023-03-15T23:59:28Z",
    "closed_at": "2023-03-17T10:45:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17099",
    "body": "### Bug description\r\n\r\nCongratulations on the release of lightning2.0.\r\nI tried to install it immediately, but pytorch-lightning was installed as well as lightning.\r\nThis was not confirmed with lightning 1.9.3.\r\nIs this expected behavior?\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\npip install --dry-run lightning\r\n```\r\n### Error messages and logs\r\n\r\n```\r\nWould install\r\nJinja2-3.1.2\r\nMarkupSafe-2.1.2\r\nPyJWT-2.6.0\r\nPyYAML-6.0\r\nPygments-2.14.0\r\naiohttp-3.8.4\r\naiosignal-1.3.1\r\nanyio-3.6.2\r\narrow-1.2.3\r\nasync-timeout-4.0.2\r\nattrs-22.2.0\r\nbeautifulsoup4-4.11.2\r\nblessed-1.20.0\r\ncertifi-2022.12.7\r\ncharset-normalizer-3.1.0\r\nclick-8.1.3\r\ncmake-3.26.0\r\ncroniter-1.3.8\r\ndateutils-0.6.12\r\ndeepdiff-6.2.3\r\ndnspython-2.3.0\r\nemail-validator-1.3.1\r\nfastapi-0.88.0\r\nfilelock-3.10.0\r\nfrozenlist-1.3.3\r\nfsspec-2023.3.0\r\nh11-0.14.0\r\nhttpcore-0.16.3\r\nhttptools-0.5.0\r\nhttpx-0.23.3\r\nidna-3.4\r\ninquirer-3.1.3\r\nitsdangerous-2.1.2\r\nlightning-2.0.0  <--\r\nlightning-cloud-0.5.31\r\nlightning-utilities-0.8.0\r\nlit-15.0.7\r\nmarkdown-it-py-2.2.0\r\nmdurl-0.1.2\r\nmpmath-1.3.0\r\nmultidict-6.0.4\r\nnetworkx-3.0\r\nnumpy-1.24.2\r\nnvidia-cublas-cu11-11.10.3.66\r\nnvidia-cuda-cupti-cu11-11.7.101\r\nnvidia-cuda-nvrtc-cu11-11.7.99\r\nnvidia-cuda-runtime-cu11-11.7.99\r\nnvidia-cudnn-cu11-8.5.0.96\r\nnvidia-cufft-cu11-10.9.0.58\r\nnvidia-curand-cu11-10.2.10.91\r\nnvidia-cusolver-cu11-11.4.0.1\r\nnvidia-cusparse-cu11-11.7.4.91\r\nnvidia-nccl-cu11-2.14.3\r\nnvidia-nvtx-cu11-11.7.91\r\nordered-set-4.1.0\r\norjson-3.8.7\r\npackaging-23.0\r\npsutil-5.9.4\r\npydantic-1.10.6\r\npython-dateutil-2.8.2\r\npython-dotenv-1.0.0\r\npython-editor-1.0.4\r\npython-multipart-0.0.6\r\npytorch-lightning-2.0.0 <--\r\npytz-2022.7.1\r\nreadchar-4.0.4\r\nrequests-2.28.2\r\nrfc3986-1.5.0\r\nrich-13.3.2\r\nsix-1.16.0\r\nsniffio-1.3.0\r\nsoupsieve-2.4\r\nstarlette-0.22.0\r\nstarsessions-1.3.0\r\nsympy-1.11.1\r\ntorch-2.0.0\r\ntorchmetrics-0.11.4\r\ntqdm-4.65.0\r\ntraitlets-5.9.0\r\ntriton-2.0.0\r\ntyping_extensions-4.5.0\r\nujson-5.7.0\r\nurllib3-1.26.15\r\nuvicorn-0.21.0\r\nuvloop-0.17.0\r\nwatchfiles-0.18.1\r\nwcwidth-0.2.6\r\nwebsocket-client-1.5.1\r\nwebsockets-10.4\r\nyarl-1.8.2\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.0\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0): 2.0\r\n#- Python version (e.g., 3.9): 3.10.8\r\n#- OS (e.g., Linux): Ubuntu22.04\r\n#- CUDA/cuDNN version: 11.7\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source): pip\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17099/comments",
    "author": "Ttayu",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-03-16T01:05:26Z",
        "body": "Hi\r\nI find this counter-intuitive too, but I think this was to maximize compatibility for users with existing code that use the `pytorch_lightning` imports. If you don't want it, you can safely `pip uninstall` this package. \r\n\r\n@Borda @carmocca @justusschock Please fill in the details in case I'm explaining the wrong reasons. "
      },
      {
        "user": "adamjstewart",
        "created_at": "2023-03-16T04:08:34Z",
        "body": "Probably unrelated, but I noticed that there's no `MANIFEST.in` for `lightning` (unlike all other namespaces) when working on #17100."
      },
      {
        "user": "Ttayu",
        "created_at": "2023-03-16T07:32:09Z",
        "body": "> Hi I find this counter-intuitive too, but I think this was to maximize compatibility for users with existing code that use the `pytorch_lightning` imports. If you don't want it, you can safely `pip uninstall` this package.\r\n> \r\n> @Borda @carmocca @justusschock Please fill in the details in case I'm explaining the wrong reasons.\r\n\r\nI agree that compatibility is a good thing. but I think that users who want to keep compatibility should use the old pytorch-lightning and fixed version. and also the docs are not fully maintained, which may cause confusion for new users who use Lightning. (Because it has been decided to completely migrate to Lightning.)"
      },
      {
        "user": "Borda",
        "created_at": "2023-03-16T08:52:40Z",
        "body": "yes, as requested in #16921 we append PL to lightning as there is zero additional dependency\r\nalso, it is installed without a specified version, so it pulls what is on PyPI or you have in the local cache from past installations you can adjust the needed version\r\n```\r\npip install lightning pytorch-lightning==1.8.6\r\n```\r\nor to be aligned you can\r\n```\r\npip install lightning --upgrade-strategy=eager\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-03-17T10:45:39Z",
        "body": "Okay, so closing this since it is expected and planned. In the future, once pytorch-lightning is no longer supported, this won't be installed anymore together."
      },
      {
        "user": "adamgayoso",
        "created_at": "2023-03-23T20:45:47Z",
        "body": "How much longer will releases be made under both names?"
      }
    ]
  },
  {
    "number": 16970,
    "title": "Start training using CLI on Slurm cluster",
    "created_at": "2023-03-06T16:12:59Z",
    "closed_at": "2023-04-14T08:48:10Z",
    "labels": [
      "bug",
      "question",
      "won't fix",
      "waiting on author",
      "lightningcli"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16970",
    "body": "### Bug description\r\n\r\nHi,\r\nIm trying to run a simple pytorch lightning model training on mnist data using the pytorch CLI (with yaml config) as a slurm job.\r\n\r\n### How to reproduce the bug\r\nIm starting the slurm job using: `sbatch train_submit.sh`\r\ntrain_submit.sh:\r\n``` bash\r\n#!/bin/bash -l\r\n\r\n# SLURM SUBMIT SCRIPT\r\n#SBATCH --nodes=1             # This needs to match Trainer(num_nodes=...)\r\n#SBATCH --ntasks-per-node=1   # This needs to match Trainer(devices=...)\r\n#SBATCH --cpus-per-task=5\r\n#SBATCH --mem-per-cpu=5240\r\n#SBATCH --gpus=1\r\n#SBATCH --time=01:00:00\r\n#SBATCH --mail-type=BEGIN,END\r\n\r\n# activate conda env\r\n# source activate $1\r\n\r\n# debugging flags (optional)\r\n# export NCCL_DEBUG=INFO\r\n# export PYTHONFAULTHANDLER=1\r\n\r\n# on your cluster you might need these:\r\n# set the network interface\r\n# export NCCL_SOCKET_IFNAME=^docker0,lo\r\n\r\n# might need the latest CUDA\r\n# module load NCCL/2.4.7-1-cuda.10.0\r\n\r\n# run script from above\r\nsrun python3 cli_test.py fit --config config.yaml\r\n```\r\nconfig.yaml file:\r\n```\r\nseed_everything_default: null\r\ntrainer:\r\n  accelerator: gpu\r\n  limit_train_batches: 100\r\n  max_epochs: 500\r\n  devices: 1\r\n  logger: true\r\n  callbacks:\r\n    - class_path: pytorch_lightning.callbacks.ModelCheckpoint\r\n      init_args:\r\n        save_top_k: 1\r\n        monitor: 'val_loss'\r\n        mode: min\r\n        filename: 'vit-best'\r\n    - class_path: pytorch_lightning.callbacks.ModelCheckpoint\r\n      init_args:\r\n        save_last: true\r\n        filename: 'vit-last'\r\nckpt_path: null\r\nlog_dir: /cluster/dir/to/log\r\n```\r\n\r\ncli_test.py:\r\n```\r\n# main.py\r\nfrom pytorch_lightning.cli import LightningCLI\r\n\r\nimport os\r\nfrom torch import optim, nn, utils, Tensor\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision.transforms import ToTensor\r\nimport pytorch_lightning as pl\r\n\r\n# define any number of nn.Modules (or use your current ones)\r\nencoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\r\ndecoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\r\n\r\n\r\n# define the LightningModule\r\nclass LitAutoEncoder(pl.LightningModule):\r\n    def __init__(self, encoder, decoder):\r\n        super().__init__()\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defines the train loop.\r\n        # it is independent of forward\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = nn.functional.mse_loss(x_hat, x)\r\n        # Logging to TensorBoard (if installed) by default\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n\r\nclass MNISTDataModule(pl.LightningDataModule):\r\n    def __init__(self, data_dir: str = os.getcwd(), batch_size: int = 32):\r\n        super().__init__()\r\n        self.data_dir = data_dir\r\n        self.batch_size = batch_size\r\n\r\n    def setup(self, stage: str):\r\n        self.mnist_test = MNIST(self.data_dir, train=False)\r\n        self.mnist_predict = MNIST(self.data_dir, train=False)\r\n        mnist_full = MNIST(self.data_dir, train=True)\r\n        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\r\n\r\n\r\ndef cli_main():\r\n    cli = LightningCLI(LitAutoEncoder, MNISTDataModule)\r\n    # note: don't call fit!!\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    cli_main()\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\nslurm-9842342.out (File where std:output is printed)\r\n```\r\n2023-03-06 17:02:07.694344: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\nusage: cli_test.py [-h] [-c CONFIG] [--print_config ^H[=flags]]\r\n                   {fit,validate,test,predict,tune} ...\r\ncli_test.py: error: 'Configuration check failed :: No action for destination key \"seed_everything_default\" to check its value.'\r\nsrun: error: eu-g2-16: task 0: Exited with exit code 2\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce GTX 1080 Ti\r\n        - available:         True\r\n        - version:           11.3\r\n* Lightning:\r\n        - lightning-utilities: 0.7.1\r\n        - pytorch-ignite:    0.4.10\r\n        - pytorch-lightning: 1.9.4\r\n        - pytorch3dunet:     1.3.3\r\n        - torch:             1.11.0+cu113\r\n        - torch-cluster:     1.6.0\r\n        - torch-fidelity:    0.3.0\r\n        - torch-geometric:   2.0.4\r\n        - torch-scatter:     2.0.9\r\n        - torch-sparse:      0.6.13\r\n        - torch-spline-conv: 1.2.1\r\n        - torchaudio:        0.11.0+cu113\r\n        - torchmetrics:      0.11.3\r\n        - torchvision:       0.12.0+cu113\r\n* Packages:\r\n        - absl-py:           1.0.0\r\n        - accesscontrol:     5.3.1\r\n        - acquisition:       4.10\r\n        - affine:            2.3.1\r\n        - aiohttp:           3.8.1\r\n        - aiohttp-cors:      0.7.0\r\n        - aioredis:          2.0.1\r\n        - aiosignal:         1.2.0\r\n        - alabaster:         0.7.12\r\n        - alembic:           1.8.1\r\n        - amply:             0.1.5\r\n        - aniso8601:         9.0.1\r\n        - anndata:           0.8.0\r\n        - antlr4-python3-runtime: 4.9.3\r\n        - anyio:             3.6.1\r\n        - app-model:         0.1.1\r\n        - appdirs:           1.4.4\r\n        - apptools:          5.1.0\r\n        - argcomplete:       2.0.0\r\n        - argh:              0.26.2\r\n        - argon2:            0.1.10\r\n        - argon2-cffi:       21.3.0\r\n        - argon2-cffi-bindings: 21.2.0\r\n        - arviz:             0.12.1\r\n        - ase:               3.22.1\r\n        - asn1crypto:        1.5.1\r\n        - astor:             0.8.1\r\n        - asttokens:         2.0.5\r\n        - astunparse:        1.6.3\r\n        - async-generator:   1.10\r\n        - async-timeout:     4.0.2\r\n        - atomicwrites:      1.4.0\r\n        - attrs:             21.4.0\r\n        - audioread:         2.1.9\r\n        - authencoding:      4.3\r\n        - autopage:          0.5.1\r\n        - autopep8:          1.6.0\r\n        - aws-requests-auth: 0.4.3\r\n        - babel:             2.10.1\r\n        - backcall:          0.2.0\r\n        - beautifulsoup4:    4.11.1\r\n        - bidict:            0.22.0\r\n        - bids-validator:    1.9.3\r\n        - biopython:         1.79\r\n        - bitstring:         3.1.9\r\n        - black:             22.3.0\r\n        - bleach:            5.0.0\r\n        - blessings:         1.7\r\n        - blurhash:          1.1.4\r\n        - bokeh:             2.4.3\r\n        - boost:             0.1\r\n        - boto3:             1.23.10\r\n        - botocore:          1.26.10\r\n        - bottleneck:        1.3.4\r\n        - btrees:            4.10.0\r\n        - build:             0.10.0\r\n        - cachetools:        5.2.0\r\n        - cachey:            0.2.1\r\n        - cellmodeller:      b-v4.3-42-g96ab099-\r\n        - certifi:           2022.5.18.1\r\n        - certipy:           0.1.3\r\n        - cffi:              1.15.0\r\n        - cftime:            1.6.0\r\n        - chainer:           7.8.1\r\n        - chameleon:         3.10.1\r\n        - chardet:           4.0.0\r\n        - charset-normalizer: 2.0.12\r\n        - chex:              0.1.3\r\n        - clang:             14.0\r\n        - click:             8.1.3\r\n        - click-plugins:     1.1.1\r\n        - cligj:             0.7.2\r\n        - clikit:            0.6.2\r\n        - cloudpickle:       2.1.0\r\n        - cmaes:             0.9.1\r\n        - cmake:             3.24.1.1\r\n        - cmd2:              2.4.1\r\n        - codecov:           2.1.12\r\n        - colorama:          0.4.4\r\n        - coloredlogs:       15.0.1\r\n        - colorful:          0.5.4\r\n        - colorlog:          6.6.0\r\n        - colorlover:        0.3.0\r\n        - colormath:         3.0.0\r\n        - commonmark:        0.9.1\r\n        - configargparse:    1.5.3\r\n        - configobj:         5.0.6\r\n        - configparser:      5.2.0\r\n        - connection-pool:   0.0.3\r\n        - contextlib2:       21.6.0\r\n        - coverage:          6.4\r\n        - crashtest:         0.3.1\r\n        - cryptography:      38.0.4\r\n        - cucim:             23.2.0\r\n        - cufflinks:         0.17.3\r\n        - cupy-cuda11x:      11.1.0\r\n        - cutadapt:          4.0\r\n        - cutensor:          1.6.0.3\r\n        - cvxopt:            1.3.0\r\n        - cvxpy:             1.2.1\r\n        - cycler:            0.11.0\r\n        - cython:            0.29.30\r\n        - dask:              2022.5.2\r\n        - databricks-cli:    0.17.4\r\n        - datasets:          2.5.1\r\n        - datetime:          4.4\r\n        - datrie:            0.8.2\r\n        - deap:              1.3.1\r\n        - debtcollector:     2.5.0\r\n        - debugpy:           1.6.0\r\n        - decorator:         5.1.1\r\n        - deepdiff:          5.8.1\r\n        - defusedxml:        0.7.1\r\n        - deprecated:        1.2.13\r\n        - deprecation:       2.1.0\r\n        - descartes:         1.1.0\r\n        - dill:              0.3.5.1\r\n        - distributed:       2022.5.2\r\n        - distro:            1.8.0\r\n        - dm-tree:           0.1.7\r\n        - dnaio:             0.9.0\r\n        - dnspython:         2.2.1\r\n        - docker:            6.0.1\r\n        - docker-pycreds:    0.4.0\r\n        - docopt:            0.6.2\r\n        - docstring-parser:  0.15\r\n        - documenttemplate:  4.0\r\n        - docutils:          0.17.1\r\n        - dpath:             2.0.6\r\n        - easydict:          1.9\r\n        - ecos:              2.0.10\r\n        - einops:            0.4.1\r\n        - entrypoints:       0.4\r\n        - envisage:          6.0.1\r\n        - ephem:             4.1.3\r\n        - esda:              2.4.1\r\n        - et-xmlfile:        1.1.0\r\n        - etils:             0.8.0\r\n        - eventlet:          0.33.1\r\n        - evo:               1.18.1\r\n        - executing:         0.8.3\r\n        - extensionclass:    4.6\r\n        - extras:            1.0.0\r\n        - fasteners:         0.17.3\r\n        - fastjsonschema:    2.15.3\r\n        - fastprogress:      1.0.2\r\n        - fastrlock:         0.8\r\n        - filelock:          3.7.0\r\n        - findlibs:          0.0.2\r\n        - fiona:             1.8.22\r\n        - fire:              0.5.0\r\n        - flask:             2.1.2\r\n        - flask-cors:        3.0.10\r\n        - flask-json:        0.3.4\r\n        - flask-restplus:    0.13.0\r\n        - flask-restx:       0.5.1\r\n        - flatbuffers:       1.12\r\n        - flit:              3.7.1\r\n        - flit-core:         3.7.1\r\n        - flowvision:        0.2.0\r\n        - follicle-tracker:  0.1.dev221+gc3cd246\r\n        - fonttools:         4.33.3\r\n        - freetype-py:       2.3.0\r\n        - frozenlist:        1.3.0\r\n        - fsspec:            2022.5.0\r\n        - funcsigs:          1.0.2\r\n        - future:            0.18.2\r\n        - futurist:          2.4.1\r\n        - gast:              0.4.0\r\n        - gdown:             4.4.0\r\n        - geopandas:         0.12.2\r\n        - gevent:            21.12.0\r\n        - giddy:             2.3.3\r\n        - gitdb:             4.0.9\r\n        - gitdb2:            4.0.2\r\n        - gitpython:         3.1.27\r\n        - gmpy2:             2.1.5\r\n        - google-api-core:   2.8.1\r\n        - google-auth:       2.6.6\r\n        - google-auth-oauthlib: 0.4.6\r\n        - google-pasta:      0.2.0\r\n        - googleapis-common-protos: 1.56.2\r\n        - googledrivedownloader: 0.4\r\n        - gpaw:              22.8.0\r\n        - gprmax:            3.1.4\r\n        - gpustat:           0.6.0\r\n        - grabbit:           0.2.6\r\n        - graphtools:        1.5.2\r\n        - greenlet:          1.1.2\r\n        - grpcio:            1.46.3\r\n        - gunicorn:          20.1.0\r\n        - h3:                3.7.4\r\n        - h5py:              3.7.0\r\n        - haversine:         2.5.1\r\n        - hdbscan:           0.8.29\r\n        - heapdict:          1.0.1\r\n        - hiredis:           2.0.0\r\n        - hsluv:             5.0.3\r\n        - html5lib:          1.1\r\n        - httpstan:          4.8.2\r\n        - huggingface-hub:   0.7.0\r\n        - humanfriendly:     10.0\r\n        - hydra-core:        1.2.0\r\n        - hyperopt:          0.2.7\r\n        - idna:              3.3\r\n        - ifcfg:             0.22\r\n        - imagecodecs:       2023.1.23\r\n        - imageio:           2.19.3\r\n        - imageio-ffmpeg:    0.4.7\r\n        - imagesize:         1.3.0\r\n        - importlib-metadata: 4.11.4\r\n        - importlib-resources: 5.7.1\r\n        - in-n-out:          0.1.7\r\n        - inequality:        1.0.0\r\n        - iniconfig:         1.1.1\r\n        - install:           1.3.5\r\n        - iopath:            0.1.6\r\n        - ipdb:              0.13.9\r\n        - ipykernel:         6.13.0\r\n        - ipython:           8.4.0\r\n        - ipython-genutils:  0.2.0\r\n        - ipywidgets:        7.7.0\r\n        - isal:              0.11.1\r\n        - iso3166:           2.0.2\r\n        - iso8601:           1.0.2\r\n        - isodate:           0.6.1\r\n        - iteration-utilities: 0.11.0\r\n        - itk:               5.3.0\r\n        - itk-core:          5.3.0\r\n        - itk-filtering:     5.3.0\r\n        - itk-io:            5.3.0\r\n        - itk-numerics:      5.3.0\r\n        - itk-registration:  5.3.0\r\n        - itk-segmentation:  5.3.0\r\n        - itsdangerous:      2.1.2\r\n        - jax:               0.3.23\r\n        - jaxlib:            0.3.22+cuda11.cudnn82\r\n        - jedi:              0.18.1\r\n        - jeepney:           0.8.0\r\n        - jieba:             0.42.1\r\n        - jinja2:            3.1.2\r\n        - jmespath:          1.0.0\r\n        - joblib:            1.1.0\r\n        - json-tricks:       3.16.1\r\n        - json5:             0.9.8\r\n        - jsonargparse:      4.20.0\r\n        - jsonlines:         1.2.0\r\n        - jsonpickle:        2.2.0\r\n        - jsonpointer:       2.3\r\n        - jsonschema:        4.5.1\r\n        - jupyter:           1.0.0\r\n        - jupyter-client:    7.3.1\r\n        - jupyter-console:   6.4.3\r\n        - jupyter-contrib-core: 0.3.3\r\n        - jupyter-core:      4.10.0\r\n        - jupyter-highlight-selected-word: 0.2.0\r\n        - jupyter-server:    1.17.0\r\n        - jupyter-telemetry: 0.1.0\r\n        - jupyterlab:        3.4.2\r\n        - jupyterlab-pygments: 0.2.2\r\n        - jupyterlab-server: 2.14.0\r\n        - jupyterlab-widgets: 1.1.0\r\n        - keras:             2.9.0\r\n        - keras-preprocessing: 1.1.2\r\n        - keyring:           23.5.1\r\n        - kiwisolver:        1.4.2\r\n        - lazy-object-proxy: 1.7.1\r\n        - libclang:          14.0.1\r\n        - libpysal:          4.6.2\r\n        - lightning-utilities: 0.7.1\r\n        - llvmlite:          0.38.1\r\n        - lmdb:              1.4.0\r\n        - locket:            1.0.0\r\n        - logutils:          0.3.5\r\n        - loompy:            3.0.7\r\n        - lxml:              4.8.0\r\n        - lz4:               4.0.1\r\n        - lzstring:          1.0.4\r\n        - mageck:            0.5.9.4\r\n        - magicgui:          0.7.0\r\n        - mako:              1.2.0\r\n        - mapclassify:       2.4.3\r\n        - markdown:          3.3.7\r\n        - markupsafe:        2.1.1\r\n        - marshmallow:       3.18.0\r\n        - mastodon.py:       1.8.0\r\n        - matplotlib:        3.5.2\r\n        - matplotlib-inline: 0.1.3\r\n        - mccabe:            0.7.0\r\n        - mercantile:        1.2.1\r\n        - mgwr:              2.1.2\r\n        - mistune:           0.8.4\r\n        - mlflow:            2.2.1\r\n        - mock:              4.0.3\r\n        - monai:             1.1.0\r\n        - more-itertools:    8.13.0\r\n        - mpi4py:            3.1.4\r\n        - mpmath:            1.2.1\r\n        - msgpack:           1.0.3\r\n        - multidict:         6.0.2\r\n        - multimapping:      4.1\r\n        - multipart:         0.2.4\r\n        - multiprocess:      0.70.13\r\n        - multiqc:           1.13\r\n        - munch:             2.5.0\r\n        - mypy-extensions:   0.4.3\r\n        - napari:            0.4.17\r\n        - napari-console:    0.0.7\r\n        - napari-plugin-engine: 0.2.0\r\n        - napari-svg:        0.1.6\r\n        - natsort:           8.1.0\r\n        - nbclassic:         0.3.7\r\n        - nbclient:          0.6.3\r\n        - nbconvert:         6.5.0\r\n        - nbformat:          5.4.0\r\n        - nbsphinx:          0.8.8\r\n        - nest-asyncio:      1.5.5\r\n        - netaddr:           0.8.0\r\n        - netcdf4:           1.5.8\r\n        - netifaces:         0.11.0\r\n        - networkx:          2.8.2\r\n        - nibabel:           3.2.2\r\n        - ninja:             1.11.1\r\n        - nipy:              0.5.0\r\n        - nltk:              3.7\r\n        - nni:               2.10\r\n        - nose:              1.3.7\r\n        - nose-timer:        1.0.1\r\n        - notebook:          6.4.11\r\n        - notebook-shim:     0.1.0\r\n        - npe2:              0.6.2\r\n        - nptyping:          2.5.0\r\n        - num2words:         0.5.10\r\n        - numba:             0.55.2\r\n        - numexpr:           2.8.1\r\n        - numpy:             1.22.4\r\n        - numpy-groupies:    0.9.16\r\n        - numpy-quaternion:  2022.4.2\r\n        - numpydoc:          1.5.0\r\n        - nvidia-ml-py3:     7.352.0\r\n        - oauthlib:          3.2.0\r\n        - omegaconf:         2.2.2\r\n        - opencensus:        0.9.0\r\n        - opencensus-context: 0.1.2\r\n        - opencv-contrib-python: 4.5.5.64\r\n        - opencv-python:     4.5.5.64\r\n        - openpyxl:          3.0.10\r\n        - openseespy:        3.3.0.1.1\r\n        - openseespylinux:   3.4.0.1\r\n        - openslide-python:  1.1.2\r\n        - opt-einsum:        3.3.0\r\n        - optax:             0.1.2\r\n        - optuna:            3.1.0\r\n        - ordered-set:       4.1.0\r\n        - os-service-types:  1.7.0\r\n        - oslo.i18n:         5.1.0\r\n        - osmnx:             1.2.2\r\n        - osqp:              0.6.2.post5\r\n        - ovary-analysis:    0.0.3\r\n        - overpy:            0.6\r\n        - packaging:         21.3\r\n        - pamela:            1.0.0\r\n        - pandas:            1.4.2\r\n        - pandas-datareader: 0.10.0\r\n        - pandoc:            2.2\r\n        - pandocfilters:     1.5.0\r\n        - parso:             0.8.3\r\n        - partd:             1.2.0\r\n        - paste:             3.5.0\r\n        - pastedeploy:       2.1.1\r\n        - pastel:            0.2.1\r\n        - pathos:            0.2.9\r\n        - pathspec:          0.9.0\r\n        - pathtools:         0.1.2\r\n        - patsy:             0.5.2\r\n        - pbr:               5.9.0\r\n        - persistence:       3.3\r\n        - persistent:        4.9.0\r\n        - pert:              2019.11\r\n        - pexpect:           4.8.0\r\n        - pickleshare:       0.7.5\r\n        - pillow:            9.1.1\r\n        - pint:              0.19.2\r\n        - pip:               22.2.2\r\n        - pkginfo:           1.8.2\r\n        - plac:              1.3.5\r\n        - platformdirs:      2.5.2\r\n        - plotly:            5.8.0\r\n        - pluggy:            1.0.0\r\n        - plumbum:           1.7.2\r\n        - ply:               3.11\r\n        - pointpats:         2.2.0\r\n        - pooch:             1.6.0\r\n        - portalocker:       2.4.0\r\n        - pox:               0.3.1\r\n        - ppft:              1.7.6.5\r\n        - prettytable:       3.3.0\r\n        - prometheus-client: 0.14.1\r\n        - promise:           2.3\r\n        - prompt-toolkit:    3.0.29\r\n        - protobuf:          3.19.4\r\n        - psutil:            5.9.1\r\n        - psygnal:           0.8.1\r\n        - ptyprocess:        0.7.0\r\n        - pulp:              2.6.0\r\n        - pure-eval:         0.2.2\r\n        - py:                1.11.0\r\n        - py-spy:            0.3.12\r\n        - py4design:         0.28\r\n        - py4j:              0.10.9.5\r\n        - pyarrow:           9.0.0\r\n        - pyasn1:            0.4.8\r\n        - pyasn1-modules:    0.2.8\r\n        - pybind11:          2.9.2\r\n        - pybis:             1.35.2\r\n        - pybufrkit:         0.2.19\r\n        - pycocotools:       2.0.4\r\n        - pycodestyle:       2.8.0\r\n        - pycollada:         0.7.2\r\n        - pycparser:         2.21\r\n        - pydantic:          1.10.5\r\n        - pydicom:           2.3.1\r\n        - pydot:             1.4.2\r\n        - pyepsg:            0.4.0\r\n        - pyface:            7.4.1\r\n        - pyfaidx:           0.6.4\r\n        - pyflakes:          2.5.0\r\n        - pyglet:            1.5.26\r\n        - pygments:          2.12.0\r\n        - pygsp:             0.5.1\r\n        - pygsti:            0.9.10.1\r\n        - pyinotify:         0.9.6\r\n        - pyjwt:             2.6.0\r\n        - pylev:             1.4.0\r\n        - pymeshfix:         0.16.2\r\n        - pymf:              0.1.9\r\n        - pymongo:           4.1.1\r\n        - pynrrd:            1.0.0\r\n        - pyomo:             6.4.1\r\n        - pyopencl:          2022.1.5\r\n        - pyopengl:          3.1.6\r\n        - pyopenssl:         22.1.0\r\n        - pyparsing:         3.0.9\r\n        - pyperclip:         1.8.2\r\n        - pyproj:            3.4.1\r\n        - pyproject-hooks:   1.0.0\r\n        - pypsa:             0.19.3\r\n        - pyqt5:             5.15.6\r\n        - pyqt5-qt5:         5.15.2\r\n        - pyqt5-sip:         12.10.1\r\n        - pyro4:             4.82\r\n        - pyrsistent:        0.18.1\r\n        - pysam:             0.19.1\r\n        - pyshp:             2.3.0\r\n        - pysimdjson:        3.2.0\r\n        - pysocks:           1.7.1\r\n        - pystan:            3.5.0\r\n        - pytest:            7.1.2\r\n        - python-dateutil:   2.8.2\r\n        - python-engineio:   4.3.2\r\n        - python-gettext:    4.0\r\n        - python-json-logger: 2.0.4\r\n        - python-louvain:    0.16\r\n        - python-magic:      0.4.27\r\n        - python-socketio:   5.6.0\r\n        - pythonwebhdfs:     0.2.3\r\n        - pytoml:            0.1.21\r\n        - pytomlpp:          1.0.11\r\n        - pytools:           2022.1.9\r\n        - pytorch-ignite:    0.4.10\r\n        - pytorch-lightning: 1.9.4\r\n        - pytorch3dunet:     1.3.3\r\n        - pytz:              2022.1\r\n        - pyutilib:          6.0.0\r\n        - pyutillib:         0.3.0\r\n        - pyvista:           0.38.3\r\n        - pywavelets:        1.3.0\r\n        - pyxlsb:            1.0.9\r\n        - pyyaml:            6.0\r\n        - pyzmq:             23.0.0\r\n        - qdldl:             0.1.5.post2\r\n        - qtconsole:         5.3.0\r\n        - qtpy:              2.1.0\r\n        - quantecon:         0.5.3\r\n        - querystring-parser: 1.2.4\r\n        - quilt3:            5.0.0\r\n        - rasterio:          1.3.6\r\n        - rasterstats:       0.18.0\r\n        - ratelimiter:       1.2.0.post0\r\n        - rdflib:            6.1.1\r\n        - readme-renderer:   35.0\r\n        - recommonmark:      0.7.1\r\n        - redis:             4.3.1\r\n        - rednose:           1.3.0\r\n        - regex:             2022.4.24\r\n        - reportlab:         3.6.9\r\n        - repoze.lru:        0.7\r\n        - requests:          2.28.2\r\n        - requests-futures:  1.0.0\r\n        - requests-oauthlib: 1.3.1\r\n        - requests-toolbelt: 0.9.1\r\n        - requests-unixsocket: 0.3.0\r\n        - requestsexceptions: 1.4.0\r\n        - resampy:           0.2.2\r\n        - responses:         0.18.0\r\n        - restrictedpython:  5.3a1.dev0\r\n        - retry:             0.9.2\r\n        - retrying:          1.3.3\r\n        - rfc3986:           2.0.0\r\n        - rich:              12.4.4\r\n        - rich-click:        1.5.2\r\n        - roman:             3.3\r\n        - rosbags:           0.9.11\r\n        - routes:            2.5.1\r\n        - rsa:               4.8\r\n        - rtree:             1.0.0\r\n        - ruamel.yaml:       0.17.21\r\n        - ruamel.yaml.clib:  0.2.6\r\n        - rvlib:             0.0.6\r\n        - s3transfer:        0.5.2\r\n        - salib:             1.4.5\r\n        - schema:            0.7.5\r\n        - scikit-build:      0.16.7\r\n        - scikit-fmm:        2022.3.26\r\n        - scikit-image:      0.19.2\r\n        - scikit-learn:      1.1.1\r\n        - scipy:             1.8.1\r\n        - scons:             4.4.0\r\n        - scooby:            0.7.1\r\n        - scs:               3.2.0\r\n        - seaborn:           0.11.2\r\n        - secretstorage:     3.3.2\r\n        - semver:            2.13.0\r\n        - send2trash:        1.8.0\r\n        - sentence-transformers: 2.2.0\r\n        - sentencepiece:     0.1.96\r\n        - sentry-sdk:        1.5.12\r\n        - serpent:           1.40\r\n        - setproctitle:      1.2.3\r\n        - setuptools:        58.1.0\r\n        - setuptools-scm:    6.4.2\r\n        - shap:              0.41.0\r\n        - shapely:           1.8.5.post1\r\n        - shortuuid:         1.0.9\r\n        - simplegeneric:     0.8.1\r\n        - simplejson:        3.17.6\r\n        - six:               1.16.0\r\n        - slicer:            0.0.7\r\n        - smart-open:        6.0.0\r\n        - smmap:             5.0.0\r\n        - smmap2:            3.0.1\r\n        - snakemake:         7.8.0\r\n        - sniffio:           1.2.0\r\n        - snowballstemmer:   2.2.0\r\n        - snuggs:            1.4.7\r\n        - sortedcontainers:  2.4.0\r\n        - soupsieve:         2.3.2.post1\r\n        - spaghetti:         1.6.5\r\n        - spectra:           0.0.11\r\n        - spglm:             1.0.8\r\n        - sphinx:            4.5.0\r\n        - sphinxcontrib-applehelp: 1.0.2\r\n        - sphinxcontrib-devhelp: 1.0.2\r\n        - sphinxcontrib-htmlhelp: 2.0.0\r\n        - sphinxcontrib-jsmath: 1.0.1\r\n        - sphinxcontrib-qthelp: 1.0.3\r\n        - sphinxcontrib-serializinghtml: 1.1.5\r\n        - sphinxcontrib-websupport: 1.2.4\r\n        - spint:             1.0.7\r\n        - spreg:             1.2.4\r\n        - spvcm:             0.3.0\r\n        - sqlalchemy:        1.4.37\r\n        - sqlparse:          0.4.2\r\n        - stack-data:        0.2.0\r\n        - staticmap:         0.5.5\r\n        - statsd:            3.3.0\r\n        - statsmodels:       0.13.2\r\n        - stevedore:         3.5.0\r\n        - stopit:            1.1.2\r\n        - subprocess32:      3.5.4\r\n        - superqt:           0.4.1\r\n        - svg.path:          6.0\r\n        - sympy:             1.10.1\r\n        - tables:            3.7.0\r\n        - tabulate:          0.8.9\r\n        - tasklogger:        1.1.2\r\n        - tblib:             1.7.0\r\n        - tempita:           0.5.2\r\n        - tenacity:          8.0.1\r\n        - tensorboard:       2.9.0\r\n        - tensorboard-data-server: 0.6.1\r\n        - tensorboard-plugin-wit: 1.8.1\r\n        - tensorboardx:      2.5\r\n        - tensorflow-estimator: 2.9.0\r\n        - tensorflow-gpu:    2.9.1\r\n        - tensorflow-io-gcs-filesystem: 0.26.0\r\n        - termcolor:         1.1.0\r\n        - terminado:         0.15.0\r\n        - terminaltables:    3.1.10\r\n        - termstyle:         0.1.11\r\n        - testpath:          0.6.0\r\n        - testresources:     2.0.1\r\n        - texttable:         1.6.4\r\n        - theano:            1.0.5\r\n        - theano-pymc:       1.1.2\r\n        - threadpoolctl:     3.1.0\r\n        - tifffile:          2022.5.4\r\n        - timezonefinder:    6.0.0\r\n        - tinycss2:          1.1.1\r\n        - tokenizers:        0.12.1\r\n        - toml:              0.10.2\r\n        - tomli:             2.0.1\r\n        - tomli-w:           1.0.0\r\n        - tomlkit:           0.11.0\r\n        - toolz:             0.11.2\r\n        - toposort:          1.7\r\n        - torch:             1.11.0+cu113\r\n        - torch-cluster:     1.6.0\r\n        - torch-fidelity:    0.3.0\r\n        - torch-geometric:   2.0.4\r\n        - torch-scatter:     2.0.9\r\n        - torch-sparse:      0.6.13\r\n        - torch-spline-conv: 1.2.1\r\n        - torchaudio:        0.11.0+cu113\r\n        - torchmetrics:      0.11.3\r\n        - torchvision:       0.12.0+cu113\r\n        - tornado:           6.1\r\n        - tqdm:              4.64.0\r\n        - traitlets:         5.2.1.post0\r\n        - traits:            6.3.2\r\n        - traitsui:          7.3.1\r\n        - transaction:       3.0.1\r\n        - transformers:      4.19.2\r\n        - trimesh:           3.12.5\r\n        - twine:             4.0.1\r\n        - typeguard:         2.13.3\r\n        - typer:             0.7.0\r\n        - typeshed-client:   2.2.0\r\n        - typing-extensions: 4.2.0\r\n        - urllib3:           1.26.9\r\n        - utm:               0.7.0\r\n        - velocyto:          0.17.17\r\n        - vine:              5.0.0\r\n        - vispy:             0.11.0\r\n        - vtk:               9.2.6\r\n        - waitress:          2.1.1\r\n        - wandb:             0.12.17\r\n        - wcwidth:           0.2.5\r\n        - webargs:           8.2.0\r\n        - webencodings:      0.5.1\r\n        - webob:             1.8.7\r\n        - websocket:         0.2.1\r\n        - websocket-client:  1.3.2\r\n        - websockets:        10.4\r\n        - webtest:           3.0.0\r\n        - werkzeug:          2.1.2\r\n        - wget:              3.2\r\n        - wheel:             0.37.1\r\n        - widgetsnbextension: 3.6.0\r\n        - wntr:              0.4.1\r\n        - wrapt:             1.14.1\r\n        - wsgiproxy2:        0.5.1\r\n        - wsme:              0.11.0\r\n        - xarray:            2022.3.0\r\n        - xarray-einstats:   0.2.2\r\n        - xlrd:              2.0.1\r\n        - xlsxwriter:        3.0.3\r\n        - xlwt:              1.3.0\r\n        - xmlrunner:         1.7.7\r\n        - xopen:             1.5.0\r\n        - xxhash:            3.0.0\r\n        - xyzservices:       2022.4.0\r\n        - yacs:              0.1.8\r\n        - yappi:             1.3.5\r\n        - yarl:              1.7.2\r\n        - yaspin:            2.1.0\r\n        - yte:               1.4.0\r\n        - z3c.pt:            3.3.1\r\n        - zc.lockfile:       2.0\r\n        - zconfig:           3.6.0\r\n        - zexceptions:       4.2\r\n        - zict:              2.2.0\r\n        - zipp:              3.8.0\r\n        - zodb:              5.7.0\r\n        - zodbpickle:        2.3\r\n        - zope:              5.5.1\r\n        - zope.annotation:   4.7.0\r\n        - zope.browser:      2.4\r\n        - zope.browsermenu:  4.4\r\n        - zope.browserpage:  4.4.0\r\n        - zope.browserresource: 4.4\r\n        - zope.cachedescriptors: 4.3.1\r\n        - zope.component:    5.0.1\r\n        - zope.configuration: 4.4.1\r\n        - zope.container:    4.5.0\r\n        - zope.contentprovider: 4.2.1\r\n        - zope.contenttype:  4.5.0\r\n        - zope.datetime:     4.3.0\r\n        - zope.deferredimport: 4.4\r\n        - zope.deprecation:  4.4.0\r\n        - zope.dottedname:   4.3\r\n        - zope.event:        4.5.0\r\n        - zope.exceptions:   4.5\r\n        - zope.filerepresentation: 5.0.0\r\n        - zope.globalrequest: 1.5\r\n        - zope.hookable:     5.1.0\r\n        - zope.i18n:         4.9.0\r\n        - zope.i18nmessageid: 5.0.1\r\n        - zope.interface:    5.4.0\r\n        - zope.lifecycleevent: 4.4\r\n        - zope.location:     4.2\r\n        - zope.pagetemplate: 4.6.0\r\n        - zope.processlifetime: 2.3.0\r\n        - zope.proxy:        4.5.0\r\n        - zope.ptresource:   4.3.0\r\n        - zope.publisher:    6.1.0\r\n        - zope.schema:       6.2.0\r\n        - zope.security:     5.3\r\n        - zope.sequencesort: 4.2\r\n        - zope.site:         4.5.0\r\n        - zope.size:         4.3\r\n        - zope.structuredtext: 4.4\r\n        - zope.tal:          4.5\r\n        - zope.tales:        5.1\r\n        - zope.testbrowser:  5.6.1\r\n        - zope.testing:      4.10\r\n        - zope.traversing:   4.4.1\r\n        - zope.viewlet:      4.3\r\n        - zstandard:         0.17.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         \r\n        - python:            3.10.4\r\n        - version:           #1 SMP Tue Nov 8 15:48:59 UTC 2022\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_\n\ncc @carmocca @mauvilsa",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16970/comments",
    "author": "leopold-franz",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-03-06T20:33:15Z",
        "body": "Hey, I think the problem is that these keys in the config.yaml are not allowed:\r\n\r\n```\r\nseed_everything_default: null\r\nlog_dir: /cluster/dir/to/log\r\n```\r\n\r\nThey don't match anything in the Trainer. \r\n\r\nPerhaps it should be \r\n```\r\nseed_everything: false\r\ntrainer:\r\n    default_root_dir:  \"/cluster/dir/to/log\"\r\n    ...\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-03-15T00:28:58Z",
        "body": "Hi\r\n\r\nI tried to help here, did you find what the problem was? Please let me know."
      },
      {
        "user": "stale[bot]",
        "created_at": "2023-04-14T06:23:47Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      },
      {
        "user": "leopold-franz",
        "created_at": "2023-04-14T09:10:22Z",
        "body": "Yes sorry I forgot to answer. I somehow messed up a lot of the key settings, so you were right. Thank you for your help"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-04-14T09:13:00Z",
        "body": "Thanks for confirming that it worked. Happy this was helpful."
      }
    ]
  },
  {
    "number": 16848,
    "title": "Can't use more than one EarlyStopping callback",
    "created_at": "2023-02-23T01:53:47Z",
    "closed_at": "2023-04-16T17:23:50Z",
    "labels": [
      "question",
      "won't fix",
      "callback: early stopping"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16848",
    "body": "### Bug description\n\nWhen I try to use more than one EarlyStopping callback, I get \r\n\r\n```\r\nRuntimeError: Found more than one stateful callback of type `EarlyStopping`. In the current configuration, this callback does not support being saved alongside other instances of the same type. Please consult the documentation of `EarlyStopping` regarding valid settings for the callback state to be checkpointable. HINT: The `callback.state_key` must be unique among all callbacks in the Trainer.\r\n```\n\n### How to reproduce the bug\n\n```python\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.callbacks import EarlyStopping\r\n\r\n\r\ncallbacks = [\r\n    EarlyStopping(\"loss/val\", min_delta=0.0, patience=3, verbose=True),\r\n    EarlyStopping(\"loss/val\", min_delta=-1.0, patience=1, verbose=True),\r\n]\n```\n\n\n### Error messages and logs\n\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[8], line 10\r\n      2 from pytorch_lightning.callbacks import EarlyStopping\r\n      5 callbacks = [\r\n      6     EarlyStopping(\"loss/val\", min_delta=0.0, patience=3, verbose=True),\r\n      7     EarlyStopping(\"loss/val\", min_delta=-1.0, patience=1, verbose=True),\r\n      8 ]\r\n---> 10 pl.Trainer(callbacks=callbacks)\r\n\r\nFile ~/.pyenv/versions/3.10.4/envs/artist-transformers/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py:348, in _defaults_from_env_vars.<locals>.insert_env_defaults(self, *args, **kwargs)\r\n    345 kwargs = dict(list(env_variables.items()) + list(kwargs.items()))\r\n    347 # all args were already moved to kwargs\r\n--> 348 return fn(self, **kwargs)\r\n\r\nFile ~/.pyenv/versions/3.10.4/envs/artist-transformers/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:466, in Trainer.__init__(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, inference_mode)\r\n    462 self._ckpt_path: Optional[str] = None\r\n    464 # init callbacks\r\n    465 # Declare attributes to be set in _callback_connector on_trainer_init\r\n--> 466 self._callback_connector.on_trainer_init(\r\n    467     callbacks,\r\n    468     enable_checkpointing,\r\n    469     enable_progress_bar,\r\n    470     default_root_dir,\r\n    471     enable_model_summary,\r\n    472     max_time,\r\n    473     accumulate_grad_batches,\r\n    474 )\r\n    476 # init data flags\r\n    477 self.check_val_every_n_epoch: Optional[int]\r\n\r\nFile ~/.pyenv/versions/3.10.4/envs/artist-transformers/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:86, in CallbackConnector.on_trainer_init(self, callbacks, enable_checkpointing, enable_progress_bar, default_root_dir, enable_model_summary, max_time, accumulate_grad_batches)\r\n     83     self._configure_fault_tolerance_callbacks()\r\n     85 self.trainer.callbacks.extend(_configure_external_callbacks())\r\n---> 86 _validate_callbacks_list(self.trainer.callbacks)\r\n     88 # push all model checkpoint callbacks to the end\r\n     89 # it is important that these are the last callbacks to run\r\n     90 self.trainer.callbacks = self._reorder_callbacks(self.trainer.callbacks)\r\n\r\nFile ~/.pyenv/versions/3.10.4/envs/artist-transformers/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:302, in _validate_callbacks_list(callbacks)\r\n    300 for callback in stateful_callbacks:\r\n    301     if callback.state_key in seen_callbacks:\r\n--> 302         raise RuntimeError(\r\n    303             f\"Found more than one stateful callback of type `{type(callback).__name__}`. In the current\"\r\n    304             \" configuration, this callback does not support being saved alongside other instances of the same type.\"\r\n    305             f\" Please consult the documentation of `{type(callback).__name__}` regarding valid settings for\"\r\n    306             \" the callback state to be checkpointable.\"\r\n    307             \" HINT: The `callback.state_key` must be unique among all callbacks in the Trainer.\"\r\n    308         )\r\n    309     seen_callbacks.add(callback.state_key)\r\n\r\nRuntimeError: Found more than one stateful callback of type `EarlyStopping`. In the current configuration, this callback does not support being saved alongside other instances of the same type. Please consult the documentation of `EarlyStopping` regarding valid settings for the callback state to be checkpointable. HINT: The `callback.state_key` must be unique among all callbacks in the Trainer.\r\n\r\n```\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow): Trainer\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 1.9.1\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0): 1.13.1\r\n#- Python version (e.g., 3.9): 3.10\r\n#- OS (e.g., Linux): OSX\r\n#- CUDA/cuDNN version: NA\r\n#- GPU models and configuration: NA\r\n#- How you installed Lightning(`conda`, `pip`, source): pip\r\n#- Running environment of LightningApp (e.g. local, cloud): local\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @carmocca @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16848/comments",
    "author": "lendle",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-02-24T10:04:36Z",
        "body": "Hey @lendle I think this is expected. The state of the callback will be indexed by a key as a function of the monitor and mode value. So you can have different Earlystopping configurations like so:\r\n\r\n\r\n```py\r\ncallbacks = [\r\n    EarlyStopping(\"a\", mode=\"min\", ...),\r\n    EarlyStopping(\"b\",mode=\"min\", ...),\r\n    EarlyStopping(\"c\",mode=\"max\", ...),\r\n]\r\n```\r\n\r\nBut multiple configurations where monitor and mode don't change is not possible. \r\n\r\nWe could add other parameters to the state key, for example patience, but that would break the use case where users want to change the value when continuing from a checkpoint. I don't think we should do that. \r\n\r\nCould you explain the use case of having two stopping callbacks with the same monitor but different patience? The stopping callback with the lower patience will always trigger first, so the other one is useless. \r\n\r\n```py\r\ncallbacks = [\r\n    EarlyStopping(\"loss/val\", min_delta=0.0, patience=3, verbose=True),\r\n    EarlyStopping(\"loss/val\", min_delta=-1.0, patience=1, verbose=True),\r\n]\r\n```\r\n"
      },
      {
        "user": "lendle",
        "created_at": "2023-02-27T22:17:55Z",
        "body": "> Could you explain the use case of having two stopping callbacks with the same monitor but different patience? The stopping callback with the lower patience will always trigger first, so the other one is useless.\r\n\r\nIn this example I have different values of min_delta. The idea is that the first one will trigger if validation loss doesn't improve for 3 epochs, and the second will trigger if it gets worse by 1 (arbitrary but reasonable value for my use case), which likely means the model is diverging. In that case, I want to stop immediately rather than waiting a couple more epochs."
      },
      {
        "user": "stale[bot]",
        "created_at": "2023-04-02T18:16:50Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      }
    ]
  },
  {
    "number": 16654,
    "title": "dependencies issue working with torch and pytorch_lightning. ",
    "created_at": "2023-02-06T17:16:02Z",
    "closed_at": "2023-02-06T21:09:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16654",
    "body": "### Bug description\n\nI have trouble getting all needed packages to be compatible to run ResNet transfer learning. I trying the import from one of the sample lightening notebooks anf got error. And then trying to install the recommended packages got following incompatibility issue.       \r\ncommand: \r\n`! pip install --quiet \"torchmetrics>=0.7, <0.12\" \"seaborn\" \"ipython[notebook]>=8.0.0, <8.9.0\" \"pytorch-lightning>=1.4, <1.9\" \"torchmetrics >=0.11.0\" \"setuptools==65.6.3\" \"pandas\" \"torchvision\" \"torch>=1.8.1, <1.14.0\"\r\n`\r\nIssue: \r\n```\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nthinc 8.0.1 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.10.4 which is incompatible.\r\nspacy 3.0.1 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.10.4 which is incompatible.\r\nlightning 1.9.0.dev0 requires lightning-utilities<1.0,>=0.4.2, but you have lightning-utilities 0.3.0 which is incompatible.\r\nconda-repo-cli 1.0.27 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\r\nconda-repo-cli 1.0.27 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\r\n```\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16654/comments",
    "author": "nkay28",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-02-06T20:40:16Z",
        "body": "@nkay28 You have torchmetrics specified twice, once with torchmetrics>=0.7, <0.12 and once with torchmetrics >=0.11.0. Also, I suggest to remove setuptools==65.6.3 which could limit the pip dep resolver. "
      },
      {
        "user": "nkay28",
        "created_at": "2023-02-06T21:09:20Z",
        "body": "Thank you very much. Resolved. "
      }
    ]
  },
  {
    "number": 16362,
    "title": "resume training falid",
    "created_at": "2023-01-14T04:14:10Z",
    "closed_at": "2023-08-28T17:33:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16362",
    "body": "### Bug description\r\n\r\nresume traning failed, when every_n_train_steps is equal to val_check_interval \r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\ncheckpoint_callback = ModelCheckpoint(\r\n        ***\r\n        every_n_train_steps=config['val_check_interval'],\r\n    )\r\n    trainer = pl.Trainer(\r\n        ***\r\n        val_check_interval=config['val_check_interval'],\r\n\r\n    )\r\n```\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\n```\r\n    def validation_epoch_end(self, outputs):\r\n        # pdb.set_trace()\r\n        imgs = np.vstack(outputs) \r\n\r\n  File \"/data_local/trainer.py\", line 1342, in _call_lightning_module_hook\r\n    output = fn(*args, **kwargs)\r\n  File \"/data_local//train.py\", line 90, in validation_epoch_end\r\n    imgs = np.vstack(outputs)\r\n\r\noutputs is empty\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 1.10):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16362/comments",
    "author": "ZHAIXINGZHAIYUE",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2023-04-14T13:23:47Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-08-28T17:33:30Z",
        "body": "Closing due to lack of provided information. \r\n\r\nIf `validation_epoch_end` doesn't receive outputs, it is probably because none were returned in the `validation_step`. "
      }
    ]
  },
  {
    "number": 16102,
    "title": "Torch sees GPU but does not use it",
    "created_at": "2022-12-17T17:36:44Z",
    "closed_at": "2022-12-18T20:54:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16102",
    "body": "### Bug description\n\nWhen I use ``torch.cuda.device_count()``\r\nit returns 1, which is correct\r\nBut then when using Lightning, it shows this in terminal\r\n``LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n``\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 1.10):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16102/comments",
    "author": "ThatGuyCalledJesse",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-12-18T20:47:01Z",
        "body": "@ThatGuyCalledJesse That's correct. If you have one GPU, then Lightning can only use one and that's device with index 0. \r\nIf you had multiple GPUs, it would show:\r\n\r\n`LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0, 1]`\r\n\r\netc.\r\n"
      },
      {
        "user": "ThatGuyCalledJesse",
        "created_at": "2022-12-18T20:54:22Z",
        "body": "I am so sorry, it's a very beginner mistake"
      },
      {
        "user": "yotamcons",
        "created_at": "2023-11-20T11:09:29Z",
        "body": "I made the same mistake, thank you for asking.\r\nWhat bothered me was the \"local rank\" which i couldn't figure out. This calls for better logging, especially in the uncomfortable new filed of gpu computations."
      },
      {
        "user": "ejkitchen",
        "created_at": "2024-06-20T19:09:28Z",
        "body": "Please keep this thread up because I made the exact same assumption as the original poster. I know CUDA devices start at 0 but I was still thrown off by this message."
      }
    ]
  },
  {
    "number": 15973,
    "title": "access to the last training epoch that triggered early stopping",
    "created_at": "2022-12-09T01:07:08Z",
    "closed_at": "2022-12-14T02:50:04Z",
    "labels": [
      "question",
      "callback: early stopping"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/15973",
    "body": "### Description & Motivation\r\n\r\nI wish there was a method to access when the last training epoch number was. The max_training_epoch differs from the last training epoch resulting from the early stopping.\n\ncc @borda @carmocca @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/15973/comments",
    "author": "jaeho3690",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-12-13T04:38:37Z",
        "body": "Hi @jaeho3690 \r\nIs this what you are looking for?\r\n\r\n```py\r\nprint(trainer.early_stopping_callback.stopped_epoch)\r\n\r\n```"
      },
      {
        "user": "jaeho3690",
        "created_at": "2022-12-14T02:49:50Z",
        "body": "yes! I was looking for this feature  :)  thank you!"
      }
    ]
  },
  {
    "number": 15722,
    "title": "\"TypeError: cannot pickle 'dict_keys' object\" when GPU DDP training.",
    "created_at": "2022-11-18T10:26:01Z",
    "closed_at": "2022-12-25T10:08:21Z",
    "labels": [
      "question",
      "strategy: ddp"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/15722",
    "body": "### Bug description\n\n `TypeError: cannot pickle 'dict_keys' object` when GPU DDP training.\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n\r\nGlobal seed set to 0\r\n/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\r\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\r\n/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:446: LightningDeprecationWarning: Setting `Trainer(gpus=8)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=8)` instead.\r\n  rank_zero_deprecation(\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nTraceback (most recent call last):\r\n  File \"exps/bevfusion/bevfusion_L_transf_256x256_24e_2key.py\", line 666, in <module>\r\n    run_cli()\r\n  File \"exps/bevfusion/bevfusion_L_transf_256x256_24e_2key.py\", line 645, in run_cli\r\n    main(args)\r\n  File \"exps/bevfusion/bevfusion_L_transf_256x256_24e_2key.py\", line 606, in main\r\n    trainer.fit(model)\r\n  File \"/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 582, in fit\r\n    call._call_and_handle_interrupt(\r\n  File \"/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py\", line 36, in _call_and_handle_interrupt\r\n    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n  File \"/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 113, in launch\r\n    mp.start_processes(\r\n  File \"/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 189, in start_processes\r\n    process.start()\r\n  File \"/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/multiprocessing/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/multiprocessing/popen_fork.py\", line 19, in __init__    self._launch(process_obj)\r\n  File \"/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nTypeError: cannot pickle 'dict_keys' object\r\n\r\n```\r\n\n\n### Environment\n\n```\r\n\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 1.8.2\r\n#- Lightning App Version (e.g., 0.5.2): \r\n#- PyTorch Version (e.g., 1.10): 1.12\r\n#- Python version (e.g., 3.9): 3.8\r\n#- OS (e.g., Linux): Ubuntu 20.04.4 LTS\r\n#- CUDA/cuDNN version: 11.6\r\n#- GPU models and configuration: 8 GPU DDP\r\n#- How you installed Lightning(`conda`, `pip`, source): pip\r\n#- Running environment of LightningApp (e.g. local, cloud): local\r\n\r\n```\r\n\n\n### More info\n\n_No response_\n\ncc @justusschock @awaelchli @akihironitta",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/15722/comments",
    "author": "xiazhongyv",
    "comments": [
      {
        "user": "xiazhongyv",
        "created_at": "2022-11-18T10:33:14Z",
        "body": "In fact, with pytorch-lightning 1.6.0 I ran into this problem when saving the model by ModelCheckpoint callback. I upgraded to version 1.8.2, and now I get this error when training starts.\r\n```\r\ndef main(args: Namespace) -> None:\r\n    if args.seed is not None:\r\n        pl.seed_everything(args.seed)\r\n\r\n    model = BEVFusionLightningModel(**vars(args))\r\n    trainer = pl.Trainer.from_argparse_args(args)\r\n    if args.evaluate:\r\n        trainer.test(model, ckpt_path=args.ckpt_path)\r\n    else:\r\n        trainer.fit(model)\r\n\r\n\r\ndef run_cli():\r\n    parent_parser = ArgumentParser(add_help=False)\r\n    parent_parser = pl.Trainer.add_argparse_args(parent_parser)\r\n    parent_parser.add_argument('-e',\r\n                               '--evaluate',\r\n                               dest='evaluate',\r\n                               action='store_true',\r\n                               help='evaluate model on validation set')\r\n    parent_parser.add_argument('-b', '--batch_size_per_device', type=int)\r\n    parent_parser.add_argument('--seed',\r\n                               type=int,\r\n                               default=0,\r\n                               help='seed for initializing training.')\r\n    parent_parser.add_argument('--ckpt_path', type=str)\r\n    parser = BEVFusionLightningModel.add_model_specific_args(parent_parser)\r\n    parser.set_defaults(\r\n        profiler='simple',\r\n        deterministic=False,\r\n        benchmark=False,\r\n        max_epochs=10,\r\n        accelerator='cuda',\r\n        num_sanity_val_steps=0,\r\n        gradient_clip_val=5,\r\n        enable_checkpointing=True,\r\n        precision=32,\r\n        default_root_dir='./outputs/'+os.path.basename(__file__)[:-3])\r\n    args = parser.parse_args()\r\n    main(args)\r\n```"
      },
      {
        "user": "xiazhongyv",
        "created_at": "2022-11-18T15:15:31Z",
        "body": "#- PyTorch Lightning Version: 1.6.0\r\n#- PyTorch Version (e.g., 1.10): 1.12\r\n#- Python version (e.g., 3.9): 3.8\r\n#- OS (e.g., Linux): Ubuntu 20.04.4 LTS\r\n#- CUDA/cuDNN version: 11.6\r\n#- GPU models and configuration: 8 GPU DDP\r\n#- How you installed Lightning(`conda`, `pip`, source): pip\r\n#- Running environment of LightningApp (e.g. local, cloud): local\r\n\r\nWhen I set enable_checkpointing=True, but do not set callbacks, it works well.\r\nWhen I set enable_checkpointing=True and set callbacks=[ModelCheckpoint()], error occurs."
      },
      {
        "user": "awaelchli",
        "created_at": "2022-11-19T16:00:37Z",
        "body": "When using ddp_spawn, all objects in your code/model need to be pickleable. \r\n\r\nYou can set a breakpoint here:\r\n\r\n```\r\n  File \"/home/xiazhongyu/anaconda3/envs/bev/lib/python3.8/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n```\r\n\r\nand inspect the object to find out. \r\nIf it is not possible to make your code picklable, use a different strategy, for example `strategy=\"ddp\"`."
      },
      {
        "user": "myc634",
        "created_at": "2023-09-10T23:22:20Z",
        "body": "Hi, I had met the same problem in mmdetedtion3d; how did you solve that?"
      }
    ]
  },
  {
    "number": 15278,
    "title": "2-3x slowdown if using PTL on cluster instead of local ",
    "created_at": "2022-10-24T16:44:42Z",
    "closed_at": "2023-04-16T23:23:58Z",
    "labels": [
      "question",
      "won't fix",
      "performance",
      "pl"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/15278",
    "body": "### Bug description\r\n\r\nWe have two ways of running our PTL workflows: on Vertex AI notebooks, and through cluster jobs submitted by Argo (via Hera Workflows) onto GKE. \r\n\r\nThe problem is that jobs submitted to Argo are 2-3x slower than jobs directly ran on notebooks. This problem does not exist with vanilla Pytorch, which has relative parity between local/cluster runs, _only_ PTL. \r\n\r\nIt is difficult to provide a fully reproducible example to an external audience due to it requiring a cluster to replicate, but I have replicated this problem using the BoringModel, which is listed below. \r\n\r\nWhen ran on a Vertex notebook, SimpleProfiler will say the runtime is 24.814 (s), CPU usage\r\n\r\nWhen ran on the cluster, SimpleProfiler will say the runtime is 65.65 (s), CPU usage\r\n\r\nThis problem exists whether there is GPU usage or not. I've added GPU versions below just in case that's relevant. \r\n\r\nI've gone through the usual tests to ensure that PTL is performing optimally, by varying the number of workers, making sure workers are persistent, ensuring adequate memory/GPU usage, and so on. But while speed may increase with these changes, there still is a consistently large difference between cluster runs and local runs. \r\n\r\nI've also done a little work with AdvancedProfile, and can't find any specific operation that is the bottleneck, everything just seems to be broadly *slower*. Again, this could be blamed on the cluster if not for the fact that this problem does not occur with normal Pytorch, only PTL. I've attached a logger example below for the BoringModel with CPU usage. \r\n\r\n\r\n\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\ndef test_x():\r\n    import os\r\n\r\n    import torch\r\n    from torch import nn\r\n    from torch.nn import functional as F\r\n    from torch.utils.data import DataLoader, random_split, Dataset\r\n    import pytorch_lightning as pl\r\n\r\n    # some other options for random data\r\n    from pl_bolts.datasets import RandomDataset, DummyDataset, RandomDictDataset\r\n    from pytorch_lightning.callbacks import LearningRateMonitor\r\n\r\n    import torch\r\n    from pytorch_lightning import LightningModule\r\n    from torch.utils.data import Dataset\r\n       \r\n        \r\n    class Net(nn.Module):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.fc1 = nn.Linear(32, 64)\r\n            self.fc2 = nn.Linear(64, 64)\r\n            self.fc3 = nn.Linear(64, 64)\r\n            self.fc4 = nn.Linear(64, 2)\r\n\r\n        def forward(self, x):\r\n            x = F.relu(self.fc1(x))\r\n            x = F.relu(self.fc2(x))\r\n            x = F.relu(self.fc3(x))\r\n            x = F.log_softmax(self.fc4(x), dim=1)\r\n\r\n            return x\r\n        \r\n    class BoringModel(LightningModule):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.model = Net()\r\n\r\n        def forward(self, x):\r\n            return self.model(x)\r\n\r\n        def loss(self, batch, prediction):\r\n            # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n            return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n        def training_step(self, batch, batch_idx):\r\n            output = self.forward(batch)\r\n            loss = self.loss(batch, output)\r\n            return {\"loss\": loss}\r\n\r\n        def training_step_end(self, training_step_outputs):\r\n            return training_step_outputs\r\n\r\n        def training_epoch_end(self, outputs) -> None:\r\n            torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n        def validation_step(self, batch, batch_idx):\r\n            output = self.forward(batch)\r\n            loss = self.loss(batch, output)\r\n            return {\"x\": loss}\r\n\r\n        def validation_epoch_end(self, outputs) -> None:\r\n            torch.stack([x['x'] for x in outputs]).mean()\r\n\r\n        def test_step(self, batch, batch_idx):\r\n            output = self.forward(batch)\r\n            loss = self.loss(batch, output)\r\n            self.log('fake_test_acc', loss)\r\n            return {\"y\": loss}\r\n\r\n        def test_epoch_end(self, outputs) -> None:\r\n            torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n        def configure_optimizers(self):\r\n            optimizer = torch.optim.SGD(self.model.parameters(), lr=0.1)\r\n            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n            return [optimizer], [lr_scheduler]\r\n\r\n    # init model\r\n    num_samples = 100000\r\n    train = RandomDataset(32, num_samples)\r\n    train = DataLoader(train, batch_size=32)\r\n    val = RandomDataset(32, num_samples)\r\n    val = DataLoader(val, batch_size=32)\r\n    test = RandomDataset(32, num_samples)\r\n    test = DataLoader(test, batch_size=32)\r\n\r\n    model = BoringModel()\r\n    lr_monitor = LearningRateMonitor(logging_interval='step')\r\n\r\n    # Initialize a trainer\r\n    trainer = pl.Trainer(\r\n        accelerator='cpu',\r\n        devices = 1,\r\n        max_epochs=2,\r\n        profiler=\"simple\",\r\n    )\r\n\r\n    # Train the model ⚡\r\n    trainer.fit(model, train)\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\nHere's one profile for a cluster run function: \r\n```\r\nProfile stats for: [Strategy]SingleDeviceStrategy.training_step\r\n         725044 function calls (675043 primitive calls) in 19.043 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n     6250    0.093    0.000   18.981    0.003 strategy.py:351(training_step)\r\n     6250    0.042    0.000   18.104    0.003 script:49(training_step)\r\n     6250    0.039    0.000   16.864    0.003 script:42(forward)\r\n31250/6250    0.233    0.000   16.808    0.003 module.py:1124(_call_impl)\r\n     6250    0.327    0.000   16.613    0.003 script:29(forward)\r\n    25000    0.099    0.000   14.786    0.001 linear.py:113(forward)\r\n    25000   14.648    0.001   14.648    0.001 {built-in method torch._C._nn.linear}\r\n     6250    0.051    0.000    1.199    0.000 script:45(loss)\r\n     6250    0.147    0.000    0.964    0.000 functional.py:3252(mse_loss)\r\n    18750    0.058    0.000    0.939    0.000 functional.py:1446(relu)\r\n    18750    0.868    0.000    0.868    0.000 {built-in method torch.relu}\r\n     6250    0.560    0.000    0.560    0.000 {built-in method torch._C._nn.mse_loss}\r\n6251/6250    0.025    0.000    0.424    0.000 {built-in method builtins.isinstance}\r\n     6250    0.066    0.000    0.399    0.000 typing_extensions.py:470(__instancecheck__)\r\n31250/18750    0.063    0.000    0.347    0.000 {built-in method builtins.next}\r\n     6251    0.031    0.000    0.297    0.000 typing_extensions.py:413(_is_callable_members_only)\r\n     6250    0.027    0.000    0.295    0.000 functional.py:1902(log_softmax)\r\n    12500    0.105    0.000    0.276    0.000 precision_plugin.py:222(train_step_context)\r\n     6252    0.154    0.000    0.241    0.000 typing_extensions.py:394(_get_protocol_attrs)\r\n     6250    0.226    0.000    0.226    0.000 {method 'log_softmax' of 'torch._C._TensorBase' objects}\r\n     6250    0.074    0.000    0.206    0.000 functional.py:44(broadcast_tensors)\r\n18750/12500    0.064    0.000    0.199    0.000 contextlib.py:116(__exit__)\r\n12500/6250    0.049    0.000    0.187    0.000 contextlib.py:107(__enter__)\r\n     6250    0.184    0.000    0.184    0.000 {built-in method torch.ones_like}\r\n    31250    0.157    0.000    0.157    0.000 {built-in method torch._C._get_tracing_state}\r\n    81250    0.126    0.000    0.126    0.000 module.py:1194(__getattr__)\r\n     6250    0.096    0.000    0.096    0.000 {built-in method torch.broadcast_tensors}\r\n    37504    0.084    0.000    0.084    0.000 {built-in method builtins.getattr}\r\n    12500    0.012    0.000    0.075    0.000 contextlib.py:237(helper)\r\n    12500    0.058    0.000    0.062    0.000 contextlib.py:81(__init__)\r\n    25000    0.055    0.000    0.055    0.000 {built-in method torch._C._has_torch_function_unary}\r\n     6250    0.022    0.000    0.051    0.000 profiler.py:55(profile)\r\n    12500    0.035    0.000    0.035    0.000 {method 'size' of 'torch._C._TensorBase' objects}\r\n     6250    0.021    0.000    0.033    0.000 _VF.py:25(__getattr__)\r\n     6250    0.023    0.000    0.030    0.000 advanced.py:66(stop)\r\n     6251    0.009    0.000    0.025    0.000 {built-in method builtins.all}\r\n     6250    0.009    0.000    0.023    0.000 {built-in method builtins.issubclass}\r\n    75024    0.022    0.000    0.022    0.000 {method 'startswith' of 'str' objects}\r\n    12502    0.011    0.000    0.016    0.000 typing_extensions.py:414(<genexpr>)\r\n     6250    0.014    0.000    0.014    0.000 _reduction.py:7(get_enum)\r\n     6250    0.004    0.000    0.014    0.000 abc.py:141(__subclasscheck__)\r\n     6250    0.010    0.000    0.010    0.000 {built-in method _abc._abc_subclasscheck}\r\n     6252    0.007    0.000    0.007    0.000 {method 'keys' of 'mappingproxy' objects}\r\n    12500    0.006    0.000    0.006    0.000 precision_plugin.py:217(forward_context)\r\n     6252    0.005    0.000    0.005    0.000 {method 'add' of 'set' objects}\r\n    12500    0.005    0.000    0.005    0.000 strategy.py:323(model)\r\n     6250    0.005    0.000    0.005    0.000 {method 'get' of 'dict' objects}\r\n     6250    0.005    0.000    0.005    0.000 strategy.py:97(precision_plugin)\r\n     6250    0.003    0.000    0.003    0.000 {built-in method torch._C._has_torch_function}\r\n     6250    0.003    0.000    0.003    0.000 {built-in method torch._C._has_torch_function_variadic}\r\n     6250    0.001    0.000    0.001    0.000 {method 'disable' of '_lsprof.Profiler' objects}\r\n     6252    0.001    0.000    0.001    0.000 {method 'keys' of 'dict' objects}\r\n     6251    0.001    0.000    0.001    0.000 {built-in method builtins.callable}\r\n        1    0.000    0.000    0.000    0.000 typing_extensions.py:566(_proto_hook)\r\n        1    0.000    0.000    0.000    0.000 {method 'get' of 'mappingproxy' objects}\r\n\r\n```\r\nAnd for local runs: \r\n```\r\nProfile stats for: [Strategy]SingleDeviceStrategy.training_step\r\n         1025044 function calls (975043 primitive calls) in 3.004 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n     6250    0.039    0.000    2.977    0.000 strategy.py:351(training_step)\r\n     6250    0.016    0.000    2.684    0.000 3931846600.py:47(training_step)\r\n     6250    0.018    0.000    2.249    0.000 3931846600.py:40(forward)\r\n31250/6250    0.192    0.000    2.224    0.000 module.py:866(_call_impl)\r\n     6250    0.104    0.000    2.163    0.000 3931846600.py:27(forward)\r\n    25000    0.075    0.000    1.394    0.000 linear.py:93(forward)\r\n    25000    0.032    0.000    1.290    0.000 functional.py:1737(linear)\r\n    25000    1.250    0.000    1.250    0.000 {built-in method torch._C._nn.linear}\r\n     6250    0.019    0.000    0.420    0.000 3931846600.py:43(loss)\r\n     6250    0.032    0.000    0.337    0.000 functional.py:2901(mse_loss)\r\n    18750    0.019    0.000    0.304    0.000 functional.py:1195(relu)\r\n    18750    0.281    0.000    0.281    0.000 {built-in method relu}\r\n     6250    0.200    0.000    0.200    0.000 {built-in method torch._C._nn.mse_loss}\r\n6251/6250    0.011    0.000    0.152    0.000 {built-in method builtins.isinstance}\r\n     6250    0.012    0.000    0.142    0.000 typing_extensions.py:470(__instancecheck__)\r\n     6250    0.008    0.000    0.133    0.000 functional.py:1651(log_softmax)\r\n     6250    0.123    0.000    0.123    0.000 {method 'log_softmax' of 'torch._C._TensorBase' objects}\r\n     6251    0.009    0.000    0.116    0.000 typing_extensions.py:413(_is_callable_members_only)\r\n     6252    0.063    0.000    0.092    0.000 typing_extensions.py:394(_get_protocol_attrs)\r\n31250/18750    0.017    0.000    0.088    0.000 {built-in method builtins.next}\r\n     6250    0.029    0.000    0.086    0.000 functional.py:46(broadcast_tensors)\r\n18750/12500    0.030    0.000    0.076    0.000 contextlib.py:116(__exit__)\r\n     6250    0.064    0.000    0.064    0.000 {built-in method ones_like}\r\n    81250    0.063    0.000    0.063    0.000 module.py:934(__getattr__)\r\n    12500    0.018    0.000    0.057    0.000 precision_plugin.py:222(train_step_context)\r\n     6250    0.045    0.000    0.045    0.000 {built-in method broadcast_tensors}\r\n    31250    0.036    0.000    0.036    0.000 {built-in method torch._C._get_tracing_state}\r\n12500/6250    0.010    0.000    0.034    0.000 contextlib.py:107(__enter__)\r\n    12500    0.007    0.000    0.023    0.000 contextlib.py:237(helper)\r\n     6250    0.007    0.000    0.021    0.000 profiler.py:55(profile)\r\n    37504    0.019    0.000    0.019    0.000 {built-in method builtins.getattr}\r\n   125000    0.017    0.000    0.017    0.000 {built-in method builtins.len}\r\n   125000    0.016    0.000    0.016    0.000 {method 'values' of 'collections.OrderedDict' objects}\r\n    12500    0.014    0.000    0.016    0.000 contextlib.py:81(__init__)\r\n     6251    0.006    0.000    0.015    0.000 {built-in method builtins.all}\r\n     6250    0.009    0.000    0.014    0.000 advanced.py:66(stop)\r\n    75024    0.013    0.000    0.013    0.000 {method 'startswith' of 'str' objects}\r\n    12500    0.013    0.000    0.013    0.000 {method 'size' of 'torch._C._TensorBase' objects}\r\n     6250    0.005    0.000    0.012    0.000 {built-in method builtins.issubclass}\r\n     6250    0.007    0.000    0.010    0.000 _VF.py:25(__getattr__)\r\n    12502    0.007    0.000    0.009    0.000 typing_extensions.py:414(<genexpr>)\r\n    31250    0.009    0.000    0.009    0.000 {built-in method torch._C._has_torch_function_variadic}\r\n     6250    0.003    0.000    0.007    0.000 abc.py:141(__subclasscheck__)\r\n     6250    0.005    0.000    0.005    0.000 _reduction.py:7(get_enum)\r\n    25000    0.005    0.000    0.005    0.000 {built-in method torch._C._has_torch_function_unary}\r\n     6250    0.004    0.000    0.004    0.000 {built-in method _abc._abc_subclasscheck}\r\n     6252    0.003    0.000    0.003    0.000 {method 'keys' of 'mappingproxy' objects}\r\n     6250    0.003    0.000    0.003    0.000 {method 'get' of 'dict' objects}\r\n    12500    0.003    0.000    0.003    0.000 strategy.py:323(model)\r\n    12500    0.003    0.000    0.003    0.000 precision_plugin.py:217(forward_context)\r\n     6250    0.002    0.000    0.002    0.000 strategy.py:97(precision_plugin)\r\n     6250    0.002    0.000    0.002    0.000 {method 'disable' of '_lsprof.Profiler' objects}\r\n     6252    0.002    0.000    0.002    0.000 {method 'add' of 'set' objects}\r\n     6250    0.002    0.000    0.002    0.000 {built-in method torch._C._has_torch_function}\r\n     6252    0.001    0.000    0.001    0.000 {method 'keys' of 'dict' objects}\r\n     6251    0.001    0.000    0.001    0.000 {built-in method builtins.callable}\r\n        1    0.000    0.000    0.000    0.000 typing_extensions.py:566(_proto_hook)\r\n        1    0.000    0.000    0.000    0.000 {method 'get' of 'mappingproxy' objects}\r\n```\r\nThis is for otherwise identical code with identical PTL versions, identical Pytorch versions, and each with 50G of RAM + 24 vCPU's. \r\n\r\nIf needed, I could post the full AdvancedProfiler for each of the BoringModel runs, this is just an example of an operation set that is noticeably slower in the remote case. \r\n\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA A100-SXM4-40GB\r\n                - NVIDIA A100-SXM4-40GB\r\n        - available:         True\r\n        - version:           11.1\r\n* Lightning:\r\n        - lightning-bolts:   0.5.0\r\n        - pytorch-lightning: 1.7.7\r\n        - torch:             1.12.1\r\n        - torchmetrics:      0.10.0\r\n        - torchvision:       0.9.1+cu111\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         \r\n        - python:            3.7.12\r\n        - version:           #1 SMP Debian 4.19.260-1 (2022-09-29)\r\n\r\n### More info\r\n\r\n_No response_\n\ncc @borda @akihironitta",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/15278/comments",
    "author": "Abhishaike",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2022-12-05T06:49:44Z",
        "body": "@awaelchli @justusschock any thought on what could be the reason? :rabbit: "
      },
      {
        "user": "stale[bot]",
        "created_at": "2023-01-08T00:24:24Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      },
      {
        "user": "itay1542",
        "created_at": "2023-05-29T15:11:46Z",
        "body": "I am experiencing the same problem. I have looked at the GPU utilization graph when trained on an Argo Workflow pod vs the utilization when trained on a notebook server.\r\nThe graph from the pod looks like gpus are at 80-100% utilization and then suddenly drop to 0, and back up to 80% and so on, while the utilization on the notebook server is consistenly above 80%.\r\nDoes anyone knows the reason for this?"
      }
    ]
  },
  {
    "number": 15240,
    "title": "'method' object not subscriptable when loading a config file with 'strategy' argument supplied to Trainer",
    "created_at": "2022-10-21T14:58:18Z",
    "closed_at": "2022-10-28T14:48:12Z",
    "labels": [
      "question",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/15240",
    "body": "### Bug description\r\n\r\nI use Weights & Biases with PL all the time and it works great. \r\n\r\n```python\r\n    wandb_logger = WandbLogger(\r\n        project=\"Project Name\",\r\n        config=\"configs/config.yaml\",\r\n        log_model=True,\r\n        mode=\"offline\",\r\n    )\r\n    config = wandb_logger.experiment.config\r\n    wandb_logger.experiment.log_code(\".\")\r\n    model_name = wandb_logger.experiment.name\r\n\r\n    # Load dataset\r\n    dataset = MSDataModule(\r\n        config[\"dataset_path\"],\r\n        config[\"batch_size\"],\r\n    )\r\n```\r\nThe above works without any problems when the 'strategy' argument is not supplied to Trainer:\r\n\r\n```python\r\n    trainer = pl.Trainer(\r\n        accelerator=\"gpu\",\r\n        devices=[1, 2, 3],\r\n        precision=16,\r\n        max_epochs=config[\"epochs\"],\r\n        callbacks=[checkpoint_callback, lr_monitor, model_summary],\r\n        gradient_clip_val=1.0,\r\n        logger=wandb_logger,\r\n    )\r\n```\r\nBut if I manually specificy the strategy (and it doesn't matter what it is) as follows:\r\n\r\n```python\r\n    trainer = pl.Trainer(\r\n        accelerator=\"gpu\",\r\n        devices=[1, 2, 3],\r\n        precision=16,\r\n        strategy='ddp'\r\n        max_epochs=config[\"epochs\"],\r\n        callbacks=[checkpoint_callback, lr_monitor, model_summary],\r\n        gradient_clip_val=1.0,\r\n        logger=wandb_logger,\r\n    )\r\n```\r\nI get an error saying:\r\n\r\nException has occurred: TypeError\r\n**_'method' object is not subscriptable\r\n  File \"/home/......py\", line 201, in <module>\r\n    config[\"dataset_path\"],_**\r\n\r\n```\r\n\r\nI thought for there was another variable in the namespace when I specify 'strategy' also called config, but even if I change the W&B config variable name to something else I get the same error.\r\n\r\n### Environment\r\n\r\n```\r\n\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow): Trainer\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 1.7.7\r\n#- PyTorch Version (e.g., 1.10): 1.12\r\n#- Python version (e.g., 3.9): 3.9\r\n#- OS (e.g., Linux): Linux\r\n#- CUDA/cuDNN version: 11.6\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/15240/comments",
    "author": "TKassis",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2022-10-21T18:09:02Z",
        "body": "maybe you are missing a comma here:\r\n```py\r\n    trainer = pl.Trainer(\r\n        accelerator=\"gpu\",\r\n        devices=[1, 2, 3],\r\n        precision=16,\r\n        strategy='ddp' # <- hereeeeeee\r\n        max_epochs=config[\"epochs\"],\r\n        callbacks=[checkpoint_callback, lr_monitor, model_summary],\r\n        gradient_clip_val=1.0,\r\n        logger=wandb_logger,\r\n    )\r\n```"
      },
      {
        "user": "TKassis",
        "created_at": "2022-10-21T20:08:48Z",
        "body": "Good find! But no that was just a typo here because I typed it in manually. My actual code has the comma :-)"
      },
      {
        "user": "rohitgr7",
        "created_at": "2022-10-21T20:19:47Z",
        "body": "can you share the complete stack trace?"
      },
      {
        "user": "TKassis",
        "created_at": "2022-10-21T20:35:25Z",
        "body": "Using 16bit native Automatic Mixed Precision (AMP)\r\nTrainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nGlobal seed set to 42\r\nTraceback (most recent call last):\r\n  File \"/home/timothy/Documents/CelestialAI/celestialai/train_foundation.py\", line 201, in <module>\r\n    config[\"dataset_path\"],\r\nTypeError: 'method' object is not subscriptable\r\nGlobal seed set to 42\r\nTraceback (most recent call last):\r\n  File \"/home/timothy/Documents/CelestialAI/celestialai/train_foundation.py\", line 201, in <module>\r\n    config[\"dataset_path\"],\r\nTypeError: 'method' object is not subscriptable\r\nGlobal seed set to 42\r\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3"
      },
      {
        "user": "awaelchli",
        "created_at": "2022-10-22T11:03:18Z",
        "body": "@TKassis The error message shows that this happens in `/home/timothy/Documents/CelestialAI/celestialai/train_foundation.py`. This is your code, right? Please take a look at line 201 in that file. The variable `config` seems to be a method, but the author of the code is accessing it as a dictionary. "
      },
      {
        "user": "TKassis",
        "created_at": "2022-10-28T14:48:11Z",
        "body": "config is actually a dictionary, but for some reason when I used the 'strategy' argument suddenly it becomes interpreted as a method. I couldn't fix the issue, I think it might be WandB related. I ended up just not using the returned dictionary from WandB. It might be something related with using WandB with multiple GPUs (although no issue with the default 'ddp' strategy unless explicitly mentioned). Anyway, I'm closing this for now as I found a workaround."
      }
    ]
  },
  {
    "number": 15142,
    "title": "Validation step super slow with ddp",
    "created_at": "2022-10-14T14:39:17Z",
    "closed_at": "2023-02-14T04:10:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/15142",
    "body": "### Bug description\r\n\r\nI'm using Pytorch Lightning with SageMaker on a ml.p3.8xlarge. I experienced an important increase in validation time so I re-executed an old experiment and still I continue to see a large increase in val time, something seems wrong:\r\n\r\nI use:\r\n```\r\n\"trainer.strategy\": \"ddp\"\r\n\"trainer.sync_batchnorm\": \"true\"\r\n```\r\n\r\nBefore this was a train log at Epoch 0, 75% (1.24it/s):\r\nEpoch 0:  75%|███████▍  | 430/577 [05:47<01:58,  1.24it/s, loss=0.443, v_num=0]\r\n\r\nand this a val log at 20% (3.39it/s):\r\nValidation DataLoader 0:  20%|██        | 13/65 [00:15<00:15,  3.39it/s]\r\n\r\nNow I get this (I launched the same job for test):\r\n\r\nTrain log at Epoch 0, 75% (1.28it/s): <- more or less the same\r\nEpoch 0:  75%|███████▍  | 430/577 [05:35<01:54,  1.28it/s, loss=0.452, v_num=0]\r\n\r\nval log at 20% (68.74s/it): <- 20x slower\r\nValidation DataLoader 0:  20%\\|██        \\| 13/65 [14:53<59:34, 68.74s/it]\r\n\r\n\r\n\r\n\r\nAs you can see now val is extremely slow, both compared to train and to what was the speed before.\r\n\r\nWhat I'm missing?\r\n\r\nI can do other experiments but it would be super useful to have some guidance on why this can happen.\r\n\r\nFIrst exp was with pytorch_lightning-1.6.4\r\nSecond (new) exp is with pytorch_lightning-1.7.7\r\n\r\n\r\nBest,\r\nM",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/15142/comments",
    "author": "mnslarcher",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-10-17T03:52:48Z",
        "body": "@mnslarcher Is the only thing that changed from first to second experiment the version Lightning? There could be many reasons why validation is slow. I would start disabling everything that's not strictly needed, like logging in val step for example. "
      },
      {
        "user": "mnslarcher",
        "created_at": "2022-10-20T07:56:19Z",
        "body": "> @mnslarcher Is the only thing that changed from first to second experiment the version Lightning? There could be many reasons why validation is slow. I would start disabling everything that's not strictly needed, like logging in val step for example.\r\n\r\nHi @awaelchli , I experimented some more, this time copying code that had been used in a successful job (SageMaker zippers the code when launching a job with the train) I also fixed the libraries that include pytorch-lightning and still the problem persists.\r\n\r\nAt this point my best guess is that something has changed in the Pytorch container I'm using (763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-training:1.9.1-gpu-py38-cu111-ubuntu20.04) that produces this strange behavior whereby the train runs fine but the validation takes forever.\r\n\r\nI don't know if it is related, but another problem we have experienced in the past is that trainer.test() blocks the job.\r\n\r\nIf I find the problem I will update this topic in case it might be useful to others."
      },
      {
        "user": "awaelchli",
        "created_at": "2022-10-22T10:19:43Z",
        "body": "@mnslarcher I thought about it but I don't have any good guesses on what might be the problem. I ran a quick test with our debugging model but couldn't find anything unusual. I couldn't use your image though, because it requires credentials. Would you be able to share a reproducible code. \r\n\r\nOne reason validation runs slow(er) could be because of collective calls (all_gather, reduce, etc.) that happen in validation_step(). Are you using torchmetrics, or self.log? "
      },
      {
        "user": "mnslarcher",
        "created_at": "2022-11-22T14:01:49Z",
        "body": "@awaelchli : we tried a lot of things but nothing, and now... it is OK again without changing a line of code. I think it is related to what AWS is running in it's containers or I have no idea what it could be, we also tried removing all logs and metrics from val but the problem remained (now it is gone again using the old code)"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-02-14T04:10:14Z",
        "body": "We can reopen again if this comes up again and needs investigation."
      }
    ]
  },
  {
    "number": 15066,
    "title": "Tensors not on the same device when specifying GPU id and using `deepspeed_stage_3(_offload)`",
    "created_at": "2022-10-10T21:40:10Z",
    "closed_at": "2023-07-04T18:58:41Z",
    "labels": [
      "question",
      "strategy: deepspeed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/15066",
    "body": "### Bug description\r\n\r\nWhen using deepspeed_stage_3 or deepspeed_stage_3_offload, if the GPU id is not specified as ```devices=[0]``` but the other id like ```device=[1]```, the error would be raised.\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        limit_test_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        accelerator='gpu',\r\n        devices=[1],\r\n        enable_model_summary=False,\r\n        precision=16,\r\n        strategy='deepspeed_stage_3',\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 71, in <module>\r\n    run()\r\n  File \"run.py\", line 66, in run\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 696, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 648, in _call_and_handle_interrupt\r\n    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 93, in launch\r\n    return function(*args, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 735, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1166, in _run\r\n    results = self._run_stage()\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1252, in _run_stage\r\n    return self._run_train()\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1283, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 271, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 203, in advance\r\n    batch_output = self.batch_loop.run(kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 87, in advance\r\n    outputs = self.optimizer_loop.run(optimizers, kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 201, in advance\r\n    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 248, in _run_optimization\r\n    self._optimizer_step(optimizer, opt_idx, kwargs.get(\"batch_idx\", 0), closure)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 358, in _optimizer_step\r\n    self.trainer._call_lightning_module_hook(\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1550, in _call_lightning_module_hook\r\n    output = fn(*args, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/core/module.py\", line 1705, in optimizer_step\r\n    optimizer.step(closure=optimizer_closure)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 168, in step\r\n    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py\", line 289, in optimizer_step\r\n    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 216, in optimizer_step\r\n    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/deepspeed.py\", line 111, in optimizer_step\r\n    closure_result = closure()\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 146, in __call__\r\n    self._result = self.closure(*args, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 132, in closure\r\n    step_output = self._step_fn()\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 407, in _training_step\r\n    training_step_output = self.trainer._call_strategy_hook(\"training_step\", *kwargs.values())\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1704, in _call_strategy_hook\r\n    output = fn(*args, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py\", line 352, in training_step\r\n    return self.model(*args, **kwargs)\r\n  File \"/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/.local/lib/python3.8/site-packages/deepspeed/runtime/engine.py\", line 1318, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1148, in _call_impl\r\n    result = forward_call(*input, **kwargs)\r\n  File \"/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 79, in forward\r\n    output = self.module.training_step(*inputs, **kwargs)\r\n  File \"run.py\", line 31, in training_step\r\n    loss = self(batch).sum()\r\n  File \"/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1148, in _call_impl\r\n    result = forward_call(*input, **kwargs)\r\n  File \"run.py\", line 28, in forward\r\n    return self.layer(x)\r\n  File \"/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1148, in _call_impl\r\n    result = forward_call(*input, **kwargs)\r\n  File \"/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\n  File \"/lib/python3.8/site-packages/torch/cuda/amp/autocast_mode.py\", line 110, in decorate_fwd\r\n    return fwd(*args, **kwargs)\r\n  File \"/home/.local/lib/python3.8/site-packages/deepspeed/runtime/zero/linear.py\", line 58, in forward\r\n    ret = torch.addmm(bias, input, weight.t())\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument mat1 in method wrapper_addmm)\r\n\r\n\r\n```\r\n\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA A100-SXM-80GB\r\n                - NVIDIA A100-SXM-80GB\r\n                - NVIDIA A100-SXM-80GB\r\n                - NVIDIA A100-SXM-80GB\r\n                - NVIDIA A100-SXM-80GB\r\n                - NVIDIA A100-SXM-80GB\r\n                - NVIDIA A100-SXM-80GB\r\n                - NVIDIA A100-SXM-80GB\r\n        - available:         True\r\n        - version:           11.3\r\n* Lightning:\r\n        - pytorch-lightning: 1.7.7\r\n        - torch:             1.12.1+cu113\r\n        - torchmetrics:      0.9.3\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.10\r\n        - version:           #1 SMP Fri May 8 10:59:10 UTC 2020\r\n\r\n```\r\n\r\n\r\n### More info\r\n\r\n_No response_\n\ncc @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/15066/comments",
    "author": "w3nhao",
    "comments": [
      {
        "user": "w3nhao",
        "created_at": "2022-10-10T23:40:57Z",
        "body": "Actually, it doesn't bother me when I specify the GPUs including cuda:0 like ```devices=[0, 1, 2, 3]``` or ```devices=4```. However, I want to start another training task and use the rest of the GPUs like ```devices=[4, 5, 6, 7]```."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-11-13T16:36:21Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      },
      {
        "user": "Borda",
        "created_at": "2022-12-05T05:37:02Z",
        "body": "@SeanNaren maybe could help here... :rabbit: "
      },
      {
        "user": "stale[bot]",
        "created_at": "2023-01-08T08:35:37Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      },
      {
        "user": "kelvins64",
        "created_at": "2023-06-08T11:30:24Z",
        "body": "The problem described here may be similar to an issue from about a year back: #9521 \r\n\r\nI encountered your described problem with lightning.Trainer's `deepspeed_stage_3_offload` strategy when using `CUDA_VISIBLE_DEVICES=0,2` and using `devices=[0,1]`. I similarly avoided the issue by passing in `devices=2`."
      }
    ]
  },
  {
    "number": 15057,
    "title": "tbptt doesn't work with validation",
    "created_at": "2022-10-10T15:08:15Z",
    "closed_at": "2023-01-16T15:39:57Z",
    "labels": [
      "question",
      "lightningmodule",
      "loops"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/15057",
    "body": "### Bug description\n\nThe `tbptt_split_batch` function doesn't seem to be called before `validation_step` and `hiddens` also doesn't seem to be passed to the `validation_step` method of the lightning module.\r\nHence validating a model with a validation_step doesn't seems possible even if training can be achived using tbptt for timeseries data that's to big to fit in on the GPU/ where the loss calculation is to big to fit on the GPU. \r\n\r\nMy current workarround is as follows:\r\n```Python\r\ndef validation_step(self, batch, batch_idx):\r\n        batch = self.tbptt_split_batch(batch, self.truncated_bptt_steps)\r\n        hiddens = None\r\n        losses = []\r\n\r\n        for chung in batch:\r\n            x = chung[0]  # x: [batch, seq, features]\r\n            y_hat, hiddens = self(x, hiddens)  # y_hat: [batch, seq, features]\r\n            loss = F.l1_loss(y_hat, x, reduction='sum')  # autoencoder model -> y_hat should be equal to x\r\n            losses.append(loss)\r\n\r\n        loss = sum([l/len(losses) for l in losses])\r\n        self.log(\"val_loss\", loss)\r\n        return {\"loss\": loss}\r\n```\r\n\r\nHowever it would be great to have the same behaviour as with the `training_step` (if  `truncated_bptt_steps` is defined the chuncks are passed to `validation_step` as well as the hiddens from the last chunk).\r\nI'm also not quite sure if my loss aggregation is the same as the one implemented for the training_step losses of the tbptt chunks (mean).\n\n### How to reproduce the bug\n\n```python\n1. Use a timeseries dataset where a single batch/sample doesn't fit in Mem or can't perform backward because the sequence length is too big.\r\n2. Define a model with `truncated_bptt_steps` set.\r\n3. Call `trainer.fit(train_dl, val_dl)`\r\n4. See Out of Memory exception because `tbptt_split_batch` isn't called on:\r\n  a. the `sanity_check`\r\n  b. the `validation_step`\r\n5. remove `validation_step` from the model and the model model runs without problems because `tbptt_split_batch` is called in the train_loop.\n```\n\n\n### Error messages and logs\n\n```\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nRunning in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n  | Name    | Type | Params\r\n---------------------------------\r\n0 | encoder | RNN  | 3.4 K \r\n1 | decoder | RNN  | 324   \r\n---------------------------------\r\n3.7 K     Trainable params\r\n0         Non-trainable params\r\n3.7 K     Total params\r\n0.015     Total estimated model params size (MB)\r\n/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\r\n  rank_zero_warn(\r\nEpoch 0:  50%|██████████████████████████████████████████████████████████                                                          | 1/2 [00:47<00:47, 47.35s/it, loss=863, split_idx=33, v_num=, train_step_loss (l1)=356.0Traceback (most recent call last):                                                                                                                                                                     | 0/1 [00:00<?, ?it/s]\r\n  File \"/home/username/cwd/scripts/./train.py\", line 94, in <module>\r\n    main(args)\r\n  File \"/home/username/cwd/scripts/./train.py\", line 71, in main\r\n    train(trainer_args=args, model=model, train_dl=train_dataloader, val_dl=validation_dataloader)\r\n  File \"/home/username/cwd/scripts/./train.py\", line 56, in train\r\n    trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 696, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 735, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1166, in _run\r\n    results = self._run_stage()\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1252, in _run_stage\r\n    return self._run_train()\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1283, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py\", line 271, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py\", line 201, in run\r\n    self.on_advance_end()\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 241, in on_advance_end\r\n    self._run_validation()\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 299, in _run_validation\r\n    self.val_loop.run()\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 155, in advance\r\n    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 143, in advance\r\n    output = self._evaluation_step(**kwargs)\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 240, in _evaluation_step\r\n    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1704, in _call_strategy_hook\r\n    output = fn(*args, **kwargs)\r\n  File \"/home/username/venv/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py\", line 370, in validation_step\r\n    return self.model.validation_step(*args, **kwargs)\r\n  File \"/home/username/cwd/scripts/utils/deeplearning/models/autoencoder.py\", line 53, in validation_step\r\n    y_hat, hiddens = self(x, hiddens)  # y_hat: [batch, seq, features]\r\n  File \"/home/username/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/username/cwd/scripts/utils/deeplearning/models/autoencoder.py\", line 27, in forward\r\n    _, enc_hiddens = self.encoder(x, enc_hiddens)  # hidden: [layer, batch, hidden]\r\n  File \"/home/username/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/username/venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py\", line 471, in forward\r\n    result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,\r\nRuntimeError: CUDA out of memory. Tried to allocate 58.22 GiB (GPU 0; 22.20 GiB total capacity; 6.67 GiB already allocated; 12.59 GiB free; 8.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\nEpoch 0:  50%|█████     | 1/2 [01:18<01:18, 78.38s/it, loss=863, split_idx=33, v_num=, train_step_loss (l1)=356.0]\r\n\r\n```\r\n\n\n### Environment\n\n```\r\n- Python: 3.9\r\n- PyTorch Lightning: 1.7.7\r\n- Pytorch: 1.12.1+cu116\r\n\r\nEnvironment: AWS EC2 g5.16xlarge\r\n- OS: Amazon Linux 2\r\n- CUDA: 11.6\r\n- GPU: NVIDIA A10G Tensor Core GPU\r\n\r\n```\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/15057/comments",
    "author": "DavidHoessle",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-10-11T08:10:04Z",
        "body": "@DavidHoessle There is no back-propagation/optimization happening in the validation step, hence there is no TBPTT. Your current \"workaround\" is fine but I'm wondering what it does for you vs. just running the full sequence through the model. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-11-13T16:35:20Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      }
    ]
  },
  {
    "number": 15028,
    "title": "Callback not invoked for the validation set with DDP",
    "created_at": "2022-10-07T08:09:22Z",
    "closed_at": "2022-12-16T11:31:23Z",
    "labels": [
      "question",
      "callback",
      "loops"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/15028",
    "body": "### Bug description\r\n\r\nI have a callback that is supposed to called during training and validation set. However, the validation part of it is never invoked. The callback is about rendering videos and logging them in wandb. The callback is supposed to happened every n epochs after the train/val step ends. The callback is never called since the videos which are supposed to create are never saved on the disk, so not wandb but pl bug. This happens  when using Gpus > 1 with ddp.\r\n\r\nThere are no error messages. The videos are not created and none of the print statements indicate that this part of the callback is ever accessed.\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nThe code of the callback:\r\n \r\n\r\n\r\nclass RenderCallback(Callback):\r\n    def __init__(self, bm_path: str = None,\r\n                 path: str = \"visuals\",\r\n                 logger_type: str = \"wandb\",\r\n                 save_last: bool = True,\r\n                 vid_format: str = \"mp4\",\r\n                 every_n_epochs: int = 20,\r\n                 num_workers: int = 0,\r\n                 nvids_to_save: int = 5,\r\n                 fps: float = 30.0,\r\n                 modelname = 'space') -> None:\r\n\r\n        if logger_type == \"wandb\":\r\n            self.log_to_logger = log_to_wandb\r\n        elif logger_type == \"tensorboard\":\r\n            self.log_to_logger = log_to_tensorboard\r\n        elif logger_type == \"none\":\r\n            self.log_to_logger = log_to_none\r\n        else:\r\n            raise NotImplementedError(\"This logger is unknown, please use tensorboard or wandb.\")\r\n\r\n        self.logger_type = logger_type\r\n        self.path = Path(path)\r\n        self.path.mkdir(exist_ok=True)\r\n\r\n        self.fps = fps\r\n        self.nvids = nvids_to_save\r\n        self.every_n_epochs = every_n_epochs\r\n        self.num_workers = num_workers\r\n        self.vid_format = vid_format\r\n        self.save_last = save_last\r\n        self.model = modelname\r\n\r\n        from hydra.utils import get_original_cwd\r\n        # self.labels_dict = read_json(f'{get_original_cwd()}/deps/inference/labels.json')\r\n        self.labels_train = read_json(f'{get_original_cwd()}/deps/inference/labels_train_spatial.json')\r\n        self.labels_val = read_json(f'{get_original_cwd()}/deps/inference/labels_val_spatial.json')\r\n\r\n        if bm_path is not None:\r\n            self.body_model_path = Path(bm_path) / 'smpl_models'\r\n\r\n    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule,\r\n                           **kwargs) -> None:\r\n        if trainer.is_global_zero:\r\n            return self.call_renderer(\"train\", trainer, pl_module)\r\n\r\n    def on_validation_epoch_end(self, trainer: Trainer,\r\n                                pl_module: LightningModule) -> None:\r\n        if trainer.is_global_zero:\r\n            return self.call_renderer(\"val\", trainer, pl_module)\r\n\r\n    def on_test_epoch_end(self, trainer: Trainer,\r\n                          pl_module: LightningModule) -> None:\r\n        return self.call_renderer(\"test\", trainer, pl_module)\r\n\r\n    def call_renderer(self, split: str, trainer: Trainer,\r\n                      pl_module: LightningModule) -> None:\r\n        if trainer.sanity_checking:\r\n            return\r\n\r\n        if self.nvids is None or self.nvids == 0:\r\n            return\r\n\r\n        # mid-epoch starting for finetuning\r\n        # if pl_module.store_examples[split] is None:\r\n        #     return\r\n        \r\n        logger.debug(f\"Render {split} samples and log to {self.logger_type}\")\r\n\r\n        # Don't log epoch 0\r\n        if trainer.current_epoch == 0 or trainer.current_epoch % self.every_n_epochs != 0:\r\n            # Log last one (return = don't log, if it is not the last one)\r\n            if trainer.current_epoch != (trainer.max_epochs - 1):\r\n                return\r\n            # Don't log last one if we don't want it\r\n            elif not self.save_last:\r\n                return\r\n        # Prepare the folder\r\n        folder = \"epoch_\" + str(trainer.current_epoch).zfill(3)\r\n        folder = self.path / folder\r\n        folder.mkdir(exist_ok=True)\r\n\r\n        # Extract the stored data\r\n        store_examples = pl_module.store_examples[split]\r\n        ref_joints_or_verts = store_examples['ref']\r\n        ref_motion_features = store_examples['ref_features']\r\n        keyids_to_render = store_examples['keyids']\r\n        ref_motion_features = ref_motion_features.features\r\n        ref_motion_features = ref_motion_features[:self.nvids]\r\n\r\n        # Render + log\r\n        # nvids = min(self.nvids, len(ref_joints_or_verts))\r\n\r\n        pl_module.eval()\r\n        if split == 'train': \r\n            render_list = [self.labels_train[key] for key in keyids_to_render]\r\n            # render_list = [lens_acts for key, lens_acts in self.labels_train.items() if key in keyids_to_render]\r\n\r\n            texts = [[t0,t1] for t0, t1, _ in render_list]\r\n            lens = [l for _, _, l in render_list]\r\n\r\n            jts_T = pl_module.forward_seq(texts, lens, return_type='joints')\r\n            jts_T = [mot.detach().cpu().numpy() for mot in jts_T]\r\n\r\n            jts_M = pl_module.forward_motion(ref_motion_features, lens, inference=True, return_type='joints')\r\n            jts_M = [mot.detach().cpu().numpy() for mot in jts_M]\r\n\r\n            texts = [f'{t0},{t1} | {keyids_to_render[i]}' for i, (t0, t1) in enumerate(texts)] \r\n\r\n        \r\n        elif split == 'val':\r\n            render_list = [self.labels_val[key] for key in keyids_to_render]\r\n\r\n            # render_list = [lens_acts for key, lens_acts in self.labels_val.items() if key in keyids_to_render]\r\n            texts = [[t0,t1] for t0, t1, _ in render_list]\r\n\r\n            lens = [l for _, _, l in render_list]\r\n\r\n            jts_T = pl_module.forward_seq(texts, lens, return_type='joints')\r\n            jts_T = [mot.detach().cpu().numpy() for mot in jts_T]\r\n\r\n            jts_M = pl_module.forward_motion(ref_motion_features, lens, inference=True, return_type='joints')\r\n            jts_M = [mot.detach().cpu().numpy() for mot in jts_M]\r\n\r\n            texts = [f'{t0},{t1} | {keyids_to_render[i]}' for i, (t0, t1) in enumerate(texts)] \r\n\r\n        # self.labels_val \r\n        # render_list_train = list(self.labels_train.values())[:self.nvids]\r\n        # render_list_val =  list(self.labels_val.values())[:self.nvids]\r\n        \r\n        # for set_name, keyids in zip(['train', 'val'], [render_list_train, render_list_val]):\r\n        #     for keyid in keyids:\r\n\r\n        import multiprocessing\r\n        list_of_logs = []\r\n        num_workers = min(self.num_workers, 3 * self.nvids)\r\n        with multiprocessing.Pool(num_workers) as pool:\r\n            iterable = ((joints[index], name, index, split,\r\n                         folder, self.fps, description, trainer.current_epoch)\r\n                        for joints, name in zip([jts_T, jts_M, ref_joints_or_verts],\r\n                                                ['text', 'motion', 'ref'])\r\n                        for index, description in zip(range(self.nvids), texts))\r\n            for output, fig_number, name, desc in pool.imap_unordered(render_and_save, iterable):\r\n                split_set = desc.split('_')[-1]\r\n                log_name = f\"visuals/{name}/{split_set}_{fig_number}\"\r\n                list_of_logs.append((output, log_name, desc))\r\n\r\n                train_logger = pl_module.logger.experiment\r\n\r\n                # self.log_to_logger(path=output, log_name=log_name, caption=desc,\r\n                #                    fps=self.fps, global_step=trainer.current_epoch,\r\n                #                    train_logger=train_logger, vid_format=self.vid_format)\r\n        import operator\r\n        list_of_logs.sort(key=operator.itemgetter(2))\r\n        for vid_path, panel_name, text_desc in list_of_logs: \r\n            log_name_start, branch_or_gt, _ = panel_name.split('/')\r\n            vid_id = vid_path.split('/')[-1].split('_')[-1].split('.')[0]\r\n            log_name = f'{log_name_start}_{split}/sample-{vid_id}/{branch_or_gt}'\r\n            self.log_to_logger(path=vid_path, log_name=log_name, caption=text_desc,\r\n                                fps=self.fps, global_step=trainer.current_epoch,\r\n                                train_logger=train_logger, vid_format=self.vid_format)\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\nThere are no error messages, the code is just never invoked.\r\n\r\n\r\n### Environment\r\n\r\n```\r\n\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 1.10):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n\r\n```\r\n\r\n\r\n### More info\r\n\r\n* Lightning:\r\n\t- pytorch-lightning: 1.7.6\r\n\t- torch:             1.12.1+cu113\r\n\t- torchaudio:        0.12.1+cu113\r\n\t- torchmetrics:      0.9.3\r\n\t- torchvision:       0.13.1+cu113\r\n* Packages:\r\n\t- absl-py:           1.0.0\r\n\t- aiohttp:           3.8.1\r\n\t- aiosignal:         1.2.0\r\n\t- antlr4-python3-runtime: 4.8\r\n\t- astroid:           2.12.10\r\n\t- asttokens:         2.0.5\r\n\t- async-timeout:     4.0.2\r\n\t- attrs:             21.4.0\r\n\t- autopep8:          1.7.0\r\n\t- backcall:          0.2.0\r\n\t- cachetools:        5.0.0\r\n\t- certifi:           2021.10.8\r\n\t- charset-normalizer: 2.0.12\r\n\t- chumpy:            0.70\r\n\t- click:             8.0.4\r\n\t- colorlog:          6.6.0\r\n\t- contourpy:         1.0.5\r\n\t- cycler:            0.11.0\r\n\t- decorator:         4.4.2\r\n\t- dill:              0.3.5.1\r\n\t- docker-pycreds:    0.4.0\r\n\t- einops:            0.4.1\r\n\t- et-xmlfile:        1.1.0\r\n\t- executing:         0.8.3\r\n\t- filelock:          3.6.0\r\n\t- fonttools:         4.37.3\r\n\t- freetype-py:       2.2.0\r\n\t- frozenlist:        1.3.0\r\n\t- fsspec:            2022.2.0\r\n\t- future:            0.18.2\r\n\t- gitdb:             4.0.9\r\n\t- gitpython:         3.1.27\r\n\t- google-auth:       2.6.2\r\n\t- google-auth-oauthlib: 0.4.6\r\n\t- grpcio:            1.44.0\r\n\t- huggingface-hub:   0.4.0\r\n\t- hydra-colorlog:    1.1.0\r\n\t- hydra-core:        1.1.1\r\n\t- idna:              3.3\r\n\t- imageio:           2.16.1\r\n\t- imageio-ffmpeg:    0.4.5\r\n\t- importlib-metadata: 4.11.3\r\n\t- ipdb:              0.13.9\r\n\t- ipython:           8.1.1\r\n\t- isort:             5.10.1\r\n\t- jedi:              0.18.1\r\n\t- joblib:            1.1.0\r\n\t- kiwisolver:        1.4.4\r\n\t- lazy-object-proxy: 1.7.1\r\n\t- loguru:            0.6.0\r\n\t- markdown:          3.3.6\r\n\t- matplotlib:        3.6.0\r\n\t- matplotlib-inline: 0.1.3\r\n\t- mccabe:            0.7.0\r\n\t- more-itertools:    8.14.0\r\n\t- moviepy:           1.0.3\r\n\t- multidict:         6.0.2\r\n\t- networkx:          2.7.1\r\n\t- numpy:             1.23.3\r\n\t- oauthlib:          3.2.0\r\n\t- omegaconf:         2.1.1\r\n\t- openai:            0.23.0\r\n\t- opencv-python:     4.5.5.64\r\n\t- openpyxl:          3.0.10\r\n\t- packaging:         21.3\r\n\t- pandas:            1.4.1\r\n\t- pandas-stubs:      1.4.4.220919\r\n\t- parso:             0.8.3\r\n\t- pathtools:         0.1.2\r\n\t- pexpect:           4.8.0\r\n\t- pickleshare:       0.7.5\r\n\t- pillow:            9.0.1\r\n\t- pip:               22.2.2\r\n\t- platformdirs:      2.5.2\r\n\t- proglog:           0.1.9\r\n\t- promise:           2.3\r\n\t- prompt-toolkit:    3.0.28\r\n\t- protobuf:          3.19.4\r\n\t- psutil:            5.9.0\r\n\t- ptyprocess:        0.7.0\r\n\t- pudb:              2022.1.1\r\n\t- pure-eval:         0.2.2\r\n\t- pyasn1:            0.4.8\r\n\t- pyasn1-modules:    0.2.8\r\n\t- pycodestyle:       2.9.1\r\n\t- pydeprecate:       0.3.1\r\n\t- pyglet:            1.5.23\r\n\t- pygments:          2.11.2\r\n\t- pylint:            2.15.3\r\n\t- pyopengl:          3.1.0\r\n\t- pyparsing:         3.0.7\r\n\t- pyrender:          0.1.45\r\n\t- python-dateutil:   2.8.2\r\n\t- pytorch-lightning: 1.7.6\r\n\t- pytz:              2021.3\r\n\t- pyyaml:            6.0\r\n\t- regex:             2022.3.15\r\n\t- requests:          2.27.1\r\n\t- requests-oauthlib: 1.3.1\r\n\t- rsa:               4.8\r\n\t- sacremoses:        0.0.49\r\n\t- scipy:             1.8.0\r\n\t- sentry-sdk:        1.5.8\r\n\t- setproctitle:      1.2.2\r\n\t- setuptools:        59.5.0\r\n\t- shortuuid:         1.0.8\r\n\t- six:               1.16.0\r\n\t- smmap:             5.0.0\r\n\t- smplx:             0.1.28\r\n\t- stack-data:        0.2.0\r\n\t- tensorboard:       2.10.0\r\n\t- tensorboard-data-server: 0.6.1\r\n\t- tensorboard-plugin-wit: 1.8.1\r\n\t- termcolor:         1.1.0\r\n\t- tokenizers:        0.11.6\r\n\t- toml:              0.10.2\r\n\t- tomli:             2.0.1\r\n\t- tomlkit:           0.11.4\r\n\t- torch:             1.12.1+cu113\r\n\t- torchaudio:        0.12.1+cu113\r\n\t- torchmetrics:      0.9.3\r\n\t- torchvision:       0.13.1+cu113\r\n\t- tqdm:              4.63.0\r\n\t- traitlets:         5.1.1\r\n\t- transformers:      4.17.0\r\n\t- trimesh:           3.10.5\r\n\t- types-pytz:        2022.2.1.0\r\n\t- typing-extensions: 4.1.1\r\n\t- urllib3:           1.26.9\r\n\t- urwid:             2.1.2\r\n\t- urwid-readline:    0.13\r\n\t- uuid:              1.30\r\n\t- wandb:             0.12.11\r\n\t- wcwidth:           0.2.5\r\n\t- werkzeug:          2.0.3\r\n\t- wheel:             0.37.1\r\n\t- wrapt:             1.14.1\r\n\t- yarl:              1.7.2\r\n\t- yaspin:            2.1.0\r\n\t- zipp:              3.7.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.9.14",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/15028/comments",
    "author": "atnikos",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-10-12T14:08:29Z",
        "body": "@athn-nik Is the validation loop running for you? Here are some reasons why validation may not run:\r\n\r\n- Trainer(limit_val_batches=0)\r\n- No val_dataloader() method implemented (either in LightningModule or LightningDataModule) or no val_dataloader passed to Trainer.fit()\r\n- No validation_step() method implemented in LightningModule\r\n- Validation interval too high in Trainer(val_check_interval=...)\r\n\r\nCan you check or share your Trainer settings, ideally the full script? \r\n"
      },
      {
        "user": "atnikos",
        "created_at": "2022-10-12T14:44:10Z",
        "body": "@awaelchli thanks for grabbing this.\r\nlimit_val_batches is always > 0.\r\nHere are the args:\r\n```\r\nauto_select_gpus: true\r\nstrategy: null # 'ddp' for multi gpu \r\nbenchmark: False\r\nmax_epochs: 1001\r\naccelerator: gpu\r\ndevices: 1\r\nlog_every_n_steps: 1\r\ndeterministic: False\r\ndetect_anomaly: False\r\nenable_progress_bar: True\r\ncheck_val_every_n_epoch: 25\r\nlimit_train_batches: 1.0\r\nlimit_val_batches: 1.0\r\nnum_sanity_val_steps: 2\r\n```\r\nI have a dataloader and all the other validation relevant metrics and losses are calculated and logged in wandb and stdout.\r\nTrainer code:\r\n```\r\nclass BaseModel(LightningModule):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.save_hyperparameters(logger=False)\r\n\r\n        # Save visuals, one validation step per validation epoch\r\n        self.store_examples = {\"train\": None,\r\n                               \"val\": None}\r\n        # Need to define:\r\n        # forward\r\n        # allsplit_step()\r\n        # metrics()\r\n        # losses()\r\n\r\n    def __post_init__(self):\r\n        trainable, nontrainable = 0, 0\r\n        for p in self.parameters():\r\n            if p.requires_grad:\r\n                trainable += np.prod(p.size())\r\n            else:\r\n                nontrainable += np.prod(p.size())\r\n        self.hparams.n_params_trainable = trainable\r\n        self.hparams.n_params_nontrainable = nontrainable\r\n\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self.allsplit_step(\"train\", batch, batch_idx)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        return self.allsplit_step(\"val\", batch, batch_idx)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        return self.allsplit_step(\"test\", batch, batch_idx)\r\n\r\n    def allsplit_epoch_end(self, split: str, outputs):\r\n        loss_tracker = self.tracker[split]\r\n        loss_dict = loss_tracker.compute()\r\n        loss_tracker.reset()\r\n\r\n        dico = {loss_tracker.loss2logname(loss, split): value.item()\r\n                for loss, value in loss_dict.items()}\r\n        # workaround for LR, assuming 1 optimizer, 1 scheduler, very weak\r\n        curr_lr = self.trainer.optimizers[0].param_groups[0]['lr']\r\n        dico.update({'Learning Rate': curr_lr})\r\n\r\n        dico.update({\"epoch\": float(self.trainer.current_epoch),\r\n                     \"step\": float(self.trainer.current_epoch)})\r\n        if split == \"val\":\r\n            metrics_dict = self.metrics.compute()\r\n\r\n            dico.update({f\"Metrics/{metric}\": value for metric, value in metrics_dict.items() if '_mean_' in metric})\r\n        self.log_dict(dico)\r\n\r\n\r\n    def training_epoch_end(self, outputs):\r\n        return self.allsplit_epoch_end(\"train\", outputs)\r\n    \r\n    \r\n    def validation_epoch_end(self, outputs):\r\n        return self.allsplit_epoch_end(\"val\", outputs)\r\n\r\n    def test_epoch_end(self, outputs):\r\n        return self.allsplit_epoch_end(\"test\", outputs)\r\n\r\n    def configure_optimizers(self):\r\n        optim_dict = {}\r\n        optimizer = instantiate(self.hparams.optim, params=self.parameters())\r\n        optim_dict['optimizer'] = optimizer\r\n\r\n        if self.hparams.lr_scheduler == 'reduceonplateau':\r\n            optim_dict['lr_scheduler'] = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold=1e-3)\r\n            optim_dict['monitor'] = 'losses/total/train'\r\n        elif self.hparams.lr_scheduler == 'steplr':\r\n            optim_dict['lr_scheduler'] = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100)\r\n\r\n        return optim_dict \r\n```\r\nThe relevant all_split_step which is called is model dependent and an example could be:\r\n```\r\ndef allsplit_step(self, split: str, batch, batch_idx):\r\n        # Prepare the generated motion features\r\n        length = batch[\"length\"]\r\n        input_motion_feats = batch[\"datastruct\"]\r\n        \r\n        total_loss, loss_dict = self.losses[split](...)\r\n        if split == 'val':\r\n            self.metrics(input_motion_feats.detach().joints,\r\n                         output_features_T.detach().joints,\r\n                         length)\r\n                \r\n        if batch_idx == 0:\r\n            nvids = self.hparams.nvids_to_save\r\n            if nvids is not None and nvids != 0:\r\n                del self.store_examples[split]\r\n                lengths = batch['length'][:nvids]\r\n                keyids = batch['keyid'][:nvids]\r\n                motion_features = batch['datastruct']\r\n                def prepare_pos(x):\r\n                    x = x.detach().joints[:nvids]\r\n                    x = x.cpu().numpy()\r\n                    return remove_padding(x, lengths)\r\n                def prepare_verts(x):\r\n                    x = x.detach().vertices[:nvids]\r\n                    x = x.cpu().numpy()\r\n                    return remove_padding(x, lengths)\r\n\r\n                \r\n                self.store_examples[split] = { \"text\": batch[\"text\"][:nvids] }\r\n                self.store_examples[split].update({\r\n                    'ref': prepare_pos(input_motion_feats),\r\n                    'ref_features': motion_features.detach(),\r\n                    'keyids': keyids\r\n                })\r\n\r\n        self.tracker[split].update(loss_dict)\r\n        return total_loss\r\n```\r\nWhere the stote_examples is what is used from the callback when the epoch ends."
      },
      {
        "user": "awaelchli",
        "created_at": "2022-10-12T15:18:19Z",
        "body": "Are you passing the dataloader correctly? Like\r\n\r\n`trainer.fit(model, train_dataloader, val_dataloader)`\r\n\r\n"
      },
      {
        "user": "atnikos",
        "created_at": "2022-10-12T23:42:52Z",
        "body": "Yes, I do this and the dataloader is implemented via LightningDataModule. The callback is not called as it does not enter the `validation_on_epoch_end` method of the callback. It does so when I use a single GPU though, and also the same method is normally accesed for the base model attached above."
      },
      {
        "user": "awaelchli",
        "created_at": "2022-10-22T10:38:00Z",
        "body": "@athn-nik I cannot reproduce this. Here is a runnable example based on your configuration (but I removed all code that was incomplete for me to use). \r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.optim import Adam\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer, Callback\r\n\r\n\r\nclass RenderCallback(Callback):\r\n\r\n\r\n    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule,\r\n                           **kwargs) -> None:\r\n        if trainer.is_global_zero:\r\n            print(\"on train epoch end in callback\")\r\n\r\n    def on_validation_epoch_end(self, trainer: Trainer,\r\n                                pl_module: LightningModule) -> None:\r\n        if trainer.is_global_zero:\r\n            # return self.call_renderer(\"val\", trainer, pl_module)\r\n            print(\"on val epoch end in callback\")\r\n\r\n    def on_test_epoch_end(self, trainer: Trainer,\r\n                          pl_module: LightningModule) -> None:\r\n        # return self.call_renderer(\"test\", trainer, pl_module)\r\n        print(\"on test epoch end in callback\")\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return Adam(self.parameters())\r\n\r\n    def training_epoch_end(self, outputs):\r\n        print(\"train epoch end on epoch\", self.current_epoch)\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        print(\"val epoch end on epoch\", self.current_epoch)\r\n\r\n    def test_epoch_end(self, outputs):\r\n        print(\"test epoch end\")\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        # auto_select_gpus=True\r\n        strategy=\"ddp\",\r\n        benchmark=False,\r\n        max_epochs=1001,\r\n        accelerator=\"cpu\",\r\n        devices=2,\r\n        log_every_n_steps=1,\r\n        deterministic=False,\r\n        detect_anomaly=False,\r\n        enable_progress_bar=False,\r\n        check_val_every_n_epoch=25,\r\n        limit_train_batches=1.0,\r\n        limit_val_batches=1.0,\r\n        num_sanity_val_steps=2,\r\n        callbacks=RenderCallback(),\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n\r\n\r\n```\r\n\r\nAs you can see from the logs I included, the validation epoch end hooks are called every 25 epochs as specified in the trainer:\r\n```\r\n....\r\ntrain epoch end on epoch 23\r\ntrain epoch end on epoch 23\r\non train epoch end in callback\r\nval epoch end on epoch 24  <--- here\r\nval epoch end on epoch 24  <--- here\r\non val epoch end in callback  <--- here\r\ntrain epoch end on epoch 24\r\ntrain epoch end on epoch 24\r\non train epoch end in callback\r\ntrain epoch end on epoch 25\r\ntrain epoch end on epoch 25\r\non train epoch end in callback\r\n....\r\n```\r\n\r\nPlease note that in your render method, you have early returns based on some conditions. Can you please check again that your observations are correct and that you were not just tricked by some missing logs? "
      },
      {
        "user": "atnikos",
        "created_at": "2022-12-16T11:31:23Z",
        "body": "Thanks a lot for your help seems likes there is a mismatch between my epoch indices checks."
      }
    ]
  },
  {
    "number": 14683,
    "title": "Meta package issues with imports",
    "created_at": "2022-09-13T10:59:57Z",
    "closed_at": "2022-10-13T19:34:37Z",
    "labels": [
      "help wanted",
      "question",
      "refactor",
      "priority: 1",
      "app (removed)"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/14683",
    "body": "## 🐛 Bug\r\n\r\n\r\n### To Reproduce\r\n\r\nIn a fresh environment (Python version does not matter):\r\n```\r\npip install --upgrade lightning\r\n```\r\n(version 0.6.0 is the latest at the time of writing)\r\n\r\n\r\n```py\r\nimport lightning as L\r\nL.app.frontend\r\n```\r\n\r\nError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'lightning.app' has no attribute 'frontend'\r\n```\r\n\r\nOn master, this gives no error!\r\n\r\n### Expected behavior\r\n\r\nNo error. These are the imports shown in our docs.\r\n\r\n### Additional context\r\n\r\nReported by @krishnakalyan3 \r\n\r\n### Environment\r\n\r\n\r\n<details>\r\n  <summary>Details</summary>\r\n    \r\n* CUDA:\r\n\t- GPU:               None\r\n\t- available:         False\r\n\t- version:           None\r\n* Lightning:\r\n\t- lightning:         2022.9.8\r\n\t- lightning-app:     0.6.0\r\n\t- lightning-cloud:   0.5.3\r\n\t- lightning-utilities: 0.3.0\r\n\t- pytorch-lightning: 1.7.5\r\n\t- torch:             1.12.1\r\n\t- torchmetrics:      0.9.3\r\n* Packages:\r\n\t- absl-py:           1.2.0\r\n\t- aiobotocore:       2.4.0\r\n\t- aiohttp:           3.8.1\r\n\t- aioitertools:      0.10.0\r\n\t- aiosignal:         1.2.0\r\n\t- anyio:             3.6.1\r\n\t- arrow:             1.2.3\r\n\t- asgiref:           3.5.2\r\n\t- async-timeout:     4.0.2\r\n\t- attrs:             22.1.0\r\n\t- botocore:          1.27.59\r\n\t- cachetools:        5.2.0\r\n\t- certifi:           2022.6.15\r\n\t- charset-normalizer: 2.1.1\r\n\t- click:             8.1.3\r\n\t- commonmark:        0.9.1\r\n\t- croniter:          1.3.7\r\n\t- deepdiff:          5.8.1\r\n\t- dnspython:         2.2.1\r\n\t- email-validator:   1.2.1\r\n\t- fastapi:           0.82.0\r\n\t- fire:              0.4.0\r\n\t- frozenlist:        1.3.1\r\n\t- fsspec:            2022.8.2\r\n\t- google-auth:       2.11.0\r\n\t- google-auth-oauthlib: 0.4.6\r\n\t- grpcio:            1.48.1\r\n\t- h11:               0.13.0\r\n\t- httptools:         0.4.0\r\n\t- idna:              3.3\r\n\t- importlib-metadata: 4.12.0\r\n\t- itsdangerous:      2.1.2\r\n\t- jinja2:            3.0.3\r\n\t- jmespath:          0.10.0\r\n\t- lightning:         2022.9.8\r\n\t- lightning-app:     0.6.0\r\n\t- lightning-cloud:   0.5.3\r\n\t- lightning-utilities: 0.3.0\r\n\t- markdown:          3.4.1\r\n\t- markupsafe:        2.1.1\r\n\t- multidict:         6.0.2\r\n\t- numpy:             1.23.2\r\n\t- oauthlib:          3.2.0\r\n\t- ordered-set:       4.1.0\r\n\t- orjson:            3.8.0\r\n\t- packaging:         21.3\r\n\t- pip:               22.1.2\r\n\t- protobuf:          3.19.4\r\n\t- py:                1.11.0\r\n\t- pyasn1:            0.4.8\r\n\t- pyasn1-modules:    0.2.8\r\n\t- pydantic:          1.10.2\r\n\t- pydeprecate:       0.3.2\r\n\t- pygments:          2.13.0\r\n\t- pyjwt:             2.4.0\r\n\t- pyparsing:         3.0.9\r\n\t- python-dateutil:   2.8.2\r\n\t- python-dotenv:     0.21.0\r\n\t- python-multipart:  0.0.5\r\n\t- pytorch-lightning: 1.7.5\r\n\t- pyyaml:            6.0\r\n\t- requests:          2.28.1\r\n\t- requests-oauthlib: 1.3.1\r\n\t- rich:              12.5.1\r\n\t- rsa:               4.9\r\n\t- s3fs:              2022.8.2\r\n\t- setuptools:        63.4.1\r\n\t- six:               1.16.0\r\n\t- sniffio:           1.3.0\r\n\t- starlette:         0.19.1\r\n\t- starsessions:      1.3.0\r\n\t- tensorboard:       2.10.0\r\n\t- tensorboard-data-server: 0.6.1\r\n\t- tensorboard-plugin-wit: 1.8.1\r\n\t- termcolor:         2.0.1\r\n\t- torch:             1.12.1\r\n\t- torchmetrics:      0.9.3\r\n\t- tqdm:              4.64.1\r\n\t- traitlets:         5.3.0\r\n\t- typing-extensions: 4.3.0\r\n\t- ujson:             5.4.0\r\n\t- urllib3:           1.26.12\r\n\t- uvicorn:           0.17.6\r\n\t- uvloop:            0.16.0\r\n\t- watchgod:          0.8.2\r\n\t- websocket-client:  1.4.1\r\n\t- websockets:        10.3\r\n\t- werkzeug:          2.2.2\r\n\t- wheel:             0.37.1\r\n\t- wrapt:             1.14.1\r\n\t- yarl:              1.8.1\r\n\t- zipp:              3.8.1\r\n* System:\r\n\t- OS:                Darwin\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         arm\r\n\t- python:            3.9.13\r\n\t- version:           Darwin Kernel Version 21.5.0: Tue Apr 26 21:08:37 PDT 2022; root:xnu-8020.121.3~4/RELEASE_ARM64_T6000\r\n\r\n</details>\r\n\r\n\n\ncc @tchaton @rohitgr7",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/14683/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2022-09-17T00:51:09Z",
        "body": "well it is about how the package is built - as a cross-import package, so this won't work\r\n```py\r\nimport lightning as L\r\nL.app.frontend\r\n```\r\nbut you can\r\n```py\r\nfrom lightning.app import frontend\r\n```\r\nor\r\n```py\r\nimport lightning as L\r\nL.app.frontend.Frontend\r\n```\r\nso if `lightning_app.__init__` would have `__all__ = [..., \"frontend\"]` your mentioned import also works..."
      },
      {
        "user": "awaelchli",
        "created_at": "2022-09-18T18:56:19Z",
        "body": "Dos that mean we should update the documentation with different imports? Or we need to add all submodules to `__all__`?"
      },
      {
        "user": "carmocca",
        "created_at": "2022-10-13T19:34:37Z",
        "body": "Since we are swapping meta-package for a copy-on-build, this shouldn't apply anymore. Closing"
      }
    ]
  },
  {
    "number": 14199,
    "title": "on_save_checkpoint runs multiple times on DDP",
    "created_at": "2022-08-14T13:42:46Z",
    "closed_at": "2022-08-14T18:23:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/14199",
    "body": "## 🐛 Bug\r\n\r\non_save_checkpoint handler of the LightningModule runs once per GPU when running on multiple GPUs with `strategy=dpp`. This can have unwanted side effects when saving additional checkpoint information.\r\n\r\nIs there a way to make this run only the master? \r\n\r\nIs this the expected behavior? I would think the former would be the more common use case.\r\n\r\n### To Reproduce\r\n\r\n```py\r\nfrom transformers import AutoModel\r\nfrom pytorch_lightning import LightningModule\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self,  model_name, save_path):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.model = AutoModel.from_pretained(self.hparams.model_name)\r\n\r\n    def forward(self, pixel_values):\r\n        outputs = self.model(pixel_values=pixel_values)\r\n        return outputs\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n    def on_save_checkpoint(self, checkpoint):\r\n        print(\"Saving model latest checkpoint in HF format..\")\r\n        self.model.save_pretrained(self.hparams.save_path)\r\n```\r\n\r\n- Lightning Component (e.g. Trainer, LightningModule):\r\n- PyTorch Lightning Version  1.6.4\r\n- OS (e.g., Linux): Linux\r\n- CUDA/cuDNN version: 11.3\r\n- GPU models and configuration:  8xT4\r\n- Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/14199/comments",
    "author": "jordanparker6",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-08-14T18:20:49Z",
        "body": "> Is this the expected behavior?\r\n\r\nYes, it definitely is. \r\n\r\n> This can have unwanted side effects when saving additional checkpoint information.\r\n\r\nWhat for example? Can you give an example?\r\n`on_save_checkpoint`'s primary use is to modify the dictionary checkpoint before it gets saved. Whether all ranks do something with this dictionary or not is up to the implementation in the strategy. Example: Sharded models may want to save a portion of the weights on each rank. \r\n\r\n> Is there a way to make this run only the master?\r\n\r\n```py\r\ndef on_save_checkpoint(self, checkpoint):\r\n        if self.global_rank == 0:\r\n            # run only on rank 0\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2022-08-14T18:23:13Z",
        "body": "Closing the issue as behavior is expected. In general, all hooks need to run on all ranks. \r\n\r\nIf you are looking to save only on one rank, this can be accounted for by:\r\n\r\n```py\r\ndef on_save_checkpoint(self, checkpoint):\r\n        if self.global_rank == 0:\r\n            print(\"Saving model latest checkpoint in HF format..\")\r\n            self.model.save_pretrained(self.hparams.save_path)\r\n```"
      },
      {
        "user": "jordanparker6",
        "created_at": "2022-08-14T22:53:45Z",
        "body": "@awaelchli  Thanks for the suggestion."
      }
    ]
  },
  {
    "number": 14156,
    "title": "Problem with self.log in DDP strategy",
    "created_at": "2022-08-10T23:33:20Z",
    "closed_at": "2022-08-11T00:22:00Z",
    "labels": [
      "question",
      "logging"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/14156",
    "body": "## 🐛 Bug\r\n\r\nProblem with using self. log in validation_step for EarlyStopping in DistributedDataParallel strategy. Error Below:\r\n\r\n```\r\nraise MisconfigurationException(\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: You called `self.log(val_loss, ...)` twice in `validation_step` with different arguments. This is not allowed\r\n```\r\n\r\n### To Reproduce\r\n\r\n```\r\ndef validation_step(self, batch, batch_idx):\r\n    ....\r\n    self.log(\r\n            \"val_loss\", \r\n            loss,\r\n            self.current_epoch,\r\n            on_epoch=True,\r\n            sync_dist=True,\r\n            )\r\n    ....\r\n```\r\n\r\n```\r\nlr_monitor = callbacks.LearningRateMonitor()\r\n\r\nearly_stopping = callbacks.EarlyStopping(\r\n        monitor=\"val_loss\", \r\n        patience=5, \r\n        mode=\"min\",\r\n        verbose=True,\r\n    )\r\n\r\ncheckpoint_callback = callbacks.ModelCheckpoint(\r\n        save_top_k=3,\r\n        monitor=\"val_loss\",\r\n        mode=\"min\",\r\n    )\r\n\r\n callbacks_list = [\r\n        lr_monitor,\r\n        early_stopping,\r\n        checkpoint_callback,\r\n        ]\r\n\r\nlogger = [loggers.tensorboard.TensorBoardLogger()]\r\n\r\ntrainer = pl.Trainer(\r\n            gpus=8,\r\n            num_nodes=2, \r\n            accelerator=\"ddp\", \r\n            plugins=DDPPlugin(find_unused_parameters=False), \r\n            callbacks=callbacks_list,\r\n            logger=logger,\r\n            accumulate_grad_batches=5\r\n            )\r\n```\r\n\r\n\r\n### Environment\r\n\r\n- Lightning Component: Trainer\r\n- PyTorch Lightning Version: 1.6.5\r\n- PyTorch Version: 1.12.0\r\n- Python version: 3.9.12\r\n- OS: Linux with Slurm\r\n- CUDA/cuDNN version: cu113\r\n\n\ncc @carmocca @edward-io @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/14156/comments",
    "author": "meshghi",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2022-08-11T00:22:00Z",
        "body": "You are not calling `self.log` correctly. You are passing an extra parameter that is getting assigned to the `prog_bar` argument:\r\n\r\n```diff\r\ndef validation_step(self, batch, batch_idx):\r\n    ....\r\n    self.log(\r\n            \"val_loss\", \r\n            loss,\r\n-           self.current_epoch,\r\n            on_epoch=True,\r\n            sync_dist=True,\r\n            )\r\n    ....\r\n```"
      }
    ]
  },
  {
    "number": 13775,
    "title": "Could trainer.validate don't enter _on_evaluation_model_train() ? ",
    "created_at": "2022-07-21T08:00:55Z",
    "closed_at": "2023-08-06T04:06:59Z",
    "labels": [
      "question",
      "won't fix",
      "pl"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/13775",
    "body": "## 🚀 Feature\r\n\r\na trainer.validate() func which don't enter self._on_evaluation_model_train().\r\n\r\n### Motivation\r\n\r\nHere I have a trainer and a corresponding traced model and I don't want the model.train(model) in the end of trainer.validate().\r\nHow could I disasble changing to train mode again in the EvaluationLoop? \r\n```\r\n# enable train mode again\r\nself._on_evaluation_model_train()\r\n```\r\nIs there any convenient way to achieve this goal?\r\n\r\nMy PL version : 1.6.4\n\ncc @akihironitta",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/13775/comments",
    "author": "rnwang04",
    "comments": [
      {
        "user": "akihironitta",
        "created_at": "2022-07-21T11:05:30Z",
        "body": "Hi @rnwang04, you can achieve it by manually calling `.train()` before running any code you want to run with the train mode."
      },
      {
        "user": "stale[bot]",
        "created_at": "2023-04-16T09:23:53Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      }
    ]
  },
  {
    "number": 13585,
    "title": "`ddp_spawn` training with multi-gpu stuck at the `sanity check`",
    "created_at": "2022-07-09T18:14:46Z",
    "closed_at": "2022-12-25T15:32:59Z",
    "labels": [
      "question",
      "waiting on author",
      "strategy: ddp",
      "pl"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/13585",
    "body": "I am using the \r\n\r\n```python\r\n    trainer = pl.Trainer(\r\n        max_epochs=num_epochs,\r\n        callbacks=callbacks,\r\n        gpus=2, \r\n        strategy='ddp_spawn',\r\n        **trainer_kwargs)\r\n```\r\n\r\n\r\nBut i am stuck at this `sanity check` for about one hour.\r\n```python\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\r\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\r\n----------------------------------------------------------------------------------------------------\r\ndistributed_backend=nccl\r\nAll distributed processes registered. Starting with 2 processes\r\n----------------------------------------------------------------------------------------------------\r\n\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n\r\n  | Name     | Type     | Params\r\n--------------------------------------\r\n0 | layer_1  | Linear   | 25.1 K\r\n1 | layer_2  | Linear   | 2.1 K \r\n2 | layer_3  | Linear   | 650   \r\n3 | accuracy | Accuracy | 0     \r\n--------------------------------------\r\n27.9 K    Trainable params\r\n0         Non-trainable params\r\n27.9 K    Total params\r\n0.112     Total estimated model params size (MB)\r\nSanity Checking: 0it [00:00, ?it/s]\r\n```\n\ncc @justusschock @kaushikb11 @awaelchli @akihironitta @rohitgr7",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/13585/comments",
    "author": "JiahaoYao",
    "comments": [
      {
        "user": "akihironitta",
        "created_at": "2022-07-10T10:07:30Z",
        "body": "Hello @JiahaoYao, is it feasible for you to share your `LightningModule`?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-08-14T03:11:23Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, PyTorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 13536,
    "title": "`LightningModules` without a `log` method?",
    "created_at": "2022-07-04T12:26:15Z",
    "closed_at": "2022-07-04T19:55:13Z",
    "labels": [
      "question",
      "lightningmodule"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/13536",
    "body": "[See additional comment at the end - to be closed]\r\n\r\nHi, I just installed torch-lightning (using `conda install -c conda-forge pytorch-lightning`) and my `LightningModules` do not seem to have a `self.log` method. Here is a MWE:\r\n```\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\nmodel = BoringModel()\r\n\r\nmodel.log\r\n\r\n```\r\nwhich leads to \r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n/users/data/coste/ReweighedEBM/main.ipynb Cell 13' in <cell line: 48>()\r\n     [43](vscode-notebook-cell://ssh-remote%2Bdigitale/users/data/coste/ReweighedEBM/main.ipynb#ch0000012vscode-remote?line=42)         return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n     [46](vscode-notebook-cell://ssh-remote%2Bdigitale/users/data/coste/ReweighedEBM/main.ipynb#ch0000012vscode-remote?line=45) model = BoringModel()\r\n---> [48](vscode-notebook-cell://ssh-remote%2Bdigitale/users/data/coste/ReweighedEBM/main.ipynb#ch0000012vscode-remote?line=47) model.log\r\n\r\nFile ~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1207, in Module.__getattr__(self, name)\r\n   1205     if name in modules:\r\n   1206         return modules[name]\r\n-> 1207 raise AttributeError(\"'{}' object has no attribute '{}'\".format(\r\n   1208     type(self).__name__, name))\r\n\r\nAttributeError: 'BoringModel' object has no attribute 'log'\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo bug. \r\n\r\n### Environment\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla P100-PCIE-16GB\r\n\t\t- Tesla P100-PCIE-16GB\r\n\t\t- Tesla P100-PCIE-16GB\r\n\t\t- Tesla P100-PCIE-16GB\r\n\t\t- Tesla P100-PCIE-16GB\r\n\t\t- Tesla P100-PCIE-16GB\r\n\t\t- Tesla P100-PCIE-16GB\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.22.3\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.12.0\r\n\t- pytorch-lightning: 0.8.5\r\n\t- tqdm:              4.64.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.9.12\r\n\t- version:           #193-Ubuntu SMP Tue Sep 17 17:42:52 UTC 2019\r\n\r\n### Additional context\r\n\r\nI just realized that my torch-lightning version seems to be 0.8.5 even though I just installed it. Why ?\r\n\r\n###  Edit/Solution\r\n\r\nIt turned out that I used `conda install -c conda-forge pytorch-lightning` to install PL (note the `-` instead of `_`). Installing things correctly leads to a recent version (1.6.4) and no bugs - can be closed.\n\ncc @carmocca @justusschock @awaelchli @borda @ananthsub @ninginthecloud @jjenniferdai @rohitgr7",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/13536/comments",
    "author": "SimonCoste",
    "comments": [
      {
        "user": "akihironitta",
        "created_at": "2022-07-04T19:55:13Z",
        "body": "Closing this issue as per your comment:\r\n\r\n> It turned out that I used `conda install -c conda-forge pytorch-lightning` to install PL (note the `-` instead of `_`). Installing things correctly leads to a recent version (1.6.4) and no bugs - can be closed.\r\n\r\n"
      }
    ]
  },
  {
    "number": 13360,
    "title": "Using Dataloader in pytorch_lightning when using DDP training?",
    "created_at": "2022-06-22T06:51:21Z",
    "closed_at": "2022-08-01T14:32:41Z",
    "labels": [
      "question",
      "won't fix",
      "distributed",
      "data handling"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/13360",
    "body": "My computer has 2 gpus and I have some problems:\r\n1. First, I used Pytorch\r\nI have 10 images, I created distributed dataloader (using sampler) follow Pytorch instruction, batchsize = 4, gpu=2.\r\n=> So with each gpu, length of batch0 is 4 and length of batch1 is 4. Sampler added 2 to batch1 to make batchsize = 4.\r\nI trained with ddp_spawn with pytorch code and everything is ok.\r\n\r\n2. Next, I used Pytorch Lightning\r\nI also use 10 images, I created dataloader (Normal Dataloader) follow Pytorch Instruction, batchsize =4, gpu=2\r\n=> so with each gpu, length of batch0 is 4 and length of batch1 is 1.\r\n\r\nNow, I want to use pytorch lightning but I want batchsize=4 same distributed sampler when working with pytorchlightning. How should I do?\r\n\r\nThanks\n\ncc @awaelchli @rohitgr7 @akihironitta @justusschock @ninginthecloud @otaj",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/13360/comments",
    "author": "NguyenDuyDuc1491995",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2022-06-22T10:22:49Z",
        "body": "> So with each gpu, length of batch0 is 4 and length of batch1 is 4. Sampler added 2 to batch1 to make batchsize = 4.\r\n\r\nthat makes total batches to be processed = 16, which seems incorrect.\r\nPL uses distributed sampler internally so it should be `batch0=4` and `batch1=1` on each GPU."
      },
      {
        "user": "NguyenDuyDuc1491995",
        "created_at": "2022-06-22T16:50:56Z",
        "body": "The problem is when I train semantic segmentation DeepLabV3 by pytorchlightning\r\nBatch0 =4 is ok, but batch1=1 is error. Because the batch is 1 so there is a problem with batchnorm.\r\n\r\nBut I used distributed dataloader with sampler in pytorch and I saw it will create batch0=4 and batch1=4 ( it will take 3 more images) so I wonder if I can create a distributed dataloader as same as above in pytorch lightning?"
      },
      {
        "user": "awaelchli",
        "created_at": "2022-06-22T17:21:49Z",
        "body": "The distributed sampler has nothing to do with batching. It only ensures that each GPU gets the same amount of samples and since your dataset of 10 images is divisible by 2, that's never a problem. \r\n\r\nTo avoid uneven batch sizes, just set `drop_last=True` in the dataloader and then you are guaranteed to get the batch size 4 for each batch. This is the same in PyTorch and PL, there should be no difference. "
      },
      {
        "user": "NguyenDuyDuc1491995",
        "created_at": "2022-06-24T07:05:37Z",
        "body": "ok, thanks"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-07-31T20:25:28Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, PyTorch Lightning Team!\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2022-08-01T14:32:41Z",
        "body": "I hope the question was answered and the issues could be solved. Let us know if there is anything else causing trouble."
      }
    ]
  },
  {
    "number": 13358,
    "title": "Disable Progress bars for Train/Validation/Predict separately",
    "created_at": "2022-06-21T23:09:03Z",
    "closed_at": "2022-06-22T16:21:06Z",
    "labels": [
      "question",
      "trainer: argument"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/13358",
    "body": "## 🚀 Feature\r\n\r\nCurrently we can turn off progress bars by setting `enable_progress_bar=False` but this is an all or nothing solution. \r\n\r\n### Motivation\r\n\r\nI am using PL trainer to make predictions in a small customer facing prototype, the progress bars make the output look very different to the previous solution we are proposing a drop in solution for. We can disable all the progress bars but it's useful to have them for monitoring training even if we do not want them when calling predict.\r\n\r\n### Pitch\r\n\r\nAdditional kwargs to disable progress bars for `Trainer.predict` calls\r\n\n\ncc @justusschock @kaushikb11 @awaelchli @borda @rohitgr7",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/13358/comments",
    "author": "CompRhys",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2022-06-22T00:23:16Z",
        "body": "You could create a new `Trainer` instance for prediction with the progress bar disabled."
      },
      {
        "user": "justusschock",
        "created_at": "2022-06-22T08:02:58Z",
        "body": "@carmocca for such cases should we create a function like `Trainer.new()` that basically creates an identical copy of the trainer and takes in some kwargs to overwrite only specific values? \r\n\r\nThis function would basically gather all the initial kwargs and update them with the given kwargs"
      },
      {
        "user": "rohitgr7",
        "created_at": "2022-06-22T10:14:41Z",
        "body": "> This function would basically gather all the initial kwargs and update them with the given kwargs\r\n\r\njust wondering how it will work in case the kwargs states are updated during `trainer.some_call`. For eg. logger, callbacks, etc."
      },
      {
        "user": "justusschock",
        "created_at": "2022-06-22T12:07:38Z",
        "body": "@rohitgr7 there are two possible options for this:\r\neither we say that this is expected (so we don't care about this) as for example when you call .test on the new trainer, you'd want the checkpointing callback to already have the correct checkpoints for best model etc. from the preceeding training\r\n\r\nThe second thing would be to reset the original state (could be done for example by dumping an initial checkpoint, which we then load again)"
      },
      {
        "user": "carmocca",
        "created_at": "2022-06-22T16:21:06Z",
        "body": "@justusschock We discussed adding such function in the past. I think it makes sense.\r\n\r\nDo you mind writing a proposal in a separate issue about it? Closing this as I don't want to sidetrack the original question."
      }
    ]
  },
  {
    "number": 13269,
    "title": "Save the best epoch in `ModelCheckpoint`",
    "created_at": "2022-06-10T22:47:47Z",
    "closed_at": "2022-06-14T22:40:26Z",
    "labels": [
      "question",
      "callback: model checkpoint"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/13269",
    "body": "## 🚀 Feature\r\n\r\nAdd `best_model_epoch` (`best_model_step`) to the `ModelCheckpoint` callback.  \r\n\r\n### Motivation\r\n\r\nAfter training, it would be good to know during which epoch/step a model reached its min/max in the `monitor` metric. \r\nRight now, it can be figured out by from the `best_model_path` by using a regex, which is rather inconvenient.\r\n\r\n```python\r\nckpt = ModelCheckpoint(...)\r\nbest_epoch = int(re.search(r'(?<=epoch\\=)(\\d+)(?=.ckpt)', ckpt.best_model_path).group(0))\r\n```\r\n\r\n### Pitch\r\n\r\nAdd two additional attributes `best_model_epoch` and `best_model_step`  which are updated in `_update_best_and_save` alongside the existing `best_xxx` attributes.\r\n\r\n### Alternatives\r\n\r\n### Additional context\r\n\r\n\n\ncc @carmocca @awaelchli @ninginthecloud @jjenniferdai @rohitgr7",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/13269/comments",
    "author": "GPla",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2022-06-14T16:03:29Z",
        "body": "Our suggestion here is that you load the checkpoint and extract the epoch value\r\n\r\n```python\r\nckpt = torch.load(\"my_model_checkpoint.ckpt\")\r\nepoch = ckpt[\"epoch\"]\r\n```"
      }
    ]
  },
  {
    "number": 13145,
    "title": "Can't load lightning on Azure Ml compute.",
    "created_at": "2022-05-24T19:07:20Z",
    "closed_at": "2022-05-25T10:58:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/13145",
    "body": "## 🐛 Bug\n\nCan't use lightning on Azure ML compute. I am hit with this error \n\n`ModuleNotFoundError                       Traceback (most recent call last)\nInput In [1], in <module>\n----> 2 import pytorch_lightning as pl\n\nModuleNotFoundError: No module named 'pytorch_lightning'`\n\nWhen I try to import lightning in my Jupiter notebook, even though lightning is installed. This is very weird and I'm wondering if anyone in this community would have an idea why? The compute is on python3.8 and torch works fine, only lightning crashes the Jupiter kernel\\has that module not found error.\n\nIf it's not the right place to ask, apologies and feel free to close it.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/13145/comments",
    "author": "MalcolmMielle",
    "comments": [
      {
        "user": "akihironitta",
        "created_at": "2022-05-24T23:00:16Z",
        "body": "@MalcolmMielle I believe you're using a different Python interpreter. Inside your notebook, could you check again if it's installed?\r\n```\r\n! pip list | grep lightning\r\n```\r\n\r\nIf not installed, you can install it from the notebook with:\r\n```\r\n! pip install pytorch-lightning\r\n```\r\n\r\nPlease note that you might need to restart the kernel after installation."
      },
      {
        "user": "MalcolmMielle",
        "created_at": "2022-05-25T06:40:23Z",
        "body": "I'll double-check but I have tried both directly on the Azure compute and on my machine. On the compute directly the Jupyter kernel crashes directly. I have also tried reinstalling with `pip reinstall pytorch-lightning` but the error stayed the same. I'll edit my message with the grep output later today.\r\n\r\nI have:\r\n\r\n```\r\n! pip list | grep lightning\r\n\r\npytorch-lightning                       1.6.3\r\n```"
      },
      {
        "user": "MalcolmMielle",
        "created_at": "2022-05-25T10:58:31Z",
        "body": "I found the solution! It's to reinstall lightning using `%conda` magic instead of `! conda` magic... so that it is visible to the kernel. I still don't know why the kernel couldn't see it before but at least it's solved."
      }
    ]
  },
  {
    "number": 12943,
    "title": "Distribution not moved to correct device",
    "created_at": "2022-05-01T04:06:14Z",
    "closed_at": "2022-06-23T04:17:41Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/12943",
    "body": "## 🐛 Bug\r\n\r\nI'm attempting to port a variational auto-encoder to use `torch.distributions`. I noticed that when I define a `torch.distributions.Distribution` as a module parameter - for example the `prior` in the example below - it doesn't get moved to the correct device. I'm fairly certain this is because you call `module.apply()` with a lambda in order to recursively move modules to the specific device - unfortunately a Distribution is not a module but it does hold tensors so this strategy fails in this edge case.\r\n\r\n### To Reproduce\r\n\r\n```\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\nfrom torch.distributions import MultivariateNormal\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n        latent_dim=2\r\n        self.prior = MultivariateNormal(\r\n\t\t\ttorch.zeros(latent_dim), \r\n\t\t\tscale_tril=torch.diag(torch.ones(latent_dim)))\r\n\r\n    def forward(self, x):\r\n        assert self.prior.loc.device == x.device, \"incorrect device\"\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        gpus=1,\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        limit_test_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce RTX 3090\r\n                - NVIDIA GeForce RTX 3090\r\n        - available:         True\r\n        - version:           11.1\r\n* Packages:\r\n        - numpy:             1.22.3\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.1+cu111\r\n        - pytorch-lightning: 1.6.1\r\n        - tqdm:              4.64.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.10\r\n        - version:           #121-Ubuntu SMP Thu Mar 24 16:04:27 UTC 2022",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/12943/comments",
    "author": "david-waterworth",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2022-05-02T18:59:20Z",
        "body": "anything that is not `nn.Module` or registered under buffer will not be moved to the device. Here the `LightningModule` follow the steps of `nn.Module` so even if you do this:\r\n```py\r\nclass BoringModel(nn.Module):  # <- using `nn.Module` here\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n        latent_dim=2\r\n        self.prior = MultivariateNormal(\r\n\t\t\ttorch.zeros(latent_dim), \r\n\t\t\tscale_tril=torch.diag(torch.ones(latent_dim)))\r\n\r\nmodel = BoringModel()\r\nmodel.cuda()\r\n```\r\n`model.prior` will stay on the CPU.\r\n\r\nI'd suggest doing\r\n```py\r\nclass LitModel(LightningModule):\r\n    def on_fit_start(self):\r\n        self.prior = MultivariateNormal(\r\n            torch.zeros(latent_dim, device=self.device), \r\n            scale_tril=torch.diag(torch.ones(latent_dim, device=self.device))\r\n        )\r\n\r\n```\r\n"
      },
      {
        "user": "david-waterworth",
        "created_at": "2022-05-02T23:02:42Z",
        "body": "Thanks, I've already done that, but I'm not sure it's a general solution - for example if want to load and validate a model from saved weights or predict my understanding is `on_fit_start` wont be called. But for now it solves the immediate issue."
      },
      {
        "user": "rohitgr7",
        "created_at": "2022-05-03T09:14:05Z",
        "body": "then you need to define it under `on_test_start/on_validation_start` as well. To make sure they are saved in the checkpoint, maybe try registering them as a buffer so that they are moved to the correct devices as well as saved to the checkpoint."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-06-05T20:09:43Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 12894,
    "title": "accumulate_grad_batches flag in trainer changes cosine decay scheduler incorrectly",
    "created_at": "2022-04-27T06:30:32Z",
    "closed_at": "2022-06-23T04:17:35Z",
    "labels": [
      "question",
      "won't fix",
      "callback: gradient accumulation",
      "optimization"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/12894",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of the bug. -->\r\n\r\nWhen using the `accumulate_grad_batches` flag in trainer, I noticed it changes my scheduler LR differently.  I am using TIMM's cosine decay scheduler.  And when I use 1 gpu, I set `accumulate_grad_batches` = 4, and when I use 4 gpus, I set `accumulate_grad_batches` = 1.  Then I noticed the LR scheduler (in wandb) displayed different LR during training.  For the `accumulate_grad_batches` = 4, the LR decayed faster.\r\n\r\nMy understand is the schedule should only depend on the epoch number, not the batch size or `accumulate_grad_batches` value.  Did anyone else notice something similar before?\r\n\r\nI am using PTL version 1.5, torch 1.11, and TIMM 0.5.4.  I am training videos (HMDB) using pytorchvideo library as well.\r\n\r\nThanks!\n\ncc @borda @rohitgr7",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/12894/comments",
    "author": "exnx",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2022-05-02T18:45:07Z",
        "body": "> the LR decayed faster.\r\n\r\nwhat do you mean by faster? if you are using the same batch_size in both the cases then based on step, while using 4 GPUs the decay will be faster as compared to the other case since the effective batch size per step is 4 times here but in the first case the update will happen after every 4 steps (accumulation factor)."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-06-05T20:09:44Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 12756,
    "title": "UserWarning: The flag devices=-1 will be ignored",
    "created_at": "2022-04-14T08:29:50Z",
    "closed_at": "2022-04-22T15:32:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/12756",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of the bug. -->\r\n\r\nI use this configuration Trainer(accelerator=\"gpu\", devices=-1, auto_select_gpus=False) as suggested by the doc but I get:\r\n\r\nUserWarning: The flag devices=-1 will be ignored, instead the device specific number 1\r\n\r\nEven if on my AWS p3.x8large I have 4 GPUs:\r\n\r\n[2022-03-31 10:29:41,160][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n\r\n### Expected behavior\r\n\r\nAll the GPUs are used.\r\n\r\n### Environment\r\n\r\nSageMaker Container: 763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-training:1.9.1-gpu-py38-cu111-ubuntu20.04\r\nPyTorch Lightning Version 1.6.0\r\nPython 3.8\r\nNCCL version 2.7.8+cuda11.1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/12756/comments",
    "author": "mnslarcher",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-04-15T22:38:01Z",
        "body": "@mnslarcher Is it possible you used \r\n\r\n```py\r\nTrainer(gpus=2, devices=-1, auto_select_gpus=False)\r\n```\r\n\r\ninstead of what you wrote here? I can only reproduce it this way (did not run on sagemaker) and the warning would be appropriate here, because the numbers are conflicting each other. \r\n\r\nThe usage\r\n```py\r\nTrainer(accelerator=\"gpu\", devices=-1, auto_select_gpus=False)\r\n```\r\n\r\nis correct. \r\n"
      },
      {
        "user": "mnslarcher",
        "created_at": "2022-04-16T06:20:18Z",
        "body": "You are right @awaelchli , to simplify (aka I'm using devices=-1 and the GPU accellerator) I leaved out details that maybe are important. No, I don't use gpu=2 or another number, in this moment I have gpu=None (null below because is a YAML).\r\n\r\nI use Hydra, below is my full config, I didn't change gpus/accelerator/devices from the command line. I manually changed\r\n\r\n\"trainer.gradient_clip_val\": 5.0,\r\n\"trainer.strategy\": \"ddp\",\r\n\"trainer.sync_batchnorm\": \"true\",\r\n\r\nI assume that \"auto\" used below in this case is == \"gpu\" given that the instance is a \"p3.x8large\" and also because the training ended up using 1 GPU/4GPUs not zero.\r\n\r\nSorry for the imprecision.\r\n\r\n```\r\n_target_: pytorch_lightning.Trainer\r\naccelerator: auto\r\naccumulate_grad_batches: null\r\namp_backend: native\r\namp_level: null\r\nauto_lr_find: false\r\nauto_scale_batch_size: false\r\nauto_select_gpus: false\r\nbenchmark: false\r\ncallbacks: ${oc.dict.values:_callback_dict}\r\nenable_checkpointing: true\r\ncheck_val_every_n_epoch: 1\r\ndefault_root_dir: ${checkpoints_dir}\r\ndetect_anomaly: false\r\ndeterministic: false\r\ndevices: -1\r\nfast_dev_run: false\r\ngpus: null\r\ngradient_clip_algorithm: null\r\ngradient_clip_val: null\r\nlimit_train_batches: 1.0\r\nlimit_val_batches: 1.0\r\nlimit_test_batches: 1.0\r\nlimit_predict_batches: 1.0\r\nlogger:\r\n  _target_: pytorch_lightning.loggers.TensorBoardLogger\r\n  save_dir: ${checkpoints_dir}/tb_logs\r\n  name: ${experiment_name}\r\n  version: null\r\n  log_graph: false\r\n  default_hp_metric: true\r\n  prefix: ''\r\n  sub_dir: null\r\nlog_every_n_steps: 50\r\nprofiler: null\r\noverfit_batches: 0.0\r\nenable_progress_bar: true\r\nplugins: null\r\nprecision: 32\r\nmax_epochs: ${training.max_epochs}\r\nmin_epochs: null\r\nmax_steps: ${training.max_steps}\r\nmin_steps: null\r\nmax_time: null\r\nnum_nodes: 1\r\nnum_processes: 1\r\nnum_sanity_val_steps: 2\r\nreload_dataloaders_every_n_epochs: 0\r\nreplace_sampler_ddp: true\r\nstrategy: null\r\nsync_batchnorm: false\r\ntpu_cores: null\r\nipus: null\r\ntrack_grad_norm: -1\r\nval_check_interval: 1.0\r\nenable_model_summary: true\r\nweights_save_path: null\r\nmove_metrics_to_cpu: false\r\nmultiple_trainloader_mode: max_size_cycle\r\n```"
      },
      {
        "user": "mnslarcher",
        "created_at": "2022-04-22T15:32:46Z",
        "body": "I found the problem... it was this old default: num_processes: 1.\r\n\r\nWeird behavior, even if accellerator = GPU (auto) this deprecated parameter bypasses the devices parameter, we can close the issue."
      },
      {
        "user": "w5688414",
        "created_at": "2024-06-08T05:12:50Z",
        "body": "set num_processes is ok"
      }
    ]
  },
  {
    "number": 12407,
    "title": "how do I know the loss is average among devices when using ddp",
    "created_at": "2022-03-22T09:24:06Z",
    "closed_at": "2022-05-02T11:06:19Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/12407",
    "body": "## 🐛 Bug\r\n\r\nAs lots of issues said, the loss reported in progress bar is not correct but summing among different devices. I am confused whether the loss is not averaged before backward. How do I check this? \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/12407/comments",
    "author": "zixiliuUSC",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2022-03-22T09:56:44Z",
        "body": "hey @zixiliuUSC !\r\nloss shown on the progress bar is only from the device with global_rank = 0 and is not averaged. The gradients are averaged (synced) instead during `optimizer.step`."
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-24T04:45:52Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 12374,
    "title": "use Trainer to train model without validation loop",
    "created_at": "2022-03-18T16:24:01Z",
    "closed_at": "2022-05-02T11:06:20Z",
    "labels": [
      "question",
      "won't fix",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/12374",
    "body": "Dear Developer, \r\n\r\nSince I have to train the model without validation loops. Is there any parameter that can make the trainer training without the validation loop? \r\n\r\ntrainer = pl.Trainer(strategy=\"dp\", accelerator=\"gpu\", devices=\"auto\", max_epochs=1, limit_val_batches=0,\r\n                         num_sanity_val_steps=0, val_check_interval=0)\r\nI tried to set those parameters, but it does not work \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/12374/comments",
    "author": "dingyic1",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2022-03-21T09:06:46Z",
        "body": "```py\r\nlimit_val_batches=0,\r\nnum_sanity_val_steps=0\r\n```\r\njust these 2 should work. Is it running the validation loop? does it show the validation progress bar?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-04-24T04:45:51Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 9643,
    "title": "ShardedDDP Warning",
    "created_at": "2021-09-22T14:59:46Z",
    "closed_at": "2021-11-04T05:49:20Z",
    "labels": [
      "question",
      "won't fix",
      "distributed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9643",
    "body": "I'm getting the following warning after every validation:\r\n\r\nwarning root: sharded ddp detected that trainable params changed, because of eval/train mode or parameter freezing/unfreeze\r\n\r\nIs this something I should worry about or can I just ignore it? Thanks!\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9643/comments",
    "author": "cutecows",
    "comments": [
      {
        "user": "SeanNaren",
        "created_at": "2021-09-22T19:01:39Z",
        "body": "This shouldn't be an issue I think, however hopefully @blefaudeux might be able to provide more insight :)"
      },
      {
        "user": "cutecows",
        "created_at": "2021-09-22T19:13:33Z",
        "body": "Thanks! I saw my validation loss spike up on the iteration after the warning. "
      },
      {
        "user": "SeanNaren",
        "created_at": "2021-09-22T19:14:58Z",
        "body": "Any chance you could try fairscale master? I think I saw a few Sharded DDP fixes get merged."
      },
      {
        "user": "blefaudeux",
        "created_at": "2021-09-22T19:34:58Z",
        "body": "the warning can be legit, but it was a little too eager in the past, it has been fixed recently indeed ! Let me know if you still see any issues with a recent fairscale (the fix is in the latest pip release), should not be related to a spike in validation normally. Thanks for the heads up @SeanNaren !"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-10-25T06:24:02Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 9542,
    "title": "WHY NOT SUPPORT PYTORCH==1.4.0 !!!",
    "created_at": "2021-09-15T12:37:24Z",
    "closed_at": "2021-09-15T14:05:43Z",
    "labels": [
      "feature",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9542",
    "body": null,
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9542/comments",
    "author": "shawnthu",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-09-15T13:26:35Z",
        "body": "Lightning not supporting 1.4.0 means we do not run tests against this version and we do not fix bugs or maintain support for features dropped above torch 1.4. But you should still be able to use 1.4 and it will probably be fine for most uses. Have you tried it? "
      },
      {
        "user": "awaelchli",
        "created_at": "2021-09-15T13:27:27Z",
        "body": "Furthermore, if you need to run older projects and make them reproducible, I recommend reverting back to a PL version that still had support for torch 1.4. "
      }
    ]
  },
  {
    "number": 8947,
    "title": "Writer Callback cannot be used with test step?",
    "created_at": "2021-08-17T03:51:59Z",
    "closed_at": "2021-09-24T10:34:51Z",
    "labels": [
      "help wanted",
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8947",
    "body": "Using this custom writer:\r\n```python\r\nfrom typing import Any, List\r\n\r\nimport torch\r\nfrom pytorch_lightning.callbacks import BasePredictionWriter\r\nfrom torch import Tensor\r\n\r\n\r\nclass PredictionWriter(BasePredictionWriter):\r\n\r\n    def __init__(self, params):\r\n        super(PredictionWriter, self).__init__(params.write_interval)\r\n        self.params=params\r\n        self.predictions = []\r\n\r\n    def write_on_batch_end(\r\n        self, trainer, pl_module, prediction: Any, batch_indices: List[int], batch: Any,\r\n        batch_idx: int, dataloader_idx: int\r\n    ):\r\n        torch.save(prediction, self.params.output_dir + dataloader_idx + f\"{batch_idx}.pt\")\r\n\r\n    def write_on_epoch_end(\r\n        self, trainer, pl_module, dataloaders: List[Any], batch_indices: List[Any]\r\n    ):\r\n        for dataloader in dataloaders:\r\n            for batch in dataloader:\r\n\r\n                # Convert any tensor values to list\r\n                batch_items = {k: v if not isinstance(v, Tensor) else v.tolist() for k, v in batch.items()}\r\n\r\n                # Switch predictions so each entry has its own dict\r\n                for values in zip(*batch_items.values()):\r\n                    prediction = dict(zip(batch_items.keys(), values))\r\n                    self.predictions.append(prediction)\r\n\r\n        self._checkpoint()\r\n\r\n\r\n    def _checkpoint(self):\r\n        # Write predictions for current file to disk\r\n        torch.save(self.predictions, f\"{self.params.dir}{self.params.name}\")\r\n```\r\n\r\nthis `test_step`:\r\n```python\r\n    def test_step(self, batch, batch_idx):\r\n        idx, text, true_cls = batch[\"idx\"], batch[\"text\"], batch[\"cls\"]\r\n        rpr = self.encoder(text)\r\n        pred_cls = torch.argmax(self.cls_head(rpr), dim=-1)\r\n\r\n        return {\r\n                \"idx\": idx,\r\n                \"true_cls\": true_cls,\r\n                \"pred_cls\": pred_cls\r\n            }\r\n```\r\nand, this `Trainer`:\r\n\r\n```python\r\n        # trainer\r\n        trainer = pl.Trainer(\r\n            gpus=params.trainer.gpus,\r\n            callbacks=[PredictionWriter(params.prediction)]\r\n        )\r\n\r\n        # testing\r\n        dm.setup('test')\r\n        trainer.test(\r\n            model=model,\r\n            datamodule=dm\r\n        )\r\n```\r\n\r\nthe predictions are not been saved as was done using `write_prediction_dict`. \r\nDo I miss something?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8947/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-08-17T05:38:12Z",
        "body": "Dear @celsofranssa,\r\n\r\nThe prédiction writer is meant to be used with trainer.predict and Lightning Module predict_step\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-09-17T06:18:09Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "jwallat",
        "created_at": "2022-05-13T13:36:55Z",
        "body": "Hi, \r\n\r\nI would like to save my predictions on the test set for further analysis/additional metrics computation, which might come later. I understand that the current PredictionsWriter module is not meant to do this. Is there another way in lightning to do this? \r\n\r\nI can currently see two options:\r\n1. Use .test() **and** .predict() with my test set, which is obviously sub-optimal\r\n2. Re-implement something similar to the PredictionsWriter that works in the test step, which also seems wrong since the functionality exists in lightning (just not for .test())\r\n\r\nIs there any other obvious solution/best practice that I am missing?\r\n\r\nBest, \r\nJonas "
      }
    ]
  },
  {
    "number": 8605,
    "title": "(ckpt_path=\"b' pytorch_lightning.utilities.exceptions.MisconfigurationException: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.",
    "created_at": "2021-07-28T17:40:49Z",
    "closed_at": "2021-09-04T17:59:36Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8605",
    "body": "I have been trying to debug this for over a day, now. \r\nThis is a snapshot of my code:\r\n```\r\nclassifier = Classifier(model.moco)\r\nclass_trainer = pl.Trainer(max_epochs=args.epochs//2, \r\n                     gpus=gpus, gradient_clip_val=1.0, \r\n                     auto_lr_find=True,\r\n                     progress_bar_refresh_rate=2,\r\n                     )\r\nprint(f'\\n Training {model_name} Classifier')                            \r\nclass_trainer.fit(\r\n        model=classifier,\r\n        train_dataloader=classifier_train_dataloader,\r\n        val_dataloaders=valid_dataloader,\r\n        )\r\n\r\nresults = class_trainer.test(dataloaders=test_dataloader) \r\n```\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_moco_.py\", line 380, in <module>\r\n    results = class_trainer.test(test_dataloaders=test_dataloader)\r\n  File \"/home/jupyter-belona/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 703, in test\r\n    self.tested_ckpt_path = self.__load_ckpt_weights(ckpt_path)\r\n  File \"/home/jupyter-belona/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1144, in __load_ckpt_weights\r\n    f'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.'\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8605/comments",
    "author": "etetteh",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2021-07-29T00:33:04Z",
        "body": "What version of Lightning are you using? \r\nIn this case, you're calling `trainer.test` without passing the model in. You're also not using any checkpoint callbacks that monitor a value to determine what the \"best\" model checkpoint to use is. \r\n\r\nHave you tried `class_trainer.test(classifier, dataloaders=test_dataloader)` ? "
      },
      {
        "user": "tchaton",
        "created_at": "2021-07-29T09:20:47Z",
        "body": "Dear @etetteh, any progress on your side ?"
      },
      {
        "user": "etetteh",
        "created_at": "2021-07-29T12:11:38Z",
        "body": "I am using the latest version of lightning.\r\nI get the same error whether I use model checkpointing, by monitoring validation accuracy or not.\r\n\r\nI just tried .test(classifier, test_dataloader), and I'm getting the same error.\r\n\r\nThe same code was working before."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-28T15:57:51Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 8559,
    "title": "Wandblogger not logging train loss after every step",
    "created_at": "2021-07-26T13:30:21Z",
    "closed_at": "2021-07-26T14:30:23Z",
    "labels": [
      "help wanted",
      "question",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8559",
    "body": "## 🐛 Bug\r\n\r\nI am using wandb with Pytorch Lightning. I am logging train/loss, val/loss, train/metric, val/metric. Everything is logged properly to wandb dashboard except the **train/loss** (after every step).\r\n\r\nHere's the main lightning module:\r\n\r\n`class ImageClassification(pl.LightningModule):\r\n    def __init__(self, model):\r\n        super().__init__()\r\n        self.model = model\r\n        self.criterion = nn.BCEWithLogitsLoss()\r\n        self.lr = CFG['lr']\r\n    \r\n    def forward(self, x):\r\n        output = self.model(x)\r\n        return output\r\n    \r\n    def configure_optimizers(self):\r\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=CFG['lr'])\r\n        return self.optimizer\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        images, targets = batch['image'], batch['target']\r\n        output = self.model(images)\r\n        loss = self.criterion(output.view(-1), targets)\r\n        \r\n        self.log('train/loss', loss, logger=True)  # the thing that is not being logged\r\n\r\n        try:\r\n            auc = roc_auc_score(targets.detach().cpu(), output.sigmoid().detach().cpu())\r\n            self.log(\"train/auc\", auc, prog_bar=True, logger=True)\r\n        except:\r\n            pass\r\n        \r\n        return {\r\n            \"loss\": loss,\r\n            \"predictions\": output,\r\n            \"targets\" : targets\r\n        }\r\n    \r\n    def training_epoch_end(self, outputs):\r\n\r\n        preds = []\r\n        targets = []\r\n\r\n        for output in outputs:\r\n            preds += output['predictions']\r\n            targets += output['targets']\r\n        \r\n        targets = torch.stack(targets)\r\n        preds = torch.stack(preds)\r\n\r\n        train_auc = roc_auc_score(targets.detach().cpu(), preds.sigmoid().detach().cpu())\r\n        self.log(\"train/auc_epoch\", train_auc,logger=True)\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        images, targets = batch['image'], batch['target']\r\n        output = self.model(images)\r\n        loss = self.criterion(output.view(-1), targets)\r\n\r\n        self.log('val/loss', loss,prog_bar=True, logger=True)\r\n\r\n        return {\r\n            \"predictions\": output,\r\n            \"targets\": targets\r\n        }\r\n    \r\n    def validation_epoch_end(self, outputs):\r\n\r\n        preds = []\r\n        targets = []\r\n\r\n        for output in outputs:\r\n            preds += output['predictions']\r\n            targets += output['targets']\r\n        \r\n        targets = torch.stack(targets)\r\n        preds = torch.stack(preds)\r\n    \r\n        val_auc = roc_auc_score(targets.detach().cpu(), preds.sigmoid().detach().cpu())\r\n        self.log(\"val/auc_epoch\", val_auc,prog_bar=True,logger=True)\r\n    \r\n    def test_step(self, batch, batch_idx):\r\n        images = batch['image']\r\n        output = self.model(images)\r\n        return output`",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8559/comments",
    "author": "Gladiator07",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-07-26T13:41:38Z",
        "body": "is the logging interval small enough? I.e., `Trainer(log_every_n_step=n)` where n must be smaller than `len(dataloader)`. "
      },
      {
        "user": "awaelchli",
        "created_at": "2021-07-26T14:23:47Z",
        "body": "```python\r\ntry:\r\n        auc = roc_auc_score(targets.detach().cpu(), output.sigmoid().detach().cpu())\r\n        self.log(\"train/auc\", auc, prog_bar=True, logger=True)\r\n    except:\r\n        pass\r\n```\r\n\r\nIf an exception raises here, it will be silently ignored and nothing gets logged. Please try to remove the try-except block here and see if it raises an exception."
      },
      {
        "user": "Gladiator07",
        "created_at": "2021-07-26T14:30:23Z",
        "body": "Thanks, changing n to 1 in `Trainer(log_every_n_step=n)` solved my problem. I was testing my code with smaller data so it couldn't log as you said. I am closing this issue."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-07-26T14:32:52Z",
        "body": "Happy to help. Glad you found the issue. Lightning should also print a warning in that case, you may have missed it or you are not on the latest version. \r\ncheers"
      }
    ]
  },
  {
    "number": 8457,
    "title": "OOM in GAN",
    "created_at": "2021-07-19T01:46:21Z",
    "closed_at": "2021-07-19T08:59:22Z",
    "labels": [
      "question",
      "working as intended"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8457",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nGPU memory quickly builds up in few epochs\r\n```python\r\n## Please reproduce using the BoringModel\r\nclass model(pl.LightningModule):\r\n    def __init__(self, d_dim=100, z_dim=64, pcs_dim=64, zd_dim=20, lr=0.0002):\r\n        super().__init__()\r\n        # self.save_hyperparameters()\r\n        self.lr = lr\r\n        self.z_dim = z_dim\r\n        self.zm_dim = pcs_dim\r\n        self.zd_dim = zd_dim\r\n        self.b1 = 0.9\r\n        self.b2 = 0.999\r\n        self.alpha = 1.0 # weight for losses\r\n        self.beta = 1.0 # weight for zm reg losses\r\n        self.k_step = 0\r\n\r\n        ### initializing networks\r\n        img_shape = (1, H, W)\r\n        self.generator = Generator(img_shape, self.z_dim, self.zm_dim)\r\n        self.discriminator = Discriminator(img_shape, self.zm_dim)\r\n        self.dae = DAE(d_dim, self.zd_dim)\r\n        self.zmzdreg = ZmZdReg(self.zm_dim, self.zd_dim)\r\n\r\n        # application of weights\r\n        self.generator.apply(self.weights_init)\r\n        self.discriminator.apply(self.weights_init)\r\n        self.dae.apply(self.weights_init)\r\n\r\n        # Important: This property activates manual optimization.\r\n        self.automatic_optimization = False  # True - Auto // # False - Manual update\r\n\r\n        # for sampling\r\n        self.validation_z = None\r\n        self.pcs_sample = None\r\n        self.imgs_ref = None\r\n        self.logging_dict = {\"g_loss\": [], \"g_avd_loss\": [], \"g_z_loss\": [],\r\n                             \"d_loss\": [], \"d_avd_loss\": [], \"d_z_loss\": [],\r\n                             \"data_loss\": [], \"reg_loss\": []\r\n                             }\r\n\r\n    def forward(self, noise, pcs):\r\n        maps_hr_out = self.generator(noise, pcs)\r\n        return maps_hr_out\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):  \r\n        imgs, pcs, data = batch\r\n\r\n        # sample noise\r\n        z = torch.randn(imgs.shape[0], self.z_dim)\r\n        z = z.type_as(imgs)\r\n\r\n        # valid and fake labels\r\n        valid = torch.ones(imgs.size(0), 1).type_as(imgs)\r\n        fake = torch.zeros(imgs.size(0), 1).type_as(imgs)\r\n\r\n        self.validation_z = z\r\n        # to ensure same set to be generated\r\n        if self.k_step == 0:\r\n            self.pcs_sample = pcs\r\n            self.imgs_ref = imgs\r\n\r\n        # optimizers, manual access\r\n        g_opt, d_opt, zd_opt, r_opt = self.optimizers()\r\n        # self.r_opt = r_opt\r\n\r\n        ########## train generator ##########\r\n        fake_img = self(z, pcs)\r\n        valid_pred, pcs_pred = self.discriminator(fake_img, pcs)\r\n        g_loss = self.alpha*self.adv_loss(valid_pred, valid) + self.beta*self.mse_loss(pcs_pred, pcs)\r\n        self.log('g_loss', g_loss, prog_bar=True)\r\n\r\n        # Update generator gradients\r\n        g_opt.zero_grad()\r\n        self.manual_backward(g_loss, retain_graph=True)\r\n        # Update generator optimizer\r\n        g_opt.step()\r\n\r\n        # dictionary logging\r\n        self.logging_dict['g_loss'].append(g_loss)\r\n        self.logging_dict['g_avd_loss'].append(self.alpha*self.adv_loss(valid_pred, valid))\r\n        self.logging_dict['g_z_loss'].append(self.beta*self.mse_loss(pcs_pred, pcs))\r\n        ######################################\r\n\r\n        ########## train discriminator ##########\r\n        valid_pred, pcs_pred = self.discriminator(imgs, pcs)\r\n        real_loss = self.alpha*self.adv_loss(valid_pred, valid) + self.beta*self.mse_loss(pcs_pred, pcs)\r\n\r\n        fake_detach = self(z, pcs).detach()\r\n        fake_valid, fake_pcs_pred = self.discriminator(fake_detach, pcs)\r\n        fake_loss = self.alpha*self.adv_loss(fake_valid, fake) + self.beta*self.mse_loss(fake_pcs_pred, pcs)\r\n\r\n        d_loss = (real_loss + fake_loss)\r\n        self.log('d_loss', d_loss, prog_bar=True)\r\n\r\n        # Update gradients\r\n        d_opt.zero_grad()\r\n        self.manual_backward(d_loss, retain_graph=True)\r\n        # Update optimizer\r\n        d_opt.step()\r\n\r\n        # dictionary logging\r\n        self.logging_dict['d_loss'].append(d_loss)\r\n        self.logging_dict['d_avd_loss'].append(self.alpha*(self.adv_loss(valid_pred, valid) + self.adv_loss(fake_valid, fake)))\r\n        self.logging_dict['d_z_loss'].append(self.beta*(self.mse_loss(pcs_pred, pcs) + self.mse_loss(fake_pcs_pred, pcs)))\r\n        ######################################\r\n```\r\n\r\n - PyTorch Lightning Version (e.g., 1.3.0): 1.3.8\r\n - PyTorch Version (e.g., 1.8) 1.9\r\n - Python version:\r\n - OS (e.g., Linux): Windows 10\r\n - CUDA/cuDNN version: 11.1\r\n - GPU models and configuration: 2070 max-q\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - If compiling from source, the output of `torch.__config__.show()`:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\nI have 4 models in total with 4 respective optimizers. But by disabling them two of them, I found that the error was being caused by the GAN parts. I honestly have no idea what could be a problem here because I did implement manual optimization for WGANGP before and I had no memory issues.\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8457/comments",
    "author": "ncuxomun",
    "comments": [
      {
        "user": "ncuxomun",
        "created_at": "2021-07-19T04:36:45Z",
        "body": "I might have solved the problem by changing to:\r\nself.manual_backward(x_loss, retain_graph=False). But I am not entirely sure. \r\nIs there a doc page where I can look up description for manual_backward operation?"
      },
      {
        "user": "justusschock",
        "created_at": "2021-07-19T07:42:56Z",
        "body": "The issue most likely is that when you append it to your logging dict (why do you do this at all, when we provide the `log` function?) You don't detach anything. Meaning that first you have an increasing number of values there (so memory will increase anyways) but second you store the autograd graph for each value which is pretty large and not necessary. So changing `self.logging_dict['some_name].append(some_tensor)` to `self.logging_dict['some_name].append(some_tensor.detach())` should help a lot. Your memory will still be growing but only a bit (the amount of memory it needs to store the tensor). Ideally you remove those calls completely and just rely on `self.log` :)"
      },
      {
        "user": "tchaton",
        "created_at": "2021-07-19T08:58:56Z",
        "body": "Hey @ncuxomun,\r\n\r\nYes, I believe it is related to storing tensors on GPU.\r\n\r\n```\r\n        self.logging_dict = {\"g_loss\": [], \"g_avd_loss\": [], \"g_z_loss\": [],\r\n                             \"d_loss\": [], \"d_avd_loss\": [], \"d_z_loss\": [],\r\n                             \"data_loss\": [], \"reg_loss\": []\r\n                             }\r\n```"
      }
    ]
  },
  {
    "number": 8455,
    "title": "RuntimeError: Early stopping conditioned on metric `val_loss` which is not available.",
    "created_at": "2021-07-18T20:06:19Z",
    "closed_at": "2021-07-19T10:15:22Z",
    "labels": [
      "question",
      "working as intended",
      "logging"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8455",
    "body": "I am trying to implement early stopping. Here is the code in my trainer:\r\n\r\n```\r\n    logger = TensorBoardLogger('tb_logs', name='my_model')\r\n\r\n    early_stopping_cb = EarlyStopping(monitor='val_loss',\r\n                                      min_delta=1e-2,\r\n                                      patience=5,\r\n                                      mode='min',\r\n                                      strict=True)\r\n    trainer = pl.Trainer(logger=logger,\r\n                         fast_dev_run=args.fast,\r\n                         limit_train_batches=100,\r\n                         callbacks=[early_stopping_cb],\r\n                         gpus=1)\r\n```\r\n\r\nIn the Lightning module I'm logging the validation loss using a TB logger:\r\n\r\n```\r\n    def validation_step(self, batch, batch_idx):\r\n.\r\n.\r\n.\r\n        loss = F.cross_entropy(input=x.transpose(1, 2),\r\n                               target=y,\r\n                               ignore_index=0)\r\n\r\n        self.logger.log_metrics({\r\n            'val_loss': loss\r\n        })\r\n        return loss\r\n```\r\n\r\nThis codes is raising the following errors:\r\n\r\n```\r\n  File \"/home/evan/deep-behavior-embedding/train.py\", line 84, in <module>\r\n    val_dataloaders=test_dataloader)\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 460, in fit\r\n    self._run(model)\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 758, in _run\r\n    self.dispatch()\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 799, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 144, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 809, in run_stage\r\n    return self.run_train()\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 871, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 584, in run_training_epoch\r\n    self.trainer.run_evaluation(on_epoch=True)\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1011, in run_evaluation\r\n    self.evaluation_loop.on_evaluation_end()\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 102, in on_evaluation_end\r\n    self.trainer.call_hook('on_validation_end', *args, **kwargs)\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1228, in call_hook\r\n    trainer_hook(*args, **kwargs)\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_hook.py\", line 227, in on_validation_end\r\n    callback.on_validation_end(self, self.lightning_module)\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/callbacks/early_stopping.py\", line 173, in on_validation_end\r\n    self._run_early_stopping_check(trainer)\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/callbacks/early_stopping.py\", line 184, in _run_early_stopping_check\r\n    or not self._validate_condition_metric(logs)  # short circuit if metric not present\r\n  File \"/home/evan/deep-behavior-embedding/.virtualenv/lib/python3.7/site-packages/pytorch_lightning/callbacks/early_stopping.py\", line 134, in _validate_condition_metric\r\n    raise RuntimeError(error_msg)\r\nRuntimeError: Early stopping conditioned on metric `val_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: ``\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nDoes this have to do with using the TB logger instead of the default one?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8455/comments",
    "author": "EvanZ",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2021-07-19T07:44:53Z",
        "body": "No, this doesn't. The default one is the TB logger. However, you should call `self.log('val_loss', loss)` instead of `self.logger.log_metrics({'val_loss': loss})` since that way we also call the logger internally, but we also know what values have been logged and thereby can enable early stopping for them :)"
      },
      {
        "user": "tchaton",
        "created_at": "2021-07-19T10:15:22Z",
        "body": "Dear @EvanZ,\r\n\r\nAs shared by @justusschock, using `self.log` should work fine :) \r\n\r\nClosing this issue, feel free to re-open if it still doesn't work.\r\n\r\nBest,\r\nT.C"
      }
    ]
  },
  {
    "number": 7775,
    "title": "training_epoch_end called before all steps of epoch were completed. always at about 0.25 size of steps.",
    "created_at": "2021-05-31T07:10:17Z",
    "closed_at": "2021-06-01T09:40:40Z",
    "labels": [
      "help wanted",
      "question",
      "working as intended"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7775",
    "body": "## 🐛 Bug\r\n\r\n```bash\r\nGPU available: False, used: False\r\nTPU available: None, using: 0 TPU cores\r\nValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\r\n  | Name                | Type                 | Params\r\n-------------------------------------------------------------\r\n\r\n-------------------------------------------------------------\r\n\r\nEpoch 0:   0%|          | 0/13 [00:00<?, ?it/s] \r\nEpoch 0:  23%|██▎       | 3/13 [01:38<05:27, 32.75s/it, loss=4.73, v_num=7]\r\n// training_epoch_end:  outputs = [{'loss': tensor(6.4593)}, {'loss': tensor(5.7653)}, {'loss': tensor(1.9642)}]\r\n\r\nValidating: 0it [00:00, ?it/s]\r\nValidating:   0%|          | 0/10 [00:00<?, ?it/s]\r\nEpoch 0:  38%|███▊      | 5/13 [01:48<02:54, 21.78s/it, loss=4.73, v_num=7]\r\nEpoch 0:  46%|████▌     | 6/13 [01:59<02:19, 19.91s/it, loss=4.73, v_num=7]\r\nEpoch 0:  54%|█████▍    | 7/13 [02:10<01:51, 18.58s/it, loss=4.73, v_num=7]\r\nEpoch 0:  62%|██████▏   | 8/13 [02:20<01:27, 17.60s/it, loss=4.73, v_num=7]\r\nEpoch 0:  69%|██████▉   | 9/13 [02:31<01:07, 16.83s/it, loss=4.73, v_num=7]\r\nEpoch 0:  77%|███████▋  | 10/13 [02:42<00:48, 16.21s/it, loss=4.73, v_num=7]\r\nEpoch 0:  85%|████████▍ | 11/13 [02:52<00:31, 15.71s/it, loss=4.73, v_num=7]\r\nEpoch 0:  92%|█████████▏| 12/13 [03:04<00:15, 15.34s/it, loss=4.73, v_num=7]\r\nEpoch 0: 100%|██████████| 13/13 [03:15<00:00, 15.00s/it, loss=4.73, v_num=7]\r\nEpoch 0: 100%|██████████| 13/13 [03:16<00:00, 15.15s/it, loss=4.73, v_num=7]\r\nEpoch 1:  23%|██▎       | 3/13 [01:42<05:42, 34.24s/it, loss=3.39, v_num=7]\r\n// training_epoch_end:  outputs = [{'loss': tensor(2.6766)}, {'loss': tensor(2.3010)}, {'loss': tensor(1.1722)}]\r\nEpoch 1:  31%|███       | 4/13 [01:48<04:04, 27.22s/it, loss=3.39, v_num=7]\r\nValidating: 0it [00:00, ?it/s]\r\nEpoch 1:  38%|███▊      | 5/13 [02:02<03:15, 24.42s/it, loss=3.39, v_num=7]\r\nCompleted 6.8 MiB/327.9 MiB (48.7 KiB/s) with 2 file(s) remaining\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nEpoch 1: 100%|██████████| 13/13 [03:48<00:00, 17.54s/it, loss=3.39, v_num=7]\r\nEpoch 2:  23%|██▎       | 3/13 [01:44<05:47, 34.72s/it, loss=2.72, v_num=7]\r\nNUM EL TRAINING: 3   [{'loss': tensor(1.2504)}, {'loss': tensor(1.4905)}, {'loss': tensor(1.4158)}]\r\nEpoch 2:  31%|███       | 4/13 [01:49<04:07, 27.48s/it, loss=2.72, v_num=7]\r\nValidating: 0it [00:00, ?it/s]\r\nEpoch 2: 100%|██████████| 13/13 [03:50<00:00, 17.75s/it, loss=2.72, v_num=7]\r\nEpoch 3:  23%|██▎       | 3/13 [01:43<05:46, 34.62s/it, loss=2.27, v_num=7]\r\n//training_epoch_end:   outputs = [{'loss': tensor(0.6632)}, {'loss': tensor(0.9215)}, {'loss': tensor(1.1396)}]\r\nEpoch 3:  31%|███       | 4/13 [01:49<04:06, 27.41s/it, loss=2.27, v_num=7]\r\nValidating: 0it [00:00, ?it/s]\r\n```\r\n\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): mac Catalina (this happens on all environments , linux etc)\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration: happens also with 0 gpus.\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7775/comments",
    "author": "ganitps",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-05-31T11:47:30Z",
        "body": "As of now, `training_epoch_end` runs before validation starts (validation runs outside the training epoch). The progress bar just shows the combined training + validation steps. So this is fine and you get all the steps for the training epoch.\r\n\r\nAfter #7357, `training_epoch_end` will run after the last validation loop each epoch. \r\nHope this clears it up :) "
      },
      {
        "user": "ganitps",
        "created_at": "2021-05-31T12:44:23Z",
        "body": "Thanks @awaelchli  for your reply\r\nThis is not what I understood form the documentation:\r\n\r\n// the pseudocode for these calls\r\n```\r\ntrain_outs = []\r\nfor train_batch in train_data:\r\n        out = training_step(train_batch)\r\n        train_outs.append(out)\r\ntraining_epoch_end(train_outs)\r\n\r\n```\r\n\r\nSo If I want to take actions when epoch ends I must do it in the last training step? (training_step_end())\r\nCan I take the latest version After #7357?"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-05-31T13:53:00Z",
        "body": "No this is still all good. The pseudo code is correct, all outputs from training_step get passed into training_epoch_end. You get as many outputs as training steps. In your case, there seems to be three training steps, so you get 3 outputs, correct? And that's when the training epoch ends and the validation starts. "
      },
      {
        "user": "ganitps",
        "created_at": "2021-05-31T14:29:49Z",
        "body": "No... I have 13 steps.\r\ntraining_epoch_end called after the third one... with 3 outputs..."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-05-31T14:38:36Z",
        "body": "> No... I have 13 steps.\r\n\r\nHow do you know?\r\nHave you printed `len(dataloader)`? Have you set `limit_train_batches=13`?\r\nFrom the output you shared it just looks like you have 3 training steps and 10 validation steps. \r\n\r\nMaybe I can help better if you try to explain what you want to achieve. "
      },
      {
        "user": "ganitps",
        "created_at": "2021-06-01T09:40:32Z",
        "body": "Dear @awaelchli  \r\nThank you so much for sticking with me, your last answer helped me understand that the epoch printouts doesn't reflect epoch steps but also validation. and every thing really works as designed :) "
      }
    ]
  },
  {
    "number": 7138,
    "title": "how to save the last epoch only?",
    "created_at": "2021-04-21T14:06:16Z",
    "closed_at": "2021-04-22T00:28:27Z",
    "labels": [
      "help wanted",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7138",
    "body": "\"monitor (Optional[str]) – quantity to monitor. By default it is None which saves a checkpoint only for the last epoch.\"\r\nwhen i trainning a model, i set the 'monitor' to None, it should save the last epoch as the doc says. but it still save depend on the val_loss, it always save the model with lowest val_loss.\r\n\r\ni also try another way, set the 'save_last' to True. while this needs to set a monitor. And if i set save_top_k to 0, it will save nothing; if set to 1, it will save 2 models, the best one and the last one. But i just want to save the last one.\r\n\r\nis this a bug or i made sth wrong?  is there a way to save model with epoch asigned myself? such as the last 3 epochs?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7138/comments",
    "author": "machin-x",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-04-21T23:20:27Z",
        "body": "Hey! Have a look at this example: \r\n\r\n\r\n```python\r\nimport os\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss, logger=False)\r\n        return {\"x\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2, num_workers=0)\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2, num_workers=0)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=5,  # this will save a checkpoint at epoch index 4 (last epoch)\r\n        weights_summary=None,\r\n        logger=False,\r\n        callbacks=[ModelCheckpoint(dirpath=\"./checkpoints\", monitor=None)]\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\nI'm choosing:\r\n`ModelCheckpoint(dirpath=\"./checkpoints\", monitor=None)`\r\n\r\nThat's all, it saves only one checkpoint, named **epoch=4-step=4.ckpt**, it corresponds to the last epoch being run. \r\nNote: for backward compatibility, when monitor=None, we choose \"val_loss\" as the monitor when it is available. You should be able to avoid that by just renaming your validation loss to \"valid_loss\" or something else :) \r\n"
      }
    ]
  },
  {
    "number": 6173,
    "title": "Pickle error and OOM when upgrading to 1.2.0",
    "created_at": "2021-02-24T10:00:39Z",
    "closed_at": "2021-04-10T06:54:15Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6173",
    "body": "When upgrading from 1.1.6 to 1.2.0, I notices 2 changes\r\n- Significant increase of gpu memory \r\n- Pickle error in my module class (the object `metric_fn` is sure not pickable but the same code worked fine in 1.1.6)\r\n\r\nDo you have an idea what changes in 1.2.0 may cause the issues ?\r\nAny suggestion for the memory problem ?\r\n\r\nMy pseudo code\r\n```python\r\nclass ClfModule(pl.LightningModule):\r\n    def __init__(self, model, tokenizer):\r\n        self.model = model\r\n        self.tokenizer = tokenizer\r\n        self.metric_fn = not_pickable_object()\r\n```\r\nI am using Huggingface transformers and datasets",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6173/comments",
    "author": "phihung",
    "comments": [
      {
        "user": "angadkalra",
        "created_at": "2021-02-24T18:41:52Z",
        "body": "I noticed GPU memory increase too. My original data image size didn't work when going from 1.1.8 to 1.2. "
      },
      {
        "user": "SkafteNicki",
        "created_at": "2021-02-26T15:14:26Z",
        "body": "Gpu memory problem should be fixed in v1.2.1.\r\nWhat accelator are you using? `ddp_spawn` requires that everything can be pickled."
      },
      {
        "user": "angadkalra",
        "created_at": "2021-02-26T19:50:44Z",
        "body": "@phihung My OOM went away with upgrade to v1.2.1 and using DDP. Please try. "
      },
      {
        "user": "tchaton",
        "created_at": "2021-03-01T11:58:21Z",
        "body": "Dear @angadkalra,\r\n\r\nWere using using AMP ? In 1.2.0, there were a bug and AMP wasn't applied properly. It is solved in 1.2.1.\r\nWhich could explain your OOM in 1.2.0 and not in 1.2.1. \r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "phihung",
        "created_at": "2021-03-01T12:18:42Z",
        "body": "Hi SkafteNicki, hi angadkalra,\r\nThanks for your help\r\nI confirm that upgrading to 1.2.1 solves OOM problem.\r\n\r\nThe pickle problem remains. I've tried the following\r\n```\r\npl.Trainer(precision=16, gpus=1)\r\npl.Trainer(precision=16, gpus=1, accelerator=\"ddp_spawn\")\r\npl.Trainer(precision=16, gpus=1, accelerator=\"ddp\")\r\npl.Trainer(precision=16, gpus=1, accelerator=\"ddp2\")\r\npl.Trainer(precision=16, gpus=1, accelerator=\"dp\")\r\n``` \r\nWhat was the default in 1.1.6 ?"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2021-03-01T13:27:56Z",
        "body": "@phihung can you post a model where I can reproduce your pickle error?\nAlso, what prevents you from transforming your non-pickle object into a pickable object? "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-03-31T23:02:32Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6159,
    "title": "Model loaded from checkpoint has bad accuracy ",
    "created_at": "2021-02-23T19:55:31Z",
    "closed_at": "2021-02-24T02:14:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6159",
    "body": "#### What is your question?\r\n\r\nI have a model that I train with EarlyStopping and ModelCheckpoint on a custom metric (MAP).\r\nThe training works fine, after 2 epochs the model reaches 96% MAP however when I load it and test it with the exact same function the MAP is 16% (same as untrained model).\r\n**I must be doing something wrong, but what ?**\r\n\r\n#### Code\r\n\r\n```python\r\ndef default_model(dataset: str):\r\n\tif torch.cuda.is_available():\r\n\t\tprint(\"Using the GPU\")\r\n\t\tdevice = torch.device(\"cuda\")\r\n\telse:\r\n\t\tprint(\"Using the CPU\")\r\n\t\tdevice = torch.device(\"cpu\")\r\n\tkwargs = {\r\n\t\t\"dataset\": dataset, \"embed_size\": 50, \"depth\": 3,\r\n\t\t\"vmap\": Graph3D.from_dataset(dataset).vocabulary,\r\n\t\t\"neg_per_pos\": 5, \"max_paths\": 255, \"device\": device\r\n\t}\r\n\ttry:\r\n\t\tmodel = TAPKG.load_from_checkpoint(\"Checkpoints/epoch=2-step=612260.ckpt\", **kwargs).to(device)\r\n\t\treturn model\r\n\texcept OSError as e:\r\n\t\tprint(f\"Couldn't load the save for the model, training instead. ({e.__class__.__name__})\")\r\n\t\tmodel = TAPKG(**kwargs).to(device)\r\n\tcpt = pl.callbacks.ModelCheckpoint(monitor=\"MAP\", mode=\"max\", dirpath=\"Checkpoints\", save_top_k=1)\r\n\ttrainer = pl.Trainer(\r\n\t\tgpus=1,\r\n\t\tcheck_val_every_n_epoch=1,\r\n\t\tcallbacks=[\r\n\t\t\tcpt,\r\n\t\t\tpl.callbacks.EarlyStopping(monitor=\"MAP\", mode=\"max\", min_delta=.002, patience=2)\r\n\t\t],\r\n\t\tauto_lr_find=True\r\n\t)\r\n\t# noinspection PyTypeChecker\r\n\ttrainer.fit(model)\r\n\tprint(cpt.best_model_path, cpt.best_model_score)\r\n\treturn model\r\n\r\ndef eval_link_completion(dataset):\r\n\tmodel = default_model(dataset)\r\n\tranks = model.link_completion_rank()\r\n\tMAP(ranks, plot=True)\r\n```\r\nRight after the training `eval_link_completion` shows a MAP of 96%, when I load the model however it's back to 16%.\r\n\r\n - OS: KUbuntu 20.04\r\n - Packaging pip\r\n - Version 1.2.0\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6159/comments",
    "author": "Inspirateur",
    "comments": [
      {
        "user": "angadkalra",
        "created_at": "2021-02-23T20:22:15Z",
        "body": "I don't think the model you are returning is the trained model, it's the original model from when you first create it. Try doing\r\n`model = model.load_from_checkpoint(cpt.best_model_path)` after trainer.fit() and return model then. "
      },
      {
        "user": "Inspirateur",
        "created_at": "2021-02-23T20:30:20Z",
        "body": "Thank you for answering, but I'm not sure I understand. \r\nWhy would loading like I do yield the original untrained model ?  \r\nHow would you suggest that I load my model instead ?  \r\nThe training takes 6h so it would be nice if I could avoid calling trainer.fit() again, but if it's mandatory for debugging I will."
      },
      {
        "user": "Inspirateur",
        "created_at": "2021-02-23T20:41:10Z",
        "body": "Wait after digging on my end it seems I might have a non deterministic process throwing the evaluation off between the runs, thank you anyway, I'll come back after investigating this."
      },
      {
        "user": "angadkalra",
        "created_at": "2021-02-23T20:42:26Z",
        "body": "Do this in default_model function:\r\n```\r\n# noinspection PyTypeChecker\r\ntrainer.fit(model)\r\nprint(cpt.best_model_path, cpt.best_model_score)\r\nmodel = model.load_from_checkpoint(cpt.best_model_path)\r\nreturn model\r\n```"
      },
      {
        "user": "Inspirateur",
        "created_at": "2021-02-24T02:14:22Z",
        "body": "Yep I'm sorry, my loading/saving code was good, I just had another issue somewhere, thanks for your time"
      },
      {
        "user": "iwinterknight",
        "created_at": "2022-11-13T00:41:09Z",
        "body": "Hi I'm facing the same issue. Could you tell me what other potential issues could cause this ?"
      },
      {
        "user": "Inspirateur",
        "created_at": "2022-11-13T19:30:23Z",
        "body": "I'm afraid i can't help you, it's been more than a year and I'd be completely unable to remember what the problem was"
      },
      {
        "user": "anushka192001",
        "created_at": "2024-05-05T17:20:04Z",
        "body": "same problem"
      },
      {
        "user": "angadkalra",
        "created_at": "2024-07-03T01:32:40Z",
        "body": "99% of the time it's because your inference code has something different than your validation set eval code. Go through each line of eval and infer code and make sure they are producing the same outputs at each step. "
      }
    ]
  },
  {
    "number": 6157,
    "title": "Load models give different results from original",
    "created_at": "2021-02-23T19:10:33Z",
    "closed_at": "2021-03-02T11:57:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6157",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nWhat is the right way to retrieve a trained model and use it after load\r\n\r\n```python\r\nclass NER_Model(pl.LightningModule):\r\n    def __init__(self, hyperparams, model_parameters, dataset_infos, extra_infos):\r\n        super(NER_Model, self).__init__()\r\n\r\n        # ---------- hyperparams\r\n        self.learning_rate = hyperparams[\"learning_rate\"]\r\n        self.train_batch_size = hyperparams[\"train_batch_size\"]\r\n        self.eval_batch_size = hyperparams[\"eval_batch_size\"]\r\n        self.eps = hyperparams[\"eps\"]\r\n        self.seed = hyperparams[\"seed\"]\r\n        self.max_epochs = hyperparams[\"max_epochs\"]\r\n        self.MAX_LEN = hyperparams[\"MAX_LEN\"]\r\n\r\n\r\n        # ---------- model_parameters\r\n        self.tokenizer_class = model_parameters[\"tokenizer_class\"]\r\n        self.tokenizer = self.tokenizer_class.tokenizer\r\n        self.model_name = model_parameters[\"model_name\"]\r\n        self.output_attentions = model_parameters[\"output_attentions\"]\r\n        self.output_hidden_states = model_parameters[\"output_hidden_states\"]\r\n        \r\n        # ---------- dataset_infos\r\n        self.all_data = dataset_infos[\"all_data\"]\r\n        self.tags_infos_dict = dataset_infos[\"tags_infos_dict\"]\r\n        self.CustomDataset = dataset_infos[\"CustomDataset\"]\r\n\r\n        # ---------- extra_infos\r\n        self.overfit = extra_infos[\"overfit\"]\r\n        self.sampler = extra_infos[\"sampler\"]\r\n        # ---------- other_infos\r\n        self.predict_proba = torch.nn.Softmax(dim=1)\r\n        self.step = \"Experiment\"\r\n        \r\n        # ---------- fixing seeds\r\n        self.seed_everything()\r\n\r\n        # # ---------- Model\r\n        self.model = AutoModelForTokenClassification.from_pretrained(\r\n          self.model_name,\r\n          num_labels=len(self.tags_infos_dict[\"tag2idx\"]),\r\n          output_attentions = self.output_attentions,\r\n          output_hidden_states = self.output_hidden_states\r\n          )\r\n        \r\n\r\n        #Getting Dataset\r\n        self.train_dataset = self.CustomDataset(\r\n            self.all_data['X_train'], self.all_data['y_train'], self.tokenizer_class,self.tags_infos_dict,self.MAX_LEN,self.step \r\n        )\r\n        self.valid_dataset = self.CustomDataset(\r\n            self.all_data['X_valid'], self.all_data['y_valid'], self.tokenizer_class,self.tags_infos_dict,self.MAX_LEN,self.step \r\n        )\r\n        self.test_dataset = self.CustomDataset(\r\n            self.all_data['X_test'], self.all_data['y_test'], self.tokenizer_class,self.tags_infos_dict,self.MAX_LEN,self.step \r\n        )\r\n\r\n        # ---------- Creating Dataframe for saving accuracy and loss\r\n        self.df_performance_train_batch = pd.DataFrame(\r\n            columns=[\"train_batch_loss\", \"train_batch_acc\"]\r\n        )\r\n        self.df_performance_train_epoch = pd.DataFrame(\r\n            columns=[\"train_epoch_loss\", \"train_epoch_acc\"]\r\n        )\r\n        self.df_performance_valid_batch = pd.DataFrame(\r\n            columns=[\"valid_batch_loss\", \"valid_batch_acc\"]\r\n        )\r\n        self.df_performance_valid_epoch = pd.DataFrame(\r\n            columns=[\"valid_epoch_loss\", \"valid_epoch_acc\"]\r\n        )\r\n        \r\n\r\n\r\n        # ---------- Creating comparision dataframe with expected and predicted results\r\n        # self.response_columns = []\r\n        # self.df_valid = pd.DataFrame(columns=self.response_columns)\r\n        # self.df_test = pd.DataFrame(columns=self.response_columns)\r\n        # self.valid_true_labels = []\r\n        # self.valid_pred_labels = []\r\n        self.test_true_tags = []\r\n        self.test_pred_tags = []\r\n        \r\n\r\n    def predict(self, X: str):\r\n        self.step = \"Deployment\"\r\n        self.test_pred_tags = []\r\n\r\n        batch = self.tokenizer.encode_plus(X, return_tensors=\"pt\")\r\n        batch[\"attention_masks\"] = torch.ones_like(batch[\"input_ids\"])\r\n        batch = dict((key, input.to( self.device)) for key, input in batch.items())\r\n\r\n        return self.test_step(batch, None)\r\n\r\n\r\n    def retrieve_tags_on_prediction(self,predictions,inputs):\r\n        predictions = predictions.detach().cpu().numpy()\r\n        inputs =inputs.to('cpu').numpy()\r\n        lables_list = []\r\n        for input,prediction in zip(inputs,predictions):\r\n          labels = []\r\n          tokens = self.tokenizer.convert_ids_to_tokens(input)\r\n          for token, pred_idx in zip(tokens, prediction):\r\n              if not token.startswith(\"##\"):\r\n                  labels.append(self.tags_infos_dict['tag_values'][pred_idx])\r\n          lables_list.append(labels)\r\n\r\n        return lables_list\r\n    \r\n\r\n    def forward(self,input_ids,attention_mask=None,labels=None):\r\n        if self.step == \"Experiment\":\r\n            # loss, logits\r\n            outputs = self.model(input_ids, \r\n                            token_type_ids=None,\r\n                            attention_mask=attention_mask, \r\n                            labels=labels)\r\n        if self.step == \"Deployment\":\r\n                  # loss, logits\r\n            outputs = self.model(input_ids)\r\n  \r\n          \r\n        return outputs\r\n\r\n\r\n    def training_step(self, batch, batch_nb):\r\n        # batch\r\n        inputs = batch['input_ids']\r\n        mask = batch['attention_masks']\r\n        targets = batch['tags']\r\n                \r\n        # fwd\r\n        outputs = self.forward(inputs, mask, targets)\r\n        loss = outputs[0]\r\n        logits = outputs[1]\r\n\r\n        # acc\r\n        predictions = torch.argmax(logits, dim=2)\r\n        pred_tags, true_tags = self.retrieve_tags(predictions,targets)\r\n        acc = torch.tensor(accuracy_score(true_tags, pred_tags))\r\n\r\n        # What to log\r\n        tensorboard_logs = {\"loss\": loss, \"acc\": acc}\r\n\r\n        self.df_performance_train_batch = self.df_performance_train_batch.append(\r\n            pd.Series(\r\n                [loss.item(), acc.item()], index=self.df_performance_train_batch.columns\r\n            ),\r\n            ignore_index=True,\r\n        )\r\n\r\n        return {\r\n            \"loss\": loss,\r\n            \"train_acc_batch\": acc,\r\n            \"train_loss_batch\": loss,\r\n            \"log\": tensorboard_logs,\r\n        }\r\n\r\n    def training_epoch_end(self, outputs):\r\n        if not outputs:\r\n            return {}\r\n\r\n        temp_avg_loss_batch = [x[\"train_loss_batch\"] for x in outputs]\r\n        temp_avg_acc_batch = [x[\"train_acc_batch\"] for x in outputs]\r\n\r\n        avg_train_loss = torch.stack(temp_avg_loss_batch).mean()\r\n        avg_train_acc = torch.stack(temp_avg_acc_batch).mean()\r\n\r\n        self.df_performance_train_epoch = self.df_performance_train_epoch.append(\r\n            pd.Series(\r\n                [avg_train_loss.item(), avg_train_acc.item()],\r\n                index=self.df_performance_train_epoch.columns,\r\n            ),\r\n            ignore_index=True,\r\n        )\r\n\r\n        tensorboard_logs = {\r\n            \"avg_train_acc\": avg_train_acc,\r\n            \"avg_train_loss\": avg_train_loss,\r\n        }\r\n\r\n        return {\"avg_train_acc\": avg_train_acc, \"log\": tensorboard_logs}\r\n\r\n    def validation_step(self, batch, batch_nb):\r\n        # batch\r\n        inputs = batch['input_ids']\r\n        mask = batch['attention_masks']\r\n        targets = batch['tags']\r\n                \r\n        # fwd\r\n        outputs = self.forward(inputs, mask, targets)\r\n        loss = outputs[0]\r\n        logits = outputs[1]\r\n\r\n        # acc\r\n        predictions = torch.argmax(logits, dim=2)\r\n        pred_tags, true_tags = self.retrieve_tags(predictions,targets)\r\n        # self.valid_true_labels.extend(true_tags)\r\n        # self.valid_pred_labels.extend(pred_tags)\r\n        acc = torch.tensor(accuracy_score(true_tags, pred_tags))\r\n\r\n        self.df_performance_valid_batch = self.df_performance_valid_batch.append(\r\n            pd.Series(\r\n                [loss.item(), acc.item()], index=self.df_performance_valid_batch.columns\r\n            ),\r\n            ignore_index=True,\r\n        )\r\n\r\n        return {\"valid_acc_batch\": acc, \"valid_loss_batch\": loss}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        if not outputs:\r\n            return {}\r\n        temp_avg_loss_batch = [x[\"valid_loss_batch\"] for x in outputs]\r\n        temp_avg_acc_batch = [x[\"valid_acc_batch\"] for x in outputs]\r\n\r\n        avg_valid_loss = torch.stack(temp_avg_loss_batch).mean()\r\n        avg_valid_acc = torch.stack(temp_avg_acc_batch).mean()\r\n\r\n        self.df_performance_valid_epoch = self.df_performance_valid_epoch.append(\r\n            pd.Series(\r\n                [avg_valid_loss.item(), avg_valid_acc.item()],\r\n                index=self.df_performance_valid_epoch.columns,\r\n            ),\r\n            ignore_index=True,\r\n        )\r\n\r\n        tensorboard_logs = {\r\n            \"avg_valid_acc\": avg_valid_acc,\r\n            \"avg_valid_loss\": avg_valid_loss,\r\n        }\r\n\r\n        return {\"avg_valid_acc\": avg_valid_acc, \"log\": tensorboard_logs}\r\n\r\n    def test_step(self, batch, batch_nb):\r\n\r\n        if self.step ==\"Experiment\": \r\n          # batch\r\n          inputs = batch['input_ids']\r\n          mask = batch['attention_masks']\r\n          targets = batch['tags']\r\n                  \r\n          # fwd\r\n          outputs = self.forward(inputs, mask, targets)\r\n          loss = outputs[0]\r\n          logits = outputs[1]\r\n\r\n          # acc\r\n          predictions = torch.argmax(logits, dim=2)\r\n          pred_tags, true_tags = self.retrieve_tags(predictions,targets)\r\n          self.test_true_tags.extend(true_tags)\r\n          self.test_pred_tags.extend(pred_tags)\r\n          acc = torch.tensor(accuracy_score(true_tags, pred_tags))             \r\n              \r\n          final_return = {\"test_acc_batch\": acc}\r\n        \r\n        if self.step ==\"Deployment\":\r\n          # batch\r\n          inputs = batch['input_ids']\r\n          mask = batch['attention_masks']\r\n\r\n          # fwd\r\n          outputs = self.forward(inputs)\r\n          logits = outputs[0]\r\n          predictions = torch.argmax(logits, dim=2)\r\n\r\n          pred_tags = self.retrieve_tags_on_prediction(predictions,inputs)\r\n          final_return = pred_tags\r\n\r\n        return final_return\r\n\r\n    def test_epoch_end(self, outputs):\r\n        if not outputs:\r\n            return {}\r\n          \r\n        if self.step == \"Experiment\":\r\n          avg_test_acc = torch.stack([x[\"test_acc_batch\"] for x in outputs]).mean()\r\n\r\n          tensorboard_logs = {\"avg_test_acc\": avg_test_acc}\r\n\r\n          retorno = {\"avg_test_acc\": avg_test_acc, \"log\": tensorboard_logs}\r\n        if self.step == \"Deployment\":\r\n          retorno = outputs\r\n\r\n        return retorno\r\n\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = AdamW(\r\n            [p for p in self.parameters() if p.requires_grad],\r\n            lr = self.learning_rate,\r\n            eps = self.eps\r\n        )\r\n\r\n        return optimizer\r\n    \r\n    def decode_tags(self,codes:np.ndarray):\r\n      targets = []\r\n      for elem in codes:\r\n        targets.append(self.tags_infos_dict[\"idx2tag\"][elem])\r\n      return np.array(targets)\r\n      \r\n    def retrieve_tags(self,predictions,targets):\r\n      targets = targets.detach().cpu().numpy()\r\n      predictions = predictions.detach().cpu().numpy()\r\n      pred_tags = [self.tags_infos_dict['tag_values'][p_i] for p, l in zip(predictions, targets) for p_i, l_i in zip(p, l) if self.tags_infos_dict['tag_values'][l_i] != self.tokenizer.pad_token]\r\n      true_tags = [self.tags_infos_dict['tag_values'][l_i] for l in targets for l_i in l if self.tags_infos_dict['tag_values'][l_i] != self.tokenizer.pad_token]\r\n      \r\n      return pred_tags, true_tags\r\n\r\n        \r\n    def seed_everything(self):\r\n        random.seed(self.seed)\r\n        os.environ['PYTHONHASHSEED'] = str(self.seed)\r\n        np.random.seed(self.seed)\r\n        torch.manual_seed(self.seed)\r\n        torch.cuda.manual_seed(self.seed)\r\n        torch.cuda.manual_seed_all(self.seed)\r\n        torch.backends.cudnn.deterministic = True\r\n        torch.backends.cudnn.benchmark = False\r\n    \r\n\r\n    def gpu_mem_restore(func):\r\n        @functools.wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            try:\r\n                return func(*args, **kwargs)\r\n            except:\r\n                type, val, tb = sys.exc_info()\r\n                traceback.clear_frames(tb)\r\n                raise type(val).with_traceback(tb) from None\r\n\r\n        return wrapper              \r\n            \r\n    @gpu_mem_restore\r\n    def train_dataloader(self):\r\n        train_sampler = RandomSampler(self.train_dataset)\r\n        train_dataloader = DataLoader(\r\n                self.train_dataset,\r\n                sampler=train_sampler,\r\n                batch_size=self.train_batch_size,\r\n                num_workers=cpu_count(),\r\n                )\r\n        return train_dataloader \r\n\r\n    @gpu_mem_restore\r\n    def val_dataloader(self):\r\n        valid_sampler = SequentialSampler(self.valid_dataset)\r\n        val_dataloader = DataLoader(\r\n                self.valid_dataset,\r\n                sampler=valid_sampler,\r\n                batch_size=self.eval_batch_size,\r\n                num_workers=cpu_count(),\r\n        )\r\n        return val_dataloader\r\n        \r\n    @gpu_mem_restore\r\n    def test_dataloader(self):\r\n        test_sampler = SequentialSampler(self.test_dataset)\r\n        test_dataloader = DataLoader(\r\n                self.test_dataset,\r\n                sampler=test_sampler,\r\n                batch_size=self.eval_batch_size,\r\n                num_workers=cpu_count(),\r\n                )\r\n        return test_dataloader\r\n\r\n#### Training and testing a prediction (Wright Result!)\r\ntrainer = pl.Trainer(\r\n    gpus=num_gpus,\r\n    max_epochs=max_epochs,\r\n    check_val_every_n_epoch=check_val_every_n_epoch,\r\n    profiler=profiler,\r\n    checkpoint_callback=checkpoint_callback,\r\n    progress_bar_refresh_rate=progress_bar_refresh_rate,\r\n    resume_from_checkpoint=resume_from_checkpoint,\r\n    gradient_clip_val=gradient_clip_val,\r\n)\r\n\r\nmodel = NER_Model(\r\n    hyperparams=hyperparams,\r\n    model_parameters=model_parameters,\r\n    dataset_infos=dataset_infos,\r\n    extra_infos=extra_infos,\r\n)\r\n\r\ntrainer.fit(model)\r\ntrainer.test(model)\r\n\r\n\r\n#### Performing inference after training\r\nmodel.predict(\"I love Brazilian beaches\")\r\nResult:\r\n[['O',\r\n  'O',\r\n  'O',\r\n  'O',\r\n  'O',\r\n  'Tag1',\r\n  'Tag2,\r\n  'O',\r\n  'O']]\r\n\r\n#### Saving **State Dict** and **Checkpoint**\r\nmodel_path_dict = \"model.pt\"\r\nmodel_path_checkpoint = \"model.ckpt\"\r\ntorch.save(model.state_dict(), model_path_dict)\r\ntrainer.save_checkpoint(model_path_checkpoint)\r\n\r\n\r\n#### Testing lightning documentation sugestion (Wrong Result!)\r\ndevice_cpu = torch.device('cpu') \r\nmodel = NER_Model.load_from_checkpoint(\r\n    checkpoint_path = model_path_checkpoint,\r\n    map_location=device,\r\n    hyperparams=hyperparams,\r\n    model_parameters=model_parameters,\r\n    dataset_infos=dataset_infos,\r\n    extra_infos=extra_infos,    \r\n)\r\nmodel.predict(\"I love Brazilian beaches\")\r\nResult:\r\n[['Tag5',\r\n  'Tag2',\r\n  'O',\r\n  'Tag4',\r\n  'Tag4',\r\n  'Tag3',\r\n  'Tag1,\r\n  'Tag5',\r\n  'Tag4',]]\r\n\r\n\r\n\r\n#### Testing loading the checkpoint (Wrong Result!)\r\n\r\nmodel = NER_Model(\r\n    hyperparams=hyperparams,\r\n    model_parameters=model_parameters,\r\n    dataset_infos=dataset_infos,\r\n    extra_infos=extra_infos,    \r\n)\r\nstorage = 'cpu'\r\ncheckpoint = torch.load(model_path_checkpoint, map_location=lambda storage, loc: storage)\r\nmodel.load_state_dict(checkpoint['state_dict'])\r\n\r\nmodel.predict(\"I love Brazilian beaches\")\r\nResult:\r\n[['Tag5',\r\n  'Tag2',\r\n  'O',\r\n  'Tag4',\r\n  'Tag4',\r\n  'Tag3',\r\n  'Tag1,\r\n  'Tag5',\r\n  'Tag4',]]\r\n\r\n\r\n#### Testing loading directly the state dict (Wrong Result!)\r\nmodel = NER_Model(\r\n    hyperparams=hyperparams,\r\n    model_parameters=model_parameters,\r\n    dataset_infos=dataset_infos,\r\n    extra_infos=extra_infos,    \r\n)\r\n\r\ndevice_cpu = torch.device('cpu')\r\nmodel.load_state_dict(torch.load(model_path_dict,map_location=device_cpu)) \r\nmodel.eval()\r\n\r\nmodel.predict(\"I love Brazilian beaches\")\r\nResult:\r\n[['Tag5',\r\n  'Tag2',\r\n  'O',\r\n  'Tag4',\r\n  'Tag4',\r\n  'Tag3',\r\n  'Tag1,\r\n  'Tag5',\r\n  'Tag4,]]\r\n```\r\n#### Conclusion\r\nBefore saving and loading weights somewhere it works well (as expected).\r\n\r\nAfter loading, in all of the tried strategies  it does not works as expected. The answer is wrong. \r\n\r\n#### What's your environment?\r\nGoogle Colab\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6157/comments",
    "author": "math-sasso",
    "comments": [
      {
        "user": "angadkalra",
        "created_at": "2021-02-23T20:26:05Z",
        "body": "Can you please reformat your post to use the proper code inserts. More readable that way. "
      },
      {
        "user": "math-sasso",
        "created_at": "2021-02-24T11:40:02Z",
        "body": "Yes. I did it. But I am trying to keep all the lightning class because the problem could be somewhere inside it."
      }
    ]
  },
  {
    "number": 6104,
    "title": "Early stopping on custom metric without validation_step",
    "created_at": "2021-02-20T16:19:44Z",
    "closed_at": "2021-02-23T18:18:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6104",
    "body": "#### What is your question?\r\n\r\nI have a metric that I can only define using every predictions on the validation split, so I cannot use `validation_step` since it only operates on batches of data.\r\nI have a callback that computes and log this metric in `on_train_epoch_end`. \r\nI am not executing the validation loop because it's useless in my case.\r\nMy question is: How can I properly use the EarlyStopping callback ? (Same question for ModelCheckpoint)\r\n\r\n#### What have you tried?\r\nI have tried manually calling `pl_module.on_validation_epoch_end()` in my callback but it doesn't seem to work because EarlyStopping never stops the model even though the patience should have dropped to 0.\r\n\r\n#### What's your environment?\r\n\r\n - OS: Kubuntu 20.04\r\n - Packaging: pip\r\n - Version: 1.1.4\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6104/comments",
    "author": "Inspirateur",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-02-21T00:37:08Z",
        "body": "If I understood you correctly, you just need to make sure to create your instance as:\r\n\r\n`EarlyStopping(monitor=\"your_metric\")`\r\n\r\nAnd then, in your LightningModule's `on_validation_epoch_end` do `self.log(\"your_metric\", value)`"
      },
      {
        "user": "Inspirateur",
        "created_at": "2021-02-23T18:18:41Z",
        "body": "Seems like that works, thank you.\r\nI just had to define an empty `validation_step` method in my lightning module so the fake validation would be quick."
      }
    ]
  },
  {
    "number": 6020,
    "title": "MKL-DNN: Is there a way to enable mkldnn as device type?",
    "created_at": "2021-02-16T21:24:24Z",
    "closed_at": "2021-03-28T18:54:45Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6020",
    "body": "Is there a way to use mkldnn as a device type in PyTorch Lightening?\r\n\r\nI have no GPUs at this moment but Intel provides Math Kernel Library that slightly increase performance of PyTorch. In pure PyTorch, I would do something like this (the following snippet helps to compare performance boost):\r\n\r\n```python\r\nimport torch\r\nprint(f'Pytorch version : {torch.__version__}')\r\nprint(*torch.__config__.show().split(\"\\n\"), sep=\"\\n\")\r\n\r\nfrom torchvision import models\r\nfrom torch.utils import mkldnn as mkldnn_utils\r\nimport time\r\n\r\ndef forward(net, use_mkldnn=False, iteration=1, batch_size=10, weight_cache = False):\r\n  net.eval()\r\n  batch = torch.rand(batch_size, 3, 512, 512)\r\n  if use_mkldnn:\r\n    net = mkldnn_utils.to_mkldnn(net)\r\n    batch = batch.to_mkldnn()\r\n    if weight_cache:\r\n        # using weight cache which will reduce weight reorder\r\n        fname = 'test.script.pt'\r\n        traced = torch.jit.trace(net, batch, check_trace=False)\r\n        script = traced.save(fname)\r\n        net = torch.jit.load(fname)\r\n\r\n  start_time = time.time()\r\n  for i in range(iteration):\r\n      with torch.no_grad():\r\n          net(batch)\r\n  return time.time() - start_time\r\n\r\nnet = models.resnet18(False)\r\niter_cnt = 100\r\nbatch_size = 1\r\nno_mkldnn   = forward(net, False, iter_cnt, batch_size)\r\nwith_mkldnn = forward(net, True,  iter_cnt, batch_size)\r\nwith_mkldnn_cache = forward(net, True,  iter_cnt, batch_size, True)\r\n\r\nprint(f\"time-normal: {no_mkldnn:.4f}s\")\r\nprint(f\"time-mkldnn: {with_mkldnn:.4f}s\")\r\nprint(f\"time-mkldnn with weight cache: {with_mkldnn_cache:.4f}s\")\r\nprint(f\"mkldnn is {no_mkldnn/with_mkldnn:.2f}x faster!\")\r\nprint(f\"mkldnn using weight cache is {no_mkldnn/with_mkldnn_cache:.2f}x faster!\")\r\n```\r\n\r\nSo is there a way lightening way to enable MKLDNN?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6020/comments",
    "author": "roma-glushko",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-02-17T01:28:08Z",
        "body": "Not currently, maybe in the future 👀 \r\n\r\ncc: @edenafek @tchaton "
      },
      {
        "user": "tchaton",
        "created_at": "2021-02-18T17:16:05Z",
        "body": "Hey @roma-glushko,\r\n\r\nCould you provide the output of \r\n```\r\nprint(f\"time-normal: {no_mkldnn:.4f}s\")\r\nprint(f\"time-mkldnn: {with_mkldnn:.4f}s\")\r\nprint(f\"time-mkldnn with weight cache: {with_mkldnn_cache:.4f}s\")\r\nprint(f\"mkldnn is {no_mkldnn/with_mkldnn:.2f}x faster!\")\r\nprint(f\"mkldnn using weight cache is {no_mkldnn/with_mkldnn_cache:.2f}x faster!\")\r\n```\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "roma-glushko",
        "created_at": "2021-02-19T10:52:15Z",
        "body": "@tchaton here you go (MacBook Pro 2016 with 2,3 GHz 8-Core Intel Core i9):\r\n\r\n```\r\ntime-normal: 28.5844s\r\ntime-mkldnn: 19.3872s\r\ntime-mkldnn with weight cache: 19.5581s\r\nmkldnn is 1.47x faster!\r\nmkldnn using weight cache is 1.46x faster!\r\n```\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-03-21T16:07:56Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "vatai",
        "created_at": "2021-06-09T03:20:40Z",
        "body": "Just to clarify, If I do the `to_mkldnn` magic in `__init__()` or `setup()` etc. manually, will PL mess with the device or where it is stored?"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-06-09T13:02:12Z",
        "body": "Gave this a try. Here is a LightningModule with dummy training code and inference with predict step that converts the model to mkldnn and also the batches. Seems to work fine. \r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom torch.utils import mkldnn as mkldnn_utils\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def predict_step(self, batch, batch_idx, dataloader_idx):\r\n        batch = batch.to_mkldnn()\r\n        return self(batch)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n\r\n    model = mkldnn_utils.to_mkldnn(model)\r\n    assert model.device == torch.device(\"cpu\")\r\n    predictions = trainer.predict(model, dataloaders=test_data)\r\n    print(predictions)  # prints mkldnn output tensors!\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n\r\n```\r\n\r\nA simple print out tells me the predictions are mkldnn tensors"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-06-09T13:06:24Z",
        "body": "> Just to clarify, If I do the to_mkldnn magic in __init__() or setup() etc. manually, will PL mess with the device or where it is stored?\r\n\r\nIt should not but I wouldn't do it in the constructor, because in training we rely on the model to be a full `nn.Module` with weights and all. If you do it there your LightningModule will not be training and hardware agnostic. "
      }
    ]
  },
  {
    "number": 5773,
    "title": "Error while using distributed_backed = \"ddp\"",
    "created_at": "2021-02-03T21:50:30Z",
    "closed_at": "2021-02-05T19:08:33Z",
    "labels": [
      "bug",
      "question",
      "distributed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5773",
    "body": "My code works perfectly fine with distributed_backend='dp', but fails when I use distributed_backend='ddp' with the following error:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/scratch/nvarshn2/explore/test_ddp.py\", line 89, in <module>\r\n        trainer.fit(model, train_data, val_data)\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 510, in fit\r\n        results = self.accelerator_backend.train()\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py\", line 158, in train\r\n        results = self.ddp_train(process_idx=self.task_idx, model=model)\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py\", line 307, in ddp_train\r\n        results = self.train_or_test()\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 74, in train_or_test\r\n        results = self.trainer.train()\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in train\r\n        self.train_loop.run_training_epoch()\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 541, in run_training_epoch\r\n        for batch_idx, (batch, is_last_batch) in train_dataloader:\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/site-packages/pytorch_lightning/profiler/profilers.py\", line 85, in profile_iterable\r\n        value = next(iterator)\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 45, in _with_is_last\r\n        it = iter(iterable)\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 352, in __iter__\r\n        return self._get_iterator()\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 294, in _get_iterator\r\n        return _MultiProcessingDataLoaderIter(self)\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 801, in __init__\r\n        w.start()\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/multiprocessing/process.py\", line 105, in start\r\n        self._popen = self._Popen(self)\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\r\n        return _default_context.get_context().Process._Popen(process_obj)\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\r\n        return Popen(process_obj)\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\r\n        self._launch(process_obj)\r\n      File \"/home/nvarshn2/.conda/envs/pytorch_lightning_with_deepseed_env/lib/python3.6/multiprocessing/popen_fork.py\", line 66, in _launch\r\n        self.pid = os.fork()\r\n    OSError: [Errno 12] Cannot allocate memory\r\nCode:\r\n\r\n    import os\r\n    import torch\r\n    from torch.utils.data import Dataset\r\n    from pytorch_lightning import LightningModule, Trainer\r\n\r\n    class RandomDataset(Dataset):\r\n        def __init__(self, size, length):\r\n            self.len = length\r\n            self.data = torch.randn(length, size)\r\n\r\n        def __getitem__(self, index):\r\n            return self.data[index]\r\n\r\n        def __len__(self):\r\n            return self.len\r\n\r\n\r\n    class BoringModel(LightningModule):\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.layer = torch.nn.Linear(32, 2)\r\n\r\n        def forward(self, x):\r\n            return self.layer(x)\r\n\r\n        def loss(self, batch, prediction):\r\n            # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n            return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n        def step(self, x):\r\n            x = self.layer(x)\r\n            out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n            return out\r\n\r\n        def training_step(self, batch, batch_idx):\r\n            output = self.layer(batch)\r\n            loss = self.loss(batch, output)\r\n            return {\"loss\": loss}\r\n\r\n        def training_step_end(self, training_step_outputs):\r\n            return training_step_outputs\r\n\r\n        def training_epoch_end(self, outputs) -> None:\r\n            torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n        def validation_step(self, batch, batch_idx):\r\n            output = self.layer(batch)\r\n            loss = self.loss(batch, output)\r\n            return {\"x\": loss}\r\n\r\n        def validation_epoch_end(self, outputs) -> None:\r\n            torch.stack([x['x'] for x in outputs]).mean()\r\n\r\n        def test_step(self, batch, batch_idx):\r\n            output = self.layer(batch)\r\n            loss = self.loss(batch, output)\r\n            return {\"y\": loss}\r\n\r\n        def test_epoch_end(self, outputs) -> None:\r\n            torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n        def configure_optimizers(self):\r\n            optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n            return [optimizer], [lr_scheduler]\r\n\r\n    if __name__ == '__main__':\r\n        train_data = torch.utils.data.DataLoader(RandomDataset(32, 64), num_workers=8)\r\n        val_data = torch.utils.data.DataLoader(RandomDataset(32, 64), num_workers=8)\r\n        model = BoringModel()\r\n        trainer = Trainer(\r\n            limit_train_batches=1,\r\n            limit_val_batches=1,\r\n            max_epochs=1,\r\n            gpus=-1,\r\n            distributed_backend=\"ddp\",\r\n        )\r\n        trainer.fit(model, train_data, val_data)\r\n\r\n\r\nNote: I am using 4 gpus and a single machine\r\nWhat could be the reason behind this?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5773/comments",
    "author": "nrjvarshney",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-02-04T23:07:27Z",
        "body": "It seems to be related to num workers being to high. You have 4 gpus process, each spawning 8 workers for data loading. I think it is why it is breaking."
      },
      {
        "user": "edenlightning",
        "created_at": "2021-02-05T19:08:33Z",
        "body": "@nrjvarshney please let us know if that resolves the issue! Feel free to open otherwise."
      }
    ]
  },
  {
    "number": 5770,
    "title": "What is the best/effiecient approach for utilizing and working on a text dataset?",
    "created_at": "2021-02-03T17:08:23Z",
    "closed_at": "2021-02-09T06:09:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5770",
    "body": "Hi,\r\nSo, I've been researching different approaches/libraries online that work for text dataset - torchtext, PyTorch-NLP, pytext, AllenNLP but most of them are designed to work individualy or on a single machine. \r\n\r\nIf I am working on text dataset for tasks like seq2seq; what is the most efficient data sampling/generation approach that can utilize all PytorchLightning features? For example: parallel training and 16-bit apex precision. \r\n\r\nIdeally what I'd like to do is be able to - create batches, vocabulary, somehow sort the dataset and batch them in such a way that I don't need to truncate the text item and don't need to do a lot of padding like torchtext does with it's BucketIterator. \r\n\r\nI've had a talk with some community members in the past where I was offered smart ways to use torchtext library in Lightning but I'd like to reduce my dependency on that legacy code until it's essential. For eg: torchtext is phasing out most of its iterators and Field and Dataset classes.  \r\n\r\nSo it really messes stuff up when all other libraries update but the old code is just made legacy.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5770/comments",
    "author": "ankitvad",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-02-08T16:00:22Z",
        "body": "Hi! Lightning does not provide any custom batch samplers or other abstractions to solve the specific issues of variable length data. However, you can use any of those you mentioned in a LightningDataModule for your particular needs.\r\n\r\n> what is the most efficient data sampling/generation approach that can utilize all PytorchLightning features?\r\n\r\nThis is too broad of a question to have an answer, but in general, the most efficient approach for PyTorch will also be the most efficient approach for Lightning."
      },
      {
        "user": "ankitvad",
        "created_at": "2021-02-09T06:09:33Z",
        "body": "That makes sense! Thanks!"
      }
    ]
  },
  {
    "number": 5705,
    "title": "Why do some metrics require `num_classes=1` for binary classification?",
    "created_at": "2021-01-29T13:16:35Z",
    "closed_at": "2021-02-04T23:34:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5705",
    "body": "## ❓ Why do some metrics require `num_classes=1` for binary classification?\r\n\r\n#### What is your question?\r\n\r\nWhy do some metrics require the argument `num_classes=1` for binary classification (and some don't) to give the correct results?\r\n\r\nI find it rather unintuitively to calculate Recall/Precision/F1 with the argument `num_classes=1` for a binary classification, whereas e.g. ConfusionMatrix requires `num_classes=2` in the same situation.\r\n\r\nFurthermore, using Recall/Precision/F1 with `num_classes=2` for a binary classification gives wrong results - so this also might be considered a bug-report.\r\n\r\nIt took me quite some time to figure out, why calculated metrics are different from what I calculated by hand from the confusion matrix.\r\n\r\n#### Code\r\n\r\n```python\r\nimport torch\r\nfrom pytorch_lightning import metrics\r\n\r\n# example data\r\npreds = [0] * 200 + [1] * 30 + [0] * 10 + [1] * 20\r\ntargets = [0] * 200 + [1] * 30 + [1] * 10 + [0] * 20\r\n\r\npreds = torch.tensor(preds)\r\ntargets = torch.tensor(targets)\r\n\r\n# define method for printing metrics\r\n\r\n\r\ndef _print_some_metrics(preds, targets, num_classes):\r\n    precision = metrics.classification.Precision(num_classes=num_classes)\r\n    recall = metrics.classification.Recall(num_classes=num_classes)\r\n    f1 = metrics.classification.F1(num_classes=num_classes)\r\n\r\n    accuracy = metrics.classification.Accuracy()\r\n    avg_precision = metrics.classification.AveragePrecision(\r\n        num_classes=1)\r\n    confusion_matrix = metrics.ConfusionMatrix(num_classes=2)\r\n\r\n    # print results\r\n    print(\"Precision:\\n{}\\n\".format(precision(preds, targets)))\r\n    print(\"Recall:\\n{}\\n\".format(recall(preds, targets)))\r\n    print(\"F1:\\n{}\\n\".format(f1(preds, targets)))\r\n\r\n    print(\"AVG Precision:\\n{}\\n\".format(avg_precision(preds, targets)))\r\n    print(\"Accuracy:\\n{}\\n\".format(accuracy(preds, targets)))\r\n    print(\"ConfMat:\\n{}\\n\".format(confusion_matrix(preds, targets)))\r\n\r\n\r\n_print_some_metrics(preds, targets, num_classes=1)\r\n_print_some_metrics(preds, targets, num_classes=2)\r\n```\r\n\r\nResults in\r\n\r\n> $ _print_some_metrics(preds, targets, num_classes=1)\r\n> Precision:\r\n> 0.6000000238418579\r\n> \r\n> Recall:\r\n> 0.75\r\n> \r\n> F1:\r\n> 0.6666666865348816\r\n> \r\n> AVG Precision:\r\n> 0.48846155405044556\r\n> \r\n> Accuracy:\r\n> 0.8846153616905212\r\n> \r\n> ConfMat:\r\n> tensor([[200.,  20.],\r\n>         [ 10.,  30.]])\r\n> \r\n> \r\n> $ _print_some_metrics(preds, targets, num_classes=2)\r\n> Precision:\r\n> 0.8846153616905212\r\n> \r\n> Recall:\r\n> 0.8846153616905212\r\n> \r\n> F1:\r\n> 0.8846153616905212\r\n> \r\n> AVG Precision:\r\n> 0.48846155405044556\r\n> \r\n> Accuracy:\r\n> 0.8846153616905212\r\n> \r\n> ConfMat:\r\n> tensor([[200.,  20.],\r\n>         [ 10.,  30.]])\r\n\r\nAs one can see, Precision/Recall/F1 give different (wrong) results when setting `num_classes=2` in a binary classification.\r\nAveragePrecision doesn't even work with the binary usecase when setting `num_classes=2` whereas ConfusionMatrix doesn't work when setting `num_classes=1`.\r\n\r\nI wonder if there is a specific reason why one would set `num_classes=1` in a binary classification (where actually 2 classes exist).\r\n\r\nWouldn't it be more straightforward to set `num_classes=2` for binary classification for all metrics?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5705/comments",
    "author": "kapsner",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2021-01-30T18:18:55Z",
        "body": "So I can try to clarify this a bit:\r\n* we are aware of this, and many of these metrics have already been changed on the `release/1.2-dev` branch. For example will your example with `precision = metrics.classification.Precision(num_classes=2)` give an error because input tensors are clearly binary but the class was initialized with `num_classes=2`.\r\n* confusion matrix needs `num_classes=2` for binary problems, because we to know the size of the tensor that we should allocate \r\n* the difference between specifying `num_classes=1` or `num_classes=2` really comes down to if you want to calculate the score on only the positive class (this is probably what you want) or both classes (which really does not make sense for binary problems, because many of the scores reduce to the same then).\r\n"
      },
      {
        "user": "kapsner",
        "created_at": "2021-01-31T09:29:11Z",
        "body": "@SkafteNicki thx for your answer and explanation\r\n\r\n> the difference between specifying num_classes=1 or num_classes=2 really comes down to if you want to calculate the score on only the positive class (this is probably what you want) or both classes (which really does not make sense for binary problems, because many of the scores reduce to the same then).\r\n\r\nI agree with you, that in the binary case one would like to calculate the score on the positive class.\r\n\r\nI wonder, if there are any use cases, where the calculation using your defined `num_classes=2` is really wanted, or if any case with 2 classes (= binary classification) would actually only use `num_classes=1` and the next valid application would be a 3-class classification using `num_classes=3`.\r\n\r\n* 2 classes: `num_classes=1`\r\n* 3 classes: `num_classes=3`\r\n* 4 classes: `num_classes=4`\r\n* ..."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2021-02-01T13:33:42Z",
        "body": "so the usecase could be that the user wants set `average=None` to get the score for each class. Again in that case we kind of need to now the size of the tensor that we should allocate (the metric states)."
      },
      {
        "user": "kapsner",
        "created_at": "2021-02-02T20:21:45Z",
        "body": "Ok, thx for your explanations. "
      }
    ]
  },
  {
    "number": 5674,
    "title": "multiple processes running all tasks after trainer.fit(accelerator=\"ddp\")",
    "created_at": "2021-01-27T08:35:09Z",
    "closed_at": "2021-02-09T22:57:42Z",
    "labels": [
      "question",
      "distributed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5674",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nWhen training with ddp the script calls multiple python scripts to run the training. This causes an issue when I use the same python script to do other stuff after Im done with training.\r\n\r\nWhat is best practice here? My only solution so far would be to condition on `os.environ[\"LOCAL_RANK\"]=='0'` when doing stuff later on. This feels a bit hacky - is there a better way to do this?\r\n\r\n#### Code\r\nRunning the example code with 2 gpus:\r\n```\r\n\r\nimport os\r\nimport torch\r\nfrom torch import nn\r\nimport torch.nn.functional as F\r\nfrom torchvision.datasets import MNIST\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchvision import transforms\r\nimport pytorch_lightning as pl\r\n\r\nclass LitAutoEncoder(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\r\n        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\r\n\r\n    def forward(self, x):\r\n        # in lightning, forward defines the prediction/inference actions\r\n        embedding = self.encoder(x)\r\n        return embedding\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defined the train loop. It is independent of forward\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = F.mse_loss(x_hat, x)\r\n        self.log('train_loss', loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\ndataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\r\ntrain, val = random_split(dataset, [55000, 5000])\r\n\r\nautoencoder = LitAutoEncoder()\r\ntrainer = pl.Trainer(\r\n    max_epochs=1, \r\n    overfit_batches = 0.05,\r\n    gpus =-1, \r\n    accelerator=\"ddp\")\r\ntrainer.fit(autoencoder, DataLoader(train), DataLoader(val))\r\n\r\nprint(f\"This will print twice {os.environ['LOCAL_RANK']}\")\r\n\r\nif os.environ[\"LOCAL_RANK\"] =='0':\r\n    print(f\"This will print once {os.environ['LOCAL_RANK']}\")\r\n\r\n```\r\n\r\n#### What's your environment?\r\n\r\n- ubuntu\r\n- pytorch 1.6.0\r\n- pytorch-lightning 1.1.6\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5674/comments",
    "author": "simeneide",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-02-05T21:48:00Z",
        "body": "> When training with ddp the script calls multiple python scripts to run the training. This causes an issue when I use the same python script to do other stuff after Im done with training.\r\n\r\nIt's by design. The best practice here is to have the same control flow in all processes, the only difference being the data. \r\n\r\n> os.environ[\"LOCAL_RANK\"]=='0' when doing stuff later on. This feels a bit hacky - is there a better way to do this?\r\n\r\nYou can also use `trainer.global_rank` or `trainer.local_rank` if you consider that less hacky? It's afaik the only way of telling in which process we are. It should be fine to let all processes except 0 die, as long as training has finished."
      }
    ]
  },
  {
    "number": 5636,
    "title": "Understanding accumulate_grad_batches  parameter? ",
    "created_at": "2021-01-24T10:10:44Z",
    "closed_at": "2021-01-24T18:19:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5636",
    "body": "I am very new to PL. As far as I understand **accumulate_grad_batches** works similar to  **'gradient_accumulation_steps'** , where the main task is to increase the effective batch size.  But I do not see any change in training epoch step count when increasing the  **accumulate_grad_batches** parameters.\r\n\r\nLet's say, I have a dataset of 1000 examples and my batch_size is one and I only use a single GPU. So in this case, if I use the value 2  for the **accumulate_grad_batches**,  the number of steps for an epoch should be shown as 500 (logger). But I still see 1000.\r\n\r\nIs it a bug or PL doesn't divide the number of steps when showing in the log?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5636/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-01-24T13:16:39Z",
        "body": "total step count will remain the same since it refers to total batches, but internally `optimizer/scheduler.step` is updated accordingly. You can check `self.global_step` with and without accumulation."
      },
      {
        "user": "tchaton",
        "created_at": "2021-01-24T18:19:18Z",
        "body": "Hey @shamanez,\r\n\r\nI believe @rohitgr7 properly answered your question.\r\nI will close this issue. Feel free to re-open it if something is missing.\r\n\r\nBest,\r\nT.C\r\n"
      },
      {
        "user": "shamanez",
        "created_at": "2021-01-24T20:35:29Z",
        "body": "Thanks a lot.sorted!"
      }
    ]
  },
  {
    "number": 5597,
    "title": "outputs of training_epoch_end for different configure_optimizers conditions ",
    "created_at": "2021-01-21T04:54:50Z",
    "closed_at": "2021-01-22T03:26:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5597",
    "body": "> **Condition One:**\r\n> when I write optimizer as follows:\r\n\r\n```\r\ndef configure_optimizers(self):\r\n  return [disOptim,genOptim],[] \r\n```\r\n\r\n> I can simply write the training_epoch_end as follows: \r\n\r\n```\r\ndef training_epoch_end(self,outputs):\r\n  sum_loss_D_real=torch.stack([x['D_loss_real'] for x in outputs[0]]).sum()\r\n```\r\n\r\n> **Condition Two:**\r\n> However when I write the optimizer as follows:\r\n\r\n```\r\ndef configure_optimizers(self):\r\n  return ({'optimizer':disOptim,'frequency':2},{'optimizer':genOptim,'frequency':1})\r\n```\r\n\r\n> I need to write the training_epoch_end as follows:\r\n\r\n```\r\ndef training_epoch_end(self,outputs):\r\n  mean_d_loss=torch.stack([outputs[i]['d_loss'] for i in range(len(outputs)) if ((i+1)%(self.hparams.n_critic+1))]).mean()\r\n```\r\n\r\n> Is there any way that I can write the optimizer the same as condition two and the training_epoch_end as condition one?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5597/comments",
    "author": "Bajo1994",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-01-21T17:52:34Z",
        "body": "I don't think it's possible since in both the cases the behaviour of `training_step` will be different since you are passing optimizer frequencies in the second case, so expected outputs will be in different format.\r\n\r\nbut you can still simply it a bit. Try:\r\n```python\r\nx=torch.stack([outputs[i].get('d_loss', torch.tensor(float('nan'))) for i in range(len(outputs))])\r\nmean_d_loss = x[x==x].mean()\r\n```"
      },
      {
        "user": "Bajo1994",
        "created_at": "2021-01-22T03:26:25Z",
        "body": "Thanks for your help!"
      }
    ]
  },
  {
    "number": 5585,
    "title": "how to set find_unused_parameters=True?",
    "created_at": "2021-01-20T02:32:43Z",
    "closed_at": "2021-02-04T23:36:30Z",
    "labels": [
      "help wanted",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5585",
    "body": "## 🐛 Bug\r\n\r\n```\r\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your \r\nmodule has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the \r\nkeyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` \r\nfunction outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel \r\nmodule wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss \r\nfunction and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\r\n```\r\n\r\n\r\nwhen I use pt 1.0.8, my model is ok, but when I switch to 1.1.4, it throws this error.  It seems 1.0.8 enable unused parameters by default, but 1.1.4 not.  How to solve this problem.\r\n\r\nI think switch `find_unused_parameters=True` by default to `False` is a breaking change, but in docs, it doesn't mention, yet no clear instructions to set to `True` .",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5585/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "siyuanfeng-tri",
        "created_at": "2021-01-21T12:59:20Z",
        "body": "i ran into the same issue. any ideas? "
      },
      {
        "user": "xiadingZ",
        "created_at": "2021-01-22T02:07:59Z",
        "body": "```\r\n    class MyDDP(DDPPlugin):\r\n\r\n        def configure_ddp(self, model, device_ids):\r\n            model = LightningDistributedDataParallel(model, device_ids, find_unused_parameters=True)\r\n            return model\r\n\r\n    my_ddp = MyDDP()\r\n\r\n    trainer = Trainer.from_argparse_args(hparams,\r\n                                         callbacks = [lr_logger, checkpoint_callback],\r\n                                         plugins = [my_ddp])\r\n```\r\nthis is my workaround. but hasn't tested whether it will influence my  model precision."
      }
    ]
  },
  {
    "number": 5554,
    "title": "validation_epoch_end or on_train_epoch_end receive extra arguments/data",
    "created_at": "2021-01-18T09:30:17Z",
    "closed_at": "2021-02-09T23:35:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5554",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nBasically I need to validate my model after each epoch, but the labels information is a little tricky(mixture of list of tuple of int and string), that it could  not be included in DataLoader. \r\n\r\nhowever, `validation_epoch_end, validation_step, etc..`  only receives DataLoader output.\r\n\r\nIs there any solution? \r\n\r\nI actually dun want to add the raw data into module which make the code difficult to understand.\r\n\r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5554/comments",
    "author": "congchan",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-18T09:37:42Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      }
    ]
  },
  {
    "number": 5552,
    "title": "How to iterate over training set AGAIN on training epoch end?",
    "created_at": "2021-01-18T07:57:20Z",
    "closed_at": "2021-01-19T11:09:43Z",
    "labels": [
      "question",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5552",
    "body": "## ❓ Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nHow to iterate over training set AGAIN on training epoch end or on validation epoch start?\r\nI have a model that works as expected on MNIST, but for my unique data, val_loss<train_loss for all samples.\r\nI have no idea what causes this, and it is too suspicious to allow me to go on.\r\n\r\n*I want to do a validation step on training data, in eval mode*\r\n\r\nI hope it will ease my mind.\r\n\r\n#### Code\r\nWell, if I had code for how to correctly do this I wouldn't ask :)\r\n\r\n#### What have you tried?\r\nThis doesn't sound like a standard use case, not even sure that's supported.\r\n\r\n#### What's your environment?\r\n\r\n - OS: [e.g. iOS, Linux, Win] Win\r\n - Packaging [e.g. pip, conda] Pip\r\n - Version [e.g. 0.5.2.1] 1.1.4\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5552/comments",
    "author": "noamzilo",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-01-18T08:33:00Z",
        "body": "Hey @noamzilo,\r\n\r\nIf you have a `val_dataloader`, the entire dataset of validation will be used once the latest batch of your train_dataset would be reached.\r\n\r\nTherefore, you can compare your metrics at this point in epoch_end hooks.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-01-18T08:37:18Z",
        "body": "It is not an accident that PyTorch is in the name of PyTorchLightning :)) \r\nYou can just do it as you would in PyTorch:\r\n\r\n```python\r\nself.eval()\r\nfor idx, batch in enumerate(self.train_dataloader()):\r\n    # do what you have to do\r\n```\r\n\r\n"
      },
      {
        "user": "noamzilo",
        "created_at": "2021-01-19T11:09:43Z",
        "body": "> \r\n> \r\n> It is not an accident that PyTorch is in the name of PyTorchLightning :))\r\n> You can just do it as you would in PyTorch:\r\n> \r\n> ```python\r\n> self.eval()\r\n> for idx, batch in enumerate(self.train_dataloader()):\r\n>     # do what you have to do\r\n> ```\r\n\r\nThanks :) \r\n\r\n"
      }
    ]
  },
  {
    "number": 5550,
    "title": "on_train_epoch_end vs training_epoch_end",
    "created_at": "2021-01-17T21:37:32Z",
    "closed_at": "2021-01-18T16:55:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5550",
    "body": "## ❓ Questions and Help\r\n\r\nWhat is the difference between on_train_epoch_end and training_epoch_end? For what applications should we use each?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5550/comments",
    "author": "Bajo1994",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-01-18T08:45:53Z",
        "body": "Hey @Bajo1994,\r\n\r\n* `training_epoch_end` will be used for the user to aggregate the outputs from training_step at the end of an epoch.\r\n\r\nExample.\r\n```\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n```\r\n\r\n* on_train_epoch_end is a hook. It would be used to add extra logic to control the behaviour of the model.\r\nBut it is left to the user to choice how he wants to use it :)\r\n\r\nI hope it helps !\r\n\r\nBest,\r\nT.C \r\n\r\n"
      },
      {
        "user": "Bajo1994",
        "created_at": "2021-01-18T14:56:47Z",
        "body": "Many thanks for a thorough response."
      }
    ]
  },
  {
    "number": 5513,
    "title": "OOM using EfficientNet B3",
    "created_at": "2021-01-14T15:45:39Z",
    "closed_at": "2021-02-04T23:37:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5513",
    "body": "Hi,\r\nI am currently doing the ISIC 2019 Challenge using pretrained EfficientNet B3 on Kaggle with a 16GB GPU. I instantly run into out of memory problems, tried reducing the batchsize and 16 bit precision, doesn't help. I don't know how to debug this and can't find anything special in my code. Here's the important parts of the model:\r\n`\r\n       class LightModel(pl.LightningModule):\r\n\r\n     def __init__(self, classes_named, id_train, id_val, class_weights, header_line, hparams):\r\n            self.model= EfficientNet.from_pretrained('efficientnet-b3', num_classes=8)    \r\n\r\n    def forward(self, x):\r\n        b = self.model(x)\r\n        return b\r\n \r\n    def prepare_data(self):\r\n        # This is called at the start of training\r\n        pass\r\n \r\n    def train_dataloader(self):\r\n        # Simply define a pytorch dataloader here that will take care of batching. Note it works well with dictionnaries !\r\n        train_dl = DataLoader(self.trainset, batch_size=self.hparams.batch_size, num_workers = 4, shuffle=True,pin_memory=True,\r\n                        )\r\n        return train_dl\r\n \r\n    def val_dataloader(self):\r\n        # Same but for validation. Pytorch lightning allows multiple validation dataloaders hence why I return a list.\r\n        val_dl = DataLoader(self.valset, batch_size=self.hparams.batch_size, shuffle=False,num_workers = 4,pin_memory=True,\r\n                                  ) \r\n        return val_dl\r\n    \r\n    def test_dataloader(self):\r\n        test_dl = DataLoader(self.testset, batch_size=self.hparams.batch_size, shuffle=False,num_workers = 4, pin_memory=True,\r\n                                  ) \r\n        return test_dl\r\n \r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr) # self.model.parameters or self.parameters\r\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=10 * self.hparams.lr, \r\n                                                        epochs=self.hparams.epochs, steps_per_epoch=len(self.train_dataloader()))\r\n        return [optimizer], [scheduler]\r\n \r\n    def step(self, batch):\r\n        # return batch loss\r\n        x, y  = batch['image'], batch['target']\r\n        y_hat = self(x) \r\n        #criterion_ = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.hparams.pos_weight))\r\n        loss  = self.criterion_(y_hat, y)\r\n        return loss, y, y_hat\r\n \r\n    def training_step(self, batch, batch_idx):\r\n        # hardware agnostic training\r\n        loss, y, y_hat = self.step(batch)\r\n        pred = y_hat.max(1, keepdim=True)[1]\r\n        acc = pred.eq(y.view_as(pred)).sum().item()/len(batch['image'])\r\n        self.log('train_loss', loss, on_epoch=True, on_step=False, logger=True)\r\n        self.log('train_acc', acc, on_epoch=True, on_step=False, logger=True)\r\n        return {'loss': loss, 'acc': acc}\r\n        \r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        loss, y, y_hat = self.step(batch)\r\n        return {'val_loss': loss.detach(),\r\n                'y': y.detach(), 'y_hat': y_hat.detach()}\r\n    \r\n    def test_step(self, batch, batch_idx):\r\n        x, _ = batch['image'], batch['target']\r\n        y_hat = self(x)\r\n        return {'y_hat': y_hat}\r\n \r\n    def validation_epoch_end(self, outputs):\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        y = torch.cat([x['y'] for x in outputs])\r\n        y_hat = torch.cat([x['y_hat'] for x in outputs])\r\n        #auc = pl.metrics.functional.classification.multiclass_auroc(pred=y_hat, target=y) if y.float().mean() > 0 else 0.5 # skip sanity check\r\n        #auc = 0.5\r\n        sm = torch.nn.Softmax(dim=1)\r\n        y_p = sm(y_hat)\r\n        prediction = y_hat.max(1, keepdim=True)[1]\r\n        acc = prediction.eq(y.view_as(prediction)).sum().item()/list(y_hat.size())[0]\r\n\r\n        conf = self.conf(y_p, y)\r\n        weighted_acc = torch.diagonal(conf)/torch.sum(conf, 1)\r\n        w_b_acc = torch.sum(weighted_acc)/8\r\n        self.log('val_loss', avg_loss, logger=True)\r\n        self.log('val_recall', self.recall(y_p, y), logger=True)\r\n        self.log('w_b_acc', w_b_acc, logger=True)\r\n        print(conf)\r\n        print(f'Epoch {self.current_epoch}: loss: {avg_loss:.2f}, acc: {acc:.4f}, auc: {self.auc(y_p, y,num_classes=8):.4f}, recall: {self.recall(y_p, y):.4f}, w_acc: {w_b_acc:.4f}')\r\n    \r\n    def test_epoch_end(self, outputs):\r\n        y_hat = torch.cat([x['y_hat'] for x in outputs])\r\n        test_df = dict()\r\n        test_df[header_line[0]] = [x[:-4] for x in self.ids_val]\r\n        decision = torch.argmax(y_hat, dim=1)\r\n        y_out = pl.metrics.utils.to_onehot(decision, num_classes=9).cpu().detach().numpy()\r\n        for i in range(y_out.shape[1]):\r\n            test_df[header_line[1 + i]] = y_out[:, i]\r\n        df = pd.DataFrame(data=test_df)\r\n        df.to_csv('/kaggle/working/submission.csv', index=False)`",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5513/comments",
    "author": "finnBsch",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2021-01-14T19:22:25Z",
        "body": "Looks fine for me. Have you trained the same network without lightning successfully? Because I remember it being a quite large network, so it is likely that you run OOM here just because of that."
      },
      {
        "user": "Borda",
        "created_at": "2021-01-15T18:07:37Z",
        "body": "@SeanNaren could sharded help as you showed that it reduces memory usage...?"
      },
      {
        "user": "finnBsch",
        "created_at": "2021-01-15T21:34:31Z",
        "body": "@justusschock No I have not. However I've read a paper which managed to train the network with 11Gb of vram in a similar setup, can't access the code though."
      },
      {
        "user": "jspaezp",
        "created_at": "2021-01-20T19:03:03Z",
        "body": "How are you defining  `self.trainset`?? ... It could also be that the data loader is filling your memory, try setting the workers to 0 and removing the memory pinning ...\nLmk if it helps"
      }
    ]
  },
  {
    "number": 5471,
    "title": "How to gather results during inference in ddp",
    "created_at": "2021-01-11T22:00:14Z",
    "closed_at": "2021-01-11T22:03:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5471",
    "body": "## ❓ Questions and Help\r\nHi, I am using multiple gpus and ddp mode for model inference. I am wondering how to gather the results from all distributed processes and save them into one file in the test_epoch_end. My code looks like this:\r\n\r\n#### Code\r\n```python\r\nclass PLModel(pl.LightningModule):\r\n    def on_test_epoch_start(self):\r\n        self.text_predictions = []\r\n    \r\n    def test_step(self, batch, batch_idx):\r\n         pred_ids = model(batch)\r\n         pred_texts = ids_to_texts(pred_ids) # some function that converts predicted ids to texts\r\n         self.text_predictions.extend(pred_texts)\r\n\r\n    def test_epoch_end(self, outputs):\r\n        # here I want to know how to gather the text_predictions from all processes\r\n\r\n        # after gathering\r\n        df_text_predictions = pandas.DataFrame{'predictions': self.text_predictions}\r\n        df_text_predictions.to_csv('predictions.csv', index=False)\r\n\r\n\r\n```\r\n#### What have you tried?\r\nI am able to save the results into separate files in each distributed process. But I have to do post-processing to put all the files together. I want to know how to gather the results automatically so that I don't need to run another script for post-processing.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5471/comments",
    "author": "tomdzh",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-11T22:01:01Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      }
    ]
  },
  {
    "number": 5454,
    "title": "Validation step is ignored when using DataModule",
    "created_at": "2021-01-11T00:49:13Z",
    "closed_at": "2021-01-12T01:34:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5454",
    "body": "#### What is your question?\r\nHi, guys!\r\nI created my own DataModule and loading it to the trainer. However, it appears that the \"fit\" is skipping the validation step.\r\nHow can I ensure that the code runs through the validation step too?\r\n\r\n#### Code\r\n```\r\nclass DataModule(pl.LightningDataModule):\r\n    def __init__(self, batch_size=25, seed=0):\r\n    # def __init__(self, dataset, batch_size=25, seed=0):\r\n        super().__init__()\r\n        self.dataset = dataset\r\n        self.batch_size = batch_size\r\n        self.seed = seed\r\n        self.split = [801, 100, 100]\r\n        # self.transform = torchvision.transforms.ToTensor()\r\n\r\n    def setup(self, stage=None):\r\n        # train/valid/test split\r\n        # and assign to use in dataloaders via self\r\n        train_set, valid_set, test_set = torch.utils.data.random_split(self.dataset, self.split, generator=torch.Generator().manual_seed(self.seed))\r\n\r\n        if stage == 'fit' or stage is None:\r\n\r\n            self.train_set = train_set\r\n            self.valid_set = valid_set\r\n\r\n        if stage == 'test' or stage is None:\r\n            self.test_set = test_set\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\r\n\r\n    def valid_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.valid_set, batch_size=self.batch_size, shuffle=False)\r\n\r\n    def test_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False)\r\n\r\nclass LitReg(pl.LightningModule):\r\n    def __init__(self, in_dims, out_dims, lr=2e-4, max_dict={}):\r\n        super().__init__()\r\n        self.in_size = in_dims\r\n        self.out_size = out_dims\r\n        self.lr = lr\r\n        self.max_dict = max_dict\r\n\r\n        # model\r\n        self.model = nn.Sequential(\r\n            nn.Linear(self.in_size, self.in_size),\r\n\r\n            nn.LeakyReLU(0.02),\r\n\r\n            nn.Linear(self.in_size, self.out_size)\r\n        )\r\n\r\n        self.model.apply(self.weights_init)\r\n\r\n    def forward(self, data):\r\n        out = self.model(data)\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y, l_rate = batch\r\n        pred_y = self.model(x)\r\n        train_loss = F.mse_loss(pred_y, y) \r\n\r\n        self.log('train_loss', train_loss, prog_bar=True)\r\n        return train_loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        self._shared_eval(batch, batch_idx, 'val')\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        self._shared_eval(batch, batch_idx, 'test')\r\n\r\n    def _shared_eval(self, batch, batch_idx, prefix):\r\n        x, y, l_rate = batch\r\n        pred_y = self.model(x)\r\n\r\n        loss = F.mse_loss(pred_y, y) \r\n        self.log(f'{prefix}_loss', loss, prog_bar=True)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), self.lr)\r\n        return optimizer\r\n\r\n    def weights_init(self, m):\r\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n            torch.nn.init.constant_(m.bias, 0)\r\n        if isinstance(m, nn.Linear):\r\n            torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n            torch.nn.init.constant_(m.bias, 0)\r\n```\r\n#### What have you tried?\r\nPlacing breakpoints to debug in VSCode, but all in vain.\r\nAlso accessed both valid and test datasets and loaders. All looks set.\r\nWhat is working? If I load the data the following way.\r\n```\r\n    **train_loader = DataLoader(X_train, batch_size=args.batch_size)\r\n    val_loader = DataLoader(X_val, batch_size=args.batch_size)\r\n    test_loader = DataLoader(X_test, batch_size=args.batch_size)\r\n\r\n    trainer.fit(model, train_loader, val_loader)**\r\n```\r\n\r\n#### What's your environment?\r\n\r\n - OS: Win\r\n - Packaging pip\r\n - Version 1.1.3\r\n\r\nThank you for your attention!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5454/comments",
    "author": "ncuxomun",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-11T00:49:59Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "s-rog",
        "created_at": "2021-01-11T03:35:03Z",
        "body": "```\r\n    def validation_step(self, batch, batch_idx):\r\n        return self._shared_eval(batch, batch_idx, 'val')\r\n```\r\nYou forgot return!"
      },
      {
        "user": "ncuxomun",
        "created_at": "2021-01-11T17:20:23Z",
        "body": "> ```\r\n>     def validation_step(self, batch, batch_idx):\r\n>         return self._shared_eval(batch, batch_idx, 'val')\r\n> ```\r\n> \r\n> You forgot return!\r\n\r\nThanks for the remark! I corrected the mistake. But I still get the same output, i.e. the validation step is skipped. Is there anything else that I am missing?"
      },
      {
        "user": "s-rog",
        "created_at": "2021-01-12T00:27:09Z",
        "body": "Oh I see that you called self.log in _shared_eval, so the return shouldn't be necessary, apologies.\r\n\r\nDoes the script run with the validation progress bar (at the end of a training epoch)?"
      },
      {
        "user": "ncuxomun",
        "created_at": "2021-01-12T00:33:25Z",
        "body": "Thanks for re-opening it. I was looking your your earlier comment))\r\nYes, it is connected via the shared function here. I also tried without it, that is enabling both val and test function, and disabling the the shared one.\r\nNo, the script does not not run with the validation progress bar. It displays \"loss\" and \"train_loss\" only. Another observation is that what I did include \"return\" and \"fit by trainer.fit(model, datamodule=dm), the training took more epochs, ignoring the early stopping callback (which is honored when I don't work with datamodule)."
      },
      {
        "user": "s-rog",
        "created_at": "2021-01-12T00:45:23Z",
        "body": "Could you try initializing the dataset in datamodules setup instead of passing it in as an arg?\r\n\r\nAlso could you check the length of the datasets before and after splitting? I suspect validation is not getting any data."
      },
      {
        "user": "ncuxomun",
        "created_at": "2021-01-12T01:20:05Z",
        "body": "Cool, thanks for suggestions. Here's what I did:\r\n1. Dataset was moved into prepare_data, where it was initialized.\r\n2. Then I placed a breakpoint into the dataset to check its dimensions. This prompted the following warning, which I did not see before: \r\n_\"UserWarning: you defined a validation_step but have no val_dataloader. Skipping validation loop warnings.warn(*args, *kwargs)_\"\r\n3. Then, I renamed \"_valid_dataloader_\" into \"**val_dataloader**\", re-ran then case and all progress bar values were displayed as required. \r\nIt was my bad, I should have called that function precisely as the said in the docs. One thing that is surprising is that this \"skipping\" part was not a big deal when I was loading the train_dataloader and val_dataloader individually instead of datamodule. Do you know what could be the issue?\r\nPlease feel free to close the issue if you think so.\r\nOnce again, thanks a lot for responding and giving suggestions that helped resolve the issue."
      },
      {
        "user": "s-rog",
        "created_at": "2021-01-12T01:34:15Z",
        "body": "Ah I see, it was a simple typo. Regarding 3, that is intended behavior, glad it's working now and no problem!\r\n\r\n"
      }
    ]
  },
  {
    "number": 5289,
    "title": "Print weights summary",
    "created_at": "2020-12-28T20:49:30Z",
    "closed_at": "2021-02-04T23:27:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5289",
    "body": "Weights summary gets printed when Trainer calls `fit()`, but the output is not persistent as I have everything wrapped up in ray tune, which overwrites the contents of the output cell in jupyter\r\n\r\nIs there something we can call to manually print the weights summary of a particular model without having to fit the model every time?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5289/comments",
    "author": "thesimonho",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-12-29T18:58:01Z",
        "body": "just do \r\n```python\r\nmodel = YourLightningModule(...)\r\nmodel.summarize(mode=...)\r\n```\r\n`mode='top', 'full'`"
      }
    ]
  },
  {
    "number": 5211,
    "title": "Most efficient way to log data to file within training/validation_step",
    "created_at": "2020-12-21T06:21:27Z",
    "closed_at": "2021-01-27T16:47:55Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5211",
    "body": "Q1: Given the data sample paths of a batch, their losses, the batch number, and the epoch number - I would like to log everything into a file during `training_step` and `validation_step`. My goal is to identify hard samples using this file.\r\nSo I know how to get the data that's given - I would like to know what the most effective way if to log this data in a file while using ddp over multiple gpus (in slurm).\r\n\r\nQ2: how can I write whether the current epoch was saved following `callbacks.model_checkpoint.ModelCheckpoint` to that same file (in v`alidation_epoch_end`)? My goal is to identify which epochs/batches were the best performing.\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5211/comments",
    "author": "bartmch",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-01-20T08:04:51Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 5166,
    "title": "How to compute metric in each step without reseting the metric",
    "created_at": "2020-12-17T08:47:01Z",
    "closed_at": "2021-01-13T09:05:03Z",
    "labels": [
      "feature",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5166",
    "body": "\r\nI want to know a metric during the process of an epoch. That is, I want to know the metric value until now. However, if I call `metric.compute()` in each step, it will reset my metric, which is not expected. I read the source code and didn't find a clever way to do that. So could you tell me how to do that?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5166/comments",
    "author": "Adoni",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2021-01-13T09:05:01Z",
        "body": "Fixed by #5409. Closing this now."
      },
      {
        "user": "Adoni",
        "created_at": "2021-01-13T10:00:11Z",
        "body": "> Fixed by #5409. Closing this now.\r\n\r\nThanks"
      }
    ]
  },
  {
    "number": 5140,
    "title": "Attempting to unscale FP16 gradients",
    "created_at": "2020-12-15T01:31:03Z",
    "closed_at": "2021-01-27T22:38:17Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5140",
    "body": "I want to train and save my model with 16-bit precision, what should I do ?\r\n\r\n```\r\ndef train():\r\n    model = Model(**config)\r\n    model.half()\r\n    data_module = MyDataModule(config['train_root'], config['folds'], config['size'],\r\n                               config['batch_size'], config['num_workers'])\r\n    for i in range(config['folds']):\r\n        model.set_net(i)\r\n        trainer = pl.Trainer(max_epochs=config['epochs'], gpus=[0, ], precision=16,\r\n                             # limit_train_batches=config['limit_train_batches'],\r\n                             # limit_val_batches=config['limit_val_batches'],\r\n                             )\r\n        trainer.fit(model=model,\r\n                    train_dataloader=data_module.train_dataloader(i),\r\n                    val_dataloaders=data_module.val_dataloader(i))\r\n\r\nprint:\r\n\r\n{'n_class': 2, 'num_workers': 4, 'batch_size': 32, 'train_root': '/root/projects/Screw/data/train', 'ckpt': 'model-7res18.pt', 'epochs': 30, 'lr': 0.001, 'size': 224, 'folds': 7, 'limit_train_batches': None, 'limit_val_batches': None}\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nUsing native 16bit precision.\r\n\r\n  | Name    | Type       | Params\r\n---------------------------------------\r\n0 | nets    | ModuleList | 78 M  \r\n1 | cur_net | Net        | 11 M  \r\n/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\r\n  warnings.warn(*args, **kwargs)\r\nValidation sanity check:  50%|██████████          | 1/2 [00:00<00:00,  1.90it/s]cur net: 0\r\n{'val_loss': tensor(0.9086, device='cuda:0'), 'val_f1': 0.5681818181818181}\r\nEpoch 0:   0%|                                           | 0/19 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/root/projects/Screw/model.py\", line 251, in <module>\r\n    train()\r\n  File \"/root/projects/Screw/model.py\", line 223, in train\r\n    val_dataloaders=data_module.val_dataloader(i))\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 446, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\", line 64, in train\r\n    results = self.train_or_test()\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 66, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 495, in train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 561, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 728, in run_training_batch\r\n    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 470, in optimizer_step\r\n    optimizer, batch_idx, opt_idx, train_step_and_backward_closure, *args, **kwargs\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 124, in optimizer_step\r\n    **kwargs,\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1372, in optimizer_step\r\n    optimizer_closure()\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 723, in train_step_and_backward_closure\r\n    self.trainer.hiddens\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 823, in training_step_and_backward\r\n    self.backward(result, optimizer, opt_idx)\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 844, in backward\r\n    result.closure_loss, optimizer, opt_idx, *args, **kwargs\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 90, in backward\r\n    closure_loss, optimizer, opt_idx, *args, **kwargs\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/plugins/native_amp.py\", line 50, in backward\r\n    self.trainer.scaler.unscale_(optimizer)\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py\", line 240, in unscale_\r\n    optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(optimizer, inv_scale, found_inf, False)\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py\", line 187, in _unscale_grads_\r\n    raise ValueError(\"Attempting to unscale FP16 gradients.\")\r\nValueError: Attempting to unscale FP16 gradients.\r\nException ignored in: <function tqdm.__del__ at 0x7efe5330fb70>\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/tqdm/std.py\", line 1128, in __del__\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/tqdm/std.py\", line 1341, in close\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/tqdm/std.py\", line 1520, in display\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/tqdm/std.py\", line 1131, in __repr__\r\n  File \"/root/anaconda3/envs/py37/lib/python3.7/site-packages/tqdm/std.py\", line 1481, in format_dict\r\nTypeError: cannot unpack non-iterable NoneType object\r\n\r\nProcess finished with exit code 1\r\n\r\n\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5140/comments",
    "author": "xinfangliu",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-01-20T18:04:54Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 5129,
    "title": "How to forbid save optimizer's state ？",
    "created_at": "2020-12-14T15:12:33Z",
    "closed_at": "2020-12-15T02:59:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5129",
    "body": "Sometime's it's time-consuming.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5129/comments",
    "author": "Maybewuss",
    "comments": [
      {
        "user": "DuinoDu",
        "created_at": "2020-12-14T15:47:55Z",
        "body": "Set `save_weights_only=True` in ModelCheckpoint."
      },
      {
        "user": "Maybewuss",
        "created_at": "2020-12-15T02:59:44Z",
        "body": "> Set `save_weights_only=True` in ModelCheckpoint.\r\n\r\nThanks !"
      }
    ]
  },
  {
    "number": 5127,
    "title": "How to add a hidden state in test_step when dealing with sequential data?",
    "created_at": "2020-12-14T12:14:33Z",
    "closed_at": "2021-02-04T23:27:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5127",
    "body": "I have a `Trainer` with `truncated_bptt_steps` option like this: \r\n\r\n`trainer = pl.Trainer(truncated_bptt_steps=100)`\r\n\r\nA `training_step` method looks like this:\r\n\r\n    def training_step(self, batch, batch_idx, hiddens):\r\n        out, hiddens = self.forward(data, hidden)\r\n\r\n        result = pl.TrainResult(minimize=loss, hiddens=hiddens)\r\n        return result\r\n\r\nI have a problem because `test_step` doesn't have a `hiddens` arguments, but `forward` method of a neural network needs it:\r\n\r\n    def forward(self, input, hiddens):\r\n        output, hiddens = self.lstm(input, hiddens)\r\n                \r\n        return output, hiddens\r\n\r\nMy current `test_step` looks like this:\r\n\r\n    def test_step(self, batch, batch_idx, hiddens) -> Any:\r\n        logits, hiddens = self.lstm(X_batch, hiddens)\r\n        ... \r\n\r\n\r\nBut when I run the whole model I have an error when reaching `test_step`:\r\n\r\n    File \"/Users/ken/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 333, in _evaluate\r\n        output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\r\n      File \"/Users/ken/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 685, in evaluation_forward\r\n        output = model.test_step(*args)\r\n    TypeError: test_step() missing 1 required positional argument: 'hiddens'",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5127/comments",
    "author": "kenenbek",
    "comments": [
      {
        "user": "timlod",
        "created_at": "2021-01-02T16:37:29Z",
        "body": "I am also wondering about this. Anyone found a solution?"
      },
      {
        "user": "timlod",
        "created_at": "2021-01-02T19:07:25Z",
        "body": "Something like this in val and `test_step` works, but is rather inelegant considering that `train_step` requires hiddens as an argument there:\r\n``` python\r\nif not hasattr(self, \"hiddens\"):\r\n    self.hiddens = None\r\nout, self.hiddens = self(x, self.hiddens)\r\n```\r\n\r\nIs there a possibility to align the training and val/test steps interfaces for these use cases?"
      }
    ]
  },
  {
    "number": 5114,
    "title": "Question about transfer learining.",
    "created_at": "2020-12-14T00:36:49Z",
    "closed_at": "2021-01-20T05:04:52Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5114",
    "body": "I have a trained LightningModule, named AutoEncoder. When using it in other LightningModule, i want to know that if both AutoEncoder.freeze() and  AutoEncoder.eval() are needed.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5114/comments",
    "author": "zhhao1",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-01-13T03:42:06Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 5110,
    "title": "The latest release 1.1.0 removed the latest training metrics from the result produced by Trainer.test method. Why?",
    "created_at": "2020-12-13T23:09:53Z",
    "closed_at": "2021-01-24T05:37:53Z",
    "labels": [
      "question",
      "won't fix",
      "logging"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5110",
    "body": "For some reason Trainer.test methods doesn't return train metrics starting with v.1.1.0 PTL.\r\nWas that done on purpose? If yes, then how can I get the latest values of training metrics?\r\n\r\n\r\n\r\nv1.1.0:\r\n--------------------------------------------------------------------------------\r\nDATALOADER:0 TEST RESULTS\r\n{'test_acc': tensor(0.6441, device='cuda:0'),\r\n 'test_acc_epoch': tensor(0.8204, device='cuda:0'),\r\n 'test_f1': tensor(0.6441, device='cuda:0'),\r\n 'test_f1_epoch': tensor(0.8204, device='cuda:0'),\r\n 'test_loss': tensor(1.4021, device='cuda:0'),\r\n 'test_loss_epoch': tensor(0.6918, device='cuda:0'),\r\n 'test_prec': tensor(0.6441, device='cuda:0'),\r\n 'test_prec_epoch': tensor(0.8204, device='cuda:0'),\r\n 'test_rec': tensor(0.6441, device='cuda:0'),\r\n 'test_rec_epoch': tensor(0.8204, device='cuda:0'),\r\n 'val_acc': tensor(0.6441, device='cuda:0'),\r\n 'val_acc_epoch': tensor(0.8204, device='cuda:0'),\r\n 'val_f1': tensor(0.6441, device='cuda:0'),\r\n 'val_f1_epoch': tensor(0.8204, device='cuda:0'),\r\n 'val_loss': tensor(1.4021, device='cuda:0'),\r\n 'val_loss_epoch': tensor(0.6918, device='cuda:0'),\r\n 'val_prec': tensor(0.6441, device='cuda:0'),\r\n 'val_prec_epoch': tensor(0.8204, device='cuda:0'),\r\n 'val_rec': tensor(0.6441, device='cuda:0'),\r\n 'val_rec_epoch': tensor(0.8204, device='cuda:0')\r\n\r\n}\r\n\r\nBefore v1.1.0:\r\n--------------------------------------------------------------------------------\r\nDATALOADER:0 TEST RESULTS\r\n{'test_acc': tensor(0.6441, device='cuda:0'),\r\n 'test_acc_epoch': tensor(0.8204, device='cuda:0'),\r\n 'test_f1': tensor(0.6441, device='cuda:0'),\r\n 'test_f1_epoch': tensor(0.8204, device='cuda:0'),\r\n 'test_loss': tensor(1.4021, device='cuda:0'),\r\n 'test_loss_epoch': tensor(0.6918, device='cuda:0'),\r\n 'test_prec': tensor(0.6441, device='cuda:0'),\r\n 'test_prec_epoch': tensor(0.8204, device='cuda:0'),\r\n 'test_rec': tensor(0.6441, device='cuda:0'),\r\n 'test_rec_epoch': tensor(0.8204, device='cuda:0'),\r\n 'val_acc': tensor(0.6441, device='cuda:0'),\r\n 'val_acc_epoch': tensor(0.8204, device='cuda:0'),\r\n 'val_f1': tensor(0.6441, device='cuda:0'),\r\n 'val_f1_epoch': tensor(0.8204, device='cuda:0'),\r\n 'val_loss': tensor(1.4021, device='cuda:0'),\r\n 'val_loss_epoch': tensor(0.6918, device='cuda:0'),\r\n 'val_prec': tensor(0.6441, device='cuda:0'),\r\n 'val_prec_epoch': tensor(0.8204, device='cuda:0'),\r\n 'val_rec': tensor(0.6441, device='cuda:0'),\r\n 'val_rec_epoch': tensor(0.8204, device='cuda:0'),\r\n 'train_acc': \r\n 'train_acc_epoch': \r\n 'train_f1': \r\n 'train_f1_epoch': \r\n 'train_loss': \r\n 'train_loss_epoch': \r\n 'train_prec': \r\n 'train_prec_epoch': \r\n 'train_rec': \r\n 'train_rec_epoch': }\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5110/comments",
    "author": "pgagarinov",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2020-12-14T11:01:45Z",
        "body": "Hey @pgagarinov,\r\n\r\nYes, when calling trainer.test(), only metrics computed around testing should be provided within the results objects.\r\nTo access all metrics, it can be done by using `trainer.callback_metrics`.\r\n\r\n@edenlightning Do you second that ?\r\n\r\nBest regards,\r\nThomas Chaton\r\n"
      },
      {
        "user": "pgagarinov",
        "created_at": "2020-12-14T11:27:30Z",
        "body": "@tchaton Actually, it was very convenient to have all metrics (trainining, validation and test) in the same result to be able to compare them without having to call callback_metrics. Now it is less convenient(("
      },
      {
        "user": "pgagarinov",
        "created_at": "2020-12-17T22:49:46Z",
        "body": "@tchaton Here is the strange thing -\r\n\r\n\r\n when I follow this line from the official documentation \r\n`\r\nMetric objects can also be directly logged, in which case Lightning will log the metric based on on_step and on_epoch flags present in self.log(...). If on_epoch is True, the logger automatically logs the end of epoch metric value by calling .compute().\r\n`\r\nand  log metric objects (like Accuracy) directly trainer.callback_metrics returns the dictionary of tensors  until the training is finished (I check from my custom callback). When the training is done callback_metrics returns a mix of tensors and objects like this:\r\n\r\nOut[1]: \r\n{'train_acc_step': tensor(0.1094),\r\n 'train_f1_step': tensor(0.0473),\r\n 'train_prec_step': tensor(0.1103),\r\n 'train_rec_step': tensor(0.1167),\r\n 'train_loss_step': tensor(2.3015),\r\n 'val_acc_epoch': tensor(0.0750),\r\n 'val_f1_epoch': tensor(0.0219),\r\n 'val_prec_epoch': tensor(0.0198),\r\n 'val_rec_epoch': tensor(0.0937),\r\n 'val_loss_epoch': tensor(2.3029),\r\n 'val_acc': Accuracy(),\r\n 'val_f1': F1(),\r\n 'val_prec': Precision(),\r\n 'val_rec': Recall(),\r\n 'val_loss': tensor(2.3024),\r\n 'train_acc_epoch': tensor(0.1094),\r\n 'train_f1_epoch': tensor(0.0388),\r\n 'train_prec_epoch': tensor(0.0755),\r\n 'train_rec_epoch': tensor(0.1041),\r\n 'train_loss_epoch': tensor(2.3010),\r\n 'train_acc': Accuracy(),\r\n 'train_f1': F1(),\r\n 'train_prec': Precision(),\r\n 'train_rec': Recall(),\r\n 'train_loss': tensor(2.3015)}\r\n\r\nIs this an expected behavior?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-01-17T02:21:59Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 5101,
    "title": "How do I pass metric to tensorboards hparam interface",
    "created_at": "2020-12-11T22:30:38Z",
    "closed_at": "2021-02-04T23:26:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5101",
    "body": "#### What is your question?\r\n\r\nI have been unable to log metrics on the hparam menu in pytorch lightning. I've been googling for a few hours and found it difficult to get a straight explanation of how to do this. The only available metric is a generic one called hp_metric.\r\n\r\n#### Code\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        avg_loss = torch.stack(outputs).mean()\r\n        self.log(\"avg_val_loss\", avg_loss)\r\n        self.logger.log_metrics({\"val_loss_metric\": avg_loss})\r\n\r\n#### What have you tried?\r\n\r\nI have used self.save_hyperparameters in __init__ and passed my config and added a self.logger.log_metrics function to my validation step. I've tried looking through the docs for tensorboard, torch and pytorch lightning and found myself unable to figure out what is needed here.\r\n\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\n - Packaging conda\r\n - Version1.0.7\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5101/comments",
    "author": "mm04926412",
    "comments": [
      {
        "user": "mm04926412",
        "created_at": "2020-12-11T23:37:23Z",
        "body": "I have also tried to log the hparams at the end of training with a callback but this seems to fail for unknown reasons\r\n\r\n    def on_train_end(self, trainer, model):\r\n        torch.save(\r\n            model.encoder.state_dict(),\r\n            \"current_production_model/production_model.state_dict\",\r\n        )\r\n        model.logger.experiment.add_hparams(\r\n            model.hparams_dict, {\"avg_val_loss\": model.avg_loss.item()}\r\n        )"
      },
      {
        "user": "DuinoDu",
        "created_at": "2020-12-12T16:00:43Z",
        "body": "Would you provide failed case screen snapshot? In my case, it runs ok."
      },
      {
        "user": "mm04926412",
        "created_at": "2020-12-12T16:14:31Z",
        "body": "I got it to work by logging with the name \"hp_metric\" instead is this how it is meant to be done? Code now seems to work so old lines have been removed from my code making a screenshot difficult but what happened was the log metric command just added the metric as a scalar and it was not available in the tensorboard hparam interface"
      },
      {
        "user": "s-rog",
        "created_at": "2020-12-14T00:35:02Z",
        "body": "Yes, if you look into tensorboard logger docs there is a default \"hp_metric\" which can be disabled.\r\n\r\nWhen it's disabled you can use:\r\n`self.logger.log_hyperparams(self.hparams, {\"custom_metric_key\": custom_metric})`"
      },
      {
        "user": "joe32140",
        "created_at": "2021-01-04T00:07:24Z",
        "body": "@mm04926412 I had the same problem with you that I can simply do `self.log('hp_metirc', loss)` in `validation_step`, but `self.logger.log_hyperparams(self.hparams, {\"custom_metric_key\": custom_metric})` doesn't work for me.\r\n\r\nHave you solved the problem?"
      }
    ]
  },
  {
    "number": 5033,
    "title": "metrics from \"sanity check\" do not appear on wandb / epoch 1 becomes 0",
    "created_at": "2020-12-09T01:17:30Z",
    "closed_at": "2021-01-26T00:47:06Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5033",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI'm using so called \"sanity check\" to run actual full validation before I do any training, using `num_sanity_val_steps=-1`\r\nhowever I could not see any metrics displayed on wandb. \r\nEventually I had to implement custom `validation_epoch_end`\r\nwhere instead of `log_dict` I'm using \r\n`self.logger.agg_and_log_metrics(metrics, step=self.current_epoch + 1)`\r\n\r\nthe question is if there is more normal / straight-forward way to do this?\r\nI still have to implement custom `training_epoch_end`, because currently validation epoch 1 corresponds to training epoch 0. Essentially how I see it is validation curve on wandb starting from epoch 0, training epoch starting from 1, and this being achieved by some sort of \"don't decrement epoch number\" kind of parameter...\r\nThanks!\r\n\r\n#### What's your environment?\r\ncurrently using PL  1.0.8 and wandb 0.10.12",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5033/comments",
    "author": "undertherain",
    "comments": [
      {
        "user": "undertherain",
        "created_at": "2020-12-10T04:13:10Z",
        "body": "looks like part of the problem might be here, in logging/base.py\r\n```\r\n        if step == self._prev_step:\r\n            self._metrics_to_agg.append(metrics)\r\n            return step, None\r\n```\r\n\r\nthough it says this is for aggregating metrics :-/\r\nand the prev_step is set to -1...\r\nI don't mind it it's epoch -1 on wandb, though still would be nicer to start eval from 0 and training from 1"
      },
      {
        "user": "undertherain",
        "created_at": "2020-12-10T04:48:47Z",
        "body": "okey, so it looks like the case it that it is not that it's not logging per se\r\nit is that the wandb logger waits to collect all metrics for a given step, and only updates when the step changes to the next one. \r\nso \r\n1. does PL ever log results of the call to log_something \r\n2. using  agg_and_log_metrics directly is a bit tricky, because if I use epoch as step, I should not be using batch as step in trainer. Which is okeish if I only log per epoch manually in both train and val. \r\n3 . Would still prefer to do it through log_dict but there's too much levels of abstraction there to figure out what's going on in that code"
      },
      {
        "user": "undertherain",
        "created_at": "2020-12-10T05:00:14Z",
        "body": "I though to find an answer in samples - and noticed that they are not logging, but returning a dict. So I rewrote my code like in samples - just to get a warning that \r\n```\r\nUserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\r\n[0]<stderr>:Please use self.log(...) inside the lightningModule instead.\r\n```\r\ncmon guys...."
      },
      {
        "user": "undertherain",
        "created_at": "2020-12-10T07:20:46Z",
        "body": "okey. short but hacky solution is\r\n```\r\n    def validation_epoch_end(self, outputs):\r\n        if self.trainer.running_sanity_check:\r\n            self.trainer.running_sanity_check = False  # so that loggers don't skip logging\r\n            self.trainer.current_epoch = -1 \r\n```\r\n- ops, this does not work, training also starts from -1 then"
      },
      {
        "user": "borisdayma",
        "created_at": "2020-12-19T20:37:16Z",
        "body": "If you want to commit all the metrics and increment the step after your sanity check, you could also use `wandb.log({})`.\r\nHowever you need to be sure that the step is actually going to increase as you cannot log at previous steps."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-01-18T22:59:24Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4999,
    "title": "Tensorboard logger: let x-axis be epochs instead of step for training logs",
    "created_at": "2020-12-07T17:12:58Z",
    "closed_at": "2021-01-14T03:54:16Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4999",
    "body": "#### What is your question?\r\n\r\nHello PyTorchLightning Community.\r\n\r\nWe upgraded from version 0.8 to the current version 1.0.8 and then got the error that `training_epoch_end` cannot return any results.\r\n\r\nWe used that before to aggregate our metrics and then return the averaged logs.\r\n\r\nWe removed the method and instead are using the self.log_dict now in training_step and it works.\r\n\r\nHowever, on the x-axis there is now a step number which is corresponding to batch update steps and not the epoch number.\r\n\r\nWhat's the recommended way to have it be the epoch number?\r\n\r\n#### Code\r\n\r\nWhat we had before:\r\n\r\n```\r\ndef training_epoch_end(self, outputs):\r\n    avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\r\n    tensorboard_logs = {'0_train/loss': avg_loss}\r\n    for name in self.metrics:\r\n        tensorboard_logs['0_train/{}'.format(name)] = torch.stack([x['metr'][name] for x in outputs]).mean()\r\n    tensorboard_logs['step'] = self.current_epoch  # This was used to log by epoch instead of by step\r\n\r\n    return {'loss': avg_loss, 'log': tensorboard_logs}\r\n```\r\n\r\n#### What have you tried?\r\n\r\nWe removed the `training_epoch_end` method and instead in our `training_step` do:\r\n\r\n`self.log_dict(metrics_dict, on_step=False, on_epoch=True)`\r\n\r\nFor validation it still works, so we have the `validation_epoch_end` method and tensorboard logs per epoch and also shows the epoch numbers on the x-axis.\r\n\r\nAny help how to have the same behavior in the current version again would be much appreciated.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4999/comments",
    "author": "mpaepper",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-01-06T22:25:08Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "versatran01",
        "created_at": "2021-01-27T19:04:32Z",
        "body": "I have the same question, I was returning a dict of metrics in `validation_epoch_end` before and the model checkpoint will read into it and monitor one of the metric.\r\nI also log these metrics to tensorboard using epoch instead of step.\r\nNow there is a warning saying `validation_epoch_end` shouldn't return anything, and I have to use `self.log` which has the same problem as above which is I'm not able to specify the x-axis to be epoch rather than step."
      },
      {
        "user": "chenlijun99",
        "created_at": "2021-07-08T07:57:05Z",
        "body": "Has anybody found a workaround to this limitation?"
      }
    ]
  },
  {
    "number": 4986,
    "title": "Where is stored logs of the NeptuneLogger when I use offline mode for a logger?",
    "created_at": "2020-12-05T18:05:23Z",
    "closed_at": "2021-01-14T03:54:17Z",
    "labels": [
      "question",
      "won't fix",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4986",
    "body": "I use Neptune Logger with the offline mode in my pipeline, but I can't find where log files are located, and I can't find parameters of NeptuneLogger to set store dir and so on from offline mode. Can someone help with it? Thanks in advance!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4986/comments",
    "author": "bes-dev",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-05T18:06:08Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-12-07T17:56:30Z",
        "body": "check this path `trainer.log_dir`."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-01-06T22:25:07Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "stonelazy",
        "created_at": "2021-05-04T10:35:07Z",
        "body": "Logs were not found in the trainer.log_dir, any other possibility ?"
      }
    ]
  },
  {
    "number": 4947,
    "title": "How to log more than one metrics in logger?",
    "created_at": "2020-12-02T15:31:25Z",
    "closed_at": "2020-12-02T16:01:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4947",
    "body": "I want to log two metircs.What should i do?\r\nself.log('my_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) \r\nThis can only log one metrics.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4947/comments",
    "author": "zhhao1",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-12-02T15:33:51Z",
        "body": "@7zhanghao  You can just write another log.\r\n```py\r\n self.log('another_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\r\n```"
      },
      {
        "user": "zhutmost",
        "created_at": "2020-12-02T15:50:54Z",
        "body": "@7zhanghao You can use `self.log_dict`"
      },
      {
        "user": "zhhao1",
        "created_at": "2020-12-03T01:02:20Z",
        "body": "> @7zhanghao You can just write another log.\r\n> \r\n> ```python\r\n>  self.log('another_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\r\n> ```\r\n\r\nThanks a lot."
      },
      {
        "user": "zhhao1",
        "created_at": "2020-12-03T01:03:29Z",
        "body": "> @7zhanghao You can use `self.log_dict`\r\n\r\nThanks a lot."
      }
    ]
  },
  {
    "number": 4940,
    "title": "typeError unexpected closure",
    "created_at": "2020-12-01T21:24:10Z",
    "closed_at": "2020-12-02T11:21:45Z",
    "labels": [
      "question",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4940",
    "body": "## ❓ Questions and Help\r\n\r\nI am using pytorch 1.6 and pytorch lightning 1.0.8.\r\n\r\nBefore I upgrade my pytorch-lighing from 1.0.2 to 1.0.8, everything works fine, today I upgrade to 1.0.8 to try some new metrics, but got this error. \r\n\r\n    main()\r\n  File \"main.py\", line 68, in main\r\n    trainer.fit(model, trainloader, evalloader)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 444, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\", line 63, in train\r\n    results = self.train_or_test()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 74, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 493, in train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 561, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 728, in run_training_batch\r\n    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 470, in optimizer_step\r\n    optimizer, batch_idx, opt_idx, train_step_and_backward_closure\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 130, in optimizer_step\r\n    using_lbfgs=is_lbfgs\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1270, in optimizer_step\r\n    optimizer.step(closure=optimizer_closure)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\r\n    return func(*args, **kwargs)\r\nTypeError: step() got an unexpected keyword argument 'closure'\r\n(py3) yikuan@deepmedicine:~/project/version_control/HiBEHRT-BYOL$ TypeError: step() got an unexpected keyword argument 'closure'",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4940/comments",
    "author": "yikuanli",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-01T21:25:02Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-12-02T08:48:19Z",
        "body": "Which optimizer and scheduler are you using? If you are using a custom optimizer, you need to update the code of the ```step()``` function to take in a closure. PyTorch optimizer class implements the step method with the closure parameter in the latest version."
      },
      {
        "user": "yikuanli",
        "created_at": "2020-12-02T11:17:52Z",
        "body": "thanks, I now upgrade it to the latest version and seems the latest bolts solve the problem."
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-12-02T11:19:44Z",
        "body": "Oh, if you were using the LARS wrapper form bolts, then the latest version has the step method taking in a closure. That should solve it.\r\n\r\nShould I close this issue then?"
      },
      {
        "user": "yikuanli",
        "created_at": "2020-12-02T11:21:16Z",
        "body": "yes, thanks a lot"
      }
    ]
  },
  {
    "number": 4939,
    "title": "Is DataModule Iteration order stable?",
    "created_at": "2020-12-01T19:46:30Z",
    "closed_at": "2020-12-02T16:02:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4939",
    "body": "When using `DataModule` to hold data from a file (one sample per line), the order is preserved?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4939/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "ananyahjha93",
        "created_at": "2020-12-02T08:51:28Z",
        "body": "If the Dataset class reads the file in a particular order and the Dataloader present in the DataModule does not use ```shuffle=True``` flag, then the order should be preserved."
      }
    ]
  },
  {
    "number": 4935,
    "title": "The correct way to resume experiment with logger",
    "created_at": "2020-12-01T14:43:51Z",
    "closed_at": "2021-02-04T23:12:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4935",
    "body": "From what I understand checkpoints don't save id of the logger run right?\r\nThen what is the correct way to resume training from checkpoint and resume the correct logger run at the same time?\r\n\r\nFor example, I'm using Wights&Biases logger where you can pass id argument which resumes run experiment:\r\n```\r\nWandbLogger(id=run_id)\r\n```\r\nHow can I know what is the correct run id without checking it manually?\r\nIs it possible to somehow store it in the checkpoint?\r\nAlso, why is checkpoint not storing things like that automatically? Seems like a useful feature.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4935/comments",
    "author": "ashleve",
    "comments": [
      {
        "user": "manifoldhiker",
        "created_at": "2020-12-24T18:02:28Z",
        "body": "Have the same question.\r\n\r\nMaybe, `ModelCheckpoint` should also have an option to save wandb state as well? "
      },
      {
        "user": "jonashaag",
        "created_at": "2020-12-26T15:11:55Z",
        "body": "Same question here."
      },
      {
        "user": "jonashaag",
        "created_at": "2021-01-25T16:16:29Z",
        "body": "Still an issue, stalebot."
      }
    ]
  },
  {
    "number": 4874,
    "title": "Metric Reset",
    "created_at": "2020-11-26T17:37:30Z",
    "closed_at": "2020-11-28T22:40:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4874",
    "body": "How can I manually reset a metric? \r\nOr metric states are reset to default values after calling the `compute()` method?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4874/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-11-27T06:12:39Z",
        "body": "`metric.compute()` resets the state. This is usually done after an epoch."
      },
      {
        "user": "celsofranssa",
        "created_at": "2020-11-28T22:40:39Z",
        "body": "> `metric.compute()` resets the state. This is usually done after an epoch.\r\n\r\nOk, thank you."
      }
    ]
  },
  {
    "number": 4836,
    "title": "Use custom batch dataloader with DDP",
    "created_at": "2020-11-24T19:45:04Z",
    "closed_at": "2021-01-03T13:29:04Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4836",
    "body": "Hello,\r\nIn our project, we use a custom data loader like so:\r\n```\r\nclass BatchDataLoader(torch.utils.data.dataloader.DataLoader):\r\n    def __init__(self, ds, batch_size, shuffle, num_workers=0):\r\n        inner_sampler = RandomSampler(ds) if shuffle else SequentialSampler(ds)\r\n        sampler = BatchSampler(inner_sampler, batch_size, False)\r\n        super().__init__(ds, batch_size=None, sampler=sampler, num_workers=num_workers)\r\n```\r\n\r\nWe use this custom dataloader so that the `__getitem__` method is directly called with the list of batch indices and returns a full batch instead of single items. We need this because for some technical reasons, the `__getitem__` method is way way faster when accessing a batch of data items rather than single ones. This has been working perfectly fine so far until we started looking into using ddp for training our model.\r\n\r\nBy default Lightning setup its own sampler for distributed training. We can override this behaviour with replace_sampler_ddp=False but then our custom sampler would be used, whereas it is not made for distributing.\r\n\r\nIf we wish to make our own custom distributed version of it, then how can we retrieve the distributed context from lightning? Is there a way to know if we are in a distributed context, and if so, how many nodes, etc? Can this be done from a Lightning data module, as this is where we instantiate the data loaders? I tried using `torch.distributed,is_available()` as a first step. While this passes on Windows, this call fails with `AssertionError: Default process group is not initialized` on Linux if we are not in a distrbuted context (for instance training on a single GPU). For info, I'm using Lightning 1.0.5 with PyTorch 1.5.1. We wish to setup our sampler automatically, without the user having to specify extra parameters.\r\n\r\nOr do you see a way to achieve the same behaviour (getting batches of indices in the `__getitem__` method of the dataset) while still benefitting from the default DistributedSampler that Lightning setup? I thought of wrapping my original dataset in another one that would create \"buckets\" of data, but then how can I make sure the dataset is reshuffled each epoch so that the buckets don't always have the same content?\r\n\r\nThanks for your help :)",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4836/comments",
    "author": "floboc",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-24T19:45:50Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "floboc",
        "created_at": "2020-11-24T20:07:19Z",
        "body": "To give more information, here is what I thought of when mentioning to wrap the dataset:\r\n\r\n```\r\nclass BatchedDataset(Dataset):\r\n    def __init__(self, source: Dataset, batch_size: int = 1024, drop_last: bool = True):\r\n        super().__init__()\r\n\r\n        self.source = source\r\n        self.batch_size = 1024\r\n        self.drop_last = drop_last\r\n        self.batch_indices = None\r\n        self.shuffle()\r\n\r\n    def shuffle(self):\r\n        self.batch_indices = torch.randperm(len(self.source)).tolist()\r\n\r\n    def __getitem__(self, index):\r\n        start_idx = index * self.batch_size\r\n        end_idx = min(len(self.source), start_idx + self.batch_size)\r\n        batch_indices = self.batch_indices[start_idx:end_idx]\r\n        batch = self.source.__getitem__(batch_indices)\r\n        return batch\r\n\r\n    def __len__(self):\r\n        if self.drop_last:\r\n            return len(self.source) // self.batch_size\r\n        else:\r\n            return (len(self.source) + self.batch_size - 1) // self.batch_size\r\n```\r\n\r\nThe data loader would then use this dataset with a default sampler, with batch_size of 1 (as the dataset already produces batches) and some shuffling (might not be needed if the dataset is shuffled at each epoch)\r\n\r\nThis could work, however, several questions remain:\r\n- How to make sure shuffle() is called at the end or beginning of each epoch, without having to setup very specific training hooks. Our code is quite general and might handle other dataset that don't require this. I would like to fullu setup the Dataset and DataLoader in my data module class\r\n- How to make sure this is working as expected across multiple nodes / process, ie making sure that it is synchronized (shuffle either called on main process and then synced, or ensuring determinism) ?\r\n\r\nThanks :)\r\n\r\nEDIT:\r\nFor the deterministic part, I think that adding a seed in the constructor then doing the shuffling like so solves the issue:\r\n```\r\n    def shuffle(self, epoch: int):\r\n        # seeeding is needed to make it dterministic accross processes\r\n        g = torch.Generator()\r\n        g.manual_seed(self.seed + epoch)\r\n        self.batch_indices = torch.randperm(len(self.source), generator=g).tolist()\r\n```\r\n\r\nNow the only part left is to ensure shuffle() is called at the beginning of each epoch. In the training loop code there is this section at the start of an epoch:\r\n```\r\n# set seed for distributed sampler (enables shuffling for each epoch)\r\n        try:\r\n            self.trainer.train_dataloader.sampler.set_epoch(epoch)\r\n        except Exception:\r\n            pass\r\n```\r\nHow to achieve the same thing on the dataset itself as I don't want to provide a custom sampler?"
      },
      {
        "user": "floboc",
        "created_at": "2020-11-24T21:47:54Z",
        "body": "Me again :)\r\nI found the current workaround to make sure my dataset is shuffled at each epoch by adding this to my module class:\r\n\r\n```\r\n    def on_train_epoch_start(self) -> None:\r\n        try:\r\n            dataset = self.trainer.train_dataloader.dataset\r\n            dataset.set_epoch(self.current_epoch)\r\n        except Exception:\r\n            pass\r\n        return super().on_train_epoch_start()\r\n```\r\n\r\nI'm still looking for the correct way to achieve this however, all this feels very hacky, and I had to make my module aware of this specific case which doesn't seem to be a good idea"
      },
      {
        "user": "s-rog",
        "created_at": "2020-11-26T05:06:35Z",
        "body": "I've looked into custom distributed samplers in pytorch before and all solutions I found are pretty hacky.\r\n\r\nLightning actually calls `set_epoch` as well for distributed samplers, it's just done automatically from the backend."
      },
      {
        "user": "tchaton",
        "created_at": "2020-11-27T12:25:00Z",
        "body": "Hey @floboc,\r\n\r\nTo make slightly cleaner, you could check the instance of your dataset instead of using a try / empty except. You might miss some errors there.\r\n\r\nI guess the only solution would be to create a LigthiningDistributedSampler you inherit from, and we would call `.set_epoch(self.current_epoch)` for you. \r\n\r\nBut I am not really sure it would be worth it.\r\n\r\nBest,\r\nT.C "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-27T13:14:47Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4789,
    "title": "Loading model",
    "created_at": "2020-11-20T14:27:36Z",
    "closed_at": "2020-11-20T14:35:24Z",
    "labels": [
      "help wanted",
      "question",
      "checkpointing"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4789",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nFor loading the model, when I use the following command it seems it doesn't update the weights in the model:\r\n```\r\nmodel = LitModel()\r\nmodel.load_from_checkpoint(checkpoint_path=<path to ckpt>)    \r\n```\r\nI need to use:\r\n```\r\nmodel = LitModel()\r\nmodel = model.load_from_checkpoint(checkpoint_path=<path to ckpt>)\r\n```\r\n\r\n### Expected behavior\r\nIt should use the new weight for the model but it seems it doesn't work. In PyTorch when we use `model.load_state_dict()', it updates the model weights.\r\n\r\n - PyTorch Version : 1.7.0\r\n - OS: Ubuntu 20\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version: 11.1\r\n - GPU models and configuration: RTX 2080Ti\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4789/comments",
    "author": "kargarisaac",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-20T14:28:19Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-11-20T14:35:23Z",
        "body": "@kargarisaac this `self.test_transforms` is a class constructor, the same as `pandas.read_csv` so the proper usage should be \r\n```python\r\nmodel = LitModel.load_from_checkpoint(checkpoint_path=<path to ckpt>)\r\n```\r\neventually, we can think about adding `inplace` argument but it may be even more confusing..."
      }
    ]
  },
  {
    "number": 4780,
    "title": "Average checkpoint",
    "created_at": "2020-11-20T03:21:08Z",
    "closed_at": "2020-11-20T14:59:47Z",
    "labels": [
      "question",
      "waiting on author",
      "design",
      "checkpointing"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4780",
    "body": "How to average n checkpoints? Is it must be done by myself ?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4780/comments",
    "author": "zhhao1",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2020-11-20T09:15:50Z",
        "body": "Hey @7zhanghao,\r\n\r\nThanks for using Pytorch Lightning. We currently don't  support n checkpoints average, but feel free to make a PR.\r\nIt don't think it should be too hard to implement.\r\n\r\nBest regards,\r\nThomas Chaton."
      },
      {
        "user": "zhhao1",
        "created_at": "2020-11-20T14:59:47Z",
        "body": "Thanks a lot. I have a try."
      }
    ]
  },
  {
    "number": 4767,
    "title": "Use different gradient_clip_val for different parameters",
    "created_at": "2020-11-19T03:33:18Z",
    "closed_at": "2020-11-20T09:02:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4767",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nCan i use different gradient_clip_val for different parameters use `Trainer(gradient_clip_val=0.5)`\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4767/comments",
    "author": "Limtle",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2020-11-20T09:02:07Z",
        "body": "Hey @Limtle,\r\n\r\nCurrently, we don't support multiple gradient_clip_val.\r\n\r\nWithout AMP and TPU, we use `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_val, norm_type=norm_type)`  to compute gradients clipping in `_clip_gradients function`. \r\n\r\nBest regards,\r\nThomas Chaton. "
      }
    ]
  },
  {
    "number": 4711,
    "title": "How to monitor more than one quantity?",
    "created_at": "2020-11-17T12:41:15Z",
    "closed_at": "2020-11-18T00:18:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4711",
    "body": "What i do if i want to monitor more than one quantity?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4711/comments",
    "author": "zhhao1",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2020-11-17T13:59:23Z",
        "body": "You can pass multiple `ModelCheckpoint` callbacks to the trainer callback list\r\n\r\n```python\r\nTrainer(callbacks=[ModelCheckpoint(monitor=\"a\"), ModelCheckpoint(monitor=\"b\")])\r\n```\r\n\r\nhowever, this is not fully supported and the saved checkpoints will contain the state for only one of the `ModelCheckpoint`s\r\n\r\nDuplicate of #2908"
      },
      {
        "user": "zhhao1",
        "created_at": "2020-11-18T00:18:14Z",
        "body": "Thanks a lot."
      }
    ]
  },
  {
    "number": 4646,
    "title": "Loading samples to RAM with DDP.",
    "created_at": "2020-11-12T21:06:02Z",
    "closed_at": "2020-11-13T10:41:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4646",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI'm facing an IO bottleneck that can be fixed with a custom `torch.utils.data.Dataset` that loads each sample to RAM. Then training goes fast as I don't need to read my samples (images) from disk (slow). Everything works well with when I'm using 1 GPU, but I'm a bit lost when I switch to multiple GPUs with DDP.\r\n\r\nDDP divides the samples to each GPU and I'm wondering when/where I should load my samples to RAM so that each process only loads its own partition of the samples?\r\n\r\n#### Code\r\n\r\n```\r\nclass RAMDataset(data.Dataset):\r\n    def __init__(self, paths,labels,transform):\r\n        \"\"\"Dataset that loads all samples to RAM.\"\"\"\r\n        self.paths = paths\r\n        self.labels = labels\r\n        self.transform = transform\r\n\r\n    def __len__(self):\r\n        return len(self.samples)\r\n\r\n    def load_to_RAM(self):\r\n        self.images = []\r\n        for path in self.paths:\r\n            with open(path, \"rb\") as f:\r\n                str_encode = f.read()\r\n                nparr = np.frombuffer(str_encode, np.uint8)\r\n                self.images.append(cv2.imdecode(nparr, cv2.IMREAD_COLOR))\r\n\r\n    def __getitem__(self, index):\r\n        # Run self.load_to_RAM() first!\r\n        image = self.transform(self.images[index])\r\n        label = self.labels[index]\r\n        return image, label\r\n```\r\n\r\n#### What have you tried?\r\n\r\nWith 1 GPU `self.load_to_RAM()` can be excecuted as soon as the Dataset has been created.\r\n\r\n```\r\ndataset = RAMDataset(paths,labels)\r\ndataset.load_to_RAM()\r\nloader = DataLoader(dataset,...)\r\ntrainer.fit(model,loader)\r\n```\r\n\r\nBut obviously this would load the samples `num_gpus` times to the RAM of the node.\r\n\r\nI quickly tried to call `self.train_dataloader.dataset.load_to_RAM()`on the hook `setup()` but got the following error...\r\n```\r\nAttributeError: '_PatchDataLoader' object has no attribute 'dataset'\r\n```\r\n..and I'm 99% this solution would also load all of the samples to RAM.\r\n\r\n#### Possible solution?\r\n\r\n1. Find out which process (which GPU and which node) is currently running.\r\n2. Get the allocated slice of the samples for this process.\r\n3. Load only that slice of the `self.paths` to RAM.\r\n\r\nTried to go through the source code but couldn't find out how I could implement this.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4646/comments",
    "author": "jopo666",
    "comments": [
      {
        "user": "s-rog",
        "created_at": "2020-11-13T00:27:32Z",
        "body": "The OS should automatically cache the dataset after the first epoch (assuming you have enough ram) so you shouldn't need to do this.\r\n\r\nI'm actually not sure if distributed sampler shards the same data/item on the same GPU through different epochs... but that's more of a pytorch question than a lightning one."
      },
      {
        "user": "jopo666",
        "created_at": "2020-11-13T07:55:29Z",
        "body": "I do have enough RAM if I manage to divide my data to multiple nodes. If the `BasicDataset()` below is cached after the first epoch wouldn't that mean only the labels and paths are cached? What I would want is that the loaded images are cached. I'll look into PyTorch and see if the same data/items are kept for each epoch.\r\n\r\n```\r\nclass BasicDataset(data.Dataset):\r\n    def __init__(self, paths,labels,transform):\r\n        self.paths = paths\r\n        self.labels = labels\r\n\r\n    def __getitem__(self, index):\r\n        image = load(self.paths[index]) # This is my bottleneck.\r\n        label = self.labels[index]\r\n        return image, label\r\n```"
      },
      {
        "user": "s-rog",
        "created_at": "2020-11-13T08:01:31Z",
        "body": "Ah you're doing cross node DDP, I was referring to single node above... I'm afraid I don't have much experience with multi node DDP.\r\n\r\nBut if each node gets the same subset of data every epoch, the OS should cache the files all the same (without needing `load_to_RAM`) after the first epoch since you have enough RAM."
      },
      {
        "user": "jopo666",
        "created_at": "2020-11-13T08:06:21Z",
        "body": "Would the actual loaded images be cached even though they are loaded inside `__getitem__()` and not just the whole `Dataset` object with only the `paths` to these files?"
      },
      {
        "user": "s-rog",
        "created_at": "2020-11-13T08:29:08Z",
        "body": "Yes, this is not stored in your program/script memory but instead it's simply file caching at the system level.\r\n\r\nI use Ubuntu but this should be the same for other linux ditros unless some memory/caching settings have been changed/limited. You can look at the cache part of your ram usage in htop as your data is loaded into memory during the first epoch."
      },
      {
        "user": "jopo666",
        "created_at": "2020-11-13T10:41:38Z",
        "body": "Perfect! Kept running tests that ran only for one epoch so I never noticed the seed-up on the second epoch... 😄 Well you learn something new everyday."
      }
    ]
  },
  {
    "number": 4638,
    "title": "how to define my own sampler in ddp training",
    "created_at": "2020-11-12T12:56:30Z",
    "closed_at": "2020-11-12T13:34:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4638",
    "body": "When using ddp as the accelerator, i want to define my own sampler in dataloader, how to do it? Noramally, i do it by overriding _collate_fn. But in pytorch-lightning, it seems that is not correct.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4638/comments",
    "author": "zhhao1",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-12T12:57:12Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "zhhao1",
        "created_at": "2020-11-12T13:34:20Z",
        "body": "I get it."
      }
    ]
  },
  {
    "number": 4607,
    "title": "How to change the Datamodule during training with a callback?",
    "created_at": "2020-11-10T15:59:21Z",
    "closed_at": "2020-12-13T17:55:10Z",
    "labels": [
      "question",
      "won't fix",
      "data handling"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4607",
    "body": "#### What is your question?\r\nHow to change the Datamodule during training with a callback?\r\nMore details:\r\nI am looking for a way to reinitialized my Datamodule with different parameter, I am currently sending the height of my images as argument to my datamodule and I want to change this height at some point during training, the simple way is to call trainer.fit multiple times with different datamodules, but I am wondering is there a way to do this on callback, in the same way as you do when you change the optimizer or lr_scheduler?\r\n\r\n\r\nSomething similar to this:\r\n```\r\ndef on_train_epoch_start(self, trainer, pl_module):\r\n            sch = optim.lr_scheduler.StepLR(optimizer, 1, 0.96)\r\n            scheduler = {\r\n                'scheduler': sch,\r\n                'interval': interval,  # or 'step'\r\n                'monitor': 'train_loss',\r\n                'reduce_on_plateau': False,\r\n                'frequency': 1,\r\n            }\r\n            trainer.lr_schedulers = trainer.configure_schedulers([scheduler])\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4607/comments",
    "author": "MohammedAljahdali",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-12-10T23:18:51Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "MohammedAljahdali",
        "created_at": "2020-12-13T17:55:10Z",
        "body": "Answer from @teddykoker:\r\n \r\n\r\n> I have done this using a callback:\r\n> ```\r\n> class Scheduler(pl.Callback):\r\n>     def _prepare_epoch(self, trainer, model, epoch):\r\n>         phase = ... \r\n>         trainer.datamodule.set_phase(phase)\r\n> \r\n>     def on_epoch_end(self, trainer, model):\r\n>         self._prepare_epoch(trainer, model, trainer.current_epoch + 1)\r\n> \r\n> class Data(pl.LightningDataModule):\r\n>     def set_phase(self, phase: dict):\r\n>         self.size = phase.get(\"size\", self.size)\r\n>         train_transforms = T.Compose(\r\n>             [\r\n>                 T.RandomResizedCrop(self.size, scale=(self.min_scale, 1.0)),\r\n>                 T.RandomHorizontalFlip(),\r\n>                 T.ToTensor(),\r\n>                 normalize,\r\n>             ]\r\n>         )\r\n>         self.train_ds = ImageFolder(self.train_dir, transform=train_transforms)\r\n> \r\n>        \r\n>     def train_dataloader(self):\r\n>         train_dl = DataLoader(\r\n>             self.train_ds,\r\n>             batch_size=self.batch_size,\r\n>             shuffle=True,\r\n>             num_workers=self.num_workers,\r\n>             pin_memory=True,\r\n>         )\r\n>         return train_dl\r\n> ```\r\n> Its important to note:\r\n> \r\n> 1. You can access your datamodule from a callback using trainer.datamodule\r\n> 2. In order to have train_dataloader(), val_dataloader() called every epoch, you must set reload_dataloaders_every_epoch=True in your trainer.\r\n\r\nThank you @teddykoker for the help. "
      }
    ]
  },
  {
    "number": 4604,
    "title": "How to load to from a checkpoint to same device when pretrained encoder was used",
    "created_at": "2020-11-10T14:35:57Z",
    "closed_at": "2020-11-12T12:17:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4604",
    "body": "## ❓ Questions and Help \r\n\r\nI implemented a `ClassificationNet` (see below) that's using a pretrained encoder. After training, I'm trying to load it to CPU using `ClassificationNet.load_from_checkpoint(pth, map_location=torch.device(\"cpu\")`, but since `map_location` in `get_encoder` is `None`, the encoder tries to load to GPU. How can I inform `get_encoder` to load to the same `map_location`? \r\nSince I just started using Lightning, I guess there's a much smarter way to circumvent this situation altogether -- I look forward to your suggestions :) Thanks!\r\n\r\n#### Code\r\n``` python\r\nclass ClassificationNet(LightningModule):\r\n    ...\r\n    self.encoder = get_encoder(pretrained=True)\r\n\r\nget_encoder(pretrained=False, map_location=None):\r\n    model = FancyModel()\r\n    if pretrained:\r\n        ckpt_data = torch.utils.model_zoo.load_url(url, map_location=map_location)\r\n    ....\r\n```\r\n\r\n - OS: Manjaro Linux\r\n - Version 1.0.5",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4604/comments",
    "author": "hakanyi",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-10T14:36:47Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-11-11T20:50:04Z",
        "body": "one solution can be having an additional argument for it.\r\n```python\r\nclass ClassificationNet(LightningModule):\r\n    def __init__(self, encoder_map_location, ...):\r\n        self.encoder = get_encoder(pretrained=True, map_location=encoder_map_location)\r\n```\r\n```python\r\nClassificationNet.load_from_checkpoint(pth, map_location=torch.device(\"cpu\"), encoder_map_location=torch.device(\"cpu\"))\r\n```\r\n\r\nor maybe without an additional argument. Not sure about this one though. But should work.\r\n```python\r\nclass ClassificationNet(LightningModule):\r\n    def __init__(self, ...):\r\n        ....\r\n    \r\n    def setup(self, stage):\r\n        self.encoder = get_encoder(pretrained=True, map_location=self.device)\r\n```\r\n```python\r\nClassificationNet.load_from_checkpoint(pth, map_location=torch.device(\"cpu\"))\r\n```"
      },
      {
        "user": "hakanyi",
        "created_at": "2020-11-12T12:17:41Z",
        "body": "Thanks @rohitgr7! The second solution works indeed and does the job just perfectly :)  "
      }
    ]
  },
  {
    "number": 4601,
    "title": "How to save gradient every step?",
    "created_at": "2020-11-10T09:23:53Z",
    "closed_at": "2020-11-16T06:27:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4601",
    "body": "#### What is your question?\r\nBecause my model have some gradient disappear , I need to save gradient every step to debug. \r\nI found this issue #49. But the document does not have its information. How to save gradient every step?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4601/comments",
    "author": "Limtle",
    "comments": [
      {
        "user": "SeanNaren",
        "created_at": "2020-11-10T11:36:00Z",
        "body": "Hey @Limtle, do you mean saving your gradients from the loss or within the parameters?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-11-10T18:09:00Z",
        "body": "you can override `on_after_backward` or `on_before_zero_grad` hook and save the save gradient as you wish. You can acces them both within a callback or the LightningModule itself.\r\n\r\nwith a callback:\r\n```python\r\ndef on_after_backward(self, trainer, pl_module):\r\n    ... save the gradients\r\n```"
      }
    ]
  },
  {
    "number": 4597,
    "title": "default value of sync_batchnorm",
    "created_at": "2020-11-10T01:31:28Z",
    "closed_at": "2020-11-11T14:15:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4597",
    "body": "Hi,\r\nI am just having a simple question here.\r\nWhy not the flag `sync_batchnorm` default be True?\r\nIf using multi-GPUs, aren't the weights be different across processes if `sync_batchnorm` is False?\r\nIs there case of not using `sync_batchnorm` in multi-GPU mode?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4597/comments",
    "author": "b02202050",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-10T01:32:12Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-10T01:51:05Z",
        "body": "Lightning default gpus is None which means it will run on cpu. So setting sync_batchnorm to True by default does not make sense.\n\nSync_batchnorm is used in multi-gpu training and you should turn it on when there is a batchnorm layer in your model.\nSo setting False by default is a good choice.\n\nClosing it as it is resolved, feel free to reopen if needed"
      },
      {
        "user": "b02202050",
        "created_at": "2020-11-10T07:45:37Z",
        "body": "Hi,\r\nThanks for the answers but why not automatically set sync_batchnorm when using multiple GPU or TPU?\r\nIs there any problem of setting sync_batchnorm when the model doesnot have batchnorm?"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-10T07:53:55Z",
        "body": "> why not automatically set sync_batchnorm when using multiple GPU or TPU?\r\n\r\nAhh Thank you! That's a good point. But not every model will have batch norm layer, so we leave the choice to the users.\r\n\r\n>  Is there any problem of setting sync_batchnorm when the model doesnot have batchnorm?\r\n\r\nI am not sure about it."
      },
      {
        "user": "itsikad",
        "created_at": "2020-11-11T13:58:36Z",
        "body": "@ydcjeff Thanks for you response.\r\n\r\nThis flag will trigger a call `torch.nn.SyncBatchNorm.convert_sync_batchnorm(self.model)` to replace all BatchNorm layers  with their synchronized counterpart, so in case the model doesn't contain BatchNorm layers, nothing will change.\r\n\r\nAFAIK SyncBatchNorm is mostly beneficial when batch_size is small (only few samples per GPU), yet I personally think that the desired behavior in DDP (and probably any other multi-process backends). However, the frequent gather operations may slow down training.\r\n\r\nI believe many users are not familiar with the default behavior of BatchNorm in DDP so printing a message can be helpful, something like: 'you're running with multi-process backend, consider setting sync_batchnorm=True' OR '..., we set sync_batchnorm=true for you'."
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-11T14:14:54Z",
        "body": "@itsikad Thank you for pointing this out. Can you create a separate issue for this?"
      },
      {
        "user": "itsikad",
        "created_at": "2020-11-12T08:34:15Z",
        "body": "@ydcjeff done"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-12T08:38:37Z",
        "body": "> @ydcjeff done\n\nThank you @itsikad "
      }
    ]
  },
  {
    "number": 4584,
    "title": "Logging RL results and tracking them with ModelCheckpoint(monitor=...)",
    "created_at": "2020-11-09T01:59:49Z",
    "closed_at": "2021-02-09T23:30:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4584",
    "body": "I am using Pytorch Lightning in an RL setting and want to save a model when it hits a new max average reward. I am using the Tensorboard logger where I return my neural network loss in the `training_step()` using: \r\n\r\n```\r\nlogs = {\"policy_loss\": pred_loss}\r\nreturn {'loss':pred_loss, 'log':logs}\r\n```\r\n\r\nAnd then I am saving my RL environment rewards using in `on_epoch_end()`: \r\n\r\n```\r\nself.logger.experiment.add_scalar(\"mean_reward\", np.mean(reward_losses), self.global_step)\r\nself.logger.experiment.add_scalars('rollout_stats', {\"std_reward\":np.std(reward_losses),\r\n                \"max_reward\":np.max(reward_losses), \"min_reward\":np.min(reward_losses)}, self.global_step)\r\n```\r\n\r\nAnd every 5 epochs I am also writing out another RL reward loss where I use the best actions rather than sampling from them: \r\n```\r\nif self.current_epoch % self.hparams['eval_every']==0 and self.logger:\r\n            output = self.collect_rollouts(greedy=True, num_episodes=self.hparams['eval_episodes'])\r\n            reward_losses = output[0]\r\n            self.logger.experiment.add_scalar(\"eval_mean\", np.mean(reward_losses), self.global_step)\r\n```\r\n\r\nMy question is, how can I set my ModelCheckpoint to monitor `eval_mean` (which is only written out every 5 epochs, this seems like it would be a problem)? I would also settle for monitoring `mean_reward` (written out every epoch)? Right now I can only successfully monitor `policy_loss` which does not always correspond to higher rewards obtained (setting monitor = to anything else throws an error).\r\n\r\nI know that in the new PL version `self.log()` should be used but after re-writing my code using this it still didn't solve my issue. \r\n\r\nI have spent a lot of time looking through the docs and for examples of this but I have found the logging docs on this to be quite sparse and difficult to even get everything to log in the first place. \r\n\r\nI am using Pytorch Lightning 1.0.5 and Pytorch 1.7.0.\r\n\r\nThank you for any help/guidance. \r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4584/comments",
    "author": "TrentBrick",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-09T02:00:30Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-11-09T16:45:12Z",
        "body": "I have multiple comments that I did not verify yet but they might help\r\n\r\n- If I'm not mistaken, `self.log` only works within a selection of hooks currently. I suggest you try to move the relevant code to `training_epoch_end` where `self.log` should work correctly.\r\n- set the monitor key in the `ModelCheckpoint(monitor=)` explicitly. \r\n- You have the problem that you can only update/log every n epochs: I see two solutions: 1) synchronize your ModelCheckpoint with the `period` parameter to only run on the epochs you update the monitor quantity. 2) Cache the last value and log it in the epochs between your regular interval, to make the ModelCheckpoint see it as unchanged. The second option may even be the default behavior by Lightning but need to verify.\r\n\r\nSo in summary, I imagine something like this:\r\n```python\r\n\r\n# Model\r\n\r\ndef training_epoch_end(self, outputs):\r\n    # ... compute reward losses\r\n    \r\n    if self.current_epoch % self.hparams['eval_every']==0:\r\n        self.last_eval_mean = # compute the new eval mean\r\n\r\n     self.log(\"eval_mean\", self.last_eval_mean)\r\n\r\n\r\n# Trainer\r\ntrainer = Trainer(callbacks=[ModelCheckpoint(monitor=\"eval_mean\")]\r\n\r\n# or maybe also try\r\ntrainer = Trainer(callbacks=[ModelCheckpoint(monitor=\"eval_mean\", period=self.hparams['eval_every'])]\r\n```"
      },
      {
        "user": "TrentBrick",
        "created_at": "2020-11-09T18:16:27Z",
        "body": "Thanks for this all of this. It sounds like the fundamental problem may be that with my code I was not logging from `training_epoch_end()`? Because I was setting `monitor=` to explicitly track either `eval_mean` or `mean_reward` but they wouldn't be detected. \r\n\r\nI will try this and let you know if it works. "
      },
      {
        "user": "NotNANtoN",
        "created_at": "2020-11-22T12:11:40Z",
        "body": "I had a very similar issue: in my reinforcement learning framework I wanted to measure the validation performance of my agent. Of course I would do so without a `validation_dataloader`, hence I thought I could just set that dataloader to `None` and define a `validation_step` myself. Unfortunately, given a `validation_dataloader` that is `None`, `validation_step` and all validation methods are not called at all.\r\nI tried to solve this via a callback to `on_train_epoch_end` or `on_epoch_end`. This worked, but in those callbacks the `self.log()` call does not work at all - most importantly, there is no feedback from pytorch_lightning that the call was unsuccesfull. Luckily enough, I found this thread here and moved my validation code into `training_epoch_end`, which works.\r\n\r\nMaybe pytorch_lightning could at least give a warning once one tries to use `self.log()` in a place where it has no effect?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-11-23T03:13:54Z",
        "body": "Regarding the `self.log()` from callbacks, @tchaton was working on this in #3813 and it should now be working."
      }
    ]
  },
  {
    "number": 4562,
    "title": "TypeError: __init__() got an unexpected keyword argument 'max_nb_epochs'",
    "created_at": "2020-11-07T04:18:10Z",
    "closed_at": "2020-11-07T04:34:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4562",
    "body": "```\r\nfrom pytorch_lightning import Trainer\r\nfrom argparse import ArgumentParser\r\n# from research_seed.digit_recognition.mnist import MNISTRecognizer\r\n\r\n\r\n# def main():\r\nmodel = MNISTRecognizer()\r\ntrainer = Trainer(max_nb_epochs=50,gpus=1)\r\ntrainer.fit(model)\r\n```\r\npytorch-lightning==0.9.1rc1\r\n\r\nThere is error:\r\nTypeError: __init__() got an unexpected keyword argument 'max_nb_epochs'",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4562/comments",
    "author": "tianke0711",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-07T04:18:53Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-07T04:34:04Z",
        "body": "max_nb_epochs is deprecated long ago.\nUse `max_epochs`.\n\nClosing it as it is resolved, feel free to reopen if needed"
      }
    ]
  },
  {
    "number": 4529,
    "title": "self.log with Metric in ddp",
    "created_at": "2020-11-05T11:52:38Z",
    "closed_at": "2020-12-14T05:49:21Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4529",
    "body": "```python\r\ndef validation_epoch_end(self, outputs):\r\n    self.log('metric', self.metric.compute()) # rank = 0\r\n\r\ndef validation_epoch_end(self, outputs):\r\n    out = self.metric.compute()\r\n    self.log('metric', out) # rank =0\r\n\r\ndef validation_epoch_end(self, outputs):\r\n    if self.global_rank == 0:\r\n        out = self.metric.compute()\r\n        self.log('metric', out) # rank =0\r\n    dist.barrier()\r\n```\r\n\r\n> global_rank\r\nThe global_rank of this LightningModule. Lightning saves logs, weights etc only from global_rank = 0. You normally do not need to use this property\r\nGlobal rank refers to the index of that GPU across ALL GPUs. For example, if using 10 machines, each with 4 GPUs, the 4th GPU on the 10th machine has global_rank = 39\r\n>\r\nAs the doc says, self.log will be call when global_rank = 0, but metric.compute will auto call self.reset().\r\nSo which example will work correctly?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4529/comments",
    "author": "Maybewuss",
    "comments": [
      {
        "user": "Maybewuss",
        "created_at": "2020-11-05T11:57:16Z",
        "body": "> The global_rank of this LightningModule. Lightning saves logs, weights etc only from global_rank = 0. You normally do not need to use this property\r\n>\r\nBesides, I want to know Lightning whether use dist.barrier() when saving log or checkpoint etc from global_rank = 0?"
      },
      {
        "user": "Maybewuss",
        "created_at": "2020-11-05T12:01:30Z",
        "body": "```python\r\ndef rank_zero_only(fn):\r\n\r\n    @wraps(fn)\r\n    def wrapped_fn(*args, **kwargs):\r\n        if rank_zero_only.rank == 0:\r\n            return fn(*args, **kwargs)\r\n\r\n    return wrapped_fn\r\n```\r\nThe decorator does not use dist.barrier()"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-11-05T18:49:04Z",
        "body": "If you are asking which of the three methods correctly logs the metric value, I see no problem with any of them (case 1 and 2 are identical). The recommended way is to just call `self.log(\"metric\", self.metric, on_epoch=True)`.\r\nI don't think we have a explicit barrier before saving log or checkpoints, but `dist.barrier()` is called every time `metric.compute()` is explicitly called.  "
      },
      {
        "user": "Maybewuss",
        "created_at": "2020-11-05T23:17:53Z",
        "body": "> If you are asking which of the three methods correctly logs the metric value, I see no problem with any of them (case 1 and 2 are identical). The recommended way is to just call `self.log(\"metric\", self.metric, on_epoch=True)`.\n> \n> I don't think we have a explicit barrier before saving log or checkpoints, but `dist.barrier()` is called every time `metric.compute()` is explicitly called.  \n\nI think case2 may cause bug in ddp because metric.compute will reset the metric and it will be call in all processes."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-11-06T10:25:02Z",
        "body": "> I think case2 may cause bug in ddp because metric.compute will reset the metric and it will be call in all processes.\r\n\r\nI don't follow why that should give a bug.\r\nDo you have code to reproduce?"
      },
      {
        "user": "Maybewuss",
        "created_at": "2020-11-06T10:47:50Z",
        "body": "> > I think case2 may cause bug in ddp because metric.compute will reset the metric and it will be call in all processes.\r\n> \r\n> I don't follow why that should give a bug.\r\n> Do you have code to reproduce?\r\n\r\nBecause self.log will be used only in one process(global_rank=0), but validation_epoch_end will be executed in all process. \r\nAnd Metric.compute() will reset metric.\r\nSo if i have 4 process, if process 1 execute validation_epoch_end earlier than process 0, values in metric will be reset, when process 0 execute self.log, it will return wrong results. Is there a problem with what i think?"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-11-06T11:18:47Z",
        "body": "I don't think so. As I described earlier, when `metric.compute()` is called the different processes will sync, such that we get the correct result. Additionally, this computed value is stored in a internal cache which means that no matter how many times compute is called after the first compute (but before any update is called) the process will return the already computed result.\r\n```\r\nval = self.metric().compute() # states are reset, but computed value is cashed\r\nval2 = self.metric() # return computed cashed value\r\nprint(val==val2) # True\r\nself.metric.update(...) # update states, computed cashed value will be set to None\r\n```"
      },
      {
        "user": "Maybewuss",
        "created_at": "2020-11-06T11:41:55Z",
        "body": "> I don't think so. As I described earlier, when `metric.compute()` is called the different processes will sync, such that we get the correct result. Additionally, this computed value is stored in a internal cache which means that no matter how many times compute is called after the first compute (but before any update is called) the process will return the already computed result.\r\n> \r\n> ```\r\n> val = self.metric().compute() # states are reset, but computed value is cashed\r\n> val2 = self.metric() # return computed cashed value\r\n> print(val==val2) # True\r\n> self.metric.update(...) # update states, computed cashed value will be set to None\r\n> ```\r\n\r\nCool design! Is there such a situation that process 1 execute metric.compute() first and execute metric.update() in validation_step before process 0 execute metric.compute...."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-11-06T13:50:49Z",
        "body": "Each process have individual metric states and are only synced on `compute()`. This means, that even if process 1 executes `metric.update` before process 0 is done logging the result from `metric.compute()`, the `update` method will only influence the state of the metric at process 1, thus process 0 should still log the correct result."
      },
      {
        "user": "Maybewuss",
        "created_at": "2020-11-07T02:02:07Z",
        "body": "> This means, that even if process 1 executes metric.update before process 0 is done logging the result from metric.compute(),  the update method will only influence the state of the metric at process 1, thus process 0 should still log the correct result.\r\n\r\nWhy process 0 should log the correct result? As you say metric.compute will sync and currently the state of metric at process1 had been changed."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-07T02:50:42Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4513,
    "title": "ModuleNotFoundError: No module named 'pytorch_lightning.trainer.seed'",
    "created_at": "2020-11-04T13:42:23Z",
    "closed_at": "2020-11-04T14:06:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4513",
    "body": "Hello, I updated pl to version 1.0.5  under my user environment of a HPC cluster managed by slurm which had installed pl version 0.7.6. When i launch a job i get the following error:\r\n\r\n `Traceback (most recent call last):\r\n  File \"train_pinn.py\", line 3, in <module>\r\n    from pytorch_lightning import trainer\r\n  File \"/home/2006011/fgonza01/.python-DL-3.6.9/site-packages/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 79, in <module>\r\n    __import__('pkg_resources').declare_namespace(__name__)\r\n  File \"/soft/Python3-DL1908/3.6.9/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2284, in declare_namespace\r\n    _handle_ns(packageName, path_item)\r\n  File \"/soft/Python3-DL1908/3.6.9/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2217, in _handle_ns\r\n    loader.load_module(packageName)\r\n  File \"/soft/Python3-DL1908/3.6.9/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 55, in <module>\r\n    from pytorch_lightning.trainer.seed import seed_everything\r\nModuleNotFoundError: No module named 'pytorch_lightning.trainer.seed'`  \r\n\r\nIt appears that version 1.0.5 no longer has a seed module under training but don't know why it is trying to import it. Any ideas how to fix this?\r\n\r\n\r\n#### What's your environment?\r\n\r\n - OS: [Linux]\r\n - Packaging pip\r\n - Version 1.0.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4513/comments",
    "author": "ferngonzalezp",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-04T13:43:07Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-04T14:06:06Z",
        "body": "You can import that with `from pytorch_lightning import seed_everything` in latest version.\r\n\r\nClosing it as it is resolved, feel free to reopen if needed"
      }
    ]
  },
  {
    "number": 4499,
    "title": "How to use lightning without any explicit train_dataloader?",
    "created_at": "2020-11-03T17:03:42Z",
    "closed_at": "2020-12-11T22:08:34Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4499",
    "body": "## ❓ Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI'm working on Neural Style Transfer (the original paper by Gatys et. al.). The method is essentially specific to one style and one content image only, which can be loaded in the `__init__()` method itself. And there's no other data set required (training or validation). Once we have the style and content images, we iteratively generate the stylized image (and the logic resides in `training_step()`).\r\n\r\nHowever, Lightning requires at minimum to define a `train_dataloader()`. How should I move forward now? Should I create a dummy dataloader? Is there a recommended way to get around this? \r\n\r\n\r\n#### What's your environment?\r\n\r\n - OS: Linux, Win\r\n - Packaging: conda\r\n - Version: 1.0.4\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4499/comments",
    "author": "MrinalJain17",
    "comments": [
      {
        "user": "Vichoko",
        "created_at": "2020-11-03T17:52:27Z",
        "body": "I think you can define a dummy `train_dataloader`, you need to be careful though in `training_step` it will be iterated and the element will be passed to the `training_step` as the `batch` argument.\r\n\r\nIf the training step involves only one step, maybe would be a good practice to pass the example through a list of one element in train_dataloader and retrieve it through the `batch` argument, instead of referencing a custom variable through the init and then calling it in the training_step."
      },
      {
        "user": "tchaton",
        "created_at": "2020-11-04T15:31:16Z",
        "body": "Hey @MrinalJain17,\r\n\r\nThanks for raising such questions. Really interesting :)\r\n\r\nMy hacky way to resolve this: Create an infinite dataloader returning None with a length chosen based on your learning rate scheduling scheme, load both images directly into the model, perform optimisation in training_step.\r\n\r\nI hope it helps.\r\nT.C\r\n"
      },
      {
        "user": "MrinalJain17",
        "created_at": "2020-11-04T18:38:51Z",
        "body": "> \r\n> \r\n> I think you can define a dummy `train_dataloader`, you need to be careful though in `training_step` it will be iterated and the element will be passed to the `training_step` as the `batch` argument.\r\n> \r\n> If the training step involves only one step, maybe would be a good practice to pass the example through a list of one element in train_dataloader and retrieve it through the `batch` argument, instead of referencing a custom variable through the init and then calling it in the training_step.\r\n\r\nHi @Vichoko . Yes, what you recommended makes complete sense. Although the reason I want to load the images through init is because style transfer requires the computation of a gram matrix - which is an expensive operation (involves a forward pass through a VGG network). We do this for both the generated and style image at each iteraton.\r\n\r\nHowever, since the style image remains the same throughout the process - its gram matrix can be pre-computed, which should effectively reduce the training time by half (since we now only pass the generated image through VGG to get its gram matrix, and use the pre-computed one to get the overall loss).\r\n\r\n\r\n\r\n> \r\n> \r\n> Hey @MrinalJain17,\r\n> \r\n> Thanks for raising such questions. Really interesting :)\r\n> \r\n> My hacky way to resolve this: Create an infinite dataloader returning None with a length chosen based on your learning rate scheduling scheme, load both images directly into the model, perform optimisation in training_step.\r\n> \r\n> I hope it helps.\r\n> T.C\r\n\r\nHey @tchaton , thank a lot for your suggestion - I'm planning to do something very similar.\r\n\r\nSomething that's bothering me is that the purpose of such a dummy dataloader is literally so that the training loop in the lightning trainer works fine - it adds no other value. I was thinking maybe I can load the images (style and content) in a datamodule (instead of the LightningModule) and compute their gram matrix there (like in `setup()` maybe). Then, return that same gram matrix for every iteration (things like batch size and length would have to be set to 1, I guess). This way, atleast the datamodule will serve a purpose. It's still hacky of course 😅. I'll probably update how things go here on this thread."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-04T19:43:59Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4465,
    "title": "How to save the latest and best checkpoint?",
    "created_at": "2020-11-01T10:25:10Z",
    "closed_at": "2020-11-01T12:01:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4465",
    "body": "I can set a checkpoing callback to save best model, but I also want it save the latest model, so that i can `resume_from_checkpoint` from latest checkpoint. how to do this?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4465/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-11-01T11:56:07Z",
        "body": "@xiadingZ  There is `save_last` parameter in ModelCheckpoint that saves the last epoch"
      },
      {
        "user": "xiadingZ",
        "created_at": "2020-11-01T12:01:07Z",
        "body": "Thanks"
      }
    ]
  },
  {
    "number": 4404,
    "title": "Last epoch run produces a tqdm exception",
    "created_at": "2020-10-28T01:00:10Z",
    "closed_at": "2020-12-07T01:50:44Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4404",
    "body": "## ❓ Questions and Help\r\n\r\nI have started using pytorch-lightning only recently. I'm just playing with MNIST data so as to get used to the API. Everything works perfectly fine but the last epoch run always produces an exception. I don't know if I'm doing something wrong or is it an issue with tqdm. Any help would be appreciated! Also, is there a way to print `loss` and `accuracy` without showing the progress bar? \r\n```\r\n | Name      | Type             | Params\r\n-----------------------------------------------\r\n0 | l1        | Linear           | 200 K \r\n1 | l2        | Linear           | 16 K  \r\n2 | l3        | Linear           | 4 K   \r\n3 | l4        | Linear           | 650   \r\n4 | drop      | Dropout          | 0     \r\n5 | criterion | CrossEntropyLoss | 0     \r\n6 | train_acc | Accuracy         | 0     \r\n7 | valid_acc | Accuracy         | 0     \r\nEpoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1017/1017 [00:09<00:00, 100.37it/s, loss=0.336, v_num=24, train_loss=2.33, train_acc=0.143Exception ignored in: <function tqdm.__del__ at 0x7fd9c4374f80>                                                                                                                                        \r\nTraceback (most recent call last):\r\n  File \"/home/anaconda3/envs/pt-gpu/lib/python3.7/site-packages/tqdm/std.py\", line 1128, in __del__\r\n  File \"/home/anaconda3/envs/pt-gpu/lib/python3.7/site-packages/tqdm/std.py\", line 1341, in close\r\n  File \"/home/anaconda3/envs/pt-gpu/lib/python3.7/site-packages/tqdm/std.py\", line 1520, in display\r\n  File \"/home/anaconda3/envs/pt-gpu/lib/python3.7/site-packages/tqdm/std.py\", line 1131, in __repr__\r\n  File \"/home/anaconda3/envs/pt-gpu/lib/python3.7/site-packages/tqdm/std.py\", line 1481, in format_dict\r\nTypeError: cannot unpack non-iterable NoneType object\r\n```\r\nHere is the python script I'm running:\r\n```python\r\nclass ResNetPL(pl.LightningModule):\r\n    def __init__(self, lr=1e-3):\r\n        super().__init__()\r\n        self.l1 = nn.Linear(28 * 28, 256)\r\n        self.l2 = nn.Linear(256, 64)\r\n        self.l3 = nn.Linear(64, 64)\r\n        self.l4 = nn.Linear(64, 10)\r\n        self.drop = nn.Dropout(p=0.6)\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        self.train_acc = pl.metrics.Accuracy()\r\n        self.valid_acc = pl.metrics.Accuracy()\r\n        self.learning_rate = lr\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.l1(x))\r\n        h1 = F.relu(self.l2(x))\r\n        h2 = F.relu(self.l3(h1))\r\n        do = self.drop(h1 + h2)\r\n        logits = self.l4(do)\r\n        return logits\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\r\n        return optimizer\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n\r\n        # forward pass\r\n        logits = self(x)  \r\n\r\n        # compute the objective function\r\n        loss = self.criterion(logits, y)\r\n\r\n        self.train_acc(logits, y)\r\n\r\n        self.log(\r\n            \"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True\r\n        )\r\n        self.log(\r\n            \"train_acc\",\r\n            self.train_acc,\r\n            on_step=False,\r\n            on_epoch=True,\r\n            prog_bar=True,\r\n        )\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n\r\n        x = x.view(x.size(0), -1)\r\n        logits = self(x)\r\n        loss = self.criterion(logits, y)\r\n\r\n        # return the accuracy\r\n        acc_valid = self.valid_acc(logits, y)\r\n        \r\n        self.log(\"loss\", loss, on_step=False, on_epoch=False, prog_bar=False)\r\n        self.log(\r\n            \"acc\",\r\n            self.valid_acc,\r\n            on_step=False,\r\n            on_epoch=False,\r\n            prog_bar=False,\r\n        )\r\n        return {\"loss_val\": loss, \"acc_val\": acc_valid}\r\n\r\n    def validation_epoch_end(self, val_step_outputs):\r\n        avg_val_loss = torch.tensor(\r\n            [x[\"loss_val\"] for x in val_step_outputs]\r\n        ).mean()\r\n        avg_val_acc = torch.tensor(\r\n            [x[\"acc_val\"] for x in val_step_outputs]\r\n        ).mean()\r\n        self.log(\r\n            \"avg_val_loss\",\r\n            avg_val_loss,\r\n            on_step=False,\r\n            on_epoch=True,\r\n            prog_bar=True,\r\n        )\r\n        self.log(\r\n            \"avg_val_acc\",\r\n            avg_val_acc,\r\n            on_step=False,\r\n            on_epoch=True,\r\n            prog_bar=True,\r\n        )\r\n\r\n    def prepare_data(self):\r\n        transform = transforms.Compose(\r\n            [\r\n                transforms.ToTensor(),\r\n                transforms.Normalize(\r\n                    (0.1307,),\r\n                    (0.3081,),\r\n                ),\r\n            ]\r\n        )\r\n        mnist_data = torchvision.datasets.MNIST(\r\n            \"data\", train=True, transform=transform, download=True\r\n        )\r\n        self.mnist_train, self.mnist_val = torch.utils.data.random_split(\r\n            mnist_data, [55000, 5000]\r\n        )\r\n\r\n    def train_dataloader(self):\r\n        train_loader = torch.utils.data.DataLoader(\r\n            self.mnist_train,\r\n            batch_size=64,\r\n            shuffle=True,\r\n            num_workers=8,\r\n        )\r\n        return train_loader\r\n\r\n    def val_dataloader(self):\r\n        val_loader = torch.utils.data.DataLoader(\r\n            self.mnist_val, batch_size=32, shuffle=False, num_workers=8\r\n        )\r\n        return val_loader\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = ResNetPL()\r\n    trainer = pl.Trainer(gpus=1, max_epochs=2)\r\n\r\n    # find the best lr\r\n    lr_finder = trainer.tuner.lr_find(model)\r\n    print(f\"Best learning rate: {lr_finder.suggestion()}\")\r\n\r\n    # plot the lr curve\r\n    fig = lr_finder.plot(suggest=True)\r\n    fig.show()\r\n\r\n    # set the best lr as the model lr\r\n    model.learning_rate = lr_finder.suggestion()\r\n    trainer.fit(model)\r\n```\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### Environment:\r\n\r\n - OS: Linux\r\n - Packaging: pip\r\n - Version: 1.0.3\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4404/comments",
    "author": "LakshyaMalhotra",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-10-28T01:00:55Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "bipinKrishnan",
        "created_at": "2020-10-28T05:39:11Z",
        "body": "Hey @LakshyaMalhotra , I was not able to reproduce your issue, I have copied your code for \"ResNetPL\" class and after that completed the training without any errors with the following code on Google Colab:\r\n\r\n```python\r\nmodel = ResNetPL()\r\ntrainer = pl.Trainer(max_epochs=2, gpus=1, progress_bar_refresh_rate=60)\r\n\r\nlr_best = trainer.tuner.lr_find(model)\r\nmodel.learning_rate = lr_best.suggestion()\r\n\r\ntrainer.fit(model)\r\n```"
      },
      {
        "user": "Vichoko",
        "created_at": "2020-10-29T20:48:04Z",
        "body": "Try setting `drop_last=True` attribute on DataLoader."
      },
      {
        "user": "LakshyaMalhotra",
        "created_at": "2020-10-30T12:30:17Z",
        "body": "@Vichoko I'm still having the same error even after setting `drop_last=True` in the dataloader."
      },
      {
        "user": "Vichoko",
        "created_at": "2020-10-30T14:39:20Z",
        "body": "Ok, my bad.\r\nNow taking a further look I noted the aggregation behaviour you added on `validation_epoch_end` isn't needed as the loss and accuracy metrics are aggregated by themselves on epoch end. I'd remove this method completely.\r\n\r\nAlso, I'd homologate the metric names on validation step as you are using 3 different types, loss and accuracy, then loss_val and acc_val and in the train step you are using train_loss and train_acc. I'm pretty sure that you don't need to return anything on train and Val steps, I'd remove those returns just to keep the code clear. And I'd stick to the train_{metric} and val_{metric} name convention.\r\n\r\nFinally, I'd remove the keyword params of all the self.log calls you are using as they are chosen automatically depending on if you are training or evaluating for the most standard configuration."
      },
      {
        "user": "LakshyaMalhotra",
        "created_at": "2020-10-30T21:28:32Z",
        "body": "@Vichoko Thanks for your reply! Like you mentioned I changed my code but still I got the same error. Following is the updated code:\r\n```\r\nimport os\r\nimport torch\r\nimport torchvision\r\nfrom torchvision import transforms\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport pytorch_lightning as pl\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\nclass ResNetPL(pl.LightningModule):\r\n    def __init__(self, lr=1e-3):\r\n        super().__init__()\r\n        self.l1 = nn.Linear(28 * 28, 256)\r\n        self.l2 = nn.Linear(256, 64)\r\n        self.l3 = nn.Linear(64, 64)\r\n        self.l4 = nn.Linear(64, 10)\r\n        self.drop = nn.Dropout(p=0.6)\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        self.train_acc = pl.metrics.Accuracy()\r\n        self.valid_acc = pl.metrics.Accuracy()\r\n        self.learning_rate = lr\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.l1(x))\r\n        h1 = F.relu(self.l2(x))\r\n        h2 = F.relu(self.l3(h1))\r\n        do = self.drop(h1 + h2)\r\n        logits = self.l4(do)\r\n        return logits\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\r\n        return optimizer\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n\r\n        # forward pass\r\n        logits = self(x)  # l: logits\r\n\r\n        # compute the objective function\r\n        train_loss = self.criterion(logits, y)\r\n\r\n        self.train_acc(logits, y)\r\n\r\n        self.log(\"train_loss\", train_loss, prog_bar=True)\r\n        self.log(\"train_acc\", self.train_acc, prog_bar=True)\r\n        return train_loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n\r\n        x = x.view(x.size(0), -1)\r\n        logits = self(x)\r\n        valid_loss = self.criterion(logits, y)\r\n\r\n        self.valid_acc(logits, y)\r\n        self.log(\"valid_loss\", valid_loss, prog_bar=True)\r\n        self.log(\"valid_acc\", self.valid_acc, prog_bar=True)\r\n\r\n    def prepare_data(self):\r\n        transform = transforms.Compose(\r\n            [\r\n                transforms.ToTensor(),\r\n                transforms.Normalize((0.1307,), (0.3081,),),\r\n            ]\r\n        )\r\n        mnist_data = torchvision.datasets.MNIST(\r\n            \"data\", train=True, transform=transform, download=True\r\n        )\r\n        self.mnist_train, self.mnist_val = torch.utils.data.random_split(\r\n            mnist_data, [55000, 5000]\r\n        )\r\n\r\n    def train_dataloader(self):\r\n        train_loader = torch.utils.data.DataLoader(\r\n            self.mnist_train, batch_size=64, shuffle=True, num_workers=8,\r\n        )\r\n        return train_loader\r\n\r\n    def val_dataloader(self):\r\n        val_loader = torch.utils.data.DataLoader(\r\n            self.mnist_val, batch_size=32, shuffle=False, num_workers=8,\r\n        )\r\n        return val_loader\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = ResNetPL()\r\n    trainer = pl.Trainer(max_epochs=2)\r\n\r\n    # find the best lr\r\n    lr_finder = trainer.tuner.lr_find(model)\r\n    print(f\"Best learning rate: {lr_finder.suggestion()}\")\r\n\r\n    # plot the lr curve\r\n    fig = lr_finder.plot(suggest=True)\r\n    fig.show()\r\n\r\n    # set the best lr as the model lr\r\n    model.learning_rate = lr_finder.suggestion()\r\n    trainer.fit(model)\r\n```\r\nThe only way the code works without giving any error is when I remove the code for finding the best `learning_rate`.  Whenever I add the `lr_finder` in the code I end up in the same error."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-29T22:50:11Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4287,
    "title": "k-fold cross validation using DataModule",
    "created_at": "2020-10-21T14:57:05Z",
    "closed_at": "2021-02-09T23:46:37Z",
    "labels": [
      "duplicate",
      "feature",
      "help wanted",
      "question",
      "data handling"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4287",
    "body": "DataModule is great! \r\nI'm wondering how it can be applied to handle with k-fold cross-validation.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4287/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-10-21T18:41:05Z",
        "body": "all k models and datamodules will be independent so create seperate datamodules and models for each."
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-10-21T22:45:55Z",
        "body": "I think there is a discussion of having a cross validation feature in Lightning in #839. "
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-10-21T22:49:42Z",
        "body": "@SkafteNicki @Borda @justusschock tagging you guys here as well."
      },
      {
        "user": "Svito-zar",
        "created_at": "2021-03-09T10:16:26Z",
        "body": "Has this feature been implemented yet?\r\nI could not find it in the documentation ..."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2021-03-09T10:38:32Z",
        "body": "@Svito-zar not implemented, closed due to being a duplicate of #839 "
      },
      {
        "user": "macabdul9",
        "created_at": "2021-06-03T15:40:56Z",
        "body": "Any update on this?"
      }
    ]
  },
  {
    "number": 4218,
    "title": "Advice on how to use a self-supervised regression scheme within a single step in pl",
    "created_at": "2020-10-18T15:34:52Z",
    "closed_at": "2020-11-24T22:10:59Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4218",
    "body": "Hi\r\nI have the following scheme:\r\n```\r\nclass refine_P(LightningModule):\r\n\r\n    def __init__(\r\n        self, hparams,\r\n    ):\r\n        self.model = #\r\n        self.regressor #\r\n   def training_step(self, batch, batch_idx, is_train=True):\r\n      out = self.model(batch)\r\n      self.regressor.reset_parameters()\r\n      base_loss = self.base_loss(out)\r\n      while condition:\r\n         out = self.regressor(out)\r\n         loss = self.regress_loss(out)\r\n         loss.backward()\r\n         ...\r\n      return base_loss\r\n```\r\nI.e - Without using pl, it will be just two training procedures, one within the other.\r\nMy question is, Can I somehow create a trainer within this module, and call this trainer each training step of `refine_P`?\r\n       ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4218/comments",
    "author": "dvirginz",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-11-17T17:17:16Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4135,
    "title": "Accuracy metric number of classes warning question",
    "created_at": "2020-10-14T04:26:41Z",
    "closed_at": "2020-10-14T16:15:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4135",
    "body": "For context, I'm running a heavy multi-label classification model with small batches (~10 elements) and a large number of classes (~50 classes). While training I'm getting this warning on **accuracy** metric calculation:\r\n\r\n> You have set 32 number of classes which is different from predicted (6) and target (28) number of classes\r\n\r\nShould I be worried? I'm not sure why this should be a warning. \r\nI keep wondering why is it bad having fewer classes in predicted output compared with the expected number of classes.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4135/comments",
    "author": "Vichoko",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-14T11:19:44Z",
        "body": "Which version of lightning are you using?\r\nWe did a revamp of metrics lately and the class based `Accuracy` metric does not throw this warning anymore (functional still does).\r\nIt was originally put in place to warn users that we estimated a different number of labels (from pred and target) than what the user provided, and it could mean that your output was biased and not estimating values within the hole label space. That said, it does not necessarily means that there is anything wrong with your model."
      },
      {
        "user": "Vichoko",
        "created_at": "2020-10-14T16:15:35Z",
        "body": "Thank you for the clarification!\r\n\r\nI'm using version 0.9.0. Actually the warning message is: \r\n\r\n> UserWarning: You have set 32 number of classes if different from predicted (6) and target (28) number of classes warnings.warn(*args, **kwargs)\r\n\r\nI updated the message to match the current version."
      }
    ]
  },
  {
    "number": 4028,
    "title": "Patent implications and coverage",
    "created_at": "2020-10-09T13:54:12Z",
    "closed_at": "2020-10-09T14:47:49Z",
    "labels": [
      "question",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4028",
    "body": "In a now removed by the mods hacker news comment @williamFalcon claimed that the Lightning patent prevents the use of the Lightning  API by other projects (even those under an Apache license). As this seems to go against the patent indemnification clause of the Apache license I'd like a clarification on the Pytorch Lightning stance regarding projects that use or extend the code or APIs.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4028/comments",
    "author": "mejran",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-10-09T13:54:54Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-10-09T14:44:09Z",
        "body": "Nope!\r\n\r\nLightning can be used to build any frameworks on top that you want (like huggingface, nemo and the dozen of other corporate and academic projects).\r\n\r\nWe want people to build on Lightning and we want companies to deliver value and products for their users.\r\n\r\nLooks like you're from mindfulmachines which is great. There's nothing preventing you from using lightning there.\r\n\r\nOur in-process patent portfolio is not to limit the use of lightning in any way but for defensibility purposes only.\r\n\r\nIf you have any other inquiries please email legal@pytorchlightning.ai for more details."
      }
    ]
  },
  {
    "number": 3976,
    "title": "How trainer figures out number of batches per epoch.",
    "created_at": "2020-10-08T05:33:58Z",
    "closed_at": "2020-11-14T14:06:28Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3976",
    "body": "@JorDikk and I recently found out that `Trainer` figures out the total number of batches per epoch though the `Sampler` `__len__` and not `Dataset` `__len__`.\r\nWhile for most cases the size of sampler would correspond to the total number of indices in the dataset (train and val),\r\nwe were using a hierarchical dataset, where each individual dataset  was a collection of smaller datasets.\r\nOur sampler too, then was a collection of smaller samplers. This created a problem as for our base \r\nsampler, the size was the number of smaller datasets, rather than the data indices.\r\nThe fix was very easy, but it would help to mention it somewhere in the Docs to avoid much confusion.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3976/comments",
    "author": "Raahul-Singh",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-10-08T05:34:37Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-07T10:54:34Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3895,
    "title": "Support for production deployment",
    "created_at": "2020-10-06T07:46:17Z",
    "closed_at": "2020-11-12T11:20:41Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3895",
    "body": "Hi PL team,\r\n\r\nI was wondering if there is something similar to Tensorflow Extended (TFX) available in Pytorch or PytorchLightning? Something like a `production deployment pipeline` for pytorch.\r\n\r\nBy the way, thanks for this wonderful project. Very helpful. \r\n\r\nThanks,\r\nSankarshan ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3895/comments",
    "author": "msank00",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-10-06T07:46:54Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-05T08:56:11Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3803,
    "title": "Access metrics in custom callbacks",
    "created_at": "2020-10-02T19:05:49Z",
    "closed_at": "2020-10-04T02:43:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3803",
    "body": "## ❓ Questions and Help\r\n\r\nI have found it useful/helpful to sometimes access metrics in custom callbacks. In v0.9.0 this works using something like this:\r\n\r\n```\r\ndef training_step(self, batch, batch_idx):\r\n    return {\"loss\": self._step(batch)}\r\n\r\ndef validation_step(self, batch, batch_idx):\r\n    return {\"val_loss\": self._step(batch)}\r\n\r\ndef training_epoch_end(self, outputs):\r\n    # ...\r\n    return {\"interesting_key_train\": interesting_value}\r\n\r\ndef validation_epoch_end(self, outputs):\r\n    # ...\r\n    return {\"interesting_key_val\": interesting_value}\r\n```\r\n\r\nThe setup allows for the values returned in the `_epoch_end` methods to be accessed via `trainer.callback_metrics`. As such, a callback could use these values, e.g.\r\n\r\n```\r\nclass CustomCallback(Callback):\r\n\r\n    def on_validation_end(self, trainer, pl_module):\r\n        metrics = trainer.callback_metrics\r\n        interesting_value = metrics[\"interesting_key_train\"]\r\n```\r\n\r\nWhen using the current master branch, the above approach is possible for values returned in `validation_epoch_end` but no longer possible for `training_epoch_end` as setting a return value in `training_epoch_end` raises the exception,\r\n\r\n```\r\nMisconfigurationException: training_epoch_end expects a return of None. HINT: remove the return statement in training_epoch_end\r\n```\r\n\r\nAdditionally the values stored in `trainer.callback_metrics` have changed. Using the example above, in v0.9.0, it is `{\"loss\": ..., \"interesting_key_train\": ..., \"interesting_key_val\": ...}` and on master it is simply `{\"interesting_key_val\": ...}`.\r\n\r\nWhat is the intended way to access metrics (in particular from the training loop) in callbacks?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3803/comments",
    "author": "pbmstrk",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2020-10-03T22:32:34Z",
        "body": "> When using the current master branch, the above approach is possible for values returned in validation_epoch_end but no longer possible for training_epoch_end as setting a return value in training_epoch_end raises the exception,\r\n\r\nCan you use `self.log(\"interesting_key_train\", interesting_value)`? \r\n\r\nThough there does seem to be an issue with accessing metrics on epoch end on master @williamFalcon "
      },
      {
        "user": "pbmstrk",
        "created_at": "2020-10-03T23:32:48Z",
        "body": "`self.log` works on master, I had tried it earlier today and run into issues but these seem to be resolved now."
      }
    ]
  },
  {
    "number": 3757,
    "title": "Getting RuntimeError: chunk expects at least a 1-dimensional tensor when using custom collate_fn in ddp setting.",
    "created_at": "2020-09-30T23:27:33Z",
    "closed_at": "2020-11-24T23:10:53Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3757",
    "body": "## ❓ How to use custom collate function in dataloader in 'ddp'?\r\n\r\nHi,\r\n\r\nI would appreciate some help in setting up a custom collate function in a ddp setting. \r\n\r\n#### Summary\r\n\r\nI defined a custom collate function since I need to pass inputs of varying size to my code. \r\n\r\n```\r\ndef custom_collate(iter_batch):\r\n    return [(dp[0], torch.tensor(dp[1])) for dp in iter_batch]\r\n```\r\n\r\nThen I define a data loader as follows:\r\n\r\n```\r\nvalid_dataloader = DataLoader(val_dataset, batch_size=self.params['batch_size'], shuffle=False,\r\n                                           num_workers=self.params['num_workers'], collate_fn=my_collate)\r\n```\r\n\r\nand the Trainer as follows:\r\n```\r\nconfigs = { 'trainer_params': {\r\n            'max_epochs': 150,\r\n            'early_stop_callback': early_stop_callback,\r\n            # 'checkpoint_callback': checkpoint_callback,\r\n            'check_val_every_n_epoch': 1,\r\n            'gpus': -1,\r\n            'precision': 16,\r\n        }}\r\nrunner = Trainer(\r\n        logger=tt_logger,\r\n        log_save_interval=100,\r\n        train_percent_check=1.,\r\n        val_percent_check=1.,\r\n        num_sanity_val_steps=5,\r\n        distributed_backend='ddp',\r\n        **config['trainer_params'],\r\n    )\r\n```\r\nHowever, I get the following error when I run the program.\r\n\r\n```\r\n/home/rashindrie/.local/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 28 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\r\n  warnings.warn(*args, **kwargs)\r\n\r\nValidation sanity check: 0it [00:00, ?it/s]Traceback (most recent call last):\r\n  File \"train_pl.py\", line 351, in <module>\r\n    runner.fit(experiment)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1064, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/pytorch_lightning/accelerators/dp_backend.py\", line 97, in train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1224, in run_pretrain_routine\r\n    self._run_sanity_check(ref_model, model)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1257, in _run_sanity_check\r\n    eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 331, in _evaluate\r\n    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 661, in evaluation_forward\r\n    output = model(*args)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 72, in forward\r\n    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 162, in scatter\r\n    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 36, in scatter_kwargs\r\n    inputs = scatter(inputs, target_gpus, dim) if inputs else []\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 28, in scatter\r\n    res = scatter_map(inputs)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 15, in scatter_map\r\n    return list(zip(*map(scatter_map, obj)))\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 15, in scatter_map\r\n    return list(zip(*map(scatter_map, obj)))\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 17, in scatter_map\r\n    return list(map(list, zip(*map(scatter_map, obj))))\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py\", line 13, in scatter_map\r\n    return Scatter.apply(target_gpus, None, dim, obj)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py\", line 89, in forward\r\n    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\r\n  File \"/home/rashindrie/.local/lib/python3.7/site-packages/torch/cuda/comm.py\", line 148, in scatter\r\n    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\r\nRuntimeError: chunk expects at least a 1-dimensional tensor\r\n```\r\n\r\n\r\n#### Environment?\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.17.3\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tqdm:              4.46.0\r\n        - ray:              0.8.7\r\n        - tensorflow:              2.1.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.7.4\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3757/comments",
    "author": "rashindrie",
    "comments": [
      {
        "user": "fabrahman",
        "created_at": "2020-10-16T18:11:47Z",
        "body": "@Rashindrie Hi, I am facing the same issue. Did you manage to solve this? Thanks"
      },
      {
        "user": "rashindrie",
        "created_at": "2020-10-18T22:04:33Z",
        "body": "Hi @fabrahman I could not solve it. I switched into using a batch size of 1 with gradient accumulation. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-17T22:42:58Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3742,
    "title": "How to use more than one optimizer at each step (jointly train multiple modules within one model)?",
    "created_at": "2020-09-30T10:15:05Z",
    "closed_at": "2020-11-07T02:06:17Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3742",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI have a model which consists of two blocks, let's call them `first_module` and `second_module`.\r\n\r\n#### Code (simplified)\r\n**Training Step**\r\n```python\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n    out = self.first_module(batch)\r\n    out = self.second_module(out)\r\n\r\n    loss = criterion(out, batch['target'])\r\n\r\n    metrics = {'train_loss': loss}\r\n    output = {'loss': loss,\r\n                  'log': metrics,\r\n                  'progress_bar': metrics}\r\n\r\n    return output\r\n```\r\n**Optimizers**\r\n```python\r\ndef configure_optimizers(self):\r\n    train_params = self.train_params\r\n\r\n    optimizer_first_module = torch.optim.Adam(\r\n        self.first_module.parameters(),\r\n        lr=train_params['lr_first_module'], betas=(0.5, 0.999))\r\n\r\n    optimizer_second_module = torch.optim.Adam(\r\n        self.second_module.parameters(),\r\n        lr=train_params['lr_second_module'], betas=(0.5, 0.999))\r\n\r\n    return [optimizer_first_module, optimizer_second_module]\r\n```\r\n\r\n**Question**\r\nHow to do `optimizer_first_module.step()` and `optimizer_second_module.step()` at each batch and ignore `batch_idx`?\r\nIt may be seen that `optimizer_step` always passes only one optimizer per step\r\n```python\r\n    def optimizer_step(\r\n        ...\r\n        optimizer: Optimizer,\r\n        optimizer_idx: int,\r\n        ...\r\n    ) -> None:\r\n```\r\n\r\n#### Possible solution (?)\r\n* Blending two optimizers into one (hacky way, not sure if pl would see the result as a correct optimizer class)\r\n* Modify training loop in PL (this option is even worse)",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3742/comments",
    "author": "olegkhomenko",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-30T10:15:44Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-30T19:04:19Z",
        "body": "```python\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n    if optimizer_idx == 0:\r\n        out = self.first_module(batch)\r\n        out = self.second_module(out)\r\n\r\n        loss = criterion(out, batch['target'])\r\n\r\n        metrics = {'train_loss': loss}\r\n        self.output = {'loss': loss,\r\n                  'log': metrics,\r\n                  'progress_bar': metrics}\r\n\r\n    return self.output\r\n\r\ndef backward(self, closure_loss, optimizer, opt_idx):\r\n    if opt_idx == 0:\r\n        super().backward(closure_loss, optimizer, opt_idx)\r\n```\r\n\r\nAs of now, lightning doesn't support returning no loss in `training_step` so you can attach it to self. But once it does you will be able to return results with no loss or None in future."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-30T23:22:14Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3727,
    "title": "Increasing batch size not speeding up training time.",
    "created_at": "2020-09-29T18:50:53Z",
    "closed_at": "2020-11-06T00:13:52Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3727",
    "body": "Not sure this is a \"how to question\", but its not a bug report, more of me trying to understand if my intuition is off or there is something wrong with my code. I'm using 4 GPUs while training a stacked LSTM and trying different batch sizes to see the effect. Generally my experience is that increasing the batch size results in finishing epochs faster, however I am not seeing that in my use case. I do see the effect on the GPU usage through nvidia-smi, the larger batch sizes show higher memory usage and utilization, but all three finish a single epoch in roughly the same time (largest batch size about 5 minutes slower). \r\n\r\nBatch size 64, 1h4m for 1st epoch:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.44       Driver Version: 440.44       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\r\n| 51%   83C    P2   183W / 250W |   3522MiB / 11178MiB |     78%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\r\n| 65%   85C    P2   206W / 250W |   4193MiB / 11178MiB |     78%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\r\n| 58%   85C    P2   201W / 250W |   3573MiB / 11178MiB |     79%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 108...  Off  | 00000000:0C:00.0 Off |                  N/A |\r\n| 50%   82C    P2   172W / 250W |   3071MiB / 11178MiB |     81%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\nBatch size 128, 1h3m to complete 1st epoch:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.44       Driver Version: 440.44       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\r\n| 47%   80C    P2   191W / 250W |   4950MiB / 11178MiB |     87%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\r\n| 49%   83C    P2   152W / 250W |   5243MiB / 11178MiB |     90%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\r\n| 48%   83C    P2   159W / 250W |   5231MiB / 11178MiB |     86%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 108...  Off  | 00000000:0C:00.0 Off |                  N/A |\r\n| 43%   74C    P2   167W / 250W |   7199MiB / 11178MiB |     91%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```                                                                               \r\nBatch size 256, 1h10m to complete 1st epoch:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.44       Driver Version: 440.44       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\r\n| 44%   76C    P2   167W / 250W |   9864MiB / 11178MiB |     95%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\r\n| 46%   79C    P2   160W / 250W |   7777MiB / 11178MiB |     98%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\r\n| 46%   78C    P2   165W / 250W |   6265MiB / 11178MiB |     96%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 108...  Off  | 00000000:0C:00.0 Off |                  N/A |\r\n| 43%   74C    P2   171W / 250W |  10173MiB / 11178MiB |     97%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\nIs this indicative of problems in my code? Or am I just wrong to assume increasing batch size from 64 will improve performance significantly, since it is already at ~ 80% GPU-utilization? Any input would be appreciated.\r\n\r\n - OS: Linux\r\n - Packaging Conda\r\n - Version 0.8.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3727/comments",
    "author": "mitchelldehaven",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-29T18:51:38Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-29T19:50:13Z",
        "body": "Maybe the way RNN's work is the reason for the same time even after increasing the batch_size since each sequence output is used for computation of activations at the next sequence step and this computation of matrix-multiplication is taking time for larger batch_size. Not 100% sure though."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-29T23:43:02Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3698,
    "title": "How to keep some LightningModule's parameters on cpu when using CUDA devices for training",
    "created_at": "2020-09-28T11:46:05Z",
    "closed_at": "2020-10-18T08:56:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3698",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n#### What is your question?\r\nI tried to transform my code into Lightning yesterday, but the CUDA OOM error occurred. My model has a very large parameter ```nn.Embedding(24000000, 128)``` (more than 22GB), which obviously exceeds the memory of my CUDA device. I implemented two classes to sovle this problem in my torch_version code, the pseudo code is as follows:\r\n\r\n#### PyTorch Code\r\n```python\r\nclass Emb(nn.Module):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.emb = nn.Emebdding(24000000, 128)\r\n        def forward(self, idx):\r\n                return self.emb(idx)\r\n\r\nclass MyModule(nn.Module):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n        def forward(self, input):\r\n                out = self.calculation(input)\r\n                return out\r\n\r\n# train part:\r\nget_emb = Emb()\r\nmodel = MyModule()\r\nmodel = model.cuda()\r\noptimizer = some_optimizer([{\"params\": e.parameters}, {\"params\": model.parametersba}], lr=1e-3)\r\nloss_metric = some_loss()\r\nfor epo in epoch:\r\n        for x, y in dataloader:\r\n                embs = get_emb(x.cpu()).cuda()\r\n                out = model(embs)\r\n                loss = loss_metric(out, y)\r\n                optimizer.zero_grad()\r\n                loss.backward()\r\n                optimizer.step()\r\n```\r\nThe torch_version code above keeps the nn.Embedding on cpu and ensures that the optimization of training is completed on CUDA devices. But I don't know how to achieve this via pytorch_lightning, because the entire 'training' part is encapsulated in training_step. The PL code  is as follows:\r\n\r\n#### PL Code\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = nn.Embedding(24000000, 128)\r\n                self.loss_metric = some_loss()\r\n        def training_step(self, batch, batch_idx):\r\n                x, y = batch\r\n                embs = self.emb(x)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n\r\n# train part\r\nmodel = MyModule()\r\ntrainer = pl.Trainer(gpus=-1)\r\ntrainer.fit(model, dataloader)\r\n```\r\nSo, is there any recommended way to keep a part of the LightningModule's parameters on cpu when using CUDA devices for training? \r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu 16.04.6 LTS\r\n - CUDA: version 10.2, 2080Ti\r\n - Version 0.9.0\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3698/comments",
    "author": "David-AJ",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-28T11:46:44Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-28T20:35:35Z",
        "body": "if you do this:\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = [nn.Embedding(24000000, 128)]\r\n                self.loss_metric = some_loss()\r\n        def forward(self, input):\r\n                x, y = input\r\n                embs = self.emb[0](x.cpu()).to(self.device)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n```\r\nit should work I guess. Can't think of a better solution than this :sweat_smile: "
      },
      {
        "user": "David-AJ",
        "created_at": "2020-09-29T03:39:41Z",
        "body": "> if you do this:\r\n> \r\n> ```python\r\n> class MyModule(pl.LightningModule):\r\n>         def __init__(self):\r\n>                 xxxxxx # some init operations\r\n>                 self.calculation = some_calculation()\r\n>                 self.emb = [nn.Embedding(24000000, 128)]\r\n>                 self.loss_metric = some_loss()\r\n>         def forward(self, input):\r\n>                 x, y = input\r\n>                 embs = self.emb[0](x.cpu()).to(self.device)\r\n>                 out = self.calculation(embs)\r\n>                 return {\"loss\": self.loss_metric(out, y)}\r\n> ```\r\n> \r\n> it should work I guess. Can't think of a better solution than this 😅\r\n\r\n@rohitgr7 Really?! In this case, will the self.emb be saved in ckpt along with other parameters of ```MyModule```? Sorry, I just noticed that I had a typo in the PL Code: ```forward(self, input) -> training_step(self, batch, batch_idx)```"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-29T07:48:51Z",
        "body": "Yeah, won't save. Didnt think of that. If any module in the lightning has a `.to` method then it will be moved to device. Somehow need to think of a way to override this `.to` method for embeddings."
      },
      {
        "user": "David-AJ",
        "created_at": "2020-09-30T03:23:40Z",
        "body": "@rohitgr7 I tried to use ```self.emb = nn.Embedding(24000000, 128).cpu()``` in lightning code, but it failed. Actually, it is very common in recommendation system to use this kind of large-scale embedding as the trainable weight of the model. For example, the sparse features of User Id (more than 24000000) can be represented by a dense embedding matrix. So is there any possible to implement this operation in Pytorch-Lightning? "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-30T20:35:33Z",
        "body": "Looked at PyTorch source code. Found something. Can you try this?  @David-AJ\r\n```python\r\n\r\nclass SpecialEmbedding(nn.Module):\r\n        def __init__(self, fin, fout):\r\n                self.emb = nn.Embedding(fin, fout)\r\n\r\n        def _apply(self, fn):\r\n                return self\r\n\r\n        def forward(self, x):\r\n                return self.emb(x)\r\n\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = SpecialEmbedding(24000000, 128)\r\n                self.loss_metric = some_loss()\r\n\r\n        def training_step(self, batch, batch_idx):\r\n                x, y = batch\r\n                embs = self.emb(x.cpu()).to(self.device)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n\r\n# train part\r\nmodel = MyModule()\r\ntrainer = pl.Trainer(gpus=-1)\r\ntrainer.fit(model, dataloader)\r\n```"
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-09T06:50:50Z",
        "body": "@rohitgr7 Thanks for your kindly help! But this ```SpecialEmbedding ```code failed again 😅\r\nthe error message is as follows:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"debug.py\", line 742, in <module>\r\n    trainer.fit(model)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1064, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/accelerators/dp_backend.py\", line 97, in train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1239, in run_pretrain_routine\r\n    self.train()\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 394, in train\r\n    self.run_training_epoch()\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 491, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 844, in run_training_batch\r\n    self.hiddens\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 1015, in optimizer_closure\r\n    hiddens)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 1197, in training_forward\r\n    output = self.model(*args)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 70, in forward\r\n    \"them on device: {}\".format(self.src_device_obj, t.device))\r\nRuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu\r\n```\r\nIt seems that pytorch_lightning forces the parameters of a module to be set on the same device?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-10-09T19:58:46Z",
        "body": "not 100% sure why is this `RuntimeError` should be raised. @awaelchli any suggestions on how to make this work/?\r\n\r\nActually now I also want to know if this is the right way or not or there is another way around since it seems super useful.\r\n\r\n@David-AJ is it working on a single GPU device with no distributed backend?"
      },
      {
        "user": "chiragraman",
        "created_at": "2020-10-09T20:03:35Z",
        "body": "@rohitgr7 did you mean to tag @David-AJ ?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-10-09T20:09:02Z",
        "body": "oops, my bad, was looking at your issue too #3998 side by side :sweat_smile: "
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-10T03:35:22Z",
        "body": "> not 100% sure why is this `RuntimeError` should be raised. @awaelchli any suggestions on how to make this work/?\r\n> \r\n> Actually now I also want to know if this is the right way or not or there is another way around since it seems super useful.\r\n> \r\n> @David-AJ is it working on a single GPU device with no distributed backend?\r\n\r\nHi @rohitgr7, I tried to run this code on a single GPU and the ```RuntimeError``` was raised again."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-11T16:15:24Z",
        "body": "do you want to train the embedding layer or is it pretrained?\r\nIf you want to train it, I'm afraid you can't have it on cpu while also using DP. The error you got above is because DataParallel detects that. "
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-12T02:58:01Z",
        "body": "@awaelchli Yes, I want to train it, how about using DDP or any other distributed backend? Actually @rohitgr7‘s first solution ```self.emb = [nn.Embedding(24000000, 128)]``` could make ```self.emb``` on CPU while training with DP, but in that case the ```self.emb``` won't be saved in the ckpt, nor can be loaded using load_from_checkpoint. Could this problem be solved by overriding the ```on_save_checkpoint``` and ```on_load_checkpoint```?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-12T03:01:40Z",
        "body": "> `self.emb = [nn.Embedding(24000000, 128)]`\r\n\r\n🤣  this is a funny trick. Very creative. Yeah, this makes torch unaware of this module, and keeps it on the cpu. \r\n\r\n> Could this problem be solved by overriding the on_save_checkpoint and on_load_checkpoint?\r\n\r\nYes, I think that would do the trick!\r\nBut will this this embedding layer not be a huge bottleneck? You will need to transfer all outputs to the GPU and this blocks execution."
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-12T03:18:03Z",
        "body": "@awaelchli \r\n\r\n> But will this this embedding layer not be a huge bottleneck? You will need to transfer all outputs to the GPU and this blocks execution.\r\n\r\nLooks like I have no choice🤣 otherwise I have to train all the module on the cpu, don't know which one could be faster. The application scenario is in the recommendation system, and in fact the number of users and items far exceeds 24 million, all these ID sparse features should be represented by the embedding layer and trained in the module. Do you have any other suggestions on how to make it faster?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-17T14:22:54Z",
        "body": "> Could this problem be solved by overriding the on_save_checkpoint and on_load_checkpoint?\r\n\r\nyes. I would do it this way. grab the state dict of the embedding layer and add it to the checkpoint dict. when loading, you do the opposite and read the state dict. \r\n\r\n> Do you have any other suggestions on how to make it faster?\r\n\r\nsorry, nothing comes to my mind :("
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-18T08:56:00Z",
        "body": "I modified my code like this:\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = [nn.Embedding(24000000, 128)]\r\n                self.loss_metric = some_loss()\r\n\r\n        def training_step(self, batch, batch_idx):\r\n                x, y = batch\r\n                embs = self.emb[0](x.cpu()).to(self.device)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n\r\n        def on_save_checkpoint(self, checkpoint):\r\n                checkpoint[\"emb\"] = self.emb\r\n\r\n        def on_load_checkpoint(self, checkpoint):\r\n                self.emb = checkpoint[\"emb\"]\r\n```\r\nIt works! Thank you @rohitgr7 and @awaelchli!"
      }
    ]
  },
  {
    "number": 3697,
    "title": "EvalResult can't log 2D tensor",
    "created_at": "2020-09-28T09:38:19Z",
    "closed_at": "2020-10-05T21:56:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3697",
    "body": "I want to calculate confusion matrix in test step, but EvalResult can't log it.  How can I do to store it in EvalResult? If I have to override `test_epoch_end`, what should I do to process logged metric and my confusion matrix as default behaviour? \r\n```\r\n    def test_step(self, batch, batch_idx):\r\n        result = pl.EvalResult()\r\n        result.log('test_loss', loss, sync_dist=True)\r\n        result.log('test_vacc', vid_acc, sync_dist=True)\r\n        #result.log('confusionmatrix', cm, sync_dist=True, sync_dist_op=torch.distributed.ReduceOp.SUM)\r\n        return result\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3697/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "LucaButera",
        "created_at": "2020-09-30T10:04:26Z",
        "body": "If you are using classic TensorBoard logging you will not be able to log a non scalar tensor, as far as i know. \r\nThe workaround I used for this is to store the confusion matrix at each ```test_step``` and then compute the average in the ```test_epoch_end```. After this you can convert the matrix into an image (this is the only way i found to log it to tensorboard. If you use a logger that can log 2D tensors do as you would). Finally I delete the confusion matrix from the result, in order to avoid errors.\r\n\r\nBelow the pseudo-code\r\n\r\n```\r\ndef test_step(self, batch, batch_idx, *args, **kwargs):\r\n    # code that leads to conf matrix\r\n    confusion_matrix = # whatever you do to compute the confusion matrix\r\n    result = EvalResult()\r\n    result.confusion_matrix = confusion_matrix\r\n    # whatever you want to log additionally\r\n    return result\r\n\r\ndef test_epoch_end(self, outputs):\r\n    confusion_matrix = torch.mean(outputs.confusion_matrix, dim=0)\r\n    # whatever you need to do to log the matrix \r\n    # for example I use a function to compute the image as a tensor and then I add the image to the logger\r\n    del outputs['confusion_matrix']\r\n    # whatever aggregation you need to perform on other metrics\r\n    return outputs\r\n```\r\n\r\nMind that, during the ```test_step``` you may need to unsqueeze the confusion matrix tensor in order to make it of shape ```[1, n_classes, n_classes]``` instead of ```[n_classes, n_classes]```. This way (assuming they all have the same shape), they will be properly stacked into a tensor of shape ```[n_batches, n_classes, n_classes]``` instead of ```[n_batches * n_classes, n_classes]``` for the ```test_epoch_end```.\r\n\r\nHope this is of some help."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-10-05T21:56:35Z",
        "body": "For anything beyond scalars, you need to use the logger directly:\r\n\r\n```python\r\ntensorboard = self.logger.experiment.\r\n\r\ntensorbaord.do_whatever_it_supports()\r\n```"
      },
      {
        "user": "xiadingZ",
        "created_at": "2020-10-06T00:43:20Z",
        "body": "> If you are using classic TensorBoard logging you will not be able to log a non scalar tensor, as far as i know.\r\n> The workaround I used for this is to store the confusion matrix at each `test_step` and then compute the average in the `test_epoch_end`. After this you can convert the matrix into an image (this is the only way i found to log it to tensorboard. If you use a logger that can log 2D tensors do as you would). Finally I delete the confusion matrix from the result, in order to avoid errors.\r\n> \r\n> Below the pseudo-code\r\n> \r\n> ```\r\n> def test_step(self, batch, batch_idx, *args, **kwargs):\r\n>     # code that leads to conf matrix\r\n>     confusion_matrix = # whatever you do to compute the confusion matrix\r\n>     result = EvalResult()\r\n>     result.confusion_matrix = confusion_matrix\r\n>     # whatever you want to log additionally\r\n>     return result\r\n> \r\n> def test_epoch_end(self, outputs):\r\n>     confusion_matrix = torch.mean(outputs.confusion_matrix, dim=0)\r\n>     # whatever you need to do to log the matrix \r\n>     # for example I use a function to compute the image as a tensor and then I add the image to the logger\r\n>     del outputs['confusion_matrix']\r\n>     # whatever aggregation you need to perform on other metrics\r\n>     return outputs\r\n> ```\r\n> \r\n> Mind that, during the `test_step` you may need to unsqueeze the confusion matrix tensor in order to make it of shape `[1, n_classes, n_classes]` instead of `[n_classes, n_classes]`. This way (assuming they all have the same shape), they will be properly stacked into a tensor of shape `[n_batches, n_classes, n_classes]` instead of `[n_batches * n_classes, n_classes]` for the `test_epoch_end`.\r\n> \r\n> Hope this is of some help.\r\n\r\nconfusion_matrix returned in this way will automatically be synced? Or I need to do manually in `test_epoch_end`?"
      },
      {
        "user": "LucaButera",
        "created_at": "2020-10-08T11:49:21Z",
        "body": "@xiadingZ The confusion matrix will not be automatically synced. As you can see I delete it from ```outputs``` as leaving it would result in an exception, since ```outputs.confusion_matrix``` is not a scalar. You should log your confusion matrix manually. For example I build an image of the confusion matrix and then I log it to tensorboard by doing\r\n\r\n```\r\nself.logger.experiment.add_image('confusion_matrix', confusion_matrix_image, self.global_step)\r\n```\r\n\r\ninside ```test_epoch_end```. \r\n\r\nTo produce the image I use ```matplotlib```, then I save it in memory and I load it with ```Pillow```. Finally I convert it to a ```torch.Tensor```.\r\n\r\nMind that implementing ```test_epoch_end``` overrides the automatic aggregation on metrics performed by ```pytorch-lightning```. Hence you need to manually aggregate your other metrics (accuracy, loss and so on). At least this is true for version 0.9.0"
      }
    ]
  },
  {
    "number": 3696,
    "title": "How to set default EarlyStopping patience?",
    "created_at": "2020-09-28T06:21:06Z",
    "closed_at": "2020-09-28T20:03:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3696",
    "body": "Is it possible to set the default EarlyStopping patience without creating a custom early stopping callback? \r\n\r\nInstead of writing:\r\n```\r\ntrainer = pl.Trainer(early_stop_callback=EarlyStopping(patience=XXX))\r\n```\r\n\r\nI'd like to overwrite the default patience directly and then use EvalResult(early_stop_on=...). ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3696/comments",
    "author": "chrismaliszewski",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-09-28T19:45:19Z",
        "body": "No. Any problem with using a custom callback??"
      },
      {
        "user": "chrismaliszewski",
        "created_at": "2020-09-28T20:03:18Z",
        "body": "No, just wondering. I just would like to keep the early stopping things in one place, EvalResult, instead of jumping elsewhere to change settings. \r\n\r\nThanks. Closing."
      }
    ]
  },
  {
    "number": 3666,
    "title": "row_log_interval / checkpoint_callback and early_stop_callback",
    "created_at": "2020-09-26T03:33:07Z",
    "closed_at": "2020-11-04T06:37:58Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3666",
    "body": "I raised an issue in the past regarding the train score not being logged, which was resolved by setting `row_log_interval` less than the number of batches per epoch.\r\n\r\nThis also appears to interfere with callbacks. My model stopped writing checkpoints (other than the first) and early stopping also wasn't invoked. I think this occurred when I increased `batch_size` and didn't reduce `row_log_interval` accordingly. Worse was a single `epoch=0.ckpt` file was actually created but no subsequent one - if I understand correctly trainer.test() then restores this checkpoint rather than testing the trained model?\r\n\r\nIs this the case or is something else going on here? If it is, it seems like there needs to be a mechanism to default `row_log_interval` to be at least once per epoch without the user having to manually calculate this? In particular `batch_size` is a hyperparameter and `row_log_interval` isn't (or shouldn't be) but it does (seem to) indirectly impact the model score if it prevents callbacks from being invoked?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3666/comments",
    "author": "david-waterworth",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-09-26T11:12:19Z",
        "body": "True, I agree. row_log_interval should apply to global step and log every N steps regardless of epoch. #3466 \r\n\r\nHowever, row_log_interval has no influence on callbacks, I am sure. It must be something else you are experiencing. For the callback, did you set the monitor and correctly? There were some bugfixes recently around checkpoint callback, check also the latest version."
      },
      {
        "user": "david-waterworth",
        "created_at": "2020-09-27T02:45:32Z",
        "body": "You're right - I tried again and I couldn't reproduce the issue.\r\n\r\nWhat I did notice was I was getting the following (once I set verbose=True)\r\n\r\n> RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Either add `val_loss` to the return of  validation_epoch end or modify your EarlyStopping callback to use any of the following: `val_early_stop_on`, `val_checkpoint_on`\r\n\r\nThis was a consequence of not refactoring correctly when I moved to the new pl.EvalResult(), my validation_step is now\r\n\r\n        loss = self.loss_function(out, tgt_out)\r\n        result = pl.EvalResult(checkpoint_on=loss, early_stop_on=loss) # added early_stop_on\r\n        result.log('val_loss', loss)\r\n\r\nAlthough now I am getting duplicate messages in my log\r\n\r\n> Epoch 00013: val_checkpoint_on reached 0.50057 (best 0.50057), saving model to ./mlruns/1/6438eeab92ba4133a42086b0c9ab57bc/checkpoints/epoch=13.ckpt as top 1\r\n> INFO:lightning:\r\n> Epoch 00013: val_checkpoint_on reached 0.50057 (best 0.50057), saving model to ./mlruns/1/6438eeab92ba4133a42086b0c9ab57bc/checkpoints/epoch=13.ckpt as top 1\r\n\r\nI checked and the callback is only getting called once - `log.info()` on line 417 of `model_checkpoint.py` is creating two entries in my vscode terminal for seom reason. I suspect it's due to how the logger is configured?\r\n\r\nAlso I noted that `strict` defaults to true in `EarlyStopping.__init__()` but by the time `_validate_condition_metric` is called `self.strict` has changed to false - so the path that should have crashed the trainer because `monitor_val is None` didn't have any effect. This seems to be due to the code below (line #152 of early_stopping.py)\r\n\r\n        # disable strict checking when using structured results\r\n        if val_es_key in trainer.callback_metrics:\r\n            self.strict = False\r\n\r\nThe issue is `val_es_key='val_early_stop_on' ` is in `trainer.callback_metrics` but it's None when I used returned the following from `validation_step`\r\n\r\n```\r\nresult = pl.EvalResult(checkpoint_on=loss)\r\n```\r\n\r\nComparing this to the code in `model_checkpoint.py` the following stricter check is made:\r\n\r\n```\r\nmetrics = trainer.callback_metrics\r\nif metrics.get('val_checkpoint_on') is not None:\r\n...\r\n```\r\n\r\nSo I would suggest line #152 of early_stopping.py should be:\r\n\r\n        # disable strict checking when using structured results\r\n        if trainer.callback_metrics.get(val_es_key) is None:\r\n            self.strict = False\r\n\r\n\r\n\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-28T01:33:00Z",
        "body": "duplicate logging: yes, I can confirm this is likely just a weird python logging that we haven't configured correctly. work on this isn in progrss here: #3514 \r\n\r\nWe're currently sorting out some issues around results and checkpoint/earlystopping monitoring values. You can expect this to be fixed and stable in v1.0. Until then, make sure that\r\n\r\n1. You use the latest 0.9.1rc4 release\r\n2. Have monitor=\"val_loss\" (or appropriate key) in your modelcheckpoint (if you're passing a custom one into the Trainer)\r\n\r\nThe issue with early stopping monitor you mention should already be fixed. It is not using `val_checkpoint_on` key anymore. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-28T03:16:23Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3648,
    "title": "How to use a custom Callback with Trainer.from_argparse_args",
    "created_at": "2020-09-24T17:53:33Z",
    "closed_at": "2020-09-24T21:33:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3648",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI'd like specify a custom Callback while passing argparse paramaters using `Trainer.from_argparse_args`\r\n\r\n#### Code\r\n\r\nFor example, I've tried something like this with no success:\r\n\r\n`trainer = Trainer(callbacks=[CustomCallback()]).from_argparse_args(args)`\r\n\r\nwhich doesn't seem to properly apply the callback.\r\n\r\nWhat is the proper way to define a custom callback within a trainer when using `from_argparse_args`?\r\n\r\n#### What have you tried?\r\n\r\nHowever, this DOES work as expected when calling from checkpoint:\r\n\r\n``` \r\ntrainer = Trainer(\r\n        resume_from_checkpoint=ckpt_fname,\r\n        callbacks=[CustomCallback()])\r\n```\r\n\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\n - Packaging: conda\r\n - Version: 0.9.0\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3648/comments",
    "author": "KirillShmilovich",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-24T17:54:13Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "carmocca",
        "created_at": "2020-09-24T20:28:55Z",
        "body": "You can do the following:\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nimport argparse\r\n\r\nargs = argparse.Namespace(max_epochs=3)  # fake args\r\ntrainer = pl.Trainer.from_argparse_args(args)\r\ntrainer.callbacks.append(CustomCallback())\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-24T21:32:27Z",
        "body": "I do this\r\n```python \r\ntrainer = Trainer.from_argparse_args(args, callbacks=[CustomCallback()])\r\n```\r\nfrom_argparse_args accepts kwargs that can override the args in the namespace."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-24T21:33:20Z",
        "body": "Pretty confident this or @carmocca's comment will answer it for you, so closing this already. Let us now if it worked!"
      }
    ]
  },
  {
    "number": 3638,
    "title": "Loading written predictions",
    "created_at": "2020-09-24T05:38:26Z",
    "closed_at": "2020-11-21T23:19:48Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3638",
    "body": "One can write predictions (or metrics, etc.) to disk with a result object using the method `write` which pickles them in `on_test_end`. I'm interested in loading them afterwards with following lines\r\n```\r\nwith open(preds_path, \"rb\") as pred_file:\r\n    preds = pickle.load(pred_file)\r\n```\r\nand I'm receiving the following error\r\n\r\n`UnpicklingError: A load persistent id instruction was encountered,\r\nbut no persistent_load function was specified.`\r\n\r\nI can't find the line where predictions are pickled and where such a function might be specified. Any help would be appreciated!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3638/comments",
    "author": "baradl",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-24T05:39:03Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-24T22:24:22Z",
        "body": "I think @nateraw worked on this predictions feature and may know the answer :) "
      },
      {
        "user": "nateraw",
        "created_at": "2020-10-15T17:45:52Z",
        "body": "The predictions are written with `torch.save` so you should use `torch.load('your_predictions.pt')`. Let me know if this helps!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-14T22:23:57Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3590,
    "title": "Weights precisions do not match up. AMP",
    "created_at": "2020-09-21T18:00:47Z",
    "closed_at": "2020-10-29T08:46:34Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3590",
    "body": "Hi, thanks for your work!\r\n\r\nMy situation: I have a loss function that is defined in a separate file and it has weights and parameters and I import it into my pl module like\r\n```\r\nfrom arc_margin_loss import arcface_loss\r\nself.loss = arcface_loss(parametes)\r\n```\r\nSo when I try to use amp (Pytorch 1.6) I get those errors that I try to compare float and half tensors.\r\nMy question is how do I avoid this problem correctly?\r\nShould I just add @autocast() decorator to the loss forward?\r\nOr maybe I should I define loss inside pl module?\r\n\r\nThanks in advance!\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3590/comments",
    "author": "IgorDavidyuk",
    "comments": [
      {
        "user": "ananyahjha93",
        "created_at": "2020-09-21T20:23:00Z",
        "body": "It will be preferable to define the loss within the PL module in this case. Is there a specific reason that you need to define the loss outside of the PL module or is it just a design decision?"
      },
      {
        "user": "IgorDavidyuk",
        "created_at": "2020-09-21T22:48:40Z",
        "body": "Thanks for the answer!\r\n\r\n> Is there a specific reason that you need to define the loss outside of the PL module or is it just a design decision?\r\n\r\nWell, I just import it from another repo. \r\nIt is just strange that autocast does not recursively apply\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T02:24:16Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3500,
    "title": "Difference in support for IterableDataset in Trainer.fit and Trainer.test",
    "created_at": "2020-09-15T06:08:34Z",
    "closed_at": "2020-10-31T13:45:32Z",
    "labels": [
      "question",
      "won't fix",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3500",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nWhen using WebDataset (which implements IterableDataset), Trainer.fit works without warnings, but Trainer.test will fail unless a nominal length is specified. Is this intended behaviour?\r\n\r\n#### What have you tried?\r\nWhen running Trainer.test without specifiying nominal length:\r\n```\r\nTraceback (most recent call last):\r\n  File \"sunset.py\", line 222, in <module>\r\n    main(parser.parse_args())\r\n  File \"sunset.py\", line 126, in main\r\n    main_test(args)\r\n  File \"sunset.py\", line 210, in main_test\r\n    trainer.test(model, test_dataloaders=test_loader, verbose=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1346, in test\r\n    if test_dataloaders and datamodule:\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 325, in __len__\r\n    length = self._IterableDataset_len_called = len(self.dataset)\r\nTypeError: 'NoneType' object cannot be interpreted as an integer\r\n```\r\n\r\nWhen running Trainer.test after specifying nominal length (and having multiple workers):\r\n```\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-processing data loading (e.g. batch size > 1), this can lead to unintended side effects since the samples will be duplicated.\r\n  warnings.warn(*args, **kwargs)\r\n```\r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu 18.04 (inside Docker Container)\r\n - Packaging: pip\r\n - Version: 0.9.0 on Python 3.6.9\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3500/comments",
    "author": "wongjoel",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-09-18T07:54:36Z",
        "body": "I would say that the case is that for training you shall define nb steps per epoch so then you just sample from dataloader as much to finish your epoch, but for testing, you shall run all and if you don't know how many we cannot provide any progress has or other estimates... So maybe we can change it just to warning and drop the progress bar in such a case... What do you think?"
      },
      {
        "user": "wongjoel",
        "created_at": "2020-09-24T11:13:42Z",
        "body": "I think that would be helpful - I would rather run without a progress bar than have a hard error preventing testing. (Sorry about the late reply, I ended up doing testing outside the framework, so I forgot about this)."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-24T13:39:31Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "hey0",
        "created_at": "2021-05-26T08:45:56Z",
        "body": "Are there any news on this issue?"
      }
    ]
  },
  {
    "number": 3473,
    "title": "Correct way of implementing early stopping",
    "created_at": "2020-09-12T10:24:28Z",
    "closed_at": "2020-09-12T13:57:42Z",
    "labels": [
      "question",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3473",
    "body": "I am trying to implement early stopping on my LSTM classifier.Running the script on colab GPU environment. Here's the code\r\n\r\n```python\r\n!pip install pytorch-lightning torchtext \r\nimport os\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nimport pytorch_lightning as pl\r\nfrom torch.utils.data import random_split\r\nfrom torchtext import data, datasets\r\nfrom torch.nn.utils.rnn import pack_padded_sequence\r\nfrom pytorch_lightning.metrics import functional as FM\r\nfrom pytorch_lightning.callbacks import EarlyStopping\r\n\r\n#### set up fields\r\nTEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\r\nLABEL = data.Field(sequential=False)\r\n\r\n##### make splits for data\r\ntrain, test = datasets.IMDB.splits(TEXT, LABEL)\r\n\r\n##### build the vocabulary\r\nTEXT.build_vocab(train)\r\nLABEL.build_vocab(train)\r\n\r\n#### make iterator for splits\r\ntrain_iter, test_iter = data.BucketIterator.splits(\r\n    (train, test), batch_size=100, device=0)\r\n\r\nclass LightningLSTM(pl.LightningModule):\r\n  def __init__(self, embedd_dim, hidden_size, output_dim, vocab_size, **kwargs):\r\n    super().__init__()\r\n    self.embedd_dim = embedd_dim\r\n    self.hidden_size = hidden_size\r\n    self.output_dim = output_dim\r\n    self.vocab_size = vocab_size\r\n    self.embedding = nn.Embedding(self.vocab_size, self.embedd_dim)\r\n    self.lstm = nn.LSTM(self.embedd_dim, self.hidden_size, batch_first=True, **kwargs)\r\n    self.linear = nn.Linear(self.hidden_size, self.output_dim)\r\n    self.softmax = nn.Softmax(1)\r\n\r\n  def forward(self, x, lengths):\r\n    output = self.embedding(x)\r\n    packed_output = pack_padded_sequence(output, lengths.cpu().numpy(), batch_first=True, enforce_sorted=False)\r\n    output, (ht, ct) = self.lstm(packed_output)\r\n    output = self.linear(ht).squeeze(0)\r\n    return output\r\n\r\n  def configure_optimizers(self):\r\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n    return optimizer\r\n\r\n  def validation_step(self, batch, batch_idx):\r\n    x, l, y = *batch.text, batch.label\r\n    y-=1\r\n    y_hat = self(x, l)\r\n    loss = F.cross_entropy(y_hat, y)\r\n    y_pred = torch.argmax(self.softmax(y_hat),1)\r\n    result = pl.EvalResult(checkpoint_on=loss, early_stop_on=loss)\r\n    acc = FM.accuracy(y_pred, y, num_classes=self.output_dim)\r\n    result.log('val_loss', loss, prog_bar=True, on_step=True)\r\n    result.log('val_acc', acc, prog_bar=True, on_step=True)\r\n    return result\r\n\r\n  def training_step(self, batch, batch_idx):\r\n    x, l, y = *batch.text, batch.label\r\n    y-=1\r\n    y_hat = self(x, l)\r\n    loss = F.cross_entropy(y_hat, y)\r\n    y_pred = torch.argmax(self.softmax(y_hat),1)\r\n    result = pl.EvalResult(checkpoint_on=loss)\r\n    acc = FM.accuracy(y_pred, y, num_classes=self.output_dim)\r\n    result = pl.TrainResult(minimize=loss)\r\n    result.log('loss', loss)\r\n    result.log('train_acc', acc)\r\n    return result\r\n\r\nEMBEDDING_DIM = 200\r\nVOCAB_SIZE = len(TEXT.vocab)\r\nOUTPUT_DIM = 2 #two labels - positive and negative\r\nHIDDEN_SIZE = 1024\r\nmodel = LightningLSTM(EMBEDDING_DIM, HIDDEN_SIZE, OUTPUT_DIM, VOCAB_SIZE)\r\n\r\nearly_stopping = EarlyStopping('val_loss', patience=3, mode='min')\r\ntrainer = pl.Trainer(gpus=1, max_epochs=10, early_stop_callback=early_stopping)\r\ntrainer.fit(model, train_iter, test_iter) `\r\n```\r\nThis is the output + warning I get when I start the training - \r\n\r\n\r\n``` \r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nCUDA_VISIBLE_DEVICES: [0]\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\r\n  warnings.warn(*args, **kwargs)\r\n\r\n  | Name      | Type      | Params\r\n----------------------------------------\r\n0 | embedding | Embedding | 50 M  \r\n1 | lstm      | LSTM      | 5 M   \r\n2 | linear    | Linear    | 2 K   \r\n3 | softmax   | Softmax   | 0     \r\nEpoch 5: 100%\r\n500/500 [02:07<00:00, 3.91it/s, loss=0.105, v_num=3, step_val_loss=1.06, step_val_acc=0.71, epoch_val_loss=0.748, epoch_val_acc=0.763]\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: \r\n                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the\r\n                    'monitor' key of EarlyStopping has no effect.\r\n                    Remove EarlyStopping(monitor='val_early_stop_on) to fix')\r\n                \r\n  warnings.warn(*args, **kwargs)\r\nSaving latest checkpoint..\r\n\r\n1\r\n ```\r\n\r\nIf I don't specifically mention `early_stop_on = loss` in the EvalResult initialization (in the validation_step method), the trainer keeps training it for the max number of epochs specified. Also, I do not get the warning when I remove the `early_stop_on` parameter. Early stopping works fine when I include the parameter.\r\n\r\nI am confused about what is the right way to implement early stopping.  `early_stopping = EarlyStopping('val_loss', patience=3, mode='min')` this line seems to implement early stopping as well. But doesn't work unless I explicitly mention in the EvalResult object.\r\n\r\nCan anyone point out if I am missing something?\r\n\r\nThanks!\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3473/comments",
    "author": "DhruvilKarani",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-12T10:25:09Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-12T13:32:17Z",
        "body": "Yes, currently you need to do it this way. The result object was meant to make early stopping etc. easier, but currently if you use a custom callback, you need to add that key manually. We will iterate on this and are currently discussing ways to make configuration of these callbacks easier and more intuitive. part of discussion is here #3286 "
      },
      {
        "user": "DhruvilKarani",
        "created_at": "2020-09-12T13:57:42Z",
        "body": "Sure. Thanks for the response!"
      }
    ]
  },
  {
    "number": 3443,
    "title": "Conda pytorch-lightning breaks torchvision",
    "created_at": "2020-09-10T11:55:00Z",
    "closed_at": "2020-09-10T13:04:47Z",
    "labels": [
      "help wanted",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3443",
    "body": "## 🐛 Bug\r\n\r\nI have to use conda on our SLURM cluster. First I installed pytorch and torchvision within a conda environment which works fine. As soon as I additionally install pytorch-lightning, torchvision breaks\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. ```conda create --name pytorch_test_env pytorch torchvision cudatoolkit --channel pytorch```\r\n2. ```conda activate pytorch_test_env```\r\n1. ```python``` and ```import torch``` and ```import torchvision``` -> works fine\r\n2. exit and run ```conda install -c conda-forge pytorch-lightning```\r\n  Problem(?): reports that pytorch will be updated: ```pytorch::pytorch-1.1.0-py3.7_cuda9.0.~ --> pkgs/main::pytorch-1.3.1-cpu_py37h62f834f_0```\r\n3. again, inside a python shell do ```import torchvision```\r\n4. The error is\r\n```\r\nImport Error: <\"path_to\">/lib/python3.7/site-packages/torchvision/_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN3c1011CPUTensorIdEv\r\n```\r\n\r\n### Expected behavior\r\n\r\nLightning will be installed and work side-by-side with my current pytorch version.\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.1\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.3.1\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.48.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         \r\n\t- python:            3.7.9\r\n\t- version:           #1 SMP Debian 4.9.228-1 (2020-07-05)\r\n```\r\n\r\n### Additional context\r\n\r\nNote that this machine does not have a GPU (see above). The GPU machine will be the one I submit a job to via SLURM.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3443/comments",
    "author": "matthaeusheer",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-09-10T12:04:23Z",
        "body": "Could you try reinstalling pytorch, torchvision, pytorch-lightning with the version you will use?\r\nFrom the description, pytorch version bumped from 1.1 to 1.3 after installing pytorch-lightning.\r\nprobably torchvision is not upgraded automatically."
      },
      {
        "user": "Borda",
        "created_at": "2020-09-10T13:04:47Z",
        "body": "yes, it seems your base PT version is 1.1 and PL requires >= 1.3 so you need to reinstall torchvision..."
      }
    ]
  },
  {
    "number": 3410,
    "title": "Loss display during epoch",
    "created_at": "2020-09-09T08:22:05Z",
    "closed_at": "2020-10-28T21:27:16Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3410",
    "body": "###  During the training, I'm using the custom loss function to train my model. However the loss are displayed as 0.000, but when I display the same value to display as different variable it gives 4.73e-5 (some value in exponential format). \r\n\r\nEpoch 80:  10%|██▌                       | 100/1013 [01:33<14:11,  1.07it/s, loss=0.000, v_num=None, train_loss=4.73e-5]\r\n\r\nboth loss and train_loss display the same value. why one displays in exponential format and other doesn't.\r\n\r\n### Is it  possible does it prohibits the model from converging. because when i use the same parameters to train the model in normal way it converges, however with the pytorch lightning the model doesn't converge beyond certain limit.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3410/comments",
    "author": "akum04",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-09T08:22:52Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T15:43:45Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3401,
    "title": "How to specify 'mode' when using checkpoint_on in the new Result types.",
    "created_at": "2020-09-08T18:40:43Z",
    "closed_at": "2020-10-29T08:46:42Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3401",
    "body": "\r\n#### What is your question?\r\nI have been using PL for a while now and am just getting around to switching all of my code over to a newer version (0.9.0 from 0.6.0). I would like to switch over to using the newer TrainResult and EvalResult types instead of plain dictionaries. In the past I used a ModelCheckpoint() that I passed into the trainer with the monitor and mode options set. Now using the EvalResult type and passing it a 'checkpoint_on' option, I dont know how to specify a mode (min or max). Is this possible? Looking through the code it seems like maybe you can still pass a 'mode' through the ModelCheckpoint() but this seems a bit strange since it splits up what you are monitoring and whether its a min or max into two different places that you have to specify (one in my model and another in the ModelCheckpoint() ). Is this right? If so would it make sense to add the ability to pass a mode in with the {Train/Eval}Result as well?   \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3401/comments",
    "author": "Rob-Fletcher",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-08T18:41:23Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Rob-Fletcher",
        "created_at": "2020-09-11T22:23:35Z",
        "body": "If there currently isnt a way to do this and you guys think its needed I can make a PR."
      },
      {
        "user": "carmocca",
        "created_at": "2020-09-13T12:26:39Z",
        "body": "`mode` is currently passed to `ModelCheckpoint`. I am with you in that it is confusing having the metric and the mode in different places. We have a discussion over at #3286 about how we should fix it."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T11:40:34Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3396,
    "title": "Gradient Flow Summary",
    "created_at": "2020-09-08T13:31:33Z",
    "closed_at": "2020-10-28T21:27:18Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3396",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   done\r\n2. search the docs.    done\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI want to add a summary where I can track the gradient flow in my model.\r\nThe out-of-the-box gradient tracking is not sufficient for me because I need a more customized behavior.\r\nI wanted to do it using a callback, but I couldn't find a way to access the gradients from a callback.\r\nI noticed there is a model hook for model.on_before_zero_grad(optimizer), which seem to have access for the gradients before resting them, but it seem to don't fit my use case, I would want access to training state information as well when logging..\r\n\r\nWhat is the recommended way to go about it?\r\n\r\nthanks,\r\nAlex\r\n\r\n#### Code\r\n\r\n#### What have you tried?\r\nI was aiming fo something of this sort:\r\n```python\r\nclass TBGradFlowImageSummary(pl.Callback):\r\n    def on_train_batch_end_before_zero_grad(self, trainer, pl_module, batch, batch_idx, dataloader_idx):\r\n        isLastBatch = batch_idx == len(trainer.train_dataloaders) -1\r\n        if isLastBatch:\r\n                pl_module.logger.experiment.log_something()\r\n```\r\n\r\n\r\n\r\n\r\n#### What's your environment?\r\n\r\n - OS: [Linux]\r\n - Version [0.9.0]\r\nthanks\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3396/comments",
    "author": "Alexfinkelshtein",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-08T13:32:14Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T13:42:22Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3387,
    "title": "some questions about loading checkpoints",
    "created_at": "2020-09-07T17:53:41Z",
    "closed_at": "2020-10-29T08:46:37Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3387",
    "body": "What I have tried:\r\n- defined a Lighting Module class: `model=LitModel(hparams)`, \r\n- then I trained the model and got a checkpoint; \r\n- since I want to visualize the features of the trainset, I write a another file and loading the checkpoint, `model = LitModel(hparams).load_from_checkpoint('lightning_logs/version_90/checkpoints/epoch=119.ckpt')`\r\n\r\nMy questions are:\r\n- I have to add a line `model.eval()` before `model(samples)`, otherwise, the accuracy is very low! however, to my best knowledge, in common pytorch code, the accuracy of the trainset has no matter with `model.eval()`, **so why `model.eval()` take effects?**\r\n- I tried to put my model onto gpu`model = LitModel(hparams).load_from_checkpoint('lightning_logs/version_90/checkpoints/epoch=119.ckpt').cuda()`, the model takes up about 460M/11G, however, when running into `model(samples.cuda())`, my gpu memory is out of use! **so, can anyone show me the correct way to visualize features using the checkpoint of Light Module?**",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3387/comments",
    "author": "aiyolo",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-09-08T17:45:55Z",
        "body": "> my gpu memory is out of use!\r\n\r\nit will go up due to activations calculated in the forward pass. I think you can use torch.no_grad() to lower memory consumption.\r\n\r\n> model.eval()\r\n\r\nIf changes the behavior of batchnorm and dropout layers."
      },
      {
        "user": "aiyolo",
        "created_at": "2020-09-10T12:03:42Z",
        "body": "but it still can't explain why the gpu memory is enough for training a model in the training phase while getting exhausted in the feature visualizing stage, they are using the same dataloader."
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-10T17:21:43Z",
        "body": "hmm... this should not happen. During inference it's just a simple pytorch model, nothing fancy. Is it possible for you to share a colab notebook??"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T13:42:14Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3379,
    "title": "TypeError: 'generator' object is not callable",
    "created_at": "2020-09-07T06:34:30Z",
    "closed_at": "2020-09-24T05:55:51Z",
    "labels": [
      "help wanted",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3379",
    "body": "I'm getting the exception TypeError: 'generator' object is not callable when I train with multiple GPU's\r\n\r\nI'm not sure where it's coming from, my datasets are subclasses of torchtext.data.Dataset and the data loaders are torchtext.data.BucketIterator. \r\n\r\nWhat's the easiest way of identifying what's causing the exception?  \r\n\r\n```\r\n    # create trainer\r\n    mlflow = loggers.MLFlowLogger(\"Transformer\")\r\n    trainer = pl.Trainer.from_argparse_args(args, early_stop_callback=True, min_epochs=10, logger=mlflow, gpus=[0,1])\r\n\r\n    # prepare data\r\n    train_dataset, val_dataset = load_datasets(path=args.path, files=args.files)\r\n    train_loader, val_loader = create_dataloaders(train_dataset, val_dataset, batch_size=args.batch_size)\r\n\r\n    # init the model\r\n    hparams = vars(args)\r\n    transformer = Transformer(src_vocab=train_dataset.vocab, tgt_vocab=val_dataset.vocab, **hparams)\r\n\r\n    # train\r\n    trainer.fit(transformer, train_loader, val_dataloaders=val_loader)\r\n    transformer.freeze()\r\n\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3379/comments",
    "author": "david-waterworth",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-09-08T05:27:15Z",
        "body": "The first step is to look at the error message, the full stack trace and read what it is saying. Can you post that please? Plus the PL version you are using. If this is a bug in Lightning that we need to fix, it would be good to have a minimal code example. \r\nMake sure that what you pass to Trainer is a torch DataLoader, not a special subclass or something else."
      },
      {
        "user": "david-waterworth",
        "created_at": "2020-09-08T06:23:52Z",
        "body": "@awaelchli  here's the stack trace. I'll try and put together a minimal example tomorrow\r\n\r\npytorch-lightning==0.9.0\r\n\r\n> Traceback (most recent call last):\r\n>   File \"/home/david/.pyenv/versions/3.8.4/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n>     return _run_code(code, main_globals, None,\r\n>   File \"/home/david/.pyenv/versions/3.8.4/lib/python3.8/runpy.py\", line 87, in _run_code\r\n>     exec(code, run_globals)\r\n>   File \"/home/david/.vscode/extensions/ms-python.python-2020.8.106424/pythonFiles/lib/python/debugpy/__main__.py\", line 45, in <module>\r\n>     cli.main()\r\n>   File \"/home/david/.vscode/extensions/ms-python.python-2020.8.106424/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py\", line 430, in main\r\n>     run()\r\n>   File \"/home/david/.vscode/extensions/ms-python.python-2020.8.106424/pythonFiles/lib/python/debugpy/../debugpy/server/cli.py\", line 267, in run_file\r\n>     runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n>   File \"/home/david/.pyenv/versions/3.8.4/lib/python3.8/runpy.py\", line 265, in run_path\r\n>     return _run_module_code(code, init_globals, run_name,\r\n>   File \"/home/david/.pyenv/versions/3.8.4/lib/python3.8/runpy.py\", line 97, in _run_module_code\r\n>     _run_code(code, mod_globals, init_globals,\r\n>   File \"/home/david/.pyenv/versions/3.8.4/lib/python3.8/runpy.py\", line 87, in _run_code\r\n>     exec(code, run_globals)\r\n>   File \"/home/david/dev/machine-learning-research/src/annotation/src/transformer.py\", line 161, in <module>\r\n>     trainer.fit(transformer, train_loader, val_dataloaders=val_loader)\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n>     result = fn(self, *args, **kwargs)\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1052, in fit\r\n>     self.accelerator_backend.train(model, nprocs=self.num_processes)\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py\", line 43, in train\r\n>     mp.spawn(self.ddp_train, nprocs=nprocs, args=(self.mp_queue, model,))\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\r\n>     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n>   File \"/home/david/.pyenv/versions/transformer/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 149, in start_processes\r\n>     process.start()\r\n>   File \"/home/david/.pyenv/versions/3.8.4/lib/python3.8/multiprocessing/process.py\", line 121, in start\r\n>     self._popen = self._Popen(self)\r\n>   File \"/home/david/.pyenv/versions/3.8.4/lib/python3.8/multiprocessing/context.py\", line 284, in _Popen\r\n>     return Popen(process_obj)\r\n>   File \"/home/david/.pyenv/versions/3.8.4/lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n>     super().__init__(process_obj)\r\n>   File \"/home/david/.pyenv/versions/3.8.4/lib/python3.8/multiprocessing/popen_fork.py\", line 19, in __init__\r\n>     self._launch(process_obj)\r\n>   File \"/home/david/.pyenv/versions/3.8.4/lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n>     reduction.dump(process_obj, fp)\r\n>   File \"/home/david/.pyenv/versions/3.8.4/lib/python3.8/multiprocessing/reduction.py\", line 60, in dump\r\n>     ForkingPickler(file, protocol).dump(obj)\r\n> TypeError: 'generator' object is not callable"
      },
      {
        "user": "david-waterworth",
        "created_at": "2020-09-10T04:06:24Z",
        "body": "@awaelchli \r\n\r\nThe script below generates the same stack trace. I suspect it's related to the torchtext objects as I've just stubbed out all the pl callbacks and it still throws the exception. Note torchtext are deprecating torchtext.data classes in the next release (although they will live in a legacy module for a while) in place of using the standard torch.data.Dataset / torch.data.Dataloader\r\n\r\nAlso sorry this downloads a lot of data (one time)! You can probably switch to a smaller dataset but I just copied the first example I found from torchtext and adapted to a DataModule.\r\n\r\n```\r\n> pip freeze\r\ntorch==1.6.0\r\ntorchtext==0.7.0\r\npytorch-lightning==0.9.0\r\nmlflow==1.11.0\r\n...\r\n```\r\n\r\n```\r\nfrom torchtext import data, datasets\r\nfrom torchtext.vocab import GloVe\r\nimport pytorch_lightning as pl\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom pytorch_lightning import loggers\r\n\r\n\r\nclass DataModule(pl.LightningDataModule):\r\n    def __init__(self, batch_size=4):\r\n        super().__init__()\r\n        self.batch_size = batch_size\r\n\r\n    def setup(self, stage=None):\r\n        # set up fields\r\n        TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\r\n        LABEL = data.Field(sequential=False)\r\n\r\n        # make splits for data\r\n        self.train, self.test = datasets.IMDB.splits(TEXT, LABEL)\r\n\r\n        # build the vocabulary\r\n        TEXT.build_vocab(self.train, vectors=GloVe(name='6B', dim=300))\r\n        LABEL.build_vocab(self.train)\r\n\r\n        self.src_vocab = TEXT.vocab\r\n        self.tgt_vocab = LABEL.vocab\r\n\r\n    def train_dataloader(self):\r\n        train_iter, _ = datasets.IMDB.iters(self.batch_size)\r\n        return train_iter\r\n    \r\n    def val_dataloader(self):\r\n        _, val_iter = datasets.IMDB.iters(self.batch_size)\r\n        return val_iter\r\n\r\n\r\nclass Transformer(pl.LightningModule):\r\n    def __init__(self, **kwargs):\r\n        super().__init__()\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument('--d_model', default=512, type=int)\r\n        parser.add_argument('--batch_size', default=256, type=int)  \r\n        parser.add_argument('--learning_rate', default=1e-5) \r\n        return parser\r\n\r\n    def loss_function(self, outputs, targets):\r\n        return F.cross_entropy(outputs, targets, ignore_index=self.tgt_vocab.stoi['<pad>'])\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    def forward(self, src, tgt):\r\n        pass\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        pass\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        pass\r\n\r\n\r\nif __name__ == '__main__':\r\n    # to avoid relative import errors ensure \"env\": { \"PYTHONPATH\": \"${workspaceFolder}/src\" } is set in launch.json\r\n    from argparse import ArgumentParser\r\n\r\n    parser = ArgumentParser()\r\n    # program args\r\n    # model args\r\n    parser = Transformer.add_model_specific_args(parser)  \r\n    # trainer args\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    # obtain args\r\n    args = parser.parse_args()\r\n\r\n    # create trainer\r\n    mlflow = loggers.MLFlowLogger(\"Transformer\")\r\n    trainer = pl.Trainer.from_argparse_args(args, gpus=[0,1], early_stop_callback=True, logger=mlflow, row_log_interval=25)\r\n\r\n    # prepare data\r\n    dm = DataModule(batch_size=args.batch_size)\r\n    dm.prepare_data()\r\n    dm.setup()\r\n\r\n    # init the model\r\n    hparams = vars(args)\r\n    transformer = Transformer(src_vocab=dm.src_vocab, tgt_vocab=dm.tgt_vocab, **hparams)\r\n\r\n    # train\r\n    trainer.fit(transformer, datamodule=dm)\r\n    transformer.freeze()\r\n```\r\n\r\n"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-10T16:27:50Z",
        "body": "@awaelchli mind taking a look?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-10T17:05:30Z",
        "body": "@david-waterworth Apologies for the late response. Thanks for the code, I can now see that you return iterators in the dataloader hooks as I suspected already earlier. This must be a torch DataLoader object, Lightning accepts only that. I do not know torchtext well enough to give you specific instructions what the best way is, but I would try to wrap the IMDB iterator in DataLoader / IterableDataset. So far, I cannot identify a bug here. "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-10T17:09:49Z",
        "body": "@mateuszpieniak has used Lightning with torchtext before, maybe he has some tips."
      },
      {
        "user": "david-waterworth",
        "created_at": "2020-09-10T23:11:04Z",
        "body": "@awaelchli thanks for looking. I need to refactor this anyway since as I mentioned `torchtext` has started to deprecate their DataLoader / Dataset in place of the `torch` equivalents. I've been putting it off since they've not yet provided a migration path for custom datasets (frustratingly they seem to be focusing on migrating the built-in datasets using private pipeline methods - I feel they should have created a migration pathway first and then used it to migrate their datasets). \r\n\r\n@mateuszpieniak any pointers would be useful - I need to add '<sos>' and '<eos>' tokens to each example and then pad each batch. Ideally, by using high-performance code someone has already thoroughly tested \r\n"
      }
    ]
  },
  {
    "number": 3341,
    "title": "How to determine the device from within a `LightningDataModule`",
    "created_at": "2020-09-03T16:45:05Z",
    "closed_at": "2020-09-28T16:15:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3341",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nIs there a recommended way (or is it even at all possible) to determine which device is used from within the new `LightningDataModule`?\r\n\r\nI ask the question because right now I decide how to set `pin_memory` when initializing the dataloaders based on the device. Prior to 0.9, the `_dataloaders()` methods were part of the `LightningModule` and I could simply access `self.device` to check the device the data would be transferred to.\r\n\r\n#### Code\r\nExample of what I did before 0.9:\r\n```python\r\ndef train_dataloader(self) -> DataLoader:\r\n    return DataLoader(..., pin_memory=self.device.type == \"cuda\")\r\n```\r\n\r\n#### What have you tried?\r\nI've checked the doc and API of the `LightningDataModule`, but I don't see any indications of how to get the device.\r\n\r\n#### What's your environment?\r\n\r\n - Version: 0.9.0\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3341/comments",
    "author": "nathanpainchaud",
    "comments": [
      {
        "user": "ananyahjha93",
        "created_at": "2020-09-03T20:26:39Z",
        "body": "you can check the trainer object for ```on_gpu``` variable and override your datamodule method to take this as a param. "
      },
      {
        "user": "nathanpainchaud",
        "created_at": "2020-09-28T16:15:23Z",
        "body": "For anyone wondering, the recent PR #3684, which adds a reference to the Trainer in the `LightningDataModule`, fixes my issue and allows us to do something like this:\r\n```python\r\ndef train_dataloader(self) -> DataLoader:\r\n    return DataLoader(..., pin_memory=self.trainer.on_gpu)\r\n```"
      }
    ]
  },
  {
    "number": 3324,
    "title": "TypeError: cannot pickle 'socket' object",
    "created_at": "2020-09-02T13:06:30Z",
    "closed_at": "2020-09-03T02:26:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3324",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nwhen I try to pass a `socket.socket` parameter to the model, `trainer.fit(model)` occurred an error:\r\n```bash\r\n  File \"/disks/disk1/damon/remote_src/YPlatformServer/src/handler.py\", line 79, in main\r\n    trainer.fit(model)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1064, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/accelerators/dp_backend.py\", line 97, in train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1169, in run_pretrain_routine\r\n    self.logger.save()\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\", line 27, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 212, in save\r\n    save_hparams_to_yaml(hparams_file, self.hparams)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\", line 368, in save_hparams_to_yaml\r\n    yaml.dump(hparams, fp)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/yaml/__init__.py\", line 290, in dump\r\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/yaml/__init__.py\", line 278, in dump_all\r\n    dumper.represent(data)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/yaml/representer.py\", line 27, in represent\r\n    node = self.represent_data(data)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/yaml/representer.py\", line 48, in represent_data\r\n    node = self.yaml_representers[data_types[0]](self, data)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/yaml/representer.py\", line 207, in represent_dict\r\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/yaml/representer.py\", line 118, in represent_mapping\r\n    node_value = self.represent_data(item_value)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/yaml/representer.py\", line 52, in represent_data\r\n    node = self.yaml_multi_representers[data_type](self, data)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/site-packages/yaml/representer.py\", line 317, in represent_object\r\n    reduce = data.__reduce_ex__(2)\r\n  File \"/disks/disk1/damon/anaconda3/envs/pytorch/lib/python3.8/socket.py\", line 272, in __getstate__\r\n    raise TypeError(f\"cannot pickle {self.__class__.__name__!r} object\")\r\nTypeError: cannot pickle 'socket' object\r\n```\r\n#### Code\r\n```python\r\n    args: dict = update_params(vars(parser_tmp), params)\r\n    args['client']: socket.socket = client\r\n    pl.seed_everything(args['seed'])\r\n    model = p_model(**args)    # it's ok here\r\n\r\n    trainer.fit(model)    # occurred an error (described above)\r\n```\r\n\r\n#### What have you tried?\r\nIt seems `trainer.fit()` will pickle the model, and I know `socket` cannot be pickled, so I use a global variable. Are there any other methods to solve this problem?\r\n#### What's your environment?\r\n\r\n - OS: [e.g. iOS, Linux, Win] Linux\r\n - Packaging [e.g. pip, conda] pip\r\n - Version [e.g. 0.5.2.1] 0.9.0\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3324/comments",
    "author": "yangboye",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-02T13:07:15Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-09-02T15:04:03Z",
        "body": "hi, why would you need to use a client/socket?"
      },
      {
        "user": "yangboye",
        "created_at": "2020-09-03T02:25:44Z",
        "body": "> hi, why would you need to use a client/socket?\r\n\r\nHi. It's my coursework, because I need to communicate with Qt(use tcp to send information). \r\n"
      },
      {
        "user": "yangboye",
        "created_at": "2020-09-03T02:26:00Z",
        "body": "It seems a stupid problem, and now I fixed it by using the execution order of `pl.LightningModule` .\r\nFrom the error information: `save_hparams_to_yaml(hparams_file, self.hparams)`, we guess problems may be arise in `hparams`, and we also know `socket` cannot be pickled. So I tried to bypass it:\r\n```python\r\n    args: dict = update_params(vars(parser_tmp), params)\r\n    pl.seed_everything(args['seed'])\r\n    model = p_model(**args)    # we first initialize `model` without `socket`\r\n    model.client: socket.socket = client    # then add the `socket` parameter to the model, now our model has a `socket` parameter\r\n    ........ \r\n    trainer.fit(model)    # now we start to train our model\r\n```\r\nIt works."
      }
    ]
  },
  {
    "number": 3323,
    "title": "Skip the train_step based on batch statistics",
    "created_at": "2020-09-02T12:10:28Z",
    "closed_at": "2020-11-02T07:33:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3323",
    "body": "I have an issue where I want to skip a training step based on the input batch statistics. I am just wondering if it is possible to skip the `train_step` in a lightning model i.e. the training would just ignore this batch and carry on with the next one?\r\n\r\nusually, I would make sure the batches are always generated in an acceptable manner but I have a hard to track bug which happens rarely and due to some deadline pressures, I would still like to continue the training by skipping this batch. Is there a way to do this with the lightning framework?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3323/comments",
    "author": "pamparana34",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-09-02T15:07:23Z",
        "body": "you can write an iff statement inside the training step and kind of do nothing if you decided to skip the training step..."
      },
      {
        "user": "BostonLobster",
        "created_at": "2020-09-09T03:07:05Z",
        "body": "@Borda But training_step requires returning a loss dict, how should I deal with this required return value? I tried simply doing nothing will cause AttributeError in training_loop.py"
      },
      {
        "user": "AsaphLightricks",
        "created_at": "2020-10-06T17:27:58Z",
        "body": "i'm having a similar issue, i want to be able to skip a batch if something goes wrong in my calculation. What should the train_step method return as to not break the overall training?"
      },
      {
        "user": "deng-cy",
        "created_at": "2020-11-02T02:40:12Z",
        "body": "You can return `None` to skip a step. "
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-02T07:33:31Z",
        "body": "According to @deng-cy comments, you should be able to skip a batch now.\r\n\r\nClosing it as it is resolved, feel free to reopen if needed"
      },
      {
        "user": "amin-nejad",
        "created_at": "2022-01-11T16:02:25Z",
        "body": "Does it only apply to the `training_step` method or will returning `None` also skip the batch in related hooks e.g. `on_train_batch_start`"
      }
    ]
  },
  {
    "number": 3292,
    "title": "How was the README animation created ?",
    "created_at": "2020-08-31T16:36:20Z",
    "closed_at": "2020-11-01T23:27:43Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3292",
    "body": "Sorry if this does not fit into the issues, as it is not really a how-to question.\r\n\r\nThe README animation is really nice. May I known which tool was used to create it? ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3292/comments",
    "author": "yamrzou",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-31T16:36:58Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-09-02T12:09:59Z",
        "body": "@williamFalcon ^^"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T16:43:48Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-25T21:55:09Z",
        "body": "William mentioned that he is using the Keynote software from Apple to create the animations."
      }
    ]
  },
  {
    "number": 3282,
    "title": "Using multiple dataloaders at training time",
    "created_at": "2020-08-31T07:45:29Z",
    "closed_at": "2020-10-29T12:43:00Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3282",
    "body": "I Try to train on two dataloaders, one attached to a dataset where each `__get_item__` call fetches a predefined batch of varying length (thus the batch_size I transfer to the `dataloader` object is 1), and one where I sample randomly from a set of sequences, thus `__get_item__` call fetches one sample each time.\r\n\r\nI'm looking for something like\r\n```\r\n            loader = DataLoader(\r\n                batched_dataset,\r\n                batch_size=1,\r\n            )\r\n            loader_tdm = DataLoader(\r\n                random_samples_dataset,\r\n                batch_size=8,\r\n            )\r\n            return [loader1,loader2]\r\n```\r\nI see this option is available for the validation and test dataloaders, while for training you suggested (previous issues) to use the `ConcatDataset`.\r\nIs there a workaround?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3282/comments",
    "author": "dvirginz",
    "comments": [
      {
        "user": "wolterlw",
        "created_at": "2020-09-14T18:55:00Z",
        "body": "being resolved in #1959"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T11:40:32Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3217,
    "title": "Logging accuracy with batch accumulation",
    "created_at": "2020-08-27T06:45:47Z",
    "closed_at": "2020-10-15T12:56:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3217",
    "body": "I wanted to ask how pytorch handles accuracy (and maybe even loss) logging when we have something like `pl.Trainer(accumulate_grad_batches=ACCUMULATIONS)`.\r\n\r\nMy training looks like this:\r\n```python\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y, weight=self.weight)\r\n        result = pl.TrainResult(loss)\r\n        result.log(\"train_loss\", loss, prog_bar=True)\r\n        result.log(\"train_accuracy\", self.accuracy(y_hat.argmax(dim=-1), y), prog_bar=True)\r\n\r\n        return result\r\n```\r\nwhere `self.accuracy = pl.metrics.classification.Accuracy()`. Is there a way to make sure that the loss and accuracy is averaged across the accumulated batches?\r\n\r\nIf this is not currently the case, I'm happy to do a PR if someone can show me where to look in the source code to make such a change.\r\n\r\nThanks in advance",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3217/comments",
    "author": "sachinruk",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-08-27T08:42:32Z",
        "body": "Hi @sachinruk \r\nNot sure about accuracy or some other metrics, but I can confirm that `loss` is averaged across `accumulate_grad_batches`"
      },
      {
        "user": "sachinruk",
        "created_at": "2020-08-27T09:34:47Z",
        "body": "looking at the progress bar, it seems like the loss and train_loss as seen above are two (slightly) different numbers. And yes loss seems to be working as expected, mainly metrics Im worried about."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-15T12:56:29Z",
        "body": "@sachinruk Class based metrics have been revamped! Please checkout the documentation for the new interface.\r\nWhile the metrics package does not directly integrate with the `accumulate_grad_batches` argument (yet), you should be able to do something like this now:\r\n``` python\r\ndef training_step(self, batch, batch_idx):\r\n    x, y = batch\r\n    y_hat = self(x)\r\n\r\n    self.accuracy.update(y_hat.argmax(dim=-1), y)\r\n    if self.trainer.accumulate_grad_batches % self.global_step == 0:\r\n        accumulated_val = self.accuracy.compute()\r\n        self.log('acc_accumulate', accumulated_val)\r\n    ...\r\n```\r\nClosing this for now."
      }
    ]
  },
  {
    "number": 3158,
    "title": "tensorboard version",
    "created_at": "2020-08-25T14:30:24Z",
    "closed_at": "2020-08-25T19:57:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3158",
    "body": "Hi,\r\nIs there a reason why tensorboard has to be 2.2.0?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3158/comments",
    "author": "ruotianluo",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-08-25T19:55:30Z",
        "body": "there was a reported issue, now we are testing the upper versions #3132 "
      }
    ]
  },
  {
    "number": 3142,
    "title": "continue training ",
    "created_at": "2020-08-25T03:02:25Z",
    "closed_at": "2020-08-25T06:52:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3142",
    "body": "## ❓ Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nI have 10 training datasets. I want to train a model sequentially on these datasets i.e train on first training dataset then train the same model on the second dataset and so on. \r\n\r\nHow to do this?\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n```\r\nmodel = ClassificationModel()\r\nfor dataset in training_datasets:\r\n    trainer = pl.Trainer(gpus=-1)\r\n    trainer.fit(model)\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3142/comments",
    "author": "nrjvarshney",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-08-25T06:52:09Z",
        "body": "the simplest way would be merging these datasets in single datamodule and train it at once, another way is passing multiple dataloaders... the least way is just loading checkpoint model after each training and run the training with another dataset on row... feel free to reopen if needed 🐰 "
      },
      {
        "user": "nrjvarshney",
        "created_at": "2020-08-25T14:50:08Z",
        "body": "Do I need to load the checkpoint model after each training, if I'm initializing the model above the loop?\r\n"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-30T12:56:38Z",
        "body": "@nrjvarshney nope, reload checkpoint is not required. Also, this will reset your optimizers and schedulers if you call `.fit` on each dataset. If you want that then it's ok, else you have to do what @Borda suggested."
      },
      {
        "user": "nrjvarshney",
        "created_at": "2021-02-02T03:13:31Z",
        "body": "@Borda , @rohitgr7 , It would be great if you could share code for this. I'm a bit confused about this. \r\nI intend to train a model sequentially on 10 datasets i.e train on dataset_1 then on dataset_2 and so on. Should the learning rate, optimizer be reset after training on dataset_1 and subsequently after every training?\r\n\r\nAlso, please differentiate between resume training (i.e start training from where it was left) and continue training (start training on a new dataset but instead of starting from scratch use a model that was already fine-tuned on some other dataset but use newly initialized learning rate, optimizer etc.) in terms of code.\r\n\r\nResume Training (ideally should be on the same dataset):\r\n```\r\nmodel = Model()\r\ntrainer = Trainer(resume_from_checkpoint='some/path/to/my_checkpoint.ckpt')\r\n```\r\nContinue Training on Dataset D2 using an already fine-tuned model on dataset D1 \r\n```\r\nmodel  = load_from_checkpoint('path/to/checkpoint.ckpt')\r\n```\r\nThen?\r\n\r\nThanks."
      }
    ]
  },
  {
    "number": 3116,
    "title": "AttributeDict vs Namespace?",
    "created_at": "2020-08-24T07:08:55Z",
    "closed_at": "2020-08-25T06:57:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3116",
    "body": "## ❓ Questions and Help\r\n\r\nI've looked at other issues that bring up using `AttributeDict` or `Namespace`, but have not seen any clarifying why pytorch lightning did not go with `Namespace` instead of `AttributeDict`. \r\n\r\n#### What is your question?\r\n\r\nWhat is the difference `argparse.Namespace` and `pytorch_lightning.utilities.AttributeDict`? What did `Namespace` lack that an `AttributeDict` was made to accomplish?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3116/comments",
    "author": "MarioIshac",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-08-25T06:57:13Z",
        "body": "`AttributeDict` has mixed features between `Dict` and `Namespace` as in past we have used dict for some places and namespace elsewhere, so do not break code and compatibility we came with this kind of hybrid 🐰 \r\nTo your question, I would recommend using `AttributeDict`\r\n\r\nfeel free to reopen if needed :]"
      }
    ]
  },
  {
    "number": 3107,
    "title": "How automaticly load best model checkpoint on Trainer instance with TestTubeLogger",
    "created_at": "2020-08-23T02:29:14Z",
    "closed_at": "2020-08-25T08:16:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3107",
    "body": "The only way I've found to resume the training of a model from the best checkpoint is to explicitly instance Trainer with following structure:\r\n\r\n```python\r\nbest_epoch = 250  ## ?? (Actually i dont know)\r\nself.trainer = ptl.Trainer(\r\n    logger=TestTubeLogger(\r\n        save_dir=self.save_dir,\r\n        version=0  # fixed \r\n      ),        \r\n    resume_from_checkpoint=str(self.save_dir / 'default' / 'version_0' / 'checkpoints' / 'epoch={}.ckpt'.format(best_epoch)),\r\n    default_root_dir=self.save_dir,                                                                                                                                                                 )\r\n```\r\nThe problem is that is not natural to know the exact epoch which has the better model.\r\n\r\nI think loading the best model is a pretty natural operation for most of the cases:\r\nTraining: You want to continue training from the best model.\r\nTest: You want to test the best model.\r\n\r\nI think a method to find the \"best checkpoint\" is the kind of boilerplate code this library tries to avoid.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3107/comments",
    "author": "Vichoko",
    "comments": [
      {
        "user": "lezwon",
        "created_at": "2020-08-23T03:54:35Z",
        "body": "Have you tried using `trainer.checkpoint_callback.best_model_path` ?"
      },
      {
        "user": "Vichoko",
        "created_at": "2020-08-23T04:28:04Z",
        "body": "But how may the trainer know the best_model_path prior to loading?\r\nI mean load the Trainer in a fresh runtime it's exactly what i want to do.\r\n"
      },
      {
        "user": "lezwon",
        "created_at": "2020-08-23T05:39:43Z",
        "body": "Oh, I'm not sure if there's anything to help with that yet. I guess right now we have to explicitly specify a path to the checkpoint."
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-23T09:28:04Z",
        "body": "> Training: You want to continue training from the best model.\r\n\r\nHow can a new Trainer instance know the best model checkpoint you saved using another Trainer instance you used to train the model before?\r\n\r\n> Test: You want to test the best model.\r\n\r\nyou can just use `trainer.test(ckpt_path='best')`"
      },
      {
        "user": "Vichoko",
        "created_at": "2020-08-24T02:59:51Z",
        "body": "Thank you for your comments.\r\n\r\nAbout the Training scheme, I'm pretty sure the TubeTestLogger did the best model loading before i updated. The feature stopped working after updating PyTorch-lightning from 0.3 to 0.9.\r\nAbout loading the best model Trainer instance I thought about picking the checkpoint path with the higher epoch from the checkpoint folder and use `resume_from_checkpoint` Trainer param to load it. I thought there'd be an easier way but I guess not. Anyway i'll keep this issue updated if i come up with any solution to this case."
      },
      {
        "user": "Borda",
        "created_at": "2020-08-25T08:16:36Z",
        "body": "I would recommend using Tensorboard logger instead TestTube logger, anyway as mentioned above the \"best\" is part of checkpointing, not logger :]"
      },
      {
        "user": "Vichoko",
        "created_at": "2020-08-25T13:56:38Z",
        "body": "@Borda Can you explain why would you prefer Tensorboard logger? I thought Test Tube logger use was encouraged as was used so much on the tutorial."
      },
      {
        "user": "Borda",
        "created_at": "2020-08-25T14:05:52Z",
        "body": "@williamFalcon ^^"
      },
      {
        "user": "Vichoko",
        "created_at": "2020-08-25T15:04:51Z",
        "body": "This is the simpler workaround Trainer and Logger scheme I made for the purpose of this issue, based on what I had.\r\n```python\r\n\r\nversion = 1\r\nlogger = TestTubeLogger(\r\n    save_dir=save_dir,\r\n    version=version  # fixed to one to ensure checkpoint load\r\n)\r\nckpt_folder = save_dir / 'default' / 'version_{}'.format(version) / 'checkpoints'\r\nbest_epoch = find_best_epoch(ckpt_folder)\r\nself.trainer = ptl.Trainer(\r\n    logger=logger,\r\n    resume_from_checkpoint=str(ckpt_folder / 'epoch={}.ckpt'.format(best_epoch)),\r\n)\r\n```\r\n\r\nAnd the ```find_best_epoch``` i defined was:\r\n```python\r\ndef find_best_epoch(ckpt_folder):\r\n    \"\"\"\r\n    Find the highest epoch in the Test Tube file structure.\r\n    :param ckpt_folder: dir where the checpoints are being saved.\r\n    :return: Integer of the highest epoch reached by the checkpoints.\r\n    \"\"\"\r\n    ckpt_files = listdir(ckpt_folder)  # list of strings\r\n    epochs = [int(filename[6:-5]) for filename in ckpt_files]  # 'epoch={int}.ckpt' filename format\r\n    return max(epochs)\r\n```\r\nI hope this may help someone."
      },
      {
        "user": "loretoparisi",
        "created_at": "2024-08-28T15:56:11Z",
        "body": "@Vichoko I think it could be useful also a `find_best_loss` function assumed you have same epoch, different losses"
      }
    ]
  },
  {
    "number": 3087,
    "title": "[question] How to override how `self.train_transform` is applied to batches in `LightningDataModule`.",
    "created_at": "2020-08-21T07:11:28Z",
    "closed_at": "2020-10-28T22:27:18Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3087",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n_I want to apply the same random transformation on both X and Y (both are batches if RGB image) of a training batch using `Kornia` **after** the batch has been moved to the GPU (or TPU)._\r\n\r\nI cannot figure out what part of **PL** I need to override to make this work.\r\n\r\nAs far as Kornia is concerned, I know how to get the state of a random transform and use it again for another batch. But I cannot understand where inside `LightningDataModule` I am supposed to write that code such that it works properly even for MultiGPU training.\r\n\r\n\r\n#### Code\r\n\r\n#### What have you tried?\r\nI can simply include the augmentation step in the `LightningModule`'s training step after receiving the batches, but that would decouple my `LightningDataModule` from it's transforms which I don't want to do.\r\n\r\nAny help would be greatly appreciated !! 🙂\r\n\r\nOn another note, the documentation isn't very clear on how the `self.train_transform` property of `LightningDataModule` is applied to batches of data and how we can change that behavior (which is pretty much what my question is).\r\n\r\n#### What's your environment?\r\n\r\n - OS: [Ubuntu 16.04 LTS]\r\n - Packaging [ conda ]\r\n - Version [0.8.5]\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3087/comments",
    "author": "akashpalrecha",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-21T07:12:07Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-21T18:48:58Z",
        "body": "you can access that using `self.trainer.datamodule.train_transforms` within `LightningModule` but I am not sure if it's the right way to do it."
      },
      {
        "user": "akashpalrecha",
        "created_at": "2020-08-22T04:29:37Z",
        "body": "@rohitgr7 This is what I'm having to do right now. I'm calling the `setup` function on my `LightningDataModule` first, and then setting a `transforms` property in the `LightningModule` which I get from my `LightningDataModule`. I use the `transforms` to then manually transform my image pairs in every training step.\r\n\r\nBut ideally, I'd want the `LightningDataModule` to take care of it internally."
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-22T07:34:27Z",
        "body": "@nateraw"
      },
      {
        "user": "versatran01",
        "created_at": "2020-08-24T16:01:19Z",
        "body": "I use kornia and I just wrote my own transform and call it in train_step.\r\nKornia's design goal is to make all these transforms differentiable, so it's not that bad to call it in the train_step."
      },
      {
        "user": "akashpalrecha",
        "created_at": "2020-08-25T05:37:16Z",
        "body": "@versatran01 Yeah, that's what I'm doing right now and there's really no issue with this _right now_ to be honest. But it prevents me from using the same data module design in other lightning modules without having to manually add the transformation code in `training_step` every time. Ideally, the data module should do everything related to data."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T22:01:56Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3065,
    "title": "Callbacks are not being called, gradient norm not getting logged,how to debug these scenarios? ",
    "created_at": "2020-08-20T03:44:45Z",
    "closed_at": "2020-10-29T06:10:42Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3065",
    "body": "## ❓ Questions and Help\r\n* I have defined a callback for the `on_epoch_end` event. Also, I have defined `track_grad_norm = 1`, which I think is also kind of a callback.\r\n* When I used `--fast_dev_run` or `--limit_train_batches` and `--limit_val_batches` the callback(including gradient norm plot) got executed properly.\r\n* When I run without the above flags, `on_epoch_end` is sometimes not executing (or silently failing?), gradient norms are not getting plotted at all. \r\n* Checkpoint saving callback is always successfully executing.\r\n\r\nNow I am unsure how do I debug this scenario. There is no error messages logged.\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\n - Packaging:- pip\r\n - Version:- 0.9.0rc15 \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3065/comments",
    "author": "dchatterjee172",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-20T03:45:24Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T02:24:20Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3009,
    "title": "how to show estimated total training time in progress bar?",
    "created_at": "2020-08-16T17:57:11Z",
    "closed_at": "2020-10-25T06:02:05Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3009",
    "body": "how to show estimated total training time in progress bar?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3009/comments",
    "author": "aiyolo",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-08-16T18:45:18Z",
        "body": "I once made a progress bar for myself which shows the total epochs and the estimated time. I'll share it here. \r\nYou can simply pass it to the Trainer like so:\r\n`Trainer(callbacks=[GlobalProgressBar()])`\r\nHowever, this will not show the old progress bar and we currently do not support multiple progress bar callbacks, but I will think of a solution for this.\r\nIn the meantime, I hope this is of any help. \r\n\r\n\r\n```python\r\nimport importlib.util\r\nimport sys\r\n\r\nfrom pytorch_lightning.callbacks.progress import ProgressBarBase\r\n\r\n# check if ipywidgets is installed before importing tqdm.auto\r\n# to ensure it won't fail and a progress bar is displayed\r\nif importlib.util.find_spec('ipywidgets') is not None:\r\n    from tqdm.auto import tqdm\r\nelse:\r\n    from tqdm import tqdm\r\n\r\n\r\nclass GlobalProgressBar(ProgressBarBase):\r\n\r\n    def __init__(self, process_position: int = 0):\r\n        super().__init__()\r\n        self._process_position = process_position\r\n        self._enabled = True\r\n        self.main_progress_bar = None\r\n\r\n    def __getstate__(self):\r\n        # can't pickle the tqdm objects\r\n        state = self.__dict__.copy()\r\n        state['main_progress_bar'] = None\r\n        return state\r\n\r\n    @property\r\n    def process_position(self) -> int:\r\n        return self._process_position\r\n\r\n    def disable(self) -> None:\r\n        self._enabled = False\r\n\r\n    def enable(self) -> None:\r\n        self._enabled = True\r\n\r\n    def on_train_start(self, trainer, pl_module):\r\n        super().on_train_start(trainer, pl_module)\r\n        self.main_progress_bar = tqdm(\r\n            desc='Total Epochs',\r\n            initial=trainer.current_epoch,\r\n            total=trainer.max_epochs,\r\n            position=(2 * self.process_position),\r\n            disable=False,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n        )\r\n\r\n    def on_train_end(self, trainer, pl_module):\r\n        self.main_progress_bar.close()\r\n\r\n    def on_epoch_end(self, trainer, pl_module):\r\n        self.main_progress_bar.update(1)\r\n\r\n```"
      },
      {
        "user": "aiyolo",
        "created_at": "2020-08-17T09:12:32Z",
        "body": "thank you very much"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T02:24:21Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "zxvix",
        "created_at": "2022-09-06T10:08:13Z",
        "body": "I'll share my solution in case anyone finds it useful. It simply adds a total remaining time field in the current progress bar. Use it by replacing `RichProgressBar` with `BetterProgressBar`.\r\n```python\r\nimport re\r\nfrom datetime import timedelta\r\nfrom typing import Union\r\n\r\nfrom pytorch_lightning.callbacks.progress.rich_progress import RichProgressBar\r\nfrom rich.progress import ProgressColumn\r\nfrom rich.style import Style\r\nfrom rich.text import Text\r\n\r\nclass RemainingTimeColumn(ProgressColumn):\r\n    \"\"\"Show total remaining time in training\"\"\"\r\n\r\n    max_refresh = 1.0\r\n\r\n    def __init__(self, style: Union[str, Style]) -> None:\r\n        self.style = style\r\n        self.estimated_time_per_epoch = None\r\n        super().__init__()\r\n\r\n    def render(self, task) -> Text:\r\n        if 'Epoch' in task.description:\r\n            # fetch current epoch number from task description\r\n            m = re.search(r'Epoch (\\d+)/(\\d+)', task.description)\r\n            current_epoch, total_epoch = int(m.group(1)), int(m.group(2))\r\n\r\n            elapsed = task.finished_time if task.finished else task.elapsed\r\n            remaining = task.time_remaining\r\n            \r\n            if remaining:\r\n                time_per_epoch = elapsed + remaining\r\n                if self.estimated_time_per_epoch is None:\r\n                    self.estimated_time_per_epoch = time_per_epoch\r\n                else:\r\n                    # smooth the time_per_epoch estimation\r\n                    self.estimated_time_per_epoch = 0.99 * self.estimated_time_per_epoch + 0.01 * time_per_epoch\r\n\r\n                remaining_total = self.estimated_time_per_epoch * (total_epoch - current_epoch - 1) + remaining\r\n\r\n                return Text(f\"{timedelta(seconds=int(remaining_total))}\", style=self.style)\r\n            \r\n        else:\r\n            return Text(\"\")\r\n\r\n\r\nclass BetterProgressBar(RichProgressBar):\r\n    def configure_columns(self, trainer) -> list:\r\n        columns = super().configure_columns(trainer)\r\n        columns.insert(4, RemainingTimeColumn(style=self.theme.time))\r\n        return columns\r\n```"
      },
      {
        "user": "Sandv",
        "created_at": "2022-12-08T03:57:05Z",
        "body": "Works perfectly. Thank you!"
      }
    ]
  },
  {
    "number": 2984,
    "title": "CrossEntropyLoss with weights",
    "created_at": "2020-08-15T02:46:24Z",
    "closed_at": "2020-08-15T09:49:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2984",
    "body": "I need weights in CrossEntropyLoss (actually multiple, but the same issue).  The documentation talks about tensors copied from other tensors, but there is no tensor to copy from in the init.  So I'm stuck.\r\nTo make the weights unquestionably simple, I use ones.\r\n\r\n```\r\nclass JJG_Transformer(pl.LightningModule):\r\n\r\n    def __init__(self, alphanet_plus_2, letter_weights_per_position):\r\n        super(JJG_Transformer, self).__init__()\r\n        self.criterions = []\r\n        for weight in self.letter_weights_per_position:\r\n            weight = torch.ones((94))\r\n            self.criterions.append( torch.nn.CrossEntropyLoss(weight=weight) )\r\n    def validation_step(self, batch, batch_idx):\r\n        batch_im, batch_true_value_NT, batch_letter_transformer_input = batch\r\n        out_NTA = self(batch_im, batch_letter_transformer_input)\r\n        loss0 = self.criterions[0](out_NTA[:,0,:], batch_true_value_NT[:,0])\r\n        loss1 = self.criterions[1](out_NTA[:,1,:], batch_true_value_NT[:,1])\r\n        loss = loss0 + loss1\r\n        tensorboard_logs = {'val_loss': loss, 'val_loss0': loss0, 'val_loss1':loss1}\r\n        return {'val_loss': loss, 'log': tensorboard_logs}\r\n\r\n```\r\n\r\n  ```\r\nFile \"/home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/Aug2020_simple_transformer/src/kiss_transformer.py\", line 254, in <module>\r\n    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 34, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1017, in fit\r\n    self.accelerator_backend.train(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 56, in train\r\n    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 219, in ddp_train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1196, in run_pretrain_routine\r\n    self._run_sanity_check(ref_model, model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1229, in _run_sanity_check\r\n    eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 325, in _evaluate\r\n    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 609, in evaluation_forward\r\n    output = model(*args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 160, in forward\r\n    output = self.module.validation_step(*inputs[0], **kwargs[0])\r\n  File \"/home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/Aug2020_simple_transformer/src/kiss_transformer.py\", line 128, in validation_step\r\n    loss0 = self.criterions[0](out_NTA[:,0,:], batch_true_value_NT[:,0])\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\", line 948, in forward\r\n    ignore_index=self.ignore_index, reduction=self.reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2422, in cross_entropy\r\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2218, in nll_loss\r\n    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\r\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'weight' in call to _thnn_nll_loss_forward\r\nTraceback (most recent call last):\r\n  File \"kiss_transformer.py\", line 254, in <module>\r\n    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 34, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1030, in fit\r\n    results = self.accelerator_backend.spawn_ddp_children(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 118, in spawn_ddp_children\r\n    results = self.ddp_train(local_rank, mp_queue=None, model=model, is_master=True)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 219, in ddp_train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1196, in run_pretrain_routine\r\n    self._run_sanity_check(ref_model, model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1229, in _run_sanity_check\r\n    eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 325, in _evaluate\r\n    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 609, in evaluation_forward\r\n    output = model(*args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 160, in forward\r\n    output = self.module.validation_step(*inputs[0], **kwargs[0])\r\n  File \"kiss_transformer.py\", line 128, in validation_step\r\n    loss0 = self.criterions[0](out_NTA[:,0,:], batch_true_value_NT[:,0])\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\", line 948, in forward\r\n    ignore_index=self.ignore_index, reduction=self.reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2422, in cross_entropy\r\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2218, in nll_loss\r\n    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\r\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'weight' in call to _thnn_nll_loss_forward\r\n```\r\n\r\n```\r\ntrainer = pl.Trainer( gpus=[0, 1],  \r\n                accumulate_grad_batches=16, \r\n                max_epochs=500, \r\n                check_val_every_n_epoch=1, \r\n                distributed_backend='ddp', \r\n```\r\n\r\npl__version__ 0.9.0rc12\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2984/comments",
    "author": "johngrabner",
    "comments": [
      {
        "user": "sykrn",
        "created_at": "2020-08-15T06:57:00Z",
        "body": "I think you just need to use `.cuda()` for the weight or criterion."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-15T09:49:13Z",
        "body": "@sykrn is on the right track. \r\nThe problem is that your criterion is in a list. nn.Module does not recognize submodules in lists. \r\nYou solve your problem by using ModuleList: \r\n\r\n```python \r\n\r\nself.criterions = nn.ModuleList()  # this is the fix\r\n\r\nfor weight in self.letter_weights_per_position:\r\n    weight = torch.ones((94))\r\n    self.criterions.append( torch.nn.CrossEntropyLoss(weight=weight) )\r\n```\r\nnow your criterions (and tensors within it) will be automatically moved to the right device!"
      },
      {
        "user": "johngrabner",
        "created_at": "2020-08-15T15:26:55Z",
        "body": "Thank you. Made the example more explicit for future references.\r\n\r\n```\r\n        # fake fixed tensor weights \r\n        weights_per_position = []\r\n        for i in range(26):\r\n            t = torch.ones((94), dtype=torch.float32)\r\n            weights_per_position.append(t)\r\n\r\nclass JJG_Transformer(pl.LightningModule):\r\n\r\n    def __init__(self, weights_per_position):\r\n\r\n        # the list to hold the criterion\r\n        self.criterions = torch.nn.ModuleList() \r\n\r\n        for weight in self.weights_per_position:\r\n            self.criterions.append( torch.nn.CrossEntropyLoss(weight=weight) )\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss0 = self.criterions[0](out_NTA[:,0,:], batch_true_value_NT[:,0])\r\n        loss1 = self.criterions[1](out_NTA[:,1,:], batch_true_value_NT[:,1])\r\n        loss = loss0 + loss1\r\n```\r\n\r\nWhat does \"properly registered\" mean in?\r\n`ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.`"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-15T22:31:32Z",
        "body": "It means:\r\n\r\n```python\r\nclass Wrong(nn.Module):\r\n\r\n    def __init__(self)\r\n        super().init()\r\n        self.list = [CrossEntropyLoss()]\r\n\r\nw = Wrong()\r\nw.children() # no children\r\n```\r\nvs.\r\n```python\r\nclass Right(nn.Module):\r\n\r\n    def __init__(self)\r\n        super().init()\r\n        self.list = nn.ModuleList([CrossEntropyLoss()])\r\n\r\nr = Right()\r\nr.children() # correctly returns the cross entropy module\r\n```\r\n\r\nOperations like model.to('cuda') will also recursively operate on all submodules (children and their children etc.). But a Python list for example is not considered a submodule, because it's not an instance of nn.Module"
      },
      {
        "user": "johngrabner",
        "created_at": "2020-08-15T22:43:28Z",
        "body": "Thank you."
      }
    ]
  },
  {
    "number": 2928,
    "title": "is limit_train_batches shuffle or random",
    "created_at": "2020-08-12T08:13:07Z",
    "closed_at": "2020-08-13T10:30:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2928",
    "body": "hi, I am using limit_train_batches . If it is set, is it means a subdataset of whole train dataset ? similar with torch.utils.data.random_split",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2928/comments",
    "author": "qmpzzpmq",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-12T08:13:53Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-08-12T09:09:10Z",
        "body": "Yes, it is a subset of the train dataset\r\nBut, it doesn't similar with `random_split`"
      },
      {
        "user": "qmpzzpmq",
        "created_at": "2020-08-12T10:08:17Z",
        "body": "@ydcjeff I mean, is it random?"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-08-12T10:30:59Z",
        "body": "I think it is not random. It is the first `limit_train_batches` of the train dataset."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-12T15:20:47Z",
        "body": "Yes exactly, @ydcjeff is right. It will fetch batches from the dataloader until it reaches that amount, so your dataset and dataloader settings regarding shuffling will be respected. "
      },
      {
        "user": "qmpzzpmq",
        "created_at": "2020-08-13T08:27:02Z",
        "body": "@awaelchli @ydcjeff thx\r\n"
      },
      {
        "user": "adosar",
        "created_at": "2024-03-24T20:10:31Z",
        "body": "> @awaelchli @ydcjeff thx\r\n\r\nWhat if the dataloader uses `shuffle == True`?"
      }
    ]
  },
  {
    "number": 2922,
    "title": "RNN batch_first performance considerations based on backing kernel",
    "created_at": "2020-08-12T00:00:07Z",
    "closed_at": "2020-10-29T06:10:37Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2922",
    "body": "Question:\r\n\r\n**Can I use `batch_first=True` everywhere without worrying about performance differences on CPU, GPU, TPU?**\r\n\r\nThe defaults PyTorch sets are `batch_first=False` for all RNNs (RNN, LSTM, GRU). Pytorch Lightning mandates `batch_first=True` for `truncated_bptt_steps`, however. Will setting it True this mean relative faster performance on CPU and slower on GPU (for example) as compared to setting it False?\r\n\r\nIf` batch_first` affects performance specific to device (CPU, GPU, TPU), should we check for device and set `batch_first` accordingly? (In this case, seems like a prime target for this framework.)\r\n\r\n(Aside: Something I've always wondered: if the CUDA kernel is better (better time/space?) when `batch_first=False`, why doesn't it internally transpose(0, 1), compute, and transpose(1, 0) back instead of exposing this complexity to the user?\r\n\r\nie., in the general case, might it not be better to use `batch_first` everywhere, and perform gymnastics internally within the specific implementation.)\r\n\r\nNot sure if this project is the best place to ask this question, but I was implementing a project using this wonderful framework and it seems to me this question falls within the \"engineering code\" purview of Pytorch Lightning.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2922/comments",
    "author": "shivdhar",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-12T00:00:47Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T03:24:26Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2912,
    "title": "Checkpoint monitor str in default Trainer",
    "created_at": "2020-08-11T08:46:19Z",
    "closed_at": "2020-10-28T22:27:16Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2912",
    "body": "## ❓ Questions and Help\r\nHi,\r\nIn the default setting (with `checkpoint_callback=True`), when the method `configure_checkpoint_callback` is invoked by the c'ntr, the model hasn't yet been loaded to the trainer (as `run_pretrain_routine` is invoked only after `fit()`).\r\n\r\nThus, when calling `self.configure_checkpoint_callback(True)`, the line `train_step_only = not self.is_overridden('validation_step')` always returns `False`; even if `validation_step` was overridden in the user-class which extends `pl.LightningModule`.\r\n\r\nThis can obviously be solved by instantiating an exterior `ModelCheckpoint()`, but I guess this is not what was intended.\r\n\r\nI'm new to this community, so I might be missing something here. \r\nThanks in advance!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2912/comments",
    "author": "beyondyoni",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-11T08:47:00Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-11T15:30:06Z",
        "body": "Nice catch! ~~Neither `validation_step` belongs to `self` (Trainer) here~~. Either it should default to `loss` or `val_loss` just like early_stopping_checkpoint."
      },
      {
        "user": "beyondyoni",
        "created_at": "2020-08-12T10:39:50Z",
        "body": "Thanks! :)\r\nI think the best solution would be invoking `configure_checkpoint_callback` after `fit` is invoked, as part of `run_pretrain_routine`"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-14T22:56:33Z",
        "body": "that might break things since in that case calling `trainer.fit` will initialize the checkpoint again if it is called more than once which I think is not right."
      },
      {
        "user": "beyondyoni",
        "created_at": "2020-08-19T11:33:38Z",
        "body": "@rohitgr7  so what's the recommended solution?\r\ninitiating the checkpoint exteriorly?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-22T17:45:24Z",
        "body": "@beyondyoni I would say change it to `val_loss` just like it's in early stopping. Mind send a PR?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T22:02:00Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2888,
    "title": "Understanding the Progress Bar",
    "created_at": "2020-08-08T14:01:20Z",
    "closed_at": "2020-08-08T14:42:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2888",
    "body": "I train on MNIST with data loaders defined below (full train / test sets with `batch_size=128`).  \r\n`'val_check_interval': 0.1`, so per training epoch, I have 10 validation runs.  \r\n\r\nNow:\r\n- 10000 (test) images / 128 (batch_size) = 78.125, so steps such as 54/79 do make sense.  \r\n- 60000 (train) images / 128 (batch_size) = 468.75, so I'd expect something like 120/469.  \r\n\r\nWhat is the \"1259\" representing in the progress bar? I can observe in tensorboard, that the epoch number goes up at exactly 459.\r\n```\r\nValidating:  68%|██████▊   | 54/79 [00:08<00:03,  6.57it/s]\r\nEpoch 4:  78%|███████▊  | 976/1259 [04:01<01:09,  4.05it/s, loss=19279.273, v_num=0]\r\n```\r\n\r\n#### Code\r\n##### Data Loaders\r\n```python\r\n    def train_dataloader(self) -> DataLoader:\r\n        \"\"\"Pytorch-lightning function.\"\"\"\r\n        transform = torchvision.transforms.Compose([torchvision.transforms.Resize((32, 32)),\r\n                                                    torchvision.transforms.ToTensor()])\r\n        train_set = torchvision.datasets.MNIST(root=DATA_DIR_PATH / 'mnist_data',\r\n                                               train=True,\r\n                                               download=True,\r\n                                               transform=transform)\r\n        return DataLoader(train_set,\r\n                          batch_size=128,\r\n                          shuffle=True,\r\n                          num_workers=0)\r\n\r\n    def val_dataloader(self) -> DataLoader:\r\n        \"\"\"Pytorch-lightning function.\"\"\"\r\n        transform = torchvision.transforms.Compose([torchvision.transforms.Resize((32, 32)),\r\n                                                    torchvision.transforms.ToTensor()])\r\n        val_set = torchvision.datasets.MNIST(root=DATA_DIR_PATH / 'mnist_data',\r\n                                             train=False,\r\n                                             download=True,\r\n                                             transform=transform)\r\n        return DataLoader(val_set,\r\n                          batch_size=128,\r\n                          shuffle=False,\r\n                          num_workers=0)\r\n```\r\n#### What's your environment?\r\n - OS: Ubuntu 20.04\r\n - Packaging: pipenv\r\n - Lightning Version: 0.8.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2888/comments",
    "author": "matthaeusheer",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-08T14:02:00Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-08T14:16:28Z",
        "body": "It's 79*10 (10 times validation per epoch) + 469(train batches) = 1259. The progress bar contains both train and validation steps."
      },
      {
        "user": "matthaeusheer",
        "created_at": "2020-08-08T14:42:38Z",
        "body": "I see, thanks @rohitgr7 :+1: "
      }
    ]
  },
  {
    "number": 2864,
    "title": "Some questions about checkpoints and learning rate",
    "created_at": "2020-08-07T14:30:47Z",
    "closed_at": "2020-08-22T13:13:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2864",
    "body": "How to pass learning rate to progression bar and how to choose metric for saving model weights? Thanks!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2864/comments",
    "author": "jovenwayfarer",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-07T14:31:29Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-08-07T15:27:42Z",
        "body": "@SkafteNicki mind have look? :]"
      },
      {
        "user": "jovenwayfarer",
        "created_at": "2020-08-08T16:09:25Z",
        "body": "also how to print metrics and learning rate after each epoch? I tried to understand PrintCallback, but could not manage. "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-09T09:48:47Z",
        "body": "to print metrics you can create a `Callback` and override `on_epoch_end` method. Inside that, you get the trainer parameter, use `trainer.lr_schedulers` to extract learning rates. To choose a metric for saving weights you have to assign the metric name to `monitor` parameter in `ModelCheckpoint`."
      },
      {
        "user": "jovenwayfarer",
        "created_at": "2020-08-09T18:03:22Z",
        "body": "@rohitgr7 Thanks! How to extract learning rate from `trainer.lr_schedulers`?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-09T18:07:55Z",
        "body": "I think it is either `trainer.lr_schedulers[0].optimizer.lr` or `trainer.lr_schedulers[0].optimizer.param_groups[0]['lr']`. Depends upon your optimizer param_group settings I think."
      },
      {
        "user": "jovenwayfarer",
        "created_at": "2020-08-22T13:13:46Z",
        "body": "I used Learning rate logger to monitor it via TensorBoard "
      }
    ]
  },
  {
    "number": 2735,
    "title": "how to resume in the same folder",
    "created_at": "2020-07-28T12:01:55Z",
    "closed_at": "2020-10-29T05:10:39Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2735",
    "body": "when i launch the pytorch lightning trainer fit(), to resume it seems you can pass one old checkpoint, but i am not sure if it is possible (or perhaps it is chosen not to) to simply resume the current training in the same \"version\" sub-folder?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2735/comments",
    "author": "etienne87",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-28T12:02:40Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "AAnoosheh",
        "created_at": "2020-07-30T12:10:48Z",
        "body": "I can't get mine to ever create a new \"version\" folder, no matter what. We seem to have opposite problems."
      },
      {
        "user": "etienne87",
        "created_at": "2020-07-30T12:33:31Z",
        "body": "i launch my training like this:\r\n```\r\ntrainer = pl.Trainer(\r\n            default_root_dir=params.log_dir,\r\n            gpus=1,\r\n            precision=16,\r\n            progress_bar_refresh_rate=20,\r\n            max_epochs=20,\r\n            check_val_every_n_epoch=4,\r\n            resume_from_checkpoint=ckpt,\r\n        )\r\n```"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T04:24:23Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "nimral",
        "created_at": "2022-12-02T16:43:51Z",
        "body": "I have the same question. When trying to resume training with \r\n`python3 main.py fit --config /default_root_dir/lightning_logs/version_34/config.yaml --ckpt_path /default_root_dir/lightning_logs/version_34/checkpoints/last.ckpt`, the new checkpoints and events get saved to `/default_root_dir/lightning_logs/version_35/` and not to `/default_root_dir/lightning_logs/version_34/`"
      },
      {
        "user": "Jerrypiglet",
        "created_at": "2023-01-05T06:52:36Z",
        "body": "Same issue. Any workarounds?"
      }
    ]
  },
  {
    "number": 2725,
    "title": "How to access wandb logging directory from lightning?",
    "created_at": "2020-07-27T16:33:29Z",
    "closed_at": "2020-08-02T19:41:08Z",
    "labels": [
      "question",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2725",
    "body": "## I want to solve simple task: store .yaml config file in the same folder where checkpoints are saved\r\n\r\nI'm using following tools:\r\n1. lightning\r\n2. hydra for reading configs\r\n3. lightning wandb logging interface\r\n\r\nI see following options:\r\n1. save everything with hydra to it's `outputs` directory. \r\n  Pros: directory is easily accessible with `os.getcwd()`, so everything can be managed manually if needed\r\n  Cons: wandb logs are in the separate folder; default checkpoints directory is another (third already) directory, automatically created by lightning\r\n2. save logs, config.yaml and checkpoints to local wandb run directory\r\n  Pros: everything you need is in one place and can be easily analysed or accessed, not spawning to much separate logging directories\r\n  Cons: directory with wandb logs is not accessible from runtime (or, better say, hidden very well), so I can't save anything there. Despite it is possible to set wandb directory manually, I don't want to do it, because it will force me to recreate it for every model launch.\r\n\r\nI've tried to google hardly, searching for the best solution, which works with checkpointing callback and wandb logger, but I ended up with saving both checkpoints and .yaml file to hydra directory, which is created for current run.\r\n\r\nMy current solution is bad, because it stores different logging entities in different places. Moreover, DDP spawns two hydra folders for a run, but that's for another issue, I suppose.\r\n\r\n#### Code\r\n```\r\n@hydra.main(config_path=\"train-config.yaml\", strict=False)\r\ndef train(config: DictConfig) -> None:\r\n    config.hydra_base_dir = os.getcwd()\r\n    original_wd = hydra.utils.get_original_cwd()\r\n    os.chdir(original_wd)\r\n\r\n    checkpoint_callback = ModelCheckpoint(\r\n        filepath=config.hydra_base_dir,\r\n        save_top_k=3,\r\n        verbose=True,\r\n        monitor=\"val_loss\",\r\n        mode=\"min\",\r\n    )\r\n    shutil.copy2(\"train-config.yaml\", os.path.join(config.hydra_base_dir, \"train-config.yaml\"))\r\n\r\n    wandb_logger = WandbLogger(\r\n        offline=False,\r\n    )\r\n\r\n    model = MyModel(config)\r\n\r\n    trainer = pl.Trainer(\r\n        max_epochs=config.train.max_epochs,\r\n        gpus=config.train.n_gpu,\r\n        auto_select_gpus=True,\r\n        distributed_backend=\"ddp\",\r\n        checkpoint_callback=checkpoint_callback,\r\n        logger=wandb_logger,\r\n    )\r\n\r\n    trainer.fit(model)\r\n\r\n```\r\n\r\n### P.S.\r\n1. I know that wandb saves configs locally to it's folder, but it is still separate from checkpoints. Moreover, it stores config in the format, which should be processed to acquire the structre of original config file.\r\n2. I also know that lightning checkpoint contains model parameters in `hparams` field, but the goal is to store run config in the separate file. \r\n\r\n#### Environment?\r\n\r\n - OS: Ubuntu 18.04\r\n - Conda, Python 3.7.7\r\n - hydra-core==0.11.3\r\n- pytorch-lightning==0.8.5\r\n- wandb==0.9.3\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2725/comments",
    "author": "topshik",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-27T16:34:10Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-07-31T15:09:49Z",
        "body": "@borisdayma mind have look? 🐰 "
      },
      {
        "user": "borisdayma",
        "created_at": "2020-07-31T17:16:59Z",
        "body": "Hi, what do you think about putting the files directly in wandb folder. You can access it with `wandb.run.dir`"
      },
      {
        "user": "topshik",
        "created_at": "2020-08-03T10:45:44Z",
        "body": "@borisdayma `wandb.run.dir` cannot be accessed after `pytorch_lightning.loggers.WandbLogger` object is inited. That is the point of the issue. This happens, because I'm initing logger not from wandb interface but pytorch lightning one. Thus, `wandb.run` object doesn't exist.\r\n\r\nOne can use code snipped provided in the issue and check that after lines \r\n```\r\nwandb_logger = WandbLogger(\r\n        offline=False,\r\n    )\r\n```"
      },
      {
        "user": "borisdayma",
        "created_at": "2020-08-03T14:11:35Z",
        "body": "If the training has not started yet then you can use \"WandbLogger.experiment.dir\""
      },
      {
        "user": "Junyoungpark",
        "created_at": "2020-12-13T15:08:08Z",
        "body": "Is this problem fixed? I encounter kinda same issue what @topshik experienced.\r\nFor me, it was not able to find the path for the specific wandb run.\r\n\r\n```\r\nwandb_logger = WandbLogger(entity='sentinel', project='aa')\r\nckpt_dir = join(wandb_logger.experiment.dir, 'ckpt')\r\nos.mkdir(ckpt_dir)\r\ncheckpoint_cb = ModelCheckpoint(monitor='multi_step_loss', \r\n                                dirpath=ckpt_dir,\r\n                                filename='{epoch:02d}-{multi_step_loss:.2f}',\r\n                                save_top_k=3,\r\n                                mode='min')\r\n```\r\n\r\nAnd the traceback is as follows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/silab9/dev/WONIK/GraphPreCO_lighting.py\", line 179, in <module>\r\n    main(config)\r\n  File \"/home/silab9/dev/WONIK/GraphPreCO_lighting.py\", line 156, in main\r\n    config.to_yaml(join(wandb_logger.experiment.dir, \"model_config.yaml\"))\r\n  File \"/home/silab9/anaconda3/envs/gpu_torch/lib/python3.7/posixpath.py\", line 80, in join\r\n    a = os.fspath(a)\r\nTypeError: expected str, bytes or os.PathLike object, not method\r\n```\r\n\r\nAs @borisdayma suggested, I also tried `wandb_logger.experiment.dir` before I setup the `Trainer`.\r\nbut I ended up with the  same error. I guess it is because the `WandbLogger` doesn't instantiate the `run` object of wandb before actually training start.\r\n\r\noh btw I used `accelerator='ddp', plugins='ddp_sharded'` for the `Trainer`.\r\n"
      },
      {
        "user": "Junyoungpark",
        "created_at": "2020-12-13T15:56:56Z",
        "body": "I found a okay solution for this\r\n1) instantiate the wandb `run` results by calling `wandb.init()`\r\n2) Pass the wandb `run` object ot the `WandbLogger` as specifying `experiment=run`\r\n\r\nLater calling `run.dir` to get the running directory of the wandb run.\r\n\r\ndisadvantage of this solution is spawning `n` independent wandb run when using `ddp`. Here, `n` is the\r\nnumber of gpus."
      },
      {
        "user": "borisdayma",
        "created_at": "2020-12-14T21:35:27Z",
        "body": "I think the main issue is that when you call `wandb_logger.experiment`, it tries to create a run with the config from `Trainer` (as it's typically how it's created).\r\n\r\nThe 2 options would be:\r\n* Option 1: access the run from the `Trainer` with `trainer.logger.experiment.dir`\r\n* Option 2: create your custom run with `wandb.init()` as you did -> the intended behavior is that the logger does not new runs. Can you confirm you use latest versions of wandb and pytorch-lightning?"
      },
      {
        "user": "Junyoungpark",
        "created_at": "2020-12-15T01:04:49Z",
        "body": "@borisdayma Thanks for the comment.\r\n\r\nThe first option would be not enough for my purpose. Maybe one of the typical usages of `wandb` is to log the experiments, including the checkpoints, and save those logs to the wandb cloud such that we can retrieve any logs from the cloud later.\r\n\r\nIn `pytorch-lightening`, logging the checkpoints are typically done with `ModelCheckpoint` callback, which is required to be instantiated before the trainer object is instantiated. It was not straightforward that altering the saving path of checkpoints to the inside wandb logging directory, which will be generated after calling `trainer.fit()` method. That was the reason I decided to use kind-of the second option.  Is that possible (or more pytorch-lightning way) to implement the codes as I wish above?\r\n\r\nI used pytorch-lightning 1.1.0 and wandb 0.10.12"
      },
      {
        "user": "borisdayma",
        "created_at": "2020-12-15T01:12:09Z",
        "body": "You should not have to change the saving directory.\r\nActually `WandbLogger(log_model=True)` should automatically log your model (and checkpoints if you have any)."
      },
      {
        "user": "Junyoungpark",
        "created_at": "2020-12-15T01:39:17Z",
        "body": "Oh, that sounds great! \r\n\r\nCan I get a code snippet or documentation about your suggestion? What should I pass `dirtpath` and `filename` for the `ModelCheckPoint` to save all checkpoints to the `wandb.run.dir`? And also can I save any extra files, such as metadata for the experiments, to the `wandb.run.dir` before calling `trainer.fit()` method?"
      },
      {
        "user": "borisdayma",
        "created_at": "2020-12-15T01:58:21Z",
        "body": "You should be able to use any value (just keep the default).\r\nAt the moment it logs only model checkpoints but we should get more flexibility later with #4903 \r\nYou could also manually use W&B artifacts but I hope to make it easy with this integration."
      },
      {
        "user": "Junyoungpark",
        "created_at": "2020-12-15T02:19:37Z",
        "body": "Thanks I will look around the other issue also 👍 "
      }
    ]
  },
  {
    "number": 2718,
    "title": "datamodule how to load checkpoint",
    "created_at": "2020-07-27T02:49:54Z",
    "closed_at": "2020-09-18T06:15:38Z",
    "labels": [
      "feature",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2718",
    "body": "## ❓ Questions and Help\r\nTrainer will automatically save a checkpoint. Is this checkpoint shared by model and datamodule, datamodule can load this checkpoint as model `load_from_checkpoint`\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2718/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "nateraw",
        "created_at": "2020-08-04T21:48:34Z",
        "body": "The model checkpoint is not shared by the datamodule. I'm not entirely sure what something like this would look like. Do you have any idea of what you'd like to see @xiadingZ ? Would love to hear your thoughts 😄 \r\n\r\nAs for right now, you save/load your model checkpoint and you can pass any datamodule through that model that is compatible with it. I have a glimpse of an idea where we log a snapshot of the datamodule's code for some reproducibility/traceability reasons....but again, I don't know what this would look like/if it would just make things more confusing."
      },
      {
        "user": "nateraw",
        "created_at": "2020-09-18T06:15:38Z",
        "body": "moving discussion to #3544 as it lays out plans to implement the feature this question is inquiring about."
      }
    ]
  },
  {
    "number": 2693,
    "title": "How to accumulate model outputs for across batch losses?",
    "created_at": "2020-07-24T21:25:39Z",
    "closed_at": "2020-09-30T02:32:15Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2693",
    "body": "I was hoping that setting accumulate_grad_batches=2 and implementing training_step_end would result in training_step_end receive output from both training_steps, so list of len 2 in args. But I only received the batch output from the latest batch.\r\n\r\nThe reason I want both batches is because I am using a contrastive loss, and I want to constrast the embedding across and within batches depending on their classes. So essentially if I have 2 batches of size 8, I would like to combine the outputs from 2 training_step to get an effective batch of 16 to perform this contrastive loss.\r\n\r\nHow could I implement this without needing to manually keep track of the current batch_idx before clearing out some number of stored training_step_outputs\r\n\r\nThis is my current hack which isn't great since it could be wrong. It leads to potential model updates before computing the contrastive loss making the embeddings no longer valid for the loss.\r\n``` python\r\n\r\nif (batch_idx + 1) % self.config.PYTORCH_LIGHTNING.accumulate_grad_batches == 0:\r\n    loss = self.contrastive_loss(self.accumulated_batches)\r\n    self.accumulated_batches = []\r\nelse:\r\n    self.accumulated_batches.append(<training_step output>)\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2693/comments",
    "author": "Tarmily-Wen-Percepta",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-24T21:26:31Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-23T00:31:57Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "Maddy12",
        "created_at": "2021-03-08T22:36:18Z",
        "body": "Is this going to be addressed in another issue?\r\n"
      }
    ]
  },
  {
    "number": 2675,
    "title": "Validation epoch skipped",
    "created_at": "2020-07-22T19:38:15Z",
    "closed_at": "2020-07-24T14:23:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2675",
    "body": "Well, for some reason sometimes validation loop is skipped. I would like to give more details, but python spits out no message.\r\nThe progress bar just jumps forward and the print function that I placed in the beginning of validation_step is not being triggered.\r\nI get this behaviour for some datasets. I use several dataloaders and for some the validation happens, but if others appear in dataloaders list the validation is kinda avoided without any message.\r\n\r\nSo my question is can I enable verbose mode or something?\r\n\r\n#### Environment\r\n\r\n - OS: Ubuntu 18.04\r\n - Packaging: pip\r\n - Version: last\r\nI use torch 1.5.1 but tried nightly build - the same issue\r\n\r\nThanks for your work.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2675/comments",
    "author": "IgorDavidyuk",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-22T19:39:08Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "IgorDavidyuk",
        "created_at": "2020-07-24T14:23:39Z",
        "body": "The problem was in dataloaders that I use, obviously.\r\nSomehow length of some dataloaders is greater than number of batches they actually spit out, so the last batch did not trigger validation epoch start. "
      },
      {
        "user": "SamPusegaonkar",
        "created_at": "2022-07-19T17:01:01Z",
        "body": "Hi @IgorDavidyuk \r\n\r\n> The problem was in dataloaders that I use, obviously. Somehow length of some dataloaders is greater than number of batches they actually spit out, so the last batch did not trigger validation epoch start.\r\n\r\nHi Can you expand on this a little? Are you saying that your last batch in your dataloader was not equal to the batch size you set?"
      }
    ]
  },
  {
    "number": 2666,
    "title": "Plotting learning rate from a lr_scheduler via a Callback",
    "created_at": "2020-07-21T23:00:50Z",
    "closed_at": "2020-08-05T07:35:41Z",
    "labels": [
      "feature",
      "good first issue",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2666",
    "body": "I think the title explains a lot. But let me elaborate, I have a LightningModule which has a configure_optimizers method returns an optimizer and a scheduler. Later in a Callback I have a `on_batch_end` function in which I try to log the learning rate.\r\n\r\nOf course if the scheduler was accessible as a class member, we could `self.scheduler.get_lr()` on it and use the value to plot. Since this is not how it has been implemented, I am wondering how to do this?\r\n\r\nWould appreciate any pointers.\r\nPytorchLightning - 0.8.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2666/comments",
    "author": "bhashithe",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-21T23:01:49Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "s-rog",
        "created_at": "2020-07-22T00:29:35Z",
        "body": "If you have the same lr throughout the network (single param group) you can get it from:\r\n`self.trainer.optimizers[0].param_groups[0]['lr']`\r\nchange the indexing based on your optimizer and param configuration."
      },
      {
        "user": "bhashithe",
        "created_at": "2020-07-22T01:04:56Z",
        "body": "That worked, even if i have multiple groups does it work the same if I do something like this?\r\n\r\n`{f'lr_group{i}': param['lr'] for i, param in enumerate(self.trainer.optimizers[0].param_groups}`"
      },
      {
        "user": "s-rog",
        "created_at": "2020-07-22T01:11:41Z",
        "body": "should work!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-22T04:49:59Z",
        "body": "There is a `LearningRateLogger` callback in lightning."
      },
      {
        "user": "Borda",
        "created_at": "2020-08-04T21:03:22Z",
        "body": "@SkafteNicki mind have a look?"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-08-05T05:58:02Z",
        "body": "As @rohitgr7 mention, the `LearningRateLogger` which can be imported as `from pytorch_lightning.callbacks import LearningRateLogger` should be able to do what you ask for."
      }
    ]
  },
  {
    "number": 2656,
    "title": "How to reload partial weights from the trained checkpoint?",
    "created_at": "2020-07-21T10:40:17Z",
    "closed_at": "2020-07-22T01:47:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2656",
    "body": "#### What is your question?\r\nNow I want to reload partial weights from trained checkpoint and let the remaining parameters trained from scratch. But I didn't find the api that allows me to reload partial parameters.\r\n\r\n#### Code\r\nmy code is like this, I find pl only support resume_from_checkpoint path to reload all weights from checkpoint.\r\n```\r\ntrainer = pl.Trainer(gpus=1, early_stop_callback=None, resume_from_checkpoint=resume_checkpoint, val_check_interval=1000)\r\n\r\ntrainer.fit(model) \r\n```\r\n\r\nWhat should I do to reload partial parameters?   Thank you!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2656/comments",
    "author": "guvcolie",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-21T10:41:11Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-21T19:49:33Z",
        "body": "It's all PyTorch! The checkpoints we save are fully compatible with torch.load, so you do your fancy reloading before fit and then give whatever weights you want trained to your optimizers :)"
      },
      {
        "user": "guvcolie",
        "created_at": "2020-07-22T01:47:43Z",
        "body": "@awaelchli  got it! thank you!"
      }
    ]
  },
  {
    "number": 2650,
    "title": "How to perform inference on multiple GPUs?",
    "created_at": "2020-07-20T20:54:17Z",
    "closed_at": "2020-09-22T14:24:28Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2650",
    "body": "I have 4 GPUs, 1m images. \r\n\r\nI would like to use all 4 of them to run the method `test`.\r\n\r\nIs there any tutorial that shows how to do this?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2650/comments",
    "author": "ternaus",
    "comments": [
      {
        "user": "shtoshni",
        "created_at": "2020-07-23T20:01:19Z",
        "body": "Pass gpus=-1 or gpus=4? I think the trainer would automatically figure it out.  "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-22T06:52:42Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2606,
    "title": "Minimal example with custom RNNs cells and sliding window support",
    "created_at": "2020-07-14T12:10:50Z",
    "closed_at": "2020-11-01T01:10:40Z",
    "labels": [
      "good first issue",
      "question",
      "won't fix",
      "example"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2606",
    "body": "## ❓ Questions and Help\r\n\r\nHello,\r\n\r\nI would like to ask you about the support of pytorch-lighting regarding custom RNN cells and sliding window predictions of a sequence ( i.e. video)\r\n\r\nI am implementing a kind of conv-LSTM variant for video prediction, where each frame is estimated from the previous frames in a sliding window manner.\r\n\r\n### Before asking:   \r\nI have searched the issues and the docs and I have found this:\r\n```python\r\n# backprop every 5 steps in a batch\r\ntrainer = Trainer(truncated_bptt_steps=5)\r\n\r\n.....\r\n\r\n# Truncated back-propagation through time\r\ndef training_step(self, batch, batch_idx, hiddens):\r\n    # hiddens are the hiddens from the previous truncated backprop step\r\n    out, hiddens = self.lstm(data, hiddens)\r\n\r\n    return {\r\n        \"loss\": ...,\r\n        \"hiddens\": hiddens  # remember to detach() this\r\n    }\r\n\r\n```\r\nI have stored all the information I need (across timesteps) in the class (as members) so this is not an issue I think.\r\n\r\n#### What is your question?\r\nHow can I provide my sequence as a sliding window and train the model for each step of the sequence (grads+backprop)?\r\nDoes the implemented supported function for truncated back-prop through time support this?\r\n\r\n\r\n#### What have you tried?\r\nI have tried to overwrite:\r\n```\r\ndef tbptt_split_batch(self, batch, split_size):\r\n....\r\n```\r\nbut is seems it is not called on the **Validation sanity check** \r\n\r\n\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\n - Packaging: pip\r\n\r\nIt would be nice to create a minimal example on how the lib could support this kind of recurrent model predictions with sliding window support. If I can make this to work I can provide a minimal collab notebook.\r\n\r\nAny ideas?\r\n\r\nThanks a lot,\r\n\r\nN.A.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2606/comments",
    "author": "black0017",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-14T12:11:49Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-24T23:48:14Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2602,
    "title": "How to get the dictionary returned by the validation_epoch_end() method?",
    "created_at": "2020-07-13T19:02:00Z",
    "closed_at": "2020-09-25T16:42:02Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2602",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nHow do I fetch the dictionary returned by the validation_epoch_end() method? I don't want to add this to tensorflow logs \r\n#### Code\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\nreturn {\"label_freq\": label_freq, log: {\"loss\": loss}}\r\n\r\nI want to get the value of label_freq\r\n\r\n#### What's your environment?\r\n\r\n - OS: [e.g. iOS, Linux, Win]\r\n - Packaging [e.g. pip, conda]\r\n - Version [e.g. 0.5.2.1]\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2602/comments",
    "author": "nrjvarshney",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-13T19:03:04Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-16T18:17:41Z",
        "body": "Do you want this after every `validation_epoch_end` or for just the last epoch?"
      },
      {
        "user": "nrjvarshney",
        "created_at": "2020-07-19T00:11:41Z",
        "body": "After every epoch. @rohitgr7 "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-17T01:09:19Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2601,
    "title": "Training log",
    "created_at": "2020-07-13T15:29:53Z",
    "closed_at": "2020-07-26T17:29:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2601",
    "body": "Why after 50% of each epoch training, the process starts to print one line per batch?\r\n\r\n```python\r\nEpoch 1:  50%|█████▌     | 126/252 [00:29<00:29,  4.25it/s, loss=3.223, v_num=0]\r\nValidating: 0it [00:00, ?it/s]\r\nEpoch 1:  50%|█████▌     | 127/252 [00:32<00:31,  3.91it/s, loss=3.223, v_num=0]\r\nEpoch 1:  51%|█████▌     | 128/252 [00:33<00:32,  3.82it/s, loss=3.223, v_num=0]\r\nEpoch 1:  51%|█████▋     | 129/252 [00:34<00:33,  3.72it/s, loss=3.223, v_num=0]\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2601/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "lucadiliello",
        "created_at": "2020-07-13T15:59:20Z",
        "body": "Try to do the validation earlier, does the problem arise at `50%` or immediately after the validation?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-14T07:17:51Z",
        "body": "which environment are you in? "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-14T07:17:59Z",
        "body": "jupyter? "
      },
      {
        "user": "celsofranssa",
        "created_at": "2020-07-14T13:30:36Z",
        "body": "> Try to do the validation earlier, does the problem arise at `50%` or immediately after the validation?\r\n\r\nI'm not calling the validation step directly. Using the following model:\r\n\r\n```python\r\nclass MyModel(LightningModule):\r\n    \"\"\"Encodes the x1 and x2 into the same space of embeddings.\"\"\"\r\n\r\n    def __init__(self, config):\r\n        super(JointEncoder, self).__init__()\r\n        self.config = config\r\n        self.x1_encoder = Encoder(config)\r\n        self.x2_encoder = Encoder(config)\r\n        self.tokenizer = Tokenizer(config)\r\n        self.loss_fn = NPairLoss()\r\n        self.mrr = MRRMetric(name=\"MRR\")\r\n\r\n    def forward(self, x1, x2):\r\n        x1 = self.x1_encoder(x1)\r\n        x2 = self.x2_encoder(x2)\r\n        return x1, x2\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=1e-6, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        predict = self(x1, x2)\r\n        loss = self.loss_fn(predict, self.train_target)\r\n        return {'loss': loss}\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        predict = self(x1, x2)\r\n        loss = self.loss_fn(predict, self.test_target)\r\n        return {'test_loss': loss}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\r\n        return {'avg_test_loss': avg_loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        x1, x2 = self(x1, x2)\r\n        mrr = self.mrr(x1, x2)\r\n        return {'val_loss': mrr}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        return {'val_loss': avg_loss}\r\n\r\n    def train_dataloader(self):\r\n        train_dataset = CodeSearchDataset(\r\n            path=self.config.dataset.train_path,\r\n            tokenizer=self.tokenizer,\r\n            max_length=self.config.preprocessing.max_length)\r\n\r\n        return DataLoader(\r\n            train_dataset,\r\n            batch_size=self.config.train.batch_size,\r\n            drop_last=True,\r\n            num_workers=self.config.preprocessing.num_workers\r\n        )\r\n\r\n    def test_dataloader(self):\r\n        test_dataset = CodeSearchDataset(\r\n            path=self.config.dataset.train_path,\r\n            tokenizer=self.tokenizer,\r\n            max_length=self.config.preprocessing.max_length)\r\n\r\n        return DataLoader(\r\n            test_dataset,\r\n            batch_size=self.config.train.batch_size,\r\n            drop_last=True,\r\n            num_workers=self.config.preprocessing.num_workers\r\n        )\r\n\r\n    def val_dataloader(self):\r\n        val_dataset = CodeSearchDataset(\r\n            path=self.config.dataset.train_path,\r\n            tokenizer=self.tokenizer,\r\n            max_length=self.config.preprocessing.max_length)\r\n\r\n        return DataLoader(\r\n            val_dataset,\r\n            batch_size=self.config.train.batch_size,\r\n            drop_last=True,\r\n            num_workers=self.config.preprocessing.num_workers\r\n        )\r\n\r\n```\r\n\r\nI've just fit with a Trainer:\r\n\r\n```python\r\nmodel = MyModel(config=cfg)\r\ntrainer = Trainer(max_epochs=cfg.train.max_epochs, gpus=1)\r\ntrainer.fit(model)\r\n```"
      },
      {
        "user": "celsofranssa",
        "created_at": "2020-07-14T13:32:14Z",
        "body": "> jupyter?\r\n\r\nUbuntu terminal."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-14T15:57:22Z",
        "body": "it is strange, I cannot reproduce this. \r\nwhich version of tqdm and pytorch lightning do you have installed? \r\nwe usually see this behaviour on jupyter notebooks where it is a known issue #1399\r\n(and PyCharm)"
      },
      {
        "user": "celsofranssa",
        "created_at": "2020-07-14T17:21:21Z",
        "body": "> it is strange, I cannot reproduce this.\r\n> which version of tqdm and pytorch lightning do you have installed?\r\n> we usually see this behaviour on jupyter notebooks where it is a known issue #1399\r\n> (and PyCharm)\r\n\r\nSo that must be it, I was actually using the PyCharm terminal on Ubuntu."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-14T17:30:34Z",
        "body": "yup, then I can confirm, for PyCharm we don't have a solution an this happens with every stacked tqdm (when there are multiple ones, so not really PL related. "
      }
    ]
  },
  {
    "number": 2591,
    "title": "Model loaded from checkpoint file has low test score",
    "created_at": "2020-07-12T14:32:08Z",
    "closed_at": "2020-09-21T08:54:28Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2591",
    "body": "I have trained a model and now I want to load it again from a checkpoint file and test it. Unfortunately, the test scores are really low (not random though). It takes a while to create a minimal viable example (project is quite complex) that's why I am posting these code snippets here. Perhaps you can already spot a mistake I made.\r\n\r\n## TransferLearningModel.py\r\n```\r\nclass TransferLearningModel(LightningModule):\r\n    \r\n    def __init__(self, config):\r\n        super().__init__()\r\n        \r\n        self.config = config\r\n        # save config dict when saving model\r\n        self.save_hyperparameters(config)\r\n       ....\r\n        self.hparams.learning_rate = self.config[\"optimizer_initial_lr\"]\r\n\r\n        ......\r\n```\r\n\r\n## training_testing.py\r\n```\r\nconfig = {\r\n    \"model_name\": \"resnext101_32x8d\", \r\n    \"amp_optimization_level\": \"O1\", \r\n    ...\r\n}\r\n\r\nmodel = TransferLearningModel(config)\r\n\r\ntrainer = Trainer(...)\r\ntrainer.fit(model)\r\n\r\n# testing\r\ntrainer.test(ckpt_path = filepath)\r\n\r\n```\r\n\r\nAny idea what I might be doing wrong?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2591/comments",
    "author": "JanRuettinger",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-12T14:33:11Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-12T16:29:47Z",
        "body": "The only thing I see is you pass the amp optimization level to the model, what's that for?\r\nIf you use the training set for testing, do you get the performance matching the training?"
      },
      {
        "user": "JanRuettinger",
        "created_at": "2020-07-14T08:13:45Z",
        "body": "I use one config dict to configure the model as well as the trainer. `amp_optimization_level` is used as a parameter for the trainer to specify the apex optimization level. Do you think that this could cause the issue?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-12T08:39:19Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2542,
    "title": "How to tokenize within getitem() of Dataset with HuggingFace tokenizers",
    "created_at": "2020-07-07T17:45:17Z",
    "closed_at": "2020-07-08T10:49:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2542",
    "body": "\r\nHi All, \r\n\r\nI see in the docs that we should tokenize everything in ```prepare_data()```, but my dataset doesn't fit into memory, so I tokenize in the ```dataset.getitem()``` call, such that this happens upon each batch call. \r\n\r\nI tried this out but the tokenizer instance (and location) changes between the ```lightningmodule.prepare_data()``` and the ```dataset.getitem()``` calls. The tokenizer instance in ```lightningmodule.prepare_data()``` is perfect, but the latter instance is just an empty copy with empty attributes - which leads to the error \"padding was requested but your tokenizer has no padding token\" - despite the tokenizer being initialized with a padding token in ```lightningmodule.prepare_data()```.\r\n\r\nAny clues?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2542/comments",
    "author": "chrisdoyleIE",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-07T17:46:17Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "chrisdoyleIE",
        "created_at": "2020-07-08T10:49:44Z",
        "body": "Answer:\r\n\r\nUpdate HuggingFace. They had an issue with tokenizers not unpickling properly. Once HF is updated, the above problem goes away. \r\n\r\nFrom a lightning perspective, best to use ```lightningmodule.setup()``` in a distributed setting if assigning any ```self.attribute = something```."
      }
    ]
  },
  {
    "number": 2520,
    "title": "How to prepare list of files for dataloader, avoiding the duplicated work?",
    "created_at": "2020-07-06T03:08:13Z",
    "closed_at": "2020-10-29T12:43:04Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2520",
    "body": "I do \r\n\r\n```\r\nself.image_paths = sorted(Path(self.hparams[\"image_path\"]).rglob(\"*.jpg\"))\r\n```\r\nin `setup`\r\n\r\nand use `self.image_paths` to initialize the data loader in `train_dataloader`\r\n\r\nI have 10m+ files and `rglob` takes some time.\r\n\r\nMy model is trained on 4 GPUs and as I understand I do `rglob` 4 times.\r\n\r\nWhat is the best way to do it, so that it is done once and not 4 times?\r\n\r\n```\r\n__init__\r\nsetup ?\r\ntrain_dataloader\r\n```\r\nsomewhere else?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2520/comments",
    "author": "ternaus",
    "comments": [
      {
        "user": "vr140",
        "created_at": "2020-07-19T16:44:52Z",
        "body": "Would the prepare_data function work here?"
      },
      {
        "user": "ternaus",
        "created_at": "2020-07-19T16:53:08Z",
        "body": "If I use\r\n\r\n```\r\ndef prepare_data():\r\n   self.file_names = find_file_names()\r\n```\r\n\r\n`self.file_names` will not be accessible from the \r\n\r\n```\r\ndef train_dataloader\r\n```\r\n\r\n"
      },
      {
        "user": "vr140",
        "created_at": "2020-07-19T16:59:55Z",
        "body": "Even if you initialize `self.file_names = None` in your __init__ method?"
      },
      {
        "user": "nateraw",
        "created_at": "2020-09-18T06:45:53Z",
        "body": "Try something like this, maybe?\r\n\r\n```python\r\nfrom pathlib import Path\r\n\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass FileDatamodule(pl.LightningDataModule):\r\n\r\n    def __init__(self, file_reference_file='file_names.txt'):\r\n        self.file_reference_file = Path(file_reference_file)\r\n\r\n    def prepare_data(self):\r\n        if not self.file_reference_file.exists():\r\n            # do your rglob stuff + find the files\r\n            # save your file to self.file_reference_file\r\n\r\n    def setup(self, stage):\r\n        self.file_names = self.file_reference_file.open().readlines()\r\n        # do whatever you need w/ file names..."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T09:24:31Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2516,
    "title": "Does \"log_save_interval\" control training log?",
    "created_at": "2020-07-05T20:46:23Z",
    "closed_at": "2020-09-03T23:09:24Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2516",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### Question \r\n\r\nDoes \"log_save_interval\" control training log? If not, what controls the log of training? I changed this tag, but training log (such as loss) will be written to disk at every step no matter how I changed it. \r\n\r\n#### Code (adapted from sample)\r\n\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n```\r\n\"\"\"\r\nRuns a model on a single node across multiple gpus.\r\n\"\"\"\r\nimport os\r\nfrom argparse import ArgumentParser\r\n\r\nfrom pytorch_lightning import Trainer, seed_everything\r\nfrom pl_examples.models.lightning_template import LightningTemplateModel\r\n\r\nseed_everything(234)\r\n\r\n\r\ndef main(args):\r\n    \"\"\" Main training routine specific for this project. \"\"\"\r\n    # ------------------------\r\n    # 1 INIT LIGHTNING MODEL\r\n    # ------------------------\r\n    model = LightningTemplateModel(**vars(args))\r\n\r\n    # ------------------------\r\n    # 2 INIT TRAINER\r\n    # ------------------------\r\n    trainer = Trainer.from_argparse_args(args)\r\n\r\n    # ------------------------\r\n    # 3 START TRAINING\r\n    # ------------------------\r\n    trainer.fit(model)\r\n\r\n\r\ndef run_cli():\r\n    # ------------------------\r\n    # TRAINING ARGUMENTS\r\n    # ------------------------\r\n    # these are project-wide arguments\r\n    root_dir = os.path.dirname(os.path.realpath(__file__))\r\n    parent_parser = ArgumentParser(add_help=False)\r\n\r\n    # each LightningModule defines arguments relevant to it\r\n    parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)\r\n    parser = Trainer.add_argparse_args(parser)\r\n    parser.set_defaults(gpus=1,log_save_interval=1000,limit_train_batches=1,check_val_every_n_epoch=1000,max_epochs=10000, num_worker=0)\r\n    args = parser.parse_args()\r\n\r\n    # ---------------------\r\n    # RUN TRAINING\r\n    # ---------------------\r\n    main(args)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run_cli()\r\n```\r\n\r\n#### What have you tried?\r\nI tried to change ``row_log_interval`` and `log_save_interval`, but it seems that neither of them worked. Training log (e.g. loss) will be recorded at every step. \r\n#### What's your environment?\r\n\r\n - OS: Win 10\r\n - Packaging: Anaconda\r\n - Version: 0.8.4\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2516/comments",
    "author": "deng-cy",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-09-03T22:55:24Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2503,
    "title": "How do I checkpoint depending on the epoch number?",
    "created_at": "2020-07-04T22:48:14Z",
    "closed_at": "2020-10-28T21:27:22Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2503",
    "body": "Just to preface, Lightning has been a joy thus far. Everything is concise, well-automated where appropriate, and I converted a Keras model to PyTorch in just a few hours, with no previous PyTorch experience.\r\n\r\nTo the point, how might I set up a `ModelCheckpoint` to save conditioned on a given arbitrary condition, such as an epoch number? It seems to only be set up to automatically saved based on metrics. It also seems that manually saving is a bit difficult since only the `Trainer` has the proper function to save; given that `LightningModule` features the ability to load from checkpoint, it seems intuitively odd that these are separated between the two classes. Thus, it's difficult to consider the sheer volume of cyclical reference hacking it'd take to pass a trainer to the `LightningModule` to have it save in `on_epoch_end()`.\r\n\r\nSo yes, without loading the question by presenting my own code attempts, what is the canonical way to save conditioned on an epoch number, or an arbitrary condition?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2503/comments",
    "author": "nervecenter",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-04T22:49:07Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-05T11:21:12Z",
        "body": "If you want to save on last k epochs or something you can set `monitor=epoch` and `mode='max'` in `ModelCheckpoint` callback. Or do you want something like this?\r\n```\r\nif epoch > 4: # epoch number:\r\n    save_checkpoint()\r\n```"
      },
      {
        "user": "nervecenter",
        "created_at": "2020-07-05T13:56:59Z",
        "body": "I want to save on specific epoch numbers, regardless of metrics. For example, 5, 10, 15, 20, 25, etc.\r\n\r\n```\r\nif epoch % 5 == 0: # epoch number:\r\n    save_checkpoint()\r\n```"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-05T14:07:05Z",
        "body": "You can specify `save_top_k = -1` and `period = 5` in `ModelCheckpoint`."
      },
      {
        "user": "nervecenter",
        "created_at": "2020-07-05T20:52:06Z",
        "body": "Very good. Is there any way to add the ability to pass in a list of epoch numbers? Then one could do:\r\n```\r\nModelCheckpoint(\r\n    \"...\",\r\n    save_top_k=-1,\r\n    period=list(range(5, 51, 5))\r\n)\r\n```\r\n\r\nto save for epochs `[5, 10, 15, 20, 25, 30, 35, 40, 45, 50]`. Also, it might be more intuitive to have manual checkpointing passed as an argument of `mode`:\r\n\r\n```\r\nModelCheckpoint(\r\n    \"...\",\r\n    mode=\"manual\",\r\n    period=list(range(5, 51, 5))\r\n)\r\n```"
      },
      {
        "user": "nervecenter",
        "created_at": "2020-07-08T12:37:27Z",
        "body": "Yeah, `save_top_k = -1` and `period = 5` don't quite meet my needs. I need to specify a specific starting epoch for the period, or a specific list of epochs to save. The only epochs I can get with these two parameters are `[1, 6, 11, 16, 21, 26, 31, 36, 41, 46]`."
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-08T15:31:42Z",
        "body": "I don't think you can do that with ModelCheckpoint yet. But you can override some of the methods of ModelCheckpoint and make it work the way you want by tweaking it a little. Let me know if you need any help overriding these functions, I'll try."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-06T15:49:47Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T13:42:19Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2469,
    "title": "How do I access the metrics from a logger/trainer?",
    "created_at": "2020-07-02T15:11:12Z",
    "closed_at": "2020-09-10T18:52:11Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2469",
    "body": "Hello,  \r\nI wanted to ask, after training a model with a trainer object (and using a logger, such as a wandblogger), I want to access the validation loss of the last epoch. It seems simple but I wan't able to find how to do this in any documentation.  \r\nI tried searching the inner parameters of the Logger with python's dir function but got lost there.  \r\nAny help would be appreciated. Thank you very much!  \r\n  \r\nI am on a windows+conda environment if it matters.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2469/comments",
    "author": "erap129",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-02T15:12:13Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-02T19:34:40Z",
        "body": "I am using `trainer.callback_metrics` to access latest metrics."
      },
      {
        "user": "erap129",
        "created_at": "2020-07-03T12:19:02Z",
        "body": "Thanks! I see this only gives metrics for the last epoch. Do you know how to access metrics for all epochs?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-03T17:52:46Z",
        "body": "I asked this on slack forums yesterday, got the response that with Wandb you can use `trainer.logger.experiment.summary`. I haven't checked it, can you try??"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-01T18:07:41Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2451,
    "title": "how to use custom dataset in pytorch-lightning module as I am encountering an error \"AttributeError: 'str' object has no attribute 'size'\"",
    "created_at": "2020-07-01T13:42:42Z",
    "closed_at": "2020-07-30T21:51:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2451",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n\r\n#### how to use custom dataset in pytorch-lightning module as I am encountering an error \"AttributeError: 'str' object has no attribute 'size'\"?\r\n\r\n#### Code\r\n\r\n\r\nclass CustomDataset(Dataset):\r\n       def read_data_set(self):\r\n       all_img_files = []\r\n       all_labels = []\r\n\r\n    class_names = os.walk(self.data_set_path).__next__()[1]\r\n    \r\n    for index, class_name in enumerate(class_names):\r\n        label = index\r\n        img_dir = os.path.join(self.data_set_path, class_name)\r\n        img_files = os.walk(img_dir).__next__()[2]\r\n        \r\n        \r\n        for img_file in img_files:\r\n            img_file = os.path.join(img_dir, img_file)\r\n            img = Image.open(img_file)\r\n            if img is not None:\r\n                all_img_files.append(img_file)\r\n                all_labels.append(label)\r\n                \r\n    return all_img_files, all_labels, len(all_img_files), len(class_names)\r\n\r\ndef __init__(self, data_set_path, transforms=None):\r\n    self.data_set_path = data_set_path\r\n    self.image_files_path, self.labels, self.length, self.num_classes = self.read_data_set()\r\n    self.transforms = transforms\r\n    \r\ndef __getitem__(self, index):\r\n    image = Image.open(self.image_files_path[index])\r\n    image = image.convert(\"RGB\")\r\n    \r\n    if self.transforms is not None:\r\n        image = self.transforms(image)\r\n        \r\n    return {'image': image, 'label': self.labels[index]}\r\n\r\ndef __len__(self):\r\n    return self.length\r\nclass MNISTClassifier(LightningModule):\r\ndef init(self):\r\nsuper(MNISTClassifier, self).init()\r\nself.layer_1 = torch.nn.Linear(28*28, 128)\r\nself.layer_2 = torch.nn.Linear(128, 256)\r\nself.layer_3 = torch.nn.Linear(256, 10)\r\n\r\ndef forward(self, x):\r\n    batch_size, channels, width, height = x.size()\r\n    \r\n    #(b_s, 1, 28, 28)\r\n    x = x.view(batch_size, -1)\r\n    \r\n    #layer1\r\n    x = self.layer_1(x)\r\n    x = torch.relu(x)\r\n    \r\n    #layer2\r\n    x = self.layer_2(x)\r\n    x = torch.relu(x)\r\n    \r\n    #layer3\r\n    x = self.layer_3()\r\n    \r\n    #probability distribution over labels\r\n    x = torch.log_softmax(x, dim = 1)\r\n    \r\n    return x\r\n\r\n\r\ndef cross_entropy_loss(self, logits, labels):\r\n    return F.null_loss(logits, labels)\r\n\r\n\r\ndef training_step(self, train_batch, batch_idx):\r\n    x, y = train_batch\r\n    logits = self.forward(x.size())\r\n    loss = self.cross_entropy_loss(logits, y)\r\n    \r\n    logs = {'train_loss': loss}\r\n    return {'loss': loss, 'log':logs}\r\n\r\n\r\n\r\n\r\ndef test_step(self, test_batch, batch_idx):\r\n    x, y = test_batch\r\n    logits = self.forward(x.size())\r\n    loss = self.cross_entropy_loss(logits, y)\r\n    \r\n    logs = {'test_loss:': loss}\r\n    return {'val_loss': loss, 'log':logs}\r\n\r\n \r\ndef train_dataloader(self):\r\n    dataset = CustomDataset(data_set_path='./files/MNIST/mnist_png/mnist_png/training/', transforms=transforms.ToTensor())\r\n    train_loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)\r\n    return train_loader\r\n\r\n\r\n\r\ndef test_dataloader(self):\r\n    dataset = CustomDataset(data_set_path='files/MNIST/mnist_png/mnist_png/testing/', transforms=transforms.ToTensor())\r\n    test_loader = DataLoader(dataset, batch_size=32, num_workers=4)\r\n    return test_loader\r\n                       \r\ndef configure_optimizers(self):\r\n    optimizer = torch.optim.Adam(self.parameters(),lr=1e-3)\r\n    return optimizer\r\nmodel = MNISTClassifier()\r\ntrainer = pl.Trainer(gpus=1, max_epochs=1)\r\ntrainer.fit(model)\r\n\r\n\r\n\r\n#### What have you tried?\r\nI am getting an error as follows:\r\nAttributeError Traceback (most recent call last)\r\nin ()\r\n8 model = MNISTClassifier()\r\n9 trainer = pl.Trainer(gpus=1, max_epochs=1)\r\n---> 10 trainer.fit(model)\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)\r\n763\r\n764 elif self.single_gpu:\r\n--> 765 self.single_gpu_train(model)\r\n766\r\n767 elif self.use_tpu: # pragma: no-cover\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)\r\n490 self.optimizers = optimizers\r\n491\r\n--> 492 self.run_pretrain_routine(model)\r\n493\r\n494 def tpu_train(self, tpu_core_idx, model):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)\r\n911\r\n912 # CORE TRAINING LOOP\r\n--> 913 self.train()\r\n914\r\n915 def test(self, model: Optional[LightningModule] = None, test_dataloaders: Optional[DataLoader] = None):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in train(self)\r\n345 # RUN TNG EPOCH\r\n346 # -----------------\r\n--> 347 self.run_training_epoch()\r\n348\r\n349 # update LR schedulers\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n417 # RUN TRAIN STEP\r\n418 # ---------------\r\n--> 419 _outputs = self.run_training_batch(batch, batch_idx)\r\n420 batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs\r\n421\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)\r\n594\r\n595 # calculate loss\r\n--> 596 loss, batch_output = optimizer_closure()\r\n597\r\n598 # check if loss or model weights are nan\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure()\r\n558 opt_idx, self.hiddens)\r\n559 else:\r\n--> 560 output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)\r\n561\r\n562 # format and reduce outputs accordingly\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in training_forward(self, batch, batch_idx, opt_idx, hiddens)\r\n724 batch = self.transfer_batch_to_gpu(batch, gpu_id)\r\n725 args[0] = batch\r\n--> 726 output = self.model.training_step(*args)\r\n727\r\n728 # TPU support\r\n\r\nin training_step(self, train_batch, batch_idx)\r\n36 def training_step(self, train_batch, batch_idx):\r\n37 x, y = train_batch\r\n---> 38 logits = self.forward(x.size())\r\n39 loss = self.cross_entropy_loss(logits, y)\r\n40\r\n\r\nAttributeError: 'str' object has no attribute 'size'   \r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu 18.04\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2451/comments",
    "author": "drone-vision",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-01T13:43:34Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-01T17:09:05Z",
        "body": "@tanubapun You are sending `x.size()` in `self.forward(x.size())`. It should be `self.forward(x)`"
      },
      {
        "user": "drone-vision",
        "created_at": "2020-07-01T18:21:36Z",
        "body": "Thanks @rohitgr7 for your response. I have changed the code to  self.forward(x) still got the same error.\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-bb768fd558f0> in <module>()\r\n      8 model = MNISTClassifier()\r\n      9 trainer = pl.Trainer(gpus=1, max_epochs=1)\r\n---> 10 trainer.fit(model)\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)\r\n    763 \r\n    764         elif self.single_gpu:\r\n--> 765             self.single_gpu_train(model)\r\n    766 \r\n    767         elif self.use_tpu:  # pragma: no-cover\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)\r\n    490             self.optimizers = optimizers\r\n    491 \r\n--> 492         self.run_pretrain_routine(model)\r\n    493 \r\n    494     def tpu_train(self, tpu_core_idx, model):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)\r\n    911 \r\n    912         # CORE TRAINING LOOP\r\n--> 913         self.train()\r\n    914 \r\n    915     def test(self, model: Optional[LightningModule] = None, test_dataloaders: Optional[DataLoader] = None):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in train(self)\r\n    345                 # RUN TNG EPOCH\r\n    346                 # -----------------\r\n--> 347                 self.run_training_epoch()\r\n    348 \r\n    349                 # update LR schedulers\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n    417             # RUN TRAIN STEP\r\n    418             # ---------------\r\n--> 419             _outputs = self.run_training_batch(batch, batch_idx)\r\n    420             batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs\r\n    421 \r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)\r\n    594 \r\n    595                 # calculate loss\r\n--> 596                 loss, batch_output = optimizer_closure()\r\n    597 \r\n    598                 # check if loss or model weights are nan\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure()\r\n    558                                                                     opt_idx, self.hiddens)\r\n    559                         else:\r\n--> 560                             output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)\r\n    561 \r\n    562                         # format and reduce outputs accordingly\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in training_forward(self, batch, batch_idx, opt_idx, hiddens)\r\n    724             batch = self.transfer_batch_to_gpu(batch, gpu_id)\r\n    725             args[0] = batch\r\n--> 726             output = self.model.training_step(*args)\r\n    727 \r\n    728         # TPU support\r\n\r\n<ipython-input-7-375f2fcd6b89> in training_step(self, train_batch, batch_idx)\r\n     36     def training_step(self, train_batch, batch_idx):\r\n     37         x, y = train_batch\r\n---> 38         logits = self.forward(x)\r\n     39         loss = self.cross_entropy_loss(logits, y)\r\n     40 \r\n\r\n<ipython-input-7-375f2fcd6b89> in forward(self, x)\r\n      8 \r\n      9     def forward(self, x):\r\n---> 10         batch_size, channels, width, height = x.size()\r\n     11 \r\n     12         #(b_s, 1, 28, 28)\r\n\r\nAttributeError: 'str' object has no attribute 'size'"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-01T18:28:19Z",
        "body": "@tanubapun Can you share the whole code in a notebook or colab or just LightningModule you created again after you made changes?"
      },
      {
        "user": "drone-vision",
        "created_at": "2020-07-01T18:50:10Z",
        "body": "Sure @rohitgr7 \r\nthis is the lightning module I am using\r\n\r\nclass MNISTClassifier(LightningModule):\r\n    def __init__(self):\r\n        super(MNISTClassifier, self).__init__()\r\n        self.layer_1 = torch.nn.Linear(28*28, 128)\r\n        self.layer_2 = torch.nn.Linear(128, 256)\r\n        self.layer_3 = torch.nn.Linear(256, 10)\r\n        \r\n        \r\n    def forward(self, x):\r\n        batch_size, channels, width, height = x.size()\r\n        \r\n        #(b_s, 1, 28, 28)\r\n        x = x.view(batch_size, -1)\r\n        \r\n        #layer1\r\n        x = self.layer_1(x)\r\n        x = torch.relu(x)\r\n        \r\n        #layer2\r\n        x = self.layer_2(x)\r\n        x = torch.relu(x)\r\n        \r\n        #layer3\r\n        x = self.layer_3()\r\n        \r\n        #probability distribution over labels\r\n        x = torch.log_softmax(x, dim = 1)\r\n        \r\n        return x\r\n    \r\n    \r\n    def cross_entropy_loss(self, logits, labels):\r\n        return F.null_loss(logits, labels)\r\n    \r\n    \r\n    def training_step(self, train_batch, batch_idx):\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        \r\n        logs = {'train_loss': loss}\r\n        return {'loss': loss, 'log':logs}\r\n    \r\n    \r\n    \r\n    def test_step(self, test_batch, batch_idx):\r\n        x, y = test_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        \r\n        logs = {'test_loss:': loss}\r\n        return {'val_loss': loss, 'log':logs}\r\n    \r\n\r\n    def train_dataloader(self):\r\n        dataset = CustomDataset(data_set_path='./files/MNIST/mnist_png/mnist_png/training/', transforms=transforms.ToTensor())\r\n        train_loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)\r\n        return train_loader\r\n\r\n    \r\n    def test_dataloader(self):\r\n        dataset = CustomDataset(data_set_path='files/MNIST/mnist_png/mnist_png/testing/', transforms=transforms.ToTensor())\r\n        test_loader = DataLoader(dataset, batch_size=32, num_workers=4)\r\n        return test_loader\r\n                           \r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(),lr=1e-3)\r\n        return optimizer\r\n        "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-01T19:04:48Z",
        "body": "@tanubapun In `__getitem__` of your `CustomDataset` you are returning `{'image': image, 'label': self.labels[index]}` but you are using `x, y = train_batch`. Your `train_batch` is still a `dict` here just that pytorch `collate_fn` create a batch in the values of this dict. Either return `image, self.labels[index]` or use `x, y= train_batch['image'], train_batch['label']`. Also change in `validation_step` and `test_step` accordingly.\r\n"
      },
      {
        "user": "drone-vision",
        "created_at": "2020-07-01T19:17:55Z",
        "body": "Thank you very much @rohitgr7, it wipes out the concerned error for the code... "
      }
    ]
  },
  {
    "number": 2404,
    "title": "Will load_from_checkpoint load Huggingface models as well? ",
    "created_at": "2020-06-28T20:12:31Z",
    "closed_at": "2020-07-01T09:13:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2404",
    "body": "#### What is your question?\r\n\r\nJust wanted to know will using the `load_from_checkpoint` for a `LightningModule` load the state_dict for the **HuggingFace** models as well?\r\n\r\nEg: for the given example in the docs, will state_dict be loaded for `BertModel.from_pretrained` thing as well? \r\nIdeally, `load_from_checkpoint` should load state_dict for Bert as well like `BertModel.from_pretrained(same_checkpoint)` would do.\r\n\r\n#### Code\r\n```\r\nclass BertMNLIFinetuner(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)\r\n        self.W = nn.Linear(bert.config.hidden_size, 3)\r\n        self.num_classes = 3\r\n\r\n\r\n    def forward(self, input_ids, attention_mask, token_type_ids):\r\n\r\n        h, _, attn = self.bert(input_ids=input_ids,\r\n                         attention_mask=attention_mask,\r\n                         token_type_ids=token_type_ids)\r\n\r\n        h_cls = h[:, 0]\r\n        logits = self.W(h_cls)\r\n        return logits, attn\r\n``` ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2404/comments",
    "author": "vibhavagarwal5",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-28T20:13:23Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "dscarmo",
        "created_at": "2020-06-28T21:26:32Z",
        "body": "When working with the Hugging Face library i just store the model string (in this case best-base-cased) as an hparam."
      },
      {
        "user": "vibhavagarwal5",
        "created_at": "2020-06-28T22:00:54Z",
        "body": "Sure, that is fine. I think you misunderstood my question. Will the state_dict from the checkpoint (using `load_from_checkpoint`) load the bert model as well or not? Or will I have to load it explicitly using `model.from_pretrained(checkpoint_path)` ?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-28T22:17:10Z",
        "body": "I don't know this Bert stuff , but if it helps:\r\n`Model.load_from_checkpoint()` will init your model with the args and kwargs from the checkpoint and then call `model.load_state_dict` to load the model weights as you would do in pure PyTorch.\r\nSo, if your` self.bert` is a `nn.Module` , then that will have the parameters loaded as well. "
      },
      {
        "user": "dscarmo",
        "created_at": "2020-06-28T22:20:43Z",
        "body": "Oh now i understand. The updated weights of your whole trained lightning module (including bert since it is a nn.Module) will be loaded. Now I am just wondering if init's `from_pretrained`  will overwrite your trained weights? \r\n\r\nI hope PL will only load the trained weights after init runs."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-28T22:33:56Z",
        "body": "yes, so the from_pretrained will run first and load your pretrained weights (since it's in the init). and then the load_state dict will overwrite them again with the weights from the checkpoint. \r\n\r\nI encourage you to make a sanity check and not blindly believe me :), for example, you could load your checkpoint manually and replace all weights with ones and then save it again. Then load the checkpoint again using pytorch lightning and print the weights of the loaded model."
      },
      {
        "user": "vibhavagarwal5",
        "created_at": "2020-06-29T04:52:30Z",
        "body": "My guess is the same that after from_pretrained weights are loaded, checkpoints weights override them.  Which makes sense. "
      },
      {
        "user": "vibhavagarwal5",
        "created_at": "2020-06-29T04:56:17Z",
        "body": "Another doubt, do the checkpoints save all sorts of callbacks as well? Coz I don't see it in the dict keys when I do torch.load(chckpoint). "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-29T05:04:13Z",
        "body": "we do that for early stopping and model checkpoint (see v0.8.2), but not for your custom callbacks. We recently discussed how to do that, here is an open discussion #2401 . "
      },
      {
        "user": "vibhavagarwal5",
        "created_at": "2020-06-29T05:17:51Z",
        "body": "v.0.8.2 is not released yet right? "
      }
    ]
  },
  {
    "number": 2397,
    "title": "Are auto-resubmited slurm jobs completely the same as if it weren't interrupted?",
    "created_at": "2020-06-28T06:17:51Z",
    "closed_at": "2020-06-29T01:49:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2397",
    "body": "In other words, does `trainer.hpc_save` persist _all_ necessary information (e.g. how many batches are processed in the current epoch) such that, when restored, it behaves completely the same (random states aside) as if it weren't interrupted?\r\n\r\nAnd a related question: if my cluster handles auto-resubmission, which means I only need the `hpc_save` part of `pytorch-lightning`'s `SIGUSR1` handling logic, is there a way I can disable to resubmission logic?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2397/comments",
    "author": "ZhaofengWu",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-29T01:49:28Z",
        "body": "1. yes it's all handled. However the epoch will start again (but everything else is saved).\r\n2. To disable it don't pass SIGUSR1 to your slurm script"
      },
      {
        "user": "ZhaofengWu",
        "created_at": "2020-06-29T02:02:09Z",
        "body": "If the epoch starts over while the model state doesn't get reset to before the epoch, wouldn't this be equivalent to training with more data for that specific epoch? That might be undesirable when we want very rigorous comparisons across different models/settings/etc. Is there a way to achieve this? I guess a simple way is to save a checkpoint at the end of every epoch and use that, but that creates a lot of unnecessary checkpoints, as mentioned in #2141."
      }
    ]
  },
  {
    "number": 2370,
    "title": "Access the logging directory through LightningModule or Trainer",
    "created_at": "2020-06-26T09:25:06Z",
    "closed_at": "2020-06-27T12:26:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2370",
    "body": "Is there a way to access the current logging directory (e.g., lightning_logs/version_x)? I've searched the documentation and the source code but haven't found a solution yet.\r\n\r\nI want to save some intermediate raw tensors to that directory.\r\n\r\nThanks,\r\nDavid",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2370/comments",
    "author": "DavidRuhe",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-26T09:25:48Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-26T20:35:57Z",
        "body": "For tensorboard I think you can use `self.logger.log_dir`, not sure about others. I think this property should be present for all the available loggers if possible."
      },
      {
        "user": "DavidRuhe",
        "created_at": "2020-06-27T12:26:34Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 2369,
    "title": "monitor the data loading time",
    "created_at": "2020-06-26T05:22:36Z",
    "closed_at": "2020-06-29T05:24:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2369",
    "body": "I am running the imagenet example and found that there is no \"data time\" as in the original pytorch example.\r\nHow to monitor the data loading time?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2369/comments",
    "author": "ruotianluo",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-26T05:23:14Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "ruotianluo",
        "created_at": "2020-06-26T05:41:58Z",
        "body": "Also, it's weird that one epoch becomes 5201; it should be 5005."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-26T07:23:34Z",
        "body": "I think the profiler can do that, if you turn it on in the Trainer. \r\n\r\n> Also, it's weird that one epoch becomes 5201; it should be 5005.\r\n\r\nAre you referring to the progress bar? I think it is because it includes the validation set. \r\n"
      },
      {
        "user": "ruotianluo",
        "created_at": "2020-06-26T13:35:08Z",
        "body": "My understanding is the profiler only output the result after training. \r\n\r\nYes. The progress bar,  thanks. "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-26T14:04:48Z",
        "body": "In the profiler you get something like this:\r\n```\r\nProfiler Report\r\n\r\nAction              \t|  Mean duration (s)\t|  Total time (s) \r\n-----------------------------------------------------------------\r\non_train_start      \t|  0.0            \t|  0.0            \r\non_epoch_start      \t|  0.0            \t|  0.0            \r\nget_train_batch     \t|  0.5312         \t|  2.656          \r\non_batch_start      \t|  0.0            \t|  0.0            \r\nmodel_forward       \t|  0.2186         \t|  1.093          \r\nmodel_backward      \t|  0.2128         \t|  1.064          \r\non_after_backward   \t|  0.0            \t|  0.0            \r\noptimizer_step      \t|  0.2124         \t|  1.062          \r\non_batch_end        \t|  0.0032         \t|  0.016          \r\non_epoch_end        \t|  0.0            \t|  0.0            \r\non_train_end        \t|  0.0            \t|  0.0            \r\n\r\n```\r\n\"get_train_batch\" is the loading time of the training batch. If you don't want to wait for these stats for the end of training, just run one epoch with `max_epochs=1` or a few steps with `max_steps=10` by setting these flags in the Trainer.  "
      },
      {
        "user": "ruotianluo",
        "created_at": "2020-07-08T04:38:43Z",
        "body": "Btw, here is what I am doing right now:\r\n\r\n```\r\n\r\n    def training_step(self, data, batch_idx):\r\n        ...\r\n        data_time = self.trainer.profiler.recorded_durations[\"get_train_batch\"][-1]\r\n        data_time = torch.tensor(data_time)\r\n\r\n```"
      },
      {
        "user": "deeptimhe",
        "created_at": "2025-02-09T18:08:11Z",
        "body": "Hi, do you have update to monitor data loading time for lightning 2.x ? There is no get_train_batch in profiler."
      }
    ]
  },
  {
    "number": 2368,
    "title": "How To: Specify input_shape when it's not known in advance",
    "created_at": "2020-06-26T03:32:49Z",
    "closed_at": "2020-09-03T22:55:26Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2368",
    "body": "\r\nAs far as I can see all of the examples assume that the input shape is known in advance, i.e. MNIST images which have fixed C,H and W. But I'm working with multi-variate time series data, superficially investigating transforms which alter the number of input series.\r\n\r\nIn the example below, `transforms` is passed to the constructor and used to enhance the dataset loaded in `prepare_data`. However these transforms may alter the shape of the data, but (unless I'm mistaken) the input and output dimensions has to be known before constructing the `LightningModule`. \r\n\r\nIs there a canonical way of handling this pattern in Lightning?\r\n\r\n```\r\nclass TCN(pl.LightningModule):\r\n    def __init__(self,  input_shape, output_shape, transform, hparams):\r\n        super().__init__()\r\n        self.hparams = hparams\r\n        self.transform = transform\r\n        self.tcn = TemporalConvNet(input_shape, [self.hparams.filters] * self.hparams.layers, self.hparams.kernel_size, dropout=self.hparams.dropout)\r\n        self.decoder = nn.Linear(self.hparams.filters, output_shape)\r\n\r\n    def prepare_data(self):\r\n         self.train_dataset, self.val_dataset = load_dataset()\r\n         self.train_dataset, self.val_dataset  = self.transform(self.train_dataset), self.transform(self.val_dataset)\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2368/comments",
    "author": "david-waterworth",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-06-26T20:26:10Z",
        "body": "You can create the `dataset` and `dataloader` outside the LightningModule and pass the input shape in the `LightningModule` contructor to build the model using that and then simply use `.fit(model, train_dataloader=..., val_dataloaders=...)`."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-25T20:38:53Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2320,
    "title": "Partially missing training_step outputs in training_epoch_end",
    "created_at": "2020-06-22T21:12:36Z",
    "closed_at": "2020-06-23T15:17:11Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2320",
    "body": "**Learning Model:** the model consists of two branches: **teacher and student**. Two losses have been used to train two branches of the learning model using two optimizers.\r\n\r\n**ISSUE:** In the **`training_step`**, I produced two set of metrics outputs for teacher (_optimizer_idx=0_) and student (_optimizer_idx=1_) branches (teacher: `{loss,log:{acc, f1}}` and student: `{loss,log:{acc,f1,precision}}` ). But in the **`training_epoch_end`**, I only got the combined outputs for only student outputs (_optimizer_idx=1_). All the teacher outputs (_optimizer_idx=0_) are missing.\r\n\r\nI also see the training loop of PL and didn't observe any issue when the training loop tries to combine the **`training_step`** outputs. I am not sure what am I missing? \r\n\r\n - Version: 0.8.2.dev0\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2320/comments",
    "author": "mmiakashs",
    "comments": [
      {
        "user": "mmiakashs",
        "created_at": "2020-06-22T22:57:06Z",
        "body": "I dig the PL training_loop, may be I found a possible issue. In the `run_training_batch` method in `training_loop.py`, see the following two lines:\r\n```\r\nLine 639: loss, batch_output = optimizer_closure()\r\nand \r\nLine 691: return 0, grad_norm_dic, all_log_metrics, batch_output\r\n```\r\nIt seems to me that the `batch_output` of the last split_batch and last opt_idx iteration is just returned. Shouldn't we collect all the split batch and optimizers iteration batch_output \r\njust like the `all_log_metrics`?\r\n\r\nMoreover, in the run_training_epoch method, it seems to me that only the last iteration batch_output is passed to the `training_epoch_end`, see the following lines in the training_loop.py\r\n```\r\nLine 459: batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs\r\nLine 464: outputs.append(batch_output)\r\nLine 526: epoch_output = model.training_epoch_end(outputs)\r\n```\r\n\r\n@williamFalcon @Borda could you please give a look into it?"
      },
      {
        "user": "mmiakashs",
        "created_at": "2020-06-22T23:38:44Z",
        "body": "> I dig the PL training_loop, may be I found a possible issue. In the `run_training_batch` method in `training_loop.py`, see the following two lines:\r\n> \r\n> ```\r\n> Line 639: loss, batch_output = optimizer_closure()\r\n> and \r\n> Line 691: return 0, grad_norm_dic, all_log_metrics, batch_output\r\n> ```\r\n> \r\n> It seems to me that the `batch_output` of the last split_batch and last opt_idx iteration is just returned. Shouldn't we collect all the split batch and optimizers iteration batch_output\r\n> just like the `all_log_metrics`?\r\n> \r\n> Moreover, in the run_training_epoch method, it seems to me that only the last iteration batch_output is passed to the `training_epoch_end`, see the following lines in the training_loop.py\r\n> \r\n> ```\r\n> Line 459: batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs\r\n> Line 464: outputs.append(batch_output)\r\n> Line 526: epoch_output = model.training_epoch_end(outputs)\r\n> ```\r\n> \r\n> @williamFalcon @Borda could you please give a look into it?\r\n\r\nI debug a bit I think the problem is with the multi optimizer only the last iteration (split batch and optimizer iteration) output is passed. If you can confirm this problem I can resolve and request a pull request. Thanks"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-08-09T10:11:20Z",
        "body": "@mmiakashs can you check that this works for you now on master?"
      },
      {
        "user": "mmiakashs",
        "created_at": "2020-08-27T16:50:00Z",
        "body": "@williamFalcon Thanks for the update. Sorry I missed that notification. I will check this after my deadline next week :) \r\nJust a followup question. Do we need to turn on any flag for this or by default it merges and gives the combined results in the epoch_end results object? "
      }
    ]
  },
  {
    "number": 2312,
    "title": "GPU out of memory error after few batches ",
    "created_at": "2020-06-22T00:57:21Z",
    "closed_at": "2020-09-01T05:18:53Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2312",
    "body": "I am trying to train a complex model involving multiple convolutions. Since my own implementation was very slow (taking ~2 hours for an epoch which increased further after a few epochs), I tried changing my code to incorporate lightning module. With lightning, I'm getting a CUDA OOM after 1/3rd of the total no of batches being processed. Since the library handles .to and device calls internally, I'm not sure what can be the reason\r\n\r\n#### Code sample\r\nclass Network(pl.LightningModule):\r\n\r\n    def __init__(self, in_ch=3, out_ch=1):\r\n        super(Network, self).__init__()\r\n\r\n        n1 = 64\r\n        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\r\n\r\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\r\n        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\r\n\r\n        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\r\n        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\r\n        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\r\n        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\r\n        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\r\n\r\n        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0])\r\n        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\r\n        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\r\n        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\r\n\r\n        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1], filters[0], filters[0])\r\n        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2], filters[1], filters[1])\r\n        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])\r\n\r\n        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1], filters[0], filters[0])\r\n        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1])\r\n\r\n        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])\r\n\r\n        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\r\n\r\n\r\n    def forward(self, x):\r\n        #pdb.set_trace()  \r\n        x0_0 = self.conv0_0(x)\r\n        x1_0 = self.conv1_0(self.pool(x0_0))\r\n        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(x1_0)], 1))\r\n\r\n        x2_0 = self.conv2_0(self.pool(x1_0))\r\n        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\r\n        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(x1_1)], 1))\r\n\r\n        x3_0 = self.conv3_0(self.pool(x2_0))\r\n        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\r\n        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1)], 1))\r\n        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(x1_2)], 1))\r\n\r\n        ##adding for padding issues\r\n        x4_0 = self.conv4_0(self.pool(x3_0))\r\n        x4_0_up = self.Up(x4_0)\r\n        \r\n        diffY = x3_0.size()[2] - x4_0_up.size()[2]\r\n        diffX = x3_0.size()[3] - x4_0_up.size()[3]\r\n        x4_0_up = F.pad(x4_0_up, (diffX // 2, diffX - diffX//2,\r\n                        diffY // 2, diffY - diffY//2))\r\n\r\n\r\n        x3_1 = self.conv3_1(torch.cat([x3_0, x4_0_up], 1))\r\n        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))\r\n        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\r\n        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(x1_3)], 1))\r\n       \r\n        output = self.final(x0_4)\r\n        output= F.sigmoid(output).squeeze(1)\r\n        return output\r\n\r\n\r\nval_paths = [line.rstrip('\\n') for line in open(config.val_data_path)]\r\ntrain_loader = get_loader(config.train_data_path, 'Train')\r\nval_loader = get_loader(val_paths, 'Val')\r\n#model = torch.nn.DataParallel(Network())\r\nmodel = Network()\r\n\r\ncheckpoint_callback = pl.callbacks.ModelCheckpoint( monitor=\"val_loss\",mode=\"min\",save_last=True, save_top_k=-1,verbose=False,) \r\ntrainer = pl.Trainer(gpus=[6,7], profiler=True, distributed_backend='ddp', num_sanity_val_steps=0,log_save_interval=1, checkpoint_callback=checkpoint_callback)\r\ntrainer.fit(model, train_dataloader=train_loader, val_dataloaders= val_loader) \r\n\r\n### Expected behavior\r\nI'm using the same hardware so unable to understand the issue\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): linux\r\n - How you installed PyTorch (`conda`, `pip`, source):conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: TITAN RTX GPUs\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2312/comments",
    "author": "PC09",
    "comments": [
      {
        "user": "Geeks-Sid",
        "created_at": "2020-06-23T10:13:59Z",
        "body": "Are you sure you are not doing a .item call?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-24T03:58:21Z",
        "body": "can you post a colab example? \r\nbut you probably need to make your batch size smaller... we have tests to ensure we don't have memory leaks... and that we get the exact same results as PyTorch"
      },
      {
        "user": "felixkreuk",
        "created_at": "2020-09-15T12:55:22Z",
        "body": "> Are you sure you are not doing a .item call?\r\n\r\n@Geeks-Sid Is calling `.item()` a bad practice? Can it increase memory use?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-09-15T13:05:41Z",
        "body": "it slows things down..."
      }
    ]
  },
  {
    "number": 2310,
    "title": "how to train a network that doesn't require any training data",
    "created_at": "2020-06-21T22:46:58Z",
    "closed_at": "2020-06-23T18:46:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2310",
    "body": "The Wake-Sleep algorithm doesn't require any data during the sleep phase (effectively it generates it's own data). pytorch-lightning, however, appears to require a `train_dataloader()` method.\r\n\r\nThe only way I have to make pytorch-lightning run at all (for this admitted unusual case) is to specify some dummy dataset in `train_dataloader`, and then to ignore the data that gets passed to `training_step`. But I don't like that cycles are spent iterating through irrelevant data then. Is there a more elegant way?\r\n\r\nI considered defining my own custom `DataLoader` that returns the simulated data that the sleep phase uses, but this started seeming like even more of a hack than the previous solution. After all, my \"dataloader\" doesn't load any data; it effectively generates new data every \"epoch\". It's seems unnatural to split the sleep phase updates in this way.\r\n\r\nIs there a more straightforward way in lightning to train a network that doesn't require any data? Thanks!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2310/comments",
    "author": "jeff-regier",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-21T22:47:54Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-23T17:00:36Z",
        "body": "Why is it not possible for you to write a Dataset class that generates the data? You mention you already have code for generating the data, so why not wrap it into a Dataset class?"
      },
      {
        "user": "jeff-regier",
        "created_at": "2020-06-23T18:46:56Z",
        "body": "I had been concerned that the Dataset class would shuffle the data unnecessarily, thus slowing down training. But an `IterableDataset` seems to work: the `__iter__` method is overridden to return an iterator that yields a whole batch at once. Maybe this is the best way."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-23T18:53:43Z",
        "body": "No, that is simply not true. The Dataloader is doing the batching and shuffling, not the dataset. Besides, shuffling does not slow training down. If your dataset has an undefined or infinite length, use the IterableDataset, otherwise use the regular Dataset class. Once you have that, just pass it to the Dataloader. All of this is regular PyTorch :)"
      }
    ]
  },
  {
    "number": 2308,
    "title": "How to make inference right",
    "created_at": "2020-06-21T19:30:23Z",
    "closed_at": "2020-06-24T22:31:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2308",
    "body": "Hello everyone. I'm new to pytorch-lightning, but already excited with this framework. It's very convenient to train my models using lightning. Now my usecase is: I have trained my model and want to do inference on my test data and get results (for example, in csv format). I'd like to do my inference pytorch-lightning-way. What is the best practice to do it?   \r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n#### What's your environment?\r\n\r\n - OS: [Linux]\r\n - Packaging [pip]\r\n - Version [0.8.1]\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2308/comments",
    "author": "Podidiving",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-21T19:30:58Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "versatran01",
        "created_at": "2020-06-24T16:10:14Z",
        "body": "My understanding is that you just load the module and call freeze() on it and use it as any nn.Module"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-24T19:50:09Z",
        "body": "You don't need to freeze(). Just run with `torch.no_grad()` or set `torch.set_grad_enabled(False)`."
      },
      {
        "user": "Podidiving",
        "created_at": "2020-06-24T20:01:59Z",
        "body": "I think, one of the coolest features of lightning is that your don’t need to specify your device (devices)\nIf you using LightningModule as plain nn.Module you should transfer your model and batches on devices manually, am I right? \nSo, I have my trained model. I’d like to make inference on test data. I can define test_step and aggregate results in on_test_epoch_end, but I cannot run Trainer without train stage. Can I get around this somehow?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-24T20:08:49Z",
        "body": "`test_step` and `test_epoch_end` are called with `trainer.test()`. AFAIK, these methods are used just to evaluate your test dataset and not return any logits. If you want to do just this evaluation, you don't have to do the transfer of model or batches. But I think if you want to get the logits from the model you need to do it manually just like a vanilla PyTorch model."
      },
      {
        "user": "Podidiving",
        "created_at": "2020-06-24T20:16:58Z",
        "body": "Ok, thanks, didn’t know about `test` method! I think, if you want to make some kind of submission data, you can return logits from `test_step` method in dict object, and then aggregate and save them in `test_epoch_end`  "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-24T20:20:41Z",
        "body": "yeah, can do that too. Nice thought. :v:  "
      }
    ]
  },
  {
    "number": 2280,
    "title": "Clarification for Lr Scheduler ReduceLROnPlateau in PL",
    "created_at": "2020-06-19T19:40:47Z",
    "closed_at": "2020-06-19T19:51:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2280",
    "body": "Maybe I missed it in PL documentation; however, when using the learning rate scheduler \" ReduceLROnPlateau\" appears to take an argument for which metric to monitor during training. \r\n\r\nDoes PL handle passing this argument? If so is there anyway for the user to specify which argument is passed to the scheduler?\r\n\r\nFrom Pytorch Docs, example shows scheduler.step takes val_loss ,as the argument for when to reduce learning rate for this particular scheduler.  \r\n\r\nExample\r\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\n>>> scheduler = ReduceLROnPlateau(optimizer, 'min')\r\n>>> for epoch in range(10):\r\n>>>     train(...)\r\n>>>     val_loss = validate(...)\r\n>>>     # Note that step should be called after validate()\r\n>>>     scheduler.step(val_loss)\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2280/comments",
    "author": "JonnyD1117",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-19T19:41:32Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "JonnyD1117",
        "created_at": "2020-06-19T19:51:12Z",
        "body": "After triple checking documentation, I found a NOTE that I missed initially, which address this issue. \r\n\r\nSorry to bother."
      }
    ]
  },
  {
    "number": 2265,
    "title": "Can we add examples of using metrics package in PyTorch Lightning workflow?",
    "created_at": "2020-06-19T07:20:04Z",
    "closed_at": "2020-06-25T06:09:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2265",
    "body": "Hi,\r\n\r\nI am really excited about the release of the metrics package. Can we add more examples of using it in the context of the lightning module?\r\n\r\nFor example, if I want to calculate the global accuracy/f1-score at the end of each epoch, is doing it this way correct? I feel it's wrong as averaging batch-f1 to get the global average can be misleading. When I tested it on a toy example and compared it to a global micro/macro/weighted f1-score, the f1-scores were not the same.\r\n\r\nThis was my initial attempt:\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    criterion = nn.CrossEntropyLoss()\r\n    x, y = batch\r\n    logits = self(x)\r\n    loss = criterion(logits, y)\r\n    batch_f1 = f1_score(logits.argmax(dim=-1), y)\r\n    return {'loss': loss, 'f1': batch_f1}\r\n```\r\nand then averaging at the end of each epoch\r\n```python\r\ndef training_epoch_end(self, outputs):\r\n    avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\r\n    f1 = torch.stack([x['f1'] for x in outputs]).mean() * 100\r\n    logs = {'avg_train_loss': avg_loss, 'f1': f1}\r\n    return {'avg_train_loss': avg_loss,\r\n            'progress_bar': logs, 'log': logs}\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2265/comments",
    "author": "amitness",
    "comments": [
      {
        "user": "amitness",
        "created_at": "2020-06-25T03:11:34Z",
        "body": "@williamFalcon "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-25T03:23:16Z",
        "body": "good suggestion.\r\nFor validation the metrics should just happen on epoch_end. Each val_step should pass the necessary tensors to calculate the \"global\" metric.\r\n\r\nBut we have a new way of doing this which is simpler and will be release in the next few weeks "
      },
      {
        "user": "amitness",
        "created_at": "2020-06-25T06:09:54Z",
        "body": "Awesome. Looking forward to it then."
      }
    ]
  },
  {
    "number": 2263,
    "title": "Full batch training",
    "created_at": "2020-06-19T06:19:52Z",
    "closed_at": "2020-06-19T12:27:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2263",
    "body": "## ❓ Questions and Help\r\n\r\nFor smaller datasets, it makes sense to do full batch training, not minibatch. How do you implement fullbatch training in pytorch lightning, given that train and validation might be different sizes?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2263/comments",
    "author": "turian",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T06:28:02Z",
        "body": "what do you mean by full batch training?\r\n\r\nThis is not lightning specific though... this is pytorch\r\n\r\n```\r\nDataloader(..., batch_size=1)\r\nDataloader(..., batch_size=32)\r\nDataloader(..., batch_size=len(dataset))\r\n```"
      },
      {
        "user": "turian",
        "created_at": "2020-06-19T12:27:00Z",
        "body": "Thank you, that is perfect."
      }
    ]
  },
  {
    "number": 2258,
    "title": "How to interpret the profiler statistics here",
    "created_at": "2020-06-19T04:14:12Z",
    "closed_at": "2020-08-27T07:23:11Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2258",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n\r\n#### What is your question?\r\nI had an existing code in Pytorch which was taking ~100mins for an epoch. So I converted it to PyTorch-lightning module format to get the timings by the profiler. Surprisingly now it took ~60 mins to train but the statistics which I got are weird. The time (if it's in sec) is very less which doesn't sum up to 60 mins. Can anyone explain why is it so?\r\n\r\nEpoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [1:02:09<00:00,  1.24s/it, loss=0.109, v_num=1]\r\n\r\n\r\nProfiler Report\r\n\r\nAction                  |  Mean duration (s)    |  Total time (s)\r\n-----------------------------------------------------------------\r\non_train_start          |  0.0015837            |  0.0015837\r\non_epoch_start          |  0.00032781           |  0.00032781\r\nget_train_batch         |  0.0094398            |  28.329\r\non_batch_start          |  1.8537e-05           |  0.055612\r\nmodel_forward           |  0.03761              |  112.83\r\nmodel_backward          |  0.025818             |  77.455\r\non_after_backward       |  3.9733e-06           |  0.01192\r\noptimizer_step          |  0.024734             |  74.201\r\non_batch_end            |  0.0013818            |  4.1453\r\non_epoch_end            |  3.6526e-05           |  3.6526e-05\r\non_train_end            |  0.00052313           |  0.00052313\r\n\r\n#### Code \r\n\r\ntrain_loader = get_loader(config.train_data_path, 'Train')\r\nmodel = Network()\r\ntrainer = pl.Trainer(max_epochs=1, gpus=2, profiler=True, distributed_backend='dp')\r\ntrainer.fit(model, train_loader) \r\n\r\n#### What have you tried?\r\nI tried to figure out possible bugs but just implemented the basic code as given in the examples section\r\n#### What's your environment?\r\n\r\nOS- linux, using conda\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2258/comments",
    "author": "PC09",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-19T04:14:50Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-18T05:01:10Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2249,
    "title": "Upgraded to 0.8.0 => `def val_dataloader` does not have access to properties from prepare_data",
    "created_at": "2020-06-19T00:20:46Z",
    "closed_at": "2020-06-19T01:45:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2249",
    "body": "Upgraded to version 0.8.0\r\n\r\nIn my code I define\r\n\r\n`self.val_samples` in `def prepare_data` and use them later in \r\n\r\n```\r\n    def val_dataloader(self):\r\n        val_aug = from_dict(self.hparams[\"val_aug\"])\r\n\r\n        return DataLoader(\r\n            Dataset(self.val_samples, val_aug),\r\n            batch_size=self.hparams[\"val_parameters\"][\"batch_size\"],\r\n            num_workers=self.hparams[\"num_workers\"],\r\n            shuffle=False,\r\n            pin_memory=True,\r\n            drop_last=False,\r\n        )\r\n```\r\nThis worked before the upgrade, but now I get\r\n```\r\nAttributeError: 'Model' object has no attribute 'val_samples'\r\n```\r\n\r\nwhat should I modify to make things work?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2249/comments",
    "author": "ternaus",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T01:23:50Z",
        "body": "yeah.. it was a bug that this worked before haha. prepare_data is only meant to be called from node=0 rank=0. Now it works as expected.\r\n\r\nprepare_data should be only to download... then do the other stuff in either setup or dataloaders\r\n\r\n```\r\ndef prepare_data(...):\r\n    # download only\r\n    # this won't happen on every GPU\r\n\r\ndef val_dataloader(...):\r\n    # do assignments or anything you want here\r\n```"
      }
    ]
  },
  {
    "number": 2232,
    "title": "Best practices when Module __init__ contains Dataset?",
    "created_at": "2020-06-18T10:02:23Z",
    "closed_at": "2020-06-19T01:37:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2232",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\n\r\nWhat are best practices when the Module __init__ contains the Dataset? This is useful when the input and output sizes are derived from the data set, and not hparams.\r\n\r\nHowever, `Module.load_from_checkpoint(...)` fails with the following error:\r\n\r\n```\r\nTypeError: __init__() missing 1 required positional argument: 'patchDistanceDataset'\r\n```\r\n\r\n#### What have you tried?\r\n\r\nThere could be a module method to load the dataset. However, this will not help __init__ to build the network because it will be unaware of the input and output sizes.\r\n\r\n#### What's your environment?\r\n\r\n - OS: OSX\r\n - Packaging pip3\r\n - Version 0.7.6\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2232/comments",
    "author": "turian",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-18T10:03:05Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T01:36:55Z",
        "body": "```\r\nModule.load_from_checkpoint(PATH, your_dataset=Dataset)\r\n```"
      }
    ]
  },
  {
    "number": 2203,
    "title": "Is there a callback for before \"configure_optimizers\" is called?",
    "created_at": "2020-06-15T22:22:27Z",
    "closed_at": "2020-08-23T23:29:01Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2203",
    "body": "Is there a callback for before `configure_optimizers` is called?\r\n\r\nI didn't notice anything in the docs so wondering if such a callback exists, or if running code just prior to `configure_optimzers` should be handled elsewhere.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2203/comments",
    "author": "ohmeow",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-15T22:23:08Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-14T23:21:41Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2179,
    "title": "train_percent_check as a method for reducing train data size.",
    "created_at": "2020-06-14T09:54:51Z",
    "closed_at": "2020-06-15T17:27:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2179",
    "body": "## ❓ Question\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\nWhen we use `train_percent_check=x` while training does it uses the same x per cent of data in every epoch?\r\nCan it be used as a method for making total data size short during training? \r\n\r\n#### Code\r\n\r\n`Trainer(gpus=4,max_epochs=20,train_percent_check=.2,log_gpu_memory=True,weights_summary=None)`\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2179/comments",
    "author": "Nilanshrajput",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-14T09:55:43Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-15T05:34:43Z",
        "body": "> When we use train_percent_check=x while training does it uses the same x per cent of data in every epoch?\r\n\r\nYes, if you set shuffle=False in your dataloader, otherwise it will sample a different subset of data every epoch.\r\n\r\n> Can it be used as a method for making total data size short during training?\r\n\r\nYes, that's what it is meant for. You can for example use it to sanity check that your model can overfit to the data.\r\n\r\n"
      },
      {
        "user": "Nilanshrajput",
        "created_at": "2020-06-15T10:39:28Z",
        "body": "What I meant was, even id don't put shuffle=False, defining a train_percent should shuffle into that first x per cent of training data every epoch. So essentially it's like our training data is cut off to x per cent, and everything else remain the same."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-15T10:45:30Z",
        "body": "Yes exactly, `train_percent_check=0.5` means use 50% of the dataset in the train_dataloader, for example. And this will be 50% every epoch."
      }
    ]
  },
  {
    "number": 2170,
    "title": "Save the whole model object",
    "created_at": "2020-06-13T09:54:06Z",
    "closed_at": "2020-06-15T05:41:44Z",
    "labels": [
      "duplicate",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2170",
    "body": "Is there anyway of saving a whole model object with PyTorch lightning? \r\n\r\ne.g. I want something like this:\r\n\r\n```\r\nmodel = load(\"mypath\")\r\nprediction = model.predict(x) \r\n\r\n```\r\n\r\nwithout needing access to the original Model class.  \r\n\r\nI know how to load things from a checkpoint, but that requires having access to the model class. \r\n\r\nIs this possible? Any help would be much appreciated\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2170/comments",
    "author": "p-christ",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-13T09:54:45Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "sambaths",
        "created_at": "2020-06-13T11:44:26Z",
        "body": "I think this is similar to #2145 "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-15T05:41:44Z",
        "body": "@p-christ have a look at the linked issue. I think the answer to your question is there. Let us know."
      }
    ]
  },
  {
    "number": 2142,
    "title": "How to use metrics classes of 0.8.0",
    "created_at": "2020-06-11T02:25:32Z",
    "closed_at": "2020-06-16T15:03:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2142",
    "body": "## ❓ Questions and Help\r\n\r\n0.8.0 has new Metric class which can auto_reduce in ddp. But no examples of them.  Can you give some examples about how to use it?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2142/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-06-11T08:31:41Z",
        "body": "@justusschock pls ^^"
      },
      {
        "user": "MrinalJain17",
        "created_at": "2020-06-14T20:39:29Z",
        "body": "For sure, examples on how to use `TensorMetric` and `NumpyMetric` would be extremely helpful."
      },
      {
        "user": "justusschock",
        "created_at": "2020-06-15T09:29:54Z",
        "body": "Hi, these examples are currently added in #2184 "
      },
      {
        "user": "Borda",
        "created_at": "2020-06-16T15:03:59Z",
        "body": "done in #2184 and #2209"
      }
    ]
  },
  {
    "number": 2124,
    "title": "Exception: process 3 terminated with signal SIGSEGV",
    "created_at": "2020-06-08T22:00:28Z",
    "closed_at": "2020-07-27T23:07:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2124",
    "body": "\r\nHi,\r\nI have been facing this issue on colab while training a GAN.\r\n\r\nEnvironment:\r\npytorch/xla: nightly\r\npytorch-lightning: master\r\n\r\nError:\r\n```python  \r\n\r\nFile \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 871, in fit\r\n    xmp.spawn(self.tpu_train, args=(model,), nprocs=self.num_tpu_cores, start_method=start_method)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 296, in spawn\r\n    start_method=start_method)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 108, in join\r\n    (error_index, name)\r\nException: process 3 terminated with signal SIGSEGV\r\n\r\n```\r\n\r\nI tried creating the metrics report. I got this and it popped out this error with it.\r\n```\r\nMetric: XrtAllocateFromTensor\r\n  TotalSamples: 4032\r\n  Accumulator: 18s194ms721.992us\r\n  Mean: 004ms953.532us\r\n  StdDev: 020ms533.255us\r\n  Rate: 26.1545 / second\r\n  Percentiles: 25%=627.008us; 50%=915.623us; 80%=005ms490.373us; 90%=009ms091.132us; 95%=010ms757.279us; 99%=012ms360.721us\r\nMetric: XrtCompile\r\n  TotalSamples: 176\r\n  Accumulator: 11m49s071ms079.670us\r\n  Mean: 04s688ms903.862us\r\n  StdDev: 05s594ms399.924us\r\n  Rate: 0.270883 / second\r\n  Percentiles: 25%=003ms325.412us; 50%=005ms311.267us; 80%=09s375ms765.141us; 90%=10s084ms749.219us; 95%=10s242ms795.199us; 99%=13s303ms006.740us\r\nMetric: XrtExecute\r\n  TotalSamples: 1522\r\n  Accumulator: 02m59s496ms518.753us\r\n  Mean: 078ms760.743us\r\n  StdDev: 057ms404.221us\r\n  Rate: 1.75372 / second\r\n  Percentiles: 25%=002ms954.943us; 50%=087ms484.699us; 80%=104ms102.739us; 90%=184ms763.321us; 95%=184ms169.253us; 99%=185ms319.953us\r\nMetric: XrtExecutorEvict\r\n  TotalSamples: 0\r\n  Accumulator: nanB\r\n  Mean: nanB\r\n  StdDev: nanB\r\n  Percentiles: \r\nMetric: XrtReadLiteral\r\n  TotalSamples: 1364\r\n  Accumulator: 954ms286.840us\r\n  Mean: 701.158us\r\n  StdDev: 324.895us\r\n  Rate: 1.75001 / second\r\n  Percentiles: 25%=545.918us; 50%=647.467us; 80%=791.931us; 90%=905.633us; 95%=001ms149.656us; 99%=002ms402.074us\r\nMetric: XrtReleaseAllocation\r\n  TotalSamples: 3645\r\n  Accumulator: 447ms619.909us\r\n  Mean: 152.585us\r\n  StdDev: 235.781us\r\n  Rate: 207.996 / second\r\n  Percentiles: 25%=023.275us; 50%=036.971us; 80%=236.681us; 90%=452.955us; 95%=695.044us; 99%=001ms069.589us\r\n\r\nGPU available: False, used: False\r\nNo environment variable for node rank defined. Set as 0.\r\nUsing 16bit precision.\r\ntraining on 8 TPU cores\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 155, in <module>\r\n    trainer.fit(gan_model) \r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 871, in fit\r\n    xmp.spawn(self.tpu_train, args=(model,), nprocs=self.num_tpu_cores, start_method=start_method)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 296, in spawn\r\n    start_method=start_method)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\r\n    while not context.join():\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 119, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 228, in _start_fn\r\n    _setup_replication()\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 221, in _setup_replication\r\n    xm.set_replication(device, [device])\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/core/xla_model.py\", line 233, in set_replication\r\n    replication_devices = xla_replication_devices(devices)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/core/xla_model.py\", line 206, in xla_replication_devices\r\n    format(len(local_devices), len(kind_devices)))\r\nRuntimeError: Cannot replicate if number of devices (1) is different from 8\r\n\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2124/comments",
    "author": "sambaths",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-08T22:01:12Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-09T00:06:26Z",
        "body": "Update to master and try again?"
      },
      {
        "user": "sambaths",
        "created_at": "2020-06-09T07:38:05Z",
        "body": "I tried updating from github, but the code is stuck at ```checkpoint_callback``` \r\nThe trainer tries to create a checkpoint, but it doesn't go beyond that.\r\nI have encountered this issue earlier also(code stuck), but most of the time it throws the above error message ( ```Exception: process 3 terminated with signal SIGSEGV```) and some times it gets stuck at this stage.\r\n\r\nAfter removing ```checkpoint_callback``` the code runs through all epochs but after all the epochs are over, the code is stuck (doesn't stop execution). \r\n\r\nIn both cases, the code gets stuck and the cell doesn't stop its execution.\r\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-07-27T20:43:29Z",
        "body": "that is spawn issue, shall be fixed in #2632 "
      }
    ]
  },
  {
    "number": 2102,
    "title": "Validation step metrics not logged",
    "created_at": "2020-06-07T05:28:55Z",
    "closed_at": "2020-08-16T18:30:40Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2102",
    "body": "## ❓ Questions and Help\r\n\r\nIt seems like data output in the `validation_step` does not get logged to tensorboard, it needs to be aggregated first in the `validation_epoch_end`, which is not the case for `training_step`.\r\n\r\nThe below would only show `val_loss` and only aggregated but all of `mae`, `mape` etc from every iteration.\r\n\r\nAs a workaround I could explicitly log, but how do I get the current iteration in the callbacks I only see how to get the current epoch.\r\n\r\n```\r\n    def step(self, y_hat, y, mode='train'):\r\n        loss = F.mse_loss(y_hat, y)\r\n        mae = F.l1_loss(y_hat, y)\r\n        mape = median_absolute_percentage_error(y_hat, y)\r\n        r2 = r2_score(y_hat, y)\r\n        out = {'loss': loss, 'mae': mae, 'mape': mape, 'R2': r2}\r\n        if mode=='train':\r\n            out['log'] = out.copy()\r\n            return out\r\n        elif mode =='val':\r\n            out = {f'{mode}_{k}': v for k,v in out.items()}\r\n            out['log'] = out.copy()\r\n            return out\r\n        else:\r\n            raise Exception('Unsupported mode')\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        return self.step(y_hat, y, 'val')\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        tensorboard_logs = {'val_loss': avg_loss}\r\n        return {'val_loss': avg_loss, 'log': tensorboard_logs}\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2102/comments",
    "author": "feribg",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-06-07T18:28:38Z",
        "body": "you can get the current step with `self.global_step` or self.trainer.global_step\r\n\r\nyes, it's true you can manually log it yourself, but there is a reason why we don't log each step of the validation, because the loggers (all as far as I know) use a global step for logging, and this means if your training epoch has n bathces, and your validation has m batches, after the first epoch you will log\r\nn + m steps for training + validation and then your training loss will contingue with step n+m+1 instead of n+1 for epoch 2. You will see a  big jump in the visualization.\r\n\r\nTensorBoard is too limited, you cannot set the abscissa to anything else than the step (as far as I know). Therefore logging validation step makes no sense."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-07T18:42:27Z",
        "body": "duplicate of #1906"
      },
      {
        "user": "feribg",
        "created_at": "2020-06-07T22:34:09Z",
        "body": "To your point re the step # being inconsistent, I'm not sure why that would be the case when the `val` metrics are always prefixed with `val_` so you will get `m` number of `val_...` metric for each validatio epoch, right? I'm struggling to understand where the mixup between train and valid comes into play when they are namespaced differently  and stored as different metrics?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-07T22:53:09Z",
        "body": "epoch 1 starts\r\nbatch 1: train_loss = ... gloabal_step = 1\r\nbatch 2: train_loss = ... gloabal_step = 2\r\nbatch 3: train_loss = ... gloabal_step = 3\r\nvalidation starts\r\nbatch 1: val_loss = ... gloabal_step = 4\r\nbatch 2: val_loss = ... gloabal_step = 5\r\nepoch 2 starts\r\nbatch 1: train_loss = ... **gloabal_step =6**\r\nbatch 2: train_loss = ... **gloabal_step = 7**\r\nbatch 3: train_loss = ... **gloabal_step = 8**\r\n\r\ntry it yourself :)\r\n\r\nthis is a limitation of the loggers, which cannot log to the past history, only at the current global step."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-07T23:08:38Z",
        "body": "why do you need to log every validation step anyway? it's not going to tell you much about your validation results"
      },
      {
        "user": "feribg",
        "created_at": "2020-06-08T15:49:14Z",
        "body": "You're right I don't really need to, just wanted to keep the plots on the same axis and the code consistent between train / valid. Feel free to close it if you don't think it's worthwhile. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-07T18:06:15Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2063,
    "title": "No validation dataset",
    "created_at": "2020-06-03T20:10:36Z",
    "closed_at": "2020-08-04T20:01:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2063",
    "body": "I am just starting to use pytorch_lightning. I have one question regarding the validation set. I may or may not have a validation set during my training. How should the `validation_step` be structured in this case?\r\n\r\nis it enough do something like this:\r\n\r\n```\r\n@pl.data_loader\r\ndef val_dataloader(self):\r\n     if has_valset:        \r\n        return self.__dataloader(train=False)\r\n    return None\r\n```\r\n\r\nSo if it  returns None, then no validation loop calls will be made?\r\n\r\nOr do I need something like:\r\n\r\n```\r\ndef validation_step(self, batch, batch_idx):\r\n    # Is this check necessary?\r\n    if not has_vaset:\r\n        return\r\n   \r\n   # process the validation batch\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2063/comments",
    "author": "pamparana34",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-06-08T00:37:46Z",
        "body": "I suggest you return the validation loader anyway, and then set the Trainer flag val_percent_check=0 if you don't want to run validation. \r\nLet me know if that works or not."
      },
      {
        "user": "Borda",
        "created_at": "2020-08-04T20:01:51Z",
        "body": "feel free to reopen if needed 🐰 "
      }
    ]
  },
  {
    "number": 2059,
    "title": "Keyboard Interrupt lunches test but wandblogger kills the process",
    "created_at": "2020-06-03T09:42:11Z",
    "closed_at": "2020-10-29T05:10:44Z",
    "labels": [
      "help wanted",
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2059",
    "body": "## 🐛 Bug\r\nI am using WandBLogger and when I have a run that I want to stop manually with a keyboard interrupt, the model correctly stops training and starts executing the Test function. The problem is that at the same time WandBLogger starts uploading the data and when it is done it kills the process, therefore the test function is not finished and that data is not uploaded to the logger ofc.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Execute a run with wandblogger that has (trainer.fit(model); trainer.test())\r\n2. Ctrl + C\r\n\r\n### Expected behavior\r\nExecute train -> Ctrl+C -> stop the training -> lunch the test -> upload everything to the logger",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2059/comments",
    "author": "Brechard",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-06-03T10:12:30Z",
        "body": "@Brechard mind specify your env and Pl version?\r\n@justusschock @jeremyjordan ^^"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-03T10:18:30Z",
        "body": "try again using master"
      },
      {
        "user": "Brechard",
        "created_at": "2020-06-03T13:01:24Z",
        "body": "I just tryed in master and it has the same problem.\r\n\r\nI am runing on an EC2 p3.2xlarge with pytorch 1.5 installed"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-02T14:07:02Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-08-04T20:03:23Z",
        "body": "@borisdayma mind have a look? 🐰 "
      },
      {
        "user": "borisdayma",
        "created_at": "2020-08-04T22:46:02Z",
        "body": "@Borda I never personally use \"Ctrl + C\". I thought it was mainly to release properly resources.\r\nWould it be expected to keep the logger alive or would there be any kind of risks of incorrect data sent to loggers?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T04:24:17Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2045,
    "title": "2 optimizers : skip updates for the second optim",
    "created_at": "2020-06-01T22:16:04Z",
    "closed_at": "2020-06-02T19:00:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2045",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI have a model with 2 optimizers : one for the backbone weight and second for \"backbone support\" weights. However, 2nd optimizer should only accumulate grads through the whole epoch and perform one update at the epoch end. \r\n\r\nLightning keeps asking for an output for the 2nd optimizer, but there is nothing to output in addition to the first optimizer results. How can I bypass this ?\r\n\r\n#### Code\r\nHere are defined the training_step and optimizer_step functions to illustrate my issue.\r\n```\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            images, target = batch\r\n            output = self(images)\r\n            loss_val = F.cross_entropy(output, target)\r\n            acc1, acc5 = self.__accuracy(output, target, topk=(1, 5))\r\n            weight_cons, act_cons, weight_p_c_cons = self.normalized_consumption()\r\n\r\n            tqdm_dict = {'Loss/train_loss': loss_val, \r\n                        'Acc/acc1': acc1,\r\n                        'Acc/acc5': acc5,}\r\n            output = OrderedDict({\r\n                'loss': loss_val,\r\n                'Loss/loss': loss_val,\r\n                'Acc/acc1': acc1,\r\n                'Acc/acc5': acc5,\r\n                'progress_bar': tqdm_dict,\r\n                'log': tqdm_dict\r\n            })\r\n            return output\r\n     if optimizer_idx == 1:\r\n         # Do nothing ?\r\n\r\ndef optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None):\r\n        if optimizer_i == 0:\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n\r\n        # update 2nd optimizer at the end of the epoch\r\n        if optimizer_i == 1 and self.__nbbatch -1 <= batch_nb:\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n```\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nI tried to pass an empty output dict in training_step if the optimizer_idx == 1. However, Lightning complains that the dict is empty (`trainer/logging.py:106` -> `Nonetype has no attribute 'items'`)\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\n - Packaging pip\r\n - Version 0.7.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2045/comments",
    "author": "sebastienwood",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-01T22:18:20Z",
        "body": "override the backward pass. \r\ncheck the current batch idx and don’t apply the backward step until the end of the epoch"
      },
      {
        "user": "sebastienwood",
        "created_at": "2020-06-02T19:00:01Z",
        "body": "I managed to make it works thanks :) \r\nI had to also return a dummy `loss` for the 2nd optimizer that lightning really wanted to have (as usual in an ordered dict)\r\n\r\nAs a side note, a strange issue (that has probably nothing to do with Lightning) is that a custom function stopped receiving the backward call when using two optimizers. "
      }
    ]
  },
  {
    "number": 1988,
    "title": "Stopping the code along with a graceful shutdown.",
    "created_at": "2020-05-28T16:12:51Z",
    "closed_at": "2020-05-29T07:27:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1988",
    "body": "##\r\nIs there a way to stop the training in the model when some criteria are satisfied. Something along the lines:\r\n```\r\nclass myCallback(Callback):\r\n    def __init__(self):\r\n        ...\r\n    def on_epoch_end(self, trainer, pl_module):\r\n        if criteria:\r\n            model.stop_training = True # stops the training; need help here\r\n```\r\nNote that I also want to have the early stopping feature where the 'val_loss' is monitored but want to stop running the code if some other criteria is satisfied. Also, is my method of having this feature in the callback module correct or should I inherit the early stopping criteria?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1988/comments",
    "author": "nsidn98",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-28T16:13:34Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-05-28T22:44:58Z",
        "body": "You could raise KeyboardInterrupt, this will lead to a graceful shutdown. There is some work in progress in this PR:\r\n#1631 \r\nBut it should already work if you raise it from within your code"
      },
      {
        "user": "nsidn98",
        "created_at": "2020-05-29T07:27:22Z",
        "body": "Got it by using `raise KeyboardInterrupt` in the code.\r\nClosing the issue"
      }
    ]
  },
  {
    "number": 1982,
    "title": "Is there a way to make the Trainer skip loading optimizer from a checkpoint?",
    "created_at": "2020-05-28T12:01:42Z",
    "closed_at": "2020-05-29T10:41:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1982",
    "body": "Use case is if I want to load a model's weights but reset the the optimizer state. Is there a flag I can pass that skips loading the optimizer?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1982/comments",
    "author": "nihalsid",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-28T22:50:59Z",
        "body": "There is no flag but you could implement the model hook on_load_checkpoint and reset the optimizer yourself."
      },
      {
        "user": "nihalsid",
        "created_at": "2020-05-29T10:41:30Z",
        "body": "Cool thanks!"
      },
      {
        "user": "colllin",
        "created_at": "2021-10-19T01:20:30Z",
        "body": "Can you clarify how to accomplish this?  I'd need to save a reference to the optimizer within `configure_optimizers`, e.g. `self.optim = whatever; return self.optim`, right?  Then in `on_load_checkpoint`, I'd... need to manually clear the optimizer state?  Is that easy to do?  If I'm understanding correctly, I can't simply re-init a fresh optimizer, since I already returned it from `configure_optimizers` and a reference was saved somewhere."
      },
      {
        "user": "colllin",
        "created_at": "2021-10-19T01:29:17Z",
        "body": "I think you can accomplish it this way without needing to know any of the optimizer's implementation details:\r\n```\r\n    # in your lightning module...\r\n    def on_load_checkpoint(self, checkpoint):\r\n        fresh_optimizer = Adam(...)\r\n        self.my_optimizer.load_state_dict(fresh_optimizer.state_dict())\r\n```"
      },
      {
        "user": "colllin",
        "created_at": "2021-10-19T01:52:47Z",
        "body": "I think I was confused about the difference between `Trainer(resume_from_checkpoint=\"...\")` vs. `LitModule.load_from_checkpoint(\"...\")`, and the docs could probably be improved here.\r\n\r\nIt appears that `LightningModule.load_from_checkpoint` doesn't even instantiate your optimizers, because only the `Trainer` does that.  Therefore, I believe:\r\n- you MUST use the Trainer's `resume_from_checkpoint` arg if you want to re-load the optimizer state (and other training state), and \r\n- you NEED NOT WORRY about accidentally loading other training state when calling `LightningModule.load_from_checkpoint`, because the lightning module isn't responsible for training state in the first place.\r\n\r\nPlease help me out if I didn't arrive at the correct conclusion."
      },
      {
        "user": "YJ-20",
        "created_at": "2024-03-19T01:30:29Z",
        "body": "If you want to reset the optimizer state in `lightning`, you can use `on_load_checkpoint` as like this.\r\n```python\r\ndef on_load_checkpoint(self, checkpoint):\r\n    checkpoint[\"optimizer_states\"] = []\r\n```"
      },
      {
        "user": "marz869",
        "created_at": "2024-03-22T17:08:12Z",
        "body": "> I think I was confused about the difference between `Trainer(resume_from_checkpoint=\"...\")` vs. `LitModule.load_from_checkpoint(\"...\")`, and the docs could probably be improved here.\r\n> \r\n> It appears that `LightningModule.load_from_checkpoint` doesn't even instantiate your optimizers, because only the `Trainer` does that. Therefore, I believe:\r\n> \r\n>     * you MUST use the Trainer's `resume_from_checkpoint` arg if you want to re-load the optimizer state (and other training state), and\r\n> \r\n>     * you NEED NOT WORRY about accidentally loading other training state when calling `LightningModule.load_from_checkpoint`, because the lightning module isn't responsible for training state in the first place.\r\n> \r\n> \r\n> Please help me out if I didn't arrive at the correct conclusion.\r\n\r\n@colllin LightningModule.load_from_checkpoint re_load optimizer, but resume_from_checkpoint, reload epoch and global step...and reset the optimizer!! So if I have a checkpoint with all states(lr scheduler, epoch, global step, ...) if I do LightningModule.load_from_checkpoint, it re_loads optimizer, lr scheduler and optimizer works as I expect, but epoch and global step is set to 0. And if in the next line I load checkpoints again in trainer.fit, then I can continue training the model from last epoch, but optimizer is reset, and lr_scheduler does not work at all!!"
      }
    ]
  },
  {
    "number": 1980,
    "title": "Collect all  losses into a list?",
    "created_at": "2020-05-28T10:09:45Z",
    "closed_at": "2020-05-29T23:08:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1980",
    "body": "What's the easiest way to return a list with all my losses in it? Note: I don't want to log it, but I want to have a list that I can plot in altair after training is done.\r\n\r\nCurrently, I'm maintaining a list outside the lightning module:\r\n\r\n```\r\nlosses = [] # <-- this one\r\n...\r\n\r\nclass MyModule(LightningModule):\r\n    def validation_epoch_end(self, outputs):\r\n            avg_loss = torch.stack([ x['val_loss'] for x in outputs ]).mean()\r\n            correct = sum([ x['correct'] for x in outputs])\r\n            accuracy = float(correct) / len(outputs)\r\n            losses.append(avg_loss)   # <--- append to outside var here\r\n            \r\n            return {'avg_loss' : avg_loss, 'accuracy': accuracy, 'log' : {'val_loss': avg_loss, 'accuracy': accuracy}}\r\n```\r\n\r\nIs there a \"lightning\" way of doing this?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1980/comments",
    "author": "drozzy",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-28T22:53:41Z",
        "body": "Why not save it into an attribute self.losses and then have it accessible via model.losses from outside?"
      },
      {
        "user": "drozzy",
        "created_at": "2020-05-29T23:08:56Z",
        "body": "Oh, right.\r\nHaha.\r\nThanks."
      }
    ]
  },
  {
    "number": 1979,
    "title": "Dynamically change optimizer frequency",
    "created_at": "2020-05-28T08:42:13Z",
    "closed_at": "2020-05-28T12:09:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1979",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI have a WGAN and the ratio between iterations on the discriminator and on the generator is fixed at 5:1. I accomplished this by passing the frequency parameter in the `configure_optimizers` method\r\n```\r\nres_1 = {\r\n            'optimizer': optimizer_d,\r\n            'frequency': 5,\r\n            'lr_scheduler': scheduler_d\r\n        }\r\n```\r\nsame for generator\r\n```\r\nres_2 = {\r\n            'optimizer': optimizer_g,\r\n            'frequency': 1, \r\n            'lr_scheduler': scheduler_g\r\n        }\r\n```\r\n\r\nHow can I dynamically change the `frequency` parameter, such that for the first `n` iterations I have a frequency `x` and after I have a frequency `y`.\r\n\r\n#### Code\r\n\r\nDon't know how to do it.\r\n\r\n#### What have you tried?\r\n\r\n#### What's your environment?\r\n\r\n - OS: OS independent\r\n - Packaging: pip\r\n - Version: 0.7.6\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1979/comments",
    "author": "lucadiliello",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-28T12:04:45Z",
        "body": "There is no official way of doing this. However, this can be achieved using a `Callback` to change the `Trainer` attribute `optimizer_frequencies`. So if you define callback like this:\r\n```\r\nclass DynamicOptimizerFrequency(Callback):\r\n    def on_epoch_end(self, trainer, model):\r\n           if trainer.current_epoch >10:\r\n               trainer.optimizer_frequency = [3,1]\r\n           if trainer.current_epoch > 20:\r\n               trainer.optimizer_frequency = [2,1]\r\n```\r\nwhere you change the code to however dynamically you want to change the frequencies. "
      },
      {
        "user": "lucadiliello",
        "created_at": "2020-05-28T12:09:29Z",
        "body": "Oh fantastic. I will try then to create a `Callback` and maybe create a pull request to let others use it. You pointed me to the right way."
      }
    ]
  },
  {
    "number": 1978,
    "title": "Best practice: How to convert a tensor created in __init__() to the same device as the model",
    "created_at": "2020-05-28T07:23:57Z",
    "closed_at": "2020-05-28T21:28:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1978",
    "body": "#### Question\r\nSay I create a tensor in the `__init__()` of a module, what is the best practice for me to convert it to the same device as the module so that it works for cpu/gpu/distributed training environment?\r\n\r\n#### Things that I tried\r\nI tried to put this type check into the `prepare_data()` function, and run the module in GPU, but it seems like the module type is still in CPU when this function is called. \r\n\r\nI then tried to put in my `forward()` function, because this is eventually where the train/validation/test will end up with, but this looks ugly because we need to do this check at every step, but essentially we only need to call do it once. Is there a better solution? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1978/comments",
    "author": "DKandrew",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-28T07:24:34Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-05-28T21:28:43Z",
        "body": "This is a PyTorch question. Register it as a buffer.\r\nSee pytorch docs for register_buffer."
      },
      {
        "user": "DKandrew",
        "created_at": "2020-05-29T05:54:16Z",
        "body": "WOWW this is awesome!! Thank you!!!"
      }
    ]
  },
  {
    "number": 1953,
    "title": "Names of subdirectories for checkpoint and logging",
    "created_at": "2020-05-26T09:23:16Z",
    "closed_at": "2020-06-11T09:02:09Z",
    "labels": [
      "question",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1953",
    "body": "## ❓ Questions and Help\r\n\r\n#### First question\r\n\r\nThe directories where checkpoints are saved are always something like \r\n```\r\n$DIR/$SUBDIR/checkpoint/\r\n```\r\nI have $DIR controlled: it can be set by `default_save_path` argument in `Trainer`, and it is also a member of the class with the same name.\r\n\r\n$SUBDIR seems randomly generated. Where is it generated? Is there any way where I can access it after training? \r\n\r\nMy use case is: I am training a `LightningModule`, and by default it is saving the best model (checkpoint) during training (the one with lowest `val_loss`). I want to load the best model just after training and, for example run a test run.\r\n\r\n#### Second question:\r\nA similar question about wandb logger. Is there any way that I can see which logger directory and subdirectory it is pointing to? \r\n\r\nMy goal would be to select the best, for example, 5 models out of 20 runs and load them by combining the metrics stored in the logger and the weights stored in the checkpoints. Alternatively, what would be the best practice here?\r\n\r\n#### What have you tried?\r\n\r\nI went through the code looking for where $SUBDIR is generated and I did not find it. I could not find anything relevant as members of the classes `Trainer` or `LightningModule` either.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1953/comments",
    "author": "fjhheras",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-26T09:23:55Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "fjhheras",
        "created_at": "2020-05-27T17:03:39Z",
        "body": "Hi, it seems that I found part of the answer: `trainer.checkpoint_callback.best_k_models` is a dictionary with the path and the metric (in my case validation loss). By default it only contains an element, which is the best model, so this works for me:\r\n\r\n```\r\npath_to_best_model =next(iter(trainer.checkpoint_callback.best_k_models.keys()))\r\nmodel.load_from_checkpoint(path_to_best_model)\r\n```"
      },
      {
        "user": "lgvaz",
        "created_at": "2020-05-29T15:35:40Z",
        "body": "If you have more than one model saved, you can use the following snippet for getting the best model:\r\n```python\r\nop = min if checkpoint_callback.mode == 'min' else max\r\nbest_model_path = op(\r\n    checkpoint_callback.best_k_models,\r\n    key=checkpoint_callback.best_k_models.get\r\n)\r\n```\r\n\r\nBTW, this snippet comes from #1799 , this PR will make it even easier to get back the best model =)"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-11T09:02:09Z",
        "body": "btw, #1799 hs landed, so I assume this can be also closed... feel free to reopen if needed :raccoon: "
      }
    ]
  },
  {
    "number": 1945,
    "title": "return \"val_loss\" in validation step and \"avg_val�_loss\" from validation_epoch_end in trainer ",
    "created_at": "2020-05-25T14:55:34Z",
    "closed_at": "2020-08-02T18:07:06Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1945",
    "body": "I want to return  the val_loss from the validation step and avg_val_loss from validation epoch end in the trainer. \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1945/comments",
    "author": "rajarajanvakil",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-25T14:56:17Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-24T17:39:03Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1930,
    "title": "How to access training and validation losses from callbacks? ",
    "created_at": "2020-05-23T08:45:53Z",
    "closed_at": "2020-05-23T18:52:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1930",
    "body": "For example, if my validation_epoch_end in the trainer returns {'avg_loss':loss, 'log':logs}, how to get the loss value from a callback method like:def on_validation_end(trainer, pl_module)?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1930/comments",
    "author": "NagarajSMurthy",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-23T08:46:28Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "MaharshiYeluri01",
        "created_at": "2020-05-23T16:00:43Z",
        "body": "you can access the current epoch variables from trainer.callback_metrics\r\n/looks something like below\r\n{'epoch': 4,\r\n 'loss': tensor(0.4924, device='cuda:0'),\r\n 'train_loss': tensor(0.4924, device='cuda:0'),\r\n 'val_auc': tensor(0.7359, dtype=torch.float64),\r\n 'val_loss': tensor(0.7714, device='cuda:0')}"
      },
      {
        "user": "NagarajSMurthy",
        "created_at": "2020-05-23T18:52:35Z",
        "body": "Thank you @MaharshiYeluri01. That answers clearly. "
      }
    ]
  },
  {
    "number": 1928,
    "title": "val_dataloader participate in training?",
    "created_at": "2020-05-23T04:44:33Z",
    "closed_at": "2020-07-31T23:50:45Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1928",
    "body": "## ❓ Questions and Help\r\n\r\nIt's very strange. I just use\r\n```\r\ntrainer.fit(net, train_dataloader=data.train_loader, val_dataloaders=data.val_loader)\r\ntrainer.test(test_dataloaders=data.test_loader)\r\n```\r\nto train, validate and test.\r\nI am sure train_loader, val_loader and test_loader are from independent dataset.\r\nBut the test result turns out to be much worse than val result, and val result is close to train result, which seems that the val set takes part in training. I can't figure out what happened. Is this a bug or my way is wrong?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1928/comments",
    "author": "wunianjian",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-23T04:45:11Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "MaharshiYeluri01",
        "created_at": "2020-05-23T16:33:03Z",
        "body": "Validation data loader never takes part in the training unless we do something explicitly in validation_step(except for calculating the loss). What's the metric you are using to compare the performance on train,val and test?"
      },
      {
        "user": "wunianjian",
        "created_at": "2020-05-23T16:36:06Z",
        "body": "RMSE. I am working on a review score prediction task."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-05-23T19:37:26Z",
        "body": "If I don't read it wrongly, there is nothing surprising about that.\r\nIf the datasets are all different, it is certainly possible or even reasonable to expect in general that the test error will be higher than the training and validation error.\r\nObvious reasons:\r\n- overfitting\r\n- early stopping\r\n\r\nthe combination of the two points could be called \"overfitting to validation set\", and so even if your test set is close to the validation set (in terms of statistics) it can yield a higher error."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-05-23T19:40:33Z",
        "body": "If there is a bug in your code, this could help:\r\n- try the validation dataset in the test loop, it should yield the same metric as in validation loop\r\n- double check that you average your metric accross the whole dataset (if averaging is at all applicable in your case)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-22T20:42:19Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1926,
    "title": "Save checkpoint for the last epoch",
    "created_at": "2020-05-22T15:02:24Z",
    "closed_at": "2020-05-25T11:47:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1926",
    "body": "How to save the checkpoint only for the last epoch?\r\nIn the docs:\r\n\r\n```\r\nif save_top_k == k, the best k models according to the quantity monitored will be saved. if save_top_k == 0, no models are saved. if save_top_k == -1, all models are saved. Please note that the monitors are checked every period epochs. if save_top_k >= 2 and the callback is called multiple times inside an epoch, the name of the saved file will be appended with a version count starting with v0.\r\n```\r\n* `k = 0` does not save any.\r\n* `k > 1` saves only a few the best\r\n* `k = -1` saves all of them\r\n\r\nCurrently, I am using k = -1, but it space consuming.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1926/comments",
    "author": "ternaus",
    "comments": [
      {
        "user": "lgvaz",
        "created_at": "2020-05-22T15:10:08Z",
        "body": "PR #1908 will add that option with a `save_last` argument to `ModelCheckpoint` =)"
      }
    ]
  },
  {
    "number": 1910,
    "title": "How to save raw predictions?",
    "created_at": "2020-05-20T22:37:43Z",
    "closed_at": "2020-07-29T06:41:11Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1910",
    "body": "I have it hard coded in my logging callback not to print them, but `trainer.test` still prints them.\r\nI'm wondering the canonical way to save  them with `wandb_logger` or another callback.\r\n(It's an NLP summarization task).\r\nThx!\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1910/comments",
    "author": "sshleifer",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-21T06:20:39Z",
        "body": "Put \r\nself.logger.experiment.log(...)\r\nin the test_end method."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-20T06:21:08Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1902,
    "title": "Use only one of the optimizers as warm-up ",
    "created_at": "2020-05-20T13:48:31Z",
    "closed_at": "2020-08-12T20:10:28Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1902",
    "body": "#### What is your question?\r\nHi! I am trying to train a GAN and I would like to train only the Discriminator for the first x steps, how can that be done? \r\n\r\nI've seen the code for step optimizers at arbitrary intervals but it is not clear to me how it works (it says update generator every 2 steps and discriminator every 4, but then do they cumulate the gradients until they update or what happens in the odd steps? Why are they cumulating gradients then?) so I ended up using the frequency in the dictionary returned in the configure_optimizers functions which gives the behaviour I wanted of train x times optimizer A, train Y times optimizer B. Now I am looking to do for the first S steps only train optimizer A, then train x times A, train Y times B.\r\n\r\n#### What's your environment?\r\n\r\n - OS: Win\r\n - Packaging: conda\r\n - Version: 0.7.6rc2\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1902/comments",
    "author": "Brechard",
    "comments": [
      {
        "user": "MaharshiYeluri01",
        "created_at": "2020-05-23T17:02:17Z",
        "body": "In the training_step function you have access to the bath iteration number(batch_nb), you can ignore the back prop step on generator for first n iterations\r\nUsually batch size while training the GAN's is less(because of size and multiple backward functions) so to eliminate the noisy updates, gradients are accumulated for some mini batches before the optimizer steps"
      },
      {
        "user": "Brechard",
        "created_at": "2020-05-28T10:40:48Z",
        "body": "thanks for the reply, how do I ignore the backprop step though? "
      },
      {
        "user": "sebastienwood",
        "created_at": "2020-06-04T17:37:12Z",
        "body": "You can override the optimizer step like so\r\n```\r\ndef optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None):\r\n        if optimizer_i == 0:\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n```\r\nNotice you can have any kind of condition to trigger the optimizer step."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-03T18:02:02Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1881,
    "title": "Accumulate Metrics for K Batches",
    "created_at": "2020-05-19T01:26:11Z",
    "closed_at": "2020-05-19T01:54:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1881",
    "body": "#### What is your question?\r\n\r\nI would like to aggregate metrics for k minibatches before logging to Tensorboard. How can I accomplish this? \r\n\r\nFor example, I would like to average my loss and accuracy for 10 minibatches and report that value to TensorBoard, not just report the statistics on the 10th minibatch.\r\n\r\n#### What have you tried?\r\n\r\nI noticed there's a `log_save_interval` argument for the trainer, but, this only logs the metrics computed per one batch. This is not that useful for me because my batch size is very small, so I need to average my statistics across multiple minibatches. \r\n\r\nThere is a `train_epoch_end` method I can override, but only gives me my outputs at the end of an epoch. I am looking for something like this where I have access to all my outputs from the previous k minibatches so I can aggregate them.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1881/comments",
    "author": "n2cholas",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-05-19T01:38:18Z",
        "body": "in validation_step log directly:\r\n\r\nself.logger.experiment is a summarywriter"
      },
      {
        "user": "n2cholas",
        "created_at": "2020-05-19T01:54:00Z",
        "body": "Thanks for the quick reply @williamFalcon! I wanted to do this without running my validation every k steps, so instead, I created a callback that does this reporting every k iterations using the `self.logger.experiment` SummaryWriter."
      },
      {
        "user": "hughperkins",
        "created_at": "2021-11-07T12:46:55Z",
        "body": "Hi @n2cholas , dont suppose... could you provide more details on how you did this please?"
      },
      {
        "user": "tchaton",
        "created_at": "2021-11-08T12:47:25Z",
        "body": "Hey @hughperkins,\r\n\r\nYou do something like this:\r\n\r\n```py\r\nclass Model(LightningModule):\r\n\r\n    ...\r\n\r\n    def training_step(..., batch_idx):\r\n\r\n        self.metric(...) # accumulate state\r\n        if batch_idx % 10 == 0:\r\n            self.log(\"my_metric\", self.metric.compute()) # perform a compute every 10 batches\r\n            self.metric.reset() # reset the metric to restart accumulating\r\n```"
      }
    ]
  },
  {
    "number": 1876,
    "title": "Load batches as float16?",
    "created_at": "2020-05-18T16:51:27Z",
    "closed_at": "2020-07-28T23:10:49Z",
    "labels": [
      "help wanted",
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1876",
    "body": "#### Can I use lightning to load training examples as `dtype = float16` to the GPU?\r\n\r\nIn order to train imagenet with larger batches I'd like to store the batches with fp16 on the GPU memory, rather than fp32. In basic examples I've tried this does not happen, and using `precision = 16` barely uses less GPU memory than `precision = 32`. Is it possible to convince lightning to load the batches as  `dtype = float16` and thus to increase the batch size?\r\n\r\n#### Code\r\n\r\nHere is a simple example that reveals the `dtype` of the batches and the output within the `forward()` loop.\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\nimport pytorch_lightning as pl\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import datasets, transforms\r\nimport os \r\nimport pytorch_lightning as pl\r\n\r\nclass LitModel(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n\r\n    def forward(self, x):\r\n        print('forward() input dtype: ', x.dtype)\r\n        out = torch.relu(self.l1(x.view(x.size(0), -1)))\r\n        print('forward() output dtype: ', out.dtype)\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        return {'loss': F.cross_entropy(y_hat, y)}\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True,\r\n                                 transform=transforms.ToTensor()), batch_size=32,num_workers = 8)\r\n    def configure_optimizers(self):\r\n         return torch.optim.Adam(self.parameters(), lr=0.02)\r\n\r\ntrainer = pl.Trainer(gpus = 1,amp_level='O1', precision=16,max_steps=10)\r\nmodel = LitModel()\r\n\r\ntrainer.fit(model)\r\n```\r\n\r\nThe output I get seems to indicate that the batches are stored as fp32 on the GPU, consistent with high memory usage:\r\n```\r\nforward() input dtype:  torch.float32\r\nforward() output dtype:  torch.float16\r\n```\r\n\r\n#### What have you tried?\r\n\r\nI'm wondering if the behavior I observe is a bug or a feature.\r\n\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\n - CUDA 10.1, torch 1.4, lightning 0.7.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1876/comments",
    "author": "tbachlechner",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-18T16:52:08Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-18T18:20:34Z",
        "body": "try torch 1.6? much more stable for fp16.\r\nthen we can look into loading fp16 directly."
      },
      {
        "user": "tbachlechner",
        "created_at": "2020-05-18T18:29:40Z",
        "body": "> try torch 1.6? much more stable for fp16.\r\n> then we can look into loading fp16 directly.\r\n\r\nThanks for your quick response!\r\n\r\nDone, my code produces the same output with torch 1.6, the batches are still float32 on the GPU."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-19T22:21:08Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "Jxu-Thu",
        "created_at": "2021-10-24T06:38:29Z",
        "body": "I encounter the same issue."
      }
    ]
  },
  {
    "number": 1848,
    "title": "Question about custom backward",
    "created_at": "2020-05-15T21:52:39Z",
    "closed_at": "2020-08-02T03:42:26Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1848",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nI want to write my own `.backward`. In the specification of method there are 4 arguments: `self`, `use_amp`, `loss` and `optimizer`:\r\n`def backward(self, use_amp, loss, optimizer)`\r\nBut to do my backward I need additional tensors from `training_step` besides loss tensor. What is the safe and proper way to do it? Seems like there is no functionality to just add `**kwargs` or `*args` in backward and return something additional in `training_step`\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1848/comments",
    "author": "agadetsky",
    "comments": [
      {
        "user": "agadetsky",
        "created_at": "2020-05-25T00:26:00Z",
        "body": "@williamFalcon can you help with this, please?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-24T02:41:09Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1771,
    "title": "Backward on tensors inside training_step",
    "created_at": "2020-05-10T12:51:14Z",
    "closed_at": "2020-05-10T19:32:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1771",
    "body": "#### What is your question?\r\n\r\nIs it safe to call .backward() on tensors inside .training_step? It is the case for example if loss depends on the gradients.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1771/comments",
    "author": "agadetsky",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-10T12:51:51Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-10T19:32:40Z",
        "body": "Might need to use retain_graph if you do. But otherwise no problem. \r\nIf you don't need lightning's .backward, override the method."
      },
      {
        "user": "agadetsky",
        "created_at": "2020-05-10T19:39:56Z",
        "body": "@williamFalcon thank you for your answer. I wanted to override .backward, I think it is more proper way to do what I want, but I need to access some tensors from the training_step, but .backward accepts only loss from training_step. Is there a proper way to get access? To save somehow what I need and then pass it to lightning's .backward?"
      }
    ]
  },
  {
    "number": 1768,
    "title": "How can I retrieve metrics from training and testing",
    "created_at": "2020-05-09T21:50:44Z",
    "closed_at": "2020-07-28T18:08:19Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1768",
    "body": "## ❓ Questions and Help\r\nHow can I extract the metrics returned from `training_step`, `training_epoch_end`, `validation_step`, `validation_epoch_end`, `test_step`, `test_epoch_end` after a `train()` or a `test()` run?\r\n\r\nI'd like to return some dictionary (e.g. `{'loss': ..., 'log': ..., 'param a: 'a', 'param b': 'b', 'param c': {...}}`) from e.g. `test_epoch_end` and retrieve this after calling `trainer.test(net)`. It seems like the data is available some where, as the Weights and Biases logger prints at least the training metrics before uploading. Where can I find those metrics from training and testing?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1768/comments",
    "author": "Uroc327",
    "comments": [
      {
        "user": "appleparan",
        "created_at": "2020-05-20T16:16:59Z",
        "body": "It seems it's a duplicate. See #1694 .\r\nBTW, If you need just postprocessing, I hacked this problem by putting everything (including plotting, I/O, etc.) in `test_epoch_end` . I think it's not best practice but I also couldn't find any method to get the test result. \r\n\r\nI just write the file that records what I need. After then, I write another script to process the file. I wish this helps you."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-19T17:09:23Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1749,
    "title": "How to use pytorch-lightning to run GAN？",
    "created_at": "2020-05-07T01:23:52Z",
    "closed_at": "2020-05-07T04:56:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1749",
    "body": "I want to implement GAN by pytorch-lightning, but I not found the demo.\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1749/comments",
    "author": "as754770178",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-07T01:24:29Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-07T04:56:45Z",
        "body": "the readme has a GAN demo on the first few lines..."
      },
      {
        "user": "Leerw",
        "created_at": "2020-09-30T06:52:31Z",
        "body": "Hi, how to train discriminator k times and train generator 1 time in training step? I have found no such demo and do not know how to implement it"
      }
    ]
  },
  {
    "number": 1726,
    "title": "Learning rate finder crashes if accumulate_grad_batches is not set to 1",
    "created_at": "2020-05-04T08:57:20Z",
    "closed_at": "2020-05-13T18:40:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1726",
    "body": "I'm not sure if it is expected behavior or a bug, but when I'm trying to find a learning rate like this:\r\n```python\r\ntrainer = pl.Trainer(gpus = [1], accumulate_grad_batches=8)\r\nlr_finder = trainer2.lr_find(model,min_lr = 1e-8, max_lr = 1e-1, num_training = 300)\r\n```\r\nIt throws an error `AttributeError: 'NoneType' object has no attribute 'item'`, which happens on the line 335 of lr_finder.py : `current_loss = trainer.running_loss.last().item()`\r\n\r\nWhen I remove  `accumulate_grad_batches=8` everything works as expected\r\nIf it is expected behavior, I suggest implementing a more expressive error message",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1726/comments",
    "author": "RafailFridman",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-04T10:33:13Z",
        "body": "Just to be sure, is it an typing error that the trainer that gets initialized is called `trainer` and the trainer that gets used with learning rate finder is called `trainer2`, or is it two different trainers?"
      },
      {
        "user": "RafailFridman",
        "created_at": "2020-05-04T10:37:53Z",
        "body": "@SkafteNicki yeah, sorry, I just tried different trainers and copied the wrong one.\r\nCan you please check on your side if this error exists?"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-04T10:52:13Z",
        "body": "This is very strange because the `accumulate_grad_batches` variable are override by the learning rate finders own argument `num_accumulation_steps` while it is running. I will look into whats coursing this error.\r\n\r\nJust to be sure, do you want to accumulate gradients during the learning rate finder or is it just for later fitting?"
      },
      {
        "user": "RafailFridman",
        "created_at": "2020-05-04T15:44:51Z",
        "body": "I want to accumulate batches in training, so I suppose I should set `accumulate_grad_batches` parameter as in the training phase. Do I understand this wrong?"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-04T19:32:11Z",
        "body": "No, nothing wrong with your understanding of the code. I have found a solution to the problem and will create a PR soon. "
      },
      {
        "user": "florisdf",
        "created_at": "2020-05-11T10:18:50Z",
        "body": "I'm having the same error. Any solutions ready to be pulled in?"
      },
      {
        "user": "jopo666",
        "created_at": "2020-05-11T10:58:22Z",
        "body": "Just use the `num_accumulation_steps` option used by the learning rate finder for now.\r\n\r\n```\r\ntrainer = pl.Trainer(gpus=1, accumulate_grad_batches=1)\r\nlr_finder = trainer.lr_find(model, num_accumulation_steps=8)\r\n```\r\n\r\n[solution doesn't work]"
      },
      {
        "user": "alexstoken",
        "created_at": "2020-05-11T20:57:48Z",
        "body": "@jopo666 @florisdf I do not think that will solve the problem if the goal is to accumulate gradients during the `lr_find` experiment. The `global_step` of the trainer, which only iterates when the learning rate is updated, runs every batch during the `lr_find` experiment, regardless of the `num_accumulate_steps`. This number resets itself after the finder is done running, but adding a print statement to line 434 or line 471 of training_loop.py will show that the learning rate (and the gradients) are updated every batch. \r\n\r\nTested on a nightly from last week. "
      }
    ]
  },
  {
    "number": 1708,
    "title": "example of doing simple inference with lightning",
    "created_at": "2020-05-02T22:10:01Z",
    "closed_at": "2020-07-11T00:04:23Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1708",
    "body": "I have an existing model where I load some pre-trained weights and then do inference (one image at a time) in pytorch. I am trying to basically convert it to a pytorch lightning module and am confused about a few things.\r\n\r\nSo currently, my `__init__` method for the model looks like this:\r\n\r\n    self._load_config_file(cfg_file)\r\n    # just creates the pytorch network\r\n    self.create_network()  \r\n    \r\n    self.load_weights(weights_file)\r\n    \r\n    self.cuda(device=0)  # assumes GPU and uses one. This is probably suboptimal\r\n    self.eval()  # inference mode\r\n\r\nWhat I can gather from the lightning docs, I can pretty much do the same, except not to do the `cuda()` call. So something like:\r\n\r\n    self.create_network()\r\n    \r\n    self.load_weights(weights_file)\r\n    self.freeze()  # inference mode\r\n    \r\nSo, my first question is whether this is the correct way to use lightning? How would lightning know if it needs to use the GPU? I am guessing this needs to be specified somewhere.\r\n\r\nNow, for the inference, I have the following setup:\r\n\r\n    def infer(frame):\r\n        img = transform(frame)  # apply some transformation to the input\r\n        img = torch.from_numpy(img).float().unsqueeze(0).cuda(device=0)\r\n        with torch.no_grad():\r\n            output = self.__call__(Variable(img)).data.cpu().numpy()\r\n        return output\r\n\r\nThis is the bit that has me confused. Which functions do I need to override to make a lightning compatible inference?\r\n\r\nAlso, at the moment, the input comes as a numpy array. Is that something that would be possible from the lightning module or do things always have to use some sort of a dataloader? \r\n\r\nAt some point, I want to extend this model implementation to do training as well, so want to make sure I do it right but while most examples focus on training models, a simple example of just doing inference at production time on a single image/data point might be useful. \r\n\r\nI am using 0.7.5 with pytorch 1.4.0 on GPU with cuda 10.1\r\n\r\nThank you for your work and patience with this newbie questions.\r\n    ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1708/comments",
    "author": "pamparana34",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-02T22:10:38Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-01T23:12:48Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "hminle",
        "created_at": "2020-08-14T16:27:47Z",
        "body": "Hi @pamparana34 , do you find a way to do simple inference with lightning?"
      }
    ]
  },
  {
    "number": 1707,
    "title": "Empty GPU memory cache after Jupyter notebook interrupted",
    "created_at": "2020-05-02T20:17:46Z",
    "closed_at": "2020-05-04T20:03:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1707",
    "body": "## ❓ Questions and Help\r\nI have used one of my GPUs for feature learning on a remote Jupyter Notebook. However the learning processing was interrupted and the network was cut off before I was able to leave the notebook properly, which resulting in the memory cache still lives in that particular GPU, as shown in `nividia-smi`.\r\n\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  GeForce RTX 208...  Off  | 00000000:41:00.0 Off |                  N/A |\r\n| 30%   28C    P8    22W / 250W |   8525MiB / 11019MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\nI do have another ongoing training process which uses another GPU. My question is how do I empty the memory cache from the unused GPU w/o affecting the ongoing process? Just to clarify, there is no process using the GPU with the memory cache from the notebook. \r\n\r\nI have tried\r\n\r\n```\r\nIn [10]: with torch.cuda.device('cuda:7'):\r\n    ...:     torch.cuda.empty_cache()\r\n```\r\n\r\nBut it did not work.\r\n\r\nThis is on a computing cluster with Ubuntu 18. I installed pytorch with pip and the version is 1.3.1 because the I am cuda 10. \r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1707/comments",
    "author": "bwang482",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-02T20:23:27Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      }
    ]
  },
  {
    "number": 1706,
    "title": "Model checkpoint and restore via GCS on Ai platform",
    "created_at": "2020-05-02T18:30:16Z",
    "closed_at": "2021-07-06T15:12:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1706",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI am training models on Google's ai platform and every checkpointing or restoration is done via Google Cloud Storage (not using pytorch-lightning for that yet). I noticed that pytorch-lightning uses `torch.save`, `torch.load` and `io` operations to work with saving and loading, which do not handle GCS by default (although I could pass file-like object to torch.save/load). Is it possible to extend the library so that is works with GCS without forking and modifying the code directly? The most important component for me is checkpoint save and restore on GCS. Is it a matter of Trainer class, Callbacks or model hooks?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1706/comments",
    "author": "dkajtoch",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-02T18:30:52Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "dkajtoch",
        "created_at": "2020-05-02T20:50:45Z",
        "body": "For the future release, my idea is that of all the necessary classes inherit from a basic `UniversalIO` class that implements atomic operations such as open, save, isdir, isfile, makedir etc. then it would be easy to define own storage backend. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-01T22:12:50Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "kaushikb11",
        "created_at": "2021-07-06T15:12:37Z",
        "body": "Tests have been added for the GCS filesystem in #7946. Closing this issue!"
      }
    ]
  },
  {
    "number": 1626,
    "title": "How can I write the validation and training losses from the progress bar to disk?",
    "created_at": "2020-04-26T22:02:31Z",
    "closed_at": "2020-07-07T12:42:22Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1626",
    "body": "Is there a way to output the results of the progress bar to file? Or will I have to write my own simple logger for the same? I want to use this as a quick sanity check before resorting to other more complex loggers.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1626/comments",
    "author": "zeeshansayyed",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-26T22:03:09Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-29T11:33:19Z",
        "body": "Hi\r\nAnywhere in your LightningModule you can access the progress_bar_dict like so:\r\n`self.trainer.progress_bar_dict\r\n`\r\nThen you can format it the way you like and print it to a file, for example in one of the hooks. I wouldn't do it in training loop though, as it could slow things down. If you do it there, maybe only print every 100 iterations or so."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-29T11:35:19Z",
        "body": "The loggers though are very easy to use. \r\nNote that the progress_bar values are smoothed over the epoch. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-28T12:17:51Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1594,
    "title": "How to resolve DeprecationWarning ",
    "created_at": "2020-04-24T15:43:11Z",
    "closed_at": "2020-04-24T18:45:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1594",
    "body": "Version: 0.7.3\r\nMy trainer\r\n```\r\ntrainer = pl.Trainer(\r\n    max_epochs=19\r\n)\r\n```\r\n\r\nWarnings:\r\n```\r\npytorch_lightning/utilities/warnings.py:18: DeprecationWarning: Argument `show_progress_bar` is now set by `progress_bar_refresh_rate` since v0.7.2 and this method will be removed in v0.9.0\r\n```\r\n\r\n```\r\nDeprecationWarning: Epochs indexing of `scheduling` starts from \"1\" until v0.6.x, but will start from \"0\" in v0.8.0.\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1594/comments",
    "author": "phihung",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-24T15:43:52Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-24T15:48:31Z",
        "body": "@Borda "
      },
      {
        "user": "Borda",
        "created_at": "2020-04-24T15:55:05Z",
        "body": "The indexing is there always and I ll check the other one which is not correct... "
      },
      {
        "user": "Borda",
        "created_at": "2020-04-24T16:59:54Z",
        "body": "there is another deprecated warning just after import:\r\n```\r\nfrom pytorch_lightning import Trainer\r\n/home/jb/.local/lib/python3.6/site-packages/gevent/monkey.py:685: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\r\n  patch_all_args = getargspec(patch_all)[0] # pylint:disable=deprecated-method\r\n```"
      }
    ]
  },
  {
    "number": 1581,
    "title": "How to count training batches with support for distributed training",
    "created_at": "2020-04-23T19:13:30Z",
    "closed_at": "2020-07-02T01:55:48Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1581",
    "body": "I am trying to write minimal code to track the total number of training batches seen so far in the logs for validation.\r\n\r\nFor non-distributed training, I simply add a `training_batches_so_far` variable in my lightning module init, increment it on `training_step()` and add it to the `progress_bar` and `log` fields in the output. \r\n\r\nHowever I want to make sure I am doing this properly for distributed training. What is the simplest way to do this? Ideally, I would like to be able to control how various metrics are accumulated (sum, avg, max). In this case, the amalgamation would be to sum the training steps seen by each worker and add that to the central total. I found related issues #702 and #1165, but it is unclear to me what the simplest / best practice is for this.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1581/comments",
    "author": "mikerossgithub",
    "comments": [
      {
        "user": "mikerossgithub",
        "created_at": "2020-04-23T21:25:44Z",
        "body": "I thought I had this figured out by accumulating batch counts in `training_epoch_end()` -- however this is called after the validation, meaning my validation epoch did not have access to the total train batches. Any help would be appreciated.\r\n\r\nMy goal here is to just write simple code that properly accumulates batch counts regardless of what type of distributed training I am using. I'm sure pytorch lightning makes this simple, but I am having a difficult time figuring out exactly where to do the increments and accumulations."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-24T01:17:56Z",
        "body": "since all  the processes across gpus have to be in sync, isn't the  total batch count the count from one gpu * num_gpus?\r\n\r\nie: what you did counting in training_step makes sense, but now multiply by world size (gpus * num_nodes)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-23T01:52:20Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1536,
    "title": "Pruning gradients before Optimizer_step",
    "created_at": "2020-04-20T17:51:48Z",
    "closed_at": "2020-06-28T19:17:50Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1536",
    "body": "I am working on a project that involves pruning out certain weights in the Neural Network.\r\nI want to know how I can set specific gradient weights to zero before the optimizer performs backpropagation.\r\n\r\nI have been able to acheive this in regular Pytorch by using the following code. \r\n\r\n`loss.backwards()`\r\n`for name, p in self.named_parameters():`\r\n`   if 'weight' in name:`\r\n`        grad_tensor = p.grad.data.cpu()`\r\n        `name = name.split(\".\")[:-1]`\r\n        `name = \".\".join(name)`\r\n        `multiplier = mask[name].cpu()`\r\n        `grad_tensor = grad_tensor * multiplier`\r\n        `p.grad.data = grad_tensor`\r\n`optimizer.step()`\r\n\r\n\r\n\r\nI was told that I need to override the backward_step?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1536/comments",
    "author": "vkrishnamurthy11",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-20T17:52:26Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-19T18:11:55Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1514,
    "title": "Hide default pl flags with -h?",
    "created_at": "2020-04-17T09:10:54Z",
    "closed_at": "2020-06-26T10:21:19Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1514",
    "body": "Is there a way to hide the pl flags? As it become super unreadable when using `-h` (pl has a lot of unused flags)\r\n\r\ne.g. given this snippet :\r\n```\r\nif __name__ == '__main__':\r\n    parser = ArgumentParser(add_help=False)\r\n    parser = Trainer.add_argparse_args(parser)\r\n\r\n    # figure out which model to use\r\n    parser.add_argument('--model_name', type=str, default='gan', help='gan or mnist')\r\n    temp_args = parser.parse_known_args()[0]\r\n\r\n    parser.add_argument('--something_important_and_unique', type=str, default='123', help='123 or 456')\r\n\r\n    # let the model add what it wants\r\n    if temp_args.model_name == 'gan':\r\n        parser = GoodGAN.add_model_specific_args(parser)\r\n    elif temp_args.model_name == 'mnist':\r\n        parser = LitMNIST.add_model_specific_args(parser)\r\n\r\n    args = parser.parse_args()\r\n\r\n    # train\r\n    main(args)\r\n```\r\nand running \r\n` python train.py -h`\r\nThis is the output:\r\n```\r\n(general)~\\laplace_pool_final>python test_something.py -h\r\nusage: test_something.py [-h] [--logger LOGGER]\r\n                         [--checkpoint_callback CHECKPOINT_CALLBACK]\r\n                         [--early_stop_callback EARLY_STOP_CALLBACK]\r\n                         [--default_save_path DEFAULT_SAVE_PATH]\r\n                         [--gradient_clip_val GRADIENT_CLIP_VAL]\r\n                         [--process_position PROCESS_POSITION]\r\n                         [--num_nodes NUM_NODES] [--gpus GPUS]\r\n                         [--num_tpu_cores NUM_TPU_CORES]\r\n                         [--log_gpu_memory LOG_GPU_MEMORY]\r\n                         [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE]\r\n                         [--overfit_pct OVERFIT_PCT]\r\n                         [--track_grad_norm TRACK_GRAD_NORM]\r\n                         [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\r\n                         [--fast_dev_run FAST_DEV_RUN]\r\n                         [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\r\n                         [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS]\r\n                         [--max_steps MAX_STEPS] [--min_steps MIN_STEPS]\r\n                         [--train_percent_check TRAIN_PERCENT_CHECK]\r\n                         [--val_percent_check VAL_PERCENT_CHECK]\r\n                         [--test_percent_check TEST_PERCENT_CHECK]\r\n                         [--val_check_interval VAL_CHECK_INTERVAL]\r\n                         [--log_save_interval LOG_SAVE_INTERVAL]\r\n                         [--row_log_interval ROW_LOG_INTERVAL]\r\n                         [--distributed_backend DISTRIBUTED_BACKEND]\r\n                         [--precision PRECISION]\r\n                         [--print_nan_grads PRINT_NAN_GRADS]\r\n                         [--weights_summary WEIGHTS_SUMMARY]\r\n                         [--weights_save_path WEIGHTS_SAVE_PATH]\r\n                         [--amp_level AMP_LEVEL]\r\n                         [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]\r\n                         [--truncated_bptt_steps TRUNCATED_BPTT_STEPS]\r\n                         [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\r\n                         [--benchmark BENCHMARK]\r\n                         [--reload_dataloaders_every_epoch RELOAD_DATALOADERS_EVERY_EPOCH]\r\n                         [--model_name MODEL_NAME]\r\n                         [--something_important_and_unique SOMETHING_IMPORTANT_AND_UNIQUE]\r\n                         [--encoder_layers ENCODER_LAYERS]\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  --logger LOGGER       autogenerated by pl.Trainer\r\n  --checkpoint_callback CHECKPOINT_CALLBACK\r\n                        autogenerated by pl.Trainer\r\n  --early_stop_callback EARLY_STOP_CALLBACK\r\n                        autogenerated by pl.Trainer\r\n  --default_save_path DEFAULT_SAVE_PATH\r\n                        autogenerated by pl.Trainer\r\n  --gradient_clip_val GRADIENT_CLIP_VAL\r\n                        autogenerated by pl.Trainer\r\n  --process_position PROCESS_POSITION\r\n                        autogenerated by pl.Trainer\r\n  --num_nodes NUM_NODES\r\n                        autogenerated by pl.Trainer\r\n  --gpus GPUS           autogenerated by pl.Trainer\r\n  --num_tpu_cores NUM_TPU_CORES\r\n                        autogenerated by pl.Trainer\r\n  --log_gpu_memory LOG_GPU_MEMORY\r\n                        autogenerated by pl.Trainer\r\n  --progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE\r\n                        autogenerated by pl.Trainer\r\n  --overfit_pct OVERFIT_PCT\r\n                        autogenerated by pl.Trainer\r\n  --track_grad_norm TRACK_GRAD_NORM\r\n                        autogenerated by pl.Trainer\r\n  --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH\r\n                        autogenerated by pl.Trainer\r\n  --fast_dev_run FAST_DEV_RUN\r\n                        autogenerated by pl.Trainer\r\n  --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES\r\n                        autogenerated by pl.Trainer\r\n  --max_epochs MAX_EPOCHS\r\n                        autogenerated by pl.Trainer\r\n  --min_epochs MIN_EPOCHS\r\n                        autogenerated by pl.Trainer\r\n  --max_steps MAX_STEPS\r\n                        autogenerated by pl.Trainer\r\n  --min_steps MIN_STEPS\r\n                        autogenerated by pl.Trainer\r\n  --train_percent_check TRAIN_PERCENT_CHECK\r\n                        autogenerated by pl.Trainer\r\n  --val_percent_check VAL_PERCENT_CHECK\r\n                        autogenerated by pl.Trainer\r\n  --test_percent_check TEST_PERCENT_CHECK\r\n                        autogenerated by pl.Trainer\r\n  --val_check_interval VAL_CHECK_INTERVAL\r\n                        autogenerated by pl.Trainer\r\n  --log_save_interval LOG_SAVE_INTERVAL\r\n                        autogenerated by pl.Trainer\r\n  --row_log_interval ROW_LOG_INTERVAL\r\n                        autogenerated by pl.Trainer\r\n  --distributed_backend DISTRIBUTED_BACKEND\r\n                        autogenerated by pl.Trainer\r\n  --precision PRECISION\r\n                        autogenerated by pl.Trainer\r\n  --print_nan_grads PRINT_NAN_GRADS\r\n                        autogenerated by pl.Trainer\r\n  --weights_summary WEIGHTS_SUMMARY\r\n                        autogenerated by pl.Trainer\r\n  --weights_save_path WEIGHTS_SAVE_PATH\r\n                        autogenerated by pl.Trainer\r\n  --amp_level AMP_LEVEL\r\n                        autogenerated by pl.Trainer\r\n  --num_sanity_val_steps NUM_SANITY_VAL_STEPS\r\n                        autogenerated by pl.Trainer\r\n  --truncated_bptt_steps TRUNCATED_BPTT_STEPS\r\n                        autogenerated by pl.Trainer\r\n  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\r\n                        autogenerated by pl.Trainer\r\n  --benchmark BENCHMARK\r\n                        autogenerated by pl.Trainer\r\n  --reload_dataloaders_every_epoch RELOAD_DATALOADERS_EVERY_EPOCH\r\n                        autogenerated by pl.Trainer\r\n  --model_name MODEL_NAME\r\n                        gan or mnist\r\n  --something_important_and_unique SOMETHING_IMPORTANT_AND_UNIQUE\r\n                        123 or 456\r\n  --encoder_layers ENCODER_LAYERS\r\n```\r\n\r\nWhere I would've preferred only to see the flags I've really created (model and `something_important_and_unique `)",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1514/comments",
    "author": "dvirginz",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-17T09:11:31Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "dvirginz",
        "created_at": "2020-04-17T10:28:26Z",
        "body": "If accepted, I can make a pr.\r\nBasically allowing `conflict_handler='resolve'` and `help=argparse.SUPPRESS`\r\nfor the `Trainer.add_argparse_args(parser` functionality "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-18T09:44:29Z",
        "body": "You should just not do `Trainer.add_argparse_args(parser)` right? Skip that and it will only add your args from model. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-17T09:45:22Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1497,
    "title": "Should the Trainer attributes be protected?",
    "created_at": "2020-04-15T07:05:40Z",
    "closed_at": "2020-10-29T06:10:41Z",
    "labels": [
      "feature",
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1497",
    "body": "## ❓ Why are the Trainer attributes not protected?\r\n\r\nSorry if this was already discussed before, but I was wondering.\r\nCurrently, the following code works and doesn't complain:\r\n\r\n```python\r\ntrainer = Trainer()\r\ntrainer.checkpoint_callback = 'bad manners'\r\n```\r\nIs there a reason why attributes like these are not protected? For example this\r\n\r\n```python\r\nclass Trainer:\r\n    def __init__(self):\r\n        self._checkpoint_callback = ...\r\n\r\n    @property\r\n    def checkpoint_callback:\r\n        return self._checkpoint_callback\r\n```\r\n\r\nwould make the attribute _read only_ and prevent anyone from accidentally changing the state of The trainer. \r\n\r\nIs the callback system possibly the reason for this, so that the callback method has full access to trainer variables? If so, I feel a property + setter approach would still be better. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1497/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-04-17T20:40:50Z",
        "body": "I agree that some shall be protected as well as some methods which are not meant to be used by user shall be protected too..."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-16T20:53:47Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "Evpok",
        "created_at": "2020-06-17T21:57:21Z",
        "body": "This is marked as wontfix so it probably won't be done, but for the record that would be a very unusual behavior for Python, “we are all consenting adults here” and I don't think that a lib going out of its way to do that kind of thing would be a good thing."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-17T22:00:01Z",
        "body": "totally agree :) \r\nthe bot added this wontfix label and wanted to close the issue due to inactivity. \r\nthese robots... always trying to take over the world"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-18T07:29:50Z",
        "body": "> This is marked as wontfix so it probably won't be done\r\n\r\nsorry for the confusion, it was an automatic bot marking non-active issues...\r\nAgree with @awaelchli we ll do it, just did not have time yet :rabbit: \r\n@Evpok mind take it over and draft a PR with these changes?"
      },
      {
        "user": "Evpok",
        "created_at": "2020-06-18T07:52:54Z",
        "body": "Well, I was trying to argue that it should *not* be done, since it adds unnecessary complexity and makes it harder to monkey patch if needed :-)"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-18T08:13:54Z",
        "body": "the point of protected is to clearly separate what is the recommended API and what is internal staff so you can still use it but to be aware that you are using something that is meant to be just internal PL use and you need to know how to use it correctly... so it does not add any complexity, all functions/classes stay as they are just minor rename with `_`"
      },
      {
        "user": "Evpok",
        "created_at": "2020-06-18T21:21:39Z",
        "body": "Right, that makes sense, sorry for being dense :-)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-17T22:11:57Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T02:24:23Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 1474,
    "title": "Customizing hparams after loading checkpoint",
    "created_at": "2020-04-13T17:39:15Z",
    "closed_at": "2020-05-13T08:27:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1474",
    "body": "## ❓ Questions and Help\r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n#### What is your question?\r\nI'm wondering what the best practice for loading a model with different hparams than what is stored in the checkpoint?\r\n\r\nI realize I could just load the model and set them afterwards e.g.: \r\n```\r\nmodel = model.load_from_checkpoint(args.checkpoint_file) # Load model\r\n\r\n# Set hparams etc..\r\nmodel.hparams.arg1 = 0.0\r\nmodel.hparams.arg2 = 1.0\r\n \r\n```\r\nBut the problem is that my  model __init__ function depends on the hparams arg1 and arg2 so they're set too late.\r\n\r\nI could also do \r\n```\r\ncheckpoint = torch.load(args.checkpoint_file)\r\ncheckpoint['hparams']['arg1'] = 0.0\r\ncheckpoint['hparams']['arg2'] = 1.0\r\nmodel = model._load_state_dict(checkpoint)\r\n```\r\n\r\nThe problem here is that i'm using the protected function _load_state_dict. Is there another way of solving this that i've missed? Or could we consider making _load_state_dict public?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1474/comments",
    "author": "tullie",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-04-16T03:06:48Z",
        "body": "@tullie good question, maybe we can have a flag for disabling hparam use?\r\n```python\r\nload_from_checkpoint(PATH, auto_hparam=False, arg1=my_arg, arg2=my_arg2, hparam={...})\r\n```\r\n\r\nload_from_checkpoint currently allows passing in the args directly as shown above\r\n```python\r\nload_from_checkpoint(PATH, auto_hparam=False, arg1=my_arg, arg2=my_arg2)\r\n```\r\nso, in this case an arg (hparam) would be a regular arg which you could then construct to be whatever you want.\r\n\r\nAlternative 2:\r\nWe add a ```hparam_updates``` arg which sets those updates in the hparams\r\n\r\n```python\r\nload_from_checkpoint(PATH, hparam_updates={'arg1': 0.0, 'arg2': 0.0})\r\n```"
      },
      {
        "user": "tullie",
        "created_at": "2020-04-21T15:35:09Z",
        "body": "I've been playing around with both these options and have most preferred alternative 2.\r\nI'll send out a PR soon. "
      }
    ]
  },
  {
    "number": 1437,
    "title": "Strange behaviour when using Module Lists",
    "created_at": "2020-04-09T21:53:00Z",
    "closed_at": "2020-04-11T13:08:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1437",
    "body": "Upon starting my training pipeline I got the follow summary of my model where I've made heavy use of module lists\r\n  | Name                 | Type       | Params\r\n------------------------------------------------\r\n0 | Shared_Layers        | ModuleList | 8 K   \r\n1 | Shared_Layers.0      | Linear     | 8 K   \r\n2 | Pooled_Layers        | ModuleList | 0     \r\n3 | Decode_Layers        | ModuleList | 73 K  \r\n4 | Decode_Layers.0      | Linear     | 73 K  \r\n5 | Classification_Layer | Linear     | 257 \r\n\r\nI'm quite concerned about what these .0's mean and why Pooled layers in particular lacks a .0 in there. Any assistance in knowing if this means there is a problem or not would be appreciated.\r\n\r\nthe init for the module is below\r\n\r\n    def __init__(self, config):\r\n        super(InfoMax, self).__init__()\r\n        self.config = config\r\n        self.af = Act_Dict[self.config[\"Network Architecture\"][\"Activation Function\"]]\r\n        self.network_config = self.config[\"Network Architecture\"]\r\n        self.bottleneck = self.network_config['Pooling_Layers'][-1]\r\n        self.max_dim = self.network_config['Shared_Layers'][-1]\r\n        self.start_dim = 32\r\n        self.Shared_Layers = nn.ModuleList(torch.nn.Linear(i, o, 1) for i, o in pairwise([self.start_dim, *self.network_config['Shared_Layers']]))\r\n        self.Pooled_Layers = nn.ModuleList(torch.nn.Linear(i, o, 1) for i, o in pairwise(self.network_config['Pooling_Layers']))\r\n        self.Decode_Layers = nn.ModuleList(torch.nn.Linear(i, o, 1) for i, o in pairwise([self.bottleneck+self.start_dim, *self.network_config['Discrimination_Layers']]))\r\n        self.Classification_Layer = torch.nn.Linear(self.Decode_Layers[-1].__dict__['out_features'], 1)         \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1437/comments",
    "author": "mm04926412",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-09T21:53:41Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-10T15:27:40Z",
        "body": "This happens when you set `weights_summary='full'` in the Trainer?"
      },
      {
        "user": "mm04926412",
        "created_at": "2020-04-11T10:14:34Z",
        "body": "I haven't set weights_summary = 'full' in the trainer no"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-11T13:02:47Z",
        "body": "I cannot reproduce what you show. I made a quick test on current master branch and get the expected output:\r\n```\r\n  | Name     | Type        | Params\r\n-------------------------------------\r\n0 | c_d1     | Linear      | 39 M  \r\n1 | mylist   | ModuleList  | 600 K \r\n2 | mylist.0 | BatchNorm1d | 100 K \r\n3 | mylist.1 | Dropout     | 0     \r\n4 | mylist.2 | Linear      | 500 K \r\n```\r\nCould you share more of your code, or even better, a colab notebook? "
      },
      {
        "user": "mm04926412",
        "created_at": "2020-04-11T13:08:06Z",
        "body": "I've discovered the source of the error is my poor use of the \"pairwise function\", I've fixed it - thank you for your time!"
      }
    ]
  },
  {
    "number": 1429,
    "title": "CUDA - numpy related copy error?",
    "created_at": "2020-04-09T14:32:21Z",
    "closed_at": "2020-04-09T16:36:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1429",
    "body": "```\r\nimport os\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\nfrom sklearn.metrics import accuracy_score\r\nfrom pytorch_lightning import Trainer\r\nfrom argparse import ArgumentParser\r\n\r\nimport pytorch_lightning as pl\r\n\r\ninput_channel = 1\r\nout_channel = 64\r\nclass MNISTModel(pl.LightningModule):\r\n\r\n    def __init__(self,parameters):\r\n        super(MNISTModel, self).__init__()\r\n        # not the best model...\r\n        self.params = parameters\r\n        \r\n        self.conv1 = torch.nn.Conv2d(in_channels=1,out_channels=10,kernel_size=3)\r\n        self.pool1 = torch.nn.MaxPool2d(2)\r\n\r\n        self.conv2 = torch.nn.Conv2d(in_channels=10,out_channels=20,kernel_size=3)\r\n        self.pool2 = torch.nn.MaxPool2d(2)\r\n\r\n        self.conv3 = torch.nn.Conv2d(in_channels=20,out_channels=30,kernel_size=3)\r\n        self.dropout1 = torch.nn.Dropout2d(p=0.25) # probabilit 0.25\r\n\r\n        self.fc3 = torch.nn.Linear(30*3*3,270)\r\n        self.fc4 = torch.nn.Linear(270,26)\r\n\r\n        self.softmax = torch.nn.LogSoftmax(dim=1)\r\n\r\n\r\n    def forward(self, x):\r\n        # called with self(x)\r\n        x = F.relu(self.conv1(x))\r\n        x = self.pool1(x)\r\n\r\n        x = F.relu(self.conv2(x))\r\n        x = self.pool2(x)\r\n\r\n        x = F.relu(self.conv3(x))\r\n        x = self.dropout1(x)\r\n        x = x.view(-1, 30 * 3 * 3) \r\n        x = F.relu(self.fc3(x))\r\n        x = F.relu(self.fc4(x))\r\n        return self.softmax(x)\r\n\r\n\r\n\r\n    def training_step(self, batch, batch_nb):\r\n        # REQUIRED\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        tensorboard_logs = {'train_loss': loss}\r\n        return {'loss': loss, 'log': tensorboard_logs}\r\n\r\n    def validation_step(self, batch, batch_nb):\r\n        # OPTIONAL\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        logits = F.log_softmax(y_hat, dim=1)\r\n        preds = torch.topk(logits, dim=1, k=1)[1].view(-1)\r\n        accuracy = accuracy_score(y,  preds)\r\n\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return {'val_loss': loss,\r\n                'accuracy': torch.tensor(accuracy)}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        # OPTIONAL\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        tensorboard_logs = {'val_loss': avg_loss}\r\n        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\r\n\r\n    def test_step(self, batch, batch_nb):\r\n        # OPTIONAL\r\n        return self.validation_step(batch, batch_nb)\r\n\r\n\r\n    def test_epoch_end(self, outputs):\r\n        # OPTIONAL\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        accuracy = torch.stack([x['accuracy'] for x in outputs]).mean()\r\n\r\n        logs = {'test_loss': avg_loss, 'test_acc': accuracy}\r\n        return {'avg_val_loss': avg_loss,\r\n                'progress_bar': logs,\r\n                'log': logs}\r\n\r\n\r\n    def configure_optimizers(self):\r\n        # REQUIRED\r\n        # can return multiple optimizers and learning_rate schedulers\r\n        # (LBFGS it is automatically supported, no need for closure function)\r\n        return torch.optim.Adadelta(self.parameters(), lr=self.params.lr)\r\n\r\n\r\n    def train_dataloader(self):\r\n        # REQUIRED\r\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32,num_workers=4)\r\n\r\n    def val_dataloader(self):\r\n        # OPTIONAL\r\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32,num_workers=4)\r\n\r\n    def test_dataloader(self):\r\n        # OPTIONAL\r\n        return DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=32,num_workers=4)\r\n\r\nif __name__ == '__main__':\r\n    args = ArgumentParser()\r\n    args.add_argument('--final_dim', type=int, default=128)\r\n    args.add_argument('--lr', type=float, default=0.02)\r\n    params = args.parse_args()\r\n   \r\n    model = MNISTModel(params)\r\n    trainer = Trainer(weights_save_path=os.getcwd(),gpus=1)\r\n    trainer.fit(model)\r\n\r\n```\r\n\r\nSorry its not exactly a snippet but not sure what seems to be the problem\r\nPyTorch version is 1.1.0\r\nTorchVision version is 0.3.0\r\nPytorch Lightning version is 0.7.2\r\nUbuntu 18.4 LTS\r\nPython version is 3.7.3\r\nCUDA Version is 10.2(Nvidia-smi , NVIDIA gt 840m 2GB VRAM)\r\n\r\nI can't find what seems to be problem\r\n\r\n\r\n```\r\nValidation sanity check:   0%|                                                                                                                                                        | 0/5 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"mni.py\", line 124, in <module>\r\n    trainer.fit(model)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 705, in fit\r\n    self.single_gpu_train(model)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 476, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 844, in run_pretrain_routine\r\n    False)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 262, in _evaluate\r\n    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 451, in evaluation_forward\r\n    output = model.validation_step(*args)\r\n  File \"mni.py\", line 69, in validation_step\r\n    accuracy = accuracy_score(y,  preds)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\", line 176, in accuracy_score\r\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\", line 72, in _check_targets\r\n    type_true = type_of_target(y_true)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/sklearn/utils/multiclass.py\", line 247, in type_of_target\r\n    if is_multilabel(y):\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/sklearn/utils/multiclass.py\", line 138, in is_multilabel\r\n    y = np.asarray(y)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 85, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/torch/tensor.py\", line 458, in __array__\r\n    return self.numpy()\r\nTypeError: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1429/comments",
    "author": "ugurkanates",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-09T14:33:04Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-09T16:36:05Z",
        "body": "accuracy_score is an SKLEARN function... it operates on numpy arrays. \r\nthe y in validation_step is a torch.Tensor...\r\n\r\nanything that goes in the accuracy_score function must be ONLY numpy arrays.\r\n\r\nbut this will slow your code a ton... instead calculate the accuracy using pure pytorch"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-09T16:36:40Z",
        "body": "we are working on a metrics package to do this, but in the meantime you can do the implementation manually"
      },
      {
        "user": "ugurkanates",
        "created_at": "2020-04-09T17:20:21Z",
        "body": "Cool for now I'm turning data to CPU with \".cpu()\"\r\n\r\nBut thanks for follow up"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-09T17:22:44Z",
        "body": "yes but every .cpu will make your code VERY slow..."
      }
    ]
  },
  {
    "number": 1384,
    "title": "Passing the early_stop_callback to the Trainer",
    "created_at": "2020-04-05T21:23:43Z",
    "closed_at": "2020-04-05T23:40:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1384",
    "body": "I am trying to pass my earlystopping callback to the trainer but I can't figure out how to do so. The trainer is initialized in my def(main) and the arguments are being passed to it using the argument parser. \r\n\r\nIf the parser only accepts arguments of type bool, int and str, how can I pass it this callback:\r\n\r\nearly_stop_callback = EarlyStopping(monitor='avg_val_loss',patience=hparams.val_patience)\r\n\r\nEDIT: and just callbacks in general. Such as on_train_end and on_test_end, how can I add them to the trainer if I have set up the code in the following manner. \r\n\r\n#### Code\r\n\r\ndef main(hparams):\r\n\r\n\t# Initialise model \r\n\tmodel = NFLRush(hparams)\r\n\r\n\t# Set early stopping \r\n\t#early_stop_callback = EarlyStopping(monitor='avg_val_loss',patience=hparams.val_patience)\r\n\r\n\t# Set trainer\r\n\ttrainer = pl.Trainer.from_argparse_args(hparams)\r\n\r\n\t# Train model \r\n\ttrainer.fit(model)\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n\t# Data import details\r\n\tfileDir = os.path.dirname(os.path.abspath(__file__))   # Directory of the Module\r\n\trootDir = os.path.dirname(fileDir)                   # Directory of the Module directory\r\n\tmodelDir = str(rootDir + '/Models/')\r\n\tlogDir = modelDir+'lightning_logs/'\r\n\r\n\tparser = ArgumentParser()\r\n\r\n\t# Trainer\r\n\tparser.add_argument('--min_epochs', type=int, default=1)\r\n\tparser.add_argument('--max_epochs', type=int, default=5)\r\n\tparser.add_argument('--overfit_pct', type=int, default=0)\r\n\tparser.add_argument('--default_save_path', type=str, default=modelDir)\r\n\r\n\t# Add hyperparameters\r\n\thparams = parser.parse_args()\r\n\r\n\t# Train\r\n\tmain(hparams)\r\n\r\n#### What have you tried?\r\n\r\nI can obviously contstruct my Trainer call differently as such:\r\n\ttrainer = pl.Trainer(experiment= exp, max_epochs=hparams.max_epochs, overfit_pct = hparams.overfit_pct, reload_dataloaders_every_epoch = True, early_stop_callback=early_stop_callback, default_save_path=hparams.default_save_path, gpus=0)\r\n\r\nbut this still does not let me activate and deactivate the callback using the previous structure. I guess I kind of solved my problem for myself.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1384/comments",
    "author": "danielgreff",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-05T21:24:27Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      }
    ]
  },
  {
    "number": 1344,
    "title": "Running multiGPU test(): one GPU overflows",
    "created_at": "2020-04-02T14:47:10Z",
    "closed_at": "2020-04-09T10:09:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1344",
    "body": "## ❓ Questions and Help\r\n\r\nHi, everyone\r\nThank you all for your contribution to such great project!\r\n\r\n#### What is your question?\r\n\r\nDuring my experiments I've faced an issue that I cannot overcome and find any information on this.\r\n\r\nI've trained some net and for now I would like to perform a test of this model. I've defined `test_step`, `test_end` and run my test on multiGPU. \r\nWhat I see is that on of three devices overflows with values and if I use `preds.cpu()` I get an error.\r\n\r\n```\r\n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/parallel/_functions.py\", line 54, in forward\r\n    assert all(map(lambda i: i.is_cuda, inputs))\r\nAssertionError\r\n```\r\n\r\nI would like to know how to preserve GPU memory from accumulating values from previous batches, or how to flush it to RAM after `test_step`\r\n\r\n\r\n#### Code\r\n\r\n```\r\nclass Net(pl.LightningModule):\r\n    def __init__(self, hparams):\r\n        super(Net, self).__init__()\r\n        self.hparams = hparams\r\n        self.submodule = pretrainedmodels.se_resnet50(\r\n            num_classes=1000, pretrained='imagenet'\r\n        )\r\n        self.submodule.last_linear = nn.Linear(\r\n            self.submodule.last_linear.in_features, self.hparams.num_categories\r\n        )\r\n\r\n    def forward(self, x):\r\n        return self.submodule(x)\r\n    \r\n    def test_step(self, test_batch, batch_idx):\r\n        x = test_batch['image']\r\n        logits = self.forward(x)\r\n        preds = torch.log_softmax(logits, dim=1)\r\n       \r\n        return {'batch_preds': preds.cpu()}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        return {'all_preds': outputs}\r\n\r\nmodel = Net.load_from_checkpoint(PATH)\r\n    model.freeze()\r\n\r\n    trainer = pl.Trainer(\r\n        gpus=[0,1,3], \r\n    )\r\n\r\n    # print(model.data_test[0])\r\n    trainer.test(model)\r\n``` \r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1344/comments",
    "author": "zetyquickly",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-02T14:47:47Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "zetyquickly",
        "created_at": "2020-04-04T08:25:30Z",
        "body": "So, the solution is to flush tensors to RAM in `test_step_end` when whole batch is gathered. \r\n```\r\n    def test_step(self, test_batch, batch_idx):\r\n        x, ids = test_batch['image'], test_batch['id']\r\n        logits = self.inference(x)\r\n        preds = F.softmax(logits, dim=1)\r\n        return {'batch_preds': preds, 'batch_ids': ids}\r\n\r\n    def test_step_end(self, outputs):\r\n        preds = outputs['batch_preds'].cpu()\r\n        ids = outputs['batch_ids'].cpu()\r\n        return {'batch_preds': preds, 'batch_ids': ids}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        all_preds = torch.cat([x['batch_preds'] for x in outputs], axis=0)\r\n        all_ids = torch.cat([x['batch_ids'] for x in outputs], axis=0)\r\n        self.preds = [all_preds, all_ids]\r\n        return {}\r\n```\r\nBut here is the question: `test_step_end` is now deprecated and will be removed in version `1.0`, how do we expected to achieve same functionallity only with `test_step` and `test_epoch_end`?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-04T12:25:51Z",
        "body": "@zetyquickly mind submitting a PR? @Borda mind helping complete?\r\n\r\n```test_step_end``` still exists.\r\n\r\n```python\r\ntest_step\r\ntest_step_end\r\ntest_epoch_end\r\n```"
      },
      {
        "user": "zetyquickly",
        "created_at": "2020-04-05T00:30:01Z",
        "body": "My bad. There is no issue \r\n\r\n`test_end` is deprecated. `test_step_end` still exists and hasn't deprecated. \r\nAnd documentation on `test_step_end` says \"Use this when testing with dp or ddp2 because test_step will operate on only part of the batch\" which is precisely my case\r\n\r\nWhat kind of changes in PR do you expect here?"
      }
    ]
  },
  {
    "number": 1310,
    "title": "How to properly move submodules to GPU?",
    "created_at": "2020-03-30T22:04:42Z",
    "closed_at": "2020-03-31T16:35:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1310",
    "body": "I've coded up a TransformerEncoder that relies on submodules. Specifically, I have a main lightning module (MainTransfomer.py) which has 2 sub (regular torch)modules. 1 is BertModel and 1 is a custom TransformerEncoderLayer. \r\n\r\nThe TransformerEncoderLayer has 2 submodules of its own named MultiHeadAttn and PosFeedForward. When I try to run my code via the lightning trainer, it seems that all my tensors/params are being moved to GPU except for when it tries to do operations within the MultiHeadAttn and PosFeedForward modules. Specifically it seems the weight matrix of a nn.Linear() within MultiHeadAttn is still sitting on cpu. \r\n\r\nSo my question boils down to what is the correct way to ensure all submodules are moved to the (correct) GPU within the lightning framework? In normal PyTorch I would just explicitly call .to(device) but I've read that it is not recommended to do this within lightning. \r\n\r\nif I explicitly set .to(device) on my TransformerEncoderLayer in my MainTransformerEncoder __init__ I don't run into this issue. \r\n\r\nI can supply code if need be, but the general setup is as described above. Main lightning module inits a torch module, which itself has 2 sub torch modules (attn & ffnn). The attn and ffnn modules don't seem to be moved to GPU by the lightning trainer.\r\n\r\nEnv:\r\n - Ubuntu16.04\r\n - conda/py3.8\r\n - Lightning v0.7.1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1310/comments",
    "author": "MatthewMatero",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-30T22:05:26Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-31T03:33:39Z",
        "body": "Strange. Lightning should move all your modules and submodules to the GPU given they are LightningModules or are part of one. To identify and fix a potential bug we would need a minimal example. Could you post it as a google colab notebook? That would be super helpful."
      },
      {
        "user": "MatthewMatero",
        "created_at": "2020-03-31T16:18:27Z",
        "body": "EDIT: The issue is resolved. I was generating my TransformerEncoderLayers as a regular python list and forgot to wrap it in nn.ModuleList() thus PyTorch/Lightning were blind to them being parameters. \r\n"
      }
    ]
  },
  {
    "number": 1225,
    "title": "How to log hparams to Tensorboard?",
    "created_at": "2020-03-24T19:09:25Z",
    "closed_at": "2020-03-29T14:27:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1225",
    "body": "Hello! I'm trying to view my hparams on tensorboard, but can't actually see them there. As I understood from documentation, to log hparams one should add self.hparams in the __init__ of the LightningModule. Here's what I'm doing:\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n    def __init__(self,hparams):\r\n        super().__init__()\r\n        self.hparams =  Namespace(**{'learning_rate': hparams.learning_rate,\r\n                                    'batch_size':hparams.batch_size,\r\n                                    'normalize_embeddings':hparams.normalize_embeddings,\r\n                                    'normalize': hparams.normalize,\r\n                                    'k_neighbors':hparams.k_neighbors,\r\n                                    'melspec_dir':hparams.melspec_dir})\r\n```\r\nMy hparams also contain Trainer hparams, so to log only the right ones, I wrote this workaround. But anyway, I could not see any of my hparams. What could be the problem?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1225/comments",
    "author": "RafailFridman",
    "comments": [
      {
        "user": "mRcSchwering",
        "created_at": "2020-03-24T22:04:38Z",
        "body": "Hi,\r\n\r\nI am struggling with hparams since 2 days now too.\r\nI do this:\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n\r\n    def __init__(self, hparams: dict):\r\n        super(MyModule, self).__init__()\r\n        self.hparams = hparams\r\n\r\n```\r\nThere might be some magic going on when the module is loaded.\r\nDid you try defining your `Namespace` outside, then passing it as `hparams`?"
      },
      {
        "user": "RafailFridman",
        "created_at": "2020-03-25T07:26:42Z",
        "body": "Thanks for the reply!\r\n> Did you try defining your `Namespace` outside, then passing it as `hparams`?\r\n\r\nhparams, that I pass to __init__ of my module is already a Namespace that contains many hyperparameters, including those that I want to log in tensorboard."
      },
      {
        "user": "mRcSchwering",
        "created_at": "2020-03-25T09:11:49Z",
        "body": "I never tried it with a namespace, I always use dicts.\r\nDid you try it with a dict?"
      },
      {
        "user": "RafailFridman",
        "created_at": "2020-03-25T09:16:00Z",
        "body": "No, I didn't\r\nAccording to #651 hparams should be a Namespace object, so I assume that will not work, but I'll check."
      },
      {
        "user": "mRcSchwering",
        "created_at": "2020-03-25T13:57:02Z",
        "body": "#1228 is not related. That is about reporting metrics such as validation loss together with the set of hyperparameters."
      },
      {
        "user": "RafailFridman",
        "created_at": "2020-03-26T10:01:23Z",
        "body": "Do you know when `self.logger.log_hyperparams(hparams)` is called? Does it happen at the end of training or every epoch?"
      },
      {
        "user": "mRcSchwering",
        "created_at": "2020-03-26T10:16:54Z",
        "body": "when you overwrite it with a `print` you can see that it gets called at the very beginning of the training (not training step or epoch, the whole training)."
      },
      {
        "user": "RafailFridman",
        "created_at": "2020-03-29T11:19:07Z",
        "body": "I'm still not able to log any hparams to Tensorboard, although I exactly follow the documentation and do this:\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n  def __init__(self, hparams):\r\n    super().__init__()\r\n    self.hparams = hparams\r\n```\r\n,where hparams, that I'm passing to init are `parser.parse_args()`\r\nDoes anybody have similar problems?"
      },
      {
        "user": "RafailFridman",
        "created_at": "2020-03-29T14:15:32Z",
        "body": "I tried explicitly add hparams to tb like this:\r\n```python\r\n self.logger.experiment.add_hparams(\r\n                                   hparam_dict= {'learning_rate': self.hparams.learning_rate},\r\n                                   metric_dict = dict())\r\n```\r\nBut that didn't work."
      },
      {
        "user": "RafailFridman",
        "created_at": "2020-03-29T14:27:34Z",
        "body": "Ok, I did it. It looks like that TensorBoard won't display hparams if some previous runs were without them. So I had to start a new tb process with a clean logs directory. \r\nEverything works normal now."
      },
      {
        "user": "lkhphuc",
        "created_at": "2020-04-08T13:00:40Z",
        "body": "Oh Jesus, I've been wasting so much time debugging Lightning. Thank @RafailFridman . "
      },
      {
        "user": "Skyy93",
        "created_at": "2020-12-15T08:50:45Z",
        "body": "@Borda \r\n\r\nExcuse me, could you please mention this behaviour in the docs? Thank you"
      },
      {
        "user": "Borda",
        "created_at": "2020-12-15T17:47:20Z",
        "body": "> Excuse me, could you please mention this behaviour in the docs? Thank you\r\n\r\nThat sounds as a good idea, mind send a PR with suggestion? "
      }
    ]
  },
  {
    "number": 1206,
    "title": "Allow custom scatter function in data parallel",
    "created_at": "2020-03-21T22:01:16Z",
    "closed_at": "2021-06-16T00:38:32Z",
    "labels": [
      "help wanted",
      "question",
      "won't fix",
      "strategy: dp (removed in pl)"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1206",
    "body": "## 🚀 Feature\r\nAllow custom scatter function to be passed in data parallel module.\r\n\r\n### Motivation\r\n\r\nIs there a way to customize scattering process in data parallel? My use case is that I have sparse tensors represented in COO format and they cannot be stored in a single tensor, but require a list to store. In this way, the built-in scatter cannot split the list properly, it tries to iterate over the list and split each of its item.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1206/comments",
    "author": "cmpute",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-06-08T23:46:47Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 1188,
    "title": "hparams need to allow None values",
    "created_at": "2020-03-18T18:17:23Z",
    "closed_at": "2020-04-03T05:33:54Z",
    "labels": [
      "help wanted",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1188",
    "body": "## 🐛 Bug\r\n\r\nI can't set `hparams.gpus` to `None`:\r\n\r\n```\r\nI0318 11:12:45.466972 15554 lightning_utils.py:182] <class '__main__.ResnetLightningExample'> hparams: Namespace(amp_level='O2', backend='', batch_size=16, debug_print_env=False, debug_skip_loaded_hparams_check=False, do_test=False, early_stop_metric='val_loss', early_stop_mode='min', early_stop_patience=10, enable_batch_size_scaling=True, enable_early_stop=False, gpus=None, learning_rate=0.01, max_epochs=1, min_epochs=1, model_load_checkpoint_path='', model_save_path='', nodes=1, use_amp=False)\r\nTraceback (most recent call last):\r\n  File \"/home/estevens/.cache/bazel/_bazel_estevens/7fa7f74bbe03dba6c4e36403df17d704/execroot/zoox/bazel-out/k8-py3-fastbuildcuda/bin/experimental/estevens/pytorch/lightning_resnet50.runfiles/zoox/experimental/estevens/pytorch/lightning_resnet50.py\", line 128, in <module>\r\n    ResnetLightningExample.init_from_cli(sys.argv[1:]).main()\r\n  File \"/home/estevens/.cache/bazel/_bazel_estevens/7fa7f74bbe03dba6c4e36403df17d704/execroot/zoox/bazel-out/k8-py3-fastbuildcuda/bin/experimental/estevens/pytorch/lightning_resnet50.runfiles/zoox/tflight/lightning_utils/lightning_utils.py\", line 255, in main\r\n    trainer.fit(self)\r\n  File \"/home/estevens/.cache/bazel/_bazel_estevens/7fa7f74bbe03dba6c4e36403df17d704/execroot/zoox/bazel-out/k8-py3-fastbuildcuda/bin/experimental/estevens/pytorch/lightning_resnet50.runfiles/pypi__pytorch_lightning_python3_deps/pytorch_lightning/trainer/trainer.py\", line 630, in fit\r\n    self.run_pretrain_routine(model)\r\n  File \"/home/estevens/.cache/bazel/_bazel_estevens/7fa7f74bbe03dba6c4e36403df17d704/execroot/zoox/bazel-out/k8-py3-fastbuildcuda/bin/experimental/estevens/pytorch/lightning_resnet50.runfiles/pypi__pytorch_lightning_python3_deps/pytorch_lightning/trainer/trainer.py\", line 748, in run_pretrain_routine\r\n    self.logger.log_hyperparams(ref_model.hparams)\r\n  File \"/home/estevens/.cache/bazel/_bazel_estevens/7fa7f74bbe03dba6c4e36403df17d704/execroot/zoox/bazel-out/k8-py3-fastbuildcuda/bin/experimental/estevens/pytorch/lightning_resnet50.runfiles/pypi__pytorch_lightning_python3_deps/pytorch_lightning/loggers/base.py\", line 18, in wrapped_fn\r\n    fn(self, *args, **kwargs)\r\n  File \"/home/estevens/.cache/bazel/_bazel_estevens/7fa7f74bbe03dba6c4e36403df17d704/execroot/zoox/bazel-out/k8-py3-fastbuildcuda/bin/experimental/estevens/pytorch/lightning_resnet50.runfiles/pypi__pytorch_lightning_python3_deps/pytorch_lightning/loggers/tensorboard.py\", line 113, in log_hyperparams\r\n    exp, ssi, sei = hparams(params, {})\r\n  File \"/home/estevens/.cache/bazel/_bazel_estevens/7fa7f74bbe03dba6c4e36403df17d704/execroot/zoox/bazel-out/k8-py3-fastbuildcuda/bin/experimental/estevens/pytorch/lightning_resnet50.runfiles/pypi__torch_python3_deps/torch/utils/tensorboard/summary.py\", line 156, in hparams\r\n    raise ValueError('value should be one of int, float, str, bool, or torch.Tensor')\r\nValueError: value should be one of int, float, str, bool, or torch.Tensor\r\n```\r\n\r\n### To Reproduce\r\n\r\nTo reproduce, have `parser.add_argument('--gpus', default=None, type=str)` and then don't give the `--gpus` CLI argument.\r\n\r\n### Expected behavior\r\n\r\nScreen out None values before sending them on.\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Ubuntun 14.04\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6.8\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration: 2080 Ti\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1188/comments",
    "author": "elistevens",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-03-18T21:21:33Z",
        "body": "this is set by the tensorboard, not lightning...\r\nWhy would you need to set `gpus` to NOne when NOne is the default value?\r\nWe did the conversion to primitives in #1130"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-03T02:51:46Z",
        "body": "I tested this and it is fixed on master. Hparams with unsupported types (such as None in your case) are converted to string and now appear in the hparams tab in Tensorboard."
      }
    ]
  },
  {
    "number": 1142,
    "title": "Race condition and repeated os.remove in load_spawn_weights",
    "created_at": "2020-03-13T16:22:29Z",
    "closed_at": "2020-05-22T04:53:54Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1142",
    "body": "In multi-gpu, multi-node training, after training is finished, the following error occurs:\r\n\r\n```0: Traceback (most recent call last):\r\n0:   File \".../estevens/pytorch/lightning_resnet50.py\", line 126, in <module>\r\n0:     ResnetLightningExample.init_from_cli(sys.argv[1:]).main()\r\n0:   File \"...//lightning_utils/lightning_utils.py\", line 215, in main\r\n0:     trainer.fit(self)\r\n0:   File \".../pytorch_lightning/trainer/trainer.py\", line 593, in fit\r\n0:     self.load_spawn_weights(model)\r\n0:   File \".../pytorch_lightning/trainer/distrib_data_parallel.py\", line 372, in load_spawn_weights\r\n0:     os.remove(path)\r\n0: FileNotFoundError: [Errno 2] No such file or directory: '/mnt/sun-pcs01/estevens/resnet50/__temp_weight_ddp_end.ckpt'\r\n```\r\n\r\nI think this is a similar issue to other `os.remove` bugs, but I'm worried that merely adding an `if rank == 0` will move the issue to a race condition (where rank 0 removes the file before other processes can load it).",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1142/comments",
    "author": "elistevens",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-03-13T16:46:05Z",
        "body": "this is to restore state when running in interactive modes (ie: colab or something).\r\n\r\nWhen running as a script, there's no need to do this.\r\n\r\nThe easiest solution is probably to only call this when on jupyter or colab?\r\nA more involved solution is making sure the barriers are set correctly?"
      },
      {
        "user": "elistevens",
        "created_at": "2020-03-13T17:00:55Z",
        "body": "Okay, it's being called from `trainer.fit`; are you saying the call needs to be removed from that function? Or that something is wrong with my script?"
      },
      {
        "user": "elistevens",
        "created_at": "2020-03-13T21:19:57Z",
        "body": "FYI, replacing my training with \r\n```python\r\nclass MonkeyPatchedTrainer(pl.Trainer):\r\n    def load_spawn_weights(self, original_model):\r\n        pass\r\n``` \r\nseems to train properly (at least, the exceptions are gone)."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-13T02:43:57Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1137,
    "title": "How to change between train and evaluate mode in forward method?",
    "created_at": "2020-03-13T08:56:34Z",
    "closed_at": "2020-03-14T01:03:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1137",
    "body": "I'm trying to use FasterRCNN from `torchvision.models.detection` which behaves in the following manner:\r\n-  During training, the model expects both the input tensors, as well as targets and will return a dict containing the classification and regression losses:\r\n```\r\nloss_dict = model(images, targets)\r\n```\r\n- During evaluation, the model requires only the input tensors, and returns the post-processed\r\n    predictions:\r\n```\r\npredictions = model(images)\r\n```\r\nHow can I toggle between these two modes in `def forward(self, images):` of a PyTorch Lightning system?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1137/comments",
    "author": "polars05",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-03-13T21:40:16Z",
        "body": "How about this:\r\n```\r\ndef forward(self, images, targets=None):\r\n    if targets is not None:\r\n        # Code for training\r\n        output = ...\r\n    else:\r\n        # Code for prediction\r\n        output = ...\r\n    return output\r\n```\r\nIn the if/else clause you could also dispatch to e.g. `self.predict(...)` and e.g. `self.forward_train(...)` to further break down the complexity of the forward pass.\r\n\r\nAt training time:\r\nIn `training_step` you always get images and targets from your training dataloader and call `forward(images, targets)`.\r\nAt test time:\r\nE.g. in `test_step`, you get only images from your dataloader or you could call directly your module with \r\n```\r\nmodel = CoolSystem(...)\r\npredictions = model(images)\r\n```"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-14T01:03:47Z",
        "body": "@awaelchli thx for help! @polars05 feel free to re-open if needed... :]"
      },
      {
        "user": "jzazo",
        "created_at": "2021-01-28T10:36:34Z",
        "body": "Can I trust model flags: `model.training` and `model.testing` to determine fit stage in forward method?\r\n\r\nI am using an unsupervised method, so I don't have labels to determine the stage. I still need to know what stage the model is in to perform masking to the input, for example, and calculate loss.\r\n\r\nEDIT: It seems `model.training` returns the eval mode, not the stage (\"fit\" or \"test\"). I wanted to distinguish validation stage from test, since I should compute the loss and mask at that stage. Is there any attribute I can use?\r\n\r\nEDIT2: `model.trainer.testing` has the value I am looking for, as it distinguishes between \"fit\" and \"test\" stages, and is `False` during training and validation. Let me know if you think of something better.\r\n\r\nThank you!"
      }
    ]
  },
  {
    "number": 1135,
    "title": "Colab TPU error",
    "created_at": "2020-03-13T05:45:22Z",
    "closed_at": "2020-04-16T07:26:35Z",
    "labels": [
      "question",
      "accelerator: tpu"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1135",
    "body": "I'm trying to run a LSTM model on TPU with colab. It throws me following error.\r\n```\r\nException in device=TPU:1: Aborted: Session 0275bc9f6430801b is not found.\r\nException in device=TPU:3: Aborted: Session 780bc43376b5f650 is not found.\r\nTraceback (most recent call last):\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 500, in tpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 500, in tpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 809, in run_pretrain_routine\r\n    False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 809, in run_pretrain_routine\r\n    False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 251, in evaluate\r\n    for batch_idx, batch in enumerate(dataloader):\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 251, in evaluate\r\n    for batch_idx, batch in enumerate(dataloader):\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py\", line 31, in __next__\r\n    return self.next()\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py\", line 31, in __next__\r\n    return self.next()\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py\", line 34, in next\r\n    xm.mark_step()\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py\", line 34, in next\r\n    xm.mark_step()\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/core/xla_model.py\", line 405, in mark_step\r\n    wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/core/xla_model.py\", line 405, in mark_step\r\n    wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))\r\nRuntimeError: Aborted: Session 0275bc9f6430801b is not found.\r\nRuntimeError: Aborted: Session 780bc43376b5f650 is not found.\r\nException in device=TPU:5: Aborted: Session e191a99b56d63c29 is not found.\r\nTraceback (most recent call last):\r\n```\r\nAnyone has any ideas what this could mean ? \r\nThanks in advance!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1135/comments",
    "author": "patil-suraj",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-13T05:46:02Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-18T22:04:30Z",
        "body": "@hadim @jeffling could you check pls?"
      },
      {
        "user": "Borda",
        "created_at": "2020-04-09T12:04:32Z",
        "body": "@patil-suraj may you pls share the example?"
      },
      {
        "user": "patil-suraj",
        "created_at": "2020-04-16T06:10:24Z",
        "body": "I tried to run it with version 0.7.3 and it ran without any error :)"
      }
    ]
  },
  {
    "number": 1134,
    "title": "No such file or directory  '.../epoch=0.ckpt'",
    "created_at": "2020-03-13T00:55:49Z",
    "closed_at": "2020-03-19T02:37:00Z",
    "labels": [
      "help wanted",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1134",
    "body": "## 🐛 Bug \r\n\r\nAfter finishing the first epoch I see the error message. I'm using `0.7.1`\r\n\r\n### To Reproduce\r\nTrain using `0.7.1`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```\r\nTraceback (most recent call last):\r\n  File \"sparsenet_trainer.py\", line 115, in <module>\r\n    main(hparams)\r\n  File \"sparsenet_trainer.py\", line 67, in main\r\n    trainer.fit(model)\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 590, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException:\r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 342, in ddp_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 830, in run_pretrain_routine\r\n    self.train()\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 343, in train\r\n    self.run_training_epoch()\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 452, in run_training_epoch\r\n    self.call_checkpoint_callback()\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 737, in call_checkpoint_callback\r\n    self.checkpoint_callback.on_validation_end(self, self.get_model())\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 204, in on_validation_end\r\n    self._do_check_save(filepath, current, epoch)\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 221, in _do_check_save\r\n    self._del_model(delpath)\r\n  File \"/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 121, in _del_model\r\n    os.remove(filepath)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/data/Code/trec-2019-deep-learning/trec2019/model/sparsenet/default/version_5/checkpoints/epoch=0.ckpt'\r\n\r\n/home/kyoungrok/anaconda3/envs/trec/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 12 leaked semaphores to clean up at shutdown\r\n  len(cache))\r\n```\r\n\r\n### Environment\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration:\r\nGPU 0: TITAN V\r\nGPU 1: TITAN V\r\n\r\nNvidia driver version: 440.64\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] numpydoc==0.9.1\r\n[pip] pytorch-lightning==0.7.1\r\n[pip] torch==1.4.0\r\n[pip] torchtext==0.5.0\r\n[pip] torchvision==0.5.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.7.1                    pypi_0    pypi\r\n[conda] torchtext                 0.5.0                    pypi_0    pypi\r\n[conda] torchvision               0.5.0                py37_cu101    pytorch\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1134/comments",
    "author": "kyoungrok0517",
    "comments": [
      {
        "user": "ghost",
        "created_at": "2020-03-13T00:59:01Z",
        "body": "If this error occurs when you're running in DDP mode the fix is in master. Refer to #1119."
      },
      {
        "user": "kyoungrok0517",
        "created_at": "2020-03-13T01:05:05Z",
        "body": "I’ll try that. Thanks!"
      }
    ]
  },
  {
    "number": 1127,
    "title": "How to log cur_val_loss to tensorboard in validation_step?",
    "created_at": "2020-03-12T16:05:53Z",
    "closed_at": "2020-06-01T23:02:25Z",
    "labels": [
      "question",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1127",
    "body": "I tried to get it to tensorboard this way\r\n```python\r\ndef validation_step(self, batch, batch_nb):\r\n        data, target= batch\r\n        output = self.forward(data)\r\n        val_loss = nn.MSELoss(reduction = \"mean\")(output, target)\r\n        return {'val_loss': val_loss, 'log':{'cur_val_loss':val_loss}}\r\n```\r\nbut that didn't work.\r\nHow can I monitor validation loss not only at the end of each epoch, but during validation?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1127/comments",
    "author": "RafailFridman",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-12T16:06:34Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-16T23:02:51Z",
        "body": "@jeffling @awaelchli pls ^^"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-16T23:22:53Z",
        "body": "I have never come across a situation where monitoring the validation loss this way is meaningful. What is your use case here? I am curious. \r\nThe reason why I say this is because the loggers are not really designed to do what you ask for (please correct me if I'm wrong). The loggers have a step that monotonically increases, it can mean training steps, epochs, or something else. But since the validation step is reset every epoch, it wouldn't make sense to use the same logger for training and validation. So I have don't have an answer for you, sorry.\r\n\r\nIs it not enough in your case to compute the mean or maybe the std for the validation loss?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-03-16T23:29:43Z",
        "body": "you can just call the logger yourself whenever... i assume it’s bc the val set is huge or something? either way we normally track the avg at the end of the val epoch. but if you need to do it intro epoch then \r\nself.logger.do_whatever_tensorboard_does"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-23T19:57:48Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "epignatelli",
        "created_at": "2020-07-25T16:28:20Z",
        "body": "Is there a counter that keeps track of the validation steps already performed? Or using `self.global_step` is the best option here?\r\n`self.logger.experiment.add_scalar(\"val_loss\", val_loss, self.global_step)`, or ..?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-25T16:31:54Z",
        "body": "yes! if you wait a day we’ll post docs on the new simple way of doing it :)"
      },
      {
        "user": "epignatelli",
        "created_at": "2020-07-25T16:59:11Z",
        "body": "Oh, thanks @williamFalcon, that's great! Is there a specific pr I can scrabble through in the meanwhile? "
      }
    ]
  },
  {
    "number": 1047,
    "title": "Exception in device=TPU:0",
    "created_at": "2020-03-05T12:40:11Z",
    "closed_at": "2020-03-06T23:54:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1047",
    "body": "This error occours when initiate the trainer\r\n\r\n```\r\nException in device=TPU:0: module 'torch_xla.core.xla_model' has no attribute 'rendezvous'\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 500, in tpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1162, in run_pretrain_routine\r\n    torch_xla.core.xla_model.rendezvous(\"pl.Trainer.run_pretrain_routine\")\r\nAttributeError: module 'torch_xla.core.xla_model' has no attribute 'rendezvous'\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1047/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-03-06T23:53:52Z",
        "body": "ummm.\r\n\r\nupdated to latest version and try again?\r\nmake sure you are using xla nightly"
      },
      {
        "user": "nikanton",
        "created_at": "2020-05-07T12:43:23Z",
        "body": "Faced with the same problem. \r\n```\r\n> Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 507, in tpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 792, in run_pretrain_routine\r\n    torch_xla.core.xla_model.rendezvous(\"pl.Trainer.run_pretrain_routine\")\r\nAttributeError: module 'torch_xla.core.xla_model' has no attribute 'rendezvous'\r\n```"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-07T12:44:59Z",
        "body": "need to use latest xla version"
      },
      {
        "user": "nikanton",
        "created_at": "2020-05-07T13:18:10Z",
        "body": "helped"
      }
    ]
  },
  {
    "number": 1002,
    "title": "Validation step isn't being ran ",
    "created_at": "2020-03-02T04:22:38Z",
    "closed_at": "2020-03-02T23:38:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1002",
    "body": "## ❓ Questions and Help\r\n\r\n#### What is your question?\r\nI have been trying to get the trainer to call the validation_step function but it doesn't seem to ever get called.  I assume I am missing something obvious but have looking at the tutorials and docs I haven't been able to find the obvious.  The code for the model and trainer are below.  What might I be missing?  Thank you for the help!\r\n\r\n#### Code\r\n```\r\nclass SegModel(pl.LightningModule):\r\n    def __init__(self, batch_size, lr):\r\n        super(SegModel, self).__init__()\r\n        self.batch_size = batch_size\r\n        self.learning_rate = lr\r\n        self.net = UNet(num_classes=1)\r\n        self.transform = transforms.Compose([\r\n            transforms.ToTensor()\r\n        ])\r\n        self.trainset = Stacker(input_images, truth_images, transform=self.transform)\r\n        self.validset = Stacker(input_images, truth_images, transform=self.transform)\r\n        self.testset = Stacker(input_images, truth_images, transform=self.transform)\r\n    \r\n    def forward(self, x):\r\n        return self.net(x)\r\n    \r\n    def training_step(self, batch, batch_nb):\r\n        img, mask = batch\r\n        img = img.float()\r\n        mask = mask.long()\r\n        out = self.forward(img)\r\n        loss_val = dice_loss(mask, out)\r\n        return {'loss': loss_val, 'log': {'train_loss': loss_val}}\r\n    \r\n    def validation_step(self, batch, batch_nb):\r\n        print(\"RUNNING VALIDATION\")\r\n        img, mask = batch\r\n        img = img.float()\r\n        mask = mask.long()\r\n        out = self.forward(img)\r\n        loss_val = dice_loss(mask, out)\r\n        return {'val_loss': loss_val, \r\n                'val_dice': dice(out, mask),\r\n                'val_iou': IoU(out, mask)\r\n               }\r\n    \r\n    def test_step(self, batch, batch_nb):\r\n        img, mask = batch\r\n        img = img.float()\r\n        mask = mask.long()\r\n        out = self.forward(img)\r\n        loss_val = dice_loss(mask, out)\r\n        return {'test_loss': loss_val, \r\n                'test_dice': dice(out, mask),\r\n                'test_iou': IoU(out, mask)\r\n               }\r\n    \r\n    def validation_end(self, outputs):\r\n        if len(outputs)==0: return {}\r\n        val_loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        val_dice_mean = torch.stack([x['val_dice'] for x in outputs]).mean()\r\n        val_iou_mean = torch.stack([x['val_iou'] for x in outputs]).mean()\r\n        return {'val_loss': val_loss_mean,\r\n                'log': {\r\n                    'val_loss': val_loss_mean,\r\n                    'val_dice': val_dice_mean,\r\n                    'val_iou': val_iou_mean\r\n                }}\r\n\r\n    def test_end(self, outputs):\r\n        if len(outputs)==0: return {}\r\n        test_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean()\r\n        test_dice_mean = torch.stack([x['test_dice'] for x in outputs]).mean()\r\n        test_iou_mean = torch.stack([x['test_iou'] for x in outputs]).mean()\r\n        print(test_dice_mean, test_iou_mean)\r\n        return {'test_loss': test_loss_mean,\r\n                'log': {\r\n                    'test_loss': test_loss_mean,\r\n                    'test_dice': test_dice_mean,\r\n                    'test_iou': test_iou_mean\r\n                }}\r\n    \r\n    def configure_optimizers(self):\r\n        opt = torch.optim.Adam(self.net.parameters(), lr=self.learning_rate)\r\n        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\r\n        return [opt], [sch]\r\n\r\n    @pl.data_loader\r\n    def train_dataloader(self):\r\n        return DataLoader(self.trainset, batch_size=self.batch_size, shuffle=True)\r\n\r\n    @pl.data_loader\r\n    def valid_dataloader(self):\r\n        return DataLoader(self.validset, batch_size=self.batch_size, shuffle=False)\r\n      \r\n    @pl.data_loader\r\n    def test_dataloader(self):\r\n        return DataLoader(self.testset, batch_size=self.batch_size, shuffle=False)\r\n\r\nmodel = SegModel(1, 0.001)\r\n\r\ntrainer = pl.Trainer(\r\n    gpus=[0], \r\n    early_stop_callback=None, \r\n    max_epochs=40,\r\n    check_val_every_n_epoch=1,\r\n)\r\n\r\ntrainer.fit(model)\r\n```\r\n\r\n#### What's your environment?\r\n\r\n - OS: Windows\r\n - Packaging: conda\r\n - Version: 0.6.1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1002/comments",
    "author": "RyMo95",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-02T04:23:15Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-02T22:04:51Z",
        "body": "Hi @RyMo95 , I believe the issue is that your method is not named correctly. \r\n\r\n`valid_dataloader` -> `val_dataloader` should resolve this, please let me know if that doesn’t work!"
      },
      {
        "user": "RyMo95",
        "created_at": "2020-03-02T23:38:10Z",
        "body": "@jeremyjordan Ah that was it, so careless! Appreciate the help :)"
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-03T01:11:13Z",
        "body": "no problem! :) "
      }
    ]
  },
  {
    "number": 970,
    "title": "Evaluation on subsampled training set during training",
    "created_at": "2020-02-27T23:31:46Z",
    "closed_at": "2020-02-28T21:16:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/970",
    "body": "## ❓ Questions and Help\r\n\r\n\r\n#### What is your question?\r\n\r\nWe can evaluate on the validation set to get validation accuracy etc. Is it possible to evaluate on a subsampled version of the training set as well, for example, the training accuracy? Is there an easy wa to do it with PL?\r\n\r\nSorry for the simple question, but I couldn't find the answer in the documentation.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/970/comments",
    "author": "zc-alexfan",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-02-27T23:32:28Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "festeh",
        "created_at": "2020-02-28T09:17:23Z",
        "body": "I'd do something like this:\r\n\r\n```\r\nclass MyCoolModule(LightningModule):\r\n    def __init__(self, ...):\r\n        self._train_loader = ...\r\n        ...\r\n\r\n    def on_epoch_end(self):\r\n        n_batches_to_eval = ...\r\n        results = self.trainer.evaluate(self, [self._train_loader], n_batches_to_eval)\r\n```\r\n"
      },
      {
        "user": "zc-alexfan",
        "created_at": "2020-02-28T21:13:25Z",
        "body": "Thanks for the reply. I was hoping something else. For example, the current `validation_step` runs in parallel with the training. Also, I need to manually subsample the training data, instead of specifying a fraction. Maybe I should change this post to a feature request. Thanks although."
      }
    ]
  },
  {
    "number": 877,
    "title": "How do I test before any training?",
    "created_at": "2020-02-17T06:34:46Z",
    "closed_at": "2020-02-17T09:40:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/877",
    "body": "## ❓ Questions and Help\r\n\r\nI am now migrating some of my previous works into lightning. I wish to see if it is able to reproduce my previous results or not. But the doc implies that all the testing has to be performed after training or after loading the previous lightning training state, which I do not have either.\r\n\r\nSo How can I test before training?\r\n\r\n#### Code\r\n\r\n```python\r\n\r\n    trainer = Trainer(logger=logger, max_epochs=5, gpus=[3], distributed_backend=None)\r\n    hparams = HParams(fold=fold, model=model_name, batch_size=8, num_workers=16)\r\n    system = MySYS(hparams, trainer)\r\n\r\n    system.model.load_state_dict(torch.load(state_dict))\r\n    trainer.test()\r\n```\r\nIt cannot work since the trainer does not initialize at all.\r\n\r\n#### What have you tried?\r\nI found inside the code for testing:\r\n```python\r\n\r\n    def test(self, model=None):\r\n        r\"\"\"\r\n\r\n        Separates from fit to make sure you never run on your test set until you want to.\r\n\r\n        Args:\r\n            model (LightningModule): The model to test.\r\n\r\n        Example::\r\n\r\n            # Option 1\r\n            # run test after fitting\r\n            trainer = Trainer()\r\n            model = LightningModule()\r\n\r\n            trainer.fit()\r\n            trainer.test()\r\n\r\n            # Option 2\r\n            # run test from a loaded model\r\n            model = LightningModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n            trainer = Trainer()\r\n            trainer.test(model)\r\n        \"\"\"\r\n        self.testing = True\r\n        if model is not None:\r\n            self.fit(model)\r\n        else:\r\n            self.run_evaluation(test=True)\r\n```\r\nWhich requires to fit the model that I do not understand at all. Why a fitting is required inside training code? If for the purpose of initialization, can't we just put some init code here?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/877/comments",
    "author": "shijianjian",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-02-17T06:35:28Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "festeh",
        "created_at": "2020-02-17T07:18:03Z",
        "body": "If you dig a bit deeper into the sources, you'll find that `fit` actually calls `evaluate` and do not fit a model if `testing` flag is True. So you can run `trainer.test(system)`, it should probably work."
      },
      {
        "user": "shijianjian",
        "created_at": "2020-02-17T09:40:20Z",
        "body": "Yes, it worked. Sorry for the dummy question."
      }
    ]
  },
  {
    "number": 811,
    "title": "Using other libraries with pytorch-lightning.",
    "created_at": "2020-02-10T22:35:33Z",
    "closed_at": "2020-02-11T17:30:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/811",
    "body": "I'm just wondering, would it be possible to use the AllenNLP/Texar-PyTorch models and data processing submodule as part of PyTorch-Lightning Trainer? Do you think using the class structure setting and the GPU training setting of PyTorch Lightning would be adaptable to AllenNLP modules?\r\nI saw that torchtext data handling approaches can be used with Lightning, if I was to call any Seq2Seq models and the data input created by using AllenNLP and then use all of pytorch lightning's approach for training on GPU and everything, would that port to AllenNLP?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/811/comments",
    "author": "ankitvad",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-02-11T17:30:39Z",
        "body": "Lightning doesn't abstract  pytorch.  As long as AllenNLP is using pure  pytorch modules  it should be fine!\r\n\r\nTry it out.\r\n\r\nCheck out the  colab on the readme  to see  how it works with transformers"
      }
    ]
  },
  {
    "number": 690,
    "title": "How to make test_end() return metrics ",
    "created_at": "2020-01-15T17:47:23Z",
    "closed_at": "2020-01-21T12:31:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/690",
    "body": "I have searched through the docs / Google as well as looked through the source code.\r\n\r\nIt seems like test_end() returns nothing (it has no `return` in the function). I was wondering if I was missing something really obvious. \r\n\r\nI would simply like to return the metrics of the test end.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/690/comments",
    "author": "Laksh1997",
    "comments": [
      {
        "user": "matthew-z",
        "created_at": "2020-01-19T14:09:06Z",
        "body": "You may try this:\r\n\r\n```py\r\nclass MyModel(pl.LightningModule):\r\n  def __init__(self, ...):\r\n    self.test_result = None\r\n  def test_end(self, outputs):\r\n    self.test_result = get_eval_metrics(outputs)\r\n\r\nmodel = MyModel()\r\ntrainer.test(model)\r\nprint(model.test_result)\r\n```\r\n\r\n"
      },
      {
        "user": "Laksh1997",
        "created_at": "2020-01-19T23:37:31Z",
        "body": "Thanks @matthew-z !"
      }
    ]
  },
  {
    "number": 672,
    "title": "Removing particular defaults from progress bar ",
    "created_at": "2020-01-08T18:10:36Z",
    "closed_at": "2020-01-21T02:15:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/672",
    "body": "Related to issue #629, since proposes to remove some default entries from the progress bar. Is there an existing way to remove entries from the tqdm_dict once the trainer is initialized? \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/672/comments",
    "author": "nihalsid",
    "comments": [
      {
        "user": "neggert",
        "created_at": "2020-01-21T02:15:03Z",
        "body": "Not at the moment. Most of the things you'd want to remove are added on the fly when the dict is retrieved."
      }
    ]
  },
  {
    "number": 655,
    "title": "How do you use the hidden state from the previous training step in the next step?",
    "created_at": "2020-01-02T00:06:32Z",
    "closed_at": "2020-01-21T12:35:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/655",
    "body": "I want to train an LSTM Language Model using Lightning, but I need to pass the previous step's hidden state to the next training step. The raw training loop looks something like this:\r\n\r\n```Python\r\nmodel.train()\r\nhidden = model.init_hidden(batch_size)\r\nfor data in train_iter:\r\n  text, target = data.text, data.target\r\n  model.zero_grad()\r\n  output, hidden = model(text, hidden)\r\n  hidden = detach_tensors(hidden)  # for truncated bptt\r\n  loss = criterion(output.view(-1, vocab_size), target.view(-1))\r\n  loss.backward()\r\n\r\n  optimizer.step()\r\n```\r\n\r\nHow can I achieve this with Lightning? Thanks!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/655/comments",
    "author": "n2cholas",
    "comments": [
      {
        "user": "BaohaoLiao",
        "created_at": "2020-01-15T14:47:29Z",
        "body": "I think you can define a variable in the model initialization for saving the hidden state of the previous step. For example:\r\nclass model(LightningModule):\r\n    def __init__(self, args):\r\n        super().__init__()\r\n        self.previous_hidden = None  # cache\r\n    def training_step():\r\n        output, hidden = model(text, self.previous_hidden)\r\n        self.previous_hidden = detach_tensors(hidden) \r\n        "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-01-21T12:35:18Z",
        "body": "Correct!"
      },
      {
        "user": "shivdhar",
        "created_at": "2020-08-11T23:03:33Z",
        "body": "Would it be fair to say that this advice has been superseded by the `truncated_bptt_steps` argument in `Trainer`?\r\n\r\nBecause this issue pops up as one of the first results for \"pytorch lightning bptt\" on Google, so a pointer to the updated method may be helpful to anyone stumbling upon this result like I did.\r\n\r\n"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-08-11T23:15:22Z",
        "body": "yes! 0.9.0 makes this very clear. check out the latest master docs"
      },
      {
        "user": "shivdhar",
        "created_at": "2020-08-12T00:05:31Z",
        "body": "Thanks for the clarity!"
      }
    ]
  },
  {
    "number": 637,
    "title": "tensorflow version",
    "created_at": "2019-12-18T20:24:39Z",
    "closed_at": "2020-01-21T02:37:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/637",
    "body": "Hi,\r\n\r\nI am getting the following error and I was wondering what tensorflow version is currently supported since I am using 1.11.0.\r\n\r\n`module 'tensorflow.io' has no attribute 'gfile'`\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/637/comments",
    "author": "vr25",
    "comments": [
      {
        "user": "neggert",
        "created_at": "2020-01-21T02:37:12Z",
        "body": "Tensorflow is no longer required. If you want to use the Tensorboard logger, you need `tensorboard>=1.14`, as specified in the requirements.txt file."
      },
      {
        "user": "Borda",
        "created_at": "2020-01-21T07:59:56Z",
        "body": "FTR, changed in #687"
      }
    ]
  },
  {
    "number": 636,
    "title": "simple text classification example with ptl",
    "created_at": "2019-12-18T18:30:42Z",
    "closed_at": "2020-01-21T12:37:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/636",
    "body": "Hi,\r\n\r\nThanks a lot for writing this useful wrapper and the documentation.\r\n\r\nI am currently working with text and the examples given in the documentation are related to images. I was wondering if some basic text-related PyTorch to PyTorch lightning framework could be provided in the doc too.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/636/comments",
    "author": "vr25",
    "comments": [
      {
        "user": "isspek",
        "created_at": "2019-12-19T00:12:10Z",
        "body": "I am interested in the example too. I couldn't figure out how to use dataloader with torchtext. It would be nice if there is a simple example for this purpose."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-01-21T12:37:28Z",
        "body": "The colab in the readme has a BERT example at the end."
      }
    ]
  },
  {
    "number": 632,
    "title": "Call or Forward?",
    "created_at": "2019-12-17T16:23:10Z",
    "closed_at": "2019-12-17T16:29:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/632",
    "body": "Hi, thanks for the nice library. In the `readme`, the example uses  `model.forward(x)` not `model(x)`. But wouldn't it usually recommended to use `model(x)` so that other things (hooks etc) can be, well, hooked as well? What's the best practice? ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/632/comments",
    "author": "keunwoochoi",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-12-17T16:29:11Z",
        "body": "forward should implement what you want to use when calling model(x). \r\n\r\nyou may need to call that in training step (usually do), which means you have to do self.forward(...) because you are in the model when you make that call. "
      }
    ]
  },
  {
    "number": 557,
    "title": "The gan template does not seem to set properly the gradients of the discriminator to zero",
    "created_at": "2019-11-28T14:16:20Z",
    "closed_at": "2020-01-22T13:15:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/557",
    "body": "In each iteration at the beginning of the discriminator training, the gradient is not set to zero. To investigate, just print the gradients of the discriminator after the line `if optimizer_i == 1:`.\r\nThe `optimizer.zero_grad()` for discriminator is then only called once all the gradients are accumulated, including the ones coming from the update of the generator.\r\n\r\nBy the way, it may be helpful to indicate somewhere that the script is performing simultaneous gradient descent instead of alternating updates.\r\n\r\nI am wondering if someone succeeds in training a GAN with this script because this does not seem to be possible to me. Thanks in advance for the clarification.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/557/comments",
    "author": "cyber-meow",
    "comments": [
      {
        "user": "kwanUm",
        "created_at": "2020-01-02T12:13:29Z",
        "body": "@cyber-meow  bumped into this as well. Does the discussion at #591 solves your issue?"
      },
      {
        "user": "cyber-meow",
        "created_at": "2020-01-22T13:15:32Z",
        "body": "Effectively this issue was addressed in #603 and some recent attempts have also been carried out to improve the solution (#712). Thank you for mentioning.  My own solution is pretty much similar to the modification of #603.\r\nAlthough I feel it is worth mentioning the templates is for simultaneous update, maybe it is trivial for most of the people working on this."
      }
    ]
  },
  {
    "number": 544,
    "title": "object has no attribute 'add_scalar'",
    "created_at": "2019-11-25T18:25:18Z",
    "closed_at": "2019-11-25T18:34:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/544",
    "body": "I'm trying to use the default logger to record scalars for tensorboard using add_scalar but I get:\r\n   \r\n self.logger.add_scalar('loss/train_loss', 42, 42)\r\nAttributeError: 'TestTubeLogger' object has no attribute 'add_scalar'\r\n\r\nThe docs say TestTubeLogger inherits from SummaryWriter so add_scalar should be ok.\r\nCan anyone help?\r\n\r\n#### What's your environment?   \r\n- conda version (no venv) : 4.7.12  \r\n- PyTorch version: 1.3.1\r\n- Lightning version: 0.5.3.2\r\n- Test-tube version: 0.7.3\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/544/comments",
    "author": "brucemuller",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-11-25T18:34:43Z",
        "body": "self.logger.experiment.add_scalar"
      },
      {
        "user": "SylarZiling",
        "created_at": "2022-11-11T00:46:58Z",
        "body": "> self.logger.experiment.add_scalar\r\n\r\nsorry I have similar problem, I used the experiment still tells me no attribute 'add_scalar\" \r\n\r\n    self.logger.experiment.add_scalar(f'metrics_{valset_idx}/{k}', v, global_step=cur_epoch)\r\nAttributeError: 'list' object has no attribute 'add_scalar'\r\n\r\nand this is experiment  [<torch.utils.tensorboard.writer.SummaryWriter object at 0x7f3af4baf990>]"
      }
    ]
  },
  {
    "number": 495,
    "title": "K-fold cross-validation",
    "created_at": "2019-11-12T02:04:31Z",
    "closed_at": "2019-12-04T12:34:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/495",
    "body": "I could not find any ways to do k-fold cross-validation. Do we have to put an outer loop over trainer with different dataset splits for manual k-fold cross-validation?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/495/comments",
    "author": "jaivardhankapoor",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-12-04T12:34:01Z",
        "body": "@jaivardhankapoor you would have to handle this in the dataloader or add an outer loop."
      }
    ]
  },
  {
    "number": 488,
    "title": "Running Average of my accuracy, losses etc. ",
    "created_at": "2019-11-10T05:16:24Z",
    "closed_at": "2019-11-11T22:41:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/488",
    "body": "#### What is your question?    \r\nI want my tqdm logger to show me a history of my training on the terminal. Right now, when a epoch ends, all data for it is scrubbed from the command line and the new epoch data is shown.\r\n\r\nAlso I want to see the running accuracy of my network and a running average of my loss on the tqdm bar. How should I go on about doing that ?\r\n#### What have you tried?    \r\nI have looked at the docs and logging but am unable to figure out how to modify the tqdm logger, more so maintain a running average\r\n#### What's your environment?   \r\n\r\n- conda version: latest\r\n- PyTorch version   : 1.3\r\n- Lightning version : pip install pytorch-lightning at the date of this issue  \r\n- Test-tube version: came boot strapped with lightning.\r\n\r\nI installed everything on the date of this issue.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/488/comments",
    "author": "karanchahal",
    "comments": [
      {
        "user": "s-rog",
        "created_at": "2019-11-11T01:36:35Z",
        "body": "Use tensorboard!\r\n\r\nFor running averages you have to implement the logic in training step"
      },
      {
        "user": "karanchahal",
        "created_at": "2019-11-11T22:41:27Z",
        "body": "Thank you, yes I've figured it out now !"
      }
    ]
  },
  {
    "number": 477,
    "title": "About the Weight Initialization in PL",
    "created_at": "2019-11-08T02:28:25Z",
    "closed_at": "2019-11-08T13:53:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/477",
    "body": "Hi,\r\n\r\nI am tring to use BERT for a project. The pretrained BERT model is part of my model. I am wondering how will PL initialize the model weights. Will it overwrite the pretrained BERT weights?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/477/comments",
    "author": "magic282",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-11-08T02:33:44Z",
        "body": "lightning doesn’t do any magic like this under the hood. you control all the weights and what gets initiated "
      },
      {
        "user": "magic282",
        "created_at": "2019-11-08T04:36:13Z",
        "body": "I see. So where should I do the weight initialization step if I want to follow the PL design idea? In the `__init__` of `pl.LightningModule`?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-11-08T13:53:42Z",
        "body": "This is up to you and you should follow standard PyTorch guidelines.\r\nNormally it's done in ```__init__```"
      }
    ]
  },
  {
    "number": 450,
    "title": "Unfreezing layers during training?",
    "created_at": "2019-11-02T03:49:48Z",
    "closed_at": "2020-01-21T12:45:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/450",
    "body": "Freezing layers at the beginning of training works, however unfreezing in ```on_epoch_start()``` during training causes the gradient to explode. Without the unfreezing part (or without freezing at all), the model trains fine with no gradient issues.\r\n\r\nI'm using DDP + Apex O2 and the loss scaling will keep going down to 0 where it would encounter 0 division and crash.\r\n\r\nIs unfreezing during training not possible in pytorch/lightning? or am I missing snippet?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/450/comments",
    "author": "s-rog",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-01-21T12:45:17Z",
        "body": "you can unfreeze whenever. if gradients explode it's for another reason"
      }
    ]
  },
  {
    "number": 267,
    "title": "How to use EarlyStopping and ModelCheckpoint",
    "created_at": "2019-09-29T19:30:26Z",
    "closed_at": "2019-10-01T10:23:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/267",
    "body": "The `logs` passed into the `on_epoch_end` method of both ModelCheckpoint and EarlyStopping seems to have the `loss` field set to a string, which leads to exception when comparing with the last best result. Following codes in trainer.py,\r\n```\r\n 407     @property\r\n 408     def __training_tqdm_dict(self):\r\n 409         tqdm_dict = {\r\n 410             'loss': '{0:.3f}'.format(self.avg_loss),\r\n 411             'epoch': '{}'.format(self.current_epoch),\r\n 412             'batch_nb': '{}'.format(self.batch_nb),\r\n 413         }\r\n```\r\nHow should I use the ModelCheckpoint and EarlyStopping, is adding my own field in `progress` field when returning from `training_step` the way to go? What is the purpose of converting to string?\r\n\r\n#### What's your environment?   \r\n- conda version (no venv)   4.7.12\r\n- PyTorch version   1.2.0\r\n- Lightning version   0.5.0\r\n- Test-tube version  0.7.1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/267/comments",
    "author": "hrukalive",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-09-29T20:51:10Z",
        "body": "tqdm_dict is strictly for logging. To use early stopping or checkpointing:\r\n\r\n1. tell the checkpoint what you want to monitor (ie: ```your_acc_or_something```).\r\n2. In training_step return something like:\r\n```\r\nreturn {'loss':some_loss, progress:{'your_acc_or_something': someValue}}\r\n```"
      }
    ]
  },
  {
    "number": 212,
    "title": "How to get current LR within lightningModule",
    "created_at": "2019-09-06T22:35:24Z",
    "closed_at": "2019-09-06T22:40:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/212",
    "body": "I'd like to log LR throughout the training process in tensorboard.  \r\nAs I understand it, that's as simple as adding it to `update_tng_log_metrics` function or directly adding metrics to test_tube.Experiment.\r\n\r\nBut I can't find where to get the optimizer/scheduler info from within the lightningModule.  How could I access this information?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/212/comments",
    "author": "shoarora",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-09-06T22:40:02Z",
        "body": "self.trainer.lr_schedulers"
      },
      {
        "user": "bparaj",
        "created_at": "2023-02-14T13:47:27Z",
        "body": "For pytorch-lightning version `1.9.0`, I am getting this error:\r\n\r\n```\r\nAttributeError: 'Trainer' object has no attribute 'lr_schedulers'\r\n```"
      },
      {
        "user": "Serious-H",
        "created_at": "2023-03-11T10:05:27Z",
        "body": "Me too. Have you solved it @bparaj "
      },
      {
        "user": "bparaj",
        "created_at": "2023-03-11T10:23:30Z",
        "body": "Assume I have `configure_optimizers()` defined as:\r\n\r\n```python3\r\n    def configure_optimizers(*args, **kwargs):\r\n        optimizer = torch.optim.Adam(model.parameters(), **config[\"optimizer\"])\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.95, step_size=5)\r\n        return [optimizer], [lr_scheduler]\r\n```\r\n\r\nI then used the following to access the last learning rate:\r\n\r\n```python3\r\nlr = trainer.lr_scheduler_configs[0].scheduler.get_last_lr()[0]\r\n```"
      }
    ]
  },
  {
    "number": 188,
    "title": "'tng_loss' vs 'loss' in in training_step",
    "created_at": "2019-09-03T08:00:05Z",
    "closed_at": "2019-09-03T10:35:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/188",
    "body": "### Before asking:   \r\n- [x] 1. search the issues.  \r\n- [x] 2. search the docs.    \r\n\r\nIf you still can't find what you need:     \r\n#### What is your question?    \r\n\r\n**What is the difference between `tng_loss` and `loss` in in `training_step`?** \r\nIt seems both are displayed in my progress bar although only `tng_loss` is part of `prog` (unless `loss` comes from somewhere else but I don't think I specified it somewhere). But `tng_loss` jumps around alot more. My guess is `tng_loss` (or whatever I specify in `prog` is per batch). If this is correct the main question is, what is `loss` in the output, is it the same as specified in `training_step` and if yes, why is it different from `tng_loss` in the displayed output and why is it displayed although it is not part of `prog`?\r\n\r\n#### Code    \r\n\r\nMy `training_step` based on the the documentation example:\r\n\r\n```\r\ndef training_step(model, data_batch, batch_nb):\r\n    input, label = data_batch\r\n\r\n    out = model.forward(input)\r\n    loss = model.loss(out, label)\r\n\r\n    output = {\r\n        'loss': loss, # required\r\n        'prog': {'tng_loss': loss, 'batch_nb': batch_nb} # optional\r\n    }\r\n    # return a dict\r\n    return output\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/188/comments",
    "author": "expectopatronum",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-09-03T10:35:13Z",
        "body": "loss and tng_loss are the same here. anything in the ‘prog’ key of the dict is for progress bar display only. \r\n\r\nthe output[“loss”] will show as a running average on the display bar (it’ll jump less)"
      }
    ]
  },
  {
    "number": 19938,
    "title": "`grep: Invalid option -- P` when running `./tests/run_standalone_tests.sh` on macOS",
    "created_at": "2024-06-03T16:43:17Z",
    "closed_at": "2024-06-05T16:32:57Z",
    "labels": [
      "bug",
      "help wanted",
      "tests",
      "ver: 2.2.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19938",
    "body": "### Bug description\n\nBSD `grep` no longer supports the `-- P` option on macOS.\r\n\r\nOne fix would be to use GNU `grep` instead, as `ggrep`.\n\n### What version are you seeing the problem on?\n\nmaster\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @borda",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19938/comments",
    "author": "Peiffap",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-06-03T23:43:37Z",
        "body": "Hey @Peiffap \r\nYou are welcome to send a PR for this if you'd like. \r\nWe don't have any tests in `run_standalone_tests.sh` that would run on macos, but for completeness, this fix would be good IMO. "
      }
    ]
  },
  {
    "number": 19454,
    "title": "Input validation for Trainer's `min_time` and `max_time` when passed as string",
    "created_at": "2024-02-12T18:29:52Z",
    "closed_at": "2024-02-24T00:47:24Z",
    "labels": [
      "feature",
      "help wanted",
      "callback: timer"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19454",
    "body": "### Description & Motivation\r\n\r\nThe argument to the `Trainer`s min_time is expected to be a string of the form /(\\d\\d):(\\d\\d):(\\d\\d):(\\d\\d)/. One way in which parsing can fail (simple example: `--min_time 60`) throws an `IndexError` and provides little context for what is wrong.\r\n\r\n### Pitch\r\n\r\nThis is an uninformative error message, and managed to briefly stump me and a colleague. It should be caught and converted into an informative one that explains the expected format.\r\n\r\nThis should probably be done at the API level rather than becoming something specific to CLI parsing. That is, since one can provide a string to this API, it should catch this error and instead raise an error with a name like `TimeParsingError` or something of that nature.\r\n\r\n### Alternatives\r\n\r\nUse `datetime.timedelta.strptime` or other standard-library elements to do the parsing, and catch the `ValueError` and raise something more informative.\r\n\r\n### Additional context\r\n\r\nI am working on PyTorch-Lightning >=1.7.0,<2.0.0 because I haven't had the chance to migrate to `LightningCLI` and such, but a quick perusal suggests this is still an issue.\r\n\r\ncc @borda @carmocca @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19454/comments",
    "author": "kylebgorman",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2024-02-12T19:44:21Z",
        "body": "@kylebgorman Would you like to contribute this suggestion?"
      },
      {
        "user": "kylebgorman",
        "created_at": "2024-02-12T19:52:27Z",
        "body": "> @kylebgorman Would you like to contribute this suggestion?\r\n\r\nSure, it seems like a small issue. (I need to clear OSS patches with my employer, but I'll do that now.)"
      }
    ]
  },
  {
    "number": 19432,
    "title": "Regression: `CSVLogger` not working on version 2.2.0",
    "created_at": "2024-02-08T08:26:35Z",
    "closed_at": "2024-02-13T18:59:05Z",
    "labels": [
      "bug",
      "help wanted",
      "logger: csv",
      "ver: 2.2.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19432",
    "body": "### Bug description\n\n`CSVLogger` throws the following error when used in version 2.2.0:\r\n```\r\n  File \"/usr/lib/python3.10/csv.py\", line 157, in writerows\r\n    return self.writer.writerows(map(self._dict_to_list, rowdicts))\r\n  File \"/usr/lib/python3.10/csv.py\", line 149, in _dict_to_list\r\n    raise ValueError(\"dict contains fields not in fieldnames: \"\r\nValueError: dict contains fields not in fieldnames: 'train_hit_rate', 'train_precision'\r\n```\r\nGoing back to 2.1.4 solves the issue.\n\n### What version are you seeing the problem on?\n\nmaster\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n  File \"/usr/lib/python3.10/csv.py\", line 157, in writerows\r\n    return self.writer.writerows(map(self._dict_to_list, rowdicts))\r\n  File \"/usr/lib/python3.10/csv.py\", line 149, in _dict_to_list\r\n    raise ValueError(\"dict contains fields not in fieldnames: \"\r\nValueError: dict contains fields not in fieldnames: 'train_hit_rate', 'train_precision'\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n\t- GPU:               None\r\n\t- available:         False\r\n\t- version:           12.1\r\n* Lightning:\r\n\t- lightning:         2.2.0\r\n\t- lightning-utilities: 0.10.1\r\n\t- pytorch-lightning: 2.2.0\r\n\t- torch:             2.2.0\r\n\t- torch-geometric:   2.4.0\r\n\t- torchmetrics:      1.3.0.post0\r\n* Packages:\r\n\t- aiohttp:           3.9.3\r\n\t- aiosignal:         1.3.1\r\n\t- async-timeout:     4.0.3\r\n\t- attrs:             23.2.0\r\n\t- certifi:           2024.2.2\r\n\t- charset-normalizer: 3.3.2\r\n\t- classify-imports:  4.2.0\r\n\t- cloudpickle:       3.0.0\r\n\t- contourpy:         1.2.0\r\n\t- cycler:            0.12.1\r\n\t- filelock:          3.13.1\r\n\t- fonttools:         4.48.1\r\n\t- frozenlist:        1.4.1\r\n\t- fsspec:            2024.2.0\r\n\t- idna:              3.6\r\n\t- jinja2:            3.1.3\r\n\t- joblib:            1.3.2\r\n\t- kiwisolver:        1.4.5\r\n\t- lightning:         2.2.0\r\n\t- lightning-utilities: 0.10.1\r\n\t- markdown-it-py:    3.0.0\r\n\t- markupsafe:        2.1.5\r\n\t- matplotlib:        3.8.2\r\n\t- mdurl:             0.1.2\r\n\t- mpmath:            1.3.0\r\n\t- multidict:         6.0.5\r\n\t- networkx:          3.2.1\r\n\t- numpy:             1.26.4\r\n\t- nvidia-cublas-cu12: 12.1.3.1\r\n\t- nvidia-cuda-cupti-cu12: 12.1.105\r\n\t- nvidia-cuda-nvrtc-cu12: 12.1.105\r\n\t- nvidia-cuda-runtime-cu12: 12.1.105\r\n\t- nvidia-cudnn-cu12: 8.9.2.26\r\n\t- nvidia-cufft-cu12: 11.0.2.54\r\n\t- nvidia-curand-cu12: 10.3.2.106\r\n\t- nvidia-cusolver-cu12: 11.4.5.107\r\n\t- nvidia-cusparse-cu12: 12.1.0.106\r\n\t- nvidia-nccl-cu12:  2.19.3\r\n\t- nvidia-nvjitlink-cu12: 12.3.101\r\n\t- nvidia-nvtx-cu12:  12.1.105\r\n\t- overrides:         7.7.0\r\n\t- packaging:         23.2\r\n\t- pandas:            2.2.0\r\n\t- pillow:            10.2.0\r\n\t- pip:               24.0\r\n\t- psutil:            5.9.8\r\n\t- pygments:          2.17.2\r\n\t- pynvml:            11.4.1\r\n\t- pyparsing:         3.1.1\r\n\t- python-dateutil:   2.8.2\r\n\t- pytorch-lightning: 2.2.0\r\n\t- pytz:              2024.1\r\n\t- pyupgrade:         3.15.0\r\n\t- pyyaml:            6.0.1\r\n\t- rdkit:             2023.9.4\r\n\t- reorder-python-imports: 3.12.0\r\n\t- requests:          2.31.0\r\n\t- rich:              13.7.0\r\n\t- ruff:              0.2.1\r\n\t- scalene:           1.5.34\r\n\t- scikit-learn:      1.4.0\r\n\t- scipy:             1.12.0\r\n\t- seaborn:           0.13.2\r\n\t- setuptools:        69.0.3\r\n\t- six:               1.16.0\r\n\t- sympy:             1.12\r\n\t- threadpoolctl:     3.2.0\r\n\t- tokenize-rt:       5.2.0\r\n\t- torch:             2.2.0\r\n\t- torch-geometric:   2.4.0\r\n\t- torchmetrics:      1.3.0.post0\r\n\t- tqdm:              4.66.1\r\n\t- triton:            2.2.0\r\n\t- typing-extensions: 4.9.0\r\n\t- tzdata:            2023.4\r\n\t- urllib3:           2.2.0\r\n\t- wheel:             0.42.0\r\n\t- yarl:              1.9.4\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.10.12\r\n\t- release:           6.5.0-14-generic\r\n\t- version:           #14~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Nov 20 18:15:30 UTC 2\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @borda",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19432/comments",
    "author": "ramon-adalia-lmd",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-02-11T00:06:16Z",
        "body": "@ramon-adalia-lmd Would you be able to provide a code example that produces this error?"
      },
      {
        "user": "ramon-adalia-lmd",
        "created_at": "2024-02-11T08:09:08Z",
        "body": "Uppon further testing, it does not seem to be specific to 2.2.0, but the bug is still there. Interestingly, the bug happens every other time I run the code. Here is an example script that triggers it:\r\n```python\r\nimport torch\r\nfrom lightning import LightningModule\r\nfrom lightning import Trainer\r\nfrom lightning.pytorch.loggers import CSVLogger\r\nfrom torchmetrics import MeanSquaredError\r\n\r\n\r\nclass Model(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(100, 1)\r\n        self.train_mse = MeanSquaredError()\r\n        self.val_mse = MeanSquaredError()\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = torch.nn.functional.mse_loss(y_hat, y)\r\n        self.train_mse.update(y_hat, y)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        self.val_mse.update(y_hat, y)\r\n\r\n    def on_train_epoch_end(self):\r\n        self.log(\"train_mse\", self.train_mse.compute(), prog_bar=True)\r\n        self.train_mse.reset()\r\n\r\n    def on_validation_epoch_end(self):\r\n        self.log(\"val_mse\", self.val_mse.compute(), prog_bar=True)\r\n        self.val_mse.reset()\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters())\r\n\r\n\r\ndef main():\r\n    X = torch.randn(100, 100)\r\n    y = torch.randn(100, 1)\r\n    Z = torch.randn(100, 100)\r\n    t = torch.randn(100, 1)\r\n    train_loader = torch.utils.data.DataLoader(list(zip(X, y)))\r\n    val_loader = torch.utils.data.DataLoader(list(zip(Z, t)))\r\n    model = Model()\r\n    trainer = Trainer(\r\n        max_epochs=5, logger=CSVLogger(\"test_logs\", name=\"test\", version=0)\r\n    )\r\n    trainer.fit(model, train_loader, val_loader)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\nRun it the first time: works. The second time: error. The third time: works. And so on..."
      },
      {
        "user": "awaelchli",
        "created_at": "2024-02-11T13:58:35Z",
        "body": "@ramon-adalia-lmd Ah ok, this is because you fixed the version to 0, so the second time it gets executed, the file is already there, the logger tries to append to the file but sees different keys. In this case, the best we can do I think is delete the file from the beginning if it exists, since the user explicitly asks `version=x` to be overwritten."
      }
    ]
  },
  {
    "number": 19276,
    "title": "Type hint for `configure_optimizers` does not include `Sequence[OptimizerLRSchedulerConfig]`",
    "created_at": "2024-01-12T11:09:04Z",
    "closed_at": "2024-02-05T22:59:58Z",
    "labels": [
      "help wanted",
      "docs"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19276",
    "body": "### 📚 Documentation\n\nThe return type hints for the `configure_optimizers` method of the `LightningModule` do not include  `Sequence[OptimizerLRSchedulerConfig]` even though it is a valid return type. The docs themself state so and it runs without problem.\r\n\r\nNevertheless, it is linted by `mypy` to be invalid and breaks our CI.\n\ncc @borda",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19276/comments",
    "author": "tilman151",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-01-12T14:58:04Z",
        "body": "I think a sequence is allowed (for manual optimization). So updating the type sounds good to me."
      },
      {
        "user": "tilman151",
        "created_at": "2024-01-24T12:11:12Z",
        "body": "@awaelchli I would be willing to do it myself. Shouldn't be that hard, hopefully."
      },
      {
        "user": "awaelchli",
        "created_at": "2024-01-24T13:29:50Z",
        "body": "@tilman151 Yes that would be great. It should just be updating that type hint I think."
      }
    ]
  },
  {
    "number": 18900,
    "title": "Symlink last checkpoint will fail on windows due to permission error",
    "created_at": "2023-10-31T07:43:11Z",
    "closed_at": "2023-11-06T15:10:56Z",
    "labels": [
      "bug",
      "help wanted",
      "callback: model checkpoint",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18900",
    "body": "### Bug description\n\n# Bug description\r\nHi,\r\nwith lightning v2.1 on Windows the creation of the symlink for the \"last\" ckpt will result in a permission error.\r\nThis is because on Windows creating a (soft) symlink requires admin permission. \r\n\n\n### What version are you seeing the problem on?\n\nv2.1\n\n### How to reproduce the bug\n\n```python\nTrain any model with ModelCheckpoint Callback (save_last=True and save_top_k!=0) on Windows without admin permissions\n```\n\n\n### Error messages and logs\n\n```\r\nlightning\\pytorch\\callbacks\\model_checkpoint.py\", line 388, in _link_checkpoint\r\n    os.symlink(filepath, linkpath)\r\nOSError: [WinError 1314] A required privilege is not held by the client\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce RTX 3080 Ti Laptop GPU\r\n        - available:         True\r\n        - version:           12.1\r\n* Lightning:\r\n        - lightning:         2.1.0\r\n        - lightning-cloud:   0.5.37\r\n        - lightning-utilities: 0.9.0\r\n        - pytorch-lightning: 2.1.0\r\n        - pytorch-optimizer: 2.11.1\r\n        - torch:             2.1.0+cu121\r\n        - torchaudio:        2.1.0+cu121\r\n        - torchmetrics:      1.2.0\r\n        - torchvision:       0.16.0+cu121\r\n* Packages:\r\n        - absl-py:           1.4.0\r\n        - aiohttp:           3.8.5\r\n        - aiosignal:         1.3.1\r\n        - alembic:           1.11.1\r\n        - ansicon:           1.89.0\r\n        - antlr4-python3-runtime: 4.9.3\r\n        - anyio:             3.7.1\r\n        - arrow:             1.2.3\r\n        - asttokens:         2.2.1\r\n        - async-timeout:     4.0.2\r\n        - attrs:             23.1.0\r\n        - autopage:          0.5.1\r\n        - av:                10.0.0\r\n        - backcall:          0.2.0\r\n        - beautifulsoup4:    4.12.2\r\n        - black:             23.7.0\r\n        - blessed:           1.20.0\r\n        - cachetools:        5.3.1\r\n        - certifi:           2022.12.7\r\n        - cfgv:              3.3.1\r\n        - charset-normalizer: 2.1.1\r\n        - click:             8.1.6\r\n        - cliff:             4.3.0\r\n        - cmaes:             0.10.0\r\n        - cmd2:              2.4.3\r\n        - colorama:          0.4.6\r\n        - colorlog:          6.7.0\r\n        - comm:              0.1.3\r\n        - contourpy:         1.1.0\r\n        - croniter:          1.3.15\r\n        - cycler:            0.11.0\r\n        - dateutils:         0.6.12\r\n        - debugpy:           1.6.7\r\n        - decorator:         5.1.1\r\n        - deepdiff:          6.3.1\r\n        - distlib:           0.3.7\r\n        - exceptiongroup:    1.1.2\r\n        - executing:         1.2.0\r\n        - fastapi:           0.100.0\r\n        - fastcore:          1.5.29\r\n        - filelock:          3.12.2\r\n        - fonttools:         4.41.0\r\n        - frozenlist:        1.4.0\r\n        - fsspec:            2023.6.0\r\n        - furl:              2.1.3\r\n        - google-auth:       2.22.0\r\n        - google-auth-oauthlib: 1.0.0\r\n        - greenlet:          2.0.2\r\n        - grpcio:            1.56.2\r\n        - h11:               0.14.0\r\n        - h5py:              3.9.0\r\n        - hydra-colorlog:    1.2.0\r\n        - hydra-core:        1.3.2\r\n        - hydra-joblib-launcher: 1.2.0\r\n        - hydra-optuna-sweeper: 1.2.0\r\n        - identify:          2.5.25\r\n        - idna:              3.4\r\n        - imageio:           2.31.6\r\n        - importlib-metadata: 6.8.0\r\n        - iniconfig:         2.0.0\r\n        - inquirer:          3.1.3\r\n        - ipykernel:         6.24.0\r\n        - ipython:           8.14.0\r\n        - itsdangerous:      2.1.2\r\n        - jedi:              0.18.2\r\n        - jinja2:            3.1.2\r\n        - jinxed:            1.2.0\r\n        - joblib:            1.3.1\r\n        - jsonschema:        4.18.4\r\n        - jsonschema-specifications: 2023.7.1\r\n        - jupyter-client:    8.3.0\r\n        - jupyter-core:      5.3.1\r\n        - kiwisolver:        1.4.4\r\n        - lightning:         2.1.0\r\n        - lightning-cloud:   0.5.37\r\n        - lightning-utilities: 0.9.0\r\n        - mako:              1.2.4\r\n        - markdown:          3.4.3\r\n        - markdown-it-py:    3.0.0\r\n        - markupsafe:        2.1.2\r\n        - matplotlib:        3.8.0\r\n        - matplotlib-inline: 0.1.6\r\n        - mdurl:             0.1.2\r\n        - mplcyberpunk:      0.7.0\r\n        - mpmath:            1.2.1\r\n        - multidict:         6.0.4\r\n        - mypy-extensions:   1.0.0\r\n        - nest-asyncio:      1.5.6\r\n        - networkx:          3.0\r\n        - nodeenv:           1.8.0\r\n        - numpy:             1.26.0\r\n        - oauthlib:          3.2.2\r\n        - omegaconf:         2.3.0\r\n        - openexr:           1.3.8\r\n        - optuna:            2.10.1\r\n        - ordered-set:       4.1.0\r\n        - orderedmultidict:  1.0.1\r\n        - packaging:         23.1\r\n        - pandas:            2.1.2\r\n        - parso:             0.8.3\r\n        - pathlib2:          2.3.7.post1\r\n        - pathspec:          0.11.2\r\n        - pbr:               5.11.1\r\n        - pefile:            2023.2.7\r\n        - pickleshare:       0.7.5\r\n        - pillow:            9.3.0\r\n        - pip:               23.3.1\r\n        - platformdirs:      3.9.1\r\n        - pluggy:            1.2.0\r\n        - pre-commit:        3.5.0\r\n        - prettytable:       3.8.0\r\n        - prompt-toolkit:    3.0.39\r\n        - protobuf:          4.23.4\r\n        - psutil:            5.9.5\r\n        - pure-eval:         0.2.2\r\n        - pyasn1:            0.5.0\r\n        - pyasn1-modules:    0.3.0\r\n        - pydantic:          1.10.11\r\n        - pygments:          2.15.1\r\n        - pyjwt:             2.4.0\r\n        - pyparsing:         3.0.9\r\n        - pyperclip:         1.8.2\r\n        - pyreadline3:       3.4.1\r\n        - pyroexr:           0.2.0\r\n        - pyrootutils:       1.0.4\r\n        - pytest:            7.4.3\r\n        - python-dateutil:   2.8.2\r\n        - python-dotenv:     1.0.0\r\n        - python-editor:     1.0.4\r\n        - python-multipart:  0.0.6\r\n        - pytorch-lightning: 2.1.0\r\n        - pytorch-optimizer: 2.11.1\r\n        - pytz:              2023.3\r\n        - pywin32:           306\r\n        - pyyaml:            6.0.1\r\n        - pyzmq:             25.1.0\r\n        - readchar:          4.0.5\r\n        - referencing:       0.30.0\r\n        - requests:          2.28.1\r\n        - requests-oauthlib: 1.3.1\r\n        - rich:              13.6.0\r\n        - rpds-py:           0.9.2\r\n        - rsa:               4.9\r\n        - scipy:             1.11.1\r\n        - seaborn:           0.13.0\r\n        - setuptools:        65.5.0\r\n        - six:               1.16.0\r\n        - sniffio:           1.3.0\r\n        - soupsieve:         2.4.1\r\n        - sqlalchemy:        2.0.19\r\n        - stack-data:        0.6.2\r\n        - starlette:         0.27.0\r\n        - starsessions:      1.3.0\r\n        - stevedore:         5.1.0\r\n        - sympy:             1.11.1\r\n        - tensorboard:       2.15.0\r\n        - tensorboard-data-server: 0.7.1\r\n        - tomli:             2.0.1\r\n        - torch:             2.1.0+cu121\r\n        - torchaudio:        2.1.0+cu121\r\n        - torchmetrics:      1.2.0\r\n        - torchvision:       0.16.0+cu121\r\n        - tornado:           6.3.2\r\n        - tqdm:              4.66.1\r\n        - traitlets:         5.9.0\r\n        - typing-extensions: 4.7.1\r\n        - tzdata:            2023.3\r\n        - urllib3:           1.26.13\r\n        - uvicorn:           0.23.1\r\n        - virtualenv:        20.24.1\r\n        - wcwidth:           0.2.6\r\n        - websocket-client:  1.6.1\r\n        - websockets:        11.0.3\r\n        - werkzeug:          2.3.6\r\n        - wheel:             0.40.0\r\n        - yarl:              1.9.2\r\n        - zipp:              3.16.2\r\n* System:\r\n        - OS:                Windows\r\n        - architecture:\r\n                - 64bit\r\n                - WindowsPE\r\n        - processor:         Intel64 Family 6 Model 154 Stepping 3, GenuineIntel\r\n        - python:            3.10.11\r\n        - release:           10\r\n        - version:           10.0.22621\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @carmocca @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18900/comments",
    "author": "aweinmann",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-11-01T18:45:44Z",
        "body": "Hey @aweinmann \r\nI'm sorry about that, we shouldn't have pushed this feature in last minute. \r\nOur CI runners must have extra permissions set so that symlink creation works, otherwise we would have seen a permission error in our Windows test suite too. \r\n\r\nSince it is not possible to reliably create a symlink on Windows without elevated permissions, we will have to guard the symlink creation and fall back to saving a copy of the file when permissions are not granted :( "
      }
    ]
  },
  {
    "number": 18641,
    "title": "Incorrect type hints for `log_dict(MetricCollection())`?",
    "created_at": "2023-09-26T16:07:16Z",
    "closed_at": "2023-09-27T16:36:04Z",
    "labels": [
      "bug",
      "help wanted",
      "good first issue",
      "code quality",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18641",
    "body": "### Bug description\n\nI'm using TorchMetrics to compute various metrics in my `LightningModule`. However, it doesn't seem like the type hints allow passing a `MetricCollection` directly to `log_dict`.\n\n### What version are you seeing the problem on?\n\nv2.0\n\n### How to reproduce the bug\n\n```python\nfrom lightning.pytorch import LightningModule\r\nfrom torch import Tensor\r\nfrom torchmetrics import MetricCollection\r\nfrom torchmetrics.classification import MulticlassAccuracy, MulticlassJaccardIndex\r\n\r\nclass MyModel(LightningModule):\r\n    def __init__(self) -> None:\r\n        self.metrics = MetricCollection([MulticlassAccuracy(1), MulticlassJaccardIndex(1)])\r\n\r\n    def training_step(self, batch: Tensor, batch_idx: int) -> Tensor:\r\n        x, y = batch\r\n        preds: Tensor = self(x)\r\n        self.metrics(preds, y)\r\n        self.log_dict(self.metrics)\r\n        return preds\n```\n\n\n### Error messages and logs\n\n```console\r\n> mypy --strict test.py\r\ntest.py:14: error: Argument 1 to \"log_dict\" of \"LightningModule\" has incompatible type \"MetricCollection\"; expected \"Mapping[str, Union[Metric, Tensor, Union[int, float]]]\"  [arg-type]\r\nFound 1 error in 1 file (checked 1 source file)\r\n```\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n\t- GPU:               None\r\n\t- available:         False\r\n\t- version:           None\r\n* Lightning:\r\n\t- efficientnet-pytorch: 0.7.1\r\n\t- lightning:         2.0.9\r\n\t- lightning-cloud:   0.5.38\r\n\t- lightning-utilities: 0.8.0\r\n\t- pytorch-lightning: 2.0.0\r\n\t- pytorch-sphinx-theme: 0.0.24\r\n\t- segmentation-models-pytorch: 0.3.3\r\n\t- torch:             2.0.1\r\n\t- torchmetrics:      1.1.1\r\n\t- torchvision:       0.15.2\r\n* Packages:\r\n\t- absl-py:           1.4.0\r\n\t- aenum:             3.1.12\r\n\t- affine:            2.1.0\r\n\t- aiohttp:           3.8.4\r\n\t- aiosignal:         1.2.0\r\n\t- alabaster:         0.7.13\r\n\t- antlr4-python3-runtime: 4.9.3\r\n\t- anyio:             3.6.2\r\n\t- appdirs:           1.4.4\r\n\t- appnope:           0.1.3\r\n\t- argon2-cffi:       21.3.0\r\n\t- argon2-cffi-bindings: 21.2.0\r\n\t- arrow:             1.2.3\r\n\t- asttokens:         2.2.1\r\n\t- astunparse:        1.6.3\r\n\t- async-lru:         1.0.3\r\n\t- async-timeout:     4.0.2\r\n\t- attrs:             23.1.0\r\n\t- babel:             2.12.1\r\n\t- backcall:          0.2.0\r\n\t- backoff:           2.2.1\r\n\t- beautifulsoup4:    4.12.2\r\n\t- black:             23.9.1\r\n\t- bleach:            6.0.0\r\n\t- blessed:           1.19.0\r\n\t- bottleneck:        1.3.7\r\n\t- build:             1.0.3\r\n\t- cachetools:        5.2.0\r\n\t- cartopy:           0.22.0\r\n\t- certifi:           2023.5.7\r\n\t- cffi:              1.15.1\r\n\t- cftime:            1.0.3.4\r\n\t- charset-normalizer: 3.1.0\r\n\t- click:             8.1.3\r\n\t- click-plugins:     1.1.1\r\n\t- cligj:             0.7.2\r\n\t- cmocean:           2.0\r\n\t- colorama:          0.4.6\r\n\t- comm:              0.1.3\r\n\t- contourpy:         1.0.7\r\n\t- coverage:          7.2.6\r\n\t- croniter:          1.3.8\r\n\t- cycler:            0.11.0\r\n\t- cython:            0.29.36\r\n\t- dateutils:         0.6.12\r\n\t- debugpy:           1.6.7\r\n\t- decorator:         5.1.1\r\n\t- deepdiff:          6.3.0\r\n\t- defusedxml:        0.7.1\r\n\t- docstring-parser:  0.15\r\n\t- docutils:          0.18.1\r\n\t- editables:         0.3\r\n\t- efficientnet-pytorch: 0.7.1\r\n\t- einops:            0.6.1\r\n\t- et-xmlfile:        1.0.1\r\n\t- executing:         1.2.0\r\n\t- fastapi:           0.98.0\r\n\t- fastjsonschema:    2.16.3\r\n\t- filelock:          3.12.0\r\n\t- fiona:             1.9.4\r\n\t- flake8:            6.1.0\r\n\t- flit-core:         3.9.0\r\n\t- fonttools:         4.39.4\r\n\t- fqdn:              1.5.1\r\n\t- frozenlist:        1.3.1\r\n\t- fsspec:            2023.1.0\r\n\t- gdal:              3.7.2\r\n\t- geocube:           0.3.2\r\n\t- geopandas:         0.11.1\r\n\t- gevent:            23.7.0\r\n\t- google-auth:       2.20.0\r\n\t- google-auth-oauthlib: 0.5.2\r\n\t- greenlet:          2.0.2\r\n\t- grpcio:            1.52.0\r\n\t- h11:               0.13.0\r\n\t- h5py:              3.8.0\r\n\t- hatch-jupyter-builder: 0.8.3\r\n\t- hatchling:         1.17.0\r\n\t- huggingface-hub:   0.14.1\r\n\t- hydra-core:        1.3.1\r\n\t- idna:              3.4\r\n\t- imagesize:         1.4.1\r\n\t- importlib-metadata: 6.6.0\r\n\t- importlib-resources: 5.12.0\r\n\t- iniconfig:         2.0.0\r\n\t- inquirer:          3.1.3\r\n\t- ipykernel:         6.23.1\r\n\t- ipython:           8.14.0\r\n\t- ipywidgets:        8.0.2\r\n\t- isoduration:       20.11.0\r\n\t- isort:             5.12.0\r\n\t- itsdangerous:      2.1.2\r\n\t- jaraco.classes:    3.2.3\r\n\t- jedi:              0.18.2\r\n\t- jinja2:            3.0.3\r\n\t- joblib:            1.2.0\r\n\t- json5:             0.9.14\r\n\t- jsonargparse:      4.25.0\r\n\t- jsonpointer:       2.0\r\n\t- jsonschema:        4.17.3\r\n\t- jupyter-client:    8.2.0\r\n\t- jupyter-core:      5.3.0\r\n\t- jupyter-events:    0.6.3\r\n\t- jupyter-lsp:       2.2.0\r\n\t- jupyter-server:    2.6.0\r\n\t- jupyter-server-terminals: 0.4.4\r\n\t- jupyterlab:        4.0.1\r\n\t- jupyterlab-pygments: 0.2.2\r\n\t- jupyterlab-server: 2.22.1\r\n\t- jupyterlab-widgets: 3.0.3\r\n\t- keyring:           23.13.1\r\n\t- kiwisolver:        1.4.4\r\n\t- kornia:            0.7.0\r\n\t- laspy:             2.2.0\r\n\t- lightly:           1.4.18\r\n\t- lightly-utils:     0.0.2\r\n\t- lightning:         2.0.9\r\n\t- lightning-cloud:   0.5.38\r\n\t- lightning-utilities: 0.8.0\r\n\t- markdown:          3.4.1\r\n\t- markdown-it-py:    3.0.0\r\n\t- markupsafe:        2.1.3\r\n\t- matplotlib:        3.8.0\r\n\t- matplotlib-inline: 0.1.6\r\n\t- mccabe:            0.7.0\r\n\t- mdurl:             0.1.2\r\n\t- mistune:           2.0.5\r\n\t- more-itertools:    9.1.0\r\n\t- mpmath:            1.2.1\r\n\t- multidict:         6.0.4\r\n\t- munch:             2.5.0\r\n\t- mypy:              1.3.0\r\n\t- mypy-extensions:   1.0.0\r\n\t- nbclient:          0.6.7\r\n\t- nbconvert:         7.4.0\r\n\t- nbformat:          5.8.0\r\n\t- nbmake:            1.4.3\r\n\t- nbsphinx:          0.8.8\r\n\t- nest-asyncio:      1.5.6\r\n\t- netcdf4:           1.6.2\r\n\t- networkx:          3.1\r\n\t- notebook-shim:     0.2.3\r\n\t- numexpr:           2.8.4\r\n\t- numpy:             1.25.2\r\n\t- oauthlib:          3.2.1\r\n\t- odc-geo:           0.1.2\r\n\t- omegaconf:         2.3.0\r\n\t- openpyxl:          3.1.2\r\n\t- ordered-set:       4.0.2\r\n\t- overrides:         7.3.1\r\n\t- packaging:         23.1\r\n\t- pandas:            2.0.2\r\n\t- pandocfilters:     1.5.0\r\n\t- parso:             0.8.3\r\n\t- pathspec:          0.11.1\r\n\t- pexpect:           4.8.0\r\n\t- pickleshare:       0.7.5\r\n\t- pillow:            9.5.0\r\n\t- pip:               23.0\r\n\t- pkginfo:           1.9.6\r\n\t- planetary-computer: 0.4.9\r\n\t- platformdirs:      3.5.3\r\n\t- pluggy:            1.0.0\r\n\t- poetry-core:       1.6.1\r\n\t- pretrainedmodels:  0.7.4\r\n\t- prometheus-client: 0.17.0\r\n\t- prompt-toolkit:    3.0.38\r\n\t- protobuf:          3.20.3\r\n\t- psutil:            5.9.5\r\n\t- ptyprocess:        0.7.0\r\n\t- pure-eval:         0.2.2\r\n\t- pyasn1:            0.4.8\r\n\t- pyasn1-modules:    0.2.8\r\n\t- pybind11:          2.10.1\r\n\t- pycocotools:       2.0.6\r\n\t- pycodestyle:       2.11.0\r\n\t- pycparser:         2.21\r\n\t- pydantic:          1.10.9\r\n\t- pydocstyle:        6.2.1\r\n\t- pyflakes:          3.1.0\r\n\t- pygeos:            0.10\r\n\t- pygments:          2.15.1\r\n\t- pyjwt:             2.4.0\r\n\t- pyparsing:         3.0.9\r\n\t- pyproj:            3.6.0\r\n\t- pyproject-hooks:   1.0.0\r\n\t- pyrsistent:        0.19.3\r\n\t- pyshp:             2.1.0\r\n\t- pystac:            1.4.0\r\n\t- pystac-client:     0.5.1\r\n\t- pytest:            7.3.2\r\n\t- pytest-cov:        4.0.0\r\n\t- python-dateutil:   2.8.2\r\n\t- python-dotenv:     0.19.2\r\n\t- python-editor:     1.0.4\r\n\t- python-json-logger: 2.0.7\r\n\t- python-multipart:  0.0.5\r\n\t- pytorch-lightning: 2.0.0\r\n\t- pytorch-sphinx-theme: 0.0.24\r\n\t- pytz:              2023.3\r\n\t- pyupgrade:         3.3.1\r\n\t- pyyaml:            6.0\r\n\t- pyzmq:             25.0.2\r\n\t- radiant-mlhub:     0.5.1\r\n\t- rarfile:           4.1\r\n\t- rasterio:          1.3.8\r\n\t- readchar:          4.0.5\r\n\t- readme-renderer:   37.3\r\n\t- requests:          2.31.0\r\n\t- requests-oauthlib: 1.3.1\r\n\t- requests-toolbelt: 1.0.0\r\n\t- rfc3339-validator: 0.1.4\r\n\t- rfc3986:           2.0.0\r\n\t- rfc3986-validator: 0.1.1\r\n\t- rich:              13.4.2\r\n\t- rioxarray:         0.4.1.post0\r\n\t- rsa:               4.9\r\n\t- rtree:             1.0.1\r\n\t- safetensors:       0.3.1\r\n\t- scikit-learn:      1.3.1\r\n\t- scipy:             1.10.1\r\n\t- segmentation-models-pytorch: 0.3.3\r\n\t- send2trash:        1.8.0\r\n\t- setuptools:        63.4.3\r\n\t- setuptools-scm:    7.1.0\r\n\t- shapely:           1.8.4\r\n\t- six:               1.16.0\r\n\t- sniffio:           1.3.0\r\n\t- snowballstemmer:   2.2.0\r\n\t- snuggs:            1.4.1\r\n\t- soupsieve:         2.4.1\r\n\t- sphinx:            5.3.0\r\n\t- sphinx-design:     0.4.1\r\n\t- sphinx-rtd-theme:  1.2.2\r\n\t- sphinxcontrib-applehelp: 1.0.2\r\n\t- sphinxcontrib-devhelp: 1.0.2\r\n\t- sphinxcontrib-htmlhelp: 2.0.0\r\n\t- sphinxcontrib-jquery: 4.1\r\n\t- sphinxcontrib-jsmath: 1.0.1\r\n\t- sphinxcontrib-programoutput: 0.15\r\n\t- sphinxcontrib-qthelp: 1.0.3\r\n\t- sphinxcontrib-serializinghtml: 1.1.9\r\n\t- stack-data:        0.6.2\r\n\t- starlette:         0.27.0\r\n\t- starsessions:      1.3.0\r\n\t- sympy:             1.11.1\r\n\t- tensorboard:       2.13.0\r\n\t- tensorboard-data-server: 0.7.0\r\n\t- tensorboard-plugin-wit: 1.8.1\r\n\t- terminado:         0.17.1\r\n\t- threadpoolctl:     3.1.0\r\n\t- timm:              0.9.2\r\n\t- tinycss2:          1.1.1\r\n\t- tokenize-rt:       4.2.1\r\n\t- tomli:             2.0.1\r\n\t- torch:             2.0.1\r\n\t- torchmetrics:      1.1.1\r\n\t- torchvision:       0.15.2\r\n\t- tornado:           6.2\r\n\t- tqdm:              4.65.0\r\n\t- traitlets:         5.9.0\r\n\t- trove-classifiers: 2023.3.9\r\n\t- twine:             4.0.2\r\n\t- typeshed-client:   2.1.0\r\n\t- typing-extensions: 4.6.3\r\n\t- tzdata:            2023.3\r\n\t- uri-template:      1.2.0\r\n\t- urllib3:           1.26.12\r\n\t- uvicorn:           0.20.0\r\n\t- vermin:            1.5.2\r\n\t- wcwidth:           0.2.5\r\n\t- webcolors:         1.11.1\r\n\t- webencodings:      0.5.1\r\n\t- websocket-client:  1.5.1\r\n\t- websockets:        10.4\r\n\t- werkzeug:          2.3.4\r\n\t- wheel:             0.41.2\r\n\t- widgetsnbextension: 4.0.3\r\n\t- xarray:            2023.7.0\r\n\t- yarl:              1.8.1\r\n\t- zipfile-deflate64: 0.2.0\r\n\t- zipp:              3.8.1\r\n\t- zope.event:        4.6\r\n\t- zope.interface:    5.4.0\r\n* System:\r\n\t- OS:                Darwin\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         arm\r\n\t- python:            3.11.4\r\n\t- release:           22.6.0\r\n\t- version:           Darwin Kernel Version 22.6.0: Wed Jul  5 22:21:53 PDT 2023; root:xnu-8796.141.3~6/RELEASE_ARM64_T6020\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @borda",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18641/comments",
    "author": "adamjstewart",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-09-26T20:05:21Z",
        "body": "@adamjstewart I think adding MetricCollection to the Union type of log_dict's input argument should fix the issue. "
      },
      {
        "user": "adamjstewart",
        "created_at": "2023-09-26T20:19:18Z",
        "body": "Alright, I can submit a PR to do that. I was originally going to add it to `_METRIC`, but that would break other things like `log`. I agree it makes more sense to do `Union[_METRIC, MetricCollection]`."
      },
      {
        "user": "adamjstewart",
        "created_at": "2023-09-26T20:20:17Z",
        "body": "Oh wait no, it would be `Union[Mapping[...], MetricCollection]`."
      }
    ]
  },
  {
    "number": 18185,
    "title": "modelcheckpoint fails if verbose=True due to bug in format string",
    "created_at": "2023-07-28T10:57:06Z",
    "closed_at": "2023-07-28T14:10:35Z",
    "labels": [
      "bug",
      "help wanted",
      "good first issue",
      "callback: model checkpoint",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18185",
    "body": "### Bug description\r\n\r\nIf verbose=True then modelcheckpoint attempts to include the loss in a formatted string. However the loss is a tensor so this cannot be formatted. Needs to convert the tensor to a number first.\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.0\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nimport torch\r\nloss = torch.Tensor([1.2])\r\nprint(f\"{loss::.2f}\")\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\n```\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py in fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\r\n    527         model = _maybe_unwrap_optimized(model)\r\n    528         self.strategy._lightning_module = model\r\n--> 529         call._call_and_handle_interrupt(\r\n    530             self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n    531         )\r\n\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)\r\n     40         if trainer.strategy.launcher is not None:\r\n     41             return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n---> 42         return trainer_fn(*args, **kwargs)\r\n     43 \r\n     44     except _TunerExitException:\r\n\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py in _fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\r\n    566             model_connected=self.lightning_module is not None,\r\n    567         )\r\n--> 568         self._run(model, ckpt_path=ckpt_path)\r\n    569 \r\n    570         assert self.state.stopped\r\n\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py in _run(self, model, ckpt_path)\r\n    971         # RUN THE TRAINER\r\n    972         # ----------------------------\r\n--> 973         results = self._run_stage()\r\n    974 \r\n    975         # ----------------------------\r\n\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py in _run_stage(self)\r\n   1014                 self._run_sanity_check()\r\n   1015             with torch.autograd.set_detect_anomaly(self._detect_anomaly):\r\n-> 1016                 self.fit_loop.run()\r\n   1017             return None\r\n   1018         raise RuntimeError(f\"Unexpected state {self.state}\")\r\n\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py in run(self)\r\n    200                 self.on_advance_start()\r\n    201                 self.advance()\r\n--> 202                 self.on_advance_end()\r\n    203                 self._restarting = False\r\n    204             except StopIteration:\r\n\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py in on_advance_end(self)\r\n    367         call._call_callback_hooks(trainer, \"on_train_epoch_end\", monitoring_callbacks=False)\r\n    368         call._call_lightning_module_hook(trainer, \"on_train_epoch_end\")\r\n--> 369         call._call_callback_hooks(trainer, \"on_train_epoch_end\", monitoring_callbacks=True)\r\n    370 \r\n    371         trainer._logger_connector.on_epoch_end()\r\n\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py in _call_callback_hooks(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\r\n    191         if callable(fn):\r\n    192             with trainer.profiler.profile(f\"[Callback]{callback.state_key}.{hook_name}\"):\r\n--> 193                 fn(trainer, trainer.lightning_module, *args, **kwargs)\r\n    194 \r\n    195     if pl_module:\r\n\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py in on_train_epoch_end(self, trainer, pl_module)\r\n    301             monitor_candidates = self._monitor_candidates(trainer)\r\n    302             if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\r\n--> 303                 self._save_topk_checkpoint(trainer, monitor_candidates)\r\n    304             self._save_last_checkpoint(trainer, monitor_candidates)\r\n    305 \r\n\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py in _save_topk_checkpoint(self, trainer, monitor_candidates)\r\n    358                     raise MisconfigurationException(m)\r\n    359                 warning_cache.warn(m)\r\n--> 360             self._save_monitor_checkpoint(trainer, monitor_candidates)\r\n    361         else:\r\n    362             self._save_none_monitor_checkpoint(trainer, monitor_candidates)\r\n\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py in _save_monitor_checkpoint(self, trainer, monitor_candidates)\r\n    659         if self.check_monitor_top_k(trainer, current):\r\n    660             assert current is not None\r\n--> 661             self._update_best_and_save(current, trainer, monitor_candidates)\r\n    662         elif self.verbose:\r\n    663             epoch = monitor_candidates[\"epoch\"]\r\n\r\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py in _update_best_and_save(self, current, trainer, monitor_candidates)\r\n    707             step = monitor_candidates[\"step\"]\r\n    708             rank_zero_info(\r\n--> 709                 f\"Epoch {epoch:d}, global step {step:d}: {self.monitor!r} reached {current:0.5f}\"\r\n    710                 f\" (best {self.best_model_score:0.5f}), saving model to {filepath!r} as top {k}\"\r\n    711             )\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/_tensor.py in __format__(self, format_spec)\r\n    868     def __format__(self, format_spec):\r\n    869         if has_torch_function_unary(self):\r\n--> 870             return handle_torch_function(Tensor.__format__, (self,), self, format_spec)\r\n    871         if self.dim() == 0 and not self.is_meta and type(self) is Tensor:\r\n    872             return self.item().__format__(format_spec)\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/overrides.py in handle_torch_function(public_api, relevant_args, *args, **kwargs)\r\n   1549         # Use `public_api` instead of `implementation` so __torch_function__\r\n   1550         # implementations can do equality/identity comparisons.\r\n-> 1551         result = torch_func_method(public_api, types, args, kwargs)\r\n   1552 \r\n   1553         if result is not NotImplemented:\r\n\r\n/usr/local/lib/python3.10/dist-packages/fastai/torch_core.py in __torch_function__(cls, func, types, args, kwargs)\r\n    380         if cls.debug and func.__name__ not in ('__str__','__repr__'): print(func, types, args, kwargs)\r\n    381         if _torch_handled(args, cls._opt, func): types = (torch.Tensor,)\r\n--> 382         res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\r\n    383         dict_objs = _find_args(args) if args else _find_args(list(kwargs.values()))\r\n    384         if issubclass(type(res),TensorBase) and dict_objs: res.set_meta(dict_objs[0],as_copy=True)\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/_tensor.py in __torch_function__(cls, func, types, args, kwargs)\r\n   1293 \r\n   1294         with _C.DisableTorchFunctionSubclass():\r\n-> 1295             ret = func(*args, **kwargs)\r\n   1296             if func in get_default_nowrap_functions():\r\n   1297                 return ret\r\n\r\n/usr/local/lib/python3.10/dist-packages/torch/_tensor.py in __format__(self, format_spec)\r\n    871         if self.dim() == 0 and not self.is_meta and type(self) is Tensor:\r\n    872             return self.item().__format__(format_spec)\r\n--> 873         return object.__format__(self, format_spec)\r\n    874 \r\n    875     @_handle_torch_function_and_wrap_type_error_to_not_implemented\r\n\r\nTypeError: unsupported format string passed to TensorBase.__format__\r\n\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_\n\ncc @borda @carmocca @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18185/comments",
    "author": "simonm3",
    "comments": [
      {
        "user": "simonm3",
        "created_at": "2023-07-28T14:10:35Z",
        "body": "Actually I think this was my mistake....will reopen if not"
      },
      {
        "user": "simonm3",
        "created_at": "2023-07-28T14:57:30Z",
        "body": "This was caused by using a fastai model which returns output as TensorBase rather than Tensor. This filters through to the loss function which also returns TensorBase. This is really a fastai issue so will report it there."
      },
      {
        "user": "awaelchli",
        "created_at": "2023-07-28T15:36:08Z",
        "body": "Sounds good, thanks for providing the context. "
      }
    ]
  },
  {
    "number": 18117,
    "title": "Add `log_weight_decay` option to `LearningRateMonitor` callback",
    "created_at": "2023-07-19T09:31:35Z",
    "closed_at": "2023-09-05T16:01:27Z",
    "labels": [
      "feature",
      "help wanted",
      "callback: lr monitor"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18117",
    "body": "### Description & Motivation\n\nIn more sophisticated training settings, we usually apply a dedicated weight decay scheduler (for example DINO). Thus, it will be great to monitor these statistics, optionally, from the `LearningRateMonitor` callback.\n\n### Pitch\n\nA solution should look something like the following, but build in the LearningRateMonitor:\r\n```py\r\n\"\"\"Learning rate monitor logger callback.\"\"\"\r\nfrom typing import Any, Dict, List\r\n\r\nfrom pytorch_lightning import callbacks\r\nfrom torch import optim\r\n\r\n\r\nclass LearningRateMonitor(callbacks.LearningRateMonitor):\r\n    \"\"\"Extends the PL version with the ability to log the weight decay.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        logging_interval: str | None = None,\r\n        log_momentum: bool = False,\r\n        log_weight_decay: bool = False,\r\n    ) -> None:\r\n        \"\"\"Initializes the callback.\"\"\"\r\n        super().__init__(logging_interval, log_momentum)\r\n        self.log_weight_decay = log_weight_decay\r\n\r\n    def _get_lr_momentum_stat(\r\n        self, optimizer: optim.Optimizer, names: List[str]\r\n    ) -> Dict[str, float]:\r\n        \"\"\"Gathers and returns the learning rate, momentum and weight decay statistics.\"\"\"\r\n        lr_momentum_wd_stat = {}\r\n        param_groups = optimizer.param_groups\r\n        use_betas = \"betas\" in optimizer.defaults\r\n\r\n        for pg, name in zip(param_groups, names):\r\n            lr = self._extract_lr(pg, name)\r\n            lr_momentum_wd_stat.update(lr)\r\n            momentum = self._extract_momentum(\r\n                param_group=pg, name=name.replace(name, f\"{name}-momentum\"), use_betas=use_betas\r\n            )\r\n            lr_momentum_wd_stat.update(momentum)\r\n            weight_decay = self._extract_weight_decay(pg, name + \"-weight_decay\")\r\n            lr_momentum_wd_stat.update(weight_decay)\r\n\r\n        return lr_momentum_wd_stat\r\n\r\n    def _extract_weight_decay(self, param_group: Dict[str, Any], name: str) -> Dict[str, Any]:\r\n        \"\"\"Extracts the weight decay statistics from a parameter group.\"\"\"\r\n        if not self.log_weight_decay:\r\n            return {}\r\n\r\n        wd = param_group[\"weight_decay\"]\r\n        return {name: wd}\r\n```\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\ncc @borda",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18117/comments",
    "author": "ioangatop",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-07-27T14:07:12Z",
        "body": "Thanks @lobantseff for this suggestion. I don't see what could stand in the way of such an addition. A PR for this would be welcomed if you are interested to contribute the feature :) "
      }
    ]
  },
  {
    "number": 17844,
    "title": "`fabric.logger.log_graph()` seems to do nothing",
    "created_at": "2023-06-16T01:31:26Z",
    "closed_at": "2023-07-17T22:18:41Z",
    "labels": [
      "bug",
      "help wanted",
      "logger",
      "logger: tensorboard",
      "fabric",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17844",
    "body": "### Bug description\r\nfabric.logger.log_graph() seems to do nothing.\r\nMy code looks like below, but no graph record in tensorboard log.\r\n```python\r\nfabric = Fabric(loggers=TensorBoardLogger(root_dir=\"./logs\", name=None))\r\nmodel, optimizer = fabric.setup(model, optimizer)\r\nfabric.logger.log_graph(model=model, input_array=next(iter(val_loader))[0])\r\n```\r\n\r\nI explored the source code of fabric.logger.log_graph()\r\n\r\n```python\r\n@rank_zero_only\r\n    def log_graph(self, model: Module, input_array: Optional[Tensor] = None) -> None:\r\n        model_example_input = getattr(model, \"example_input_array\", None)\r\n        input_array = model_example_input if input_array is None else input_array\r\n\r\n        if input_array is None:\r\n            rank_zero_warn(\r\n                \"Could not log computational graph to TensorBoard: The `model.example_input_array` attribute\"\r\n                \" is not set or `input_array` was not given.\"\r\n            )\r\n        elif not isinstance(input_array, (Tensor, tuple)):\r\n            rank_zero_warn(\r\n                \"Could not log computational graph to TensorBoard: The `input_array` or `model.example_input_array`\"\r\n                f\" has type {type(input_array)} which can't be traced by TensorBoard. Make the input array a tuple\"\r\n                f\" representing the positional arguments to the model's `forward()` implementation.\"\r\n            )\r\n        elif callable(getattr(model, \"_on_before_batch_transfer\", None)) and callable(\r\n            getattr(model, \"_apply_batch_transfer_handler\", None)\r\n        ):\r\n            # this is probably is a LightningModule\r\n            input_array = model._on_before_batch_transfer(input_array)  # type: ignore[operator]\r\n            input_array = model._apply_batch_transfer_handler(input_array)  # type: ignore[operator]\r\n            self.experiment.add_graph(model, input_array)\r\n```\r\nIt seems self.experiment.add_graph(model, input_array) should be reduced one indent, because that line no executed.\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version: 2.0.3\r\n#- PyTorch Version: 2.0.1\r\n#- Python version: 3.10\r\n```\r\n\r\n</details>\r\n\n\ncc @awaelchli @borda @Blaizzy @carmocca @justusschock",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17844/comments",
    "author": "shihaoyin",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-06-20T01:26:41Z",
        "body": "Yes @shihaoyin \r\nFor Fabric it would make sense to move this outside the else block IMO. Contributions for this change are welcome :) "
      },
      {
        "user": "Muthukamalan",
        "created_at": "2024-07-23T14:29:59Z",
        "body": "any solutions to this?"
      }
    ]
  },
  {
    "number": 17649,
    "title": "Loosen check on model step output types from `dict` to `Mapping`",
    "created_at": "2023-05-16T19:28:06Z",
    "closed_at": "2023-09-29T00:31:42Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "code quality"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17649",
    "body": "### Description & Motivation\r\n\r\nI want to be able to use custom classes that implement the `Mapping` interface as step outputs, and lightning should be able to work with them. `isinstance(..., dict)` is unnecessarily restrictive.\r\n\r\n### Pitch\r\n\r\nby example: line 63 of `loops/optimization/automatic.py` should go from this:\r\n```\r\n       if isinstance(training_step_output, dict):\r\n```\r\nto this:\r\n```\r\n      if isinstance(training_step_output, Mapping)\r\n```\r\n\r\netc. This should (unless I am missing something) be a backward-compatible, mechanical change.\r\n\r\nI think this change is in line with Pythonic design principles: duck typing and depending on abstractions, not concretions.\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\nI wanted to use a class like this:\r\n```\r\n@dataclass(kw_only=True)\r\nclass HomogeneousAttrDataclass(Generic[T], Mapping[str, T]):\r\n\r\n    def __iter__(self) -> Iterator[str]:\r\n        return iter(it.name for it in fields(self))\r\n\r\n    def __len__(self) -> int:\r\n        return len(fields(self))\r\n\r\n    def __getitem__(self, key: str) -> T:\r\n        return getattr(self, key)\r\n```\r\n\r\nto be able to use nice IDE-supported dataclasses as step output types in my model, but found that lightning shuts that down with the `isinstance` check. There is a way to work around this by subclassing dict directly and then \"hotwiring\" the dict methods to point to dataclass fields (or whatever custom mapping impl people might want to use), but it's really ugly.\r\n\r\n\r\n\r\ncc @borda",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17649/comments",
    "author": "qdbp",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2023-05-17T01:00:03Z",
        "body": "Sounds good to me 🐿️ "
      },
      {
        "user": "nik777",
        "created_at": "2023-09-26T01:44:12Z",
        "body": "Hi, @awaelchli\r\nI would like to give it a try. Please assign. "
      },
      {
        "user": "nik777",
        "created_at": "2023-09-26T20:48:19Z",
        "body": "@awaelchli do we need a test for it? Also I somehow managed to unassign myself so please assign me again :-). "
      },
      {
        "user": "awaelchli",
        "created_at": "2023-09-26T20:52:41Z",
        "body": "We would need a test for this yes, but we can help with that later on your PR if you struggle. "
      }
    ]
  },
  {
    "number": 17606,
    "title": "`trainer.predict()` fails to find the last checkpoint from `ModelCheckpoint` callback",
    "created_at": "2023-05-11T11:35:59Z",
    "closed_at": "2023-11-27T16:03:47Z",
    "labels": [
      "help wanted",
      "docs",
      "callback: model checkpoint",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17606",
    "body": "### Bug description\n\nWhen using `ModelCheckpoint` callback, and then trying to use `trainer.predict()` no checkpoints are found.\n\n### What version are you seeing the problem on?\n\nv2.0\n\n### How to reproduce the bug\n\n```python\nimport os\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom torch import optim, nn, utils\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision.transforms import ToTensor\r\nimport pytorch_lightning as pl\r\n\r\n# define any number of nn.Modules (or use your current ones)\r\nencoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\r\ndecoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\r\n\r\n# define the LightningModule\r\nclass LitAutoEncoder(pl.LightningModule):\r\n    def __init__(self, encoder, decoder):\r\n        super().__init__()\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n\r\n    def forward(self, batch):\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        return x_hat\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defines the train loop.\r\n        # it is independent of forward\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = nn.functional.mse_loss(x_hat, x)\r\n        # Logging to TensorBoard (if installed) by default\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n\r\n# init the autoencoder\r\nautoencoder = LitAutoEncoder(encoder, decoder)\r\n\r\ndataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\r\ntrain_loader = utils.data.DataLoader(dataset)\r\n\r\ncheckpoint_callback = ModelCheckpoint(\r\n    dirpath=os.path.join(os.getcwd(), \"checkpoints\", f\"run-x\",),\r\n    monitor=\"train_loss\",\r\n    save_top_k=3,\r\n    mode=\"min\",\r\n)\r\n\r\ntrainer = pl.Trainer(\r\n    limit_train_batches=100, max_epochs=10, callbacks=[checkpoint_callback]\r\n)\r\n\r\ntrainer.fit(model=autoencoder, train_dataloaders=train_loader)\r\n\r\ntrainer.predict(dataloaders=train_loader, ckpt_path=\"last\")\n```\n\n\n### Error messages and logs\n\n```\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n\r\n  | Name    | Type       | Params\r\n---------------------------------------\r\n0 | encoder | Sequential | 50.4 K\r\n1 | decoder | Sequential | 51.2 K\r\n---------------------------------------\r\n101 K     Trainable params\r\n0         Non-trainable params\r\n101 K     Total params\r\n0.407     Total estimated model params size (MB)\r\nC:\\Users\\user\\Anaconda3\\envs\\trgl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\r\n  rank_zero_warn(\r\nEpoch 9: 100%|██████████| 100/100 [00:00<00:00, 274.22it/s, v_num=5]\r\n`Trainer.fit` stopped: `max_epochs=10` reached.\r\nC:\\Users\\user\\Anaconda3\\envs\\trgl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:189: UserWarning: .predict(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded.\r\n  rank_zero_warn(\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nC:\\Users\\user\\Anaconda3\\envs\\trgl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\r\n  rank_zero_warn(\r\nPredicting DataLoader 0:  61%|██████    | 36659/60000 [00:38<00:24, 945.32it/s]C:\\Users\\user\\Anaconda3\\envs\\trgl\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\r\n  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\r\nPredicting DataLoader 0:  61%|██████    | 36659/60000 [00:38<00:24, 940.85it/s]\r\n\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Quadro RTX 3000\r\n\t- available:         True\r\n\t- version:           11.8\r\n* Lightning:\r\n\t- lightning-utilities: 0.8.0\r\n\t- pytorch-lightning: 2.0.2\r\n\t- torch:             2.0.1\r\n\t- torchmetrics:      0.11.4\r\n\t- torchvision:       0.15.2\r\n* Packages:\r\n\t- absl-py:           1.4.0\r\n\t- aiohttp:           3.8.4\r\n\t- aiosignal:         1.3.1\r\n\t- anyio:             3.5.0\r\n\t- appdirs:           1.4.4\r\n\t- argon2-cffi:       21.3.0\r\n\t- argon2-cffi-bindings: 21.2.0\r\n\t- asttokens:         2.0.5\r\n\t- async-timeout:     4.0.2\r\n\t- attrs:             22.1.0\r\n\t- babel:             2.11.0\r\n\t- backcall:          0.2.0\r\n\t- beautifulsoup4:    4.12.2\r\n\t- black:             23.3.0\r\n\t- bleach:            4.1.0\r\n\t- brotlipy:          0.7.0\r\n\t- cachetools:        5.3.0\r\n\t- certifi:           2022.12.7\r\n\t- cffi:              1.15.1\r\n\t- cfgv:              3.3.1\r\n\t- charset-normalizer: 2.0.4\r\n\t- click:             8.1.3\r\n\t- colorama:          0.4.6\r\n\t- comm:              0.1.2\r\n\t- contourpy:         1.0.7\r\n\t- cryptography:      39.0.1\r\n\t- cycler:            0.11.0\r\n\t- debugpy:           1.5.1\r\n\t- decorator:         5.1.1\r\n\t- defusedxml:        0.7.1\r\n\t- distlib:           0.3.6\r\n\t- docker-pycreds:    0.4.0\r\n\t- editdistance:      0.6.2\r\n\t- entrypoints:       0.4\r\n\t- exceptiongroup:    1.1.1\r\n\t- executing:         0.8.3\r\n\t- fastjsonschema:    2.16.2\r\n\t- filelock:          3.12.0\r\n\t- fonttools:         4.39.3\r\n\t- frozenlist:        1.3.3\r\n\t- fsspec:            2023.5.0\r\n\t- gitdb:             4.0.10\r\n\t- gitpython:         3.1.31\r\n\t- google-auth:       2.17.3\r\n\t- google-auth-oauthlib: 1.0.0\r\n\t- grpcio:            1.54.0\r\n\t- identify:          2.5.24\r\n\t- idna:              3.4\r\n\t- iniconfig:         2.0.0\r\n\t- ipykernel:         6.19.2\r\n\t- ipython:           8.12.0\r\n\t- ipython-genutils:  0.2.0\r\n\t- ipywidgets:        8.0.4\r\n\t- isort:             5.12.0\r\n\t- jedi:              0.18.1\r\n\t- jinja2:            3.1.2\r\n\t- joblib:            1.2.0\r\n\t- json5:             0.9.6\r\n\t- jsonschema:        4.17.3\r\n\t- jupyter:           1.0.0\r\n\t- jupyter-client:    8.1.0\r\n\t- jupyter-console:   6.6.3\r\n\t- jupyter-core:      5.3.0\r\n\t- jupyter-server:    1.23.4\r\n\t- jupyterlab:        3.5.3\r\n\t- jupyterlab-pygments: 0.1.2\r\n\t- jupyterlab-server: 2.22.0\r\n\t- jupyterlab-widgets: 3.0.5\r\n\t- kiwisolver:        1.4.4\r\n\t- lightning-utilities: 0.8.0\r\n\t- lxml:              4.9.2\r\n\t- markdown:          3.4.3\r\n\t- markupsafe:        2.1.1\r\n\t- matplotlib:        3.7.1\r\n\t- matplotlib-inline: 0.1.6\r\n\t- mistune:           0.8.4\r\n\t- mpmath:            1.2.1\r\n\t- multidict:         6.0.4\r\n\t- mypy-extensions:   1.0.0\r\n\t- nbclassic:         0.5.5\r\n\t- nbclient:          0.5.13\r\n\t- nbconvert:         6.5.4\r\n\t- nbformat:          5.7.0\r\n\t- nest-asyncio:      1.5.6\r\n\t- networkx:          2.8.4\r\n\t- nodeenv:           1.7.0\r\n\t- notebook:          6.5.4\r\n\t- notebook-shim:     0.2.2\r\n\t- numpy:             1.24.3\r\n\t- numpyencoder:      0.3.0\r\n\t- oauthlib:          3.2.2\r\n\t- packaging:         23.0\r\n\t- pandas:            2.0.1\r\n\t- pandocfilters:     1.5.0\r\n\t- parso:             0.8.3\r\n\t- pathspec:          0.11.1\r\n\t- pathtools:         0.1.2\r\n\t- patsy:             0.5.3\r\n\t- pickleshare:       0.7.5\r\n\t- pillow:            9.5.0\r\n\t- pip:               23.0.1\r\n\t- platformdirs:      3.5.0\r\n\t- pluggy:            1.0.0\r\n\t- ply:               3.11\r\n\t- pre-commit:        3.3.1\r\n\t- prometheus-client: 0.14.1\r\n\t- prompt-toolkit:    3.0.36\r\n\t- protobuf:          4.23.0\r\n\t- psutil:            5.9.0\r\n\t- pure-eval:         0.2.2\r\n\t- pyasn1:            0.5.0\r\n\t- pyasn1-modules:    0.3.0\r\n\t- pycparser:         2.21\r\n\t- pygments:          2.15.1\r\n\t- pyopenssl:         23.0.0\r\n\t- pyparsing:         3.0.9\r\n\t- pyqt5:             5.15.7\r\n\t- pyqt5-sip:         12.11.0\r\n\t- pyrsistent:        0.18.0\r\n\t- pysocks:           1.7.1\r\n\t- pytest:            7.3.1\r\n\t- python-dateutil:   2.8.2\r\n\t- pytorch-lightning: 2.0.2\r\n\t- pytz:              2022.7\r\n\t- pywin32:           305.1\r\n\t- pywinpty:          2.0.10\r\n\t- pyyaml:            6.0\r\n\t- pyzmq:             25.0.2\r\n\t- qtconsole:         5.4.2\r\n\t- qtpy:              2.2.0\r\n\t- requests:          2.29.0\r\n\t- requests-oauthlib: 1.3.1\r\n\t- rsa:               4.9\r\n\t- scipy:             1.10.1\r\n\t- seaborn:           0.12.2\r\n\t- send2trash:        1.8.0\r\n\t- sentry-sdk:        1.22.2\r\n\t- setproctitle:      1.3.2\r\n\t- setuptools:        66.0.0\r\n\t- shortuuid:         1.0.11\r\n\t- sip:               6.6.2\r\n\t- six:               1.16.0\r\n\t- smmap:             5.0.0\r\n\t- sniffio:           1.2.0\r\n\t- soupsieve:         2.4\r\n\t- stack-data:        0.2.0\r\n\t- statsmodels:       0.14.0\r\n\t- sympy:             1.11.1\r\n\t- tensorboard:       2.13.0\r\n\t- tensorboard-data-server: 0.7.0\r\n\t- terminado:         0.17.1\r\n\t- tinycss2:          1.2.1\r\n\t- tokenize-rt:       5.0.0\r\n\t- toml:              0.10.2\r\n\t- tomli:             2.0.1\r\n\t- torch:             2.0.1\r\n\t- torchmetrics:      0.11.4\r\n\t- torchvision:       0.15.2\r\n\t- tornado:           6.2\r\n\t- tqdm:              4.65.0\r\n\t- traitlets:         5.7.1\r\n\t- typing-extensions: 4.5.0\r\n\t- tzdata:            2023.3\r\n\t- urllib3:           1.26.15\r\n\t- virtualenv:        20.23.0\r\n\t- wandb:             0.15.2\r\n\t- wcwidth:           0.2.5\r\n\t- webencodings:      0.5.1\r\n\t- websocket-client:  0.58.0\r\n\t- werkzeug:          2.3.4\r\n\t- wheel:             0.38.4\r\n\t- widgetsnbextension: 4.0.5\r\n\t- win-inet-pton:     1.1.0\r\n\t- yarl:              1.9.2\r\n* System:\r\n\t- OS:                Windows\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- WindowsPE\r\n\t- processor:         Intel64 Family 6 Model 165 Stepping 2, GenuineIntel\r\n\t- python:            3.10.11\r\n\t- release:           10\r\n\t- version:           10.0.19045\r\n\r\n</details>\n\n### More info\n\n_No response_\n\ncc @borda @carmocca @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17606/comments",
    "author": "olipinski",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2023-06-18T06:23:26Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-11-25T23:11:18Z",
        "body": "Loading using `trainer.fit/validate/test/predict(..., ckpt_path=\"last\")` is only valid when the callback was configured with `save_last=True`, but this isn't so obvious from the docs and should be clarified. "
      }
    ]
  },
  {
    "number": 17412,
    "title": "`LearningRateFinder` defining max validation batches for entire training loop",
    "created_at": "2023-04-18T23:31:20Z",
    "closed_at": "2023-05-18T12:46:46Z",
    "labels": [
      "bug",
      "help wanted",
      "tuner",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17412",
    "body": "### Bug description\r\n\r\nWhen the LearningRateFinder callback is used, the num_training_steps parameter that is passed on init (default: 100) ends up defining how many validation batches to run during the entire length of training. Meaning that if num_training_steps in the learning rate finder is less than the total number of batches in your validation set, then all validation loops while training will only see a subset of the validation data. \r\n\r\n### What version are you seeing the problem on?\r\n\r\n2.0+\r\n\r\n### How to reproduce the bug\r\n\r\nThis code will fail because `trainer.num_val_batches[0] = 5`. \r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom lightning.pytorch import LightningModule, Trainer\r\nfrom lightning.pytorch.callbacks import LearningRateFinder\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self, lr=0.1):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n        self.lr = lr\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=self.lr)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 100), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 100), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n        callbacks=[LearningRateFinder(num_training_steps=5)],\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    assert trainer.num_val_batches[0] == 50\r\n    trainer.validate(model, dataloaders=val_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nUsing the same base code from above but removing the `LearningRateFinder`, this code passes. \r\n\r\n```python\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    assert trainer.num_val_batches[0] == 50\r\n    trainer.validate(model, dataloaders=val_data)\r\n```\r\n\r\nHowever, `num_val_batches` does get updated once `.validate()` is called. Putting the `LearningRateFinder` back in but moving the assert statement, this code passes:\r\n\r\n```python\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n        callbacks=[LearningRateFinder(num_training_steps=5)],\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.validate(model, dataloaders=val_data)\r\n    assert trainer.num_val_batches[0] == 50\r\n```\r\n\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.0.1.post0\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0): 2.0.0\r\n#- Python version (e.g., 3.9): 3.10.9\r\n#- OS (e.g., Linux): Darwin\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source): pip\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\nI did some more digging into why this might be happening and it looks like the problem is likely coming from the fact that `trainer.fit_loop.epoch_loop.val_loop.setup_data()` is getting called for the first time while the learning rate finder is running, so `trainer.fit_loop.epoch_loop.val_loop._max_batches` gets set according to the parameters that the learning rate finder has passed in. \r\n\r\nEven though the learning rate finder restores the parameters that the trainer initially set once it is done, the `setup_data()` method never runs a full setup again, so the `_max_batches` attribute never gets updated again. \r\n\r\nOne solution to fix this might be to redo the data setup once the learning rate finder has completed, like how setup is redone when `.validate()` is called",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17412/comments",
    "author": "blainehoak",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-04-23T01:18:30Z",
        "body": "@blainehoak Thanks for reporting. Help on this would be appreciated :) You are right, the finder is probably not resetting all variables correctly. "
      }
    ]
  },
  {
    "number": 17359,
    "title": "Add a performance warning to pl.Trainer parameter detect_anomaly.",
    "created_at": "2023-04-13T09:32:11Z",
    "closed_at": "2023-04-15T02:08:50Z",
    "labels": [
      "feature",
      "help wanted",
      "trainer: argument"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/17359",
    "body": "### Description & Motivation\n\nThis detect_anomaly parameter greatly affects performance and is usually only used in debugging.  \r\nSometimes after debugging, user forget setting detect_anomaly to False, resulting in a performance loss during training.   \r\nHope add a warning to notice user that detect_anomaly flag will cause performance lose.  \r\n\r\nIn my experiment, the train speed when detect_anomaly is turned off is 2.5x when it is turned on.\n\n### Pitch\n\nAdd a performance warning when detect_anomaly enabled.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\ncc @borda @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/17359/comments",
    "author": "One-sixth",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-04-13T17:42:29Z",
        "body": "I am not opposed to this. Since the flag is only meant for debugging stability. \r\n@carmocca @justusschock any objection to this? "
      },
      {
        "user": "carmocca",
        "created_at": "2023-04-13T18:25:56Z",
        "body": "I agree. @One-sixth Would you like to contribute it?"
      },
      {
        "user": "One-sixth",
        "created_at": "2023-04-14T16:25:06Z",
        "body": "@carmocca Yes, I opened a PR."
      }
    ]
  },
  {
    "number": 16561,
    "title": "Learning Rate Finder stops early because max epochs reached",
    "created_at": "2023-01-30T15:09:54Z",
    "closed_at": "2023-04-16T23:23:52Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "tuner"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16561",
    "body": "### Bug description\r\n\r\nIf `max_epoch` is set to less than number of steps in `lr_find` function, the `lr_finder` is stopped with message: \r\n``` `Trainer.fit` stopped: `max_epochs=x` reached.```\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\npython\r\nimport os\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningModule, Trainer, callbacks\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\nclass EpochAutoLRFinder(callbacks.LearningRateFinder):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n\r\n    def on_fit_start(self, trainer, pl_module) -> None:\r\n        return\r\n\r\n    def on_train_epoch_start(self, trainer, pl_module) -> None:\r\n        self.lr_find(trainer, pl_module)\r\n\r\n        if (\r\n            self.optimal_lr is not None\r\n            and (lr_suggestion := self.optimal_lr.suggestion()) is not None\r\n        ):\r\n            for group in trainer.optimizers[0].param_groups:\r\n                group[\"lr\"] = lr_suggestion\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=100,\r\n        limit_val_batches=100,\r\n        limit_test_batches=100,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=20,\r\n        enable_model_summary=False,\r\n        callbacks=[EpochAutoLRFinder()],\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\n\ncc @borda",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16561/comments",
    "author": "DominikSpiljak",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-01-31T00:40:40Z",
        "body": "This could potentially be solved by a check in the LR finder before running the trainer."
      },
      {
        "user": "stale[bot]",
        "created_at": "2023-03-19T12:33:13Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      }
    ]
  },
  {
    "number": 16262,
    "title": "Bug with isolate_rng \"Cannot re-initialize CUDA in forked subprocess\"",
    "created_at": "2023-01-05T12:28:01Z",
    "closed_at": "2023-01-19T00:09:26Z",
    "labels": [
      "bug",
      "help wanted",
      "reproducibility"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16262",
    "body": "### Bug description\r\n\r\nI was using isolate_rng in pytorch dataset for some augmentations in pytorch lightning 1.7.1 but after update to latest 1.8.6 I experience error now.\r\n\r\n```\r\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n\r\n```\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nfrom torch.utils.data import DataLoader\r\nimport pytorch_lightning as pl\r\nfrom torch.utils.data import Dataset\r\nimport torch\r\nfrom torch import nn\r\nimport torch.nn.functional as F\r\n\r\nclass LitAutoEncoder(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\r\n        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\r\n\r\n    def forward(self, x):\r\n        # in lightning, forward defines the prediction/inference actions\r\n        embedding = self.encoder(x)\r\n        return embedding\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defines the train loop. It is independent of forward\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = F.mse_loss(x_hat, x)\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\nclass DummyDataset(Dataset):\r\n    def __init__(self) -> None:\r\n        super().__init__()\r\n        \r\n    def __getitem__(self,  index: int):\r\n        with pl.utilities.seed.isolate_rng():\r\n            return [torch.ones(28,28),torch.ones(28,28)] \r\n    \r\n    def __len__(self):\r\n        return 10\r\n\r\ndataset = DummyDataset()\r\n\r\ntrainer = pl.Trainer(logger=None, accelerator='gpu')\r\ntrainer.fit(LitAutoEncoder(), DataLoader(dataset, num_workers=2))\r\n```\r\nif you remove num_workers=2 the bug will go away but I need multiple workers.\r\n\r\n\r\n\r\n### Error messages and logs\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"check_bug_in_pl.py\", line 47, in <module>\r\n    trainer.fit(LitAutoEncoder(), DataLoader(dataset, num_workers=2))\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\", line 603, in fit\r\n    call._call_and_handle_interrupt(\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py\", line 38, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\", line 645, in _fit_impl\r\n    self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1098, in _run\r\n    results = self._run_stage()\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1177, in _run_stage\r\n    self._run_train()\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1200, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/loop.py\", line 199, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 267, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/loop.py\", line 199, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 188, in advance\r\n    batch = next(data_fetcher)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/fetching.py\", line 184, in __next__\r\n    return self.fetching_function()\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/fetching.py\", line 265, in fetching_function\r\n    self._fetch_next_batch(self.dataloader_iter)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/fetching.py\", line 280, in _fetch_next_batch\r\n    batch = next(iterator)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/supporters.py\", line 568, in __next__\r\n    return self.request_next_batch(self.loader_iters)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/supporters.py\", line 580, in request_next_batch\r\n    return apply_to_collection(loader_iters, Iterator, next)\r\n  File \"/home/jovyan/.local/lib/python3.8/site-packages/lightning_utilities/core/apply_func.py\", line 47, in apply_to_collection\r\n    return function(data, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 530, in __next__\r\n    data = self._next_data()\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1224, in _next_data\r\n    return self._process_data(data)\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1250, in _process_data\r\n    data.reraise()\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/_utils.py\", line 457, in reraise\r\n    raise exception\r\nRuntimeError: Caught RuntimeError in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"check_bug_in_pl.py\", line 38, in __getitem__\r\n    with pl.utilities.seed.isolate_rng():\r\n  File \"/usr/lib/python3.8/contextlib.py\", line 113, in __enter__\r\n    return next(self.gen)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/seed.py\", line 42, in isolate_rng\r\n    states = _collect_rng_states()\r\n  File \"/usr/local/lib/python3.8/dist-packages/lightning_lite/utilities/seed.py\", line 112, in _collect_rng_states\r\n    \"torch.cuda\": torch.cuda.get_rng_state_all(),\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/cuda/random.py\", line 39, in get_rng_state_all\r\n    results.append(get_rng_state(i))\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/cuda/random.py\", line 22, in get_rng_state\r\n    _lazy_init()\r\n  File \"/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\", line 206, in _lazy_init\r\n    raise RuntimeError(\r\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n\r\nEpoch 0:   0%|          | 0/10 [00:00<?, ?it/s]   \r\n```\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 1.8.6\r\n#- Lightning App Version (e.g., 0.5.2): \r\n#- PyTorch Version (e.g., 1.10): 1.11.0+cu102\r\n#- Python version (e.g., 3.9):  3.8.10\r\n#- OS (e.g., Linux): ubuntu\r\n#- CUDA/cuDNN version: 10.2\r\n#- GPU models and configuration: Tesla T4 \r\n#- How you installed Lightning: pip\r\n#- Running environment of LightningApp : local\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_\n\ncc @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16262/comments",
    "author": "sergeevii123",
    "comments": [
      {
        "user": "sergeevii123",
        "created_at": "2023-01-05T12:49:55Z",
        "body": "Error with this code starts to appear after pytorch-lightning 1.8.0"
      },
      {
        "user": "sergeevii123",
        "created_at": "2023-01-05T12:56:02Z",
        "body": "seems I've found what this is about \r\n```\r\n # torch.cuda rng_state is only included since v1.8.\r\n    if \"torch.cuda\" in rng_state_dict:\r\n        torch.cuda.set_rng_state_all(rng_state_dict[\"torch.cuda\"])\r\n```\r\nThe question is can we make isolate_rng with parameter to not include cuda even if trainer has \"gpu\" accelerator? Or it's better to write my own version locally?"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-01-05T19:15:08Z",
        "body": "@sergeevii123 Thanks for the reproducible code. Did you try wrapping the code with this guard?\r\n\r\n```py\r\nif __name__ == \"__main__\": \r\n    dataset = DummyDataset()\r\n    trainer = pl.Trainer(logger=None, accelerator='gpu')\r\n    trainer.fit(LitAutoEncoder(), DataLoader(dataset, num_workers=2))\r\n```\r\n\r\nI will take a closer look otherwise."
      },
      {
        "user": "sergeevii123",
        "created_at": "2023-01-05T19:24:40Z",
        "body": "> @sergeevii123 Thanks for the reproducible code. Did you try wrapping the code with this guard?\r\n> \r\n> ```python\r\n> if __name__ == \"__main__\": \r\n>     dataset = DummyDataset()\r\n>     trainer = pl.Trainer(logger=None, accelerator='gpu')\r\n>     trainer.fit(LitAutoEncoder(), DataLoader(dataset, num_workers=2))\r\n> ```\r\n> \r\n> I will take a closer look otherwise.\r\n\r\nTried but result is same. It works if you remove collecting cuda state from `_collect_rng_states` function and skip setting cuda state in _set_rng_states. Maybe this collection and setting could be skipped if start method of trainer is not spawn\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-01-06T09:12:23Z",
        "body": "Adding an argument to exclude the cuda seed would be possible, but let me first check what the root cause is and whether we can fix it properly. "
      },
      {
        "user": "awaelchli",
        "created_at": "2023-01-06T23:42:15Z",
        "body": "@sergeevii123 I cannot reproduce this with the pytorch version 1.11.0 and PL version 1.8.6 like you have reported. This is weird, I can't explain how you get this error. \r\n\r\nSomehow you get your processes being created with the fork start method. Is there anything special about your environment? Are you running this in a notebook? I'm not sure what I'm doing different."
      },
      {
        "user": "sergeevii123",
        "created_at": "2023-01-09T07:52:25Z",
        "body": "> @sergeevii123 I cannot reproduce this with the pytorch version 1.11.0 and PL version 1.8.6 like you have reported. This is weird, I can't explain how you get this error.\r\n> \r\n> Somehow you get your processes being created with the fork start method. Is there anything special about your environment? Are you running this in a notebook? I'm not sure what I'm doing different.\r\n\r\nInteresting, I'm running this in dev-contaier(docker image) on the kubeflow as a python script. Maybe it somehow changes the start method because some of the docker settings or environment settings🤔\r\nHow can I log the start method in my script?\r\n \r\nDocker image is `nvidia/cuda:11.3.1-cudnn8-devel-ubuntu20.04`"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-01-11T22:19:51Z",
        "body": "Hey @sergeevii123 sorry for the late answer. You can check the start method like this:\r\n\r\n```py\r\nimport torch.multiprocessing as mp\r\nprint(mp.get_start_method())\r\n```"
      },
      {
        "user": "sergeevii123",
        "created_at": "2023-01-14T12:28:57Z",
        "body": "> Hey @sergeevii123 sorry for the late answer. You can check the start method like this:\r\n> \r\n> ```python\r\n> import torch.multiprocessing as mp\r\n> print(mp.get_start_method())\r\n> ```\r\n\r\n@awaelchli \r\nhere is the method`starting method fork`\r\n\r\nthe code that I've used\r\n```\r\nfrom torch.utils.data import DataLoader\r\nimport pytorch_lightning as pl\r\nfrom torch.utils.data import Dataset\r\nimport torch\r\nfrom torch import nn\r\nimport torch.nn.functional as F\r\n\r\nimport torch.multiprocessing as mp\r\n\r\n\r\nclass LitAutoEncoder(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\r\n        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\r\n\r\n    def forward(self, x):\r\n        # in lightning, forward defines the prediction/inference actions\r\n        embedding = self.encoder(x)\r\n        return embedding\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defines the train loop. It is independent of forward\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = F.mse_loss(x_hat, x)\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\nclass DummyDataset(Dataset):\r\n    def __init__(self) -> None:\r\n        super().__init__()\r\n        \r\n    def __getitem__(self,  index: int):\r\n        with pl.utilities.seed.isolate_rng():\r\n            return [torch.ones(28,28),torch.ones(28,28)] \r\n    \r\n    def __len__(self):\r\n        return 10\r\n        \r\nif __name__ == \"__main__\": \r\n    print(\"starting method\", mp.get_start_method())\r\n    dataset = DummyDataset()\r\n    trainer = pl.Trainer(logger=None, accelerator='gpu')\r\n    trainer.fit(LitAutoEncoder(), DataLoader(dataset, num_workers=2))\r\n```\r\nError with `RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method` persists"
      },
      {
        "user": "sergeevii123",
        "created_at": "2023-01-14T12:31:15Z",
        "body": "Maybe this starting method could be checked in the `isolate_rng` so it will not result in the error"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-01-18T01:38:35Z",
        "body": "@sergeevii123 I think it would be fine to add an argument to the function `include_cuda=True|False`. \r\n\r\nOne could detect the start method yes, but that alone doesn't tell us whether we are running the function in a forked process or the main one, which I don't know how to detect like that.  So, if it's between the two, I would prefer the optional argument."
      },
      {
        "user": "sergeevii123",
        "created_at": "2023-01-18T17:03:02Z",
        "body": "> @sergeevii123 I think it would be fine to add an argument to the function `include_cuda=True|False`.\r\n> \r\n> One could detect the start method yes, but that alone doesn't tell us whether we are running the function in a forked process or the main one, which I don't know how to detect like that. So, if it's between the two, I would prefer the optional argument.\r\n\r\nAgree, sounds good. I can make PR to add the argument"
      },
      {
        "user": "jamesgunnfiveai",
        "created_at": "2023-01-19T14:47:01Z",
        "body": "Hi folks, just hit this same issue in lightning 1.9.0, but via a different path. I'm using fault tolerant training on a map-style dataset, using the \"ddp\" strategy. The worker loop ends up calling `state_dict` on `CaptureMapDataset` which makes a call to `_collect_rng_states()` and ends up triggering the same code path as stated above, leading to the same crash.\r\n\r\n```\r\nOriginal Traceback (most recent call last):\r\n  File \".../torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \".../torch/utils/data/_utils/fetch.py\", line 52, in fetch\r\n    return self.collate_fn(data)\r\n  File \".../pytorch_lightning/utilities/auto_restart.py\", line 418, in _capture_metadata_collate\r\n    metadata = dataset.state_dict()\r\n  File \".../pytorch_lightning/utilities/auto_restart.py\", line 275, in state_dict\r\n    return {self.worker_id: {\"rng_states\": _collect_rng_states()}}\r\n  File \".../lightning_fabric/utilities/seed.py\", line 111, in _collect_rng_states\r\n    \"torch.cuda\": torch.cuda.get_rng_state_all(),\r\n  File \".../torch/cuda/random.py\", line 39, in get_rng_state_all\r\n    results.append(get_rng_state(i))\r\n  File \".../torch/cuda/random.py\", line 22, in get_rng_state\r\n    _lazy_init()\r\n  File \".../torch/cuda/__init__.py\", line 207, in _lazy_init\r\n    raise RuntimeError(\r\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n```\r\n\r\nWould you prefer a new bug for this one, or is there already sufficient info on this ticket to propose a way of solving this?"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-01-22T15:54:43Z",
        "body": "@jamesgunnfiveai Yes, in the `CaptureMapDataset.state_dict`, we should call the function `_collect_rng_states` with `include_cuda=True` only on systems that don't use fork as the default (macos and windows) for dataloader processes, and on Linux we should call `include_cuda=False`. That's my guess. A PR for this would be welcome :) "
      },
      {
        "user": "jamesgunnfiveai",
        "created_at": "2023-01-23T10:15:56Z",
        "body": "@awaelchli - I can certainly take a look, though I'm not familiar at all with these lower level cuda calls. Do you think it would be more robust to instead check whether we're _in_ a forked process (and then set `include_cuda=False`) rather than just checking the system default? It's perfectly reasonable to collect the cuda rng states if we're using num_workers=0 for example, even on Linux.\r\nI could modify `state_dict` to look something like this instead, which should handle all cases?\r\n```\r\ndef state_dict(self) -> Dict[int, Dict[str, Any]]:\r\n    return {self.worker_id: {\"rng_states\": _collect_rng_states(include_cuda=torch.cuda.is_initialized())}}\r\n```\r\nIf you think this is ok, I'll raise a PR. The only concern I would still have with this is that nothing will collect the cuda random state for num_workers>0 on linux, because no worker has access to initialised CUDA. But that could be discussed on the PR."
      },
      {
        "user": "awaelchli",
        "created_at": "2023-01-23T10:29:00Z",
        "body": "Sounds good. Unfortunately, with a quick research I couldn't find a good way to check whether we are in a forked process. \r\n`torch.cuda.is_initialized()` could be a way around, but technically no what we want. \r\n\r\n> The only concern I would still have with this is that nothing will collect the cuda random state for num_workers>0 on linux, because no worker has access to initialised CUDA. But that could be discussed on the PR.\r\n\r\nUnfortunately I have no better idea."
      },
      {
        "user": "jamesgunnfiveai",
        "created_at": "2023-01-23T10:59:12Z",
        "body": "> Unfortunately I have no better idea.\r\n\r\nNor do I! I'll get a PR sorted."
      }
    ]
  },
  {
    "number": 13203,
    "title": "DummyLogger AttributeError when calling WandbLogger methods with fast_dev_run",
    "created_at": "2022-06-01T14:55:41Z",
    "closed_at": "2022-06-22T15:51:43Z",
    "labels": [
      "feature",
      "help wanted",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/13203",
    "body": "## 🐛 Bug\r\nWhen using the `fast_dev_run` flag `pytorch-lightning` replaces the logger with a `DummyLogger` object. But with a WandbLogger I also log media with `self.logger.log_text` in my pl module.  I therefore get an AttributeError that the DummyLogger does not have the attribute `log_text`. \r\n\r\n### To Reproduce\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.loggers import WandbLogger\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        self.logger.log_text(\r\n            \"samples\",\r\n            columns=[\"epoch\", \"temperatue\", \"text\"],\r\n            data=[[1, 0.7, \"generated_text\"]],\r\n        )\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    wandb_logger = WandbLogger(project=\"myproject\", offline=False)\r\n\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        logger=wandb_logger,\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        limit_test_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n        fast_dev_run=True\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe `DummyLogger` ignores attribute access of external loggers. \r\n\r\n### Environment\r\n\r\n\r\n```bash\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.22.3\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.11.0\r\n\t- pytorch-lightning: 1.6.3\r\n\t- tqdm:              4.64.0\r\n* System:\r\n\t- OS:                Darwin\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         i386\r\n\t- python:            3.9.10\r\n\t- version:           Darwin Kernel Version 21.4.0: Fri Mar 18 00:45:05 PDT 2022; root:xnu-8020.101.4~15/RELEASE_X86_64\r\n```\r\n\r\n### Additional context\r\n\r\nRequires probably a `wandb` setup.\r\n\n\ncc @borda @awaelchli @edward-io @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/13203/comments",
    "author": "HallerPatrick",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2022-06-02T11:41:56Z",
        "body": "This suggestion makes sense to me. But it should still print a warning that \"The DummyLogger is being used and the method `log_text` was called on it but it does not exist.\""
      },
      {
        "user": "HallerPatrick",
        "created_at": "2022-06-02T12:46:35Z",
        "body": "I could take over this, if it is okay\r\n"
      }
    ]
  },
  {
    "number": 12177,
    "title": "[RFC] Where to save checkpoints and profiler output in case of multiple loggers?",
    "created_at": "2022-03-02T07:34:55Z",
    "closed_at": "2022-08-26T17:23:55Z",
    "labels": [
      "help wanted",
      "discussion",
      "logger",
      "checkpointing",
      "profiler"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/12177",
    "body": "### Background \r\nCurrently, in the case that the user does not specify a `dirpath` to `ModelCheckpoint` or a `dirpath` to the `Profiler`, we write our output to the Logger, when there is exactly one Logger. However, in the case of multiple loggers we just write to `default_root_dir`.\r\n\r\nThe exact priority followed for one logger:\r\n1. if a `dirpath` is provided, write to it\r\n2. if not, write to the Logger\r\n\r\nThe exact priority followed for multiple loggers (or no loggers):\r\n1. if a `dirpath` is provided, write to it\r\n2. if not, write to `default_root_dir`\r\n\r\nThe purpose of this issue is to make sure everyone is aware of this behavior, and to see if there is a way to improve it.\r\n\r\n### Discussion Questions\r\nI'd like to discuss two questions here.\r\n1. Can we confirm that this is the behavior we want for when we have one logger?\r\n2. What behavior do we want for when we have multiple loggers? I see a few options:\r\n    A. Write to `default_root_dir` (keep current behavior)\r\n    B. Write to first logger in the list of loggers\r\n    C. Write to all loggers in the list of loggers \r\n\r\n\r\n\n\ncc @borda @awaelchli @edward-io @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy @ninginthecloud @otaj @carmocca @kaushikb11 @nbcsm @guotuofeng",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/12177/comments",
    "author": "daniellepintz",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-03-12T02:34:30Z",
        "body": "> Can we confirm that this is the behavior we want for when we have one logger?\r\n\r\nI can confirm. From my side it is clear this is a good default behavior and would prefer to keep it this way.\r\n\r\n\r\n> What behavior do we want for when we have multiple loggers? I see a few option\r\n\r\n**I vote for option B.** This gives the benefit of versioning. The default root dir does not get polluted and you don't have to \"invent\" a name for the folder. Option C is not practical IMO due to the duplication of large checkpoint files."
      },
      {
        "user": "daniellepintz",
        "created_at": "2022-03-12T03:43:35Z",
        "body": "Thanks @awaelchli for your input!! \r\n\r\nI like option B as well! I agree option C seems impractical."
      },
      {
        "user": "daniellepintz",
        "created_at": "2022-03-12T03:46:28Z",
        "body": "Would be great to get some more opinions @PyTorchLightning/core-lightning"
      },
      {
        "user": "tchaton",
        "created_at": "2022-03-14T19:30:52Z",
        "body": "Same here, I would vote for B. Let's make sure this gets documented."
      },
      {
        "user": "daniellepintz",
        "created_at": "2022-03-15T13:51:44Z",
        "body": "I think we can go with option B, since two people are in favor and most other people probably don't have an opinion"
      }
    ]
  },
  {
    "number": 11703,
    "title": "Using the step argument raises an error for WandbLogger's log_image",
    "created_at": "2022-02-02T18:16:34Z",
    "closed_at": "2022-02-05T01:02:41Z",
    "labels": [
      "bug",
      "help wanted",
      "logger: wandb"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/11703",
    "body": "## 🐛 Bug\r\n\r\nWhen using the WandbLogger log_image method, adding the step argument raises an error:\r\n```\r\n  File \"/home/nathan/work/these/engine/venv/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py\", line 417, in log_image\r\n    if len(v) != n:\r\nTypeError: object of type 'int' has no len()\r\n```\r\n\r\n### To Reproduce\r\n```\r\n\r\nfrom pytorch_lightning.loggers import WandbLogger\r\nimport torch\r\nlogger=WandbLogger()\r\nfake_img = torch.randn((50,50))\r\nlogger.log_image(key='fake', images=[fake_img], step=0)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe image is logged with appropriate step in Wandb.\r\n\r\n### Environment\r\n\r\n- PyTorch Lightning Version (e.g., 1.5.0): stable\r\n\r\n\r\n### Additional context\r\n\r\nCan be easily fixed by swapping a few lines in the method.\r\n\n\ncc @awaelchli @morganmcg1 @AyushExel @borisdayma @scottire @manangoel99",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/11703/comments",
    "author": "NathanGodey",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2022-02-03T09:23:49Z",
        "body": "hey @NathanGodey \r\n\r\ngood catch! mind send a PR?"
      }
    ]
  },
  {
    "number": 11043,
    "title": "RichProgressBar doesn't display progress bar when using Comet logger.",
    "created_at": "2021-12-13T14:44:46Z",
    "closed_at": "2022-10-26T00:06:18Z",
    "labels": [
      "bug",
      "help wanted",
      "progress bar: rich",
      "logger: comet"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/11043",
    "body": "## 🐛 Bug\r\n\r\nRichProgressBar doesn't display progress bar when using Comet logger.\r\nI verified it works correctly with tensorboard and wandb.\r\n\r\n\r\n### To Reproduce\r\n```python\r\nimport comet_ml\r\nimport os\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom pytorch_lightning.loggers import CometLogger\r\nfrom pytorch_lightning.callbacks import RichProgressBar\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size: int, length: int):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"x\"] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def predict_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n\r\nmodel = BoringModel()\r\n\r\nlogger = CometLogger(api_key=os.environ.get(\"COMET_API_TOKEN\"))\r\n\r\ntrainer = Trainer(logger=logger, max_epochs=100, callbacks=[RichProgressBar()])\r\n# trainer = Trainer(logger=logger, max_epochs=100)\r\n\r\ntrainer.fit(model=model)\r\n```\r\n\r\n### Environment\r\n- PyTorch Lightning Version 1.5.5\r\n- PyTorch Version 1.10.0\r\n- Python version 3.8\r\n- OS Ubuntu 20.04\n\ncc @kaushikb11 @rohitgr7 @SeanNaren",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/11043/comments",
    "author": "ashleve",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2022-01-14T16:31:33Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-02-26T01:38:29Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "JackLin-Authme",
        "created_at": "2022-03-21T07:41:22Z",
        "body": "Is there any update for this bug?"
      },
      {
        "user": "ItamarKanter",
        "created_at": "2022-06-26T08:08:11Z",
        "body": "Any update on this issue?\r\nI'm experiencing the same problem when using `comet` logger and `RichProgressBar`"
      },
      {
        "user": "awaelchli",
        "created_at": "2022-10-26T00:06:18Z",
        "body": "@ItamarKanter @JackLin-Authme I just tried this and can see the rich progress bar working fine. Is it possible that I am using a newer version of either rich or comet that now fixed the problem? Do you still have documentation of what version(s) you were using?\r\n\r\nI'm closing the issue now, but if you find any more issues related to this we can continue the investigation. "
      },
      {
        "user": "Pedrexus",
        "created_at": "2023-02-05T06:48:41Z",
        "body": "I still experience the issue. Adding more information on this, the progress bar DO show, however only after it has been completed. Moreover, any `rich.print` calls show no color, including the progress bar itself. The only solution I found is to stop using the Comet logger.\r\n\r\npackage versions:\r\npytorch-lightning     1.9.0\r\ncomet-ml                 3.32.0\r\nrich                          13.3.1\r\n\r\n"
      }
    ]
  },
  {
    "number": 10506,
    "title": "Multiple ModelCheckpoint callbacks format checkpoint name as {epoch}-{step}-v{version}.ckpt regardless of monitor",
    "created_at": "2021-11-12T13:48:51Z",
    "closed_at": "2021-11-15T10:20:28Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/10506",
    "body": "## 🐛 Bug\r\n\r\nUsing more than one ModelCheckpoint is now supported as of Lightning 1.5, but if monitors are provided, the filenames of the stored checkpoints should auto-insert the metric name by default. \r\n\r\nInstead, they are simply the standard multiple-checkpoints-in-same-directory format using the version numbers as so, for example using three callbacks with different monitors each gives:\r\n```\r\nepoch=1-step=200.ckpt\r\nepoch=1-step=200-v1.ckpt\r\nepoch=1-step=200-v2.ckpt\r\n```\r\n\r\n### To Reproduce\r\n\r\n### Expected behavior\r\n\r\nThe filenames should be named as so:\r\n```\r\nepoch=1-step=200-metric1=XX.ckpt\r\nepoch=1-step=200-metric2=YY.ckpt\r\nepoch=1-step=200-metric3=ZZ.ckpt\r\n```\r\n\r\n### Environment\r\n\r\n- PyTorch Lightning Version: 1.5.0\r\n- PyTorch Version: 1.10.0\r\n- Python version: 3.8\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/10506/comments",
    "author": "AAnoosheh",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-11-12T14:32:08Z",
        "body": "you can set you filename like this:\r\n```py\r\nModelCheckpoint(filename='{epoch}-{step}-{metric1}', monitor='metric1'...)\r\nModelCheckpoint(filename='{epoch}-{step}-{metric2}', monitor='metric2', ...)\r\nModelCheckpoint(filename='{epoch}-{step}-{metric3}', monitor='metric3', ...)\r\n```"
      },
      {
        "user": "tchaton",
        "created_at": "2021-11-15T10:20:28Z",
        "body": "Hey @AAnoosheh,\r\n\r\nI believe @rohitgr7 gave you the way forward. You can easily create multiple ModelCheckpoint with your metric keys and it should work as expected. Don't forget to pass them all when restoring your training from a checkpoint.\r\n\r\nClosing this issue for now, feel free to re-open it if you have more questions."
      }
    ]
  },
  {
    "number": 10285,
    "title": "UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop",
    "created_at": "2021-11-01T05:06:39Z",
    "closed_at": "2021-12-08T11:31:38Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/10285",
    "body": "## 🐛 Bug\r\n\r\nUsing pytorch-lightning 1.5.0rc1, I will get UserWarning:\r\n```\r\npytorch_lightning/trainer/configuration_validator.py:156: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\r\n  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\r\n```\r\nBut with pytorch-lightning 1.4.9, there is no such warning.\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom collections import OrderedDict\r\nimport pytorch_lightning as pl\r\n\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\n\r\n\r\nclass TestLrModule(pl.LightningModule):\r\n    def __init__(self, input_size, hidden_size):\r\n        super(TestLrModule, self).__init__()\r\n        self._fc = OrderedDict([\r\n            ('fc0', nn.Linear(input_size, hidden_size)),\r\n            ('tan0', nn.ReLU()),\r\n            ('fc1', nn.Linear(hidden_size, 1)),\r\n        ])\r\n        self.fc = nn.Sequential(self._fc)\r\n        self._loss_fn = nn.MSELoss()\r\n\r\n    def forward(self, x):\r\n        y = self.fc(x)\r\n        return y.squeeze(dim=1)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return self._loss_fn(y_hat, y)\r\n\r\n    def training_epoch_end(self, outputs):\r\n        loss = torch.mean(torch.stack([x['loss'] for x in outputs]))\r\n        self.log('train_loss', loss, on_epoch=True)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return self._loss_fn(y_hat, y)\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        loss = torch.mean(torch.stack(outputs))\r\n        self.log('val_loss', loss, on_epoch=True)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=2e-3, weight_decay=1e-4)\r\n\r\n\r\nsample, feature = 4000, 10\r\nrx, ry = torch.rand(sample, feature), torch.rand(sample)\r\ntest_sample = int(sample * 0.2)\r\ntest_rx, test_ry = torch.rand(test_sample, feature), torch.rand(test_sample)\r\n\r\ntrain_data = DataLoader(TensorDataset(rx, ry), batch_size=32, num_workers=2)\r\nvalid_data = DataLoader(TensorDataset(test_rx, test_ry), batch_size=32, num_workers=2)\r\n\r\nm = TestLrModule(rx.shape[1], 16)\r\ntrainer = pl.Trainer(max_epochs=20)\r\ntrainer.fit(m, train_data, valid_data)\r\n```\r\n\r\n### Environment\r\n\r\n- PyTorch Version  1.8.0\r\n- Python version: 3.8.5\r\n- OS (e.g., Linux): linux\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/10285/comments",
    "author": "7starsea",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-11-01T09:14:50Z",
        "body": "it was fixed recently. Can you try master?"
      },
      {
        "user": "7starsea",
        "created_at": "2021-11-01T13:33:53Z",
        "body": "Thanks. With the master version, the ```UserWarning: you defined a validation_step but have no val_dataloader``` disappears. \r\n\r\nBy the way, I am not sure should I take care of the following ```UserWarning```:\r\n```configuration_validator.py:102: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).```\r\n\r\nThe sample code is like\r\n```\r\nclass TestLrModule(pl.LightningModule):\r\n    # standard training/validation_step here\r\n    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\r\n        optimizer.zero_grad(set_to_none=True)\r\n\r\ntrainer = pl.Trainer(max_epochs=max_epochs, callbacks=[early_stop_callback],\r\n                          check_val_every_n_epoch=4, accumulate_grad_batches=6)\r\n```\r\n\r\nThanks."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-12-08T03:34:02Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-12-08T11:31:38Z",
        "body": "@7starsea apologies for the late reply. While accumulating, optimization doesn't happen at every step thus optimizer_step won't be called right after every training batch but only when the accumulation interval is complete. This is just a warning for the user to make sure they write their own custom logic within the `optimizer_step` taking the accumulation flag, set inside Trainer, into consideration."
      },
      {
        "user": "gezabohus",
        "created_at": "2024-07-14T11:02:21Z",
        "body": "This is still happening, pytorch-lightning 2.3.3, python 3.10."
      },
      {
        "user": "llctrautmann",
        "created_at": "2024-08-02T21:06:30Z",
        "body": "I am having the same issue with pytorch-lightning 2.3.3, python 3.11. Happy to share my training loop if that helps. "
      }
    ]
  },
  {
    "number": 10245,
    "title": "Empty list for gpus throws MisconfigurationException instead of running on CPU",
    "created_at": "2021-10-29T13:03:46Z",
    "closed_at": "2021-11-01T18:37:39Z",
    "labels": [
      "feature",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/10245",
    "body": "## 🐛 Bug\r\n\r\nWhen setting gpus to an empty list, it throws a MisconfigurationException instead of just not using a gpu, as I would expect.\r\n\r\n### To Reproduce\r\n\r\nRun any model with `Trainer(gpus=[])`\r\n\r\n### Expected behavior\r\n\r\nruns on CPU instead of crashing\r\n\r\n### Environment\r\nN/A\r\n\r\n### Additional context\r\nN/A\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/10245/comments",
    "author": "victorjoos",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-10-29T13:04:59Z",
        "body": "@kaushikb11 Mind checking this ?"
      },
      {
        "user": "tchaton",
        "created_at": "2021-10-29T13:05:32Z",
        "body": "Dear @victorjoos,\r\n\r\nWould you be willing to make a contribution. Should be fairly simple to resolve :)\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "victorjoos",
        "created_at": "2021-10-29T13:10:57Z",
        "body": "Yes, the PR is coming :)"
      },
      {
        "user": "kaushikb11",
        "created_at": "2021-10-29T13:13:34Z",
        "body": "@victorjoos Awesome! Assigned you to the issue. Feel free to ask questions if you have any."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-10-29T20:40:52Z",
        "body": "I would argue this not be treated as a bug as we had an explicit test for the empty list be unsupported. I think treating this as an enhancement would be more fitting :) Just a remark. Anyway, thanks for the PR"
      }
    ]
  },
  {
    "number": 10169,
    "title": "DDP on SLURM doesn't work when  `TrainingTypePlugin` is passed as strategy to `Trainer` ",
    "created_at": "2021-10-27T04:34:02Z",
    "closed_at": "2021-11-01T11:41:58Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/10169",
    "body": "## 🐛 Bug\r\nWhen passing `TrainingTypePlugin` as strategy to `Trainer`, as opposed to passing a string, DDP doesn't work on SLURM.\r\n\r\nFrom a quick look, it seems the issue is that `self._distrib_type` is not set, causing properties of `AcceleratorConnector` to return a wrong value: `use_ddp` returns false instead of true which makes `_configure_slurm_ddp` do nothing.\r\n\r\nCode for reproducing:\r\n```\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.plugins import DDPPlugin\r\nfrom pytorch_lightning.plugins.environments import SLURMEnvironment\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n\r\n    # Choose one of these:\r\n    strategy = DDPPlugin()\r\n    # strategy = \"ddp\"\r\n\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n        gpus=2,\r\n        strategy=strategy,\r\n    )\r\n    information = (\r\n        f\"PID {os.getpid()}: Trainer strategy: {trainer._accelerator_connector.strategy}\"\r\n        + \"\\n\"\r\n        + f\"PID {os.getpid()}: Is the trainer using SLURMEnvironment? {isinstance(trainer._accelerator_connector.cluster_environment, SLURMEnvironment)}\"\r\n    )\r\n    print(information + \"\\n\")\r\n\r\n    # Launching these will result in two separated trainings when strategy == DDPPlugin():\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/10169/comments",
    "author": "eladsegal",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2021-10-27T07:40:57Z",
        "body": "cc @awaelchli who knows more about slurm than I do.\r\n\r\n@eladsegal could you maybe provide a minimal example using the Boringmodel so that we can verify and debug this?"
      },
      {
        "user": "eladsegal",
        "created_at": "2021-10-27T16:41:16Z",
        "body": "Sure, I've updated the issue"
      },
      {
        "user": "kaushikb11",
        "created_at": "2021-10-29T13:32:05Z",
        "body": "Interesting, so this bug existed even before the `strategy` flag was introduced.\r\nFor instance, doing\r\n```python\r\nTrainer(gpus=2, plugins=DDPPlugin())\r\n```\r\n\r\nWorking on fixing this! 🎃"
      }
    ]
  },
  {
    "number": 10080,
    "title": "AttributeError: 'LightningDistributedModule' object has no attribute 'require_backward_grad_sync'",
    "created_at": "2021-10-22T12:10:39Z",
    "closed_at": "2021-12-10T08:10:13Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/10080",
    "body": "## 🐛 Bug\r\n\r\nWhen I use **manual_backward** ，this bug appear 。\r\nI know the new version may fix this, but for some reasons I can only use version 1.2.4, what should I do?\r\n\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         True\r\n        - version:           11.1\r\n* Packages:\r\n        - numpy:             1.21.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.8.1+cu111\r\n        - pytorch-lightning: 1.2.4\r\n        - tqdm:              4.62.3\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                -\r\n        - processor:         x86_64\r\n        - python:            3.7.11\r\n        - version:           #4 SMP Mon Mar 30 12:42:28 HKT 2020\r\n\r\n\r\nThanks for your help!!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/10080/comments",
    "author": "sunshuofeng",
    "comments": [
      {
        "user": "kaushikb11",
        "created_at": "2021-10-22T12:28:44Z",
        "body": "Hey @sunshuofeng, could you update PyTorch Lightning to the latest?"
      },
      {
        "user": "sunshuofeng",
        "created_at": "2021-10-22T13:15:45Z",
        "body": "I wish I could do this, but due to some version conflict issues, I can only use version 1.2\r\n\r\nWhat I want to do is optimize center Loss, so there will be two optimizers,I've tried many other solutions\r\n\r\nOptimization of Center Loss requires a coefficient multiplication of the gradient, and an error-free approach is：\r\n\r\n```\r\ndef optimizer_step（.......）：\r\n       ## optimize model\r\n       if optimizer_idx == 0:\r\n            optimizer.step(closure=optimizer_closure)\r\n\r\n       ## optimize center loss\r\n       else:\r\n            optimizer_closure()\r\n            for param in center_loss.parameters():\r\n                     param.grad.data*=weight\r\n           optimizer.step()\r\n```\r\n        \r\n\r\n\r\nBut this will propagate forward twice, which is obviously not necessary,So I tried the following:\r\n\r\n```\r\ndef optimizer_step（.......）：\r\n       if optimizer_idx == 0:\r\n            optimizer,optimizer_center=self.optimizers()\r\n            optimizer_closure()\r\n            for param in center_loss.parameters():\r\n                     param.grad.data*=weight\r\n             optimizer.step()\r\n             optimizer_center.step()\r\n```\r\n      \r\n\r\nBut the error will show that param.grad is None\r\n\r\nSo I guess I'll just have to use manual optimization"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-10-22T15:32:47Z",
        "body": "Perhaps you could share more about what the version conflict issues are and we could try to help with that?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-11-22T17:06:08Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 10062,
    "title": "self.trainer.training/testing/validating/predicting all are False",
    "created_at": "2021-10-21T07:32:33Z",
    "closed_at": "2021-10-26T11:12:57Z",
    "labels": [
      "help wanted",
      "docs"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/10062",
    "body": "During the checking phase before the first training step, the all self.trainer.training/testing/validating/predicting status in Datamodule are Fasle.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/10062/comments",
    "author": "qmpzzpmq",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-10-21T09:41:58Z",
        "body": "in which hook are you checking?"
      },
      {
        "user": "qmpzzpmq",
        "created_at": "2021-10-21T10:02:51Z",
        "body": "@rohitgr7 on_after_batch_transfer\r\n"
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-10-21T10:16:42Z",
        "body": "must be happening during sanity_check. for that it's `self.trainer.sanity_checking`. you can disable it using `Trainer(num_sanity_val_steps=0)` if you want."
      },
      {
        "user": "qmpzzpmq",
        "created_at": "2021-10-21T10:43:22Z",
        "body": "@rohitgr7 I prefer to keep it but I don't know there is a state. Is possible to descrip it in datamodule on_after_batch_transfer 's doc?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-10-21T10:46:02Z",
        "body": "yeah sure.. PRs are always welcomed :)"
      }
    ]
  },
  {
    "number": 9930,
    "title": "`move_data_to_device` fails with dataclasses with `field(init=False)` fields",
    "created_at": "2021-10-14T12:56:01Z",
    "closed_at": "2021-10-17T07:10:47Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9930",
    "body": "## 🐛 Bug\r\n\r\n`move_data_to_device` has special handling of dataclasses, but in case the dataclass has a non-init field, `elem_type(**out)` in `apply_to_collection` will fail\r\n\r\n### To Reproduce\r\n```\r\nfrom pytorch_lightning.utilities import move_data_to_device\r\nimport torch\r\nfrom dataclasses import dataclass, field\r\nfrom typing import NamedTuple, List\r\n\r\n@dataclass\r\nclass Data:\r\n    example_id: List[str]\r\n    x: torch.Tensor\r\n    y: int = field(init=False)\r\n\r\n    def __post_init__(self):\r\n        self.y = 3\r\n\r\nbatch_size = 5\r\n\r\nbatch = Data(\r\n    example_id=[f\"e-{i}\" for i in range(batch_size)],\r\n    x = torch.rand(batch_size),\r\n)\r\n\r\nassert batch.y == 3\r\ndevice = torch.device(\"cuda:0\")\r\n\r\nmove_data_to_device(batch=batch, device=device)\r\n```\r\n### Expected behavior\r\nShould run without exceptions\r\n### Environment\r\n* Packages:\r\n        - numpy:             1.21.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.1\r\n        - pytorch-lightning: 20211014\r\n        - tqdm:              4.62.3\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         x86_64\r\n        - python:            3.9.7\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9930/comments",
    "author": "ronif",
    "comments": [
      {
        "user": "ronif",
        "created_at": "2021-10-14T12:56:54Z",
        "body": "I think this is an easy fix. Can submit a PR if needed."
      },
      {
        "user": "justusschock",
        "created_at": "2021-10-14T22:52:52Z",
        "body": "@ronif that would be great!"
      }
    ]
  },
  {
    "number": 9928,
    "title": "loading large model not finished after 16 hours",
    "created_at": "2021-10-14T12:23:20Z",
    "closed_at": "2023-12-31T00:26:38Z",
    "labels": [
      "bug",
      "help wanted",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9928",
    "body": "## 🐛 Bug\r\n\r\nAfter saving a checkpoint of a large pytorch_forecasting model (10'100 targets forecasting based on previous 10'000 values ), loading the checkpoint lasts forever (i killed loading after 16 hours).\r\n\r\nWith 5 targets forecasting based on previous 5 values -> works\r\nWith 50 targets forecasting based on previous 50 values -> works\r\nWith 500 targets forecasting based on previous 500 values -> works\r\nWith 5000 targets forecasting based on previous 5000 values -> in test\r\n\r\nAtm. trying to find the amount where it stops working.\r\n\r\n\r\n### To Reproduce\r\n\r\nThe model hast 10'100 target values, the prediction is 24 time intervals based on 48 past time intervals.\r\nThe predictions are quantiles(0.25,0.5,0.75)\r\n\r\n### Expected behavior\r\n\r\nthe model can be loaded in a reasonable timeframe.\r\n\r\n### Environment\r\n\r\n\r\n\r\n- PyTorch 1.4.9\r\n- PyTorch 1.9.1\r\n- Python version: 3.8.10\r\n- OS (e.g., Linux): Ubuntu 20.04\r\n- CUDA/cuDNN version: -\r\n- GPU models and configuration: -\r\n- How you installed PyTorch (`conda`, `pip`, source): pip\r\n\r\n### Additional context\r\n\r\nLoading and saving of small models works well.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9928/comments",
    "author": "mijosch",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2021-10-14T12:48:44Z",
        "body": "Hi, can you provide an exemplary implementation of your model so that we can investigate? Can you also try to simply load the checkpoint with torch.load? If that also takes that long the issue is not on our side but on the PyTorch side.\n\nHow large is your checkpoint file?"
      },
      {
        "user": "mijosch",
        "created_at": "2021-10-14T12:55:18Z",
        "body": "I can open it with torch.load() it is about 280MB in size.\r\n\r\nthis is the dataset, as I already wrote it is a table with ~10100 columns \r\nI am just running the tests again with measure the time needed for loading.\r\n\r\ntraining = TimeSeriesDataSet(\r\n    data[lambda x: x.time_idx <= training_cutoff],\r\n    time_idx=\"time_idx\",\r\n    target=list(columns),\r\n    group_ids=[\"group\"],\r\n    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\r\n    max_encoder_length=max_encoder_length,\r\n    min_prediction_length=1,\r\n    max_prediction_length=max_prediction_length,\r\n    static_categoricals=[],\r\n    static_reals=[],\r\n    time_varying_known_categoricals=[],\r\n    #variable_groups=[\"group\"] ,  # group of categorical variables can be treated as one variable\r\n    time_varying_known_reals=[\"time_idx\"],\r\n    #time_varying_unknown_categoricals=[],\r\n    time_varying_unknown_reals=list(data.columns.values),\r\n    target_normalizer=MultiNormalizer(tn), \r\n    add_relative_time_idx=True,\r\n    add_target_scales=True,\r\n    add_encoder_length=True,\r\n    allow_missing_timesteps=True,\r\n)\r\n\r\ntn is a list of TorchNormalizer() with the count of the columns"
      },
      {
        "user": "mijosch",
        "created_at": "2021-10-14T13:23:29Z",
        "body": "2 targets from 2 columns :\r\ntorch.load -> 0.032s\r\nsetting state_dict -> 0.0006s\r\nstate_dict load -> 0.017\r\ntft.load_from_checkoint -> 0.076s\r\n\r\n10 targets from10 columns:\r\ntorch.load -> 0.08s\r\nsetting state_dict -> 0.00119s\r\nstate_dict load -> 0.068\r\ntft.load_from_checkoint -> 0.195s\r\n\r\n100 targets from 100 columns:\r\ntorch.load -> 0.61s\r\nsetting state_dict -> 0.0095s\r\nstate_dict load -> 3.015\r\ntft.load_from_checkoint -> 3.74s\r\n\r\n1000 targets from 1000 columns:\r\ntorch.load -> 6.4s\r\nsetting state_dict -> 0.122s\r\nstate_dict load -> 352.154\r\ntft.load_from_checkoint -> 351.1s\r\n\r\n2000 targets from 2000 columns:\r\ntorch.load -> 12.3s\r\nsetting state_dict -> 0.25s\r\nstate_dict load -> ----\r\ntft.load_from_checkoint -> -----\r\n\r\n3000 targets from 3000 columns:\r\ntorch.load -> 16.7s\r\nsetting state_dict -> 0.39s\r\nstate_dict load -> ----\r\ntft.load_from_checkoint -> -----\r\n\r\n2 targets from 10100 columns:\r\ntorch.load -> 17.12s\r\nsetting state_dict -> 0.415s\r\ntft.load_from_checkoint -> -----\r\n\r\n\r\nwhere setting state_dict :\r\n-->>tft.state_dict = ckpnt['state_dict']\r\n\r\nwhere state_dict load :\r\n-->>tft.load_state_dict(ckpnt['state_dict'])\r\n\r\nSo this is not usable for me...\r\nBasically I just want to load the trained model for inference, I don't need all the other stuff. (sadly pickle the whole model after training does not work)\r\nThe last increase of 10x leads to nearly 100x higher load time while the pure load times seems to be linear.\r\nNot sure if the issue is in the pytorch library itself.\r\n\r\n\r\nEdit:\r\nso I added some more tests, it seems like the load_state_dict function is what makes it slow. I dont know if assigning the state_dict is the right way but it works faster -> have not tested inference yet\r\n\r\nEdit Edit:\r\nload_state_dict and just set state_dict give different results, so you need the load_state_dict.\r\n"
      },
      {
        "user": "justusschock",
        "created_at": "2021-10-14T22:54:12Z",
        "body": "I see, can you try to do it with a torch.nn.Module as a baseclass instead? just to see if this is something we are doing different from the original pytorch code?"
      },
      {
        "user": "mijosch",
        "created_at": "2021-10-15T06:35:03Z",
        "body": "I have other models in pure pytorch with custom written training loops that have the size of about 200MB and never had issues with loading them but they are just fully convolutional models with maybe less keys in the state_dict. The one here is a temporal fusion transformer and I dont know the structure of it.\r\n\r\nBut I dont think that the size is the problem, I think the key count is the issue.\r\nThe 2 targets from 2 columns already has 292 keys in the state_dict.\r\nThe 10 targets from 10 columns has 668 keys in the state_dict.\r\nThe 100 targets from 100 columns has 4898 keys in the state_dict.\r\nThe 1000 targets from 1000 columns has 47198 keys in the state_dict.\r\n\r\nThe increase in keys is also some what linear I guess but the load time isnt.\r\n\r\nLoading the pickle from drive does not work in parallel but cant you assign the state_dict in parallel? \r\nI think one reason for being slow is, that the function does just use one thread (one CPU core is at 100% the rest is idling around). \r\nSo using a threadpoolexecutor and start a thread for each key in the state_dict might be an improvement.\r\nIs this a topic for pytorch?\r\n"
      },
      {
        "user": "justusschock",
        "created_at": "2021-10-18T09:53:43Z",
        "body": "Hi sorry for the late response. I just double checked and we are not doing anything special here. We just rely on the function provided by pytorch, so I do think it would be best if you open an issue with them :)"
      }
    ]
  },
  {
    "number": 9862,
    "title": "AttributeError: module 'tqdm' has no attribute 'auto' on PL import",
    "created_at": "2021-10-07T18:12:56Z",
    "closed_at": "2021-10-08T14:17:09Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9862",
    "body": "## 🐛 Bug\r\nWhen I import pl\r\n`import pytorch_lightning as pl`\r\n\r\nI get the following error:\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\nC:\\Users\\S2F5D~1.RED\\AppData\\Local\\Temp/ipykernel_2984/1918236736.py in <module>\r\n      7 \r\n      8 import torch\r\n----> 9 from pytorch_lightning import  LightningModule\r\n     10 \r\n     11 get_ipython().run_line_magic('matplotlib', 'inline')\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\pytorch_lightning\\__init__.py in <module>\r\n     18 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT)\r\n     19 \r\n---> 20 from pytorch_lightning import metrics  # noqa: E402\r\n     21 from pytorch_lightning.callbacks import Callback  # noqa: E402\r\n     22 from pytorch_lightning.core import LightningDataModule, LightningModule  # noqa: E402\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\pytorch_lightning\\metrics\\__init__.py in <module>\r\n     13 # limitations under the License.\r\n     14 \r\n---> 15 from pytorch_lightning.metrics.classification import (  # noqa: F401\r\n     16     Accuracy,\r\n     17     AUC,\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\pytorch_lightning\\metrics\\classification\\__init__.py in <module>\r\n     12 # See the License for the specific language governing permissions and\r\n     13 # limitations under the License.\r\n---> 14 from pytorch_lightning.metrics.classification.accuracy import Accuracy  # noqa: F401\r\n     15 from pytorch_lightning.metrics.classification.auc import AUC  # noqa: F401\r\n     16 from pytorch_lightning.metrics.classification.auroc import AUROC  # noqa: F401\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\pytorch_lightning\\metrics\\classification\\accuracy.py in <module>\r\n     14 from typing import Any, Callable, Optional\r\n     15 \r\n---> 16 from torchmetrics import Accuracy as _Accuracy\r\n     17 \r\n     18 from pytorch_lightning.metrics.utils import deprecated_metrics, void\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\torchmetrics\\__init__.py in <module>\r\n     12 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT)\r\n     13 \r\n---> 14 from torchmetrics import functional  # noqa: E402\r\n     15 from torchmetrics.audio import PIT, SI_SDR, SI_SNR, SNR  # noqa: E402\r\n     16 from torchmetrics.average import AverageMeter  # noqa: E402\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\torchmetrics\\functional\\__init__.py in <module>\r\n     58 from torchmetrics.functional.retrieval.reciprocal_rank import retrieval_reciprocal_rank\r\n     59 from torchmetrics.functional.self_supervised import embedding_similarity\r\n---> 60 from torchmetrics.functional.text.bert import bert_score\r\n     61 from torchmetrics.functional.text.bleu import bleu_score\r\n     62 from torchmetrics.functional.text.rouge import rouge_score\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\torchmetrics\\functional\\text\\bert.py in <module>\r\n    245 \r\n    246 \r\n--> 247 def _get_progress_bar(dataloader: DataLoader, verbose: bool = False) -> Union[DataLoader, tqdm.auto.tqdm]:\r\n    248     \"\"\"Helper function returning either the dataloader itself when `verbose = False`, or it wraps the dataloader with\r\n    249     `tqdm.auto.tqdm`, when `verbose = True` to display a progress bar during the embbeddings calculation.\"\"\"\r\n\r\nAttributeError: module 'tqdm' has no attribute 'auto'\r\n```\r\n\r\n### Expected behavior\r\nSuccessful import\r\n\r\n### Environment\r\nLibraries installed using conda\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           None\r\n* Packages:\r\n        - numpy:             1.21.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.1 \r\n        - pytorch-lightning: 1.4.9\r\n        - tqdm:              4.62.2\r\n* System:\r\n        - OS:                Windows\r\n        - architecture:\r\n                - 64bit\r\n                - WindowsPE\r\n        - processor:         Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\r\n        - python:            3.9.7\r\n        - version:           10.0.19042\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9862/comments",
    "author": "sidwa",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "created_at": "2021-10-08T09:57:54Z",
        "body": "hi,\r\n\r\nCan you try downgrading tqdm or Pytorch lightning?\r\n\r\nRegards"
      },
      {
        "user": "sidwa",
        "created_at": "2021-10-08T13:45:22Z",
        "body": "do you have a last working version in mind?"
      },
      {
        "user": "sidwa",
        "created_at": "2021-10-08T14:17:03Z",
        "body": "Import works with tqdm version 4.50.0 (arbitrarily chosen older version)."
      },
      {
        "user": "Programmer-RD-AI",
        "created_at": "2021-10-08T14:34:41Z",
        "body": "> do you have a last working version in mind?\r\n\r\nI am new in the PyTorch-Community so I don't know\r\n\r\nRegards.\r\n\r\n"
      },
      {
        "user": "Programmer-RD-AI",
        "created_at": "2021-10-08T14:38:51Z",
        "body": "you can try 1.4.8 or 1.4.7"
      },
      {
        "user": "sidwa",
        "created_at": "2021-10-08T14:58:08Z",
        "body": "@Programmer-RD-AI  Problem's fixed, I downgraded the tqdm package. Besides, 1.4.x isn't supported in the latest version of lightning, it needs >1.6 (environment.yml). Thank you for the suggestions"
      },
      {
        "user": "Programmer-RD-AI",
        "created_at": "2021-10-08T14:59:10Z",
        "body": "ok no problem \r\n\r\n"
      },
      {
        "user": "Rustemhak",
        "created_at": "2023-05-29T21:29:14Z",
        "body": "> @Programmer-RD-AI Problem's fixed, I downgraded the tqdm package. Besides, 1.4.x isn't supported in the latest version of lightning, it needs >1.6 (environment.yml). Thank you for the suggestions\r\n\r\n\r\nCan you please tell the version of what > 1.6? And I don’t understand, what does environment.yml have to do with it?\r\nIt is desirable that you can please share the final versions, if you still have it.\r\n"
      }
    ]
  },
  {
    "number": 9771,
    "title": "DeepSpeed & Lightning are both calling the scheduler",
    "created_at": "2021-09-30T16:33:31Z",
    "closed_at": "2021-10-01T14:35:44Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9771",
    "body": "## 🐛 Bug\r\n\r\nA user in our slack channel noticed that the scheduler step is being called by DeepSpeed and Lightning. Lightning should not call this, and DeepSpeed should be in control.\r\n\r\n### To Reproduce\r\n\r\nThis test fails!\r\n```python\r\nimport os\r\n\r\nimport mock\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)\r\n        return {\r\n            \"optimizer\": optimizer,\r\n            \"lr_scheduler\": {\r\n                \"scheduler\": scheduler,\r\n            },\r\n        }\r\n\r\n\r\n@mock.patch('torch.optim.lr_scheduler.StepLR.step', autospec=True)\r\ndef test_deepspeed(mock_step):\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=0,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        fast_dev_run=True,\r\n        gpus=1,\r\n        weights_summary=None,\r\n        plugins='deepspeed'\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    # once before training, and once during training.\r\n    assert mock_step.call_count == 2\r\n```\r\n\r\n### Expected behaviour\r\n\r\nAbove should succeed. I also see there may be an issue with `test` also calling the step function internally in DeepSpeed, but thats for a different issue.\r\n\r\ncc @tchaton \r\n\r\n### Environment\r\n\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce RTX 3090\r\n        - available:         True\r\n        - version:           11.1\r\n* Packages:\r\n        - numpy:             1.21.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.1\r\n        - pytorch-lightning: 1.5.0dev\r\n        - tqdm:              4.49.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.8\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9771/comments",
    "author": "SeanNaren",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-09-30T18:03:52Z",
        "body": "Does the plugin need to wrap these schedulers in order to prevent the duplicate step call? Do you see another solution?"
      },
      {
        "user": "SeanNaren",
        "created_at": "2021-09-30T18:37:10Z",
        "body": "@awaelchli what do you mean by wrap? I need to investigate what is happening internally, as I think from my initial glance we don't save the LR states outside of deepspeed, so I wonder if there is any point exposing the LR scheduler outside of deepspeed (in lightning)"
      }
    ]
  },
  {
    "number": 9697,
    "title": "IsADirectoryError: [Errno 21] Is a directory: '/home/pc/SR/dC/1-Data_Preparation'",
    "created_at": "2021-09-25T04:04:35Z",
    "closed_at": "2021-09-27T07:14:18Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9697",
    "body": "I am using **Jupyter Lab Server**. It has pre-installed tf2.3_py3.6 kernel installed in it. It has 2 GPUS in it.\r\n\r\nPyTorch Lightning Version (e.g., 1.3.0): '1.4.6'\r\nPyTorch Version (e.g., 1.8): '1.6.0+cu101'\r\nPython version: 3.6\r\nOS (e.g., Linux): system='Linux'\r\nCUDA/cuDNN version: 11.2\r\nGPU models and configuration: Mentioned below\r\nHow you installed PyTorch (conda, pip, source): pip\r\n\r\n\r\nNVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-SXM2...  Off  | 00000000:62:00.0 Off |                    0 |\r\n| N/A   36C    P0    57W / 300W |   2842MiB / 32510MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla V100-SXM2...  Off  | 00000000:89:00.0 Off |                    0 |\r\n| N/A   32C    P0    43W / 300W |      3MiB / 32510MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                              \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\nI have trained a model, and saved the best model.\r\n\r\n```\r\nclass SRTagger(pl.LightningModule):\r\n\r\n  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\r\n    super().__init__()\r\n    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\r\n    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\r\n    self.n_training_steps = n_training_steps\r\n    self.n_warmup_steps = n_warmup_steps\r\n    self.criterion = nn.BCELoss()\r\n\r\n  def forward(self, input_ids, attention_mask, labels=None):\r\n    output = self.bert(input_ids, attention_mask=attention_mask)\r\n    output = self.classifier(output.pooler_output)\r\n    output = torch.sigmoid(output)    \r\n    loss = 0\r\n    if labels is not None:\r\n        loss = self.criterion(output, labels)\r\n    return loss, output\r\n\r\n  def training_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\r\n    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\r\n\r\n  def validation_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\r\n    return loss\r\n\r\n  def test_step(self, batch, batch_idx):\r\n    input_ids = batch[\"input_ids\"]\r\n    attention_mask = batch[\"attention_mask\"]\r\n    labels = batch[\"labels\"]\r\n    loss, outputs = self(input_ids, attention_mask, labels)\r\n    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\r\n    return loss\r\n\r\n  def training_epoch_end(self, outputs):\r\n    \r\n    labels = []\r\n    predictions = []\r\n    for output in outputs:\r\n      for out_labels in output[\"labels\"].detach().cpu():\r\n        labels.append(out_labels)\r\n      for out_predictions in output[\"predictions\"].detach().cpu():\r\n        predictions.append(out_predictions)\r\n\r\n    labels = torch.stack(labels).int()\r\n    predictions = torch.stack(predictions)\r\n\r\n    for i, name in enumerate(LABEL_COLUMNS):\r\n      class_roc_auc = auroc(predictions[:, i], labels[:, i])\r\n      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\r\n\r\n\r\n  def configure_optimizers(self):\r\n\r\n    optimizer = optim.RAdam(self.parameters(), lr=2e-5)\r\n\r\n    scheduler = get_linear_schedule_with_warmup(\r\n      optimizer,\r\n      num_warmup_steps=self.n_warmup_steps,\r\n      num_training_steps=self.n_training_steps\r\n    )\r\n\r\n    return dict(\r\n      optimizer=optimizer,\r\n      lr_scheduler=dict(\r\n        scheduler=scheduler,\r\n        interval='step'\r\n      )\r\n    )\r\n\r\n```\r\n**After training, I want to load my best model without training it again**\r\n\r\n\r\n```\r\ncheckpoint_callback = ModelCheckpoint(\r\n  dirpath=\"checkpoints\",\r\n  filename=\"best-checkpoint\",\r\n  save_top_k=1,\r\n  verbose=True,\r\n  monitor=\"val_loss\",\r\n  mode=\"min\"\r\n)\r\n\r\nlogger = TensorBoardLogger(\"lightning_logs\", name=\"SReply\")\r\n\r\n# And early stopping triggers when the loss hasn't improved for the last \r\n# 2 epochs (you might want to remove/reconsider this when training on real-world projects):\r\n\r\n\r\nearly_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\r\n# We can start the training process:\r\n# checkpoint_callback supports only a bool value. If set to True, it will create a model checkpoint\r\n# instance internally, but if you want to assign your own custom instance then pass it within callbacks:\r\n\r\ntrainer = pl.Trainer(\r\n  logger=logger,\r\n  callbacks=[early_stopping_callback,checkpoint_callback],\r\n  max_epochs=N_EPOCHS,\r\n  gpus=1,\r\n  progress_bar_refresh_rate=50,\r\n  amp_level='O3'\r\n  )\r\n\r\ntrained_model = SRTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path,n_classes=len(LABEL_COLUMNS))\r\ntrained_model.eval()\r\ntrained_model.freeze()\r\n```\r\n\r\n\r\n**Error**\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nIsADirectoryError                         Traceback (most recent call last)\r\n/usr/local/bin/kernel-launchers/python/scripts/launch_ipykernel.py in <module>\r\n----> 1 trained_model = SRTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path,n_classes=len(LABEL_COLUMNS))\r\n      2 trained_model.eval()\r\n      3 trained_model.freeze()\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/core/saving.py in load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\r\n    129             checkpoint = pl_load(checkpoint_path, map_location=map_location)\r\n    130         else:\r\n--> 131             checkpoint = pl_load(checkpoint_path, map_location=lambda storage, loc: storage)\r\n    132 \r\n    133         if hparams_file is not None:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/cloud_io.py in load(path_or_url, map_location)\r\n     30         return torch.hub.load_state_dict_from_url(str(path_or_url), map_location=map_location)\r\n     31     fs = get_filesystem(path_or_url)\r\n---> 32     with fs.open(path_or_url, \"rb\") as f:\r\n     33         return torch.load(f, map_location=map_location)\r\n     34 \r\n\r\n~/.local/lib/python3.6/site-packages/fsspec/spec.py in open(self, path, mode, block_size, cache_options, **kwargs)\r\n    980                 autocommit=ac,\r\n    981                 cache_options=cache_options,\r\n--> 982                 **kwargs,\r\n    983             )\r\n    984             if not ac and \"r\" not in mode:\r\n\r\n~/.local/lib/python3.6/site-packages/fsspec/implementations/local.py in _open(self, path, mode, block_size, **kwargs)\r\n    143         if self.auto_mkdir and \"w\" in mode:\r\n    144             self.makedirs(self._parent(path), exist_ok=True)\r\n--> 145         return LocalFileOpener(path, mode, fs=self, **kwargs)\r\n    146 \r\n    147     def touch(self, path, **kwargs):\r\n\r\n~/.local/lib/python3.6/site-packages/fsspec/implementations/local.py in __init__(self, path, mode, autocommit, fs, compression, **kwargs)\r\n    234         self.compression = get_compression(path, compression)\r\n    235         self.blocksize = io.DEFAULT_BUFFER_SIZE\r\n--> 236         self._open()\r\n    237 \r\n    238     def _open(self):\r\n\r\n~/.local/lib/python3.6/site-packages/fsspec/implementations/local.py in _open(self)\r\n    239         if self.f is None or self.f.closed:\r\n    240             if self.autocommit or \"w\" not in self.mode:\r\n--> 241                 self.f = open(self.path, mode=self.mode)\r\n    242                 if self.compression:\r\n    243                     compress = compr[self.compression]\r\n\r\nIsADirectoryError: [Errno 21] Is a directory: '/home/pc/SR/dC/1-Data_Preparation'\r\n```\r\n\r\n**This error. comes when I try to load my model second time after closing and reopening the jupyter notebook. I run the code except training it.**\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9697/comments",
    "author": "pratikchhapolika",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2021-09-25T04:27:46Z",
        "body": "> This error. comes when I try to load my model second time after closing and reopening the jupyter notebook. I run the code except training it.\r\n\r\nDid you confirm that the directory `'/home/pc/SR/dC/1-Data_Preparation'` exists? "
      },
      {
        "user": "pratikchhapolika",
        "created_at": "2021-09-25T04:34:33Z",
        "body": "> > This error. comes when I try to load my model second time after closing and reopening the jupyter notebook. I run the code except training it.\r\n> \r\n> Did you confirm that the directory `'/home/pc/SR/dC/1-Data_Preparation'` exists?\r\n\r\nYes I am inside this directory : `'/home/pc/SR/dC/1-Data_Preparation'` and when I do\"\r\n\r\n`!ls '/home/pc/SR/dC/1-Data_Preparation/checkpoints`\r\n\r\nI get :  **best-checkpoint.ckpt**"
      },
      {
        "user": "pratikchhapolika",
        "created_at": "2021-09-27T02:27:04Z",
        "body": "@ananthsub any update on this?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-09-27T06:04:15Z",
        "body": "```py\r\ntrainer = pl.Trainer(\r\n  logger=logger,\r\n  callbacks=[early_stopping_callback,checkpoint_callback],\r\n  max_epochs=N_EPOCHS,\r\n  gpus=1,\r\n  progress_bar_refresh_rate=50,\r\n  amp_level='O3'\r\n  )\r\n\r\ntrained_model = SRTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path,n_classes=len(LABEL_COLUMNS))\r\ntrained_model.eval()\r\n```\r\nyou have created a new trainer instance and using a new checkpoint instance.. so `best_model_path` doesn't exist at this point."
      },
      {
        "user": "pratikchhapolika",
        "created_at": "2021-09-27T06:41:53Z",
        "body": "> ```python\r\n> trainer = pl.Trainer(\r\n>   logger=logger,\r\n>   callbacks=[early_stopping_callback,checkpoint_callback],\r\n>   max_epochs=N_EPOCHS,\r\n>   gpus=1,\r\n>   progress_bar_refresh_rate=50,\r\n>   amp_level='O3'\r\n>   )\r\n> \r\n> trained_model = SRTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path,n_classes=len(LABEL_COLUMNS))\r\n> trained_model.eval()\r\n> ```\r\n> \r\n> you have created a new trainer instance and using a new checkpoint instance.. so `best_model_path` doesn't exist at this point.\r\n\r\nSo what solution do you suggest?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-09-27T07:01:21Z",
        "body": "you can just pass the path of the best checkpoint as a string or maybe extract it from one of your saved checkpoints to reload it correctly."
      },
      {
        "user": "pratikchhapolika",
        "created_at": "2021-09-27T07:14:10Z",
        "body": "> you can just pass the path of the best checkpoint as a string or maybe extract it from one of your saved checkpoints to reload it correctly.\r\n\r\nSRTagger.load_from_checkpoint(\"path_to_cpkt_file\",n_classes=len(LABEL_COLUMNS))\r\ntrained_model.eval()\r\n\r\nClosing it."
      }
    ]
  },
  {
    "number": 9571,
    "title": "Validation sanity check seems to cause issues with very deep spectral_norm nets",
    "created_at": "2021-09-16T16:55:45Z",
    "closed_at": "2021-10-30T00:26:26Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9571",
    "body": "## 🐛 Bug\r\n\r\nI had a relatively complex GAN setup that was working fine. I attempted to use spectral norm changing every conv block from\r\n`nn.Conv1d(in, out, 3, 1 1, bias=False)` \r\nto\r\n`import torch.nn.utils.spectral_norm  as spectral_norm\r\nspectral_norm(nn.Conv1d(in, out, 3, 1 1, bias=False))` \r\nThis works outside of pytorch_lightning but when converted to pytorch_lightning code it would crash.\r\nThe crash was caused because the model was creating extremely large values that would eventually generate an inf in the loss function.  \r\n\r\nI am not allowed to share my code but I will share a number of things that fixed the issue and my conclusions from them:\r\n\r\n1. Without spectral norm everything worked fine. I therefore assume the issues is caused by or exacerbated by spectral_norm\r\n2. With the alternative spectral_norm implementation (from torch.nn.utils.parametrizations import spectral_norm) I ran into a completely different set of errors and did not pursue this avenue any further: \r\n> RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]] is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\r\n3. Reducing the number of layers in the network fixed the issue (not a satisfactory fix though).\r\n4. Doing a forward pass with the network (train mode) with a single example before calling trainer.fit(model, pldataloader) solved the issue.\r\n5. setting num_sanity_val_steps=0 fixed the issue (not a satisfactory fix though as I would quite like a sanity check). \r\n\r\nThe problem it seems then is the very first validation_step (sanity pass) generates extremely high values that crash the script before training can begin. After a single forward pass not in eval mode everything is fine and validation_step will never crash the script again. I don't know how these things are implemented under the hood so either this is a bug or perhaps the sanity check should include a single training_step or a single forward pass not in eval mode first to ensure spectral_norm is setup properly. Again I don't know how these things are implemented and am simply guessing at this stage. \r\n\r\n\r\n- PyTorch Lightning Version ( 1.4.5):\r\n- PyTorch Version (1.9)\r\n- Python version: (3.9.6)\r\n- OS (e.g., Linux): Ubuntu 18 \r\n- CUDA/cuDNN version: cudatoolkit 11.1.74\r\n- GPU models and configuration: Quadro p620\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9571/comments",
    "author": "SCMusson",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2021-09-17T07:30:16Z",
        "body": "Hi @SCMusson ,\r\n\r\nI am not sure how we can fix this. Because if we would replace all spectral norms with something else for sanity checking this wouldn't be a real check.\r\n\r\nRe 1.) I think this should be raised to the pytorch people. Mind opening an issue there?\r\nRe 2.) I can understand, seams unrelated to this exact issue.\r\nRe 3.) Also totally understandable. Does not work in practice.\r\nRe 4.) From looking at the implementation in PyTorch, I could not see a reason for that right away. Please also mention this as well in the issue @PyTorch.\r\nRe 5.) I agree. Does that also work if you sanity check in your pure PyTorch implementation?\r\n\r\nBest,\r\nJustus\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-10-18T18:50:46Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 9548,
    "title": "Validation sometimes fails when training on multiple gpus, real error hidden by bad re-raise.",
    "created_at": "2021-09-15T23:03:27Z",
    "closed_at": "2021-09-21T20:10:48Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9548",
    "body": "I am having a strange issue training my PL model with multiple GPUs, using `accelerator=\"dp\"` (have not tried others). My code works fine when training using one GPU, but I get an error somewhere in `parallel_apply` with multiple GPUs, then it appears the the error isn't handled correctly by re-raise so I am not sure what is going on.  The error is also appearing inconsistently, but always during validation (sometimes during sanity check, sometimes later).\r\n\r\nI am running on Environment:\r\n>  AzureML, VM Standard_NC24 (24 cores, 224 GB RAM, 1440 GB disk)\r\n>  Python 3.7\r\n>  Pytorch Lightning Version: 1.4.7\r\n>  Pytorch Version: 1.8.0\r\n\r\nFull Error:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 94, in <module>\r\n    trainer.fit(vae)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 552, in fit\r\n    self._run(model)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 917, in _run\r\n    self._dispatch()\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 985, in _dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 995, in run_stage\r\n    return self._run_train()\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1044, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\r\n    epoch_output = self.epoch_loop.run(train_dataloader)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 112, in run\r\n    self.on_advance_end()\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 177, in on_advance_end\r\n    self._run_validation()\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 257, in _run_validation\r\n    self.val_loop.run()\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 110, in advance\r\n    dataloader_iter, self.current_dataloader_idx, dl_max_batches, self.num_dataloaders\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 111, in advance\r\n    output = self.evaluation_step(batch, batch_idx, dataloader_idx)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 158, in evaluation_step\r\n    output = self.trainer.accelerator.validation_step(step_kwargs)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 211, in validation_step\r\n    return self.training_type_plugin.validation_step(*step_kwargs.values())\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/dp.py\", line 96, in validation_step\r\n    return self.model(*args, **kwargs)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 167, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 177, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 86, in parallel_apply\r\n    output.reraise()\r\n  File \"/azureml-envs/azureml_3a678fb3c063cd17fe3b93d28a5df6cf/lib/python3.7/site-packages/torch/_utils.py\", line 428, in reraise\r\n    raise self.exc_type(message=msg)\r\nTypeError: __init__() missing 1 required positional argument: 'exception_message'```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9548/comments",
    "author": "jmerkow",
    "comments": [
      {
        "user": "SeanNaren",
        "created_at": "2021-09-16T09:11:02Z",
        "body": "Is there a reason you're using DP? Any chance you could try `DDP` instead?"
      },
      {
        "user": "jmerkow",
        "created_at": "2021-09-18T17:08:42Z",
        "body": "So running with `\"ddp\"` seems to make it really really slow (1.5 hours per epoch, to 9 hours per epoch).  \r\n\r\nI am getting a new error, I am not certain it is related, but it could be error that was hidden by `\"dp\"` bad re-raise. (it has the same stack until `result = self.forward(*input, **kwargs)` where it went to parallel apply with DP.\r\n\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 165, in <module>\r\n    trainer.fit(vae)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 552, in fit\r\n    self._run(model)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 917, in _run\r\n    self._dispatch()\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 985, in _dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 995, in run_stage\r\n    return self._run_train()\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1044, in _run_train\r\n    self.fit_loop.run()\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\r\n    epoch_output = self.epoch_loop.run(train_dataloader)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 130, in advance\r\n    batch_output = self.batch_loop.run(batch, self.iteration_count, self._dataloader_idx)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 100, in run\r\n    super().run(batch, batch_idx, dataloader_idx)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 147, in advance\r\n    result = self._run_optimization(batch_idx, split_batch, opt_idx, optimizer)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 201, in _run_optimization\r\n    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 403, in _optimizer_step\r\n    using_lbfgs=is_lbfgs,\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1616, in optimizer_step\r\n    optimizer.step(closure=optimizer_closure)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 206, in step\r\n    self.__optimizer_step(closure=closure, profiler_name=profiler_name, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 128, in __optimizer_step\r\n    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 296, in optimizer_step\r\n    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 303, in run_optimizer_step\r\n    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 226, in optimizer_step\r\n    optimizer.step(closure=lambda_closure, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/torch/optim/adam.py\", line 66, in step\r\n    loss = closure()\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 235, in _training_step_and_backward_closure\r\n    result = self.training_step_and_backward(split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 536, in training_step_and_backward\r\n    result = self._training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 306, in _training_step\r\n    training_step_output = self.trainer.accelerator.training_step(step_kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 193, in training_step\r\n    return self.training_type_plugin.training_step(*step_kwargs.values())\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 384, in training_step\r\n    return self.model(*args, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 705, in forward\r\n    output = self.module(*inputs[0], **kwargs[0])\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py\", line 82, in forward\r\n    output = self.module.training_step(*inputs, **kwargs)\r\n  File \"/mnt/batch/tasks/shared/LS_root/jobs/mlops_shared/azureml/chexpert-images-vae-new_1631984220_3cd86d7b/wd/azureml/chexpert-images-vae-new_1631984220_3cd86d7b/model.py\", line 239, in training_step\r\n    loss, logs, image_batch_recon = self.step(batch)\r\n  File \"/mnt/batch/tasks/shared/LS_root/jobs/mlops_shared/azureml/chexpert-images-vae-new_1631984220_3cd86d7b/wd/azureml/chexpert-images-vae-new_1631984220_3cd86d7b/model.py\", line 217, in step\r\n    p, q, z = self.sample(mu, logvar)\r\n  File \"/mnt/batch/tasks/shared/LS_root/jobs/mlops_shared/azureml/chexpert-images-vae-new_1631984220_3cd86d7b/wd/azureml/chexpert-images-vae-new_1631984220_3cd86d7b/model.py\", line 154, in sample\r\n    q = torch.distributions.Normal(mu, std)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/torch/distributions/normal.py\", line 50, in __init__\r\n    super(Normal, self).__init__(batch_shape, validate_args=validate_args)\r\n  File \"/azureml-envs/azureml_ea1196074ca4d939b40cb444c370ed4b/lib/python3.7/site-packages/torch/distributions/distribution.py\", line 53, in __init__\r\n    raise ValueError(\"The parameter {} has invalid values\".format(param))\r\nValueError: The parameter loc has invalid values\r\n```"
      },
      {
        "user": "jmerkow",
        "created_at": "2021-09-21T20:10:48Z",
        "body": "I figured this issue out, I have code that is similar to the vae in lighting bolts, essentially if the encoder fails (i.e. while using auto_find_lr), it produces nans, which torch.distribution.Normal cannot handle. if you update q to `q = torch.distributions.Normal(mu.nan_to_num(0), std.nan_to_num(float(\"inf\")))` it will produce inf which should cause the early stopper to fire.  I can make a PR on LB so fix this issue."
      }
    ]
  },
  {
    "number": 9541,
    "title": "GPU memory issue when resuming from a checkpoint (fault tolerant enabled only)",
    "created_at": "2021-09-15T11:38:23Z",
    "closed_at": "2021-10-12T09:24:46Z",
    "labels": [
      "bug",
      "duplicate",
      "help wanted",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9541",
    "body": "## 🐛 Bug\r\n\r\nWhen resuming from a checkpoint created by fault-tolerant training, memory gets allocated on GPU 0 even though we specify to use GPU ids > 0. \r\n\r\nReported by Gurvinder Singh on Slack.\r\n\r\n### To Reproduce\r\n\r\n(yet to confirm reproducible) \r\n\r\n```python\r\n\r\nimport os\r\nimport torch\r\nfrom torch.utils.data import Dataset, DataLoader\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\r\nfrom deepspeed.ops.adam import FusedAdam\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n        return loss\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(320, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    checkpoint_callback = ModelCheckpoint(\r\n        dirpath='tests/',\r\n        filename='{epoch:02d}',\r\n    )\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        gpus=-1,\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        precision=16,\r\n        accelerator='ddp',\r\n        max_epochs=10,\r\n        #max_epochs=100,\r\n        plugins=[DeepSpeedPlugin(stage=2)],\r\n        weights_summary=None,\r\n        callbacks=[checkpoint_callback],\r\n        #resume_from_checkpoint='tests/epoch=9.ckpt',\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, test_dataloaders=test_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\nFrom Gurvinder:\r\n\r\n> I can confirm that this happen only when using fault tolerant training. you can easily reproduce this using our test script. create a normal checkpoint and then resume from it (just put sleep on training_step to be able to see the gpu memory usage, otherwise its too fast :stuck_out_tongue: ) then repeat the same process with fault tolerant enabled checkpoint. In FT, you will see extra memory being used on GPU0. you must run this test on server with GPU > 1. I just tested now with current master 637f59f1d2caaad57af3ddfa05b1089e6da6d3d0\r\n\r\n### Expected behavior\r\n\r\nmemory allocation only on the GPU that we actually want to use for training.\r\n\r\n### Environment\r\n\r\nTBD\r\n\r\n\r\n\r\n\r\n- PyTorch Lightning Version (e.g., 1.3.0):\r\n- PyTorch Version (e.g., 1.8)\r\n- Python version:\r\n- OS (e.g., Linux):\r\n- CUDA/cuDNN version:\r\n- GPU models and configuration:\r\n- How you installed PyTorch (`conda`, `pip`, source):\r\n- If compiling from source, the output of `torch.__config__.show()`:\r\n- Any other relevant information:\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9541/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-09-22T14:09:55Z",
        "body": "After closer investigation, it looks like this is specific to the usage of the deepspeed plugin here and a duplicate of the observations that #9521 made. The fault-tolerant settings here don't seem to matter and when switching to DDP I see no unusual memory allocations. "
      },
      {
        "user": "SeanNaren",
        "created_at": "2021-10-12T09:29:33Z",
        "body": "Thanks! Should now be fixed on lightning master, and with the latest Deepspeed version (`pip install deepspeed -U`)"
      }
    ]
  },
  {
    "number": 9488,
    "title": "Getting error with Pytorch lightning when passing model checkpoint",
    "created_at": "2021-09-13T14:43:28Z",
    "closed_at": "2021-09-13T17:52:04Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9488",
    "body": "I am training a multi-label classification problem using Hugging face models.\r\n\r\nI am using `Pytorch lightning` to train the model.\r\n \r\n\r\nHere is the code:\r\n\r\nAnd early stopping triggers when the loss hasn't improved for the last \r\n\r\n    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\r\n\r\nWe can start the training process:\r\n\r\n\r\n    checkpoint_callback = ModelCheckpoint(\r\n      dirpath=\"checkpoints\",\r\n      filename=\"best-checkpoint\",\r\n      save_top_k=1,\r\n      verbose=True,\r\n      monitor=\"val_loss\",\r\n      mode=\"min\"\r\n    )\r\n\r\n\r\n    trainer = pl.Trainer(\r\n      logger=logger,\r\n      callbacks=[early_stopping_callback],\r\n      max_epochs=N_EPOCHS,\r\n     checkpoint_callback=checkpoint_callback,\r\n      gpus=1,\r\n      progress_bar_refresh_rate=30\r\n    )\r\n    # checkpoint_callback=checkpoint_callback,\r\n\r\nAs soon as I run this, I get error:\r\n\r\n\r\n    ~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py in _configure_checkpoint_callbacks(self, checkpoint_callback)\r\n         75             if isinstance(checkpoint_callback, Callback):\r\n         76                 error_msg += \" Pass callback instances to the `callbacks` argument in the Trainer constructor instead.\"\r\n    ---> 77             raise MisconfigurationException(error_msg)\r\n         78         if self._trainer_has_checkpoint_callbacks() and checkpoint_callback is False:\r\n         79             raise MisconfigurationException(\r\n    \r\n    MisconfigurationException: Invalid type provided for checkpoint_callback: Expected bool but received <class 'pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint'>. Pass callback instances to the `callbacks` argument in the Trainer constructor instead.\r\n\r\n\r\n**How can I fix this issue?**",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9488/comments",
    "author": "pratikchhapolika",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-09-13T14:53:17Z",
        "body": "`checkpoint_callback` supports only a bool value. If set to True, it will create a model checkpoint instance internally, but if you want to assign your own custom instance then pass it within callbacks:\r\n```py\r\ntrainer = Trainer(callbacks=[checkpoint_callback, early_stopping_callback], ...)\r\n```"
      },
      {
        "user": "pratikchhapolika",
        "created_at": "2021-09-13T17:52:04Z",
        "body": "> `checkpoint_callback` supports only a bool value. If set to True, it will create a model checkpoint instance internally, but if you want to assign your own custom instance then pass it within callbacks:\r\n> \r\n> ```python\r\n> trainer = Trainer(callbacks=[checkpoint_callback, early_stopping_callback], ...)\r\n> ```\r\n\r\nThanks. Closing."
      }
    ]
  },
  {
    "number": 9346,
    "title": "Setting `max_time` in trainer triggers deprecation warning",
    "created_at": "2021-09-06T10:08:06Z",
    "closed_at": "2021-09-13T12:06:17Z",
    "labels": [
      "bug",
      "help wanted",
      "deprecation"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9346",
    "body": "## 🐛 Bug\r\n\r\nSetting `max_time` in `Trainer` triggers a deprecation warning:\r\n```\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/callback_hook.py:103: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\r\n  \"The signature of `Callback.on_train_epoch_end` has changed in v1.3.\"\r\n```\r\n\r\n### To Reproduce\r\nReproduced by adding the `max_time` flag in the BoringModel notebook.\r\n\r\n### Expected behavior\r\nNo warning should appear.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.0\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.0+cu102\r\n        - pytorch-lightning: 1.4.5\r\n        - tqdm:              4.62.2",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9346/comments",
    "author": "manuel-munoz-aguirre",
    "comments": [
      {
        "user": "kaushikb11",
        "created_at": "2021-09-13T12:06:17Z",
        "body": "Umm, Why didn't this issue close automatically after the PR getting merged lol? @awaelchli "
      },
      {
        "user": "awaelchli",
        "created_at": "2021-09-13T12:08:59Z",
        "body": "It should have, no idea why. Thanks for taking care of it."
      }
    ]
  },
  {
    "number": 9329,
    "title": "Allow pure fp16 training and test for native amp backend",
    "created_at": "2021-09-05T06:13:47Z",
    "closed_at": "2021-10-15T17:35:22Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9329",
    "body": "## 🚀 Feature\r\n\r\nAdd an option that converts the whole network in half-precision mode and performs forward and backward passes in the half-precision mode only.\r\n\r\n\r\n### Motivation\r\n\r\nCurrently, there are two options for `precision: 16` in pl and all of them are relies on amp_backend: \r\n\r\n1. Apex backend provides four levels for the amp from `O0` (pure fp32) to `O3` (pure fp16). This provides all required customization for training and testing processes.\r\n2. Native backend is not affected by amp_level and provides only one mode for the amp (according to my experiments it is an equivalent for Apex `O1`). And there is no option for the native backend to be pure fp16 for train and test. Therefore if I want to run a model in fp16 mode I have to use Apex.\r\n\r\n### Pitch\r\n\r\nIt would be nice if an additional option for the backend would be added: `amp_backend: None`.  In combination with `precision: 16`, this option disables amp and uses pure fp16 mode.\r\n\r\n### Alternatives\r\n\r\nAnother way, how it may be done is to connect `amp_backend: native` with `amp_level`. It may be less intuitive initially, but provide consistency between Apex and native backends like:\r\n\r\n1.  `O0` is pure fp32 for both apex and native;\r\n2.  `O1` is an equivalent for current and apex native backends;\r\n3.  `O2` is available only for apex (as I see there is only one mode for native amp);\r\n4.  `O3` is pure fp16 for both apex and native.\r\n\r\nI would like to hear any comments and suggestions about this idea.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9329/comments",
    "author": "Animatory",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-10-07T11:02:44Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 9290,
    "title": "'DataHooks' object has no attribute 'on_init_start'",
    "created_at": "2021-09-02T17:06:11Z",
    "closed_at": "2021-09-02T17:18:11Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9290",
    "body": "## 🐛 Bug\r\n\r\nI am getting an error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/clearml/binding/hydra_bind.py\", line 146, in _patched_task_function\r\n    return task_function(a_config, *a_args, **a_kwargs)\r\n  File \"run.py\", line 38, in main\r\n    config.target.checkpoint_path = train(config)\r\n  File \"/workdir/src/train.py\", line 61, in train\r\n    trainer: Trainer = instantiate(\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/instantiate/_instantiate2.py\", line 180, in instantiate\r\n    return instantiate_node(config, *args, recursive=_recursive_, convert=_convert_)\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/instantiate/_instantiate2.py\", line 249, in instantiate_node\r\n    return _call_target(target, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/instantiate/_instantiate2.py\", line 64, in _call_target\r\n    raise type(e)(\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/instantiate/_instantiate2.py\", line 62, in _call_target\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 40, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\", line 426, in __init__\r\n    self.on_init_start()\r\n  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/callback_hook.py\", line 62, in on_init_start\r\n    callback.on_init_start(self)\r\nAttributeError: Error instantiating 'pytorch_lightning.trainer.trainer.Trainer' : 'DataHooks' object has no attribute 'on_init_start'\r\n```\r\n\r\n### To Reproduce\r\n\r\nIt's quite hard to reproduce since my project is large.\r\n\r\n### Expected behavior\r\n\r\nI guess should be no error :)\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce RTX 3090\r\n                - NVIDIA GeForce RTX 3090\r\n        - available:         True\r\n        - version:           11.1\r\n* Packages:\r\n        - numpy:             1.21.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.0+cu111\r\n        - pytorch-lightning: 1.4.1\r\n        - tqdm:              4.59.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.5\r\n        - version:           #29~20.04.1-Ubuntu SMP Wed Aug 11 15:58:17 UTC 2021\r\n\r\n\r\n### Additional context\r\n\r\nPreviously I had no such error, but I've changed something and now I am unable to run the pipeline.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9290/comments",
    "author": "smivv",
    "comments": [
      {
        "user": "smivv",
        "created_at": "2021-09-02T17:18:11Z",
        "body": "Actually found the problem :)"
      },
      {
        "user": "ananthsub",
        "created_at": "2021-09-02T17:20:29Z",
        "body": "> Actually found the problem :)\r\n\r\nCould you comment here what the problem was and how you resolved it to benefit others that come to this issue?"
      },
      {
        "user": "smivv",
        "created_at": "2021-09-02T17:23:22Z",
        "body": "Well, actually I made quite stupid mistake and set DataHook to callbacks 😄 "
      }
    ]
  },
  {
    "number": 9264,
    "title": "Add option to not flatten config in `WandbLogger.log_hyperparams`",
    "created_at": "2021-09-02T00:18:36Z",
    "closed_at": "2021-11-12T03:12:11Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9264",
    "body": "## 🚀 Feature\r\n\r\nMake the config flattening that happens in `WandbLogger.log_hyperparams` optional.\r\n\r\n### Motivation\r\n\r\nWandb support nested config by default. Furthermore, by flattening the configuration it makes it more difficult to load an experiment from a wandb run – instead of taking the config directly from the run, you have to un-flatten it first (or really, not use log_hyperparams and instead do it manually).\r\n\r\n### Pitch\r\n\r\nAdd `flatten: bool = True` as a kwarg in `log_hyperparams`.\r\n\r\nPerhaps this should be a flag in the base class too? Unsure if other loggers require hyperparam flattening so outside my ken.\r\n\r\nHappy to make a PR if we approve.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9264/comments",
    "author": "xvr-hlt",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-09-07T18:35:04Z",
        "body": "or maybe you can override `WandbLogger.log_hyperparams` and remove that LOC from your custom logger. just a workaround though."
      },
      {
        "user": "borisdayma",
        "created_at": "2021-09-24T16:12:51Z",
        "body": "Actually should we flatten at all?\r\nWhat was the reason for it?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-10-25T06:23:57Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 9177,
    "title": "`automatic_optimization = False` breaks pytorch profiler",
    "created_at": "2021-08-28T15:34:13Z",
    "closed_at": "2021-09-06T10:45:34Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 1",
      "profiler"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9177",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nRun basic_examples/profiler_example.py with `--trainer.profiler pytorch` work as expected, but adding `self.automatic_optimization = False` in `ModelToProfile` breaks it with an AssertionError (even if `manual_backward` is used properly).\r\n\r\n### Expected behavior\r\n\r\nProfiling in manual optimization mode shall work as expected.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce RTX 2060\r\n        - available:         True        \r\n        - version:           11.1        \r\n* Packages:\r\n        - numpy:             1.20.1      \r\n        - pyTorch_debug:     False       \r\n        - pyTorch_version:   1.9.0+cu111 \r\n        - pytorch-lightning: 1.4.4       \r\n        - tqdm:              4.59.0      \r\n* System:\r\n        - OS:                Windows     \r\n        - architecture:\r\n                - 64bit\r\n                - WindowsPE\r\n        - processor:         AMD64 Family 23 Model 96 Stepping 1, AuthenticAMD\r\n        - python:            3.8.8\r\n        - version:           10.0.19041\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9177/comments",
    "author": "jjyyxx",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-09-03T13:48:59Z",
        "body": "Hey @jjyyxx,\r\n\r\nThanks for reporting this bug. I can confirm I was able to reproduce it.\r\n\r\nI will investigate this deeper.\r\n\r\nBest,\r\nT.C"
      }
    ]
  },
  {
    "number": 9176,
    "title": "on_save_checkoint never called",
    "created_at": "2021-08-28T15:11:45Z",
    "closed_at": "2021-08-29T00:01:00Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9176",
    "body": "## 🐛 Bug\r\n\r\nI wrote a `Callback` class and found `on_save_checkpoint` had never been called\r\n\r\n### To Reproduce\r\n\r\nMy callback class:\r\n```\r\nfrom pytorch_lightning.callbacks import Callback\r\nfrom os.path import join\r\nimport torch\r\nimport os\r\nimport pytorch_lightning as pl\r\nfrom typing import Dict, Any, Optional\r\n\r\n\r\nclass JitSave(Callback):\r\n\r\n    def __init__(self):\r\n        self.outputs = None\r\n        self.n_dataloaders = None\r\n\r\n    def on_save_checkpoint(\r\n        self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule', checkpoint: Dict[\r\n                str, Any]\r\n    ) -> dict:\r\n        # Torch.jit.save\r\n        jit_model_dir = join(\r\n            join(os.getcwd(), \"checkpoints\"), f\"jit_{self.logger[0].version}\" + \"{}.pt\"\r\n        )\r\n        torch.jit.save(self.model.cpu().to_torchscript(), jit_model_dir.format(\"cpu\"))\r\n        torch.jit.save(self.model.to_torchscript(), jit_model_dir.format(\"gpu\"))\r\n        print(f\"torch.jit.save path :\\n{jit_model_dir}\")\r\n        # return {\"jitsave_path\": jit_model_dir}\r\n        return checkpoint\r\n\r\n    def setup(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule', stage: Optional[str] = None) -> None:\r\n        self.n_dataloaders = len(pl_module.val_dataloader())\r\n\r\n    def _reset(self):\r\n        self.outputs = [[] for _ in range(self.n_dataloaders)]\r\n\r\n    def on_epoch_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None:\r\n        self._reset()\r\n\r\n    def on_validation_epoch_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None:\r\n        for dataloader_idx, output in enumerate(self.outputs):\r\n            pass\r\n```\r\n`on_validation_epoch_end` works but on_save_checkpoint not.\r\n\r\nThis is my `ModelCheckpoint`:\r\n\r\n```\r\nmodel_checkpoint:\r\n  _target_: pytorch_lightning.callbacks.ModelCheckpoint\r\n  monitor: \"val/f1\" # name of the logged metric which determines when model isimproving\r\n  mode: \"max\" # can be \"max\" or \"min\"\r\n  save_top_k: 1 # save k best models (determined by above metric)\r\n  save_last: False # additionaly always save model from last epoch\r\n  verbose: False\r\n  dirpath: \"checkpoints/\"\r\n  filename: \"epoch_{epoch:03d}\"\r\n  auto_insert_metric_name: False\r\n  save_weights_only: True\r\n```\r\n\r\nCallbacks are passed to the trainer:\r\n\r\n```\r\ncallbacks: List[Callback] = []\r\n    if \"callbacks\" in config:\r\n        for _, cb_conf in config.callbacks.items():\r\n            if \"_target_\" in cb_conf:\r\n                log.info(f\"Instantiating callback <{cb_conf._target_}>\")\r\n                callbacks.append(hydra.utils.instantiate(cb_conf))\r\n```\r\n\r\n```\r\ntrainer: Trainer = hydra.utils.instantiate(\r\n        config.trainer, callbacks=callbacks, logger=logger, _convert_=\"partial\"\r\n    )\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\n`on_save_checkpoint` should be called.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.5\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.0+cu102\r\n        - pytorch-lightning: 1.4.2\r\n        - tqdm:              4.62.1\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.10\r\n        - version:           #60~20.04.1-Ubuntu SMP Thu May 6 09:52:46 UTC 2021",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9176/comments",
    "author": "zhiyuanpeng",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2021-08-28T17:14:14Z",
        "body": "you're specifying `save_weights_only=True` so no callback states are added to the checkpoint. this means the callback's `on_save_checkpoint` is never called"
      },
      {
        "user": "zhiyuanpeng",
        "created_at": "2021-08-29T00:00:56Z",
        "body": "> you're specifying `save_weights_only=True` so no callback states are added to the checkpoint. this means the callback's `on_save_checkpoint` is never called\r\n\r\nThanks for your reply. My problem is solved.\r\n"
      }
    ]
  },
  {
    "number": 9155,
    "title": "AttributeError: Can't pickle local object when attempting multi-GPU training",
    "created_at": "2021-08-27T00:24:54Z",
    "closed_at": "2021-08-27T18:42:58Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9155",
    "body": "## 🐛 Bug\r\n\r\nRunning the provided script with multiple GPUs causes the following error:\r\n```\r\n$ python pickle_test.py\r\n.../lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:746: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `accelerator=\"ddp_spawn\"` for you.\r\n  rank_zero_warn(\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\n.../lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:99: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping val loop\r\n  rank_zero_warn(f\"you passed in a {loader_name} but have no {step_name}. Skipping {stage} loop\")\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\nTraceback (most recent call last):\r\n  File \"pickle_test.py\", line 81, in <module>\r\n    test_x(tmpdir)\r\n  File \"pickle_test.py\", line 77, in test_x\r\n    trainer.fit(model=model, datamodule=dm)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 553, in fit\r\n    self._run(model)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in _run\r\n    self._dispatch()\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _dispatch\r\n    self.accelerator.start_training(self)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 158, in start_training\r\n    mp.spawn(self.new_process, **self.mp_spawn_kwargs)\r\n  File \".../lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \".../lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 179, in start_processes\r\n    process.start()\r\n  File \".../lib/python3.8/multiprocessing/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \".../lib/python3.8/multiprocessing/context.py\", line 283, in _Popen\r\n    return Popen(process_obj)\r\n  File \".../lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \".../lib/python3.8/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \".../lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \".../lib/python3.8/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'LightningDataModule.from_datasets.<locals>.train_dataloader'\r\n```\r\n\r\n### To Reproduce\r\n\r\nThe following script causes the bug:\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning import LightningDataModule\r\nfrom torch.nn import functional as F\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import MNIST\r\n\r\ntmpdir = '../../data'\r\n\r\n\r\ndef mnist(root: str, normalize: bool = False):\r\n    tlist = [transforms.ToTensor()]\r\n\r\n    if normalize:\r\n        tlist.append(transforms.Normalize((0.5,), (0.5,)))\r\n\r\n    transform = transforms.Compose(tlist)\r\n\r\n    trainset = MNIST(root=root, train=True, download=True, transform=transform)\r\n    testset = MNIST(root=root, train=False, download=True, transform=transform)\r\n    return trainset, testset\r\n\r\n\r\ndef mnist_datamodule(data_path: str, batch_size: int, num_workers: int):\r\n    train, val = mnist(data_path, normalize=True)\r\n    return LightningDataModule.from_datasets(train, val, None, batch_size=batch_size, num_workers=num_workers)\r\n\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningModule\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fc1 = torch.nn.Linear(28 * 28, 32)\r\n        self.fc2 = torch.nn.Linear(32, 10)\r\n\r\n    def forward(self, x):\r\n        x = torch.flatten(x, 1)\r\n        x = F.sigmoid(self.fc1(x))\r\n        x = F.softmax(self.fc2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ndef test_x(tmpdir):\r\n    # init model\r\n    model = BoringModel()\r\n\r\n    # Initialize a trainer\r\n    trainer = pl.Trainer(\r\n        max_epochs=1,\r\n        progress_bar_refresh_rate=20,\r\n        gpus=2\r\n    )\r\n\r\n    dm = mnist_datamodule(tmpdir, 16, 1)\r\n\r\n    # Train the model ⚡\r\n    trainer.fit(model=model, datamodule=dm)\r\n\r\n\r\nif __name__ == '__main__':\r\n    test_x(tmpdir)\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\nRunning on a SLURM cluster\r\n- PyTorch Lightning Version (e.g., 1.3.0): 1.4.1\r\n- PyTorch Version (e.g., 1.8): 1.9.0\r\n- Python version: 3.8.0\r\n- OS (e.g., Linux): Linux HPCC\r\n- CUDA/cuDNN version: 10.1\r\n- GPU models and configuration: 2x 2080\r\n- How you installed PyTorch (`conda`, `pip`, source): conda\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9155/comments",
    "author": "import-antigravity",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-08-27T11:26:21Z",
        "body": "@import-antigravity this is because `LightningModule.from_datasets` patches out the data loader methods. \r\nYou will have to select `accelerator=\"ddp\"` as a workaround."
      },
      {
        "user": "import-antigravity",
        "created_at": "2021-08-27T15:28:24Z",
        "body": "> @import-antigravity this is because `LightningModule.from_datasets` patches out the data loader methods.\r\n> You will have to select `accelerator=\"ddp\"` as a workaround.\r\n\r\nThat solved it, thanks"
      },
      {
        "user": "tchaton",
        "created_at": "2021-08-27T18:42:55Z",
        "body": "Dear @import-antigravity,\r\n\r\nClosing this issue as it is expected behaviour with the current design and there is an alternative.\r\n\r\nBest,\r\nT.C"
      }
    ]
  },
  {
    "number": 9115,
    "title": "DeepSpeed: RuntimeError: Input type (FloatTensor) and weight type (HalfTensor) should be the same ",
    "created_at": "2021-08-25T18:32:41Z",
    "closed_at": "2023-04-29T21:25:38Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9115",
    "body": "# DeepSpeed: RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same \r\n\r\nTraining Unet with precision = 16 and deepspeed_stage_3 leads to type incompatible error.  error happens with both amp_level=O1 set and at default values.\r\n\r\nTrainer config: \r\n```python\r\ntrainer = pl.Trainer(\r\n        max_epochs=epochs,\r\n        gpus=-1,\r\n        logger=logger,\r\n        callbacks=[image_logger, lr_monitor, checkpoint],\r\n        accelerator=\"ddp\",\r\n        gradient_clip_val=0.1,\r\n        track_grad_norm=2,\r\n        precision=16,\r\n        # amp_level=\"O1\",\r\n        plugins=\"deepspeed_stage_3\",\r\n    )\r\n```\r\n\r\n**Expected behavior**\r\nRuns the same way as it would without the deepspeed plugin.\r\n\r\n**System info (please complete the following information):**\r\n\r\nenvironment: \r\n```\r\ndeepspeed                0.5.0\r\npytorch-lightning        1.4.2\r\npytorch-metric-learning  0.9.98\r\npytorch-msssim           0.2.1\r\ntorch                    1.8.1\r\ntorch-optimizer          0.0.1a9\r\ntorchaudio               0.8.0a0+e4e171a\r\ntorchmetrics             0.5.0\r\ntorchsampler             0.1.1\r\ntorchvision              0.9.1\r\n```\r\n\r\n - OS: Ubuntu 18.04\r\n - GPU 4x RTX 2080Ti, Driver 440.100, Cuda 10.2\r\n\r\n**Additional context**\r\nfull trace for the error:\r\n```\r\n\r\n  File \"models/unet.py\", line 352, in forward\r\n    a, _, _, _ = self._forward(x)\r\n  File \"models/unet.py\", line 318, in _forward\r\n    x = self.conv1(x)  # →                       [h/2, w/2]\r\n  File \"/miniconda3/envs/py3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/miniconda3/envs/py3/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 119, in forward\r\n    input = module(input)\r\n  File \"/miniconda3/envs/py3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/miniconda3/envs/py3/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 399, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n  File \"/miniconda3/envs/py3/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 396, in _conv_forward\r\n    self.padding, self.dilation, self.groups)\r\nRuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9115/comments",
    "author": "IsCoelacanth",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-08-25T18:47:02Z",
        "body": "@SeanNaren "
      },
      {
        "user": "awaelchli",
        "created_at": "2023-04-29T21:25:38Z",
        "body": "Closing the issue since it is quite old. I haven't seen this issue pop up in recent versions. If you do, please let me know. "
      }
    ]
  },
  {
    "number": 9095,
    "title": "PTL 1.4.4: CUDA OOM due to logging GPU metrics with torchmetrics",
    "created_at": "2021-08-24T21:28:43Z",
    "closed_at": "2021-09-04T05:01:17Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0",
      "logging"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9095",
    "body": "As stated in the title, reverting to 1.4.3 seems to solve the problem.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9095/comments",
    "author": "anhnht3",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-08-25T05:59:43Z",
        "body": "Dear @anhnht3,\r\n\r\nWould you mind providing a reproducible script ?\r\n\r\nBest,\r\nT.C"
      }
    ]
  },
  {
    "number": 9012,
    "title": "Torch auto_scale_batch_size doesn't work with ReduceLROnPlateau optimizer when monitor set to \"val_loss\", only \"train_loss\" metric is available while finding optimal batch size. ",
    "created_at": "2021-08-20T09:40:13Z",
    "closed_at": "2021-10-09T21:17:06Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9012",
    "body": "## 🐛 Bug\r\nWhen using auto batch finder with ReduceLROnPlateau LR Scheduler in model, following error is thrown\r\n\r\nMisconfigurationException: ReduceLROnPlateau conditioned on metric valid_loss which is not available. Available metrics are: ['train_loss']. Condition can be set using `monitor` key in lr scheduler dict\r\n\r\n\r\n### To Reproduce\r\n\r\n```\r\n\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer, LightningDataModule\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass DummyDataModule(LightningDataModule):\r\n    def __init__(self, batch_size=32):\r\n        super().__init__()\r\n        self.batch_size = batch_size\r\n\r\n    def prepare_data(self):\r\n        pass\r\n\r\n    def setup(self, stage):\r\n        pass\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64), batch_size=self.batch_size)\r\n    def val_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64), batch_size=self.batch_size, shuffle=False)\r\n    def test_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64), batch_size=self.batch_size, shuffle=False)\r\n\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self, lr=1e-3, bs=32):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n        self.learning_rate = lr\r\n        self.batch_size = bs\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(params=self.layer.parameters(), lr=self.learning_rate)\r\n        return {\r\n            \"optimizer\": optimizer,\r\n            \"lr_scheduler\": {\r\n                \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n                    optimizer=optimizer,\r\n                    mode=\"min\",\r\n                ),\r\n                \"monitor\": \"valid_loss\",\r\n            }\r\n        }\r\n\r\n\r\ndef run():\r\n\r\n    dm = DummyDataModule()\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n        auto_scale_batch_size=\"binsearch\"\r\n    )\r\n\r\n    trainer.tuner.scale_batch_size(model, datamodule=dm)\r\n    trainer.fit(model, datamodule=dm)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n\r\n\r\n```\r\n\r\n\r\nTraceback:\r\n```\r\n\r\nGPU available: False, used: False\r\nTPU available: False, using: 0 TPU cores\r\n---------------------------------------------------------------------------\r\n\r\nMisconfigurationException                 Traceback (most recent call last)\r\n\r\n<ipython-input-28-b6404140b776> in <module>\r\n     95 \r\n     96 if __name__ == \"__main__\":\r\n---> 97     run()\r\n\r\n<ipython-input-28-b6404140b776> in run()\r\n     90     )\r\n     91 \r\n---> 92     trainer.tuner.scale_batch_size(model, datamodule=dm)\r\n     93     trainer.fit(model, datamodule=dm)\r\n     94 \r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py in scale_batch_size(self, model, train_dataloader, val_dataloaders, datamodule, mode, steps_per_trial, init_val, max_trials, batch_arg_name)\r\n    117         \"\"\"\r\n    118         self.trainer.auto_scale_batch_size = True\r\n--> 119         result = self.trainer.tune(\r\n    120             model,\r\n    121             train_dataloader=train_dataloader,\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py in tune(self, model, train_dataloader, val_dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs)\r\n    686         )\r\n    687 \r\n--> 688         result = self.tuner._tune(model, scale_batch_size_kwargs=scale_batch_size_kwargs, lr_find_kwargs=lr_find_kwargs)\r\n    689 \r\n    690         assert self.state.stopped\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py in _tune(self, model, scale_batch_size_kwargs, lr_find_kwargs)\r\n     47             if isinstance(self.trainer.auto_scale_batch_size, str):\r\n     48                 scale_batch_size_kwargs.setdefault(\"mode\", self.trainer.auto_scale_batch_size)\r\n---> 49             result['scale_batch_size'] = scale_batch_size(self.trainer, model, **scale_batch_size_kwargs)\r\n     50 \r\n     51         # Run learning rate finder:\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/tuner/batch_size_scaling.py in scale_batch_size(trainer, model, mode, steps_per_trial, init_val, max_trials, batch_arg_name)\r\n     73     new_size, _ = _adjust_batch_size(trainer, batch_arg_name, value=init_val)  # initially set to init_val\r\n     74     if mode == 'power':\r\n---> 75         new_size = _run_power_scaling(trainer, model, new_size, batch_arg_name, max_trials)\r\n     76     elif mode == 'binsearch':\r\n     77         new_size = _run_binsearch_scaling(trainer, model, new_size, batch_arg_name, max_trials)\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/tuner/batch_size_scaling.py in _run_power_scaling(trainer, model, new_size, batch_arg_name, max_trials)\r\n    148         try:\r\n    149             # Try fit\r\n--> 150             trainer.tuner._run(model)\r\n    151             # Double in size\r\n    152             new_size, changed = _adjust_batch_size(trainer, batch_arg_name, factor=2.0, desc='succeeded')\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py in _run(self, *args, **kwargs)\r\n     62         self.trainer.state.status = TrainerStatus.RUNNING  # last `_run` call might have set it to `FINISHED`\r\n     63         self.trainer.training = True\r\n---> 64         self.trainer._run(*args, **kwargs)\r\n     65         self.trainer.tuning = True\r\n     66 \r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\r\n    756 \r\n    757         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n--> 758         self.dispatch()\r\n    759 \r\n    760         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n    797             self.accelerator.start_predicting(self)\r\n    798         else:\r\n--> 799             self.accelerator.start_training(self)\r\n    800 \r\n    801     def run_stage(self):\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n     94 \r\n     95     def start_training(self, trainer: 'pl.Trainer') -> None:\r\n---> 96         self.training_type_plugin.start_training(trainer)\r\n     97 \r\n     98     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\r\n    142     def start_training(self, trainer: 'pl.Trainer') -> None:\r\n    143         # double dispatch to initiate the training loop\r\n--> 144         self._results = trainer.run_stage()\r\n    145 \r\n    146     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\r\n    807         if self.predicting:\r\n    808             return self.run_predict()\r\n--> 809         return self.run_train()\r\n    810 \r\n    811     def _pre_training_routine(self):\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n    869                 with self.profiler.profile(\"run_training_epoch\"):\r\n    870                     # run train epoch\r\n--> 871                     self.train_loop.run_training_epoch()\r\n    872 \r\n    873                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n    575         # update epoch level lr_schedulers if no val loop outside train loop is triggered\r\n    576         if not should_check_val or should_train_only:\r\n--> 577             self.trainer.optimizer_connector.update_learning_rates(interval='epoch')\r\n    578 \r\n    579         if should_train_only:\r\n\r\n~/opt/anaconda3/envs/q38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/optimizer_connector.py in update_learning_rates(self, interval, monitor_metrics, opt_indices)\r\n     63                         if lr_scheduler.get('strict', True):\r\n     64                             avail_metrics = list(self.trainer.logger_connector.callback_metrics.keys())\r\n---> 65                             raise MisconfigurationException(\r\n     66                                 f'ReduceLROnPlateau conditioned on metric {monitor_key}'\r\n     67                                 f' which is not available. Available metrics are: {avail_metrics}.'\r\n\r\nMisconfigurationException: ReduceLROnPlateau conditioned on metric valid_loss which is not available. Available metrics are: ['train_loss']. Condition can be set using `monitor` key in lr scheduler dict\r\n\r\n```\r\n\r\n### Environment\r\n\r\n- PyTorch Lightning Version 1.3.8:\r\n- PyTorch Version 1.8.0\r\n- Python version: 3.8\r\n- OS (e.g., Linux): Mac OSX\r\n- CUDA/cuDNN version: N/A\r\n- GPU models and configuration: N/A\r\n- How you installed PyTorch (`conda`, `pip`, source): conda\r\n- If compiling from source, the output of `torch.__config__.show()`: N/A\r\n- Any other relevant information:",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9012/comments",
    "author": "ra2630",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-10-01T06:29:08Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 9008,
    "title": "UNet with Variable Input Shapes throws RuntimeError: CUDA error: an illegal memory access was encountered",
    "created_at": "2021-08-20T07:47:40Z",
    "closed_at": "2021-10-01T22:45:28Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9008",
    "body": "## 🐛 Bug\r\n\r\nWhen training a UNet for segmentation, which is fully convolutional and takes variable input shapes (each dimension is an integer multiple of 16), the trainer throws the error after a few samples.\r\n\r\nI do have multiple gpus installed, but this error was thrown using just one (and it didn't matter which one).\r\n\r\nI altered my code so it could be reproduced with a mockup dataset, however I wasn't able to build a \"simpler\" UNet  which is why this code is really long. Apologies in advance. The UNet is tested however, and using a simple pytorch style for-loop instead of Trainer.fit(module) doesn't throw the same error. \r\n\r\nAt the very end of the script there is a function called \"run_test_no_trainer()\" which is an example pytorch loop that doesn't throw the cuda memory access error.  \r\n\r\n### To Reproduce\r\n\r\n```\r\n\r\n\r\nimport pytorch_lightning as pl\r\nfrom torch import nn\r\nimport torch\r\nfrom torch.utils.data import Dataset, DataLoader\r\nfrom torch.nn import CrossEntropyLoss\r\nimport numpy as np\r\n\r\npl.seed_everything(0)\r\n\r\n\r\nclass VariableSizeMockupData(Dataset):\r\n    def __init__(self):\r\n        super(VariableSizeMockupData,self).__init__()\r\n\r\n    def __getitem__(self, item):\r\n        j, k, l = np.random.randint(1, 4, size=3)\r\n        sample = torch.rand(20, j * 16, k * 16, l * 16)\r\n        labels = torch.zeros(j * 16, k * 16, l * 16, dtype=torch.long)\r\n        return sample, labels\r\n\r\n    def __len__(self):\r\n        return 1000\r\n\r\nclass UNetConvBlockDown(pl.LightningModule):\r\n    def __init__(self, in_channels, mid_channels, out_channels):\r\n        super(UNetConvBlockDown, self).__init__()\r\n\r\n        self.in_channels = in_channels\r\n        self.out_channels = out_channels\r\n\r\n        self.conv1 = nn.Conv3d(kernel_size=3,\r\n                               in_channels=in_channels,\r\n                               out_channels=mid_channels,\r\n                               padding=1)\r\n\r\n        self.conv2 = nn.Conv3d(kernel_size=3,\r\n                               in_channels=mid_channels,\r\n                               out_channels=out_channels,\r\n                               padding=1)\r\n        self.maxpool = nn.MaxPool3d(2)\r\n        self.BN_in = nn.BatchNorm3d(mid_channels)\r\n        self.BN_out = nn.BatchNorm3d(out_channels)\r\n        self.relu = nn.ReLU()\r\n\r\n    def forward(self, x):\r\n        x = self.BN_in(self.conv1(x))\r\n        x = self.relu(x)\r\n        x = self.BN_out(self.conv2(x))\r\n        x = self.relu(x)\r\n        y = self.maxpool(x)\r\n        return y, x\r\n\r\n\r\nclass UNetConvBlockUp(pl.LightningModule):\r\n    def __init__(self, in_channels, mid_channels, out_channels):\r\n        super(UNetConvBlockUp, self).__init__()\r\n\r\n        self.in_channels = in_channels\r\n        self.out_channels = out_channels\r\n\r\n        self.conv1 = nn.Conv3d(kernel_size=3,\r\n                               in_channels=in_channels,\r\n                               out_channels=mid_channels,\r\n                               padding=1)\r\n\r\n        self.conv2 = nn.Conv3d(kernel_size=3,\r\n                               in_channels=mid_channels,\r\n                               out_channels=out_channels,\r\n                               padding=1)\r\n\r\n        self.upconv = nn.ConvTranspose3d(kernel_size=3,\r\n                                         in_channels=out_channels,\r\n                                         out_channels=out_channels,\r\n                                         stride=2,\r\n                                         padding=1,\r\n                                         output_padding=1)\r\n\r\n        self.BN_in = nn.BatchNorm3d(mid_channels)\r\n        self.BN_mid = nn.BatchNorm3d(out_channels)\r\n        self.BN_out = nn.BatchNorm3d(out_channels)\r\n        self.relu = nn.ReLU()\r\n\r\n    def forward(self, x):\r\n        x = self.BN_in(self.conv1(x))\r\n\r\n        x = self.relu(x)\r\n        x = self.BN_mid(self.conv2(x))\r\n        x = self.relu(x)\r\n        x = self.upconv(x)\r\n        x = self.BN_out(x)\r\n        x = self.relu(x)\r\n        return x\r\n\r\n\r\nclass UNet3DWithSkipConnections(pl.LightningModule):\r\n    \"\"\" Base for a 3D UNet.\r\n        First Layer consists of \"in_channels\" channels, then goes up to 64 channels.\r\n        For every subsequent layer, goes to 64 * (2**n)\r\n    \"\"\"\r\n\r\n    def __init__(self, depth=4, in_channels=8, num_classes=4):\r\n        super(UNet3DWithSkipConnections, self).__init__()\r\n        self.dataset = VariableSizeMockupData()\r\n        self.depth = depth\r\n        self.in_channels = in_channels\r\n        self.num_outputs = num_classes + 1  # +1 for no class.\r\n        self.criterion = CrossEntropyLoss(reduction=\"sum\")\r\n        self.block_in = UNetConvBlockDown(in_channels=in_channels,\r\n                                          mid_channels=32,\r\n                                          out_channels=64)\r\n\r\n        self.blocks_down = nn.ModuleList([UNetConvBlockDown(in_channels=64 * (2 ** i),\r\n                                                            mid_channels=64 * (2 ** i),\r\n                                                            out_channels=64 * (2 ** (i + 1)))\r\n                                          for i in range(depth - 2)])\r\n\r\n        self.blocks_up = nn.ModuleList([])\r\n        for i in range(depth - 1):\r\n            if i != 0:\r\n                in_channels = 64 * 2 ** (depth - i) + 64 * 2 ** (depth - (i + 1))\r\n                mid_channels = 64 * 2 ** (depth - (i + 1))\r\n                out_channels = 64 * 2 ** (depth - (i + 1))\r\n            else:\r\n                in_channels = 64 * (2 ** (depth - 2))\r\n                mid_channels = 64 * (2 ** (depth - 2))\r\n                out_channels = 64 * (2 ** (depth - 1))\r\n\r\n            self.blocks_up.append(UNetConvBlockUp(in_channels=in_channels,\r\n                                                  mid_channels=mid_channels,\r\n                                                  out_channels=out_channels))\r\n\r\n        self.out_block = nn.Sequential(\r\n            nn.Conv3d(kernel_size=3, in_channels=64 + 128, out_channels=64, padding=1),\r\n            nn.BatchNorm3d(num_features=64),\r\n            nn.ReLU(),\r\n\r\n            nn.Conv3d(kernel_size=3, in_channels=64, out_channels=64, padding=1),\r\n            nn.BatchNorm3d(num_features=64),\r\n            nn.ReLU(),\r\n\r\n            nn.Conv3d(kernel_size=3, in_channels=64, out_channels=self.num_outputs, padding=1)\r\n        )\r\n\r\n    def forward(self, x):\r\n        x_out, r = self.block_in(x)\r\n        res = [r]\r\n        for node in self.blocks_down:\r\n            x_out, r = node(x_out)\r\n            res.append(r)\r\n\r\n        i = len(res) - 1\r\n\r\n        x_out = self.blocks_up[0](x_out)\r\n\r\n        for node in self.blocks_up[1:]:\r\n            x_in = torch.hstack((x_out, res[i]))\r\n            x_out = node(x_in)\r\n            i -= 1\r\n\r\n        return self.out_block(torch.hstack((x_out, res[0])))\r\n\r\n    def step(self, batch):\r\n        x, y = batch\r\n        mask_pred = self(x)\r\n        loss = self.criterion(mask_pred, y)\r\n        return loss\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self.step(batch)\r\n        self.log(\"train_loss\", loss, on_step=True)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self.step(batch)\r\n        self.log(\"val_loss\", loss, on_step=True)\r\n        return loss\r\n\r\n    def test_step(self, batch):\r\n        loss = self.step(batch)\r\n        self.log(\"test_loss\", loss, on_step=True)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters())\r\n\r\n    def unit_test(self):\r\n        \"\"\"For checking if the model works on basic inputs\"\"\"\r\n        batch_size, channels, x_, y_, z_ = 4, self.in_channels, 16, 16, 16\r\n        test_block = torch.rand(batch_size, channels, x_, y_, z_).cuda(1)\r\n        block = self.forward(test_block)\r\n        block = torch.squeeze(block)[None, ...]\r\n        true_mask = torch.zeros(1, 16, 16, 16, dtype=torch.long).cuda(1)\r\n        loss = self.criterion(block, true_mask)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.dataset, batch_size=1, num_workers=0)\r\n\r\n\r\ndef run_test_trainer():\r\n    trainer = pl.Trainer(gpus=[1])\r\n    module = UNet3DWithSkipConnections(in_channels=20, num_classes=4)\r\n    trainer.fit(module)\r\n\r\ndef run_test_no_trainer():\r\n    module = UNet3DWithSkipConnections(in_channels=20, num_classes=4)\r\n    for sample in module.train_dataloader():\r\n        out = module(sample[0])\r\n\r\nif __name__ == \"__main__\":\r\n    run_test_trainer()\r\n```\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nIt shouldn't throw that error.\r\n\r\n### Environment\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce RTX 2080 Ti\r\n                - NVIDIA GeForce RTX 2080 Ti\r\n                - NVIDIA GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           11.1\r\n* Packages:\r\n        - numpy:             1.20.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.8.1\r\n        - pytorch-lightning: 1.3.5\r\n        - tqdm:              4.61.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.10\r\n        - version:           #71~20.04.1-Ubuntu SMP Thu Jul 15 17:46:08 UTC 2021\r\n\r\n### Additional context\r\n\r\n```\r\nThe error stack trace reads as follows: \r\n\r\nTraceback (most recent call last):\r\n  File \"experiments/ProjectName/VariableInputSizeModel.py\", line 205, in <module>\r\n    run_test()\r\n  File \"experiments/ProjectName/VariableInputSizeModel.py\", line 202, in run_test\r\n    trainer.fit(module)\r\n  File \"/home/OlfwayAdbayIgbay/anaconda3/envs/ptl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 458, in fit\r\n    self._run(model)\r\n  File \"/home/OlfwayAdbayIgbay/anaconda3/envs/ptl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 756, in _run\r\n    self.dispatch()\r\n  File \"/home/OlfwayAdbayIgbay/anaconda3/envs/ptl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 797, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/home/OlfwayAdbayIgbay/anaconda3/envs/ptl/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/homeOlfwayAdbayIgbay/anaconda3/envs/ptl/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 144, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"/home/OlfwayAdbayIgbay/anaconda3/envs/ptl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 807, in run_stage\r\n    return self.run_train()\r\n  File \"/home/OlfwayAdbayIgbay/anaconda3/envs/ptl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 869, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/OlfwayAdbayIgbay/anaconda3/envs/ptl/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 499, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n  File \"/home/OlfwayAdbayIgbayanaconda3/envs/ptl/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 752, in run_training_batch\r\n    batch_outputs = self._process_closure_result(\r\n  File \"/home/OlfwayAdbayIgbay/anaconda3/envs/ptl/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 813, in _process_closure_result\r\n    self.accumulated_loss.append(opt_closure_result.loss)\r\n  File \"/home/OlfwayAdbayIgbay/anaconda3/envs/ptl/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py\", line 74, in append\r\n    x = x.to(self.memory)\r\n```\r\n\r\n\r\nThere is also no memory access error when on cpu.\r\n\r\n\r\n### Update & Highly Impractical Workaround:\r\nThe problem seems to be connected to the sequence, in which the samples are passed. When using different random seeds, some of the seeds do not throw this error. The probability of not throwing this error seems to be lower than 1%. \r\n\r\nSo running the script in a for-loop until a working seed is found and it doesn't break off can be used as a workaround. However as soon as any hyperparameters are changed, the model breaks down again and you will have to find a new seed. So this is not reproducible, and also you will have to delete ~100 failed runs from your logger. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9008/comments",
    "author": "OlfwayAdbayIgbay",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-09-22T11:53:34Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 8993,
    "title": "Trainer failing silently for multi-node processing",
    "created_at": "2021-08-19T06:49:43Z",
    "closed_at": "2023-08-12T19:26:40Z",
    "labels": [
      "bug",
      "help wanted",
      "environment: slurm",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8993",
    "body": "## 🐛 Bug\r\n\r\nWhen I run the Trainer.fit command with multiple nodes the program fails silently and hangs forever.\r\nIf I specify 1 node with multiple GPUs, the process runs. But as soon as I specify 2 or more, the process just hangs indefinitely.\r\n\r\n### To Reproduce\r\n\r\n```\r\nclass DataModuleFromConfig(pl.LightningDataModule):\r\n    def __init__(self, batch_size, train=None, validation=None, test=None,\r\n                 wrap=False, num_workers=None, distributed=False):\r\n        super().__init__()\r\n        self.batch_size = batch_size\r\n        self.dataset_configs = dict()\r\n        self.num_workers = num_workers if num_workers is not None else batch_size*2\r\n        if train is not None:\r\n            self.dataset_configs[\"train\"] = train\r\n            self.train_dataloader = self._train_dataloader\r\n        if validation is not None:\r\n            self.dataset_configs[\"validation\"] = validation\r\n            self.val_dataloader = self._val_dataloader\r\n        if test is not None:\r\n            self.dataset_configs[\"test\"] = test\r\n            self.test_dataloader = self._test_dataloader\r\n        self.wrap = wrap\r\n\r\n    def _train_dataloader(self):\r\n        return DataLoader(self.datasets[\"train\"], \r\n                          batch_size=self.batch_size,\r\n                        #   num_workers=self.num_workers, \r\n                          shuffle=True\r\n                          )\r\n\r\n    def _val_dataloader(self):\r\n        return DataLoader(self.datasets[\"validation\"],\r\n                          batch_size=self.batch_size,\r\n                        #   num_workers=self.num_workers\r\n                          )\r\n\r\n    def _test_dataloader(self):\r\n        return DataLoader(self.datasets[\"test\"], \r\n                          batch_size=self.batch_size,\r\n                        #   num_workers=self.num_workers\r\n                          )\r\n\r\ndef train(opt, unknown, now, logdir, nowname):\r\n\r\n    ckptdir = os.path.join(logdir, \"checkpoints\")\r\n    cfgdir = os.path.join(logdir, \"configs\")\r\n    seed_everything(opt.seed)\r\n\r\n    try:\r\n        # init and save configs\r\n        configs = [OmegaConf.load(cfg) for cfg in opt.base]\r\n        print('Config from:', opt.base)\r\n        cli = OmegaConf.from_dotlist(unknown)\r\n        config = OmegaConf.merge(*configs, cli)\r\n        lightning_config = config.pop(\"lightning\", OmegaConf.create())\r\n        # merge trainer cli with config\r\n        trainer_config = lightning_config.get(\"trainer\", OmegaConf.create())\r\n        # default to ddp\r\n        trainer_config[\"accelerator\"] = \"ddp\"\r\n        opt.ngpus = opt.ngpus if torch.cuda.is_available() else 0\r\n        trainer_config[\"gpus\"] = opt.ngpus if torch.cuda.is_available() else 0\r\n        trainer_config[\"num_nodes\"] = opt.nodes\r\n        if not torch.cuda.is_available():\r\n            del trainer_config[\"accelerator\"]\r\n            cpu = True\r\n        else:\r\n            gpuinfo = trainer_config[\"gpus\"]\r\n            print(f\"Running on GPUs {gpuinfo}\")\r\n            cpu = False\r\n        trainer_opt = argparse.Namespace(**trainer_config)\r\n        lightning_config.trainer = trainer_config\r\n        print(lightning_config)\r\n\r\n        # trainer and callbacks\r\n        trainer_kwargs = dict()\r\n        \r\n        # add callback which sets up log directory\r\n        default_callbacks_cfg = {\r\n            \"checkpointing\": {\r\n                \"target\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\r\n                \"params\": {\r\n                    \"dirpath\": ckptdir,\r\n                    \"filename\": \"{epoch}\",\r\n                    \"verbose\": True,\r\n                }\r\n            },\r\n            \"learning_rate_logger\": {\r\n                \"target\": \"pytorch_lightning.callbacks.LearningRateMonitor\",\r\n                \"params\": {\r\n                    \"logging_interval\": \"step\",\r\n                }\r\n            },\r\n        }\r\n        \r\n        # Save best models\r\n        if config.model.monitor is not None:\r\n            print(f\"Monitoring {config.model.monitor} as checkpoint metric.\")\r\n            default_callbacks_cfg[\"checkpointing\"][\"params\"][\"monitor\"] = config.model.monitor\r\n            default_callbacks_cfg[\"checkpointing\"][\"params\"][\"mode\"] = \"min\"\r\n            default_callbacks_cfg[\"checkpointing\"][\"params\"][\"save_top_k\"] = config.model.save_top_k\r\n            default_callbacks_cfg[\"checkpointing\"][\"params\"][\"every_n_val_epochs\"] = 1\r\n            \r\n        callbacks_cfg = lightning_config.callbacks or OmegaConf.create()\r\n        callbacks_cfg = OmegaConf.merge(default_callbacks_cfg, callbacks_cfg)\r\n        trainer_kwargs[\"callbacks\"] = [instantiate_from_config(callbacks_cfg[k]) for k in callbacks_cfg]\r\n        trainer_kwargs[\"default_root_dir\"] = logdir\r\n        trainer_kwargs[\"weights_save_path\"] = logdir\r\n        trainer_kwargs[\"max_epochs\"] = config.model.max_epochs\r\n\r\n        trainer = Trainer.from_argparse_args(trainer_opt, **trainer_kwargs)\r\n        print('gpus', trainer.gpus, 'nodes', trainer.num_nodes, \\\r\n                'processes', trainer.num_processes, 'devices', trainer.devices)\r\n        \r\n        data = instantiate_from_config(config.data)\r\n        data.prepare_data()\r\n        data.setup()\r\n\r\n        # configure learning rate\r\n        bs, base_lr = config.data.params.batch_size, config.model.base_learning_rate\r\n        total_gpu = opt.ngpus * opt.nodes if opt.ngpus > 0 else 1\r\n        accumulate_grad_batches = lightning_config.trainer.accumulate_grad_batches or 1\r\n        lightning_config.trainer.accumulate_grad_batches = accumulate_grad_batches\r\n\r\n        # Initialize model\r\n        print('Creating model...')\r\n        model = instantiate_from_config(config.model)\r\n\r\n        model.learning_rate = accumulate_grad_batches * total_gpu * bs * base_lr\r\n        print(\"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (total_num_gpus) * {} (batchsize) * {:.2e} (base_lr)\".format(\r\n            model.learning_rate, accumulate_grad_batches, total_gpu, bs, base_lr))\r\n\r\n        # run\r\n        if opt.train:\r\n            try:\r\n                data_train = data.train_dataloader()\r\n                data_val = data.val_dataloader()\r\n                start_time = time.time()\r\n                trainer.fit(model, data_train, data_val)\r\n                training_time = datetime.timedelta(seconds=int(time.time() - start_time))\r\n\r\ndef main():\r\n\r\n    parser = get_parser()\r\n    parser = Trainer.add_argparse_args(parser)\r\n    opt, unknown = parser.parse_known_args()\r\n    now = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\r\n\r\n     if opt.name:\r\n            name = \"_\" + opt.name\r\n     elif opt.base:\r\n            cfg_fname = os.path.split(opt.base[0])[-1]\r\n            cfg_name = os.path.splitext(cfg_fname)[0]\r\n            name = \"_\" + cfg_name\r\n     else:\r\n            name = \"\"\r\n     nowname = now + name + opt.postfix\r\n     logdir = os.path.join(\"logs\", nowname)\r\n     os.makedirs(logdir, exist_ok=True)\r\n\r\n    # executor is the submission interface (logs are dumped in the folder)\r\n    executor = submitit.AutoExecutor(folder=logdir)\r\n    num_gpus_per_node = opt.ngpus\r\n    nodes = opt.nodes\r\n    \r\n    executor.update_parameters(\r\n        mem_gb=80 * num_gpus_per_node,\r\n        timeout_min=1500,\r\n        slurm_partition={NAME},\r\n        gpus_per_node=num_gpus_per_node, \r\n        tasks_per_node=num_gpus_per_node,\r\n        cpus_per_task=10, \r\n        nodes=nodes,\r\n        slurm_constraint=\"volta32gb\",\r\n    )\r\n\r\n    # Run on cluster\r\n    job = executor.submit(train, opt, unknown, now, logdir, nowname)\r\n   \r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### Expected behavior\r\n\r\nTraining the model on multiple nodes.\r\n\r\n### Environment\r\n\r\n* CUDA:                                                                \r\n        - GPU:                                                             \r\n                - Quadro GP100                                              \r\n                - Quadro GP100                                                \r\n        - available:         True                                               \r\n        - version:           10.2 \r\n        \r\n* Packages:                                                                     \r\n        - numpy:             1.19.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.0                              \r\n        - pytorch-lightning: 1.4.2                     \r\n        - tqdm:              4.61.0                                    \r\n* System:                                                    \r\n        - OS:                Linux                       \r\n        - architecture:                                     \r\n                - 64bit                                                     \r\n                - ELF                                                  \r\n        - processor:         x86_64                                   \r\n        - python:            3.8.5                                      \r\n        - version:           #57-Ubuntu SMP Thu Oct 15 10:57:00 UTC 2020\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8993/comments",
    "author": "MartaTintore",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-08-19T12:48:43Z",
        "body": "Hey @MartaTintore \r\nThere are a lot of unknowns in your posted code. Let's take a step back and make sure the basics work. Can we try this?\r\n\r\n```python\r\nimport os\r\nimport submitit\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.plugins.environments import SLURMEnvironment\r\n\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\nNUM_GPUS_PER_NODE = 8\r\nNUM_NODES = 2\r\n\r\n\r\ndef train():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        gpus=NUM_GPUS_PER_NODE,\r\n        num_nodes=NUM_NODES,\r\n        accelerator=\"ddp\",\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n    )\r\n    assert trainer.world_size == 16\r\n    assert isinstance(trainer.training_type_plugin.cluster_environment, SLURMEnvironment)\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n\r\n\r\ndef main():\r\n    logdir = \"debug_log_dir\"\r\n    os.makedirs(logdir, exist_ok=True)\r\n\r\n    # executor is the submission interface (logs are dumped in the folder)\r\n    executor = submitit.AutoExecutor(folder=logdir)\r\n    executor.update_parameters(\r\n        mem_gb=80 * NUM_GPUS_PER_NODE,\r\n        timeout_min=1500,\r\n        slurm_partition={NAME},\r\n        gpus_per_node=NUM_GPUS_PER_NODE,\r\n        tasks_per_node=NUM_GPUS_PER_NODE,\r\n        cpus_per_task=10,\r\n        nodes=NUM_NODES,\r\n        slurm_constraint=\"volta32gb\",\r\n    )\r\n    job = executor.submit(train)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\nYou may have to fill in a detail here an there, I have not run this myself. \r\n"
      },
      {
        "user": "MartaTintore",
        "created_at": "2021-08-19T13:27:02Z",
        "body": "Hi,\r\n\r\nThank you for the reply.\r\n\r\nBy running the basics I realized the problem was with the flags with which I was launching the job. \r\n`args.ngpus `is the flag I created myself to run the command on the terminal and set the number of gpus per node I request from the cluster. \r\nHowever, Pytorch Lightning requires the number of gpus as well by passing: `--gpus` in the same command.\r\n\r\nConclusion: I need to specify both (i) `--gpus` and (ii) `--ngpus` to make sure that (i) PL knows which GPUs to use and (ii) I request the same number of GPUs from the cluster. \r\nMaybe it would be nice to have a warning/message when the flag `--gpus` is not specified? It didnt complain about GPUs not being specified for PL...\r\n\r\nThanks again for your help!"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-08-19T14:25:29Z",
        "body": "@MartaTintore happy to hear that you were able to find the problem. \r\n\r\nYes, that's good feedback. Lightning could do a better job at validating that the Lightning parameters match the world size set by the slurm environment. \r\n\r\nHowever, to this point:\r\n\r\n> Maybe it would be nice to have a warning/message when the flag --gpus is not specified? It didnt complain about GPUs not being specified for PL...\r\n\r\nUnfortunately I believe we cannot know this! It's perfectly valid to run one or multiple training processes purely on CPU, but Lightning cannot know that unless the user provides this information somehow."
      }
    ]
  },
  {
    "number": 8918,
    "title": "self.log validation_step Misconfiguration error",
    "created_at": "2021-08-15T04:15:58Z",
    "closed_at": "2021-08-16T17:21:03Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0",
      "logging"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8918",
    "body": "## 🐛 Bug\r\n\r\nAfter an update from 1.3.0 to 1.4.2, existing code breaks with:\r\n\r\n```\r\n  File \"/home/rongcuid/miniconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py\", line 464, in log\r\n    raise MisconfigurationException(\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: You called `self.log(loss/validate, ...)` twice in `validation_step` with different arguments. This is not allowed\r\n```\r\nThere is only one `self.log` statement in my validation loop. However, there is an identical counterpart in the training step, which uses \"loss/train\" as name. It worked in 1.3.\r\n\r\n### To Reproduce\r\n\r\n\r\n### Expected behavior\r\n\r\nLogs validation loss correctly.\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- NVIDIA GeForce GTX 1080\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.21.1\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.9.0\r\n\t- pytorch-lightning: 1.4.2\r\n\t- tqdm:              4.62.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.11\r\n\t- version:           #21~1626191760~20.04~55de9c3-Ubuntu SMP Wed Jul 21 20:31:55 UTC\r\n```\r\n\r\n### Additional context",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8918/comments",
    "author": "rongcuid",
    "comments": [
      {
        "user": "rongcuid",
        "created_at": "2021-08-15T04:21:33Z",
        "body": "As a reference, my logging statement is the following:\r\n\r\n```\r\n        self.log(\"loss/validate\", {\r\n            \"total\": loss.detach(),\r\n            \"mel\": loss_mel.detach(),\r\n            \"spec\": loss_lin.detach(),\r\n            \"done\": loss_done.detach(),\r\n        })\r\n```\r\n\r\nOnly logging dictionary like this in `validation_step` produces this error. Doing so in training step does not cause this issue, and logging a value (such as `loss`) also does not cause this issue."
      },
      {
        "user": "carmocca",
        "created_at": "2021-08-15T13:27:13Z",
        "body": "Hi!\r\n\r\nAs a workaround, I believe setting `Trainer(num_sanity_val_steps=0)` will avoid the error.\r\n\r\nThank you for your patience!"
      },
      {
        "user": "rongcuid",
        "created_at": "2021-08-15T17:38:17Z",
        "body": "The workaround would crash similarly on second validation epoch"
      },
      {
        "user": "tchaton",
        "created_at": "2021-08-16T09:18:51Z",
        "body": "Dear @rongcuid, \r\n\r\nThanks for finding this bug. It seems the meta object is being dropped.\r\nA PR should resolve this.\r\n\r\nBest,\r\nT.C"
      }
    ]
  },
  {
    "number": 8876,
    "title": "Unable to save prediction dict in predict_step",
    "created_at": "2021-08-12T21:38:19Z",
    "closed_at": "2021-08-13T20:29:52Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8876",
    "body": "In `pytorch-lightning~=1.4.1` the predictions is normally saved using:\r\n\r\n```python\r\ndef test_step(self, batch, batch_idx):\r\n        idx, text, true_cls = batch[\"idx\"], batch[\"text\"], batch[\"cls\"]\r\n        rpr = self.encoder(text)\r\n        pred_cls = torch.argmax(self.cls_head(rpr), dim=-1)\r\n\r\n        self.write_prediction_dict(\r\n            {\r\n                \"idx\": idx,\r\n                \"rpr\": rpr,\r\n                \"true_cls\": true_cls,\r\n                \"pred_cls\": pred_cls\r\n            },\r\n            self.hparams.prediction.dir + self.hparams.prediction.name)\r\n```\r\nHowerver in `predict_step` using the same logic:\r\n\r\n```python\r\n    def predict_step(self, batch, batch_idx, dataloader_idx=None):\r\n        idx, text, true_cls = batch[\"idx\"], batch[\"text\"], batch[\"cls\"]\r\n        rpr = self.encoder(text)\r\n        pred_cls = torch.argmax(self.cls_head(rpr), dim=-1)\r\n\r\n        self.write_prediction_dict(\r\n            {\r\n                \"idx\": idx,\r\n                \"rpr\": rpr,\r\n                \"true_cls\": true_cls,\r\n                \"pred_cls\": pred_cls\r\n            },\r\n            self.hparams.representation.dir + self.hparams.representation.name)\r\n```\r\nraises the following error message:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 151, in perform_tasks\r\n    predict(params)\r\n  File \"main.py\", line 136, in predict\r\n    datamodule=dm\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 776, in predict\r\n    results = self._run(model)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 918, in _run\r\n    self._dispatch()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 984, in _dispatch\r\n    self.accelerator.start_predicting(self)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\", line 98, in start_predicting\r\n    self.training_type_plugin.start_predicting(trainer)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 169, in start_predicting\r\n    self._results = trainer.run_stage()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 995, in run_stage\r\n    return self._run_predict()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1095, in _run_predict\r\n    return self.predict_loop.run()\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/prediction_loop.py\", line 92, in advance\r\n    dataloader_iter, self.current_dataloader_idx, dl_max_batches, self.num_dataloaders, self.return_predictions\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\", line 111, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/prediction_epoch_loop.py\", line 98, in advance\r\n    self._predict_step(batch, batch_idx, dataloader_idx)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/prediction_epoch_loop.py\", line 131, in _predict_step\r\n    predictions = self.trainer.accelerator.predict_step(step_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\", line 241, in predict_step\r\n    return self.training_type_plugin.predict_step(*step_kwargs.values())\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 184, in predict_step\r\n    return self.model.predict_step(*args, **kwargs)\r\n  File \"/content/TeCBench/source/model/TeCModel.py\", line 97, in predict_step\r\n    self.hparams.representation.dir + self.hparams.representation.name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py\", line 605, in write_prediction_dict\r\n    self.write_prediction(k, v, filename)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py\", line 578, in write_prediction\r\n    self.trainer._evaluation_loop.predictions._add_prediction(name, value, filename)\r\n  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/properties.py\", line 606, in _evaluation_loop\r\n    raise RuntimeError(\"The `Trainer._evaluation_loop` property isn't defined. Accessed outside of scope\")\r\nRuntimeError: The `Trainer._evaluation_loop` property isn't defined. Accessed outside of scope\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8876/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2021-08-13T13:25:39Z",
        "body": "is this related also to #8875?"
      },
      {
        "user": "celsofranssa",
        "created_at": "2021-08-13T13:33:01Z",
        "body": "Hello @Borda,\r\nThese two issues have the same objective: save the prediction dictionary. In #8875, I was thinking about the new feature using  `BasePredictionWriter` callback, and here, I was unable to save predictions dictionary using `write_prediction_dict` which is available in PL 1.4.1.\r\n"
      },
      {
        "user": "tchaton",
        "created_at": "2021-08-13T13:44:43Z",
        "body": "Dear @celsofranssa,\r\n\r\nYes, I would recommend to use the `BasePredictionWriter`, this is safer and would work better depending on which accelerators you might be using.\r\n\r\nBest,\r\nT.C "
      },
      {
        "user": "celsofranssa",
        "created_at": "2021-08-13T20:29:52Z",
        "body": "Thank you @tchaton,\r\nCurrently, I am trying to apply `BasePredictionWriter` to save the predictions as it was done using `write_prediction_dict` (#8875 ). \r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 8872,
    "title": "Deepspeed accelerator calls datamodule.train_dataloader() prior to setup()",
    "created_at": "2021-08-12T16:32:45Z",
    "closed_at": "2021-09-07T16:24:01Z",
    "labels": [
      "bug",
      "help wanted",
      "data handling",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8872",
    "body": "## 🐛 Bug\r\n\r\nDeepspeed accelerator calls datamodule.train_dataloader() prior to setup(). This does not happen with other accelerators.\r\n\r\n### To Reproduce\r\n\r\n```py\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer, LightningDataModule\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass PlDataModule(LightningDataModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self._setup = False\r\n\r\n    def setup(self, stage):\r\n        self._setup = True\r\n\r\n    def train_dataloader(self):\r\n        assert self._setup\r\n        return DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    def val_dataloader(self):\r\n        assert self._setup\r\n        return DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    def test_dataloader(self):\r\n        assert self._setup\r\n        return DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n        accelerator=\"deepspeed\",\r\n    )\r\n    trainer.fit(model, datamodule=PlDataModule())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### Expected behavior\r\n\r\n`train_dataloader` is never called before `setup`.\r\n\r\n### Additional context\r\n\r\nBacktrace\r\n\r\n```\r\n  /home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py(553)fit()\r\n-> self._run(model)\r\n  /home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py(865)_run()\r\n-> self.accelerator.setup_environment()\r\n  /home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu.py(30)setup_environment()\r\n-> super().setup_environment()\r\n  /home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py(76)setup_environment()\r\n-> self.training_type_plugin.setup_environment()\r\n  /home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py(166)setup_environment()\r\n-> self.setup_distributed()\r\n  /home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/deepspeed.py(341)setup_distributed()\r\n-> self._format_config()\r\n  /home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/deepspeed.py(545)_format_config()\r\n-> self._format_batch_size_and_grad_accum_config()\r\n  /home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/deepspeed.py(555)_format_batch_size_and_grad_accum_config()\r\n-> batch_size = self._auto_select_batch_size()\r\n  /home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/deepspeed.py(566)_auto_select_batch_size()\r\n-> train_dataloader = self.lightning_module.train_dataloader()\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8872/comments",
    "author": "leezu",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-08-27T11:25:58Z",
        "body": "Dear @leezu,\r\n\r\nThe `DeepSpeed Plugin` is doing a hack to automatically resolved your batch_size, but won't actually use this dataloader for training.\r\nAnd there is no simple way to improve this unless you provide the batch_size within the deepspeed config. \r\n\r\nAnd, the DataLoader used for training will be defined within a distributed setting.\r\n\r\nMy advice is to use `is_distributed_available()` function if you have custom samplers.\r\n\r\nI will be closing this issue for now.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "leezu",
        "created_at": "2021-08-27T22:19:24Z",
        "body": "@tchaton thank you for looking into a fix. \r\n\r\n> And there is no simple way to improve this unless you provide the batch_size within the deepspeed config.\r\n\r\nEven if `setup` can't be called automatically, how about adding sanity assertions that ensure train_dataloader is never called before `setup`? In that case users can at least be informed about the issue"
      },
      {
        "user": "tchaton",
        "created_at": "2021-08-30T09:29:18Z",
        "body": "@SeanNaren Any idea ?"
      },
      {
        "user": "SeanNaren",
        "created_at": "2021-08-31T09:18:26Z",
        "body": "Thanks for the issue @leezu!\r\n\r\nTo skip the auto-infer of the batch size for logging, you can pass the batch size directly to the plugin like such:\r\n\r\n```python\r\nTrainer(plugins=DeepSpeedPlugin(logging_batch_size_per_gpu=32))\r\n```\r\n\r\nIf I modify the case, it works :)\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningModule, Trainer, LightningDataModule\r\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass PlDataModule(LightningDataModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self._setup = False\r\n\r\n    def setup(self, stage):\r\n        self._setup = True\r\n\r\n    def train_dataloader(self):\r\n        assert self._setup\r\n        return DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    def val_dataloader(self):\r\n        assert self._setup\r\n        return DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    def test_dataloader(self):\r\n        assert self._setup\r\n        return DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n        plugins=DeepSpeedPlugin(logging_batch_size_per_gpu=32),\r\n        gpus=1\r\n    )\r\n    trainer.fit(model, datamodule=PlDataModule())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nThis should skip the auto-infer of the batch size. We should add a warning here so that users are aware! Will add that in a PR"
      }
    ]
  },
  {
    "number": 8823,
    "title": "Allow `--gpus=None` to be specified on the CLI.",
    "created_at": "2021-08-10T03:30:58Z",
    "closed_at": "2021-08-10T16:32:13Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8823",
    "body": "Currently specifying `--gpus=None` breaks the utils.argparse logic. I want to allow the string None to be a valid option. In other words, I want the user to be able to explicitly specify the default value for this argument, which is currently not possible.\r\n\r\nWhy? In my workflow I customized my argparser. This caused the `--gpus` argument to default to something non-None. Then, for whatever reason, my nvidia drivers stopped working (as they sometimes do), so I wanted to fallback on the CPU. When I tried to set `--gpus=None` argparse balked at me because it was not a valid gpu option. But removing the `--gpus` option no longer defaulted to None. Therefore, there was no way for the user to overwrite my short-sighted defaults and simply get the CPU.\r\n\r\nIf the only way to set the value of an argument is by removing the specification, then that can cause issues like the one I had. Regardless, I think it is good design such that you can always _change_ the value of an argument to achieve a particular funtionality. \r\n\r\nAs an example say I have a script:\r\n\r\n```bash\r\npython fit.py \\\r\n    --gpus=1 \\\r\n    --num_workers=2 \r\n```\r\n\r\nthe diff from the above to the CPU version where it removes the line:\r\n\r\n```bash\r\npython fit.py \\\r\n    --num_workers=2 \r\n```\r\n\r\nno longer gives a reader any indication that a --gpus option was ever there, or is something that could be specified and changed, whereas \r\n\r\n```bash\r\npython fit.py \\\r\n    --gpus=None \\\r\n    --num_workers=2 \r\n```\r\n\r\nPreserves the original `--gpus` arg as something important that this script might vary. (It also makes it much eaiser to futher parametarize that argument in bash itself). I believe that having the option to simply change the value rather than being forced to remove the entire line is desirable. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8823/comments",
    "author": "Erotemic",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-08-10T07:01:35Z",
        "body": "@Erotemic in your use case what is the advantage of None over just setting gpus=0?\r\n\r\nNote we don't have parsing for \"None\" in other trainer arguments."
      },
      {
        "user": "Erotemic",
        "created_at": "2021-08-10T16:32:10Z",
        "body": "Well, now I feel silly.\r\n\r\nI'm so used to setting the specific device I want to use, I didn't even consider that. It still feels a little weird that the user can't set the argument explicitly to the default, but as long as 0 is effectively the default, then I'm happy enough to close the issue and PR. No need to add complexity when its not needed."
      }
    ]
  },
  {
    "number": 8739,
    "title": "`rank_zero_info`  does not present the current running file name and line number ",
    "created_at": "2021-08-05T06:00:59Z",
    "closed_at": "2021-09-14T06:27:13Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8739",
    "body": "## 🚀 Feature\r\n\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\n\r\nAlthough pl has already provided the `rank_zero_info` function, which can be used for distributed training situation, it does not present the current running file name and line number.\r\n\r\nFor example, the filename information is always `pytorch_lightning.utilities.distributed`, which is not helpful for us to quickly locate where the errors or warnings occur.\r\n\r\n```bash\r\n[2021-08-04 22:22:53,893][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True\r\n[2021-08-04 22:22:53,894][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores\r\n[2021-08-04 22:22:53,894][pytorch_lightning.utilities.distributed][INFO] - IPU available: False, using: 0 IPUs\r\n```\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\nI've implemented my own `logger` by referring to your code, the source code is as follows:\r\n\r\n```python\r\n# logger.py\r\nimport os\r\nimport traceback\r\nimport logging\r\nimport colorlog\r\nfrom functools import wraps\r\n\r\n\r\ndef rank_zero_only(fn):\r\n    \r\n    @wraps(fn)\r\n    def wrapped_fn(*args, **kwargs):\r\n        if rank_zero_only.rank == 0:\r\n            s =  traceback.extract_stack()\r\n            filename, lineno, name, line = s[-2]\r\n            args = list(args)\r\n            args[0] = f'[{filename}:{lineno}] - {args[0]}'\r\n            args = tuple(args)\r\n            return fn(*args, **kwargs)\r\n\r\n    return wrapped_fn\r\n\r\n\r\n# TODO: this should be part of the cluster environment\r\ndef _get_rank() -> int:\r\n    rank_keys = ('RANK', 'SLURM_PROCID', 'LOCAL_RANK')\r\n    for key in rank_keys:\r\n        rank = os.environ.get(key)\r\n        if rank is not None:\r\n            return int(rank)\r\n    return 0\r\n\r\n\r\n# add the attribute to the function but don't overwrite in case Trainer has already set it\r\nrank_zero_only.rank = getattr(rank_zero_only, 'rank', _get_rank())\r\n\r\n\r\ndef get_logger(name=__name__, level=logging.INFO, rank_zero=True) -> logging.Logger:\r\n    \"\"\"Initializes multi-GPU-friendly python logger.\"\"\"\r\n\r\n    logger = logging.getLogger(name)\r\n    logger.setLevel(level)\r\n    logger.propagate = False\r\n    if logger.hasHandlers():\r\n        logger.handlers.clear()\r\n    formatter = '%(cyan)s[%(asctime)s%(reset)s] [%(green)s%(levelname)-4s%(reset)s] %(message)s'\r\n    sh = colorlog.StreamHandler()\r\n    sh.setFormatter(\r\n        colorlog.ColoredFormatter(formatter)\r\n    )\r\n\r\n    # this ensures all logging levels get marked with the rank zero decorator\r\n    # otherwise logs would get multiplied for each GPU process in multi-GPU setup\r\n    if rank_zero:\r\n        for level in (\"debug\", \"info\", \"warning\", \"error\", \"exception\", \"fatal\", \"critical\"):\r\n            setattr(logger, level, rank_zero_only(getattr(logger, level)))\r\n            setattr(sh, level, rank_zero_only(getattr(logger, level)))\r\n\r\n    logger.addHandler(sh)\r\n    sh.close()\r\n    return logger\r\n\r\n```\r\n\r\nrunning example\r\n```python\r\nlogger = get_logger(__name__, logging.INFO, rank_zero=True)\r\nlogger.info('hello world')\r\n```\r\n\r\n\r\nIt can correctly print the current running file and line number, but the feature of `rank_zero` does not work as all ranks can still print messages. I think the possible reason is that `_get_rank()` function fails to recognize the rank of each process in `horovod` mode, so I wonder how can I solve this problem? \r\nBesides, will the logging module of pl  support present the current running file name and line number, or support user customization?\r\n\r\nThanks!\r\n\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8739/comments",
    "author": "marsggbo",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-09-04T07:03:22Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 8678,
    "title": "multigpu ddp: Code after fit executed many times",
    "created_at": "2021-08-02T13:28:33Z",
    "closed_at": "2021-08-03T08:37:52Z",
    "labels": [
      "bug",
      "help wanted",
      "distributed",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8678",
    "body": "## 🐛 Bug\r\n\r\nAfter training model with the Trainer.fit on 4-gpu machine with the accelerator=\"ddp\", my code which goes after that executed 3 (?) times. \r\nI receive 2 exceptions \"FileNotFoundError\" and then printing of successful weights saving.\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\n```py\r\n....\r\ntrainer = pl.Trainer(\r\n    gpus=-1,\r\n    precision=16 if train_opt.get(\"fp16\", False) else 32,\r\n    accelerator=\"ddp\",\r\n    accumulate_grad_batches=train_opt.get(\"grad_accum\", 1),\r\n    max_epochs=train_opt.get(\"epochs\", 20),\r\n    default_root_dir=train_opt.get(\"root_dir\", None),\r\n    callbacks=callbacks,\r\n    logger=logger,\r\n    log_every_n_steps=1,\r\n)\r\n....\r\ntrainer.fit(model, dataloaders[0], dataloaders[1])\r\nif trainer.state.status != TrainerStatus.FINISHED:\r\n    raise InterruptedError()\r\n\r\npath = checkpoint_callback.best_model_path\r\n\r\nos.makedirs(os.path.dirname(target_path), exist_ok=True)\r\nmodel.load_state_dict(torch.load(str(path))[\"state_dict\"])\r\ntorch.save(model.model.state_dict(), target_path)\r\n```\r\n\r\n### Expected behavior\r\n\r\nA single execution of the code after trainer.fit\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0\r\n\t- pytorch-lightning: 1.4.0rc0\r\n\t- tqdm:              4.61.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.7\r\n\t- version:           #1 SMP Tue May 11 20:50:07 UTC 2021\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8678/comments",
    "author": "johngull",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-08-03T08:36:48Z",
        "body": "Dear @johngull,\r\n\r\nThis is an expected behaviour.\r\n\r\nUsing accelerator `ddp, this will create multiple independent processes and you script will be run `world_size` times.\r\n\r\n```py\r\n....\r\ntrainer = pl.Trainer(\r\n    gpus=-1,\r\n    precision=16 if train_opt.get(\"fp16\", False) else 32,\r\n    accelerator=\"ddp\",\r\n    accumulate_grad_batches=train_opt.get(\"grad_accum\", 1),\r\n    max_epochs=train_opt.get(\"epochs\", 20),\r\n    default_root_dir=train_opt.get(\"root_dir\", None),\r\n    callbacks=callbacks,\r\n    logger=logger,\r\n    log_every_n_steps=1,\r\n)\r\n....\r\ntrainer.fit(model, dataloaders[0], dataloaders[1])\r\n\r\n# You should manipulate checkpoints only on rank 0 :)\r\nif trainer.is_global_zero:\r\n    path = checkpoint_callback.best_model_path\r\n    os.makedirs(os.path.dirname(target_path), exist_ok=True)\r\n    model.load_state_dict(torch.load(str(path))[\"state_dict\"])\r\n    torch.save(model.model.state_dict(), target_path)\r\n```\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "tchaton",
        "created_at": "2021-08-03T08:37:52Z",
        "body": "Dear @johngull,\r\n\r\nI will be closing this issue. Feel free to re-open it if you still have questions.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "johngull",
        "created_at": "2021-08-03T08:45:12Z",
        "body": "Hello @tchaton,\r\n\r\nThank you a lot for the clarification and the tip on how to fix it.\r\nI have several questions here.\r\n\r\n- Shall I wrap everything else before trainer.fit also?\r\n- Is there another acceleration method that is faster than data-parallel but doesn't have such behavior?\r\n\r\nThanks.\r\n"
      },
      {
        "user": "tchaton",
        "created_at": "2021-08-03T13:57:50Z",
        "body": "Hey @tchaton,\r\n\r\nMy pleasure :)\r\n\r\n`Shall I wrap everything else before trainer.fit also?`\r\n\r\nThe processes are being created on `trainer.fit` call, therefore the trainer isn't aware of its rank before. Alternatively, you could use `ddp_spawn`.\r\n\r\nYes, `ddp_spawn`.\r\n\r\nBest,\r\nT.C"
      }
    ]
  },
  {
    "number": 8531,
    "title": "Skip batch when cuda out of memory",
    "created_at": "2021-07-23T06:55:42Z",
    "closed_at": "2021-08-29T18:20:36Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8531",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nWhen training on a large dataset, sometimes I'll meet a `cuda out of memory` batch and stop my process. However, reducing batch_size will hurt performance and waste computation resource since this extreme case is rare. Also, my batch is dynamically generated, so I can't filter it before training_step. The easiest trade-off is dropping this batch (in forward, backward or optimizer.update). Can you provide this option, or how can I implement it  in `LightningModule`'s hooks?\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8531/comments",
    "author": "dalek-who",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-08-22T15:35:44Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 8507,
    "title": "`Observer`s are updated with validation data in quantization aware training",
    "created_at": "2021-07-21T15:16:25Z",
    "closed_at": "2021-10-25T15:46:10Z",
    "labels": [
      "bug",
      "feature",
      "help wanted",
      "design",
      "callback",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8507",
    "body": "## 🐛 Bug\r\n\r\n`Observer`s are updated (trained) with validation data in quantization aware training. Should `pytorch_lightning.callbacks.QuantizationAwareTraining` apply `torch.quantization.disable_observer` on `pl_module` in `on_validation_start` or `on_validation_epoch_start`?\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch import optim\r\nfrom torch.utils import data\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass BoringModel(pl.LightningModule):\r\n\r\n  def __init__(self):\r\n    super().__init__()\r\n    self.layer = nn.Conv2d(1, 1, (1, 1))\r\n\r\n  def forward(self, x):\r\n    return self.layer(x)\r\n\r\n  def training_step(self, batch, batch_idx):\r\n    raise NotImplementedError\r\n\r\n  def validation_step(self, batch, batch_idx):\r\n    self(batch)\r\n\r\n    output_observer = self.layer.activation_post_process.activation_post_process\r\n    # AssertionError.\r\n    assert all(\r\n        torch.isinf(val)\r\n        for val in [output_observer.min_val, output_observer.max_val]\r\n    )\r\n\r\n  def configure_optimizers(self):\r\n    return optim.SGD(self.parameters(), lr=0.1)\r\n\r\n\r\nclass RandomDataset(data.IterableDataset):\r\n\r\n  def __iter__(self):\r\n    yield from torch.rand(32, 1, 1, 1)\r\n\r\ntrain_data_loader = data.DataLoader(RandomDataset(), batch_size=32)\r\nval_data_loader = data.DataLoader(RandomDataset(), batch_size=32)\r\n\r\nmodel = BoringModel()\r\ntrainer = pl.Trainer(\r\n    num_sanity_val_steps=1,\r\n    callbacks=[pl.callbacks.QuantizationAwareTraining()]\r\n)\r\ntrainer.fit(\r\n    model, train_dataloader=train_data_loader, val_dataloaders=val_data_loader\r\n)\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo `AssertionError` occurs. The observer should not be trained with validation data.\r\n\r\n### Environment\r\n\r\n - PyTorch Lightning Version (e.g., 1.3.0): 1.3.8\r\n - PyTorch Version (e.g., 1.8): 1.9\r\n - Python version: 3.7.3\r\n - OS (e.g., Linux): Windows\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - How you installed PyTorch (`conda`, `pip`, source): `pip`\r\n - If compiling from source, the output of `torch.__config__.show()`:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8507/comments",
    "author": "manipopopo",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2021-07-22T08:22:06Z",
        "body": "> Should `pytorch_lightning.callbacks.QuantizationAwareTraining` apply `torch.quantization.disable_observer` on `pl_module` in `on_validation_start` or `on_validation_epoch_start`?\r\n\r\nthat is a good question and I feel that users may want both... so how about disable the validation data usage by default but still add an argument for enabling it? so we would implement it in `on_validation_start`/`on_validation_end`\r\n@PyTorchLightning/core-contributors @awaelchli other thoughts?\r\n@manipopopo interested in sending such PR?"
      },
      {
        "user": "manipopopo",
        "created_at": "2021-07-22T15:43:37Z",
        "body": "@Borda I'm glad to send a PR if the issue is confirmed."
      },
      {
        "user": "kandluis",
        "created_at": "2021-07-22T19:25:07Z",
        "body": "Yeah, this is a good issue. Most users would probably want to avoid calibrating with the validation set. My 2 cents here is that the model used for validation be as close as possible to the \"final\" model. From that POV, would it make sense to `convert` the model as well before the validation data is run? \r\n\r\nThat makes the design more complex, but would also make the validation metrics more accurate. "
      },
      {
        "user": "Borda",
        "created_at": "2021-07-22T20:04:24Z",
        "body": "> That makes the design more complex, but would also make the validation metrics more accurate.\r\n\r\nso the conclusion is to add an argument that would disable observing for validation, right?"
      },
      {
        "user": "manipopopo",
        "created_at": "2021-07-23T05:06:03Z",
        "body": "> would it make sense to convert the model as well before the validation data is run\r\n\r\nIt would be nice if we can performance validation with converted models. \r\n\r\nBut I'm not sure how to properly handle the prepared model and the converted model in current `pytorch_lightning.callbacks.Callback`. And maybe the validation loop would be slow as we move the converted model to CPU to run the quantized operators.\r\n\r\nMaybe we could just disable the observers for the moment?\r\n\r\n```python\r\n\r\nclass QuantizationAwareTraining(Callback):\r\n\r\n    def __init__(\r\n        self,\r\n        # ...\r\n        val_disable_observers=True,\r\n        test_disable_observers=True,\r\n        predict_disable_observers=True,\r\n    ):\r\n        # ...\r\n```"
      },
      {
        "user": "Borda",
        "created_at": "2021-07-23T07:41:48Z",
        "body": "> Maybe we could just disable the observers for the moment?\r\n\r\n@manipopopo lets split it, lest make one with the observer:\r\n```py\r\nenable_observers: Sequence[str] = ('train', 'valid')\r\n```\r\nso just list the phases which are used for collection stats, but we shall never use it for testing\r\n\r\nAnother PR would be adding an argument to run validation on the quantized model, but that shall be also on GPU if available"
      },
      {
        "user": "kandluis",
        "created_at": "2021-07-23T20:22:02Z",
        "body": "> > That makes the design more complex, but would also make the validation metrics more accurate.\r\n> \r\n> so the conclusion is to add an argument that would disable observing for validation, right?\r\n\r\nYes, I think this seems reasonable. I would say that by default we can disable calibration on the validation set as suggested above to keep this PR targetted. "
      }
    ]
  },
  {
    "number": 8425,
    "title": "DagsHub Logger ",
    "created_at": "2021-07-14T23:06:19Z",
    "closed_at": "2021-07-15T13:05:59Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8425",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nI would like to add a dagshub logger to PyTorch lightning\r\n\r\nI can work on this Can I please be assigned to it?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8425/comments",
    "author": "gagan3012",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2021-07-15T08:11:45Z",
        "body": "cc: @PyTorchLightning/core-contributors "
      }
    ]
  },
  {
    "number": 8402,
    "title": "Multiprocessing bug after switch to latest Lightning version",
    "created_at": "2021-07-13T18:59:29Z",
    "closed_at": "2021-07-14T23:53:24Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8402",
    "body": "> Epoch 0:   4%|████▏                                                                                                    | 4/100 [00:06<02:28,  1.55s/it, \r\n\r\n> loss=nan]Traceback (most recent call last):\r\n> \r\n>   File \"finetune.py\", line 27, in <module>\r\n>     run(args, Summarizer)\r\n>   File \"/home/griffin/kabupra/graph/main.py\", line 117, in run\r\n>     trainer.fit(model)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 460, in fit\r\n>     self._run(model)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 758, in _run\r\n>     self.dispatch()\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 799, in dispatch\r\n>     self.accelerator.start_training(self)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\r\n>     self.training_type_plugin.start_training(trainer)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 144, in start_training\r\n>     self._results = trainer.run_stage()\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 809, in run_stage\r\n>     return self.run_train()\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 871, in run_train\r\n>     self.train_loop.run_training_epoch()\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 491, in run_training_epoch\r\n>     for batch_idx, (batch, is_last_batch) in train_dataloader:\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/profiler/profilers.py\", line 112, in profile_iterable\r\n>     value = next(iterator)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py\", line 534, in prefetch_iterator\r\n>     for val in it:\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py\", line 464, in __next__\r\n>     return self.request_next_batch(self.loader_iters)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py\", line 478, in request_next_batch\r\n>     return apply_to_collection(loader_iters, Iterator, next)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py\", line 85, in apply_to_collection\r\n>     return function(data, *args, **kwargs)\r\n>   File \"/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\", line 521, in __next__\r\n>     data = self._next_data()\r\n>   File \"/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\", line 1186, in _next_data\r\n>     idx, data = self._get_data()\r\n>   File \"/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\", line 1152, in _get_data\r\n>     success, data = self._try_get_data()\r\n>   File \"/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\", line 990, in _try_get_data\r\n>     data = self._data_queue.get(timeout=timeout)\r\n>   File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 116, in get\r\n>     return _ForkingPickler.loads(res)\r\n>   File \"/usr/lib/python3/dist-packages/torch/multiprocessing/reductions.py\", line 289, in rebuild_storage_fd\r\n>     fd = df.detach()\r\n>   File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 58, in detach\r\n>     return reduction.recv_handle(conn)\r\n>   File \"/usr/lib/python3.8/multiprocessing/reduction.py\", line 189, in recv_handle\r\n>     return recvfds(s, 1)[0]\r\n>   File \"/usr/lib/python3.8/multiprocessing/reduction.py\", line 164, in recvfds\r\n>     raise RuntimeError('received %d items of ancdata' %\r\n> RuntimeError: received 0 items of ancdata\r\n> Traceback (most recent call last):\r\n>   File \"finetune.py\", line 27, in <module>\r\n>     run(args, Summarizer)\r\n>   File \"/home/griffin/kabupra/graph/main.py\", line 117, in run\r\n>     trainer.fit(model)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 460, in fit\r\n>     self._run(model)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 758, in _run\r\n>     self.dispatch()\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 799, in dispatch\r\n>     self.accelerator.start_training(self)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 96, in start_training\r\n>     self.training_type_plugin.start_training(trainer)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 144, in start_training\r\n>     self._results = trainer.run_stage()\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 809, in run_stage\r\n>     return self.run_train()\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 871, in run_train\r\n>     self.train_loop.run_training_epoch()\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 491, in run_training_epoch\r\n>     for batch_idx, (batch, is_last_batch) in train_dataloader:\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/profiler/profilers.py\", line 112, in profile_iterable\r\n>     value = next(iterator)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py\", line 534, in prefetch_iterator\r\n>     for val in it:\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py\", line 464, in __next__\r\n>     return self.request_next_batch(self.loader_iters)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py\", line 478, in request_next_batch\r\n>     return apply_to_collection(loader_iters, Iterator, next)\r\n>   File \"/home/griffin/sauce/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py\", line 85, in apply_to_collection\r\n>     return function(data, *args, **kwargs)\r\n>   File \"/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\", line 521, in __next__\r\n>     data = self._next_data()\r\n>   File \"/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\", line 1186, in _next_data\r\n>     idx, data = self._get_data()\r\n>   File \"/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\", line 1152, in _get_data\r\n>     success, data = self._try_get_data()\r\n>   File \"/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\", line 990, in _try_get_data\r\n>     data = self._data_queue.get(timeout=timeout)\r\n>   File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 116, in get\r\n>     return _ForkingPickler.loads(res)\r\n>   File \"/usr/lib/python3/dist-packages/torch/multiprocessing/reductions.py\", line 289, in rebuild_storage_fd\r\n>     fd = df.detach()\r\n>   File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 58, in detach\r\n>     return reduction.recv_handle(conn)\r\n>   File \"/usr/lib/python3.8/multiprocessing/reduction.py\", line 189, in recv_handle\r\n>     return recvfds(s, 1)[0]\r\n>   File \"/usr/lib/python3.8/multiprocessing/reduction.py\", line 164, in recvfds\r\n>     raise RuntimeError('received %d items of ancdata' %\r\n> RuntimeError: received 0 items of ancdata\r\n\r\nHi - I just switched over to the latest version of Lightning and for some reason I am getting this error.  Does anyone know why this might be the case?  I am loading each example to a file during training so maybe there are too many files open?  I've reduced the num_workers to a small int and it's still happening.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8402/comments",
    "author": "griff4692",
    "comments": [
      {
        "user": "griff4692",
        "created_at": "2021-07-14T23:53:24Z",
        "body": "For others, I resolved this by adding\r\n\r\n`torch.multiprocessing.set_sharing_strategy('file_system')`"
      },
      {
        "user": "huangjin520",
        "created_at": "2021-08-06T15:41:19Z",
        "body": "> For others, I resolved this by adding\r\n> \r\n> `torch.multiprocessing.set_sharing_strategy('file_system')`\r\n\r\nHi , I meet the same question as you . So I think you solution will help me out the dillemma. In order to apply your approach ,i want to know  **where the code** `torch.multiprocessing.set_sharing_strategy('file_system')` **should be placed** , and **what's the meaning of 'file_system'.**\r\nThanks for your reply sincerely!"
      }
    ]
  },
  {
    "number": 8351,
    "title": "_pickle.PicklingError: Can't pickle <class 'TypeError'>: it's not the same object as builtins.TypeError",
    "created_at": "2021-07-09T11:40:19Z",
    "closed_at": "2021-08-29T18:20:37Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8351",
    "body": "## 🐛 Bug\r\n\r\nEncountering the following issue:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'TypeError'>: it's not the same object as builtins.TypeError\r\nTraceback (most recent call last):\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'TypeError'>: it's not the same object as builtins.TypeError\r\n```\r\n\r\n* This happens when using just 1 GPU without multi-gpu\r\n* I have manually pickled my model, data loader and all modules and have fixed any issues\r\n\r\n - PyTorch Lightning Version 1.3.8\r\n - PyTorch Version 1.9\r\n - Python version: 3.8.2\r\n - OS: Linux\r\n - CUDA/cuDNN version: cuda/10.2-cudnn7.5.1\r\n - GPU models and configuration:\r\n - How you installed PyTorch: pip\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8351/comments",
    "author": "cyrusvahidi",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-07-09T14:54:31Z",
        "body": "Add \r\n\r\n```\r\nif __name__ == \"__main__\" \r\n```\r\n\r\nwhere your entry point to the script is. \r\nChances are high you get this because you have num_workers > 0 in your DataLoader."
      },
      {
        "user": "cyrusvahidi",
        "created_at": "2021-07-09T17:45:54Z",
        "body": "thanks, but no change\r\n\r\n1. that was already in the script entry point\r\n2. changing num_workers also makes no difference\r\n\r\nafter manual inspection, I noticed that injection of callables with `gin-config` caused some pickling issues, but the issue persists even after solving these"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-07-09T17:56:28Z",
        "body": "@rastegah sorry for the standard answer but this was my best guess. \r\nIt looks like you are not posting the full error stack trace so we don't know where this is originating from. And without a code sample it will also be hard to know what's going on. "
      },
      {
        "user": "cyrusvahidi",
        "created_at": "2021-07-14T10:17:34Z",
        "body": "Here is the entry point to the script: \r\n```\r\nimport fire\r\nimport os\r\nimport gin\r\n\r\nfrom dptm.model import lightning_run\r\nfrom dptm.utils import gin_register_and_parse\r\n\r\n@gin.configurable\r\ndef run_train(gin_file: str = \"gin/dptm.gin\"):\r\n    gin_config_path = os.path.join(os.getcwd(), gin_file)\r\n    gin_register_and_parse(gin_config_path)\r\n\r\n    lightning_run(gin_config_path)\r\n\r\ndef main():\r\n  fire.Fire(run_train)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n```\r\n@gin.configurable\r\ndef lightning_run(gin_config: str = None,\r\n                  n_epochs: float = 50, \r\n                  batch_size: float = 1, \r\n                  patience: int = 5,\r\n                  log: bool = False,\r\n                  data_module: Callable = LitDataModule):\r\n    dataset = LitDataModule()\r\n\r\n    model = LitModule()\r\n\r\n    # Initialize a trainer\r\n    logger = init_logger(gin_config) if log else None\r\n    trainer = pl.Trainer(gpus=1,\r\n                                   max_epochs=n_epochs,\r\n                                   progress_bar_refresh_rate=20, \r\n                                   logger=logger)\r\n\r\n    # Train the model ⚡\r\n    trainer.fit(model, dataset)\r\n    trainer.test(model)\r\n```\r\n\r\nAnd the stack trace occurs during validation sanity check:\r\n\r\n`Validation sanity check: 0it [00:00, ?it/s]Traceback (most recent call last):\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'TypeError'>: it's not the same object as builtins.TypeError\r\nTraceback (most recent call last):\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/import/linux/python/3.8.2/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'TypeError'>: it's not the same object as builtins.TypeError`\r\n\r\nI've spent hours trying to locate the issue. Manually pickled all modules and data modules. I'm usually an analogous environment and setup to other projects that do not encounter this error."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-07-14T10:23:22Z",
        "body": "Can you set `num_workers=0` just to make sure this is not due to the multiprocessing in dataloaders?\r\nAnd are you 100% sure this is the full stack trace?\r\nThere is nothing more above ` Validation sanity check: 0it [00:00, ?it/s]Traceback (most recent call last): File`?"
      },
      {
        "user": "cyrusvahidi",
        "created_at": "2021-07-14T11:44:53Z",
        "body": "Setting `num_workers=0` does work now! Any idea how to resolve this? "
      },
      {
        "user": "awaelchli",
        "created_at": "2021-07-14T15:03:09Z",
        "body": "Yes, that means you have somewhere a non-picklable object that gets accessed or imported in the code that runs in the worker processes. That's usually everything inside your dataset code and everything that gets imported there. PyTorch / Python will pickle all these objects to recreate the state in spawned processes. You would run into this issue even without Lightning I'm pretty sure. \r\nTo test it, simply do this: \r\n\r\n```python\r\ndef main():\r\n\r\n    dataset = MyDaset(...)\r\n    dataloader = DataLoader(..., num_workers=2)\r\n    data = next(iter(dataloader))\r\n    \r\n    # comment out all Lightning code\r\n    # trainer.fit()\r\n```\r\n    \r\nand if you run into a pickle error we know for sure what's the problem."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-22T15:35:43Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 8302,
    "title": "TypeError: training_step_end() missing 1 required positional argument: 'batch_idx'",
    "created_at": "2021-07-06T06:39:00Z",
    "closed_at": "2021-07-06T07:13:44Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8302",
    "body": "My first time using lightning. Basically, I am trying to convert the following code into lightning format:\r\n```\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\nmodel.to(device)\r\n\r\navg_loss = 0.\r\navg_output_std = 0.\r\nfor e in range(epochs):\r\n\r\n    for (x0, x1), _, _ in dataloader_train_simsiam:\r\n\r\n        # move images to the gpu\r\n        x0 = x0.to(device)\r\n        x1 = x1.to(device)\r\n\r\n        # run the model on both transforms of the images\r\n        # the output of the simsiam model is a y containing the predictions\r\n        # and projections for each input x\r\n        y0, y1 = model(x0, x1)\r\n\r\n        # backpropagation\r\n        loss = criterion(y0, y1)\r\n        loss.backward()\r\n\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n\r\n        # calculate the per-dimension standard deviation of the outputs\r\n        # we can use this later to check whether the embeddings are collapsing\r\n        output, _ = y0\r\n        output = output.detach()\r\n        output = torch.nn.functional.normalize(output, dim=1)\r\n\r\n        output_std = torch.std(output, 0)\r\n        output_std = output_std.mean()\r\n\r\n        # use moving averages to track the loss and standard deviation\r\n        w = 0.9\r\n        avg_loss = w * avg_loss + (1 - w) * loss.item()\r\n        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\r\n\r\n    # the level of collapse is large if the standard deviation of the l2\r\n    # normalized output is much smaller than 1 / sqrt(dim)\r\n    collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\r\n    # print intermediate results\r\n    print(f'[Epoch {e:3d}] '\r\n        f'Loss = {avg_loss:.2f} | '\r\n        f'Collapse Level: {collapse_level:.2f} / 1.00')\r\n```\r\nWhat I have done so far is this:\r\n```\r\nclass SimSiamModel(pl.LightningModule):\r\n    def __init__(self, backbone, num_ftrs, pred_hidden_dim, out_dim, num_mlp_layers):\r\n        super().__init__()\r\n\r\n        # create a moco based on ResNet\r\n        self.resnet_simsiam = \\\r\n            lightly.models.SimSiam(\r\n                            backbone,\r\n                            num_ftrs=num_ftrs,\r\n                            proj_hidden_dim=pred_hidden_dim,\r\n                            pred_hidden_dim=pred_hidden_dim,\r\n                            out_dim=out_dim,\r\n                            num_mlp_layers=num_mlp_layers\r\n                        )\r\n\r\n        # create our loss with the optional memory bank\r\n        self.criterion = lightly.loss.SymNegCosineSimilarityLoss()\r\n\r\n    def forward(self, x):\r\n        self.resnet_simsiam(x)\r\n\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        (x0, x1), _, _ = batch\r\n        y0, y1 = self.resnet_simsiam(x0, x1)\r\n        loss = self.criterion(y0, y1)\r\n        self.log('train_loss_ss', loss)\r\n        return loss, y0\r\n    \r\n    def training_step_end(self, batch, batch_idx):\r\n        loss, y0 = self.training_step(self, batch, batch_idx)\r\n        output, _ = y0\r\n        output = output.detach()\r\n        output = torch.nn.functional.normalize(output, dim=1)\r\n\r\n        output_std = torch.std(output, 0)\r\n        output_std = output_std.mean()\r\n\r\n        # use moving averages to track the loss and standard deviation\r\n        w = 0.9\r\n        avg_loss = 0.\r\n        avg_output_std = 0.\r\n        avg_loss = w * avg_loss + (1 - w) * loss.item()\r\n        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\r\n        \r\n        return avg_loss, avg_output_std\r\n        \r\n    def training_epoch_end(self, batch, batch_idx, ):\r\n        avg_loss, avg_output_std = self.training_step_end(self, batch, batch_idx)\r\n        collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\r\n        \r\n        self.log('loss', round(avg_loss,2), prog_bar=True)\r\n        self.log('Collapse Level', round(collapse_evel,2), prog_bar=True)\r\n        \r\n    def configure_optimizers(self):\r\n        lr = 0.05 * batch_size / 256\r\n        optimizer = AdamP(self.resnet_simsiam.parameters(), lr=lr,\r\n                                weight_decay=1e-4)\r\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\r\n        return [optimizer], [scheduler]\r\n```\r\nWhen I run the code, I get the error:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-16-eb464d96d0bd> in <module>\r\n     18     trainer.fit(\r\n     19         model,\r\n---> 20         train_loader_simsiam\r\n     21         )\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    458         )\r\n    459 \r\n--> 460         self._run(model)\r\n    461 \r\n    462         assert self.state.stopped\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\r\n    756 \r\n    757         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n--> 758         self.dispatch()\r\n    759 \r\n    760         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n    797             self.accelerator.start_predicting(self)\r\n    798         else:\r\n--> 799             self.accelerator.start_training(self)\r\n    800 \r\n    801     def run_stage(self):\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n     94 \r\n     95     def start_training(self, trainer: 'pl.Trainer') -> None:\r\n---> 96         self.training_type_plugin.start_training(trainer)\r\n     97 \r\n     98     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\r\n    142     def start_training(self, trainer: 'pl.Trainer') -> None:\r\n    143         # double dispatch to initiate the training loop\r\n--> 144         self._results = trainer.run_stage()\r\n    145 \r\n    146     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\r\n    807         if self.predicting:\r\n    808             return self.run_predict()\r\n--> 809         return self.run_train()\r\n    810 \r\n    811     def _pre_training_routine(self):\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n    869                 with self.profiler.profile(\"run_training_epoch\"):\r\n    870                     # run train epoch\r\n--> 871                     self.train_loop.run_training_epoch()\r\n    872 \r\n    873                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n    497             # ------------------------------------\r\n    498             with self.trainer.profiler.profile(\"run_training_batch\"):\r\n--> 499                 batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    500 \r\n    501             # when returning -1 from train_step, we end epoch early\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    736 \r\n    737                         # optimizer step\r\n--> 738                         self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n    739                         if len(self.trainer.optimizers) > 1:\r\n    740                             # revert back to previous state\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_step(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n    440             on_tpu=self.trainer._device_type == DeviceType.TPU and _TPU_AVAILABLE,\r\n    441             using_native_amp=using_native_amp,\r\n--> 442             using_lbfgs=is_lbfgs,\r\n    443         )\r\n    444 \r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py in optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\r\n   1401 \r\n   1402         \"\"\"\r\n-> 1403         optimizer.step(closure=optimizer_closure)\r\n   1404 \r\n   1405     def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer, optimizer_idx: int):\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py in step(self, closure, *args, **kwargs)\r\n    212             profiler_name = f\"optimizer_step_and_closure_{self._optimizer_idx}\"\r\n    213 \r\n--> 214         self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\r\n    215         self._total_optimizer_step_calls += 1\r\n    216 \r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py in __optimizer_step(self, closure, profiler_name, **kwargs)\r\n    132 \r\n    133         with trainer.profiler.profile(profiler_name):\r\n--> 134             trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\r\n    135 \r\n    136     def step(self, *args, closure: Optional[Callable] = None, **kwargs):\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py in optimizer_step(self, optimizer, opt_idx, lambda_closure, **kwargs)\r\n    327         )\r\n    328         if make_optimizer_step:\r\n--> 329             self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\r\n    330         self.precision_plugin.post_optimizer_step(optimizer, opt_idx)\r\n    331         self.training_type_plugin.post_optimizer_step(optimizer, opt_idx, **kwargs)\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py in run_optimizer_step(self, optimizer, optimizer_idx, lambda_closure, **kwargs)\r\n    334         self, optimizer: Optimizer, optimizer_idx: int, lambda_closure: Callable, **kwargs: Any\r\n    335     ) -> None:\r\n--> 336         self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\r\n    337 \r\n    338     def optimizer_zero_grad(self, current_epoch: int, batch_idx: int, optimizer: Optimizer, opt_idx: int) -> None:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in optimizer_step(self, optimizer, lambda_closure, **kwargs)\r\n    191 \r\n    192     def optimizer_step(self, optimizer: torch.optim.Optimizer, lambda_closure: Callable, **kwargs):\r\n--> 193         optimizer.step(closure=lambda_closure, **kwargs)\r\n    194 \r\n    195     @property\r\n\r\n~/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py in wrapper(*args, **kwargs)\r\n     63                 instance._step_count += 1\r\n     64                 wrapped = func.__get__(instance, cls)\r\n---> 65                 return wrapped(*args, **kwargs)\r\n     66 \r\n     67             # Note that the returned function here is no longer a bound method,\r\n\r\n~/.local/lib/python3.6/site-packages/torch/optim/optimizer.py in wrapper(*args, **kwargs)\r\n     86                 profile_name = \"Optimizer.step#{}.step\".format(obj.__class__.__name__)\r\n     87                 with torch.autograd.profiler.record_function(profile_name):\r\n---> 88                     return func(*args, **kwargs)\r\n     89             return wrapper\r\n     90 \r\n\r\n~/.local/lib/python3.6/site-packages/timm/optim/adamp.py in step(self, closure)\r\n     56         loss = None\r\n     57         if closure is not None:\r\n---> 58             loss = closure()\r\n     59 \r\n     60         for group in self.param_groups:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in train_step_and_backward_closure()\r\n    731                         def train_step_and_backward_closure():\r\n    732                             result = self.training_step_and_backward(\r\n--> 733                                 split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\r\n    734                             )\r\n    735                             return None if result is None else result.loss\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    821         with self.trainer.profiler.profile(\"training_step_and_backward\"):\r\n    822             # lightning module hook\r\n--> 823             result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    824             self._curr_step_result = result\r\n    825 \r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    293             self.trainer.logger_connector.cache_logged_metrics()\r\n    294 \r\n--> 295             training_step_output = self.trainer.call_hook(\"training_step_end\", training_step_output)\r\n    296 \r\n    297             self._check_training_step_output(training_step_output)\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in call_hook(self, hook_name, *args, **kwargs)\r\n   1233             if is_overridden(hook_name, model_ref):\r\n   1234                 hook_fx = getattr(model_ref, hook_name)\r\n-> 1235                 output = hook_fx(*args, **kwargs)\r\n   1236 \r\n   1237             # if the PL module doesn't have the hook then call the accelerator\r\n\r\nTypeError: training_step_end() missing 1 required positional argument: 'batch_idx'\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8302/comments",
    "author": "etetteh",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-07-06T07:13:44Z",
        "body": "Dear @etetteh,\r\n\r\nHere is the correct signature for `training_step_end`. It just takes outputs from the `training_step` for you to make more processing on them. `batch_idx` shouldn't be there.\r\n\r\n```\r\n    def training_step_end(self, outputs):\r\n        # only use when  on dp\r\n        outputs = torch.cat(outputs, dim=1)\r\n        softmax = softmax(outputs, dim=1)\r\n        out = softmax.mean()\r\n        return out\r\n```\r\n\r\nSmall advice. When implementing model with Lightning, use auto-completion from your IDE. It will automatically add the right function signature.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "etetteh",
        "created_at": "2021-07-06T09:01:01Z",
        "body": "Thanks for the advice on the IDE. The idx issue got resolved, but the training_step_end function is not performing the expected job. My goal is to get the loss and y0 from the training_step, to perform the operations under the training_step_end\r\n"
      }
    ]
  },
  {
    "number": 8269,
    "title": "TypeError: val_dataloader() missing 1 required positional argument: 'self'",
    "created_at": "2021-07-03T17:49:00Z",
    "closed_at": "2021-08-12T01:15:30Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8269",
    "body": "## 🐛 Bug\r\n\r\nI am working on an Image Classification Task with PyTorch Lightning. However I get a `TypeError` while implementing it. I have created the datamodule and model as shown in the PyTorch Lightning examples. The model I am using is `VGG16` with Batch Normalization.\r\n\r\nIn the `FruitsDataModule` I get the error only for the `val_dataloader` and not for the `train_dataloader` which is confusing because both of these funtions are doing the exact same thing just with different data.\r\n\r\nThe relevant code has been shown below.\r\n\r\n# Imports\r\n```\r\nimport pandas as pd \r\nimport numpy as np \r\n\r\nimport torch \r\nimport torch.nn as nn\r\nimport torchvision.datasets as datasets \r\nimport torchvision.transforms as transforms\r\nfrom torch.utils.data import DataLoader\r\nimport torchvision.models as models \r\n\r\nfrom tqdm import tqdm\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import roc_auc_score\r\n\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning import Trainer\r\n```\r\n\r\n# Config\r\n```\r\nclass config:\r\n    BATCH_SIZE = 128\r\n    LEARNING_RATE = 1e-3\r\n    DATA_DIRECTORY = \"../input/fruits/fruits-360\"\r\n    TRAIN_DATA_PATH = DATA_DIRECTORY + \"/Training\"\r\n    TEST_DATA_PATH = DATA_DIRECTORY + \"/Test\"\r\n    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n    EPOCHS = 10\r\n    SEED = 42\r\n```\r\n\r\n# DataModule\r\n```\r\nclass FruitsDataModule(pl.LightningDataModule):\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n        self.transform = transforms.Compose(\r\n            [\r\n                transforms.ToTensor()\r\n            ]\r\n        )\r\n        \r\n    def setup(self, stage=None):\r\n        \r\n        if stage == 'fit' or stage is None:\r\n\r\n            full_train_dataset = datasets.ImageFolder(\r\n                root = config.TRAIN_DATA_PATH,\r\n                transform = self.transform\r\n            )\r\n            \r\n            train_dataset, val_dataset = train_test_split(\r\n                full_train_dataset, \r\n                test_size=0.33,\r\n                random_state = 42\r\n            )\r\n            \r\n        if stage == 'test' or stage is None:\r\n\r\n            test_dataset = datasets.ImageFolder(\r\n                root = config.TEST_DATA_PATH,\r\n                transform = self.transform\r\n            )\r\n            \r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n            train_dataset,\r\n            batch_size = config.BATCH_SIZE,\r\n            shuffle = True\r\n        )\r\n        \r\n    def val_dataloader(self):\r\n        return DataLoader(\r\n            val_dataset,\r\n            batch_size = config.BATCH_SIZE,\r\n        )\r\n    \r\n    def test_dataloader(self):\r\n        return DataLoader(\r\n            test_dataset,\r\n            batch_size = config.BATCH_SIZE,\r\n        )\r\n```\r\n\r\n# Model\r\n```\r\nclass VGGModel(pl.LightningModule):\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n        \r\n        self.model = models.vgg16_bn(pretrained=True)\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        \r\n    def forward(self, x):\r\n        x = self.model(x)\r\n        return x\r\n    \r\n    def step(self, batch):\r\n        x, y = batch\r\n        logits = self.forward(x)\r\n        loss = self.criterion(logits, y)\r\n        preds = torch.argmax(logits, dim=1)\r\n        return loss, preds, y\r\n        \r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        loss, preds, targets = self.step(batch)\r\n        \r\n        # log train metrics\r\n        acc = roc_auc_score(preds, targets)\r\n        self.log(\"train/loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\r\n        self.log(\"train/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\r\n        \r\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\r\n    \r\n    \r\n   \r\n    def validation_step(self, batch, batch_idx):\r\n        loss, preds, targets = self.step(batch)\r\n\r\n        # log val metrics\r\n        acc = roc_auc_score(preds, targets)\r\n        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\r\n        self.log(\"val/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\r\n\r\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\r\n    \r\n    \r\n    def test_step(self, batch, batch_idx):\r\n        loss, preds, targets = self.step(batch)\r\n\r\n        # log test metrics\r\n        acc = roc_auc_score(preds, targets)\r\n        self.log(\"test/loss\", loss, on_step=False, on_epoch=True)\r\n        self.log(\"test/acc\", acc, on_step=False, on_epoch=True)\r\n\r\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\r\n    \r\n   \r\n    \r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr = config.LEARNING_RATE)\r\n        return optimizer\r\n```\r\n\r\n# Training\r\n```\r\nmodel = VGGModel()\r\n\r\ntrainer = pl.Trainer(\r\n    max_epochs=1,\r\n    gpus=[0],\r\n    precision=32,\r\n    progress_bar_refresh_rate=20\r\n)\r\n\r\ntrainer.fit(model, datamodule = FruitsDataModule)\r\n```\r\n\r\n# Error Log\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-6-5990df7ecb15> in <module>\r\n      8 )\r\n      9 \r\n---> 10 trainer.fit(model, datamodule = FruitsDataModule)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    497 \r\n    498         # dispath `start_training` or `start_testing` or `start_predicting`\r\n--> 499         self.dispatch()\r\n    500 \r\n    501         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n    544 \r\n    545         else:\r\n--> 546             self.accelerator.start_training(self)\r\n    547 \r\n    548     def train_or_test_or_predict(self):\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n     71 \r\n     72     def start_training(self, trainer):\r\n---> 73         self.training_type_plugin.start_training(trainer)\r\n     74 \r\n     75     def start_testing(self, trainer):\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\r\n    112     def start_training(self, trainer: 'Trainer') -> None:\r\n    113         # double dispatch to initiate the training loop\r\n--> 114         self._results = trainer.run_train()\r\n    115 \r\n    116     def start_testing(self, trainer: 'Trainer') -> None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n    605             self.progress_bar_callback.disable()\r\n    606 \r\n--> 607         self.run_sanity_check(self.lightning_module)\r\n    608 \r\n    609         # set stage for logging\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_sanity_check(self, ref_model)\r\n    852         # to make sure program won't crash during val\r\n    853         if should_sanity_check:\r\n--> 854             self.reset_val_dataloader(ref_model)\r\n    855             self.num_sanity_val_batches = [\r\n    856                 min(self.num_sanity_val_steps, val_batches) for val_batches in self.num_val_batches\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py in reset_val_dataloader(self, model)\r\n    362         has_step = is_overridden('validation_step', model)\r\n    363         if has_loader and has_step:\r\n--> 364             self.num_val_batches, self.val_dataloaders = self._reset_eval_dataloader(model, 'val')\r\n    365 \r\n    366     def reset_test_dataloader(self, model) -> None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py in _reset_eval_dataloader(self, model, mode)\r\n    276         # always get the loaders first so we can count how many there are\r\n    277         loader_name = f'{mode}_dataloader'\r\n--> 278         dataloaders = self.request_dataloader(getattr(model, loader_name))\r\n    279 \r\n    280         if not isinstance(dataloaders, list):\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py in request_dataloader(self, dataloader_fx)\r\n    396             The dataloader\r\n    397         \"\"\"\r\n--> 398         dataloader = dataloader_fx()\r\n    399         dataloader = self._flatten_dl_only(dataloader)\r\n    400 \r\n\r\nTypeError: val_dataloader() missing 1 required positional argument: 'self'\r\n```\r\n\r\nHow to fix this?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8269/comments",
    "author": "ishandutta0098",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-07-03T23:16:02Z",
        "body": "Change \r\n\r\n`trainer.fit(model, datamodule = FruitsDataModule)\r\n`\r\nto \r\n\r\n`trainer.fit(model, datamodule = FruitsDataModule())\r\n`\r\nmaybe?\r\n\r\nYour code is not runnable, it is missing all imports. "
      },
      {
        "user": "ishandutta0098",
        "created_at": "2021-07-04T05:49:13Z",
        "body": "@awaelchli I tried this, but it did not work. I have edited my issue and have included all the `imports` and `config` as well."
      },
      {
        "user": "akihironitta",
        "created_at": "2021-07-04T06:10:23Z",
        "body": "@ishandutta0098 Are you sure that it doesn't work for the same error? With @awaelchli's fix, it seems to work now. (confirmed on Google Colab)\r\n\r\n<details>\r\n<summary>full script</summary>\r\n\r\n```python\r\nimport pandas as pd \r\nimport numpy as np \r\n\r\nimport torch \r\nimport torch.nn as nn\r\nimport torchvision.datasets as datasets \r\nimport torchvision.transforms as transforms\r\nfrom torch.utils.data import DataLoader\r\nimport torchvision.models as models \r\n\r\nfrom tqdm import tqdm\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import roc_auc_score\r\n\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning import Trainer\r\n\r\nclass config:\r\n    BATCH_SIZE = 128\r\n    LEARNING_RATE = 1e-3\r\n    DATA_DIRECTORY = \"../input/fruits/fruits-360\"\r\n    TRAIN_DATA_PATH = DATA_DIRECTORY + \"/Training\"\r\n    TEST_DATA_PATH = DATA_DIRECTORY + \"/Test\"\r\n    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n    EPOCHS = 10\r\n    SEED = 42\r\n\r\nclass FruitsDataModule(pl.LightningDataModule):\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n        self.transform = transforms.Compose(\r\n            [\r\n                transforms.ToTensor()\r\n            ]\r\n        )\r\n        \r\n    def setup(self, stage=None):\r\n        \r\n        if stage == 'fit' or stage is None:\r\n\r\n            full_train_dataset = datasets.ImageFolder(\r\n                root = config.TRAIN_DATA_PATH,\r\n                transform = self.transform\r\n            )\r\n            \r\n            train_dataset, val_dataset = train_test_split(\r\n                full_train_dataset, \r\n                test_size=0.33,\r\n                random_state = 42\r\n            )\r\n            \r\n        if stage == 'test' or stage is None:\r\n\r\n            test_dataset = datasets.ImageFolder(\r\n                root = config.TEST_DATA_PATH,\r\n                transform = self.transform\r\n            )\r\n            \r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n            train_dataset,\r\n            batch_size = config.BATCH_SIZE,\r\n            shuffle = True\r\n        )\r\n        \r\n    def val_dataloader(self):\r\n        return DataLoader(\r\n            val_dataset,\r\n            batch_size = config.BATCH_SIZE,\r\n        )\r\n    \r\n    def test_dataloader(self):\r\n        return DataLoader(\r\n            test_dataset,\r\n            batch_size = config.BATCH_SIZE,\r\n        )\r\n\r\nclass VGGModel(pl.LightningModule):\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n        \r\n        self.model = models.vgg16_bn(pretrained=True)\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        \r\n    def forward(self, x):\r\n        x = self.model(x)\r\n        return x\r\n    \r\n    def step(self, batch):\r\n        x, y = batch\r\n        logits = self.forward(x)\r\n        loss = self.criterion(logits, y)\r\n        preds = torch.argmax(logits, dim=1)\r\n        return loss, preds, y\r\n        \r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        loss, preds, targets = self.step(batch)\r\n        \r\n        # log train metrics\r\n        acc = roc_auc_score(preds, targets)\r\n        self.log(\"train/loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\r\n        self.log(\"train/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\r\n        \r\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\r\n    \r\n    \r\n   \r\n    def validation_step(self, batch, batch_idx):\r\n        loss, preds, targets = self.step(batch)\r\n\r\n        # log val metrics\r\n        acc = roc_auc_score(preds, targets)\r\n        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\r\n        self.log(\"val/acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\r\n\r\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\r\n    \r\n    \r\n    def test_step(self, batch, batch_idx):\r\n        loss, preds, targets = self.step(batch)\r\n\r\n        # log test metrics\r\n        acc = roc_auc_score(preds, targets)\r\n        self.log(\"test/loss\", loss, on_step=False, on_epoch=True)\r\n        self.log(\"test/acc\", acc, on_step=False, on_epoch=True)\r\n\r\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\r\n    \r\n   \r\n    \r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr = config.LEARNING_RATE)\r\n        return optimizer\r\n\r\nmodel = VGGModel()\r\n\r\ntrainer = pl.Trainer(\r\n    max_epochs=1,\r\n    gpus=[0],\r\n    precision=32,\r\n    progress_bar_refresh_rate=20\r\n)\r\n\r\ntrainer.fit(model, datamodule = FruitsDataModule())\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>full stack traces</summary>\r\n\r\n```\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-2-132ed7ff8256> in <module>()\r\n    146 )\r\n    147 \r\n--> 148 trainer.fit(model, datamodule = FruitsDataModule())\r\n\r\n8 frames\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    458         )\r\n    459 \r\n--> 460         self._run(model)\r\n    461 \r\n    462         assert self.state.stopped\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\r\n    713         self.accelerator.connect(model)\r\n    714         self.accelerator.setup_environment()\r\n--> 715         self.call_setup_hook(model)  # allow user to setup lightning_module in accelerator environment\r\n    716         self.call_configure_sharded_model(model)  # allow user to setup in model sharded environment\r\n    717         self.accelerator.setup(self, model)  # note: this sets up self.lightning_module\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in call_setup_hook(self, model)\r\n   1162 \r\n   1163         if self.datamodule is not None:\r\n-> 1164             self.datamodule.setup(stage=fn)\r\n   1165         self.setup(model, stage=fn)\r\n   1166         model.setup(stage=fn)\r\n\r\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py in wrapped_fn(*args, **kwargs)\r\n    382 \r\n    383             if not has_run:\r\n--> 384                 return fn(*args, **kwargs)\r\n    385 \r\n    386         return wrapped_fn\r\n\r\n<ipython-input-2-132ed7ff8256> in setup(self, stage)\r\n     43             full_train_dataset = datasets.ImageFolder(\r\n     44                 root = config.TRAIN_DATA_PATH,\r\n---> 45                 transform = self.transform\r\n     46             )\r\n     47 \r\n\r\n/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py in __init__(self, root, transform, target_transform, loader, is_valid_file)\r\n    311                                           transform=transform,\r\n    312                                           target_transform=target_transform,\r\n--> 313                                           is_valid_file=is_valid_file)\r\n    314         self.imgs = self.samples\r\n\r\n/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py in __init__(self, root, loader, extensions, transform, target_transform, is_valid_file)\r\n    143         super(DatasetFolder, self).__init__(root, transform=transform,\r\n    144                                             target_transform=target_transform)\r\n--> 145         classes, class_to_idx = self.find_classes(self.root)\r\n    146         samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)\r\n    147 \r\n\r\n/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py in find_classes(self, directory)\r\n    219             (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\r\n    220         \"\"\"\r\n--> 221         return find_classes(directory)\r\n    222 \r\n    223     def __getitem__(self, index: int) -> Tuple[Any, Any]:\r\n\r\n/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py in find_classes(directory)\r\n     38     See :class:`DatasetFolder` for details.\r\n     39     \"\"\"\r\n---> 40     classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\r\n     41     if not classes:\r\n     42         raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '../input/fruits/fruits-360/Training'\r\n```"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-03T10:52:31Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 8268,
    "title": "Load model from previous git commit",
    "created_at": "2021-07-03T12:14:12Z",
    "closed_at": "2021-07-05T23:28:37Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8268",
    "body": "## 🚀 Feature\r\n\r\nBeing able to load a model from a given git commit.\r\n\r\n### Motivation\r\n\r\nThe idea is to been able to load a model even if the codebase of the model have evolved since we trained that model. This would allow for better retrocompatibility in a developement phase and allow to think of the model more freely without having in mind compatibility issues\r\n\r\n### Pitch\r\n\r\nThe idea is that when calling MyLightingModule.load_from_checkpoint, on can give a specific git commit id so that the model is loaded according to the code base at the time when the model was trained.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8268/comments",
    "author": "nicolas-dufour",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-07-05T23:28:37Z",
        "body": "So you are proposing that the `checkpoint` saved includes the commit hash of your model code?\r\n\r\nWe cannot know this inside Lightning, however, you could add this information yourself to the checkpoint for future reproducibility.\r\n\r\nKeep in mind that when you call `MyLightingModule.load_from_checkpoint(ckpt)`, you are doing it for a specific `MyLightingModule` code, so it would require to reload the entire source tree dynamically and reload/restart the python process so the expected code is used.\r\n\r\nSo yeah, not sure if this is even technically possible. Closing for now, but feel free to elaborate on the proposal"
      },
      {
        "user": "nicolas-dufour",
        "created_at": "2021-07-15T14:42:28Z",
        "body": "My idea is not necessarily linked to git, mainly, for me the problem is the code dependence when loading models. Isn't it possible to load models in a code independent way (Some kind of pickling). I'm not at all sure it's doable, just wanted to share a feature that i think would make my workflow much easier."
      }
    ]
  },
  {
    "number": 8261,
    "title": "CLI: instantiate return objects",
    "created_at": "2021-07-02T12:41:33Z",
    "closed_at": "2021-08-14T10:51:02Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "argparse (removed)"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8261",
    "body": "## 🚀 Feature\r\n\r\nwhile working with Tuner I found it quite useful to great multiple copies of the same model or data, so for I would extend and feel it is quite natures of all methods `instantiate_`\r\n\r\n### Motivation\r\n\r\nget more freedom to customize user scripts\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n```py\r\n    def instantiate_trainer(self, inplace: bool: True) -> Optional[Trainer]:\r\n        ...\r\n        if inplace:\r\n            self.trainer = trainer\r\n        else:\r\n            return trainer\r\n```\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8261/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-07-06T07:29:48Z",
        "body": "Hey @Borda,\r\n\r\nMind giving more context where you found this to be useful ?\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-05T10:13:00Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 8228,
    "title": "Option of doing optimizer.zero_grad(set_to_none=True) with accumulate_grad_batches > 1",
    "created_at": "2021-06-30T18:20:46Z",
    "closed_at": "2021-07-02T17:20:39Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8228",
    "body": "## 🚀 Feature\r\nAn option to set gradients to None instead of zero, while also setting `accumulate_grad_batches > 1`. There might be other incompatible `Trainer` flags that I'm not aware of.\r\n\r\n### Motivation\r\nTraining Speed improvements.\r\n\r\nSee #6534 for a previous discussion on how to do this for a simpler training situation.\r\n\r\n### Pitch\r\n\r\nIf I have\r\n```\r\ntrainer = pl.Trainer(accumulate_grad_batches=2, ....)\r\n```\r\nand\r\n```\r\ndef optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\r\n    optimizer.zero_grad(set_to_none=True)\r\n```\r\nI get\r\n```pytorch_lightning.utilities.exceptions.MisconfigurationException: When overriding `LightningModule` optimizer_step or optimizer_zero_grad, `accumulate_grad_batches` in `Trainer` should be 1. It ensures optimizer_step or optimizer_zero_grad are called on every batch.```\r\n\r\nThis should work as written (or something quite close to it).\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8228/comments",
    "author": "rahumble",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-07-02T17:20:39Z",
        "body": "Hi!\r\n\r\nThis constraint has been relaxed so it's possible to override the hook in master. A warning is printed now.\r\n\r\nDone in #7980 "
      }
    ]
  },
  {
    "number": 8186,
    "title": "Enable logging event in loggers.",
    "created_at": "2021-06-29T00:56:58Z",
    "closed_at": "2021-08-06T17:02:01Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8186",
    "body": "It might be very useful if we can log events relevant to anything that happens during the training. The event metadata should be generic enough so that we can log information at various phases and steps during the training run, for ex. dataloader starting, stopping, restarting, checkpoint saved, or loaded etc.. Users can also use this event logging to log anything specific to their own implementation, for ex. data downloaded, tokenized, processed, cleaned, etc. from inside their data module.\r\n\r\n### Motivation\r\nHaving this feature will enable us and users to be able to see more information about the training run, and be able to troubleshoot the issues more easily.\r\n\r\n### Pitch\r\n\r\nWe can add a new method to the LightningLoggerBase to log event. The event can be associated with a scalar numeric or string value, and an option dictionary of key-value pairs for any additional metadata that users want to associate with the event.\r\n\r\n\t\r\n```\r\ndef log_event(self, event: str, value: Optional[float], value_string: Optional[str], additional_fields: Optional[Dict[str, str]], step: Optional[int] = None):\r\n    pass\r\n```\r\n\r\nLoggers can implement this method if they want to listen to the events, and output the event+metadata to any data sink they want.\r\n\r\nWe don't need to make this function abstract. That way not all logger implementations need to add it, when it doesn't make sense.\r\n\r\nWe can then invoke the log_event function from anywhere in the code as long as reference to the trainer.logger is available. If the logger specified by user in the trainer params implements the log_event function, it will get called.\r\n\r\nFor ex.\t\r\n```\r\nif self._trainer_logger is not None:\r\n    self._trainer_logger.log_event(\r\n        \"CheckpointLoadTotalTime\",\r\n        total_time,\r\n        None,\r\n        {\r\n            \"resume_from_checkpoint\": path,\r\n        }\r\n    )\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8186/comments",
    "author": "camruta",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-07-30T00:29:40Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 8104,
    "title": "manual_backward logical bug",
    "created_at": "2021-06-23T15:44:27Z",
    "closed_at": "2021-06-24T08:26:41Z",
    "labels": [
      "bug",
      "help wanted",
      "working as intended"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8104",
    "body": "\r\nActually, this problem has been raised in issue #7698 , but it has been closed. \r\n\r\nThis problem still exists in V1.3.7. My current solution is to comment `track_and_norm_grad` function. I would suggest adding a conditional judgment of grad clipping.\r\n\r\n```python\r\n\r\n    def backward(self, result, optimizer, opt_idx, *args, **kwargs):\r\n        self.trainer.dev_debugger.track_event(\"backward_call\")\r\n\r\n        should_accumulate = self.should_accumulate()\r\n\r\n        # backward can be called manually in the training loop\r\n        if isinstance(result, torch.Tensor):\r\n            self.trainer.accelerator.backward(result, optimizer, opt_idx, should_accumulate, *args, **kwargs)\r\n        else:\r\n            result.closure_loss = self.trainer.accelerator.backward(\r\n                result.closure_loss, optimizer, opt_idx, should_accumulate, *args, **kwargs\r\n            )\r\n\r\n        if not self.should_accumulate() and self.grad_clip:\r\n            # track gradients\r\n            self.track_and_norm_grad(optimizer=optimizer)\r\n```\r\n\r\n> I recently also encounter this problem. I think there is indeed some logic error in `manual_loss`.\r\n> \r\n> ```python\r\n>     ....\r\n>     self.manual_backward(loss)\r\n> ```\r\n> \r\n> * `lightning.py: manual_backward`\r\n> \r\n> ```python\r\n>     def manual_backward(self, loss: Tensor, optimizer: Optional[Optimizer] = None, *args, **kwargs) -> None:\r\n>         ...\r\n>         # backward\r\n>         self._running_manual_backward = True\r\n>         self.trainer.train_loop.backward(loss, optimizer=None, opt_idx=None, *args, **kwargs)\r\n>         self._running_manual_backward = False\r\n> ```\r\n> \r\n> * `trainer.train_loop.backward(loss, optimizer=None,...)`\r\n> \r\n> the argument `optimizer` is `None`, but the last line requires a `optimizer`\r\n> \r\n> ```python\r\n>     def backward(self, result, optimizer, opt_idx, *args, **kwargs):\r\n>         self.trainer.dev_debugger.track_event(\"backward_call\")\r\n> \r\n>         should_accumulate = self.should_accumulate()\r\n> \r\n>         # backward can be called manually in the training loop\r\n>         if isinstance(result, torch.Tensor):\r\n>             self.trainer.accelerator.backward(result, optimizer, opt_idx, should_accumulate, *args, **kwargs)\r\n>         else:\r\n>             result.closure_loss = self.trainer.accelerator.backward(\r\n>                 result.closure_loss, optimizer, opt_idx, should_accumulate, *args, **kwargs\r\n>             )\r\n> \r\n>         if not self.should_accumulate():\r\n>             # track gradients\r\n>             self.track_and_norm_grad(optimizer=optimizer)   # raise error here\r\n> ```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8104/comments",
    "author": "marsggbo",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-06-23T23:09:51Z",
        "body": "Hi!\r\n\r\nSetting `gradient_clip_val` or `accumulate_grad_batches` on your `Trainer` with `automatic_optimization=False` (aka using `manual_backward`) is not allowed. #7788 (the PR that closed the issue you linked) implements a better error message for that case but still stops the program.\r\n\r\nYou are not seeing this in the 1.3.7 release because the changes are in master and will be released with 1.4.0.\r\n\r\nBut in your case, you just need to remove the aforementioned flags in your Trainer definition."
      }
    ]
  },
  {
    "number": 8047,
    "title": "Add max_depth option to ModelSummary",
    "created_at": "2021-06-20T17:05:22Z",
    "closed_at": "2021-07-03T21:13:46Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8047",
    "body": "## 🚀 Feature\r\nIt would be helpful to control the depth of modules displayed in ModelSummary when working with deep nested architectures, beyond the \"full\" and \"top\" options available.\r\n\r\n### Motivation\r\n\r\nWhen working with very deep architectures with nested modules, its hard to get an overview of the model architecture being used, as current implementation will print a very long output with repeated blocks of layers when using ```mode=\"full\"``` or almost no information when ```mode=\"top\"```.\r\n\r\n### Pitch\r\n\r\nI propose adding an optional ```max_depth``` parameter to ModelSummary which is used to filter out summary entries with depth > max_depth. The default value would maintain the current functionality.\r\n\r\n```python\r\nclass ModelSummary(object):\r\n    \"\"\"*docs*\"\"\"\r\n    def __init__(self, model, mode: str = MODE_DEFAULT, max_depth: Optional[int] = None):\r\n        self._model = model\r\n        self._mode = mode\r\n        self._layer_summary = self.summarize()\r\n\r\n        # (proposed max_depth feature):\r\n        if max_depth is not None:\r\n            # remove summary entries with depth > max_depth\r\n            for k in [k for k in self._layer_summary.keys() if k.count(\".\") > max_depth]:\r\n                del self._layer_summary[k]\r\n```\r\nThe parameter would also be added to ```LightningModule.summarize()``` method, exposing the functionality to all ```pl``` modules:\r\n```python\r\n# max_depth usage example\r\nmodel.summarize(mode=\"full\", max_depth=2)\r\n```\r\n\r\n### Alternatives\r\n* Should ```max_depth``` be applied globally to self._layer_summary (as pitched) or only inside the ```__str__``` method (thus preserving the full representation if needed?)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8047/comments",
    "author": "ManuelPalermo",
    "comments": [
      {
        "user": "ManuelPalermo",
        "created_at": "2021-06-20T17:07:26Z",
        "body": "I can also submit a pull request for this once a solution is approved."
      },
      {
        "user": "tchaton",
        "created_at": "2021-06-21T07:37:21Z",
        "body": "Dear @ManuelPalermo,\r\n\r\nSounds interesting. Feel free to open a PR with this new max_depth features.\r\n\r\nBest,\r\nT.C"
      }
    ]
  },
  {
    "number": 8026,
    "title": "Process is exiting  with exit code 139 (interrupted by signal 11: SIGSEGV) if not passing logger param to the Trainer",
    "created_at": "2021-06-18T08:18:32Z",
    "closed_at": "2021-07-30T00:29:43Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "logger",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8026",
    "body": "Process is exiting  with exit code 139 (interrupted by signal 11: SIGSEGV) if not passing logger param to the Trainer\r\n\r\nThis code working fine:\r\n```\r\nmyModule = myLightningModule()\r\ntrainer = pl.Trainer(max_epochs=FEATURE_EXTRACTION_EPOCHS, gpus=GPU_COUNT, accelerator=\"dp\", precision=16, logger=None)\r\ntrainer.validate(myModule, val_dataloaders = myModule.valDl)\r\n```\r\n\r\n\r\nThis one crashing a process:\r\n\r\n    myModule = myLightningModule()\r\n    trainer = pl.Trainer(max_epochs=FEATURE_EXTRACTION_EPOCHS, gpus=GPU_COUNT, accelerator=\"dp\", precision=16)\r\n    trainer.validate(myModule, val_dataloaders = myModule.valDl)\r\n\r\n\r\n\r\n\r\ntorch.__version__\r\n1.8.0\r\npytorch_lightning.__version__\r\n1.3.4",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8026/comments",
    "author": "snarb",
    "comments": [
      {
        "user": "kaushikb11",
        "created_at": "2021-06-18T08:37:23Z",
        "body": "@snarb Could you kindly create a minimal script/notebook to reproduce this issue?"
      },
      {
        "user": "tchaton",
        "created_at": "2021-06-21T07:46:48Z",
        "body": "Dear @snarb,\r\n\r\nIt seems the logger is the source of failure in examples right ? Mind creating a reproducible script with the BoringModel ?\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "snarb",
        "created_at": "2021-06-22T11:10:43Z",
        "body": "Minimal code for reproducing:\r\n\r\n    import torchvision # This torchvision import +  pl.Trainer() causes crash. If we are removing this import - no crash\r\n    import pytorch_lightning as pl\r\n    from torch.nn import functional as F\r\n    from torch import nn\r\n    from pytorch_lightning.core.lightning import LightningModule\r\n    \r\n    \r\n    class LitMNIST(LightningModule):\r\n    \r\n      def __init__(self):\r\n        super().__init__()\r\n    \r\n        # mnist images are (1, 28, 28) (channels, width, height)\r\n        self.layer_1 = nn.Linear(28 * 28, 128)\r\n        self.layer_2 = nn.Linear(128, 256)\r\n        self.layer_3 = nn.Linear(256, 10)\r\n    \r\n      def forward(self, x):\r\n        batch_size, channels, width, height = x.size()\r\n    \r\n        # (b, 1, 28, 28) -> (b, 1*28*28)\r\n        x = x.view(batch_size, -1)\r\n        x = self.layer_1(x)\r\n        x = F.relu(x)\r\n        x = self.layer_2(x)\r\n        x = F.relu(x)\r\n        x = self.layer_3(x)\r\n    \r\n        x = F.log_softmax(x, dim=1)\r\n        return x\r\n    \r\n    \r\n    def main():\r\n        postersModel = LitMNIST()\r\n        trainer = pl.Trainer()\r\n        # trainer = pl.Trainer(logger=None)                  - call with logger=None will not cause crash\r\n        trainer.validate(postersModel)\r\n        print('Done')\r\n    \r\n    if __name__ == \"__main__\":\r\n        main()\r\n\r\n\r\nSo looks like torchvision import together with not setting logger param to None somehow causes the crash. Maybe versions conflict.\r\ntorchvision.__version__\r\n0.9.0\r\n\r\nThe  crash happens at  pytorch_lightning/loggers/tensorboard.py:206 \r\n self.experiment.add_scalar(k, v, step))\r\n\r\n\r\n           "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-22T22:52:32Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 7996,
    "title": "Make more detailed and non confusing output for ModelCheckpoint callback",
    "created_at": "2021-06-16T01:27:53Z",
    "closed_at": "2021-07-23T14:03:11Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7996",
    "body": "## 🚀 Feature\r\nShow result of the best model selection algorithm inside ModelCheckpoint callback\r\n\r\n### Motivation\r\n\r\nIts unclear and sometimes confusing what has been 'checkpointed' and why, especially in combination with mlflow autolog: you have no idea what model has been sent into mlflow\r\n\r\n### Pitch\r\n\r\nTypical claaback creation looks like this: \r\n```python \r\ncheckpoint_callback = ModelCheckpoint(monitor='precision_val')\r\n```\r\nAnd it gonna be very helpful to produce output like this:\r\n```\r\nBest result for precision_val is 0.894 at epoch no. 45, model saved in $path or $logger you're using\r\n```\r\n\r\n### Alternatives\r\n\r\nMy regular train/test cycle utilizes bestmodel selection algo and produces output like i've proposed\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7996/comments",
    "author": "notonlyvandalzzz",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-06-16T07:12:19Z",
        "body": "Dear @notonlyvandalzzz,\r\n\r\nSounds definitely interesting. Would you like to open a PR with this addition ?\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-16T10:47:51Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 7932,
    "title": "Training slows down significantly for small dataset sizes",
    "created_at": "2021-06-10T22:14:52Z",
    "closed_at": "2021-06-24T08:21:23Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7932",
    "body": "## 🐛 Bug\r\n\r\nWhen the dataset size is small (i.e. comparable to the minibatch size), it slows training down significantly.\r\n\r\nNo GPU, batch size 64, dataset size 1024: 185 iterations/second\r\nNo GPU, batch size 64, dataset size 100: 47 iterations/second\r\n\r\n1 GPU, batch size 64, dataset size 1024: 110 iterations/second\r\n1 GPU, batch size 64, dataset size 100: 23 iterations/second\r\n\r\n1 GPU, batch size 800, dataset size 1024: 19 iterations/second\r\n1 GPU, batch size 800, dataset size 10000: 90 iterations/second\r\n1 GPU, batch size 64, dataset size 10000: 235 iterations/second\r\n\r\n\r\n\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n```python\r\nimport os, sys\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nfrom torch.utils.data import Dataset, DistributedSampler, DataLoader\r\n\r\nfrom pl_examples import cli_lightning_logo\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.utilities.seed import seed_everything\r\nfrom pytorch_lightning.callbacks.progress import ProgressBar, ProgressBarBase, tqdm, reset, convert_inf\r\n\r\nclass CustomProgressBar(ProgressBar):\r\n    def init_train_tqdm(self) -> tqdm:\r\n        \"\"\" Override this to customize the tqdm bar for training. \"\"\"\r\n        bar = tqdm(\r\n            desc='Training',\r\n            initial=self.trainer.global_step,\r\n            position=(2 * self.process_position),\r\n            disable=self.is_disabled,\r\n            leave=True,\r\n            dynamic_ncols=True,\r\n            file=sys.stdout,\r\n            smoothing=0,\r\n        )\r\n        return bar\r\n    def on_train_start(self, trainer, pl_module):\r\n        super(ProgressBar, self).on_train_start(trainer, pl_module)\r\n        self.main_progress_bar = self.init_train_tqdm()\r\n        self.prev_train_gs = -1\r\n        reset(self.main_progress_bar, self.trainer.max_steps)\r\n\r\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\r\n        super(ProgressBar, self).on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)\r\n        if self.prev_train_gs != self.trainer.global_step and self._should_update(self.trainer.global_step, self.trainer.max_steps):\r\n            self._update_bar(self.main_progress_bar)\r\n            self.main_progress_bar.set_postfix(trainer.progress_bar_dict)\r\n            self.prev_train_gs = self.trainer.global_step\r\n\r\n    def on_train_epoch_start(self, trainer, pl_module):\r\n        super(ProgressBar, self).on_train_epoch_start(trainer, pl_module)\r\n\r\n    def on_train_end(self, trainer, pl_module):\r\n        super(ProgressBar, self).on_train_end(trainer, pl_module)\r\n\r\nclass RandomDataset(Dataset):\r\n    \"\"\"\r\n    >>> RandomDataset(size=10, length=20)  # doctest: +ELLIPSIS\r\n    <...bug_report_model.RandomDataset object at ...>\r\n    \"\"\"\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    \"\"\"\r\n    >>> BoringModel()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    BoringModel(\r\n      (layer): Linear(...)\r\n    )\r\n    \"\"\"\r\n\r\n    def __init__(self, train_data, test_data, bs):\r\n        \"\"\"\r\n        Testing PL Module\r\n\r\n        Use as follows:\r\n        - subclass\r\n        - modify the behavior for what you want\r\n\r\n        class TestModel(BaseTestModel):\r\n            def training_step(...):\r\n                # do your own thing\r\n\r\n        or:\r\n\r\n        model = BaseTestModel()\r\n        model.training_epoch_end = None\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self.layer1 = torch.nn.Linear(32, 32)\r\n        self.layer2 = torch.nn.Linear(32, 32)\r\n        self.layer3 = torch.nn.Linear(32, 2)\r\n\r\n        self.train_data = train_data\r\n        self.test_data = test_data\r\n        self.bs = bs\r\n\r\n    def forward(self, x):\r\n        return self.layer3(torch.relu(self.layer2(torch.relu(self.layer1(x)))))\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self.forward(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.forward(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(list(self.layer1.parameters()) + list(self.layer2.parameters()) + list(self.layer3.parameters()), lr=0.001)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        train_loader = DataLoader(self.train_data, shuffle=True, num_workers=1, batch_size=self.bs)\r\n        return train_loader\r\n\r\nparser = ArgumentParser()\r\nparser.add_argument(\"--gpus\", type=int, default=0)\r\nparser.add_argument(\"--num_processes\", type=int, default=1)\r\nparser.add_argument(\"--dataset_size\", type=int, default=1024)\r\nparser.add_argument(\"--mb_size\", type=int, default=64)\r\nargs = parser.parse_args()\r\n\r\n\r\ndef test_run():\r\n    # data\r\n    train_data = torch.randn(args.dataset_size, 32)\r\n    test_data = torch.randn(256, 32)\r\n\r\n    # model\r\n    model = BoringModel(train_data, test_data, bs=args.mb_size)\r\n    trainer = Trainer(\r\n        gpus=args.gpus,\r\n        logger=False,\r\n        max_steps=5000,\r\n        limit_val_batches=0,\r\n        num_processes=args.num_processes,\r\n        weights_summary=None,\r\n        reload_dataloaders_every_epoch=False,\r\n        callbacks=[CustomProgressBar()]\r\n    )\r\n\r\n    # fit\r\n    trainer.fit(model)\r\n\r\n    print(f\"{trainer.accelerator_backend=}\")\r\n    print(f\"{trainer.gpus=}\")\r\n    print(f\"{trainer.num_processes=}\")\r\n    print(f\"{trainer.global_step=}\")\r\n\r\nif __name__ == \"__main__\":\r\n    test_run()\r\n```\r\n\r\n### To Reproduce\r\n\r\nRun the following command: `python bug_report.py --gpus 1 --dataset_size 10000 --mb_size 64`\r\nfor varying values of gpus, dataset_size, and mb_size.\r\n\r\n### Expected behavior\r\n\r\nIterations/second is unaffected by dataset size.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.8.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.9\r\n\r\n### Additional context\r\n\r\nMy guess is that this is caused by inter-epoch reloading of the dataset. The code should be restructured to pre-load a fixed number of minibatches ahead, rather than caring about the location of epoch boundaries.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7932/comments",
    "author": "jbuckman",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-06-14T06:51:52Z",
        "body": "Dear @jbuckman,\r\n\r\nCould you use profiler=\"simple\" or profiler=\"advanced\" to explore the source of the problem.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "Borda",
        "created_at": "2021-06-24T08:21:02Z",
        "body": "I would say that the case here is that you would need to assume always some overhead and for a small dataset the initial phase is dominant compare to the full run, you can see a parallel with a car riding 100 or 1000 meters, in both cases, you need to start from zero and as long you go you benefit from no need starting again..."
      }
    ]
  },
  {
    "number": 7878,
    "title": "Add ```dataclass``` support in ```move_data_to_device```",
    "created_at": "2021-06-08T07:05:56Z",
    "closed_at": "2021-07-18T23:41:40Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7878",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\nNamedTuple is often used to wrap batch data, but if a class inherit NamedTuple, it can't be inherited again, so it's difficult to add some new fields. A better choice is ```dataclass```, but it's not supported by ```move_data_to_device```\r\n\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\r\n```python\r\nfrom pytorch_lightning.utilities import move_data_to_device\r\nimport torch\r\nfrom dataclasses import dataclass\r\nfrom typing import NamedTuple, List\r\n\r\nclass Data_1(NamedTuple):\r\n    example_id: List[str]\r\n    x: torch.Tensor\r\n    y: torch.Tensor\r\n\r\n@dataclass\r\nclass Data_2:\r\n    example_id: List[str]\r\n    x: torch.Tensor\r\n    y: torch.Tensor\r\n\r\nbatch_size = 5\r\n\r\nbatch_1 = Data_1(\r\n    example_id=[f\"e-{i}\" for i in range(batch_size)],\r\n    x = torch.rand(batch_size),\r\n    y = torch.rand(batch_size),\r\n)\r\n\r\nbatch_2 = Data_2(\r\n    example_id=[f\"e-{i}\" for i in range(batch_size)],\r\n    x = torch.rand(batch_size),\r\n    y = torch.rand(batch_size),\r\n)\r\n\r\ndevice = torch.device(\"cuda:0\")\r\n\r\nmove_data_to_device(batch=batch_1, device=device)\r\n# Data_1(\r\n# example_id=['e-0', 'e-1', 'e-2', 'e-3', 'e-4'], \r\n# x=tensor([0.3385, 0.6415, 0.8117, 0.6030, 0.2551], device='cuda:0'), \r\n# y=tensor([0.2586, 0.8260, 0.0066, 0.0321, 0.3881], device='cuda:0')\r\n# )\r\n\r\nmove_data_to_device(batch=batch_2, device=device)\r\n# Data_2(\r\n# example_id=['e-0', 'e-1', 'e-2', 'e-3', 'e-4'], \r\n# x=tensor([0.9459, 0.0063, 0.3763, 0.4537, 0.6941]), \r\n# y=tensor([0.8080, 0.8041, 0.2999, 0.5154, 0.3706])\r\n# )\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7878/comments",
    "author": "dalek-who",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-06-08T07:54:45Z",
        "body": "Dear @dalek-who,\r\n\r\nGreat suggestion. Would you mind making a PR to add support for it ?\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "dalek-who",
        "created_at": "2021-06-09T03:11:38Z",
        "body": "@tchaton \r\nI'm note sure whether I should change ```apply_to_collection``` to support it, here's my implementation:\r\n```python\r\ndef apply_to_collection(\r\n    data: Any,\r\n    dtype: Union[type, tuple],\r\n    function: Callable,\r\n    *args,\r\n    wrong_dtype: Optional[Union[type, tuple]] = None,\r\n    **kwargs\r\n) -> Any:\r\n    \"\"\"\r\n    Recursively applies a function to all elements of a certain dtype.\r\n\r\n    Args:\r\n        data: the collection to apply the function to\r\n        dtype: the given function will be applied to all elements of this dtype\r\n        function: the function to apply\r\n        *args: positional arguments (will be forwarded to calls of ``function``)\r\n        wrong_dtype: the given function won't be applied if this type is specified and the given collections is of\r\n            the :attr:`wrong_type` even if it is of type :attr`dtype`\r\n        **kwargs: keyword arguments (will be forwarded to calls of ``function``)\r\n\r\n    Returns:\r\n        the resulting collection\r\n    \"\"\"\r\n    elem_type = type(data)\r\n\r\n    # Breaking condition\r\n    if isinstance(data, dtype) and (wrong_dtype is None or not isinstance(data, wrong_dtype)):\r\n        return function(data, *args, **kwargs)\r\n\r\n    # Recursively apply to collection items\r\n    if isinstance(data, Mapping):\r\n        return elem_type({\r\n            k: apply_to_collection(v, dtype, function, *args, wrong_dtype=wrong_dtype, **kwargs)\r\n            for k, v in data.items()\r\n        })\r\n\r\n    if isinstance(data, tuple) and hasattr(data, '_fields'):  # named tuple\r\n        return elem_type(\r\n            *(apply_to_collection(d, dtype, function, *args, wrong_dtype=wrong_dtype, **kwargs) for d in data)\r\n        )\r\n\r\n    if isinstance(data, Sequence) and not isinstance(data, str):\r\n        return elem_type([\r\n            apply_to_collection(d, dtype, function, *args, wrong_dtype=wrong_dtype, **kwargs) for d in data\r\n        ])\r\n\r\n    #############################\r\n    # my pitch:\r\n    if dataclasses.is_dataclass(data) and not isinstance(data, type):\r\n        return elem_type(**{\r\n            field: apply_to_collection(getattr(data, field), dtype, function, *args, wrong_dtype=wrong_dtype, **kwargs)\r\n            for field in data.__dataclass_fields__\r\n        })\r\n    #############################\r\n\r\n    # data is neither of dtype, nor a collection\r\n    return data\r\n\r\n```"
      },
      {
        "user": "tchaton",
        "created_at": "2021-06-09T05:45:42Z",
        "body": "Yes, looks great ! Please open a PR and add a test."
      },
      {
        "user": "dalek-who",
        "created_at": "2021-06-10T13:09:23Z",
        "body": "OK, I'll have a try!"
      },
      {
        "user": "dalek-who",
        "created_at": "2021-06-11T03:14:20Z",
        "body": "@tchaton I've finished the change and ready to make a pr, but how can I update the changelog to include the pr ```#xxx``` id itself?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-11T08:45:14Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 7853,
    "title": "horovod bug: conflict of `group` argument in `all_gather` function",
    "created_at": "2021-06-07T04:07:17Z",
    "closed_at": "2021-07-21T09:23:28Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7853",
    "body": "## 🐛 Bug\r\n\r\n```python\r\n...\r\nfrom pytorch_lightning.utilities.distributed import group, rank_zero_only, ReduceOp\r\n...\r\n\r\nclass HorovodPlugin(ParallelPlugin):\r\n    ....\r\n\r\n    def all_gather(\r\n        self,\r\n        result: Union[torch.Tensor],\r\n        group: Optional[Any] = group.WORLD,\r\n        sync_grads: bool = False\r\n    ) -> torch.Tensor:\r\n        if group is not None and group != group.WORLD:\r\n            raise ValueError(\r\n                \"Horovod does not support allgather using a subcommunicator at this time. \"\r\n                \"Unset `group`.\"\r\n            )\r\n\r\n        if len(result.shape) == 0:\r\n            # Convert scalars to single dimension tensors\r\n            result = result.reshape(1)\r\n\r\n        # sync and gather all\r\n        self.join()\r\n        gathered = hvd.allgather(result)\r\n        gathered_result = list(gathered.split(1, dim=0))\r\n        return gathered_result\r\n```\r\n\r\nwe can see that the input argument `group` is conflicted with the imported module `group`",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7853/comments",
    "author": "marsggbo",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-06-07T10:14:15Z",
        "body": "Dear @marsggbo,\r\n\r\nThanks for reporting this issue. Would you mind to make a PR ?\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "marsggbo",
        "created_at": "2021-06-07T11:50:28Z",
        "body": "PR #7840 solves this problem. I just simply replace module `group` with `GROUP`"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-07-21T09:23:38Z",
        "body": "@marsggbo thanks for your help!"
      }
    ]
  },
  {
    "number": 7839,
    "title": "horovod bug: TypeError: __init__() missing 2 required positional arguments: 'named_parameters' and 'compression'",
    "created_at": "2021-06-05T04:25:05Z",
    "closed_at": "2021-07-21T08:11:26Z",
    "labels": [
      "bug",
      "help wanted",
      "distributed",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7839",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.metrics.functional import accuracy\r\nfrom pl_bolts.datasets import DummyDataset\r\n\r\ntrain = DummyDataset((1, 28, 28), (1,))\r\ntrain = DataLoader(train, batch_size=32)\r\n\r\nval = DummyDataset((1, 28, 28), (1,))\r\nval = DataLoader(val, batch_size=32)\r\n\r\ntest = DummyDataset((1, 28, 28), (1,))\r\ntest = DataLoader(test, batch_size=32)\r\n\r\nclass LitAutoEncoder(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\r\n        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # --------------------------\r\n        # REPLACE WITH YOUR OWN\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = F.mse_loss(x_hat, x)\r\n        self.log('train_loss', loss)\r\n        return loss\r\n        # --------------------------\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        # --------------------------\r\n        # REPLACE WITH YOUR OWN\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = F.mse_loss(x_hat, x)\r\n        self.log('val_loss', loss)\r\n        # --------------------------\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        # --------------------------\r\n        # REPLACE WITH YOUR OWN\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = F.mse_loss(x_hat, x)\r\n        self.log('test_loss', loss)\r\n        # --------------------------\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n# init model\r\nae = LitAutoEncoder()\r\n\r\n# Initialize a trainer\r\ntrainer = pl.Trainer(gpus=1, max_epochs=2, progress_bar_refresh_rate=20, accelerator='horovod')\r\n\r\n# Train the model ⚡\r\ntrainer.fit(ae, train, val)\r\ntrainer.test(test_dataloaders=test)\r\n```\r\n\r\n\r\n### Error information\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 36, in main\r\n    return train(config)\r\n  File \"/home/marsggbo/code/hyperbox/src/train.py\", line 82, in train\r\n    trainer.test()\r\n  File \"/home/marsggbo/.conda/envs/torch18/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 579, in test\r\n    results = self._run(model)\r\n  File \"/home/marsggbo/.conda/envs/torch18/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 753, in _run\r\n    self.pre_dispatch()\r\n  File \"/home/marsggbo/.conda/envs/torch18/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 778, in pre_dispatch\r\n    self.accelerator.pre_dispatch(self)\r\n  File \"/home/marsggbo/.conda/envs/torch18/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 108, in pre_dispatch\r\n    self.training_type_plugin.pre_dispatch()\r\n  File \"/home/marsggbo/.conda/envs/torch18/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/horovod.py\", line 98, in pre_dispatch\r\n    h_opt = hvd.DistributedOptimizer(\r\n  File \"/home/marsggbo/.conda/envs/torch18/lib/python3.8/site-packages/horovod/torch/optimizer.py\", line 590, in DistributedOptimizer\r\n    return cls(optimizer.param_groups, named_parameters, compression, backward_passes_per_step, op,\r\n  File \"/home/marsggbo/.conda/envs/torch18/lib/python3.8/site-packages/horovod/torch/optimizer.py\", line 41, in __init__\r\n    super(self.__class__, self).__init__(params)\r\nTypeError: __init__() missing 2 required positional arguments: 'named_parameters' and 'compression'\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - Tesla V100-PCIE-32GB\r\n                - Tesla V100-PCIE-32GB\r\n                - Tesla V100-PCIE-32GB\r\n                - Tesla V100-PCIE-32GB\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.20.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.7.1+cu101\r\n        - pytorch-lightning: 1.3.4\r\n        - tqdm:              4.61.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.10\r\n        - version:           #1 SMP Fri Oct 18 17:15:30 UTC 2019\r\n* horovod: 0.22.0\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7839/comments",
    "author": "marsggbo",
    "comments": [
      {
        "user": "marsggbo",
        "created_at": "2021-06-05T08:51:47Z",
        "body": "I found the cause. After finishing `trainer.fit(...)`, the optimizers have already wrapped as `hvd.DistributedOptimizer`. When we start running `trainer.test()`, the framework will wrap the optimizers again, which is why the error is thrown. I've send PR #7840 to solve this problem."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-06-05T10:20:10Z",
        "body": "Great job! Are you able to finish the PR? \r\nI suggest to simply add a trainer.test() call to one of the existing horovod test cases (look for them in `tests/models/test_horovod.py`) to cover this problem. "
      }
    ]
  },
  {
    "number": 7815,
    "title": "Callback.on_validation_batch_end isn't triggered",
    "created_at": "2021-06-03T09:24:23Z",
    "closed_at": "2021-10-22T03:42:00Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7815",
    "body": "## 🐛 Bug\r\n\r\nvalidation_step is implemented\r\n\r\nTrainer.fit() runned\r\n\r\nCallback.on_train_batch_end is triggered\r\nCallback.on_batch_end is triggered\r\nCallback.on_validation_epoch_end is triggered\r\n\r\n\r\nCallback.on_validation_batch_end is not triggered !!!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7815/comments",
    "author": "Rinatum",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-06-03T11:52:23Z",
        "body": "```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer, Callback\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\nclass MyCallback(Callback):\r\n\r\n    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\r\n        print(\"cocofruit\")\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=3,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n        progress_bar_refresh_rate=0,\r\n        callbacks=[MyCallback()],\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, test_dataloaders=test_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n\r\n```\r\n\r\nHave a look here at this minimal example. Hope this helps"
      },
      {
        "user": "Rinatum",
        "created_at": "2021-06-03T11:58:55Z",
        "body": "> ```python\r\n> import os\r\n> \r\n> import torch\r\n> from torch.utils.data import DataLoader, Dataset\r\n> \r\n> from pytorch_lightning import LightningModule, Trainer, Callback\r\n> \r\n> \r\n> class RandomDataset(Dataset):\r\n> \r\n>     def __init__(self, size, length):\r\n>         self.len = length\r\n>         self.data = torch.randn(length, size)\r\n> \r\n>     def __getitem__(self, index):\r\n>         return self.data[index]\r\n> \r\n>     def __len__(self):\r\n>         return self.len\r\n> \r\n> \r\n> class BoringModel(LightningModule):\r\n> \r\n>     def __init__(self):\r\n>         super().__init__()\r\n>         self.layer = torch.nn.Linear(32, 2)\r\n> \r\n>     def forward(self, x):\r\n>         return self.layer(x)\r\n> \r\n>     def training_step(self, batch, batch_idx):\r\n>         loss = self(batch).sum()\r\n>         self.log(\"train_loss\", loss)\r\n>         return {\"loss\": loss}\r\n> \r\n>     def validation_step(self, batch, batch_idx):\r\n>         loss = self(batch).sum()\r\n>         self.log(\"valid_loss\", loss)\r\n> \r\n>     def configure_optimizers(self):\r\n>         return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n> \r\n> \r\n> class MyCallback(Callback):\r\n> \r\n>     def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\r\n>         print(\"cocofruit\")\r\n> \r\n> \r\n> def run():\r\n>     train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n>     val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n>     test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n> \r\n>     model = BoringModel()\r\n>     trainer = Trainer(\r\n>         default_root_dir=os.getcwd(),\r\n>         limit_train_batches=1,\r\n>         limit_val_batches=3,\r\n>         num_sanity_val_steps=0,\r\n>         max_epochs=1,\r\n>         weights_summary=None,\r\n>         progress_bar_refresh_rate=0,\r\n>         callbacks=[MyCallback()],\r\n>     )\r\n>     trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n>     trainer.test(model, test_dataloaders=test_data)\r\n> \r\n> \r\n> if __name__ == '__main__':\r\n>     run()\r\n> ```\r\n> \r\n> Have a look here at this minimal example. Hope this helps\r\n\r\nare u sure that your version of PL is 1.3.3?\r\n\r\n\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-06-03T12:04:19Z",
        "body": "Yes. \r\n"
      },
      {
        "user": "tchaton",
        "created_at": "2021-06-03T16:40:17Z",
        "body": "Dear @Rinatum,\r\n\r\nDid you manage to get it work using @awaelchli example ?\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "Rinatum",
        "created_at": "2021-06-03T16:43:30Z",
        "body": "> Dear @Rinatum,\r\n> \r\n> Did you manage to get it work using @awaelchli example ?\r\n> \r\n> Best,\r\n> T.C\r\n\r\nNo, it doesn't work . I will try to show my case a bit later"
      },
      {
        "user": "tchaton",
        "created_at": "2021-06-07T10:15:13Z",
        "body": "Dear @Rinatum,\r\n\r\nWould you mind sharing a reproducible script with the associated reported behaviour ?\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-10-22T03:41:51Z",
        "body": "Closing due to inactivity and verified again that the script I posted triggers the `on_validation_batch_end` callback hook (PL 1.5-dev). More information would be needed to verify an issue and continue work here. Thank you for sending the bug report."
      }
    ]
  },
  {
    "number": 7789,
    "title": "Add training step callback hook before .backward",
    "created_at": "2021-06-01T10:45:06Z",
    "closed_at": "2021-07-09T06:15:57Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7789",
    "body": "## 🚀 Feature\r\nThe proposal is to add a callback hook for the training step before .backward. Current hooks run either after .backward (on_after_backward) or after optimizer.step (on_train_batch_end, on_batch_end). The proposed callback hook would complement the existing hooks.\r\n\r\n### Motivation\r\n\r\nAdding this hook allows one to create callbacks related to loss calculations as well as batch operations for loss calculations. The current hooks cannot be used for this because the automatic pl optimizer is already called before the existing hooks. \r\n\r\nAdding the proposed hook would increase the flexibility and usability of pytorch lightning without modifying existing functionality. \r\n\r\n### Pitch\r\n\r\nConcretely, it would suffice to add an additional callback hook, named for example \"on_before_backward\",  that takes in _trainer_ and _pl_module_ as arguments.\r\n\r\n### Alternatives\r\n\r\nWithout the proposed callback, the alternative for the end user is to modify their own code to create their own hook in all the pl modules they use.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7789/comments",
    "author": "GR4HAM",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-06-03T17:10:27Z",
        "body": "Dear @GR4HAM,\r\n\r\nDo you want to give it a try ?\r\n\r\nHere are the steps:\r\n\r\n* 1. Add the new hook to the pytorch_lightning/callbacks/base.py::Callback\r\n* 2. Add the new hook inside pytorch_lightning/trainer/callbacks.py\r\n* 3. Find in the TrainLoop where the hook should be call and use self.trainer.call_hook(\"on_before_backward\")\r\n* 4. Add the new callback to the FXValidator (same as `training_step`) \r\n* 5: Resolve failing tests. Lambda Callback + FXValidator.\r\n\r\nBest,\r\nT.C"
      }
    ]
  },
  {
    "number": 7753,
    "title": "More Flexibility in for Logging Data",
    "created_at": "2021-05-29T00:23:22Z",
    "closed_at": "2021-09-04T17:59:35Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7753",
    "body": "## 🚀 Feature\r\n\r\nI've been trying to figure out how to log my data and I can't make it work in a clean way. I think this could be resolved if the logging procedure was a little more flexible.\r\n\r\nI work on problems related to JPEG, when I'm ready to evaluate, I need to take three datasets (or more) and evaluate my model on qualities in `range(10, 101, 10)` (so 10, 20, 30, ... 100). I need to output **separate** evaluation metrics for each dataset for each of these qualities (so 30 metrics total output at the end), however, when I go to view these metrics in whatever logger I'm using, I want the X-axis to be quality (instead of step) and the Y axis to by whatever my metric is. \r\n\r\nThis doesn't seem possible to do with the current logging setup, ive tried all kinds of tricks like aggregating the metrics in `test_epoch_end`, setting `trainer.global_step = quality`, the only thing that kind of worked is calling my loggers `log_metrics` manually where I can provide the step number. \r\n\r\nIs there a way to do this and if not I'd like to request the feature. \r\n\r\nSecondly, with so many datasets, having them get spit out named \"dataset_idx_0, dataset_idx_1 ...\" is not very helpful. I've gotten around this by hardcoding a list of the names that I can look up when I make the logging keys so it comes out like 'dataset/quality/metric' but that's also kind of an ugly solution and I need to keep updating it. It would be nice if the `test_dataloader` function could return a map of `dataset name: dataloader` and then the `dataset_idx` parameter passed to `test_step` could be the string key. \r\n\r\n### Motivation\r\n\r\nSee above\r\n\r\n### Pitch\r\n\r\nI think we'd need to work out exactly how the logger could be made more flexible but off the top of my head I should be able to set a step number manually (e.g. `self.log(..., step=quality)` ) and PL should take care of aggregating metrics reported in the same step. Either that or there could be a separate `test_on_dataset_end` or something that gets called whenever a dataloader is exhausted (this is different from the `on_epoch_ends` which would be called when **all** dataloaders are exhausted). \r\n\r\nThe second part where dataloaders have string names is pretty straightforward.\r\n\r\n### Alternatives\r\n\r\nI tried a lot, the only thing that works is bypassing the nice logger abilities of PL and going to log_metrics manually, which I'm pretty sure is a bad idea.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7753/comments",
    "author": "Queuecumber",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-06-03T18:42:43Z",
        "body": "Dear @Queuecumber,\r\n\r\nThanks for sharing your thoughts, it is definitely interesting.\r\n\r\n1 . For logging on quality, we will have to think how to handle this as it is pretty special use case.\r\n\r\nBut IMO, I think Logging directly using `self.trainer.logger.log_metrics` is definitely the cleanest way for me.\r\nUsing TorchMetric Metric, you can do this:\r\n\r\nPseudo code with Accuracy.\r\n```\r\nfrom torchmetrics import Accuracy\r\n\r\nclass MyModel(LightningModule):\r\n\r\n    def __init__(self, model):\r\n        self.model = model\r\n\r\n    def setup(self):\r\n        dm = self.trainer.datamodule\r\n        self.map_dl_idx_to_quality = dm.map_dl_idx_to_quality\r\n        self.map_dl_idx_to_name = dm.map_dl_idx_to_name\r\n        self.num_dataloaders = dm.num_dataloaders\r\n        self.val_acc = [Accuracy(num_classes=dm.num_classes) for _ in range(self.num_dataloaders)]\r\n\r\n    def validation_step(self, batch, batch_idx, dataloader_idx):\r\n        ...\r\n        self.val_acc[dataloader_idx])(preds, labels)\r\n    \r\n    def validation_epoch_end(self, outputs):\r\n        for dl_idx, quality in self.map_dl_idx_to_quality.items():\r\n            name = self.map_dl_idx_to_quality[dl_idx]\r\n            quality = self.map_dl_idx_to_name[dl_idx]\r\n            metrics = {f\"{name}/{quality}/{val_acc}\": self.val_acc[dl_idx]}\r\n            self.trainer.logger.log_metrics(metrics, quality)\r\n            self.val_acc[dl_idx].reset()\r\n``` \r\n\r\n2. For \"dataset_idx_0, dataset_idx_1\", I agree this isn't the best solution right now. We definitely need to find a better way to handle this.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "Queuecumber",
        "created_at": "2021-06-03T19:20:38Z",
        "body": "for 1) the best way to handle it is with tabular data, since I'm using WandB I can log a separate table for each dataset with columns [quality, metric a, metric b, metric c, ...] then do some fancy plotting to get the X axis to be quality and the Y axis to be metric. The way you showed wont exactly work, while the metrics are correctly separated by quality there isnt a good way to put quality on the X axis of a plot. It is a bit corner-cased although I'm sure there are generalizations that would be useful for other tasks, and having a dataset-level end function would be nice. That said it might make sense to leave this one out of PL in general. What would help is a retooling of the logger API to support {images, videos, tables} natively and forward those calls to the underlying experiment object if it supports them (I think this is in the works?)\r\n\r\nFor part 2 I think it should be fairly straightforward to support a dict return value from `test_dataloader` (and the like) with arbitrary keys mapping to dataloaders and to pass those keys instead of the integer `dataloader_idx` (or append their str or repl automatically to log entries). \r\n\r\nIf this sound alright to you I'm happy to PR this as I say on all my issues, but I need to have some feedback on said hypothetical PR from the team so that it gets merged in a timely manner. Lack of engagement on issues/PRs from the team has been a major pain point for me in working with PL thus far.\r\n\r\n"
      },
      {
        "user": "carmocca",
        "created_at": "2021-06-04T13:12:40Z",
        "body": "> (1)\r\n\r\nI agree the ties to `global_step` when logging can be limiting for several use cases. Eventually we should be more flexible with what axis is what.\r\n\r\n> Secondly, with so many datasets, having them get spit out named ...\r\n\r\nNote that you could just not pass multiple datasets and instead call `trainer.{validate,test}(dataloader)` for each dataset.\r\n\r\n> Lack of engagement on issues/PRs from the team has been a major pain point for me in working with PL thus far.\r\n\r\nSorry for this. Time is limited and there are many open fronts that have starved our attention on other matters. We'll try to do better 😃 "
      },
      {
        "user": "Queuecumber",
        "created_at": "2021-06-04T13:14:27Z",
        "body": "> Note that you could just not pass multiple datasets and instead call trainer.{validate,test}(dataloader) for each dataset.\r\n\r\nThis is worth investigating on my end, although it kind of breaks the nice automated flow you get from using datamodules and having fit do your validation periodically "
      },
      {
        "user": "carmocca",
        "created_at": "2021-06-04T14:48:26Z",
        "body": "Just to clarify, you are interested in doing this in the validation part of fitting, not when running validation separately, correct?"
      },
      {
        "user": "Queuecumber",
        "created_at": "2021-06-04T14:50:02Z",
        "body": "My use case is actually for `test`, but I think it would generally be useful support something other than an integer index for dataloaders"
      },
      {
        "user": "carmocca",
        "created_at": "2021-06-04T14:54:21Z",
        "body": "cc @justusschock @ananthsub for opinions on the dataloader idx part"
      },
      {
        "user": "Queuecumber",
        "created_at": "2021-06-10T19:48:22Z",
        "body": "This is kind of what I was talking about, its been almost a week since you pinged those guys for input and we've got nothing"
      },
      {
        "user": "edenlightning",
        "created_at": "2021-06-14T15:56:57Z",
        "body": "Thanks for pinging us @Queuecumber! We are trying our best to answer all issues as fast as possible. "
      },
      {
        "user": "justusschock",
        "created_at": "2021-06-14T16:17:22Z",
        "body": "@Queuecumber Sorry for the delay!\r\n\r\nRegarding your questions:\r\n\r\n1.) I think we could go the way with the mapping. \"Old-Style\" Sequences could most likely be converted to mappings by just having their index as key. The only \"issue\" would then be the transition from ints (now) to strings (ideally all keys would be strings at some point, even the ones created by sequence-enumeration).\r\n\r\nI just wouldn't like to automatically add the name to the key of the logged entity. You would have the option to do so manually, but doing that by default would also limit some users in another way.\r\n\r\n2.) I definitely agree that we should provide an optional `step` argument here, which defaults to the global_step if not provided. However, that would then also mean that you have to provide that `step` by yourself (based on the updated dataloader names). The only blocker I see for this: How do you aggregate? The global step is only increasing, so for each step, there is only one logged value (and we don't have to keep them in memory once the current epoch is over). When you can provide very custom steps, this means that you can log to the same step multiple times? What should happen in that case? Should we just overwrite it? Accumulation would be difficult since we cannot keep every value in memory that was ever logged."
      },
      {
        "user": "Queuecumber",
        "created_at": "2021-06-14T18:18:20Z",
        "body": "> @Queuecumber Sorry for the delay!\r\n> \r\n> Regarding your questions:\r\n> \r\n> 1.) I think we could go the way with the mapping. \"Old-Style\" Sequences could most likely be converted to mappings by just having their index as key. The only \"issue\" would then be the transition from ints (now) to strings (ideally all keys would be strings at some point, even the ones created by sequence-enumeration).\r\n\r\nI think this works and shouldn't be too hard to implement. I was actually thinking keys could be anything suitable for a dict key and it's up to the user what to do with the keys.\r\n\r\n> I just wouldn't like to automatically add the name to the key of the logged entity. You would have the option to do so manually, but doing that by default would also limit some users in another way.\r\n\r\nI think we could keep the default behavior of appending the key as long as it has a str or repr we can use, I agree it needs to be handled carefully though so it doesn't surprise people\r\n \r\n> 2.) I definitely agree that we should provide an optional `step` argument here, which defaults to the global_step if not provided. However, that would then also mean that you have to provide that `step` by yourself (based on the updated dataloader names). The only blocker I see for this: How do you aggregate? The global step is only increasing, so for each step, there is only one logged value (and we don't have to keep them in memory once the current epoch is over). When you can provide very custom steps, this means that you can log to the same step multiple times? What should happen in that case? Should we just overwrite it? Accumulation would be difficult since we cannot keep every value in memory that was ever logged.\r\n\r\nThis is a tricky one, and probably it needs to be thought out very carefully before we proceed to a PR. For my case, I would want the values averaged for every step, but I can see sum, replace, and custom accumulator all being useful, which all have different overhead associated with them. "
      },
      {
        "user": "justusschock",
        "created_at": "2021-06-15T07:17:02Z",
        "body": "> I think this works and shouldn't be too hard to implement. I was actually thinking keys could be anything suitable for a dict key and it's up to the user what to do with the keys.\r\n\r\nDo you want to give the PR a shot? That'd be great! Please ping me directly on it and I promise I won't take that long to reply this time :)\r\n\r\n> I think we could keep the default behavior of appending the key as long as it has a str or repr we can use, I agree it needs to be handled carefully though so it doesn't surprise people\r\n\r\nSo you always want to convert it to a string and then treat it as we currently do? I'm fine with that (we just need to provide a meaningful error when it cannot be converted)\r\n\r\n> This is a tricky one, and probably it needs to be thought out very carefully before we proceed to a PR. For my case, I would want the values averaged for every step, but I can see sum, replace, and custom accumulator all being useful, which all have different overhead associated with them.\r\n\r\nYes. i think the main question for me is here: When do you want the accumulation to happen? Currently the only option is to accumulate at the end of the epoch (afaik). My main concern is how we avoid blowing up the memory too much. After we figured that out, the different reductions should actually be no-brainers\r\n\r\n"
      },
      {
        "user": "Queuecumber",
        "created_at": "2021-06-21T15:43:35Z",
        "body": "I should be able to get to this PR this week, will ping you when I send it in"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-21T16:25:55Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "Queuecumber",
        "created_at": "2021-07-28T19:36:31Z",
        "body": "Still on my TODO list, sorry for the delay"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-28T15:57:52Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 7726,
    "title": "on_load_checkpoint never called",
    "created_at": "2021-05-26T14:09:10Z",
    "closed_at": "2021-05-26T14:55:00Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 1",
      "priority: 2"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7726",
    "body": "## 🐛 Bug\r\n\r\nI am a new user of PL, so this may be an error of API comprehension on my side.\r\n\r\nI fail to get anything done on the loading of the checkpoint when I resume:\r\n\r\n## Please reproduce using the BoringModel\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nimport torch\r\n\r\n\r\nclass Solver(pl.LightningModule):\r\n    def __init__(self,):\r\n        super(Solver, self).__init__()\r\n        dx = 10\r\n        dy = 1\r\n        n = 100\r\n        self.model = torch.nn.Linear(dx, dy)\r\n        self.dataset = list(zip(torch.rand(n, dx), torch.rand(n, dy)))\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def val_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def configure_optimizers(self,):\r\n        return torch.optim.Adam(self.model.parameters(), lr=1e-3)\r\n\r\n    def _step(self, batch):\r\n        x, y = batch\r\n        y_hat = self.model(x)\r\n        return torch.nn.functional.mse_loss(y_hat, y)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self._step(batch)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        self._step(batch)\r\n\r\n\r\nclass Checkpoint(ModelCheckpoint):\r\n    def on_load_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"loading...\")\r\n        import pdb # <----------------- Never called?\r\n\r\n        pdb.set_trace()\r\n        foo = checkpoint['bar']\r\n\r\n    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"saving...\")\r\n        checkpoint[\"foo\"] = \"bar\"\r\n\r\n\r\nsolver = Solver()\r\ncheckpoint = Checkpoint(dirpath=\"./\", save_last=True)\r\ntrainer = pl.Trainer(callbacks=[checkpoint], max_epochs=3)\r\ntrainer.fit(solver)\r\n\r\ntrainer = pl.Trainer(\r\n    callbacks=[checkpoint], resume_from_checkpoint=\"last.ckpt\", max_epochs=5\r\n)\r\ntrainer.fit(solver)\r\n\r\n```\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - Quadro GP100\r\n                - Quadro GP100\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.8.1+cu102\r\n        - pytorch-lightning: 1.3.2\r\n        - tqdm:              4.50.2\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.5\r\n        - version:           #57-Ubuntu SMP Thu Oct 15 10:57:00 UTC 2020\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7726/comments",
    "author": "kingjr",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-05-26T14:50:24Z",
        "body": "Dear @kingjr,\r\n\r\nThis is working.\r\n\r\nExplanation: `on_load_checkpoint` is called only if `on_save_checkpoint` returned something which isn't None. \r\n\r\n```\r\nfrom typing import Callable\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint, Callback\r\nimport torch\r\n\r\n\r\nclass Solver(pl.LightningModule):\r\n    def __init__(self,):\r\n        super(Solver, self).__init__()\r\n        dx = 10\r\n        dy = 1\r\n        n = 100\r\n        self.model = torch.nn.Linear(dx, dy)\r\n        self.dataset = list(zip(torch.rand(n, dx), torch.rand(n, dy)))\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def val_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def configure_optimizers(self,):\r\n        return torch.optim.Adam(self.model.parameters(), lr=1e-3)\r\n\r\n    def _step(self, batch):\r\n        x, y = batch\r\n        y_hat = self.model(x)\r\n        return torch.nn.functional.mse_loss(y_hat, y)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self._step(batch)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        self._step(batch)\r\n\r\n\r\nclass Checkpoint(ModelCheckpoint):\r\n    def on_load_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"loading...\")\r\n        import pdb; pdb.set_trace()\r\n        foo = checkpoint['bar']\r\n\r\n    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"saving...\")\r\n        checkpoint[\"foo\"] = \"bar\"\r\n        return checkpoint\r\n\r\n\r\nsolver = Solver()\r\ncheckpoint = Checkpoint(dirpath=\"./\", save_last=True)\r\ntrainer = pl.Trainer(callbacks=[checkpoint], max_epochs=3)\r\ntrainer.fit(solver)\r\n\r\ntrainer = pl.Trainer(\r\n    callbacks=[checkpoint], resume_from_checkpoint=\"last.ckpt\", max_epochs=5\r\n)\r\ntrainer.fit(solver)\r\n```"
      },
      {
        "user": "tchaton",
        "created_at": "2021-05-26T14:51:37Z",
        "body": "If you have no further questions, I will close this issue."
      },
      {
        "user": "kingjr",
        "created_at": "2021-05-26T14:55:00Z",
        "body": "excellent, thank you!"
      }
    ]
  },
  {
    "number": 7695,
    "title": "all_gather should raise an error if it didn't gather anything.",
    "created_at": "2021-05-25T00:42:45Z",
    "closed_at": "2021-07-03T01:57:56Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7695",
    "body": "## 🚀 Feature\r\nCurrently, `all_gather` passes silently when non-compatible type is passed into it.\r\n\r\n### Motivation\r\n\r\nThis can lead to bugs, when the wrong variable is passed in and nothing is gathered.\r\n\r\n### Pitch\r\n\r\nKeep track if anything was actually gathered in `all_gather` and raise an error if not.\r\n\r\n### Alternatives\r\n\r\nCheck type during `all_gather` and if non-gatherable stuff is passed in, raise an Error.\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7695/comments",
    "author": "Rizhiy",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2021-05-25T15:17:17Z",
        "body": "@Rizhiy Thanks for the issue! Want to try sending a PR with the fix?"
      },
      {
        "user": "Rizhiy",
        "created_at": "2021-05-26T13:05:13Z",
        "body": "@edenlightning will take a look on weekend."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-26T00:14:26Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 7663,
    "title": "Full precision (fp32) support for ZeRO Stage2 and Stage3",
    "created_at": "2021-05-23T12:35:31Z",
    "closed_at": "2021-07-30T06:29:40Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7663",
    "body": "## 🚀 Feature\r\nDeepspeed v0.3.16 supports fp32 for ZeRO Stage2 and Stage3.\r\nIs it possible to have it in pytorch-lightning?\r\n\r\n\r\n### Motivation\r\nTo train fp32 models with higher speed\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7663/comments",
    "author": "qiyifei1",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2021-05-25T15:19:38Z",
        "body": "@SeanNaren "
      },
      {
        "user": "abhinavrai44",
        "created_at": "2021-06-23T05:38:27Z",
        "body": "I am also facing issues when I try to use mix precision. Please share a solution if it is possible"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-23T05:51:50Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "SeanNaren",
        "created_at": "2021-07-30T09:18:01Z",
        "body": "This is possible now in Lightning 1.4, just install the latest DeepSpeed and do not pass the precision=16 flag!"
      }
    ]
  },
  {
    "number": 7544,
    "title": "Training fails at the end of the epoch when returning None in the training step",
    "created_at": "2021-05-14T09:17:48Z",
    "closed_at": "2021-05-14T13:32:46Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7544",
    "body": "## 🐛 Bug\r\n\r\nSometimes my training loss in a batch is nan. Hence, I return None as loss so that the model will not backpropagate through it as suggested here: #4956. It works fine during the epoch; however, the code fails at the end of the epoch in the function reduce_across_time (line 532).\r\n\r\n```python\r\n           if isinstance(value, list):\r\n                value = torch.tensor(value)\r\n```\r\n\r\nIn case of None, value will be equal to [None] and torch cannot create a proper tensor out of it (*** RuntimeError: Could not infer dtype of NoneType)\r\n\r\nIs it me doing something wrong, or is it a bug in Lightning? Is there any workaround?\r\n\r\nPytorch Version \r\npytorch-lightning-1.3.1\r\ntorch 1.8.1+cu11\r\npython 3.7.9",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7544/comments",
    "author": "TommasoBendinelli",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-05-14T10:32:10Z",
        "body": "Thanks for reporting this. Can you simulate it with our bug report model please? Would help me alot thanks!"
      },
      {
        "user": "TommasoBendinelli",
        "created_at": "2021-05-14T10:41:40Z",
        "body": "Sure, this reproduce the bug\r\n```python\r\nimport os\r\nimport random\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        if batch_idx == 2:\r\n            loss = None\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=5,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=10,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, test_dataloaders=test_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```"
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-05-14T10:53:36Z",
        "body": "I think its because of this\r\n```python\r\nif batch_idx == 2:\r\n    loss = None\r\nself.log(\"train_loss\", loss)\r\n```\r\n`None` values are being logged and stored here which are then accumulated at epoch end which is then throwing this error.\r\nThis should work\r\n```python\r\nif batch_idx == 2:\r\n    loss = None\r\nelse:\r\n    self.log(\"train_loss\", loss)\r\n```\r\nor lightning should handle this internally?"
      },
      {
        "user": "TommasoBendinelli",
        "created_at": "2021-05-14T11:01:04Z",
        "body": "Ahh, I see, it makes sense.  When averaging the loss across multiple batches, how does lightning handles the fact that a batch was skipped due to the loss being None? Does it simply not include it in the average? "
      },
      {
        "user": "TommasoBendinelli",
        "created_at": "2021-05-14T11:05:04Z",
        "body": "Perfect thank you."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-05-14T11:08:32Z",
        "body": "Sorry, had to delete my answer and double check but yes, it averages only over the metrics logged, not over all training_steps. "
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-05-14T11:13:16Z",
        "body": "to be specific it does weighted average by default using batch_size. In your case, it hasn't reached up till that point because this error is thrown while converting the logs list to PyTorch tensor and since it contains NaN values, it is throwing the error. Ideally, if a batch is skipped then it shouldn't contribute while aggregating the results so you can have an else statement there which will just work fine."
      }
    ]
  },
  {
    "number": 7542,
    "title": "Cli with TestTubeLogger does not save the configuration file to the version folder",
    "created_at": "2021-05-14T07:32:47Z",
    "closed_at": "2021-08-26T10:28:15Z",
    "labels": [
      "bug",
      "help wanted",
      "logger",
      "priority: 2",
      "argparse (removed)"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7542",
    "body": "## 🐛 Bug\r\n\r\nCli with TestTubeLogger does not save the configuration file to the version folder.\r\n\r\n## Please reproduce using the BoringModel\r\n```shell\r\npython train.py\r\n```\r\n\r\ntrain.py: \r\n```python\r\n#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\r\nfrom pytorch_lightning.utilities.cli import LightningCLI\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n    def __len__(self):\r\n        return self.len\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    def val_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    def test_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\ndef run():\r\n    early_stopping = EarlyStopping(monitor=\"valid_loss\")\r\n    checkpoint_callback = ModelCheckpoint(dirpath=\"logs\", monitor=\"valid_loss\")\r\n\r\n    cli = LightningCLI(\r\n        BoringModel,\r\n        seed_everything_default=123,\r\n        trainer_defaults={\r\n            \"max_epochs\": 2,\r\n            \"callbacks\": [\r\n                checkpoint_callback,\r\n                early_stopping,\r\n            ],\r\n            \"logger\": {\r\n                \"class_path\": \"pytorch_lightning.loggers.TestTubeLogger\",\r\n                \"init_args\": {\r\n                    \"save_dir\": \"logs\",\r\n                    \"create_git_tag\": True,\r\n                },\r\n            },\r\n        },\r\n    )\r\n    cli.trainer.test(cli.model)\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\n### Expected behavior\r\n\r\nconfig files are saved to `logs/version_d`.\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- TITAN RTX\r\n\t\t- TITAN RTX\r\n\t\t- TITAN RTX\r\n\t\t- TITAN RTX\r\n\t\t- TITAN RTX\r\n\t\t- TITAN RTX\r\n\t\t- TITAN RTX\r\n\t\t- TITAN RTX\r\n\t\t- TITAN RTX\r\n\t\t- TITAN RTX\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.19.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.8.1+cu102\r\n\t- pytorch-lightning: 1.3.1\r\n\t- tqdm:              4.50.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.5\r\n\t- version:           #146-Ubuntu SMP Tue Apr 13 01:11:19 UTC 2021\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7542/comments",
    "author": "tshu-w",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-05-14T08:58:27Z",
        "body": "I will send a fix standardizing `log_dir` for all loggers. Right now TestTubeLogger does not specify the version number in the log dir."
      }
    ]
  },
  {
    "number": 7490,
    "title": "[DeepSpeedPlugin] Trainer.track_grad_norm support",
    "created_at": "2021-05-11T22:03:24Z",
    "closed_at": "2021-07-02T04:45:34Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "3rd party",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7490",
    "body": "## 🚀 Feature\r\nTrainer.track_grad_norm is currently silently ignored when using DeepSpeedPlugin stage 2.\r\n\r\n### Motivation\r\n\r\nTrainer.track_grad_norm works well with ddp_sharded. It would be nice to support in DeepSpeedPlugin too, given DeepSpeedPlugin's empirical better scaling efficiency.\r\n\r\n### Alternatives\r\n\r\nUse ddp_sharded to when track_grad_norm is needed. Add an error message when using track_grad_norm with DeepSpeedPlugin.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7490/comments",
    "author": "leezu",
    "comments": [
      {
        "user": "edgarriba",
        "created_at": "2021-05-21T08:20:00Z",
        "body": "@kaushikb11 @SeanNaren any insights on this ?"
      },
      {
        "user": "tchaton",
        "created_at": "2021-05-21T10:14:06Z",
        "body": "Hey @leezu,\r\n\r\nThat's a good question. As DeepSpeed is sharding the gradients, I think there won't be a simple solution unless they expose a way to do so.\r\nMind opening a feature request on DeepSpeed repo ?\r\n\r\nBest,\r\nT.C "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-25T01:25:06Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 7469,
    "title": "Loss.item() not contiguous with deepspeed plugin",
    "created_at": "2021-05-10T17:34:47Z",
    "closed_at": "2021-06-26T00:14:31Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7469",
    "body": "## 🐛 Bug\r\n\r\nI am trying to fine-tune xlm-roberta-large model using deepspeed plugin\r\n\r\n```\r\n  self.log('val_loss', loss.item(), on_epoch=True, prog_bar=True, logger=True, sync_dist=True, reduce_fx=torch.sum)\r\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 304, in log\r\n    self.device,\r\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/pytorch_lightning/core/step_result.py\", line 150, in log\r\n    value = sync_fn(value, group=sync_dist_group, reduce_op=sync_dist_op)\r\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 308, in reduce\r\n    output = sync_ddp_if_available(output, group, reduce_op)\r\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py\", line 132, in sync_ddp_if_available\r\n    return sync_ddp(result, group=group, reduce_op=reduce_op)\r\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py\", line 165, in sync_ddp\r\n    torch.distributed.all_reduce(result, op=op, group=group, async_op=False)\r\n  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 941, in all_reduce\r\n    work = _default_pg.allreduce([tensor], opts)\r\nRuntimeError: Tensors must be CUDA and dense\r\n```\r\nThis works without any problems with ddp and ddp_sharded plugins.\r\n\r\n - PyTorch Version (e.g., 1.0): 1.7\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10\r\n - GPU models and configuration: Tesla V100\r\n - Any other relevant information:\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7469/comments",
    "author": "dapurv5",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2021-05-18T11:53:19Z",
        "body": "@SeanNaren mind have look?"
      },
      {
        "user": "yifuwang",
        "created_at": "2021-05-19T08:05:01Z",
        "body": "Can't tell the exact issue without a repro, but this can happen when sparse grad is enabled on part of the model, since PyTorch's NCCL backend doesn't yet support collectives on sparse tensors.\r\n\r\nIf this is the case, a workaround worth trying is to use the gloo backend which supports allreduce on sparse tensors (with `PL_TORCH_DISTRIBUTED_BACKEND=gloo`)."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-18T12:49:24Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 7445,
    "title": "Progress bar shows twice the correct number of batches",
    "created_at": "2021-05-08T13:28:48Z",
    "closed_at": "2021-09-05T23:06:33Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author",
      "priority: 2"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7445",
    "body": "## 🐛 Bug\r\n\r\nFor a mini dataset of 100 examples with batch_size=1 (dataset.__len__() returns 100), the progress bar shows 100.\r\n\r\nI can't think of any reason why it's doubling in this manner.  I've tried different value for `val_check_interval`, `batch_size`, cpu and single / multiple GPU, and the problem persists.  Thanks for the help -- this isn't getting in the way of anything (Lightning is really helpful!) -- it's just a feng shui issue for me!\r\n\r\n## Main Training Script\r\n```python\r\n    args = parser.parse_args()\r\n    data_dir = os.path.join(NYP_DIR, 'planning')    \r\n    cui_vocab = get_vocab()\r\n    cui_V = len(cui_vocab)\r\n    model = EntityPlanner(args, cui_V + 5)\r\n    data_module = EntityPlanningDataModule(data_dir, cui_vocab=cui_vocab, debug=args.debug)\r\n    weight_dir = os.path.expanduser('~/weights/planning')\r\n    os.makedirs(weight_dir, exist_ok=True)\r\n    experiment_dir = os.path.join(weight_dir, args.experiment)\r\n    os.makedirs(os.path.join(experiment_dir, 'wandb'), exist_ok=True)  # Only way to make sure it's writable\r\n\r\n    logger = pl_loggers.WandbLogger(\r\n        name=args.experiment,\r\n        save_dir=experiment_dir,\r\n        offline=args.debug or args.offline,\r\n        project='xxx',\r\n        entity='xxx',\r\n    )\r\n\r\n    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() and not args.cpu else None\r\n    if num_gpus is not None and args.debug:\r\n        num_gpus = 1\r\n    print('Num GPUs --> {}'.format(num_gpus))\r\n    pin_memory = num_gpus is not None and not args.debug\r\n    precision = 16 if num_gpus is not None else 32\r\n\r\n    checkpoint_callback = ModelCheckpoint(\r\n        monitor='val_loss',\r\n        save_top_k=1,\r\n        save_last=True,\r\n        mode='min'\r\n    )\r\n\r\n    early_stopping = EarlyStopping('val_correlation', patience=3)\r\n    callbacks = [early_stopping, checkpoint_callback]\r\n\r\n    accumulate_grad_batches = args.batch_size if num_gpus is None or num_gpus == 1 else args.batch_size // num_gpus\r\n\r\n    trainer = pl.Trainer.from_argparse_args(\r\n        args,\r\n        logger=logger,\r\n        precision=precision,\r\n        accelerator=None if num_gpus is None or num_gpus == 1 else 'ddp',\r\n        gpus=num_gpus,\r\n        terminate_on_nan=True,\r\n        default_root_dir=experiment_dir,\r\n        gradient_clip_val=0.1,\r\n        val_check_interval=0.0 if args.debug else 0.2,\r\n        num_sanity_val_steps=0 if args.debug else 2,\r\n        accumulate_grad_batches=accumulate_grad_batches\r\n    )\r\n\r\n    if args.auto_lr_find:\r\n        trainer.tune(model, data_module)\r\n    else:\r\n        print('Starting training...')\r\n        trainer.fit(model, data_module)\r\n        print(f'Best weights saved --> {checkpoint_callback.best_model_path}')`\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7445/comments",
    "author": "griff4692",
    "comments": [
      {
        "user": "edgarriba",
        "created_at": "2021-06-07T15:53:16Z",
        "body": "@griff4692 could you provide more context about what would be your expectation for the progress bar, or could you provide the code to your data module to fully reproduce this issue. I've been doing some local tests and everything seems to work as expected."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-09-05T23:06:33Z",
        "body": "the progress bar shows the combined number of batches within an epoch, both training and validation. if validation runs multiple times per epoch, then this will also be reflected as a multiple of batches in the progress bar. this is expected behavior"
      }
    ]
  },
  {
    "number": 7436,
    "title": "TypeError: __init__() got an unexpected keyword argument 'pin_memory'",
    "created_at": "2021-05-07T17:29:29Z",
    "closed_at": "2021-05-08T16:55:24Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7436",
    "body": "Hi! Thank you for your grrreat project. I encounter error log liker below:\r\n  \r\n- Error log\r\n```\r\nFile \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py\", line 409, in reset_val_dataloader\r\n    self.num_val_batches, self.val_dataloaders = self._reset_eval_dataloader(model, 'val')\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py\", line 358, in _reset_eval_dataloader\r\n    self.auto_add_sampler(dl, shuffle=False, mode=self.state.stage) for dl in dataloaders if dl is not None\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py\", line 358, in <listcomp>\r\n    self.auto_add_sampler(dl, shuffle=False, mode=self.state.stage) for dl in dataloaders if dl is not None\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py\", line 142, in auto_add_sampler\r\n    dataloader = self.replace_sampler(dataloader, sampler, mode=mode)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py\", line 213, in replace_sampler\r\n    dataloader = type(dataloader)(**dl_args)\r\nTypeError: __init__() got an unexpected keyword argument 'pin_memory'\r\n```\r\n  \r\n- My code sample:\r\n```python\r\nclass AudioDataLoader(DataLoader):\r\n    \"\"\" Audio Data Loader \"\"\"\r\n    def __init__(\r\n            self,\r\n            dataset: torch.utils.data.Dataset,\r\n            num_workers: int,\r\n            batch_sampler: torch.utils.data.sampler.Sampler,\r\n    ) -> None:\r\n        super(AudioDataLoader, self).__init__(dataset=dataset, num_workers=num_workers, batch_sampler=batch_sampler)\r\n        self.collate_fn = _collate_fn\r\n\r\n\r\nclass BucketingSampler(Sampler):\r\n    \"\"\" Samples batches assuming they are in order of size to batch similarly sized samples together. \"\"\"\r\n    def __init__(self, data_source, batch_size: int = 32, drop_last: bool = False) -> None:\r\n        super(BucketingSampler, self).__init__(data_source)\r\n        self.batch_size = batch_size\r\n        self.data_source = data_source\r\n        ids = list(range(0, len(data_source)))\r\n        self.bins = [ids[i:i + batch_size] for i in range(0, len(ids), batch_size)]\r\n        self.drop_last = drop_last\r\n\r\n    def __iter__(self):\r\n        for ids in self.bins:\r\n            np.random.shuffle(ids)\r\n            yield ids\r\n\r\n    def __len__(self):\r\n        return len(self.bins)\r\n\r\n    def shuffle(self, epoch):\r\n        np.random.shuffle(self.bins)\r\n\r\nclass LightningCustomDataModule(pl.LightningDataModule):\r\n    def train_dataloader(self) -> DataLoader:\r\n        train_sampler = BucketingSampler(self.dataset['train'], batch_size=self.batch_size)\r\n        return AudioDataLoader(\r\n            dataset=self.dataset['train'],\r\n            num_workers=self.num_workers,\r\n            batch_sampler=train_sampler,\r\n        )\r\n```\r\n  \r\nPlease let me know if you have any doubts.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7436/comments",
    "author": "sooftware",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-05-08T01:43:01Z",
        "body": "Hi\r\nYou need to properly forward all kwargs when you subclass the dataloader:\r\n\r\n```python\r\nclass AudioDataLoader(DataLoader):\r\n    \"\"\" Audio Data Loader \"\"\"\r\n    def __init__(\r\n            self,\r\n            dataset: torch.utils.data.Dataset,\r\n            num_workers: int,\r\n            batch_sampler: torch.utils.data.sampler.Sampler,\r\n            **kwargs,  # <-------- HERE\r\n    ) -> None:\r\n        super(AudioDataLoader, self).__init__(dataset=dataset, num_workers=num_workers, batch_sampler=batch_sampler, **kwargs)  # <-------- HERE\r\n        self.collate_fn = _collate_fn\r\n\r\n```\r\n\r\ncheers"
      },
      {
        "user": "sooftware",
        "created_at": "2021-05-08T06:43:57Z",
        "body": "Thank you! I will try!"
      }
    ]
  },
  {
    "number": 7432,
    "title": "Cuda out of memory while testing",
    "created_at": "2021-05-07T15:23:59Z",
    "closed_at": "2021-06-01T14:51:14Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7432",
    "body": "## 🐛 Bug\r\n\r\nTesting after tuning and fitting raise a CUDA out of memory error. \r\n\r\n### Expected behavior\r\n\r\n1. Initialize model with pytorch-lightning Data Models\r\n2. Tune the model \r\n3. Train the model\r\n4. Test the model => See a CUDA out of memory error\r\n\r\n```python\r\ndm = BertDataModule(name, lang)\r\ndm.setup()\r\n\r\nmodel = BertClassifier(nb_class=dm.num_classes)\r\n\r\ntrainer = pl.Trainer(gpus=1, max_epochs=30, auto_lr_find=True,\r\n                     auto_scale_batch_size=True, stochastic_weight_avg=True,\r\n                     callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.001, mode='min')])        \r\ntune_res = trainer.tune(model, dm)\r\nfit_res = trainer.fit(model, dm) # => 3.1Gb on 4Gb Memory during training\r\ntest_res = trainer.test()  # => OOM error\r\n```\r\n\r\nRest of the code \r\n<details>\r\n<summary>Rest of the code</summary>\r\n<br>\r\n\r\n```python\r\n\r\nclass IntentLoader(Dataset[LoaderType]):\r\n    def __init__(self, dataset_path: str):\r\n        self.samples: list[DatasetEntry] = []\r\n        self.labels_to_int: dict[str, int] = {}\r\n        self.int_to_label: dict[int, str] = {}\r\n\r\n        intents: set[str] = set()\r\n\r\n        with open(dataset_path, 'r') as data_file:\r\n            data: BPDataset = json.load(data_file)\r\n\r\n            if data[\"type\"] == \"intent\" and data[\"lang\"] == \"en\":\r\n                self.samples = get_x_utts_per_class(\r\n                    data[\"samples\"],\r\n                    nb_samples_per_class=20,\r\n                    need_at_least=2\r\n                )\r\n\r\n                for s in self.samples:\r\n                    intents.add(s['label'])\r\n            else:\r\n                raise AssertionError(\r\n                    f\"Lang should be en and type intent for {dataset_path}\"\r\n                )\r\n\r\n        intents.discard('oos')\r\n        self.labels_to_int = {l: i for i, l in enumerate(intents)}\r\n        self.int_to_label = {i: l for l, i in self.labels_to_int.items()}\r\n\r\n        self.labels_to_int['oos'] = len(self.labels_to_int.keys())\r\n        self.int_to_label[len(self.labels_to_int.keys())] = 'oos'\r\n\r\n    def get_num_classes(self) -> int:\r\n        return len(self.labels_to_int.keys())\r\n\r\n    def __getitem__(self, index: int) -> tuple[str, int]:\r\n        sample = self.samples[index]\r\n        return (sample['text'], self.labels_to_int[sample[\"label\"]])\r\n\r\n    def __len__(self):\r\n        return len(self.samples)\r\n\r\n    def __iter__(self):\r\n        self.iter_step = 0\r\n        return self\r\n\r\n    def __next__(self):\r\n        if self.iter_step < len(self.samples):\r\n            self.iter_step += 1\r\n            return self.__getitem__(self.iter_step-1)\r\n        else:\r\n            raise StopIteration\r\n\r\n\r\nclass BertDataModule(pl.LightningDataModule):\r\n\r\n    def __init__(self, dataset_name: DatasetNames, lang: str):\r\n        super().__init__()  # type: ignore\r\n\r\n        self.data_path = os.path.join(\r\n            \"..\", \"..\", \"Datasets\", \"processed\",\r\n            lang, \"intent\", f\"{dataset_name}.json\"\r\n        )\r\n\r\n        self.data_path_test = self.data_path.replace(\r\n            dataset_name, f\"{dataset_name}_test\"\r\n        )\r\n\r\n        self.data_dir = os.path.dirname(self.data_path)\r\n        self.batch_size: int = 16\r\n        self.num_classes: int = 0\r\n\r\n    def setup(self, stage: Optional[str] = None):\r\n        self.train_dataset: Union[Subset[LoaderType],\r\n                                  IntentLoader] = IntentLoader(self.data_path)\r\n\r\n        self.num_classes = self.train_dataset.get_num_classes()\r\n\r\n        if os.path.exists(self.data_path_test):\r\n            self.valid_dataset: Union[Subset[LoaderType],\r\n                                      IntentLoader] = IntentLoader(self.data_path_test)\r\n        else:\r\n            train_size = math.floor(len(self.train_dataset)*0.8)\r\n            valid_size = len(self.train_dataset) - train_size\r\n\r\n            self.train_dataset, self.valid_dataset = random_split(\r\n                self.train_dataset, [train_size, valid_size]\r\n            )\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader[LoaderType](\r\n            self.train_dataset,\r\n            batch_size=self.batch_size,\r\n            num_workers=multiprocessing.cpu_count(),\r\n            drop_last=True\r\n        )  # type: ignore\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader[LoaderType](\r\n            self.valid_dataset,\r\n            batch_size=self.batch_size,\r\n            num_workers=multiprocessing.cpu_count(),\r\n        )  # type: ignore\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader[LoaderType](\r\n            self.valid_dataset,\r\n            batch_size=1,\r\n            num_workers=multiprocessing.cpu_count(),\r\n        )  # type: ignore\r\n\r\n\r\nclass BertClassifier(pl.LightningModule):\r\n    def __init__(self,  nb_class: int):\r\n        super().__init__()\r\n        self.learning_rate = 0.0001\r\n        self.lr = 0.0001\r\n        self.batch_size = 16\r\n        self.best_acc: float = 0\r\n\r\n        # type: ignore\r\n        self.bert: PreTrainedModel = AutoModel.from_pretrained(\r\n            'distilbert-base-uncased')\r\n\r\n        # type: ignore\r\n        self.tok: PreTrainedTokenizer = AutoTokenizer.from_pretrained(\r\n            'distilbert-base-uncased')\r\n\r\n        self.metric = torchmetrics.Accuracy()\r\n\r\n        bert_dim: int = getattr(self.bert.config, 'dim',\r\n                                None) or self.bert.config.hidden_size  # type: ignore\r\n\r\n        self.fc1 = nn.Linear(bert_dim, bert_dim)\r\n        self.norm1 = nn.BatchNorm1d(bert_dim)\r\n\r\n        self.fc2 = nn.Linear(bert_dim, (bert_dim+nb_class)//2)\r\n        self.norm2 = nn.BatchNorm1d((bert_dim+nb_class)//2)\r\n\r\n        self.out = nn.Linear((bert_dim+nb_class)//2, nb_class)\r\n\r\n        self.relu = nn.LeakyReLU()\r\n        self.soft = nn.Softmax(dim=1)\r\n        self.drop = nn.Dropout()\r\n\r\n    def forward(self, text: Union[list[str], tuple[str]], **kwargs) -> torch.Tensor:\r\n        encodings: BatchEncoding = self.tok.batch_encode_plus(\r\n            list(text), return_tensors='pt', padding=True\r\n        )\r\n        input_ids: torch.Tensor = encodings['input_ids']\r\n        attention_mask: torch.Tensor = encodings['attention_mask']\r\n\r\n        bert_output = self.bert(\r\n            input_ids.to(self.device),\r\n            attention_mask.to(self.device).bool()\r\n        )\r\n\r\n        bert_feat = torch.sum(bert_output['last_hidden_state'], dim=1)\r\n\r\n        bert_feat_scaled = bert_feat / torch.linalg.norm(bert_feat)\r\n\r\n        out = self.drop(self.norm1(self.relu(self.fc1(bert_feat_scaled))))\r\n        out = self.drop(self.norm2(self.relu(self.fc2(out))))\r\n        out = self.soft(self.relu(self.out(out)))\r\n\r\n        return out\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.AdamW(\r\n            self.parameters(), lr=(self.lr or self.learning_rate)\r\n        )\r\n\r\n    def training_step(self, train_batch: tuple[list[str], torch.Tensor], batch_idx: int):\r\n        text, label = train_batch\r\n\r\n        logits = self(text)\r\n        loss = F.cross_entropy(logits, label)\r\n\r\n        self.log('train_loss', loss, on_epoch=True)  # type: ignore\r\n        return loss\r\n\r\n    def validation_step(self, val_batch: tuple[list[str], torch.Tensor], batch_idx: int):\r\n        text, label = val_batch\r\n\r\n        logits = self(text)\r\n        loss = F.cross_entropy(logits, label)\r\n\r\n        acc = self.metric(logits, label).item()\r\n        if acc > self.best_acc:\r\n            self.best_acc = acc\r\n\r\n        self.log('val_loss', loss, on_epoch=True)  # type: ignore\r\n        self.log('val_acc', acc, on_epoch=True)  # type: ignore\r\n        return loss\r\n\r\n    def test_step(self, test_batch: tuple[list[str], torch.Tensor], batch_idx: int):\r\n        text, label = test_batch\r\n\r\n        logits = self(text)\r\n        loss = F.cross_entropy(logits, label)\r\n\r\n        acc = self.metric(logits, label).item()\r\n\r\n        self.log('test_loss', loss, on_epoch=True)  # type: ignore\r\n        self.log('test_acc', acc, on_epoch=True)  # type: ignore\r\n        return loss\r\n```\r\n\r\n</details>\r\n### Environment\r\n\r\n* `IDE`: VsCode\r\n \r\n - PyTorch Version (e.g., 1.0): 1.8.1\r\n - OS (e.g., Linux): Linux 5.12.1 Zen (Archlinux)\r\n - How you installed PyTorch (`conda`, `pip`, source): Pip\r\n - Python version: 3.9.4\r\n - CUDA/cuDNN version: Cuda compilation tools, release 11.3, V11.3.58\r\n - GPU models and configuration: GTX 1050 Ti\r\n\r\n### Additional context\r\nMy guess is that the model is loaded twice, one for training, then it is not discarded and loaded another time when trying to test (with fit model still present).\r\n\r\nI'm saying so because I'm printing in the `test_step` and it's not printed so the error come before doing the actual forward for tests. \r\n\r\nMaybe a solution would be to discard the model before loading the best checkpoint\r\n\r\nNOTE : Without `auto_lr` and `auto_batch_size` everything works as expected. \r\nNOTE 2 : With `auto_lr` OR `auto_batch_size` I got OOM error.....  (and OOM as well with both)",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7432/comments",
    "author": "ierezell",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2021-05-11T07:26:45Z",
        "body": "it is happening only with Cuda or can you replicate it also with pure python (pip) packages?"
      },
      {
        "user": "edenlightning",
        "created_at": "2021-06-01T14:51:14Z",
        "body": "Feel free to reopen with reproducible example!"
      }
    ]
  },
  {
    "number": 7429,
    "title": "DDP fails with DDPPlugin and num_nodes>1 (with SLURM)",
    "created_at": "2021-05-07T13:26:15Z",
    "closed_at": "2021-05-08T11:25:51Z",
    "labels": [
      "bug",
      "help wanted",
      "distributed",
      "environment: slurm",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7429",
    "body": "## 🐛 Bug\r\n\r\n`DDPPlugin` crashes my training scripts when training models on multiple nodes (using SLURM).\r\n\r\nWhen I use multiple GPUs on 1 node with the plugin -> all gucci.\r\nWhen I use multiple GPUs on multiple nodes **without** the plugin -> all gucci.\r\nWhen I use multiple GPUs on multiple nodes with the plugin -> crashes 😢 \r\n\r\nSo when I run...\r\n```\r\nsbatch submit.sh debug.py --num_nodes 2 --num_gpus 4 --ddp_plugin\r\n```\r\n\r\nCode fails with...\r\n```\r\n----------------------------------\r\nTotal of 8 GPUs over 2 nodes.\r\nConda environment = DDP_Fail\r\npytorch-lightning 1.3.0\r\nRunning at my_secret_server.fi\r\nPython command:\r\n  python3 ~/debug.py --num_gpus 4 --num_nodes 2 --ddp_plugin\r\n----------------------------------\r\ninitializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8\r\ninitializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8\r\ninitializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8\r\ninitializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8\r\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8\r\ninitializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nMulti-processing is handled by Slurm.\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nSet SLURM handle signals.\r\nSet SLURM handle signals.\r\nSet SLURM handle signals.\r\nSet SLURM handle signals.\r\nSet SLURM handle signals.\r\nSet SLURM handle signals.\r\nSet SLURM handle signals.\r\nSet SLURM handle signals.\r\n\r\nTraceback (most recent call last):\r\n\r\n...\r\n\r\nValueError: Invalid rank 5, rank should be in the interval [0, 3]\r\nValueError: Invalid rank 6, rank should be in the interval [0, 3]\r\nValueError: Invalid rank 7, rank should be in the interval [0, 3]\r\n```\r\n\r\n### To Reproduce\r\n\r\n#### `debug.py`:\r\n\r\n```python\r\nimport os\r\nimport argparse\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.plugins import DDPPlugin\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run(args):\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    \r\n    if args.ddp_plugin:\r\n        plugin = DDPPlugin(find_unused_parameters=False)\r\n    else:\r\n        plugin = None\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n        gpus=args.num_gpus,\r\n        num_nodes=args.num_nodes,\r\n        accelerator='ddp' if args.num_gpus*args.num_nodes > 1 else None,\r\n        plugins=plugin,\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, test_dataloaders=test_data)\r\n\r\n\r\ndef get_args():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--num_nodes', type=int, default=1, metavar='',\r\n                        help='[Default: %(default)s]')\r\n    parser.add_argument('--num_gpus', type=int, default=1, metavar='',\r\n                        help='[Default: %(default)s]')\r\n    parser.add_argument('--ddp_plugin', action='store_true',\r\n                        help='[Default: %(default)s]')\r\n    args = parser.parse_args()\r\n    return args\r\n\r\nif __name__ == '__main__':\r\n    args = get_args()\r\n    run(args)\r\n```\r\n\r\n#### `submit.sh`\r\n\r\n```bash\r\n#!/bin/bash\r\n\r\n#SBATCH --nodes=2\r\n#SBATCH --gres=gpu:4\r\n#SBATCH --ntasks-per-node=4\r\n#SBATCH --cpus-per-task=14\r\n#SBATCH -o ~/logs/ddp_fail_%j.txt\r\n\r\necho \"----------------------------------\"\r\necho \"Total of 8 GPUs over 2 nodes.\"\r\necho Conda environment = $CONDA_DEFAULT_ENV\r\necho $(pip list | grep lightning)\r\n\r\necho \"Running at $(hostname)\"\r\necho \"Python command:\"\r\nCMD=\"    python3 ~/$@\"\r\necho $CMD\r\necho \"----------------------------------\"\r\nsrun $CMD\r\necho \"Done!\"\r\n```\r\n\r\n### Commands\r\n\r\n```bash\r\n# These are okay.\r\nsbatch submit.sh debug.py --num_nodes 1 --num_gpus 4 \r\nsbatch submit.sh debug.py --num_nodes 2 --num_gpus 4 \r\nsbatch submit.sh debug.py --num_nodes 1 --num_gpus 4 --ddp_plugin\r\n\r\n# This fails.\r\nsbatch submit.sh debug.py --num_nodes 2 --num_gpus 4 --ddp_plugin\r\n```\r\n\r\n### Expected behaviour\r\n\r\nCode shouldn't fail..? :D\r\n\r\n### Environment\r\n\r\n - PyTorch Version: 1.8.0\r\n - OS: Ubuntu\r\n - How you installed PyTorch: conda\r\n - Python version: 3.8",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7429/comments",
    "author": "jopo666",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-05-07T14:20:41Z",
        "body": "Hi\r\nBased on your description, this is a known issue and I linked the PR that fixes it. \r\nTemp solution for you: Set num_nodes in the plugin: \r\n\r\n`Trainer(plugins=[DDPPlugin(num_nodes=2, ...)], ...)`\r\n\r\n"
      }
    ]
  },
  {
    "number": 7364,
    "title": "Limiting Train Size per epoch for large dataset",
    "created_at": "2021-05-05T02:41:30Z",
    "closed_at": "2021-05-09T12:45:25Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7364",
    "body": "## 🚀 Feature\r\nLimit train batch work on non debugging setting.\r\n\r\n### Motivation\r\n#5725 suffering from the same problem, I think limiting the train batch size could help. But I am not sure how the behaviour after setting the limit train batch. Would it train on the same portion of data every epoch?\r\n\r\n### Pitch\r\nThis would be a nice work around and if it could guarantee it would step through all datasets, it would be wonderful.\r\n\r\n### Additional context\r\n\r\nI was working on a datasets of size nearly 0.5TB\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7364/comments",
    "author": "edwardpwtsoi",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-05-05T14:10:02Z",
        "body": "Dear @edwardpwtsoi,\r\n\r\n`limit_train_batches` should return different batches after each epoch as the dataset is being shuffled.\r\n\r\nBest\r\nT.C"
      }
    ]
  },
  {
    "number": 7346,
    "title": "[Bug] Logging Separately Per Epoch",
    "created_at": "2021-05-04T13:21:51Z",
    "closed_at": "2021-05-07T22:58:03Z",
    "labels": [
      "bug",
      "help wanted",
      "logging"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7346",
    "body": "## 🐛 Bug\r\n\r\nLogs separate plots for each epoch\r\n\r\n### To Reproduce\r\n\r\n```python\r\nimport os\r\nimport torch\r\nfrom torch.utils.data import Dataset, DataLoader\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom torch.nn import functional as F\r\nfrom torchmetrics.classification import Accuracy\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n    def __getitem__(self, index):\r\n        return self.data[index], 1\r\n    def __len__(self):\r\n        return self.len\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n        self.loss_fn = torch.nn.CrossEntropyLoss()\r\n        self.train_accuracy = Accuracy()\r\n        self.val_accuracy = Accuracy()\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_id):\r\n        return self.step(batch, step_type=\"train\")\r\n\r\n    def validation_step(self, batch, batch_id):\r\n        return self.step(batch, step_type=\"val\")\r\n\r\n    def step(self, batch, step_type: str):\r\n        data, y = batch\r\n        y_pred = self(data)\r\n        loss = self.loss_fn(y_pred, y)\r\n        self.log(f\"{step_type}_loss\", loss, prog_bar=True, sync_dist=False)\r\n        # Compute accuracy\r\n        y_pred = F.softmax(y_pred, dim=1)\r\n        self.__getattr__(f\"{step_type}_accuracy\")(y_pred, y)\r\n        self.log(f\"{step_type}_accuracy\", self.__getattr__(f\"{step_type}_accuracy\"), on_step=True, on_epoch=True)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=5,\r\n        limit_val_batches=5,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=4,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, test_dataloaders=test_data)\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7346/comments",
    "author": "justusschock",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-05-05T10:15:18Z",
        "body": "Can you describe what the bug is here? And what you expect to happen?\r\nThe logging to separate plots was a design choice. Logging a metric in val_step is not meaningful because there is no ordering. "
      },
      {
        "user": "ethanwharris",
        "created_at": "2021-05-05T10:21:23Z",
        "body": "@awaelchli There are a few issues with the current behaviour:\r\n- If you are training for a long time (100 - 200 epochs would be typical), then this creates an unusable number of graphs.\r\n- The original intention (per a comment in the code) was that each epoch would overlay on a single graph, but that is not the case and not easy to do.\r\n- Also, how this interacts with other loggers is unknown, so it's odd that appending `/epoch_*` is happening always when it targets a specific feature of tensorboard.\r\n\r\nThe best solution is that people shouldn't really be logging per val step, but when they do the behaviour should at least be consistent with training."
      }
    ]
  },
  {
    "number": 7333,
    "title": "Add test running predict inside the LightningModule hooks",
    "created_at": "2021-05-03T20:22:44Z",
    "closed_at": "2022-02-01T17:34:29Z",
    "labels": [
      "feature",
      "help wanted",
      "ci"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7333",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\n\r\nThis should work. Add a test to make sure this works reliably in multi-gpu setting. \r\n\r\n```python\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def on_training_epoch_start():\r\n        # calling trainer.predict inside trainer.fit routine.\r\n        # is this okay?\r\n        pred_outputs = self.trainer.predict(dataloaders=self.datamodule.train_loader)\r\n        # do some work with pred_outputs\r\n# start training\r\nmodel = MyModel()\r\ntrainer = Trainer(model, accelerator='ddp')\r\ntrainer.fit()\r\n\r\n```\r\n### Pitch\r\n\r\nAdd a test. Integrate findings in future loop design. \r\n\r\n### Alternatives\r\n\r\nUser can write their own predict loop inside the hooks, but is error prone and untested.\r\n\r\n### Additional context\r\n\r\nAsked on slack by Dilip Thiagarajan \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7333/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2022-02-01T17:34:29Z",
        "body": "We do not support this as it would overwrite the trainer state.\r\n\r\nIt's something we could explore but it can open a can of worms as then people would expect to be able to call `fit` again, or `validate`, etc.\r\n\r\nThe recommended approach at the moment is to chain `fit` and `predict` calls one after the other, although there are other limitations there such as having to recreate the `Trainer` instance."
      },
      {
        "user": "bw4sz",
        "created_at": "2023-12-01T23:44:32Z",
        "body": "This continues to be a really sore area for me. @carmocca have you seen any cases about when to recreate a trainer instances, anything I should look out for? I love trainer.predict because its so much more efficient than looping through batches manually, but it can't be used inside callbacks (as in many issues in this repo), so now i'm creating a new trainer each time the callback is made. "
      }
    ]
  },
  {
    "number": 7290,
    "title": "Remove DataLoader warning for num_workers when using GPU",
    "created_at": "2021-04-30T01:18:36Z",
    "closed_at": "2021-04-30T02:42:20Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7290",
    "body": "## 🐛 Bug\r\n\r\nWhen `num_workers` is specified as zero on a multicore machine in a DataLoader, you get the following warning:\r\n\r\n    UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. \r\n    Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\r\n\r\nThis is fine when the model is being fit using CPUs, but is not sound advice when using GPUs and data are being loaded to the GPU with `.to(device)`. Can this message be turned off when cuda is specified as the device?\r\n\r\n## Environment\r\n\r\n* OS: Ubuntu\r\n* Python version: 3.7.10\r\n* PyTorch version: 1.8\r\n* Lightning version: 1.0.8\r\n* CUDA/cuDNN version: 11\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7290/comments",
    "author": "fonnesbeck",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-04-30T02:10:34Z",
        "body": "Hi @fonnesbeck \r\nWhere did you get that from? Can you name some references? I don't understand how one would reason against using multiple workers per GPU. \r\n\r\nWithout a doubt, giving a good rule of thumb is difficult as there are many factors that play into the efficiency of loading data with multiprocessing. "
      },
      {
        "user": "fonnesbeck",
        "created_at": "2021-04-30T02:42:20Z",
        "body": "OK, I think I have misunderstood what I have read in various places (the pytorch forum appears to be down right now, so I can't reference some of them). I'm getting RunTime errors with this on AWS, but this appears to be related to the multiprocessing settings. \r\n\r\nI will close this."
      }
    ]
  },
  {
    "number": 7269,
    "title": "Support overfit_batches for datamodule that returns a dict",
    "created_at": "2021-04-29T08:30:37Z",
    "closed_at": "2021-06-16T01:38:23Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7269",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nCurrently, when datamodule returns dict in training, with --overfit_batches flags, the return is only a string.\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7269/comments",
    "author": "zhengzangw",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2021-05-09T12:59:15Z",
        "body": "@zhengzangw can you exaplin your motivation here? what are you trying to accomplish?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-09T00:46:43Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 7260,
    "title": "automatic metric selection filename ModelCheckpoint",
    "created_at": "2021-04-28T21:41:12Z",
    "closed_at": "2021-05-11T10:08:02Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7260",
    "body": "## 🚀 Feature\r\n\r\nwhen configuring the `filename` for the `ModelCheckpoint`, it might be nice to have an option to automatically paste the metric used for monitoring.\r\n\r\n### Motivation\r\n\r\nThis would simplify setting up the ModelCheckpoint saving by dynamically creating the filename template.\r\n\r\n### Pitch\r\n\r\nReserve \"{\\_\\_monitor\\_\\_}\" to be automatically set to the variable used for monitoring by `monitor=`. For example:\r\n\r\n```\r\n    cp = ModelCheckpoint(\r\n        dirpath=dirpath,\r\n        filename='{epoch}-{step}-{__monitor__:.2f}',\r\n        save_top_k=3,\r\n        mode='max',\r\n        monitor='val_accuracy',\r\n        )\r\n```\r\n\r\ncould save the checkpoint to `epoch=1-step=2-val_accuracy=0.99.ckpt`.\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7260/comments",
    "author": "cemde",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-04-29T08:29:01Z",
        "body": "Could we just treat the monitor internally in the callback the same as the callback metrics? Then we wouldn't need a special syntax and can just do: `filename='{epoch}-{step}-{monitor:.2f}',` for the same result?"
      },
      {
        "user": "tchaton",
        "created_at": "2021-05-05T16:26:01Z",
        "body": "Yes, I think it would be pretty neat."
      },
      {
        "user": "tchaton",
        "created_at": "2021-05-05T16:26:22Z",
        "body": "@cemde, \r\n\r\nwould you like to give it a try and make a PR ?"
      },
      {
        "user": "carmocca",
        "created_at": "2021-05-10T12:10:54Z",
        "body": "I don't see what's the benefit of adding support for this considering how easy it is for the users to do it themselves with plain python f-strings\r\n\r\n```python\r\nmonitor='val_accuracy'\r\nmc = ModelCheckpoint(\r\n    dirpath=dirpath,\r\n    filename='{epoch}-{step}-' + f'{{{monitor}:.2f}}',\r\n    save_top_k=3,\r\n    mode='max',\r\n    monitor=monitor,\r\n)\r\n```\r\n\r\nwhere the filename will resolve to `'{epoch}-{step}-{val_accuracy:.2f}'`"
      }
    ]
  },
  {
    "number": 7257,
    "title": "Lightning + DDP + Metrics: RuntimeError: Tensors must be CUDA and dense",
    "created_at": "2021-04-28T15:46:29Z",
    "closed_at": "2021-04-30T20:19:53Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7257",
    "body": "## 🐛 Bug\r\n\r\nHi folks,\r\n\r\nI have a problem when using lightning, DDP and torchmetrics. When training a small denoising network on MNIST with 2 gpus using DDP and torchmetrics to compute training and validation PSNR I get the following error:\r\n\r\n`RuntimeError: Tensors must be CUDA and dense`\r\n\r\n## Code for reproduction\r\n\r\n```\r\nimport argparse\r\nfrom typing import Optional\r\nimport torch\r\nfrom torch import nn\r\nfrom torchvision import transforms, datasets\r\nfrom torch.utils.data import DataLoader, random_split\r\nimport pytorch_lightning as pl\r\nfrom torchmetrics import PSNR\r\n\r\n\r\ndef add_gaussian_noise(cleanTensor, sigma=.1):\r\n\t# adds gausian noise of standard deviation sigma\r\n\tnoiseTensor = torch.normal(mean=torch.zeros_like(cleanTensor), std=sigma)\r\n\tnoisyTensor = cleanTensor + noiseTensor\r\n\treturn noiseTensor, noisyTensor\r\n\r\nclass LitConvAE(pl.LightningModule):\r\n\tdef __init__(self, hparams):\r\n\t\tsuper().__init__()\r\n\t\t# network architecture\r\n\t\tself.encoder = nn.Sequential(\r\n\t\t\tnn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=2, padding=1),\r\n\t\t\tnn.ReLU(True),\r\n\t\t\tnn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1))\r\n\t\tself.decoder = nn.Sequential(\r\n\t\t\tnn.ConvTranspose2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1),\r\n\t\t\tnn.ConvTranspose2d(in_channels=8, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1),\r\n\t\t\tnn.ReLU(True))\r\n\t\t# Model-specific parameters\r\n\t\tself.learning_rate = hparams.learning_rate\r\n\t\tself.noise_sigma = hparams.noise_sigma\r\n\t\t# save all hyperparameters to a .yaml file\r\n\t\tself.save_hyperparameters()\r\n\t\t# metrics from torchmetrics\r\n\t\tself.train_psnr = PSNR(data_range=1, dim=(-2, -1))\r\n\t\tself.val_psnr = PSNR(data_range=1, dim=(-2, -1))\r\n\r\n\tdef forward(self, x):\r\n\t\t# typically defines inference behavior\r\n\t\tx = self.encoder(x)\r\n\t\tx = self.decoder(x)\r\n\t\treturn x\r\n\r\n\tdef training_step(self, batch, batch_idx):\r\n\t\t# training behavior can be different from that of inference\r\n\t\tclean_batch, _ = batch  # do not care about image class for denoising\r\n\t\tnoise_batch, noisy_batch = add_gaussian_noise(clean_batch, self.noise_sigma)\r\n\t\tdenoised_batch = self.decoder(self.encoder(noisy_batch))\r\n\t\tloss = nn.functional.mse_loss(denoised_batch, clean_batch, reduction='sum')  # squared l2 norm\r\n\t\tself.log('train_loss', loss)  # log at each step\r\n\t\tself.train_psnr(denoised_batch, clean_batch)\r\n\t\tself.log('train_psnr', self.train_psnr, on_step=False, on_epoch=True)  # log at each end of epoch\r\n\t\treturn loss\r\n\r\n\tdef validation_step(self, batch, batch_idx):\r\n\t\t# training behavior can be different from that of inference\r\n\t\tclean_batch, _ = batch  # do not care about image class for denoising\r\n\t\tnoise_batch, noisy_batch = add_gaussian_noise(clean_batch, self.noise_sigma)\r\n\t\tdenoised_batch = self.decoder(self.encoder(noisy_batch))\r\n\t\tself.val_psnr(denoised_batch, clean_batch)\r\n\t\tself.log('validation_psnr', self.val_psnr, on_step=False, on_epoch=True)\r\n\r\n\tdef configure_optimizers(self):\r\n\t\toptimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\r\n\t\treturn optimizer\r\n\r\n\t@staticmethod\r\n\tdef add_model_specific_args(parent_parser):\r\n\t\t# Model-specific arguments\r\n\t\tparser = parent_parser.add_argument_group(\"LitConvAE\")\r\n\t\tparser.add_argument('--noise_sigma', type=float, default=.2, help='noise standard deviation (between 0. and 1.)')\r\n\t\tparser.add_argument('--learning_rate', type=float, default=1e-3, help='learning rate')\r\n\t\treturn parent_parser\r\n\r\n\r\nclass MNISTDataModule(pl.LightningDataModule):\r\n\tdef __init__(self, batch_size=32, dataset_dir='./', data_transform=transforms.ToTensor(), num_workers=4):\r\n\t\tsuper().__init__()\r\n\t\tself.batch_size = batch_size\r\n\t\tself.dataset_dir = dataset_dir\r\n\t\tself.data_transform = data_transform\r\n\t\tself.num_workers = num_workers\r\n\r\n\tdef prepare_data(self):\r\n\t\t# Use this method to do things that might write to disk\r\n\t\t# or that need to be done only from a single process in distributed settings.\r\n\t\tdatasets.MNIST(root=self.dataset_dir, train=True, download=False)\r\n\t\tdatasets.MNIST(root=self.dataset_dir, train=False, download=False)\r\n\r\n\tdef setup(self, stage: Optional[str] = None):\r\n\t\t# data operations you might want to perform on every GPU\r\n\t\tif stage == 'fit' or stage is None:\r\n\t\t\tdataset_full = datasets.MNIST(self.dataset_dir, train=True, transform=self.data_transform)\r\n\t\t\ttrain_split = 11 * len(dataset_full) // 12\r\n\t\t\tprint(f\"\\ntrain / val split: {[train_split, len(dataset_full) - train_split]} \\n\")\r\n\t\t\tself.dataset_train, self.dataset_val = random_split(dataset_full, [train_split, len(dataset_full) - train_split])\r\n\r\n\t\t# Assign test dataset for use in dataloader(s)\r\n\t\tif stage == 'test' or stage is None:\r\n\t\t\tself.dataset_test = datasets.MNIST(self.data_dir, train=False, transform=self.data_transform)\r\n\r\n\tdef train_dataloader(self):\r\n\t\treturn DataLoader(self.dataset_train, batch_size=self.batch_size, num_workers=self.num_workers)\r\n\r\n\tdef val_dataloader(self):\r\n\t\treturn DataLoader(self.dataset_val, batch_size=self.batch_size, num_workers=self.num_workers)\r\n\r\n\tdef test_dataloader(self):\r\n\t\treturn DataLoader(self.dataset_test, batch_size=self.batch_size, num_workers=self.num_workers)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\t# Parse arguments\r\n\tparser = argparse.ArgumentParser(description=\"Denoise MNIST with a convolutional Autoencoder\")\r\n\t# DataModule-specific arguments\r\n\tparser.add_argument('--batch_size', type=int, default=32, help='number of examples per batch')\r\n\tparser.add_argument('--num_workers', type=int, default=4, help='number of separate processes for the DataLoader (default: 4)')\r\n\t# Trainer arguments\r\n\tparser.add_argument('--gpus', type=int, default=2, help='how many gpus to select')\r\n\tparser.add_argument('--accelerator', type=str, default='ddp', help=\"which multi-GPU backend you want to use (default: 'ddp')\")\r\n\tparser.add_argument('--max_epochs', type=int, default=10, help='number of epochs you want the model to train for')\r\n\t# Program-specific arguments\r\n\tparser.add_argument('--data_dir', type=str, default=r'path/to_mnist_dir', help='path to the parent directory of MNIST torchvision dataset')\r\n\r\n\t# add model specific args\r\n\tparser = LitConvAE.add_model_specific_args(parser)\r\n\r\n\thyperparams = parser.parse_args()\r\n\r\n\t# initialize the neural network\r\n\tmodel = LitConvAE(hyperparams)\r\n\r\n\tdataModule = MNISTDataModule(batch_size=hyperparams.batch_size, dataset_dir=hyperparams.data_dir, data_transform=transforms.ToTensor())\r\n\r\n\ttrainer = pl.Trainer.from_argparse_args(hyperparams)\r\n\r\n\t# the training and validation loops happen here\r\n\ttrainer.fit(model, dataModule)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTraining should perform correctly.\r\n\r\nIt looks like something funny is happening with using the PSNR torchmetric + DDP **during the validation loop**: if I comment out the `self.val_psnr = PSNR(data_range=1, dim=(-2, -1))` line and the `def validation_step(self, batch, batch_idx):` block, training works just fine.\r\nAlso, training and validation work fine when using a single GPU and no DDP.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.8.1\r\n - PyTorch Lightning Version (e.g., 1.0): 1.2.9\r\n - torchmetrics Version: 0.2.0\r\n - OS: Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): installed everything using conda\r\n - Python version: 3.7.10\r\n - CUDA/cuDNN version: 11\r\n - GPU models and configuration: 2 Titan Xp (looks like the problem is the same with 8 V100 GPUs)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7257/comments",
    "author": "amonod",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2021-04-29T10:48:03Z",
        "body": "Hi @amonod, could you please provide the full traceback :]"
      },
      {
        "user": "amonod",
        "created_at": "2021-04-29T12:58:03Z",
        "body": "Hi @SkafteNicki, sure thing, there you go:\r\n\r\n```\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\n\r\ntrain / val split: [55000, 5000] \r\n\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\n\r\ntrain / val split: [55000, 5000] \r\n\r\nLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\r\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\r\nTraceback (most recent call last):\r\n  File \"lightning_ddp_metric_issue.py\", line 139, in <module>\r\n    trainer.fit(model, dataModule)\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 496, in fit\r\n    self.pre_dispatch()\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 525, in pre_dispatch\r\n    self.accelerator.pre_dispatch()\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 83, in pre_dispatch\r\n    self.training_type_plugin.pre_dispatch()\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 281, in pre_dispatch\r\n    self.configure_ddp()\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 226, in configure_ddp\r\n    **self._ddp_kwargs,\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 446, in __init__\r\n    self._sync_params_and_buffers(authoritative_rank=0)\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 460, in _sync_params_and_buffers\r\n    authoritative_rank)\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 1156, in _distributed_broadcast_coalesced\r\n    self.process_group, tensors, buffer_size, authoritative_rank\r\nRuntimeError: Tensors must be CUDA and dense\r\nTraceback (most recent call last):\r\n  File \"/home/amonod/Documents/GitHub/pytorch-tutorial/lightning_ddp_metric_issue.py\", line 139, in <module>\r\n    trainer.fit(model, dataModule)\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 496, in fit\r\n    self.pre_dispatch()\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 525, in pre_dispatch\r\n    self.accelerator.pre_dispatch()\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 83, in pre_dispatch\r\n    self.training_type_plugin.pre_dispatch()\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 281, in pre_dispatch\r\n    self.configure_ddp()\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 226, in configure_ddp\r\n    **self._ddp_kwargs,\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 446, in __init__\r\n    self._sync_params_and_buffers(authoritative_rank=0)\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 460, in _sync_params_and_buffers\r\n    authoritative_rank)\r\n  File \"/home/amonod/miniconda/envs/pytorch181/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 1156, in _distributed_broadcast_coalesced\r\n    self.process_group, tensors, buffer_size, authoritative_rank\r\nRuntimeError: Tensors must be CUDA and dense\r\n```"
      }
    ]
  },
  {
    "number": 7221,
    "title": "Unable to use prepare_data on new pytorchs DataLoader for nlp tasks ",
    "created_at": "2021-04-26T21:40:23Z",
    "closed_at": "2021-06-09T00:46:54Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7221",
    "body": "hi there, pytorch has recently revamped their torchtext modules for dataloaders. and the current method is quite tedious. \r\ndue to this i am unable to use prepare_data method for distributed Training. as other hyperparameters are needed to be imitalized from the prepare data. \r\nMy code works without the prepare_data in pytorch lightning on a single GPU. but it is not possible for me to implement the prepare_data method for me. \r\n\r\nhere is the following gist of my code \r\n\r\n\r\n```\r\nnum_epochs = 10000\r\nlearning_rate = 3e-4\r\nbatch_size = 4\r\nembedding_size = 512 \r\nnum_heads = 8 \r\nnum_encoder_layers = 6 \r\nnum_deocder_layers = 6 \r\ndropout = 0.3 \r\nmax_len = 128 \r\nforward_expansion = 4 \r\n\r\ngom_tokenizer = get_tokenizer('spacy', language='xx_sent_ud_sm')\r\nhin_tokenizer = get_tokenizer('spacy', language='xx_sent_ud_sm')\r\n\r\n\r\ndef build_vocab(filepath, tokenizer1, tokenizer2):\r\n    counter1 = Counter()\r\n    counter2 = Counter()\r\n    with open(filepath) as csv_file:\r\n        csv_reader = csv.reader(csv_file, delimiter=',')\r\n        reader = unicode_csv_reader(csv_file)\r\n\r\n        for string_ in reader:\r\n            counter1.update(tokenizer1(string_[0]))\r\n            counter2.update(tokenizer2(string_[1]))\r\n    return Vocab(counter1, specials=['<unk>', '<pad>', '<bos>', '<eos>']), Vocab(counter2, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\r\n\r\n\r\ngom_vocab, hin_vocab = build_vocab('train.csv', gom_tokenizer, hin_tokenizer)\r\n\r\n\r\ndef data_process(filepath):\r\n    csv_file = open(filepath, encoding='utf8')\r\n    raw_data_iter = iter(unicode_csv_reader(csv_file))\r\n    data = []\r\n    for (raw_gom, raw_hin) in raw_data_iter:\r\n        gom_tensor_ = torch.tensor([gom_vocab[token] for token in gom_tokenizer(raw_gom)],\r\n                                   dtype=torch.long)\r\n        hin_tensor_ = torch.tensor([hin_vocab[token] for token in hin_tokenizer(raw_hin)],\r\n                                   dtype=torch.long)\r\n        data.append((gom_tensor_, hin_tensor_))\r\n    return data\r\n\r\ngom_vocab, hin_vocab = build_vocab('train.csv', gom_tokenizer, hin_tokenizer)\r\ntrain_data = data_process('train.csv')\r\ntest_data = data_process('test.csv')\r\n\r\nsrc_vocab_size = len(gom_vocab)\r\ntrg_vocab_size = len(hin_vocab)\r\nsrc_pad_idx = gom_vocab.stoi['<pad>']\r\ntrg_pad_idx = hin_vocab.stoi['<pad>']\r\npad_idx = gom_vocab['<pad>']\r\nbos_idx = gom_vocab['<bos>']\r\neos_idx = gom_vocab['<eos>']\r\n\r\ndef generate_batch(data_batch):\r\n    gom_batch, hin_batch = [], []\r\n    for (gom_item, hin_item) in data_batch:\r\n        gom_batch.append(\r\n            torch.cat([torch.tensor([bos_idx]), gom_item, torch.tensor([eos_idx])], dim=0))\r\n        hin_batch.append(\r\n            torch.cat([torch.tensor([bos_idx]), hin_item, torch.tensor([eos_idx])], dim=0))\r\n    gom_batch = pad_sequence(gom_batch, padding_value=pad_idx)\r\n    hin_batch = pad_sequence(hin_batch, padding_value=pad_idx)\r\n    return gom_batch, hin_batch    \r\n\r\n\r\n\r\nclass Transformer(pl.LightningModule):\r\n    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=512, num_layers=6, forward_expansion=4, heads=8, dropout=0.3, max_length=128):\r\n        super(Transformer, self).__init__()\r\n        self.encoder = Encoder(src_vocab_size, embed_size, num_layers,\r\n                               heads, forward_expansion, dropout, max_length)\r\n\r\n        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers,\r\n                               heads, forward_expansion, dropout, max_length)\r\n\r\n        self.src_pad_idx = src_pad_idx\r\n        self.trg_pad_idx = trg_pad_idx\r\n        \r\n        self.loss_fn = nn.CrossEntropyLoss(ignore_index=self.src_pad_idx)\r\n        #self.acc_metrics = torchmetrics.Accuracy()\r\n        \r\n        \r\n    def make_src_mask(self, src):\r\n        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\r\n        # (N, 1, 1, src_len)\r\n        return src_mask.to(self.device)\r\n\r\n    def make_trg_mask(self, trg):\r\n        N, trg_len = trg.shape\r\n        trg_mask = torch.tril(torch.ones((trg_len, trg_len))\r\n                              ).expand(N, 1, trg_len, trg_len)\r\n        return trg_mask.to(self.device)\r\n\r\n    def forward(self, src, trg):\r\n        src = src.transpose(1, 0)  # batch_first\r\n        trg = trg.transpose(1, 0)  # batch_first\r\n\r\n        src_mask = self.make_src_mask(src)\r\n        trg_mask = self.make_trg_mask(trg)\r\n        enc_src = self.encoder(src, src_mask)\r\n        out = self.decoder(trg, enc_src, src_mask, trg_mask)\r\n        return out\r\n    def train_dataloader(self):\r\n        return DataLoader(train_data, shuffle=True, batch_size=batch_size, collate_fn=generate_batch) # collate_fn for similar sort batches \r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(test_data, shuffle=True, batch_size=batch_size, collate_fn=generate_batch) # collate_fn for simialr sort batches \r\n\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\r\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n        optimizer, factor=0.1, patience=10, verbose=True)\r\n        return  {'optimizer': optimizer, 'lr_scheduler':scheduler, 'monitor':'val_loss'}\r\n        \r\n    \r\n    def training_step(self, train_batch, batch_idx):\r\n        src, trg = train_batch\r\n        output = self(src, trg[:-1, :])\r\n        output = output.transpose(1, 0)\r\n        output = output.reshape(-1, output.shape[2])\r\n        trg = trg[1:].reshape(-1)\r\n        train_loss = self.loss_fn(output, trg)\r\n        self.log('train_loss', train_loss, prog_bar=True)\r\n        return train_loss\r\n    \r\n    def validation_step(self, val_batch, batch_idx):\r\n        src, trg = val_batch\r\n        output = self(src, trg[:-1, :])\r\n        output = output.transpose(1, 0)\r\n        output = output.reshape(-1, output.shape[2])\r\n        trg = trg[1:].reshape(-1)\r\n        val_loss = self.loss_fn(output, trg)\r\n        self.log('val_loss', val_loss, prog_bar=True)\r\n        return val_loss\r\n    \r\n```\r\n\r\n\r\nthe src_vocab_size, pad_idx, etc need to be defined to instantiate the model. \r\ni even tried the DataModule but that just makes things worst. \r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7221/comments",
    "author": "StephennFernandes",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-06-01T09:32:26Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 7143,
    "title": "model_checkpoint.best_model_path not set when fast_dev_run=True",
    "created_at": "2021-04-21T19:14:10Z",
    "closed_at": "2021-06-09T00:46:52Z",
    "labels": [
      "help wanted",
      "won't fix",
      "discussion",
      "working as intended",
      "checkpointing"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7143",
    "body": "## 🐛 Bug\r\n\r\nWhen using `fast_dev_run=True`, it looks like the no model is serialized and `pl.callbacks.ModelCheckpoint.best_model_path` is not set.\r\n\r\nThis is unfortunate, because it makes testing scripts relying on `pl.callbacks.ModelCheckpoint` harder, since you need to special-case the `fast_dev_run` to handle things differently.\r\n\r\n### To Reproduce\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch.nn as nn\r\nimport torch\r\n\r\n\r\nclass BoringModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.linear = nn.Linear(1, 1)\r\n        self.loss = nn.MSELoss()\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.linear.parameters())\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        X, y = batch\r\n        return self.loss(self.linear.forward(X), y)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        X, y = batch\r\n        loss = self.loss(self.linear.forward(X), y)\r\n        self.log(\"val_loss\", loss)\r\n\r\n\r\ndata = [\r\n    (torch.tensor([X]), 5 * torch.tensor([X]) + 1) for X in torch.linspace(0, 100, 24)\r\n]\r\ndataloader = torch.utils.data.DataLoader(dataset=data, batch_size=2)\r\n\r\nmodel_checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\r\ntrainer = pl.Trainer(fast_dev_run=True, callbacks=[model_checkpoint], max_epochs=10)\r\ntrainer.fit(BoringModel(), dataloader, dataloader)\r\nassert model_checkpoint.best_model_path != \"\"   # this fails when `fast_dev_run=True`\r\n```\r\n### Environment\r\n\r\n```python\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.8.1\r\n\t- pytorch-lightning: 1.2.7\r\n\t- tqdm:              4.60.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.6.10\r\n\t- version:           #1 SMP Wed Apr 14 15:25:53 UTC 2021\r\n```\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7143/comments",
    "author": "alanhdu",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-04-21T22:19:39Z",
        "body": "Hi, this is intended and actually a feature :)) `fast_dev_run` does not log and not save any artifacts. fast dev run is meant to test the loop / hooks for any obvious runtime errors in user code, but we don't want it to create files that pollute the workspace unnecessarily, because users may run several times before the get the fix right. And because it does not save any files, the best model path is not known. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-01T09:32:29Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "sachinruk",
        "created_at": "2022-07-26T07:16:24Z",
        "body": "For anyone else who came here I achieved the desired effect by doing something like:\r\n```python\r\npl.Trainer(\r\n    limit_train_batches=1 if debug else 1.0,\r\n    limit_val_batches=1 if debug else 1.0,\r\n)\r\n```"
      }
    ]
  },
  {
    "number": 7142,
    "title": "Problem with using multiple TPU cores and the WandbLogger",
    "created_at": "2021-04-21T18:56:36Z",
    "closed_at": "2021-09-14T06:27:07Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "accelerator: tpu",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7142",
    "body": "#### Query from User on Lightning Slack\r\n\r\nThere seems to be a problem with using multiple TPU cores and the WandbLogger. When I run with a single TPU core everything works fine & the results from my experiment get logged. However, if I then switch to 8 cores everything seems to run ok but nothing gets logged to WandB.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7142/comments",
    "author": "kaushikb11",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-06-01T10:32:18Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "tgisaturday",
        "created_at": "2021-08-06T11:51:23Z",
        "body": "@kaushikb11 I'll give a try and fix it."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-09-06T19:28:38Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 7125,
    "title": "training step doesn't iterate over batch in order",
    "created_at": "2021-04-20T17:26:24Z",
    "closed_at": "2021-04-21T12:38:17Z",
    "labels": [
      "bug",
      "help wanted",
      "working as intended"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7125",
    "body": "The lightning training steps don't seem to iterate over the batches in the dataloader in order.\r\n\r\nHow to reproduce:\r\n\r\nSo I initialize my train_dataloader without shuffling:\r\n\r\n```python\r\ndef train_dataloader(self):\r\n    return DataLoader(\r\n        dataset=self.train_dataset,\r\n        batch_size=self.args.batch_size,\r\n        num_workers=self.args.num_workers,\r\n        pin_memory=True,\r\n        drop_last=True\r\n    )\r\n```\r\n\r\nI then put a debugger inside training_step:\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n     pdb.set_trace()\r\n     .....\r\n```\r\n\r\nTo my frustration, the batch in the first break point does not correspond to the batch I get by iterating the dataloader manually as below:\r\n\r\n`batch = next(iter(module.train_dataloader()))`\r\n\r\nWhy is that?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7125/comments",
    "author": "kevinghst",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-04-20T23:10:09Z",
        "body": "Hi!\r\n\r\nKeep in mind you must set the seed again every time the RNG is modified (any random number has been generated).\r\nThe following code shows that the batches match. Notice all the required `seed_everything` calls\r\n\r\n```python\r\n# No issues with this dataset as it has no randomness\r\nclass IndexDataset(Dataset):\r\n    def __getitem__(self, index):\r\n        return torch.tensor(index)\r\n\r\n    def __len__(self):\r\n        return 10\r\n\r\n# Reproduces what you show if the `seed_everything` calls are removed\r\nclass RandomDataset(Dataset):\r\n    def __getitem__(self, index):\r\n        return torch.randn(1)\r\n\r\n    def __len__(self):\r\n        return 10\r\n\r\n\r\ndef test_bug(tmpdir):\r\n\r\n    class TestModel(BoringModel):\r\n        def training_step(self, batch, batch_idx):\r\n            if batch_idx == 0:\r\n                self.b0 = batch\r\n            print(f'training_step {batch_idx}: {batch}')\r\n\r\n        def train_dataloader(self):\r\n            return DataLoader(RandomDataset(), shuffle=False)\r\n\r\n    model = TestModel()\r\n    model.val_dataloader = None\r\n    model.training_epoch_end = None\r\n    trainer = Trainer(default_root_dir=tmpdir, progress_bar_refresh_rate=0, fast_dev_run=3)\r\n    tr_dl = model.train_dataloader()\r\n\r\n    # Get the first batch\r\n    pl.seed_everything(0)  # Seed the RNG state\r\n    b0_0 = next(iter(tr_dl))\r\n\r\n    # Again for sanity checking\r\n    pl.seed_everything(0)  # Need this again as the RNG state has changed\r\n    b0_1 = next(iter(tr_dl))\r\n    print('Dataloader iter batch 0:', b0_0, b0_1)\r\n\r\n    # Should be equal\r\n    assert b0_0 == b0_1\r\n\r\n    pl.seed_everything(0)  # Need this again as the RNG state has changed\r\n    trainer.fit(model)\r\n\r\n    # Should be equal\r\n    assert b0_0 == model.b0\r\n```"
      },
      {
        "user": "Borda",
        "created_at": "2021-04-21T12:38:17Z",
        "body": "@kevinghst feel free to reopen if needed :]"
      }
    ]
  },
  {
    "number": 7089,
    "title": "Using SWA callback triggers useless deepcopy of data loaders",
    "created_at": "2021-04-18T17:54:00Z",
    "closed_at": "2021-07-20T18:31:49Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7089",
    "body": "## 🐛 Bug\r\n\r\nUsing SWA callback triggers a deepcopy of the whole PyTorch Lightning module (\"pl_module\") at the on_before_accelerator_backend_setup hook (lines 140-142 of pytorch_lightning/callbacks/swa.py file). The PyTorch Lightning has the dataloaders as attributes, hence using deepcopy leads to the (useless) duplication of training and validation data loaders. \r\n\r\n\r\n### Expected behavior\r\n\r\nNo copy of the data loaders when initializing the _average_model attribute of the StochasticWeightAveraging callback. \r\n\r\n\r\n### Additional context\r\n\r\nIf data loaders occupy more than half of RAM, this deepcopy will lead to memory overflow. \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7089/comments",
    "author": "alexandrebone",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-04-19T08:45:24Z",
        "body": "Dear @alexandrebone,\r\n\r\nThe `dataloaders` are wrapped into functions and it shouldn't cause an increase of memory during the deepcopy.\r\n\r\nHowever, I am not sure about dataset. I will check.\r\n\r\nBest,\r\nT.c\r\n\r\n"
      },
      {
        "user": "edenlightning",
        "created_at": "2021-05-24T13:44:16Z",
        "body": "@tchaton ping :)"
      },
      {
        "user": "chrisby",
        "created_at": "2021-06-03T08:37:25Z",
        "body": "I also ran in OOM with SWA which does not run OOM when SWA is turned off. \r\nI guess in my case it was the deepcopy of the whole model, though."
      }
    ]
  },
  {
    "number": 7073,
    "title": "DDP for multi-node jobs in non-slurm environments",
    "created_at": "2021-04-17T16:37:28Z",
    "closed_at": "2021-04-21T23:38:17Z",
    "labels": [
      "help wanted",
      "docs",
      "distributed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7073",
    "body": "## 🐛 Bug\r\n\r\nWhen running DDP with num_nodes > 1 without a SLURM environment, the global rank is not computed correctly: global rank ends up being always equal to local rank, disregarding number of nodes. My hunch for why this is happening is that num_processes is not computed correctly.\r\n\r\nMy current solution is to create a custom plugin and explicitly pass num_nodes, please see below. I would need a bit of guidance to see how we can fix the current pytorch-lightning code in a less hacky way.\r\n\r\nThe following code works using the latest dev version (1.3.0rc1)\r\n\r\n```\r\n...\r\n\r\ntrainer = pl.Trainer(\r\n  gpus=hparams.gpus,\r\n  max_epochs=hparams.max_epochs,\r\n  num_nodes=hparams.num_nodes,\r\n  distributed_backend='ddp' if hparams.gpus > 1 else None,\r\n  sync_batchnorm=hparams.gpus > 1,\r\n  precision=hparams.precision,\r\n  fast_dev_run=hparams.fast_dev_run,\r\n  plugins=CustomPlugin(num_nodes=hparams.num_nodes),\r\n  logger=logger)\r\n\r\n...\r\n```\r\n\r\n```\r\nclass CustomEnvironment(LightningEnvironment):\r\n    def __init__(self, num_nodes):\r\n        super().__init__()\r\n        self._creates_children = num_nodes > 1\r\n\r\n    def creates_children(self):\r\n        return self._creates_children\r\n\r\n    def global_rank(self):\r\n        rank = None\r\n        if 'RANK' in os.environ:\r\n            rank = int(os.environ.get(\"RANK\"))\r\n        return rank\r\n\r\n    def master_address(self):\r\n        return os.environ.get(\"MASTER_ADDR\", \"127.0.0.1\")\r\n\r\n    def master_port(self):\r\n        return int(os.environ.get(\"MASTER_PORT\", super().master_port()))\r\n\r\n    def world_size(self):\r\n        world_size = None\r\n        if 'WORLD_SIZE' in os.environ:\r\n           world_size = int(os.environ.get('WORLD_SIZE'))\r\n        return world_size\r\n\r\n    def node_rank(self):\r\n        return int(os.environ.get(\"NODE_RANK\", super().node_rank()))\r\n\r\n    def local_rank(self) -> int:\r\n        return int(os.environ.get(\"LOCAL_RANK\", super().local_rank()))\r\n\r\n\r\nclass CustomPlugin(DDPPlugin):\r\n    def __init__(self, num_nodes, **kwargs: Union[Any, Dict[str, Any]]) -> None:\r\n        self.num_nodes = num_nodes\r\n        super().__init__(\r\n            cluster_environment=CustomEnvironment(num_nodes),\r\n            find_unused_parameters=False,\r\n            **kwargs,\r\n        )\r\n\r\n    @property\r\n    def distributed_sampler_kwargs(self):\r\n        distributed_sampler_kwargs = dict(num_replicas=self.world_size, rank=self.global_rank)\r\n        return distributed_sampler_kwargs\r\n\r\n    def set_world_ranks(self):\r\n        self.local_rank = self.cluster_environment.local_rank()\r\n        self.node_rank = self.cluster_environment.node_rank()\r\n        self.global_rank = self.cluster_environment.global_rank()\r\n        self.world_size = self.cluster_environment.world_size()\r\n        \r\n        # local run, this falls back to the current logic which is broken for\r\n        # ddp and num_nodes > 1, I suspect the problem is in self.num_processes\r\n        if self.global_rank is None:\r\n            if self.num_nodes > 1:\r\n                raise ValueError(\"Expected num_nodes > 1.\")\r\n            self.global_rank = self.num_processes * self.node_rank + self.local_rank\r\n        if self.world_size is None:\r\n            if self.num_nodes > 1:\r\n                raise ValueError(\"Expected num_nodes > 1.\")\r\n            self.world_size = self.num_nodes * self.num_processes\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7073/comments",
    "author": "sordonia",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-04-17T17:28:54Z",
        "body": "How did you set the `NODE_RANK` environment variable when launching the processes? \r\nWith the following debug script:\r\n\r\n```python\r\nimport os\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        print(\"step\", self.trainer.global_rank, self.trainer.local_rank, self.trainer.node_rank, self.trainer.world_size)\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n        return {\"x\": loss}\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n        return {\"y\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2, num_workers=0)\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=2, num_workers=0)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        gpus=2,\r\n        num_nodes=2,\r\n        accelerator=\"ddp\",\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n    )\r\n    print(\"init\", trainer.global_rank, trainer.local_rank, trainer.node_rank, trainer.world_size)\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\nIf we run with e.g.` NODE_RANK=1 python debug.py`\r\nThe stdout prints (global_rank, local_rank, node_rank, world_size):\r\n\r\n```\r\ninit 2 0 1 4\r\ninit 3 1 1 4\r\n```\r\n\r\nfor the two processes on that node, which looks correct to me.\r\nEDIT: the code here works on 1.3.0rc2"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-17T17:33:53Z",
        "body": "Apologies, I just noticed you said **1.3.0.rc1** and I thought you are running the latest.  But actually, the latest release is rc2 which has some cluster related things fixed. Mind try the update?\r\nYou may have to run master branch because of #7061 which only very recently got fixed."
      },
      {
        "user": "sordonia",
        "created_at": "2021-04-17T18:12:38Z",
        "body": "It's a bit weird, I get:\r\n\r\n2021-04-17 18:01:18 [1,3]<stderr>:  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 113, in _call_children_scripts\r\n2021-04-17 18:01:18 [1,3]<stderr>:    assert self.local_rank == 0\r\n\r\nExactly 6 times (out of 8 gpus, which makes sense), then the two local_rank = 0 continue with training. Is this how it is supposed to work by design?"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-17T18:35:00Z",
        "body": "With my example code above or with yours? Try mine first so we can be sure everything is setup correctly.\r\nThe local_rank 0 should launch all other processes on that machine and then continue training, but not until the all processes have initialized. \r\n\r\nOn node 0 you need to call\r\n`NODE_RANK=0 MASTER_ADDR=... MASTER_PORT=... python debug.py`\r\nOn node 1 you need to call\r\n`NODE_RANK=1 MASTER_ADDR=... MASTER_PORT=... python debug.py`\r\n"
      },
      {
        "user": "sordonia",
        "created_at": "2021-04-17T19:57:49Z",
        "body": "With my setup.\r\n\r\nI see the problem: currently the platform I am currently using launches 8 processes automatically (creates_children must be set to return True), that's why 6 of them fail on the assert.\r\n\r\nCan we customize the creates_children return value of the environment without a custom one?"
      },
      {
        "user": "sordonia",
        "created_at": "2021-04-17T21:00:41Z",
        "body": "Update. If I set 1 process per node in the platform, it works great, thanks for the support and for the great tool.\r\n\r\nIf there's any interest in making `creates_children` customizable I can make a PR."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-17T23:38:14Z",
        "body": "You are welcome. But what is this mysterious platform you are talking about? Does it have a name?\r\n\r\nWhat did you have in mind? A setter? \r\nWhen we created the cluster environments, the intention was that they are a sort of look up table, a source of information about the type of environment. The idea is that this information is mostly static, because the cluster environment is not really expected to change dynamically. But I'm open to suggestion in #6303 "
      },
      {
        "user": "sordonia",
        "created_at": "2021-04-17T23:51:24Z",
        "body": "Yes :) AzureML.\r\n\r\nI thought about an option to Trainer: Trainer(..., environment_creates_children=True, ...), which would entail that PL must assume the environment creates world_size processes (False by default).\r\n\r\nLet me take a look"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-17T23:58:12Z",
        "body": "That would interfere with all cluster environments (currently we have slurm, torchelastic, and lightning). \r\nThe best way currently would be to implement a `AzureMLEnvironment` inheriting from `ClusterEnvironment`.\r\nThat's the intended way to enable new clusters in Lightning.\r\n"
      },
      {
        "user": "sordonia",
        "created_at": "2021-04-18T00:06:28Z",
        "body": "Is there a straightforward way for the user to specify to use the `AzureMLEnvironment` ?"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-18T01:09:17Z",
        "body": "I suggest this:\r\n\r\n```python\r\nclass AzureEnvironment(ClusterEnvironment):\r\n    ... # here override all abstract methods\r\n\r\ntrainer = Trainer(plugins=[AzureEnvironment()])\r\n```\r\n\r\nNeed to document this\r\n"
      }
    ]
  },
  {
    "number": 7031,
    "title": "Can't get attribute '_gpus_arg_default'",
    "created_at": "2021-04-15T05:01:46Z",
    "closed_at": "2021-04-20T07:34:04Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7031",
    "body": "## 🐛 Bug\r\nwhen I use the code\r\n```model = model.load_from_checkpoint(ckpt_path)```\r\nIt pops out a problem: ```Can't get attribute '_gpus_arg_default' on <module 'pytorch_lightning.utilities.argparse' from '/opt/conda/envs/lasaft/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py'>```\r\n\r\n### To Reproduce\r\nVery hard to describe the reproduction, because I could not load model.\r\n\r\n### Expected behavior\r\nSucceed in loading the model\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0):1.7.1\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: V100\r\n - Any other relevant information: PL version is 1.3.0rc1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7031/comments",
    "author": "sun-peach",
    "comments": [
      {
        "user": "quancs",
        "created_at": "2021-04-15T09:55:23Z",
        "body": "I met the same problem when I upgrade from 1.2.2 to 1.2.8 and from 1.2.6 to 1.2.8"
      },
      {
        "user": "quancs",
        "created_at": "2021-04-15T09:58:51Z",
        "body": "And when I debugged it, I found that the “pytorch_lightning.utilities.argparse” in 1.2.8 does not have \"_gpus_arg_default\", but “pytorch_lightning.utilities.argparse” in 1.2.6 does have \"_gpus_arg_default\"."
      },
      {
        "user": "SeanNaren",
        "created_at": "2021-04-15T11:08:47Z",
        "body": "I haven't been able to reproduce this issue. \r\n\r\nTaking the boring model and running this:\r\n\r\n```python\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import Trainer, LightningModule\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        \"\"\"\r\n        Testing PL Module\r\n\r\n        Use as follows:\r\n        - subclass\r\n        - modify the behavior for what you want\r\n\r\n        class TestModel(BaseTestModel):\r\n            def training_step(...):\r\n                # do your own thing\r\n\r\n        or:\r\n\r\n        model = BaseTestModel()\r\n        model.training_epoch_end = None\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = BoringModel()\r\n\r\n    trainer = Trainer(\r\n        fast_dev_run=True,\r\n        gpus=-1,\r\n    )\r\n    trainer.fit(model)\r\n    trainer.save_checkpoint('model.pt')\r\n\r\n    BoringModel.load_from_checkpoint('model.pt')\r\n```\r\n\r\nWorks on 1.2.8. I then tried saving a checkpoint using 1.2.6, and then loading it in 1.2.8 as such:\r\n\r\n```python \r\nBoringModel.load_from_checkpoint('model.pt')\r\n```\r\n\r\nAnd this worked fine. Could you give more details as to how this bug appears so I can reproduce @quancs @sun-peach?"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-15T11:17:48Z",
        "body": "I think the problem is that the args were passed into the model and saved as hyper parameters in 1.2.6 but in 1.2.8 we removed the `_gpus_arg_default` function, so pickle can't find the source code for that :( \r\nChange was made here: #6898 \r\n\r\nYou can probably fix it by doing right before you load your checkpoint.\r\n**Workaround:**\r\n```python\r\nfrom pytorch_lightning.utilities import argparse \r\nsetattr(argparse, \"_gpus_arg_default\", lambda x: 0)\r\n```\r\nmy bad. I didn't think of this. shoot.\r\nwe should re-introduce this function and keep it there so old checkpoints who have that flaw can still be loaded."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-15T16:19:25Z",
        "body": "This was reproducible for me when I passed in the argparse args into the model and they get saved.\r\nI verified that this is fixed in my PR #7043, checkpoint loads without problem.\r\n\r\n```python\r\nimport argparse\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import Trainer, LightningModule\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self, **kwargs):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser = Trainer.add_argparse_args(parser)\r\n    args = parser.parse_args()\r\n    model = BoringModel(**vars(args))\r\n    trainer = Trainer.from_argparse_args(args)\r\n    trainer.fit(model)\r\n    \r\n    # comment the line below on < 1.2.8\r\n    trainer.save_checkpoint('model.pt')\r\n    \r\n    # comment the line below on 1.2.8\r\n    BoringModel.load_from_checkpoint('model.pt')\r\n```"
      }
    ]
  },
  {
    "number": 7027,
    "title": "RuntimeError: CUDA error: invalid device ordinal in pytorch_lightning version:1.2.7",
    "created_at": "2021-04-15T01:48:28Z",
    "closed_at": "2021-04-25T08:39:31Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author",
      "distributed",
      "environment: slurm"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7027",
    "body": "## 🐛 Bug\r\n\r\nAn error is reported when using multiple nodes and multiple GPUs on slurm. It seems it always initialize on only one node.\r\nI only meet this question in version 1.2.7, but all is ok in version 1.1.2.\r\nLOCAL_RANK: 0 -CUDA_VISIBLE_DEVICES: [0,1]\r\ninitializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\r\nLOCAL_RANK: 0 -CUDA_VISIBLE_DEVICES: [0,1]\r\ninitializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\r\nLOCAL_RANK: 0 -CUDA_VISIBLE_DEVICES: [0,1]\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\r\nLOCAL_RANK: 0 -CUDA_VISIBLE_DEVICES: [0,1]\r\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\r\nmy trainer is:\r\ntrainer = Trainer(gpus=2, accelerator='ddp', num_nodes=2)\r\nmy slrum script is:\r\n#SBATCH --nodes=2\r\n#SBATCH --gres=gpu:2\r\n#SBATCH --ntasks-per-node=2\r\n\r\n - PyTorch Version:  1.7.1\r\n - OS:  Centos\r\n - How you installed PyTorch: pip\r\n - Python version: 3.7.0\r\n - CUDA/cuDNN version: 10.1\r\n\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7027/comments",
    "author": "zhhao1",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-04-15T12:53:06Z",
        "body": "Is it working on 1.2.8? I tried to fix it here: #6941 \r\nAnd I got confirmation from another slurm user that it's working now."
      },
      {
        "user": "zhhao1",
        "created_at": "2021-04-15T13:33:33Z",
        "body": "> Is it working on 1.2.8? I tried to fix it here: #6941\r\n> And I got confirmation from another slurm user that it's working now.\r\n\r\nIs there any flags need to be set in Trainer?"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-15T15:24:08Z",
        "body": "no, just use sbatch to submit your job as usual. I think  the only thing you need to make sure is that you set the same number of nodes and gpus in the Trainer as in the submission script. \r\nAnd lightning should detect that you run with slurm based on the env vars (e.g. SLURM_PROCID etc.)"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-21T11:59:19Z",
        "body": "any update here? have you managed to get it working with Lightning >= 1.2.8?"
      },
      {
        "user": "zhhao1",
        "created_at": "2021-04-25T08:39:31Z",
        "body": "> any update here? have you managed to get it working with Lightning >= 1.2.8?\r\n\r\nSorry, there are still some problems, but you mentioned that someone has successfully used it. I guess this may be my own problem. I'm trying to find the problem."
      }
    ]
  },
  {
    "number": 7010,
    "title": "Name dataloader when returned in test_dataloader of LightningDataModule",
    "created_at": "2021-04-14T12:35:49Z",
    "closed_at": "2021-06-01T10:32:27Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7010",
    "body": "## 🚀 Feature\r\nThe ability that one can name `dataloader `when returned in `test_dataloader()` of `LightningDataModule`.\r\n\r\n### Motivation\r\n\r\nI may want to access the name of current dataloader's name when I have multiple `dataloader `returned from `test_dataloader()`, so that I can save my test results according to this speccific name.\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7010/comments",
    "author": "Luciennnnnnn",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2021-04-17T04:16:41Z",
        "body": "Does the dataloader index work for you? You can maintain a map of index to name inside your module for convenience for naming"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-05-18T10:32:47Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6965,
    "title": "epoch option to log_model using WandbLogger",
    "created_at": "2021-04-12T05:50:22Z",
    "closed_at": "2021-06-16T00:38:28Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6965",
    "body": "## 🚀 Feature\r\n\r\nWandbLogger should have an optional parameter, `log_model_every_k_epochs`, in which case the model is logged periodically.\r\n\r\n### Motivation\r\n\r\nWandbLogger appears only to log the model on finalize. This is different than the default ptl behavior, and also means that model logging happens unpredictably.\r\n\r\n### Pitch\r\n\r\nIf Google Colab terminates your training, WandbLogger doesn't save your model.\r\n\r\nIf you cancel training and start editing your Jupyter notebook, WandbLogger doesn't save your model.\r\n\r\nThere is a high potential for human error in forgetting to manually call WandbLogger.finalize()\r\n\r\n### Alternatives\r\n\r\nI believe one can manually call WandbLogger.finalize() if they are monitoring the job. I don't know if this means that the logger cannot be reused or the training job continued.\r\n\r\nAlso, this workaround doesn't help in the setting where Colab shuts down an instance without warning.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6965/comments",
    "author": "turian",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2021-04-12T09:58:15Z",
        "body": "n00b question: Would it be easier to extend the ModelCheckpoint callback for specific WandB functionality? "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-08T23:46:55Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6964,
    "title": "load_from_checkpoint function returning error - AttributeError: Can't get attribute '_gpus_arg_default'",
    "created_at": "2021-04-11T16:47:28Z",
    "closed_at": "2021-04-12T10:43:31Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6964",
    "body": "I am getting this error when trying to load model from checkpoint:\r\n\r\nmodel = MyModel.load_from_checkpoint(path/to/model/Google Drive)\r\n\r\nAttributeError: Can't get attribute '_gpus_arg_default' on <module 'pytorch_lightning.utilities.argparse' from '/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/argparse.py'>\r\n\r\nThis is happening on Google Colab. This was working for me before, but suddenly I get this error with the same models that I have loaded previously without issue.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6964/comments",
    "author": "psbreuning",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-04-11T20:23:02Z",
        "body": "Hi, we fixed this recently in master #6898 \r\nThis won't happen in the future, but unfortunately your checkpoint contains that serialised function and the only way I know to undo it is to \r\n1. downgrade lightning to the version that produced the checkpoint (in your case, was it 2.0, 2.1?)\r\n2. load the checkpoint with `torch.load()`, manually remove the `gpu_arg_default` from the hparams inside the dict\r\n3. save the checkpoint again with `torch.save`.\r\n\r\nSorry for the inconvenience, I know this is very annoying!"
      }
    ]
  },
  {
    "number": 6951,
    "title": "LearningRateMonitor wont log if interval = 'step' or if scheduler name is not declared",
    "created_at": "2021-04-10T21:52:46Z",
    "closed_at": "2021-08-09T01:30:42Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6951",
    "body": "## 🐛 Bug\r\n\r\nLearningRateMonitor wont log lr if a name is not declared or if interval = 'step'\r\n\r\nworks\r\n```\r\n scheduler = {'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer,\r\n                                                            max_lr=[cfg.max_lr/cfg.encoder_lr_frac, cfg.max_lr],\r\n                                                            epochs=self.epochs,\r\n                                                            steps_per_epoch = len(self.loaders_dict['train'])),\r\n                     'name': 'random_name',\r\n                     'frequency': 1,\r\n                     'interval': 'epoch,\r\n                     \"monitor\": 'train_loss'}\r\n```\r\n\r\ndoesnt work with step\r\n```\r\n scheduler = {'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer,\r\n                                                            max_lr=[cfg.max_lr/cfg.encoder_lr_frac, cfg.max_lr],\r\n                                                            epochs=self.epochs,\r\n                                                            steps_per_epoch = len(self.loaders_dict['train'])),\r\n                     'name': 'random_name',\r\n                     'frequency': 1,\r\n                     'interval': 'step', #CHANGED TO STEP\r\n                     \"monitor\": 'train_loss'}\r\n```\r\n\r\ndoesnt work without name\r\n```\r\n scheduler = {'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer,\r\n                                                            max_lr=[cfg.max_lr/cfg.encoder_lr_frac, cfg.max_lr],\r\n                                                            epochs=self.epochs,\r\n                                                            steps_per_epoch = len(self.loaders_dict['train'])),\r\n                    # 'name': 'random_name',\r\n                     'frequency': 1,\r\n                     'interval': 'epoch',\r\n                     \"monitor\": 'train_loss'}\r\n```\r\nUsed linux, pytorch 1.7, latest version of PL\r\n\r\n\r\n## Source of error\r\n\r\nIn the callback, on_train_batch_start, the function _should_log finds that **trainer.log_every_n_steps = 50**.\r\nIf i set trainer.log_every_n_steps = 1, then the logger works. So I assume that when scheduler['interval'] = 'step', the trainer doesnt updated the log_every_n_steps.\r\n\r\n```\r\ndef on_train_batch_start(self, trainer, *args, **kwargs):\r\n        if not self._should_log(trainer):\r\n            return\r\n\r\n        #[logging code]\r\n\r\ndef _should_log(trainer) -> bool:\r\n        should_log = ((trainer.global_step + 1) % trainer.log_every_n_steps == 0 or trainer.should_stop)\r\n\r\n        return should_log\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6951/comments",
    "author": "felipemello1",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2021-04-12T07:53:32Z",
        "body": "Hi @fmellomascarenhas, \r\nYou're right, we keep the logs internally, but just don't flush them to the logger every step.\r\n\r\nFlushing is a very time consuming operation that would slow down training significantly, if done at every iteration. \r\n\r\nDo you think this should be changed? Since actually, there is a flag to change this (as you noted)"
      },
      {
        "user": "felipemello1",
        "created_at": "2021-04-12T12:28:54Z",
        "body": "Hi @justusschock, thanks for answering! I think that there are two fixes:\r\n\r\n1) the logging should have the same behavior, regardless of the scheduler name being provided;\r\n2) it should also have the same behavior for epochs and steps. if the developer explicitily declares in the scheduler \"frequency: 1\", then the frequency should be 1. It worked for epochs, but didn't work for step. \r\n\r\nDo you agree?"
      },
      {
        "user": "justusschock",
        "created_at": "2021-04-12T13:42:47Z",
        "body": "Hi @fmellomascarenhas \r\n\r\nFor 1.) I agree!\r\n\r\nFor 2.) Actually I don't think so. Since the frequency of the scheduler and the frequency of the logger are two completely separated things. That means, that the lr is updated by the scheduler, just not yet logged (due to logger frequency).\r\n\r\nWould you mind sending a PR for 1.)?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-01T02:51:41Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6894,
    "title": "MLFlowLogger should defaults to enironment variable MLFLOW_TRACKING_URI",
    "created_at": "2021-04-08T15:19:07Z",
    "closed_at": "2021-05-12T09:26:58Z",
    "labels": [
      "feature",
      "help wanted",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6894",
    "body": "## 🚀 Feature\r\nThe default value of `tracking_uri` in the initializer of `MLFlowLogger` should be `MLFLOW_TRACKING_URI` environment variable if set.\r\n\r\n### Motivation\r\n1. `mlflow` works in this way\r\n2. it allows to have one tracking server with multiple processes using it (otherwise processes conflicts)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6894/comments",
    "author": "00sapo",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-05-08T21:28:12Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "edenlightning",
        "created_at": "2021-05-09T14:10:57Z",
        "body": "Thanks for the issue @00sapo! feel free to send a PR."
      }
    ]
  },
  {
    "number": 6889,
    "title": "PL computes wrong accuracy with drop_last=False in PyTorch Geometric",
    "created_at": "2021-04-08T10:29:38Z",
    "closed_at": "2021-05-05T14:12:52Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6889",
    "body": "## 🐛 Bug\r\n\r\nPyTorch Lightning computes wrong accuracy when using a `DataLoader` with `drop_last=False` in PyTorch Geometric.\r\nThere seems to be an issue in which PL cannot determine the correct `batch_size` of mini-batches.\r\n\r\n```python\r\nfrom typing import Optional\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torch.nn import Linear\r\nfrom pytorch_lightning.metrics import Accuracy\r\nfrom pytorch_lightning import (LightningDataModule, LightningModule, Trainer,\r\n                               seed_everything)\r\n\r\nfrom torch_geometric.data import DataLoader\r\nfrom torch_geometric.datasets import TUDataset\r\nfrom torch_geometric.nn import GCNConv, global_mean_pool\r\n\r\n\r\nclass Dataset(LightningDataModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def prepare_data(self):\r\n        TUDataset('./data', name='MUTAG')\r\n\r\n    def setup(self, stage: Optional[str] = None):\r\n        dataset = TUDataset('./data', name='MUTAG')\r\n        self.train_dataset = dataset[:3]\r\n        self.val_dataset = dataset[3:6]\r\n        self.test_dataset = dataset[6:9]\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.train_dataset, batch_size=2)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.val_dataset, batch_size=2)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.val_dataset, batch_size=2)\r\n\r\n\r\nclass GNN(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv = GCNConv(7, 64)\r\n        self.lin = Linear(64, 2)\r\n        self.acc = Accuracy()\r\n\r\n    def forward(self, x, edge_index, batch):\r\n        x = self.conv(x, edge_index).relu()\r\n        x = global_mean_pool(x, batch)\r\n        return self.lin(x)\r\n\r\n    def training_step(self, data, batch_idx):\r\n        data = data.to(self.device)\r\n        y_hat = self(data.x, data.edge_index, data.batch)\r\n        train_loss = F.cross_entropy(y_hat, data.y)\r\n        return train_loss\r\n\r\n    def validation_step(self, data, batch_idx):\r\n        data = data.to(self.device)\r\n        y_hat = self(data.x, data.edge_index, data.batch)\r\n        acc = self.acc(y_hat.softmax(dim=-1), data.y)\r\n        self.log('val_acc', acc, on_step=False, on_epoch=True)\r\n        return acc\r\n\r\n    def test_step(self, data, batch_idx):\r\n        data = data.to(self.device)\r\n        y_hat = self(data.x, data.edge_index, data.batch)\r\n        acc = self.acc(y_hat.softmax(dim=-1), data.y)\r\n        print('batch_size', data.num_graphs, 'accuracy', acc, 'shape', y_hat.shape)\r\n        self.log('test_acc', acc, on_step=False, on_epoch=True)\r\n        return acc\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.01)\r\n\r\n\r\ndef main():\r\n    seed_everything(42)\r\n    datamodule = Dataset()\r\n    model = GNN()\r\n    trainer = Trainer(max_epochs=1, progress_bar_refresh_rate=0)\r\n    trainer.fit(model, datamodule=datamodule)\r\n    trainer.test()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nHere, I am using a dataset with 3 examples and utilize a `batch_size` of 2. In `test_step`, the accuracy of each individual mini-batch is:\r\n```\r\nbatch_size 2 accuracy 0.5 shape [2, 2]\r\nbatch_size 1 accuracy 0.0 shape [1, 2]\r\n```\r\nwhile PyTorch Lightning reports an overall accuracy of `0.25`.\r\n\r\n### Expected behavior\r\n\r\nReport accuracy of `0.33`.\r\n\r\n### Environment\r\n\r\n* `torch-geometric==master`\r\n\r\n### Additional context\r\n\r\nIt seems like PL has problems determining the correct `batch_size` of batches when data doesn't follow the conventional `[batch_size, ...]` format. However, it shouldn't have a problem in doing so since the `batch_size` can be easily inferred from the `self.acc(y_hat, y_pred)` call.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6889/comments",
    "author": "rusty1s",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-04-30T17:07:24Z",
        "body": "Hey @rusty1s,\r\n\r\nThere is a definitely a bug in Lighting, but hard to resolve without deeper refactor.\r\nHowever, you can easily make this work by providing the metric directly to `self.log` as it is supported and much safer.\r\nAlso, there should be 1 metric per stage :)\r\n\r\n```\r\nclass GNN(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv = GCNConv(7, 64)\r\n        self.lin = Linear(64, 2)\r\n        self.val_acc = Accuracy()\r\n        self.test_acc = Accuracy()\r\n\r\n    def forward(self, x, edge_index, batch):\r\n        x = self.conv(x, edge_index).relu()\r\n        x = global_mean_pool(x, batch)\r\n        return self.lin(x)\r\n\r\n    def training_step(self, data, batch_idx):\r\n        data = data.to(self.device)\r\n        y_hat = self(data.x, data.edge_index, data.batch)\r\n        train_loss = F.cross_entropy(y_hat, data.y)\r\n        return train_loss\r\n\r\n    def validation_step(self, data, batch_idx):\r\n        data = data.to(self.device)\r\n        y_hat = self(data.x, data.edge_index, data.batch)\r\n        self.val_acc(y_hat.softmax(dim=-1), data.y)\r\n        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True)\r\n        return acc\r\n\r\n    def test_step(self, data, batch_idx):\r\n        data = data.to(self.device)\r\n        y_hat = self(data.x, data.edge_index, data.batch)\r\n        self.test_acc(y_hat.softmax(dim=-1), data.y)\r\n        self.log('test_acc', self.test_acc, on_step=False, on_epoch=True)\r\n        return acc\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.01)\r\n\r\n\r\ndef main():\r\n    seed_everything(42)\r\n    datamodule = Dataset()\r\n    model = GNN()\r\n    trainer = Trainer(max_epochs=1, progress_bar_refresh_rate=0)\r\n    trainer.fit(model, datamodule=datamodule)\r\n    trainer.test()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```"
      },
      {
        "user": "weihua916",
        "created_at": "2021-05-10T17:17:11Z",
        "body": "Has this issue been resolved on master? I installed master, and still see the accuracy of 0.25 when running Matthias' code."
      },
      {
        "user": "weihua916",
        "created_at": "2021-05-10T17:27:06Z",
        "body": "Sorry, I just saw the comment to do the work-around. Thanks!"
      }
    ]
  },
  {
    "number": 6876,
    "title": "Latest FairScale + Sharded Training crashes using default trainer parameters",
    "created_at": "2021-04-07T20:22:04Z",
    "closed_at": "2021-04-08T18:04:26Z",
    "labels": [
      "bug",
      "help wanted",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6876",
    "body": "## 🐛 Bug\r\n\r\nWhen validation/training is used (as default with the boring model) sharded crashes. This is because internally SDP relies on knowing the training state of the model, and when we run the validation sanity check, we do not set the eval mode correctly on the SDP model itself, so it waits for grads to be reduced since the module is in `train` mode.\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    \"\"\"\r\n    >>> RandomDataset(size=10, length=20)  # doctest: +ELLIPSIS\r\n    <...bug_report_model.RandomDataset object at ...>\r\n    \"\"\"\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    \"\"\"\r\n    >>> BoringModel()  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\r\n    BoringModel(\r\n      (layer): Linear(...)\r\n    )\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        \"\"\"\r\n        Testing PL Module\r\n\r\n        Use as follows:\r\n        - subclass\r\n        - modify the behavior for what you want\r\n\r\n        class TestModel(BaseTestModel):\r\n            def training_step(...):\r\n                # do your own thing\r\n\r\n        or:\r\n\r\n        model = BaseTestModel()\r\n        model.training_epoch_end = None\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self.layer(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ndef test_run():\r\n    # fake data\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n    test_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n\r\n    # model\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        max_epochs=1,\r\n        plugins='ddp_sharded',\r\n        gpus=1,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model, train_data, val_data)\r\n    trainer.test(test_dataloaders=test_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    test_run()\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6876/comments",
    "author": "SeanNaren",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2021-04-07T20:25:32Z",
        "body": "1 workaround is setting `num_sanity_val_steps=0` but that's very much a short-term fix"
      }
    ]
  },
  {
    "number": 6869,
    "title": "\"TypeError: can't pickle _thread.lock objects\" when logging tables to WandB",
    "created_at": "2021-04-07T14:22:20Z",
    "closed_at": "2021-08-09T01:30:38Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "waiting on author",
      "3rd party",
      "priority: 2"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6869",
    "body": "## 🐛 Bug\r\n\r\n### To Reproduce\r\n\r\nTry to log tables using `WandLogger`, e.g.:\r\n\r\n```python\r\n    def validation_epoch_end(self, outputs: List[Any]) -> None:\r\n\r\n        df = pd.DataFrame(\r\n            {\r\n                'my_stats': [1,2,3]\r\n            }\r\n        )\r\n\r\n        table = wandb.Table(dataframe=df)\r\n        self.log(\"examples\", table)\r\n```\r\n\r\nAfter the first epoch (i.e. when the model checkpoint is saved), the error occurs:\r\n\r\n`TypeError: can't pickle _thread.lock objects`\r\n\r\n\r\n### Expected behavior\r\n\r\nPickling models should succeed.\r\n\r\n### Environment\r\n\r\nCUDA:\r\n- GPU:\r\n- NVIDIA GeForce RTX 3090\r\n- NVIDIA GeForce RTX 2060 SUPER\r\n- available: True\r\n- version: 11.1\r\nPackages:\r\n- numpy: 1.20.1\r\n- pyTorch_debug: False\r\n- pyTorch_version: 1.8.0+cu111\r\n- pytorch-lightning: 1.2.4\r\n- tqdm: 4.59.0\r\nSystem:\r\n- OS: Linux\r\n- architecture:\r\n- 64bit\r\n- ELF\r\n- processor:\r\n- python: 3.7.9\r\n- version: #1 SMP Tue Jun 23 12:58:10 UTC 2020",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6869/comments",
    "author": "ejohb",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2021-04-12T10:40:27Z",
        "body": "@tchaton when you log an object it has to be also picklable?"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-14T22:45:34Z",
        "body": "```python\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nimport wandb\r\nfrom torch.nn import functional as F\r\n\r\nimport pytorch_lightning as pl\r\nfrom pl_examples.basic_examples.mnist_datamodule import MNISTDataModule\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom pytorch_lightning.loggers import WandbLogger\r\n\r\n\r\nclass LitClassifier(pl.LightningModule):\r\n\r\n    def __init__(self, hidden_dim=128, learning_rate=1e-3):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\r\n        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        x = torch.relu(self.l1(x))\r\n        x = torch.relu(self.l2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('valid_loss', loss)\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        import pandas as pd\r\n        df = pd.DataFrame(\r\n            {\r\n                'my_stats': [1, 2, 3]\r\n            }\r\n        )\r\n\r\n        table = wandb.Table(dataframe=df)\r\n        self.log(\"examples\", table)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument('--hidden_dim', type=int, default=128)\r\n        parser.add_argument('--learning_rate', type=float, default=0.0001)\r\n        return parser\r\n\r\n\r\ndef main():\r\n    pl.seed_everything(1234)\r\n    parser = ArgumentParser()\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    parser = LitClassifier.add_model_specific_args(parser)\r\n    parser = MNISTDataModule.add_argparse_args(parser)\r\n    args = parser.parse_args()\r\n\r\n    dm = MNISTDataModule.from_argparse_args(args)\r\n    model = LitClassifier(args.hidden_dim, args.learning_rate)\r\n    trainer = pl.Trainer.from_argparse_args(\r\n        args, \r\n        logger=WandbLogger(project=\"test\", name=\"test\"), \r\n        callbacks=[ModelCheckpoint(monitor=\"valid_loss\")]\r\n    )\r\n    trainer.fit(model, datamodule=dm)\r\n    trainer.test(model, datamodule=dm)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\ncannot reproduce, ran this on master and also reverted back to 1.2.4. \r\nPlease let us know which modification we need to make to the above script to reproduce it.\r\nAnd since you experience this with wandb, please specify the wandb version.\r\nthanks!"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-21T12:56:32Z",
        "body": "Hi, do you have any further updates on this? Do you have the original code that produced this error?"
      },
      {
        "user": "ejohb",
        "created_at": "2021-04-21T12:57:44Z",
        "body": "> Hi, do you have any further updates on this? Do you have the original code that produced this error?\r\n\r\nSorry I'm not currently working on the related project but will come back to you when I resume."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-01T02:51:45Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6833,
    "title": "Loss not logged for 16bit precision and manual optimization",
    "created_at": "2021-04-05T06:52:46Z",
    "closed_at": "2021-04-27T18:17:21Z",
    "labels": [
      "bug",
      "help wanted",
      "3rd party",
      "logging"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6833",
    "body": "When training a GAN and thus using manual optimization, I include these lines:\r\n\r\n```\r\nself.log(\"loss_d\", loss_d, prog_bar=True)\r\nself.log(\"loss_g\", loss_g, prog_bar=True)\r\n```\r\n\r\nThe logging works as intended, both for my logger (W&B) and to the command line progress bar so long as I use 32 bit precision. However, when I use 16 bit precision, no logging occurs to either. I have tried printing the losses, and they appear to be reasonable values and on the correct devices.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6833/comments",
    "author": "timothybrooks",
    "comments": [
      {
        "user": "timothybrooks",
        "created_at": "2021-04-05T07:07:43Z",
        "body": "I have also tried printing `self._results` and it appears to sometimes be missing entries when using 16 bit, whereas it always includes all entries in 32 bit. Also I disable 16 bit via `with torch.cuda.amp.autocast(enabled=False):` out of necessity in a couple select parts of the algorithm in case that is relevant."
      },
      {
        "user": "justusschock",
        "created_at": "2021-04-05T10:27:16Z",
        "body": "Hi @timothybrooks do you have an example to reproduce this behavior?"
      },
      {
        "user": "timothybrooks",
        "created_at": "2021-04-06T23:40:05Z",
        "body": "Its entangled in a large project, but I've extracted a sketch of the training step which is the most relevant portion:\r\n\r\n```python\r\n\r\n@property\r\ndef automatic_optimization(self) -> bool:\r\n    return False\r\n\r\ndef training_step(...):\r\n    \r\n    # Discriminator.\r\n    optimizer_idx = 0\r\n    optimizer: optim.Adam = self.optimizers()[optimizer_idx]\r\n    self.toggle_optimizer(optimizer, optimizer_idx)\r\n\r\n    loss_d = self.compute_loss_d(...)\r\n    self.log(\"loss_d\", loss_d, prog_bar=True)\r\n\r\n    optimizer.zero_grad()\r\n    self.manual_backward(loss, optimizer)\r\n    optimizer.step()\r\n    self.untoggle_optimizer(optimizer_idx)\r\n\r\n    # Generator.\r\n    optimizer_idx = 1\r\n    optimizer: optim.Adam = self.optimizers()[optimizer_idx]\r\n    self.toggle_optimizer(optimizer, optimizer_idx)\r\n\r\n    loss_g = self.compute_loss_g(...)\r\n    self.log(\"loss_g\", loss_g, prog_bar=True)\r\n\r\n    optimizer.zero_grad()\r\n    self.manual_backward(loss, optimizer)\r\n    optimizer.step()\r\n    self.untoggle_optimizer(optimizer_idx)\r\n```\r\n\r\nPortions of code inside `compute_loss_d` and `compute_loss_g` disable autocasting. I'm unsure how much of this is needed to reproduce the bug, and whether manual optimization is part of the issue or whether it is simply an issue with mixed precision."
      },
      {
        "user": "denadai2",
        "created_at": "2021-04-23T22:15:47Z",
        "body": "I have this bug as well. I do not find why but it's NOT related to manual optimization. It appears to be related to float=16"
      },
      {
        "user": "tchaton",
        "created_at": "2021-04-27T08:05:22Z",
        "body": "Hey @denadai2 @timothybrooks,\r\n\r\nI confirm I can reproduce the bug.\r\n\r\n```\r\ndef test_multiple_optimizers_logging(tmpdir):\r\n    \"\"\"\r\n    Tests that only training_step can be used\r\n    \"\"\"\r\n\r\n    class TestModel(BoringModel):\r\n\r\n        def __init__(self):\r\n            super().__init__()\r\n            self.automatic_optimization = False\r\n\r\n        def training_step(self, batch, batch_idx):\r\n            # Discriminator.\r\n            optimizer_idx = 0\r\n            optimizer = self.optimizers()[optimizer_idx]\r\n            self.toggle_optimizer(optimizer, optimizer_idx)\r\n\r\n            loss_d = self.loss(batch, self.layer(batch))\r\n            self.log(\"loss_d\", loss_d, prog_bar=True)\r\n\r\n            optimizer.zero_grad()\r\n            self.manual_backward(loss_d, optimizer)\r\n            optimizer.step()\r\n            self.untoggle_optimizer(optimizer_idx)\r\n\r\n            # Generator.\r\n            optimizer_idx = 1\r\n            optimizer = self.optimizers()[optimizer_idx]\r\n            self.toggle_optimizer(optimizer, optimizer_idx)\r\n\r\n            loss_g = self.loss(batch, self.layer(batch))\r\n            self.log(\"loss_g\", loss_g, prog_bar=True)\r\n\r\n            optimizer.zero_grad()\r\n            self.manual_backward(loss_g, optimizer)\r\n            optimizer.step()\r\n            self.untoggle_optimizer(optimizer_idx)\r\n\r\n            import pdb; pdb.set_trace()\r\n\r\n        def configure_optimizers(self):\r\n            optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n            optimizer_2 = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n            return optimizer, optimizer_2\r\n\r\n    model = TestModel()\r\n    model.training_epoch_end = None\r\n    model.val_dataloader = None\r\n\r\n    limit_train_batches = 2\r\n    trainer = Trainer(\r\n        default_root_dir=tmpdir,\r\n        limit_train_batches=limit_train_batches,\r\n        limit_val_batches=2,\r\n        max_epochs=1,\r\n        log_every_n_steps=1,\r\n        weights_summary=None,\r\n        gpus=1,\r\n        precision=16,\r\n    )\r\n\r\n    trainer.fit(model)\r\n\r\n    expected = {'epoch', 'loss_d', 'loss_g'}\r\n    logged = set(trainer.logged_metrics.keys())\r\n    assert expected == logged\r\n    expected = {'loss_d', 'loss_g'}\r\n    logged = set(trainer.progress_bar_metrics.keys())\r\n    assert expected == logged\r\n```"
      },
      {
        "user": "tchaton",
        "created_at": "2021-04-27T18:17:49Z",
        "body": "Hey @timothybrooks,\r\n\r\nShould work on master. Mind checking ?\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "denadai2",
        "created_at": "2021-04-27T19:30:25Z",
        "body": "my script works :) thx!"
      }
    ]
  },
  {
    "number": 6819,
    "title": "is_cuda_out_of_memory Misses CUDA OOM Errors",
    "created_at": "2021-04-04T15:14:49Z",
    "closed_at": "2021-04-15T01:22:12Z",
    "labels": [
      "bug",
      "help wanted",
      "tuner"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6819",
    "body": "## 🐛 Bug\r\n\r\n\r\n\r\n`pytorch_lightning.utilities.memory.is_cuda_out_of_memory` does not work reliably, causing things like `auto_scale_batch_size` tuning to fail.\r\n\r\n## Please reproduce using the BoringModel\r\n\r\nI'm afraid I don't know what BoringModel is.\r\n\r\n### To Reproduce\r\n\r\nThis returns false. `is_cuda_out_of_memory(RuntimeError('CUDA error: out of memory'))`\r\n\r\nThe function only checks for the string `CUDA out of memory.`\r\n\r\n### Expected behavior\r\n\r\nThe above should return `True`.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce RTX 3090\r\n                - NVIDIA GeForce RTX 2060 SUPER\r\n        - available:         True\r\n        - version:           11.1\r\n* Packages:\r\n        - numpy:             1.20.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.8.0+cu111\r\n        - pytorch-lightning: 1.2.4\r\n        - tqdm:              4.59.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:\r\n        - python:            3.7.9\r\n        - version:           #1 SMP Tue Jun 23 12:58:10 UTC 2020\r\n\r\n```\r\nroot@eb:~# neofetch\r\n       _,met$$$$$gg.          root@eb\r\n    ,g$$$$$$$$$$$$$$$P.       -------\r\n  ,g$$P\"     \"\"\"Y$$.\".        OS: Debian GNU/Linux 10 (buster) on Windows 10 x86_64\r\n ,$$P'              `$$$.     Kernel: 4.19.128-microsoft-standard\r\n',$$P       ,ggs.     `$$b:   Uptime: 19 hours, 44 mins\r\n`d$$'     ,$P\"'   .    $$$    Packages: 594 (dpkg)\r\n $$P      d$'     ,    $$P    Shell: bash 5.0.3\r\n $$:      $$.   -    ,d$$'    Terminal: /dev/pts/4\r\n $$;      Y$b._   _,d$P'      CPU: AMD Ryzen Threadripper 3970X 32- (64) @ 3.693GHz\r\n Y$$.    `.`\"Y$$$$P\"'         Memory: 23149MiB / 257643MiB\r\n `$$b      \"-.__\r\n  `Y$$\r\n   `Y$$.\r\n     `$$b.\r\n       `Y$$b.\r\n          `\"Y$b._\r\n              `\"\"\"\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6819/comments",
    "author": "ejohb",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-04-08T23:24:16Z",
        "body": "@EdwardJB great observation. \r\ndo you want to contribute a fix with your suggestion of checking the runtime error message?"
      }
    ]
  },
  {
    "number": 6804,
    "title": "'LightningShardedDataParallel' object has no attribute 'require_backward_grad_sync'",
    "created_at": "2021-04-03T05:05:10Z",
    "closed_at": "2021-04-10T16:14:37Z",
    "labels": [
      "bug",
      "help wanted",
      "distributed",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6804",
    "body": "When trying to use sharded DDP with the latest Lightning version 1.2.6 and Fairscale 0.3.3, I get the following error. Note that I am also using manual optimization and disabling automatic, as I am training a GAN.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_train.py\", line 41, in train\r\n    trainer.fit(model, data_loader)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\r\n    self.dispatch()\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\r\n    self._results = trainer.run_train()\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 657, in run_training_batch\r\n    self._curr_step_result = self.training_step(\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\r\n    training_step_output = self.trainer.accelerator.training_step(args)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 156, in training_step\r\n    return self.training_type_plugin.training_step(*args)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 297, in training_step\r\n    return self.model(*args, **kwargs)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/fairscale/nn/data_parallel/sharded_ddp.py\", line 223, in forward\r\n    return self.module(*inputs, **kwargs)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 48, in forward\r\n    output = self.module.training_step(*inputs, **kwargs)\r\n  File \"/home/timbrooks/code/human_gan/models/human_gan.py\", line 121, in training_step\r\n    self._discriminator_step(real_image, keypoints)\r\n  File \"/home/timbrooks/code/human_gan/models/human_gan.py\", line 182, in _discriminator_step\r\n    self.manual_backward(loss, optimizer)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1228, in manual_backward\r\n    self.trainer.train_loop.backward(loss, optimizer=None, opt_idx=None, *args, **kwargs)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 782, in backward\r\n    self.trainer.accelerator.backward(result, optimizer, opt_idx, should_accumulate, *args, **kwargs)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 254, in backward\r\n    self.training_type_plugin.pre_backward(closure_loss, should_accumulate, optimizer, optimizer_idx)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 283, in pre_backward\r\n    if not self.lightning_module.automatic_optimization and self.model.require_backward_grad_sync:\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/fairscale/nn/data_parallel/sharded_ddp.py\", line 362, in __getattr__\r\n    return getattr(self.module, name)\r\n  File \"/home/timbrooks/anaconda3/envs/human_gan_fairseq/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 947, in __getattr__\r\n    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\r\nAttributeError: 'LightningShardedDataParallel' object has no attribute 'require_backward_grad_sync'\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6804/comments",
    "author": "timothybrooks",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-04-08T23:42:25Z",
        "body": "@SeanNaren is the correct fix here to just not access `self.model.require_backward_grad_sync` in the sharded plugin?\r\n\r\n"
      }
    ]
  },
  {
    "number": 6798,
    "title": "Training stalls with DDP and iterable training dataset at validation step for any val_check_interval>1",
    "created_at": "2021-04-01T21:57:08Z",
    "closed_at": "2022-02-01T17:37:29Z",
    "labels": [
      "feature",
      "help wanted",
      "waiting on author",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6798",
    "body": "## 🐛 Bug\r\n\r\nTraining stalls with DDP and iterable training dataset at `validation step` for any `val_check_interval>1`. It works fine for `val_check_interval=1`. \r\n\r\n### To Reproduce\r\n\r\nHere is a toy example to reproduce: \r\n```\r\nimport numpy as np\r\nimport pybmi.torch as pt\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom torch import nn\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nfrom pytorch_lightning.loggers import TensorBoardLogger\r\nfrom torch.utils.data import IterableDataset, DataLoader, ChainDataset, get_worker_info\r\nimport torch.distributed as dist \r\n    \r\n\r\nclass MyIterableDataset(IterableDataset):\r\n    \r\n    def __init__(self, ds_list):\r\n        self.seq_len =  3\r\n        self.batch_size = 2\r\n        self.ds_list = ds_list\r\n        \r\n    def process_data(self, worker_ds):\r\n        l = []\r\n        for d in worker_ds:\r\n            l.append(d)\r\n        pdata = np.concatenate(l)\r\n        batch_size_total = self.batch_size * self.seq_len\r\n        n_batches = len(pdata)//batch_size_total\r\n        pdata = pdata[:n_batches * batch_size_total]\r\n        pdata = pdata.reshape((self.batch_size, -1)).astype(np.float32)\r\n        for n in range(0, pdata.shape[1], self.seq_len):\r\n            itr = pdata[:, n:n+self.seq_len] \r\n            yield itr\r\n    \r\n    def __iter__(self):\r\n        rank = dist.get_rank()\r\n        world_size = dist.get_world_size()\r\n        worker_info = get_worker_info()\r\n        total_chunks = world_size * worker_info.num_workers\r\n        chunk_id = (rank * worker_info.num_workers) + worker_info.id        \r\n        ds_list = np.array(self.ds_list,dtype = 'object')\r\n        split = np.array_split(ds_list, total_chunks)\r\n        return (self.process_data(split[chunk_id]))\r\n    \r\nclass TestDataset():\r\n    def __init__(self):   \r\n        self.maxlen = 40\r\n    def process_data(self,ds):\r\n        out = np.zeros([self.maxlen,len(ds)])\r\n        n = []\r\n        for ii,pdata in enumerate(ds):\r\n            n.append(len(pdata))\r\n            out[:n[-1],ii] = pdata\r\n        return out[:max(n),:].astype(np.float32)\r\n    \r\na = np.arange(20)*1.\r\nb = np.arange(23)*-1.\r\nc = np.arange(31)*0.01\r\nd = np.arange(17)*-.01\r\ndata = [a,b,c,d]\r\n\r\n\r\nclass PnRDataModule(pl.LightningDataModule):\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n        self.batch_size = None\r\n        self.val_batch_size = 3\r\n        self.num_workers = 2\r\n        \r\n    def setup(self, stage=None):\r\n        if stage == 'fit' or stage is None:\r\n            self.training_data = MyIterableDataset(data)\r\n            self.val_data = TestDataset().process_data(data)\r\n          \r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n            self.training_data,\r\n            batch_size=self.batch_size,\r\n            num_workers=self.num_workers,\r\n            pin_memory=True,\r\n        )\r\n    def val_dataloader(self): \r\n        return DataLoader(\r\n            self.val_data,\r\n            batch_size=self.val_batch_size,\r\n            num_workers=2,\r\n            pin_memory=True,\r\n            drop_last = True,\r\n        )\r\n\r\nclass Model(nn.Module):\r\n    \r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.ff1 = nn.Linear(1,100)\r\n        self.ff2 = nn.Linear(100,1)\r\n    def forward(self, x):\r\n        x1 = self.ff1(x)\r\n        y = self.ff2(x1)\r\n        return y\r\n    \r\n    \r\nclass LM(pl.LightningModule):\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model = Model()\r\n        self.criterion = torch.nn.MSELoss(reduction='mean')\r\n    def forward(self,x):\r\n        logits= self.model.forward(x)\r\n        return logits\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        batch = batch.unsqueeze(2)\r\n        batch = batch.permute([1,0,2])\r\n        logits = self.model.forward(batch)\r\n        loss = self.criterion(logits, batch) \r\n        self.log('train_loss', loss, prog_bar=True)\r\n        return  loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        logits = self.model.forward(batch.unsqueeze(2))\r\n        loss = self.criterion(logits.squeeze(2), batch) \r\n        self.log('val_loss', loss, prog_bar=True)\r\n        return loss\r\n    \r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\r\n        return optimizer\r\n\r\ndef main():\r\n\r\n    dm = PnRDataModule()\r\n    model = LM()\r\n    checkpoint_callback = ModelCheckpoint(dirpath='Models/MMD',monitor=\"val_loss\", mode=\"min\")\r\n    trainer = pl.Trainer(max_epochs=10, gpus=2,callbacks=[checkpoint_callback], accelerator='ddp',val_check_interval=2)\r\n    trainer.fit(model, dm)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### Expected behavior\r\n\r\nTraining to work with any `val_check_interval`. \r\n \r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.8.0 \r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version: 11.1\r\n - GPU models and configuration: 4x V100 GPU\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @tchaton",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6798/comments",
    "author": "farshchian",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-04-06T07:56:59Z",
        "body": "Dear @farshchian,\n\nDid you try running it with master ?\n\nBest,\nT.C"
      },
      {
        "user": "ethanwharris",
        "created_at": "2021-04-07T08:15:55Z",
        "body": "Hi @farshchian, I've done some digging and this bug is triggered because your data loader yields a different number of batches on each device. I'm not sure there's any way to fix that in lightning, we could maybe add a check to see if the steps match up?"
      },
      {
        "user": "farshchian",
        "created_at": "2021-04-07T17:35:39Z",
        "body": "Hi @ethanwharris . Thank you for following this up! The scenario, in which data loaders yields different number of batches on each device is likely to occur with iterbale datasets, since their predominant use case is to stream online data in which the length of data is not known in advance. I was wondering why this blocks the training for `val_check_intervals>1` but not for the intervals of 1? Can't the same logic applied here such that the training loop keeps track of instances where all data loaders report the `StopIteration` flag and then run the validation loop when the numbers of instances is equal to `val_check_intervals`?"
      },
      {
        "user": "ethanwharris",
        "created_at": "2021-04-08T10:21:04Z",
        "body": "Hi @farshchian, AFAICT the reason for the hang with `val_check_interval>1` is that one node tries to train while the other is validating so they fall out of sync, whereas when `val_check_interval=1` this doesn't happen as every train step is followed by val.\r\n\r\nRegarding a way to actually support this use case, I'm not sure it makes sense. The hanging is cause by the models on different nodes waiting to receive gradient infromation from eachother to aggregate and update weights. How does this work if only one model has gradient information for a step? At a minimum this would change the effective learning rate for a batch, but also the other nodes would just have to wait for these extra batches to complete? Let me know your thoughts :smiley:"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-05-04T12:38:09Z",
        "body": "I would say this issue here is essentially blocked by #3325 or would even be solved by this. Uneven inputs is possible with the no_sync context manager in pure pytorch, but during this phase no collective ops are allowed. This is a problem because many features in Lightning rely on it, including torchmetrics are not be aware of a no_sync context. At the moment I do not know a good approach. Needs more brainstorming."
      },
      {
        "user": "tchaton",
        "created_at": "2021-07-19T17:32:11Z",
        "body": "Hey @ananthsub @awaelchli,\r\n\r\nI started to investigate adding support for uneven number of batches.\r\nFrom my first observations, Lightning is getting blocked on moving data to the gpus or accessing the loss.\r\nI couldn't reproduce the behaviour with the simple model.join.\r\n\r\nHere is the test I started to build on:\r\n\r\n```py\r\n@RunIf(min_gpus=2, special=True)\r\ndef test_uneven_batches_ddp(tmpdir):\r\n\r\n    from torch.distributed.distributed_c10d import get_rank\r\n\r\n    class CustomDataset(IterableDataset):\r\n\r\n        def __init__(self, size, length):\r\n            self.len = length\r\n            self.data = torch.randn(length, size)\r\n\r\n        def __iter__(self):\r\n            self.count = 0\r\n            return self\r\n\r\n        def __next__(self):\r\n            if self.count >= self.len:\r\n                raise StopIteration\r\n            data = self.data[self.count]\r\n            self.count += 1\r\n            return data\r\n\r\n    class UnevenDataModule(LightningDataModule):\r\n\r\n        def train_dataloader(self):\r\n            return DataLoader(CustomDataset(32, 2 + 5 * get_rank()), pin_memory=True)\r\n\r\n        def val_dataloader(self):\r\n            return DataLoader(CustomDataset(32, 2 + 5 * get_rank()))\r\n\r\n\r\n    class DebugModel(BoringModel):\r\n\r\n        def training_step(self, batch, batch_idx):\r\n            print(self.trainer.global_rank, batch_idx)\r\n            loss = super().training_step(batch, batch_idx)\r\n            print(loss)\r\n            return loss\r\n\r\n\r\n    model = DebugModel()\r\n    dm = UnevenDataModule()\r\n    dm.val_dataloader = None\r\n    model.val_dataloader = None\r\n    trainer = Trainer(\r\n        default_root_dir=tmpdir,\r\n        max_epochs=2,\r\n        accelerator=\"ddp\",\r\n        gpus=2,\r\n        limit_val_batches=0,\r\n    )\r\n    trainer.fit(model, dm)\r\n```\r\n\r\n```py\r\nfrom pytorch_lightning.core import optimizer\r\nimport torch\r\nfrom torch import nn\r\nimport torch.distributed as dist\r\nimport torch.multiprocessing as mp\r\nfrom time import sleep\r\nfrom torch.optim import Adam\r\nimport tests.helpers.utils as tutils\r\ndef _setup_ddp(rank, worldsize):\r\n    import os\r\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\r\n    # initialize the process group\r\n    dist.init_process_group(\"nccl\", rank=rank, world_size=worldsize)\r\ndef _ddp_test_fn(rank, worldsize):\r\n    _setup_ddp(rank, worldsize)\r\n    torch.cuda.set_device(rank)\r\n    _model = nn.Linear(1, 1, bias=False).to(rank)\r\n    model = torch.nn.parallel.DistributedDataParallel(\r\n        _model, device_ids=[rank], output_device=rank\r\n    )\r\n    optimizer = Adam(model.parameters())\r\n    # Rank 1 gets one more input than rank 0.\r\n    inputs = [torch.tensor([1]).float() for _ in range(1 + rank)]\r\n    with model.join():\r\n        for inp in inputs:\r\n            print(\"starting\", rank)\r\n            loss = model(inp).sum()\r\n            loss.backward()\r\n            print(rank, loss)\r\n            optimizer.step()\r\n    print(rank, _model.weight)\r\nif __name__ == '__main__':\r\n    tutils.set_random_master_port()\r\n    worldsize = 2\r\n    mp.spawn(_ddp_test_fn, args=(worldsize, ), nprocs=worldsize)\r\n```\r\n"
      },
      {
        "user": "carmocca",
        "created_at": "2022-02-01T17:37:29Z",
        "body": "Closing in favor of the feature request that would support this: #3325"
      }
    ]
  },
  {
    "number": 6778,
    "title": "No TPU devices were found.",
    "created_at": "2021-04-01T05:38:18Z",
    "closed_at": "2021-04-02T05:16:18Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6778",
    "body": "Thanks for great framework.  \r\nI tried to train with tpu (Google Cloud Platform Environment). I encounter error like this:\r\n```\r\nkaki_ai@kaki-ins:~/kopite-bot$ python3 train_blender.py\r\n16:14:31 | Overriding opt[\"no_cuda\"] to True (previously: False)\r\n16:14:31 | Loading model with `--beam-block-full-context false`\r\n16:14:31 | loading dictionary from /home/kaki_ai/ParlAI/data/models/blender/blender_90M/model.dict\r\n16:14:31 | num words = 54944\r\n16:14:32 | DEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\r\n16:14:33 | Total parameters: 87,508,992 (87,508,992 trainable)\r\n16:14:33 | Loading existing model params from /home/kaki_ai/ParlAI/data/models/blender/blender_90M/model\r\nTraceback (most recent call last):\r\n  File \"train_blender.py\", line 47, in <module>\r\n    val_dataloader=test_loader,\r\n  File \"/home/kaki_ai/kopite-bot/training/lightning_base.py\", line 135, in fit\r\n    accumulate_grad_batches=self.accumulate_grad_batches,\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 39, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 321, in __init__\r\n    replace_sampler_ddp, deterministic, precision, amp_backend, amp_level, plugins\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 91, in __init__\r\n    self.tpu_cores = device_parser.parse_tpu_cores(tpu_cores)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/device_parser.py\", line 113, in parse_tpu_cores\r\n    raise MisconfigurationException('No TPU devices were found.')\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: No TPU devices were found.\r\n```\r\n\r\nIf you have any doubts, please help me. Thank you!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6778/comments",
    "author": "sooftware",
    "comments": [
      {
        "user": "kaushikb11",
        "created_at": "2021-04-01T06:50:27Z",
        "body": "Hi @sooftware,\r\nDid you export the env variables required to connect to TPU? TPU_IP_ADDRESS & XRT_TPU_CONFIG.\r\n\r\nIf yes, could you try the master and update us on it? We pushed an update last night #6767. Thanks!"
      },
      {
        "user": "sooftware",
        "created_at": "2021-04-01T06:57:19Z",
        "body": "Thank you! I will try."
      },
      {
        "user": "kaushikb11",
        "created_at": "2021-04-01T07:01:14Z",
        "body": "Haha, don't close the issue. Would like to know if it's working as expected. Thanks!"
      },
      {
        "user": "sooftware",
        "created_at": "2021-04-01T07:03:33Z",
        "body": "@kaushikb11 Ah Okay! Whether it works or not, I'll leave the results here!"
      },
      {
        "user": "sooftware",
        "created_at": "2021-04-02T00:22:40Z",
        "body": "I set `export XRT_TPU_CONFIG=\"tpu_worker;0;$TPU_IP_ADDRESS:xxxx\"` and It works. Thank you!"
      }
    ]
  },
  {
    "number": 6615,
    "title": "Calling trainer.test() when using fast_dev_run throws confusing error",
    "created_at": "2021-03-21T12:14:02Z",
    "closed_at": "2021-03-29T13:29:55Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6615",
    "body": "## 🐛 Bug\r\n\r\nCalling trainer.test() when using fast_dev_run throws confusing error:\r\n```bash\r\nTraceback (most recent call last):                                                                                                                                                                 \r\n  File \"main.py\", line 89, in <module>\r\n    trainer.test(test_dataloaders=test)\r\n  File \"/home/ash/miniconda3/envs/tmp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 916, in test\r\n    results = self.__test_using_best_weights(ckpt_path, test_dataloaders)\r\n  File \"/home/ash/miniconda3/envs/tmp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 927, in __test_using_best_weights\r\n    raise MisconfigurationException(\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: ckpt_path is \"best\", but ModelCheckpoint is not configured to save the best model.\r\n```\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n```python\r\n\r\nfrom pytorch_lightning import LightningModule\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, num_samples):\r\n        self.len = num_samples\r\n        self.data = torch.randn(num_samples, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x['x'] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        self.log('fake_test_acc', loss)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\nnum_samples = 10000\r\n\r\ntrain = RandomDataset(32, num_samples)\r\ntrain = DataLoader(train, batch_size=32)\r\n\r\nval = RandomDataset(32, num_samples)\r\nval = DataLoader(val, batch_size=32)\r\n\r\ntest = RandomDataset(32, num_samples)\r\ntest = DataLoader(test, batch_size=32)\r\n\r\nmodel = BoringModel()\r\n\r\ntrainer = pl.Trainer(\r\n    fast_dev_run=True\r\n)\r\n\r\ntrainer.fit(model, train, val)\r\n\r\ntrainer.test(test_dataloaders=test)\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6615/comments",
    "author": "ashleve",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-03-21T14:45:03Z",
        "body": "Dear @hobogalaxy,\r\n\r\nThis happens because trainer.fit didn't generate a checkpoint and you don't provide model to trainer.test.\r\nTherefore, it doesn't find a model to load.\r\nThis is expected behaviour but I agree the MisconfigurationException could be more explicit on what's wrong.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "ashleve",
        "created_at": "2021-03-21T15:07:07Z",
        "body": "@tchaton Thank you for explanation. It is confusing though and it probably happens to many users. I think lightning should detect the case when fast_dev_run is set to True and either:\r\n1. Throw python warning instead of exception and simply not execute testing\r\n2. Raise exception that informs explicitly you shouln't call trainer.test() when using fast_dev_run"
      },
      {
        "user": "Borda",
        "created_at": "2021-03-23T13:05:09Z",
        "body": "@hobogalaxy mind send a PR with one of your suggestions?"
      },
      {
        "user": "carmocca",
        "created_at": "2021-03-23T22:00:36Z",
        "body": "I think I prefer (2) as it is more explicit"
      }
    ]
  },
  {
    "number": 6606,
    "title": "[RFC] Let Logger base class inherit from Callback",
    "created_at": "2021-03-20T18:20:02Z",
    "closed_at": "2022-02-17T17:58:39Z",
    "labels": [
      "feature",
      "help wanted",
      "logger",
      "callback"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6606",
    "body": "## 🚀 Feature\r\n\r\nWe could consider having loggers inherit the Callback base class.\r\n\r\n### Motivation\r\n\r\nThis would enable third party loggers to perform additional features during the progression of training.\r\nOne example is uploading checkpoint files in WandB: #6231\r\nIt would make it significantly easier to add these features without tightly coupling them to the rest of the Trainer logic.\r\n\r\nAlso recently there was a feature request to add state persistence to loggers #6361. We would face similar challenges as we have with callbacks. But if we allow loggers to implement the callback methods, and when we solve #6467, then this would also enable #6361 to move forward without problem.\r\n\r\n### Pitch\r\n\r\n```python\r\n# now\r\nclass LightningLoggerBase(ABC):\r\n\r\n# pitch\r\nclass LightningLoggerBase(Callback, ABC):\r\n```\r\n\r\n\r\n### Alternatives\r\n\r\nKeep Loggers as they are. For new features, we would have to introduce special new hook just for the logger.\r\n\n\ncc @borda @awaelchli @edward-io @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy @daniellepintz",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6606/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-03-21T09:33:20Z",
        "body": "Sound like a good idea ! Do you see any cons ?"
      },
      {
        "user": "ananthsub",
        "created_at": "2021-03-22T01:40:00Z",
        "body": "by implementing it as a callback, some question emerge like:\r\n- do we need to re-order callback execution to handle loggers now? what if other callbacks want to log through the module? \r\n- would this make loggers harder to use outside of lightning? "
      },
      {
        "user": "borisdayma",
        "created_at": "2021-03-24T17:10:50Z",
        "body": "I think it's possible loggers would have to occur last.\r\n\r\nAlso it would be good to take advantage to add a `after_save_checkpoint` method that happens **after** saving the model to upload it (`on_save` happens before)."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-04-24T06:43:52Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6575,
    "title": "trainer.test() fails when using both auto_lr_find and ModelPruning",
    "created_at": "2021-03-18T05:56:25Z",
    "closed_at": "2021-05-15T22:47:38Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6575",
    "body": "## 🐛 Bug\r\n\r\nHi, wasn't able to reproduce properly with BoringModel, but did with small CIFAR10 example.\r\n\r\n#### Description\r\n\r\n`trainer.test()` errors out when I use both `ModelPruning` callback and `auto_lr_find`. Disabling either of these makes `trainer.test()` work again. I'm using mainline of pytorch-lightning.\r\n\r\n#### Example\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchvision.datasets import CIFAR10\r\nfrom torchvision import transforms\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.metrics.functional import accuracy\r\nfrom pytorch_lightning.callbacks import ModelPruning\r\n\r\nclass LitCIFAR10(pl.LightningModule):\r\n    \r\n    def __init__(self, data_dir='./', hidden_size=64, learning_rate=2e-4, batch_size=128):\r\n\r\n        super().__init__()\r\n\r\n        # Set our init args as class attributes\r\n        self.data_dir = data_dir\r\n        self.hidden_size = hidden_size\r\n        self.learning_rate = learning_rate\r\n        self.bs = batch_size\r\n\r\n        # Hardcode some dataset specific attributes\r\n        self.num_classes = 10\r\n        self.dims = (3, 32, 32)\r\n        channels, width, height = self.dims\r\n        mean = [0.4913997551666284, 0.48215855929893703, 0.4465309133731618]\r\n        std = [0.24703225141799082, 0.24348516474564, 0.26158783926049628]\r\n        self.transform = transforms.Compose([\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean, std)\r\n        ])\r\n\r\n        # Define PyTorch model\r\n        self.model = nn.Sequential(\r\n            nn.Flatten(),\r\n            nn.Linear(channels * width * height, hidden_size),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(hidden_size, hidden_size),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.1),\r\n            nn.Linear(hidden_size, self.num_classes)\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.model(x)\r\n        return F.log_softmax(x, dim=1)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x)\r\n        loss = F.nll_loss(logits, y)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x)\r\n        loss = F.nll_loss(logits, y)\r\n        preds = torch.argmax(logits, dim=1)\r\n        acc = accuracy(preds, y)\r\n\r\n        # Calling self.log will surface up scalars for you in TensorBoard\r\n        self.log('val_loss', loss, prog_bar=True)\r\n        self.log('val_acc', acc, prog_bar=True)\r\n        return loss\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        # Here we just reuse the validation_step for testing\r\n        return self.validation_step(batch, batch_idx)\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\r\n        return optimizer\r\n\r\n    ####################\r\n    # DATA RELATED HOOKS\r\n    ####################\r\n\r\n    def prepare_data(self):\r\n        # download\r\n        return\r\n        CIFAR10(self.data_dir, train=True, download=True)\r\n        CIFAR10(self.data_dir, train=False, download=True)\r\n\r\n    def setup(self, stage=None):\r\n\r\n        # Assign train/val datasets for use in dataloaders\r\n        if stage == 'fit' or stage is None:\r\n            cifar10_full = CIFAR10(self.data_dir, train=True, transform=self.transform)\r\n            self.cifar10_train, self.cifar10_val = random_split(cifar10_full, [45000, 5000])\r\n\r\n        # Assign test dataset for use in dataloader(s)\r\n        if stage == 'test' or stage is None:\r\n            self.cifar10_test = CIFAR10(self.data_dir, train=False, transform=self.transform)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.cifar10_train, batch_size=self.bs, num_workers=os.cpu_count())\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.cifar10_val, batch_size=self.bs, num_workers=os.cpu_count())\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.cifar10_test, batch_size=self.bs, num_workers=os.cpu_count())\r\n    \r\n\r\nmodel = LitCIFAR10()\r\n\r\nprune = ModelPruning(\r\n            pruning_fn='l1_unstructured',\r\n            parameter_names=['weight', 'bias'],\r\n            amount=0.02,\r\n            use_global_unstructured=True,\r\n        )\r\n\r\ntrainer = pl.Trainer(gpus=1, max_epochs=2, \r\n                     auto_lr_find=True, # either comment this line out,\r\n                     callbacks=[prune] # or comment this line out\r\n                    )\r\n\r\ntrainer.tune(model)\r\ntrainer.fit(model)\r\ntrainer.test() # and this will then work\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6575/comments",
    "author": "austinmw",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-03-21T09:40:04Z",
        "body": "Dear @austinmw,\r\n\r\nWould you mind sharing the traceback ?\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "austinmw",
        "created_at": "2021-03-22T20:32:21Z",
        "body": "@tchaton Sure, traceback:\r\n> RuntimeError: Error(s) in loading state_dict for LitCIFAR10:\r\n> \tMissing key(s) in state_dict: \"model.1.weight_orig\", \"model.1.bias_orig\", \"model.1.weight_mask\", \"model.1.bias_mask\", \"model.4.weight_orig\", \"model.4.bias_orig\", \"model.4.weight_mask\", \"model.4.bias_mask\", \"model.7.weight_orig\", \"model.7.bias_orig\", \"model.7.weight_mask\", \"model.7.bias_mask\". \r\n> \tUnexpected key(s) in state_dict: \"model.1.weight\", \"model.1.bias\", \"model.4.weight\", \"model.4.bias\", \"model.7.weight\", \"model.7.bias\". \r\n\r\nAlso, semi-unrelated, is it intended that the `trainer.tune()` step for `auto_lr_find` counts as one max_epoch step for the subsequent `trainer.fit()`?\r\n"
      },
      {
        "user": "lyra-victor",
        "created_at": "2021-04-07T23:28:53Z",
        "body": "I had a similar issue and noticed when I put a `breakpoint()` in `trainer.fit` as the first line in the function and inspect `self.callbacks`, the only callback in there is a `pytorch_lightning.tuner.lr_finder._LRCallback object` so it is overwriting any callback you pass in to the trainer initialization and not executing those."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-05-08T14:02:40Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6544,
    "title": "Random job failures caused by the CheckpointConnector on slurm managed hpc",
    "created_at": "2021-03-16T09:39:35Z",
    "closed_at": "2021-05-24T13:34:30Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0",
      "waiting on author",
      "checkpointing",
      "environment: slurm"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6544",
    "body": "## 🐛 Bug\r\n\r\nI recently created a small machine learning code using `Pytorch-lightning and Optuna`. The code allows to train and hyper-optimise a number of pre-selected models for a particular purpose. The Optuna hyper-parameter optimisation trials are stored in sqlite file (this is one of the storage options allowed by Optuna). Everything works fine if I run it on a non-slurm managed computing system. However, the moment I run it on slurm platform, my jobs crash randomly (sometimes they run to completion and sometimes they crash straight away). I have checked that it has nothing to do with how many parallel jobs I run, as even a single job can crash. Examining the Python traceback (please see the traceback below), I could see that the issue happens in the `trainer/connectors/checkpoint_connector.py` module. Namely, in the `CheckpointConnector.restore_weights` method at the following line of code:\r\n```python\r\ndid_restore_hpc_weights = self.restore_hpc_weights_if_needed(model)\r\n```\r\nThe exact error message says that no database journal file can be found. I am no sqlite expert, but I think the journal files are created during transactions as a backup option in case something goes wrong and are destroyed afterwards. I am not sure why pytorch-ligthning tries to use them. Apart from that, I have noticed that If I run a number of jobs in parallel, where the jobs are disjoint and each using a different sqlite file, if one job crashes the other crash as well. This is very weird given that the jobs should be completely separate. I managed to find a workaround for this issue by modifying the `CheckpointConnector.restore_weights` method as follows:\r\n```python\r\n        # if script called from hpc resubmit, load weights\r\n        #did_restore_hpc_weights = self.restore_hpc_weights_if_needed(model)\r\n        did_restore_hpc_weights = False\r\n```\r\n\r\nThis seems to fix the random jobs crash problem, though I am not sure if there are any side-effects. Howevr, all my jobs train and optimise fine with the above addition. I would welcome any comments, it could be that the actual issue is located elsewhere (e.g. Optuna), but I thought it is important to post it here. Please see below additional details:\r\n\r\n- OS - Linux\r\n- conda environment:\r\n    - name: osc_env\r\n    - channels:\r\n        - rdkit\r\n       - defaults\r\n       - pytorch\r\n       - dglteam\r\n    - dependencies:\r\n        - python=3.7.7\r\n        - pytorch=1.6.0\r\n        - torchvision=0.7.0\r\n        - torchaudio=0.6.0\r\n        - cudatoolkit=10.2\r\n        - rdkit=2020.09.1.0\r\n        - dgl-cuda10.2=0.5.2\r\n        - pip=20.2.4\r\n    - pip:\r\n        - pytorch_lightning==1.0.4\r\n        - scikit-learn==0.23.2\r\n        - tqdm==4.51.0\r\n        - pyyaml==5.3.1\r\n        - networkx==2.5\r\n        - matplotlib==3.3.2\r\n        - seaborn==0.11.0\r\n       - optuna==2.3.0\r\n       - dgllife==0.2.6\r\n\r\n\r\n- traceback\r\n```\r\n2021-03-16 09:24:59,517 ERROR    finished with exception - optunawrapper.py 111  start_hpo root [[MainThread]]\r\nTraceback (most recent call last):\r\n  File \"/rds/user/dln22/hpc-work/ML_jobs/ZhouLiML-git/oscml/hpo/optunawrapper.py\", line 91, in start_hpo\r\n    gc_after_trial=True)\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/optuna/study.py\", line 315, in optimize\r\n    show_progress_bar=show_progress_bar,\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/optuna/_optimize.py\", line 114, in _optimize\r\n    for _ in _iter\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/joblib/parallel.py\", line 1061, in __call__\r\n    self.retrieve()\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/joblib/parallel.py\", line 940, in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/multiprocessing/pool.py\", line 657, in get\r\n    raise self._value\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/joblib/parallel.py\", line 263, in __call__\r\n    for func, args, kwargs in self.items]\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/joblib/parallel.py\", line 263, in <listcomp>\r\n    for func, args, kwargs in self.items]\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/optuna/_optimize.py\", line 156, in _optimize_sequential\r\n    trial = _run_trial(study, func, catch)\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/optuna/_optimize.py\", line 189, in _run_trial\r\n    value = func(trial)\r\n  File \"/rds/user/dln22/hpc-work/ML_jobs/ZhouLiML-git/oscml/hpo/optunawrapper.py\", line 25, in decorator\r\n    raise exc\r\n  File \"/rds/user/dln22/hpc-work/ML_jobs/ZhouLiML-git/oscml/hpo/optunawrapper.py\", line 16, in decorator\r\n    value = objective(trial)\r\n  File \"/rds/user/dln22/hpc-work/ML_jobs/ZhouLiML-git/oscml/hpo/objective.py\", line 230, in objective\r\n    trial, trial_number, total_number_trials, str(cv_index))\r\n  File \"/rds/user/dln22/hpc-work/ML_jobs/ZhouLiML-git/oscml/hpo/objective.py\", line 107, in fit_or_test\r\n    trainer.fit(model, train_dataloader=train_dl, val_dataloaders=val_dl)\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 440, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/pytorch_lightning/accelerators/cpu_accelerator.py\", line 45, in train\r\n    self.trainer.train_loop.setup_training(model)\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 165, in setup_training\r\n    self.trainer.checkpoint_connector.restore_weights(model)\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 67, in restore_weights\r\n    did_restore_hpc_weights = self.restore_hpc_weights_if_needed(model)\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\", line 198, in restore_hpc_weights_if_needed\r\n    files = [os.path.basename(f['name']) for f in fs.listdir(folderpath)]\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/fsspec/spec.py\", line 1081, in listdir\r\n    return self.ls(path, detail=detail, **kwargs)\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/fsspec/implementations/local.py\", line 50, in ls\r\n    return [self.info(f) for f in paths]\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/fsspec/implementations/local.py\", line 50, in <listcomp>\r\n    return [self.info(f) for f in paths]\r\n  File \"/home/dln22/.conda/envs/osc_env/lib/python3.7/site-packages/fsspec/implementations/local.py\", line 60, in info\r\n    out = os.stat(path, follow_symlinks=False)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/rds/user/dln22/hpc-work/ML_jobs/ZhouLiML-git/m3_atfp_hopv15.db-journal'\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6544/comments",
    "author": "dln22",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-04-18T21:51:10Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "tchaton",
        "created_at": "2021-04-19T14:57:14Z",
        "body": "Dear @dln22,\n\nCould you share a reproducible script which fails sometimes with the BoringModel ?\n\nBest,\nT.C"
      },
      {
        "user": "edenlightning",
        "created_at": "2021-05-24T13:34:30Z",
        "body": "Please feel free to reopen with a reproducible example."
      },
      {
        "user": "championsnet",
        "created_at": "2024-10-07T15:31:42Z",
        "body": "I am getting the same error occasionally. Did you find any solutions?"
      }
    ]
  },
  {
    "number": 6519,
    "title": "relative refresh rate in progress bar",
    "created_at": "2021-03-15T07:18:15Z",
    "closed_at": "2021-04-26T05:39:24Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6519",
    "body": "## 🚀 Feature\r\n\r\nDynamic refresh rate, define float as percent and default 0.01\r\n\r\n### Motivation\r\n\r\nas a user refresh, every percent of progress is mostly enough\r\n\r\n### Pitch\r\n\r\nThe validation in the notebook never triggers progress as default is 20 and this process has only 2 steps\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6519/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-04-18T21:51:10Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6517,
    "title": "Error on example model",
    "created_at": "2021-03-15T01:03:28Z",
    "closed_at": "2021-06-06T14:52:56Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6517",
    "body": "## 🐛 Bug\r\n\r\nI receive on every GAN model that I try to execute.\r\n\r\n```\r\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \r\nmaking sure all `forward` function outputs participate in calculating loss. \r\nIf you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\r\n```\r\n\r\nI tried compete reinstall of pytorch in different conda environment, tried both 1.8.0 and 1.9.0.dev\r\nI get error on both GAN example and my custom model.\r\nMy custom model has only 1 parameter (each of the networks), that I multiply with input, but it gives error too.\r\n\r\n### To Reproduce\r\n\r\nGAN model from examples\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\nPyTorch 1.8.0 (or latest dev)\r\nPyTorch-Lightning 1.2.3\r\n\r\n### Additional context\r\n\r\nI run on 2 Nvidia V100 gpus in ddp mode\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6517/comments",
    "author": "denix56",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-03-15T20:57:38Z",
        "body": "Fixed by #6460?"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-03-15T21:39:40Z",
        "body": "`python pl_examples/domain_templates/generative_adversarial_net.py --num_processes 2 --accelerator ddp_cpu`\r\n\r\nfails on 1.2.3\r\nworks on master\r\n\r\nfix will be released as part of the bug release this week\r\n\r\nworkaround for you until then:\r\n\r\n`Trainer(..., plugins=[DDPPlugin(find_unused_parameters=True)])`"
      },
      {
        "user": "denix56",
        "created_at": "2021-03-26T23:02:41Z",
        "body": "@awaelchli \r\nI have lightning 1.2.5 and the bug still exists"
      },
      {
        "user": "MarsSu0618",
        "created_at": "2021-03-28T03:01:54Z",
        "body": "Hi, @awaelchli\r\nI encounter the same problem.\r\n```\r\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /opt/conda/conda-bld/pytorch_1579022060824/work/torch/csrc/distributed/c10d/reducer.cpp:514)\r\n```\r\nMy environment:\r\n```\r\npytorch: 1.4\r\npytorch-lightning: 1.1.1\r\n```\r\nSo can i upgrade version to solve it?"
      },
      {
        "user": "tchaton",
        "created_at": "2021-03-29T08:52:50Z",
        "body": "Dear @denix56, @MarsSu0618,\r\n\r\nWould you mind sharing a reproducible example for us to debug this behaviour ?\r\n\r\nAnd please, update to PyTorch 1.6 / master on Lightning.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "Borda",
        "created_at": "2021-04-01T12:05:24Z",
        "body": "Hi @denix56, @MarsSu0618 could you check with the latest 1.2.6?\r\nAlso, I would not recommend relying on PT 1.9dev as it is not a stable release"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-27T12:49:48Z",
        "body": "Would it make sense to catch this PyTorch warning and replace it with a Lightning-friendly warning?\nFor PyTorch users it is clear that one has to set the flag in DDP but for Lightning users this is hidden away, and they can only know by going to our documentation. \n\nWe can catch the warning in forward in the wrapper, then replace it with a simple instruction to set the plugin parameter.\nWhat do you think @SeanNaren @ananthsub @tchaton (tagging some random people who may know what I'm talking about?)"
      },
      {
        "user": "SeanNaren",
        "created_at": "2021-04-27T12:58:13Z",
        "body": "Considering `find_unused_parameters` is now True by default, this should solve the issue here right @awaelchli?\r\n\r\nThere is a warning printed now since `find_unused_parameters` is set to True by default, which suggests to people to turn it off. If we could catch this and tell people to use `accelerator=ddp_find_unused_parameters_false` instead that would be great  (once #7224 is merged) :) "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-05-30T13:53:10Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6508,
    "title": "trainer.predict method should return iterator instead of list",
    "created_at": "2021-03-13T19:44:32Z",
    "closed_at": "2021-04-26T05:39:21Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6508",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\ntrainer.predict method should return iterator instead of a list of predictions to avoid out of memory error\r\n\r\n### Motivation\r\nreturning list of predictions can easily cause out of memory on GPUs\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nwrap the trainer.predict method to an iterator, so getting predictions can be something like:\r\n\r\n```\r\nfor pred in trainer.predict(model, datamodule=dm):\r\n    # do something with pred\r\n```\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6508/comments",
    "author": "csarron",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-04-18T21:51:13Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "thomasahle",
        "created_at": "2022-04-07T19:51:34Z",
        "body": "This would be very useful for those of us that can't fit the entire predicted dataset in memory."
      },
      {
        "user": "ValeKnappich",
        "created_at": "2022-11-21T13:57:35Z",
        "body": "I find it incredible that this has not been given more attention. Wouldn't this be a significant improvement with very little effort? "
      }
    ]
  },
  {
    "number": 6481,
    "title": "LayerSummary does not work with ScriptModules",
    "created_at": "2021-03-11T16:03:41Z",
    "closed_at": "2021-03-15T02:17:42Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6481",
    "body": "## 🐛 Bug\r\n\r\nI am trying to do finetuning on a pre-trained model which is saved as TorchScript. Unfortunately, it looks like Lightning's `LayerSummary` does not support scripted modules:\r\n\r\n### To Reproduce\r\n\r\nRun\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass Module(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.linear = torch.jit.script(nn.Linear(5, 1))   # Notice the scripting!\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.linear.parameters())\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self.linear(batch).sum()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    m = Module()\r\n\r\n    datasets = [torch.rand([5]) for __ in range(100)]\r\n    train_loader = torch.utils.data.DataLoader(datasets, batch_size=8)\r\n\r\n    trainer = pl.Trainer(\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n    )\r\n    trainer.fit(m, train_loader)\r\n```\r\n\r\nfails with\r\n```\r\nTraceback (most recent call last):\r\n  File \"scratch.py\", line 29, in <module>\r\n    trainer.fit(m, train_loader)\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 513, in fit\r\n    self.dispatch()\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 553, in dispatch\r\n    self.accelerator.start_training(self)\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 74, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 111, in start_training\r\n    self._results = trainer.run_train()\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 609, in run_train\r\n    self._pre_training_routine()\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 595, in _pre_training_routine\r\n    ref_model.summarize(mode=self.weights_summary)\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\", line 1456, in summarize\r\n    model_summary = ModelSummary(self, mode=mode)\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/memory.py\", line 184, in __init__\r\n    self._layer_summary = self.summarize()\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/memory.py\", line 236, in summarize\r\n    summary = OrderedDict((name, LayerSummary(module)) for name, module in self.named_modules)\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/memory.py\", line 236, in <genexpr>\r\n    summary = OrderedDict((name, LayerSummary(module)) for name, module in self.named_modules)\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/memory.py\", line 67, in __init__\r\n    self._hook_handle = self._register_hook()\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/memory.py\", line 91, in _register_hook\r\n    return self._module.register_forward_hook(hook)\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/torch/jit/_script.py\", line 723, in fail\r\n    raise RuntimeError(name + \" is not supported on ScriptModules\")\r\nRuntimeError: register_forward_hook is not supported on ScriptModules\r\nException ignored in: <bound method LayerSummary.__del__ of <pytorch_lightning.core.memory.LayerSummary object at 0x7f5815a7a9e8>>\r\nTraceback (most recent call last):\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/memory.py\", line 72, in __del__\r\n    self.detach_hook()\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/memory.py\", line 98, in detach_hook\r\n    if self._hook_handle is not None:\r\nAttributeError: 'LayerSummary' object has no attribute '_hook_handle'\r\n```\r\n\r\n### Expected behavior\r\n\r\nThis should work as if I had just done `nn.Linear`.\r\n\r\nFor now, I can work around this by setting `weight_summary=None`.\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.8.0\r\n\t- pytorch-lightning: 1.2.2\r\n\t- tqdm:              4.59.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.6.10\r\n\t- version:           #1 SMP Fri Feb 26 16:21:30 UTC 2021\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6481/comments",
    "author": "alanhdu",
    "comments": [
      {
        "user": "alanhdu",
        "created_at": "2021-03-11T16:33:12Z",
        "body": "```python\r\nimport torch\r\nfrom torch import nn\r\n\r\nimport pytorch_lightning as pl\r\n\r\npl.core.memory.LayerSummary(torch.jit.script(nn.Linear(5, 1)))\r\n```\r\n\r\nis an even shorter reproducer failing with\r\n```\r\nTraceback (most recent call last):\r\n  File \"scratch.py\", line 6, in <module>\r\n    pl.core.memory.LayerSummary(torch.jit.script(nn.Linear(5, 1)))\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/memory.py\", line 67, in __init__\r\n    self._hook_handle = self._register_hook()\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/memory.py\", line 91, in _register_hook\r\n    return self._module.register_forward_hook(hook)\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/torch/jit/_script.py\", line 723, in fail\r\n    raise RuntimeError(name + \" is not supported on ScriptModules\")\r\nRuntimeError: register_forward_hook is not supported on ScriptModules\r\nException ignored in: <bound method LayerSummary.__del__ of <pytorch_lightning.core.memory.LayerSummary object at 0x7f038e71b3c8>>\r\nTraceback (most recent call last):\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/memory.py\", line 72, in __del__\r\n    self.detach_hook()\r\n  File \"/home/alandu/miniconda3/envs/ctrldev/lib/python3.6/site-packages/pytorch_lightning/core/memory.py\", line 98, in detach_hook\r\n    if self._hook_handle is not None:\r\nAttributeError: 'LayerSummary' object has no attribute '_hook_handle'\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-03-14T05:46:46Z",
        "body": "Thanks for reporting, didn't think of this special case :) \r\n\r\nWe can unfortunately not support layer summary for script modules, because scripted modules don't allow attaching hooks (see error message: register_forward_hook is not supported on ScriptModules) and therefore we can't track the in- and outputs.\r\n\r\nHowever, I can make it so that no error occurs and the rest of the summary shows the in-/out sizes correctly. "
      }
    ]
  },
  {
    "number": 6447,
    "title": "Drop to REPL on SIGINT (control-c)",
    "created_at": "2021-03-09T20:56:48Z",
    "closed_at": "2021-05-18T10:32:50Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "design",
      "callback"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6447",
    "body": "## 🚀 Feature\r\nDuring a training run, if the user sends SIGINT (e.g. control-c) drop to a shell (ipython would be amazing)\r\n\r\n### Motivation\r\n\r\nSometimes I want to stop training right now and do something, probably save an explicitly named checkpoint. I know it's not \"the right way\" to work, but it is what I (and others?) do or want to do in practice. \r\n\r\nAs an example, I want to stop training now to try something else, but I want a snapshot of my model at its current state, that is named so I can find it later easily. \r\n\r\n\r\n### Additional context\r\n\r\nThis would be gross to implement. I imagine it would involve python threads, where a thread is listening for sigint and sets a flag. Then somewhere around on_training_batch_end, if the flag is set we drop to a shell (don't know how) \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6447/comments",
    "author": "talolard",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-03-10T20:29:07Z",
        "body": "This is a cool idea!\r\n\r\nIf/when #3632 is resurrected, it could be implemented as a callback that registers itself on the ctrl+c signal. But unfortunately, I don't think this will be ready soon\r\n\r\ncc: @awaelchli "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-04-10T05:54:53Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-05-10T14:08:17Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6442,
    "title": "Hyperparameter control panel for training",
    "created_at": "2021-03-09T15:57:32Z",
    "closed_at": "2021-04-18T21:51:26Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6442",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nExtending the integration of `argparser` to a full fledged GUI web-based control panel. This can enable dynamic tuning even during model training. \r\nFeatures:\r\n* in-situ modification on hyperparameters:\r\n    * the most important one - learning rate\r\n    * boolean attributes of a model that control dataflow\r\n    * ...\r\n* web-based tensorboard-like GUIs for modifying hyperparameters, tracking modifications and training controls.\r\n* auto checkpoint before adopting new parameters \r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nA typical use case I met is to modify learning rate when performance on training/validation set plateaus. Sometimes I don't want setting up a lr scheduler to do that. Instead, I'd like to take a look at the curve on tensorboard and decide whether to change `lr` or not. However, it seems now we cannot directly manually change `lr` when the training process is on. The best I can imagine is that there's a control panel for me during training, and I can change hyperparameters like `lr` during training, like setting a new value and informing the trainer to adopt the new value in the next epoch/batch. \r\n\r\nThis control panel should not be limited to hyperparameters, but also attributes in models. For example, I'd like to composite a new autoencoder-like model and I want to use an existing SOTA as the encoder. Typically, I will need to freeze the encoder during training **until** the decoder is good enough. Wouldn't it be great that after we see from the tensorboard that training loss is low enough, we say okay, unfreeze the encoder, adopt a new set of training parameters, checkpoint and continue training?\r\n\r\nThis control panel can also facilitate initial hyperparameter setting, as the current parameter setting is done by passing arguments. Wouldn't it be great if we can set a set of parameters via GUIs and click a button to start training?\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nWe should create a tensorboard-like control panel to modernize parameter setting before and during model training.\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nAs far as I know, there's no such control panel. Correct me if you find any alternatives and thanks in advance.\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nI think the APIs of such control panel should be similar to those of `ArgParser` and tensorboard. See a proposed prototype code below:\r\n```python\r\n# in main\r\ndatamodule = SomeDataModule()\r\nmodel = SomeModel()\r\ncontroller = ControlPanel(ip=\"0.0.0.0\", port=12345)\r\ncontroller.register_tunable(\"datamodule/batch_size\", datamodule.batch_size)\r\ncontroller.register_tunable(\"model_training/lr\", model.lr)\r\npl.Trainer.register_control_panel_tunables(controller)\r\ncontroller.wait_for_start() #will block here until user click start button\r\ntrainer = Trainer.from_control_panel(controller)\r\ntrainer.fit(model, datamodule)\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6442/comments",
    "author": "ifsheldon",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-03-10T22:45:26Z",
        "body": "Hi!\r\n\r\nThis is a very complex feature and would require a lot of development, including an UI tool. We'll consider it and maybe implement it in the future 😉 \r\n\r\nThanks for the proposal!"
      },
      {
        "user": "ifsheldon",
        "created_at": "2021-03-11T23:08:50Z",
        "body": "Yep, I understand it will be a big project, and it should be consider as a big plugin for pytorch-lightning. I'll be happy to help if you decide to work on it.\r\n\r\nAnd a question that is worth asking is  \"which parameters(including training settings) are tunable during training?\".\r\nAs I understand, we can for sure do the pre-training setting feature, which is just a GUI version of `ArgParser`, and we can implement a lr scheduler that can \"talk\" to another process to enable manual lr modification during training. But I'm not very sure whether pytorch-lightning and pytorch will allow dynamic freezing or unfreezing(see the autoencoder use case) some `Parameter` or layers of a model during training. \r\n\r\nSo, the ultimate question will be how dynamic pytorch/pytorch-lightning training pipelines can be to enable users to change any plausible settings without interrupting the training process.\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-04-11T18:49:43Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6430,
    "title": "Expected behavior when 1 rank crashes in distributed training?",
    "created_at": "2021-03-09T08:47:41Z",
    "closed_at": "2021-04-18T22:51:01Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6430",
    "body": "What's the expected error handling in distributed training when one rank raises an exception but others run fine? Currently we wrap the train loop in a try/catch and checkpoint if anything fails. This means is if one rank raises an exception but not all, that one rank goes through the checkpointing flow and hangs when we try to broadcast the filename in the model checkpoint callback. This hang obscures the primary error message for why that rank failed\r\n\r\nThis is also an issue if one rank skips a batch during training but others don't. The implicit synchronization is broken as a result",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6430/comments",
    "author": "ananthsub",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-04-10T05:54:57Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6421,
    "title": "trainer.test is breaking when a model is not passed",
    "created_at": "2021-03-08T21:56:10Z",
    "closed_at": "2021-03-25T16:23:02Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6421",
    "body": "From the docs:\r\n\r\n```\r\n# (1) load the best checkpoint automatically (lightning tracks this for you)\r\ntrainer.test()\r\n```\r\n\r\nTrainer.test should use the best checkpoint when a model isn't provided, and currently, that doesn't work.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6421/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-03-08T23:27:30Z",
        "body": "Here is an example that shows that it works:\r\n\r\n```python\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\n\r\nimport pytorch_lightning as pl\r\nfrom pl_examples.basic_examples.mnist_datamodule import MNISTDataModule\r\nfrom pytorch_lightning import Trainer\r\n\r\n\r\nclass LitClassifier(pl.LightningModule):\r\n\r\n    def __init__(self, hidden_dim=128, learning_rate=1e-3):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\r\n        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        x = torch.relu(self.l1(x))\r\n        x = torch.relu(self.l2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('valid_loss', loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('test_loss', loss)\r\n\r\n    def on_test_start(self):\r\n        checkpoint = torch.load(self.trainer.checkpoint_callback.best_model_path)\r\n        assert torch.allclose(checkpoint[\"state_dict\"][\"l1.weight\"], self.l1.weight)\r\n        assert torch.abs(self.l1.weight).sum().item() > 0\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument('--hidden_dim', type=int, default=128)\r\n        parser.add_argument('--learning_rate', type=float, default=0.0001)\r\n        return parser\r\n\r\n\r\ndef cli_main():\r\n    pl.seed_everything(1234)\r\n    parser = ArgumentParser()\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    parser = LitClassifier.add_model_specific_args(parser)\r\n    parser = MNISTDataModule.add_argparse_args(parser)\r\n    args = parser.parse_args()\r\n\r\n    dm = MNISTDataModule.from_argparse_args(args)\r\n\r\n    model = LitClassifier(args.hidden_dim, args.learning_rate)\r\n    trainer = Trainer(\r\n        max_epochs=2,\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n    )\r\n    trainer.fit(model, datamodule=dm)\r\n\r\n    # erase model weight\r\n    torch.fill_(model.l1.weight.data, 0)\r\n    assert torch.abs(model.l1.weight).sum().item() == 0\r\n    trainer.test()\r\n    assert torch.abs(model.l1.weight).sum().item() > 0\r\n\r\n\r\nif __name__ == '__main__':\r\n    cli_main()\r\n\r\n```\r\n\r\nIf you look at the assertion there in on_test_start, the weights are correctly loaded.\r\nPlease let me know under what circumstances it doesn't work. A reproducible example would be very much appreciated. Feel free to take my code and modify it."
      },
      {
        "user": "edenlightning",
        "created_at": "2021-03-25T16:23:02Z",
        "body": "Closing for now, feel free to open with a code example!"
      }
    ]
  },
  {
    "number": 6395,
    "title": "fast_dev_run fail on log_hyperparams",
    "created_at": "2021-03-07T16:49:53Z",
    "closed_at": "2021-03-09T23:18:39Z",
    "labels": [
      "bug",
      "help wanted",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6395",
    "body": "## 🐛 Bug\r\nIssue when running: `fast_dev_run=True`\r\n\"TypeError: log_hyperparams() takes 2 positional arguments but 3 were given\"\r\n\r\n### To Reproduce\r\n\r\nWhen using the following: Where self.hp_metrics is a list of strings where each string is an available metric that is being logged, example \"accuracy/val\".\r\n```\r\ndef on_train_start(self):\r\n        if self.logger:\r\n            self.logger.log_hyperparams(self.hparams, {metric:0 for metric in self.hp_metrics})\r\n```\r\n### Expected behavior\r\n\r\nAssume the unit test is wrong since the documentation say that self.logger.log_hyperparams takes one positional argument and one dictionary. The code run fine without fast_dev_run=True and everything is logged correctly to tensorboard.\r\n\r\n### Environment\r\n\r\npytorch_lightning 1.2.2",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6395/comments",
    "author": "FredrikM97",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-03-07T20:01:36Z",
        "body": "```python\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        return output.sum()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        return optimizer\r\n\r\n    def on_train_start(self):\r\n        if self.logger:\r\n            self.logger.log_hyperparams(self.hparams, {\"x\": 0})\r\n\r\n\r\nif __name__ == '__main__':\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n    model = BoringModel()\r\n    trainer = Trainer(fast_dev_run=True)\r\n    trainer.fit(model, train_data)\r\n```\r\nminimal repro example"
      }
    ]
  },
  {
    "number": 6378,
    "title": "trainer.fit must be called before trainer.predict else predict fails with Misconfiguration Exception",
    "created_at": "2021-03-06T16:08:32Z",
    "closed_at": "2021-03-07T03:24:33Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6378",
    "body": "## 🐛 Bug\r\n\r\nI am trying to use the new `predict` API of the trainer by loading a checkpoint. But it seems that `trainer.fit` must be called before `trainer.predict` else the config validator fails:\r\n\r\n```\r\nGPU available: False, used: False\r\nTPU available: None, using: 0 TPU cores\r\nTraceback (most recent call last):\r\n  File \"pytorch-lightning_bug.py\", line 49, in <module>\r\n    run_bug()\r\n  File \"pytorch-lightning_bug.py\", line 45, in run_bug\r\n    trainer.predict(model, train_data)\r\n  File \"/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1035, in predict\r\n    results = self.fit(model)\r\n  File \"/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 462, in fit\r\n    self.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\r\n  File \"/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py\", line 120, in setup_fit\r\n    self.trainer.config_validator.verify_loop_configurations(model)\r\n  File \"/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py\", line 34, in verify_loop_configurations\r\n    self.__verify_train_loop_configuration(model)\r\n  File \"/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py\", line 46, in __verify_train_loop_configuration\r\n    raise MisconfigurationException(\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: No `training_step()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.\r\n```\r\n\r\n### To Reproduce\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass SimpleModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n\r\ndef run_bug():\r\n    # fake data\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n\r\n    # model\r\n    model = SimpleModel()\r\n    # load the checkpoint …\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n    )\r\n    # use the train data for prediction\r\n    trainer.predict(model, train_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run_bug()\r\n```\r\n\r\n### Expected behavior\r\n\r\nGet the predictions of the data source.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.2.2\r\n - OS (e.g., Linux): macOS, Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.9.1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6378/comments",
    "author": "nosebrain",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-03-07T03:24:33Z",
        "body": "Hi! Thanks for reporting! This is fixed in current master\r\n\r\nPlease keep in mind that `trainer.predict` is experimental and subject to change."
      }
    ]
  },
  {
    "number": 6348,
    "title": "dp + manual_optimization is not working on PL 1.2.1",
    "created_at": "2021-03-04T17:26:42Z",
    "closed_at": "2021-04-28T08:01:07Z",
    "labels": [
      "bug",
      "help wanted",
      "strategy: dp (removed in pl)",
      "priority: 2"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6348",
    "body": "## 🐛 Bug\r\n\r\ndp + manual optimization is not working on PL 1.2.1\r\nI am setting automatic optimization = False in my model and giving `Trainer()` `accelerator='dp'`\r\n\r\n### Expected behavior\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 644, in run_train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 653, in run_training_batch\r\n    self._curr_step_result = self.training_step(\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\r\n    training_step_output = self.trainer.accelerator.training_step(args)\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 157, in training_step\r\n    return self.training_type_plugin.training_step(*args)\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/dp.py\", line 61, in training_step\r\n    return self.model(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 161, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 171, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/usr/local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 86, in parallel_apply\r\n    output.reraise()\r\n  File \"/usr/local/lib/python3.8/site-packages/torch/_utils.py\", line 428, in reraise\r\n    raise self.exc_type(msg)\r\nKeyError: Caught KeyError in replica 1 on device 1.\r\nOriginal Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\r\n    output = module(*input, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 74, in forward\r\n    output = super().forward(*inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 48, in forward\r\n    output = self.module.training_step(*inputs, **kwargs)\r\n  File \"/workdir/flow-based-test/StableGlowModel.py\", line 131, in training_step\r\n    opt.step()\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 219, in step\r\n    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 135, in __optimizer_step\r\n    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 278, in optimizer_step\r\n    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 283, in run_optimizer_step\r\n    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 160, in optimizer_step\r\n    optimizer.step(closure=lambda_closure, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 26, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/torch/optim/adamax.py\", line 67, in step\r\n    exp_avg, exp_inf = state['exp_avg'], state['exp_inf']\r\nKeyError: 'exp_avg'\r\n```\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.7.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8.7\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration:\r\n - Any other relevant information:",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6348/comments",
    "author": "okojoalg",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-03-05T13:09:12Z",
        "body": "thanks for reporting this\r\nis this reproducible with the bug_report_model?"
      },
      {
        "user": "okojoalg",
        "created_at": "2021-03-06T03:00:18Z",
        "body": "thanks for your comment.\r\nit took time to reproduce the bug report with a simple model.\r\nwe can reproduce it with the following model.\r\n\r\n``` python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.utils.data import DataLoader\r\nimport torchvision\r\nimport torchvision.datasets as datasets\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.utilities.seed import seed_everything\r\n\r\n\r\nclass Model(pl.LightningModule):\r\n\r\n    def __init__(self, channel=3):\r\n        super(Model, self).__init__()\r\n        self.automatic_optimization = False\r\n        self.nn = nn.Sequential(*[nn.Conv2d(channel, channel, 1) for _ in range(32)])\r\n\r\n    def forward(self, input):\r\n        z = self.nn(input)\r\n        return z\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adamax(self.parameters(), lr=1e-4)\r\n        return [optimizer]\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        z = self(batch[0])\r\n        loss = torch.nn.functional.mse_loss(z, batch[0])\r\n        opt = self.optimizers()\r\n        self.manual_backward(loss, opt)\r\n        opt.step()\r\n        opt.zero_grad()\r\n\r\n\r\nif __name__ == '__main__':\r\n    train_ds = datasets.CIFAR10(\r\n        root='data',\r\n        train=True,\r\n        download=True,\r\n        transform=torchvision.transforms.Compose([\r\n            torchvision.transforms.ToTensor(),\r\n        ]))\r\n\r\n    train_dl = DataLoader(\r\n        train_ds,\r\n        shuffle=True,\r\n        batch_size=32,\r\n        num_workers=8,\r\n        drop_last=True)\r\n\r\n    trainer_kwargs = {\r\n        'gpus': 2,\r\n        'accelerator': 'dp',\r\n    }\r\n\r\n    seed_everything(42)\r\n    model = Model()\r\n    trainer = pl.Trainer(**trainer_kwargs)\r\n    trainer.fit(model, train_dl)\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-03-06T18:17:57Z",
        "body": "thanks for the repro script, very helpful. So far what I have observed is that the failure is random!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-04-10T05:55:00Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-04-27T10:16:44Z",
        "body": "Hi, I found out why this happens. In Lightning, the training_step runs inside the DataParallel.forward, which means this runs on the replicas. But the DP replicas don't maintain state from the root device, therefore it gives a KeyError. For reference, I made a pure PyTorch example (that throws the KeyError): \r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass Model(nn.Module):\r\n\r\n    def __init__(self, channel=3):\r\n        super(Model, self).__init__()\r\n        self.nn = nn.Sequential(*[nn.Conv2d(channel, channel, 1) for _ in range(32)])\r\n        self.optimizer = torch.optim.Adamax(self.parameters(), lr=1e-4)\r\n\r\n    def forward(self, input):\r\n        z = self.nn(input)\r\n        z.sum().backward()\r\n        # this will throw KeyError, because this code runs on the replicas and optimizer state is not preserved\r\n        self.optimizer.step()\r\n        self.optimizer.zero_grad()\r\n        return z\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        z = self(batch[0])\r\n        loss = torch.nn.functional.mse_loss(z, batch[0])\r\n        opt = self.optimizers()\r\n        self.manual_backward(loss, opt)\r\n        opt.step()\r\n        opt.zero_grad()\r\n\r\n\r\nif __name__ == '__main__':\r\n    device = torch.device(\"cuda\", 0)\r\n    model = Model().to(device)\r\n    model = nn.DataParallel(model, device_ids=[0, 1])\r\n    model(torch.rand(2, 3, 5, 5).to(device))\r\n```\r\n\r\nAdamax has a state, but it does not get copied to the replicas. Same with other optimizers like Adam. \r\nIf you try SGD (which doesn't maintain state) you will see it works.\r\n\r\n\r\nI don't see an obvious way to fix this (unless we run training_step outside the wrapper), as this is how DP is implemented in PyTorch. I believe the best we can do is bring awareness in our docs that DP does not maintain state. And this is not just specific to the optimizers here. Any state assigned to the module will be discarded outside the replicas.\r\nI hope this answer helps and sorry for the delay."
      },
      {
        "user": "bestpredicts",
        "created_at": "2021-12-09T01:06:37Z",
        "body": "mark"
      }
    ]
  },
  {
    "number": 6333,
    "title": "Support ModelCheckpoint saving at step intervals and fractional epoch intervals",
    "created_at": "2021-03-03T22:07:39Z",
    "closed_at": "2021-03-11T22:44:29Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6333",
    "body": "## 🚀 Feature\r\nCurrently, ModelCheckpoint supports the `period` option, which specifies the epoch interval for saving checkpoints, and must be an integer. In general and especially for extremely large datasets, it would be useful to support finer control over when to save checkpoints.\r\n\r\n### Motivation\r\n\r\nI am training models with huge datasets, thus making the interval between epochs so large that saving checkpoints only at the end of epochs does not satisfy my needs.\r\n\r\n### Pitch\r\n\r\nI propose that similar to the `val_check_interval` training flag, ModelCheckpoint should support fractional epoch intervals, e.g. `period=0.25` would indicate that a model checkpoint should be saved at each quarter of an epoch. It is desirable to sometimes specify intervals in terms of batch steps rather than epochs, so I also propose adding a parameter to support this, such as `step_period`, where the caller can specify the number of steps in between saving checkpoints.\r\n\r\n### Alternatives\r\n\r\nUsers can implement custom callbacks that save checkpoints at the end of a batch step. However, it would be great to leverage all the smarts of ModelCheckpoint (such as top k logic), which quickly makes the custom callback redundant and complex. It is also a feature which I believe would be commonly used enough that it would be valuable to expose to users without the need to write custom callbacks.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6333/comments",
    "author": "timothybrooks",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2021-03-03T22:16:00Z",
        "body": "Would this work for you? #6145 \r\n#6286 is another extension i'd like to land to give checkpointing more flexibility"
      },
      {
        "user": "timothybrooks",
        "created_at": "2021-03-03T22:37:25Z",
        "body": "Yes, from the description #6145 would be exactly what I need! Time based is a good idea as well, although less important for my particular use case. Is there a target release for #6145?"
      },
      {
        "user": "ananthsub",
        "created_at": "2021-03-03T23:43:04Z",
        "body": "i'm actively working on #6145 - ideally this will be available by the next patch release, but if not, definitely within the next few weeks"
      }
    ]
  },
  {
    "number": 6295,
    "title": "all_gather for TPU doesn't support backward gradients.",
    "created_at": "2021-03-02T10:47:41Z",
    "closed_at": "2021-08-24T12:28:50Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix",
      "accelerator: tpu"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6295",
    "body": "Currently, we rely on `AllGatherGrad` to compute gather for GPUs.\r\n\r\nTODO:\r\n- [] Extend this class to support TPU\r\n- [] Add tests\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6295/comments",
    "author": "tchaton",
    "comments": [
      {
        "user": "ethanwharris",
        "created_at": "2021-03-19T16:30:05Z",
        "body": "Although all_gather will now work, it still doesn't support grad. so re-opening :)"
      },
      {
        "user": "vatch123",
        "created_at": "2021-06-14T12:53:39Z",
        "body": "Is anyone looking into this? I would like to take it up but I would need some help fixing this."
      },
      {
        "user": "edenlightning",
        "created_at": "2021-07-01T20:22:16Z",
        "body": "sorry for the late reply, @vatch123 would be great if you can tackle this! @kaushikb11 can prob help."
      },
      {
        "user": "vatch123",
        "created_at": "2021-07-18T06:37:00Z",
        "body": "Ya sorry for the delayed response. Would love to start on this if someone can describe the issue a little more. Like what is the current state and what do we want to achieve? Thanks"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-08-17T06:45:03Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6252,
    "title": "Improve verbosity and progress bar display for early stopping",
    "created_at": "2021-02-28T19:13:14Z",
    "closed_at": "2021-04-10T06:54:11Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6252",
    "body": "## 🚀 Feature\r\n\r\nWhen using EarlyStopping, it would be be great if the progress bar added two values, like \"espatience\" (the number of epochs of patience left before it might stop early) and \"estarget\" (which is the objective including min_delta, that must be achieved to avoid early stopping).\r\n\r\n### Motivation\r\n\r\nEarlyStopping verbose=True has almost no effect.\r\n\r\n### Pitch\r\n\r\nWhen doing a run, it would be great to have progress indication of the status of early stopping. i.e. how long until the next early stopping check will be done, and what objective value must be achieved for early stopping not to trigger.\r\n\r\n### Alternatives\r\n\r\nWatching the progress bar blindly and feeding pigeons bread crumbs when they fly onto my balcony.\r\n\r\nOccasionally hitting carriage return at the end of an epoch so I can see what the early stopping objective was at that epoch.\r\n\r\n### Additional context\r\n\r\nn/a",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6252/comments",
    "author": "turian",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-03-01T10:32:54Z",
        "body": "Dear @turian,\r\n\r\nThat's a great idea. Would you like to give it a try and make a PR ?\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "turian",
        "created_at": "2021-03-01T21:18:31Z",
        "body": "@tchaton I am not really familiar with the internals of the progress bar. If you think it would be simple, could you offer me a few pointers? Thank you for the wonderful product."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-03-31T23:02:32Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "turian",
        "created_at": "2021-04-02T19:51:23Z",
        "body": "@tchaton do you have any pointers for me?"
      }
    ]
  },
  {
    "number": 6245,
    "title": "Possibility to rename logged metrics postfix 'dataloader_idx_X' to the name of the respective dataset for multiple validation datasets",
    "created_at": "2021-02-27T19:35:22Z",
    "closed_at": "2021-03-02T16:03:36Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "logging"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6245",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nIt would be nice to have a possibility to rename currently automatically set metrics postfix `dataloader_idx_X` to the custom name for better metrics visualization. \r\nSay I have an imagenet and cifar datasets and accuracy metrics. Now I got `accuracy_dataloader_idx_0` and \r\n `accuracy_dataloader_idx_1` metrics postfix but it would be much more convinient to have the metrics name like `accuracy_imagenet` and `accuracy_cifar`",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6245/comments",
    "author": "Vozf",
    "comments": [
      {
        "user": "Vozf",
        "created_at": "2021-02-27T19:39:07Z",
        "body": "This could at least be possible with user manual metrics name override if the auto postfix feature could be disabled via flag.\r\nNow `_dataloader_idx_0` is a mandatory postfix for multiple val datasets as far as I know"
      },
      {
        "user": "tchaton",
        "created_at": "2021-03-01T10:34:22Z",
        "body": "Dear @Vozf,\r\n\r\nIt is in the feature roadmap :)\r\nIt shouldn't be too hard. \r\nWould you like to give it a try and open a PR. We will help you if you get blocked.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "Vozf",
        "created_at": "2021-03-01T10:49:27Z",
        "body": "What should be the api for that?"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2021-03-01T19:54:20Z",
        "body": "hi @Vozf, I have open PR #6274 which should give you the option for custom naming.\r\nsorry if you already started working with on this, I was trying to solve #6091 which is basically the same issue as this"
      },
      {
        "user": "Vozf",
        "created_at": "2021-03-01T19:57:00Z",
        "body": "Hi, I've not really started as I've had no good api ideas. So no progress wasted. Thanks for the pr though"
      }
    ]
  },
  {
    "number": 6233,
    "title": "Customize multiple dataloaders behaviour in train vs val/test",
    "created_at": "2021-02-26T21:59:16Z",
    "closed_at": "2021-04-10T06:54:16Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6233",
    "body": "User should be able to determine if multiple dataloaders will run simultaneously (passed as a list/dict), or get called sequentially.\r\nThis can be done with a trainer flag with multiple options.\r\n\r\nBackground:\r\nCurrently, in training step you get the different batches at the same time as list or dict, but for validation step it gets called multiple times with a dataloader_idx. Just thought it would be nicer if how multiple dataloaders are handled is consistent across training, val and test.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6233/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "mees",
        "created_at": "2021-03-01T09:33:40Z",
        "body": "@williamFalcon's suggestion was to have something like Trainer(multi_dataset_style=“sequential | simultaneous”) to enable consistent multiple dataloader behaviors."
      },
      {
        "user": "tchaton",
        "created_at": "2021-03-01T10:39:03Z",
        "body": "Hey @mees @edenlightning,\r\n\r\nAny consensus on the feature to be implemented ? \r\n\r\nNote: `Sequential` training would be quick a large refactor internally to enable this feature.\r\n\r\nBest,\r\nT.C\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-03-31T23:02:33Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6188,
    "title": "Add verbose option to prog_bar to print summary of every epoch",
    "created_at": "2021-02-24T21:06:32Z",
    "closed_at": "2021-04-30T20:45:19Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6188",
    "body": "Similar to ModelCheckpoint(verbose=true), we can add `verbose_progress_bar` trainer flag, to print the logs to the screen after every epoch",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6188/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-04-23T04:56:10Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6165,
    "title": "Newer torchtext (>= nightly 2021-02-19) breaks PyTorch Lightning",
    "created_at": "2021-02-23T23:37:15Z",
    "closed_at": "2021-02-26T13:53:08Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6165",
    "body": "## 🐛 Bug\r\n\r\n```python3\r\nimport pytorch_lightning as pl\r\n  File \"/../python3.7/site-packages/pytorch_lightning/utilities/apply_func.py\", line 28, in <module>\r\n    from torchtext.data import Batch\r\nImportError: cannot import name 'Batch' from 'torchtext.data' (/.../python3.7/site-packages/torchtext/data/__init__.py)\r\n```\r\n\r\n### To Reproduce\r\n\r\nInstall nightly torchtext after 02/19/2021 and install pytorch Lightning and try to import lightning on python interpreter\r\n\r\n### Expected behavior\r\n\r\nLatest pytorch lightning should be compatible with nightly torchtext or at lest a nightly ightning should be made available with such fix",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6165/comments",
    "author": "thiagocrepaldi",
    "comments": [
      {
        "user": "soyoung97",
        "created_at": "2021-03-30T08:18:39Z",
        "body": "For my case, torchtext version of 0.9.0 got this issue, so downgrading to `pip install torchtext==0.8.1` solved the version conflict."
      }
    ]
  },
  {
    "number": 6138,
    "title": "Validation loss saved in filename by ModelCheckpoint is incorrect when using DDP with multiple GPUs",
    "created_at": "2021-02-22T18:29:10Z",
    "closed_at": "2021-04-21T11:08:30Z",
    "labels": [
      "bug",
      "help wanted",
      "distributed",
      "checkpointing",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6138",
    "body": "## 🐛 Bug\r\n\r\nWhen using DDP with 2 GPUs and logging validation loss in `validation_step` with `self.log('val_loss', loss, sync_dist=True)` , ModelCheckpoint callback embeds validation loss that is multiplied by 2 (number of GPUs?) in the filename. This happens in Lightning 1.2.0.\r\n\r\nThis is a message printed by ModelCheckpoint callback:\r\n```\r\nEpoch 0, global step 0: val_loss reached 2.20627 (best 2.20627), saving model to \"some_path/epoch=0-val_loss=4.41254.ckpt\" as top 1\r\n```\r\n\r\n### To Reproduce\r\n\r\n```\r\ndef test_run():\r\n    from pytorch_lightning.callbacks import ModelCheckpoint\r\n\r\n    class TestModel(BoringModel):\r\n\r\n        def validation_step(self, batch, batch_idx) -> None:\r\n            output = self.layer(batch)\r\n            loss = self.loss(batch, output)\r\n            self.log('val_loss', loss, sync_dist=True)\r\n\r\n        def validation_epoch_end(self, outputs) -> None:\r\n            pass\r\n\r\n    # fake data\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n\r\n    # model\r\n    model = TestModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n        accelerator='ddp',\r\n        gpus=-1,\r\n        callbacks=[ModelCheckpoint(dirpath=os.getcwd(), filename='{epoch}-{val_loss:.5f}', monitor='val_loss',\r\n                                   verbose=True)]\r\n    )\r\n\r\n    trainer.fit(model, train_data, val_data)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe loss embedded in the filename should be the same as the loss in the message and logger.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.7.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.8.6\r\n - CUDA/cuDNN version: 11.0\r\n - GPU models and configuration: 2 * GeForce RTX 2080 Ti",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6138/comments",
    "author": "dpieczynski",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-04-21T11:04:55Z",
        "body": "@rivi I can't reproduce it with your instructions on master branch and also not on 1.2.8. I get:\r\n\r\n`Epoch 0, global step 0: val_loss reached 0.77993 (best 0.77993), saving model to \"/home/adrian/repositories/pytorch-lightning/epoch=0-val_loss=0.77993.ckpt\" as top 1`\r\n\r\nEDIT:\r\nI can confirm the problem exists up to 1.2.3 and is solved in 1.2.4 and above. \r\nUpgrading to 1.2.8 (latest) will solve this problem for you! \r\n\r\nEDIT: probably this PR fixed it #6410"
      }
    ]
  },
  {
    "number": 6121,
    "title": "DDPCPU not woking with DDPPlugin(find_unused_parameters=True) in Ver 1.2.0 ",
    "created_at": "2021-02-22T11:27:31Z",
    "closed_at": "2021-07-02T11:00:24Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0",
      "distributed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6121",
    "body": "## 🐛 Bug\r\n\r\nwhen using accelerator=\"ddp_cpu\" together with plugins=[DDPPlugin(find_unused_parameters=True)] to create a trainer, the trainer will cause the program tries to re-run its self (and recreate the trainer) and finally then failed at checking gpu devices. \r\n\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n```python\r\n    trainer = Trainer(\r\n        max_epochs=1,\r\n        gpus=0,\r\n        accelerator=\"ddp_cpu\",\r\n        num_processes=4,\r\n        plugins=[DDPPlugin(find_unused_parameters=True)],\r\n    )\r\n```\r\n\r\n### To Reproduce\r\n\r\n### Expected behavior\r\n\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.7.1\r\n - OS (e.g., Linux): osX\r\n - How you installed PyTorch (`conda`, `pip`, source): pip \r\n - Build command you used (if compiling from source): \r\n - Python version: 3.8.7 \r\n - CUDA/cuDNN version: No\r\n - GPU models and configuration: gpus=None\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6121/comments",
    "author": "rudaoshi",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-02-25T23:01:02Z",
        "body": "Hey! Thanks for reporting. I will get on this asap. I know what the fix is. \r\n\r\nSide note: Before 1.2 `accelerator=\"ddp_cpu\"` would always just use the spawn method, i.e., the \"ddp_spawn\" equivalent you may know for gpus. However, now in 1.2, and since you pass the DDPPlugin specifically to the plugins list, it forces the normal DDP on CPU. We simply have never tested this case, but I'll look into it now and send a PR :) "
      },
      {
        "user": "rudaoshi",
        "created_at": "2021-03-05T02:22:24Z",
        "body": "Thank  you for your response. So is there a DDPCPUPlugin for ddp_cpu accelerator so that I can set the find_unused_parameters = true?\r\n\r\nActually, I'd rather suggest that set the find_unused_parameters default to true.  Since the pretrained model is the main stream methodology in deep learning, and there are aways unused parameters in the pretrained model. "
      }
    ]
  },
  {
    "number": 6116,
    "title": "Access Optimizer and Optimizer Index in on_after_backward() hook",
    "created_at": "2021-02-21T14:33:53Z",
    "closed_at": "2021-08-01T23:02:44Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6116",
    "body": "## 🚀 Feature\r\nLet the on_after_backward() hook have access to the optimizer that will use the recently computed gradients to perform a parameter update. \r\n\r\n### Motivation\r\nThe documentation suggests that on_after_backward() is the ideal place to inspect gradient information. When using multiple optimizers, it helps to know which optimizer will use the recently computed gradients to perform a parameter update. \r\n\r\n### Pitch\r\nIf I'm not mistaken, this can be done by adding two arguments (optimizer and optimizer_idx) to the hook. TrainingLoop calls on_after_backward within the TrainingLoop.training_step_and_backward(...) method, which has access to the optimizer and optimizer index, so no extra \"plumbing\" would be needed I think. \r\n\r\n### Alternatives\r\nOne might alternatively use the LightningModule.optimizer_step() method to inspect gradient information. However, on_after_backward() seems like a better place to do this. \r\n\r\n### Additional context\r\nThis is related to issue #1527. \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6116/comments",
    "author": "cemanil",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-02-21T23:43:35Z",
        "body": "Sounds good to me. Would you like to work on this?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-31T21:56:04Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "carmocca",
        "created_at": "2021-08-01T23:02:44Z",
        "body": "The `on_before_optimizer_step` hook has been added for this in 1.4"
      }
    ]
  },
  {
    "number": 6095,
    "title": "Define null values for enums",
    "created_at": "2021-02-20T02:26:37Z",
    "closed_at": "2021-06-16T00:38:30Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix",
      "let's do it!",
      "refactor"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6095",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nCreate null values for enums instead of using `None`. For example, `utilities.enums.DistributedType` should have a `DistributedTypes.None` enum value in addition to existing enum values.\r\n### Motivation\r\nPython builtin `None` is hard to track, and I think it is always a good practice to have default null values for enums in compensation for the lack of null safety of Python. It is especially beneficial when a variable holding a type of enum will be `None`, which is the case for `DistributedType` in `trainer.connectors.accelerator_connector`.\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nDefine null values for enums, we can:\r\n* keep track of when a variable of a enum type will be `None`, facilitating code analysis\r\n* remind code collaborators of the possibility of being None when using an enum type\r\n* (If we practice this broadly, we can at least) ensure code collaborators that a variable holding a value of a enum type will always valid(i.e. not literally `None`)\r\n* \"prevent\" `Null pointer exception`/ `None does not have method/attribute` error.\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nWhen fixing #5966 in #5970, I have to take special care of `None` of `self._distrib_type`, which is a bit annoying.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6095/comments",
    "author": "ifsheldon",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-02-21T00:31:43Z",
        "body": "I personally don't think this is worth it. It's just moving the problem from one place to another. Also makes the `Optional` type unintuitive.\r\n\r\nCan you showcase some pieces of code who would greatly benefit from this?"
      },
      {
        "user": "ifsheldon",
        "created_at": "2021-02-21T08:20:45Z",
        "body": "It is not just about `Optional`. You can see some code like(e.g. in `accelerator_connector.py`)\r\n```\r\ndef __init__(self):\r\n    self.attribute = None #initialize\r\n    if condition:\r\n        self.attribute = Enum.A\r\n    else:\r\n        self.attribute = Enum.B\r\n    # many code \r\n    if condition:\r\n        self.attribute = None\r\n    # many code\r\n    if self.attribute is not None:\r\n         # do something\r\n```\r\nIn this situation, `Optional` will not help at all. The points really matter are those I mentioned in the pitch, especially the first and the second points.\r\nFor the first point, if you have defined `SomeEnum.None`, you can, through static code analysis, find out where it is used while you cannot just find `None` since any variable can be `None` without any context. `SomeEnum.None` can give more context when code contributors implement the enum and use the enum.\r\nAnd it's cheap if you are not intended to fully review the legacy code. You just add another value to an enum. That's it."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-08T23:46:52Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 6082,
    "title": "Parameterized freeze/unfreeze functions for finetuning",
    "created_at": "2021-02-19T15:57:16Z",
    "closed_at": "2021-04-30T20:45:16Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6082",
    "body": "## 🚀 Feature\r\n\r\nAdd an optional parameter to the freeze und unfreeze methods to define which layers should be freezed/unfreezed.\r\n\r\n### Motivation\r\n\r\nReduce the amount of code to freeze/unfreeze a number of layers.\r\n\r\n### Pitch\r\n\r\nWhen finetuning networks we often want to freeze/unfreeze a certain number of layers for training. While it's nice that lightning comes quick access to freeze/unfreeze them all. Wouldn't it be cool it we could to that to only a fraction of parameters?\r\n\r\nPer default the functions don't take additional parameters. We could add an optional one preferrably with different data types. Something like:\r\n\r\n- float (0.0 - 1.0) - freezes/unfreezes a fraction of parameters\r\n- int (0 - len(params)) - freezes/unfreezes the first n (last n when negative) parameters\r\n- list - freezes/unfreezes a list of parameters\r\n\r\nWhat do you think?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6082/comments",
    "author": "pietz",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-03-21T16:07:53Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "edenlightning",
        "created_at": "2021-03-22T20:43:09Z",
        "body": "@tchaton can we integrate the flash behavior?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-04-23T04:56:22Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "ananthsub",
        "created_at": "2021-05-01T00:05:12Z",
        "body": "@pietz does overriding freeze/unfreeze in the lightning module work for you? This approach might not work for lightning modules which contain multiple modules inside (eg GANs). Rather than pass flags, I'd prefer for\nusers to be able to fully specify this "
      }
    ]
  },
  {
    "number": 6077,
    "title": "Mixed precision not working with v 1.2",
    "created_at": "2021-02-19T10:34:21Z",
    "closed_at": "2021-02-19T17:00:28Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6077",
    "body": "## 🐛 Bug\r\n\r\nAfter updating to 1.2 from 1.1.1, automatic mixed precision stopped working. Everything's float32 and getting CUDA OOM when I shouldn't get it (with float16 tensors). Worked fine on 1.1.1.\r\n\r\nHere's my Trainer args (maybe there's a conflicting combo of args or something):\r\n\r\n```\r\nTrainer(logger=logger,\r\n        callbacks=[checkpoint_callback, lr_monitor],\r\n        default_root_dir=None,\r\n        gradient_clip_val=args.gradient_clip_val,\r\n        gpus=args.gpus,\r\n        auto_select_gpus=False,\r\n        log_gpu_memory=None,\r\n        progress_bar_refresh_rate=1,\r\n        check_val_every_n_epoch=args.check_val_every_n_epoch,\r\n        overfit_batches=0.,\r\n        fast_dev_run=False,\r\n        accumulate_grad_batches=1,\r\n        max_epochs=args.max_epochs,\r\n        limit_train_batches=vars(args).get('limit_train_batches', 1.),\r\n        val_check_interval=args.val_check_interval,\r\n        limit_val_batches=args.limit_val_batches,\r\n        accelerator='ddp',\r\n        sync_batchnorm=True,\r\n        precision=args.precision,\r\n        weights_summary='top',\r\n        weights_save_path=None,\r\n        num_sanity_val_steps=args.num_sanity_val_steps,\r\n        resume_from_checkpoint=None,\r\n        benchmark=False,\r\n        deterministic=False,\r\n        reload_dataloaders_every_epoch=True,\r\n        terminate_on_nan=False,\r\n        prepare_data_per_node=True,\r\n        amp_backend='native',\r\n        profiler=args.profiler)\r\n```\r\n\r\n\r\n### Environment\r\n\r\n- GCP VM with V100 GPU(s)\r\n- NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0\r\n\r\nDon't really have the time to go deeper than this, but just rolled back to 1.1.1 and everything's fine.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6077/comments",
    "author": "harpone",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-02-19T14:11:56Z",
        "body": "Minimal example to reproduce:\r\n\r\n```python\r\nfrom argparse import ArgumentParser\r\nimport torch\r\nfrom torch.nn import functional as F\r\n\r\nimport pytorch_lightning as pl\r\nfrom pl_examples.basic_examples.mnist_datamodule import MNISTDataModule\r\n\r\n\r\nclass LitClassifier(pl.LightningModule):\r\n\r\n    def __init__(self, hidden_dim=128, learning_rate=1e-3):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\r\n        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        x = torch.relu(self.l1(x))\r\n        x = torch.relu(self.l2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        print(x.dtype)\r\n        y_hat = self(x)\r\n        print(y_hat.dtype)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument('--hidden_dim', type=int, default=128)\r\n        parser.add_argument('--learning_rate', type=float, default=0.0001)\r\n        return parser\r\n\r\n\r\ndef cli_main():\r\n    pl.seed_everything(1234)\r\n    parser = ArgumentParser()\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    parser = LitClassifier.add_model_specific_args(parser)\r\n    parser = MNISTDataModule.add_argparse_args(parser)\r\n    args = parser.parse_args()\r\n    \r\n    dm = MNISTDataModule.from_argparse_args(args)\r\n    model = LitClassifier(args.hidden_dim, args.learning_rate)\r\n    \r\n    trainer = pl.Trainer.from_argparse_args(args, precision=16, gpus=1, max_steps=1)\r\n    print(trainer.training_type_plugin)\r\n    print(trainer.precision_plugin.precision)\r\n    \r\n    trainer.fit(model, datamodule=dm)\r\n\r\n\r\nif __name__ == '__main__':\r\n    cli_main()\r\n\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-02-19T17:02:00Z",
        "body": "@harpone thanks for reporting, should be fixed on master :) "
      }
    ]
  },
  {
    "number": 5969,
    "title": "Lightning throws \"bypassing sigterm\" on Slurm Cluster for unknown reason",
    "created_at": "2021-02-14T17:35:12Z",
    "closed_at": "2021-04-03T04:05:32Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "environment: slurm",
      "priority: 2"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5969",
    "body": "## 🐛 Bug\r\n\r\nI have a Python Script for training a model with DDP and multiple V100 GPUs. This works fine on development machines (DGX Station). When switching to SLURM cluster, Lightning detects Slurm and throws \"bypassing sigterm\" during the validation check iteration. As suggested in #5225 when deleting all variables from `os.environ` that belong to SLURM, the script works.\r\n\r\nI am unsure if this is a Bug in my code, in Lightning or of the Slurm cluster I am running on. Any insight into possible reasons why Slurm is throwing the sigterms is greatly appreciated!\r\n\r\n### Environment\r\n\r\n - PyTorch Version: 1.5\r\n - Lightning Version: 1.1.0\r\n - OS (e.g., Linux): Red Hat 7\r\n - How you installed PyTorch: conda\r\n - Python version: 3.7.7\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: 6x V100 32GB on IBM AC922\r\n\r\n### Additional context\r\n\r\nI know this is hard to reproduce.. But maybe somebody has an idea?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5969/comments",
    "author": "vitusbenson",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-02-15T18:04:48Z",
        "body": "Dear @vitusbenson,\n\nDid you made any progress on this issue ?\n\nBest,\nT.C"
      },
      {
        "user": "vitusbenson",
        "created_at": "2021-02-21T11:04:00Z",
        "body": "Dear @tchaton,\r\n\r\nthanks for asking, so far I have not.\r\n\r\nCheers,\r\nVitus\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-03-26T04:13:28Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "chengjiali",
        "created_at": "2021-07-01T08:07:14Z",
        "body": "I have the same error. Is it solved?"
      },
      {
        "user": "koulakis",
        "created_at": "2021-12-22T20:15:40Z",
        "body": "I had the same issue, I was running on 4 gpus in a slurm cluster and got the error. In my case, the underlying issue was that I was using a wrong path for the training output. So somehow the original exception was never propagated and got the \"bypassing sigterm\" error instead. "
      },
      {
        "user": "nicolebussola",
        "created_at": "2022-05-05T09:44:23Z",
        "body": "same here\r\n"
      },
      {
        "user": "bhaktatejas922",
        "created_at": "2022-09-26T19:23:40Z",
        "body": "Same issue here\r\n"
      },
      {
        "user": "YannDubs",
        "created_at": "2022-09-26T20:14:54Z",
        "body": "@nicolebussola @bhaktatejas922  @chengjiali  did you ever figure it out?"
      },
      {
        "user": "hesic73",
        "created_at": "2023-03-11T12:23:48Z",
        "body": "Same issue. "
      },
      {
        "user": "digantamisra98",
        "created_at": "2023-03-12T02:24:07Z",
        "body": "Same issue"
      },
      {
        "user": "malbergo",
        "created_at": "2023-03-20T21:51:47Z",
        "body": "same :( "
      },
      {
        "user": "bhaktatejas922",
        "created_at": "2023-03-20T21:55:22Z",
        "body": "We still have the issue. I think it might have something to do with the dataloader? We avoid the issue now mostly by increasing the default time out on slurm to be 5 minutes. We start seeing weird behavior with training times in epox after the first one going up and up and up though. Really tough issue to pin down. Really have no idea what's happening"
      },
      {
        "user": "bhaktatejas922",
        "created_at": "2023-03-20T21:56:25Z",
        "body": "I was thinking that using an in-memory data set like Nvidia dali would help. If anyone tries this please let me know"
      },
      {
        "user": "yumengzhang1998",
        "created_at": "2023-06-26T14:11:35Z",
        "body": "Same issue here, has anyone figured out?"
      },
      {
        "user": "navidivan",
        "created_at": "2024-09-09T16:17:21Z",
        "body": "same issue. If I forcefully stop the process, it corrupts my conda environment and it becomes unusable. does anyone experience the same thing?"
      }
    ]
  },
  {
    "number": 5945,
    "title": "init_optimizers(self, model)",
    "created_at": "2021-02-12T19:49:45Z",
    "closed_at": "2021-02-15T08:55:41Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5945",
    "body": "## 🐛 Bug\r\n\r\npytorch_lightning/trainer/optimizers.py in init_optimizers(self, model) fails to load monitor value when configure_optimizers() returns a tuple with multiple optimizers in dictionary form, each with its own LR Scheduler and Monitor Value. The bug seems to occur in the `elif` clause in line 56, where no monitor value is attempted to be extracted, so monitor val is always None in this case.\r\n\r\n## Please reproduce using SomeModel class\r\n\r\nExample class below:\r\n\r\n\r\n```\r\nclass SomeModel(TrainingBehavior):\r\n    \"\"\"\r\n    Regular ResNet model wrapper\r\n    \"\"\"\r\n    def __init__(self, hparams={}, num_outputs=11, input_size=sample_input_size, stacks=5):\r\n        super(SomeModel, self).__init__()\r\n        self.classifier = LegitClassifier()\r\n        self.descriminator = SomeDescriminator()\r\n\r\n    .........\r\n\r\n    def configure_optimizers(self):\r\n        optimizer1 = optim.Adam(self.classifier.parameters())\r\n        optimizer2 = optim.Adam(self.descriminator.parameters())\r\n        scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer1, verbose=True, patience=4, factor=0.1)\r\n        scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer2, verbose=True, patience=4, factor=0.1)\r\n        return (\r\n            {'optimizer': optimizer1, 'lr_scheduler': scheduler1, 'monitor': 'val_loss'},\r\n            {'optimizer': optimizer2, 'lr_scheduler': scheduler2, 'monitor': 'val_loss'},\r\n        )\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nYou get an exception when trainer attempts to load model.\r\n\r\n```\r\ntrainer = pl.Trainer()\r\ntrainer.fit(model)\r\n```\r\n\r\n\r\n**---------------------------------------------------------------------------\r\nMisconfigurationException                 Traceback (most recent call last)\r\n<ipython-input-15-a8c717bc330c> in <module>\r\n     53                              limit_test_batches=0.01)\r\n     54 \r\n---> 55         trainer.fit(model)\r\n     56         trainer.test(model)\r\n     57         model_metrics.append(trainer.progress_bar_metrics)\r\n\r\n/net/10.57.1.2/vol/homes/martinezniev1/complexcode/env/lib64/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    492         # ----------------------------\r\n    493         self.accelerator_backend = self.accelerator_connector.select_accelerator()\r\n--> 494         self.accelerator_backend.setup(model)\r\n    495 \r\n    496         # ----------------------------\r\n\r\n/net/10.57.1.2/vol/homes/martinezniev1/complexcode/env/lib64/python3.6/site-packages/pytorch_lightning/accelerators/dp_accelerator.py in setup(self, model)\r\n     53         # CHOOSE OPTIMIZER\r\n     54         # allow for lr schedulers as well\r\n---> 55         self.setup_optimizers(model)\r\n     56 \r\n     57         # init torch data parallel\r\n\r\n/net/10.57.1.2/vol/homes/martinezniev1/complexcode/env/lib64/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py in setup_optimizers(self, model)\r\n    148             return\r\n    149 \r\n--> 150         optimizers, lr_schedulers, optimizer_frequencies = self.trainer.init_optimizers(model)\r\n    151         self.trainer.optimizers = optimizers\r\n    152         self.trainer.lr_schedulers = lr_schedulers\r\n\r\n/net/10.57.1.2/vol/homes/martinezniev1/complexcode/env/lib64/python3.6/site-packages/pytorch_lightning/trainer/optimizers.py in init_optimizers(self, model)\r\n     78             )\r\n     79 \r\n---> 80         lr_schedulers = self.configure_schedulers(lr_schedulers, monitor=monitor)\r\n     81         _validate_scheduler_optimizer(optimizers, lr_schedulers)\r\n     82 \r\n\r\n/net/10.57.1.2/vol/homes/martinezniev1/complexcode/env/lib64/python3.6/site-packages/pytorch_lightning/trainer/optimizers.py in configure_schedulers(self, schedulers, monitor)\r\n    130                 if monitor is None:\r\n    131                     raise MisconfigurationException(\r\n--> 132                         '`configure_optimizers` must include a monitor when a `ReduceLROnPlateau` scheduler is used.'\r\n    133                         ' For example: {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"metric_to_track\"}'\r\n    134                     )\r\n\r\nMisconfigurationException: `configure_optimizers` must include a monitor when a `ReduceLROnPlateau` scheduler is used. For example: {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"metric_to_track\"}**\r\n\r\n### Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.7\r\n - OS (e.g., Linux): Linux\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.2\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5945/comments",
    "author": "clmartinez151",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-02-12T20:20:57Z",
        "body": "I guess it's not a valid config. you can pass it like:\r\n```python\r\nopt1 = ...\r\nopt2 = ...\r\nsched1 = {'scheduler': ..., monitor: ...}\r\nsched2 = {'scheduler': ..., monitor: ...}\r\nreturn (\r\n    {'optimizer': opt1, 'lr_scheduler': sched1},\r\n    {'optimizer': opt2, 'lr_scheduler': sched2},\r\n)\r\n```"
      },
      {
        "user": "tchaton",
        "created_at": "2021-02-15T08:55:41Z",
        "body": "Dear @clmartinez151,\r\n\r\nI think @rohitgr7 answered your question, closing this issue for now.\r\n\r\nBest,\r\nT.C"
      }
    ]
  },
  {
    "number": 5943,
    "title": "Reduce LR On Plateau after validation epoch when val_check_interval < 1",
    "created_at": "2021-02-12T17:24:46Z",
    "closed_at": "2021-03-22T07:34:06Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5943",
    "body": "## 🚀 Feature\r\nAdd the possibility to update learning rates after every validation epochs instead of training steps/epochs. \r\n\r\n### Motivation\r\nI am currently training a model on a very large dataset for which I like to use val_check_interval < 1.0 to have intermediary validation steps during a given epoch. \r\nI also like to use ReduceLROnPlateau and monitor a metric computed on the validation set. \r\n\r\nCurrently it seems the ReduceLROnPlateau only works if you call it at the end of an epoch (interval=\"epoch\",val_check_interval=1.0) or during training steps modulo the inputed frequency). This leads to updating the LR between validation epochs, when the validation metric is not modified. \r\nIf your patience is lower than the number of steps between two validation epochs the learning rate will be updated. \r\n\r\n### Pitch\r\nTo enable this we could add an interval flag to the current ones : \"step\", \"epoch\" which could be named \"val_epoch\". \r\nAfter on_validation_epoch_end, `trainer.optimizer_connector.update_learning_rates(interval=\"val_epoch\")` would be called.\r\n\r\n### Alternatives\r\nCurrently, to update the learning rates after the validation epochs called during a training epoch I use a Callback : \r\n\r\n```\r\nclass UpdateLRAfterValidation(Callback):\r\n    def __init__(self):\r\n        pass\r\n    def on_validation_epoch_end(self, trainer, pl_module):\r\n        trainer.optimizer_connector.update_learning_rates(interval=\"epoch\")\r\n```\r\nAlthough I am not sure if this is called after my lightning module's on_validation_epoch_end during which I log the validation metrics.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5943/comments",
    "author": "MatthieuToulemont",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-03-15T06:57:59Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "rubencart",
        "created_at": "2023-01-05T10:39:22Z",
        "body": "Is there an update on this? Has it been merged already in another PR? Would be useful to me."
      },
      {
        "user": "MaugrimEP",
        "created_at": "2023-10-06T20:24:03Z",
        "body": "I faced the same issue, setting `trainer.check_val_every_n_epoch=2` and using a ReduceOnPlateau."
      }
    ]
  },
  {
    "number": 5935,
    "title": "ModelCheckpoint doesn't delete checkpoints from s3 storage using Tensorboard Logger",
    "created_at": "2021-02-12T08:53:40Z",
    "closed_at": "2021-03-28T18:54:51Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "waiting on author",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5935",
    "body": "## 🐛 Bug\r\n\r\nWhen using ModelCheckpoint with TensorboardLogger with a S3 bucket url path the models checkpoints are correclty uploaded into the cloud directory set by the logger but but past epochs versions are not deleted. If, instead, I use directly the ModelCheckpoint with `dirpath=<s3-url>` while saving tensorboard logs locally, then the checkpoints are both uploaded and deleted correctly on my s3 bucket.\r\n\r\n### Expected behavior\r\n\r\nModelCheckpoint should delete past checkpoints on cloud storage also when using the TensorboardLogger\r\n\r\n\r\n### Environment\r\n\r\n\r\n - PyTorch Version: 1.7.1\r\n - OS: Ubuntu 20.04\r\n - Python version: 3.8",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5935/comments",
    "author": "umbertopietroni",
    "comments": [
      {
        "user": "umbertopietroni",
        "created_at": "2021-02-12T09:54:59Z",
        "body": "At the moment I'm using this workaround and the ModelCheckpoint correctly deletes old checkpoints from the s3 bucket\r\n\r\n```\r\n        # tb_logs =  os.path.join(\"s3://\", <s3_bucket>, <model_dir>) \r\n        logger = TensorBoardLogger(tb_logs, name=tb_name)\r\n\r\n\r\n        # workaround to delete old checkpoints\r\n        version = (\r\n            logger.version\r\n            if isinstance(logger.version, str)\r\n            else f\"version_{logger.version}\"\r\n        )\r\n        dir_path = os.path.join(\r\n            str(logger.save_dir), str(logger.name), version, \"checkpoints\"\r\n        )\r\n\r\n        self.checkpoint_callback = ModelCheckpoint(\r\n            dirpath=dir_path,\r\n            monitor=monitor,\r\n            mode=mode,\r\n            save_last=save_last,\r\n            save_top_k=save_top_k,\r\n            verbose=verbose,\r\n        )\r\n\r\n```\r\n\r\nOtherwise, If I leave dir_path = None then I have the issue"
      },
      {
        "user": "tchaton",
        "created_at": "2021-02-15T18:01:24Z",
        "body": "Dear @umbertopietroni,\n\nWould you like make a fix PR ?\n\nBest,\nT.C"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-03-19T20:09:30Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "kpoeppel",
        "created_at": "2021-12-07T15:51:14Z",
        "body": "I would also be interested in a solution to this, as I cannot use the workaround right away with multiple loggers."
      }
    ]
  },
  {
    "number": 5933,
    "title": "Add `dim` to `pytorch_lightning.metrics.PSNR`",
    "created_at": "2021-02-12T03:35:16Z",
    "closed_at": "2021-02-17T10:55:40Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5933",
    "body": "## 🚀 Feature\r\n\r\nAdd `dim` to `pytorch_lightning.metrics.PSNR` so that users can specify the dimension (or dimensions) for the mean squared error reduction if _PSNR_ computation.\r\n\r\n### Motivation\r\n\r\nSuppose we have two image pairs\r\n\r\n```python\r\n(pred_image_1, target_image_1)\r\n(pred_image_2, target_image_2)\r\n```\r\n\r\n`pytorch_lightning.metrics.PSNR` computes the _PSNR_ as\r\n\r\n```python\r\nsquared_error_1 = (pred_image_1 - target_image_1) ** 2\r\nsquared_error_2 = (pred_image_2 - target_image_2) ** 2\r\nmean_squared_error = (squared_error_1 + squared_error_2) / (squared_error_1.numel() + squared_error_2.numel())\r\npsnr = -10.0 * log(mean_squared_error)\r\n```\r\n\r\nIf `pred_image_1.numel()` >> `pred_image_2.numel()`, the quality of `pred_image_1` may bias the output `pnsr`.\r\nIt will be helpful if we can compute the `psnr`s of each image pair separately and average `psnr`s:\r\n\r\n```python\r\nsquared_error_1 = (pred_image_1 - target_image_1) ** 2\r\npsnr_1 = -10.0 * log(squared_error_1.mean())\r\nsquared_error_2 = (pred_image_2 - target_image_2) ** 2\r\npsnr_2 = -10.0 * log(squared_error_2.mean())\r\npsnr = (psnr_1 + psnr_2) / 2\r\n```\r\n\r\n### Pitch\r\n\r\nAdd `dim` to `pytorch_lightning.metrics.PSNR`:\r\n\r\n```python\r\nclass PSNR(Metric):\r\n    def __init__(\r\n        self,\r\n        data_range: Optional[float] = None,\r\n        base: float = 10.0,\r\n        dim: Union[int, Sequence[int]] = (),  # New.\r\n        reduction: str = 'elementwise_mean',  # Unused?\r\n        compute_on_step: bool = True,\r\n        dist_sync_on_step: bool = False,\r\n        process_group: Optional[Any] = None,\r\n    ):\r\n        # ...\r\n\r\n        self._dim = tuple(dim) if isinstance(dim, Sequence) else dim\r\n        self.add_state(\"sum_psnr\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\r\n        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\r\n        # ...\r\n\r\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\r\n        psnrs = compute_psnr(preds, target, dim=self._dim) # New.\r\n        self.psnrs += psnrs.sum()\r\n        self.total += psnrs.numel()\r\n\r\n    def compute(self):\r\n        return self.psnrs / self.total\r\n\r\n\r\npreds_nchw = torch.rand([32, 3, 224, 224]\r\ntargets_nchw = torch.rand([32, 3, 224, 224]\r\nmetric = PSNR(dim=(1, 2, 3))\r\naverage_psnr_of_n_images = metric(preds_nchw, targets_nchw)\r\n```\r\n\r\n### Alternatives\r\n\r\nIf the original behavior should be kept, maybe we can do\r\n\r\n```python\r\nclass PSNR(Metric):\r\n    def __init__(\r\n        self,\r\n        data_range: Optional[float] = None,\r\n        base: float = 10.0,\r\n        dim: Optional[Union[int, Sequence[int]]] = None,  # `None` for original behavior\r\n        reduction: str = 'elementwise_mean',  # Unused?\r\n        compute_on_step: bool = True,\r\n        dist_sync_on_step: bool = False,\r\n        process_group: Optional[Any] = None,\r\n    ):\r\n        # ...\r\n\r\n        self._dim = tuple(dim) if isinstance(dim, Sequence) else dim\r\n        if self._dim is None:\r\n            # Original behavior.\r\n            self.add_state(\"sum_squared_error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\r\n        else:\r\n            self.add_state(\"sum_psnr\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\r\n        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\r\n        # ...\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5933/comments",
    "author": "manipopopo",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2021-02-12T11:26:31Z",
        "body": "Hi @manipopopo,\r\nThanks for your suggestion. I think it is a good idea to add a `dim` argument to PSNR. Would you be up for sending a PR with the enhancement?"
      },
      {
        "user": "manipopopo",
        "created_at": "2021-02-12T11:34:28Z",
        "body": "Hi @SkafteNicki , I’d be happy to. Maybe I can send a PR tomorrow.\r\n"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2021-02-12T11:35:43Z",
        "body": "@manipopopo sound good to me :]\r\nPlease ping me in your PR."
      }
    ]
  },
  {
    "number": 5897,
    "title": "Auto_scale_batch_size fails for to bigger batch sizes, cuDNN failure",
    "created_at": "2021-02-10T08:51:55Z",
    "closed_at": "2021-02-15T15:44:41Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5897",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI'm using a pre-trained ResNet50 on 224x224 images with 16-bit precision, I wanted to test the auto_scale_batch_size functionality.\r\n\r\nThe output in the terminal is the following:\r\nGPU available: True, used: True\r\nTPU available: None, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nUsing native 16bit precision.\r\nBatch size 2 succeeded, trying batch size 4\r\nBatch size 4 succeeded, trying batch size 8\r\nBatch size 8 succeeded, trying batch size 16\r\nBatch size 16 succeeded, trying batch size 32\r\nBatch size 32 succeeded, trying batch size 64\r\nBatch size 64 succeeded, trying batch size 128\r\nBatch size 128 succeeded, trying batch size 256\r\n\r\nAll good till then. On batch size 256 the GPU's memory certainly is not sufficient and it fails on a 2d convolution.\r\nThe auto_scaling should be aborted at this point and the batch_size fixed to 128.\r\nInstead the script fails with the message \"RuntimeError: Unable to find a valid cuDNN algorithm to run convolution\".\r\n\r\n### To Reproduce\r\nI'm not doing anything exceptional:\r\n1. Parsing arguments with \"--auto_scale_batch_size\", \"true\"\r\n2. Initiating model, datamodule and trainer using the parsed arguments\r\n3. trainer.tune(model, dm)\r\n\r\n### Question\r\nIs this a problem on the Pytorch Lightning side not capturing the exception or is this anyhow linked to Cuda and installing a different version could be enough?\r\n\r\n\r\n### Environment\r\n1x GeForce RTX 2080Ti\r\nUbuntu 20.10\r\nPython 3.8.5\r\nPytorch Lightning 1.1.8\r\nPytorch 1.7.1\r\nCuda 11.2\r\nEnv created with miniconda, packages installed with pip\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5897/comments",
    "author": "FlorianMF",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2021-02-10T08:56:45Z",
        "body": "Hi @FlorianMF,\r\n\r\nHave you tried to run that network with batchsize 256 without the auto_scaling? Because we don't do anything special on the convolutional side and this seems to be a bug in PyTorch.\r\n\r\nAlso could you try with `torch.backends.cudnn.benchmark = True` and/or `torch.backends.cudnn.enabled = False`? "
      },
      {
        "user": "FlorianMF",
        "created_at": "2021-02-10T09:22:45Z",
        "body": "Hi @justusschock, \r\nI get the same error without auto_scaling for a batch_size=256, as well when adding 'torch.backends.cudnn.benchmark = True' at the top of the script.\r\nWhen adding \"torch.backends.cudnn.enabled = False\" the error message \"RuntimeError: CUDA out of memory\" confirms my assumption.\r\n\r\nSo, am I 'forced' to use 'auto_scale_batch_size' once to get the maximum batch size and then rerun the script without the flag? I think your goal proposing this functionality is that this is not needed though.\r\n"
      },
      {
        "user": "justusschock",
        "created_at": "2021-02-10T10:03:05Z",
        "body": "No, usually you don't have to rerun it. But with torch.backends.cudnn.enabled = False, PyTorch will use some other algorithm for convolutions, which may be more memory demanding. So for some reason there seems to be a bug within PyTorch for the cudnn convolution. Our tuner only listens to the `Runtime: CUDA out of memory`-Error as everything else could also be a user-error.\r\n\r\nUnfortunately there is nothing else we can do on that"
      },
      {
        "user": "FlorianMF",
        "created_at": "2021-02-10T10:24:26Z",
        "body": "You're right. The error is captured when 'torch.backends.cudnn.enabled = False' is added at the top and the script continues."
      },
      {
        "user": "tchaton",
        "created_at": "2021-02-15T15:44:41Z",
        "body": "Hey @FlorianMF,\n\nClosing this issue as it seems resolved.\n\nBest,\nT.C"
      }
    ]
  },
  {
    "number": 5896,
    "title": "Access dataset directly in validation_epoch_end",
    "created_at": "2021-02-10T07:11:22Z",
    "closed_at": "2021-02-11T01:52:59Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5896",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation / Pitch\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nNot sure if duplicated but:\r\n\r\nI would propose to allow users to add additional arguments to be passed into functions such as `validation_epoch_end()`. \r\n\r\nOne of the examples being: during validation, we might need to fetch some additional information from the dataset (e.g., `len(dataset)`), and currently it is a bit hard to achieve without passing the dataset into the model during `init`.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5896/comments",
    "author": "Cuberick-Orion",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-02-10T22:33:31Z",
        "body": "the datasets/dataloaders are available already, because they are attached to the trainer. \r\nExample:\r\n```python\r\nval_dataloader = self.trainer.val_dataloaders[0]  # one or multiple\r\nval_dataset = val_dataloader.dataset\r\nprint(len(val_dataset))\r\n```"
      },
      {
        "user": "Cuberick-Orion",
        "created_at": "2021-02-11T01:52:56Z",
        "body": "Thanks for the tip! I am closing this issue."
      }
    ]
  },
  {
    "number": 5894,
    "title": "training with ddp get replicas mismatch error",
    "created_at": "2021-02-10T04:45:25Z",
    "closed_at": "2021-04-18T22:51:08Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "distributed",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5894",
    "body": "Hi,\r\n\r\nI've been getting this replicas error with ddp training.\r\nsetup: windows 10, torch 1.7.1, pytorch-lightning 1.1.7, on a 3 gpus machine.\r\n\r\nThe model training was working well with ddp on another machine 2 gpus (same setup w/ win10, torch 1.7.1 and pl 1.1.7)\r\n\r\nthe code crashed after printed the following error message:\r\n\r\n```python\r\nself.reducer = dist.Reducer(\r\nRuntimeError: replicas[0][0] in this process with sizes [12, 6] appears not to match sizes of the same param in process 0.\r\n\r\n```\r\n(Note:  the  sizes [12, 6] in error message changes in different run, could be any numbers, such as sizes[128, 45], etc.)\r\n\r\nI then tried with setting accelerator='ddp_spawn',  this makes the replicas error disappear.  But just as being warned in documentation, ddp_spawn is very unstable, e.g, smaller batches, lower gpu utilizations, longer training time, etc.  and the training can hardly proceed to 7-8 epches, because it always mysteriously crashes with memory error. \r\n\r\nSo still need to figure out how to revert back to **ddp** mode. \r\n\r\nasked the Pytorch forum, their answer is as follow:\r\n\r\n **\"This happens if the model parameters are not the same across all replicas in DDP. Have you tried printing the sizes of all the params in the model from each rank (using model.parameters())? This would be the first thing to verify mismatched sizes.\"**\r\n\r\nand I did printed the number model parameters in each process, they are the same. (The printing is after model initiation, but before the Trainer initiation, which will then initialize underline ddp, which is where error happened)\r\n\r\nI understand that, in ddp mode, the program is restarted in each process, while in ddp_spawn mode, it's been carrying on in the subprocess -- **does this different approaches of multiprocessing caused the model or model parameters that were copied to each gpu were different?**\r\n\r\nBelow is how lightning Trainer is initialed and then fit is called (very standard steps):\r\n```python\r\nself.trainer = pl.Trainer(\r\nmax_epochs=configs[“max_epochs”],\r\ngpus=[0, 1, 3],\r\naccelerator=‘ddp’,\r\nweights_summary=“top”,\r\ngradient_clip_val=0.1,\r\nlimit_train_batches=30,\r\ncallbacks=[lr_logger, early_stop_callback, checkpoint_callback],\r\n)\r\n\r\nmodel = …\r\n\r\nself.trainer.fit(\r\nmodel,\r\ntrain_dataloader=self.train_dataloader,\r\nval_dataloaders=self.val_dataloader,\r\n)\r\n```\r\n\r\nPlease help!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5894/comments",
    "author": "BlockWaving",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-02-15T15:43:15Z",
        "body": "Dear @BlockWaving,\n\nThanks for reporting this bug.\nWould this fail if you provide `gpus=[0, 1, 2],` instead of `gpus=[0, 1, 3],`\n\nBest,\nT.C"
      },
      {
        "user": "BlockWaving",
        "created_at": "2021-02-16T15:25:59Z",
        "body": "thanx for catching the typo,  the actual testing was [0, 1, 2]"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-03-08T02:39:09Z",
        "body": "I have never seen this error you are reporting.\r\nI would do the following: Replace your model with one of ours, the bug report model or whatever. \r\nIf the error does not occur with our model, then the problem is with your model. \r\nWith these types of problems, it is only possible to guess forever. We will probably not be able to help much unless we have a script that reproduced the issue. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-04-07T02:50:53Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "pengyuange",
        "created_at": "2021-11-08T12:30:48Z",
        "body": "My code happened to the same problem.\r\nwhen i looked at my training log, I found there is a moduledict in my module, and the key is get from a list loop.\r\nwhen i sorted the list, the problem solved.\r\nhope it is helpful for u. "
      }
    ]
  },
  {
    "number": 5838,
    "title": "MultiTask Training on multi-gpus returns NaN and inf in model output during Validation phase",
    "created_at": "2021-02-05T19:24:27Z",
    "closed_at": "2021-02-06T21:11:31Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5838",
    "body": "## 🐛 Bug\r\n\r\nI'm trying to train a multihead ResNet on images. Train data size is 2700 and valid data size is 600. Each batch is [n, 3, 224, 224, 320] and normalized to have values [0,1]. I've already trained a single head resnet on this dataset many times and never encountered a problem with any datapoint so I am sure the data is good. \r\n\r\nWhen I train the model using 2 heads on 1 GPU for 5 epochs, everything runs smoothly. If I try 2 GPUs, after the progress bar is done on train phase and starts validation phase, I start getting all NaNs from the feature extractor part of my network. I put in breakpoints to check the batch data and it has no NaNs or infs anywhere. I even made my validation data the same as my train data to check if the valid data was broken and feature extractor still outputs NaNs, even after just going through the same data during train phase with no error!\r\n\r\nI can't find any pytorch lightning examples of MultiHead training, so if those exist please point me to those. I've followed the multi-gpu training docs for pytorch lightning to a T so have no idea what's going on. I'm also setting the random seed at beginning of program using pl.seed_everything(64). I'm using DDP and 16bit precision.\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.7.1+cu101\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: 4 V100 on google cloud VM\r\n - Any other relevant information: 32 cores, 128GB mem\r\n\r\n### Additional context\r\n\r\nRunning ResNet101 on 3D images. Trying to train 3 classification heads (each head only looks at certain classes). Implemented my own MultiHeadLoss wrapper to calculate total loss from each head: \r\n\r\n```\r\nclass MultiHeadLoss(nn.Module):\r\n    \"\"\"Sum of all losses.\"\"\"\r\n\r\n    def __init__(self, model_heads):\r\n        super().__init__()\r\n        self.model_heads = model_heads\r\n\r\n    def forward(self, outputs_targets_dict):\r\n        losses = []\r\n        for head in self.model_heads:\r\n            if head.name in outputs_targets_dict:\r\n                output = outputs_targets_dict[head.name][\"output\"]\r\n                target = outputs_targets_dict[head.name][\"target\"]\r\n                head_loss = head.loss_fn(output, target)\r\n                losses.append(head_loss)\r\n\r\n        return sum(losses)\r\n```\r\n\r\nMy network module looks like this:\r\n```\r\nclass ModelTemplate(pl.LightningModule):\r\n    def __init__(self, model_body=gin.REQUIRED, model_heads=gin.REQUIRED):\r\n        super().__init__()\r\n        self.model_body = model_body\r\n        self.model_heads = nn.ModuleList()\r\n        [self.model_heads.append(head(self.model_body.out_features)) for head in model_heads]\r\n\r\n    def forward(self, x):\r\n        features = self.model_body.forward(x)\r\n        head_out = [head(features) for head in self.model_heads]\r\n        return head_out\r\n```\r\nEach head looks like this:\r\n```\r\nclass Classification(pl.LightningModule):\r\n    def __init__(self, in_features, task_labels, name=\"classification\"):\r\n        super().__init__()\r\n        self.register_buffer(\"task_labels\", torch.Tensor(task_labels))\r\n        self.name = name\r\n        self.fc = nn.Linear(in_features, len(task_labels))\r\n        self.loss_fn = WeightedFocalLoss(criterion=nn.CrossEntropyLoss(reduction=\"none\"))\r\n        self.activation_fn = nn.Softmax(dim=1)\r\n\r\n    def forward(self, x):\r\n        logits = self.fc(x)\r\n        return logits\r\n```\r\nRun the train job like this:\r\n```\r\ntrainer = Trainer(\r\n        default_root_dir=results_dir,\r\n        callbacks=callbacks,\r\n        accelerator=\"ddp\",\r\n        gpus=gin.query_parameter(\"Trainer.gpus\"),\r\n        max_epochs=gin.query_parameter(\"Trainer.max_epochs\"),\r\n        precision=16,\r\n        num_sanity_val_steps=False,\r\n        logger=mlf_logger,\r\n        profiler=\"simple\",\r\n    )\r\n    trainer.fit(model=train_module, datamodule=datamodule)\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5838/comments",
    "author": "angadkalra",
    "comments": [
      {
        "user": "angadkalra",
        "created_at": "2021-02-06T21:11:22Z",
        "body": "Please close this issue, problem solved. Combination of 16-bit precision, multihead training with high learning rate, and no gradient clipping caused an overflow in a model layer that resulted in Inf in output. "
      },
      {
        "user": "thak123",
        "created_at": "2021-03-23T09:38:41Z",
        "body": "@angadkalra Do you have the pseudo-code of MTL somewhere?"
      },
      {
        "user": "angadkalra",
        "created_at": "2021-03-24T17:23:50Z",
        "body": "> @angadkalra Do you have the pseudo-code of MTL somewhere?\r\n\r\nIs the code provided above not enough? "
      }
    ]
  },
  {
    "number": 5827,
    "title": "Completely overwrite validation/test block (including the batch-level loop)",
    "created_at": "2021-02-05T04:48:54Z",
    "closed_at": "2021-03-11T13:51:47Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "refactor",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5827",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nIn certain cases, for example, **an image+text multimodal retrieval model**, the training and validation/testing logic can be very different. Specifically:\r\n* In training, for each input query, we construct the corresponding batches by sampling randomly from the dataset;\r\n* In validation/testing, for each input query, we need to loop through *all* candidates within the dataset\r\n\r\nI'll be happy to go into the details, but in short, for these use cases, we often need a COMPLETELY different pipeline for validation/testing, which doesn't necessarily fit into the 'loop' as we would use in typical training scenarios.\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nI would propose to have a higher-level function that could:\r\n* Overwrite the entire validation/testing logic, *including* the batch-level loop - basically, redefines the validation/testing behaviors\r\n* Have full control over what elements go to what device (i.e., manually define `.cuda()` or `.cpu()` - this is because, for retrieval-type tasks, it is highly impractical to stuff everything in VRAM at the same time, chances are we need to take things in and out of GPUs.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5827/comments",
    "author": "Cuberick-Orion",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-02-08T15:44:40Z",
        "body": "> In validation/testing, for each input query, we need to loop through all candidates within the dataset\r\n\r\nAre you talking about looping in your validation_step (for each batch) over a whole dataset? You should be able to do this today.\r\n\r\n> Overwrite the entire validation/testing logic, including the batch-level loop - basically, redefines the validation/testing behaviors\r\n\r\nI agree this would be a cool thing to allow. But that requires a large refactor on the internals as of today. Hopefully we can allow it soon.\r\n\r\n> Have full control over what elements go to what device \r\n\r\nYou can manually set your devices already where appropriate."
      },
      {
        "user": "Cuberick-Orion",
        "created_at": "2021-02-09T03:44:30Z",
        "body": "Thanks for the reply.\r\n\r\nTo go into details on the validation process, I will be taking the retrieval task as an example:\r\n* Task definition: \r\n  * `input_query` (image and/or text) is to be matched with a `candidate` (image) within a large corpus\r\n* The data flow in validation:\r\n  * `input_query -> feature extraction (module_0) -> feature vectors (size: N x num_hid)` Do this for N input samples;\r\n  * `candidate -> feature extraction (module_1) -> feature (size: M x num_hid)` Do this for M candidate images;\r\n  * Then, compute similarity of features through e.g., l2-norm. Generate a similarity matrix of size `N x M`\r\n  * Identify the most similar candidate for each input (operate on each row)\r\n\r\nImagine that the size of the inputs and candidates (`N` and `M`) are quite large- which is usually the case, we can't really fit this pipeline into the de-facto \"loop\" of validations. Chances are, we need to take over the loop and redesign something else, for example:\r\n* process `input_query` in batches and save all features to `cpu()`\r\n* process `candidate` in batches and save them to `cpu()`\r\n* perform the necessary comparison on `cpu()`\r\n\r\nI understand we can manually designate the device now (just again to point out that these things can be quite large), but for the former two steps, it could be tricky to realize as of this moment.\r\n\r\nMy workaround currently is to create two dataloaders in validation (where the first returns `input_query` and the second returns `candidate`) and perform the corresponding `feature_extraction` in `validation_step`, then collect all processed features in `val_epoch_end`. But it is a bit counter-intuitive compared to what people usually codes for validating this task. The more straightforward way is to just bypass the pre-defined validation loop and create my own validation function, which receives the `val_dataloader` and `model` and returns the computed `score`.\r\n\r\nAll in all, what I am trying to say is - the case where users need to (prefer to) define their own validation functions is not unreasonable or unheard of, and perhaps it would be better if lightning allows us to do so. Thanks.\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-03-11T13:01:21Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "carmocca",
        "created_at": "2021-03-11T13:51:47Z",
        "body": "> All in all, what I am trying to say is - the case where users need to (prefer to) define their own validation functions is not unreasonable or unheard of, and perhaps it would be better if lightning allows us to do so.\r\n\r\nI hear you. We'd like to eventually refactor the loops so users can do this sort of modification more easily.\r\n\r\nClosing this for the moment. Thanks for your proposal ❤️ "
      }
    ]
  },
  {
    "number": 5759,
    "title": "Problem with syncing logged values with multi-gpu and ddp",
    "created_at": "2021-02-03T02:44:24Z",
    "closed_at": "2021-02-03T23:18:06Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5759",
    "body": "## 🐛 Bug\r\n\r\nWhen logging values with `sync_dist` and ddp (on two GPUs), the logged value is changed and the wrong value of averaged metric is logged.\r\nIt can be reproduced with dummy `training_step()` and `batch_size=1` \r\n```\r\n    def training_step(self, batch, batch_idx):\r\n        loss = torch.tensor(1.0, device=self.device, requires_grad=True)\r\n        self.log('loss', loss, on_epoch=True, sync_dist=True)\r\n        return loss\r\n```\r\nThe logged and returned value is 2.0\r\n### To Reproduce\r\nThis was discovered running ddp with two GPUs\r\n\r\n### Expected behavior\r\n\r\nLogged value 1.0, value of loss not changed\r\n\r\n### Environment\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX TITAN X\r\n\t\t- GeForce GTX TITAN X\r\n\t\t- GeForce GTX TITAN X\r\n\t\t- GeForce GTX TITAN X\r\n\t- available:         True\r\n\t- version:           11.0\r\n* Packages:\r\n\t- numpy:             1.19.4\r\n\t- pyTorch_debug:     True\r\n\t- pyTorch_version:   1.7.0\r\n\t- pytorch-lightning: 1.1.6\r\n\t- tqdm:              4.54.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         \r\n\t- python:            3.7.8\r\n\t- version:           1 SMP PREEMPT Sun, 27 Dec 2020 10:50:46 +0000\r\n\t\r\n### Additional context\r\nLogging was done with Comet.ml",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5759/comments",
    "author": "stachu86",
    "comments": [
      {
        "user": "ankur56",
        "created_at": "2021-02-03T19:01:16Z",
        "body": "I tried it. I got 1.0 as expected using 4 GPUs and DDP on a single node."
      },
      {
        "user": "stachu86",
        "created_at": "2021-02-03T23:18:06Z",
        "body": "@ankur56 thanks for testing this! After your reply I looked closely to my setup, and it turns out my remote debug configuration was using environment with an old pytorch-lightning version (1.0.6). Indeed, this bug is already fixed (in old version I fixed it with `loss.clone()` in `log` call).\r\n\r\n\r\n\r\n\r\n "
      }
    ]
  },
  {
    "number": 5672,
    "title": "Calling trainer.fit fails with: AttributeError: 'dict' object has no attribute 'pretty'",
    "created_at": "2021-01-27T07:03:05Z",
    "closed_at": "2021-02-02T16:40:21Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5672",
    "body": "## 🐛 Bug\r\n```\r\ndataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\r\ntrain, val = random_split(dataset, [55000, 5000])\r\n\r\nautoencoder = LitAutoEncoder()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(autoencoder, DataLoader(train))\r\n```\r\n\r\nCalling trainer.fit fails with AttributeError: 'dict' object has no attribute 'pretty'\r\n\r\n\r\n## To Reproduce\r\n\r\n```\r\nGPU available: True, used: False\r\nTPU available: None, using: 0 TPU cores\r\n/home/nithin/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\r\n  warnings.warn(*args, **kwargs)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-ec8282b1f4ec> in <module>\r\n      4 autoencoder = LitAutoEncoder()\r\n      5 trainer = pl.Trainer()\r\n----> 6 trainer.fit(autoencoder, DataLoader(train))\r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    508         self.call_hook('on_fit_start')\r\n    509 \r\n--> 510         results = self.accelerator_backend.train()\r\n    511         self.accelerator_backend.teardown()\r\n    512 \r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in train(self)\r\n     54 \r\n     55     def train(self):\r\n---> 56         self.trainer.setup_trainer(self.trainer.model)\r\n     57         return self.train_or_test()\r\n     58 \r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in setup_trainer(self, model)\r\n    442             self.logger.log_hyperparams(ref_model.hparams_initial)\r\n    443             self.logger.log_graph(ref_model)\r\n--> 444             self.logger.save()\r\n    445 \r\n    446         # wait for all to join if on distributed\r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py in wrapped_fn(*args, **kwargs)\r\n     38     def wrapped_fn(*args, **kwargs):\r\n     39         if rank_zero_only.rank == 0:\r\n---> 40             return fn(*args, **kwargs)\r\n     41 \r\n     42     return wrapped_fn\r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/loggers/tensorboard.py in save(self)\r\n    234         # save the metatags file if it doesn't exist\r\n    235         if not self._fs.isfile(hparams_file):\r\n--> 236             save_hparams_to_yaml(hparams_file, self.hparams)\r\n    237 \r\n    238     @rank_zero_only\r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pytorch_lightning/core/saving.py in save_hparams_to_yaml(config_yaml, hparams)\r\n    386         with fs.open(config_yaml, \"w\", encoding=\"utf-8\") as fp:\r\n    387             try:\r\n--> 388                 OmegaConf.save(hparams, fp)\r\n    389                 return\r\n    390             except (UnsupportedValueType, ValidationError):\r\n\r\n~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/omegaconf/omegaconf.py in save(config, f, resolve)\r\n    268         :param resolve: True to save a resolved config (defaults to False)\r\n    269         \"\"\"\r\n--> 270         data = config.pretty(resolve=resolve)\r\n    271         if isinstance(f, (str, pathlib.Path)):\r\n    272             with io.open(os.path.abspath(f), \"w\", encoding=\"utf-8\") as file:\r\n\r\nAttributeError: 'dict' object has no attribute 'pretty'\r\n\r\n```\r\n## Expected behavior\r\n\r\nCalling **trainer.fit(autoencoder, DataLoader(train))** should train MNIST classifier without errors.\r\n\r\n## Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX 1050 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1\r\n\t- pytorch-lightning: 1.1.6\r\n\t- tqdm:              4.50.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.3\r\n\t- version:           #70~18.04.1-Ubuntu SMP Tue Jan 12 17:18:00 UTC 2021\r\n\r\n### Additional context\r\n\r\nThe program was running on **jupyter notebook**.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5672/comments",
    "author": "nithinivi",
    "comments": [
      {
        "user": "omry",
        "created_at": "2021-01-28T05:03:29Z",
        "body": "Try to upgrade to OmegaConf 2.0, you have an incompatible version installed."
      },
      {
        "user": "Borda",
        "created_at": "2021-01-29T21:04:31Z",
        "body": "@nithinivi what OmegaConf are you using?"
      },
      {
        "user": "nithinivi",
        "created_at": "2021-01-30T03:52:21Z",
        "body": "@Borda @omry The version number of OmegaConf  is  2.0.0"
      },
      {
        "user": "omry",
        "created_at": "2021-01-30T09:21:02Z",
        "body": "@nithinivi,\r\n\r\nconfig.pretty() is deprecated in OmegaConf 2.0 and the source code no longer contain calls to it.\r\nThe stack trace is definitely not from OmegaConf 2.0. If you think this is wrong please provide repro instructions from scratch (including the creation of a virtualenv or a conda environment).\r\n\r\nBy the way, please upgrade to latest OmegaConf 2.0 release (currently 2.0.6)."
      },
      {
        "user": "nithinivi",
        "created_at": "2021-02-01T06:31:32Z",
        "body": "I created a new conda env and installed the torch and pytorch-ligthing.\r\nTried out the same code and the issue was not reproduced. So I do believe this was a issues with the environment setup and it's not using OmegaConf 2.0.\r\n\r\nI have installed python using **pyenv**  and executing the code in a jupyter notebook."
      },
      {
        "user": "roytseng-tw",
        "created_at": "2021-02-02T15:26:32Z",
        "body": "I had the same issue with omegaconf 2.0.1rc11.\r\nAfter update to the latest version 2.0.6, the issue is gone."
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-02-02T16:40:21Z",
        "body": "I believe issue is resolved now. Please feel free to reopen if required :)"
      }
    ]
  },
  {
    "number": 5642,
    "title": "Apex with multiple optimizers error \"element 0 of tensors does not require grad and does not have grad_fn\"",
    "created_at": "2021-01-24T22:05:23Z",
    "closed_at": "2021-01-27T02:24:27Z",
    "labels": [
      "bug",
      "help wanted",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5642",
    "body": "## 🐛 Bug\r\n\r\n```bash\r\n File \"repro apex.py\", line 51, in <module>\r\n    trainer.fit(model)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 481, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/accelerators/gpu_accelerator.py\", line 67, in train\r\n    results = self.train_or_test()\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/accelerators/accelerator.py\", line 68, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 532, in train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 572, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 729, in run_training_batch\r\n    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 505, in optimizer_step\r\n    model_ref.optimizer_step(\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/core/lightning.py\", line 1263, in optimizer_step\r\n    optimizer.step(closure=optimizer_closure)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/core/optimizer.py\", line 278, in step\r\n    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/core/optimizer.py\", line 133, in __optimizer_step\r\n    trainer.precision_connector.backend.optimizer_step(trainer, optimizer, closure)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/plugins/apex.py\", line 138, in optimizer_step\r\n    closure()\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 719, in train_step_and_backward_closure\r\n    result = self.training_step_and_backward(\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 827, in training_step_and_backward\r\n    self.backward(result, optimizer, opt_idx)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 847, in backward\r\n    result.closure_loss = self.trainer.accelerator_backend.backward(\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/accelerators/accelerator.py\", line 97, in backward\r\n    closure_loss = self.trainer.precision_connector.backend.backward(\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/plugins/apex.py\", line 53, in backward\r\n    model.backward(closure_loss, optimizer, opt_idx)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/core/lightning.py\", line 1155, in backward\r\n    loss.backward(*args, **kwargs)\r\n  File \"/home/aw18f408/.conda/envs/lightning/lib/python3.8/site-packages/torch/tensor.py\", line 221, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/aw18f408/.conda/envs/lightning/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 130, in backward\r\n    Variable._execution_engine.run_backward(\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n\r\n```\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\n```python\r\nimport torch\r\nfrom torch import optim\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass AMPModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        output = self(batch)\r\n        loss = output.mean()\r\n        return {\"loss\": loss}\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def configure_optimizers(self):\r\n        optimizer1 = torch.optim.Adam(self.parameters(), lr=0.01)\r\n        optimizer2 = optim.SGD(self.parameters(), lr=0.01)\r\n        return [optimizer1, optimizer2]\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = AMPModel()\r\n    trainer = Trainer(\r\n        max_epochs=1,\r\n        precision=16,\r\n        amp_backend='apex',\r\n        gpus=1,\r\n    )\r\n    trainer.fit(model)\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo crash\r\n\r\n### Environment\r\n\r\n\r\n* CUDA:\r\n        - GPU:\r\n               - GeForce RTX 2080 Ti\r\n               - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           11.0\r\n* Packages:\r\n        - numpy:             1.19.5\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.7.1\r\n        - pytorch-lightning: 1.2.0dev\r\n        - tqdm:              4.56.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n               - 64bit\r\n               - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.3\r\n        - version:           #1 SMP Thu Apr 9 13:49:54 UTC 2020\r\n\r\n### Additional context\r\n\r\ndiscovered in #5507, in the `test tests/models/test_amp::test_amp_with_apex`\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5642/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-01-24T22:52:09Z",
        "body": "Hey @awaelchli ,\r\n\r\nA fix is already in review. Blocked by will 😁\r\n\r\nLightning is toggling on the current optimizer (setting requieres_grad=True) and untoggling on the second one (setting to False). \r\n\r\nAs self.parameters is used for both optimizers, your model doesn t have any trainable parameters and the loss doesn t have graph_fn.\r\n\r\nThe fix will restore the pre-toggle state.\r\n\r\nClosing duplicated issue.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-01-24T22:54:09Z",
        "body": "Thanks, I was searching but couldn't find a related issue. Would be great if you could link it :) "
      },
      {
        "user": "awaelchli",
        "created_at": "2021-01-27T02:24:27Z",
        "body": "Fixed by the linked PR. Thanks @tchaton "
      },
      {
        "user": "dave-epstein",
        "created_at": "2021-07-31T22:14:42Z",
        "body": "Hi, I'm still getting this issue training a model with multiple optimizers. I get the issue when using FSDP or DeepSpeed. I'm really stumped on the bug, and help would be appreciated. Thanks!"
      }
    ]
  },
  {
    "number": 5641,
    "title": "Log fails: \"Tensors must be CUDA and dense\" with multi-GPUs using ddp",
    "created_at": "2021-01-24T21:26:14Z",
    "closed_at": "2021-01-25T18:30:45Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5641",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI'm encountering the following error when running my code (see below) with multi-GPUs (single GPU and CPU works fine). `accelerator` used is `ddp`.\r\n```\r\nline 117, in test_epoch_end\r\nwork = _default_pg.allreduce([tensor], opts)\r\nRuntimeError: self.log(\"avg_test_acc\", avg_test_acc, sync_dist=True)Tensors must be CUDA and dense\r\n```\r\nHowever, when I remove the `sync_dist=True` all goes well.\r\n\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\nThe code, at it's core, looks like this:\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom torchvision import datasets, transforms\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.autograd import Variable\r\nfrom argparse import ArgumentParser\r\nfrom pytorch_lightning.metrics.functional import accuracy\r\nfrom torch.nn import functional as F\r\nfrom argparse import ArgumentParser\r\nimport mlflow\r\nfrom data_loading.data_loader import MNISTDataModule\r\nfrom model.model import LightningMNISTClassifier\r\nimport os\r\n\r\nclass MNISTDataModule(pl.LightningDataModule):\r\n    def __init__(self, **kwargs):\r\n        super(MNISTDataModule, self).__init__()\r\n        self.df_train = None\r\n        self.df_test = None\r\n        self.train_data_loader = None\r\n        self.test_data_loader = None\r\n        self.args = kwargs\r\n\r\n        # transforms for images\r\n        self.transform = transforms.Compose(\r\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\r\n        )\r\n\r\n    def setup(self, stage=None):\r\n        self.df_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        self.df_test = datasets.MNIST(\r\n            \"dataset\", download=True, train=False, transform=self.transform\r\n        )\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n            self.df_train, batch_size=self.args['training_batch_size'], num_workers=self.args[\"num_workers\"], shuffle=True\r\n        )\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(\r\n            self.df_test, batch_size=self.args['test_batch_size'], num_workers=self.args[\"num_workers\"], shuffle=False\r\n        )\r\nclass LightningMNISTClassifier(pl.LightningModule):\r\n    def __init__(self, len_test_set: int, **kwargs):\r\n        super(LightningMNISTClassifier, self).__init__()\r\n        self.optimizer = None\r\n        self.conv1 = torch.nn.Conv2d(1, 32, 3, 1)\r\n        self.conv2 = torch.nn.Conv2d(32, 64, 3, 1)\r\n        self.dropout1 = torch.nn.Dropout2d(0.25)\r\n        self.fc1 = torch.nn.Linear(9216, 128)\r\n        self.dropout2 = torch.nn.Dropout2d(0.25)\r\n        self.fc2 = torch.nn.Linear(128, 10)\r\n        self.args = kwargs\r\n        self.len_test_set = len_test_set\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument(\"--num_workers\", type=int, default=3, metavar=\"N\", help=\"number of workers (default: 3)\")\r\n        parser.add_argument(\"--lr\", type=float, default=0.01, help=\"learning rate (default: 0.01)\")\r\n        parser.add_argument('--training-batch-size', type=int, default=64, help='Input batch size for training')\r\n        parser.add_argument('--test-batch-size', type=int, default=1000, help='Input batch size for testing')\r\n\r\n        return parser\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.conv1(x))\r\n        x = F.relu(self.conv2(x))\r\n        x = F.max_pool2d(x, 2)\r\n        x = torch.flatten(self.dropout1(x), 1)\r\n        x = F.relu(self.fc1(x))\r\n        x = self.dropout2(x)\r\n        x = self.fc2(x)\r\n        output = F.log_softmax(x, dim=1)\r\n\r\n        return output\r\n\r\n    def cross_entropy_loss(self, logits, labels):\r\n        return F.nll_loss(logits, labels)\r\n\r\n    def training_step(self, train_batch, batch_idx):\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"loss\": loss}\r\n\r\n    def training_epoch_end(self, training_step_outputs):\r\n        train_avg_loss = torch.stack([train_output[\"loss\"] for train_output in training_step_outputs]).mean()\r\n        self.log(\"train_loss\", train_avg_loss)\r\n\r\n    def test_step(self, test_batch, batch_idx):\r\n        \"\"\"\r\n        Predicts on the test dataset to compute the current accuracy of the model.\r\n\r\n        :param test_batch: Batch data\r\n        :param batch_idx: Batch indices\r\n\r\n        :return: output - Testing accuracy\r\n        \"\"\"\r\n\r\n        x, y = test_batch\r\n        output = self.forward(x)\r\n        _, y_hat = torch.max(output, dim=1)\r\n        test_acc = accuracy(y_hat.cpu(), y.cpu())\r\n        # sum up batch loss\r\n        data, target = Variable(x), Variable(y)\r\n        test_loss = F.nll_loss(output, target, reduction='sum').data.item()\r\n        # get the index of the max log-probability\r\n        pred = output.data.max(1)[1]\r\n        correct = pred.eq(target.data).cpu().sum().item()\r\n        return {\"test_acc\": test_acc, \"test_loss\": test_loss, \"correct\": correct}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average test accuracy score\r\n\r\n        :param outputs: outputs after every epoch end\r\n\r\n        :return: output - average test loss\r\n        \"\"\"\r\n        avg_test_acc = torch.stack([test_output[\"test_acc\"] for test_output in outputs]).mean()\r\n        avg_test_loss = sum([test_output[\"test_loss\"] for test_output in outputs])/self.len_test_set\r\n        test_correct = sum([test_output[\"correct\"] for test_output in outputs])\r\n        self.log(\"avg_test_acc\", avg_test_acc, sync_dist=True)\r\n        self.log(\"avg_test_loss\", avg_test_loss, sync_dist=True)\r\n        self.log(\"test_correct\", test_correct, sync_dist=True)\r\n\r\n    def prepare_data(self):\r\n        \"\"\"\r\n        Prepares the data for training and prediction\r\n        \"\"\"\r\n        return {}\r\n\r\n    def configure_optimizers(self):\r\n        \"\"\"\r\n        Initializes the optimizer and learning rate scheduler\r\n\r\n        :return: output - Initialized optimizer and scheduler\r\n        \"\"\"\r\n        self.optimizer = torch.optim.Adam(self.parameters())\r\n        return [self.optimizer]\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    parser = ArgumentParser(description=\"PyTorch Autolog Mnist Example\")\r\n    use_cuda = torch.cuda.is_available()\r\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n\r\n    parser = pl.Trainer.add_argparse_args(parent_parser=parser)\r\n    parser = LightningMNISTClassifier.add_model_specific_args(parent_parser=parser)\r\n\r\n    mlflow.pytorch.autolog()\r\n    # parse cli arguments\r\n    args = parser.parse_args()\r\n    dict_args = vars(args)\r\n\r\n    set_general_random_seeds(dict_args['general_seed'])\r\n    set_pytorch_random_seeds(dict_args['pytorch_seed'], True)\r\n\r\n    if \"accelerator\" in dict_args and dict_args[\"accelerator\"] == \"None\":\r\n        dict_args[\"accelerator\"] = None\r\n\r\n    dm = MNISTDataModule(**dict_args)\r\n\r\n    dm.prepare_data()\r\n    dm.setup(stage=\"fit\")\r\n    model = LightningMNISTClassifier(len_test_set=len(dm.df_test), **dict_args)\r\n    trainer = pl.Trainer.from_argparse_args(args)\r\n     \r\n\r\n    trainer.deterministic = True\r\n    trainer.benchmark = False\r\n    trainer.fit(model, dm)\r\n    trainer.test()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\nTrain and test successfully without mentioned error above when using multiple GPUs (like it runs successfully on single GPU and CPU).\r\n\r\n### Environment\r\n\r\n* CUDA \r\n\t* GPU:\r\n\t\t* NVIDIA [Tesla V100 PCIe 32GB] \r\n\t* available: True\r\n\t* Version 11.2\r\n* Packages\r\n\t* cudatoolkit=10.1\r\n\t* numpy                     =1.19.1\r\n\t* torchvision             =  0.7.0 \r\n\t* pytorch-lightning=1.1.5\r\n\t* pycuda=2019.1.2\r\n\t* python=3.8.2\r\n\t* pytorch=1.6.0\r\n\t\r\n\r\n - OS: Linux Ubuntu Ubuntu 18.04.3 LTS\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5641/comments",
    "author": "Imipenem",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-01-25T01:38:22Z",
        "body": "Remove the .cpu() call \r\n`correct = pred.eq(target.data).cpu().sum().item()`\r\nshould be \r\n`correct = pred.eq(target.data).sum()`"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-01-25T01:40:28Z",
        "body": "And if you want to compute accuracy on multi gpu correctly, I recommend directly using the Accuracy metric (from pytorch_lightning.metrics)"
      },
      {
        "user": "Imipenem",
        "created_at": "2021-01-25T18:30:45Z",
        "body": "many thanks for the prompt answer, will use PyTorch lightning metrics ;)"
      }
    ]
  },
  {
    "number": 5637,
    "title": "Add a bot for changelog",
    "created_at": "2021-01-24T11:45:06Z",
    "closed_at": "2021-03-29T17:08:11Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5637",
    "body": "## 🚀 Feature\r\nAdd a bot to check whether CHANGELOG.md is updated or not in a PR\r\n\r\n### Motivation\r\nWe can add a bot to check if CHANGELOG.md is updated or now wherever it's required. We can mark this as required.\r\n\r\n**release branch**\r\nIf possible to do with milestones then milestones set to `release/xxx` must have a CHANGELOG since all the changes made considering base as the release branch are features/enhancement. Other changes made there are like adding a new changelog section or syncing master to release branch, CHANGELOG is anyways updated.\r\n\r\n**master**\r\nFor the master branch, if a PR has labels `documentation`, `tests/ci`, `tutorial/examples` then we can ignore this requirement. For rest it should be marked as required.\r\n\r\n### Alternatives\r\nManually check if it's updated or not.\r\n\r\nOf course open to suggestions here. Please feel free to edit/update as per requirement.\r\n\r\ncc @Borda ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5637/comments",
    "author": "rohitgr7",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2021-02-16T18:37:01Z",
        "body": "Great idea!"
      },
      {
        "user": "Borda",
        "created_at": "2021-02-16T18:42:19Z",
        "body": "@rohitgr7 sorry for the long delay, yes according to labels or milestones it sounds good\r\ntalking about branch name - that would be suboptimal as you cannot change branch name once PR is created..."
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-02-16T18:45:34Z",
        "body": "@Borda I meant the base branch up there. But I guess that won't be a problem since we have only the master branch now. But we can still do it with just labels/milestones."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-03-19T19:11:32Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 5626,
    "title": "Failing to log to Neptune.ai when resuming from checkpoint",
    "created_at": "2021-01-23T11:17:18Z",
    "closed_at": "2021-01-23T17:49:29Z",
    "labels": [
      "bug",
      "duplicate",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5626",
    "body": "## 🐛 Bug\r\n\r\nI'm trying to resume training from a checkpoint, and I'm trying to resume logging using the Neptune.ai logger, but it throws this particular error:\r\n\r\n```\r\nneptune.api_exceptions.ChannelsValuesSendBatchError: Received batch errors sending channels' values to experiment CAS-68. \r\nCause: Error(code=400, message='X-coordinates must be strictly increasing for channel: b7ab6110-7a1b-4093-a35f-dc0904ff943f. Invalid point: InputChannelValue(timestamp=2021-01-23T11:02:05.882Z, x=98.0, numericValue=0.30072227120399475, text', type=None) (metricId: 'b7ab6110-7a1b-4093-a35f-dc0904ff943f', x: 98.0) Skipping 3 values.\r\n```\r\nAt the point that I'm resuming, I've already trained for 7000 steps, yet the logger tries to log at step `x=98.0` as seen in the error message. \r\n\r\n## Please reproduce using the BoringModel\r\n\r\nI'm not sure how I should reproduce this bug since it's more related to how Neptune.ai's logger specifically works, but I can try to come up with a repro soon.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version: 11.1\r\n - GPU models and configuration: Tesla V100\r\n - Any other relevant information:",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5626/comments",
    "author": "briankosw",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-01-23T17:49:29Z",
        "body": "Duplicate? #5130 \r\nA PR for a fix is in development.\r\nClosing this because the other issue has a repro script already, and the error here seems to be idenical. Thanks for reporting"
      }
    ]
  },
  {
    "number": 5615,
    "title": "configure_callbacks hook for LightningModule",
    "created_at": "2021-01-22T16:20:08Z",
    "closed_at": "2021-02-13T00:27:44Z",
    "labels": [
      "feature",
      "help wanted",
      "priority: 0",
      "callback"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5615",
    "body": "## 🚀 Feature\r\n\r\n### Motivation\r\n\r\nProvide model-specific callbacks in a LightningModule\r\n\r\n### Pitch\r\n\r\nAdd a configure_callbacks method to LightningModule\r\nAppend the returned callbacks to the trainer callbacks. \r\n\r\n### Additional context\r\n\r\ncc @edenlightning \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5615/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2021-01-22T17:30:49Z",
        "body": "Improvements:\n* Supporting priorities for callbacks\n\nCallbackA(priority=1)\nCallbackB(priority=1)\nCallbackC(priority=2)\nIn this case, A, B are called sequentially (order does not matter between A, B because both are priority 1.\n\nAnd C is called after\n\nA, B\nC\nor\n\nB, A\nC"
      },
      {
        "user": "edenlightning",
        "created_at": "2021-01-22T17:31:39Z",
        "body": "Another improvments: Support child callbacks, to customize callback order\n\nAs a user, I want callbacks to be called in a specific order.\n\na = CallbackA()\nb = CallbackB()\nc = CallbackC()\nd = CallbackD()\n\na.child_callback(b)\na.child_callback(c)\nc.chold_callback(d)\nThe dag looks like:\n\na -> B\n-> C -> D"
      }
    ]
  },
  {
    "number": 5592,
    "title": "tensorboard displays incorrect Learning-rates",
    "created_at": "2021-01-20T18:59:24Z",
    "closed_at": "2021-01-20T22:06:53Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5592",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nFor example, if the learning-rate is euler's e-6 (i.e. 0.00247875217), then it is displayed as 2.4788e-3 on the vertical axis of tensorboard's graph. The correct value should be 2.4788 * (10 raised to the power of -3).\r\nAnother example, if the learning-rate is euler's e-9 (i.e. 0.0001234098), then it is displayed as 1.2341e-4 on the vertical axis of tensorboard's graph. The correct value should be 1.2341 * (10 raised to the power of -4).\r\n\r\nSo, instead of \"e\" there should be \"10\".\r\nI am using the LearningRateMonitor.\r\n\r\n## Please reproduce using the BoringModel\r\nUsing PDB, I traced the code when learning rate is 0.00247875217. Everything is fine up until the following:\r\n\r\n> /home/vin/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/lr_monitor.py(156)_extract_lr()\r\n-> return {name: lr}\r\n(Pdb) name, lr\r\n('lr-Adam', 0.00247875217)\r\n\r\n> /home/vin/.local/lib/python3.8/site-packages/tensorboard/summary/writer/event_file_writer.py(118)add_event()\r\n-> self._async_writer.write(event.SerializeToString())\r\n(Pdb) event\r\nwall_time: 1611158418.5658185\r\nsummary {\r\n  value {\r\n    tag: \"lr-Adam\"\r\n    simple_value: 0.0024787522852420807\r\n  }\r\n}\r\n\r\nHowever, I am unable to trace how this number is incorrectly displayed. \r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\nFor the learning-rate, the TensorBoard should disply \"10\" instead of \"e\".\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - OS (e.g., Linux): Linux-Ubuntu\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8.5\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: Nvidea GTX-1080\r\n - Any other relevant information: Pytorch Lightning version 1.1.4\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5592/comments",
    "author": "vineetk1",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-01-20T21:48:08Z",
        "body": "It's just scientific notation. Try printing `1.2341e-4` in your python console. You will get `~0.0001234098`"
      }
    ]
  },
  {
    "number": 5572,
    "title": "When overwriting the setup() function in the LightningDataModule, it is forced to add an unnecessary parameter.",
    "created_at": "2021-01-19T14:10:32Z",
    "closed_at": "2021-01-19T14:38:56Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5572",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## When overwriting the setup() function in the LightningDataModule, it is forced to add an unnecessary parameter.\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0):1.7\r\n - OS (e.g., Linux):Windows\r\n - How you installed PyTorch (`conda`, `pip`, source):pip\r\n - Python version:3.6.12\r\n - CUDA/cuDNN version:11.0\r\n - GPU models and configuration: \r\n - Any other relevant information: def setup(self)\r\n\r\n### Additional context\r\n\r\nI have to add another argument to setup(self) for it to work, such as setup(self,a), which I won't actually use at all.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5572/comments",
    "author": "Toyhom",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-01-19T14:36:44Z",
        "body": "the other parameter refers to `stage` which can be 'fit'(called with `trainer.fit`) or 'test'(called with `trainer.test`). Using this you can avoid loading both train & val and test data all at once.\r\nsomething like:\r\n```python\r\ndef setup(self, stage):\r\n    if stage == 'fit':  # will be 'fit' when calling trainer.fit()\r\n        # load train & val data only\r\n    elif stage == 'test':  # will be 'test' when calling trainer.test()\r\n        # load test data only\r\n```"
      },
      {
        "user": "Toyhom",
        "created_at": "2021-01-19T14:38:56Z",
        "body": "Thank you for your answer."
      }
    ]
  },
  {
    "number": 5515,
    "title": "Improve handling progressbar refresh rate in Google Colab",
    "created_at": "2021-01-14T16:21:29Z",
    "closed_at": "2021-01-20T01:18:06Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5515",
    "body": "## 🚀 Feature\r\n\r\nSet progress bar refresh rate automatically in Colab (as is right now), but still allow user to override if needed.\r\n\r\n### Motivation\r\n\r\nWant no warnings in colab, but at the same time not crash the browser and also not prevent user from overriding the value.\r\n\r\n### Pitch\r\n\r\nBefore:\r\n```python\r\nTrainer(progress_bar_refresh_rate: int = 1)\r\n```\r\n\r\nNow:\r\n\r\nRemove warning and implement the following:\r\n\r\n```python\r\nTrainer(progress_bar_refresh_rate: int = None)\r\n\r\nif colab and refresh_rate = None:\r\n    refresh_rate = 20\r\nelif refresh_rate = None\r\n   refresh_rate = 1\r\n\r\n# otherwise we keep value passed in by user\r\n```\r\n\r\n\r\n### Alternatives\r\n\r\nCurrently, user can't override the value in Colab. \r\n\r\n### Additional context\r\n\r\nDiscussed in slack with @edenlightning \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5515/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2021-01-14T17:39:10Z",
        "body": "well just for the record, the user can override this today in colab AFAIK. please also note that the warning can be removed."
      }
    ]
  },
  {
    "number": 5462,
    "title": "TPU and multo-GPU for RL",
    "created_at": "2021-01-11T12:15:57Z",
    "closed_at": "2021-03-22T07:34:04Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5462",
    "body": "## 🚀 Feature\r\nRL on TPUs and multi-GPUs\r\n\r\n### Motivation\r\n\r\nRun \"seed rl\" and other  asynchronous and distributed learning in/for RL\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5462/comments",
    "author": "paantya",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-11T12:16:40Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-02-10T17:22:27Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "paantya",
        "created_at": "2021-02-10T19:39:31Z",
        "body": "не нужно закрывать"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-03-15T06:58:00Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 5447,
    "title": "Remove unused import in `accelerators`",
    "created_at": "2021-01-10T12:31:49Z",
    "closed_at": "2021-01-10T19:53:55Z",
    "labels": [
      "feature",
      "help wanted",
      "refactor"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5447",
    "body": "## 🚀 Feature\r\n`pytorch_lightning/acclerators/*.py`\r\nThere are many unused import in `pytorch_lightning/acclerators`. These should be removed.\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5447/comments",
    "author": "archsyscall",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2021-01-10T19:53:55Z",
        "body": "it seems to be already done in `release/1.2-dev` also see #4805 "
      }
    ]
  },
  {
    "number": 5363,
    "title": "Use scheduler.get_last_lr() instead of manually searching for optimizers.param_groups",
    "created_at": "2021-01-05T16:38:39Z",
    "closed_at": "2021-02-09T20:30:11Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5363",
    "body": "## 🐛 Bug / Feature Request\r\nI am not sure whether to classify this issue as a feature request or bug, but in your codebase, you get the learning rate from schedulers as ``scheduler.optimizer.param_groups[0]['lr']``.\r\nHowever, PyTorch provides the ``get_last_lr()`` method for this.\r\n\r\nI created my own scheduler, which allows me to combine multiple schedulers together (eg. burn-in LR for the first few steps, and then step down LR later during training). \r\nAs this scheduler holds multiple different schedulers itself, there is no ``scheduler.optimizer`` argument and thus I cannot use this with PyTorch Lightning.\r\nI did however implement the ``get_last_lr()`` method, which gets the LR from the underlying schedulers, so if you could modify the PyTorch Lightning code to use that method, it should all work!\r\n\r\n## Problems\r\nObviously, nothing is ever as straight forward with the PyTorch scheduler API, which means that there might be problems with this solution. One small issue I found is that ``ReduceLROnPlateau`` is behaving differently and does not implement the ``get_last_lr()`` method. (The PyTorch team should really look into unifying their API, but that should be discussed there...) \r\n\r\nIf you look at GH-5364, which I made on top of the `release/1.2-dev` branch, you will see that for the ``ReduceLROnPlateau`` scheduler, I still use the old method of finding the LR through the ``scheduler.optimizer`` object.\r\nHowever, for any other scheduler, we use the ``get_last_lr()`` method.\r\n\r\nI could adapt the code to be even more defensive and check whether there is a ``get_last_lr()`` method and only use it if it is available, but I don't know whether that is necessary, as all other official schedulers have the same base Scheduler class which implements this method.\r\n\r\n---\r\n\r\nI send in a Draft PR with the necessary changes.\r\nKind Regards,\r\n0phoff",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5363/comments",
    "author": "0phoff",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2021-01-06T12:10:45Z",
        "body": "The original reason why we get the value from `scheduler.optimizer.param_groups[0]['lr']` instead of using `get_last_lr()` was that `get_last_lr` was first introduced in pytorch v.1.3 (i think) and at that point in time, our code were backward compatible to v.1.1. We could therefore not rely on `get_last_lr` when this was first implemented.\r\nHowever, since v1.3 is minimum requirement now I see no problem with this change.\r\n"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2021-02-09T20:30:07Z",
        "body": "Closing as we decided in the corresponding PR (#5364) to not support this for now. "
      }
    ]
  },
  {
    "number": 5304,
    "title": "TestTubeLogger fails to log tensorboard hparams",
    "created_at": "2020-12-30T17:23:01Z",
    "closed_at": "2021-01-02T16:34:48Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5304",
    "body": "## 🐛 Bug\r\n\r\nI recall TestTubeLogger used to give me an HParams tab in tensorboard. Possibly since the changes described #2974 #3610 this is no longer the case, and the \"fix\" seems to entail discarding TestTubeLogger and using TensorboardLogger instead, but I am unable to do such a thing due to heavy code dependence on TestTubeLogger. There is never an HParams tab in TB regardless of calls to `tt_logger.log_hyperparams(config)` and `self.save_hyperparameters(config)` in my LightningModule constructor.\r\n\r\nSwitching to TensorboardLogger **does** cause the HParams tab to appear as expected, as suggested in the aforementioned issues. This issue is limited to PL's TestTubeLogger.\r\n\r\nDoes TestTubeLogger have the capability of writing HParams data for TB? Perhaps I am mistaken, and this was never possible. \r\n\r\nThanks!\r\n\r\n### Environment\r\npytorch-lightning==1.1.2\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5304/comments",
    "author": "thavlik",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-30T17:23:43Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "s-rog",
        "created_at": "2020-12-31T06:35:12Z",
        "body": "I'm pretty sure test tube logger only logs hparams to the text tab of tensorboard (unless there was a PR I missed). If you need the hparams view, you'd have to use tb logger... there shouldn't be too much code you need to change?\r\n\r\nAlso the linked tb logger changes should not affect test tube.\r\n"
      },
      {
        "user": "thavlik",
        "created_at": "2021-01-02T16:34:48Z",
        "body": "I was indeed mistaken, and the HParams tab appears during my hparam search as a result of calls to `tune.report()` (I use PL for the main implementation and wrap it with ray/tune for hparam searches).\r\n\r\nThank you for the clarification!"
      }
    ]
  },
  {
    "number": 5275,
    "title": "Clean printing to terminal alongside progress bar",
    "created_at": "2020-12-27T07:49:20Z",
    "closed_at": "2021-02-22T09:40:18Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5275",
    "body": "## 🚀 Feature\r\nA way to print to terminal without breaking up the progress bar.\r\n\r\n### Motivation\r\nA lot of people print stuff to terminal while training/validating/testing, and currently a simple call to print() will break the progress bar. A way to get around this is to set up a custom progress bar, with methods for calling tqdm.write, and passing that as a callback to the trainer. However, this feels like a lot of effort for just getting clean terminal output alongside the progress bar.\r\n\r\n### Pitch\r\n\r\nAbility to get the ProgressBar - or the current active tqdm instance (main_progress_bar, val_progres_bar, etc) - through the Trainer or the LightningModule.\r\nSomething like pbar = trainer.pbar would feel intuitive.\r\nThen, the user can call pbar.write(), and get clean printing like tqdm.write().",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5275/comments",
    "author": "juneskiafc",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-27T07:49:58Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-12-27T13:21:56Z",
        "body": "hey! that's a neat idea, I really like it. How about we integrate it with the already existing `LightningModule.print`?\r\nYou can currently access the progress bar like so:\r\n```python\r\nself.trainer.progress_bar_callback.main_progress_bar.write(\"hello\")\r\n```"
      },
      {
        "user": "juneskiafc",
        "created_at": "2020-12-28T23:23:10Z",
        "body": "That sounds great! I actually didn't know you could do trainer.progres_bar_callback, maybe I missed it in the docs.\r\nI think that way of accessing the tqdm.write() method is okay, maybe that should be added to the docs under the Trainer properties section for now?\r\nI do think there is value forcing LightningModule.print to print without interfering with the progress bar, though.\r\n"
      },
      {
        "user": "asnorkin",
        "created_at": "2021-01-05T14:26:20Z",
        "body": "I'll happily work on this :)\r\n\r\nI propose to change current LightningModule.print() method:\r\n```\r\nclass LightningModule:\r\n    ...\r\n    def print(self, *args, **kwargs) -> None:\r\n        if self.trainer.is_global_zero:\r\n            print(*args, **kwargs)\r\n```\r\nto this one:\r\n```\r\nclass LightningModule:\r\n    ...\r\n    def print(self, *args, **kwargs) -> None:\r\n        if self.trainer.is_global_zero:\r\n            progress_bar = self.trainer.progress_bar_callback\r\n            if progress_bar.is_enabled:\r\n                progress_bar.print(*args, **kwargs)\r\n            else:\r\n                print(*args, **kwargs)\r\n```\r\n\r\nAnd add progress_bar.print(*args, **kwargs) method to the ProgressBar class."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-01-05T14:31:00Z",
        "body": "sounds reasonable! in your snippet above, make sure to also include a None check for progress bar callback on trainer. \r\ncheers"
      }
    ]
  },
  {
    "number": 5274,
    "title": "Loading checkpoint creates new logging directory and tries to load checkpoint from that directory",
    "created_at": "2020-12-27T02:46:57Z",
    "closed_at": "2020-12-28T00:03:10Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5274",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI'm doing training and then loading the checkpoint later to do some experiments.  I'm using hydra.  What's happening is the checkpoints are being created as expected.  However, after I load the checkpoint with load_from_checkpoint a new output directory is created, and the checkpoint attempting to be read from that new output directory (and so of course it can't find it).\r\n```\r\nif cfg.train is True :\r\n        trainer = Trainer(max_epochs=cfg.max_epochs, gpus=cfg.gpus)\r\n        model = Net(cfg)\r\n        trainer.fit(model)\r\n        trainer.test(model)\r\n    else :\r\n        # plot some data\r\n        model = Net.load_from_checkpoint(cfg.checkpoint)\r\n        model.eval()\r\n        ...\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5274/comments",
    "author": "jloveric",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-12-27T07:37:45Z",
        "body": "Hi, could you help us reproduce this with the minimal bug report model found in the pl_examples folder? The `load_from_checkpoin`t function does not create new directories by itself, so there must be some other code in your case that creates this. When you use the `Trainer.test` method make sure to pass in the loaded model, otherwise it will attempt to load one."
      },
      {
        "user": "jloveric",
        "created_at": "2020-12-28T00:03:10Z",
        "body": "Turns out you need the full path otherwise it tries to load from a new path that it creates.  Not sure if this is just with Hydra or not."
      }
    ]
  },
  {
    "number": 5258,
    "title": "During manual training, automatically create corresponding dataloader for different settings (distributed, multi-gpu etc.)",
    "created_at": "2020-12-24T11:13:58Z",
    "closed_at": "2021-01-30T13:45:29Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5258",
    "body": "## 🚀 Feature\r\nA function that automatically converts the newly created dataloader inside the training loop for different settings, such as multi-GPU, distributed, etc. \r\n\r\n### Motivation\r\nIn reinforcement learning, it is widespread to create new data from the environment. These new training data will be fed into a new dataloader. In fact, in some algorithms, we even need to create dataloader from a batch (the most predominant approach is self-imitation learning). In this situation, the only way is to manually change the dataloader or sampler, which will be very inconvenient. \r\n\r\n### Pitch\r\nIt would be beneficial if there is a hook. As long as the new DataLoader is created, a method will automatically convert it to appropriate dataloaders specified by the training methods. \r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5258/comments",
    "author": "rwbfd",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2021-01-23T11:31:25Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 5219,
    "title": "Typing: LightningModule casted to double turns into nn.Module",
    "created_at": "2020-12-21T14:43:32Z",
    "closed_at": "2021-01-18T17:14:22Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 2"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5219",
    "body": "## 🐛 Bug\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass PLModule(pl.LightningModule):\r\n    pass\r\n\r\n\r\nmodule = PLModule().double()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(module)\r\n\r\n```\r\n\r\nYou will get a typing warning on the last line in PyCharm:\r\n`Expected type 'LightningModule', got 'Module' instead `\r\n\r\n### Expected behavior\r\n\r\nNo typing warnings\r\n\r\n### Environment\r\n\r\nPyCharm 2020.3\r\npytorch-lightning 1.1.1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5219/comments",
    "author": "pisarik",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-21T14:44:15Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "tchaton",
        "created_at": "2021-01-18T17:12:50Z",
        "body": "Hey @pisarik,\n\nI tried with BoringModel and everything seems fine. \n\n```\ndef test_lightning_module_conversion(tmpdir):\n\n    class Model(BoringModel):\n        def step(self, x):\n            x = self(x.double())\n            out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\n            return out\n\n        def training_step(self, batch, batch_idx):\n            output = self.layer(batch.double())\n            loss = self.loss(batch, output)\n            return {\"loss\": loss}\n\n        def validation_step(self, batch, batch_idx):\n            output = self.layer(batch.double())\n            loss = self.loss(batch, output)\n            return {\"x\": loss}\n\n    model = Model()\n    model = model.double()\n    assert isinstance(model, LightningModule)\n    model.training_epoch_end = None\n\n    trainer = Trainer(\n        max_epochs=1,\n        default_root_dir=tmpdir,\n        limit_train_batches=8,\n        accumulate_grad_batches=1,\n    )\n\n    trainer.fit(model)\n```\n\nI am closing this issue as I couldn't reproduce the bug with provided sample."
      },
      {
        "user": "pisarik",
        "created_at": "2021-01-20T19:39:45Z",
        "body": "@tchaton The reason why it works for you is because you added the line `assert isinstance(model, LightningModule)`.\r\nPycharm recognizes such lines and assumes that after assert the `model` is of type `LightningModule`, otherwise it would raise an error.\r\n\r\nIf you will remove the line with assert, then you will get exactly the same typing warning as I described.\r\nHowever, if you will further remove `.double()`, then the typing warning disappear. I.e. casting model to double somehow breaks typings (perhaps because it returns `Module` instead of `LightningModule`)."
      }
    ]
  },
  {
    "number": 5144,
    "title": "Trainer test cannot load from checkpoint when training on multiple GPUs",
    "created_at": "2020-12-15T10:30:46Z",
    "closed_at": "2021-01-25T22:00:19Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5144",
    "body": "## 🐛 Bug\r\n\r\nThe Trainer.test() looks for `epoch=X-v0.ckpt` when only `epoch=X.ckpt` exists, thus the result is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/wojciech/tmp/pytorch-lightining/main.py\", line 16, in <module>\r\n    result = trainer.test()\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 721, in test\r\n    results = self.__test_using_best_weights(ckpt_path, test_dataloaders)\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 751, in __test_using_best_weights\r\n    ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 31, in load\r\n    with fs.open(path_or_url, \"rb\") as f:\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/fsspec/spec.py\", line 897, in open\r\n    f = self._open(\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/fsspec/implementations/local.py\", line 115, in _open\r\n    return LocalFileOpener(path, mode, fs=self, **kwargs)\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/fsspec/implementations/local.py\", line 197, in __init__\r\n    self._open()\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/fsspec/implementations/local.py\", line 202, in _open\r\n    self.f = open(self.path, mode=self.mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/wojciech/tmp/pytorch-lightining/lightning_logs/version_10/checkpoints/epoch=0-v0.ckpt'\r\n```\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nExecute **several times** on >1 gpu machine:\r\n\r\n```python\r\n#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import NeptuneLogger\r\n\r\nimport os\r\nfrom typing import Any, Optional\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningDataModule\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision.transforms import transforms\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom pytorch_lightning.core.lightning import LightningModule\r\nfrom torch.optim import Adam\r\n\r\n\r\nclass MNISTModule(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # mnist images are (1, 28, 28) (channels, width, height)\r\n        self.layer_1 = torch.nn.Linear(28 * 28, 128)\r\n        self.layer_2 = torch.nn.Linear(128, 256)\r\n        self.layer_3 = torch.nn.Linear(256, 10)\r\n\r\n    def forward(self, x):\r\n        batch_size, channels, width, height = x.size()\r\n\r\n        # (b, 1, 28, 28) -> (b, 1*28*28)\r\n        x = x.view(batch_size, -1)\r\n        x = self.layer_1(x)\r\n        x = F.relu(x)\r\n        x = self.layer_2(x)\r\n        x = F.relu(x)\r\n        x = self.layer_3(x)\r\n\r\n        x = F.log_softmax(x, dim=1)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x)\r\n        loss = F.nll_loss(logits, y)\r\n        self.log('train_loss', loss, on_step=True, on_epoch=True)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x)\r\n        loss = F.nll_loss(logits, y)\r\n        self.log('val_loss', loss, on_step=False, on_epoch=True)\r\n        return loss\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x)\r\n        loss = F.nll_loss(logits, y)\r\n        self.log('test_loss', loss, on_step=False, on_epoch=True)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        opt = Adam(self.parameters(), lr=1e-3)\r\n        return opt\r\n\r\n\r\n\r\n# noinspection PyAttributeOutsideInit\r\nclass MNISTDataModule(LightningDataModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.train_dims = None\r\n        self.vocab_size = 0\r\n\r\n    def prepare_data(self):\r\n        # called only on 1 GPU\r\n        MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\r\n        MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\r\n\r\n    def setup(self, stage: Optional[str] = None):\r\n        # called on every GPU\r\n        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\r\n        self.train = MNIST(os.getcwd(), train=True, download=False, transform=transform)\r\n        self.test = MNIST(os.getcwd(), train=False, download=False, transform=transform)\r\n\r\n        self.train, self.val = torch.utils.data.random_split(self.train, (50000, 10000))\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.train, batch_size=64, shuffle=True, drop_last=True, num_workers=2)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.val, batch_size=512, drop_last=False)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.test, batch_size=512, drop_last=False)\r\n\r\n\r\nif __name__ == '__main__':\r\n    dm = MNISTDataModule()\r\n    model = MNISTModule()\r\n\r\n    params = dict(param1='a', param2=1)\r\n    trainer = Trainer(gpus=2, max_epochs=1, accelerator='ddp')\r\n    trainer.fit(model, datamodule=dm)\r\n\r\n    result = trainer.test()\r\n    print(result)\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo exception.\r\n\r\n### Environment\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce GTX TITAN X\r\n                - GeForce GTX TITAN X\r\n                - GeForce GTX TITAN X\r\n                - GeForce GTX TITAN X\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.4\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.7.1\r\n        - pytorch-lightning: 1.1.0 [Also 1.0.8]\r\n        - tqdm:              4.54.1\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.5\r\n        - version:           #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020\r\n\r\n```\r\n### Additional context\r\n\r\nThis happens only when `gpus=2` and acceleration=`ddp`. There must be some race condition since this problem occurs every now and then only. \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5144/comments",
    "author": "wjaskowski",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-12-16T07:16:22Z",
        "body": "@wjaskowski I believe I found the fix for it. Thanks for including a reproducible script, it helped alot. \r\nIf you find the time, would you mind checking if the fix on my branch works for you? (see linked PR)"
      },
      {
        "user": "wjaskowski",
        "created_at": "2020-12-16T08:43:44Z",
        "body": "I tried the branch but the problem seems to be still there:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/wojciech/tmp/pytorch-lightining/bug3.py\", line 113, in <module>\r\n    result = trainer.test()\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 754, in test\r\n    results = self.__test_using_best_weights(ckpt_path, test_dataloaders)\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 784, in __test_using_best_weights\r\n    ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py\", line 31, in load\r\n    with fs.open(path_or_url, \"rb\") as f:\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/fsspec/spec.py\", line 897,\r\nin open\r\n    f = self._open(\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/fsspec/implementations/local.py\", line 115, in _open\r\n    return LocalFileOpener(path, mode, fs=self, **kwargs)\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/fsspec/implementations/local.py\", line 197, in __init__\r\n    self._open()\r\n  File \"/home/wojciech/miniconda3/envs/ml/lib/python3.8/site-packages/fsspec/implementations/local.py\", line 202, in _open\r\n    self.f = open(self.path, mode=self.mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/wojciech/tmp/pytorch-lightining/lightning_logs/version_45/checkpoints/epoch=0-step=259-v0.ckpt'\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-12-17T06:26:40Z",
        "body": "this is surprising. with your exact script I can get the error on master within 2-3 trials, but on the bugfix branch (bugfix/ddp-ckpt) I ran it probably 20+ times and it never occurs. \r\n"
      },
      {
        "user": "wjaskowski",
        "created_at": "2020-12-17T08:49:05Z",
        "body": "I will carefully give it a try once again when I will regain access to a machine with >1 GPUs."
      },
      {
        "user": "edenlightning",
        "created_at": "2021-01-08T21:17:22Z",
        "body": "@wjaskowski any update?"
      },
      {
        "user": "edenlightning",
        "created_at": "2021-01-25T22:00:19Z",
        "body": "Feel free to reopen if needed!"
      },
      {
        "user": "sustcsonglin",
        "created_at": "2021-03-12T02:04:17Z",
        "body": "I have the same issues... "
      },
      {
        "user": "blacksnail789521",
        "created_at": "2023-03-23T14:27:15Z",
        "body": "Same issue in 2023"
      }
    ]
  },
  {
    "number": 5115,
    "title": "Proposal: Initialize DataModule from Dataset(s)",
    "created_at": "2020-12-14T03:52:18Z",
    "closed_at": "2021-02-11T14:32:41Z",
    "labels": [
      "feature",
      "help wanted",
      "discussion"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5115",
    "body": "## 🚀 Feature\r\n\r\nAdd functionality to initialize a `pl.DataModule` from pre-existing datasets, which could help eliminate some of the boilerplate caused from initializing `torch.utils.data.DataLoader`s.\r\n\r\n### Motivation\r\n\r\nA common pattern we have seen some users using is creating a DataModule wrapper that instantiates the necessary dataloaders given a train dataset, and optional validation and test datasets (this is my own wrapper):\r\n\r\n```python\r\nclass DataModule(pl.LightningDataModule):\r\n  def __init__(\r\n        self,\r\n        train_ds: Optional[Dataset] = None,\r\n        valid_ds: Optional[Dataset] = None,\r\n        test_ds: Optional[Dataset] = None,\r\n        batch_size: int = 1,\r\n        num_workers: Optional[int] = None,\r\n    ):\r\n\r\n        super().__init__()\r\n        self._train_ds = train_ds\r\n        self._valid_ds = valid_ds\r\n        self._test_ds = test_ds\r\n\r\n        if self._train_ds is not None:\r\n            self.train_dataloader = self._train_dataloader\r\n\r\n        if self._valid_ds is not None:\r\n            self.val_dataloader = self._val_dataloader\r\n\r\n        if self._test_ds is not None:\r\n            self.test_dataloader = self._test_dataloader\r\n\r\n        self.batch_size = batch_size\r\n\r\n        if num_workers is None:\r\n            num_workers = os.cpu_count()\r\n\r\n        if num_workers is None:\r\n            warnings.warn(\"could not infer cpu count automatically, setting it to zero\")\r\n            num_workers = 0\r\n        self.num_workers = num_workers\r\n\r\n    def _train_dataloader(self):\r\n        return DataLoader(\r\n            self._train_ds,\r\n            batch_size=self.batch_size,\r\n            shuffle=True,\r\n            num_workers=self.num_workers,\r\n            pin_memory=True,\r\n        )\r\n\r\n    def _val_dataloader(self):\r\n        return DataLoader(\r\n            self._valid_ds,\r\n            batch_size=self.batch_size,\r\n            num_workers=self.num_workers,\r\n            pin_memory=True,\r\n        )\r\n\r\n    def _test_dataloader(self):\r\n        return DataLoader(\r\n            self._test_ds,\r\n            batch_size=self.batch_size,\r\n            num_workers=self.num_workers,\r\n            pin_memory=True,\r\n        )\r\n```\r\n\r\n### Pitch\r\n\r\nAdd optional arguments for train, val, test datasets, batch size and num_workers to the base `DataModule` class:\r\n```python\r\ntrain_ds = MyDataset(...)\r\nval_ds = MyDataset(...)\r\ndata = pl.DataModule(train_ds=train_ds, val_ds=val_ds, batch_size=64, num_workers=8)\r\n```\r\n\r\n**Or**, have a factory method doing the same thing:\r\n\r\n```python\r\ntrain_ds = MyDataset(...)\r\nval_ds = MyDataset(...)\r\ndata = pl.DataModule.from_datasets(train_ds=train_ds, val_ds=val_ds, batch_size=64, num_workers=8)\r\n```\r\n\r\nWould like to here from the community if they have any ideas of preferences here! ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5115/comments",
    "author": "teddykoker",
    "comments": [
      {
        "user": "s-rog",
        "created_at": "2020-12-14T05:34:45Z",
        "body": "+1 for `from_datasets()`"
      },
      {
        "user": "teddykoker",
        "created_at": "2020-12-14T15:58:31Z",
        "body": "@ananthsub what do you think of `from_datasets()`?"
      },
      {
        "user": "ananthsub",
        "created_at": "2020-12-16T08:40:45Z",
        "body": "@teddykoker what do you think about baking these in bolts for some time and then promoting it as an API on the data module after we work out any kinks? Some cons for the current proposal are:\r\n- are all datasets required? optional? should we go with a property/setter for each dataset instead of putting everything in the constructor?\r\n- how do we specify dataloader arguments across each {train/val/test} phase effectively?"
      }
    ]
  },
  {
    "number": 5103,
    "title": "pass a variable instead of string to self.save_hyperparameters()",
    "created_at": "2020-12-12T10:57:00Z",
    "closed_at": "2020-12-20T08:16:37Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5103",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\npass a `variable` instead of `string` to function `self.save_hyperparameters()`\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n```\r\n>>>  class ManuallyArgsModel(LightningModule):\r\n...     def __init__(self, arg1, arg2, arg3):\r\n...         super().__init__()\r\n...         # manually assign arguments\r\n...         self.save_hyperparameters('arg1', 'arg3')\r\n```\r\nI think it is too weird to pass strings as arguments to `self.save_hyperparamets()`, using variables instead,such as `self.save_hyperparametes(arg1, arg3)`,  is much more elegant. \r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5103/comments",
    "author": "aiyolo",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-12-13T19:11:10Z",
        "body": "the reason why it is a string is because in order to save the parameters one also has to know the name of the variable."
      },
      {
        "user": "aiyolo",
        "created_at": "2020-12-14T16:47:59Z",
        "body": "we know `print('val')` and `print(var)` are very different things, similarly, `save_hyperparemetes('var')` may mislead users that they are saving a string rather than a variable, this is what I'm concerned. So is it possible to improve it?"
      },
      {
        "user": "ZJX-CV",
        "created_at": "2021-07-02T09:20:54Z",
        "body": "If we use this function，will params be saved in ckpt file automatically？"
      }
    ]
  },
  {
    "number": 5087,
    "title": "Edge case bug when validating on multiple data sets if .log is not called at least once for each data set.",
    "created_at": "2020-12-11T12:07:11Z",
    "closed_at": "2021-01-17T18:34:58Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "priority: 2",
      "logging"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5087",
    "body": "## 🐛 Bug\r\n\r\nWhen validating on multiple data sets, if you call `self.log(..)` for any data set, `self.log(..)` also has to have been called for all previous data sets at least once. If not, you get a `KeyError` when the results are auto reduced. I encountered this because for one of my validation sets I only created and logged images.\r\n\r\n## Reproduce\r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\n\r\n\r\nclass Module(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model = torch.nn.Linear(1, 1)\r\n\r\n    def validation_step(self, batch, batch_idx, dataset_idx):\r\n        if dataset_idx == 0:\r\n            # When you uncomment the following line, everything works.\r\n            # self.log(\"test1\", 0.)\r\n            # Just calling self.log for dataset 0 and not for dataset 1 also works\r\n            pass\r\n        else:\r\n            self.log(\"test2\", 0.)\r\n\r\n    def val_dataloader(self):\r\n        return (\r\n            DataLoader(TensorDataset(torch.ones(2, 1))),\r\n            DataLoader(TensorDataset(torch.ones(2, 1)))\r\n        )\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return torch.mean(self.model(batch[0]))\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(TensorDataset(torch.ones(2, 1)))\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.parameters(), lr=0.01)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    trainer = pl.Trainer()\r\n    trainer.fit(Module())\r\n\r\n```\r\n\r\nError you get:\r\n```\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 690, in run_sanity_check\r\n    _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 622, in run_evaluation\r\n    deprecated_eval_results = self.evaluation_loop.evaluation_epoch_end()\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 203, in evaluation_epoch_end\r\n    self.trainer.logger_connector.evaluation_epoch_end(self.testing)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py\", line 225, in evaluation_epoch_end\r\n    self.cached_results.has_batch_loop_finished = True\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py\", line 441, in has_batch_loop_finished\r\n    self.auto_reduce_results_on_epoch_end()\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py\", line 431, in auto_reduce_results_on_epoch_end\r\n    hook_result.auto_reduce_results_on_epoch_end()\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py\", line 195, in auto_reduce_results_on_epoch_end\r\n    epoch_metrics = self._internals[dl_idx]\r\nKeyError: 0\r\n```\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.4\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.7.1\r\n        - pytorch-lightning: 1.1.0\r\n        - tqdm:              4.54.1\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.6.9\r\n        - version:           #62-Ubuntu SMP Mon Nov 23 19:20:19 UTC 2020\r\n\r\n```\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5087/comments",
    "author": "philipbecker",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-11T12:08:00Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-01-10T18:06:11Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "janvainer",
        "created_at": "2021-11-18T20:42:13Z",
        "body": "Happened to me too."
      }
    ]
  },
  {
    "number": 5061,
    "title": "Fix metrics backwards compatibility ",
    "created_at": "2020-12-10T16:09:02Z",
    "closed_at": "2020-12-11T21:11:22Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5061",
    "body": "Moved to pl.metrics.utils (some users have complained about this)\r\n\r\nTODO: add function back to functional (with deprecation warning) that simply calls new one",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5061/comments",
    "author": "teddykoker",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-12-10T18:51:40Z",
        "body": "it just about importing, so the fix is add in functional `__init__` corrected in #5062"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-12-11T16:52:13Z",
        "body": "@teddykoker @Borda can we close this?"
      },
      {
        "user": "Borda",
        "created_at": "2020-12-11T17:17:27Z",
        "body": "> @teddykoker @Borda can we close this?\r\n\r\nit is in #5067"
      }
    ]
  },
  {
    "number": 5059,
    "title": "Add option for overriding scheduler step",
    "created_at": "2020-12-10T15:15:47Z",
    "closed_at": "2021-06-06T14:52:55Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5059",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\nCurrently there is no way to overide the `scheduler.step()`.  `scheduler.step()` calls for all the schedulers are called sequentially i.e,\r\n```python\r\nfor scheduler in schedulers:\r\n    scheduler.step()\r\n```\r\nThere should be something a hook that enables to override the `scheduler.step()` call something similar to the existing hook for optimizers `LightningModule.optimizer_step()`\r\n\r\n### Pitch\r\nsomething similar to :\r\n```python\r\ndef scheduler_step(self, epoch, batch_idx, scheduler, scheduler_idx, *args, **kwargs)\r\n    if scheduler_idx == 0:\r\n       ....   # do something\r\n       scheduler.step()\r\n    \r\n    if scheduler_idx == 1:\r\n       ....   # do something here\r\n       scheduler.step()\r\n ....\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5059/comments",
    "author": "benihime91",
    "comments": [
      {
        "user": "benihime91",
        "created_at": "2020-12-10T15:16:33Z",
        "body": "@tchaton can you have a look as discussed in slack ?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-12-10T18:26:23Z",
        "body": "just thinking, should it be given in the arguments itself within `optimizer_step`??"
      },
      {
        "user": "benihime91",
        "created_at": "2020-12-11T06:30:15Z",
        "body": "Something like this probably wherein `scheduler` and `scheduler_idx` are passed as arguments to `optimizer_step`\r\n\r\n```python\r\ndef optimizer_step(current_epoch, batch_nb, optimizer, optimizer_idx, scheduler, scheduler_idx, *args, **kwargs):\r\n      pass\r\n```"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-12-11T07:09:22Z",
        "body": "will `scheduler_idx` be useful here? wouldn't it always be equal to `optimizer_idx`??"
      },
      {
        "user": "benihime91",
        "created_at": "2020-12-11T07:11:04Z",
        "body": "Now that you mention it , that seems to the case so only `optmizer_idx` should suffice."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-01-10T07:47:40Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-05-30T13:53:12Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 5028,
    "title": "\"Shuffle\" in validation dataloader: is it really best practices?",
    "created_at": "2020-12-08T20:15:36Z",
    "closed_at": "2020-12-09T13:40:38Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5028",
    "body": "## 🐛 Bug\r\n\r\nIn my `LightningModule`'s `val_dataloader` method, I have this dataloader: \r\n\r\n```python\r\ndataloader = DataLoader(self.datasets[split], batch_size=batch_size,\r\n                                shuffle=True, num_workers=self.hparams.compute.num_workers,\r\n                                pin_memory=torch.cuda.is_available(), drop_last=False)\r\nreturn dataloader\r\n```\r\n\r\nI receive this warning: \r\n```\r\n.../pytorch_lightning/utilities/distributed.py:45: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\r\n  warnings.warn(*args, **kwargs)\r\n```\r\nHowever, it's quite important for me to shuffle my validation batches. For example, I visualize the first few batches in my validation to get an idea of random model performance on my images-- without shuffling, I'd only be able to inspect the same images every epoch. \r\n\r\n### Expected behavior\r\n\r\nNo warning\r\n\r\n### Additional information\r\n\r\nThis is more of a discussion than a bug report, but it didn't neatly fit into any categories. Do we really think it's important enough to warn the user when using shuffle in validation? I've tried suppressing it, but I can't figure out where exactly it's called. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5028/comments",
    "author": "jbohnslav",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2020-12-09T01:34:04Z",
        "body": "Wouldn't you want to always inspect the same images to properly assess the model performance?\r\nIf you keep looking at different samples each time, it is harder to know if the change in performance is caused by the model improvement or by how well the model generalizes to the samples in particular.\r\n\r\n> I've tried suppressing it, but I can't figure out where exactly it's called.\r\n\r\nYou shouldn't need to, see this example of using `filterwarnings`\r\n\r\n```python\r\nimport warnings\r\n\r\ndef test():\r\n     warnings.warn(\"this is a test\", UserWarning)\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"this is a test\")\r\ntest() # No warning!\r\n```"
      },
      {
        "user": "jbohnslav",
        "created_at": "2020-12-09T13:38:26Z",
        "body": "> Wouldn't you want to always inspect the same images to properly assess the model performance?\r\n\r\nI'm working with video data, so the first N batches in an unshuffled dataset would be the first ~minute of the first video. This isn't very informative-- it's much better to get a random sample. \r\n\r\n```warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"this is a test\")```\r\nThanks for the sample! I thought one had to use it as a context manager. This solves the problem. "
      },
      {
        "user": "bjourne",
        "created_at": "2025-02-17T16:13:05Z",
        "body": "But the issue remains. WHY does PL \"strongly recommend\" that you don't shuffle validation data? Afaict, the warning is completely pointless."
      }
    ]
  },
  {
    "number": 5027,
    "title": "On \"import pytorch-lightning\": AttributeError: python: undefined symbol: THCudaHalfTensor_normall",
    "created_at": "2020-12-08T19:40:09Z",
    "closed_at": "2020-12-09T14:56:05Z",
    "labels": [
      "help wanted",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5027",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nSimply by importing pytorch-lightning, I receive the following error: `AttributeError: python: undefined symbol: THCudaHalfTensor_normall`\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 1, in <module>\r\n    import pytorch_lightning\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/pytorch_lightning/__init__.py\", line 56, in <module>\r\n    from pytorch_lightning import metrics\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/pytorch_lightning/metrics/__init__.py\", line 14, in <module>\r\n    from pytorch_lightning.metrics.metric import Metric\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/pytorch_lightning/metrics/metric.py\", line 26, in <module>\r\n    from pytorch_lightning.utilities.apply_func import apply_to_collection\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/__init__.py\", line 25, in <module>\r\n    from apex import amp\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/apex-0.1-py3.7-linux-x86_64.egg/apex/__init__.py\", line 12, in <module>\r\n    from . import optimizers\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/apex-0.1-py3.7-linux-x86_64.egg/apex/optimizers/__init__.py\", line 2, in <module>\r\n    from .fp16_optimizer import FP16_Optimizer\r\n  File \"/data/nv419/anaconda3/lib/python3.7/site-packages/apex-0.1-py3.7-linux-x86_64.egg/apex/optimizers/fp16_optimizer.py\", line 8, in <module>\r\n    lib.THCudaHalfTensor_normall.argtypes=[ctypes.c_void_p, ctypes.c_void_p]\r\n  File \"/data/nv419/anaconda3/lib/python3.7/ctypes/__init__.py\", line 377, in __getattr__\r\n    func = self.__getitem__(name)\r\n  File \"/data/nv419/anaconda3/lib/python3.7/ctypes/__init__.py\", line 382, in __getitem__\r\n    func = self._FuncPtr((name_or_ordinal, self))\r\nAttributeError: python: undefined symbol: THCudaHalfTensor_normall\r\n```\r\n\r\n### To Reproduce\r\n`import pytorch-lightning`\r\n\r\n### Expected behavior\r\nFor pytorch-lightning to import and be used correctly\r\n\r\n### Environment\r\n* CUDA:\r\n        - GPU:\r\n                - Tesla V100-PCIE-32GB\r\n                - Tesla V100-PCIE-32GB\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.17.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.6.0+cu101\r\n        - tqdm:              4.54.1\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                -\r\n        - processor:         x86_64\r\n        - python:            3.7.4\r\n        - version:           #59~18.04.1-Ubuntu SMP Wed Oct 21 12:14:56 UTC 2020\r\n\r\n### Additional context\r\npytorch-lightning version is 1.0.8 (couldn't import it in obviously...)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5027/comments",
    "author": "nihirv",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-08T19:41:02Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-12-09T03:04:09Z",
        "body": "From your error message it looks like you have apex installed. \r\nTry to import apex: \r\n`from apex import amp`\r\nand you will probably see the same error. Check your apex installation, maybe it is incompatible with your pytorch version or it wasn't correctly compiled. "
      },
      {
        "user": "nihirv",
        "created_at": "2020-12-09T14:56:05Z",
        "body": "> From your error message it looks like you have apex installed.\r\n> Try to import apex:\r\n> `from apex import amp`\r\n> and you will probably see the same error. Check your apex installation, maybe it is incompatible with your pytorch version or it wasn't correctly compiled.\r\n\r\nThis was the issue! Fixed it by uninstalling apex (`pip uninstall apex`). Thanks!\r\n\r\nClosing the issue\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-12-09T14:57:02Z",
        "body": "Awesome! You're welcome"
      }
    ]
  },
  {
    "number": 5013,
    "title": "Accuracy metric for preds at half precision is zero with pl=1.0.8",
    "created_at": "2020-12-08T10:28:16Z",
    "closed_at": "2020-12-08T10:49:07Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5013",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nThe accuracy metric is wrong if `preds` are given with half precision. See example. \r\n\r\n### To Reproduce\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n```python\r\nimport torch\r\nfrom pytorch_lightning.metrics import Accuracy\r\n\r\nacc = Accuracy(threshold=0.5)\r\ntarget = torch.Tensor([1, 1, 0, 0])\r\npreds = torch.Tensor([0.7, 0.4, 0.8, 0.4])\r\n\r\nprint(acc(preds, target))  -> 0.5\r\nprint(acc(preds.half(), target))  -> 0.0\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\nThe accuracy metric should not fail silently. Either an Error needs to be raised when preds are half precision or it should work correctly.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.7.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: ...\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nThis might already be fixed in master. I filed the issue regardless because I don't have time to check.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5013/comments",
    "author": "luzuku",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-08T10:29:01Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "luzuku",
        "created_at": "2020-12-08T10:48:58Z",
        "body": "It is indeed fixed in `master` with the change from `preds.dtype == torch.float` to `preds.is_floating_point()`. It is also a duplicate of #4840. Sorry, my bad."
      },
      {
        "user": "tadejsv",
        "created_at": "2020-12-08T10:54:29Z",
        "body": "It is fixed in master but would be broken again in #4838, so thanks for catching this :)"
      },
      {
        "user": "tadejsv",
        "created_at": "2020-12-08T11:36:10Z",
        "body": "@luzuku And one small note: if you create tensors with `torch.Tensor`, the created tensor will be a float tensor. Targets as floats will not be supported as inputs in classification functions anymore. If you want to preserve dtypes, create the tensor with `torch.tensor` (note that `tensor` is not capitalized)"
      }
    ]
  },
  {
    "number": 4996,
    "title": "Use lightning with dgl",
    "created_at": "2020-12-07T13:37:52Z",
    "closed_at": "2021-01-16T06:27:16Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4996",
    "body": "```\r\ndef train_dataloader(self):\r\n        return DataLoader(self.train_dataset,\r\n                          batch_size=self.hparams.batch_size,\r\n                          shuffle=True,\r\n                          num_workers=4,\r\n                          collate_fn=self.batcher)\r\n```\r\nI use collate_fn to batch dgl graphs, but when I training model, trigger this warning:\r\n`\r\nDGLWarning: DGLGraph.__len__ is deprecated.Please directly call DGLGraph.number_of_nodes.\r\n`\r\nHow tor fix it ?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4996/comments",
    "author": "Maybewuss",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-12-09T03:16:57Z",
        "body": "Could you provide a bit more information? It looks like this warning is coming from the DGL library. It would be nice to have a minimal example to run. Make sure your train_dataset is a `torch.nn.Dataset` that has the `__len__` method defined. "
      },
      {
        "user": "Maybewuss",
        "created_at": "2020-12-09T14:44:59Z",
        "body": "```\r\nimport pytorch_lightning as pl\r\nimport dgl\r\nfrom torch.utils.data import DataLoader, Dataset\r\nimport torch.nn as nn\r\nfrom torch.optim import AdamW\r\nimport torch\r\n\r\nclass Dataset(object):\r\n    def __init__(self, instances):\r\n        self.instances = instances\r\n\r\n    def __getitem__(self, idx):\r\n        \"\"\"Get the instance with index idx.\r\n        \"\"\"\r\n        return self.instances[idx]\r\n\r\n    def __len__(self):\r\n        return len(self.instances)\r\n\r\nclass Model(pl.LightningModule):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__()\r\n        self.net = nn.Linear(1, 1)\r\n        \r\n    def forward(self, *args, **kwargs):\r\n        x = torch.randn(2, 1)\r\n        return self.net(x)\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        loss = self().sum()\r\n        return loss\r\n    \r\n    def configure_optimizers(self):\r\n        opt = AdamW(self.net.parameters(), lr=100)\r\n        return opt\r\n\r\ngraph = dgl.heterograph(\r\n    {\r\n        ('drug', 'interacts', 'drug'): [(0,1), (1, 2)],\r\n        ('drug', 'treats', 'disease'): [(2, 3), (0, 3)],\r\n        \r\n    }\r\n)\r\n\r\ndata = [graph, graph, graph]\r\ndataset = Dataset(data)\r\ndata_loader = DataLoader(dataset, batch_size=1, collate_fn=dgl.batch)\r\n\r\nmodel = Model()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(model, train_dataloader=data_loader)\r\n\r\n```\r\n\r\n`\r\n/Applications/anaconda3/envs/py36/lib/python3.6/site-packages/dgl/base.py:45: DGLWarning: DGLGraph.__len__ is deprecated.Please directly call DGLGraph.number_of_nodes.\r\n`\r\n\r\nDoes lightning use len function when load data from dataloader ?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-12-10T04:16:03Z",
        "body": "Your code looks fine. I still don't think Lightning is causing the warning. Can you just run only this code:\r\n\r\n```python \r\ndata = [graph, graph, graph]\r\ndataset = Dataset(data)\r\ndata_loader = DataLoader(dataset, batch_size=1, collate_fn=dgl.batch)\r\nnext(iter(dataloader))\r\n```\r\nDoes it throw the warning?\r\n"
      },
      {
        "user": "Maybewuss",
        "created_at": "2020-12-10T04:19:27Z",
        "body": "> Your code looks fine. I still don't think Lightning is causing the warning. Can you just run only this code:\n> \n> \n> \n> ```python \n> \n> data = [graph, graph, graph]\n> \n> dataset = Dataset(data)\n> \n> data_loader = DataLoader(dataset, batch_size=1, collate_fn=dgl.batch)\n> \n> next(iter(dataloader))\n> \n> ```\n> \n> Does it throw the warning?\n> \n> \n\nI tried, no warning throw."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-01-09T05:26:10Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4994,
    "title": "Add multi-task support to EarlyStopping ",
    "created_at": "2020-12-07T08:25:11Z",
    "closed_at": "2021-01-14T10:56:28Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4994",
    "body": "## 🚀 Feature\r\n\r\nMake `EarlyStopping` watch multiple values and only stop when all of them no longer improve.\r\n\r\n### Motivation\r\n\r\nI have been training a multi-task model with multiple outputs. Each output is validated and logged by the model. \r\nAs of today, early stopping can only watch one of them. Hence, the user has to choose which task is the main one.\r\n\r\n### Pitch\r\n\r\nMake `EarlyStopping` watch multiple values and only stop when all of them no longer improve.\r\n\r\n### Alternatives\r\n\r\nOne could add multiple `EarlyStopping` callbacks (one for each task) but this would stop as soon as one of the task no longer improves (even though another one might still be improving).\r\n\r\n### Additional context\r\n\r\nNote that `ModelCheckpoint` does not have this problem: one can add multiple instances of `ModelCheckpoint` (one for each task) and you get what you expect: checkpointing based on each task's validation metric.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4994/comments",
    "author": "hbredin",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2020-12-07T13:57:06Z",
        "body": "I love the idea! It might be easier to create a `MultiTaskEarlyStopping` than updating the current `EarlyStopping` class.\r\n\r\nWould you be willing to work on it?\r\n\r\n> Note that `ModelCheckpoint` does not have this problem: one can add multiple instances of `ModelCheckpoint` (one for each task) and you get what you expect: checkpointing based on each task's validation metric.\r\n\r\nThis is not perfectly supported and only one ModelCheckpoint state will be saved."
      },
      {
        "user": "hbredin",
        "created_at": "2020-12-08T10:13:03Z",
        "body": "I need this feature at some point so am happy to contribute in the future but cannot provide an ETA (definitely not before 2021)."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-01-07T10:25:01Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "ncuxomun",
        "created_at": "2021-03-17T18:36:31Z",
        "body": "Anyone implemented MultiTaskEarlyStopping manually?"
      }
    ]
  },
  {
    "number": 4973,
    "title": "Abstract matplotlib figure logging from individual logger API",
    "created_at": "2020-12-04T14:58:13Z",
    "closed_at": "2021-02-21T15:11:04Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4973",
    "body": "## 🚀 Feature\r\nImplement default log_figure for all implemented loggers\r\n\r\n### Motivation\r\n\r\nCurrently, logging figures relies on calling the API of the individual logger directly. This is not really convenient for a variety of reasons:\r\n- It is cumbersome to change loggers\r\n- It is cumbersome to disable logging (e.g. for debugging) --> ```if self.logger is not None: \\n[...]```\r\n- It is not really nice if you have multiple loggers\r\n\r\n### Pitch\r\nI propose something like\r\n```logger.log_figure(figure_name, plt.figure, step, close_figure, kwargs)```\r\nwhere the kwargs are passed on to the respective logger implementation (i.e. if one wants something specific).\r\n\r\n### Additional context\r\nShould a log_image method also be considered? Should it rather be ```log_figures``` (plural, i.e. passing multiple figures)?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4973/comments",
    "author": "Haydnspass",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-12-06T02:40:47Z",
        "body": "This is a very difficult feature to implement. There is a great disparity in the API between the loggers regarding the logging of images and graphs and it would be quite the challenge to bring this under a common denominator without sacrificing functionality. \r\n\r\nYou mention that kwargs could be used to pass down specific arguments to a logger but that will defeat the purpose because changing the logger externally will break the LightningModule as it would today anyway. \r\n\r\nOne other point one has to consider is that if the third party api changes our wrappers will also break very easily and adds maintenance cost. "
      },
      {
        "user": "Haydnspass",
        "created_at": "2020-12-07T10:50:32Z",
        "body": "I agree that the kwargs option kind of contradicts the purpose of changing loggers and is difficult for multiple loggers, it still solves the temporary disabling logger issue though. But I agree, if one wants more than a default implementation one should possibly ask the user to still us the actual API via ```logger.experiment.[]```\r\n\r\nHowever, I don't quite agree with the API. To me, it is not really different than the differences in API for metrics among the different loggers. Parsing a matplotlib figure is a pretty standard thing which is supported by almost all of the loggers and there's not much up for discussion what to do with the figure is it?\r\n\r\nI think figure logging is an important point to debug and monitor the performance of a model and should be well supported."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-12-07T13:14:54Z",
        "body": "I misunderstood, I didn't know you were only considering \"matplotlib\" figures. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-01-06T16:52:28Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "Haydnspass",
        "created_at": "2021-01-15T12:07:47Z",
        "body": "Can someone help me with MLFlow. How do you usually log figures over multiple epochs there?\r\nMy suggestion would be something like\r\n\r\n```\r\nfilename = name + f\"_step{step}.png\"\r\nfigure.savefig(filename)\r\nself.experiment.log_artifact(?)\r\n```\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-02-14T12:54:35Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4958,
    "title": "`validation_epoch_end` might need current epoch",
    "created_at": "2020-12-03T11:40:27Z",
    "closed_at": "2020-12-03T11:47:23Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4958",
    "body": "Hi,\r\n\r\nI would like to save some files with suffix of filenames using current epoch on `validation_epoch_end`. However, it seems that it's not a parameter of `validation_epoch_end` and I can only get it from `pl.Trainer`. Is there any another way instead of just adding additional parameter to `validation_epoch_end`? \r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4958/comments",
    "author": "kchuang625",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-03T11:41:13Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      }
    ]
  },
  {
    "number": 4927,
    "title": "clip_gradient with clip_grad_value ",
    "created_at": "2020-12-01T05:13:22Z",
    "closed_at": "2021-01-07T08:25:00Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4927",
    "body": "## 🚀 Feature\r\nThe current clip_gradient uses clip_grad_norm; can we add clip_grad_value?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4927/comments",
    "author": "ruotianluo",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-12-31T07:36:15Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4909,
    "title": "metrics fail if inputs are on gpu",
    "created_at": "2020-11-30T11:54:46Z",
    "closed_at": "2020-12-29T09:06:28Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4909",
    "body": "The metrics precision and recall fail if inputs are on gpu.\r\n```\r\n>>> from pytorch_lightning.metrics import Precision\r\n>>> target = torch.tensor([0, 1, 2, 0, 1, 2]).cuda()\r\n>>> preds = torch.tensor([0, 2, 1, 0, 0, 1]).cuda()\r\n>>> precision = Precision(num_classes=3)\r\n>>> precision(preds, target)\r\ntensor(0.3333)\r\n```\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-39-834a61ec6e2d> in <module>\r\n      3 preds = torch.tensor([0, 2, 1, 0, 0, 1]).cuda()\r\n      4 precision = Precision(num_classes=3)\r\n----> 5 precision(preds, target)\r\n      6 tensor(0.3333)\r\n\r\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\r\n    725             result = self._slow_forward(*input, **kwargs)\r\n    726         else:\r\n--> 727             result = self.forward(*input, **kwargs)\r\n    728         for hook in itertools.chain(\r\n    729                 _global_forward_hooks.values(),\r\n\r\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/pytorch_lightning/metrics/metric.py in forward(self, *args, **kwargs)\r\n    154         # add current step\r\n    155         with torch.no_grad():\r\n--> 156             self.update(*args, **kwargs)\r\n    157         self._forward_cache = None\r\n    158 \r\n\r\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/pytorch_lightning/metrics/metric.py in wrapped_func(*args, **kwargs)\r\n    200         def wrapped_func(*args, **kwargs):\r\n    201             self._computed = None\r\n--> 202             return update(*args, **kwargs)\r\n    203         return wrapped_func\r\n    204 \r\n\r\n/anaconda/envs/azureml_py36/lib/python3.6/site-packages/pytorch_lightning/metrics/classification/precision_recall.py in update(self, preds, target)\r\n    130 \r\n    131         # multiply because we are counting (1, 1) pair for true positives\r\n--> 132         self.true_positives += torch.sum(preds * target, dim=1)\r\n    133         self.predicted_positives += torch.sum(preds, dim=1)\r\n    134 \r\n\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4909/comments",
    "author": "simonm3",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-11-30T12:23:54Z",
        "body": "@teddykoker mind have a look?"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-11-30T15:21:46Z",
        "body": "We could move the metrics to GPUS if both provided arguments are on GPUs."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-11-30T15:27:27Z",
        "body": "Metrics are subclass of `nn.Module` and their states behave similar to the parameters and buffers of other modules. Therefore, calling `precision.cuda()` before evaluating the metric should solve your problem. "
      },
      {
        "user": "simonm3",
        "created_at": "2020-11-30T17:36:15Z",
        "body": "It is cleaner to be able to use the same code on cpu or gpu without having multiple places where you have to test and convert."
      },
      {
        "user": "Borda",
        "created_at": "2020-11-30T23:21:49Z",
        "body": "> It is cleaner to be able to use the same code on cpu or gpu without having multiple places where you have to test and convert.\r\n\r\nI agree here, this would be very useful, basically follow the agnostic philosophy as Trainer does...\r\ncc: @PyTorchLightning/core-contributors "
      },
      {
        "user": "s-rog",
        "created_at": "2020-12-01T02:55:37Z",
        "body": "Shouldn't this be moved to gpu automatically inside a pl module though? I'll give it a test shortly..."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-12-01T15:59:36Z",
        "body": "If the metric is initialized inside a `LightningModule` or a `nn.Module` it will automatically be moved to the correct device, whenever the `.to(...)` is called. However, if used outside a model the user would need to transfer the metric to the correct device. This is completely inline with other pytorch modules, that also needs to be moved to correct device by the user (take `torch.nn.Conv2d` for example)."
      },
      {
        "user": "simonm3",
        "created_at": "2020-12-01T20:56:56Z",
        "body": "Not sure what you mean by inside a model. I have:\r\n\r\n```\r\nclass Module(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        model = resnet18(pretrained=True)\r\n        self.metrics = dict(precision=Precision())\r\n        self.loss = partial(F.cross_entropy, weight=torch.tensor([0.1, 0.9]))\r\n```\r\nIn training_step and validation_step I call self.loss and each of the self.metrics. precision(y,ypred) fails if ypred and y are both gpu. so I have to force them to cpu.\r\n\r\nThe loss also fails unless I explicitly change the above to weight=torch.tensor([0.1, 0.9]).cuda(). Everything else seems to detect whether it is running on cpu or gpu.\r\n"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-12-01T21:08:51Z",
        "body": "Then the problem has to with how pytorch register child modules. Essentially, anything put inside a `list` or `dict` will not be registered correctly. Instead you should be using `nn.ModuleList` and `nn.ModuleDict`. Addtionally, you can use the module version of the loss, and the weight should also automatically transferred to the correct device. This version of your code should work:\r\n``` python\r\nclass Module(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        model = resnet18(pretrained=True)\r\n        self.metrics = torch.nn.ModuleDict({'precision' : Precision()})\r\n        self.loss = torch.nn.CrossEntropyLoss(weight=torch.tensor([0.1, 0.9]))\r\n```"
      },
      {
        "user": "s-rog",
        "created_at": "2020-12-02T00:21:05Z",
        "body": "Yep I've had no issues using metrics inside pl modules. This seems like an intended behavior?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-12-02T03:38:50Z",
        "body": "agree with @SkafteNicki this is most likely the reason. that's a silly mistake every pytorch user encounters at least once in their life xD \r\none just has to know this :) "
      },
      {
        "user": "edenlightning",
        "created_at": "2020-12-11T21:31:00Z",
        "body": "@SkafteNicki would you mind adding a warning or explanation in our docs?"
      },
      {
        "user": "Borda",
        "created_at": "2020-12-11T21:36:14Z",
        "body": "> agree with @SkafteNicki this is most likely the reason. that's a silly mistake every pytorch user encounters at least once in their life xD\r\n> one just has to know this :)\r\n\r\nwell, I as a user would appreciate simple experimenting with some automatic moving and do not need to care about it... \r\nwould it be a problem to add a check if both inputs are on the same device already move the class itself there too?"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-12-14T14:30:37Z",
        "body": "@Borda it would probably not be a problem implementing that the metric states gets moved to the same device as the input (maybe still warn the user)"
      }
    ]
  },
  {
    "number": 4902,
    "title": "conflicts of warm-up and lr scheduler",
    "created_at": "2020-11-30T03:32:24Z",
    "closed_at": "2021-01-06T08:06:08Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4902",
    "body": "In docs, warm-up is done as below:\r\n```\r\n# learning rate warm-up\r\ndef optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_idx, second_order_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False):\r\n    # warm up lr\r\n    if self.trainer.global_step < 500:\r\n        lr_scale = min(1., float(self.trainer.global_step + 1) / 500.)\r\n        for pg in optimizer.param_groups:\r\n            pg['lr'] = lr_scale * self.hparams.learning_rate\r\n\r\n    # update params\r\n    optimizer.step()\r\n    optimizer.zero_grad()\r\n```\r\nThis will change lr in [0, 500] steps, but lr_scheduler will also change lr in [0, 500] steps,  this warm-up method will override lr_scheduler's change.  How can I use warm-up in [0, 500] steps, then starts lr_scheduler in [500, ...] steps.  Or I use warm-up in [0, 3] epochs, then starts lr_scheduler in [3, ...] epochs",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4902/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-12-30T06:38:02Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4901,
    "title": "Fine-grained control of Parallel Trainer: GPU Assignment in DDP",
    "created_at": "2020-11-30T00:17:12Z",
    "closed_at": "2021-01-06T22:25:16Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4901",
    "body": "Dear all,\r\n\r\nreally enjoy using Lightning - amazing framework! I have recently begun to run into limits with its distributed trainer implementation,when seeking to assign different models which feed into the same gradient flow to specific GPUs, but have been unable to find any documentation for it and kind of presume for it to not exist in Lightning yet.\r\n\r\nThe specific use-case is that at GAN training time I seek to include a pre-trained model to nudge my generator into a certain direction. The way I am currently doing this is by \"nudging\" with the pre-trained model every third epoch. But this causes some data movement, which I would like to minimize. E.g. for \r\n\r\n```\r\ndef training_step(self):\r\n    (..)\r\n    if optimizer_idx == 0:\r\n        if self.trainer.current_epoch % 3 == 0:\r\n            model_1\r\n        else:\r\n            model_2\r\n\r\ndef configure_optimizers(self):\r\n    (..)\r\n    return [opt_g, opt_d], _\r\n\r\n```\r\nIn the example of a 4-GPU cluster, I would now like to be to assign/launch 3 instances of model_2 on GPU <0, 1, 2> and launch one instance of  model_1 on GPU3 with all of them using the same optimizer.\r\n\r\nThis functionality should also be highly advantageous for (sparse) mixture of experts and sharding-reliant models.\r\n\r\nThank you for reading my issue,\r\nLudger",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4901/comments",
    "author": "ludgerpaehler",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2020-11-30T14:56:16Z",
        "body": "Hi Ludger, \r\n\r\nI am afraid that you are right and this is currently not supported. There could be a hack with instantiating the model when you already now what GPU you're on (e.g. in your `setup`).\r\n\r\nThat means, that in your `LightningModule` you would do something like:\r\n\r\n```python\r\nclass CustomGAN(LightningModule):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__()\r\n        self.model_1 = None\r\n        self.model_2 = None\r\n\r\n    def setup(self):\r\n        rank = self.trainer.global_rank\r\n\r\n        if rank % 3 == 0:\r\n            self.model_1 = ... #define model 1\r\n        else:\r\n            self.model_2 = ... # define model 2\r\n\r\n```\r\n\r\nNote that this is not the expected behaviour and basically abuses the way DDP is meant, but that way you should be able to use the already implemented DDP for your goals. \r\nAlso note, that I have not tested this myself and I am not sure, if this messes up anything with optimizer creator or DDP syncing across the processes. But that would be my way to try these things."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-30T18:46:43Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4900,
    "title": "test loading legacy checkpoints",
    "created_at": "2020-11-29T22:07:13Z",
    "closed_at": "2021-01-08T15:36:50Z",
    "labels": [
      "feature",
      "help wanted",
      "ci",
      "checkpointing",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4900",
    "body": "## 🚀 Feature\r\n\r\ncreate persistent storage with all legacy version checkpoints and try to load and continue training",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4900/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-12-16T14:28:38Z",
        "body": "@tchaton asked me to work on this, so I will assign myself and create a workflow."
      },
      {
        "user": "Borda",
        "created_at": "2020-12-16T14:41:04Z",
        "body": "you will need to create an S3. bucket and get access there for saving checkpoiints"
      }
    ]
  },
  {
    "number": 4897,
    "title": "Access to total number of batches in addition to batch_idx",
    "created_at": "2020-11-29T19:00:52Z",
    "closed_at": "2020-11-30T15:19:36Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4897",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nIn addition to batch_idx, provide another parameter total_batches for methods and hooks in LightningModule.\r\n\r\n### Motivation\r\n\r\nIn LightningModule, even though methods and hooks like training_step and optimizer_step provides batch_idx as an argument, the information about the total number of batches for a epoch is missing. This is not very convenient when the relative progress of a epoch is needed, for example, when implementing features like learning rate warmup, which needs the progress of the first epoch. Other than that, there might be many other scenarios where the total number of batches is useful.\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx, total_batches, optimizer_idx, hiddens):\r\n    ...\r\n```\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nOne alternative is to pass the length of the DataLoader to the LightningModule before training, but this won't work if the total number of batches is dynamic, say when automatic batch size is enabled or reload_dataloaders_every_epoch flag is turned on.\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4897/comments",
    "author": "simonrouse9461",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-11-29T20:17:15Z",
        "body": "you can access them using\r\n```\r\ntraining batches -> self.trainer.num_training_batches (int)\r\nvalidation batches -> self.trainer.num_val_batches (List[int])\r\ntest batches -> self.trainer.num_test_batches (List[int])\r\n```"
      },
      {
        "user": "simonrouse9461",
        "created_at": "2020-11-30T15:19:36Z",
        "body": "Cool! This looks good."
      }
    ]
  },
  {
    "number": 4877,
    "title": "RuntimeError: All input tensors must be on the same device. Received cuda:2 and cuda:0",
    "created_at": "2020-11-27T08:54:18Z",
    "closed_at": "2020-11-28T12:02:21Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4877",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nRuntimeError: All input tensors must be on the same device. Received cuda:2 and cuda:0\r\nWhen I try to train with 4 GPUs (it happens if the number of GPUS > 1), there is an error raised. When the validation epoch completed and before train continuously, this error happened. \r\n\r\nBesides, this error wouldn't happen when I only use 1 GPU to train. I think this is a bug of pl?\r\n\r\n\r\n### Environment\r\n\r\n - PyTorch Version : 1.7.0\r\n- PyTorch-Lightning version: 1.0.7\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: py3.6.2\r\n - CUDA/cuDNN version: installed cudatoolkit with version: 10.0.130\r\n - GPU models and configuration: 2080Ti 10GB\r\n - Any other relevant information: \r\n\r\nFile \"DCK3.py\", line 584, in <module>\r\n    trainer.fit(dck, datamodule=dm)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 446, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/accelerators/dp_accelerator.py\", line 106, in train\r\n    results = self.train_or_test()\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 66, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 495, in train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 589, in run_training_epoch\r\n    self.trainer.run_evaluation(test_mode=False)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 611, in run_evaluation\r\n    eval_loop_results = self.evaluation_loop.log_epoch_metrics(deprecated_eval_results, epoch_logs, test_mode)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 214, in log_epoch_metrics\r\n    test_mode\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py\", line 127, in on_evaluation_epoch_end\r\n    self._log_on_evaluation_epoch_end_metrics(epoch_logs)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py\", line 195, in _log_on_evaluation_epoch_end_metrics\r\n    reduced_epoch_metrics = dl_metrics[0].__class__.reduce_on_epoch_end(dl_metrics)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py\", line 469, in reduce_on_epoch_end\r\n    recursive_stack(result)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py\", line 608, in recursive_stack\r\n    result[k] = collate_tensors(v)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py\", line 630, in collate_tensors\r\n    return torch.stack(items)\r\nRuntimeError: All input tensors must be on the same device. Received cuda:2 and cuda:3",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4877/comments",
    "author": "yongqianxiao",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-27T08:55:01Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "zhhao1",
        "created_at": "2020-11-27T12:19:09Z",
        "body": "It's a bug of dp training.\r\nSolution：\r\n1、change to ddp.\r\n2、see question #4138 "
      },
      {
        "user": "yongqianxiao",
        "created_at": "2020-11-27T16:17:16Z",
        "body": "> It's a bug of dp training.\r\n> Solution：\r\n> 1、change to ddp.\r\n> 2、see question #4138\r\n\r\nThanks for the help. The second solution works for me. `ddp` mode is not OK because I train at one computer."
      },
      {
        "user": "zhhao1",
        "created_at": "2020-11-27T16:21:22Z",
        "body": "> > It's a bug of dp training.\r\n> > Solution：\r\n> > 1、change to ddp.\r\n> > 2、see question #4138\r\n> \r\n> Thanks for the help. The second solution works for me. `ddp` mode is not OK because I train at one computer.\r\n\r\nDDP can be used in one computer with n gpus. In my experiment, ddp is more effective."
      },
      {
        "user": "yongqianxiao",
        "created_at": "2020-11-27T16:33:00Z",
        "body": "> > > It's a bug of dp training.\r\n> > > Solution：\r\n> > > 1、change to ddp.\r\n> > > 2、see question #4138\r\n> > \r\n> > \r\n> > Thanks for the help. The second solution works for me. `ddp` mode is not OK because I train at one computer.\r\n> \r\n> DDP can be used in one computer with n gpus. In my experiment, ddp is more effective.\r\n\r\nDDP is not worked for me. Errors raised when I changed it to ddp:\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/queues.py\", line 241, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n_pickle.PicklingError: Can't pickle <class 'MemoryError'>: it's not the same object as builtins.MemoryError\r\nTraceback (most recent call last):\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 872, in _try_get_data\r\n    data = self._data_queue.get(timeout=timeout)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/queues.py\", line 104, in get\r\n    if timeout < 0 or not self._poll(timeout):\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\r\n    return self._poll(timeout)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\r\n    r = wait([self], timeout)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\r\n    ready = selector.select(timeout)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/selectors.py\", line 376, in select\r\n    fd_event_list = self._poll.poll(timeout)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 51307) is killed by signal: Killed. \r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/XYQ/DCK_torch/src_torch/DCK3.py\", line 584, in <module>\r\n    trainer.fit(dck, datamodule=dm)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 446, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py\", line 148, in train\r\n    results = self.ddp_train(process_idx=self.task_idx, model=model)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py\", line 282, in ddp_train\r\n    results = self.train_or_test()\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 66, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 495, in train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 554, in run_training_epoch\r\n    for batch_idx, (batch, is_last_batch) in train_dataloader:\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/profiler/profilers.py\", line 82, in profile_iterable\r\n    value = next(iterator)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 46, in _with_is_last\r\n    last = next(it)\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\r\n    data = self._next_data()\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1068, in _next_data\r\n    idx, data = self._get_data()\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1034, in _get_data\r\n    success, data = self._try_get_data()\r\n  File \"/home/nudt302/anaconda3/envs/pltorch17tf14/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 885, in _try_get_data\r\n    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e\r\nRuntimeError: DataLoader worker (pid(s) 51307) exited unexpectedly\r\n```"
      },
      {
        "user": "zhhao1",
        "created_at": "2020-11-28T14:30:27Z",
        "body": "Sorry, i haven't meet such question.If you use the default sampler when using ddp, does this question appear?\r\nIf you define you own sampler, you need inherit your sampler class from distirbutedsampler, and set replace_sampler_ddp=False in traier."
      },
      {
        "user": "yongqianxiao",
        "created_at": "2020-11-28T15:07:17Z",
        "body": "> Sorry, i haven't meet such question.If you use the default sampler when using ddp, does this question appear?\r\n> If you define you own sampler, you need inherit your sampler class from distirbutedsampler, and set replace_sampler_ddp=False in traier.\r\n\r\nI customize the dataset but use the default sampler (I didn't give the sampler when I define the Dalaloader)."
      },
      {
        "user": "zhhao1",
        "created_at": "2020-11-28T15:29:09Z",
        "body": "If you don't do some special to sampler,  the pytorch-ligtning will auto add DistributedSampler to your dataloader.\r\nFor your question \"RuntimeError: DataLoader worker (pid(s) 51307) exited unexpectedly\", csdn have some solutions. And i think it is not the bug of pytorch-lightning.\r\nI think a simple solution is to set num_works=0 or 1. I recommend that you can refer to CSDN or stackoverflow, which have many solutions. "
      },
      {
        "user": "yongqianxiao",
        "created_at": "2020-11-28T15:45:53Z",
        "body": "> If you don't do some special to sampler, the pytorch-ligtning will auto add DistributedSampler to your dataloader.\r\n> For your question \"RuntimeError: DataLoader worker (pid(s) 51307) exited unexpectedly\", csdn have some solutions. And i think it is not the bug of pytorch-lightning.\r\n> I think a simple solution is to set num_works=0 or 1. I recommend that you can refer to CSDN or stackoverflow, which have many solutions.\r\n\r\nThanks for continuous replying. I had the setup of num_works=8 and I will try num_works=1 for later."
      }
    ]
  },
  {
    "number": 4870,
    "title": "Auto-scale batch size triggers \"on_train_end\"",
    "created_at": "2020-11-26T16:09:03Z",
    "closed_at": "2023-09-02T01:18:20Z",
    "labels": [
      "bug",
      "help wanted",
      "tuner",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4870",
    "body": "When running Trainer with `auto_scale_batch_size` set to true, the `on_train_end` function is called at the start, but never at the end, because there is a check to not run teardown multiple times.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4870/comments",
    "author": "victorjoos",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-26T16:09:48Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "justusschock",
        "created_at": "2020-11-30T15:01:29Z",
        "body": "cc @SkafteNicki  :D \r\n\r\nThis is probably just a variable reset somewhere, right?"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-12-14T17:22:04Z",
        "body": "@SkafteNicki mind taking a look?"
      },
      {
        "user": "tchaton",
        "created_at": "2021-09-15T11:09:17Z",
        "body": "@SkafteNicki Any updates ?"
      },
      {
        "user": "carmocca",
        "created_at": "2023-09-02T01:18:20Z",
        "body": "This doesn't seem to be an issue anymore:\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader\r\n\r\nfrom lightning.pytorch import LightningModule, Trainer\r\nfrom lightning.pytorch.demos.boring_classes import RandomDataset\r\nfrom lightning.pytorch.tuner import Tuner\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n        self.batch_size = 1\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def on_train_end(self) -> None:\r\n        print(\"On train end\")\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64), batch_size=self.batch_size)\r\n\r\n\r\ndef run():\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n    )\r\n    tuner = Tuner(trainer)\r\n    tuner.scale_batch_size(model)\r\n    trainer.fit(model)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```"
      }
    ]
  },
  {
    "number": 4849,
    "title": "Failed to get correct device id inside on_fit_start() or setup()",
    "created_at": "2020-11-25T09:15:46Z",
    "closed_at": "2020-11-26T19:19:52Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4849",
    "body": "## 🐛 Bug\r\nIn DDP training mode, calling self.device in on_fit_start() or setup() function will return \"cuda\", while calling self.device in training_step will return \"cuda:id\" where id corrspondes to the correct device id. \r\n\r\n### To Reproduce\r\n~~~\r\nBoringModel(pl.LightningModule):\r\n    def on_fit_start(self):\r\n        print(self.device)      # will always return \"cuda\" regardless which device it on\r\n    def setup(self, stage):\r\n        print(self.device)      # will always return \"cuda\" regardless which device it on\r\n    def training_step(self, **kwargs):\r\n        print(self.device)      # will return device ids, e.g. \"cuda:0\"\r\n~~~\r\n\r\n### Expected behavior\r\non_fit_start() and/or setup() should be able to get correct device ids as same in training_step().\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4849/comments",
    "author": "pableeto",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2020-11-25T13:37:17Z",
        "body": "Actually I think this is kind of the correct behaviour. `self.device` only reflects changes of device types from model parameters. \r\n\r\nIf you call `model.cuda()` on your model, `self.device` only returns `cuda` (which means, the device should be the torch default gpu) and when you call `model.cuda(1)` your device will explicitly be the first gpu.\r\n\r\n```\r\n>>> model.device\r\ndevice(type='cpu')\r\n>>> model.cuda()\r\nBoringModel()\r\n>>> model.device\r\ndevice(type='cuda')\r\n>>> model.cuda(1)\r\nBoringModel()\r\n>>> model.device\r\ndevice(type='cuda', index=1)\r\n```\r\n\r\nHowever, I opened a PR (#4851) to always include a device index there."
      }
    ]
  },
  {
    "number": 4807,
    "title": "Lightning Model module , get_model_size method",
    "created_at": "2020-11-22T14:10:19Z",
    "closed_at": "2020-12-30T03:26:40Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "discussion",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4807",
    "body": "## 🚀 Feature\r\nA get_model_size method in model module which return model size in megabyte based on precision.\r\n\r\n### Motivation\r\nJust thinking out loud..\r\n\r\n### Additional context\r\nWe can add this in mode summary too\r\n```\r\n# ------------\r\n# model\r\n# ------------\r\nmodel = LitClassifier(args.hidden_dim, args.learning_rate)\r\n>> print(model.size)\r\n>> 104.14281 Mb with fp32.\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4807/comments",
    "author": "kartik4949",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-11-22T15:40:20Z",
        "body": "That's a fun suggestion :) \r\nI suppose a method is not suitable since this is just a information query and does not muatate the state of the model. "
      },
      {
        "user": "kartik4949",
        "created_at": "2020-11-22T15:43:30Z",
        "body": "@awaelchli  what i meant was,\r\n```\r\n.\r\n.\r\ndef _get_model_size():\r\n   _precision = self.get_precision()\r\n   _total_params = self.get_params()\r\n   # calculate model_size based on _precision and _total_params\r\n   return _size\r\n\r\n@property\r\ndef size(self):\r\n   return self._get_model_size()\r\n\r\n.\r\n\r\n```\r\n\r\n:)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-22T16:36:32Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4804,
    "title": "transform module with explicit transformations coupled with torch.transforms",
    "created_at": "2020-11-22T10:53:02Z",
    "closed_at": "2020-11-22T12:44:07Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4804",
    "body": "## 🚀 Feature\r\nThinking to file a pr for transform module which consist inhouse transformations which are not present torch.transforms.\r\n### Motivation\r\nGridmask, mosaic, mixup, cutmix, etc\r\nabove augmentations are missing in most of the frameworks and they are de facto for kaggle, general computer vision solutions, etc\r\n\r\n### Reference\r\nmy library which consist all of this module -> github.com/kartik4949/TensorPipe\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4804/comments",
    "author": "kartik4949",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-11-22T11:38:01Z",
        "body": "@kartik4949 this would be more suited with bolts."
      },
      {
        "user": "kartik4949",
        "created_at": "2020-11-22T11:43:10Z",
        "body": "@ydcjeff makes sense, will look what bolts is,\r\nthanks\r\n"
      }
    ]
  },
  {
    "number": 4802,
    "title": "Adding a visualize method in LightningDataModule.",
    "created_at": "2020-11-22T10:19:53Z",
    "closed_at": "2020-11-22T12:37:07Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4802",
    "body": "## 🚀 Feature\r\nSo this feature will visualize your train/val/test data during training or you can explicitly call this method to visualize\r\n\r\nIt will visualize image data with transformations or without , it can help check the transformation sanity check for the problem statement you are solving.\r\n\r\nInitialize it will visualize classification labels, later will add bbox ,keypoints , etc support.\r\n### Motivation\r\n\r\nAlmost all framework lack this feature as default.\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\ncan add the function method in utils or visualize module.\r\nand just use it inside visualize method of lightingdatamodule.\r\n### Additional context\r\nExample Usage\r\n\r\n```\r\n parser = ArgumentParser()\r\n parser = MNISTDataModule.add_argparse_args(parser)\r\n dm = MNISTDataModule.from_argparse_args(args)\r\n # visualize\r\n dm.visualize(data='train', samples=10, index = 'random', save='visualize_train/', transforms=True)\r\n # saves 10 random images in the passed path with transformations.\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4802/comments",
    "author": "kartik4949",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-22T10:20:31Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-11-22T12:35:52Z",
        "body": "I believe this is specific to classification or image-data related tasks only, not a general one. You can still add your own methods and use them in LightningDataModule. If it's required I believe it's better suited for the bolts library."
      },
      {
        "user": "kartik4949",
        "created_at": "2020-11-22T12:37:44Z",
        "body": "> I believe this is specific to classification or image-data related tasks only, not a general one. You can still add your own methods and use them in LightningDataModule. If it's required I believe it's better suited for the bolts library.\r\n\r\nYeah didn't knew about bolt, will move this to bolt!, thanks"
      }
    ]
  },
  {
    "number": 4796,
    "title": "Lower parity tests",
    "created_at": "2020-11-20T17:24:25Z",
    "closed_at": "2021-05-05T14:14:44Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "ci"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4796",
    "body": "## 🐛 Bug\r\n\r\nWe have observed that the trainer reached the initial threshold for parity\r\nNote that we are still almost the same fast, but we may find some weeks spots\r\n\r\n## Please reproduce using \r\n\r\nsee GPU tests in the benchmark folder\r\n\r\n### Additional context\r\n\r\nactual version 1.0.7 has avg diff 0.84s",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4796/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2020-12-02T15:00:01Z",
        "body": "@Borda can you explain what is needed TODO here?"
      },
      {
        "user": "Borda",
        "created_at": "2020-12-02T15:18:43Z",
        "body": "> @Borda can you explain what is needed TODO here?\r\n\r\nthis is @tchaton's task but I guess he already sent one fix..."
      },
      {
        "user": "tchaton",
        "created_at": "2021-05-05T14:14:44Z",
        "body": "Closing this issue @Borda as we managed to improve parity tests. "
      }
    ]
  },
  {
    "number": 4790,
    "title": "a read-from-here folder or file",
    "created_at": "2020-11-20T14:31:46Z",
    "closed_at": "2020-12-28T09:54:16Z",
    "labels": [
      "help wanted",
      "won't fix",
      "docs",
      "discussion"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4790",
    "body": "## 🚀 Feature\r\nI was trying to read the project I found it hectic to continue in a normal fashion, It would have been a great help if there was a file that directed me to start reading from this folder then that one &c.\r\n\r\n### Motivation\r\n\r\nThis simple file could help and encourage others to read the project and also be less time consuming when doing so.\r\nCorrect me if I'm wrong and please help me see what my mistake was.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4790/comments",
    "author": "Devansh-Walia",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-20T14:32:37Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-11-20T15:35:33Z",
        "body": "@Devansh-Walia I am not sure if I follow your suggestion, are you talking about some PL component or any other project using PL?"
      },
      {
        "user": "Devansh-Walia",
        "created_at": "2020-11-21T05:25:19Z",
        "body": "actually no. let me try and explain. \r\nwhat I wanted to say was: A there should be a file that could direct people who are interested in reading the code of the project that highlights the workflow of the project. ( which module comes first which second)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-21T06:25:01Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4733,
    "title": "Python malloc error with iPython",
    "created_at": "2020-11-18T07:55:23Z",
    "closed_at": "2020-11-18T10:03:35Z",
    "labels": [
      "bug",
      "help wanted",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4733",
    "body": "## 🐛 Bug\r\n\r\nWhen in iPython, doing: \r\n\r\n```python\r\nfrom pytorch_lightning import Trainer\r\n```\r\n\r\nor\r\n\r\n```python\r\nimport pytorch_lightning\r\n```\r\n\r\n\r\ngives: \r\n\r\n```python\r\nIn [1]: from pytorch_lightning import Trainer\r\npython(82101,0x10e7f1dc0) malloc: can't allocate region\r\n:*** mach_vm_map(size=18446744071734263808, flags: 100) failed (error code=3)\r\npython(82101,0x10e7f1dc0) malloc: *** set a breakpoint in malloc_error_break to debug\r\ninit_dgelsd failed init\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-9a5a68534ecc> in <module>\r\n----> 1 from pytorch_lightning import Trainer\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/__init__.py in <module>\r\n     54     # We are not importing the rest of the lightning during the build process, as it may not be compiled yet\r\n     55 else:\r\n---> 56     from pytorch_lightning import metrics\r\n     57     from pytorch_lightning.callbacks import Callback\r\n     58     from pytorch_lightning.core import LightningDataModule, LightningModule\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/metrics/__init__.py in <module>\r\n     12 # See the License for the specific language governing permissions and\r\n     13 # limitations under the License.\r\n---> 14 from pytorch_lightning.metrics.metric import Metric\r\n     15 \r\n     16 from pytorch_lightning.metrics.classification import (\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/metrics/metric.py in <module>\r\n     21 \r\n     22 import os\r\n---> 23 import torch\r\n     24 from torch import nn\r\n     25 \r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/torch/__init__.py in <module>\r\n    188     if USE_GLOBAL_DEPS:\r\n    189         _load_global_deps()\r\n--> 190     from torch._C import *\r\n    191 \r\n    192 # Appease the type checker; ordinarily this binding is inserted by the\r\n\r\nImportError: numpy.core.multiarray failed to import\r\n\r\n```\r\n\r\nThough, when just using Python in terminal, does not have the same results and the import is happening as it should be. \r\n\r\nThis is a problem because different tools (e.g. Vimspector debugging tool) cannot as well import Trainer and have the exact same error. \r\n\r\n### To Reproduce\r\nInstall pytorch-lightning, open iPython, import Trainer from pytorch_lightning. \r\n\r\n### Expected behavior\r\n\r\nThe importing should happen normally, without errors. \r\n\r\n### Environment\r\n\r\n```bash\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.2\r\n\t- pyTorch_debug:     True\r\n\t- pyTorch_version:   1.7.0\r\n\t- pytorch-lightning: 1.0.7\r\n\t- tqdm:              4.51.0\r\n* System:\r\n\t- OS:                Darwin\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         i386\r\n\t- python:            3.7.9\r\n\t- version:           Darwin Kernel Version 19.6.0: Thu Oct 29 22:56:45 PDT 2020; root:xnu-6153.141.2.2~1/RELEASE_X86_64\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4733/comments",
    "author": "dr-costas",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-18T07:56:01Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-11-18T08:36:17Z",
        "body": "Hi, are you able to reproduce it in Google collab or any other online iPython env?"
      },
      {
        "user": "dr-costas",
        "created_at": "2020-11-18T08:43:44Z",
        "body": "Just tried, and nope. I cannot reproduce it there. "
      },
      {
        "user": "dr-costas",
        "created_at": "2020-11-18T10:03:35Z",
        "body": "After a bit of digging, I downgraded `numpy` to version 1.18 and all are OK. "
      }
    ]
  },
  {
    "number": 4694,
    "title": "fix failing examples",
    "created_at": "2020-11-16T13:59:53Z",
    "closed_at": "2021-02-15T15:36:14Z",
    "labels": [
      "bug",
      "help wanted",
      "example",
      "ci",
      "strategy: dp (removed in pl)",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4694",
    "body": "## 🐛 Bug\r\n\r\nWe have uncommented PL examples in #4551 but two of them turned to be failing and shall be fixed:\r\n```\r\nFAILED pl_examples/test_examples.py::test_examples_dp_image_classifier[\\n--max_epochs 1 --batch_size 32 --limit_train_batches 2 --limit_val_batches 2 --gpus 2 --distributed_backend dp --precision 16 ]\r\nFAILED pl_examples/test_examples.py::test_examples_dp_autoencoder[\\n--max_epochs 1 --batch_size 32 --limit_train_batches 2 --limit_val_batches 2 --gpus 2 --distributed_backend dp --precision 16 ]\r\n```\r\n\r\n### To Reproduce\r\n\r\nuncomment two remaining examples with TODO comment\r\n\r\n### Environment\r\n\r\nDrone Multi-GPU testing\r\n\r\n### Additional context\r\n\r\nget back full example testing...\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4694/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2020-12-10T16:21:36Z",
        "body": "TODO: add doctest so we can verify that the API is up tp date"
      },
      {
        "user": "Borda",
        "created_at": "2020-12-11T11:15:41Z",
        "body": "> TODO: add doctest so we can verify that the API is up tp date\r\n\r\nadding doctests in #5079 #5083 #5085"
      },
      {
        "user": "Borda",
        "created_at": "2021-01-07T16:24:09Z",
        "body": "Atm considering adding ddp examples only for master as they take quite some time to run, see #4995 (which shall be the last one from fixing sequence) "
      }
    ]
  },
  {
    "number": 4691,
    "title": "batch size behaves strange for ddp",
    "created_at": "2020-11-16T03:57:06Z",
    "closed_at": "2020-11-16T05:14:49Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4691",
    "body": "## 🐛 Bug\r\n\r\nI am running SimCLR code. I notice the batch size plays an important role for contrastive learning, so I want to increase batch size.\r\n\r\nI can run batch_size=64, on a single 2080Ti. When I use 4 GPUs by setting distributed_backend='ddp'. I still have to set batch_size=64, since each GPU is fully occupied by this setting when I checked by `nvidia-smi`. To be more clear, if I increase batch_size, there will be an error for CUDA out of memory.\r\n\r\nHowever, from what I understand I should be able to set batch_size = 4*64 or so.\r\n\r\n## Please reproduce using [the BoringModel and post here]\r\nI use the SimCLR example provided by pl.\r\n\r\n### To Reproduce\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\nI should be able to set batch_size = 4*64 or so.\r\n\r\n### Environment\r\n\r\nI am using 1.0.2 version from conda-forge.  I understand the lastest version is .5 or .6, but I am pretty sure there are other bugs, so using the latest version I even cannot start train SimCLR.\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: PyCharm on Ubuntu \r\n\r\n\r\n\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4691/comments",
    "author": "FrankXinqi",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-16T03:57:47Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "s-rog",
        "created_at": "2020-11-16T05:14:49Z",
        "body": "ddp batch size is per GPU so you are already getting 64*4"
      }
    ]
  },
  {
    "number": 4653,
    "title": "accumulate_grad_batches ignores last batches in epoch if number of steps is not divisible by accumulate_grad_batches?",
    "created_at": "2020-11-13T09:39:10Z",
    "closed_at": "2020-11-13T11:00:50Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4653",
    "body": "Suppose I have accumulate_grad_batches=256 and number of steps in my epoch is 260. Loss is updated only on step number 256 every epoch. I suppose it means that the last 4 batches grads are ignored. Is that correct?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4653/comments",
    "author": "Vozf",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-11-13T10:31:27Z",
        "body": "I suppose we do not ignore for last batches. Can you share a minimal example if it's not working?"
      },
      {
        "user": "Vozf",
        "created_at": "2020-11-13T10:32:54Z",
        "body": "So what is done with last 6 batches? Is it aggreagated over 6 batches instead of asked 256?"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-13T10:36:18Z",
        "body": "We call `.backward` and `optimizer.step` `optimizer.zero_grad()` for the last 4 batches."
      },
      {
        "user": "Vozf",
        "created_at": "2020-11-13T10:44:52Z",
        "body": "So first you accumulate 256 batches and call backward and then you accumulate 4 batches and call backward, correct?"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-13T10:59:08Z",
        "body": "Yep we accumulate 256 if possible and accumulate the rest of batches if it's not divisible by 256"
      },
      {
        "user": "Vozf",
        "created_at": "2020-11-13T11:00:49Z",
        "body": "Ok, thanks for clarification"
      }
    ]
  },
  {
    "number": 4645,
    "title": "Unneeded elements in state dict",
    "created_at": "2020-11-12T19:22:54Z",
    "closed_at": "2020-11-16T12:33:46Z",
    "labels": [
      "bug",
      "help wanted",
      "checkpointing"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4645",
    "body": "State_dict keys after training resnet18:\r\n```\r\ndict_keys(['model.conv1.weight', 'model.bn1.weight', 'model.bn1.bias', 'model.bn1.running_mean', 'model.bn1.running_var', 'model.bn1.num_batches_tracked', 'model.layer1.0.conv1.weight', 'model.layer1.0.bn1.weight', 'model.layer1.0.bn1.bias', 'model.layer1.0.bn1.running_mean', 'model.layer1.0.bn1.running_var', 'model.layer1.0.bn1.num_batches_tracked', 'model.layer1.0.conv2.weight', 'model.layer1.0.bn2.weight', 'model.layer1.0.bn2.bias', 'model.layer1.0.bn2.running_mean', 'model.layer1.0.bn2.running_var', 'model.layer1.0.bn2.num_batches_tracked', 'model.layer1.1.conv1.weight', 'model.layer1.1.bn1.weight', 'model.layer1.1.bn1.bias', 'model.layer1.1.bn1.running_mean', 'model.layer1.1.bn1.running_var', 'model.layer1.1.bn1.num_batches_tracked', 'model.layer1.1.conv2.weight', 'model.layer1.1.bn2.weight', 'model.layer1.1.bn2.bias', 'model.layer1.1.bn2.running_mean', 'model.layer1.1.bn2.running_var', 'model.layer1.1.bn2.num_batches_tracked', 'model.layer2.0.conv1.weight', 'model.layer2.0.bn1.weight', 'model.layer2.0.bn1.bias', 'model.layer2.0.bn1.running_mean', 'model.layer2.0.bn1.running_var', 'model.layer2.0.bn1.num_batches_tracked', 'model.layer2.0.conv2.weight', 'model.layer2.0.bn2.weight', 'model.layer2.0.bn2.bias', 'model.layer2.0.bn2.running_mean', 'model.layer2.0.bn2.running_var', 'model.layer2.0.bn2.num_batches_tracked', 'model.layer2.0.downsample.0.weight', 'model.layer2.0.downsample.1.weight', 'model.layer2.0.downsample.1.bias', 'model.layer2.0.downsample.1.running_mean', 'model.layer2.0.downsample.1.running_var', 'model.layer2.0.downsample.1.num_batches_tracked', 'model.layer2.1.conv1.weight', 'model.layer2.1.bn1.weight', 'model.layer2.1.bn1.bias', 'model.layer2.1.bn1.running_mean', 'model.layer2.1.bn1.running_var', 'model.layer2.1.bn1.num_batches_tracked', 'model.layer2.1.conv2.weight', 'model.layer2.1.bn2.weight', 'model.layer2.1.bn2.bias', 'model.layer2.1.bn2.running_mean', 'model.layer2.1.bn2.running_var', 'model.layer2.1.bn2.num_batches_tracked', 'model.layer3.0.conv1.weight', 'model.layer3.0.bn1.weight', 'model.layer3.0.bn1.bias', 'model.layer3.0.bn1.running_mean', 'model.layer3.0.bn1.running_var', 'model.layer3.0.bn1.num_batches_tracked', 'model.layer3.0.conv2.weight', 'model.layer3.0.bn2.weight', 'model.layer3.0.bn2.bias', 'model.layer3.0.bn2.running_mean', 'model.layer3.0.bn2.running_var', 'model.layer3.0.bn2.num_batches_tracked', 'model.layer3.0.downsample.0.weight', 'model.layer3.0.downsample.1.weight', 'model.layer3.0.downsample.1.bias', 'model.layer3.0.downsample.1.running_mean', 'model.layer3.0.downsample.1.running_var', 'model.layer3.0.downsample.1.num_batches_tracked', 'model.layer3.1.conv1.weight', 'model.layer3.1.bn1.weight', 'model.layer3.1.bn1.bias', 'model.layer3.1.bn1.running_mean', 'model.layer3.1.bn1.running_var', 'model.layer3.1.bn1.num_batches_tracked', 'model.layer3.1.conv2.weight', 'model.layer3.1.bn2.weight', 'model.layer3.1.bn2.bias', 'model.layer3.1.bn2.running_mean', 'model.layer3.1.bn2.running_var', 'model.layer3.1.bn2.num_batches_tracked', 'model.layer4.0.conv1.weight', 'model.layer4.0.bn1.weight', 'model.layer4.0.bn1.bias', 'model.layer4.0.bn1.running_mean', 'model.layer4.0.bn1.running_var', 'model.layer4.0.bn1.num_batches_tracked', 'model.layer4.0.conv2.weight', 'model.layer4.0.bn2.weight', 'model.layer4.0.bn2.bias', 'model.layer4.0.bn2.running_mean', 'model.layer4.0.bn2.running_var', 'model.layer4.0.bn2.num_batches_tracked', 'model.layer4.0.downsample.0.weight', 'model.layer4.0.downsample.1.weight', 'model.layer4.0.downsample.1.bias', 'model.layer4.0.downsample.1.running_mean', 'model.layer4.0.downsample.1.running_var', 'model.layer4.0.downsample.1.num_batches_tracked', 'model.layer4.1.conv1.weight', 'model.layer4.1.bn1.weight', 'model.layer4.1.bn1.bias', 'model.layer4.1.bn1.running_mean', 'model.layer4.1.bn1.running_var', 'model.layer4.1.bn1.num_batches_tracked', 'model.layer4.1.conv2.weight', 'model.layer4.1.bn2.weight', 'model.layer4.1.bn2.bias', 'model.layer4.1.bn2.running_mean', 'model.layer4.1.bn2.running_var', 'model.layer4.1.bn2.num_batches_tracked', 'model.fc.weight', 'model.fc.bias', \r\n\r\n'train_accuracy.correct', 'train_accuracy.total', 'val_accuracy.correct', 'val_accuracy.total'])\r\n```\r\nI have elements with keys \r\n`'train_accuracy.correct', 'train_accuracy.total', 'val_accuracy.correct', 'val_accuracy.total` in the state dict. Which are metrics that are logged during training.\r\n\r\nIf it is a feature - it is a strange feature. Now I need to manually delete them so that I will be able to use the checkpoint with the resnet18 model.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4645/comments",
    "author": "ternaus",
    "comments": [
      {
        "user": "Vozf",
        "created_at": "2020-11-12T19:32:15Z",
        "body": "Metric.add_state(..., persistent=False) to fix this, but I agree that True as default is strange"
      }
    ]
  },
  {
    "number": 4632,
    "title": "Default behavior of sync_batchnorm with multi-processing backends",
    "created_at": "2020-11-12T08:33:32Z",
    "closed_at": "2021-07-17T22:20:42Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "discussion",
      "distributed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4632",
    "body": "## 🚀 Feature\r\n\r\nFollowing the discussion in #4597, notify the user on the default behavior of `sync_batchnorm` when it is not explicitly passed to `pl.Trainer` by the user in multi-processing backends, e.g. DDP.\r\n\r\n### Motivation\r\n\r\nMake the transition from training on single GPU/single process backend, e.g. DP, to multi-processing multi-GPU seamless\r\n\r\n### Pitch\r\n\r\nUsers may be unfamiliar/unaware of the default behavior of batchnorm layers when using multi-processing backends and may expect batchnorm layers to be synced and updated during backpropagation. \r\n\r\nSyncBatchNorm is mostly beneficial when batch_size is small (only few samples per GPU), however, the frequent gather operations may slow down training.\r\n\r\nAlternatives:\r\n\r\n1. Set to True by default and notify user: 'Running in <multi-process> backend and sync_batchnorm was not set, we enable it for you. Please note it is mostly beneficial with small batch size but may slow down training.' I personally think this is the preferred behavior since it maintains the same behavior when moving from DP to DDP.\r\n\r\n2. Set to False by default and notify user: 'Running in <multi-process> backend, sync_batchnorm was not set and defaults to False. Consider setting sync_batchnorm=True, it may be beneficial with small batch-size.'\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4632/comments",
    "author": "itsikad",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-11-20T16:05:44Z",
        "body": "@PyTorchLightning/core-contributors any thoughts here?"
      },
      {
        "user": "s-rog",
        "created_at": "2020-11-23T00:35:05Z",
        "body": "I prefer the current False default, but I imagine a message reminding you to turn it on when it's not your intention might get annoying..."
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-11-23T05:32:53Z",
        "body": "@Borda I agree with the fact that the default syncbn behavior should be False and a warning to the users could be annoying. Setting syncbn to True is not required in majority of the cases."
      },
      {
        "user": "edenlightning",
        "created_at": "2021-05-09T14:57:17Z",
        "body": "@ananyahjha93 is this still relevant?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-06-08T23:46:48Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "itsikad",
        "created_at": "2021-06-10T11:33:42Z",
        "body": "@edenlightning Should I close this PR? "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-07-10T11:35:03Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4613,
    "title": "Strange issue with DataParallel",
    "created_at": "2020-11-10T23:51:44Z",
    "closed_at": "2020-12-24T23:01:38Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "strategy: dp (removed in pl)",
      "priority: 2"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4613",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nI'm having an issue with tensors being on different devices when using `distributed_backend=dp`. But it only occurs under what seems like some pretty specific circumstances. I can only reproduce the bug when I have done BOTH of the following:\r\n\r\n- replaced the `forward` method of the internal network (see the code sample below)\r\n- replaced the `BatchSampler` in the training dataloader with my own version\r\n\r\nEverything is fine if I do only one of those two things. And I don't have any problem doing both of them if I train on a single GPU. What's doubly strange is that I only replace the `BatchSampler` for the train loader, but I encounter the error during the initial validation sanity check. I really have no idea what's going on here.\r\n\r\n\r\n### To Reproduce\r\n\r\nHere's code to reproduce. Sorry it's a little long.\r\n\r\n```python\r\nimport os\r\nfrom collections import defaultdict\r\nimport random\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\nfrom pytorch_lightning import Trainer, LightningModule\r\nimport torchvision\r\n\r\n\r\nclass BalancedBatchSampler(torch.utils.data.sampler.BatchSampler):\r\n    '''Samples a batch by first randomly choosing n_classes classes and then\r\n    n_samples images from each class'''\r\n    def __init__(self, dataset, batch_size, n_samples):\r\n        self.targets = dataset.label\r\n        self.target_idx = defaultdict(list)\r\n        for i, t in enumerate(self.targets):\r\n            self.target_idx[t].append(i)\r\n        for ii in self.target_idx.values():\r\n            random.shuffle(ii)\r\n        self.used_count = {t: 0 for t in self.target_idx}\r\n        self.n_classes = batch_size // n_samples\r\n        self.n_samples = n_samples\r\n        self.dataset = dataset\r\n        self.batch_size = self.n_samples * self.n_classes\r\n\r\n    def __iter__(self):\r\n        self.count = 0\r\n        nsamp = self.n_samples\r\n        while self.count + self.batch_size < len(self.dataset):\r\n            classes = random.sample(self.target_idx.keys(), self.n_classes)\r\n            indices = []\r\n            for c in classes:\r\n                used = self.used_count[c]\r\n                indices.extend(self.target_idx[c][used:used+nsamp])\r\n                self.used_count[c] += nsamp\r\n                if self.used_count[c] + nsamp > len(self.target_idx[c]):\r\n                    random.shuffle(self.target_idx[c])\r\n                    self.used_count[c] = 0\r\n            yield indices\r\n            self.count += self.n_classes * self.n_samples\r\n\r\n    def __len__(self):\r\n        return len(self.dataset) // self.batch_size\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, *size)\r\n        self.label = torch.randint(0,10,(length,))\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index], self.label[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\ndef ResnetBase(resnet_model, forward_func=None):\r\n    '''Return a torchvision.models.ResNet model without the average pooling\r\n    and fully connected layers at the end. An optional forward_func can be\r\n    passed to redefine the forward pass of the model (for instance, to return\r\n    intermediate layer outputs).'''\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self.bn1(x)\r\n        x = self.relu(x)\r\n        x = self.maxpool(x)\r\n        x = self.layer1(x)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n        x = self.layer4(x)\r\n        return x\r\n\r\n    if forward_func is None: forward_func = forward\r\n\r\n    cout = resnet_model.fc.in_features\r\n    resnet_model.cout = cout\r\n    resnet_model.forward = forward_func.__get__(resnet_model)\r\n\r\n    delattr(resnet_model, 'avgpool')\r\n    delattr(resnet_model, 'fc')\r\n\r\n    return resnet_model\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self, layer):\r\n        \"\"\"\r\n        Testing PL Module\r\n\r\n        Use as follows:\r\n        - subclass\r\n        - modify the behavior for what you want\r\n\r\n        class TestModel(BaseTestModel):\r\n            def training_step(...):\r\n                # do your own thing\r\n\r\n        or:\r\n\r\n        model = BaseTestModel()\r\n        model.training_epoch_end = None\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        net = torchvision.models.resnet18(pretrained=False)\r\n        #self.net = net  # This works fine\r\n        self.net = ResnetBase(net)  # This does not\r\n        self.layer = torch.nn.Conv2d(3, 32, 3)\r\n        self.use_layer = layer\r\n\r\n    def forward(self, x):\r\n        if self.use_layer:\r\n            x = self.layer(x)\r\n        else:\r\n            x = self.net(x)\r\n        return x\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        output = self(x)\r\n        loss = self.loss(batch, output)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        output = self(x)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ndef run_test(args):\r\n    # fake data\r\n    train_data = RandomDataset((3,64,64), 128)\r\n    val_data = RandomDataset((3,64,64), 128)\r\n    if args.sampler:\r\n        sampler = BalancedBatchSampler(train_data, 32, 4)\r\n    else:\r\n        sampler = None\r\n    train_data = torch.utils.data.DataLoader(train_data, batch_sampler=sampler)\r\n    val_data = torch.utils.data.DataLoader(val_data)\r\n\r\n    # model\r\n    model = BoringModel(args.layer)\r\n    trainer = Trainer.from_argparse_args(\r\n        args, \r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=5,\r\n        limit_val_batches=5,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n        distributed_backend='dp',\r\n        gpus=2,\r\n    )\r\n    trainer.fit(model, train_data, val_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    import argparse\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--sampler', action='store_true')\r\n    parser.add_argument('--layer', action='store_true')\r\n    parser = Trainer.add_argparse_args(parser)\r\n    args = parser.parse_args()\r\n    run_test(args)\r\n```\r\n\r\nIf you save the above in `bug.py` and then run\r\n```\r\npython bug.py --sampler\r\n```\r\nYou should get the following error\r\n```\r\nTraceback (most recent call last):\r\n  File \"dpbug.py\", line 195, in <module>\r\n    run_test(args)\r\n  File \"dpbug.py\", line 185, in run_test\r\n    trainer.fit(model, train_data, val_data)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 440, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/accelerators/dp_accelerator.py\", line 97, in train\r\n    results = self.train_or_test()\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 68, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 485, in train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 544, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 713, in run_training_batch\r\n    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 452, in optimizer_step\r\n    self.trainer.accelerator_backend.optimizer_step(\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 115, in optimizer_step\r\n    model_ref.optimizer_step(\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1216, in optimizer_step\r\n    optimizer.step(closure=optimizer_closure)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/torch/optim/sgd.py\", line 86, in step\r\n    loss = closure()\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 703, in train_step_and_backward_closure\r\n    result = self.training_step_and_backward(\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 798, in training_step_and_backward\r\n    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 305, in training_step\r\n    training_step_output = self.trainer.accelerator_backend.training_step(args)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/accelerators/dp_accelerator.py\", line 111, in training_step\r\n    output = self.trainer.model(*args)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 87, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 151, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 310, in parallel_apply\r\n    raise output\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 263, in _worker\r\n    output = module.training_step(*input, **kwargs)\r\n  File \"dpbug.py\", line 147, in training_step\r\n    output = self(x)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"dpbug.py\", line 138, in forward\r\n    x = self.net(x)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"dpbug.py\", line 85, in forward\r\n    x = self.conv1(x)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 419, in forward\r\n    return self._conv_forward(input, self.weight)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 415, in _conv_forward\r\n    return F.conv2d(input, weight, self.bias, self.stride,\r\nRuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 2 does not equal 1 (while checking arguments for cudnn_convolution)\r\n```\r\n\r\nRunning it as either\r\n```\r\npython bug.py --sampler --layer\r\n```\r\nor\r\n```\r\npython bug.py\r\n```\r\ngives no error\r\n\r\nI'm on lightning version `1.0.4`.\r\n\r\n\r\n### Expected behavior\r\n\r\nNormal operation without strange device mismatch error.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.4\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: 1080 Ti\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4613/comments",
    "author": "catalys1",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-12-17T21:12:31Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "ZhaoyangZh",
        "created_at": "2021-05-31T18:03:41Z",
        "body": "Does anyone have any idea on this? I got similar issue."
      }
    ]
  },
  {
    "number": 4609,
    "title": "Test step to handle non-scalar outputs",
    "created_at": "2020-11-10T21:36:18Z",
    "closed_at": "2020-12-27T19:50:57Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4609",
    "body": "## 🚀 Feature\r\nHandle output from test loop _not_ being a single value.\r\n\r\n### Motivation\r\nI often need to use a callback to do some processing on test values (to make plots, etc.), which I like to separate from the core module code. In this case, I would like to use `on_test_batch_end` to build a list of predicted values, calculated in the core `test_step`. \r\n\r\n### Pitch\r\nTo make this work, I need to output an object from `test_step`, something like `{\"loss\": loss, \"predictions\": preds, \"truth\": truth}`. However, the test loop runs `.item()` on any torch tensors, which doesn't work if the outputs are non-scalar. It would be cool if the test loop handled this situation, otherwise the output from test loop (and therefore any inputs to callbacks) is limited to scalar tensors. \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4609/comments",
    "author": "murnanedaniel",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-12-20T17:07:18Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4589,
    "title": "Support custom accelerator nicknames",
    "created_at": "2020-11-09T14:47:31Z",
    "closed_at": "2020-12-16T23:47:01Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4589",
    "body": "Add a mapping between custom accelerators and \"nicknames\" so that you can \n\n`Trainer(accelerator=‘nickname’)\n`",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4589/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2020-11-09T22:03:36Z",
        "body": "Just curious, what is the benefit of allowing this?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-09T22:59:12Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4565,
    "title": "progress bar refresh_rate prevents old bars being cleared",
    "created_at": "2020-11-07T11:17:26Z",
    "closed_at": "2020-11-23T23:01:34Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4565",
    "body": "If you set refresh_rate=100 then the sanity check and validation bars continue being displayed forever even though leave=False. Also the validation bars only show part complete.\r\n\r\nI note also that the mininterval parameter for tqdm bars is ignored.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4565/comments",
    "author": "simonm3",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-07T11:18:11Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-11-07T17:34:03Z",
        "body": "In which terminal emulator / environment did you observe this behavior? \r\n\r\nCan you verify it with our bug report model?"
      },
      {
        "user": "simonm3",
        "created_at": "2020-11-07T19:10:42Z",
        "body": "In a jupyter notebook but likely will be same in any environment. If possible would be simpler to use tqdm for all the progress bar code rather than having a bespoke \"refresh\" mechanism. It gets to the progress point that should be the end of the bar but does not update it any more."
      },
      {
        "user": "simonm3",
        "created_at": "2020-11-07T19:12:17Z",
        "body": "works fine as long as the refresh is every iteration so bug is probably in the lightning refresh mechanism.\r\n\r\n"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-11-23T15:44:09Z",
        "body": "@awaelchli friednly ping :)"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-11-23T15:46:24Z",
        "body": "The PR is finished, it's ready for review"
      }
    ]
  },
  {
    "number": 4563,
    "title": "IOU Class Metric Module",
    "created_at": "2020-11-07T08:03:35Z",
    "closed_at": "2021-01-12T11:07:00Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4563",
    "body": "## 🚀 Feature\r\nIs there a reason why IOU doesn't have a class metric module (currently the only classification class metrics implemented are: Accuracy, Precision, Recall, Fbeta)? Is this already on the roadmap?\r\n\r\nI implemented a version below. Does this look good? If so, should I submit a PR?\r\nI was initially worried that the reason it wasn't implemented yet was non-trivial issues with syncing across devices in ddp, but I don't see any issues with my implementation... Did I miss something?\r\n\r\n\r\n```\r\nimport torch\r\nfrom typing import Any, Callable, Optional\r\n\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.metrics.metric import Metric\r\nfrom pytorch_lightning.metrics.functional.classification import stat_scores_multiple_classes\r\nfrom pytorch_lightning.metrics.functional.reduction import reduce\r\n\r\n\r\nclass IOU(Metric):\r\n    \"\"\"\r\n    Computes IOU.\r\n    Args:\r\n        ...\r\n        compute_on_step:\r\n            Forward only calls ``update()`` and return None if this is set to False. default: True\r\n        dist_sync_on_step:\r\n            Synchronize metric state across processes at each ``forward()``\r\n            before returning the value at the step. default: False\r\n        process_group:\r\n            Specify the process group on which synchronization is called. default: None (which selects the entire world)\r\n        dist_sync_fn:\r\n            Callback that performs the allgather operation on the metric state. When `None`, DDP\r\n            will be used to perform the allgather. default: None\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        num_classes: int,\r\n        ignore_index: Optional[int] = None,\r\n        absent_score: float = 0.0,\r\n        reduction: str = 'elementwise_mean',\r\n        compute_on_step: bool = True,\r\n        dist_sync_on_step: bool = False,\r\n        process_group: Optional[Any] = None,\r\n        #dist_sync_fn: Callable = None,\r\n    ):\r\n        super().__init__(\r\n            compute_on_step=compute_on_step,\r\n            dist_sync_on_step=dist_sync_on_step,\r\n            process_group=process_group,\r\n            #dist_sync_fn=dist_sync_fn,\r\n        )\r\n\r\n        self.num_classes = num_classes\r\n        self.ignore_index = ignore_index\r\n        self.absent_score = absent_score\r\n        self.reduction = reduction\r\n        self.add_state(\"tps\", default=torch.zeros(num_classes), dist_reduce_fx=\"sum\")\r\n        self.add_state(\"fps\", default=torch.zeros(num_classes), dist_reduce_fx=\"sum\")\r\n        self.add_state(\"fns\", default=torch.zeros(num_classes), dist_reduce_fx=\"sum\")\r\n        self.add_state(\"sups\", default=torch.zeros(num_classes), dist_reduce_fx=\"sum\")\r\n        \r\n\r\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\r\n        \"\"\"\r\n        Update state with predictions and targets.\r\n        Args:\r\n            preds: Predictions from model\r\n            target: Ground truth values\r\n        \"\"\"\r\n\r\n        tps, fps, _, fns, sups = stat_scores_multiple_classes(preds, target, self.num_classes)\r\n\r\n        self.tps += tps\r\n        self.fps += fps\r\n        self.fns += fns\r\n        self.sups += sups\r\n\r\n    def compute(self):\r\n        \"\"\"\r\n        Computes mean squared error over state.\r\n        \"\"\"\r\n\r\n        scores = torch.zeros(self.num_classes, device=self.tps.device, dtype=torch.float32)\r\n\r\n        for class_idx in range(self.num_classes):\r\n            if class_idx == self.ignore_index:\r\n                continue\r\n\r\n            tp = self.tps[class_idx]\r\n            fp = self.fps[class_idx]\r\n            fn = self.fns[class_idx]\r\n            sup = self.sups[class_idx]\r\n\r\n            # If this class is absent in the target (no support) AND absent in the pred (no true or false\r\n            # positives), then use the absent_score for this class.\r\n            if sup + tp + fp == 0:\r\n                scores[class_idx] = self.absent_score\r\n                continue\r\n\r\n            denom = tp + fp + fn\r\n            # Note that we do not need to worry about division-by-zero here since we know (sup + tp + fp != 0) from above,\r\n            # which means ((tp+fn) + tp + fp != 0), which means (2tp + fp + fn != 0). Since all vars are non-negative, we\r\n            # can conclude (tp + fp + fn > 0), meaning the denominator is non-zero for each class.\r\n            score = tp.to(torch.float) / denom\r\n            scores[class_idx] = score\r\n\r\n        # Remove the ignored class index from the scores.\r\n        if self.ignore_index is not None and self.ignore_index >= 0 and self.ignore_index < self.num_classes:\r\n            scores = torch.cat([\r\n                scores[:self.ignore_index],\r\n                scores[self.ignore_index + 1:],\r\n            ])\r\n\r\n        print(scores)\r\n\r\n        return reduce(scores, reduction=self.reduction)\r\n```\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4563/comments",
    "author": "zmurez",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-07T08:04:12Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-11-07T10:20:10Z",
        "body": "Hi @zmurez \r\nIt is definitely on the roadmap that all metrics in the functional backend should have a counterpart in the class backend. After we revamped class metric in v1.0 we are slowly adding class interfaces. We have just not reached IOU jet. \r\nThat said, we would be more than happy to receive a PR :]\r\n\r\nDo note the following:\r\nWe are also in the process of unifying the functional and class based backends. In practice this means that all computations should happen in the functional backend and the class backend should just call the functional backend. Please see how the code is organized for all regression metrics."
      },
      {
        "user": "heng-yuwen",
        "created_at": "2021-01-02T23:23:11Z",
        "body": "Hi, I wonder what does dist_sync_on_step mean here? shouldn't we pass a True when use method like ddp?"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2021-01-03T10:54:49Z",
        "body": "@123mutourener thats up for the user to decide. If I set it to `False` I will only get the metric from process 0. Assuming a uniform distribution of samples over all processes this may be a good enough proxie for what you want to log. Of cause setting it to `True` will give a more precise result but it will slow down training (due to added communication between processes) "
      },
      {
        "user": "heng-yuwen",
        "created_at": "2021-01-03T23:59:22Z",
        "body": "@SkafteNicki Thanks! That is helpful."
      }
    ]
  },
  {
    "number": 4522,
    "title": "Gpus=1 + precision not working when using only certain layers",
    "created_at": "2020-11-04T20:24:18Z",
    "closed_at": "2022-01-12T02:27:24Z",
    "labels": [
      "bug",
      "help wanted",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4522",
    "body": "## 🐛 Bug\n\nMaking a finetuning model where the backbone isn't training breaks 16-bit.\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4522/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-04T20:25:06Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "SeanNaren",
        "created_at": "2020-11-05T12:26:24Z",
        "body": "Just to keep all the details here, this seems to be a side effect of amp. When we call `self.trainer.scaler.step(optimizer)` internally the scaler does an inf check on the optimizer's parameters, which is the assertion being thrown. This check needs to coincide with ensuring that the parameters are even updated within this step."
      },
      {
        "user": "edenlightning",
        "created_at": "2020-11-17T18:12:35Z",
        "body": "@SeanNaren follow up with pytorch team"
      },
      {
        "user": "simonm3",
        "created_at": "2021-03-13T14:42:35Z",
        "body": "Probably related to this I am unable to train an efficientnet using precision=16. I get the error AssertionError: \"No inf checks were recorded for this optimizer.\""
      },
      {
        "user": "SeanNaren",
        "created_at": "2021-07-26T10:33:41Z",
        "body": "@simonm3 would you be able to provide more details? I'm circling back to this after a long while and need to come up with a reproducible script (cc @williamFalcon, who I think had a reproducible script at some point).\r\n\r\nBelow I have an example of a backbone which doesn't record gradients, however a single layer on top recording gradients. Maybe we can start there and modify:\r\n\r\n```python\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.backbone = torch.nn.Linear(32, 32)\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        with torch.no_grad():\r\n            x = self.backbone(x)\r\n        x = self.layer(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        precision=16,\r\n        gpus=1,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, dataloaders=test_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```"
      },
      {
        "user": "tchaton",
        "created_at": "2021-09-15T11:02:01Z",
        "body": "Hey @simonm3,\r\n\r\nAny updates ?"
      },
      {
        "user": "carmocca",
        "created_at": "2022-01-12T02:27:24Z",
        "body": "Closing this issue as it's very old and has no repro.\r\n\r\nI'm sure somebody will report it again if it's still a problem."
      }
    ]
  },
  {
    "number": 4502,
    "title": "Test drone on pytorch 1.7, 1.8",
    "created_at": "2020-11-03T18:11:35Z",
    "closed_at": "2021-07-01T06:54:20Z",
    "labels": [
      "help wanted",
      "ci"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4502",
    "body": "extend teste PT versions with multi-GPU",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4502/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-11-16T10:39:30Z",
        "body": "First, we need to extend Drone capacity, to be able to run multiple test suits per PR/commit"
      },
      {
        "user": "Borda",
        "created_at": "2020-12-20T12:12:20Z",
        "body": "related task #5205"
      },
      {
        "user": "edenlightning",
        "created_at": "2021-06-30T23:04:17Z",
        "body": "@Borda relevant?"
      }
    ]
  },
  {
    "number": 4496,
    "title": "Trainer.test() fail when trys to log_hyperparameter",
    "created_at": "2020-11-03T13:43:15Z",
    "closed_at": "2020-11-14T14:32:55Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 2"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4496",
    "body": "## 🐛 Bug\r\n\r\nThis the error code\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 31, in <module>\r\n    cli_main()\r\n  File \"test.py\", line 28, in cli_main\r\n    trainer.test(model, test_dataloaders=dm.test_dataloader())\r\n  File \"C:\\Users\\Mohammed\\.conda\\envs\\dl_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 710, in test\r\n    results = self.__test_given_model(model, test_dataloaders)\r\n  File \"C:\\Users\\Mohammed\\.conda\\envs\\dl_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 775, in __test_given_model\r\n    results = self.fit(model)\r\n  File \"C:\\Users\\Mohammed\\.conda\\envs\\dl_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 440, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"C:\\Users\\Mohammed\\.conda\\envs\\dl_env\\lib\\site-packages\\pytorch_lightning\\accelerators\\gpu_accelerator.py\", line 51, in train\r\n    self.trainer.train_loop.setup_training(model)\r\n  File \"C:\\Users\\Mohammed\\.conda\\envs\\dl_env\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\", line 135, in setup_training\r\n    self.trainer.logger.log_hyperparams(ref_model.hparams_initial)\r\n  File \"C:\\Users\\Mohammed\\.conda\\envs\\dl_env\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py\", line 35, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\Mohammed\\.conda\\envs\\dl_env\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py\", line 159, in log_hyperparams\r\n    params = self._flatten_dict(params)\r\n  File \"C:\\Users\\Mohammed\\.conda\\envs\\dl_env\\lib\\site-packages\\pytorch_lightning\\loggers\\base.py\", line 228, in _flatten_dict\r\n    return {delimiter.join(keys): val for *keys, val in _dict_generator(params)}\r\n  File \"C:\\Users\\Mohammed\\.conda\\envs\\dl_env\\lib\\site-packages\\pytorch_lightning\\loggers\\base.py\", line 228, in <dictcomp>\r\n    return {delimiter.join(keys): val for *keys, val in _dict_generator(params)}\r\nTypeError: sequence item 1: expected str instance, numpy.int32 found\r\n```\r\nI think this error is caused by something related to the saved hparams\r\nso I tried to print the hparams of my trained model, this is the result of printing my model.hparams:\r\n```\r\n\"batch_size\":    16\r\n\"callback\":      False\r\n\"ckpt_path\":     checkpoints\\sgd_cnn_rnn_5\\last.ckpt\r\n\"decoder\":       {1: 'aa', 2: 'la', 3: 'sh', 4: 'ra', 5: 'ya', 6: 'ay', 7: 'da', 8: 'wa', 9: 'sp', 10: 'ta', 11: 'te', 12: 'ka', 13: 'sa', 14: 'gh', 15: 'ee', 16: 'kh', 17: 'na', 18: 'de', 19: 'fa', 20: 'ha', 21: 'ba', 22: 'he', 23: 'hamala', 24: 'to', 25: 'ma', 26: 'th', 27: 'ke', 28: 'se', 29: 'ze', 30: 'aala', 31: 'dh', 32: 'ae', 33: 'za', 34: 'al', 35: 'aela', 36: 'ja', 37: 'hh', 38: 'mala', 39: 'ah', 40: '7', 41: '0', 42: '2', 43: 'jala', 44: 'hala', 45: 'hana', 46: '8', 47: '1', 48: 'khla', 49: '9', 50: '6', 51: 'am', 52: 'ahla'}\r\n\"epochs\":        400\r\n\"factor\":        0.5\r\n\"height\":        32\r\n\"hidden_dims\":   64\r\n\"learning_rate\": 0.003\r\n\"model_type\":    cnn_rnn\r\n\"momentum\":      0.9\r\n\"n_classes\":     53\r\n\"name\":          sgd_cnn_rnn_5_continue\r\n\"notes\":         None\r\n\"optimizer\":     sgd\r\n\"patience\":      5\r\n\"rnn_dims\":      256\r\n\"test_paths\":    None\r\n\"train_paths\":   None\r\n\"val_paths\":     None\r\n\"weight_decay\":  0.0001\r\n```\r\nThis is my testing code\r\n```\r\nparser = ArgumentParser()\r\nparser.add_argument('--test_paths', type=str, default=None, help=\"comma separate different dirs\")\r\nparser.add_argument('--ckpt_path', type=str, required=True)\r\nparser.add_argument('--height', type=int, default=32)\r\nparser.add_argument('--batch_size', type=int, default=16)\r\nargs = parser.parse_args()\r\nif args.test_paths:\r\n    test_dirs = args.test_paths.split(',')\r\n    dm = IFN_ENITDataModule(height=args.height, batch_size=args.batch_size, test_dirs=test_dirs)\r\nelse:\r\n    dm = IFN_ENITDataModule(height=args.height, batch_size=args.batch_size)\r\ndm.setup(\"test\")\r\nmodel = ResNetwork.load_from_checkpoint(checkpoint_path=args.ckpt_path, n_classes=dm.num_classes+1, decoder=dm.decoder)\r\nprint(model.hparams_initial)\r\nwandb = WandbLogger(project='Word Recognition', name=model.hparams.name, version=model.hparams.name)\r\ntrainer = pl.Trainer(gpus=1, precision=16)\r\ntrainer.test(model, test_dataloaders=dm.test_dataloader())\r\n```\r\n\r\nI have pytorch_lightning version 1.0.4",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4496/comments",
    "author": "MohammedAljahdali",
    "comments": [
      {
        "user": "MohammedAljahdali",
        "created_at": "2020-11-14T14:32:55Z",
        "body": "I think storing a dict in hparam causes this issue, however it can be solved by setting `logger=False` in the trainer."
      },
      {
        "user": "acxz",
        "created_at": "2020-12-13T01:05:15Z",
        "body": "It would be nice if by default the logger can store as much as it can, instead of having to disable it when a non-{int, float, long} type is in the hparams.\r\n\r\nI think this issue should still remain open.\r\n\r\nHere is my particular error with using tensorboard as the logger:\r\n```\r\n  File \"examples/gp_regression_example.py\", line 398, in <module>\r\n    main()\r\n  File \"examples/gp_regression_example.py\", line 181, in main\r\n    trainer.fit(model, datamodule=data_module)\r\n  File \"/home/acxz/venvs/test-venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 470, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/acxz/venvs/test-venv/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\", line 63, in train\r\n    self.trainer.train_loop.setup_training(model)\r\n  File \"/home/acxz/venvs/test-venv/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 145, in setup_training\r\n    self.trainer.logger.log_hyperparams(ref_model.hparams_initial)\r\n  File \"/home/acxz/venvs/test-venv/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\", line 39, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/acxz/venvs/test-venv/lib/python3.8/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 169, in log_hyperparams\r\n    exp, ssi, sei = hparams(params, metrics)\r\n  File \"/home/acxz/venvs/test-venv/lib/python3.8/site-packages/torch/utils/tensorboard/summary.py\", line 192, in hparams\r\n    ssi.hparams[k].number_value = v\r\nTypeError: array([ 0.08109958, -1.001376  ], dtype=float32) has type numpy.ndarray, but expected one of: int, long, float\r\n```"
      },
      {
        "user": "acxz",
        "created_at": "2022-09-12T14:43:29Z",
        "body": "Closed by #9031"
      }
    ]
  },
  {
    "number": 4491,
    "title": "TypeError: __init__() missing 2 required positional arguments: 'doc' and 'pos'",
    "created_at": "2020-11-03T01:36:55Z",
    "closed_at": "2020-11-03T03:32:43Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4491",
    "body": "## 🐛 Bug\r\n\r\n```\r\nraceback (most recent call last):\r\n  File \"main.py\", line 305, in <module>\r\n    main(hparams)\r\n  File \"main.py\", line 179, in main\r\n    trainer.fit(model, datamodule=video_datamodule)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 440, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_slurm_accelerator.py\", line 59, in train\r\n    self.ddp_train(process_idx=self.task_idx, model=model)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_slurm_accelerator.py\", line 179, in ddp_train\r\n    results = self.train_or_test()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 68, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 485, in train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 537, in run_training_epoch\r\n    for batch_idx, (batch, is_last_batch) in train_dataloader:\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/profiler/profilers.py\", line 80, in profile_iterable\r\n    value = next(iterator)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py\", line 47, in _with_is_last\r\n    for val in it:\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 345, in __next__\r\n    data = self._next_data()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 838, in _next_data\r\n    return self._process_data(data)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 881, in _process_data\r\n    data.reraise()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/torch/_utils.py\", line 395, in reraise\r\n    raise self.exc_type(msg)\r\nTypeError: __init__() missing 2 required positional arguments: 'doc' and 'pos'\r\n```\r\ncan someone give suggestion about this problem?   my dataset will load one image and one json file on each `getitem`, if I only load image, it will be ok. But if I load image and json file, it will throw this error randomly. It doesn't show that it's because json file's problem, and I have tested all json file, they can  open normally. I can't figure out what causes this bug.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4491/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "DuinoDu",
        "created_at": "2020-11-03T03:17:36Z",
        "body": "Can you post your sample code?"
      },
      {
        "user": "xiadingZ",
        "created_at": "2020-11-03T03:32:40Z",
        "body": "I have found the problem. It's because some json file is empty.  this bug it's from `json.load`, not pytorch"
      }
    ]
  },
  {
    "number": 4487,
    "title": "using autograd in neural network calculation breaks the validation step",
    "created_at": "2020-11-02T19:56:36Z",
    "closed_at": "2020-11-03T09:41:49Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4487",
    "body": "## 🐛 Bug \r\n\r\nHi,\r\n\r\njust a small bug report, maybe this can be a feature in the future.\r\nAssuming you want to have a derivative in your neural network as output,\r\n\r\nthe option: torch.set_grad_enabled(False), which is activated during validation, breaks the PyTorch lightning module.\r\n\r\nSo if you want to train and validate a model like this (see Code) in pytorch ligthning, a workaround is to  torch.set_grad_enabled(True) at the beginning of the validation step.\r\n\r\n```\r\nimport torch \r\n\r\nclass Feedforward(torch.nn.Module):\r\n        def __init__(self, input_size, hidden_size):\r\n            super(Feedforward, self).__init__()\r\n            self.input_size = input_size\r\n            self.hidden_size  = hidden_size\r\n            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\r\n            self.relu = torch.nn.ReLU()\r\n            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\r\n     \r\n        \r\n        def forward(self, x):\r\n            hidden = self.fc1(x)\r\n            relu = self.relu(hidden)\r\n            output = self.fc2(relu)\r\n            output = output.sum()\r\n            output =torch.autograd.grad(outputs=output, inputs=x, retain_graph=True, create_graph=True)\r\n            return output[0]\r\n\r\ntest_input = torch.rand((10,3),requires_grad=True)\r\ntest_output = torch.rand((10,3))\r\n\r\n\r\nmodel = Feedforward(3,10)\r\noptim = torch.optim.Adam(model.parameters())\r\noptim.zero_grad()\r\nloss_fn = torch.nn.L1Loss()\r\nmodel.train()\r\nout = model(test_input)\r\nloss = loss_fn(out, test_output)\r\nloss.backward()\r\noptim.step() \r\n\r\n```\r\n\r\nI dont know if this behavior is intendet and if you want to find a workaround, but I leave this here as a Feature Request/ Bug Report.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4487/comments",
    "author": "marcimarc1",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-02T19:57:17Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "tchaton",
        "created_at": "2020-11-03T09:41:50Z",
        "body": "Dear @marcimarc1,\n\nYes, `autograd` is blocked in validation_step for performance reason. As most people just do metric computation, they don't need gradients to be computed. I hope it makes sense.\n\nBest regards,\nT.C"
      },
      {
        "user": "fcocquemas",
        "created_at": "2021-05-24T02:37:00Z",
        "body": "Sorry to comment on a closed issue, but is there a workaround for this behavior? The metric I would like to validate on is calculated on the gradient of the network I'm training. Thank you!"
      },
      {
        "user": "marcimarc1",
        "created_at": "2021-05-25T08:16:52Z",
        "body": "Hi, you can enable the gradients manually, it increases computation time, but it works. It was already mentioned in my first entry of the problem. "
      },
      {
        "user": "fcocquemas",
        "created_at": "2021-05-25T12:26:47Z",
        "body": "Oh, thank you so much, I'd completely missed it!"
      }
    ]
  },
  {
    "number": 4467,
    "title": "calling `self.log(..., on_step=True)` in optimizer_step fails",
    "created_at": "2020-11-01T14:18:21Z",
    "closed_at": "2020-11-02T15:28:17Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4467",
    "body": "## 🐛 Bug\r\n\r\ncalling `self.log(..., on_step=True)` in optimizer_step fails.\r\n\r\n### Expected behavior\r\n\r\nIt should get treated same way if I were to `.log()` in training_step\r\n\r\n### Additional context\r\n\r\nI believe it's connected to #4439. Tried debugging and hotfixing it but the whole .log() behavior (controlled by `self._current_fx_name`) seems not to be implemented too well.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4467/comments",
    "author": "quinor",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2020-11-02T14:48:22Z",
        "body": "Hey @quinor,\r\n\r\nYou are right, we are currently refactoring logging to enable `self.log` everywhere.\r\n\r\nI am curious, what do you intent to log in `optimizer_step`.\r\n\r\nBest regards,\r\nT.C"
      },
      {
        "user": "quinor",
        "created_at": "2020-11-02T15:00:31Z",
        "body": "hey @tchaton,\r\n\r\nAs for your question: I'm trying to log the LR. To be precise,\r\n\r\n```python\r\nself.log(\"train/weight_lr\", optimizer.param_groups[0][\"lr\"], on_step=True)\r\n```"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-11-02T15:15:04Z",
        "body": "@quinor there is LearningRateMonitor you can use to log learning rates."
      },
      {
        "user": "edenlightning",
        "created_at": "2020-11-02T15:28:17Z",
        "body": "Closing issue for now. @quinor feel free to reopen if you have more questions!"
      },
      {
        "user": "vincentzlt",
        "created_at": "2020-11-15T14:29:59Z",
        "body": "have the same issue. Want to log batch accuracy at optimizer_step after gradient accumulation. Seems no other better place to log this.."
      }
    ]
  },
  {
    "number": 4444,
    "title": "Error with DDP and automatic_optimization=False",
    "created_at": "2020-10-30T17:29:56Z",
    "closed_at": "2020-11-10T19:44:52Z",
    "labels": [
      "bug",
      "help wanted",
      "distributed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4444",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen running with `distributed_backend=\"ddp\"` and `automatic_optimization=False`, there is an error when returning a detached tensor. More details below. I'm using lightning version 1.0.4; I also saw the same thing in 1.0.3. First, here's the code:\r\n\r\n```python\r\n# ddp_bug.py\r\nimport os\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\nfrom pytorch_lightning import Trainer, LightningModule\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self, detach=False):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n        self.detach = detach\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        opt = self.optimizers()\r\n        output = self.layer(batch)\r\n        loss1 = self.loss(batch, output)\r\n        self.manual_backward(loss1, opt)\r\n        opt.step()\r\n        opt.zero_grad()\r\n\r\n        if self.detach:\r\n            loss1 = loss1.detach()\r\n\r\n        return loss1\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ndef run_test(args):\r\n    class TestModel(BoringModel): pass\r\n\r\n    # fake data\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n    test_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n\r\n    # model\r\n    model = TestModel(args.detach)\r\n    trainer = Trainer.from_argparse_args(\r\n        args, \r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=5,\r\n        limit_val_batches=5,\r\n        max_epochs=1,\r\n        weights_summary=None,\r\n        automatic_optimization=False,\r\n    )\r\n    trainer.fit(model, train_data, val_data)\r\n    trainer.test(test_dataloaders=test_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    import argparse\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--detach', action='store_true')\r\n    parser = Trainer.add_argparse_args(parser)\r\n    args = parser.parse_args()\r\n    run_test(args)\r\n```\r\n\r\n### To Reproduce\r\n\r\nIf you save the script above as `ddp_bug.py` and run the following command:\r\n```\r\npython ddp_bug.py --gpus=2 --distributed_backend=ddp --detach\r\n```\r\nthe resulting error is thrown:\r\n```\r\n    self.trainer.accelerator_backend.backward(result, optimizer, opt_idx, *args, **kwargs)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 97, in backward\r\n    model.backward(closure_loss, optimizer, opt_idx, *args, **kwargs)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1103, in backward\r\n    loss.backward(*args, **kwargs)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/torch/tensor.py\", line 185, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/catalys1/pylt/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 125, in backward\r\n    Variable._execution_engine.run_backward(\r\nRuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons:\r\n1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across\r\nmultiple concurrent forward-backward passes\r\n2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to\r\nwrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward\r\npasses multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases yet.\r\n```\r\n\r\nThe following commands don't throw an error\r\n```\r\npython ddp_bug.py --gpus=2 --distributed_backend=ddp\r\n\r\npython ddp_bug.py --gpus=2 --distributed_backend=dp --detach\r\n\r\npython ddp_bug.py --gpus=1 --detach\r\n```\r\n\r\n### Expected behavior\r\n\r\nMy understanding is that setting `automatic_optimization=False` prevents any optimization steps from happening outside of the training_step function. It looks like maybe this isn't being honored? If it were, there should be no difference between returning a detached or not-detached tensor I would think.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: 1080 Ti\r\n\r\n### Additional context\r\n\r\nI encountered this bug because I'm working on a model that requires multiple backwards passes per batch.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4444/comments",
    "author": "catalys1",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2020-11-02T15:26:02Z",
        "body": "Hey @catalys1,\r\n\r\nInteresting behaviour. I was able to reproduce this bug. Let me investigate deeper :)\r\n\r\nBest,\r\nT.C"
      }
    ]
  },
  {
    "number": 4442,
    "title": "Random CUDA OOM error when starting SLURM jobs",
    "created_at": "2020-10-30T15:49:28Z",
    "closed_at": "2020-12-25T06:18:22Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "priority: 0",
      "environment: slurm"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4442",
    "body": "## 🐛 Bug\r\n\r\nWhen submitting jobs to SLURM, some jobs (around 1-2% of them) will randomly encounter a CUDA OOM error during the setup prior to training. I can confirm it's not an issue with the configuration of the job vs hardware itself, since I can resubmit the exact same job script and it will work. I also know that my resource consumption is not even close to the limit, since the jobs that do work use ~10Gb on 32Gb GPUs.\r\n\r\nI already checked with `nvidia-smi` that the GPUs I get allocated are empty at the start of the training, confirming that it's not a problem with the cluster:\r\n```shell script\r\necho $(nvidia-smi --query-gpu=memory.used --format=csv,noheader) # At the beginning of the script\r\n# Output: 0 MiB\r\n```\r\n\r\nHere is the output I get when my jobs crash:\r\n```shell script\r\nCometLogger will be initialized in online mode\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nUsing native 16bit precision.\r\nTraceback (most recent call last):\r\n  File \"my_code.py\", line XX, in XX\r\n    trainer.fit(model)\r\n  File \"my_virtualenv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 424, in fit\r\n    self.accelerator_backend.setup(model)\r\n  File \"my_virtualenv/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\", line 36, in setup\r\n    model.cuda(self.trainer.root_gpu)\r\n  File \"my_virtualenv/lib/python3.8/site-packages/pytorch_lightning/utilities/device_dtype_mixin.py\", line 124, in cuda\r\n    return super().cuda(device=device)\r\n  File \"my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 463, in cuda\r\n    return self._apply(lambda t: t.cuda(device))\r\n  File \"my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 359, in _apply\r\n    module._apply(fn)\r\n  File \"my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 359, in _apply\r\n    module._apply(fn)\r\n  File \"my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 359, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated 2 more times]\r\n  File \"my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 381, in _apply\r\n    param_applied = fn(param)\r\n  File \"my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 463, in <lambda>\r\n    return self._apply(lambda t: t.cuda(device))\r\nRuntimeError: CUDA error: out of memory\r\n```\r\n\r\n## Please reproduce using [the BoringModel and post here]\r\nUnfortunately, given the nature of the bug, it's not something I can reproduce with the `BoringModel`.\r\n\r\n### To Reproduce\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\nHere are any of the SLURM parameters that could be relevant (only excludes logging and job time):\r\n```shell script\r\n#SBATCH --gres=gpu:v100l:1\r\n#SBATCH --cpus-per-task=8\r\n#SBATCH --mem=48000M\r\n```\r\n\r\nThe related non-default parameter to the trainer and dataloaders are the following:\r\n```shell script\r\nTrainer(..., benchmark=true, precision=16, ...)\r\n\r\n# Inside the lightning module\r\nDataLoader(..., shuffle=True, num_workers=7, pin_memory=self.on_gpu, ...)\r\n```\r\nFrom experience, I set `num_workers` to 1 lower than the number of CPUs I request. On my local machine, that's never caused me any issues.\r\n\r\n### Expected behavior\r\nSLURM jobs should not encounter random CUDA OOM error when configured with the necessary ressources.\r\n\r\n### Environment\r\nPyTorch and CUDA are provided by the cluster's manager, so that they are optimized for the hardware. PyTorch Lightning I installed myself using pip.\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla V100-SXM2-32GB\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.19.1\r\n\t- pyTorch_debug:     True\r\n\t- pyTorch_version:   1.7.0\r\n\t- pytorch-lightning: 1.0.4\r\n\t- tqdm:              4.50.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         \r\n\t- python:            3.8.0\r\n\t- version:           #1 SMP Tue Feb 4 23:02:59 UTC 2020\r\n\r\n### Additional context\r\nThe PyTorch version reported here is 1.7, but I updated it just yesterday. I encountered the reported bug using both PyTorch 1.6 and 1.7.\r\n\r\nEDIT: Fixed mistakes when detailing trainer & dataloader arguments.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4442/comments",
    "author": "nathanpainchaud",
    "comments": [
      {
        "user": "robogast",
        "created_at": "2020-11-16T10:54:52Z",
        "body": "I encounter similar issues:\r\n- for _some_, but not all jobs (about 50%) I get an OOM error right at the beginning of training.\r\n- If a job doesn't immediately crash, memory usage is about ~75% (about 17/24 GB).\r\n\r\n@nathanpainchaud `num_workers` is not an argument to `pl.Trainer`, I assume you mean it to be the argument to your `DataLoader`?"
      },
      {
        "user": "nathanpainchaud",
        "created_at": "2020-11-16T15:41:21Z",
        "body": "> @nathanpainchaud `num_workers` is not an argument to `pl.Trainer`, I assume you mean it to be the argument to your `DataLoader`?\r\n\r\nYeah, my bad. I'll edit the post to make it clear."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-18T04:09:17Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "whmrtm",
        "created_at": "2021-01-01T23:00:13Z",
        "body": "Had a similar issue, and tried different torch versions (1.4 ,1.5, 1.7)"
      }
    ]
  },
  {
    "number": 4418,
    "title": "Support teardown for Lightning DataModule",
    "created_at": "2020-10-29T01:23:51Z",
    "closed_at": "2020-12-05T09:50:26Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "data handling"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4418",
    "body": "## 🚀 Feature\r\n`teardown` as a hook can be useful for data modules.\r\n\r\n### Motivation\r\n\r\nThis could be used for:\r\n- Clean up downloaded data after training finishes\r\n- Closing any open connections a dataloader makes\r\n- etc\r\n\r\n### Pitch\r\n\r\nThis has natural connections to `prepare_data` and `setup` and could be implemented very similarly to how those are supported across the data module and lightning module.\r\n\r\nBy default this should do nothing\r\n\r\ncc @nateraw ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4418/comments",
    "author": "ananthsub",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-11-28T08:04:30Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4406,
    "title": "Enable trainer val_check_interval to be greater than number of the training batches",
    "created_at": "2020-10-28T09:02:33Z",
    "closed_at": "2022-04-21T09:35:53Z",
    "labels": [
      "feature",
      "help wanted",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4406",
    "body": "Currently I can't set val_check_interval greater than number of the training batches\r\nError occurs\r\n```\r\n/site-packages/pytorch_lightning/trainer/data_loading.py\", line 203, in reset_train_dataloader\r\n    raise ValueError(\r\nValueError: `val_check_interval` (1000000) must be less than or equal to the number of the training batches (5). If you want to disable validation set `limit_val_batches` to 0.0 instead.\r\n```\r\nBut it is a useful feature when validation takes considerate time. I want to do validation after multiple train epochs(but fixed number of steps, so I can't use `check_val_every_n_epoch`)\n\ncc @borda @tchaton @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4406/comments",
    "author": "Vozf",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2020-10-28T10:18:10Z",
        "body": "This would probably require #4086 first, right?"
      },
      {
        "user": "Vozf",
        "created_at": "2020-10-28T10:22:33Z",
        "body": "Yeah, seems like so"
      },
      {
        "user": "Mi-Przystupa",
        "created_at": "2021-11-11T21:13:33Z",
        "body": "Has this issue been resolved? I'm using the latest version of pylightning and I seem to get this error when my number of training batches are less than my val_check_interval "
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-12-15T04:59:03Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4400,
    "title": "Add tests for parsing.py",
    "created_at": "2020-10-27T23:06:20Z",
    "closed_at": "2021-03-04T18:58:10Z",
    "labels": [
      "help wanted",
      "ci"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4400",
    "body": "",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4400/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "akihironitta",
        "created_at": "2020-10-30T18:17:56Z",
        "body": "Can I work on this issue?"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-10-31T16:07:17Z",
        "body": "> Can I work on this issue?\n\nSure!"
      }
    ]
  },
  {
    "number": 4389,
    "title": "DDP init hangs with num_nodes >= 2 (with wrong global ranks?)",
    "created_at": "2020-10-27T13:03:32Z",
    "closed_at": "2021-01-19T12:58:33Z",
    "labels": [
      "bug",
      "help wanted",
      "distributed",
      "environment: slurm",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4389",
    "body": "## 🐛 Bug\r\npractically, I'm doing the following:\r\n```srun python train.py --num-nodes 2 --gpus 4```\r\nWhich results in the following hanging state, where multiple processes are assigned the same global rank:\r\n```\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8\r\nLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\ninitializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8\r\nLOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\ninitializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8\r\nLOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\ninitializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8\r\nLOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\ninitializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8\r\nLOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\ninitializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\n```\r\n\r\nIf the ddp init is handled by slurm (by manually setting --tasks-per-node=4), global ranks are initiated correctly, but I get a rendezvous error:\r\n```python\r\nFile \"/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 172, in _env_rendezvous_handler\r\n    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\r\nValueError: host not found: Name or service not known\r\n```\r\n\r\n(but I'm not sure that is related, it might also be an error on my end)\r\n\r\n - PyTorch Version: 1.6\r\n - OS: Linux\r\n - How you installed PyTorch: conda\r\n- How you installed PyTorch-Lightning: pip from source (pytorch-lightning-1.0.4rc1)\r\n - Python version: 3.8\r\n - CUDA/cuDNN/NCCL version: 10.1.243 / 7.6.5.32 / 2.6.4",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4389/comments",
    "author": "robogast",
    "comments": [
      {
        "user": "Xiaohan-Wang",
        "created_at": "2020-10-28T01:42:15Z",
        "body": "Same question here.\r\nI tried `trainer = pl.Trainer(gpus=[4,7])`, and then the output was\r\n```\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4,7]\r\nLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [4,7]\r\ninitializing ddp: GLOBAL_RANK: 7, MEMBER: 8/2\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\r\nTHCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/cuda/Module.cpp line=59 error=101 : invalid device ordinal\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-28T21:03:48Z",
        "body": "@Xiaohan-Wang your error is not related to this issue here. what you report we fixed it in Lightning 1.0.4 (latest).\r\n\r\n@robogast I know DDP a bit but unfortunately not much about SLURM.  \"host not found: Name or service not known\" sounds suspiciously like master address / port are not set. Can you confirm you have set them correctly? \r\nmy suspicion is that it sees the same node twice somehow. \r\nWas it working fine before 1.0.4?"
      },
      {
        "user": "robogast",
        "created_at": "2020-10-29T15:33:38Z",
        "body": "~I fixed the latter issue by moving to a newer NCCL toolchain, so that was probably some config error on my end.\r\nThe former issue still stands though, also with my updated NCCL.~\r\n\r\nEdit: It seems I just got lucky a couple of times with the init? I still run into this problem, see comment below."
      },
      {
        "user": "hakanyi",
        "created_at": "2020-10-30T18:18:43Z",
        "body": "Same problem here -- let me know if I can help debugging. Thanks!\r\n\r\n```\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\n...\r\nLOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nUsing native 16bit precision.\r\ninitializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\ninitializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\r\n```"
      },
      {
        "user": "robogast",
        "created_at": "2020-11-02T12:54:54Z",
        "body": "About the latter issue, where I got:\r\n```python\r\nFile \"/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 172, in _env_rendezvous_handler\r\n    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\r\nValueError: host not found: Name or service not known\r\n```\r\nI added a print statement just above this LOC, and I got the following: \r\n```python\r\nmaster_addr r29n5,r33n6,r34n1, master_port 22220, world_size 16, start_daemon False, timeout 0:30:00\r\n```\r\nSo the reason it doesn't work is that the `os.environ['MASTER_ADDR']` is set to the address of multiple nodes.\r\nI can hotfix it for myself with a `master_addr = master_addr.split(',')][0]`in `rendezvous.py`, but I hope someone can figure out why these addresses are wrongly set anyway (my guess is probably somewhere in `accelerators/accelerator.py` or `accelerators/ddp_slurm_accelerator.py`?)\r\n\r\nEdit 1:\r\nBy the way:\r\nIn `accelerator.py`, in `def init_ddp_connection` there is an unused function arg `is_slurm_managing_tasks`, perhaps this was forgotten to be passed on in some manner or form?\r\n\r\nEdit 2:\r\nFor completion, I'll post a slightly bigger section of the error trace I get:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/home/robertsc/hpc-generative-models/reproducibility/pytorch/vq-vae-2-pytorch/vqvae/3d/train.py\", line 121, in <module>\r\n    main(args)\r\n  File \"/home/robertsc/hpc-generative-models/reproducibility/pytorch/vq-vae-2-pytorch/vqvae/3d/train.py\", line 105, in main\r\n    trainer.fit(model, datamodule)\r\n  File \"/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 440, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_slurm_accelerator.py\", line 59, in train\r\n    self.ddp_train(process_idx=self.task_idx, model=model)\r\n  File \"/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_slurm_accelerator.py\", line 136, in ddp_train\r\n    self.init_ddp_connection(\r\n  File \"/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 212, in init_ddp_connection\r\n    torch_distrib.init_process_group(\r\n  File \"/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 422, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 174, in _env_rendezvous_handler\r\n    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\r\nValueError: host not found: Name or service not known\r\n```"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-18T04:09:16Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-01-18T00:52:19Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-01-18T02:29:22Z",
        "body": "@robogast see here someone figured it out, the fix is as you say to split the names: #5533"
      },
      {
        "user": "mehmetfdemirel",
        "created_at": "2021-12-27T23:02:56Z",
        "body": "I am having the same problem, and do not understand what I need to do. I have installed `pytorch-lightning` via `pip`, which I thought would contain the updated source code. [The file](pytorch_lightning/trainer/connectors/slurm_connector.py) that they made the \"fix\" on in #5533 doesn't even exist anymore. Any suggestions?"
      }
    ]
  },
  {
    "number": 4380,
    "title": "WandbLogger _sanitize_callable_params throws AttributeError if param does not have __name__",
    "created_at": "2020-10-26T20:04:56Z",
    "closed_at": "2020-11-02T09:04:51Z",
    "labels": [
      "bug",
      "help wanted",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4380",
    "body": "## 🐛 Bug\r\n\r\nUsing WandB logger throws an error:\r\n\r\n```bash\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap  \r\n    fn(i, *args)\r\n  File \"/home/adrian/repositories/pytorch-lightning/pytorch_lightning/accelerators/ddp_spawn_accelerator.py\", line 145, in ddp_train\r\n    self.trainer.train_loop.setup_training(model)\r\n  File \"/home/adrian/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 135, in setup_training\r\n    self.trainer.logger.log_hyperparams(ref_model.hparams_initial)\r\n  File \"/home/adrian/repositories/pytorch-lightning/pytorch_lightning/utilities/distributed.py\", line 35, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/adrian/repositories/pytorch-lightning/pytorch_lightning/loggers/wandb.py\", line 138, in log_hyperparams\r\n    params = self._sanitize_callable_params(params)\r\n  File \"/home/adrian/repositories/pytorch-lightning/pytorch_lightning/loggers/base.py\", line 194, in _sanitize_callable_params\r\n    return {key: _sanitize_callable(val) for key, val in params.items()}\r\n  File \"/home/adrian/repositories/pytorch-lightning/pytorch_lightning/loggers/base.py\", line 194, in <dictcomp>\r\n    return {key: _sanitize_callable(val) for key, val in params.items()}\r\n  File \"/home/adrian/repositories/pytorch-lightning/pytorch_lightning/loggers/base.py\", line 191, in _sanitize_callable\r\n    return val.__name__\r\n  File \"/home/adrian/.conda/envs/lightning/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 772, in __getattr__\r\n    type(self).__name__, name))\r\ntorch.nn.modules.module.ModuleAttributeError: 'Backbone' object has no attribute '__name__'\r\n\r\n```\r\n\r\n### To Reproduce\r\n\r\n```python\r\n\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nimport pytorch_lightning as pl\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader, random_split\r\n\r\nfrom pytorch_lightning.loggers import WandbLogger\r\n\r\ntry:\r\n    from torchvision.datasets.mnist import MNIST\r\n    from torchvision import transforms\r\nexcept Exception as e:\r\n    from tests.base.datasets import MNIST\r\n\r\n\r\nclass Backbone(torch.nn.Module):\r\n    def __init__(self, hidden_dim=128):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, hidden_dim)\r\n        self.l2 = torch.nn.Linear(hidden_dim, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        x = torch.relu(self.l1(x))\r\n        x = torch.relu(self.l2(x))\r\n        return x\r\n\r\n\r\nclass LitClassifier(pl.LightningModule):\r\n    def __init__(self, backbone, learning_rate=1e-3):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        self.backbone = backbone\r\n\r\n    def forward(self, x):\r\n        # use forward for inference/predictions\r\n        embedding = self.backbone(x)\r\n        return embedding\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.backbone(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('train_loss', loss)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.backbone(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('valid_loss', loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.backbone(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('test_loss', loss)\r\n\r\n    def configure_optimizers(self):\r\n        # self.hparams available because we called self.save_hyperparameters()\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument('--learning_rate', type=float, default=0.0001)\r\n        return parser\r\n\r\n\r\ndef cli_main():\r\n    pl.seed_everything(1234)\r\n\r\n    # ------------\r\n    # args\r\n    # ------------\r\n    parser = ArgumentParser()\r\n    parser.add_argument('--batch_size', default=32, type=int)\r\n    parser.add_argument('--hidden_dim', type=int, default=128)\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    parser = LitClassifier.add_model_specific_args(parser)\r\n    args = parser.parse_args()\r\n\r\n    # ------------\r\n    # data\r\n    # ------------\r\n    dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\r\n    mnist_test = MNIST('', train=False, download=True, transform=transforms.ToTensor())\r\n    mnist_train, mnist_val = random_split(dataset, [55000, 5000])\r\n\r\n    train_loader = DataLoader(mnist_train, batch_size=args.batch_size)\r\n    val_loader = DataLoader(mnist_val, batch_size=args.batch_size)\r\n    test_loader = DataLoader(mnist_test, batch_size=args.batch_size)\r\n\r\n    # ------------\r\n    # model\r\n    # ------------\r\n    model = LitClassifier(Backbone(hidden_dim=args.hidden_dim), args.learning_rate)\r\n\r\n    logger = WandbLogger(project=\"test\", name=\"test\")\r\n\r\n    # ------------\r\n    # training\r\n    # ------------\r\n    trainer = pl.Trainer.from_argparse_args(args, max_steps=1, limit_train_batches=2, logger=logger)\r\n    trainer.fit(model, train_loader, val_loader)\r\n\r\n    # ------------\r\n    # testing\r\n    # ------------\r\n    result = trainer.test(model, test_dataloaders=test_loader)\r\n    print(result)\r\n\r\n\r\nif __name__ == '__main__':\r\n    cli_main()\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\nenvironment does not matter\r\n\r\n### Additional context\r\n\r\nA recent PR  #4320 introduced the function that throws the error.\r\n\r\ncc @tchaton \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4380/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-10-26T20:10:11Z",
        "body": "Looks like we need to be careful with objects that don't have `__name__`"
      },
      {
        "user": "tchaton",
        "created_at": "2020-10-29T09:27:54Z",
        "body": "```\r\ndef test_wandb_sanitize_callable_params_with_model(tmpdir):\r\n    \"\"\"\r\n    Callback function are not serializiable. Therefore, we get them a chance to return\r\n    something and if the returned type is not accepted, return None.\r\n    \"\"\"\r\n    class ExtendedModel(BoringModel):\r\n        @staticmethod\r\n        def add_model_specific_args(parser):\r\n            parser = ArgumentParser(parents=[parser], add_help=False)\r\n            parser.add_argument('--learning_rate', type=float, default=0.0001)\r\n            return parser        \r\n\r\n    parser = ArgumentParser()\r\n    parser.add_argument('--batch_size', default=32, type=int)\r\n    parser.add_argument('--hidden_dim', type=int, default=128)\r\n    parser = Trainer.add_argparse_args(parser)\r\n    parser = ExtendedModel.add_model_specific_args(parser)\r\n    args = parser.parse_args('--max_steps 1'.split(' '))\r\n\r\n    model = ExtendedModel()\r\n    logger = WandbLogger(project=\"test\", name=\"test\", offline=True)\r\n    # ------------\r\n    trainer = Trainer.from_argparse_args(args, max_steps=1, limit_train_batches=2, logger=logger)\r\n    trainer.fit(model)\r\n```\r\nI created this code to reproduce the bug, but I couldn't. I have removed the __name__ by default which None"
      }
    ]
  },
  {
    "number": 4370,
    "title": "Make profiling possible on validation steps.",
    "created_at": "2020-10-26T13:41:17Z",
    "closed_at": "2020-12-05T19:43:29Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4370",
    "body": "## 🚀 Make profiling possible on validation steps.\r\nIt would be really nice, if we could also profile function calls at validation and/or test stage.\r\n\r\n### Motivation\r\nIn my case the implementation and functionality of the validation step is really different than the one from the training step so it would be nice to also profile this one. Different metrics are getting calculated and some other stuff is going on.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4370/comments",
    "author": "pafi-code",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-10-29T16:42:04Z",
        "body": "@pafi-code are you interested in implementing this feature?\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-28T18:59:15Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4302,
    "title": "Using `terminate_on_nan=True` on TPU leads to huge slowdowns",
    "created_at": "2020-10-22T09:55:40Z",
    "closed_at": "2020-10-29T07:04:21Z",
    "labels": [
      "bug",
      "help wanted",
      "accelerator: tpu"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4302",
    "body": "## 🐛 Bug\r\n\r\nUsing `trainer = Trainer(terminate_on_nan=True, ...)` leads to calling `TrainerTrainingTricksMixin().detect_nan_tensors()`, which is very slow on TPU. About 5x slower forward passes for resnet50 scale nets.\r\n\r\nShould maybe raise a warning if using TPUs and `terminate_on_nan`.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4302/comments",
    "author": "harpone",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-10-29T02:37:11Z",
        "body": "yes terminate on nan always leads to a slow down, also for GPU and CPU not just TPU. I don't see a way to speed it up, and it's not clear to me why it is a bug?\r\nIt should only be used for debugging."
      },
      {
        "user": "harpone",
        "created_at": "2020-10-29T07:04:21Z",
        "body": "Seemed pretty fast on GPU, but OK. No need to speed it up but a warning would be nice."
      }
    ]
  },
  {
    "number": 4280,
    "title": "auto_lr_find with ddp and slurm",
    "created_at": "2020-10-21T10:11:41Z",
    "closed_at": "2021-01-07T05:25:00Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "distributed",
      "checkpointing",
      "tuner",
      "environment: slurm",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4280",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIf `auto_lr_find` is used in `ddp` mode with `num_nodes > 1` and slurm you will get an error that the checkpoint file was not found and therefore could not be deleted:\r\n\r\n```python\r\n  File \"MyModel.py\", line 63, in <module>\r\n    trainer.tune(model, dm)\r\n  File \"/gpfs/home/username/miniconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 804, in tune\r\n    self.tuner.tune(model, train_dataloader, val_dataloaders, datamodule)\r\n  File \"/gpfs/home/username/miniconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\", line 54, in tune\r\n    self.internal_find_lr(self.trainer, model)\r\n  File \"/gpfs/home/username/miniconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\", line 131, in internal_find_lr\r\n    return _run_lr_finder_internally(trainer, model)\r\n  File \"/gpfs/home/username/miniconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py\", line 43, in _run_lr_finder_internally\r\n    lr_finder = lr_find(trainer, model)\r\n  File \"/gpfs/home/username/miniconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py\", line 188, in lr_find\r\n    os.remove(save_path)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/gpfs/home/username/my_project/lr_find_temp.ckpt'\r\n```\r\n\r\n### To Reproduce\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\nUnfortunately the BoringModel doesn't use a LightningDataModule and therefore `trainer.tune(model, dataloader)` would not work. That's why I created a minimal working example, which uses a LightningDataModule:\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch.nn as nn\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torchvision import transforms\r\nfrom torch.utils.data import DataLoader, Dataset\r\nimport numpy as np\r\n\r\n\r\nclass MyDataset(Dataset):\r\n    def __init__(self, latent_dim=10000, n_samples=10000):\r\n        self.data = np.random.random([n_samples, latent_dim])\r\n\r\n    def __getitem__(self, index):\r\n        return torch.from_numpy(self.data[index]).float()\r\n\r\n    def __len__(self):\r\n        return len(self.data)\r\n\r\n\r\nclass MyDataModule(pl.LightningDataModule):\r\n    def __init__(self, batch_size=8):\r\n        super().__init__()\r\n        self.batch_size = batch_size\r\n\r\n    def setup(self, stage):\r\n        self.train_dataset = MyDataset()\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.train_dataset, batch_size=self.batch_size)\r\n\r\n\r\nclass MyTestModel(pl.LightningModule):\r\n    def __init__(self, lr=1e-3, batch_size=8, **kwargs):\r\n        super().__init__()\r\n        self.learning_rate = lr\r\n        self.save_hyperparameters()\r\n        width = 10000\r\n        self.linear1 = nn.Linear(10000, width)\r\n        self.linear2 = nn.Linear(width, 10000)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\r\n\r\n    def forward(self, x):\r\n        x = self.linear1(x)\r\n        x = self.linear2(x)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x = batch\r\n        y_hat = self(x)\r\n        loss = F.mse_loss(y_hat, x)\r\n        return loss\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    dm = MyDataModule()\r\n    model = MyTestModel()\r\n\r\n    trainer = pl.Trainer(auto_lr_find=True, gpus=1)\r\n    trainer.tune(model, dm)\r\n    trainer.fit(model, dm)\r\n```\r\n\r\nThe slurm file would be:\r\n\r\n```bash\r\n#!/bin/bash\r\n\r\n# SLURM SUBMIT SCRIPT\r\n#SBATCH --job-name=Test-DDP\r\n#SBATCH --nodes=2\r\n#SBATCH --ntasks-per-node=1\r\n#SBATCH --time=08:00:00\r\n#SBATCH --account=username\r\n#SBATCH --partition=pGPU\r\n#SBATCH --exclusive\r\n\r\n\r\nmodule load compilers/cuda/10.1\r\nexport CUDA_VISIBLE_DEVICES=\"0\"\r\nnvidia-smi\r\nsrun /gpfs/home/username/miniconda3/envs/pytorch/bin/python MyModel.py \\\r\n      --gpus 1 --num_nodes 2 --distributed_backend \"ddp\"\r\n\r\n```\r\n\r\n### Environment\r\n - PyTorch Version (e.g., 1.0): 1.0.2\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): - \r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: Many nodes with Tesla-V100, each node having exactly one.\r\n - Any other relevant information: Slurm for scheduling jobs.\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\nI think the problem is that each node tries to delete the checkpoint file. The first node succeeds with that, but when the\r\nsecond node tries the file is not there anymore.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4280/comments",
    "author": "t-schanz",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-10-21T10:12:24Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-10-29T20:47:52Z",
        "body": "Wow that's interesting. @SkafteNicki is this related to tune? or prob to checkpointing?"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-11-13T15:10:27Z",
        "body": "@SkafteNicki "
      },
      {
        "user": "limberc",
        "created_at": "2020-12-01T02:14:11Z",
        "body": "I have encountered the same issue too.\r\n\r\nIt seems that all DDP related method is conflict to any `auto` flag. `auto_lr_find` and `auto_scale_batch_size` are not supported when using DDP."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-31T04:52:11Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "IsCoelacanth",
        "created_at": "2021-01-07T08:34:47Z",
        "body": "facing the same issue looks like it's an issue when using DDP with more than 1 GPU, the checkpoint for the auto LR is created by the controller / main process, while at the end of auto-tune each spawn tries to delete the checkpoint file."
      },
      {
        "user": "yuys0602",
        "created_at": "2021-05-05T14:50:48Z",
        "body": "facing the same issue, and want to know how to solve this problem."
      }
    ]
  },
  {
    "number": 4272,
    "title": "SimCLR init_encoder should have an *if* condition to replace first conv block",
    "created_at": "2020-10-21T05:51:05Z",
    "closed_at": "2020-10-22T07:26:28Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4272",
    "body": "## 🐛 Bug\r\nIn the init_encoder of simclr_module.py , the method replaces the first convolution block regardless of the image size. This is only useful when image size is small(eg 32 for CIFAR10), however this is undesirable for large image sizes(eg. 256) as it doesn't allow the image to shrink and creates unnecessary computational overhead making training difficult. \r\n\r\n**Here is the relevant code block from the lightning repo :**\r\n```python\r\ndef init_encoder(self):\r\n        encoder = resnet50_bn(return_all_feature_maps=False)\r\n\r\n        #when using cifar10, replace the first conv so image doesn't shrink away\r\n        encoder.conv1 = nn.Conv2d(\r\n            3, 64,\r\n            kernel_size=3,\r\n            stride=1,\r\n            padding=1,\r\n            bias=False\r\n        )\r\n        return encoder\r\n```\r\n## **Possible solution** \r\nPlease add an if statement taking into account the image size, or additionally pass an argument in the main function to do so. It would give user more flexibility according to their dataset instead of copying whole module and replacing this line in their own system (like I am doing so now). ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4272/comments",
    "author": "alyashgo",
    "comments": [
      {
        "user": "ananyahjha93",
        "created_at": "2020-10-21T22:00:31Z",
        "body": "@alyashgo This is being updated as a part of Simclr update, we are ensuring that simclr will run on TPUs on colab and also sync negative samples with gradients across accelerators.\r\n\r\nAlso, can you keep the simclr issue to bolts, we are focussing on bolts actively after v1 release of lightning."
      },
      {
        "user": "alyashgo",
        "created_at": "2020-10-22T07:25:50Z",
        "body": "Thanks! It'll also be helpful if you can add custom encoder support like the moco repo. \r\nSure, will close this and redirect to bolts. "
      }
    ]
  },
  {
    "number": 4268,
    "title": "EarlyStopping mode auto is unknown, fallback to auto mode.",
    "created_at": "2020-10-21T00:06:22Z",
    "closed_at": "2020-10-21T18:14:13Z",
    "labels": [
      "bug",
      "help wanted",
      "good first issue",
      "callback"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4268",
    "body": "In the process of refactoring as I upgraded lighting and it looks like there's been a slight change to the callback interface.\r\n\r\nThis code:\r\n\r\n`early_stop_callback = pl.callbacks.EarlyStopping(verbose=True)`\r\n\r\nResults in the message:\r\n\r\n> EarlyStopping mode auto is unknown, fallback to auto mode.\r\n> EarlyStopping mode set to min for monitoring early_stop_on.\r\n\r\nThe default for `mode` is 'auto' so the first message doesn't make sense\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4268/comments",
    "author": "david-waterworth",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2020-10-21T07:43:33Z",
        "body": "Hey @david-waterworth,\r\n\r\nThanks for using Pytorch Lightning and finding this bug !\r\n\r\nI think it should be this way.\r\n\r\n```\r\nEarlyStopping mode not provided, fallback to auto mode.\r\n...\r\nEarlyStopping mode set to min for monitoring early_stop_on.\r\n```\r\n\r\nBest regards,\r\nThomas Chaton.\r\n"
      }
    ]
  },
  {
    "number": 4265,
    "title": "slurm auto re-queue inconsistency",
    "created_at": "2020-10-20T18:28:50Z",
    "closed_at": "2020-12-25T06:18:21Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix",
      "checkpointing",
      "environment: slurm",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4265",
    "body": "Hi! I submitted a slurm job-array with pytorch lightning functionality. I used the suggested signal (#SBATCH --signal=SIGUSR1@90) and set distributed_backend to 'ddp' in the Trainer call. I did notice successful auto-resubmission this morning whenever my jobs were pre-emptied; however, I now notice that several of them have not completed and are not queued either. Wondering if this has been reported by someone earlier and any clue why this could happen? Is there a maximum to the number of times the jobs would be re-queued or other slurm rules that may prevent requeuing, etc.? My jobs might have been pre-emptied several times as I was running them on the low priority \"non-capped\" queue so as to occupy maximum number of gpus whenever they become available (already using my quota of high/medium priority queues). Thanks in advance! ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4265/comments",
    "author": "aabrol",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-10-20T18:32:10Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "aabrol",
        "created_at": "2020-10-23T04:57:59Z",
        "body": "Another issue I noticed is that re-queuing is not working as expected. Here are details relevant to this issue:\r\n\r\nTraining doesn't seem to resume from the \"checkpoint saved on slurm-preemption\" as number of epochs on tensorboard seem to well exceed max_epcohs for the training and there are clear breaks in the epoch plot as well as the performance metric plots (while plotting horizontal axis in relative/wall mode in tensorboard). \r\n\r\nAs clear from the trainer call in submitted code below, I am saving/logging by the PTL defaults (path/logger). A specific job is always logged to the same (originally created) lightning_logs/version_num directory every time it is preempted, however a separate \"events.out.tfevents.\" file seems to be created each time this job is preemptied. Additionally, lightning_logs/version_num/checkpoints lists several checkpoints [e.g. epoch=37.ckpt  epoch=41-v0.ckpt  epoch=4.ckpt  epoch=9.ckpt]. \r\n\r\nI was hoping to be able to resume training from checkpoint created on pre-emption, and any points explaining the behavior I observe would be appreciated. Thanks!\r\n\r\nHere is the slurm script \"slurm_run_train.sh\" submitted as \"sbatch --array=[0-9] slurm_run_train.sh\".\r\n\r\n```\r\n#!/bin/bash\r\n#SBATCH -N 1 \r\n#SBATCH -n 1\r\n#SBATCH -p qGPUM\r\n#SBATCH --gres=gpu:1\r\n#SBATCH -c 8\r\n#SBATCH --mem-per-cpu=4000\r\n#SBATCH -t 7200\r\n#SBATCH -J xyz\r\n#SBATCH -e ./slurmlogs/err%A-%a.csv\r\n#SBATCH -o ./slurmlogs/out%A-%a.csv\r\n#SBATCH -A xxyyzz\r\n#SBATCH --oversubscribe \r\n#SBATCH --mail-type=ALL\r\n#SBATCH --mail-user=xyz@xyz.edu\r\n#SBATCH --signal=SIGUSR1@90\r\n\r\nsleep 7s\r\nsource activate /home/users/xyz/anaconda3/envs/chumps/\r\npython run_train.py\r\nsleep 7s\r\n```\r\n\r\nHere is relevant part of run_train.py\r\n\r\n```\r\nif __name__ == '__main__':\r\n    rep = int(os.environ['SLURM_ARRAY_TASK_ID'])\r\n    cfg = Config( ... )\r\n    train(cfg)\r\n```\r\n\r\nHere is the called train method:\r\n\r\n```\r\ndef train(cfg: Config):\r\n    from xyz import xyznet as Model\r\n    epochs = 100\r\n    model = Model(cfg)\r\n    trainer = Trainer(gpus=1, progress_bar_refresh_rate=0, max_epochs=epochs, distributed_backend='ddp')\r\n    trainer.fit(model)\r\n    trainer.test() \r\n```\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-18T04:09:18Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4238,
    "title": "Metrics do not support multilabel tasks.",
    "created_at": "2020-10-19T18:00:26Z",
    "closed_at": "2020-10-22T16:13:54Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4238",
    "body": "## 🐛 Bug\r\n\r\nScikit-learn metrics deal well will multilabel tasks, but this doesn't seem to be supported in Pytorch-Lightning metrics.  There is this #3350 , but it seems to confuse multiclass with multilabel (multiple values to predict). \r\n\r\n### To Reproduce\r\nGiven predictions tensor: \r\n```\r\ntensor([[0., 0.],\r\n              [0., 0.],\r\n              [0., 0.],\r\n              [0., 0.],\r\n              [0., 0.],\r\n              [0., 0.],\r\n              [0., 0.],\r\n              [0., 0.]])\r\n```\r\nand labels tensor:\r\n ```\r\ntensor([[1, 0],\r\n              [1, 0],\r\n              [1, 0],\r\n              [1, 0],\r\n              [1, 0],\r\n              [1, 0],\r\n              [1, 0],\r\n              [1, 0]])\r\n\r\n```\r\nThe call to `f1_score(met_preds,labels, class_reduction='macro')` yields `tensor(0.3333)`, because it flattens the tensors and macro-averages per class.\r\n\r\n### Expected behavior\r\n\r\nI would expect it to be consistent with the call to\r\n`sk_f1_score(labels.numpy(), met_preds.numpy(), average='macro')`, which yields `0.0`, because it treats each column separately and macro-averages them per task.\r\n\r\nThis discrepancy also occurs for other metrics. For example sklearn deals with multilabel accuracy by using subset accuracy (0 here), but PL produces an accuracy score of 0.5.\r\n\r\n### Environment\r\n - PyTorch Version : 1.6\r\n - OS (e.g., Linux): OSX\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.7.8\r\n- Pytorch-Lightning version : 1.0.2\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4238/comments",
    "author": "jdhorwood",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-10-19T18:01:08Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-19T18:13:01Z",
        "body": "@justusschock mind have look? :]"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-10-20T20:50:42Z",
        "body": "@teddykoker mind taking a look and update the documentation accordingly if needed?"
      },
      {
        "user": "teddykoker",
        "created_at": "2020-10-20T22:51:12Z",
        "body": "Could you try the class interface? We have tested multilabel f1 with the class metrics, I think we are in the process of making sure we have the same functionality for functional"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-21T07:46:51Z",
        "body": "As @teddykoker stated we are in the process of unifying the metrics class based metrics with the functional metrics (starting with all the regression metrics here #4166 ). Until then, please use the `Fbeta` metric (however, note that there is currently a bug in that metric, that should be taken care of by this PR #4183 ):\r\n```\r\nmetric = Fbeta(beta=1.0, multilabel=True, average='macro)\r\nmetric(preds, target)\r\n>>> tensor(0.)\r\n```"
      },
      {
        "user": "jdhorwood",
        "created_at": "2020-10-21T21:39:27Z",
        "body": "Thanks! Will stick to class-based metrics for the time being."
      },
      {
        "user": "edenlightning",
        "created_at": "2020-10-22T16:09:08Z",
        "body": "@teddykoker @SkafteNicki can this be closed? is this fixed by #4166?"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-22T16:13:49Z",
        "body": "Yes, let's close it. I will unify fbeta functional and class metric within a couple of days, so functional also support multilabel. "
      }
    ]
  },
  {
    "number": 4216,
    "title": "Checkpoint is saving the model based on the last val_metric_step value and not val_metric_epoch",
    "created_at": "2020-10-18T13:19:20Z",
    "closed_at": "2020-11-24T09:41:32Z",
    "labels": [
      "help wanted",
      "docs",
      "checkpointing"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4216",
    "body": "## 🐛 Bug\r\n\r\nCheckpoint callback did not save some models even thought they achieved better result in the monitored metric, than the currently top k saved models\r\n\r\n### Expected behavior\r\n\r\nCheckpoint callback saving the best scoring models based on a metric\r\n\r\n### Environment\r\n\r\nI am using pytorch-lightning 1.0.2\r\n\r\n#### Update:\r\nI changed the checkpoint call back to add the value I am monitoring to the name of the saved checkpoint, what I notice it's not the epoch value, but the last step in the epoch value, so it's not taking the metric average value, but taking only the last one.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4216/comments",
    "author": "MohammedAljahdali",
    "comments": [
      {
        "user": "ananyahjha93",
        "created_at": "2020-10-19T15:46:43Z",
        "body": "Can you post some code to reproduce this? Or code snippet for training_step and validation_step"
      },
      {
        "user": "MohammedAljahdali",
        "created_at": "2020-10-21T11:43:00Z",
        "body": "This is what I log on my train and validation step:\r\n```\r\nvalues = {'val_loss': loss, 'val_cer': cer_avg}\r\nself.log_dict(values, logger=True, prog_bar=True, on_step=True, on_epoch=True)\r\n```\r\nAnd this my checkpoint callback :\r\n`checkpoint_callback = ModelCheckpoint(filepath='checkpoints/model_64_3/word_recog-{epoch:02d}-{val_cer:.2f}',save_last=True, mode='min', monitor='val_cer', save_top_k=5)`"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-10-22T16:22:04Z",
        "body": "@awaelchli or @justusschock maybe related to other issues?"
      },
      {
        "user": "MohammedAljahdali",
        "created_at": "2020-10-22T20:00:27Z",
        "body": "update:\r\nNow I set on_step=False, and the checkpoint seems to correctly saving the best model"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-23T23:26:16Z",
        "body": "I know what's going on here. \r\nWhen you log both on step and on epoch, i.e.\r\n\r\n```python\r\nself.log_dict(values, on_step=True, on_epoch=True)\r\n```\r\nLighting will create the keys \r\n\r\n- val_cer_step\r\n- val_cer_epoch\r\n\r\nThis is needed because it cannot log the val_cer on epoch and val_cer on step to the same graph in tensorboard. \r\nSo your ModelCheckpoint should monitor the epoch metric: \r\n```python\r\ncheckpoint_callback = ModelCheckpoint(\r\n    dirpath=\"checkpoints/model_64_3\", \r\n    filename=\"/word_recog-{epoch:02d}-{val_cer_epoch:.2f}\",  # <--- note epoch suffix here\r\n    save_last=True, \r\n    mode='min', \r\n    monitor='val_cer_epoch',   # <--- note epoch suffix here\r\n    save_top_k=5\r\n)\r\n```\r\n\r\nI will send a PR that updates the docs explaining this behaviour."
      },
      {
        "user": "MohammedAljahdali",
        "created_at": "2020-10-23T23:33:23Z",
        "body": "To be honest I changed my code currently, so I can not test this, but I believe that I set monitor=val_cer_epoch and the checkpoint did not save the height cer. But about the name of the checkpoint file I think it was I mistake, and I should have set it to val_cer_epoch.\n\nThank you for the help, and I hope that this issue was helpful to this great Library."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-23T23:37:47Z",
        "body": "> But about the name of the checkpoint file I think it was I mistake, and I should have set it to val_cer_epoch.\r\n\r\nYes, that would also explain this, because otherwise it would show the val_cer of the last batch in the validation loop in the name of the checkpoint, even if it saves the correct checkpoint"
      },
      {
        "user": "MohammedAljahdali",
        "created_at": "2020-10-23T23:45:13Z",
        "body": "I maybe wrong but, I checked the date of the saved checkpoint and the date of highest val_cer_epoch via tensorboard, and it wasn't the same. \nThis is why I was sure it wasn't saving the best checkpoint. "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-23T23:47:01Z",
        "body": "ok, just note that if you want to get the highest value as the best, then you need to set mode=\"max\", but you have mode=\"min\". "
      },
      {
        "user": "MohammedAljahdali",
        "created_at": "2020-10-23T23:49:21Z",
        "body": "Sorry my bad when I said the highest I meant the best value, CER is character error rate so the lower the better."
      }
    ]
  },
  {
    "number": 4212,
    "title": "[Feature] Add on_after_backward in Callback. Enable ModelGradTrackerCallback",
    "created_at": "2020-10-17T18:16:48Z",
    "closed_at": "2020-10-17T19:13:11Z",
    "labels": [
      "duplicate",
      "feature",
      "help wanted",
      "good first issue",
      "callback"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4212",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nThe call_hook `on_after_backward ` is already implemented, but not added in Callback class.\r\nIt boils down to add it within Callback Class\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nAdding this new hook within callback could be used to implement something like`ModelGradTrackerCallback`.\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4212/comments",
    "author": "tchaton",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-10-17T18:38:11Z",
        "body": "#3657 already requested :)"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-17T19:13:44Z",
        "body": "pls, update the existing one if needed :]"
      }
    ]
  },
  {
    "number": 4194,
    "title": "How to use Truncated Backpropagation Through Time ",
    "created_at": "2020-10-16T14:24:40Z",
    "closed_at": "2021-01-30T09:50:56Z",
    "labels": [
      "help wanted",
      "good first issue",
      "won't fix",
      "example"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4194",
    "body": "Hi all,\r\n\r\nI am trying to use Truncated Backpropagation Through Time to train a multivariate time series dataset. \r\n\r\nBut I found the documentation insufficient. \r\n\r\nCould anyone provide an example?\r\n\r\nThanks in advance  ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4194/comments",
    "author": "arthurpiazzi",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-10-16T14:25:24Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-16T18:27:31Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-11-23T23:26:27Z",
        "body": "If anyone in the community is interested in providing an example here, feel free to send a PR. Or even a tutorial in form of a Juypter Notebook / Google Colab would be awesome!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-24T06:34:42Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-01-23T08:43:11Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4193,
    "title": "max_steps has no effect in combination with gradient accumulation",
    "created_at": "2020-10-16T13:08:16Z",
    "closed_at": "2020-10-22T12:59:00Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4193",
    "body": "## 🐛 Bug\r\n\r\n```python \r\n\r\nimport os\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\nfrom pytorch_lightning import Trainer, LightningModule\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        \"\"\"\r\n        Testing PL Module\r\n\r\n        Use as follows:\r\n        - subclass\r\n        - modify the behavior for what you want\r\n\r\n        class TestModel(BaseTestModel):\r\n            def training_step(...):\r\n                # do your own thing\r\n\r\n        or:\r\n\r\n        model = BaseTestModel()\r\n        model.training_epoch_end = None\r\n\r\n        \"\"\"\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self.layer(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x['x'] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ndef run_test():\r\n    class TestModel(BoringModel):\r\n\r\n        def on_train_epoch_start(self) -> None:\r\n            print('override any method to prove your bug')\r\n\r\n    # fake data\r\n    train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n    val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n    test_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\r\n\r\n    # model\r\n    model = TestModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        # -----------------------------------------------------------------------------\r\n        max_steps=10,  # HERE HAS NO EFFECT IN COMBINATION WITH accumulate_grad_batches\r\n        # -----------------------------------------------------------------------------\r\n        accumulate_grad_batches=2,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model, train_data, val_data)\r\n    trainer.test(test_dataloaders=test_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run_test()\r\n\r\n\r\n```\r\n\r\nThis training runs for all 1000 epochs instead of just max steps.\r\n\r\n### Expected behavior\r\n\r\nTraining runs for max_steps. \r\n\r\n### Additional context\r\n\r\nThe tests with TODO marker can be found here  #4190\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4193/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2020-10-20T20:49:41Z",
        "body": "@SeanNaren please take a look!"
      }
    ]
  },
  {
    "number": 4179,
    "title": "Hparams are not automatically saved to WandB logger in 1.0.2",
    "created_at": "2020-10-15T17:25:38Z",
    "closed_at": "2020-10-15T19:18:42Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4179",
    "body": "## 🐛 Bug\r\n\r\nWhen I update to 1.0.2, when I assign `self.hparams = args` in Lightning module, the hparams are not logged in WandB anymore. This bug is not present in 1.0.0 however. Snippets of my code.\r\n\r\n```python\r\n...\r\nparser.add_argument(\"--description\", type=str, default=\"Trainig\")\r\n...\r\nargs = parser.parse_args()\r\nmain(args)\r\n\r\n# Inside main\r\nlogger = WandbLogger(name=args.description, project=\"myProject\")\r\nmodel = MyModule(args)\r\n\r\n# Inside LightningModule\r\nclass MyModule(pl.LightningModule):\r\n    def __init__(self, args):\r\n        super().__init__()\r\n        self.hparams = args\r\n        ...\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4179/comments",
    "author": "huyvnphan",
    "comments": [
      {
        "user": "nateraw",
        "created_at": "2020-10-15T18:59:43Z",
        "body": "If you aren't doing anything fancy w/ your args, its usually easier to explicitly type them. That's my quick answer, but it doesn't solve your problem. I've never used Wandb, but I'm giving it a go right now 😄 . will let ya know"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-15T19:16:26Z",
        "body": "well we are saving the initial arguments, so you should be calling `self.save_hyperparameters()`\r\nthis is not just the case of WandB, it is for all loggers since #4163 \r\n\r\nEDIT: the reccomended flow is:\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n    def __init__(self, arg1, arg2):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n        ...\r\n```"
      },
      {
        "user": "nateraw",
        "created_at": "2020-10-15T19:18:03Z",
        "body": "Yeah your issue is solved if you just switch to using `**kwargs`. Also, make sure you're calling `self.save_hyperparameters()`.\r\n\r\n```python\r\nclass YourModel(pl.LightningModule):\r\n    def __init__(self, **kwargs):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n# ...\r\n```\r\n\r\nThis made it work fine for me. seeing the correct hparams on WandB"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-15T19:18:42Z",
        "body": "Feel free to reopen if needed :]"
      },
      {
        "user": "nateraw",
        "created_at": "2020-10-15T19:21:12Z",
        "body": "> well we are saving the initial arguments, so you should be calling `self.save_hyperparameters()`\r\n> this is not just the case of WandB, it is for all loggers since #4163\r\n> \r\n> EDIT: the reccomended flow is:\r\n> \r\n> ```python\r\n> class MyModule(pl.LightningModule):\r\n>     def __init__(self, arg1, arg2):\r\n>         super().__init__()\r\n>         self.save_hyperparameters()\r\n>         ...\r\n> ```\r\n\r\nI also recommend writing the args explicitly. code just reads better that way (imo)."
      }
    ]
  },
  {
    "number": 4171,
    "title": "DDP error after upgrading to v1.0.1",
    "created_at": "2020-10-15T12:43:25Z",
    "closed_at": "2020-10-24T21:33:48Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0",
      "distributed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4171",
    "body": "## 🐛 Bug\r\n\r\nI'll start by saying that before I've upgrade to v1.0.1, I've used v0.9.0 with no apparent DDP issues.\r\n\r\nAfter I've upgraded to v1.0.1, I'm having issues training with multiple GPUs using DDP backend.\r\n\r\n**Initiating multi-gpu training in the following scenarios will result with an error:**\r\n\r\n1. The first GPU (i.e. ID 0) is not included in the GPUs list. for example:\r\n`python main.py --distributed_backend 'ddp' --gpus 1,2,3`\r\n\r\n2. The GPUs list is not sequential. For example:\r\n`python main.py --distributed_backend 'ddp' --gpus 0,2,3`\r\n\r\nThe above will result with the following error message:\r\n`RuntimeError: cuda runtime error (101) : invalid device ordinal at /opt/conda/conda-bld/pytorch_1595629427478/work/torch/csrc/cuda/Module.cpp:59`\r\n\r\n**Initiating multi-gpu training in the following scenarios will work as expected:**\r\n\r\n`python main.py --distributed_backend 'ddp' --gpus 2`\r\nor\r\n`python main.py --distributed_backend 'ddp' --gpus 0,1,2`\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce GTX 1080 Ti\r\n                - GeForce GTX 1080 Ti\r\n                - GeForce GTX 1080 Ti\r\n                - GeForce GTX 1080 Ti\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.6.0\r\n        - pytorch-lightning: 1.0.1\r\n        - tqdm:              4.50.2\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                -\r\n        - processor:         x86_64\r\n        - python:            3.7.8\r\n        - version:           #119-Ubuntu SMP Tue Sep 8 12:30:01 UTC 2020\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4171/comments",
    "author": "sheffier",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-10-15T14:48:24Z",
        "body": "ok got it. yes, was able to reproduce on my end. it's really hard to test these cases with only 2 GPUs haha. anyhow, this is a bit more involved so it might take a few days. In the meantime set CUDA_VISIBLE_DEVICES to the gpus you need and pass in the number of GPUs.\r\n\r\nsorry for the inconvenience!\r\n\r\nonce it's fixed, we'll ping you here"
      },
      {
        "user": "sheffier",
        "created_at": "2020-10-15T17:45:08Z",
        "body": "Great, thank!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-21T12:07:39Z",
        "body": "duplicate of #3791 \r\nI'm working on this but no big breakthrough yet. I'm facing some difficulties because there are several global/env variables that determine the GPU selection. For ddp this is quite difficult to debug."
      }
    ]
  },
  {
    "number": 4150,
    "title": "Accuracy metric throwing out-of-range value (>1)",
    "created_at": "2020-10-14T16:23:07Z",
    "closed_at": "2020-10-14T16:40:19Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4150",
    "body": "## 🐛 Bug\r\n\r\nUsing class-based `metrics.Accuracy(num_classes)` with a fixed number of classes calculated in train and eval step outputs accuracy bigger than one.\r\n\r\n`Epoch 11:   2%|▏         | 66/2775 [02:00<1:22:17,  1.82s/it, loss=2.367, v_num=1, train_loss=2.36, train_acc=1.67, val_loss=2.41, val_acc=1.27]`\r\n\r\n### To Reproduce\r\n\r\nThe issue appears inconsistently.\r\n\r\n### Expected behavior\r\n\r\nAccuracy should be a value between 0 and 1.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - OS (e.g., Linux): Ubuntu\r\n - How you installed PyTorch (`conda`, `pip`, source): Conda\r\n - Python version: 3.8.5\r\n- Pytorch-lightning version: 0.9.0\r\n - Any other relevant information: I'm using multi-gpu with ddp\r\n\r\n### Additional context\r\n\r\nI'm getting these warnings from the accuracy metric:\r\n\r\n```\r\n/miniconda3/envs/py385/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: You have set 32 number of classes if different from predicted (4) and target (29) number of classes\r\n  warnings.warn(*args, **kwargs)\r\n```\r\n\r\nActually I just wanted to report it even if I don't have a reproduce because I didn't see any other reports of this. \r\nMaybe accuracy should have a warning in these cases, but I have no clue where this out-of-bounds value is coming from in the first place.\r\n\r\nMeanwhile, I'll switch to an ad-hoc accuracy calculation to check if the problem persists. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4150/comments",
    "author": "Vichoko",
    "comments": [
      {
        "user": "teddykoker",
        "created_at": "2020-10-14T16:40:19Z",
        "body": "PyTorch Lightning 1.0 has a new accuracy metric that is correctly implemented and rigorously tested, I would recommend updating to that."
      },
      {
        "user": "Vichoko",
        "created_at": "2020-10-14T18:07:08Z",
        "body": "Okay, I'll update and keep informed if the issue persists."
      }
    ]
  },
  {
    "number": 4099,
    "title": "Slurm resubmit at the end of epoch.",
    "created_at": "2020-10-12T19:30:22Z",
    "closed_at": "2020-11-18T20:30:04Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4099",
    "body": "From my understanding, the current resubmit will stop the model at the middle of epoch, which may have problem with dataloader resuming.\r\n\r\nIs it possible that lightning automatically estimates that if a new epoch can be finished within the time limit, and decide if to halt or continue at the end of each epoch.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4099/comments",
    "author": "ruotianluo",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-11-11T19:48:56Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 4079,
    "title": "ModelCheckpoint save_function() not set?",
    "created_at": "2020-10-11T15:29:35Z",
    "closed_at": "2020-10-11T15:37:26Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4079",
    "body": "I am training a PL model using the following code snippet:\r\n\r\n```python\r\n    # logger\r\n    tb_logger = pl_loggers.TensorBoardLogger(cfg.logs.path, name='rnn_exp')\r\n\r\n    # checkpoint callback\r\n    checkpoint_callback = ModelCheckpoint(\r\n        filepath=cfg.checkpoint.path + \"encoder_rnn{epoch:02d}\",\r\n        save_top_k=1,\r\n        mode=\"min\" # monitor is defined in val_step: EvalResult(checkpoint_on=val_loss)\r\n    )\r\n\r\n    # early stopping callback\r\n    early_stopping_callback = EarlyStopping(\r\n        monitor=\"val_loss\",\r\n        patience=cfg.val.patience,\r\n        mode=\"min\"\r\n    )\r\n\r\n    tokenizer = ...\r\n    dm = MyDataModule(cfg, tokenizer)\r\n\r\n    model = RNNEncoder(cfg)\r\n\r\n    trainer = Trainer(\r\n        fast_dev_run=False,\r\n        max_epochs=cfg.train.max_epochs,\r\n        gpus=1,\r\n        logger=tb_logger,\r\n        callbacks=[checkpoint_callback, early_stopping_callback]\r\n    )\r\n\r\n    # training\r\n    dm.setup('fit')\r\n    trainer.fit(model, datamodule=dm)\r\n```\r\n\r\nHowever, after the first epoch, the model presents the following error, probably when calling the model checkpoint callback:\r\n\r\n```python\r\n    trainer.fit(model, datamodule=dm)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1073, in fit\r\n    results = self.accelerator_backend.train(model)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_backend.py\", line 51, in train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1239, in run_pretrain_routine\r\n    self.train()\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 394, in train\r\n    self.run_training_epoch()\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 516, in run_training_epoch\r\n    self.run_evaluation(test_mode=False)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 603, in run_evaluation\r\n    self.on_validation_end()\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_hook.py\", line 176, in on_validation_end\r\n    callback.on_validation_end(self, self.get_model())\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py\", line 27, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 380, in on_validation_end\r\n    self._do_check_save(filepath, current, epoch, trainer, pl_module)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 421, in _do_check_save\r\n    self._save_model(filepath, trainer, pl_module)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 212, in _save_model\r\n    raise ValueError(\".save_function() not set\")\r\nValueError: .save_function() not set\r\n\r\n```\r\nCould you tell me if I forgot to configure something?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4079/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-10-11T15:33:26Z",
        "body": "currently you need to set the ModelCheckpoint via `Trainer(checkpoint_callback=...)`\r\n#3990 will enable passing it to callbacks"
      },
      {
        "user": "celsofranssa",
        "created_at": "2020-10-11T15:37:26Z",
        "body": "Thanks, @awaelchli, \r\n\r\nI've just thought that. Thanks a lot for the help."
      }
    ]
  },
  {
    "number": 4001,
    "title": "on_train_epoch_end and on_epoch_end are out of order",
    "created_at": "2020-10-08T19:23:57Z",
    "closed_at": "2020-10-20T19:07:27Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4001",
    "body": "<!-- \r\n### Common bugs:\r\nThe two hooks `on_train_epoch_end` and `on_epoch_end` are called out of order.\r\n-->\r\n\r\n## 🐛 Bug\r\n\r\nConsider the following order in which the `LightningModule` hooks are called from #2816 (I have confirmed that in PytorchLightning version 0.10 this is still an issue):\r\n```\r\non_epoch_start\r\non_train_epoch_start\r\non_validation_start\r\non_validation_epoch_start\r\non_validation_epoch_end\r\non_validation_end\r\non_epoch_end\r\non_train_epoch_end\r\n```\r\nNaturally one would expect the opening and closing scope hooks to match. However, `on_train_epoch_end` is called after `on_epoch_end`, which seems incorrect. It is natural to open the epoch scope before the train epoch scope (as is being done currently), in which case the epoch scope should be closed _after_ closing the train epoch scope (which is not currently being done)\r\n\r\n - PyTorch Version (e.g., 1.0): **1.6.0**\r\n - OS (e.g., Linux): **Ubuntu 18.04**\r\n - How you installed PyTorch (`conda`, `pip`, source): `pip`\r\n - Build command you used (if compiling from source):\r\n - Python version: **3.8.5**\r\n - CUDA/cuDNN version: **NA**\r\n - GPU models and configuration: **NA**\r\n - Any other relevant information: **NA**\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4001/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-10-08T19:24:37Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-08T20:29:59Z",
        "body": "@wyessen it seems that the flow is incorrect the `on_train_epoch_end` shall be before `on_validation_start`\r\ncc: @williamFalcon "
      },
      {
        "user": "ghost",
        "created_at": "2020-10-08T21:38:37Z",
        "body": "@Borda \r\n\r\nTrue, but depends how we should look at it: should validation be considered part of the training epoch scope? If so, then the current flow is fine; otherwise, you're right, it's incorrect. \r\n\r\nSo, the original bug report complains about incorrect closing of the scope given the order in which the scopes were opened. You raise a valid issue, and in the broader scheme of things the current flow should be reconsidered."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-10-08T21:57:15Z",
        "body": "yes. validation is part of the flow. as mentioned many times, big research requires checking val multiple times within an epoch\r\n\r\nTrain: --------------------------- (1 epoch = 2 days)\r\nVal                --             --             --\r\n\r\nThen we have:\r\n\r\ne1 = on_epoch_start\r\ne2 = on_epoch_end\r\nt1 = on_train_epoch_start\r\nt2 = on_train_epoch_end\r\nv1 = on_val_epoch_start\r\nv2 = on_val_epoch_end\r\n\r\nTrain:          e1 t1 ------------------------------------------------------------- t2 e2\r\nVal   :                    ________(e1 v1 --val--- v2 e2) ___ (e1 v1 --val---v2 e2)"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-08T22:06:56Z",
        "body": "here is an added test to check the actual flow #4010\r\nI think the confusion comes from using smaller models or just one validation per epoch, then you would expect to have called the validation after training..."
      },
      {
        "user": "ghost",
        "created_at": "2020-10-08T22:55:43Z",
        "body": "@williamFalcon @Borda So as to not hijack the original bug report, I want to clarify:\r\n\r\nThe flow executed by PytorchLightning is _incorrect_ in the sense that opening of a scope (with `_start`) does not match closing of the scope with (with `_end`). In particular `on_epoch_end` is called _before_ `on_train_epoch_end`, which is not correct. "
      },
      {
        "user": "ghost",
        "created_at": "2020-10-20T17:53:27Z",
        "body": "@SeanNaren Why did you close this issue? Your PR does not fix this. "
      },
      {
        "user": "SeanNaren",
        "created_at": "2020-10-20T18:58:03Z",
        "body": "Apologies, I think the PR associated with this issue was incorrect!\r\n\r\nEDIT: after looking at the associated PR and the discussion here, I do think this PR addresses the issue of ensuring the order is correct. Was there anything in particular that wasn't addressed @wyessen?"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-20T19:06:41Z",
        "body": "@wyessen this was closed with @williamFalcon explanation that the behavior is as expected 🐰 "
      },
      {
        "user": "SeanNaren",
        "created_at": "2020-10-20T19:07:27Z",
        "body": "Will close again for now..."
      },
      {
        "user": "ghost",
        "created_at": "2020-10-20T19:10:46Z",
        "body": "@Borda the behavior is not expected, please read my explanation (@williamFalcon was responding to @Borda’s message, which was different from my original issue). "
      }
    ]
  },
  {
    "number": 3953,
    "title": "non intuitive batch_size in ddp",
    "created_at": "2020-10-07T18:47:09Z",
    "closed_at": "2020-10-07T21:38:23Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3953",
    "body": "Is there a way in PyTorchLightning to set your desired batch size, say 512 and then have the effective batch size per processor (which is normally batch_size*num_gpus) be computed automatically?  Right now your effective batch size scales with the number of gpus so these calculations must be computed outside of pytorchlightning (as far as my tests have shown)...  This seems like something that PL could/should be able to handle.  You'd likely also have to set a maximum processor batch_size so that it could determine an accumulate_grad_batches so as not to use too much memory.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3953/comments",
    "author": "jloveric",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-10-07T19:35:59Z",
        "body": "great suggestion!\r\n\r\nas a researcher this kind of magic scares me... i'd prefer to be explicit. but would love to hear from the community\r\n@ananthsub  @tullie "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-07T19:52:38Z",
        "body": "I find the current way more intuitive and super helpful actually.\r\n\r\nI can easly choose the right batch size by training initially on only one gpu to figure out which size fills the memory nicely (or use the automatic batch size finder). \r\nThen, if I want to train on X gpus, I can just set gpus=X in the trainer and scale the learning rate roughly by X. "
      }
    ]
  },
  {
    "number": 3936,
    "title": "Deepcopy fails on self.callback_metrics",
    "created_at": "2020-10-07T13:02:13Z",
    "closed_at": "2020-10-07T17:17:28Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3936",
    "body": "## 🐛 Bug\r\n\r\nI have target in output dict from validation_step (I write output from my model to postprocess and aggregate later in validation_epoch_end)\r\n\r\nCannot copy callback metrics (which are outputs from train/val step function)\r\nError is caused by 529 line at `pytorch_lightning/trainer/training_loop.py`: \r\n\r\n`monitor_metrics = deepcopy(self.callback_metrics)`\r\n\r\nI tried to change this line to `monitor_metrics = self.callback_metrics` but training hangs\r\n\r\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 71, in <module>\r\n    main()\r\n  File \"train.py\", line 60, in main\r\n    trainer.fit(model)\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1058, in fit\r\n    results = self.accelerator_backend.spawn_ddp_children(model)\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 123, in spawn_ddp_children\r\n    results = self.ddp_train(local_rank, mp_queue=None, model=model, is_master=True)\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 224, in ddp_train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1239, in run_pretrain_routine\r\n    self.train()\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 394, in train\r\n    self.run_training_epoch()\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 529, in run_training_epoch\r\n    monitor_metrics = deepcopy(self.callback_metrics)\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/copy.py\", line 230, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/copy.py\", line 210, in _deepcopy_tuple\r\n    y = [deepcopy(a, memo) for a in x]\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/copy.py\", line 210, in <listcomp>\r\n    y = [deepcopy(a, memo) for a in x]\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/copy.py\", line 153, in deepcopy\r\n    y = copier(memo)\r\n  File \"/home/george/miniconda3/envs/docr/lib/python3.8/site-packages/torch/tensor.py\", line 38, in __deepcopy__\r\n    raise RuntimeError(\"Only Tensors created explicitly by the user \"\r\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment\r\n```\r\n\r\n\r\n\r\nThe problem caused because my model output is not returning leaf tensors and deepcopy cannot copy that.\r\n\r\n### Expected behavior\r\n\r\nTraining is not crashing\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.6.0\r\n        - pytorch-lightning: 0.10.0rc1\r\n        - tqdm:              4.48.2\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.5\r\n        - version:           #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3936/comments",
    "author": "thepowerfuldeez",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-10-07T16:01:28Z",
        "body": "@thepowerfuldeez  can you add a way to reproduce this using BoringModel? Thanks!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-10-07T17:17:28Z",
        "body": "sounds like you have to call .detach() on what you are passing in to the dict. If you submit a PR with the failing test using BoringModel we can add support for automatically doing this!"
      }
    ]
  },
  {
    "number": 3922,
    "title": "UserWarning for testing_epoch_end in 0.9.1rc4",
    "created_at": "2020-10-06T23:47:36Z",
    "closed_at": "2020-10-07T10:34:39Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3922",
    "body": "Just informing about the user warning I was displayed:\r\n\r\n```YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: The testing_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule warnings.warn(*args, **kwargs)```\r\n\r\nalthough I don't have the ```testing_epoch_end``` method in my class. **UPDATE:** The warning does not appear when I implement it. \r\n\r\nI'm using 0.9.1rc4.\r\n\r\nIf it's being resolved elsewhere and I missed that, feel free to close the issue.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3922/comments",
    "author": "chrismaliszewski",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-10-07T09:40:19Z",
        "body": "@chrismaliszewski it seems as there is a missing check if the method is overwritten, mind sends a PR?"
      },
      {
        "user": "chrismaliszewski",
        "created_at": "2020-10-07T10:34:39Z",
        "body": "It's fine now. Closing."
      }
    ]
  },
  {
    "number": 3905,
    "title": "copy badges to release package",
    "created_at": "2020-10-06T15:38:00Z",
    "closed_at": "2020-12-01T21:58:33Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3905",
    "body": "## 🚀 Feature\r\n\r\nParse the Readme and replace generated badges by downloaded ones\r\n\r\n0. process in setup.py\r\n1. parse all badges from online CI and save them as png (svg is problematic, does not work for all plaforms)\r\n2 replace badges in Readme with the downloaded ones\r\n\r\n### Motivation\r\n\r\npipy page does not work well with generated badges and also projecting the master state to a given release does not make sense.. also no need to keep link to CI :]",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3905/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "Abelarm",
        "created_at": "2020-10-12T10:10:03Z",
        "body": "HI,\r\n\r\nI would like to take care of this, if it's ok with you.\r\n\r\nThanks\r\n"
      },
      {
        "user": "Abelarm",
        "created_at": "2020-10-19T10:12:52Z",
        "body": "HI @Borda,\r\n\r\nI am working on it, I was able to download the png file for some of the badge and save it, but sadly not all the badges have the .png version buy only the svg. How do you think I should proceed?\r\nI just save the svg files and load it?\r\n\r\nOther questions: \r\n- I was planning to save the badges in `docs/source/_images/badges` is it fine for you?\r\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-19T12:30:22Z",
        "body": "> HI @Borda,\r\n> \r\n> I am working on it, I was able to download the png file for some of the badge and save it, but sadly not all the badges have the .png version buy only the svg. How do you think I should proceed?\r\n> I just save the svg files and load it?\r\n\r\nI think that my version now with locally saved badges is fine now, not sure, maybe SVG will work too and the issue was just the online version... later we can try something else :]\r\n\r\n> * I was planning to save the badges in `docs/source/_images/badges` is it fine for you?\r\n\r\nyes, sounds good to me :] just add also this location to MANIFEST.in"
      }
    ]
  },
  {
    "number": 3879,
    "title": "Support multi ModelCheckpoint checkpoint_on metrics  ",
    "created_at": "2020-10-05T17:12:09Z",
    "closed_at": "2020-11-11T23:52:34Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3879",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nCurrently, callback ModelCheckpoint allows to save the best model based on one provided metric to monitor.\r\nThe new feature proposal would be to extend this to support multiple monitored metrics to checkpoint on.\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nWhen a torch-lightning weights is used for paper benchmarking, it would be possible to make an inference with the best model for a given metric.\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nApproach 1:\r\n\r\n```\r\ncheckpoint_callback_1 = ModelCheckpoint(monitor='val_loss',\r\n        ...     filepath='my/path/sample-mnist-{epoch:02d}-{val_loss:.2f}'\r\n        ... )\r\n\r\ncheckpoint_callback_2 = ModelCheckpoint(monitor='val_miou,\r\n        ...     filepath='my/path/sample-mnist-{epoch:02d}-{val_miou:.2f}'\r\n        ... )\r\n\r\ncheckpoint_callback_n = ModelCheckpoint(monitor='val_acc,\r\n        ...     filepath='my/path/sample-mnist-{epoch:02d}-{val_acc:.2f}'\r\n        ... )\r\n\r\n trainer = Trainer(callbacks=[checkpoint_callback_1, ... checkpoint_callback_n])\r\n```\r\n\r\nor \r\n\r\nApproach 2 (prefered):\r\n\r\nExtend monitor, save_top_k, mode, period to support list inputs like\r\n\r\n```\r\ncheckpoint_multi_monitor_callback = ModelCheckpoint(monitor=['val_loss', val_miou],\r\n                                             save_top_k =[1, 2], # or could be broadcasted if int provided\r\n                                              mode=[\"min\", \"max\"] # or could be broadcasted if str provided\r\n                                              period=[1, 2],  # or could be broadcasted if int provided\r\n        ...     filepath='my/path/sample-mnist-{epoch:02d}-{monitor:.2f}'\r\n        ... )\r\n\r\n trainer = Trainer(callbacks=[checkpoint_multi_monitor_callback])\r\n```\r\n\r\nAlso,\r\n\r\n```\r\nModelIO.load_from_checkpoint\r\n```\r\n\r\ncould be updated to support loading the weights for a specified monitored metrics.\r\n\r\n```\r\n    @classmethod\r\n    def load_from_checkpoint(\r\n        cls,\r\n        checkpoint_path: str,\r\n        *args,\r\n        map_location: Optional[Union[Dict[str, str], str, torch.device, int, Callable]] = None,\r\n        hparams_file: Optional[str] = None,\r\n        strict: bool = True,\r\n        monitored_metric: str = None, # If provided, it will try to load from the best weight from this metric.\r\n        **kwargs,\r\n    ):\r\n```\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3879/comments",
    "author": "tchaton",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2020-10-05T17:19:22Z",
        "body": "Hey @williamFalcon, any thoughts on it ? I could work on it this week 👍 "
      },
      {
        "user": "ananthsub",
        "created_at": "2020-10-05T17:25:17Z",
        "body": "This should come after the 1.0 release. For option 1, the trainer needs to ensure that the checkpoint callbacks are the last callbacks to run. For option 2, properties like the best model path/score need to be extended. "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-10-05T17:44:11Z",
        "body": "IMO Approach 1 seems easy to work with, plus Approach 2 will force to have the same custom directory right?"
      },
      {
        "user": "tchaton",
        "created_at": "2020-10-05T18:02:16Z",
        "body": "Hey @rohitgr7, you are totally right ! Approach 1 would enable to have minimum API changes and work with custom directory for monitored metric."
      },
      {
        "user": "carmocca",
        "created_at": "2020-10-05T19:43:28Z",
        "body": "There is some previous discussion in #3621.\n\nI created that PR but didnt continue it yet. Feel free to continue yourself."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-04T23:45:20Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3866,
    "title": "Unusual printing statements after 90% epoch completition",
    "created_at": "2020-10-05T11:58:38Z",
    "closed_at": "2020-10-05T13:47:48Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3866",
    "body": "i've encountered this unusual print statements while training\r\nIt seems that this printing starts when epoch is 90% complete and both loss and train_loss is same until 100% completition\r\nThis behaviour is same on TPU's as well as on GPU's\r\n\r\n```bash\r\n2020-10-05 11:32:55.426605: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nCUDA_VISIBLE_DEVICES: [0]\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\r\n  warnings.warn(*args, **kwargs)\r\n\r\n  | Name      | Type      | Params\r\n----------------------------------------\r\n0 | model     | Predictor | 44 M  \r\n1 | criterion | MSELoss   | 0     \r\nEpoch 0:  90% 1531/1702 [12:41<01:25,  2.01it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nValidating: 0it [00:00, ?it/s]\r\nEpoch 0:  90% 1532/1702 [12:41<01:24,  2.01it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  90% 1533/1702 [12:41<01:23,  2.01it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  90% 1534/1702 [12:41<01:23,  2.01it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  90% 1535/1702 [12:41<01:22,  2.01it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  90% 1536/1702 [12:42<01:22,  2.02it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  90% 1537/1702 [12:42<01:21,  2.02it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  90% 1538/1702 [12:42<01:21,  2.02it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  90% 1539/1702 [12:42<01:20,  2.02it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  90% 1540/1702 [12:42<01:20,  2.02it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1541/1702 [12:42<01:19,  2.02it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1542/1702 [12:43<01:19,  2.02it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1543/1702 [12:43<01:18,  2.02it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1544/1702 [12:43<01:18,  2.02it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1545/1702 [12:43<01:17,  2.02it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1546/1702 [12:43<01:17,  2.02it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1547/1702 [12:43<01:16,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1548/1702 [12:44<01:16,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1549/1702 [12:44<01:15,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1550/1702 [12:44<01:14,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1551/1702 [12:44<01:14,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1552/1702 [12:44<01:13,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1553/1702 [12:44<01:13,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1554/1702 [12:45<01:12,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1555/1702 [12:45<01:12,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1556/1702 [12:45<01:11,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  91% 1557/1702 [12:45<01:11,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1558/1702 [12:45<01:10,  2.03it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1559/1702 [12:45<01:10,  2.04it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1560/1702 [12:46<01:09,  2.04it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1561/1702 [12:46<01:09,  2.04it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1562/1702 [12:46<01:08,  2.04it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1563/1702 [12:46<01:08,  2.04it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1564/1702 [12:46<01:07,  2.04it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1565/1702 [12:46<01:07,  2.04it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1566/1702 [12:47<01:06,  2.04it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1567/1702 [12:47<01:06,  2.04it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1568/1702 [12:47<01:05,  2.04it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1569/1702 [12:47<01:05,  2.04it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1570/1702 [12:47<01:04,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1571/1702 [12:47<01:04,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1572/1702 [12:48<01:03,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1573/1702 [12:48<01:02,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  92% 1574/1702 [12:48<01:02,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1575/1702 [12:48<01:01,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1576/1702 [12:48<01:01,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1577/1702 [12:48<01:00,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1578/1702 [12:49<01:00,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1579/1702 [12:49<00:59,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1580/1702 [12:49<00:59,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1581/1702 [12:49<00:58,  2.05it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1582/1702 [12:49<00:58,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1583/1702 [12:49<00:57,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1584/1702 [12:49<00:57,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1585/1702 [12:50<00:56,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1586/1702 [12:50<00:56,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1587/1702 [12:50<00:55,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1588/1702 [12:50<00:55,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1589/1702 [12:50<00:54,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1590/1702 [12:50<00:54,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  93% 1591/1702 [12:51<00:53,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1592/1702 [12:51<00:53,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1593/1702 [12:51<00:52,  2.06it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1594/1702 [12:51<00:52,  2.07it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1595/1702 [12:51<00:51,  2.07it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1596/1702 [12:51<00:51,  2.07it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1597/1702 [12:52<00:50,  2.07it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1598/1702 [12:52<00:50,  2.07it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1599/1702 [12:52<00:49,  2.07it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1600/1702 [12:52<00:49,  2.07it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1601/1702 [12:52<00:48,  2.07it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1602/1702 [12:52<00:48,  2.07it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1603/1702 [12:53<00:47,  2.07it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1604/1702 [12:53<00:47,  2.07it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1605/1702 [12:53<00:46,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1606/1702 [12:53<00:46,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1607/1702 [12:53<00:45,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  94% 1608/1702 [12:53<00:45,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1609/1702 [12:54<00:44,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1610/1702 [12:54<00:44,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1611/1702 [12:54<00:43,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1612/1702 [12:54<00:43,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1613/1702 [12:54<00:42,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1614/1702 [12:54<00:42,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1615/1702 [12:55<00:41,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1616/1702 [12:55<00:41,  2.08it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1617/1702 [12:55<00:40,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1618/1702 [12:55<00:40,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1619/1702 [12:55<00:39,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1620/1702 [12:55<00:39,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1621/1702 [12:56<00:38,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1622/1702 [12:56<00:38,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1623/1702 [12:56<00:37,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1624/1702 [12:56<00:37,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  95% 1625/1702 [12:56<00:36,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1626/1702 [12:56<00:36,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1627/1702 [12:57<00:35,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1628/1702 [12:57<00:35,  2.09it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1629/1702 [12:57<00:34,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1630/1702 [12:57<00:34,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1631/1702 [12:57<00:33,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1632/1702 [12:57<00:33,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1633/1702 [12:58<00:32,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1634/1702 [12:58<00:32,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1635/1702 [12:58<00:31,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1636/1702 [12:58<00:31,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1637/1702 [12:58<00:30,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1638/1702 [12:58<00:30,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1639/1702 [12:59<00:29,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1640/1702 [12:59<00:29,  2.10it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1641/1702 [12:59<00:28,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  96% 1642/1702 [12:59<00:28,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1643/1702 [12:59<00:27,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1644/1702 [12:59<00:27,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1645/1702 [13:00<00:27,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1646/1702 [13:00<00:26,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1647/1702 [13:00<00:26,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1648/1702 [13:00<00:25,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1649/1702 [13:00<00:25,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1650/1702 [13:00<00:24,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1651/1702 [13:00<00:24,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1652/1702 [13:01<00:23,  2.11it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1653/1702 [13:01<00:23,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1654/1702 [13:01<00:22,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1655/1702 [13:01<00:22,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1656/1702 [13:01<00:21,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1657/1702 [13:01<00:21,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1658/1702 [13:02<00:20,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  97% 1659/1702 [13:02<00:20,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1660/1702 [13:02<00:19,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1661/1702 [13:02<00:19,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1662/1702 [13:02<00:18,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1663/1702 [13:02<00:18,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1664/1702 [13:03<00:17,  2.12it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1665/1702 [13:03<00:17,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1666/1702 [13:03<00:16,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1667/1702 [13:03<00:16,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1668/1702 [13:03<00:15,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1669/1702 [13:03<00:15,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1670/1702 [13:04<00:15,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1671/1702 [13:04<00:14,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1672/1702 [13:04<00:14,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1673/1702 [13:04<00:13,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1674/1702 [13:04<00:13,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1675/1702 [13:04<00:12,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  98% 1676/1702 [13:05<00:12,  2.13it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1677/1702 [13:05<00:11,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1678/1702 [13:05<00:11,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1679/1702 [13:05<00:10,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1680/1702 [13:05<00:10,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1681/1702 [13:05<00:09,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1682/1702 [13:06<00:09,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1683/1702 [13:06<00:08,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1684/1702 [13:06<00:08,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1685/1702 [13:06<00:07,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1686/1702 [13:06<00:07,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1687/1702 [13:06<00:06,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1688/1702 [13:07<00:06,  2.14it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1689/1702 [13:07<00:06,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1690/1702 [13:07<00:05,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1691/1702 [13:07<00:05,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1692/1702 [13:07<00:04,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0:  99% 1693/1702 [13:07<00:04,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0: 100% 1694/1702 [13:08<00:03,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0: 100% 1695/1702 [13:08<00:03,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0: 100% 1696/1702 [13:08<00:02,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0: 100% 1697/1702 [13:08<00:02,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0: 100% 1698/1702 [13:08<00:01,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0: 100% 1699/1702 [13:08<00:01,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0: 100% 1700/1702 [13:09<00:00,  2.15it/s, loss=0.158, v_num=0, train_loss=0.149]\r\nEpoch 0: 100% 1701/1702 [13:09<00:00,  2.16it/s, loss=0.158, v_num=0, train_loss=0.149]/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: The metric you returned None must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of loss in validation_epoch_end()?\r\n  warnings.warn(*args, **kwargs)\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: Can save best model only with loss available, skipping.\r\n  warnings.warn(*args, **kwargs)\r\nEpoch 0: 100% 1702/1702 [13:09<00:00,  2.16it/s, loss=0.158, v_num=0, train_loss=0.149, valid_loss=0.162]\r\nEpoch 1:  73% 1246/1702 [10:19<03:46,  2.01it/s, loss=0.161, v_num=0, train_loss=0.191, valid_loss=0.162]/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\r\n  warnings.warn(*args, **kwargs)\r\nSaving latest checkpoint..\r\nEpoch 1:  73% 1246/1702 [10:19<03:46,  2.01it/s, loss=0.161, v_num=0, train_loss=0.191, valid_loss=0.162]\r\n```\r\n\r\nThis issue can be reproduced in current stable and in 0.10.1rc1 also",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3866/comments",
    "author": "saahiluppal",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-10-05T12:10:18Z",
        "body": "Colab and pycharm has this kind of issue, I guess.\r\nThe main progress bar is the combination of train dataloder + val dataloader (if exist)\r\nIf you run the python file as normal python script in colab and your train dataloader takes 90% of the (train + val) dataloader, it will show up like this once it reaches validation step. B/C I used to run like that.\r\nThis is not a lightning issue, it's tqdm.\r\n\r\nThis is also same with pycharm but I am not so sure about that."
      }
    ]
  },
  {
    "number": 3832,
    "title": "Mismatch between on_validation_epoch_end and on_train_epoch_end",
    "created_at": "2020-10-03T23:53:18Z",
    "closed_at": "2020-10-04T11:12:32Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3832",
    "body": "## 🐛 Bug\r\n\r\nThe behaviour of the callback hooks `on_train_epoch_end` and `on_validation_epoch_end` do not match. While `on_train_epoch_end` can access metrics of the same epoch from the `validation_epoch_end` method the opposite is not true. More concretely, when accessing `trainer.callback_metrics` from `on_validation_epoch_end` the values logged in `training_epoch_end` correspond to the previous epoch rather than the current one.\r\n\r\n### To Reproduce\r\n\r\nOn master:\r\n\r\n```\r\nclass LitModel(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer_1 = torch.nn.Linear(28 * 28, 128)\r\n        self.layer_2 = torch.nn.Linear(128, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        x = self.layer_1(x)\r\n        x = F.relu(x)\r\n        x = self.layer_2(x)\r\n        return x\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('loss', loss)\r\n        return {'loss': loss}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('val_loss', loss)\r\n        return {\"val_loss\": loss}\r\n\r\n    def training_epoch_end(self, outputs):\r\n        loss_val = torch.stack([x['loss'] for x in outputs]).mean()\r\n        self.log('train_loss_epoch', loss_val)\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        loss_val = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        self.log('val_loss_epoch', loss_val)\r\n\r\nclass CustomCallback(Callback):\r\n\r\n\r\n    def on_epoch_end(self, trainer, pl_module):\r\n        metrics = trainer.callback_metrics\r\n        epoch = trainer.current_epoch\r\n        print(f\"Epoch {epoch}: {metrics}\")\r\n\r\n    def on_validation_epoch_end(self, trainer, pl_module):\r\n        print(f\"Val_epoch_end: {trainer.callback_metrics}\")\r\n\r\n    def on_train_epoch_end(self, trainer, pl_module):\r\n        print(f\"Train_epoch_end: {trainer.callback_metrics}\")\r\n        print(\"\\n\")\r\n\r\n\r\nmodel = LitModel()\r\n\r\ndataset = MNIST(os.getcwd(), download=True, train=False, transform=transforms.ToTensor())\r\ntrain, val = random_split(dataset, [9000, 1000])\r\n\r\ntrain_loader = DataLoader(train, batch_size=32)\r\nval_loader = DataLoader(val, batch_size=32)\r\n\r\n# train\r\ntrainer = pl.Trainer(gpus=1, progress_bar_refresh_rate=0, max_epochs=10, \r\n                     num_sanity_val_steps=0, callbacks=[CustomCallback()])\r\ntrainer.fit(model, train_loader, val_loader)\r\n```\r\n\r\n### Observed behaviour\r\n\r\n```\r\nVal_epoch_end: {'loss': tensor(0.3406, device='cuda:0'), 'val_loss': tensor(0.1225, device='cuda:0'), 'val_loss_epoch': tensor(0.2968, device='cuda:0')}\r\nEpoch 0: {'loss': tensor(0.3406, device='cuda:0'), 'val_loss': tensor(0.1225, device='cuda:0'), 'val_loss_epoch': tensor(0.2968, device='cuda:0'), 'train_loss_epoch': tensor(0.6143, device='cuda:0')}\r\nTrain_epoch_end: {'loss': tensor(0.3406, device='cuda:0'), 'val_loss': tensor(0.1225, device='cuda:0'), 'val_loss_epoch': tensor(0.2968, device='cuda:0'), 'train_loss_epoch': tensor(0.6143, device='cuda:0')}\r\n\r\n\r\nVal_epoch_end: {'loss': tensor(0.2575, device='cuda:0'), 'val_loss': tensor(0.0525, device='cuda:0'), 'val_loss_epoch': tensor(0.2291, device='cuda:0'), 'train_loss_epoch': tensor(0.6143, device='cuda:0')}\r\nEpoch 1: {'loss': tensor(0.2575, device='cuda:0'), 'val_loss': tensor(0.0525, device='cuda:0'), 'val_loss_epoch': tensor(0.2291, device='cuda:0'), 'train_loss_epoch': tensor(0.2746, device='cuda:0')}\r\nTrain_epoch_end: {'loss': tensor(0.2575, device='cuda:0'), 'val_loss': tensor(0.0525, device='cuda:0'), 'val_loss_epoch': tensor(0.2291, device='cuda:0'), 'train_loss_epoch': tensor(0.2746, device='cuda:0')}\r\n\r\n\r\nVal_epoch_end: {'loss': tensor(0.1669, device='cuda:0'), 'val_loss': tensor(0.0316, device='cuda:0'), 'val_loss_epoch': tensor(0.1905, device='cuda:0'), 'train_loss_epoch': tensor(0.2746, device='cuda:0')}\r\nEpoch 2: {'loss': tensor(0.1669, device='cuda:0'), 'val_loss': tensor(0.0316, device='cuda:0'), 'val_loss_epoch': tensor(0.1905, device='cuda:0'), 'train_loss_epoch': tensor(0.2091, device='cuda:0')}\r\nTrain_epoch_end: {'loss': tensor(0.1669, device='cuda:0'), 'val_loss': tensor(0.0316, device='cuda:0'), 'val_loss_epoch': tensor(0.1905, device='cuda:0'), 'train_loss_epoch': tensor(0.2091, device='cuda:0')}\r\n```\r\n\r\n### Expected behavior\r\n\r\nThough I imagine there may not be many situations where this may have an impact (as accessing training metrics using the `on_validation_epoch_end` hook would be unusual) I find the behaviour somewhat confusing. I suspect this may be due to the order in which the hooks are executed. Is this intended behaviour, or should there be consistency in the behaviour of both hooks? \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3832/comments",
    "author": "pbmstrk",
    "comments": [
      {
        "user": "pbmstrk",
        "created_at": "2020-10-04T00:22:52Z",
        "body": "From #2816 and determining when `training_epoch_end` and `validation_epoch_end` are called explains the behaviour - the hook `on_validation_epoch_end` is called before `training_epoch_end` which is why the values from the training epoch are not yet available and the values correspond to the previous epoch. Not sure if the behaviour warrants a fix - as not necessarily a bug, though somewhat counterintuitive when first looking at it?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-10-04T04:29:37Z",
        "body": "Not every epoch takes 2 minutes :) \r\n\r\nOn many research lines one needs to check validation multiple times within an epoch.\r\n\r\nFor example:\r\nTrain --------------------------------------\r\nValid -C--------------C-----------------C-\r\n\r\nThis comes up during things like BERT + NLP, imagenet and huge datasets where an epoch might take days.\r\n\r\nThis behavior is expected.\r\n\r\n\r\nSecond, callback metrics are not meant to be accessed (but can be). Instead, logged_metrics and prog_bar_metrics are there in case people want the metrics.\r\n"
      },
      {
        "user": "eladar",
        "created_at": "2020-12-24T10:49:47Z",
        "body": "Just making sure - \r\nFor a specific epoch, there is no hook that contains metrics arriving from both 'validation_epoch_end' and 'training_epoch_end' ?\r\n(meaning there is no hooks that is being called after 'training_epoch_end')\r\n\r\nif I need to access both, the only option is to use hooks called before the next epoch starts?\r\n"
      },
      {
        "user": "AJSVB",
        "created_at": "2022-02-24T17:50:14Z",
        "body": "> This behavior is expected.\r\n\r\nIf not already the case, could the documentation precise this point?"
      }
    ]
  },
  {
    "number": 3827,
    "title": "Deprecate EvalModelTemplatein favor of BoringModel and another simple model that does actually learn",
    "created_at": "2020-10-03T20:20:28Z",
    "closed_at": "2020-12-05T20:09:48Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "ci",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3827",
    "body": "## 🚀 Feature\r\n\r\ncorrect actual `EvalModelTemplate` to use new API unless it is testing other purposes or deprecated API\r\n\r\n### Motivation\r\n\r\nbetter testing of the actual API",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3827/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-10-04T20:28:29Z",
        "body": "we need to deprecate evalmodeltemplate.\r\n\r\n"
      },
      {
        "user": "gianscarpe",
        "created_at": "2020-10-29T19:48:03Z",
        "body": "Hi @williamFalcon, I would like to start contributing to pytorch-lightning. Is this still a first good issue?\r\n"
      },
      {
        "user": "gianscarpe",
        "created_at": "2020-10-30T16:37:17Z",
        "body": "Update:\r\nThis line is causing three tests in `tests/backend/test_ddp.py` to fail.\r\n`from tests.base import EvalModelTemplate` in `tests/backends/model_dpd.py` \r\n\r\nThe error is `\\nModuleNotFoundError: No module named \\'tests.base\\'\\n'`. I think it could be related to this issue"
      },
      {
        "user": "gianscarpe",
        "created_at": "2020-11-05T09:50:02Z",
        "body": "I fixed the issue related to my previous comment. I proceed with a PR, and start a new branch for implementing an EvalModel"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-11-05T11:05:33Z",
        "body": "welcome to PL! excited to help you contribute. \r\n\r\nPlease do the deprecation only a few files at a time and make sure to ping @SeanNaren and @tchaton for guidance and advice! \r\n"
      },
      {
        "user": "gianscarpe",
        "created_at": "2020-11-16T16:17:43Z",
        "body": "Hi @SeanNaren and @tchaton \r\nI'm working on deprecating EvalModelTemplate and adopting BoringModel throught the test suite. And I have a couple of questions. I wrote a dispatcher function in `develop_pipelines` to call the right `run_prediction` if the model is of class `BoringModel`. Actually, BoringModel is just a perceptron with a hidden layer and build a build a random dataset. Do we need a more \"advanced\" test model?\r\nAt the moment I've refactored `test_gpu`  and I'm working on `test_cpu`. I open a draft pull request to share my progress and my solution at #4623 \r\nLet me know if it sounds good to you and if you have any suggestions!\r\n\r\n"
      },
      {
        "user": "gianscarpe",
        "created_at": "2020-11-20T15:54:34Z",
        "body": "[[> Hi @SeanNaren and @tchaton\r\n> I'm working on deprecating EvalModelTemplate and adopting BoringModel throught the test suite. And I have a couple of questions. I wrote a dispatcher function in `develop_pipelines` to call the right `run_prediction` if the model is of class `BoringModel`. Actually, BoringModel is just a perceptron with a hidden layer and build a build a random dataset. Do we need a more \"advanced\" test model?\r\n> At the moment I've refactored `test_gpu` and I'm working on `test_cpu`. I open a draft pull request to share my progress and my solution at #4623\r\n> Let me know if it sounds good to you and if you have any suggestions!\r\n\r\nHi @SeanNaren @tchaton  @williamFalcon , I have an additional question: in `test_cpu.py`, are test_tbtt_cpu_model_result and test_tbptt_cpu_model_result_auto_reduce deprecated? They both use `TrainResult`, which should be deprecated by version 1.0"
      },
      {
        "user": "SeanNaren",
        "created_at": "2020-11-21T11:46:10Z",
        "body": "Hey @gianscarpe apologies for the radio silence, got lost in my notifications. Had a read over your draft PR, awesome stuff :)\r\n\r\nRegarding the dispatcher function I think it's fine. The BoringModel is the baseline, all other models should override within the tests :)\r\n\r\nTake it one PR at a time (maybe one file/feature at a time?) and it will be a smoother experience! It may take time but will get done eventually.\r\n\r\nRegarding the tbptt tests I'm not sure as this is a specific functionality required for TBPTT (passing previous hiddens), @tchaton recently looked over the API so may be able to comment!\r\n\r\n\r\n"
      },
      {
        "user": "gianscarpe",
        "created_at": "2020-11-23T14:13:15Z",
        "body": "> Hey @gianscarpe apologies for the radio silence, got lost in my notifications. Had a read over your draft PR, awesome stuff :)\r\n> \r\n> Regarding the dispatcher function I think it's fine. The BoringModel is the baseline, all other models should override within the tests :)\r\n> \r\n> Take it one PR at a time (maybe one file/feature at a time?) and it will be a smoother experience! It may take time but will get done eventually.\r\n> \r\n> Regarding the tbptt tests I'm not sure as this is a specific functionality required for TBPTT (passing previous hiddens), @tchaton recently looked over the API so may be able to comment!\r\n\r\nThanks for the reply! I followed your advice and opened a dedicate PR for `test_gpu` and `test_cpu` only #4820"
      }
    ]
  },
  {
    "number": 3826,
    "title": "correct the EvalModelTemplate methods - template",
    "created_at": "2020-10-03T20:17:14Z",
    "closed_at": "2020-11-03T16:05:02Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "ci"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3826",
    "body": "## 🚀 Feature\r\n\r\nThe `EvalModelTemplate` has some standard methods like `train_step` the template is such as `<model_method>__<other_describtion>`\r\n\r\n### Motivation\r\n\r\nit is much easier to read the used methods if there are visually separated by two `_`",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3826/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "arp95",
        "created_at": "2020-10-13T14:54:03Z",
        "body": "Hi Borda,\r\n\r\nI would like to take this issue if its fine with you."
      },
      {
        "user": "Borda",
        "created_at": "2020-11-03T16:05:02Z",
        "body": "This won't be needed anymore as we are going to deprecate EvalModelTemplate in favour variation of BoringModels"
      }
    ]
  },
  {
    "number": 3797,
    "title": "Broken ddp_cpu backend",
    "created_at": "2020-10-02T15:05:18Z",
    "closed_at": "2020-10-02T17:51:03Z",
    "labels": [
      "bug",
      "duplicate",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3797",
    "body": "## 🐛 Bug\r\n\r\nBroken on current master.\r\n\r\n### To Reproduce\r\n\r\n```python\r\ndef test(tmpdir):\r\n    import pytorch_lightning as pl\r\n    trainer = pl.Trainer(\r\n        default_root_dir=tmpdir,\r\n        max_epochs=3,\r\n        limit_train_batches=10,\r\n        limit_val_batches=10,\r\n        distributed_backend=\"ddp_cpu\",\r\n    )\r\n    model = DummyModule() # Linear layer on MNIST\r\n    trainer.fit(model)\r\n```\r\n\r\nError:\r\n\r\n```\r\nE       -- Process 0 terminated with the following error:\r\nE       Traceback (most recent call last):\r\nE         File \"/home/carmocca/projects/PyLaia/venv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\nE           fn(i, *args)\r\nE         File \"/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/accelerators/ddp_cpu_spawn_backend.py\", line 137, in ddp_train\r\nE           results = self.train_or_test()\r\nE         File \"/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/accelerators/base_backend.py\", line 43, in train_or_test\r\nE           results = self.trainer.train()\r\nE         File \"/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 464, in train\r\nE           self.run_sanity_check(self.get_model())\r\nE         File \"/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 649, in run_sanity_check\r\nE           _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)\r\nE         File \"/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 571, in run_evaluation\r\nE           output = self.evaluation_loop.evaluation_step(test_mode, batch, batch_idx, dataloader_idx)\r\nE         File \"/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py\", line 149, in evaluation_step\r\nE           output = self.trainer.accelerator_backend.validation_step(args)\r\nE         File \"/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/accelerators/ddp_cpu_spawn_backend.py\", line 157, in validation_step\r\nE           output = self.training_step(args)\r\nE         File \"/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/accelerators/ddp_cpu_spawn_backend.py\", line 153, in training_step\r\nE           output = self.trainer.model(*args)\r\nE         File \"/home/carmocca/projects/PyLaia/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\nE           result = self.forward(*input, **kwargs)\r\nE         File \"/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/overrides/data_parallel.py\", line 209, in forward\r\nE           warn_missing_output(fx_called)\r\nE         File \"/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/overrides/data_parallel.py\", line 228, in warn_missing_output\r\nE           warning_cache.warn(m)\r\nE       UnboundLocalError: local variable 'm' referenced before assignment\r\n```\r\n\r\n### Expected behavior\r\n\r\nDoes not fail",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3797/comments",
    "author": "carmocca",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-10-02T17:22:57Z",
        "body": "@williamFalcon could it be linked to you refactoring?\r\nseems to be similar to #3798"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-10-02T17:46:55Z",
        "body": "just a typo... easy fix"
      }
    ]
  },
  {
    "number": 3786,
    "title": "automatically override progress_bar_refresh_rate in google colab",
    "created_at": "2020-10-02T04:41:58Z",
    "closed_at": "2020-11-23T20:43:33Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3786",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nIf running on google colab, automatically assign `progress_bar_refresh_rate=20` if the user didn't override the default of 1.\r\n\r\n### Motivation\r\n\r\nGoogle colab is notorious for crashing because of the tqdm progress bar while training. We usually solve that by assigning a larger number to `progress_bar_refresh_rate` flag of Trainer. However, this is super annoying since you have to do it all the time.\r\n\r\nAlso, when writing up examples for beginners, it would be nice to just say `pl.Trainer()` instead of having to pass the refresh rate flag, as that may confuse newcomers. \r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nPut this logic in `CallbackConnector`, or `ProgressBarBase`. I'm not sure which makes more sense.\r\n\r\n```python\r\nimport sys\r\n\r\n\r\ndef check_if_on_google_colab():\r\n    return 'google.colab' in sys.modules\r\n\r\n```\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3786/comments",
    "author": "nateraw",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-10-02T14:36:36Z",
        "body": "Yes please, this is nice. It crashes my whole Chrome if I forget it, it's annoying.\r\nI would put it into the callback connector for the default case because in the progressbarBase it could override the settings if a user has a custom bar and that's probably undesired?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-03T17:04:15Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3778,
    "title": "training_step log requires that tbptt_reduce_fx is also set",
    "created_at": "2020-10-01T22:03:07Z",
    "closed_at": "2020-10-04T21:10:26Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3778",
    "body": "## 🐛 Bug\r\n\r\n`training_step` `log` requires that `tbptt_reduce_fx` is also set.\r\n\r\n#### Code sample\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    ...\r\n    self.log(\"train_loss\", loss, on_step=False, on_epoch=True, sync_dist=True)\r\n    self.log(\r\n        \"foo\",\r\n        torch.tensor(self.current_epoch),\r\n        on_step=False,\r\n        on_epoch=True,\r\n        reduce_fx=max,\r\n        #tbptt_reduce_fx=max, ERROR when commented\r\n    )\r\n    return loss\r\n\r\ndef validation_step(self, batch, batch_idx):\r\n    # No issues here\r\n    self.log(\r\n        \"bar\",\r\n        torch.tensor(self.global_step),\r\n        on_step=False,\r\n        on_epoch=True,\r\n        reduce_fx=max,\r\n    )\r\n```\r\n\r\nError:\r\n```\r\ntime_outputs = [{'tr_loss': tensor(6.2107), 'foo': tensor(0), 'minimize': tensor(6.2107)}]\r\n\r\n    @classmethod\r\n    def reduce_across_time(cls, time_outputs):\r\n        # auto-reduce across time for tbptt\r\n        meta = time_outputs[0]['meta']\r\n    \r\n        # in 1.0 the results have 'extra'. Once we deprecate 0.10.0 we may not need this\r\n        if 'extra' in time_outputs[0]:\r\n            [x.pop('extra', None) for x in time_outputs]\r\n    \r\n        result = cls()\r\n        result = recursive_gather(time_outputs, result)\r\n        recursive_stack(result)\r\n    \r\n        for k, value in result.items():\r\n            if k in ['meta', 'extra']:\r\n                continue\r\n    \r\n            # pick the reduce fx\r\n            if k in ['checkpoint_on', 'early_stop_on', 'minimize']:\r\n                tbptt_reduce_fx = torch.mean\r\n            else:\r\n                tbptt_reduce_fx = meta[k]['tbptt_reduce_fx']\r\n>           result[k] = tbptt_reduce_fx(value)\r\nE           RuntimeError: Can only calculate the mean of floating types. Got Long instead.\r\n\r\n../../venv/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py:423: RuntimeError\r\n```\r\n\r\n### Expected behavior\r\n\r\nThat `tbptt_reduce_fx` is not required\r\n\r\n### Environment\r\n\r\nMaster @ commit 8be002c",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3778/comments",
    "author": "carmocca",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-10-02T13:01:25Z",
        "body": "good catch. yeah, that's a bug.\r\n\r\nwant to submit a PR?"
      },
      {
        "user": "carmocca",
        "created_at": "2020-10-02T14:31:12Z",
        "body": "I am not familiar with the tbptt logic, so maybe its better someone else does it.\r\n\r\nI'll create a PR with the failing test so somebody can work on it."
      },
      {
        "user": "PiotrDabkowski",
        "created_at": "2020-12-22T23:48:14Z",
        "body": "What the hell is this tbptt, this looks crazy specific, why is this necessary? Why not keep things simple?"
      }
    ]
  },
  {
    "number": 3771,
    "title": "Handling AttributeErrors while cleaning params namespace while setting up fit",
    "created_at": "2020-10-01T14:03:25Z",
    "closed_at": "2020-11-04T18:10:58Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3771",
    "body": "## 🐛 Bug\r\n\r\n`is_picklable` in `parsing.py` does not handle AttributeError thrown by `pickle.dumps()` - specifically, the following :\r\n`AttributeError: Can't pickle local object 'ArgumentParser.__init__.<locals>.identity'`\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\nHere's a stack trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/chirag/miniconda3/envs/ml/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/chirag/miniconda3/envs/ml/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/chirag/Projects/mingle/social-processes/run/run_synthetic_social.py\", line 120, in <module>\r\n    main()\r\n  File \"/home/chirag/Projects/mingle/social-processes/run/run_synthetic_social.py\", line 116, in main\r\n    trainer.fit(process, datamodule=dm)\r\n  File \"/home/chirag/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 425, in fit\r\n    self.train_loop.setup_fit(model, train_dataloader, val_dataloaders, datamodule)\r\n  File \"/home/chirag/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 90, in setup_fit\r\n    parsing.clean_namespace(model.hparams)\r\n  File \"/home/chirag/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py\", line 75, in clean_namespace\r\n    del_attrs = [k for k, v in hparams_dict.items() if not is_picklable(v)]\r\n  File \"/home/chirag/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py\", line 75, in <listcomp>\r\n    del_attrs = [k for k, v in hparams_dict.items() if not is_picklable(v)]\r\n  File \"/home/chirag/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py\", line 62, in is_picklable\r\n    pickle.dumps(obj)\r\nAttributeError: Can't pickle local object 'ArgumentParser.__init__.<locals>.identity'\r\n```\r\nI forked the repo, found the file, and made the following change to `is_picklable`:\r\n```\r\ndef is_picklable(obj: object) -> bool:\r\n    \"\"\"Tests if an object can be pickled\"\"\"\r\n\r\n    try:\r\n        pickle.dumps(obj)\r\n        return True\r\n    except (pickle.PicklingError, AttributeError):\r\n        return False\r\n```\r\n\r\nI then installed the package from my local repo, ran the same code and got the following warnings:\r\n```\r\n/home/chirag/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: attribute 'trials' removed from hparams because it cannot be pickled\r\n  warnings.warn(*args, **kwargs)\r\n/home/chirag/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: attribute 'optimize_parallel' removed from hparams because it cannot be pickled\r\n  warnings.warn(*args, **kwargs)\r\n/home/chirag/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: attribute 'optimize_parallel_gpu' removed from hparams because it cannot be pickled\r\n  warnings.warn(*args, **kwargs)\r\n/home/chirag/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: attribute 'optimize_parallel_cpu' removed from hparams because it cannot be pickled\r\n  warnings.warn(*args, **kwargs)\r\n/home/chirag/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: attribute 'generate_trials' removed from hparams because it cannot be pickled\r\n  warnings.warn(*args, **kwargs)\r\n/home/chirag/miniconda3/envs/ml/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: attribute 'optimize_trials_parallel_gpu' removed from hparams because it cannot be pickled\r\n  warnings.warn(*args, **kwargs)\r\n```\r\n\r\nThese aren't params added by the end user I believe, and for some reason pickle raises an `AttributeError` rather than `pickling.PicklingError` for these.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - Quadro P4000\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.6.0\r\n        - pytorch-lightning: 0.9.1rc4\r\n        - tqdm:              4.48.2\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.5\r\n        - version:           #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020\r\n### Additional context\r\n\r\nNot sure if the solution is to also handle `AttributeError`, or if a more elegant alternative is needed.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3771/comments",
    "author": "chiragraman",
    "comments": [
      {
        "user": "chiragraman",
        "created_at": "2020-10-08T16:12:44Z",
        "body": "I believe these are added when using the `HyperOptArgumentParser` from test_tube.  "
      },
      {
        "user": "edenlightning",
        "created_at": "2020-10-22T16:07:32Z",
        "body": "Thanks for the issue @chiragraman! Any way you can send a PR to fix?"
      },
      {
        "user": "chiragraman",
        "created_at": "2020-10-22T16:09:37Z",
        "body": "@edenlightning sure thing! Is the suggested solution to also catch `AttributeErrors` alright?"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-10-27T04:05:10Z",
        "body": "@Borda might have opinions on this"
      }
    ]
  },
  {
    "number": 3769,
    "title": "Hyperparameters for DataModules",
    "created_at": "2020-10-01T13:31:46Z",
    "closed_at": "2021-07-09T15:10:00Z",
    "labels": [
      "feature",
      "help wanted",
      "data handling",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3769",
    "body": "## 🚀 Feature\r\n\r\nAdd a `save_hyperparameters` function to `LightningDataModule` and let the `Trainer` log it together with the model's hyperparameters.\r\n\r\n### Motivation\r\n\r\nDataModules are a great way to decouple the model code from the data it runs on.\r\nPeople using datasets where pre-processing is not as fixed as in the common benchmarks need their DataModules to be configurable.\r\nExamples would be:\r\n\r\n* size of sliding windows\r\n* maximum sequence length\r\n* type of scaling (min-max, standardization, etc.)\r\n\r\nLogging these hyperparameters is just as important for evaluating your model performance as the model's hyperparameters.\r\nTherefore, they should be automatically logged by the trainer, too.\r\n\r\n### Pitch\r\n\r\nYou are still searching for the perfect way to pre-process your data for maximum performance?\r\nKeep all your efforts in order by logging the hyperparameters of your `LightningDataModule`.\r\n\r\n### Alternatives\r\n\r\nRight now, I manually define a hyperparameter dictionary as a member of my DataModule.\r\nAfterward, I call `update` on the hparams property of my LightningModule.\r\nThis is pretty low-level code at the top-level of my script.\r\n\r\n### Additional context\r\n\r\nCode example for current solution:\r\n\r\n```\r\nclass MyDataModule(pl.LightningDataModule):\r\n    def __init__(self,\r\n                 fd,\r\n                 batch_size,\r\n                 max_rul=125,\r\n                 window_size=30,\r\n                 percent_fail_runs=None,\r\n                 percent_broken=None,\r\n                 feature_select=None):\r\n\r\n        ...\r\n\r\n        self.hparams = {'fd': self.fd,\r\n                        'batch_size': self.batch_size,\r\n                        'window_size': self.window_size,\r\n                        'max_rul': self.max_rul,\r\n                        'percent_broken': self.percent_broken,\r\n                        'percent_fail_runs': self.percent_fail_runs}\r\n\r\n...\r\n\r\ndata = datasets.MyDataModule(...)\r\nmodel.hparams.update(data.hparams)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3769/comments",
    "author": "tilman151",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-10-01T13:32:32Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "nateraw",
        "created_at": "2020-10-02T02:45:21Z",
        "body": "yup, been battling with myself whether or not this is necessary. But I think it is...do you want to submit a PR for this?"
      },
      {
        "user": "tilman151",
        "created_at": "2020-10-02T05:32:59Z",
        "body": "Sure, I'll give it a shot."
      },
      {
        "user": "adriantre",
        "created_at": "2020-10-02T12:08:02Z",
        "body": "@tilman151 I like this idea!.\r\n\r\nWhat do you think about an `add_datamodule_specific_args()` static method in LighningDataModule as well? Do you see the use case? For defining defaults for parameters like train/val/test split, positive/negative balance in the datamodule."
      },
      {
        "user": "SurajDonthi",
        "created_at": "2020-10-02T12:14:50Z",
        "body": "@adriantre I think you can still define a static method similar to that of `LightningModule` which should work."
      },
      {
        "user": "adriantre",
        "created_at": "2020-10-02T12:47:14Z",
        "body": "@SurajDonthi Yes, you are right of course 😅"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-03T17:04:14Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "tilman151",
        "created_at": "2020-11-05T06:24:21Z",
        "body": "@edenlightning I am really sorry to bother you, but could you explain why this issue was closed? The associated PR was still under review."
      },
      {
        "user": "edenlightning",
        "created_at": "2021-02-22T21:29:16Z",
        "body": "sorry, this is an automatic bot. apologize for that! I know it's been a while.. "
      }
    ]
  },
  {
    "number": 3752,
    "title": "Default reduction always applied by `Metric`, even when requesting `'none'` reduction",
    "created_at": "2020-09-30T19:46:37Z",
    "closed_at": "2020-10-05T14:13:24Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3752",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nMetric reduction doesn't behave the same between the functional and class API when using `reduction='none'`. The functional API applies no reduction as expected, but the class API seems to apply the default reduction regardless.\r\n\r\nI haven't investigated the code yet to find the specific cause of the bug, so I'm not sure how widespread this bug is, but I've encountered it using both the `DiceCoefficient` and my own implementation of the differentiable dice, inheriting from `TensorMetric`.\r\n\r\n### To Reproduce\r\n\r\nGiven a pair of `pred` and `target`, I get the following behavior with 3 class + background segmentation data:\r\n```python\r\n>>> from pytorch_lightning.metrics import DiceCoefficient\r\n>>> from pytorch_lightning.metrics.functional import dice_score\r\n>>> DiceCoefficient(reduction=\"none\")(pred, target)\r\ntensor(0.0800)\r\n>>> dice_score(pred, target, reduction=\"none\")\r\ntensor([0.0876, 0.0937, 0.0586], device='cuda:0')\r\n```\r\nwhere I would have expected both version to give the same result.\r\n\r\nThe class API seems to apply the default reduction of `'elementwise_mean'` even though I requested `'none'`, since:\r\n```python\r\n>>> dice_score(x_hat, x, reduction=\"none\").mean()\r\ntensor(0.0800, device='cuda:0')\r\n```\r\n\r\n### Expected behavior\r\nReduction behavior should be consistent between class and functional API, and to behave like the current functional API.\r\n\r\n### Environment\r\nI just now installed Lightning from Git to ensure that it's not a bug that's already been solved since the last release.\r\n\r\n* CUDA:\r\n        - GPU: TITAN Xp\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.6.0\r\n        - pytorch-lightning: 0.9.1rc4\r\n        - tqdm:              4.49.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture: 64bit, ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.5\r\n        - version:           #51~18.04.1-Ubuntu SMP Sat Sep 5 14:35:50 UTC 2020\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3752/comments",
    "author": "nathanpainchaud",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-05T13:44:51Z",
        "body": "Hi @nathanpainchaud, running your code example on master produces the correct result (your issue was probably solved by PR #3517). Could you please try upgrading?"
      },
      {
        "user": "nathanpainchaud",
        "created_at": "2020-10-05T14:13:24Z",
        "body": "Hi @SkafteNicki. I can confirm that master now runs fine on my end as well. Thanks for the follow up!"
      }
    ]
  },
  {
    "number": 3738,
    "title": "RuntimeError: Input and hidden tensors are not at the same device, found",
    "created_at": "2020-09-30T08:05:07Z",
    "closed_at": "2020-09-30T12:34:03Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3738",
    "body": "## 🐛 Bug\r\n\r\nI train LSTM for character level  text generation. At first I initialize hidden and cell with zeros using `torch.zeros`. Unfortunately this tensors are defaultly assigned to the cpu so I get the following error while training\r\n\r\n```python\r\nRuntimeError: Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu\r\n```\r\n\r\n### To Reproduce\r\n\r\n#### Model\r\n\r\n```python\r\nclass RNN(pl.LightningModule):\r\n    lr = 0.0005\r\n\r\n    def __init__(self, input_size, hidden_size, embeding_size, n_categories, n_layers, output_size, p):\r\n        super().__init__()\r\n\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        \r\n        self.n_layers = n_layers\r\n        self.hidden_size = hidden_size\r\n        \r\n        \r\n        self.embeding = nn.Embedding(input_size+n_categories, embeding_size)\r\n        self.lstm = nn.LSTM(embeding_size+n_categories, hidden_size, n_layers, dropout=p)\r\n        self.out_fc = nn.Linear(hidden_size, output_size)\r\n        \r\n        self.dropout = nn.Dropout(p)\r\n        \r\n\r\n    def forward(self, batch_of_category, batch_of_letter, hidden, cell):\r\n        ## letter level operations\r\n        \r\n        embeding = self.dropout(self.embeding(batch_of_letter))\r\n        category_plus_letter = torch.cat((batch_of_category, embeding), 1)\r\n\r\n        #sequence_length = 1\r\n        category_plus_letter = category_plus_letter.unsqueeze(1)\r\n        \r\n        out, (hidden, cell) = self.lstm(category_plus_letter, (hidden, cell))\r\n        out = self.out_fc(out)\r\n        out = out.squeeze(1)\r\n        \r\n        return out, (hidden, cell)\r\n        \r\n\r\n    def configure_optimizers(self):\r\n        optimizer = Adam(self.parameters(), self.lr)\r\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\r\n\r\n        return [optimizer], [scheduler]\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        item_dict = batch\r\n        loss = 0\r\n        batch_of_category = item_dict[\"category_tensors\"]\r\n\r\n        #we loop over letters, single batch at the time \r\n        \r\n        hidden = torch.zeros(self.n_layers, 1, self.hidden_size).cuda()\r\n        cell = torcAh.zeros(self.n_layers, 1, self.hidden_size).cuda()\r\n        \r\n        for t in range(item_dict[\"input_tensors\"].size(1)):\r\n            batch_of_letter = item_dict[\"input_tensors\"][:, t]\r\n            \r\n            output, (hidden, cell) = self(batch_of_category, batch_of_letter, hidden, cell)\r\n            \r\n            loss += criterion(output, item_dict[\"target_tensors\"][:, t])\r\n\r\n        loss = loss/(t+1)\r\n\r\n\r\n        tensorboard_logs = {'train_loss': loss}\r\n\r\n        return {'loss': loss, 'log': tensorboard_logs}\r\n    \r\n    \r\n    def init_hidden(self, batch_size):\r\n        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\r\n        cell = torch.zeros(self.n_layers, batch_size, self.hidden_size)\r\n        \r\n        return hidden, cell\r\n```\r\n\r\n#### Batch\r\n\r\n```\r\n(['Russian', 'English', 'Russian', 'English'],\r\n ['Piskarenkov', 'Clarkson', 'Pochkaev', 'Woods'],\r\n tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\r\n         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]]),\r\n tensor([[42,  9, 19, 11,  1, 18,  5, 14, 11, 15, 22],\r\n         [29, 12,  1, 18, 11, 19, 15, 14,  0,  0,  0],\r\n         [42, 15,  3,  8, 11,  1,  5, 22,  0,  0,  0],\r\n         [49, 15, 15,  4, 19,  0,  0,  0,  0,  0,  0]]),\r\n tensor([[ 9, 19, 11,  1, 18,  5, 14, 11, 15, 22, 59],\r\n         [12,  1, 18, 11, 19, 15, 14, 59,  0,  0,  0],\r\n         [15,  3,  8, 11,  1,  5, 22, 59,  0,  0,  0],\r\n         [15, 15,  4, 19, 59,  0,  0,  0,  0,  0,  0]]))\r\n```\r\n\r\n#### Trainer \r\n\r\n```python\r\ndm = NamesDatamodule(1)\r\n\r\nrnn_model = RNN(input_size=ds.n_tokens,\r\n            hidden_size=256,\r\n            embeding_size = 128, \r\n            n_layers=2,    \r\n            n_categories=ds.n_categories,\r\n            output_size=ds.n_tokens,\r\n            p=0.3)\r\n\r\n\r\ntrainer = Trainer(max_epochs=3, \r\n                  logger=None,\r\n                  gpus=1,\r\n                  early_stop_callback=False,\r\n                  checkpoint_callback=False,\r\n                  )\r\n\r\ntrainer.fit(rnn_model, dm)\r\n```\r\n\r\n### Expected behavior\r\n\r\nHidden values should automatically be assigned to the `device`\r\n\r\n### Environment\r\n\r\nGoogle Colab\r\n\r\n - Pytroch 1.6.0+cu101\r\n - Lightning 0.9.1rc3\r\n - Python version:\r\n - GPU models and configuration: single colab GPU\r\n\r\n### Additional context\r\n\r\nProblem can be solved by adding `.cuda()` to the variables but it is not a solution that I think should be necessary \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3738/comments",
    "author": "tugot17",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-30T08:05:49Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-30T10:00:04Z",
        "body": "Try changing to\r\n```python\r\nhidden = torch.zeros(self.n_layers, 1, self.hidden_size)..to(self.device)\r\ncell = torch.zeros(self.n_layers, 1, self.hidden_size).to(self.device)\r\n```"
      },
      {
        "user": "tugot17",
        "created_at": "2020-09-30T11:55:25Z",
        "body": "@rohitgr7 Yeah, this fixes the problem, however I'm not entirely sure it will also work in case if I used more then a single machine to train the model "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-30T12:02:18Z",
        "body": "`self.device` will always give you the device for the current process(ddp) or current batch(dp) being executed."
      },
      {
        "user": "tugot17",
        "created_at": "2020-09-30T12:34:03Z",
        "body": "Ok, I can close the issue. Thanks for your help :)  "
      }
    ]
  },
  {
    "number": 3732,
    "title": "Support best model checkpoint path even if save_top_k=-1",
    "created_at": "2020-09-29T23:58:20Z",
    "closed_at": "2020-09-30T04:41:01Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3732",
    "body": "## 🚀 Feature\r\nSupport best model checkpoint path even if `save_top_k=-1`\r\n### Motivation\r\nFor the model checkpoint callback, the callback could still track the best checkpoint path even if `save_top_k=-1`. The only case where we couldn't track the best checkpoint is if the `monitor` metric isn't specified. What do you think?\r\n\r\n### Pitch\r\n\r\nUpdate the model checkpoint callback to only skip tracking the best checkpoint if `monitor` is `None`\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3732/comments",
    "author": "ananthsub",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-09-30T03:58:06Z",
        "body": "Duplicate #2586??"
      },
      {
        "user": "ananthsub",
        "created_at": "2020-09-30T04:41:01Z",
        "body": "> Duplicate #2586??\r\n\r\nyes it is. I didn't see that. I'll close this issue out in favor of that one"
      }
    ]
  },
  {
    "number": 3730,
    "title": "test_step hangs after one iteration when on multiple GPUs",
    "created_at": "2020-09-29T22:15:32Z",
    "closed_at": "2020-10-03T18:05:32Z",
    "labels": [
      "bug",
      "help wanted",
      "distributed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3730",
    "body": "\r\n## 🐛 Bug\r\n\r\nWhen running the same code on a computer with 1 gpu, test_step runs as normal and logs what it should.\r\nHow ever on a node with 4 gpus, it hangs after 1 iteration! \r\n\r\n#### Code sample\r\n\r\n```python\r\n images, masks = batch[\"image\"], batch[\"mask\"]\r\n        if images.shape[1] != self.hparams.n_channels:\r\n            raise AssertionError(\r\n                f\"Network has been defined with {self.n_channels} input channels, \"\r\n                f\"but loaded images have {images.shape[1]} channels. Please check that \"\r\n                \"the images are loaded correctly.\"\r\n            )\r\n\r\n        masks = (\r\n            masks.type(torch.float32)\r\n            if self.hparams.n_classes == 1\r\n            else masks.type(torch.long)\r\n        )\r\n\r\n        masks_pred = self(images)  # Forward pass\r\n        loss = self.loss_funciton(masks_pred, masks)\r\n        result = pl.EvalResult(loss, checkpoint_on=loss)\r\n        result.log(\"test_loss\", loss, on_step=True, on_epoch=True, sync_dist=True)\r\n        rand_idx = randint(0, self.hparams.batch_size - 1)\r\n        onehot = torch.sigmoid(masks_pred[rand_idx]) > 0.5\r\n        for tag, value in self.named_parameters():\r\n            tag = tag.replace(\".\", \"/\")\r\n            self.logger.experiment.add_histogram(tag, value, self.current_epoch)\r\n        mask_grid = torchvision.utils.make_grid([masks[rand_idx], onehot], nrow=2)\r\n        self.logger.experiment.add_image(\r\n            \"TEST - Target vs Predicted\", mask_grid, self.current_epoch\r\n        )\r\n        alpha = 0.5\r\n        image_grid = torchvision.utils.make_grid(\r\n            [\r\n                images[rand_idx],\r\n                torch.clamp(\r\n                    kornia.enhance.add_weighted(\r\n                        src1=images[rand_idx],\r\n                        alpha=1.0,\r\n                        src2=onehot,\r\n                        beta=alpha,\r\n                        gamma=0.0,\r\n                    ),\r\n                    max=1.0,\r\n                ),\r\n            ]\r\n        )\r\n        self.logger.experiment.add_image(\r\n            \"TEST - Image vs Predicted\", image_grid, self.current_epoch\r\n        )\r\n        pred = (torch.sigmoid(masks_pred) > 0.5).float()\r\n        f1 = f1_score(pred, masks, self.hparams.n_classes + 1)\r\n        rec = recall(pred, masks, self.hparams.n_classes + 1)\r\n        pres = precision(pred, masks, self.hparams.n_classes + 1)\r\n        result.log(\"test_f1\", f1, on_epoch=True)\r\n        result.log(\"test_recall\", rec, on_epoch=True)\r\n        result.log(\"test_precision\", pres, on_epoch=True)\r\n\r\n        return result\r\n```\r\n### Expected behavior\r\nI expect it to finish the testing-epoch. \r\n\r\n### Environment\r\nEnvironment 1\r\nCUDA:\r\n- GPU:\r\n- GeForce RTX 2070 SUPER\r\n- available: True\r\n- version: 10.2\r\nPackages:\r\n- numpy: 1.19.2\r\n- pyTorch_debug: False\r\n- pyTorch_version: 1.6.0\r\n- pytorch-lightning: 0.9.0\r\n- tqdm: 4.49.0\r\nSystem:\r\n- OS: Linux\r\n- architecture:\r\n- 64bit\r\n- ELF\r\n- processor: x86_64\r\n- python: 3.6.9\r\n- version: #52~18.04.1-Ubuntu SMP Thu Sep 10 12:50:22 UTC 2020\r\n\r\nEnvironment 2\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce RTX 2080 Ti\r\n\t\t- GeForce RTX 2080 Ti\r\n\t\t- GeForce RTX 2080 Ti\r\n\t\t- GeForce RTX 2080 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.19.1\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tqdm:              4.49.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.0\r\n\t- version:           #208-Ubuntu SMP Sun Apr 5 23:45:10 UTC 2020\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3730/comments",
    "author": "vegovs",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-09-29T22:51:55Z",
        "body": "Can you post which trainer settings you are using?"
      },
      {
        "user": "vegovs",
        "created_at": "2020-09-30T13:19:11Z",
        "body": "```python\r\n    def training_step(self, batch, batch_idx):\r\n        images, masks = batch[\"image\"], batch[\"mask\"]\r\n        if images.shape[1] != self.hparams.n_channels:\r\n            raise AssertionError(\r\n                f\"Network has been defined with {self.hparams.n_channels} input channels, \"\r\n                f\"but loaded images have {images.shape[1]} channels. Please check that \"\r\n                \"the images are loaded correctly.\"\r\n            )\r\n\r\n        masks = (\r\n            masks.type(torch.float32)\r\n            if self.hparams.n_classes == 1\r\n            else masks.type(torch.long)\r\n        )\r\n\r\n        masks_pred = self(images)  # Forward pass\r\n        loss = self.loss_funciton(masks_pred, masks)\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log(\"train_loss\", loss, sync_dist=True)\r\n        if batch_idx == 0:\r\n            self.logg_images(images, masks, masks_pred, \"TRAIN\")\r\n        pred = (torch.sigmoid(masks_pred) > 0.5).float()\r\n        f1 = f1_score(pred, masks, self.hparams.n_classes + 1)\r\n        rec = recall(pred, masks, self.hparams.n_classes + 1)\r\n        pres = precision(pred, masks, self.hparams.n_classes + 1)\r\n        result.log(\"train_f1\", f1, on_epoch=True)\r\n        result.log(\"train_recall\", rec, on_epoch=True)\r\n        result.log(\"train_precision\", pres, on_epoch=True)\r\n\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        images, masks = batch[\"image\"], batch[\"mask\"]\r\n        if images.shape[1] != self.hparams.n_channels:\r\n            raise AssertionError(\r\n                f\"Network has been defined with {self.n_channels} input channels, \"\r\n                f\"but loaded images have {images.shape[1]} channels. Please check that \"\r\n                \"the images are loaded correctly.\"\r\n            )\r\n\r\n        masks = (\r\n            masks.type(torch.float32)\r\n            if self.hparams.n_classes == 1\r\n            else masks.type(torch.long)\r\n        )\r\n\r\n        masks_pred = self(images)  # Forward pass\r\n        loss = self.loss_funciton(masks_pred, masks)\r\n        result = pl.EvalResult(loss, checkpoint_on=loss)\r\n        result.log(\"val_loss\", loss, sync_dist=True)\r\n        if batch_idx == 0:\r\n            self.logg_images(images, masks, masks_pred, \"VAL\")\r\n        pred = (torch.sigmoid(masks_pred) > 0.5).float()\r\n        f1 = f1_score(pred, masks, self.hparams.n_classes + 1)\r\n        rec = recall(pred, masks, self.hparams.n_classes + 1)\r\n        pres = precision(pred, masks, self.hparams.n_classes + 1)\r\n        result.log(\"val_f1\", f1, on_epoch=True)\r\n        result.log(\"val_recall\", rec, on_epoch=True)\r\n        result.log(\"val_precision\", pres, on_epoch=True)\r\n\r\n        return result\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        images, masks = batch[\"image\"], batch[\"mask\"]\r\n        if images.shape[1] != self.hparams.n_channels:\r\n            raise AssertionError(\r\n                f\"Network has been defined with {self.n_channels} input channels, \"\r\n                f\"but loaded images have {images.shape[1]} channels. Please check that \"\r\n                \"the images are loaded correctly.\"\r\n            )\r\n\r\n        masks = (\r\n            masks.type(torch.float32)\r\n            if self.hparams.n_classes == 1\r\n            else masks.type(torch.long)\r\n        )\r\n\r\n        masks_pred = self(images)  # Forward pass\r\n        loss = self.loss_funciton(masks_pred, masks)\r\n        result = pl.EvalResult(loss, checkpoint_on=loss)\r\n        result.log(\"test_loss\", loss, on_step=True, on_epoch=True, sync_dist=True)\r\n        self.logg_images(images, masks, masks_pred, \"TEST\")\r\n        pred = (torch.sigmoid(masks_pred) > 0.5).float()\r\n        f1 = f1_score(pred, masks, self.hparams.n_classes + 1)\r\n        rec = recall(pred, masks, self.hparams.n_classes + 1)\r\n        pres = precision(pred, masks, self.hparams.n_classes + 1)\r\n        result.log(\"test_f1\", f1, on_epoch=True)\r\n        result.log(\"test_recall\", rec, on_epoch=True)\r\n        result.log(\"test_precision\", pres, on_epoch=True)\r\n\r\n        return result\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-30T13:22:18Z",
        "body": "What arguments do you pass to Trainer(...)\r\nDo you use distributed_backend=ddp?"
      },
      {
        "user": "vegovs",
        "created_at": "2020-09-30T14:45:41Z",
        "body": "\r\n```python\r\n    try:\r\n        trainer = Trainer.from_argparse_args(\r\n            args,\r\n            gpus=-1,\r\n            precision=16,\r\n            distributed_backend=\"ddp\",\r\n            callbacks=[lr_monitor],\r\n            early_stop_callback=early_stopping,\r\n            accumulate_grad_batches=1\r\n            if not os.getenv(\"ACC_GRAD\")\r\n            else int(os.getenv(\"ACC_GRAD\")),\r\n            gradient_clip_val=0.0\r\n            if not os.getenv(\"GRAD_CLIP\")\r\n            else float(os.getenv(\"GRAD_CLIP\")),\r\n            max_epochs=1000 if not os.getenv(\"EPOCHS\") else int(os.getenv(\"EPOCHS\")),\r\n            default_root_dir=os.getcwd()\r\n            if not os.getenv(\"DIR_ROOT_DIR\")\r\n            else os.getenv(\"DIR_ROOT_DIR\"),\r\n        )\r\n        trainer.fit(model)\r\n        trainer.test(model)\r\n    except KeyboardInterrupt:\r\n        torch.save(model.state_dict(), \"INTERRUPTED.pth\")\r\n        logging.info(\"Saved interrupt\")\r\n        try:\r\n            sys.exit(0)\r\n        except SystemExit:\r\n            os._exit(0)\r\n```"
      },
      {
        "user": "vegovs",
        "created_at": "2020-09-30T14:48:57Z",
        "body": "Is using DDP the issue? I used DDP on the environment with one GPU as well. "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-30T15:01:01Z",
        "body": "yes, unfortunately it looks like so. calling `trainer.fit `then `trainer.test` currently does not work with DDP due to the fact that it is launching in a separate script. We haven't found a good solution for this yet. For the one gpu case, it should work though. A solution is to switch to ddp_spawn, but it is not ideal. then all your classes need to be pickleable.\r\nor you may move trainer.test to a separate script and call it independently."
      }
    ]
  },
  {
    "number": 3709,
    "title": "Add options to checkpoint metrics when too big to fit in memory",
    "created_at": "2020-09-28T20:06:44Z",
    "closed_at": "2020-11-04T23:45:28Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3709",
    "body": "In certain cases when you have really large models metrics can be too big to fit in memory, so need to be checkpointed to some file.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3709/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-10-28T20:27:14Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3702,
    "title": "Loads args from .evn files some type of config file. ",
    "created_at": "2020-09-28T13:58:22Z",
    "closed_at": "2020-11-02T14:24:20Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3702",
    "body": "## 🚀 Feature / Motivation\r\nI often run my training scripts on different systems with different setups. It would be nice to be able to read args from a configuration file. Like If i on one node have a default directory somewhere other than root, i just put it in the config file. And the trainer loads the args from the file. All i have to do i run python train.py and not python train.py --every-arg-needed etc. \r\n### Pitch\r\nRead arguments from a configuration file at startup, possibly a .env file? \r\n\r\n### Alternatives\r\nMake our own parsers with the same arguments and set defaul=os.getenv(\"DEFAULT_ROOT\") or something. This is what i am doing now. \r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3702/comments",
    "author": "vegovs",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-28T13:59:06Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-28T20:27:15Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3677,
    "title": "remove value field from Result objects 'meta' key",
    "created_at": "2020-09-27T09:50:41Z",
    "closed_at": "2020-11-07T02:06:15Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3677",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nremove value field from Result objects 'meta' key\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nThe value field in the Result obj's  meta data is\r\n- a duplicate of the raw data in the Result obj, \r\n- not being used,\r\n- not updated in the gathered results presented to the user in [training,validation,test]_epoch_end.\r\n\r\nEspecially this last point can lead to confusion.\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nThe value field should no longer be written to the meta data in the Result obj.\r\n\r\n\r\nI can submit a PR if this change is approved.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3677/comments",
    "author": "gerardsn",
    "comments": [
      {
        "user": "teddykoker",
        "created_at": "2020-09-30T17:18:55Z",
        "body": "Hey @gerardsn thanks for the issue! We are currently making some changes with regards to result, so I'll check back in a couple days to see if this is still relevant :)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-30T23:22:16Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3676,
    "title": "Accuracy metric return tuple of length num_gpus on ddp in 0.9.1rc4",
    "created_at": "2020-09-27T09:20:23Z",
    "closed_at": "2020-10-01T13:37:52Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3676",
    "body": "code:\r\n```\r\n        vid_acc = self.accuracy_video(video_labels_hat, video_labels)\r\n        print(len(vid_acc), vid_acc)\r\n        monitor = 0-vid_acc\r\n```\r\nIn 0.9.1rc3, vid acc is a tensor, but in rc4, it changes to a tuple. I want to use `-vid_acc` as monitor, and I think it should be a tensor. \r\nUsing rc4, in macos's cpu mode, it's a tensor. But in linux ddp mode, it's a tuple of length `num_gpus`.   ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3676/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-09-28T00:22:38Z",
        "body": "Thanks for the report. Apologies but I'm afraid I don't understand how this relates to our accuracy metric. \r\nLooking at the code `pytorch_lightning.metrics.classification.Accuracy` I don't see a hint why this would return a tuple. \r\nCould you share the code of your function `accuracy_video`?\r\n\r\ncc @SkafteNicki @justusschock "
      },
      {
        "user": "xiadingZ",
        "created_at": "2020-09-28T00:48:34Z",
        "body": "This is my code:\r\n0.9.1rc3:\r\n```\r\nfrom pytorch_lightning.metrics import Accuracy\r\nself.accuracy_video = Accuracy(reduce_op='avg')\r\n```\r\n\r\n0.9.1rc4:\r\n```\r\nfrom pytorch_lightning.metrics import Accuracy\r\nself.accuracy_video = Accuracy()\r\n```\r\nIn 0.9.1rc4, `reduce_op` is changed to  `class_reduction`, and I think it does what `reduce_op='avg'` do by befault"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-28T01:14:43Z",
        "body": "yes it does. \r\nand I a cannot reproduce on the master branch what you describe in the issue. \r\nWhen running with ddp and dpp_spawn on 2 gpus, I always get a single scalar accuarcy per gpu. So on each gpu, it's a scalar as expected. \r\n"
      },
      {
        "user": "xiadingZ",
        "created_at": "2020-09-28T01:24:43Z",
        "body": "I write a example to reproduce:\r\n```\r\nimport os\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\nfrom torch.utils.data import DataLoader\r\nimport pytorch_lightning as pl\r\nfrom torch.utils.data import random_split\r\nfrom pytorch_lightning.callbacks import LearningRateLogger\r\nfrom pytorch_lightning.metrics import Accuracy\r\nimport argparse\r\n\r\nclass LitModel(pl.LightningModule):\r\n\r\n    def __init__(self, hparams, *args, **kwargs):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, hparams.c)\r\n        self.ac = Accuracy()\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log('train_loss', loss, )\r\n        return result\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        label_hat = torch.argmax(y_hat, dim=1).type_as(y)\r\n        acc = self.ac(label_hat, y)\r\n        print(type(acc), acc)\r\n        monitor = 0-acc\r\n        result = pl.EvalResult(checkpoint_on=monitor)\r\n        result.log('acc', acc, sync_dist=True, prog_bar=True)\r\n        return result\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.0005)\r\n    \r\n    def train_dataloader(self):\r\n        dataset = MNIST('./', download=False, transform=transforms.ToTensor())\r\n        train_loader = DataLoader(dataset, batch_size=128)\r\n        return train_loader\r\n\r\n    def val_dataloader(self):\r\n        dataset = MNIST('./', download=False, transform=transforms.ToTensor())\r\n        train_loader = DataLoader(dataset, batch_size=128)\r\n        return train_loader\r\n    \r\n    \r\ndef main(hparams):\r\n    model = LitModel(hparams)\r\n\r\n    trainer = pl.Trainer(\r\n        gpus=hparams.gpus,\r\n        num_nodes=hparams.num_nodes,\r\n        distributed_backend='ddp'\r\n    )\r\n\r\n    trainer.fit(model)\r\n\r\n\r\nif __name__ == '__main__':\r\n    # TRAIN\r\n    parser = argparse.ArgumentParser(conflict_handler=\"resolve\")\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    parser.add_argument('--c', type=int, default=10)\r\n    hparams = parser.parse_args()\r\n    hparams.gpus=2\r\n    hparams.num_nodes=2\r\n    hparams.max_epochs=5\r\n    main(hparams)\r\n```\r\n\r\nHere is error:\r\n```\r\n<class 'tuple'> (tensor([0.0312], device='cuda:1'), tensor([0.0781], device='cuda:1'), tensor([0.0938], device='cuda:1'), ten\r\nsor([0.1406], device='cuda:1'))\r\n<class 'tuple'> (tensor([0.0312], device='cuda:1'), tensor([0.0781], device='cuda:1'), tensor([0.0938], device='cuda:1'), tensor([0.1406], device='cuda:1'))\r\n/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\r\n  warnings.warn(*args, **kwargs)\r\n<class 'tuple'> (tensor([0.0312], device='cuda:0'), tensor([0.0781], device='cuda:0'), tensor([0.0938], device='cuda:0'), tensor([0.1406], device='cuda:0'))\r\n<class 'tuple'> (tensor([0.0312], device='cuda:0'), tensor([0.0781], device='cuda:0'), tensor([0.0938], device='cuda:0'), tensor([0.1406], device='cuda:0'))\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 78, in <module>\r\n    main(hparams)\r\n  File \"train.py\", line 66, in main\r\n    trainer.fit(model)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 302, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 133, in train\r\n    self.ddp_train_tmp(process_idx=self.task_idx, mp_queue=None, model=model)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_base_backend.py\", line 170, in ddp_train_tmp\r\n    results = self.train_or_test()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/base_backend.py\", line 36, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 324, in train\r\n    self.run_sanity_check(self.get_model())\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in run_sanity_check\r\n    _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 430, in run_evaluation\r\n    output = self.evaluation_loop.evaluation_step(test_mode, batch, batch_idx, dataloader_idx)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 145, in evaluation_step\r\n    output = self.trainer.accelerator_backend.validation_step(args)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_base_backend.py\", line 50, in validation_step\r\n    output = self.training_step(args)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_base_backend.py\", line 46, in training_step\r\n    output = self.trainer.model(*args)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 558, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 182, in forward\r\n    output = self.module.validation_step(*inputs[0], **kwargs[0])\r\n  File \"train.py\", line 38, in validation_step\r\n    monitor = 0-acc\r\nTypeError: unsupported operand type(s) for -: 'int' and 'tuple'\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 78, in <module>\r\n    main(hparams)\r\n  File \"train.py\", line 66, in main\r\n    trainer.fit(model)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 302, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 133, in train\r\n    self.ddp_train_tmp(process_idx=self.task_idx, mp_queue=None, model=model)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_base_backend.py\", line 170, in ddp_train_tmp\r\n    results = self.train_or_test()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/base_backend.py\", line 36, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 324, in train\r\n    self.ruTraceback (most recent call last):\r\n  File \"train.py\", line 78, in <module>\r\n    main(hparams)\r\n  File \"train.py\", line 66, in main\r\n    trainer.fit(model)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 302, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 133, in train\r\n    self.ddp_train_tmp(process_idx=self.task_idx, mp_queue=None, model=model)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_base_backend.py\", line 170, in ddp_train_tmp\r\n    results = self.train_or_test()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/base_backend.py\", line 36, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 324, in train\r\n    self.run_sanity_check(self.get_model())\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in run_sanity_check\r\n    _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 430, in run_evaluation\r\n    output = self.evaluation_loop.evaluation_step(test_mode, batch, batch_idx, dataloader_idx)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 145, in evaluation_step\r\n    output = self.trainer.accelerator_backend.validation_step(args)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_base_backend.py\", line 50, in validation_step\r\n    output = self.training_step(args)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_base_backend.py\", line 46, in trn_sanity_check(self.get_model())\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in run_sanity_check\r\n    _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 430, in run_evaluation\r\n    output = self.evaluation_loop.evaluation_step(test_mode, batch, batch_idx, dataloader_idx)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 145, in evaluation_step\r\n    output = self.trainer.accelerator_backend.validation_step(args)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_base_backend.py\", line 50, in validation_step\r\n    output = self.training_step(args)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_base_backend.py\", line 46, in training_step\r\n    output = self.trainer.model(*args)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 558, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 182, in forward\r\n    output = self.module.validation_step(*inputs[0], **kwargs[0])\r\n  File \"train.py\", line 38, in validation_step\r\n    monitor = 0-acc\r\nTypeError: unsupported operand type(s) for -: 'int' and 'tuple'\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-28T01:47:38Z",
        "body": "ok, can reproduce. \r\nThe solution is to use the functional:\r\n`pytorch_lightning.metrics.classification.accuracy`\r\ninstad of the module. \r\n\r\nnot sure yet why the two behave differently"
      },
      {
        "user": "justusschock",
        "created_at": "2020-09-28T06:30:41Z",
        "body": "The issue is that functional accuracy directly calculates it and modular accuracy also does some syncing. But it definitely shouldn't be like this. I'll have a look at it."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-09-28T11:18:21Z",
        "body": "This has to do with how aggregation currently is implemented in the class metrics is implemented.\r\nIt will be solved when #3517 is merged."
      }
    ]
  },
  {
    "number": 3670,
    "title": "Allow .write and .write_dict with TrainResult.",
    "created_at": "2020-09-26T22:11:14Z",
    "closed_at": "2020-12-06T19:37:49Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3670",
    "body": "## 🚀 Feature\r\nAllow `.write` and `.write_dict` with `TrainResult`.\r\n\r\n### Motivation\r\n- Can be used to check the results generated by GAN (generator) over the course of an epoch to visualize and check the learning process of the generator.\r\n\r\n### Pitch\r\nSimilar to that with `EvalResult`.\r\n\r\nLet me know what you guys think. :)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3670/comments",
    "author": "rohitgr7",
    "comments": [
      {
        "user": "kyoungrok0517",
        "created_at": "2020-10-04T07:04:52Z",
        "body": "It'll be even better if batched writing is allowed. It seems like the actual writing happens after all the inference is finished, which can blow GPU memory when the output is huge (e.g. embeddings)"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-11-06T17:28:00Z",
        "body": "is it good to add this feature now?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-06T17:48:30Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3663,
    "title": "EvalResult.write() should use self.logger",
    "created_at": "2020-09-26T01:18:36Z",
    "closed_at": "2020-11-04T06:37:57Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3663",
    "body": "## 🚀 Feature\r\nI quite like using `EvalResult.write()` to generate a report from the `test_step`. However, I feel this should be integrated with `self.logger`\r\n\r\n### Motivation\r\n\r\nBy using self.logger for all logging, consistency is maintained - files end up in the same location and it's easy to enable / disable logging. In particular, I'm using mlflow. I'd like my predictions.pt to end up as a mlflow artifact. At the moment I'm using the code below (in `test_step`) - this works fine but I'm using file:./mlflow as the tracking url - not sure this would work with an http uri.\r\n\r\n        filename  = os.path.join(self.logger.save_dir,self.logger.experiment_id,self.logger.run_id,'artifacts','predictions.pt')\r\n        result.write('src', [';'.join(src)], filename=filename)\r\n        result.write('tgt', [';'.join(tgt)], filename=filename)\r\n        result.write('preds', [';'.join(preds)], filename=filename)\r\n\r\nAlso it would be nice to be able to control the logging format - I'm using nlp so I'm logging sentences. It would be nicer if the file was txt / html rather than pt (i.e. one of the supported mlflow artifact formats). Also `result.write('src', 'the quick brown fox', filename=filename)` fails - it needs to be wrapped as a singleton array i.e. result.write('src', ['the quick brown fox'], filename=filename)` - it might be nice is strings were handled as a special case.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3663/comments",
    "author": "david-waterworth",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-10-28T04:16:25Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3657,
    "title": "Add on_after_backwards callback option",
    "created_at": "2020-09-25T15:24:32Z",
    "closed_at": "2020-10-28T12:15:23Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3657",
    "body": "## 🚀 Feature\r\nAdd an on_after_backward callback option.\r\n\r\n### Motivation\r\nCurrently there are no callbacks for after the backwards step so non-essential code like logging gradients clutters the LightningModule. \r\n\r\n### Pitch\r\n\r\nExpand the callback options to include a hook for `on_after_backward()` to execute callbacks immediately after the backwards pass\r\n\r\n### Alternatives\r\n\r\nSticking my non-essential gradient logging code in my LightningModule\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3657/comments",
    "author": "rbracco",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-09-25T19:43:58Z",
        "body": "useful!"
      },
      {
        "user": "FelixLorenz",
        "created_at": "2020-09-28T11:23:35Z",
        "body": "We nearly need the same feature: `on_before_backward` as an abstract method in the `Callback` base class would be awesome!\r\n\r\nCurrently we are using the same alternative (in the `training_step`) but this couples one last bit of our custom callback with the code of the lightning module..."
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-10-01T21:47:30Z",
        "body": "maybe we should add all the optimizer-related hooks to callbacks too if it makes sense.\r\nWhat do you think @awaelchli?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-02T00:27:53Z",
        "body": "@rohitgr7 which hooks do you have in mind?\r\n@FelixLorenz it shouldn't be abstract"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-10-02T16:10:31Z",
        "body": "@awaelchli all of them? `on_after_backward`, `on_before_backward`, `on_before_zero_grad`."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-02T16:23:22Z",
        "body": "I think that would be nice to have."
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-10-02T16:37:33Z",
        "body": "okay, will add them.\r\nWaiting for more approvals to see if these hooks are good to add or not.\r\n@PyTorchLightning/core-contributors "
      }
    ]
  },
  {
    "number": 3652,
    "title": "Creation of many data module instances incurs RecursionError",
    "created_at": "2020-09-25T01:21:50Z",
    "closed_at": "2020-09-25T11:09:50Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3652",
    "body": "## 🐛 Bug\r\n\r\nThank you for a nice framework!\r\n\r\nWhen I repeated hundreds of experiments, each time with a new instance of a single LightningDataModule class, RecursionError was raised. I also found that creating data modules and calling setup() were enough to reproduce the issue.\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nPlease look at the following code sample and error messages:\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n\r\n```\r\nimport pytorch_lightning as pl\r\n\r\nclass DummyDM(pl.LightningDataModule):\r\n    def setup(self, stage=None):\r\n        pass\r\n\r\nif __name__ == \"__main__\":\r\n    MAX_ITERS = 1000\r\n    for i in range(MAX_ITERS):\r\n        try:\r\n            dm = DummyDM()\r\n            dm.setup()\r\n        except RecursionError:\r\n            print(f\"RecursionError occured in the {i}-th iteration!\")\r\n            raise\r\n```\r\n\r\n#### Error messages\r\n\r\n```\r\nRecursionError occured in the 998-th iteration!\r\nTraceback (most recent call last):\r\n  File \"test_dm.py\", line 18, in <module>\r\n    dm.setup()\r\n  File \"/workspace/src/.venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\", line 85, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/workspace/src/.venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\", line 85, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/workspace/src/.venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\", line 85, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  [Previous line repeated 995 more times]\r\n  File \"/workspace/src/.venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\", line 69, in wrapped_fn\r\n    if fn.__name__ == 'setup':\r\nRecursionError: maximum recursion depth exceeded in comparison\r\n```\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\nThe above code sample is expected to exit without any outputs.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - PytorchLightning Version: 0.9.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source): n/a\r\n - Python version: 3.8.2\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: 1080Ti",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3652/comments",
    "author": "yoshum",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-25T01:22:29Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "antoinebrl",
        "created_at": "2020-09-25T10:54:35Z",
        "body": "I faced the same issue. I opened a PR with a fix #3654."
      }
    ]
  },
  {
    "number": 3646,
    "title": "Support checkpointing for Sub-Epoch period ",
    "created_at": "2020-09-24T16:43:08Z",
    "closed_at": "2020-10-07T02:12:19Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3646",
    "body": "## Question\r\n\r\nWhen setting period to a fractional value, checkpointing doesn’t trigger correctly. Additionally I think period should default to val_check_interval, if it doesn’t already.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun any model and set checkpoint to run at a fractional value. Only the first checkpoint will be saved.\r\n\r\n### Expected behavior\r\nA checkpoint should be saved every specified period\r\n\r\n### Environment\r\n- Lighting Version: 0.9.0\r\n - PyTorch Version (e.g., 1.0): 1.6\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3646/comments",
    "author": "monney",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-09-24T22:07:50Z",
        "body": "I think you need to set `period=0`, then it could work, just looking at the code:\r\n```python\r\n        if (\r\n            self.epoch_last_check is not None\r\n            and (epoch - self.epoch_last_check) < self.period\r\n        ):\r\n            # skipping in this term\r\n            return\r\n```\r\nTry it :)\r\nperiod can only be an integer. Setting it to the val_check_interval does not make sense."
      },
      {
        "user": "monney",
        "created_at": "2020-09-24T23:42:09Z",
        "body": "@awaelchli Thanks! This works, but I find it a bit unintuitive. I didn’t get any warnings or anything trying to set it to a fractional value (the same as Val interval) so I assumed that’s what I had to do. Perhaps we could auto set this if Val interval is below 1? Or trigger a warning stating the correctly value to set in this condition?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-25T15:05:35Z",
        "body": "Yes I agree, period=0 only works because of an implementation detail and it is not meant to be used like that. It's a hack. The sub-epoch checkpointing is not supported currenlty. We're looking into that. If you're feeling lucky, give it a try and send a draft PR? :rocket: It is a tricky one though.\r\n\r\nLet's classify this as a feature requrest instead of bug?"
      },
      {
        "user": "monney",
        "created_at": "2020-09-25T15:27:25Z",
        "body": "Ill give it a look over and see if Im able to do it. Feature request sounds good, since it's working as intended"
      },
      {
        "user": "ananthsub",
        "created_at": "2020-09-27T23:39:24Z",
        "body": "@awaelchli what do you think about these options for checkpointing? I think this suite could be really helpful:\r\n- Currently supported: Checkpoint every N epochs (after validation)\r\n+ Checkpoint every N training batches\r\n+ Checkpoint after N time period (timedelta specified by users)\r\n+ Support for checkpointing on training epoch end if validation steps aren't supported"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-28T02:12:46Z",
        "body": "I think yes, these are all fine use cases.\r\nGiven that the current ModelCheckpoint callback is quite complex, it may be hard or become impossible to maintain all these options in a single class. We could consider splitting the functionality into several callbacks. A combination of these features would mean passing several callbacks to the Trainer. But then there are new challenges, like clashing filenames etc. \r\n\r\n> Support for checkpointing on training epoch end if validation steps aren't supported\r\n\r\nis that not already supported?"
      },
      {
        "user": "monney",
        "created_at": "2020-10-06T21:44:04Z",
        "body": "@awaelchli i believe this is fixed on master (At least for the case of checkpointing with sub epoch validations), since it now checks to make sure we haven’t saved on the same global step, instead of the same epoch. Can you confirm?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-06T22:14:47Z",
        "body": "```python \r\nimport torch\r\nimport pytorch_lightning as pl\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader, random_split\r\n\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\n\r\nfrom torchvision.datasets.mnist import MNIST\r\nfrom torchvision import transforms\r\n\r\n\r\nclass LitClassifier(pl.LightningModule):\r\n    def __init__(self, hidden_dim=128, learning_rate=1e-3):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\r\n        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        x = torch.relu(self.l1(x))\r\n        x = torch.relu(self.l2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('valid_loss', loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('test_loss', loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument('--hidden_dim', type=int, default=128)\r\n        parser.add_argument('--learning_rate', type=float, default=0.0001)\r\n        return parser\r\n\r\n\r\ndef cli_main():\r\n    pl.seed_everything(1234)\r\n\r\n    # ------------\r\n    # args\r\n    # ------------\r\n    parser = ArgumentParser()\r\n    parser.add_argument('--batch_size', default=32, type=int)\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    parser = LitClassifier.add_model_specific_args(parser)\r\n    args = parser.parse_args()\r\n\r\n    # ------------\r\n    # data\r\n    # ------------\r\n    dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\r\n    mnist_test = MNIST('', train=False, download=True, transform=transforms.ToTensor())\r\n    mnist_train, mnist_val = random_split(dataset, [55000, 5000])\r\n\r\n    train_loader = DataLoader(mnist_train, batch_size=args.batch_size)\r\n    val_loader = DataLoader(mnist_val, batch_size=args.batch_size)\r\n    test_loader = DataLoader(mnist_test, batch_size=args.batch_size)\r\n\r\n    # ------------\r\n    # model\r\n    # ------------\r\n    model = LitClassifier(args.hidden_dim, args.learning_rate)\r\n\r\n    # ------------\r\n    # training\r\n    # ------------\r\n    trainer = pl.Trainer.from_argparse_args(\r\n        args,\r\n        max_epochs=3,\r\n        val_check_interval=0.25,\r\n        gpus=1,\r\n        checkpoint_callback=ModelCheckpoint(\r\n            filepath=\"lightning_logs/test/{epoch:d}-{valid_loss:.2f}\",\r\n            save_top_k=-1\r\n        )\r\n    )\r\n    trainer.fit(model, train_loader, val_loader)\r\n\r\n    # ------------\r\n    # testing\r\n    # ------------\r\n    trainer.test(test_dataloaders=test_loader)\r\n\r\n\r\nif __name__ == '__main__':\r\n    cli_main()\r\n\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-06T22:15:18Z",
        "body": "Yes! I just checked it. Above is the code that I tested with\r\nval_check_interval = .25\r\nIt saves 4 checkpoints per epoch"
      },
      {
        "user": "monney",
        "created_at": "2020-10-07T02:12:19Z",
        "body": "Awesome, closing for now, the other features can likely be added separately"
      }
    ]
  },
  {
    "number": 3634,
    "title": "AttributeError: 'dict' object has no attribute 'callback_metrics' when using validation_epoch_end callbac",
    "created_at": "2020-09-23T22:46:59Z",
    "closed_at": "2020-09-25T20:03:31Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3634",
    "body": "## 🐛 Bug\r\n\r\nHello.  I am  trying to setup the early stop callback, and according to the warning that I get I need to use the validation_epoch_end callback. When I do that, I get the following error:\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n<ipython-input-25-9cbc4363b76a> in <module>()\r\n     10 \r\n     11 # Train the model ⚡\r\n---> 12 trainer.fit(model)\r\n\r\n10 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in __update_callback_metrics(self, eval_results, using_eval_result)\r\n    419             if isinstance(eval_results, list):\r\n    420                 for eval_result in eval_results:\r\n--> 421                     self.callback_metrics = eval_result.callback_metrics\r\n    422             else:\r\n    423                 self.callback_metrics = eval_results.callback_metrics\r\n\r\nAttributeError: 'dict' object has no attribute 'callback_metrics'\r\n```\r\n\r\n``` python\r\nclass MyNN(pl.LightningModule):\r\n    def __init__(self, input_size=3, seq_len=107, pred_len=68, hidden_size=50, num_layers=1, dropout=0, lr=1e-2):\r\n        super().__init__()\r\n        \r\n        self.pred_len = pred_len\r\n        \r\n        self.lr = lr\r\n        \r\n        self.rnn = nn.LSTM(\r\n            input_size=input_size, \r\n            hidden_size=hidden_size, \r\n            num_layers=num_layers, \r\n            dropout=dropout, \r\n            bidirectional=True,\r\n            batch_first=True\r\n        )\r\n        \r\n        self.linear = nn.Linear(hidden_size*2, 5)\r\n\r\n        self.example_input_array = torch.Tensor(np.zeros(input_size).reshape(1, 1, -1))\r\n    \r\n    def forward(self, X):\r\n        lstm_output, (hidden_state, cell_state) = self.rnn(X)\r\n        \r\n        labels = self.linear(lstm_output[:, :self.pred_len, :])\r\n        \r\n        return labels\r\n    \r\n    def training_step(self, batch, batch_nb):\r\n        x, y = batch\r\n        loss = scoring(self(x.float()), y.float())\r\n\r\n\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log('train_loss', loss, logger=True)\r\n        return result\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x.float())\r\n        loss = scoring(logits, y)\r\n\r\n        result = pl.EvalResult(checkpoint_on=loss)\r\n        result.log('val_loss', loss)\r\n        return result\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\r\n    \r\n    def val_dataloader(self):\r\n        return DataLoader(MyValSet(), batch_size=64)\r\n    \r\n    def train_dataloader(self):\r\n        return DataLoader(MyDataset(), batch_size=64, shuffle=True)\r\n\r\n\r\n    def training_epoch_end(self, outputs):\r\n        #  the function is called after every epoch is completed\r\n\r\n        # calculating average loss  \r\n        avg_loss = outputs[\"train_loss\"].mean()\r\n\r\n        # creating log dictionary\r\n        tensorboard_logs = {'train_loss': avg_loss}\r\n\r\n        epoch_dictionary={\r\n            # required\r\n            'train_loss': avg_loss,\r\n            \r\n            # for logging purposes\r\n            'log': tensorboard_logs}\r\n\r\n        return epoch_dictionary\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        #  the function is called after every epoch is completed\r\n\r\n        # calculating average loss  \r\n        avg_loss = outputs[\"val_loss\"].mean()\r\n\r\n        tensorboard_logs = {'val_loss': avg_loss}\r\n        \r\n        epoch_dictionary={\r\n            # required\r\n            'val_loss': avg_loss,\r\n            \r\n            # for logging purposes\r\n            'log': tensorboard_logs}\r\n\r\n        return epoch_dictionary\r\n```\r\n\r\nand my training loop is:\r\n\r\n```python\r\nLEARNING_RATE = 1e-3\r\nNUM_LAYERS = 2\r\nDROPOUT = 0.1\r\nHIDDEN_SIZE = 100\r\nEPOCHS = 100\r\n\r\n# Initialize a trainer\r\ntrainer = pl.Trainer(gpus=1, max_epochs=EPOCHS, progress_bar_refresh_rate=20, early_stop_callback=True, auto_lr_find=True)\r\nmodel = MyNN(num_layers=NUM_LAYERS, dropout=DROPOUT)\r\n\r\n# Train the model ⚡\r\ntrainer.fit(model)\r\n```\r\n\r\nI am using Google Colab, with the following versions:\r\n```\r\npytorch-lightning==0.9.0\r\ntorch==1.6.0+cu101\r\n```\r\n\r\nAm I doing something wrong, or what is the issue here? \r\nThank you! :)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3634/comments",
    "author": "djrmarques",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-23T22:47:45Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-24T22:34:42Z",
        "body": "Pretty sure the problem is, you cannot mix results objects in your step methods with dict in your epoch_end methods. \r\nUse either dicts everywhere or results everywhere, but not both. Let me know if that solves your problem. \r\n\r\nYou mention colab, if you need further help, mind sharing the colab link so we can have a look help you better."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-24T22:53:24Z",
        "body": "oh, sorry, did not read the first sentence in your message. \r\nFor early stopping, use this\r\n\r\n```python\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x.float())\r\n        loss = scoring(logits, y)\r\n\r\n        result = pl.EvalResult(checkpoint_on=loss, early_stop_on=loss)  # <--- add this\r\n        result.log('val_loss', loss)\r\n        return result\r\n```\r\nThere should be no need for the validation_epoch_end, you can savely remove it. the validation loss will be reduced and logged automatically at the end of epoch."
      },
      {
        "user": "djrmarques",
        "created_at": "2020-09-25T20:03:31Z",
        "body": "So I ended up putting it all into dictionaries and it worked. \r\n\r\nBut before that I tried to use the results object in both the validation and train set, and the model was running, but for some reason the train_loss was not logging on tensorboard, but maybe I was doing something wrong. I will leave it for now like this, because I want to finish my model, but after that I will try to set up the results object and see if all goes well. \r\n\r\nThank you! "
      }
    ]
  },
  {
    "number": 3628,
    "title": "automatically copy state-dict when using ddp",
    "created_at": "2020-09-23T14:17:55Z",
    "closed_at": "2020-09-24T13:51:35Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3628",
    "body": "## 🚀 Feature\r\nCopy model state-dict from rank 0 process to other processes when using ddp\r\n\r\n### Motivation\r\nThis would mean that the user does not need to worry about initializing models with the same weights\r\n\r\n### Alternatives\r\nAlternatively lightning could at least check if the weights are the same and if not warn the user / thrown an exception\r\n\r\n\r\nNot sure if this is possible and how easy it can be accomplished, but I could imagine that this could be a source for errors.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3628/comments",
    "author": "AljoSt",
    "comments": [
      {
        "user": "AljoSt",
        "created_at": "2020-09-23T17:20:38Z",
        "body": "hm I suppose just setting the seed would be enough? Or can there be situations where it's not the case?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-23T22:40:26Z",
        "body": "Yes exactly, in ddp we always need to set the seed. \r\nAlthough what @AljoSt suggests may make sense on first thought, I do not believe that simply copying the model state dict is enough. Why only that? The assumption here is that the only non deterministic state is the model, but that is simply not true in general. We may have random state in dataset, imported libraries, and other external influences. "
      },
      {
        "user": "AljoSt",
        "created_at": "2020-09-24T13:51:35Z",
        "body": "yes, good point. I'll close this then!"
      }
    ]
  },
  {
    "number": 3626,
    "title": "#3598 does not allow monitoring tensors logged via `TrainResult`",
    "created_at": "2020-09-23T13:34:01Z",
    "closed_at": "2020-09-25T01:53:48Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3626",
    "body": "## 🐛 Bug\r\n\r\n### Code sample\r\n\r\n```python\r\n@pytest.mark.parametrize(\"monitor\", [\"tr_foo\", \"tr_bar\", \"va_foo\", \"va_bar\"])\r\ndef test(tmpdir, monitor):\r\n    model = DeterministicModel()\r\n\r\n    def training_step(batch, batch_idx):\r\n        acc = model.step(batch, batch_idx)\r\n        result = TrainResult(minimize=acc)\r\n        result.log(\"tr_foo\", torch.randn(1), on_step=False, on_epoch=True)\r\n        result.log(\"tr_bar\", torch.randn(1), on_step=False, on_epoch=True)\r\n        return result\r\n\r\n    def validation_step(*args, **kwargs):\r\n        result = EvalResult()\r\n        result.log(\"va_foo\", torch.randn(1), on_step=False, on_epoch=True)\r\n        result.log(\"va_bar\", torch.randn(1), on_step=False, on_epoch=True)\r\n        return result\r\n\r\n    model.training_step = training_step\r\n    model.validation_step = validation_step\r\n    model.validation_step_end = None\r\n    model.validation_epoch_end = None\r\n\r\n    trainer = Trainer(\r\n        default_root_dir=tmpdir,\r\n        early_stop_callback=EarlyStopping(monitor=monitor),\r\n        checkpoint_callback=ModelCheckpoint(monitor=monitor),\r\n        limit_train_batches=3,\r\n        limit_val_batches=3,\r\n        max_epochs=2,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model)\r\n```\r\n\r\n`tr_foo` and `tr_bar` fail. `va_foo` and `va_bar` work.\r\n\r\n### Expected behavior\r\n\r\nNo failure and callbacks correctly monitor their `monitor`\r\n\r\n### Environment\r\n\r\nCurrent master\r\n\r\n@williamFalcon ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3626/comments",
    "author": "carmocca",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-09-23T13:37:43Z",
        "body": "amazing. thanks for finding. can you submit a PR with the tests? i’ll make the fixes on that PR"
      },
      {
        "user": "carmocca",
        "created_at": "2020-09-23T14:13:59Z",
        "body": "Done"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-09-25T01:55:09Z",
        "body": "please see the comments in the PR... this is not a valid use.\r\n\r\nIf you notice, you don't log anything until the end of the epoch... but yet you ask to monitor a value WITHIN a training epoch. This is of course a contradiction.\r\n\r\nIf you want to monitor something within an epoch, then log that value within the epoch. Otherwise monitor epoch metrics"
      },
      {
        "user": "carmocca",
        "created_at": "2020-09-25T02:25:28Z",
        "body": "So, if I understood it correctly, is it because training has this structure?\r\n\r\n```python\r\nfor epoch in epochs\r\n    for train_batch in train_dataloader():\r\n        ...\r\n        set_train_step_metrics()\r\n\r\n        if should_check_val:\r\n            for val_batch in val_dataloader():\r\n                ...\r\n                set_val_step_metrics()\r\n\r\n            # validation is over\r\n            set_val_epoch_metrics()\r\n      \r\n            # training epoch metrics are set\r\n            # later so they cannot be monitored yet\r\n            modelcheckpoints.save()\r\n\r\n    # training is over\r\n    # they get set here\r\n    # but checkpoints have already been saved\r\n    set_train_epoch_metrics()\r\n```"
      }
    ]
  },
  {
    "number": 3624,
    "title": "Support all_gather usecase for sync_ddp ",
    "created_at": "2020-09-23T12:50:26Z",
    "closed_at": "2020-10-31T10:57:39Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3624",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nall_gather function support for EvalResult sync_dist option.\r\n\r\n### Motivation\r\nCurrent sync_dist implementation is for manipulating output tensors from each validation loop of several GPUs. ('mean', 'sum')\r\nThere are use cases which need all_gather function rather than all_reduce function.\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3624/comments",
    "author": "oyj0594",
    "comments": [
      {
        "user": "oyj0594",
        "created_at": "2020-09-23T12:50:52Z",
        "body": "@SkafteNicki "
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-23T12:51:16Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-09-23T13:40:53Z",
        "body": "@oyj0594 do you have time for submitting a PR yourself?"
      },
      {
        "user": "oyj0594",
        "created_at": "2020-09-24T08:54:47Z",
        "body": "@SkafteNicki Okay let me work on this. If there is problem with my schedule, I will contact you."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-24T10:54:20Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3619,
    "title": "ModelCheckpoint period should not always save on the first epoch",
    "created_at": "2020-09-23T02:00:38Z",
    "closed_at": "2020-09-29T13:36:46Z",
    "labels": [
      "bug",
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3619",
    "body": "## 🚀 Feature\r\n\r\n`period` should work so:\r\n\r\n```\r\n# ModelCheckpoint on_validation_end\r\nif (epoch + 1) % period:\r\n    # do not save\r\n    return\r\n```\r\n\r\ne.g:\r\n- `period == 1`: save on epoch 0, 1, 2...\r\n- `period == 2`: save on epoch 1, 3, 5...\r\n- `period == 3`: save on epoch 2, 5, 8...\r\n\r\ncurrently, it always runs on the first epoch and then runs every `period` epochs\r\n\r\ne.g:\r\n- `period == 1`: save on epoch 0, 1, 2...\r\n- `period == 2`: save on epoch 0, 2, 4...\r\n- `period == 3`: save on epoch 0, 3, 6...\r\n\r\nThis would also allow having `period = 0` which would never save, just as `save_top_k = 0`\r\n\r\n### Motivation\r\n\r\nI want to save a checkpoint every `period` epochs but current behaviour forces to always save on the first one.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3619/comments",
    "author": "carmocca",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-09-23T02:34:00Z",
        "body": "makes sense. submit a PR?"
      },
      {
        "user": "ananthsub",
        "created_at": "2020-09-23T17:05:57Z",
        "body": "@williamFalcon why is epoch 0-indexed in one place and 1-indexed in other places? can it be consistent across lightning instead of these +1s? I think the same issue affects batch indices too"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-09-23T17:43:14Z",
        "body": "yes, we did a refactor a while to make everything zero indexed. where did we miss spots?"
      }
    ]
  },
  {
    "number": 3585,
    "title": "Trainer: Separate framework options from backend options",
    "created_at": "2020-09-21T15:29:54Z",
    "closed_at": "2020-11-16T06:17:02Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3585",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nStop mixing framework and backend options in the Trainer's constructor.\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nI find it confusing both as a user and a backend implementer because it's not obvious which options affect which backend.\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nThe backend options could be passed as a separate, specific object:\r\n```\r\ntrainer = Trainer( default_root_dir=\".\", pl.accelerators.GPU(gpus=8))\r\ntrainer = Trainer(  default_root_dir=\".\", pl.accelerators.TPU(tpu_cores=8))\r\n```\r\nThis would remove the need for `select_accelerator` or at least simplify it by making it a simple \r\n```\r\nif isinstance(opts, pl.accelerators.GPU):\r\n...\r\n```\r\n\r\nThoughts ?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3585/comments",
    "author": "AnthonyBarbier",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-21T15:30:40Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-09-21T16:28:05Z",
        "body": "@williamFalcon what do you think about this one?"
      },
      {
        "user": "lezwon",
        "created_at": "2020-10-10T03:54:54Z",
        "body": "This seems like a good approach. Also selecting the accelerators should be mutually exclusive imo."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-09T05:01:44Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3582,
    "title": "Out of memory when trainer.save_checkpoint(\"example.ckpt\")",
    "created_at": "2020-09-21T12:39:05Z",
    "closed_at": "2020-09-22T04:54:19Z",
    "labels": [
      "bug",
      "help wanted",
      "checkpointing"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3582",
    "body": "The sbatch session crashes and I get the following error when I include trainer.save_checkpoint(\"example.ckpt\") in my code.\r\n\r\n> /var/spool/slurmd/job220424/slurm_script: line 15: 39865 Killed                  python roco_train_mlm_lightning.py --run_name debug --precision 16 --mlm_prob 0.15\r\n> slurmstepd: error: Detected 3 oom-kill event(s) in step 220424.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.\r\n\r\nSame thing happens when I run the code in notebook on the remoteserver. The kernel dies. Please help. Thank you. \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3582/comments",
    "author": "VirajBagal",
    "comments": [
      {
        "user": "ananyahjha93",
        "created_at": "2020-09-21T16:27:02Z",
        "body": "@VirajBagal which version of lightning are you using? Currently I am not able to reproduce this with 0.9.0."
      },
      {
        "user": "VirajBagal",
        "created_at": "2020-09-22T04:19:41Z",
        "body": "@ananyahjha93  Let me give some details.\r\n\r\n```\r\n    def validation_step(self, batch, batch_idx):\r\n\r\n        loss, acc = self.shared_step(batch, batch_idx)\r\n        result = pl.EvalResult(loss)\r\n\r\n        container = {'val_loss': loss, 'val_acc': acc}        \r\n        result.log_dict(container, on_step = True, on_epoch = True, prog_bar = True, logger = True)\r\n\r\n        return result\r\n```\r\nNow, I want to save checkpoints/modelweights at the end of every validation epoch if my 'total val loss' is smaller than previous 'best val loss'. How do I do that?  The output is following:\r\n\r\n```\r\n\r\n```\r\nEpoch 0:  87%|████████▋ | 13/15 [00:05<00:00,  2.56it/s, loss=2.698, v_num=08zk, step_train_loss=1.17, step_train_acc=0]\r\nEpoch 0:  93%|█████████▎| 14/15 [00:05<00:00,  2.66it/s, loss=2.698, v_num=08zk, step_train_loss=1.17, step_train_acc=0]\r\nEpoch 0: 100%|██████████| 15/15 [00:05<00:00,  2.58it/s, loss=2.698, v_num=08zk, step_train_loss=1.17, step_train_acc=0, step_val_loss=1.08, step_val_acc=0, epoch_val_loss=1.11, epoch_val_acc=0]\r\n```\r\n```\r\n\r\nI tried manually ModelCheckpoint as well:\r\n\r\n```\r\ncheckpoint_callback = ModelCheckpoint(\r\n    filepath=os.getcwd(),\r\n    save_top_k=1,\r\n    verbose=True,\r\n    monitor='epoch_val_loss',\r\n    mode='min',\r\n    prefix=''\r\n)\r\n```\r\n\r\nBut it didnt help. I get the following error: \r\n\r\n> /home/viraj.bagal/anaconda3/envs/medvqa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: \r\n>                     When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the\r\n>                     'monitor' key of ModelCheckpoint has no effect.\r\n>                     Remove ModelCheckpoint(monitor='epoch_val_loss) to fix')\r\n>                 \r\n>   warnings.warn(*args, **kwargs)\r\n> /home/viraj.bagal/anaconda3/envs/medvqa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: The metric you returned None must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of epoch_val_loss in validation_epoch_end()?\r\n>   warnings.warn(*args, **kwargs)\r\n> /home/viraj.bagal/anaconda3/envs/medvqa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: Can save best model only with epoch_val_loss available, skipping.\r\n>   warnings.warn(*args, **kwargs)\r\n"
      },
      {
        "user": "VirajBagal",
        "created_at": "2020-09-22T04:53:48Z",
        "body": "Sorry. Its my mistake. This solved the issue.\r\n`\r\nresult = pl.EvalResult(checkpoint_on = loss)`"
      }
    ]
  },
  {
    "number": 3570,
    "title": "[Metrics] Top K Accuracy",
    "created_at": "2020-09-20T13:28:38Z",
    "closed_at": "2020-10-30T23:22:23Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3570",
    "body": "## 🚀 Feature\r\nWe currently have accuracy implementation which is top-1 Accuracy to be formal.\r\n\r\nCan we provide a generic top-k Accuracy Implementation?\r\n\r\n### Motivation\r\n\r\nQuite often in classification tasks, we need top-3 Accuracy and top-5 Accuracy and so on. It depends on the tasks. These stats are often reported in standard papers for classification tasks be it NLP or Computer Vision.\r\n\r\n### Pitch\r\n\r\nI just checked the implementation of accuracy we have currently, can we add an additional parameter `topk = 1`.\r\n\r\nHere is an implementation of top-k accuracy which I use often. This code is used in imagenet classification tasks, I have no idea if it will work for other cases. \r\n\r\n```\r\ndef accuracy(output, target, topk=(1,)):\r\n    \"\"\"\r\nComputes the accuracy over the k top predictions for the specified values of k\r\noutput and targets are just tensors.\r\n\"\"\"\r\n    maxk = max(topk)\r\n    batch_size = target.size(0)\r\n    _, pred = output.topk(maxk, 1, True, True)\r\n    pred = pred.t()\r\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\r\n    return [correct[:k].view(-1).float().sum(0) * 100.0 / batch_size for k in topk]\r\n```\r\n\r\n### Alternatives\r\n\r\nNo currently found, will dig into this if needed. Probably above implementation is not according to API lightning has.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3570/comments",
    "author": "oke-aditya",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-09-23T06:55:02Z",
        "body": "@oke-aditya this sounds like a good addition. Would you be up for doing a PR (please note that we are trying to fix bugs related to the metric package at the moment)?\r\nMaybe it would be better to have a separate `topk_accuracy` metric as the output/predictions for this metric needs to class logits/probabilities whereas we for standard accuracy predictions can also be predicted class labels.\r\n\r\n"
      },
      {
        "user": "oke-aditya",
        "created_at": "2020-09-23T07:02:06Z",
        "body": "I will try, it is bit tough and new for me though."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-09-23T07:18:05Z",
        "body": "alright ping me if you need help "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-23T13:33:18Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3532,
    "title": "Support automatic torchscript checkpointing.",
    "created_at": "2020-09-17T19:56:05Z",
    "closed_at": "2021-11-21T07:35:04Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "let's do it!"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3532",
    "body": "## 🚀 Feature\r\n\r\nSupport exporting to TorchScript natively in the ModelCheckpoint callback. This would be a natural follow-up to #3080. \r\n\r\n### Motivation\r\n\r\nIt's common for users to rely on checkpoints (eg, usually last, but sometimes other intermediate models especially if using model ensembles or picking the best model the last k models). Frequently these models need to be served in a non-Python environment, meaning users have to convert their checkpoints to TorchScript.\r\n\r\nWith the implementation of #3080 it seems natural to add more support for TorchScript throughout the Lightning framework. In particular, we should allow users to export TorchScript models easily in checkpoints.\r\n\r\n### Pitch\r\n\r\nWe can rely on the `to_torchscript` method in LightningModule's to access a TorchScript version of the model when checkpointing. We would then provide an parameter in the ModelCheckpoint callback \r\n\r\n```python\r\nclass ModelCheckpoint(Callback):\r\n    r\"\"\"\r\n    Save the model after every epoch if it improves.\r\n\r\n    After training finishes, use :attr:`best_model_path` to retrieve the path to the\r\n    best checkpoint file and :attr:`best_model_score` to retrieve its score.\r\n\r\n    Args:\r\n      ...\r\n      save_torchscript: When set, each exported model includes a Torchscript\r\n            version. Default: ``False``.\r\n      ...\r\n    \"\"\"       \r\n```\r\n\r\n### Additional context\r\n\r\nSee #3080. \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3532/comments",
    "author": "kandluis",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-17T19:56:54Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-09-18T08:26:54Z",
        "body": "@kandluis do you have any estimate about time delay with exporting the model also to TorchScript?\r\njust thinking that in such case you are probably fine just with exporting the best one or k-best, right?"
      },
      {
        "user": "ananthsub",
        "created_at": "2020-09-21T01:17:32Z",
        "body": "This would happen whenever we call save_model. That means we'd honor the checkpoint callback's args for save_top_k for torchscript as well. There'd be a 1:1 mapping between checkpoints and models exported to torchscript. That also means we'll delete saved torchsrcipt models if they're no longer the top-K"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T15:43:44Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "keunwoochoi",
        "created_at": "2021-10-11T03:14:25Z",
        "body": "+1 for this!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-11-12T03:11:56Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "rusmux",
        "created_at": "2023-03-01T23:00:26Z",
        "body": "@Borda I think it would be nice to implement this feature. This would spare developers from writing their own callbacks. Currently, to convert a checkpoint to another format, such as ONNX, one would need to create an exact instance of the model used for training, with the same initial arguments. And after that, load the checkpoint weights into it. With checkpoints in the TorchScript format, there is no need for that. "
      },
      {
        "user": "pfeatherstone",
        "created_at": "2023-04-13T09:26:37Z",
        "body": "I would also love this feature. I'm currently using the following:\r\n```\r\nclass JitCheckpoint(Checkpoint):\r\n    def __init__(self):\r\n        self.lastLoss = 100000\r\n\r\n    def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\r\n        if trainer.is_global_zero:\r\n            assert \"loss/val\" in trainer.callback_metrics, \"monitor not in callback_metrics\"\r\n            assert \"acc/val\" in trainer.callback_metrics, \"monitor not in callback_metrics\"\r\n            loss = trainer.callback_metrics[\"loss/val\"].item()\r\n            acc  = trainer.callback_metrics[\"acc/val\"].item()\r\n            if loss < self.lastLoss:\r\n                print(\"Saving checkpoint last loss {} new loss {}\".format(self.lastLoss, loss))\r\n                self.lastLoss = loss\r\n                ckpdir        = trainer.log_dir + \"/checkpoints/\"\r\n                filePath      = ckpdir + \"epoch={}_loss={:.4f}_acc={:.2f}\".format(trainer.current_epoch, loss, acc)\r\n                jitPath       = filePath + '.pt'\r\n                onnxPath      = filePath + '.onnx'\r\n                os.makedirs(ckpdir, exist_ok=True)\r\n\r\n                input = torch.randn(2, 20*1024, 2)\r\n                pl_module.to_torchscript(file_path=jitPath, method='trace', example_inputs=[input])\r\n                pl_module.to_onnx(file_path=onnxPath, input_sample=input)\r\n        trainer.strategy.barrier()\r\n```\r\n\r\nHowever my code hangs while all my GPUS are throttling at 100% doing nothing.\r\nI tried adding ``` trainer.strategy.barrier()``` but that had no effect. "
      }
    ]
  },
  {
    "number": 3518,
    "title": "Logging to progress bar doesn't work when calling trainer.test()",
    "created_at": "2020-09-16T09:06:12Z",
    "closed_at": "2020-09-16T12:46:48Z",
    "labels": [
      "bug",
      "help wanted",
      "working as intended"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3518",
    "body": "## 🐛 Bug\r\n\r\nThe metric logged by `EvalResult.log()` doesn't show up in the progress bar when setting `prog_bar=True`.\r\n\r\n#### Code sample\r\n```\r\nimport time\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\nimport pytorch_lightning as pl\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.layer = nn.Linear(64, 1)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        X, y = batch\r\n        loss = F.mse_loss(self.forward(X), y)\r\n\r\n        result = pl.EvalResult()\r\n        result.log(\"test_loss\", loss, prog_bar=True)\r\n        time.sleep(1)\r\n        return result\r\n\r\ntest_ds = TensorDataset(torch.randn(64 * 5, 64), torch.randn(64 * 5, 1))\r\ntest_loader = DataLoader(test_ds, batch_size=64)\r\n\r\nmodel = MyModel()\r\ntrainer = pl.Trainer(logger=False)\r\ntrainer.test(model, test_dataloaders=test_loader)\r\n```\r\n\r\n### Expected behavior\r\n\r\n`test_loss` shows up in the progress bar when running the above script.\r\n\r\n### Environment\r\n\r\n - PyTorch Version: 1.6\r\n - OS: Linux\r\n - How you installed PyTorch: pip\r\n - Python version: 3.8.5",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3518/comments",
    "author": "minhduc0711",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-09-16T10:47:14Z",
        "body": "logging metrics to the progress bar when running over the test or validation set doesn't make sense. You should compute the metric over the whole dataset and report it at the end."
      },
      {
        "user": "minhduc0711",
        "created_at": "2020-09-16T15:04:40Z",
        "body": "I wanted to quickly check if the batch testing results were sensible or not, so I think that's one possible use case.\r\n\r\nBut if this is by design, then why even include `on_step` and `on_epoch` in `EvalResult().log`? What are the intended behaviors for these parameters?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-16T15:09:05Z",
        "body": "@williamFalcon "
      },
      {
        "user": "dragondx",
        "created_at": "2021-12-06T01:02:46Z",
        "body": "Any ways to customize this behavior?"
      }
    ]
  },
  {
    "number": 3511,
    "title": "handle aggregation of different batches",
    "created_at": "2020-09-15T19:19:37Z",
    "closed_at": "2020-11-21T03:17:30Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3511",
    "body": "## 🚀 Feature\r\n\r\nresolve handling the aggregation of different batches (for classification there are 'micro', 'macro' and 'weighted' for example) and include these in the class-based interface\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3511/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-14T18:17:53Z",
        "body": "I guess this can be close with the new interface in place"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-14T03:01:51Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3510,
    "title": "split metric in pre- and post- sync/reduce computation steps",
    "created_at": "2020-09-15T19:07:37Z",
    "closed_at": "2020-10-20T02:09:20Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3510",
    "body": "## 🚀 Feature\r\n\r\nsplit metric in pre- and post- sync/reduce computation steps\r\ndecide what to reduce how (usually, this resolves in gathering results of the pre-sync outputs and aggregating them) \r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3510/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-14T18:18:09Z",
        "body": "I guess this can be close with the new interface in place"
      }
    ]
  },
  {
    "number": 3466,
    "title": "Logging Intervals and Evaluation Intervals",
    "created_at": "2020-09-11T18:31:10Z",
    "closed_at": "2020-09-30T10:26:28Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3466",
    "body": "## 🚀 Feature\r\nLogging and validation intervals should be performed using the global step.\r\n\r\n### Motivation\r\nCurrently, the row_log_interval and val_check_interval do not work as intended. If my batch size is large or my training set size is small, the number of batches per epoch is also small. Row log interval and validation check interval seem to use the steps within the epoch, rather than the global step count. \r\n\r\nThis means that logging does not occur if row_log_interval > num batches per epoch. For the validation check interval, the trainer throws an exception.\r\n\r\n### Pitch\r\nBoth intervals should rely on the global step count  or count steps across epochs to solve the issues mentioned above.\r\n\r\n### Alternatives\r\nBefore initializing the trainer, I've used the dataloader/dataset to calculate the number of batches per epoch, and then used check_val_every_n_epochs. However this is not ideal if the get_dataloader is baked into the lightning module. It also does not allow for fine grain control over when to run validation.\r\n\r\n### Additional context\r\nIt might also be nice to have val_check_interval based on max_steps. For example, if we have max_steps=10000 and want to validate ten times through training, we could set val_check_interval=0.1 or equivalently, val_check_interval=1000",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3466/comments",
    "author": "nick-fung",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-09-12T13:26:21Z",
        "body": "Does it make sense to base logging intervals on global_step instead of epoch batch index @PyTorchLightning/core-contributors ?"
      }
    ]
  },
  {
    "number": 3451,
    "title": "Very slow training on SLURM cluster",
    "created_at": "2020-09-10T20:42:20Z",
    "closed_at": "2020-09-23T12:30:00Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3451",
    "body": "## 🐛 Bug\r\n\r\nWhen switching from my local machine (old and slow 960m laptop GPU) to a SLURM cluster with Titan X GPU's, I see a significant drop in performance from 4.71it/s to 7s/it (!) - so basically training becomes unusably slow.\r\n\r\n### To Reproduce\r\n1. Take a model and submit it as a SLURM job. Happy to provide further details if requested, not sure what is relevant here.\r\n\r\n```\r\n2020-09-10 22:11:58,979 : lightning    : INFO       GPU available: True, used: True\r\n2020-09-10 22:11:58,979 : lightning    : INFO       TPU available: False, using: 0 TPU cores\r\n2020-09-10 22:11:58,979 : lightning    : INFO       CUDA_VISIBLE_DEVICES: [0,1]\r\n2020-09-10 22:12:01,698 : lightning    : INFO       Set SLURM handle signals.\r\nEpoch 1:  40%|███▉      | 150/377 [19:11<29:02,  7.68s/it, loss=2.401, v_num=19]\r\n```\r\n\r\n#### Code sample\r\nThe is setup as follows\r\n```python\r\nlogger = TensorBoardLogger(str(DATA_DIR_PATH / 'lightning_logs'), name=Path(__file__).stem)\r\ntrainer_kwargs = {'logger': logger,\r\n                  'default_root_dir': str(DATA_DIR_PATH / 'lightning_logs'),\r\n                  'val_check_interval': 0.2,  # check (1 / value) * times per train epoch\r\n                  'gpus': 1,\r\n                  # 'distributed_backend': 'ddp',  # checked with ddp enabled, 2 GPUs, multiple workers etc., same slow behaviour\r\n                  'fast_dev_run': False}\r\ntrainer = pl.Trainer(**trainer_kwargs)\r\n```\r\n\r\n### Environment\r\n* Packages:\r\n\t- numpy:                 1.18.1\r\n\t- pyTorch_debug:   False\r\n\t- pyTorch_version: 1.4.0\r\n\t- pytorch-lightning: 0.8.5\r\n\t- tensorboard:        1.15.0\r\n\t- tqdm:                   4.43.0\r\n\r\n - PyTorch Version (e.g., 1.0): \r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.7.9\r\n - CUDA/cuDNN version: cudatoolkit 10.0\r\n - GPU models and configuration: TitanX (on SLURM cluster)",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3451/comments",
    "author": "matthaeusheer",
    "comments": [
      {
        "user": "bartonp2",
        "created_at": "2020-09-17T11:17:15Z",
        "body": "I have the same problem on a cluster with LSF. Basic profiling shows most time is spent in the forwards and backwards pass, as well as the optimisation step. Furthermore, checking GPU utilisation appears to be 0% even though pytorch lightning found the gpu device and said it was using it."
      },
      {
        "user": "bartonp2",
        "created_at": "2020-09-17T12:46:52Z",
        "body": "For me the problem turned out to the setting of the  'distributed_backend' option in the trainer. I had set this to 'None' for testing purposes through the command line. Removing this command line option or changing it back to 'ddp' solved the issue. GPU utilisation is now back up."
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-22T18:51:31Z",
        "body": "@matthaeusheer were you able to solve the issue?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-09-23T12:30:00Z",
        "body": "this looks like user error. glad you got it fixed!"
      }
    ]
  },
  {
    "number": 3430,
    "title": "Issue with trainer.test in \"ddp\" distributed mode",
    "created_at": "2020-09-09T20:36:18Z",
    "closed_at": "2020-09-10T16:21:19Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3430",
    "body": "Hi -\r\n\r\nI have the following pseudo code workflow:\r\n\r\n> trainer = Trainer(distributed_backend='ddp', ...)\r\nmodel = new custom LightningModule\r\ntrainer.fit(model, ...)\r\nmodel.freeze()\r\ntrain.test(model, ...)\r\n\r\nThe error that I get is this:\r\n\r\n`AssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.`\r\n\r\nWhat is the best way to address this?\r\n\r\nThanks very much,\r\nGriffin\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3430/comments",
    "author": "griff4692",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-09-09T20:51:45Z",
        "body": "That is a PyTorch `AssertionError`. Try it without calling `model.freeze()`?"
      },
      {
        "user": "griff4692",
        "created_at": "2020-09-16T18:01:22Z",
        "body": "Thanks! this worked"
      }
    ]
  },
  {
    "number": 3426,
    "title": "Model checkpoint not saving hyperparameters correctly",
    "created_at": "2020-09-09T17:13:40Z",
    "closed_at": "2020-09-09T20:44:07Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3426",
    "body": "When using the ModelCheckpoint, my hyperparameters are not being saved with the checkpoints.  So I get an AttributeError when attempting to load from checkpoints.\r\n\r\nTo reproduce:\r\n```\r\nimport pytorch_lightning as pl\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport argparse\r\nfrom bunch import Bunch\r\n\r\nimport pytorch_lightning as pl\r\nclass LitModel(pl.LightningModule):\r\n\r\n    def __init__(self, args):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n        print('args:', args)\r\n        print(args.to_print)\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return pl.TrainResult(loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.02)\r\n\r\n    \r\ntrain_loader = DataLoader(MNIST(os.getcwd(), download=True, transform=transforms.ToTensor()))\r\ncheckpoint_callback = pl.callbacks.ModelCheckpoint(\r\n        os.path.join(os.getcwd(), 'chkpts'),\r\n        save_top_k=1,\r\n        verbose=True,\r\n        monitor='loss',\r\n        mode='min'\r\n    )\r\ntrainer = pl.Trainer(checkpoint_callback=checkpoint_callback,\r\n                    train_percent_check=0.1,\r\n                    val_percent_check=0,\r\n                    max_epochs=1)\r\n\r\nhparams = argparse.Namespace()\r\nhparams.to_print = 'foo'\r\nmodel = LitModel(hparams)\r\n\r\ntrainer.fit(model, train_loader)\r\n\r\nmod = LitModel.load_from_checkpoint(ckpt_path)\r\n```\r\n\r\nProduces the following Error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-38-868d05212321> in <module>\r\n     49 trainer.fit(model, train_loader)\r\n     50 \r\n---> 51 mod = LitModel.load_from_checkpoint(ckpt_path)\r\n\r\n~/miniconda3/envs/camtraps/lib/python3.6/site-packages/pytorch_lightning/core/saving.py in load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, strict, *args, **kwargs)\r\n    151         checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY].update(kwargs)\r\n    152 \r\n--> 153         model = cls._load_model_state(checkpoint, *args, strict=strict, **kwargs)\r\n    154         return model\r\n    155 \r\n\r\n~/miniconda3/envs/camtraps/lib/python3.6/site-packages/pytorch_lightning/core/saving.py in _load_model_state(cls, checkpoint, strict, *cls_args, **cls_kwargs)\r\n    188             cls_args, cls_kwargs = [], {}\r\n    189 \r\n--> 190         model = cls(*cls_args, **cls_kwargs)\r\n    191         # load the state_dict on the model automatically\r\n    192         model.load_state_dict(checkpoint['state_dict'], strict=strict)\r\n\r\n<ipython-input-38-868d05212321> in __init__(self, args)\r\n     15         self.l1 = torch.nn.Linear(28 * 28, 10)\r\n     16         print('args:', args)\r\n---> 17         print(args.to_print)\r\n     18 \r\n     19     def forward(self, x):\r\n\r\nAttributeError: 'dict' object has no attribute 'to_print'\r\n```\r\n\r\nThe print statements indicate that `args` is an empty dict when attempting to load from checkpoint. \r\n\r\nWhen inspecting the checkpoint\r\n```\r\nckpt_path = os.path.join(os.getcwd(), '_ckpt_epoch_0.ckpt')\r\nckpt = torch.load(ckpt_path)\r\nprint(ckpt.keys())\r\n```\r\n\r\nI get the following:\r\n```\r\ndict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'checkpoint_callback_best_model_score', 'checkpoint_callback_best_model_path', 'optimizer_states', 'lr_schedulers', 'state_dict'])\r\n```\r\n\r\nMy understanding is there should be a `hyper_parameters` in the checkpoint.\r\n\r\n\r\nSystem:\r\n- PyTorch Version 1.3.1\r\n- pytorch-lightning: 0.9.0 installed conda\r\n- OS: Ubuntu 18.04\r\n- Python 3.6",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3426/comments",
    "author": "davidwhealey",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-09T17:14:23Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-09T19:29:10Z",
        "body": "you need to call `self.save_hyperparameters()` in `__init__` to make it work."
      },
      {
        "user": "davidwhealey",
        "created_at": "2020-09-09T20:44:04Z",
        "body": "Worked, thank you!"
      }
    ]
  },
  {
    "number": 3403,
    "title": "Iterations completing out of order (possibly) in ddp with torchelastic?",
    "created_at": "2020-09-08T21:15:40Z",
    "closed_at": "2020-10-03T18:05:33Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author",
      "distributed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3403",
    "body": "This might be bug or might be expected.  I'm running a pytorchlightning with torchelastic and ddp.  I'm noticing the iterations are being dumped out of order (below iteration 632 preceeds iteration 574).  This could be due to delays in parallel writing... or perhaps just issues in logging.  Is this expected behavior?\r\n```\r\nValidating: 60it [00:21,  3.61it/s]\u001b[A\r\nEpoch 26: : 632it [08:13,  1.28it/s, loss=0.111, v_num=0]\r\n\r\nValidating: 62it [00:22,  4.62it/s]\u001b[A\r\nValidating: 0it [00:00, ?it/s]\u001b[A\r\nEpoch 26: : 572it [07:51,  1.21it/s, loss=0.111, v_num=0]\r\n\r\nValidating: 2it [00:00, 18.62it/s]\u001b[A\r\nEpoch 26: : 574it [07:52,  1.22it/s, loss=0.111, v_num=0]\r\n```\r\nRunning with 6 gpus in ddp.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3403/comments",
    "author": "jloveric",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-09-09T11:43:40Z",
        "body": "This happens with torchelastic only?\r\ncan we check that the rank is correctly set, by printing `trainer.global_rank` somewhere in the training_step for example? I suspect these progress bars are from different ranks. It should only show on rank 0.\r\nwhich PL version?\r\n"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-16T18:26:04Z",
        "body": "@jloveric mind giving more details?"
      }
    ]
  },
  {
    "number": 3389,
    "title": "NeptuneLogger doesn't support fetching active experiment.",
    "created_at": "2020-09-07T20:44:13Z",
    "closed_at": "2020-10-28T21:27:19Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3389",
    "body": "Hi!\r\n### 🚀 Feature\r\nCurrently NeptuneLogger doesn't support reconnecting to active experiment. Each initialization always creates a new one and there is no option to pass existing experiment id. \r\nHowever, this feature is already supported by Neptune library and actually implemented in _create_or_get_experiment() method inside NeptuneLogger class. \r\n\r\n### Proposal\r\nMake it possible to pass optional argument **experiment_id** to class initialization. \r\nWith this replacement _create_or_get_experiment() function will be able to fetch existing experiment during class initialization.\r\n\r\nIf you don't mind i can make a PR with this small enhancement.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3389/comments",
    "author": "blatr",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-07T20:44:52Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-09-08T21:09:47Z",
        "body": "@jakubczakon mind have look? 🐰 "
      },
      {
        "user": "pitercl",
        "created_at": "2020-09-09T12:28:11Z",
        "body": "Hi @blatr, I thinks it's a nice idea!\r\nPlease be aware, that there's already a PR in progress  (#3256) that'll probably come in a superficial conflict with yours, but I don't see any actual problems with this idea."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T13:42:21Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3377,
    "title": "val_dataloader is called twice in each worker",
    "created_at": "2020-09-07T03:00:59Z",
    "closed_at": "2020-10-27T10:07:30Z",
    "labels": [
      "help wanted",
      "won't fix",
      "docs"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3377",
    "body": "## 🐛 Bug\r\n\r\nI'm trying a `LightningDataModule` class to manage the data.\r\nUsing horovod backend, if that matters. \r\nI've noticed that each rank is calling `train_dataloader` once, but `val_dataloader` two times somehow.\r\n\r\n### To Reproduce\r\n\r\nrun LIghtning with Dataclass and horovod, add some debug print on when `val_dataloader` is called\r\n\r\nsoemthing like\r\n```\r\n    def train_dataloader(self):\r\n        print(f\"\\n#####worker {hvd.rank()} of {hvd.size()} creating train_loader\\n\")\r\n        return load_ds_from_dir(os.path.join(self.path, \"train\"), self.batch_size)\r\n\r\n    def val_dataloader(self):\r\n        print(f\"\\n#####worker {hvd.rank()} of {hvd.size()} creating val\\n\")\r\n        return load_ds_from_dir(os.path.join(self.path, \"validation\"), self.batch_size)\r\n```\r\n\r\n### Expected behavior\r\n\r\nexpect val loader to be called only once... \r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.19.1\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.2\r\n\t- version:           #1 SMP Fri Apr 20 16:44:24 UTC 2018\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3377/comments",
    "author": "undertherain",
    "comments": [
      {
        "user": "undertherain",
        "created_at": "2020-09-07T03:26:43Z",
        "body": "I see... it is doing  `run_sanity_check` before training and then resets a val dataloader\r\nthat's surprising..... why not be more explicit about this functionality in the tutorial or better make it false by default? "
      },
      {
        "user": "undertherain",
        "created_at": "2020-09-07T03:34:40Z",
        "body": "okey, `num_sanity_val_steps=0,` solved it for me\r\nbut I'll keep this open so that devs have a chance to see and maybe consider making it zero by default, or at least leave a word of warning in the docs. I mean, I was not using any sort of lazy loading data, but reading terabytes of tensors to memory on extra time of val loader create..."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-08T05:33:23Z",
        "body": "This is not a bug, it is intentionally on by default. The sanity check is there to report exceptions early instead of only after a long training epoch. We can add a note to the docs that the dataloader gets reloaded. \r\nI'm removing the bug label, as this is expected behavior as far as I know."
      },
      {
        "user": "undertherain",
        "created_at": "2020-09-08T05:51:15Z",
        "body": "Thanks for considering to update the docs!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T15:43:46Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3364,
    "title": "load_from_checkpoint not initializing right when using transformers",
    "created_at": "2020-09-05T19:43:54Z",
    "closed_at": "2020-09-05T20:04:04Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3364",
    "body": "## 🐛 Bug\r\n\r\nI created a LightningModel which is a wrapper over a hugging face transformer. I was able to train the model and save it to disk. However, load_from_checkpoint doesn't seem to pass the right parameters to LightningModel init, due to which the function fails with an error. \r\n\r\n### To Reproduce\r\n\r\nCheck code sample\r\n\r\n#### Code sample\r\n\r\nModel \r\n\r\n    class BertBasedClassifier(LightningModule):\r\n        def __init__(self, model_name='distilbert-base-uncased', num_labels=4):\r\n            super().__init__()\r\n            self.model = AutoModelForSequenceClassification.from_pretrained(\r\n                model_name, num_labels=num_labels)\r\n\r\nload_from_checkpoint\r\n\r\n    model = BertBasedClassifier.load_from_checkpoint('checkpoints/full_yahoo_answers_classifier_baseline_without_wandb/lightning_logs/version_1/checkpoints/epoch=0.ckpt') \r\n\r\n### Expected behavior\r\n\r\nI've debuged the error a bit, and it's because the the right parameters  aren't being sent to `BertBasedClassifier` `__init__()`. The parameters that go to the model are \r\n\r\n    Model name  {}\r\n    num_labels  4\r\n\r\nThe parameters that should go to model are \r\n\r\n    Model name 'distilbert-base-uncased'\r\n    num_labels 10\r\n\r\nI've tried explicitly passing parameters to as a part of `load_from_checkpoint` That didn't work either.\r\n\r\n    model = BertBasedClassifier.load_from_checkpoint('checkpoints/full_yahoo_answers_classifier_baseline_without_wandb/lightning_logs/version_1/checkpoints/epoch=0.ckpt', num_labels=10)\r\n\r\nParameters that model gets are \r\n\r\n    Model name  {'num_labels': 10}\r\n    num_labels  10\r\n\r\nHow to resolve the model name issue? Or can I first create a model normally and then load weights into it?\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce RTX 2080 Ti\r\n\t\t- GeForce RTX 2080 Ti\r\n\t\t- GeForce RTX 2080 Ti\r\n\t\t- GeForce RTX 2080 Ti\r\n\t\t- GeForce RTX 2080 Ti\r\n\t\t- GeForce RTX 2080 Ti\r\n\t\t- GeForce RTX 2080 Ti\r\n\t\t- GeForce RTX 2080 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.19.1\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.48.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.9\r\n\t- version:           #100~16.04.1-Ubuntu SMP Wed Apr 22 23:56:30 UTC 2020",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3364/comments",
    "author": "sgondala",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-05T19:44:39Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "sgondala",
        "created_at": "2020-09-05T20:04:04Z",
        "body": "Looks like I didn't use `self.save_hyperparameters()` right. Using that fixed the issue.\r\n"
      }
    ]
  },
  {
    "number": 3330,
    "title": "Trainer.profiler should allow str",
    "created_at": "2020-09-02T18:30:15Z",
    "closed_at": "2020-10-27T10:57:16Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3330",
    "body": "## 🚀 Feature\r\n\r\nCurrently, the `profile` argument of the Trainer is `bool`. It should be `Union[bool, str]`.\r\n\r\n### Motivation\r\n\r\nIt would avoid having to set the profiler manually by creating and passing the object to the Trainer.\r\nIt is also the way `auto_scale_batch_size` and `auto_lr_find` work.\r\n\r\n### Pitch\r\n\r\n```\r\n# equivalent to --profiler true\r\ntrainer = Trainer(profiler=True)\r\n\r\n# equivalent to --profiler simple\r\ntrainer = Trainer(profiler=SimpleProfiler())\r\n\r\n# equivalent to --profiler advanced\r\ntrainer = Trainer(profiler=AdvancedProfiler())\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3330/comments",
    "author": "carmocca",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-09-02T20:35:29Z",
        "body": "@jeremyjordan any thoughts?"
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-09-16T00:38:14Z",
        "body": "Sounds good to me, want to submit a PR @carmocca?\r\n\r\n>It should be Union[bool, str].\r\n\r\nTechnically, it should be `Union[bool, str, BaseProfiler]`."
      },
      {
        "user": "carmocca",
        "created_at": "2020-09-16T01:13:45Z",
        "body": "> Sounds good to me, want to submit a PR @carmocca?\r\n\r\nBit busy right now, maybe at a later point.\r\n\r\n@Borda Tag it \"good first issue\" in the meantime?"
      },
      {
        "user": "ddrevicky",
        "created_at": "2020-09-25T11:14:32Z",
        "body": "@Borda Assign this to me please :)."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-25T14:05:24Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "ananthsub",
        "created_at": "2020-10-25T16:36:35Z",
        "body": "if you're using Hydra, this instantiation via config becomes a lot easier. This has parallels to the distributed_backend / plugin interfaces too, which are increasingly supporting explicit instantiations passed to the trainer. Supporting string with lightning-provided implementations along with custom classes passed to the Trainer seems like a nice convergence! @williamFalcon "
      }
    ]
  },
  {
    "number": 3315,
    "title": "Support multiple levels of subclassing in `from_argparse_args`",
    "created_at": "2020-09-02T02:46:40Z",
    "closed_at": "2020-10-28T20:27:23Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3315",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nImprove `from_argparse_args` methods in `LightningDataModule` and `Trainer` so that they can support more than one level of subclassing from their base Lightning class.\r\n\r\n### Motivation\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nUsers might want to develop a custom hierarchy of subclasses to either `LigthingDataModule` or `Trainer` to deal with common/boilerplate code (e.g. systematically add common arguments to data modules like `batch_size` or `num_workers`). However, the generic `from_argparse_args` methods fail them in this case, and users must implement by hand a custom initialization logic.\r\n\r\n### Pitch\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nIt would be easy to implement a recursive inspection of the parent's init signature to include its args as well until we hit the base Lightning class.\r\n\r\nEdit: To help discuss the issue, I opened a draft PR #3316 which already implements the desired behavior.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3315/comments",
    "author": "nathanpainchaud",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-02T02:47:20Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-09-02T15:07:59Z",
        "body": "Let's continue the discussion on the created PR..."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T16:43:45Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3311,
    "title": "Example code does not run",
    "created_at": "2020-09-01T20:20:34Z",
    "closed_at": "2020-09-01T21:02:53Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3311",
    "body": "## 🐛 Bug\r\n\r\nOfficial example code, only modifying # of GPUs, does not run.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```py\r\nimport os\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\nimport pytorch_lightning as pl\r\n\r\nclass MNISTModel(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super(MNISTModel, self).__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def training_step(self, batch, batch_nb):\r\n        x, y = batch\r\n        loss = F.cross_entropy(self(x), y)\r\n        tensorboard_logs = {'train_loss': loss}\r\n        return {'loss': loss, 'log': tensorboard_logs}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.02)\r\n\r\ntrain_loader = DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=3000)\r\n\r\nmnist_model = MNISTModel()\r\ntrainer = pl.Trainer(gpus=2)    <-- ONLY LINE I CHANGE\r\ntrainer.fit(mnist_model, train_loader)  \r\n```\r\n```\r\nExceptionTraceback (most recent call last)\r\n<ipython-input-3-8d3997ec0919> in <module>\r\n      3 mnist_model = MNISTModel()\r\n      4 trainer = pl.Trainer(gpus=2)\r\n----> 5 trainer.fit(mnist_model, train_loader)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py in wrapped_fn(self, *args, **kwargs)\r\n     46             if entering is not None:\r\n     47                 self.state = entering\r\n---> 48             result = fn(self, *args, **kwargs)\r\n     49 \r\n     50             # The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n   1050             self.accelerator_backend = DDPSpawnBackend(self)\r\n   1051             self.accelerator_backend.setup()\r\n-> 1052             self.accelerator_backend.train(model, nprocs=self.num_processes)\r\n   1053             results = self.accelerator_backend.teardown(model)\r\n   1054 \r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py in train(self, model, nprocs)\r\n     41 \r\n     42     def train(self, model, nprocs):\r\n---> 43         mp.spawn(self.ddp_train, nprocs=nprocs, args=(self.mp_queue, model,))\r\n     44 \r\n     45     def teardown(self, model):\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in spawn(fn, args, nprocs, join, daemon, start_method)\r\n    198                ' torch.multiprocessing.start_process(...)' % start_method)\r\n    199         warnings.warn(msg)\r\n--> 200     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)\r\n    156 \r\n    157     # Loop on join until it returns True or raises an exception.\r\n--> 158     while not context.join():\r\n    159         pass\r\n    160 \r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in join(self, timeout)\r\n    111                 raise Exception(\r\n    112                     \"process %d terminated with exit code %d\" %\r\n--> 113                     (error_index, exitcode)\r\n    114                 )\r\n    115 \r\n\r\nException: process 0 terminated with exit code 1\r\n```\r\n\r\nIf I change the one line to:\r\n\r\n`trainer = pl.Trainer(gpus=2, distributed_backend=\"ddp\")`, I get:\r\n\r\n```\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nCUDA_VISIBLE_DEVICES: [0,1]\r\n```\r\n\r\nHanging permanently (no progress after > 1 hour). If I inturrupt this manually, I see the model is stuck on:\r\n\r\n`f.write(fp.getbuffer())`\r\n\r\nWithin a multiprocess call.\r\n\r\nHowever, I only got this traceback maybe 3 times out of 30, and didn't record it, and couldn't recreate it again to get the full traceback for this error.\r\n\r\nwith:\r\n\r\n`trainer = pl.Trainer(gpus=2, distributed_backend=\"ddp_spawn\")`, I get:\r\n\r\n```\r\nExceptionTraceback (most recent call last)\r\n<ipython-input-3-e6d489f1a457> in <module>\r\n      3 mnist_model = MNISTModel()\r\n      4 trainer = pl.Trainer(gpus=2, distributed_backend=\"ddp_spawn\")\r\n----> 5 trainer.fit(mnist_model, train_loader)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py in wrapped_fn(self, *args, **kwargs)\r\n     46             if entering is not None:\r\n     47                 self.state = entering\r\n---> 48             result = fn(self, *args, **kwargs)\r\n     49 \r\n     50             # The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n   1050             self.accelerator_backend = DDPSpawnBackend(self)\r\n   1051             self.accelerator_backend.setup()\r\n-> 1052             self.accelerator_backend.train(model, nprocs=self.num_processes)\r\n   1053             results = self.accelerator_backend.teardown(model)\r\n   1054 \r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py in train(self, model, nprocs)\r\n     41 \r\n     42     def train(self, model, nprocs):\r\n---> 43         mp.spawn(self.ddp_train, nprocs=nprocs, args=(self.mp_queue, model,))\r\n     44 \r\n     45     def teardown(self, model):\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in spawn(fn, args, nprocs, join, daemon, start_method)\r\n    198                ' torch.multiprocessing.start_process(...)' % start_method)\r\n    199         warnings.warn(msg)\r\n--> 200     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)\r\n    156 \r\n    157     # Loop on join until it returns True or raises an exception.\r\n--> 158     while not context.join():\r\n    159         pass\r\n    160 \r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in join(self, timeout)\r\n    111                 raise Exception(\r\n    112                     \"process %d terminated with exit code %d\" %\r\n--> 113                     (error_index, exitcode)\r\n    114                 )\r\n    115 \r\n\r\nException: process 0 terminated with exit code 1\r\n```\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4 / 1.6\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.6, 3.7\r\n - CUDA/cuDNN version: CUDA: 10.1, 10.2, 11.0\r\n - GPU models and configuration: GTX1080ti, GTX1080, GTX1070, P100\r\n\r\nI've tried running this on 5 different VMs, and got the same issues on every single one.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3311/comments",
    "author": "Sun694",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-01T20:21:18Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Sun694",
        "created_at": "2020-09-01T21:02:53Z",
        "body": "Ended up being a combination of a cuDNN version issue and a jupyter notebook issue."
      }
    ]
  },
  {
    "number": 3276,
    "title": "Logging non-tensor scalar with result breaks subsequent epoch aggregation",
    "created_at": "2020-08-31T01:33:53Z",
    "closed_at": "2020-10-13T10:42:12Z",
    "labels": [
      "bug",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3276",
    "body": "## 🐛 Bug\r\nLogging non-tensor scalar with result breaks subsequent epoch/tbptt aggregation\r\n(on both 0.9 and master)\r\n\r\n```\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\r\n    fn(i, *args)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py\", line 165, in ddp_train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1237, in run_pretrain_routine\r\n    self.train()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 396, in train\r\n    self.run_training_epoch()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 543, in run_training_epoch\r\n    self.run_training_epoch_end(epoch_output, checkpoint_accumulator, early_stopping_accumulator, num_optimizers)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 672, in run_training_epoch_end\r\n    epoch_log_metrics, epoch_progress_bar_metrics = self.__auto_reduce_results_on_epoch_end(epoch_output)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 696, in __auto_reduce_results_on_epoch_end\r\n    tbptt_outs = tbptt_outs[0].__class__.reduce_across_time(tbptt_outs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py\", line 392, in reduce_across_time\r\n    result[k] = tbptt_reduce_fx(value)\r\nTypeError: mean(): argument 'input' (position 1) must be Tensor, not list\r\n```\r\n\r\n### To Reproduce\r\n```\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch[0], batch[1]\r\n        x = self.forward(x)\r\n        loss = self.loss(x, y)\r\n        result = pl.TrainResult(loss)\r\n        result.log(\"non tensor scalar\", 1.0)\r\n        result.log(\"loss\", loss, on_step=False, on_epoch=True)\r\n```\r\n### To Fix\r\n```\r\nresult.log(\"non tensor scalar\", torch.tensor(1.0))\r\n```\r\n### Expected behavior\r\nIn `log()` of result objects, value should accept non tensor values as `value: Any` and not cause issues with other metrics to be logged\r\n\r\n### Additional context\r\n`log()` can be changed to only accept tensors, or have a built-in conversion, will update as I investigate further",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3276/comments",
    "author": "s-rog",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2020-09-01T14:51:45Z",
        "body": "@williamFalcon "
      },
      {
        "user": "Borda",
        "created_at": "2020-09-15T16:52:44Z",
        "body": "@williamFalcon is it still there? @s-rog mind check if it is still on master? or better add test for such case..."
      },
      {
        "user": "s-rog",
        "created_at": "2020-09-16T01:26:44Z",
        "body": "I'll take a look again when I get a chance, haven't probed much due to the refactors... Are they mostly done?"
      },
      {
        "user": "s-rog",
        "created_at": "2020-09-16T02:02:51Z",
        "body": "@Borda the bug is still here, the following template tests for this issue as well as #3278\r\n\r\nFor this problem the easiest fix would be to force type to tensors. Though that's probably just a bandaid solution, thoughts?\r\n\r\n<details>\r\n<summary>Test template for reference</summary>\r\n\r\n```\r\n#!/opt/conda/bin/python\r\n\"\"\"\r\nRuns a model on a single node across multiple gpus.\r\n\"\"\"\r\nimport os\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch.nn.functional as F\r\n\r\nimport pytorch_lightning as pl\r\nfrom pl_examples.models.lightning_template import LightningTemplateModel\r\nfrom pytorch_lightning import Trainer, seed_everything\r\n\r\nseed_everything(234)\r\n\r\nclass custom_template(LightningTemplateModel):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n    \r\n    def on_epoch_start(self):\r\n        print(\"on_epoch_start\")\r\n\r\n    def on_fit_start(self):\r\n        print(\"on_fit_start\")\r\n        \r\n    def training_step(self, batch, batch_idx):\r\n        \"\"\"\r\n        Lightning calls this inside the training loop with the data from the training dataloader\r\n        passed in as `batch`.\r\n        \"\"\"\r\n        # forward pass\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        result = pl.TrainResult(loss)\r\n        result.log(\"non tensor scalar\", 1.0)\r\n        result.log(\"loss\", loss, on_step=False, on_epoch=True)\r\n        return result\r\n        \r\n\r\ndef main(args):\r\n    \"\"\" Main training routine specific for this project. \"\"\"\r\n    # ------------------------\r\n    # 1 INIT LIGHTNING MODEL\r\n    # ------------------------\r\n    model = custom_template(**vars(args))\r\n\r\n    # ------------------------\r\n    # 2 INIT TRAINER\r\n    # ------------------------\r\n    trainer = Trainer.from_argparse_args(args)\r\n\r\n    # ------------------------\r\n    # 3 START TRAINING\r\n    # ------------------------\r\n    trainer.fit(model)\r\n\r\n\r\ndef run_cli():\r\n    # ------------------------\r\n    # TRAINING ARGUMENTS\r\n    # ------------------------\r\n    # these are project-wide arguments\r\n    root_dir = os.path.dirname(os.path.realpath(__file__))\r\n    parent_parser = ArgumentParser(add_help=False)\r\n\r\n    # each LightningModule defines arguments relevant to it\r\n    parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)\r\n    parser = Trainer.add_argparse_args(parser)\r\n    parser.set_defaults(gpus=1, distributed_backend=None)\r\n    args = parser.parse_args()\r\n\r\n    # ---------------------\r\n    # RUN TRAINING\r\n    # ---------------------\r\n    main(args)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run_cli()\r\n```\r\n</details>"
      },
      {
        "user": "s-rog",
        "created_at": "2020-09-17T01:01:44Z",
        "body": "I looked into it a bit and `reduce_across_time` is getting called on all metrics if one metric in results is logged with `on_epoch=True` which makes `on_epoch=True` only compatible with tensor scalars since the default tbptt fn is `torch.mean`\r\n\r\nThis is probably not intended behavior?"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-17T15:24:59Z",
        "body": "@justusschock @SkafteNicki "
      },
      {
        "user": "justusschock",
        "created_at": "2020-09-17T15:57:06Z",
        "body": "@edenlightning this is not related to metrics. \n\n@s-rog I guess a simple type check that converts scalars to scalars tensor should do the trick? If so, could you open a PR with this fix?"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-05T07:45:50Z",
        "body": "fixed by #3855"
      },
      {
        "user": "s-rog",
        "created_at": "2020-10-13T08:16:31Z",
        "body": "```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_spawn_accelerator.py\", line 152, in ddp_train\r\n    results = self.train_or_test()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 53, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 483, in train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 598, in run_training_epoch\r\n    self.num_optimizers\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py\", line 339, in log_train_epoch_end_metrics\r\n    epoch_log_metrics, epoch_progress_bar_metrics = self.__auto_reduce_results_on_epoch_end(epoch_output)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py\", line 449, in __auto_reduce_results_on_epoch_end\r\n    tbptt_outs = tbptt_outs[0].__class__.reduce_across_time(tbptt_outs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py\", line 483, in reduce_across_time\r\n    result[k] = tbptt_reduce_fx(value.float())\r\nAttributeError: 'list' object has no attribute 'float'\r\n```\r\n\r\n@Borda I don't think the issue was fixed completely, this is on rc5 (using self.log)"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-13T09:49:37Z",
        "body": "@s-rog mind add a test for this case?"
      },
      {
        "user": "s-rog",
        "created_at": "2020-10-13T10:43:22Z",
        "body": "william beat me to it :]"
      }
    ]
  },
  {
    "number": 3250,
    "title": "Adding input sanitation for distributed backend and related trainer flags",
    "created_at": "2020-08-28T19:51:47Z",
    "closed_at": "2020-10-28T21:27:14Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3250",
    "body": "## 🚀 Feature\r\nError should be thrown or a warning raised if the passed distributed_backend flag doesn't match one of the expected types (e.g ddp, ddp-spawn, etc). This maybe applicable to other trainer flags too.\r\n\r\n### Motivation\r\nThis is really minor, but I just spent an embarrassing amount of time trying to figure out why my ddp models were not using GPUs. After stepping through the the pytorch-lightning accelerator selection logic, I eventually realized I had a typo in my distributed_backend flag. This mistake could have easily been caught by input sanitation. \r\n\r\nFor more detail, when you enter an invalid backend, like `dpp` instead of `ddp`, and select `gpus = 4`, the logs at the top of the program show the 4 available GPUs and lead the user to believe that nothing is wrong. However, the logic of the accelerator selection silently chooses `cpu` in this case, result in surprising behavior.\r\n\r\n### Pitch\r\nFor various flags, like \"distributed_backend\" unrecognized values, e.g \"dpp\" instead of \"ddp\", pl should raise an error or warn the\r\nuser that this isn't recognized / valid value for the flag.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3250/comments",
    "author": "yala",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-28T19:52:26Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "yukw777",
        "created_at": "2020-08-28T20:53:04Z",
        "body": "@yala yo what’s up Adam! Remember me from good old days at ASAPP? haha We can def add some checks around inputs. I’m actually surprised  we don’t have one right now. Maybe you can send out a PR if you have some time!"
      },
      {
        "user": "yala",
        "created_at": "2020-09-03T22:53:07Z",
        "body": "Wow, hey Peter!\r\nAre you working at lightning now? Anyways, this is a great project!\r\nI'll put together a short PR at some point next week if i get the time. "
      },
      {
        "user": "yukw777",
        "created_at": "2020-09-06T21:08:19Z",
        "body": "@yala i just help out as much as I can as a core contributor. let me know how it goes!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T15:43:48Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3249,
    "title": "Enable passing result from *_step Model Hook to corresponding *_batch_end Callback",
    "created_at": "2020-08-28T19:18:16Z",
    "closed_at": "2020-10-27T22:51:29Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3249",
    "body": "## 🚀 Feature\r\nGive users the option to pass a result at the end of `*_step` directly to the corresponding `*_batch_end` `Callback`.\r\n\r\nExample:\r\n`validation_step` outputs prediction and probabilities. Pass these to `validation_batch_end` user-defined `Callback` for advanced logging.\r\n\r\n### Motivation\r\n\r\nThis will remove the need for some boilerplate in the model code and also remove the need for calling the model's `forward()` on the batch inside the callback just to do logging.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3249/comments",
    "author": "romesco",
    "comments": [
      {
        "user": "romesco",
        "created_at": "2020-08-28T19:23:05Z",
        "body": "I can't manually assign, but please assign to @romesco and @williamFalcon !"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-17T19:49:10Z",
        "body": "@romesco want to try and submit a PR for this?"
      },
      {
        "user": "romesco",
        "created_at": "2020-09-18T06:29:04Z",
        "body": "Yep! I started it I'll put out the draft PR tomorrow or so. Got caught pushing for a deadline so some of my open source stuff got backlogged =]."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T09:24:33Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "romesco",
        "created_at": "2020-10-27T22:51:29Z",
        "body": "Need to revisit this since I previously started working on this when `*Result` was still a part of the API. I'll probably take a step back and think about what we really want to achieve here."
      }
    ]
  },
  {
    "number": 3232,
    "title": "Switching to using LightningDataModule see \"has no attribute 'save_hyperparameters'\"",
    "created_at": "2020-08-27T20:27:43Z",
    "closed_at": "2020-09-18T06:16:54Z",
    "labels": [
      "bug",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3232",
    "body": "I'm trying to use the LightningDataModule and so I just copied the data members in my existing LightningModule to a LightningDataModule.  The LightningModule takes a config as input and then I call self.save_hyperparameters(cfg).  Obviously that same function doesn't work in the LightningDataModule (nor does it make sense).  Is there another function in LightningDataModule to save data the parameters (or configuration)?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3232/comments",
    "author": "jloveric",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-08-27T23:30:38Z",
        "body": "@nateraw is working on this as far as I know. "
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-01T14:58:16Z",
        "body": "@nateraw any update?"
      },
      {
        "user": "nateraw",
        "created_at": "2020-09-18T06:16:54Z",
        "body": "Moving discussion to #3544 as it lays out plans to implement the feature mentioned here."
      }
    ]
  },
  {
    "number": 3231,
    "title": "Validation running loss",
    "created_at": "2020-08-27T19:48:52Z",
    "closed_at": "2020-08-27T23:39:14Z",
    "labels": [
      "duplicate",
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3231",
    "body": "## 🚀 Feature\r\nTrainer should compute a validation running loss just as it does for training.\r\n\r\n### Motivation\r\nI want to keep track of the running validation loss for each epoch. AFAIK this is not supported out-of-the-box by lightning.\r\n\r\n### Pitch\r\n`EvalResult` could include something similar to the `minimize` argument.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3231/comments",
    "author": "carmocca",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-08-27T19:54:58Z",
        "body": "if by running_loss you are referring to the a valid loss at each step then you can set `on_step=True` in `EvalResult().log('valid_loss', loss, on_step=True)`"
      },
      {
        "user": "carmocca",
        "created_at": "2020-08-27T20:00:31Z",
        "body": "I would like to do the following:\r\n```\r\nTR - Epoch 0: 100%|█████████████████████████| 30/30 [00:06<00:00,  4.39it/s, loss={running_training_loss}]\r\nVA - Epoch 0: 100%|█████████████████████████| 15/15 [00:01<00:00, 15.18it/s, loss={running_val_loss}]\r\nTR - Epoch 1: 100%|█████████████████████████| 30/30 [00:07<00:00,  4.05it/s, loss={running_training_loss}]\r\nVA - Epoch 1: 100%|█████████████████████████| 15/15 [00:01<00:00, 12.91it/s, loss={running_val_loss}]\r\n```\r\nso both the training and validation bars include running losses. The trainer already computes the `running_training_loss`."
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-27T20:03:58Z",
        "body": "try: `EvalResult().log('valid_loss', loss, on_step=True, prog_bar=True)`"
      },
      {
        "user": "carmocca",
        "created_at": "2020-08-27T20:13:40Z",
        "body": "I don't think that solves what I want. That just adds `step_val_loss=200, epoch_val_loss=147` (the last step and epoch validation losses) to the training progress bar.\r\nWhat I want is the validation progress bar to include the running validation loss, which gets updated as the validation runs.\r\n\r\nSo:\r\n```\r\nTR - Epoch 0: 100%|█████████████████████████| 30/30 [00:06<00:00,  4.39it/s, loss={running_training_loss}]\r\nVA - Epoch 0: 100%|█████████████████████████| 15/15 [00:01<00:00, 15.18it/s]\r\nTR - Epoch 1: 100%|█████████████████████████| 30/30 [00:07<00:00,  4.05it/s, loss={running_training_loss}, step_val_loss={last_step_val_loss}, epoch_val_loss={last_epoch_val_loss}]\r\nVA - Epoch 1: 100%|█████████████████████████| 15/15 [00:01<00:00, 12.91it/s]\r\n```\r\ncompared to what I wrote in my last comment."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-27T20:26:41Z",
        "body": "Same as #1239 ?"
      },
      {
        "user": "carmocca",
        "created_at": "2020-08-27T21:12:47Z",
        "body": "Seems like it."
      },
      {
        "user": "carmocca",
        "created_at": "2020-08-28T00:09:42Z",
        "body": "@awaelchli Is this a feature you would merge?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-28T00:17:26Z",
        "body": "Yes, see the linked issue, it is a todo feature. I think it is useful. If you have an idea how to implement it, go for it. Just be aware, there are some internal refactors happening at the moment all around Trainer, so might want to wait a week or so. "
      }
    ]
  },
  {
    "number": 3227,
    "title": "Support multiple checkpointing schemes",
    "created_at": "2020-08-27T17:36:49Z",
    "closed_at": "2020-10-28T22:27:21Z",
    "labels": [
      "duplicate",
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3227",
    "body": "Give a list of checkpoint callbacks, as I want to use multiple checkpointing schemes (e.g. save best for different metrics) or save best k1 checkpoints but also save every k2 epochs",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3227/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2020-08-27T18:02:25Z",
        "body": "Duplicate of #2908"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T22:01:50Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3218,
    "title": "Can EvalResult show a max metric value in progress bar?",
    "created_at": "2020-08-27T06:51:17Z",
    "closed_at": "2020-08-27T13:07:00Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3218",
    "body": "## 🚀 Feature\r\nnow EvalResult can only show current epoch's metric value in progress bar, can you add a flag to make it show max/min value of some metric, such as max accuracy up to current epoch?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3218/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-08-27T12:16:18Z",
        "body": "I think this feature is a bit too specific to add a flag for it. What do @PyTorchLightning/core-contributors think?\r\nFor what you want to do, I have this solution overriding `get_progress_bar_dict` in LightningModule:\r\n\r\n```python\r\ndef get_progress_bar_dict(self):\r\n    bar_dict = super().get_progress_bar_dict()\r\n    bar_dict[\"max\"] = max(self.epoch_losses)  # save your epoch metrics in this list\r\n    return bar_dict\r\n```"
      },
      {
        "user": "xiadingZ",
        "created_at": "2020-08-27T12:35:07Z",
        "body": "`EvalResult`'s `checkpoint_on`  is to get the best metric model, I think is convenient to print this best metric on the progress bar to show the best case. Now I have to open tensorboard of each log_dir to see the best metric I get."
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-27T12:55:54Z",
        "body": "One can maintain the best metric state in module itself and log it everytime with prog_bar=True"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-08-27T13:05:35Z",
        "body": "or change the reduce_fx to torch.max (only for epoch metrics though)\r\n\r\n```python\r\nresult = ...\r\nresult.log('x', x, reduce_fx=torch.max)\r\n```\r\n\r\nor to do it on every step then track the max yourself."
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-27T13:40:07Z",
        "body": "> result = ...\r\nresult.log('x', x, reduce_fx=torch.max)\r\n\r\n@williamFalcon  `reduce_fx` is used to aggregate step metrics right? Will this work when we need to track the best epoch_metric from all previous epochs?"
      }
    ]
  },
  {
    "number": 3215,
    "title": "validation_epoch_end not logging validation_step EvalResult values",
    "created_at": "2020-08-27T05:00:30Z",
    "closed_at": "2020-10-05T15:13:16Z",
    "labels": [
      "bug",
      "help wanted",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3215",
    "body": "## 🐛 Bug\r\n\r\nWhen overwriting `validation_epoch_end` the EvalResult values from `validation_step` are not logged.\r\n\r\nFor my experiments I keep track of several metrics that I only log to TensorBoard at the end of each validation epoch. For most metrics I can specify `EvalResult().log(on_epoch=True)`, but one of the metrics I can only calculate at the end of the epoch. If I use `validation_epoch_end` to calculate this metric, the results from `validation_step` are not logged. \r\n\r\n### To Reproduce\r\n```\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n\r\n        result = pl.EvalResult(checkpoint_on=loss)\r\n        result.log('val/loss', loss, on_step=False, on_epoch=True)\r\n        result.log('val/y_hat', y_hat, logger=False, on_step=False, on_epoch=False)\r\n        return result\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        y_hat = outputs['val/y_hat']\r\n        example = y_hat.exp().mean()\r\n\r\n        result = pl.EvalResult()\r\n        result.log('val/epoch_end_metric', example)\r\n        return result\r\n```\r\nThe code above writes 'val/epoch_end_metric' to TensorBoard, but does nothing with the result from `validation_step`. \r\n\r\nAs a result, the checkpoint metric specified in `validation_step` is also lost. Warnings are printed to screen, but it still prints a statement at the end of fit claiming to save the checkpoint (which is not happening).\r\n\r\n```\r\n...\r\n\r\n  | Name | Type   | Params\r\n--------------------------------\r\n0 | l1   | Linear | 7 K   \r\n\r\nEpoch 0:  50%|██████▌      | 5/10 [00:00<00:00, 549.90it/s, loss=2.454, v_num=2]\r\nValidating: 0it [00:00, ?it/s] ---/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: The metric you returned None must be a `torch.Tensor` instance, checkpoint not saved HINT: what is the value of loss in validation_epoch_end()?\r\n  warnings.warn(*args, **kwargs)\r\n---/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: Can save best model only with loss available, skipping.\r\n  warnings.warn(*args, **kwargs)\r\n\r\n...\r\n\r\nEpoch 4: 100%|████████████| 10/10 [00:00<00:00, 911.31it/s, loss=1.847, v_num=2]\r\n                              Saving latest checkpoint..\r\nEpoch 4: 100%|████████████| 10/10 [00:00<00:00, 761.27it/s, loss=1.847, v_num=2]\r\n```\r\n\r\n### Expected behavior\r\n\r\nResults from `validation_step` and `validation_epoch_end` should both be logged according to what is specified. The `.log(..., logger, on_step, on_epoch)` method provides enough granularity to specify how each metric should be processed for both validation methods.\r\n\r\nI have tried using `return [result, outputs]` in `validation_epoch_end` to work around this, but the metrics in the EvalResult are assumed to be reduced to scalars if `validation_epoch_end` is overwritten and are therefore not handled correctly.\r\n\r\n### Environment\r\n\r\n - PyTorch-Lightning Version: 0.9.0\r\n - PyTorch Version: 1.6.0\r\n - OS: linux\r\n - How you installed PyTorch: pip\r\n - Python version: 3.6.9\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3215/comments",
    "author": "gerardsn",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-27T05:01:12Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "gerardsn",
        "created_at": "2020-08-27T05:04:11Z",
        "body": "Any workarounds or exisiting solutions are most welcome.\r\n\r\n<details><summary>Click for full example code</summary>\r\n<p>\r\n\r\n```import os\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\n\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass LitModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        result = pl.TrainResult(loss)\r\n        result.log('train/loss', loss, on_step=False, on_epoch=True)\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n\r\n        result = pl.EvalResult(checkpoint_on=loss)\r\n        result.log('val/loss', loss, on_step=False, on_epoch=True)\r\n        result.log('val/y_hat', y_hat, logger=False, on_step=False, on_epoch=False)\r\n        return result\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        y_hat = outputs['val/y_hat']\r\n        example = y_hat.exp().mean()\r\n\r\n        result = pl.EvalResult()\r\n        result.log('val/epoch_end_metric', example)\r\n        return result\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.02)\r\n\r\n\r\ntrain_loader = DataLoader(MNIST(os.getcwd(), download=True, transform=transforms.ToTensor()))\r\ntrainer = pl.Trainer(max_epochs=5, limit_train_batches=5, limit_val_batches=5)\r\nmodel = LitModel()\r\n\r\ntrainer.fit(model, train_loader, train_loader)```\r\n\r\n"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-08-27T08:40:09Z",
        "body": "Hi @gerardsn, if you want to do simple things (like `.exp().mean()` in your example) at the end of epoch, you could try with your own reduction function for `reduce_fx`. The `reduce_fx` is applied at the end of the epoch.\r\nHere's the modified example from your comment\r\n```py\r\nimport os\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\n\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass LitModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        result = pl.TrainResult(loss)\r\n        result.log('train/loss', loss, on_step=False, on_epoch=True)\r\n        return result\r\n\r\n    def exp_mean(self, y_hat):\r\n        return torch.exp(torch.mean(y_hat))\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n\r\n        result = pl.EvalResult(checkpoint_on=loss)\r\n        result.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\r\n        result.log('val/epoch_end_metric', y_hat, on_step=False, on_epoch=True, prog_bar=True, reduce_fx=self.exp_mean)\r\n        return result\r\n\r\n    # def validation_epoch_end(self, outputs):\r\n    #     y_hat = outputs['val/y_hat']\r\n    #     example = y_hat.exp().mean()\r\n\r\n    #     result = pl.EvalResult()\r\n    #     result.log('val/epoch_end_metric', example)\r\n    #     return result\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.02)\r\n\r\n\r\ntrain_loader = DataLoader(MNIST(os.getcwd(), download=True, transform=transforms.ToTensor()))\r\ntrainer = pl.Trainer(max_epochs=5, limit_train_batches=5, limit_val_batches=5)\r\nmodel = LitModel()\r\n\r\ntrainer.fit(model, train_loader, train_loader)\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-27T08:45:43Z",
        "body": "@gerardsn If you implement validation_epoch_end, it is always assumed that you want to do your own reduction, so it will pass all results in there before logging. I'm not sure, @williamFalcon is it a bug or is it expected behaviour?"
      },
      {
        "user": "gerardsn",
        "created_at": "2020-08-27T09:55:23Z",
        "body": "@awaelchli I think this expected behaviour, but it is not very user user-friendly. I am trying to log 5 metrics in the `validation_step`, and only 1 in the `validation_epoch_end`. In the current setup I would have to loop over the metrics again at the end of each epoch and reduce them all myself. Since the input to `validation_epoch_end` is essentially an EvalResult it would be nice to just keep logging metrics on to this and reduce them in the back."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-08-27T11:09:10Z",
        "body": "you don’t need to do this...\r\n\r\n### log step only\r\nlog_dict(the four metrics, on_step=True, on_epoch=False)\r\n\r\n### log epoch only\r\nlog(5th metric, on_step=False, on_epoch=True)\r\n\r\n\r\nfyi... the default in validation step is on_step=False, on_epoch=True"
      },
      {
        "user": "gerardsn",
        "created_at": "2020-08-27T11:49:11Z",
        "body": "@williamFalcon @awaelchli  I think this would be a good solution in many cases and is one of the options I considered. Unfortunately it didn't work well for me. One of the metrics I'm tracking is the Dice score and working with 3D medical data I can only process a very small batch size on each step. With such a small batch the metric value varies significantly on each step making it difficult to give meaningful interpretation.\r\n\r\nI have currently patched the issue by adding\r\n```\r\nif using_eval_result:\r\n    eval_results_step = self.__auto_reduce_result_objs(outputs)\r\n```\r\nat the start of `__run_eval_epoch_end()` in the evaluation_loop and replacing the return statement `return eval_results` with `return eval_results + eval_results_step`. This isn't ideal since the callback_metrics are sensitive to the order in which the results are returned, but it works for my case. \r\n\r\nDigging through this part of the loop it doesn't seem too difficult to pass the result from the validation_step to the validation_epoch_end call and keep logging to the same Result. Replacing the default reduce_fx=torch.mean in the on_validation_epoch_end callback with no reduction should prevent issues when trying to reduce scalar results added at the end of the epoch. However, I don't know how this effects other parts of the loop and if this can easily be extended to validation_step_end, but I think it would be a valuable change that increases user-friendliness and makes usage a bit more intuitive."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-08-27T14:22:13Z",
        "body": "happy to think about this after the refactors this week!"
      },
      {
        "user": "gerardsn",
        "created_at": "2020-08-30T11:51:55Z",
        "body": "I have submitted a PR request with minimal code changes that achieves similar behaviour as described below. However, the PR proposes an over simplified solution that may result in other issues. As I have only addressed the issue in the evaluation loop it causes the behaviour of the training and evaluation loop to diverge.\r\n\r\n@williamFalcon I've dug a bit deeper and I think it is possible to add this behaviour without creating breaking changes through the following proposals.\r\n\r\n1. Add option for setting default `reduce_fx` and `tbptt_reduce_fx` functions to `Result()`. This provides some more flexibility for users and allows changing the default option on epoch end from torch.mean to nothing (not sure if ‘nothing’ should be no_op / Identity or that the reduction method should skip reduction when the reduction function value is set to None).\r\n\r\n2. The first few lines of `Result.reduce_on_epoch_end()` can be replaced by `Result.gather()`. I propose to take these lines out and assume the input to `reduce_on_epoch_end()` is the output of `gather()`. By separating the 2 methods it is possible to gather all outputs in a single Result object before the call to `validation_epoch_end()`. (Another option is to check if the input to `reduce_on_epoch_end()` is a list (current expected input is list of Results) to keep current behavior and apply proposed behavior if the input is a Result object.) This object is then passed into `validation_epoch_end()` where other results can then be logged onto it and everything can then be reduced at the end. Required changes for this are:\r\n   - Update `batch_sizes` meta data in Result.gather() such that it is available for reduction.\r\n   - Remove “gather lines” from Result.reduce_on_epoch_end() and change expected input to be a Result object. (Or determine behavior depending on input)\r\n   - In the training_loop’s `__auto_reduce_results_on_epoch_end` add call to gather() and pass results to reduce_on_epoch_end() call. (Optional if the latter's behaviour is input dependent.)\r\n   - The validation_loop needs several changes:\r\n     - In ` __run_eval_epoch_end`:\r\n       - remove all ` __gather_epoch_end_eval_results()` calls and call it once at the start (if `using_eval_result`) to produce list of gathered results per dataloader.\r\n       - change the default `reduce_fx` and `tbptt_reduce_fx` for new log entries to no reduction.\r\n       - Reduce all results after call to overridden epoch_end methods.\r\n     - Simplify ` __gather_epoch_end_eval_results()` to remove redundant parts.\r\n     - Update ` __auto_reduce_result_objs()` to reflect changed behavior of `Result.reduce_on_epoch_end()`. \r\n(Ideally I think the `checkpoint_on` and `early_stop_on` reduction should also move to `Result.reduce_on_epoch_end()` and change the reduction function with the proper batch weighted mean, but I am not sure how this affects other areas or how to handle this if these values are set on epoch end (since batch weighted mean will fail in this case).)\r\n\r\n3. (Not required for issue at hand.) Is the Result object’s meta.{name}.value used anywhere? Currently Result.gather() and Result.reduce_on_epoch_end() do not update this value and retain only the value present in the first batch of the epoch. This can be very confusing. If this key is not used, I propose getting rid of it. (It is updated in Result.padded_gather() but not sure if it actually used anywhere.)\r\n\r\nThe majority of proposed changes are for the Result object and none of the suggestions creates breaking changes as far as I can tell. It simplifies the overall code a bit, adds functionality, and shouldn't add overhead since I am still performing the same operations on the data albeit in a sightly different order. However, I have only tested this on a single GPU, so I do not know if it works in distributed setups, and I have not checked if this works for overriding training_epoch_end. Let me know what you think.\r\n"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-16T17:24:25Z",
        "body": "@williamFalcon please take a look!"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-10-02T20:47:11Z",
        "body": "@williamFalcon please see if this can be closed"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-10-05T15:13:36Z",
        "body": "yes, please use the new API which fixes this!"
      },
      {
        "user": "bugsuse",
        "created_at": "2022-05-21T14:34:57Z",
        "body": "@williamFalcon Hello, could you point out which one API could fix this? I want to calculate metrics at the end of each epoch. Thanks so much!"
      }
    ]
  },
  {
    "number": 3185,
    "title": "Value out of range (expected to be in range of [-1, 0], but got 1)",
    "created_at": "2020-08-26T05:19:53Z",
    "closed_at": "2020-09-01T13:17:53Z",
    "labels": [
      "bug",
      "help wanted",
      "accelerator: tpu"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3185",
    "body": "## Value out of range (expected to be in range of [-1, 0], but got 1)\r\n**Exception in device=TPU:5: torch_xla/csrc/helpers.cpp:97 : Check failed: min_shape_dim <= dim && dim <= max_shape_dim**\r\n\r\nFollowing is the stack trace when I am using all 8 TPU cores on Kaggle. The exact same code with \r\n\r\n> sync_dist=False \r\n\r\nworks completely fine on Kaggle GPU.\r\n\r\n \r\n\r\n> Value out of range (expected to be in range of [-1, 0], but got 1)Exception in device=TPU:5: torch_xla/csrc/helpers.cpp:97 : Check failed: min_shape_dim <= dim && dim <= max_shape_dim \r\n> *** Begin stack trace ***\r\n> \ttensorflow::CurrentStackTrace()\r\n> \ttorch_xla::XlaHelpers::GetCanonicalDimensionIndex(long long, long long)\r\n> \ttorch_xla::XlaHelpers::MakeTransposePermutation(long long, long long, long long)\r\n> \ttorch_xla::XLATensor::transpose(torch_xla::XLATensor const&, long long, long long)\r\n> \ttorch_xla::AtenXlaType::t(at::Tensor const&)\r\n> \tc10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&> >, at::Tensor (at::Tensor const&)>::call(c10::OperatorKernel*, at::Tensor const&)\r\n> \t\r\n> \tat::t(at::Tensor const&)\r\n> \t\r\n> \t\r\n> \t\r\n> \tat::Tensor::t() const\r\n> \t\r\n> \t_PyMethodDef_RawFastCallKeywords\r\n> \t_PyMethodDescr_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \t_PyFunction_FastCallDict\r\n> \t_PyObject_Call_Prepend\r\n> \tPyObject_Call\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallDict\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallDict\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallDict\r\n> \t_PyObject_Call_Prepend\r\n> \t\r\n> \t_PyObject_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \t_PyFunction_FastCallDict\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \tPyEval_EvalCodeEx\r\n> \tPyEval_EvalCode\r\n> \t\r\n> \t_PyMethodDef_RawFastCallKeywords\r\n> \t_PyCFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyGen_Send\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyGen_Send\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyGen_Send\r\n> \t_PyMethodDef_RawFastCallKeywords\r\n> \t_PyMethodDescr_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \t_PyFunction_FastCallDict\r\n> \t_PyObject_Call_Prepend\r\n> \tPyObject_Call\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t\r\n> \t_PyMethodDef_RawFastCallKeywords\r\n> \t_PyCFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t\r\n> \t_PyMethodDef_RawFastCallKeywords\r\n> \t_PyCFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t\r\n> \t_PyMethodDef_RawFastCallKeywords\r\n> \t_PyCFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyEval_EvalCodeWithName\r\n> \t_PyFunction_FastCallDict\r\n> \t_PyObject_Call_Prepend\r\n> \tPyObject_Call\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyGen_Send\r\n> \t_PyMethodDef_RawFastCallKeywords\r\n> \t_PyMethodDescr_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallKeywords\r\n> \t_PyEval_EvalFrameDefault\r\n> \t_PyFunction_FastCallDict\r\n> *** End stack trace ***\r\n> \r\n\r\n\r\nHere is the code for my LightningModule\r\n\r\n\r\n```python\r\nclass GraphAEStage1(pl.LightningModule):\r\n  def __init__(self, config):\r\n    super(GraphAEStage1, self).__init__()\r\n    max_deg = config['poc_max_degree']\r\n    input_dim = config['input_dim']\r\n    hidden_feat = config['hidden_feat_dim']\r\n    layerid = config['layerid']\r\n    self.lr = config['lr']\r\n  \r\n    self.encoder = GraphEncoderStage1(max_deg, input_dim, hidden_feat[layerid])\r\n    self.decoder = GraphDecoderStage1(max_deg, input_dim, hidden_feat[layerid])\r\n    \r\n    self.encoder.apply(self.init_weights)\r\n    \r\n    self.crit = torch.nn.L1Loss()\r\n    self.node_mae = MAE()\r\n    self.adj_mae = MAE()\r\n    \r\n    self.decoder.self_linear.weight = nn.Parameter(self.encoder.self_linear.weight.permute(1,0))\r\n    for d in range(config['poc_max_degree']):\r\n        self.decoder.neigh_linear[d].weight = nn.Parameter(self.encoder.neigh_linear[d].weight.permute(1,0))\r\n\r\n  def training_step(self, batch, batch_idx):\r\n    loss, tf_node, _, _, _ = self.shared_step(batch)\r\n    result = pl.TrainResult(minimize=loss)\r\n    result.log('train_loss', loss, prog_bar = True, logger = True, on_step = False, on_epoch = True, sync_dist = True)\r\n    return result\r\n\r\n  def validation_step(self, batch, batch_idx):\r\n    loss, tf_node, recon_node, val_node_mae, val_adj_mae = self.shared_step(batch)\r\n    \r\n    logs = {'val_loss': loss, 'node_mae': val_node_mae, 'adj_mae': val_adj_mae}\r\n    result = pl.EvalResult(checkpoint_on = loss)\r\n    result.log_dict(logs, prog_bar = True, logger = True, on_step = False, on_epoch = True, sync_dist = True)\r\n    \r\n    return result               \r\n\r\n  def shared_step(self, batch):\r\n    node, adj, deg, mask = batch\r\n    tf_node, avg_adj = self.encoder(node, adj, deg, mask)\r\n    recon_node, recon_adj = self.decoder(tf_node, deg)\r\n\r\n    \r\n    node_loss = self.criterion(recon_node, node)\r\n    adj_loss = self.criterion(recon_adj, avg_adj)\r\n    loss = node_loss + adj_loss\r\n    \r\n    node_mae = self.node_mae(node, recon_node)\r\n    adj_mae = self.adj_mae(recon_adj, avg_adj)\r\n    \r\n    return loss, tf_node, recon_node, node_mae, adj_mae\r\n\r\n  def configure_optimizers(self):\r\n    optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\r\n    return optimizer\r\n\r\n  def criterion(self, output, target):\r\n    return self.crit(output, target)\r\n\r\n  def init_weights(self, m):\r\n    if type(m) == nn.Linear:\r\n        torch.nn.init.xavier_uniform(m.weight)\r\n        m.bias.data.fill_(0.00)  \r\n```\r\n\r\nThere are similar issues reported on PyTorch XLA repo, but many of them were using CrossEntropyLoss. I am not using CrossEntropyLoss here. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3185/comments",
    "author": "VirajBagal",
    "comments": [
      {
        "user": "ananyahjha93",
        "created_at": "2020-08-26T15:24:23Z",
        "body": "@lezwon @Borda mind taking a look at this?"
      },
      {
        "user": "lezwon",
        "created_at": "2020-08-26T15:41:59Z",
        "body": "@VirajBagal could you share the notebook?"
      },
      {
        "user": "VirajBagal",
        "created_at": "2020-08-26T16:32:46Z",
        "body": "@lezwon Due to some privacy issues, I can't share it publicly here. I have shared it with you on Kaggle. Please check the \"Shared With You\" tab on Kaggle. "
      },
      {
        "user": "lezwon",
        "created_at": "2020-08-26T16:46:24Z",
        "body": "Sure.. Will do 😊👍"
      },
      {
        "user": "lezwon",
        "created_at": "2020-08-28T17:42:29Z",
        "body": "It seems xla does not support the `tensor.t()` operation on 1-dimensional tensors. @zcain117 could you help us out? `tensor([1,2], device=xm.xla_device()).t()` seems to be throwing the error mentioned when called. PyTorch usually returns 1d tensors as is."
      }
    ]
  },
  {
    "number": 3169,
    "title": "RuntimeError: Cannot replicate if number of devices (1) is different from 8 Exception in device=TPU:2: Cannot replicate if number of devices (1) is different from 8",
    "created_at": "2020-08-25T17:49:27Z",
    "closed_at": "2020-08-26T04:48:31Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3169",
    "body": "I am getting the above (title) error on Kaggle when I am doing \r\n\r\n`trainer.fit(model, dm)`\r\n\r\nwhere `trainer = Trainer(tpu_cores = 8, logger = wandblogger, max_epochs = 20)`\r\n\r\nFollowing is the trace:\r\n\r\n>   File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\r\n    _start_fn(index, pf_cfg, fn, args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\r\n    _setup_replication()\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\r\n    _start_fn(index, pf_cfg, fn, args)\r\nException in device=TPU:2: Cannot replicate if number of devices (1) is different from 8\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/core/xla_model.py\", line 287, in xla_replication_devices\r\n    format(len(local_devices), len(kind_devices)))\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\r\n    xm.set_replication(device, [device])\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/core/xla_model.py\", line 315, in set_replication\r\n    replication_devices = xla_replication_devices(devices)\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\r\n    _setup_replication()\r\n\r\n\r\n\r\nI am using Kaggle Kernel. Here is my LightningDataModule.\r\n\r\n```python\r\nclass ProteinModule(pl.LightningDataModule):\r\n  def __init__(self, config, path, invalid_proteins):\r\n    super().__init__()\r\n\r\n    self.path = path\r\n    self.bs = config['bs']\r\n    self.state = config['seed']\r\n    self.invalid = invalid_proteins\r\n\r\n  def setup(self, stage=None):\r\n\r\n    all_protein_ids = os.listdir(self.path)\r\n    all_protein_ids = [protein for protein in all_protein_ids if protein not in self.invalid]\r\n    train_ids, val_ids = train_test_split(all_protein_ids, test_size = 0.2, random_state = self.state)\r\n\r\n    self.traindataset = ProteinDataset(path, train_ids)\r\n    self.valdataset = ProteinDataset(path, val_ids)\r\n\r\n\r\n  def train_dataloader(self):\r\n\r\n    return DataLoader(self.traindataset, batch_size = self.bs, shuffle = True)\r\n\r\n  def val_dataloader(self):\r\n\r\n    return DataLoader(self.valdataset, batch_size = self.bs, shuffle = False)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3169/comments",
    "author": "VirajBagal",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-08-25T18:24:46Z",
        "body": "Did you call `xm.xla_device()` somewhere? Or maybe try restarting the kernel and run again."
      },
      {
        "user": "VirajBagal",
        "created_at": "2020-08-26T04:47:39Z",
        "body": "Thanks, @rohitgr7, I was using `xm.xla_device()` in one place. This error is resolved but I have got another error now. I'll open another issue for it. "
      }
    ]
  },
  {
    "number": 3144,
    "title": "ONNX model does not save on GPU",
    "created_at": "2020-08-25T03:35:04Z",
    "closed_at": "2020-08-26T16:22:20Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3144",
    "body": "## 🐛 Bug\r\n\r\nAttempting to export on ONNX after training model on GPU, throws an error is the `input_sample` or `example_input_array` is not a CUDA tensor.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Train a model on GPU\r\n2. Try to export to ONNX when  `self.example_input_array = torch.zeros(1, 1, 500, 500)` or `input_sample = torch.zeros(1, 1, 500, 500)`\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-32-cd8009a0b6a3> in <module>\r\n      1 filepath = 'model.onnx'\r\n----> 2 model.to_onnx(filepath, export_params=True)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py in to_onnx(self, file_path, input_sample, **kwargs)\r\n   1721         if 'example_outputs' not in kwargs:\r\n   1722             self.eval()\r\n-> 1723             kwargs['example_outputs'] = self(input_data)\r\n   1724 \r\n   1725         torch.onnx.export(self, input_data, file_path, **kwargs)\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    548             result = self._slow_forward(*input, **kwargs)\r\n    549         else:\r\n--> 550             result = self.forward(*input, **kwargs)\r\n    551         for hook in self._forward_hooks.values():\r\n    552             hook_result = hook(self, input, result)\r\n\r\n<ipython-input-24-51cae3b5e57f> in forward(self, inputs)\r\n     20 \r\n     21     def forward(self, inputs):\r\n---> 22         return self.model(inputs)\r\n     23 \r\n     24     def training_step(self, batch, batch_idx):\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    548             result = self._slow_forward(*input, **kwargs)\r\n    549         else:\r\n--> 550             result = self.forward(*input, **kwargs)\r\n    551         for hook in self._forward_hooks.values():\r\n    552             hook_result = hook(self, input, result)\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py in forward(self, input)\r\n     98     def forward(self, input):\r\n     99         for module in self:\r\n--> 100             input = module(input)\r\n    101         return input\r\n    102 \r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    548             result = self._slow_forward(*input, **kwargs)\r\n    549         else:\r\n--> 550             result = self.forward(*input, **kwargs)\r\n    551         for hook in self._forward_hooks.values():\r\n    552             hook_result = hook(self, input, result)\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py in forward(self, input)\r\n    351 \r\n    352     def forward(self, input):\r\n--> 353         return self._conv_forward(input, self.weight)\r\n    354 \r\n    355 class Conv3d(_ConvNd):\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight)\r\n    348                             _pair(0), self.dilation, self.groups)\r\n    349         return F.conv2d(input, weight, self.bias, self.stride,\r\n--> 350                         self.padding, self.dilation, self.groups)\r\n    351 \r\n    352     def forward(self, input):\r\n\r\nRuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same\r\n```\r\n\r\n\r\n#### Code sample\r\n\r\nfilepath = 'model.onnx'\r\nmodel.to_onnx(filepath, export_params=True)\r\n\r\n### Expected behavior\r\n\r\nShould automatically convert `example_input_array` or `input_sample` to the device type and save the model to ONNX.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3144/comments",
    "author": "lezwon",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-08-25T07:09:53Z",
        "body": "I would say that the problem could be the distributed way, mind check running only on a single GPU?"
      },
      {
        "user": "lezwon",
        "created_at": "2020-08-25T09:31:24Z",
        "body": "I ran this on Kaggle notebook. When I tried to save after training, it threw the error."
      }
    ]
  },
  {
    "number": 3113,
    "title": "TypeError in closure_loss = closure_loss / self.accumulate_grad_batches for Cross_entropy loss",
    "created_at": "2020-08-23T14:55:43Z",
    "closed_at": "2020-08-24T18:43:38Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3113",
    "body": "## 🐛 Bug\r\n\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n   1055         # (if accumulate_grad_batches = 1 no effect)\r\n   1056         closure_loss = training_step_output.minimize if is_result_obj else training_step_output.batch_loss\r\n-> 1057         closure_loss = closure_loss / self.accumulate_grad_batches\r\n   1058 \r\n   1059         # the loss will get scaled for amp. avoid any modifications to it\r\n\r\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\r\n```\r\n#### Code sample\r\n\r\n```\r\nclass CustomModel(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.cnn_model = nn.Sequential(\r\n            nn.Conv2d(1, 6, kernel_size = 5),\r\n            nn.ReLU(),\r\n            nn.AvgPool2d(2, stride = 2),\r\n            nn.Conv2d(6, 16, kernel_size = 5),\r\n            nn.ReLU(),\r\n            nn.AvgPool2d(2, stride = 2),\r\n            nn.Conv2d(16,32,kernel_size = 5),\r\n            nn.ReLU(),\r\n            nn.AvgPool2d(2, stride = 2))\r\n\r\n        self.fc_model = nn.Sequential(\r\n            nn.Linear(2592, 1024), # (N, 2592) -> (N, 1024)\r\n            nn.ReLU(),\r\n            nn.Linear(1024, 30))  # (N, 1024)  -> (N, 30)) #30 classes\r\n\r\n    def forward(self, x):\r\n        x = self.cnn_model(x)\r\n        # print(x.shape) \r\n        x = x.view(x.size(0), -1)\r\n        # print(x.shape)    \r\n        x = self.fc_model(x)\r\n        # print(x.shape)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        # acc = FM.accuracy(y_hat, y)\r\n        result = pl.TrainResult()\r\n        print('f')\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        acc = FM.accuracy(y_hat, y)\r\n        result = pl.EvalResult(checkpoint_on=loss)\r\n        result.log('val_loss', loss, prog_bar=True)\r\n        result.log('val_acc', acc, prog_bar=True)\r\n        print('f')\r\n        return result\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\r\n        return optimizer\r\n\r\n    def train_dataloader(self):\r\n        train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=32)\r\n        # print(\"Length of the train_loader:\", len(train_loader))\r\n        return train_loader\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(dataset=val_dataset, shuffle=False, batch_size=32)\r\n```\r\n\r\nThe error occurs when I am fitting the model to train. Using lightning 0.9.0 on colab. I am loading dataset by mounting drive and using torchvision datasets.ImageFolder function.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3113/comments",
    "author": "srijansingh53",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-23T14:56:22Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-23T15:47:18Z",
        "body": "changing `result = pl.TrainResult()` to `result = pl.TrainResult(minimize=loss)` is all you need."
      },
      {
        "user": "srijansingh53",
        "created_at": "2020-08-24T18:43:38Z",
        "body": "yes, it worked. Thank you @rohitgr7 . Closing the issue"
      }
    ]
  },
  {
    "number": 3101,
    "title": "TypeError: Can't instantiate abstract class GraphAEStage1 with abstract methods forward",
    "created_at": "2020-08-22T10:13:05Z",
    "closed_at": "2020-08-29T12:55:16Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3101",
    "body": "Below is my LightningModule. GraphEncoderStage1 and GraphDecoderStage1 in the `__init__`   are the usual nn.Modules with their own `__init__` and forward methods. When I instantiate GraphAEStage1 as\r\n\r\n`model = GraphAEStage1(config, layer_id = 0)`\r\n\r\n I get the following error: \r\n\r\n> TypeError: Can't instantiate abstract class GraphAEStage1 with abstract methods forward\r\n\r\nPlease help. \r\n\r\n```python\r\nclass GraphAEStage1(pl.LightningModule):\r\n  def __init__(self, config, layer_id):\r\n    super(GraphAEStage1, self).__init__()\r\n    \r\n    if layer_id==0:\r\n      self.encoder = GraphEncoderStage1(config['poc_max_degree'], config['input_dim'], config['hidden_feat_dim'][layer_id])\r\n      self.decoder = GraphDecoderStage1(config['poc_max_degree'], config['input_dim'], config['hidden_feat_dim'][layer_id])\r\n    else:\r\n      self.encoder = GraphEncoderStage1(config['poc_max_degree'], config['hidden_feat_dim'][layer_id-1], config['hidden_feat_dim'][layer_id])\r\n      self.decoder = GraphDecoderStage1(config['poc_max_degree'], config['hidden_feat_dim'][layer_id-1], config['hidden_feat_dim'][layer_id])\r\n\r\n    # Weight sharing\r\n    \r\n    for i in range(max_degree):\r\n      self.decoder.neigh_weights[i] = nn.Parameter(torch.t(self.encoder.neigh_weights[i]))\r\n    \r\n    self.decoder.self_weights = nn.Parameter(torch.t(self.encoder.self_weights))\r\n\r\n  def training_step(self, batch, batch_idx):\r\n\r\n    loss, tf_node = self.shared_step(batch)\r\n    result = pl.TrainResult(minimize=loss)\r\n    return result.log('train_loss', loss, prog_bar = True, logger = True, on_step = True, on_epoch = True,\r\n                          reduce_fx = torch.mean), tf_node\r\n\r\n  def validation_step(self, batch, batch_idx):\r\n\r\n    loss, tf_node = self.shared_step(batch)\r\n    result = pl.EvalResult(checkpoint_on=loss)\r\n    return result.log('val_loss', loss, prog_bar = True, logger = True, on_step = True, on_epoch = True,\r\n                          reduce_fx = torch.mean), tf_node                \r\n\r\n\r\n  def shared_step(self, batch):\r\n\r\n    node, adj, deg, mask = batch\r\n    tf_node, avg_adj = self.encoder(node, adj, deg, mask)\r\n    recon_node, recon_adj = self.decoder(tf_node, deg)\r\n\r\n    node_loss = nn.functional.mse_loss(recon_node, node)\r\n    adj_loss = nn.functional.mse_loss(recon_adj, avg_adj)\r\n    loss = node_loss + adj_loss\r\n\r\n    return loss, tf_node\r\n\r\n  def configure_optimizers(self):\r\n\r\n    optimizer = torch.optim.Adam(self.parameters(), lr=config['lr'])\r\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = config['patience'], factor = config['factor'])\r\n    return [optimizer], [scheduler]\r\n```\r\n  ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3101/comments",
    "author": "VirajBagal",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-22T10:13:46Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-29T13:22:50Z",
        "body": "Hey @VirajBagal, did you find the reason for the error here??"
      },
      {
        "user": "VirajBagal",
        "created_at": "2020-08-30T05:19:16Z",
        "body": "Hey @rohitgr7 , the following helped me. So I closed the issue. Thanks for asking! \r\n\r\n> LightningModule is like nn.Module only so you need to replace def shared_step(self, batch) with def forward(self, batch) then you can use either self(batch) or self.forward(batch) instead of self.shared_step(batch)."
      }
    ]
  },
  {
    "number": 3076,
    "title": "Support Trainer.add_argparse_args for ArgumentGroup",
    "created_at": "2020-08-20T15:04:03Z",
    "closed_at": "2020-11-12T09:41:52Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "discussion"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3076",
    "body": "## 🚀 Feature\r\nSupport `Trainer.add_argparse_args` for `argparse._ArgumentGroup`\r\n\r\n### Motivation\r\n\r\nAdding all of the Trainers args to an existing parser clutters its help message with many arguments. Ideally, I would like to have these in a separate argument group.\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nCurrently:\r\n```python\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--my_arg\")\r\nparser = pl.Trainer.add_argparse_args(parser)\r\nparser.print_help()\r\n```\r\n```bash\r\nusage: script.py [-h] [--my_arg MY_ARG] [--logger [LOGGER]] [--checkpoint_callback [CHECKPOINT_CALLBACK]] [--early_stop_callback [EARLY_STOP_CALLBACK]] [--default_root_dir DEFAULT_ROOT_DIR] [--gradient_clip_val GRADIENT_CLIP_VAL] (... and many more)\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  --my_arg MY_ARG\r\n  --logger [LOGGER]     autogenerated by pl.Trainer\r\n  --checkpoint_callback [CHECKPOINT_CALLBACK]\r\n                        autogenerated by pl.Trainer\r\n  --early_stop_callback [EARLY_STOP_CALLBACK]\r\n                        autogenerated by pl.Trainer\r\n  --default_root_dir DEFAULT_ROOT_DIR\r\n                        autogenerated by pl.Trainer\r\n  --gradient_clip_val GRADIENT_CLIP_VAL\r\n                        autogenerated by pl.Trainer\r\n  (... and many more)\r\n\r\n```\r\n\r\nDesired:\r\n```python\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--my_arg\")\r\npl_parser = parser.add_argument_group(title=\"pytorch-lightning Trainer arguments\")\r\npl_parser = pl.Trainer.add_argparse_args(pl_parser)\r\nparser.print_help()\r\n```\r\n```bash\r\nusage: script.py [-h] [--my_arg MY_ARG] [--logger [LOGGER]] [--checkpoint_callback [CHECKPOINT_CALLBACK]] [--early_stop_callback [EARLY_STOP_CALLBACK]] [--default_root_dir DEFAULT_ROOT_DIR] [--gradient_clip_val GRADIENT_CLIP_VAL] (... and many more)\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  --my_arg MY_ARG\r\n\r\npytorch-lightning Trainer arguments:\r\n  --logger [LOGGER]     autogenerated by pl.Trainer\r\n  --checkpoint_callback [CHECKPOINT_CALLBACK]\r\n                        autogenerated by pl.Trainer\r\n  --early_stop_callback [EARLY_STOP_CALLBACK]\r\n                        autogenerated by pl.Trainer\r\n  --default_root_dir DEFAULT_ROOT_DIR\r\n                        autogenerated by pl.Trainer\r\n  --gradient_clip_val GRADIENT_CLIP_VAL\r\n                        autogenerated by pl.Trainer\r\n  (... and many more)\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3076/comments",
    "author": "carmocca",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-20T15:05:08Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-08-20T17:40:11Z",
        "body": "@nateraw include this in your thing\r\n"
      },
      {
        "user": "nateraw",
        "created_at": "2020-08-20T17:47:52Z",
        "body": "@ananyahjha93 this will be encapsulated by it for sure.\r\n\r\n@carmocca feature like this is coming! We are rethinking how it all should look. Thanks for the thoughtful feature request 😄 "
      },
      {
        "user": "edenlightning",
        "created_at": "2020-10-29T22:03:15Z",
        "body": "@nateraw update on your decision here"
      },
      {
        "user": "carmocca",
        "created_at": "2020-11-12T09:41:52Z",
        "body": "Closing this issue. I am using jsonargparse now, given it fits exactly my needs as demonstrated in #4492.\r\n\r\nAlso, there are several issues on this matter, closing this one so there is one less place to follow."
      }
    ]
  },
  {
    "number": 3047,
    "title": "ModelCheckpoint instance to JSON",
    "created_at": "2020-08-19T08:18:34Z",
    "closed_at": "2020-09-27T12:39:41Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3047",
    "body": "## 🚀 Feature\r\n\r\nHave a `to_json(path)` function that writes the `best_k_models` to a json file in `path`.\r\n\r\n### Motivation\r\n\r\nWhy having a JSON is good? \r\nThen you don't need to load the checkpoints to know the metrics and things like a posteriori stochastic weight averaging gets  easier. \r\n\r\nIt used to be easier in 0.6, but loss values are now tensors, so it's not directly serializable.\r\nThis is currently needed: \r\n```python\r\nbest_k = {k: v.item() for k, v in checkpoint.best_k_models.items()}\r\nwith open(json_path, \"w\") as f:\r\n    json.dump(best_k, f, indent=0)\r\n```\r\n\r\nInstead, we could have \r\n```python\r\ncheckpoint.to_json(json_path)\r\n```\r\n\r\nI'm ready to make a PR for it. Just tell me if this would be useful for you. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3047/comments",
    "author": "mpariente",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-08-19T14:09:33Z",
        "body": "should this file be saved with every checkpoint, or one file that gets updated/overwritten? "
      },
      {
        "user": "mpariente",
        "created_at": "2020-08-19T15:04:43Z",
        "body": "I just thought about a single function that can be called by the user at the end of training. \r\nBut actually, updating/overwritting at every checkpoint also would be cool, at least the user never has to call it. \r\nLink to the PR #3048 "
      }
    ]
  },
  {
    "number": 3036,
    "title": "freeze TB to 2.2.0",
    "created_at": "2020-08-18T16:43:27Z",
    "closed_at": "2020-08-18T20:26:18Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3036",
    "body": "",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3036/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-18T16:44:08Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      }
    ]
  },
  {
    "number": 3033,
    "title": "0.9.0 rc16  tensorboard 2.3.0 can't find hparams data",
    "created_at": "2020-08-18T08:37:23Z",
    "closed_at": "2020-08-18T17:54:02Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3033",
    "body": "tensorboard version: 2.3.0\r\npytorch-lightning version: 0.9.0 rc16\r\n\r\nmy code:\r\n```\r\nclass Video_Base(pl.LightningModule):\r\n    def __init__(self, hparams, *args, **kwargs):\r\n        self.hparams = hparams\r\n```\r\ntensorbord 2.2.0 is ok, 2.3.0 can't show hparams, says no hparams data found",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3033/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "ananyahjha93",
        "created_at": "2020-08-18T16:23:16Z",
        "body": "@xiadingZ instead of setting self.hparams, can you try using ```self.save_hyperparameters()``` to save anything into self.hparams passed to the LightningModule?"
      },
      {
        "user": "Borda",
        "created_at": "2020-08-18T16:51:54Z",
        "body": "lets freeze TB==2.2.0 for now and reopen this after 0.9 is out..."
      },
      {
        "user": "teddykoker",
        "created_at": "2020-08-18T17:54:02Z",
        "body": "Freezing TB, #3039, will reopen after 0.9"
      }
    ]
  },
  {
    "number": 3032,
    "title": "Epoch counting is one-off in multiple instances",
    "created_at": "2020-08-18T07:32:29Z",
    "closed_at": "2020-08-20T10:27:49Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3032",
    "body": "🐛 Bug\r\n\r\nTwo issues occur:\r\n\r\n1. The final epoch does not save a checkpoint during training.\r\n2. Resuming from a checkpoint N will start the epochs at N+2.\r\n\r\n### Expected behavior\r\n\r\n1. Final checkpoint should save a .ckpt file, as usual.\r\n2. Should resume from epoch N+1.\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla V100-DGXS-16GB\r\n\t\t- Tesla V100-DGXS-16GB\r\n\t\t- Tesla V100-DGXS-16GB\r\n\t\t- Tesla V100-DGXS-16GB\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.18.1\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0\r\n\t- pytorch-lightning: 0.9.0rc12\r\n\t- tensorboard:       2.2.1\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         x86_64\r\n\t- python:            3.7.7\r\n\t- version:           #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3032/comments",
    "author": "AAnoosheh",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-08-18T18:04:58Z",
        "body": "@AAnoosheh Honestly I do not understand how the PR you linked relates to the bug your report. Did you mean to link another issue? \r\n\r\n> The final epoch does not save a checkpoint during training.\r\n\r\nI don't experience this. The epoch number is 0-indexed, and by default it only saves best checkpoints. Could one of these reasons be why you may think this is a bug?\r\n\r\nHow can I reproduce the second issue?"
      },
      {
        "user": "AAnoosheh",
        "created_at": "2020-08-18T18:22:15Z",
        "body": "Sorry I should have clarified I use the following to save every epoch:\r\n\r\n`pl.callbacks.ModelCheckpoint(save_top_k=-1, verbose=True)`\r\n\r\nThe second is done via `Trainer(resume_from_checkpoint=some_ckpt_file)`\r\n\r\nI assume some change was made to move epochs to 0-index, when previously they were 1-indexed, and there's a mismatch now.\r\n\r\nEDIT:\r\nI also have no idea how a PR was linked in my comment. Those numbers came out of nowhere from the auto-generated issue template."
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-08-19T14:35:57Z",
        "body": "@AAnoosheh so when you run ```pl.callbacks.ModelCheckpoint(save_top_k=-1, verbose=True)``` all the checkpoints are saved, however we do not save the last one as 'last.ckpt'. Also, the checkpoints are numbered from 0, so if you run for 4 epochs, the last checkpoint saved will be 'epoch=3.ckpt' and when you resume, it resumes from the expected 5th epoch. \r\n\r\nUpdating tests and code for this"
      }
    ]
  },
  {
    "number": 3027,
    "title": "parsing of track_grad_norm when passed as argument",
    "created_at": "2020-08-18T00:52:56Z",
    "closed_at": "2020-08-20T17:49:35Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3027",
    "body": "When using ` Trainer.from_argparse_args` & passing `track_grad_norm` as cli arg.\r\nFollowing error is thrown:\r\n\r\n```\r\ntrack_grad_norm can be an int, a float or 'inf' (infinity norm).\r\n```\r\n### To Reproduce\r\n\r\nRun `python demo.py --gpus \"1,\" --track_grad_norm 2`\r\n\r\n#### Code sample\r\n```\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\nfrom pl_bolts.models.gans import BasicGAN\r\n\r\ntrainer = Trainer()\r\n\r\nparser = argparse.ArgumentParser(description='demo')\r\nparser = trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\n\r\nmodel = BasicGAN(args)\r\ntrainer = Trainer.from_argparse_args(args)\r\ntrainer.fit(model)\r\n```\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.6.0\r\n        - pytorch-lightning: 0.9.0rc16\r\n        - tensorboard:       2.2.0\r\n        - tqdm:              4.45.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                -\r\n        - processor:         x86_64\r\n        - python:            3.6.10\r\n        - version:           #75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3027/comments",
    "author": "tshrjn",
    "comments": [
      {
        "user": "tshrjn",
        "created_at": "2020-08-18T01:27:23Z",
        "body": "Also, I think bolts can easily be used for continuous testing with a few basic models & `iter.combination` of all Trainer `args` for say 10 steps each. This would greatly improve the catching of small issues."
      },
      {
        "user": "tshrjn",
        "created_at": "2020-08-18T01:57:27Z",
        "body": "Another similar issue is with `limit_train_batches` which according to the docs can be a float, but is expected by the `argparse` to be a float. Thus, again motivating a better testing mechanism."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-18T17:43:00Z",
        "body": "argparse does not support multiple types. The only solution I see is to define it as string and parse it back to the real type when we load the args."
      }
    ]
  },
  {
    "number": 3025,
    "title": "Auto-scaling batch-size not compatible with half precision training",
    "created_at": "2020-08-17T23:02:32Z",
    "closed_at": "2020-08-19T20:41:34Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3025",
    "body": "<!-- A clear and concise description of what the bug is. -->\r\n\r\nUsing `precision=16` and `auto_scale_batch_size=True` yields `'NoneType' object has no attribute 'state_dict'` error\r\n\r\n### Expected behavior\r\n\r\nLargest batch size should be found when using 16 bit precision\r\n\r\n### Environment\r\n- PyTorch Version (e.g., 1.0): 1.6\r\n - OS (e.g., Linux): Windows 10\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 10/7\r\n - GPU models and configuration: 2080ti",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3025/comments",
    "author": "iantimmis",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-17T23:03:17Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-08-18T16:55:14Z",
        "body": "it seems that loading has some issue\r\n@iantimmis are interested in sending a PR?"
      }
    ]
  },
  {
    "number": 3018,
    "title": "Add support for changing the checkpoint directories names",
    "created_at": "2020-08-17T16:41:13Z",
    "closed_at": "2020-11-10T18:45:48Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3018",
    "body": "In checkpoint callbacks it always creates a folder called checkpoints in the provided root.\r\nUsers should be able to change that name from \"checkpoints\" to anything else.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3018/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2020-09-17T19:59:39Z",
        "body": "@williamFalcon what is left to do?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-03T17:04:18Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 3016,
    "title": "Memory allocated on gpu:0 when using torch.cuda.empty_cache()",
    "created_at": "2020-08-17T14:02:11Z",
    "closed_at": "2020-09-18T12:23:00Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3016",
    "body": "## 🐛 Bug\r\nPytorch lightning calls torch.cuda.empty_cache() at times, e.g. at the end of the training loop. When the trainer is set to run on GPUs other than gpu:0, it still allocates memory on gpu:0 when running torch.cuda.empty_cache(). Apparently this is the initial device context, but it can be avoided. For example,\r\n```\r\nwith torch.cuda.device('cuda:1'):\r\n    torch.cuda.empty_cache()\r\n```\r\nIf the cache is emptied in this way, it will not allocate memory on any other gpu other than the one specified\r\n\r\nThis seems to be the same issue as in #458, but was never resolved and is still an issue.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Create a pl.Trainer with gpus=[1]\r\n2. Fit a model on gpu:1\r\n3. torch.cuda.empty_cache() runs in run_training_teardown at the end of the training loop\r\n4. `nvidia-smi` shows memory usage on gpu:0\r\n5. If gpu:0 already had high memory allocation because of another job, then it will throw a CUDA out of memory error\r\n```\r\n.../pytorch_lightning/trainer/training_loop.py in run_training_teardown(self)\r\n   1153             model = self.get_model()\r\n   1154             model.cpu()\r\n-> 1155             torch.cuda.empty_cache()\r\n   1156 \r\n   1157     def training_forward(self, batch, batch_idx, opt_idx, hiddens):\r\n\r\n.../torch/cuda/memory.py in empty_cache()\r\n     84     \"\"\"\r\n     85     if is_initialized():\r\n---> 86         torch._C._cuda_emptyCache()\r\n     87 \r\n     88 \r\n\r\nRuntimeError: CUDA error: out of memory\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n```\r\ntrainer = Trainer(gpus=[1])\r\ntrainer.fit(task, train_dataloader, val_dataloader)\r\n```\r\n\r\n### Expected behavior\r\nOnly gpu:1 should be used when training this model.\r\n\r\n### Environment\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce RTX 2080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.18.1\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.0\r\n\t- pytorch-lightning: 0.9.0rc12\r\n\t- tensorboard:       2.2.1\r\n\t- tqdm:              4.46.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.3\r\n\t- version:           18.04.1-Ubuntu\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3016/comments",
    "author": "kmistry-wx",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-17T14:02:49Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "nateraw",
        "created_at": "2020-08-17T17:18:04Z",
        "body": "Mind submitting a PR for this?"
      },
      {
        "user": "justusschock",
        "created_at": "2020-08-18T07:00:47Z",
        "body": "I remember that there was a bug in pytorch, which always created a cuda context (and thus allocated memory) on the default gpu (which is the first one). Not sure if this is also the issue here though"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-08-19T23:36:41Z",
        "body": "yeah, i think this is a pytorch bug... will investigate after 0.9"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-16T17:23:37Z",
        "body": "@kmistry-wx does the issue persist?"
      },
      {
        "user": "kmistry-wx",
        "created_at": "2020-09-18T08:10:28Z",
        "body": "@edenlightning After upgrading (PyTorch 1.5.0 -> 1.6.0, Pytorch Lightning 0.9.0rc12 -> 0.9.0), I can no longer reproduce. A trainer run on gpu:1 seems to only use resources from gpu:1 now\r\n"
      },
      {
        "user": "D-X-Y",
        "created_at": "2021-03-30T08:41:21Z",
        "body": "I'm using 1.8.0 on a Linux with CUDA 11.2. I also found this problem. Everything of my model is on GPU:1, when I call torch.cuda.empty_cache. It increased some memory usage on GPU:0"
      },
      {
        "user": "xingqian2018",
        "created_at": "2021-04-06T17:41:55Z",
        "body": "Same issue has been found on 1.6.0"
      },
      {
        "user": "YunruiZhang",
        "created_at": "2023-04-06T04:19:18Z",
        "body": "Same issue found on 1.13.1"
      }
    ]
  },
  {
    "number": 3006,
    "title": "Accuracy metrics: In the case of all class indices of a target tensor is 0, throwing error.",
    "created_at": "2020-08-16T16:18:50Z",
    "closed_at": "2020-10-01T11:00:43Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3006",
    "body": "## 🐛 Bug\r\n\r\n### version: `0.9.0rc12`\r\n\r\n### To Reproduce\r\n\r\n#### Code sample\r\n\r\n```python\r\nfrom pytorch_lightning.metrics.functional import accuracy\r\n\r\npred = torch.tensor([0, 1, 2, 3])\r\ntarget = torch.tensor([0, 0, 0, 0])\r\n\r\n# calculates accuracy across all GPUs and all Nodes used in training\r\naccuracy(pred, target)\r\n```\r\n### RuntimeError  \r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-16-5d9063513e76> in <module>\r\n      5 \r\n      6 # calculates accuracy across all GPUs and all Nodes used in training\r\n----> 7 accuracy(pred, target)\r\n\r\n~/anaconda3/envs/pl/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py in accuracy(pred, target, num_classes, reduction)\r\n    268     \"\"\"\r\n    269     if not (target > 0).any() and num_classes is None:\r\n--> 270         raise RuntimeError(\"cannot infer num_classes when target is all zero\")\r\n    271 \r\n    272     tps, fps, tns, fns, sups = stat_scores_multiple_classes(\r\n\r\nRuntimeError: cannot infer num_classes when target is all zero\r\n```\r\n### Expected output\r\n\r\n```\r\ntensor(0.2500)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3006/comments",
    "author": "pchandra90",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-16T16:19:29Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-16T17:22:57Z",
        "body": "@justusschock @SkafteNicki This is a recurring confusion, we got asked about this several times now. Do we need to change the behavior here, or add a note/warning to the docs? \r\n\r\nIn sklearn for example, there is no such error and it returns 0.25"
      },
      {
        "user": "justusschock",
        "created_at": "2020-08-16T17:25:42Z",
        "body": "We can think about that. I just thought that for just one class, most metrics either aren't well defined or aren't descriptive. \n\nThoughts @SkafteNicki ?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-16T17:31:06Z",
        "body": "another option would be to convert it to a warning instead of error."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-08-16T17:36:07Z",
        "body": "We should probably change it. With so many people being confused about, I think it is a strong indicator that we are not doing what people expect, and we risk that people will not use the metrics if they are counter-intuitive. "
      },
      {
        "user": "pchandra90",
        "created_at": "2020-08-16T17:59:59Z",
        "body": "In one batch, a single class scenario is common in the case of validation dataset without shuffling. \r\n\r\nEven after passing the number of classes argument, it's throwing a warning. "
      },
      {
        "user": "Borda",
        "created_at": "2020-09-15T17:39:27Z",
        "body": "@pchandra90 is this still an actual issue, mind test master? 🦝 "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-15T18:51:22Z",
        "body": "still raises an error, should be changed to a simple warning. @pchandra90 interested in sending a PR?"
      },
      {
        "user": "pchandra90",
        "created_at": "2020-09-16T05:15:50Z",
        "body": "@Borda, Tested the master, there is no change in the issue status. **Here is the code to reproduce:**\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch\r\n\r\nfrom pytorch_lightning.metrics.functional import accuracy\r\n\r\nprint('PyTorch Lightning Version: {}'.format(pl.__version__))\r\n\r\npred = torch.tensor([0, 1, 2, 3])\r\ntarget = torch.tensor([0, 0, 0, 0])\r\n\r\nprint('Is the num_classes argument specified: No')\r\naccuracy(pred, target)\r\n```\r\n\r\n**RuntimeError:**\r\n\r\n```python\r\nPyTorch Lightning Version: 0.9.1rc3\r\nIs the num_classes argument specified: No\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-1-2c64fef4d70c> in <module>\r\n     10 \r\n     11 print('Is the num_classes argument specified: No')\r\n---> 12 accuracy(pred, target)\r\n\r\n~/anaconda3/envs/pl_dev/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py in accuracy(pred, target, num_classes, class_reduction)\r\n    268     \"\"\"\r\n    269     if not (target > 0).any() and num_classes is None:\r\n--> 270         raise RuntimeError(\"cannot infer num_classes when target is all zero\")\r\n    271 \r\n    272     tps, fps, tns, fns, sups = stat_scores_multiple_classes(\r\n\r\nRuntimeError: cannot infer num_classes when target is all zero\r\n\r\n```\r\n\r\n---\r\n\r\n```python\r\nimport pytorch_lightning as pl\r\nimport torch\r\n\r\nfrom pytorch_lightning.metrics.functional import accuracy\r\n\r\nprint('PyTorch Lightning Version: {}'.format(pl.__version__))\r\n\r\npred = torch.tensor([0, 1, 2, 3])\r\ntarget = torch.tensor([0, 0, 0, 0])\r\n\r\nprint('Is the num_classes argument specified: Yes')\r\naccuracy(pred, target, num_classes=10)\r\n```\r\n**Output with the warning:**\r\n\r\n```python\r\nPyTorch Lightning Version: 0.9.1rc3\r\nIs the num_classes argument specified: Yes\r\n/home/prakash/anaconda3/envs/pl_dev/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: You have set 10 number of classes if different from predicted (4) and target (1) number of classes\r\n  warnings.warn(*args, **kwargs)\r\ntensor(0.2500)\r\n```\r\n\r\n@rohitgr7 , Yes, I can send PR to fix the issue. However, can not do it before the weekend. "
      }
    ]
  },
  {
    "number": 2971,
    "title": "TensorBoardLogger not saving hparams without metrics",
    "created_at": "2020-08-14T03:50:00Z",
    "closed_at": "2020-08-14T06:42:42Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2971",
    "body": "## 🐛 Bug\r\n\r\n`log_hyperparams` for `TensorBoardLogger` saves no data with default `metrics=None`, only hparam entries/names show up in sidebar\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport pytorch_lightning as pl\r\nlogger = pl.loggers.TensorBoardLogger(\"./test_logs\")\r\ntest_dict = {\"test\":0}\r\nlogger.log_hyperparams(test_dict)                      ## no data saved\r\nlogger.log_hyperparams(test_dict, test_dict)           ## works\r\nlogger.log_metrics(test_dict)                          ## works\r\nlogger.experiment.add_hparams(test_dict, test_dict)    ## works but saves in a different events file\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n### Expected behavior\r\nhparams data is saved and viewable via tensorboard with default args\r\n\r\n### Proposed solution\r\nFor default hparams logging without metrics, add a placeholder metric? I can do a PR if this is appropriate.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2971/comments",
    "author": "s-rog",
    "comments": [
      {
        "user": "s-rog",
        "created_at": "2020-08-14T06:00:31Z",
        "body": "```\r\nlogger.log_hyperparams({\"hp_1\":1,\"hp_2\":2}, {\"test_m\":0})\r\n### training\r\nlogger.log_hyperparams({\"hp_1\":10,\"hp_2\":20}, {\"test_m\":1})\r\n```\r\n\r\nby calling log_hyperparams again at the end of training with the same metric name, the value can be updated... however it does mess up the graph for said metric (it sets the step to 0)... so there would have to be a dedicated metric for hparams\r\n(calling `log_hyperparams` again cannot update the hparams)\r\n\r\n\r\ncode to reproduce the messed up graph:\r\n```\r\nimport pytorch_lightning as pl\r\nlogger = pl.loggers.TensorBoardLogger(\"./test_logs\")\r\nlogger.log_metrics({\"metric\":0.0}, step=0)\r\nlogger.log_metrics({\"metric\":0.5}, step=1)\r\nlogger.log_metrics({\"metric\":1.0}, step=2)\r\nlogger.log_hyperparams({\"hp_1\":10,\"hp_2\":20}, {\"metric\":1.0})\r\n```\r\nsimilar behavior can be seen in pytorch summarywriter:\r\n```\r\nfrom torch.utils.tensorboard import SummaryWriter\r\nwriter = SummaryWriter(log_dir=\"./test_logs\")\r\nwriter.add_scalar(\"metric\", 0.0, global_step=0)\r\nwriter.add_scalar(\"metric\", 0.5, global_step=1)\r\nwriter.add_scalar(\"metric\", 1.0, global_step=2)\r\nwriter.add_hparams({\"hp_1\":10,\"hp_2\":20}, {\"metric\": 1.0})\r\n```\r\nand the summarywriter docs states that the hparams metric key should be unique as well"
      },
      {
        "user": "justusschock",
        "created_at": "2020-08-14T06:39:47Z",
        "body": "This is duplicate of #2406 . The issue we have there, is that usually you log these params at the end, but we cannot guarantee them to be logged for crashes then."
      },
      {
        "user": "s-rog",
        "created_at": "2020-08-14T06:42:42Z",
        "body": "My bad, will move discussions over there"
      }
    ]
  },
  {
    "number": 2958,
    "title": "Make it easy to disable logging/checkpoints",
    "created_at": "2020-08-13T17:47:54Z",
    "closed_at": "2020-08-13T18:02:29Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2958",
    "body": "## 🚀 Feature\r\n\r\nIntroduce an easy way to disable logging and checkpoints for `Trainer` instances.\r\n\r\n### Motivation\r\n\r\nSometimes when training a model you don't want to keep any logs or checkpoints, and there doesn't appear to be an obvious way to do that.\r\n\r\n### Pitch\r\n\r\nThe most obvious way to implement this would be to make it so when `log_save_interval=0` the logger never writes to the disk.\r\n\r\n### Alternatives\r\n\r\nAs I understand it the current way to do this would be to make some sort of dummy class which inherits `LightningLoggerBase` but doesn't actually do anything. This strikes me as unnecessarily involved.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2958/comments",
    "author": "import-antigravity",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-08-13T17:55:23Z",
        "body": "set `checkpoint_callback=False` and `logger=False`."
      },
      {
        "user": "import-antigravity",
        "created_at": "2020-08-13T17:56:28Z",
        "body": "Okay, awesome. Thanks!"
      }
    ]
  },
  {
    "number": 2946,
    "title": "Neptune logger with a validation epoch end conflict due to the 'epoch' key added on the fly.",
    "created_at": "2020-08-13T12:23:36Z",
    "closed_at": "2020-08-15T12:36:01Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2946",
    "body": "Hi everybody! First thanks for this lib, it is very handy!\r\n\r\n## 🐛 Bug\r\n\r\nWhen using pytorch lightning in conjunction with the neptune logger, one can see this kind of error popping every time an epoch ends:\r\n`\r\nneptune.api_exceptions.ChannelsValuesSendBatchError: Received batch errors sending channels' values to experiment SOC-114. Cause: Error(code=400, message='X-coordinates must be strictly increasing for channel: e4e2635d-b707-46fa-9a1b-996dd009790f. Invalid point: InputChannelValue(timestamp=2020-08-13T11:55:38.422Z, x=5.0, numericValue=2.0, textValue=null, image', type=None) (metricId: 'e4e2635d-b707-46fa-9a1b-996dd009790f', x: 5.0) Skipping 1 values.\r\n`\r\n\r\nthe import part in this error is the following line: `X-coordinates must be strictly increasing`\r\n\r\nThis is because, in `trainer/logging.py`, the `epoch` key is added on the fly on line 69: \r\n```python\r\nscalar_metrics['epoch'] = self.current_epoch\r\n```\r\n\r\nBut why does Neptune complains?\r\n\r\nIf you log all the timesteps (using `row_log_interval = 1`), at the end of an epoch, 2 calls are emitted to the logger: One to log the training logs and one for the validation logs. \r\nBoth of those have the same `step` value which is the current training `step` value. Since the key `epoch` is duplicated in both those calls, Neptune receives the key `epoch` twice with the same `step` value leading to the exception.\r\n\r\n### To Reproduce\r\n\r\nlaunch training with:\r\n- Neptune logger\r\n- training logs\r\n- validation logs\r\n- row_log_interval=1\r\n\r\n### Expected behaviour\r\n\r\nDon't add the `epoch` key on the fly which force the logger to log it.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2946/comments",
    "author": "morgangiraud",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-13T12:24:16Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-08-14T00:18:53Z",
        "body": "thanks!\r\n\r\nyeah, unfortunately, this might be a problem on their end since we have to track the epoch.\r\n\r\nIn the meantime, can you post a colab that replicates this issue?\r\n\r\nthanks!"
      },
      {
        "user": "morgangiraud",
        "created_at": "2020-08-14T07:42:31Z",
        "body": "Hi,\r\n\r\nThanks for answering.\r\n\r\nI've been looking at the code and I'm not sure why you need that `epoch` at that moment.\r\nWhen I look at the following code, I see that the `epoch` key is added on only one part of the \"if statement\". So the code used after this part can't rely on this key to exist. What am I missing?\r\n\r\n```\r\nif \"step\" in scalar_metrics and step is None:\r\n    step = scalar_metrics.pop(\"step\")\r\nelse:\r\n    # added metrics by Lightning for convenience\r\n    scalar_metrics['epoch'] = self.current_epoch\r\n    step = step if step is not None else self.global_step\r\n```\r\n"
      },
      {
        "user": "morgangiraud",
        "created_at": "2020-08-15T15:46:09Z",
        "body": "Thanks for the quick fix 👍 "
      }
    ]
  },
  {
    "number": 2939,
    "title": "mlflow checkpoints in the wrong location ",
    "created_at": "2020-08-12T22:58:48Z",
    "closed_at": "2020-08-15T10:54:07Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2939",
    "body": "I'm not sure if I'm doing something wrong, I'm using mlflow instead of tensorboard as a logger. I've used the defaults i.e.\r\n\r\n```\r\nmlflow = loggers.MLFlowLogger()\r\ntrainer = pl.Trainer.from_argparse_args(args, logger=mlflow)\r\n```\r\n\r\nI'm ending up with the following folder structure\r\n\r\n\\mlflow\r\n\\mlflow\\1\r\n\\mlflow\\1\\\\{guid}\\artifacts\r\n\\mlflow\\1\\\\{guid}\\metrics\r\n\\mlflow\\1\\\\{guid}\\params\r\n\\mlflow\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\mlflow` folder. \r\n\r\nPerhaps this is an mlflow rather than pytorch-lightning issue? \r\n\r\nI'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2939/comments",
    "author": "david-waterworth",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-08-13T06:30:27Z",
        "body": "@david-waterworth mind try the latest 0.9rc12?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-14T06:15:19Z",
        "body": "It was fixed here: #2502 \r\nThe checkpoints subfolder will go here: `mlflow\\1{guid}\\checkpoints`, is that what you want @david-waterworth ?\r\n"
      },
      {
        "user": "david-waterworth",
        "created_at": "2020-08-14T06:19:01Z",
        "body": "Thanks @awaelchli  yes that's what I want - thanks!"
      }
    ]
  },
  {
    "number": 2938,
    "title": "EOFError exception raised when using max_steps in distributed training",
    "created_at": "2020-08-12T22:52:31Z",
    "closed_at": "2020-08-15T12:25:39Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2938",
    "body": "## 🐛 Bug\r\n\r\nWhen using `max_steps` to stop a distributed training (`ddp`) in the middle of an epoch, the following exception is raised:\r\n\r\n```\r\nEpoch 1364:  64%|█████████████              | 7/11 [00:11<00:06,  1.62s/it, loss=2.057, v_num=13, lr=2.47e-11]\r\nException in thread Thread-4:\r\nTraceback (most recent call last):\r\n  File \"***/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"***/lib/python3.8/threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"***/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 25, in _pin_memory_loop\r\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\r\n  File \"***/lib/python3.8/multiprocessing/queues.py\", line 116, in get\r\n    return _ForkingPickler.loads(res)\r\n  File \"***/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 282, in rebuild_storage_fd\r\n    fd = df.detach()\r\n  File \"***/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\r\n    with _resource_sharer.get_connection(self._id) as conn:\r\n  File \"***/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\r\n    c = Client(address, authkey=process.current_process().authkey)\r\n  File \"***/lib/python3.8/multiprocessing/connection.py\", line 508, in Client\r\n    answer_challenge(c, authkey)\r\n  File \"***/lib/python3.8/multiprocessing/connection.py\", line 752, in answer_challenge\r\n    message = connection.recv_bytes(256)         # reject large message\r\n  File \"***/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"***/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"***/lib/python3.8/multiprocessing/connection.py\", line 383, in _recv\r\n    raise EOFError\r\nEOFError\r\n```\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Train some model in distributed mode (`ddp`) such that `num_steps` is defined in the trainer, and it stops training in the middle of an epoch.\r\n\r\n### Expected behavior\r\n\r\nThe training should end without any error.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0\r\n\t- pytorch-lightning: 0.9.0rc12\r\n\t- tensorboard:       2.3.0\r\n\t- tqdm:              4.47.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.5\r\n\t- version:           #25~18.04.1-Ubuntu SMP Fri Jun 5 15:18:30 UTC 2020\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2938/comments",
    "author": "bryant1410",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-12T22:53:11Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-08-13T06:31:03Z",
        "body": "@awaelchli mind have look? :]"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-08-15T12:14:01Z",
        "body": "i couldn't replicate this...\r\n\r\nMind trying master?\r\n\r\nWill reopen if still an issue"
      }
    ]
  },
  {
    "number": 2924,
    "title": "Smaller last batch skews Train/EvalResult",
    "created_at": "2020-08-12T03:57:39Z",
    "closed_at": "2020-08-12T12:02:01Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2924",
    "body": "## 🚀 Feature\r\nSupported weighted average for Result\r\n### Motivation\r\nThe last batch of dataloader is usually smaller than a regular batch. Hence simply average the statistic of all batches will skew the epoch statistic.\r\n_For example_    \r\n```\r\nresult.log('accuracy', 0.7) # batch size 256\r\nresult.log('accuracy', 0.8) # batch size 256\r\nresult.log('accuracy', 0.9) # batch size 10 (last batch)\r\n```\r\nWrong epoch accuracy =` (0.7 + 0.8 + 0.9) / 3`\r\nCorrect epoch accuracy = `(256 * 0.7 + 256 * 0.8 + 10 * 0.9) / ( 256 + 256 + 10)`\r\n\r\n### Pitch\r\n\r\nSupport input for weight, or batch size in Result\r\n`result.log('accuracy', 0.7, batch_size) \r\n`\r\n### Alternatives\r\n\r\nI don't see any alternatives other than switch back the the old way of using training/validation_epoch_end and do the weighted average manually there.\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2924/comments",
    "author": "huyvnphan",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-12T03:58:17Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "justusschock",
        "created_at": "2020-08-12T07:12:10Z",
        "body": "@huyvnphan thanks for the issue, you're totally right. I'll try to tackle this issue :)\r\n\r\nUnfortunately I cannot give you an ETA so far :)\r\n\r\ncc @williamFalcon "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-08-12T10:20:19Z",
        "body": "good catch! \r\n\r\nThere are a few options for this:\r\n\r\n1. you can add a reduce function yourself that can do a weighted average result(key, val, reduce_fx=torch.mean).  torch.mean is the current default but you can implement a weighted average if you want. \r\n\r\n2. we can switch the default to a weighted average. \r\n\r\n3. maybe the better solution is to track the batch sizes internally (which we already do since we have the batch during this step), and do this automatically. I think this is more in line with your suggestion. \r\n\r\n\r\n4. add drop_last=True to the dataloader (generally this is what i do most of the time haha). This solves other problems you might end up with beyond this particular issue.\r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "number": 2868,
    "title": "Throw warning for changing val_loss",
    "created_at": "2020-08-07T17:53:24Z",
    "closed_at": "2020-08-17T14:29:29Z",
    "labels": [
      "bug",
      "help wanted",
      "good first issue",
      "let's do it!"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2868",
    "body": "Add warning to user that when changing val_loss to another keyword it will break checkpointing, early stopping, and other features relying on it. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2868/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "shivin7",
        "created_at": "2020-08-11T21:31:23Z",
        "body": "Hi @Borda, newcomer here. Can I work on this?"
      },
      {
        "user": "Borda",
        "created_at": "2020-08-11T21:48:13Z",
        "body": "@shivin7 perfect, go ahead! 🎉 "
      }
    ]
  },
  {
    "number": 2823,
    "title": "Resume training with resetting / increasing max number of epochs",
    "created_at": "2020-08-04T15:55:34Z",
    "closed_at": "2020-10-29T05:10:41Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2823",
    "body": "Hi! I would like to know how can one continue training from existing checkpoint if after resuming you got saved learning rate, current epoch and other significant info which interrupts training immediately.\r\nLet's say I train classifier using ReduceLROnPlateau and saving best epoch via ModelCheckpoint callback. I set `max_epochs` as 10, train for 5 epochs, at 9 epoch lr scheduler got activated and metric improves. So i have learning rate reduced at 10th epoch and best checkpoint also leads to 10th epoch.\r\n\r\nThen I resume training from this checkpoint. I also have set `max_epochs` to 10 and start from another learning rate. But all that I got is my current epoch set to 10, learning rate changes to which one the saving with the checkpoint callback was performed and training stops because 10 is the last epoch. How can we improve such situations?\r\n\r\nThis would be also very useful when training using stages. You might have first stage for pretraining for 100 epochs and you would like to train for another 50 epochs at another dataset etc, but you might get checkpoint at let's say epoch 77 and you will not be able to train second stage because `max_epochs` would be set to 50.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2823/comments",
    "author": "thepowerfuldeez",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-08-04T19:26:55Z",
        "body": "we have a discussion about it in #2146\r\ncc: @williamFalcon "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T04:24:20Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "hwidong-na",
        "created_at": "2022-05-20T01:03:31Z",
        "body": "I want to iterate small number of epochs for each outer loop iteration. Here is a workaround by calling `reset_on_epoch()`.\r\n\r\n```\r\n    ...\r\n    trainer = pl.Trainer(\r\n        ...\r\n        max_epochs=num_inner_epochs,\r\n        ...\r\n    )\r\n    for epoch in range(num_outer_epochs):\r\n        trainer.fit_loop.epoch_progress.reset_on_epoch()\r\n        ...\r\n```"
      }
    ]
  },
  {
    "number": 2808,
    "title": "NumpyMetric not mapping back to GPU in multi-GPU training",
    "created_at": "2020-08-03T16:29:26Z",
    "closed_at": "2020-08-04T17:17:14Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2808",
    "body": "## 🐛 Bug\r\n\r\nI created a NumpyMetric class for an involved metric that requires numpy operations; however, the metric fails when training on multiple GPUs. After some debugging, this appears to be due to the resulting tensor not being mapped back to the appropriate GPU (or any GPU for that matter).\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Define a NumpyMetric class\r\n```python\r\nclass MyNumpyMetric(NumpyMetric):\r\n    def forward(self, y_hat, y):\r\n        # complicated numpy stuff (no calls to .cpu() or .cuda() or .to() or anything like that)\r\n        return metric\r\n```\r\n2. Instantiate it in the `__init__` and `validation_step` of my PyTorchLightning module, e.g.,\r\n```python\r\nclass MyNetwork(pl.LightningModule):\r\n    def __init__(self, args):\r\n        # other init stuff\r\n        self.my_metric = MyNumpyMetric('my_metric')\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        # other validation stuff\r\n        my_metric = self.my_metric(y_hat, y)  # where y_hat and y are tensors, no .cpu(), .cuda(), .to() called on either\r\n        out_dict = dict(val_my_metric=my_metric)\r\n        return out_dict\r\n```\r\n3. Run:\r\n```python\r\nmodel = MyNetwork(args)\r\ntrainer = Trainer(\r\n    benchmark=True,\r\n    check_val_every_n_epoch=1,\r\n    accumulate_grad_batches=1,\r\n    min_epochs=n_epochs,\r\n    max_epochs=n_epochs,\r\n    fast_dev_run=False,\r\n    gpus=2,\r\n    distributed_backend='dp'\r\n)\r\ntrainer.fit(model)\r\n```\r\n4. See:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"./tiramisu3d.py\", line 574, in <module>\r\n    trainer.fit(model)\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 997, in fit\r\n    results = self.dp_train(model)\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 270, in dp_train\r\n    result = self.run_pretrain_routine(model)\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1193, in run_pretrain_routine\r\n    eval_results = self._evaluate(model,\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 293, in _evaluate\r\n    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 444, in evaluation_forward\r\n    output = model(*args)\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 66, in forward\r\n    return self.gather(outputs, self.output_device)\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 168, in gather\r\n    return gather(outputs, output_device, dim=self.dim)\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py\", line 68, in gather\r\n    res = gather_map(outputs)\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py\", line 61, in gather_map\r\n    return type(out)(((k, gather_map([d[k] for d in outputs]))\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py\", line 61, in <genexpr>\r\n    return type(out)(((k, gather_map([d[k] for d in outputs]))\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py\", line 55, in gather_map\r\n    return Gather.apply(target_device, dim, *outputs)\r\n  File \"/iacl/pg20/jacobr/miniconda3/envs/msseg-9.2/lib/python3.8/site-packages/torch/nn/parallel/_functions.py\", line 54, in forward\r\n    assert all(map(lambda i: i.is_cuda, inputs))\r\n```\r\n\r\n#### Code sample\r\nI will try to do this soon.\r\n\r\n### Expected behavior\r\nI expected no error to occur. The documentation states: \"[NumpyMetric] already handles DDP sync and input/output conversions.\" However, this doesn't appear to be the case in my implementation.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla M40 24GB\r\n\t\t- Tesla M40 24GB\r\n\t- available:         True\r\n\t- version:           9.2\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.1\r\n\t- pytorch-lightning: 0.8.5\r\n\t- tensorboard:       2.3.0\r\n\t- tqdm:              4.48.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.3\r\n\t- version:           #1 SMP Wed Sep 26 11:06:22 UTC 2018\r\n\r\n### Additional context\r\n\r\nPyTorch and PyTorch Lightning were installed with conda (along with all of the other packages).\r\n\r\nI was able to work around this error by adding the following `.to()` call to the validation step:\r\n\r\n```python\r\ndef validation_step(self, batch, batch_idx):\r\n    # other validation stuff\r\n    my_metric = self.my_metric(y_hat, y)\r\n    my_metric = my_metric.to(y_hat.device)\r\n    out_dict = dict(val_my_metric=my_metric)\r\n    return out_dict\r\n```\r\n\r\nI presume, however, that this is not the intended way to use the NumpyMetric class.\r\n\r\nFWIW, I briefly looked at the code to see if I could just submit a PR with the fix (if this isn't user error), but it wasn't clear to me where the best places to look were. If you point me in the right direction, I might be able to submit a PR with the fix.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2808/comments",
    "author": "jcreinhold",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-03T16:30:39Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-03T18:33:12Z",
        "body": "Hi, good news, I believe I have fixed this issue already in #2657, at least it looks very similar. The fix is not released yet (but soon). So if you need it now, install Lightning from master branch. (your workaround is also fine)"
      },
      {
        "user": "Borda",
        "created_at": "2020-08-04T17:17:14Z",
        "body": "@jcreinhold mind try `master` and feel free to reopen if needed 🐰 "
      }
    ]
  },
  {
    "number": 2788,
    "title": "Enable option to use Apex when PyTorch 1.6 is installed",
    "created_at": "2020-08-01T12:48:13Z",
    "closed_at": "2020-08-08T09:07:33Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "let's do it!",
      "design"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2788",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nCurrently, if PyTorch 1.6 is installed, PyTorch Lightning will use native AMP by default. It would be useful to have an option to force PL to use Apex if needed, which is not currently possible\r\n\r\n### Motivation\r\n\r\nNative AMP is great but under certain conditions may perform differently to Apex. For debugging purposes and backwards compatibility, having the option to still use Apex might be useful.\r\n\r\nIn my particular problem, I needed to roll back to PT 1.5 since I was seeing different behaviour between the two. Having the option to switch between the two AMP methods would make it easier to diagnose my issue.\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\nHave a flag to force the Trainer to use Apex over native AMP\r\n\r\n<!-- A clear and concise description of what you want to happen. -->",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2788/comments",
    "author": "Anjum48",
    "comments": [
      {
        "user": "calclavia",
        "created_at": "2020-08-03T02:13:32Z",
        "body": "Same here. I notice some results could no longer be replicated now that I upgraded to PT 1.6 and lightning 0.8.5 (which may be related to FP16)."
      }
    ]
  },
  {
    "number": 2785,
    "title": "Pass second-order closure to all optimizers (not just LBFGS)",
    "created_at": "2020-07-31T21:28:18Z",
    "closed_at": "2020-10-21T18:34:30Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2785",
    "body": "## 🚀 Feature\r\n\r\nI could be wrong, but I noticed the following in the code of lightning module's optimizer_step\r\n\r\n```python\r\n        if on_tpu:\r\n            xm.optimizer_step(optimizer)\r\n        elif using_native_amp:\r\n            self.trainer.scaler.step(optimizer)\r\n        elif using_lbfgs:\r\n            optimizer.step(second_order_closure)\r\n        else:\r\n            optimizer.step()\r\n\r\n```\r\n\r\nIf someone uses a custom optimizer that needs the closure returning the loss multiple times, it won't work. \r\n\r\n### Pitch\r\n\r\nSince all classes that inherit from `torch.optim.Optimizer` have the step method accept a closure (even if they don't need it), we could just do\r\n\r\n```python\r\n        if on_tpu:\r\n            xm.optimizer_step(optimizer)\r\n        elif using_native_amp:\r\n            self.trainer.scaler.step(optimizer)\r\n        # elif using_lbfgs:\r\n        #     optimizer.step(second_order_closure)\r\n        else:\r\n            optimizer.step(second_order_closure)\r\n```\r\nand drop the \"using_lbfgs\" argument?\r\n\r\n### Alternatives\r\n\r\nThe user has to override the optimizer_step themself.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2785/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-08-02T19:34:02Z",
        "body": "it sounds as a good way to go to me...\r\ncc: @williamFalcon "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-08-07T13:53:29Z",
        "body": "we had that before but removed it for some reason. I can't remember it though. But i originally agreed.\r\n\r\nLet's do this after 0.9.0 since we need to dig back to why it changed"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-22T15:53:08Z",
        "body": "@awaelchli want to take a look at this now? or better post v1?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-28T05:35:09Z",
        "body": "@williamFalcon LBFGS not being compatible with native amp is the only exception I found in the code. maybe you mean that?\r\n@edenlightning I don't think it's an essential for v1.0, but if you wish to have the final api without this argument \"using_lbfgs\" for optimizer step, then I could send a PR."
      }
    ]
  },
  {
    "number": 2769,
    "title": "Bug in `LightningModule.load_from_checkpoint`",
    "created_at": "2020-07-30T22:03:12Z",
    "closed_at": "2020-10-05T16:44:24Z",
    "labels": [
      "bug",
      "duplicate",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2769",
    "body": "Suppose you have a class `Model(LightningModule)` (a `LightningModule` subclass) and as parameters it takes\r\n`params: argparse.Namespace` (without a default value), along with other keyword arguments which might take default arguments. Loading from a checkpoint\r\n\r\n```python\r\nmodel = Model(params=params, **kwargs)\r\n...  # saves a checkpoint\r\nmodel = Model.load_from_checkpoint(params=params, **kwargs)  \r\n# same behavior arises if params is passed as positional argument\r\n```\r\n\r\nwill throw the error `TypeError: __init__() got multiple values for argument 'params'`.\r\nThis comes from the fact that in line `205` in `pytorch_lightning/core/saving.py`\r\n\r\n```python\r\nmodel = cls(*cls_args, **cls_kwargs)\r\n```\r\n\r\nwhen inspected, `cls_args` is `({'params': params},)` and `cls_kwargs` is, of course, `{'params': params}`.\r\nThis stems from the fact in lines `189-196` (in `saving.py`):\r\n\r\n```python\r\n            if args_name == 'kwargs':\r\n                # in case the class cannot take any extra argument filter only the possible\r\n                cls_kwargs.update(**model_args)\r\n            elif args_name:\r\n                if args_name in cls_init_args_name:\r\n                    cls_kwargs.update({args_name: model_args})\r\n            else:\r\n                cls_args = (model_args,) + cls_args\r\n```\r\n\r\nthe `else` clause is getting called and `model_args` contains a copy of kwargs!\r\n\r\nTo be honest, it's very unclear to me exactly what `model_args` is supposed to be here, but what is clear is that\r\nthis block needs more checking so that we don't just stick copies of items in `cls_kwargs` into `cls_args`. Just doing\r\nsomething silly like replacing the `else` clause with\r\n\r\n```python\r\nfrom typing import Sequence\r\n...\r\n             elif isinstance(model_args, Sequence):\r\n                cls_args = tuple(model_args) + cls_args\r\n```\r\n\r\nfixes the problem for me, but as I said, I'm unclear on exactly how model_args is being used, so I'm sure there is a better\r\nsolution.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2769/comments",
    "author": "acturner",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-30T22:04:14Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "acturner",
        "created_at": "2020-08-24T14:34:24Z",
        "body": "@Borda any updates on this?"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-02T14:16:41Z",
        "body": "@acturner it is fixed by #2776 mind try..."
      }
    ]
  },
  {
    "number": 2765,
    "title": "Add a test case for running trainer.test without trainer.fit on DDP",
    "created_at": "2020-07-30T14:55:50Z",
    "closed_at": "2020-08-16T15:19:58Z",
    "labels": [
      "bug",
      "help wanted",
      "ci",
      "distributed"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2765",
    "body": "## 🐛 Bug\r\n\r\nRunning trainer.test(model) using DDp without running trainer.fit hangs.\r\n\r\n### To Reproduce\r\n\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, num_samples=100, dim=5):\r\n        self.num_samples = num_samples\r\n        self.dim = dim\r\n    def __len__(self):\r\n        return self.num_samples\r\n    def __getitem__(self, item):\r\n        x = torch.rand(self.dim)\r\n        y = x.sum()\r\n        return x, y\r\nclass Model(pl.LightningModule):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.layer = torch.nn.Linear(5, 1)\r\n    def forward(self, x):\r\n        y = self.layer(x)\r\n        return y\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.1)\r\n        return optimizer\r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n            dataset=RandomDataset(num_samples=100, dim=5),\r\n            batch_size=32,\r\n        )\r\n    def test_dataloader(self):\r\n        return DataLoader(\r\n            dataset=RandomDataset(num_samples=64, dim=5),\r\n            batch_size=8\r\n        )\r\n    def training_step(self, batch, batch_idx, optimizer_idx=0):\r\n        x, y = batch\r\n        x = x.view(-1, 5)\r\n        y = y.view(-1, 1)\r\n        y_dash = self(x)\r\n        loss = ((y - y_dash) ** 2).sum()\r\n        return {'loss': loss, 'log': {'train_loss': loss / x.size(0)}}\r\n    def test_step(self, batch, batch_idx, dataloader_idx=0):\r\n        return self.training_step(batch, batch_idx)\r\n    def test_epoch_end(self, outputs):\r\n        loss = torch.stack([log['loss'] for log in outputs]).mean()\r\n        return {'test_loss': loss}\r\nif __name__ == '__main__':\r\n    model = Model()\r\n    trainer = pl.Trainer(\r\n        max_steps=20,\r\n        amp_level='O1',\r\n        gpus=2,\r\n        precision=16,\r\n        distributed_backend='ddp'\r\n    )\r\n    # comment below / remove comment below\r\n    # trainer.fit(model)\r\n    trainer.test(model)\r\n```\r\n\r\n### Expected behavior\r\nShould be able to run test with DDP.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.5\r\n - PL: 0.8.5\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2765/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "sailordiary",
        "created_at": "2020-07-31T01:11:06Z",
        "body": "I am experiencing the same issue with my program. "
      },
      {
        "user": "asrafulashiq",
        "created_at": "2020-07-31T02:56:33Z",
        "body": "Same problem here. I cannot use trainer.test in ddp mode without running trainer.fit first."
      }
    ]
  },
  {
    "number": 2749,
    "title": "[DataModule] PyTorch datasets as DataModules out of the box",
    "created_at": "2020-07-29T11:12:17Z",
    "closed_at": "2020-07-29T20:29:45Z",
    "labels": [
      "feature",
      "help wanted",
      "discussion"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2749",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nPyTorch already has datasets (MNIST, CIFAR, etc). It would be very convenient to provide those datasets out of the box as DataModules\r\n\r\n### Motivation\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nTo reduce the boilerplate. I mean, if I had the possibility not to reimplement / copy-paste the same code again, I would rather not do that, and I'd use the already implemented solutions. The entire PyTorchLightning was built with this in mind, so this is only natural.\r\n\r\n### Pitch\r\n<!-- A clear and concise description of what you want to happen. -->\r\nTo have the ability to write something along the lines of\r\n```python3\r\nimport pytorch_lightning as pl\r\nimport pytorch_lightning.datasets as pld\r\n\r\n# implementation of model and trainer instantiation\r\ntrainer.fit(model, pld.MNIST())\r\n```\r\n\r\n### Alternatives\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nAlternatively, it could be implemented as a PyTorchLightning Bolt, instead of here.\r\n\r\n### Additional context\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nNone",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2749/comments",
    "author": "InCogNiTo124",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-29T11:13:05Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-07-29T11:42:04Z",
        "body": "I think it is a good suggestion, @PyTorchLightning/core-contributors "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-29T11:53:39Z",
        "body": "yes. this is i think what we have already started in bolts!\r\n\r\nwant to add the missing torchvision datasets to it? \r\n"
      },
      {
        "user": "InCogNiTo124",
        "created_at": "2020-07-29T12:17:31Z",
        "body": "I'd like to, but I'm unsure if I have the time to do it in case this is very important. I could probably slowly do it over 2-3 weeks, if that's not an issue :)"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-29T12:19:17Z",
        "body": "no problem. Maybe create GH issues for each dataset? \r\n\r\nand do one at a time?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-29T12:19:40Z",
        "body": "(gh issues in bolts)\r\nfyi @nateraw "
      },
      {
        "user": "InCogNiTo124",
        "created_at": "2020-07-29T12:24:19Z",
        "body": "Makes sense. I'll open a separate issue per dataset in Bolts.\r\n\r\nAlso, what do you thinkk about leaving this issue open until everything is implemented?"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-07-29T20:13:48Z",
        "body": "Not sure what the value is in having duplicate tickets, but we can leave this open for now until new issues are opened in Bolts. Make sense?"
      },
      {
        "user": "nateraw",
        "created_at": "2020-07-29T20:29:45Z",
        "body": "@InCogNiTo124 lets move the discussion to bolts repo for now. We're building out all sorts of support for different datasets there. \r\n\r\nThe datasets you mentioned aren't from`torch`, to my understanding. They're from `torchvision`, which isn't included as a requirement here. If we want to support `torchvision` or `sklearn` datasets directly in lightning, we can have that in a future PR. \r\n\r\nThanks for the feedback on the new `LightningDataModule` - Looking forward to hearing your thoughts on the bolts datamodules we've built out 😄 "
      }
    ]
  },
  {
    "number": 2715,
    "title": "When I call Trainer.test it calls both \"test\" and \"fit\" stages of setup - should it just be \"test\"",
    "created_at": "2020-07-26T18:26:50Z",
    "closed_at": "2020-07-31T15:31:23Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2715",
    "body": "## 🐛 Bug\r\nWhen I call Trainer.test it calls both \"test\" and \"fit\" stages of \"setup\" function.  Shouldn't it just call test?\r\n\r\n### To Reproduce\r\n\r\nYou can actually look through the code to see that this will happen since self.fit() is called.  Follow trainer.test() then choose the function self.__test_given_model(model, test_dataloaders) and then self.fit(model) is called.  Also, you can just print out \"stage\" in the setup function and you will see both \"fit\" and \"test\" are called when trainer.test(model=model, test_dataloaders=dataloaders) is called.  I'm using the \"fit\" value for training, should I be using something else?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2715/comments",
    "author": "jloveric",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-07-26T21:15:22Z",
        "body": "Can you check on master? It's fixed #2624."
      },
      {
        "user": "Borda",
        "created_at": "2020-07-31T15:03:26Z",
        "body": "@yukw777 mind have look? :]"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-31T15:14:46Z",
        "body": "@Borda it's already fixed on master."
      }
    ]
  },
  {
    "number": 2707,
    "title": "Improve Design of DataModule",
    "created_at": "2020-07-25T20:29:35Z",
    "closed_at": "2020-09-04T10:04:46Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2707",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\n\r\nSince the introduction of datamodules we have duplicated code. For example, the train/val/test_dataloader methods in LightningModule and DataModule have the same name, same docs, same signature but live in two files. \r\nAlso the argparse methods are copy pasted from Trainer, it will become impossible to maintain this in the future. \r\n\r\n### Pitch\r\n\r\nFactor out the hook definitions:\r\n\r\n```python\r\nclass DataHooks:\r\n\r\n    def prepare_data(...):\r\n    \"\"\" docs... \"\"\"\r\n\r\n    def setup(...):\r\n    \"\"\" docs... \"\"\"\r\n\r\n    def train_dataloader(...):\r\n    \"\"\" docs... \"\"\"\r\n\r\n    etc.\r\n```\r\n\r\nand then in the other classes, inherit from it:\r\n\r\n```python\r\nclass LightningModule(..., ModelHooks, DataHooks, ...)\r\n    ...\r\n\r\nclass DataModule(..., DataHooks, ...)\r\n    ...\r\n```\r\n\r\n**Note also:** \r\n- we even have duplicated tests\r\n\r\n### Alternatives\r\n\r\nI see no alternatives. This is the best way I know.\r\n\r\n@PyTorchLightning/core-contributors makes sense?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2707/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-07-25T21:12:08Z",
        "body": "yes, just make them abstract..."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-26T02:05:39Z",
        "body": "A few things to keep in mind:\r\n\r\n1. We want datamodules to be usable even if you're not using lightning for training.\r\n2. We are moving away from Mixins.\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-26T02:30:50Z",
        "body": "> We want datamodules to be usable even if you're not using lightning for training.\r\n\r\nThis won't be changed by my suggestion. It's just refactoring, nothing else.\r\n\r\n> We are moving away from Mixins.\r\n\r\nWhich mixins? I am aware that in Lightning some classes are named mixins but conceptually they are actually not and should not be called that. Mixins have their place and one should not confuse multiple inheritance with mixins. Could you clarify what you mean?\r\n"
      }
    ]
  },
  {
    "number": 2679,
    "title": "Default checkpoint location problematic when using docker ",
    "created_at": "2020-07-23T17:53:41Z",
    "closed_at": "2020-08-11T14:11:44Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2679",
    "body": "The default behavior of `ModelCheckpoint` is to use `os.getcwd()`. Outside my docker container, this ended up being the same directory where my tensorboard logs were saved (e.g. `/my/dir/tb_logs/default/version_0/checkpoints/`).  But inside the docker container, it saved to the internal working directory (e.g. `/home/default/version_0/checkpoints/`). Since this location disappeared along with the container, the checkpoint was gone, and there was no warning raised to explain why.\r\n\r\nRequiring a checkpoint directory isn't desirable, but I'd like to help others avoid this grief in the future. Is there a better way to infer a default location than `os.getcwd()`? Something as simple as a print statement with the checkpoint location would have saved me a lot of time troubleshooting.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2679/comments",
    "author": "drStacky",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-23T17:54:41Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-08T05:21:33Z",
        "body": "You can set the `default_root_dir` arg in the Trainer. Is that what you want? Otherwise there is an option verbose in the ModelCheckpoint callback which, when turned on, should print the file path everytime it saves."
      },
      {
        "user": "drStacky",
        "created_at": "2020-08-11T14:11:44Z",
        "body": "Somehow I misread the explanation of `default_root_dir`. I thought it only changed the name of the `default` directory, not the whole path. This is exactly what I needed. Thanks!"
      }
    ]
  },
  {
    "number": 2676,
    "title": "Namespace Cleaning",
    "created_at": "2020-07-22T19:45:01Z",
    "closed_at": "2020-08-28T07:07:44Z",
    "labels": [
      "feature",
      "help wanted",
      "refactor"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2676",
    "body": "## 🚀 Feature\r\nChange how the namespace is cleaned to be fully general. Right now, it filters all callables, but this filters some things which are pickable and callable.\r\n\r\n### Motivation\r\nCurrently, users can interact with callable objects only in the init method. It's useful to  use these elsewhere, for example I pass in optim.Adam or optim.SGD to switch between optimizers, more flexible than hardcoding strings.\r\n\r\n### Pitch\r\n\r\nrewrite clean_namespace to be something like:\r\n\r\n```\r\nfor k,v in hparams.items():\r\n    try:\r\n        pickle.dumps(v)\r\n    except:\r\n        del_attrs.append(k)\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2676/comments",
    "author": "monney",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-08-04T21:10:42Z",
        "body": "@monney mind draft a PR to clarify what you mean?"
      },
      {
        "user": "monney",
        "created_at": "2020-08-08T01:09:55Z",
        "body": "@Borda I created a PR, it should also fix #2444 right now it's failing a test case, but my change shouldn't make it fail anything new. I tried the test case on my machine and it seems to work, so I'm not sure why it's failing"
      },
      {
        "user": "monney",
        "created_at": "2020-08-08T01:33:11Z",
        "body": "The test looks broken to me, it's checking for nn.CrossEntropyLoss, but it should be checking for nn.CosineEmbeddingLoss The version I have does that."
      }
    ]
  },
  {
    "number": 2670,
    "title": "bug in pytorch_lightning.metrics.functional.auroc",
    "created_at": "2020-07-22T09:40:31Z",
    "closed_at": "2020-08-03T22:29:51Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2670",
    "body": "the code:\r\n```\r\ndef validation_epoch_end(self, outputs):\r\n        .........\r\n        print(total_y_hat.device)\r\n        print(total_y_true.device)\r\n        print(total_y_hat)\r\n        print(total_y_true)\r\n        print(total_y_hat.shape)\r\n        print(total_y_true.shape)\r\n        auc_score = auroc(total_y_hat, total_y_true)\r\n```\r\nthe output is:\r\n```\r\nGet data done!\r\nValidation sanity check:  50%|█████     | 1/2 [00:00<00:00,  1.06it/s]\r\n\r\ncuda:0\r\ncuda:0\r\ntensor([0.5084, 0.5084, 0.5084,  ..., 0.5084, 0.5084, 0.5084], device='cuda:0')\r\ntensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\r\ntorch.Size([16384])\r\ntorch.Size([16384])\r\nTraceback (most recent call last):\r\n  File \"lighting_sales.py\", line 443, in <module>\r\n    main(hparams)\r\n  File \"lighting_sales.py\", line 392, in main\r\n    trainer.fit(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in fit\r\n    self.single_gpu_train(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 176, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1076, in run_pretrain_routine\r\n    False)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 330, in _evaluate\r\n    eval_results = model.validation_epoch_end(outputs)\r\n  File \"lighting_sales.py\", line 252, in validation_epoch_end\r\n    auc_score = auroc(total_y_hat, total_y_true)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py\", line 817, in auroc\r\n    return _auroc(pred=pred, target=target, sample_weight=sample_weight, pos_label=pos_label)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py\", line 766, in new_func\r\n    x, y = func_to_decorate(*args, **kwargs)[:2]\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py\", line 815, in _auroc\r\n    return roc(pred, target, sample_weight, pos_label)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py\", line 553, in roc\r\n    pos_label=pos_label)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py\", line 504, in _binary_clf_curve\r\n    torch.tensor([target.size(0) - 1])])\r\nRuntimeError: All input tensors must be on the same device. Received cuda:0 and cpu\r\n```\r\n          \r\n           ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2670/comments",
    "author": "BeHappyForMe",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-22T09:41:24Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-07-22T11:19:04Z",
        "body": "That bug is fixed on master. See #2657 "
      },
      {
        "user": "BeHappyForMe",
        "created_at": "2020-07-22T11:27:49Z",
        "body": "thank u very much"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-22T20:31:02Z",
        "body": "Does it work with master branch? If not, do you use the functional or module interface for the metric?"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-08-03T22:29:51Z",
        "body": "@BeHappyForMe I'm closing this but please open if still experiencing this with master."
      }
    ]
  },
  {
    "number": 2652,
    "title": "Different values between validation/testing when sets are the same",
    "created_at": "2020-07-20T23:50:59Z",
    "closed_at": "2020-10-05T02:33:25Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2652",
    "body": "\r\n\r\n## 🐛 Bug\r\n\r\nHi. I've come across a weird bug (?). To test out my workflow, I trained, validated and tested a video classifier on the same set. First bug, I get different accuracy and loss values between my validation and test sets (even though they are the same). Second bug, when loading the model from a check point, the test accuracy is again different. Third issue (maybe not a bug), I notice training/evaluation is slower when running from a checkpoint.  \r\n\r\n### To Reproduce\r\n\r\nHere are some code snippets from my project.\r\n\r\nMain: \r\n\r\n```python \r\n    checkpoint_callback = ModelCheckpoint(monitor='val_acc', save_top_k=1, mode='max')\r\n\r\n    # Create PTL trainer\r\n    trainer = Trainer.from_argparse_args(Namespace(**dict(train_config)),\r\n                                         default_root_dir=os.getcwd()\r\n                                                           + '/'\r\n                                                           + config[\"project\"][\"models\"]\r\n                                                           + experiment_name,\r\n                                         checkpoint_callback=checkpoint_callback)\r\n\r\n    # Create model from checkpoint\r\n    if model_config[\"checkpoint\"][\"load\"]:\r\n        model = globals()[hparams[\"model\"][\"name\"]].load_from_checkpoint(model_config[\"checkpoint\"][\"path\"])\r\n        trainer = Trainer(resume_from_checkpoint=model_config[\"checkpoint\"][\"path\"])\r\n        \r\n        # Evaluate test set from loaded model\r\n        if model_config[\"test_only\"]:\r\n            trainer.test(model)\r\n            exit()\r\n\r\n    # Train from scratch\r\n    else:\r\n        model = globals()[hparams[\"model\"][\"name\"]](Namespace(**dict(hparams)))\r\n\r\n    # Train and test with best model\r\n    trainer.fit(model)\r\n    trainer.test(ckpt_path='best')\r\n```\r\n\r\nValidation and test steps from the model:\r\n```python\r\n    def validation_step(self, batch, batch_idx):\r\n        batch, y = batch\r\n        y_hat = self.forward(batch)\r\n\r\n        loss = F.cross_entropy(y_hat, y.long())\r\n        labels_hat = torch.argmax(y_hat, dim=1)\r\n        n_correct_pred = torch.sum(y == labels_hat).item()\r\n\r\n        return {'val_loss': loss, \"n_correct_pred\": n_correct_pred, \"n_pred\": len(y)}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        val_acc = torch.tensor(sum([x['n_correct_pred'] for x in outputs]) / sum(x['n_pred'] for x in outputs))\r\n        tensorboard_logs = {'val_loss': avg_loss, 'val_acc': val_acc, 'step': self.current_epoch}\r\n\r\n        return {'log': tensorboard_logs}\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        batch, y = batch\r\n        y_hat = self.forward(batch)\r\n\r\n        loss = F.cross_entropy(y_hat, y.long())\r\n        labels_hat = torch.argmax(y_hat, dim=1)\r\n        n_correct_pred = torch.sum(y == labels_hat).item()\r\n\r\n        return {'test_loss': loss, \"n_correct_pred\": n_correct_pred, \"n_pred\": len(y)}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\r\n        test_acc = sum([x['n_correct_pred'] for x in outputs]) / sum(x['n_pred'] for x in outputs)\r\n        tensorboard_logs = {'test_loss': avg_loss, 'test_acc': test_acc, 'step': self.current_epoch}\r\n\r\n        return {'log': tensorboard_logs}\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n### Environment\r\n\r\n\r\n - PyTorch Version: 1.5.1\r\n - PyTorch Lightning version: 0.8.5\r\n - OS: Linux\r\n - How you installed PyTorch : pip\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: GeForce GTX 860M\r\n\r\n### Additional context\r\n\r\nI am using 16 bits precision but the same bugs appear when using 32 bits.\r\n\r\nThanks'!\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2652/comments",
    "author": "adeboissiere",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-09-15T17:41:34Z",
        "body": "@adeboissiere is this still an actual issue, mind test master? 🦝 \r\notherwise could you please share a complete example to reproduce..."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-10-05T02:33:25Z",
        "body": "This is likely because you are not turning off dropout. If you use the class based dropout you need to let the layer know when it is in training or not"
      }
    ]
  },
  {
    "number": 2616,
    "title": "torch.save(model) in dump_checkpoint",
    "created_at": "2020-07-15T13:31:41Z",
    "closed_at": "2020-10-29T06:10:34Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2616",
    "body": "## 🚀 Feature\r\nCurrent PL save the only weight of network, but param searched network change their architecture flexibly while training.\r\n\r\n`Trainer.(..., save_entier_model=True)` could be option\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n After training 'param searched network' with pl and closed session, recognized I cannot load weight! :(\r\n\r\nSaving the only state_dict is reasonable in most cases, but give the option to users for saving the whole network will be nice in some cases. \r\n\r\n### Alternatives\r\n\r\nManually save by users\r\n\r\n### Additional context\r\n\r\nAlways thx for cool dev project!\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2616/comments",
    "author": "davinnovation",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-07-15T17:49:11Z",
        "body": "> After training mnasnet with pl and closed session, recognized I cannot load weight! :(\r\n\r\nCan you share the model code and error?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-08T05:54:19Z",
        "body": "The description is not very clear, but think @davinnovation wants to do architecture search (correct me if I'm wrong), where they train several model architectures and they want to save the full architecture to the checkpoint to easily load and remember the model architecture.\r\n@davinnovation can you confirm that this is what you mean?\r\n\r\nin this case I would recommend onnx, jit, and all these other model serializers."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T03:24:29Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2612,
    "title": "load_from_checkpoint raises TypeError when **kwargs is provided",
    "created_at": "2020-07-14T23:32:33Z",
    "closed_at": "2020-09-23T14:45:40Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2612",
    "body": "## 🐛 Bug\r\n\r\nThe issue happens in case of .save_hyperparameters() is not used.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior: follow code sample\r\n\r\n#### Code sample\r\n\r\n```\r\nimport os\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\n\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.core.lightning import LightningModule\r\n\r\nclass LitModel(LightningModule):\r\n\r\n    def __init__(self, some_stuff):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        tensorboard_logs = {'train_loss': loss}\r\n        return {'loss': loss, 'log': tensorboard_logs}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.001)\r\n\r\n    def train_dataloader(self):\r\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\r\n        loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)\r\n        return loader\r\n\r\n\r\nmodel = LitModel(42)\r\ntrainer = pl.Trainer(fast_dev_run=True)\r\ntrainer.fit(model)\r\n\r\ntrainer.save_checkpoint('checkpoint.pl')\r\nmodel = LitModel.load_from_checkpoint('checkpoint.pl', some_stuff=42)\r\n\r\n# TypeError: __init__() got multiple values for argument 'some_stuff'\r\n```\r\n\r\n### Expected behavior\r\n\r\nModel is able to load\r\n\r\n### Environment\r\n - PyTorch Version (e.g., 1.0): 1.5.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.6\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nI noticed that loading starts working if you call save_hyperparameters. It also works correctly if some_stuff is not inside save_hyperparameters, e.g.  if you change `__init__` to this one\r\n\r\n```\r\n    def __init__(self, some_stuff, lr=42):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n        self.lr = 42\r\n        self.save_hyperparameters('lr')\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2612/comments",
    "author": "Guitaricet",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-14T23:33:38Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-07-15T22:33:18Z",
        "body": "ok, it seems a bit tricky how to correctly interpret method arguments and still keep back-compatibility..."
      },
      {
        "user": "Guitaricet",
        "created_at": "2020-07-15T22:46:14Z",
        "body": "Maybe let's wait until the next major version to fix it, but document the behaviour.\r\nIt's not a bug, if it is documented, right? 😆 \r\n\r\nAnd in my (reasonably complex) case it was resolved when I added `.save_hyperparameters()` at the end of `__init__`. No other code changes were needed."
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-16T18:04:30Z",
        "body": "Similar #2550?"
      },
      {
        "user": "Guitaricet",
        "created_at": "2020-07-16T23:10:19Z",
        "body": "Yes, exactly the same issue"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-15T00:00:48Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-23T14:45:40Z",
        "body": "@Borda please check if issue persists on master"
      }
    ]
  },
  {
    "number": 2592,
    "title": "Log debug information to a file",
    "created_at": "2020-07-12T19:55:41Z",
    "closed_at": "2021-09-05T21:57:17Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2592",
    "body": "## 🚀 Feature\r\n\r\nCreate a log file for each run with debug information.\r\n\r\n### Motivation\r\n\r\nUser can read the logs and understand in which order things got executed. For example, logs could contain this information:\r\n\r\nprepare_data_called\r\nsetup called\r\nstarted train loop\r\ntraining step called\r\nbackward called\r\nzero grad called\r\netc...\r\n\r\n### Pitch\r\n\r\nUse Python logging and redirect all messages with level \"debug\" to a file in e.g. the logger folder. \r\n\r\n### Additional context\r\n\r\nSuggested by @williamFalcon in a slack discussion.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2592/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-11-19T01:46:33Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "vasudev-sharma",
        "created_at": "2021-06-25T08:45:51Z",
        "body": "@awaelchli can I work on this issue? If yes, could you or any maintainers assign me?"
      },
      {
        "user": "Borda",
        "created_at": "2021-06-25T09:09:18Z",
        "body": "@vasudev-sharma Great, looking forward :rabbit: "
      },
      {
        "user": "awaelchli",
        "created_at": "2021-06-25T12:54:50Z",
        "body": "This issue is a bit old and I'm not sure anymore if we have a strong need for it. But if you wish, please feel free to explore it and make a proof of concept. You may want to take a look at where we call the hooks in the `Trainer.call_hook()` method. This could be a place where we do a debug log :) "
      }
    ]
  },
  {
    "number": 2583,
    "title": "Change checkpoint filename without overwriting full filepath",
    "created_at": "2020-07-11T13:41:13Z",
    "closed_at": "2020-09-18T15:51:42Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2583",
    "body": "## 🚀 Feature\r\nBy default, the checkpoint path resolves to `lightning_logs/{version}/checkpoints/{epoch}.ckpt`. It is currently not possible to change the checkpoint filename without replacing the full path.\r\n\r\n### Motivation\r\nThe full path is replacable with the `ModelCheckpoint.filepath` argument. This feature is undisirable because I want to use the default path resolver that uses the options from `Trainer` instead of hardcoding a path.\r\n\r\nA satisfying path construction workaraound based on values in `Trainer` is not possible because the `Trainer` is constructed *after* the `ModelCheckpoint`.\r\n\r\n### Pitch\r\nI would like to add a `filename` parameter to the `ModelCheckpoint` constructor. This parameter is complementary to `filepath` and `filepath` will keep working like it does for backward compatibility.\r\n\r\n- If only `filepath` is provided, the original behaviour is used.\r\n- If both `filepath` and `filename` are provided, the checkpoint path will resolve to `filepath`/`filename`.\r\n- If only `filename` is provided, the checkpoint path will resolve to `resolved directory`/`filename` (instead of `resolved directory`/`epoch`).",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2583/comments",
    "author": "wietsedv",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-09-09T14:12:30Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2571,
    "title": "Flush Events for Tensorboard Logger",
    "created_at": "2020-07-10T01:07:58Z",
    "closed_at": "2020-09-18T15:51:41Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2571",
    "body": "## 🚀 Feature\r\nAdd an option to automatically flush events every epoch for TensorBoardLogger\r\n\r\n### Motivation\r\nTensorboardX and Tensorboard only refresh once their \"buffer\" is filled. Often, the user would like to see an update before the buffer is filled (say, after every epoch). In tensorboardX/tensorflow, this is accomplished by calling \"flush_events.\" \r\n\r\n### Alternatives\r\nIf you could share how to manually call flush_events so I can insert it into my training loop, that would be great. This feature is not currently described anywhere in the documentation.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2571/comments",
    "author": "raunakdoesdev",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-10T01:09:04Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-10T08:45:58Z",
        "body": "Did you try `self.logger.experiment.flush()`?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-10T10:21:48Z",
        "body": "@awaelchli i thought we did this automatically?\r\n@tullie "
      },
      {
        "user": "tullie",
        "created_at": "2020-07-10T15:08:32Z",
        "body": "SummaryWriter writes asynchronously every 120 seconds or when the max_queue_size is 10. These defaults can be changed with the SummaryWriter args: max_queue_size, flush_secs.\r\n\r\nFor our use case at work we can't write every iteration (I think it's too slow because we write to an nfs). Are users fine with explicitly flushing themself with self.logger.experiment.flush()? Otherwise we could make it an arg for the logger interface?\r\n\r\n\r\n\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-10T15:22:17Z",
        "body": "We could generally just add a kwargs argument to the wrapper and pass them down to the SummaryWriter. I don't see a reason why to hide these args. "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-10T15:26:42Z",
        "body": "maybe we can tie it to the flag about intervals of saving to disk? that flag already exists today. If we enable -1 it will use the default tensorboard interval or the default logger interval"
      },
      {
        "user": "raunakdoesdev",
        "created_at": "2020-07-10T17:20:31Z",
        "body": "I'll do `self.logger.experiment.flush()` for now. Thanks @awaelchli \r\n\r\nI think flushing every epoch should be default behavior, with less frequent flushing an optional parameter you can pass (similar to how validation is handled)."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-09T14:12:31Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2558,
    "title": "trainer.test(model) on 0.8.4 with a checkpoint saved in 0.6.0 expects attribute 'checkpoint_callback_best_model_path'",
    "created_at": "2020-07-08T21:39:51Z",
    "closed_at": "2020-10-05T02:28:34Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2558",
    "body": "\r\n## 🐛 Bug\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Save a checkpoint in 0.6.0\r\n2. Load the model in 0.8.4 (no problem)\r\n`model = My_model.load_from_checkpoint(checkpoint_path)`\r\n2. Run `trainer.test(model) `\r\n4. See error\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-3-eb476187792a> in <module>\r\n     84 \r\n     85 \r\n---> 86 trainer.test(model)\r\n     87 print(\"Test finished!\\n\")\r\n\r\n~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in test(self, model, test_dataloaders, ckpt_path)\r\n   1240         if model is not None:\r\n   1241             self.model = model\r\n-> 1242             self.fit(model)\r\n   1243 \r\n   1244         # on tpu, .spawn means we don't have a trained model\r\n\r\n~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)\r\n    977 \r\n    978         elif self.single_gpu:\r\n--> 979             self.single_gpu_train(model)\r\n    980 \r\n    981         elif self.use_tpu:  # pragma: no-cover\r\n\r\n~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)\r\n    183             self.reinit_scheduler_properties(self.optimizers, self.lr_schedulers)\r\n    184 \r\n--> 185         self.run_pretrain_routine(model)\r\n    186 \r\n    187     def tpu_train(self, tpu_core_idx, model):\r\n\r\n~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)\r\n   1110 \r\n   1111         # restore training and model before hpc call\r\n-> 1112         self.restore_weights(model)\r\n   1113 \r\n   1114         # when testing requested only run test and return\r\n\r\n~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py in restore_weights(self, model)\r\n    180         if not did_restore_hpc_weights:\r\n    181             if self.resume_from_checkpoint is not None:\r\n--> 182                 self.restore(self.resume_from_checkpoint, on_gpu=self.on_gpu)\r\n    183 \r\n    184         # wait for all models to restore weights\r\n\r\n~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py in restore(self, checkpoint_path, on_gpu)\r\n    312 \r\n    313         # load training state (affects trainer only)\r\n--> 314         self.restore_training_state(checkpoint)\r\n    315 \r\n    316     def dump_checkpoint(self, weights_only: bool = False) -> dict:\r\n\r\n~/.conda/envs/qwe3/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py in restore_training_state(self, checkpoint)\r\n    427                 )\r\n    428                 checkpoint_callbacks[-1].best_model_score = checkpoint['checkpoint_callback_best']\r\n--> 429             checkpoint_callbacks[-1].best_model_path = checkpoint['checkpoint_callback_best_model_path']\r\n    430 \r\n    431         if early_stopping_callbacks:\r\n\r\nKeyError: 'checkpoint_callback_best_model_path'\r\n```\r\n\r\nBut checkpoint's attribute ''checkpoint_callback_best_model_path' doesn't exist in my old PTL version...\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n### Temporal Hack\r\n\r\n\r\n1. Load the model\r\ncheckpoint_path = 'lightning_logs/version_149/checkpoints/_ckpt_epoch_3.ckpt'\r\ncheckpoint = torch.load(checkpoint_path)\r\n\r\n2. Add a dummy path to the required argument\r\ncheckpoint['checkpoint_callback_best_model_path'] = ''\r\n\r\n3. Save this checkpoint\r\ntorch.save(checkpoint, checkpoint_path)\r\n\r\n4. Load from the new checkpoint in 0.8.4 and run `trainer.test(model)` without any problem\r\n\r\n### Environment\r\n\r\n\r\n\r\n - PyTorch Version: 1.5.1\r\n - OS: Linux\r\n - How you installed PyTorch: conda\r\n - Python version: 3.6.10\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: Tesla V100-SXM3-32GB\r\n\r\nProbably I am misunderstanding something, but this is my quick fix, thanks for your help!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2558/comments",
    "author": "pherrusa7",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-08T21:40:48Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-07-08T22:46:35Z",
        "body": "yes, I guess we shall support loading compatibility from the beginning.. @PyTorchLightning/core-contributors \r\n@pherrusa7 mind send a PR with mapping this as a mirror of existing attribute?"
      },
      {
        "user": "pherrusa7",
        "created_at": "2020-07-09T13:52:55Z",
        "body": "Dear @Borda, I have no time this week, but if the issue is still open in the next weeks I can do it :)"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-08-03T20:08:40Z",
        "body": "@pherrusa7 how about now? :)"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-16T18:35:17Z",
        "body": "@pherrusa7 can you verify this is still broken in master?"
      },
      {
        "user": "dvirginz",
        "created_at": "2020-09-24T04:53:04Z",
        "body": "It is still broken"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-09-29T22:49:31Z",
        "body": "@dvirginz \r\nI tested this by saving a checkpoint in 0.6 and loading it with the same code on master branch. \r\nThere is no error anymore and `trainer.test(model)` will use the loaded model. I tested on cpu and also multi gpu. \r\nSo the latest version should work for you. \r\n\r\nNote, it seems that in 0.6 Lightning was not tracking the path to the best model checkpoint. So we can't know which of your old checkpoints is the best. There is also a conversion script in utilities \"upgrade_checkpoint.py\", this will upgrade old checkpoints to the current format, if needed. I recommend you upgrade all your checkpoints that you intend to use, and work with the latest PL version.\r\n\r\n"
      }
    ]
  },
  {
    "number": 2554,
    "title": "Support easily reading and writing checkpoints from different systems instead of just to disk",
    "created_at": "2020-07-08T19:32:06Z",
    "closed_at": "2021-02-16T18:17:54Z",
    "labels": [
      "duplicate",
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2554",
    "body": "## 🚀 Feature\r\nI want to easily be able to extend model_checkpoint so that it can be used with blob stores instead of always writing to disk.  One way to do this is to abstract to replace the calls to os with an interface.\r\n### Motivation\r\n\r\nI want to read and write my files from a blob store, not to disk.\r\n\r\n### Pitch\r\n\r\nI want to easily be able to swap out reads and writes to disk with writes to a blob store.\r\n\r\n### Alternatives\r\n\r\nWrite a nearly identical class to ModelCheckpoint\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2554/comments",
    "author": "jloveric",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-08T19:33:01Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T15:43:53Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-25T05:06:49Z",
        "body": "hey @jloveric, model checkpoint and all IO in general now uses fsspec. Is that what you need?"
      }
    ]
  },
  {
    "number": 2538,
    "title": "tensor_metric decorator does not let you return Tuple or List ouputs",
    "created_at": "2020-07-07T10:51:10Z",
    "closed_at": "2020-07-07T13:17:24Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2538",
    "body": "## 🐛 Bug\r\n\r\nWhen creating a metric function that returns multiple outputs in the form of a Tuple or List the metric class complains that it can't convert a Tuple or List to a tensor, even though the contents of the Tuple/List are tensors.\r\n\r\n### To Reproduce\r\n\r\nAn example of this would be a function  to return the topk accuracy \r\n\r\n```\r\n\r\n    @tensor_metric()\r\n    def accuracy(output, target, topk=(1,)):\r\n        \"\"\"Computes the precision@k for the specified values of k\"\"\"\r\n\r\n        maxk = max(topk)\r\n        batch_size = target.size(0)\r\n    \r\n        _, pred = output.topk(maxk, 1, True, True)\r\n        pred = pred.t()\r\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\r\n    \r\n        res = []\r\n        for k in topk:\r\n            correct_k = correct[:k].view(-1).float().sum(0)\r\n            res.append(correct_k.mul_(100.0 / batch_size))\r\n        return res\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        acc = self.accuracy(y_hat, y, topk=(1, 5))\r\n\r\n```\r\n\r\n### Error Output\r\n\r\n```\r\n\r\nEpoch 1:   0%|          | 0/1876 [00:00<?, ?it/s] Traceback (most recent call last):\r\n  File \"/home/local/CORP/dbyrne/Documents/Projects/RL/pytorch-lightning-bolts/pl_bolts/models/mnist_module.py\", line 138, in <module>\r\n    trainer.fit(model)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 912, in fit\r\n    self.dp_train(model)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 258, in dp_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1093, in run_pretrain_routine\r\n    self.train()\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 375, in train\r\n    self.run_training_epoch()\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 458, in run_training_epoch\r\n    _outputs = self.run_training_batch(batch, batch_idx)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 634, in run_training_batch\r\n    loss, batch_output = optimizer_closure()\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 598, in optimizer_closure\r\n    output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 754, in training_forward\r\n    output = self.model(*args)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 65, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 69, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 209, in parallel_apply\r\n    raise output\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 166, in _worker\r\n    output = module.training_step(*input, **kwargs)\r\n  File \"/home/local/CORP/dbyrne/Documents/Projects/RL/pytorch-lightning-bolts/pl_bolts/models/mnist_module.py\", line 52, in training_step\r\n    acc = self.accuracy(y_hat, y, topk=(1, 5))\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/metrics/converters.py\", line 58, in new_func\r\n    result = function_to_decorate(*args, **kwargs)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/metrics/converters.py\", line 59, in new_func\r\n    return func_to_apply(result, *dec_args, **dec_kwargs)\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/pytorch_lightning/metrics/converters.py\", line 84, in _convert_to_tensor\r\n    raise TypeError(f\"The given type ('{type(data).__name__}') cannot be converted to a tensor!\")\r\nTypeError: The given type ('list') cannot be converted to a tensor!\r\nException ignored in: <function tqdm.__del__ at 0x7f8c82724ae8>\r\nTraceback (most recent call last):\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/tqdm/std.py\", line 1086, in __del__\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/tqdm/std.py\", line 1293, in close\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/tqdm/std.py\", line 1471, in display\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/tqdm/std.py\", line 1089, in __repr__\r\n  File \"/home/local/CORP/dbyrne/anaconda3/envs/core_rl/lib/python3.7/site-packages/tqdm/std.py\", line 1433, in format_dict\r\nTypeError: cannot unpack non-iterable NoneType object\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n### Environment\r\n\r\n - PyTorch Version: 1.4\r\n - OS: Linux\r\n - How you installed PyTorch: Conda\r\n - Python version: 3.7.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: RTX 2080",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2538/comments",
    "author": "djbyrne",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-07-07T12:54:59Z",
        "body": "Could you try to use the decorator `tensor_collection_metric`? The intention with `tensor_metric` is that should decorate metric functions that returns a single tensor, whereas `tensor_collection_metric` is meant to be used on a collection of metrics.\r\n\r\nOtherwise you could just stack your `res` list into a tensor: `res=torch.cat(res)`."
      },
      {
        "user": "djbyrne",
        "created_at": "2020-07-07T13:17:24Z",
        "body": "Hey @SkafteNicki, yes that works perfectly thank you! Apologies, I do remember reading that tensor_collection_metric is needed for multiple outputs a few days ago and completely forgot. I will close the Issue "
      }
    ]
  },
  {
    "number": 2530,
    "title": "Checkpoint save and restore mid-epoch",
    "created_at": "2020-07-06T15:55:23Z",
    "closed_at": "2020-09-13T18:14:25Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2530",
    "body": "## 🚀 Feature\r\nAdd option to checkpoint save and restore mid-epoch.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2530/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-09-04T17:43:26Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2498,
    "title": "TPU hangs when using only a train loop (ie: no val loop)",
    "created_at": "2020-07-04T14:25:36Z",
    "closed_at": "2020-08-03T22:00:37Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2498",
    "body": "I think it's somehow related to checkpointing.\r\n\r\nEasiest way to debug is to get on colab.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2498/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-04T14:26:26Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-05T07:36:53Z",
        "body": "@williamFalcon I struggle to reproduce this. PL does not even recognize the TPUs in the runtime, XLA_AVAILABLE gets set to false (mnist tpu colab). Tried different pytorch and xla versions, all the same. \r\nCould you share the colab in which you observed the hangs?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-12T23:43:07Z",
        "body": "@williamFalcon checked again and now I am able to run the mnist colab without validation and it does not hang anymore (latest master). Not sure what fixed it."
      }
    ]
  },
  {
    "number": 2497,
    "title": "Add Float64 support",
    "created_at": "2020-07-04T13:27:39Z",
    "closed_at": "2021-03-24T10:17:58Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "let's do it!"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2497",
    "body": "Float64 precision is currently not supported. \r\nWhy? This should be pretty straightforward, raising an Exception if the device or other configurations are not compatible with it.\r\nAre you planning to add support soon? \r\nWhat is the best workaround currently? Will it work if I do it manually via `model.to(torch.float64)` and similar for each dataloader or are there some caveats?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2497/comments",
    "author": "richardk53",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-04T13:28:29Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-04T14:39:55Z",
        "body": "i didn’t know it wasn’t supported haha. what’s special about 64 that it doesn’t work?"
      },
      {
        "user": "richardk53",
        "created_at": "2020-07-04T15:44:25Z",
        "body": "I see ;-) \r\nI thought that the argument `precision=64` in the Trainer would take care of casting both the model and the batches to the desired precision, similarly to how it takes care of moving the model and data to the correct device(s). \r\n\r\nHowever, taking Step 1 and Step 2 from the Quickstart in the docs and replacing the trainer by `trainer = Trainer(gpus=1, precision=64, use_amp=False)` does not make a difference, everything is still FP32. \r\nAlso, the documentation states that precision should be 16 or 32. \r\n\r\nFor this example I can simply do `model = model.to(torch.float64)` and in the train_dataloader something like `dataset = MNIST(os.getcwd(), train=True, download=True, transform=lambda x: transforms.ToTensor()(x).to(torch.float64))` to make it work. \r\n\r\nBut I think it would be a nice feature to have this done by the Trainer using the precision argument. What do you think?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-04T15:46:33Z",
        "body": "oooohhh love the idea...\r\nWant to take a stab at the PR? we'll help you finish it :) "
      },
      {
        "user": "richardk53",
        "created_at": "2020-07-04T16:06:48Z",
        "body": "sure :) "
      },
      {
        "user": "Borda",
        "created_at": "2020-08-04T20:39:50Z",
        "body": "@richardk53 how is it going?"
      },
      {
        "user": "richardk53",
        "created_at": "2020-08-06T06:44:58Z",
        "body": "Hey, sorry, haven’t had the time for this yet. "
      }
    ]
  },
  {
    "number": 2495,
    "title": "`precision=16` displaying wrong loss in progress bar",
    "created_at": "2020-07-04T12:30:29Z",
    "closed_at": "2020-07-05T02:52:50Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2495",
    "body": "## 🐛 Bug\r\n\r\nWhen training on a GPU (*using native AMP*) and setting `precision=16`, the loss displayed by the progress bar is some crazy large number.\r\n\r\nStopping the example bellow in the middle of Epoch 1 gives a loss of ~15 000. If I train with `precision=32`, this loss is the true value of ~0.23.\r\n\r\nThe `loss` tensor is OK, if I add a print statement in the training loop it displays normal values.\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n```python\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom torch import nn\r\n\r\nfrom pytorch_lightning.core.lightning import LightningModule\r\nfrom pytorch_lightning import Trainer\r\n\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchvision.datasets import MNIST\r\n\r\nimport os\r\nfrom torchvision import datasets, transforms\r\n\r\nclass LitMNIST(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer_1 = torch.nn.Linear(28 * 28, 128)\r\n        self.layer_2 = torch.nn.Linear(128, 256)\r\n        self.layer_3 = torch.nn.Linear(256, 10)\r\n\r\n    def forward(self, x):\r\n        batch_size, channels, width, height = x.size()\r\n        x = x.view(batch_size, -1)\r\n        x = self.layer_1(x)\r\n        x = torch.relu(x)\r\n        x = self.layer_2(x)\r\n        x = torch.relu(x)\r\n        x = self.layer_3(x)\r\n        x = torch.log_softmax(x, dim=1)\r\n        return x\r\n\r\n    def train_dataloader(self):\r\n        transform=transforms.Compose([transforms.ToTensor(),\r\n                                      transforms.Normalize((0.1307,), (0.3081,))])\r\n        mnist_train = MNIST(os.getcwd(), train=True, download=False, transform=transform)\r\n        return DataLoader(mnist_train, batch_size=64)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        logits = self(x)\r\n        loss = F.nll_loss(logits, y)\r\n\r\n        # add logging\r\n        logs = {'loss': loss}\r\n        return {'loss': loss, 'log': logs}\r\n    \r\n\r\nmodel = LitMNIST()\r\ntrainer = Trainer(gpus=1, precision=16)\r\ntrainer.fit(model)\r\n```\r\n\r\n### Environment\r\n\r\n - PyTorch Version:  v1.7.0.dev20200704 (nightly)\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 10.2 (installed with conda from pytorch chanel)\r\n - GPU models and configuration: RTX 2070 SUPER\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2495/comments",
    "author": "tadejsv",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-04T12:31:18Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Renthal",
        "created_at": "2020-10-30T10:18:01Z",
        "body": "I have the same issue, but not on the sample code provided. The only difference in my sample code is that using a custom dataset and L1 loss the numbers are (initially) over 200.0 (as opposed to MNIST where it starts very low). \r\nAll is fine until after ~10 epochs where the loss in the pbar becomes 'inf'; whereas the logs shows the correct value being around 20."
      }
    ]
  },
  {
    "number": 2487,
    "title": "Dynamic Data Loaders",
    "created_at": "2020-07-03T16:30:22Z",
    "closed_at": "2020-09-11T18:31:41Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2487",
    "body": "### 🚀 A dataloader with changable sampling behavior\r\nA `DataLoader` that takes in a confusion matrix, or class specific recall at the end of the epoch. It then oversamples classes that are the least known to the network in the next epoch and hereby helps the network train more efficiently.\r\n\r\n#### Motivation\r\nI often work with highly imbalanced datasets, which I have previously been dealing with artificially oversampling classes that are less prevalent (_label-balancing_). Also I have been experimenting with extending this stratification from the plain labels to other confounders aswell (_confounder-balancing_). (* example provided at the end)\r\n\r\nIt would be great to make this process **dynamic**! Since the data stays the same and duplicating images seems a waste of memory and only the distribution changes over which samples are drawn from the data. \r\n\r\n#### The idea is the following:\r\nAt the end of the epoch, I check how well the model is doing on the validation set and see if there are any subgroups (either on label or confounder level) and would pass the information on to my `train dataloader` (or its `pytorch.utils.data.Sampler`). Sticking to the example in the footnotes, I'd get the following validation errors:\r\n\r\nDog pics taken on the inside 0.4 MSE\r\nDog pics taken on the outside 0.3 MSE\r\nCat pics taken on the inside 0.5 MSE\r\nCat pics taken on the outside 0.2 MSE\r\n\r\nThen I would call something like:\r\n`self.train_dataloader.sampler.distribution = [0.4, 0.3, 0.5, 0.2]`  \r\nand `self.train_dataloader.sampler` would yield indices antiproportional to that distribution from its `data_source`, e.g. via `torch.nn.functional.softmax(-torch.Tensor([0.4, 0.3, 0.5, 0.2])) = `\r\n\r\n#### My concern\r\nWhile pytorch offeres many strategies to tackle this, e.g. subclassing `torch.utils.data.DataLoader` or `torch.utils.data.Sampler`, I am not sure, how Pytorch Lightning is handling the `dataloaders` in the back, where they are stored and what the implications of that would be for the awesome functionalities Pytorch Lightning offeres (especially data-parallel methods etc.)\r\n\r\nAs this would be my first Pytorch Lightning project, I am yet unfamiliar with the inside functionality, so any ideas from experts with the API are highly appreaciated. I'll keep you posted with my progress! Best wishes and thanks for your input!\r\n\r\n\r\n___________________________________________________________________\r\n#### Example stratification (_label-/confounder-balancing_):\r\nIf I tried to classify cats vs. dogs from images and I have 20% dogs in my dataset, I would duplicate each dog picture 4 times and had a balanced dataset (_label-balancing_). If the pictures in my dataset had one of two backgrounds (outside/inside) and I had an imbalance like 25% dogs-inside 25% dogs-outside 40% cats-inside 10% cats-outside, then I had 50/50 labels, but my network is more likely to classify pictures taken inside to be from cats, since the label is more prevalent in that subclass. Knowing that this irregularity exists, I would oversample cat pictures taken outside and a few more dog pics from both classes so I'd have a balance dataset (_confounder-balancing_). Of course, these imbalances are not exclusive and for me it just boils down to, finding out as much as I can over my data and trying my best to reduce the all imbalances I find, unless the groups are super small, or the clientdoesn't care and thinks classifying images based on the background of the image is something he wants to pay me for (happen's more often than you think).\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2487/comments",
    "author": "jonesy-git",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-03T16:31:16Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-02T18:15:04Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "inder-preet-kakkar",
        "created_at": "2021-01-29T17:39:26Z",
        "body": "@jonesy-git This is a good idea I too want to implement something like that. This issue should have been open."
      }
    ]
  },
  {
    "number": 2470,
    "title": "WandB Logger always resumes even when `Trainer(resume_from_checkpoing=None)`",
    "created_at": "2020-07-02T16:05:46Z",
    "closed_at": "2020-07-28T20:31:16Z",
    "labels": [
      "bug",
      "help wanted",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2470",
    "body": "## 🐛 Bug\r\n\r\n@awaelchli As requested, bug above in the title.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nmodel = CoolSystem.load_from_checkpoint(checkpoint)\r\nlogger = WandbLogger()\r\ntrainer = Trainer(resume_from_checkpoint=None)\r\ntrainer.fit(model)\r\n```\r\n\r\nThe above resumes the wandb run.\r\n\r\nPL 0.8.4\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6 nightly\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): Conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration: V100\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2470/comments",
    "author": "Laksh1997",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-07-05T06:10:27Z",
        "body": "@Laksh1997 I simply cannot reproduce this, I tried in several ways, online, offline, with and without checkpoints etc.. Did you set any environment variables related to WandB?\r\nIf possible, could you post a full code sample or Google colab?\r\nThanks"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-07-28T20:31:16Z",
        "body": "Closing this. @Laksh1997 please reopen if still relevant."
      }
    ]
  },
  {
    "number": 2443,
    "title": "Eval-only AMP",
    "created_at": "2020-07-01T07:49:13Z",
    "closed_at": "2020-10-28T20:27:20Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2443",
    "body": "## 🚀 Feature\r\nA trainer flag to enable Automatic Mixed Precision only during Validation/Testing.\r\n\r\n### Motivation\r\n\r\nSome models cannot be trained with AMP because of peculiarities in their loss functions (e.g. Weakly Supervised methods based on WSDDN, which require the use of `F.binary_cross_entropy` which does not work under AMP). This doesn't prevent such models from running under AMP during evaluation though since the incompatible functions are in the loss. Especially for validation where the most precise accuracy metrics may not be necessary, this would be very useful.\r\n\r\n### Pitch\r\n\r\nAdd a new flag (or flags) on `Trainer` along the lines of `use_amp_val`. \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2443/comments",
    "author": "bradezard131",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-01T07:50:00Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-07-01T09:56:21Z",
        "body": "cc: @williamFalcon "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-30T10:45:13Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-30T14:04:44Z",
        "body": "is this still an issue??"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-21T16:43:51Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2409,
    "title": "save the model/training source code to model checkpoint or logs directory",
    "created_at": "2020-06-29T02:52:22Z",
    "closed_at": "2020-09-09T19:44:53Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2409",
    "body": "## 🚀 Feature\r\n\r\nSave the model/training source code to model checkpoint or logs directory\r\n\r\n### Motivation\r\n\r\nNow, the hparams has been saved in yaml file.  Sometimes, we not only change the hparams but also the network arch, the pre-process flow, so if we save the relate source code to model, we will get all the information to restore the model, because the source code is with the model hparams and all other things.\r\n\r\n### Pitch\r\n\r\nadd Trainer args parse params, or Checkpoint callback params,  one for wether to save source code, and one for which codes to save\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2409/comments",
    "author": "ares89",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-29T02:53:17Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "dscarmo",
        "created_at": "2020-06-29T20:57:32Z",
        "body": "I achieve this through saving the commit hash (assuming your code is in a git repo). \r\nSome loggers will catch this automatically, or you can have a function to add this to your parameters automatically.\r\n\r\nWould be nice as a feature anyway. "
      },
      {
        "user": "versatran01",
        "created_at": "2020-07-02T19:36:53Z",
        "body": "I think sacred can do this. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-31T19:38:36Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2408,
    "title": "Cluster job that spawns its own processes for use with DDP",
    "created_at": "2020-06-29T02:33:22Z",
    "closed_at": "2020-09-06T09:49:47Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2408",
    "body": "## 🚀 Feature\r\n\r\nNot sure if the title is appropriate.  This feature would support the use case where\r\n\r\n- The manager sets `MASTER_ADDR` and `MASTER_PORT`\r\n- User knows how to set `LOCAL_RANK`, `GLOBAL_RANK`, and `WORLD_SIZE`\r\n- Each node has `N_g` GPUs\r\n- `N_j` jobs are spawned (in my case, MPI on SLURM) for each gpu, i.e., `world_size= N_j * N_g`\r\n- Each job can see both GPUs on each node, i.e., `local_rank = global_rank % N_g` and `torch.cuda.set_device(local_rank)`\r\n\r\n### Motivation\r\n\r\nI'm able to write a class that overrides `pl.Trainer` to support this, but thought 1) this might be a use case for others and 2) I'd prefer not to override your code as much as possible.   Here is the `sbatch` file header\r\n\r\n```bash\r\n#!/bin/bash \r\n#SBATCH --job-name job\r\n#SBATCH -o jobs/%j.log \r\n#SBATCH -N 4 \r\n#SBATCH --tasks-per-node=2 \r\n#SBATCH --partition=gaia \r\n#SBATCH --gres=gpu:volta:2 \r\n\r\nexport MASTER_ADDR=$(hostname -s) \r\nexport MASTER_PORT=$(python -c 'import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()') \r\n\r\nmpirun <options> <command>\r\n```\r\n\r\nEach job sees 2 GPUs (and the device ids are not integers, another issue).   To setup my run I set the following environment variables:\r\n\r\n```python\r\nglobal_rank, world_size, hostname = get_dist_env()\r\nos.environ['WORLD_SIZE'] = f'{world_size}'\r\nos.environ['NODE_RANK'] = f'{global_rank}'\r\nos.environ['LOCAL_RANK'] = f'{global_rank % 2}'\r\n```\r\n\r\nwhere `get_dist_env` knows how to get `world_size` and `global_rank` from the environment.  For `mpirun` this is\r\n\r\n```python\r\nworld_size = int(os.getenv('OMPI_COMM_WORLD_SIZE'))\r\nglobal_rank = int(os.getenv('OMPI_COMM_WORLD_RANK'))\r\n```\r\n\r\nWith those variables (which I think are standard in your code) I should be able to run in `DDP` mode.  Yet, the issue is, because each node sees both GPUs, I cannot define a setting in `Trainer` that will allow this to execute correctly.  Either I set `num_gpus=1` and the `local_rank` is not calculated correctly or if I set `num_gpus=2` then your code will try to spawn an additional job.\r\n\r\n### Pitch\r\n\r\nI'm not sure what the best API approach is, but if the user sets `MASTER_ADDR`, `MASTER_PORT`, `WORLD_SIZE`, `GLOBAL_RANK`, and `LOCAL_RANK` then that should be everything you need to execute a distributed job.  \r\n\r\n### Additional context\r\n\r\nI'm clearly not expert in distributed processing so I'm not sure if I'm asking for something that only works on my cluster with my settings and cannot be generalized.  In this case, I am able to override `Trainer` to support my use case without you needing to change anything.\r\n\r\nThanks for a great package!\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2408/comments",
    "author": "jgbos",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-29T02:34:16Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-28T09:38:38Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2400,
    "title": "CrossEntropyLoss fails to run with GPU",
    "created_at": "2020-06-28T15:04:30Z",
    "closed_at": "2020-06-29T01:47:41Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2400",
    "body": "## 🐛 Bug\r\n\r\nUsing the following `training_step` method which uses `nn.CrossEntropyLoss()` loss function:\r\n\r\n```python\r\n    def training_step(self, batch, batch_idx):\r\n        x1, x2 = batch[\"x1\"], batch[\"x2\"]\r\n        predict = self(x1, x2)\r\n        target = torch.arange(x1.size()[0])\r\n        loss = self.loss_fn(predict, target)\r\n        return {'loss': loss}\r\n```\r\nfails to run with GPU throwing the following error:\r\n\r\n```python\r\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #2 'target' in call to _thnn_nll_loss_forward\r\n```\r\nThe function `self.loss_fn` is shown below:\r\n\r\n```python\r\nimport torch\r\nfrom pytorch_lightning import LightningModule\r\nfrom torch import nn\r\n\r\n\r\nclass NPairsLoss(LightningModule):\r\n    \"\"\"\r\n    The N-Pairs Loss.\r\n    It measures the loss given predicted tensors x1, x2 both with shape [batch_size, hidden_size],\r\n    and target tensor y which is the identity matrix with shape  [batch_size, batch_size].\r\n    \"\"\"\r\n\r\n    def __init__(self, alpha=100):\r\n        super(NPairsLoss, self).__init__()\r\n        self.ce = nn.CrossEntropyLoss()\r\n        self.alpha = alpha\r\n\r\n    def similarities(self, x1, x2):\r\n        \"\"\"\r\n        Calculates the cosine similarity matrix for every pair (i, j),\r\n        where i is an embedding from x1 and j is another embedding from x2.\r\n\r\n        :param x1: a tensors with shape [batch_size, hidden_size].\r\n        :param x2: a tensors with shape [batch_size, hidden_size].\r\n        :return: the cosine similarity matrix with shape [batch_size, batch_size].\r\n        \"\"\"\r\n        x1 = x1 / torch.norm(x1, dim=1, keepdim=True)\r\n        x2 = x2 / torch.norm(x2, p=2, dim=1, keepdim=True)\r\n        return self.alpha * torch.matmul(x1, x2.t())\r\n\r\n    def forward(self, predict, target):\r\n        \"\"\"\r\n        Computes the N-Pairs Loss between the target and predictions.\r\n        :param predict: the prediction of the model,\r\n        Contains the batches x1 (image embeddings) and x2 (description embeddings).\r\n        :param target: the identity matrix with shape  [batch_size, batch_size].\r\n        :return: N-Pairs Loss value.\r\n        \"\"\"\r\n        x1, x2 = predict\r\n        predict = self.similarities(x1, x2)\r\n        # by construction the probability distribution must be concentrated on the diagonal of the similarities matrix.\r\n        # so, Cross Entropy can be used to measure the loss.\r\n        return self.ce(predict, target)\r\n```\r\nIs `target = torch.arange(x1.size()[0])` not being created in the GPU?\r\n\r\n### Expected behavior\r\n\r\nThat  target tensor (`target = torch.arange(x1.size()[0])`) is created on the GPU. \r\n\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce RTX 2080\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.19.0\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.1\r\n\t- pytorch-lightning: 0.8.1\r\n\t- tensorboard:       2.2.2\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.7.3\r\n\t- version:           #41-Ubuntu SMP Tue Dec 3 00:27:35 UTC 2019\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2400/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-06-28T17:14:21Z",
        "body": "No, you have to move `target = torch.arange(x1.size()[0])` to the GPU(or any other device you want) because it's not present in the batch from the dataloader.\r\nYou can use `target = torch.arange(x1.size()[0]).to(x.get_device())`."
      },
      {
        "user": "celsofranssa",
        "created_at": "2020-06-28T18:20:46Z",
        "body": "Ok, thanks @rohitgr7."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-29T01:48:09Z",
        "body": "You can also use:\r\n```\r\narget = torch.arange(x1.size()[0]).to(self.device)\r\n```\r\n\r\nthe PL module knows what device it is on."
      },
      {
        "user": "taylorchu",
        "created_at": "2020-07-04T07:56:32Z",
        "body": "@williamFalcon is there a reason why this is not managed by lightning?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-04T11:02:20Z",
        "body": "@taylorchu If you pass that from the DataLoader(or Dataset) itself it will be handled automatically, but if a tensor is created in between the procedure by the user itself, one has to move it to the device manually the PyTorch way."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-04T12:40:21Z",
        "body": "yup... no way around it as mentioned above"
      }
    ]
  },
  {
    "number": 2365,
    "title": "trainer.test() should return the test score",
    "created_at": "2020-06-25T19:49:59Z",
    "closed_at": "2020-07-10T14:44:17Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2365",
    "body": "\r\n## 🐛 Bug\r\n\r\ntrainer.test() has no return value. However, after testing one might like to have the test score returned, and saved.\r\n\r\n#### Code sample\r\n\r\n```\r\ntest_score = trainer.test()\r\n```\r\n\r\n### Expected behavior\r\n\r\ntrainer.test() should return the final test score.\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.0\r\n\t- pytorch-lightning: 0.7.6\r\n\t- tensorboard:       2.2.2\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Darwin\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         i386\r\n\t- python:            3.7.7\r\n\t- version:           Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64\r\n```\r\n\r\n### Additional context\r\n\r\nAlthought in some settings, `test` is only done immediately before publication and thus should not be used for model selection, there are other settings where validation is used for early stopping and `test` is used for final held-out evaluation. In these settings, being able to save the test score is important.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2365/comments",
    "author": "turian",
    "comments": [
      {
        "user": "ZhaofengWu",
        "created_at": "2020-06-28T05:36:20Z",
        "body": "In addition to scores, it could also be predictions when we don't have the test set label (e.g. GLUE). Ideally we can get access to the return value of `test_epoch_end`."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-10T14:44:17Z",
        "body": "This now works in 0.8.5. Trainer.test() returns whatever you return in `test_epoch_end`."
      }
    ]
  },
  {
    "number": 2331,
    "title": "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn",
    "created_at": "2020-06-23T14:48:52Z",
    "closed_at": "2020-06-23T21:08:59Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2331",
    "body": "Hi, \r\n \r\nGot RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n\r\nBelow details:\r\n```\r\nclass Autoencoder(pl.LightningModule):\r\n    \r\n    def __init__(self, hparams: argparse.Namespace):\r\n        super(Autoencoder,self).__init__() \r\n        self.hparams = hparams\r\n           \r\n        self.layer_e_1 = nn.Conv1d(hparams.in_channels, hparams.out_channels, hparams.kernel_size)\r\n        self.layer_e_2 = nn.Conv1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)\r\n        self.layer_d_1 = nn.ConvTranspose1d(hparams.in_channels,hparams.out_channels,hparams.kernel_size)\r\n        self.layer_d_2 = nn.ConvTranspose1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)\r\n        \r\n        Train_x_t_e = CarrierDataset()\r\n    \r\n    def forward(self,x):\r\n        x = self.layer_e_1(x)\r\n        x = F.relu(x)\r\n        x = self.layer_e_2(x)\r\n        encoded = F.relu(x)\r\n        x = self.layer_d_1(encoded)\r\n        x = F.relu(x)\r\n        decoded = self.layer_d_2(x)\r\n        return decoded, encoded\r\n    \r\n    \r\n    def training_step(self, train_batch, batch_idx):\r\n        x, _ = train_batch\r\n        decoded, encoded = self.forward(x)\r\n        mse = nn.MSELoss()\r\n        loss = mse(x, decoded)\r\n        return {'loss': loss}\r\n    \r\n    def validation_step(self,val_batch, batch_idx):\r\n        x, _ = val_batch\r\n        decoded, encoded = self.forward(x)\r\n        mse = nn.MSELoss()\r\n        loss = mse(x, decoded)\r\n        return {'val_loss': loss}\r\n    \r\n    def train_dataloader(self):\r\n        loader = torch.utils.data.DataLoader(\r\n            dataset=Train_x_t_e,\r\n            batch_size=self.hparams.batch_size,\r\n            shuffle=True,\r\n            pin_memory=True)\r\n        return loader\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(dataset=Train_x_t_e, batch_size=self.hparams.batch_size)\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\r\n        return optimizer\r\n\r\nclass CarrierDataset(data.Dataset):\r\n    \r\n    def __init__(self):\r\n        Train_x_t_e1 = torch.load('Tensor_100K_1.pt')\r\n        Train_x_t_e2 = torch.load('Tensor_100K_2.pt')\r\n        ag_t = torch.cat((Train_x_t_e1,Train_x_t_e2))\r\n        #df = read_csv(ds)\r\n        self.len = ag_t.shape[0]\r\n        self.ag_t = ag_t\r\n        self.size = sys.getsizeof(ag_t)\r\n        \r\n    def __getitem__(self,index):\r\n        return self.ag_t[index], self.ag_t[index]\r\n    \r\n    def __len__(self):\r\n        return self.len\r\n    \r\n    def __size__(self):\r\n        return self.size    \r\n```\r\nGot the following error:\r\n```\r\nRuntimeErrorTraceback (most recent call last)\r\n<ipython-input-277-b675dec29cfe> in <module>\r\n     16 print(\"Parameters:\")\r\n     17 print(args)\r\n---> 18 main(args)\r\n\r\n<ipython-input-276-23c135174ba6> in main(hparams)\r\n     19     )\r\n     20 \r\n---> 21     trainer.fit(model)\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)\r\n    885             self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)\r\n    886 \r\n--> 887             self.run_pretrain_routine(model)\r\n    888 \r\n    889         # return 1 when finished\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)\r\n   1013 \r\n   1014         # CORE TRAINING LOOP\r\n-> 1015         self.train()\r\n   1016 \r\n   1017     def test(\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in train(self)\r\n    345                 # RUN TNG EPOCH\r\n    346                 # -----------------\r\n--> 347                 self.run_training_epoch()\r\n    348 \r\n    349                 # update LR schedulers\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n    417             # RUN TRAIN STEP\r\n    418             # ---------------\r\n--> 419             _outputs = self.run_training_batch(batch, batch_idx)\r\n    420             batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs\r\n    421 \r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)\r\n    595 \r\n    596                 # calculate loss\r\n--> 597                 loss, batch_output = optimizer_closure()\r\n    598 \r\n    599                 # check if loss or model weights are nan\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure()\r\n    573                     model_ref = self.get_model()\r\n    574                     with self.profiler.profile('model_backward'):\r\n--> 575                         model_ref.backward(self, closure_loss, optimizer, opt_idx)\r\n    576 \r\n    577                     # track metrics for callbacks\r\n\r\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/core/hooks.py in backward(self, trainer, loss, optimizer, optimizer_idx)\r\n    153                     scaled_loss.backward()\r\n    154         else:\r\n--> 155             loss.backward()\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\r\n    196                 products. Defaults to ``False``.\r\n    197         \"\"\"\r\n--> 198         torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n    199 \r\n    200     def register_hook(self, hook):\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     98     Variable._execution_engine.run_backward(\r\n     99         tensors, grad_tensors, retain_graph, create_graph,\r\n--> 100         allow_unreachable=True)  # allow_unreachable flag\r\n    101 \r\n    102 \r\n\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2331/comments",
    "author": "soulhi-vz",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-06-23T17:28:50Z",
        "body": "Try: `loss = mse(decoded, x)`."
      },
      {
        "user": "soulhi-vz",
        "created_at": "2020-06-23T17:36:49Z",
        "body": "with \"loss = mse(decoded, x)\"\r\nsame issue: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-23T17:47:51Z",
        "body": "Can you print `decoded.requires_grad` after `self.forward(x)` in training_step? See whether it's `True` or `False`."
      },
      {
        "user": "soulhi-vz",
        "created_at": "2020-06-23T17:51:56Z",
        "body": "Hi Rohit,\r\nGot:\r\ndecoded.requires_grad:\r\nFalse"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-23T18:01:04Z",
        "body": "That means no problem in the loss.backward() as of now. Can you check the same after `decoded = self.layer_d_2(x)`? Or share a colab notebook?"
      },
      {
        "user": "soulhi-vz",
        "created_at": "2020-06-23T18:07:37Z",
        "body": "decoded.requires_grad after layer_d_2:\r\nFalse\r\ndecoded.requires_grad:\r\nFalse"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-23T19:38:16Z",
        "body": "`decoded.requires_grad` should be `True`. Not sure why it's happening. Mind share a colab notebook??"
      },
      {
        "user": "soulhi-vz",
        "created_at": "2020-06-23T20:32:06Z",
        "body": "Hi Rohit,\r\nI am not using colab. Here the rest of the code:\r\n\r\ndef main(hparams) -> None:\r\n    # initialize the DQNLightning\r\n    model = Autoencoder(hparams)\r\n    print(\"Model\")\r\n    print(model)\r\n    print(\"hparam\")\r\n    print(hparams)\r\n    #print(\"params list\")\r\n    #for parameter in model.parameters():\r\n    #    print(parameter)\r\n    trainer = pl.Trainer(\r\n        fast_dev_run = True\r\n        #gpus=10,\r\n        #distributed_backend='dp',\r\n        #max_epochs=500,\r\n        #early_stop_callback=False,\r\n        #val_check_interval=100,\r\n        #show_progress_bar=False\r\n    )\r\n\r\n    trainer.fit(model)\r\n\r\ntorch.manual_seed(0)\r\nnp.random.seed(0)\r\n# Pass hyper parameters to the model through the arg parser\r\n# Create parser\r\n# argeparse used to write freindly command-line interface\r\nparser = argparse.ArgumentParser()\r\n# Add arguments\r\nparser.add_argument(\"--batch_size\", type=int, default=1024, help=\"size of the batches\")\r\nparser.add_argument(\"--lr\", type=float, default=1e-2, help=\"learning rate\")\r\nparser.add_argument(\"--in_channels\", type=int, default=17, help=\"in channels\")\r\nparser.add_argument(\"--out_channels\", type=int, default=100, help=\"out channels\")\r\nparser.add_argument(\"--kernel_size\", type=int, default=3, help=\"kernel size\")\r\n\r\n# Parse arguments\r\nargs, _ = parser.parse_known_args()\r\nprint(\"Parameters:\")\r\nprint(args)\r\nmain(args)\r\n\r\n"
      },
      {
        "user": "soulhi-vz",
        "created_at": "2020-06-23T21:08:59Z",
        "body": "Hi Rohit,\r\nAfter restarting the kernel, the issue went away. Thanks for your help. Will close the issue now.\r\n/Said"
      },
      {
        "user": "JonnyD1117",
        "created_at": "2020-11-13T05:40:36Z",
        "body": "I have a similar problem connected to using pl.metrics.functional package for the 'dice_score'. Model was previously training fine with BCELoss, but when I switched to dice_score I get this same error. \r\n\r\nDoes this mean that PL version of the dice_score is not capable of back-prop and can only be used as a fitness measure and not a loss function?"
      }
    ]
  },
  {
    "number": 2325,
    "title": "Using `overfit_batches` with multiple expected validation dataloaders can cause problems",
    "created_at": "2020-06-23T00:33:23Z",
    "closed_at": "2020-08-31T01:38:37Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2325",
    "body": "As the title says, my `validation_epoch_end` expects outputs from 2 dataloaders, but when using overfit_batches it only receives 1, this causes my code to crash.\r\nThe most simple solution I can think of is to include the `number_of_validation_dataloaders` in the `validation_epoch_end` method to handle more easily this situation.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2325/comments",
    "author": "sebamenabar",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-23T00:34:02Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-22T00:41:07Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2282,
    "title": "optimizer got an empty parameter list",
    "created_at": "2020-06-19T20:39:51Z",
    "closed_at": "2020-06-20T23:04:53Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2282",
    "body": " Hi,\r\nGot the following error:\r\nValueError: optimizer got an empty parameter list with both options below:\r\n\r\ndef configure_optimizers(self):\r\n        # option1 optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\r\n        # option 2\r\n        optimizer = torch.optim.Adam(params = list(self.parameters()), lr=self.hparams.lr)\r\n        return optimizer\r\n\r\nclass Autoencoder(pl.LightningModule):\r\n    \r\n    def __init__(self, hparams: argparse.Namespace):\r\n        super(Autoencoder,self).__init__() \r\n        self.hparams = hparams\r\n           \r\n        self_layer_e_1 = nn.Conv1d(hparams.in_channels, hparams.out_channels, hparams.kernel_size)\r\n        self_layer_e_2 = nn.Conv1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)\r\n        self_layer_d_1 = nn.ConvTranspose1d(hparams.in_channels,hparams.out_channels,hparams.kernel_size)\r\n        self_layer_d_2 = nn.ConvTranspose1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)\r\n        \r\n    \r\n    def forward(self,x):\r\n        x = self_layer_e_1(x)\r\n        x = nn.ReLu(x)\r\n        x = self_layer_e_2(x)\r\n        encoded = nn.ReLU(x)\r\n        x = self_layer_d_1(encoded)\r\n        x = nn.ReLU(x)\r\n        decoded = self_layer_d_2(x)\r\n        decoded = self.decoder(encoded)\r\n        return self.decoded, self.encoded\r\n    \r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        x, _ = batch\r\n        decoded, encoded = self.forward(x)\r\n        loss = MSE(x, decoded)\r\n        return loss\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        return self._shared_eval(batch, batch_idx, 'val')\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        return self._shared_eval(batch, batch_idx, 'test')\r\n        \r\n    def _shared_eval(self, batch, batch_idx, prefix):\r\n        x, y = batch\r\n        decoded, encoded = self.forward(x)\r\n        loss = F.nll_loss(x, decoded)\r\n        return {f'{prefix}_loss': loss}\r\n    \r\n    def train_dataloader(self):\r\n        return DataLoader(self.CarrierDataset, batch_size=self.hparams.batch_size)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.CarrierDataset, batch_size=hparams.batch_size)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self,CarrierDataset, batch_size=hparams.batch_size)\r\n\r\n    def configure_optimizers(self):\r\n        #optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\r\n        optimizer = torch.optim.Adam(params = list(self.parameters()), lr=self.hparams.lr)\r\n        return optimizer",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2282/comments",
    "author": "soulhi-vz",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-19T20:40:31Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "versatran01",
        "created_at": "2020-06-19T21:56:35Z",
        "body": "You need to use `self.xxx = nn.Conv2d(a,b,c)`  instead of `self_xxx = nn.Conv2d(a,b,c)`  for `nn.Module` to register them as parameters, otherwise your module has no paramters, thuse the optimizer gets nothing."
      },
      {
        "user": "soulhi-vz",
        "created_at": "2020-06-20T23:04:53Z",
        "body": "It works now. Thanks for the catch !!!!"
      }
    ]
  },
  {
    "number": 2266,
    "title": "Improve Exception Handling",
    "created_at": "2020-06-19T08:25:06Z",
    "closed_at": "2020-12-04T09:26:11Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "let's do it!",
      "priority: 2"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2266",
    "body": "## 🚀 Code Quality Improvement\r\n\r\nI came across this a few times already:\r\n```python\r\ntry:\r\n   # something\r\nexcept Exception:\r\n   # something\r\n```\r\nIt is the worst possible way to handle Exceptions. It is better to catch the specific exception or at least log a message. \r\n\r\n### Alternatives\r\n\r\nNone. Sooner or later someone has to deal with this anyway :)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2266/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-06-19T08:35:08Z",
        "body": "@awaelchli so what do you propose to have more exact Exceptions like `ImportError`?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-19T08:45:21Z",
        "body": "Yes exactly, the specific exception we try to catch. \r\nAlso, sometimes we don't need exception handling at all:\r\n```python\r\ntry: \r\n    x = myobject.attribute\r\nexcept Exception\r\n    x = 0\r\n```\r\ncan be replaced with\r\n`x = getattr(myobject, \"attribute\", 0)`"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-08T00:57:50Z",
        "body": "Another code smell we frequently see (in many places in the code):\r\n\r\n```python\r\ntry:\r\n    from some_package import some_module\r\nexcept ImportError:\r\n    SOMEPACKAGE_AVAILABLE = False\r\nelse:\r\n    SOMEPACKAGE_AVAILABLE = True\r\n```\r\n**Better:**\r\n\r\ncatch `ModuleNotFoundError` instead\r\n\r\n**Even better (and shorter):**\r\n```python\r\nimport importlib\r\nSOMEPACKAGE_AVAILABLE = importlib.util.find_spec(\"some_package\") is not None\r\n```"
      },
      {
        "user": "Borda",
        "created_at": "2020-08-08T06:16:36Z",
        "body": "About the import error, I remember that thare is something twisted about python version and avalanche exceptions, I guess that py3.6 doesn't have the one suggest (need to check)\r\n\r\nAlso is there a simple way how to import jist a function from a package that you are not sure if it is installed? "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-09T08:05:47Z",
        "body": "> Also is there a simple way how to import jist a function from a package that you are not sure if it is installed?\r\n\r\n\r\nI think the simplest is this:\r\n```python\r\nif importlib.util.find_spec(\"some_package\") is not None:\r\n    from some_package import my_function\r\n\r\n```\r\nbecause when we import the function in Python it always has to import the whole package anyway."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-25T00:48:05Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2254,
    "title": "Single node DDP: \"Default process group is not initialized\"",
    "created_at": "2020-06-19T02:37:22Z",
    "closed_at": "2020-07-10T01:20:18Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2254",
    "body": "## 🐛 Bug\r\nUnable to start single node ddp training on 0.8.0\r\n\r\n### To Reproduce\r\n~~was going to run the gpu_template but... #2235~~\r\nboth methods of running the template result in the same error\r\n```\r\n$ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp_spawn\r\n$ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp\r\n```\r\n```\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\r\n    main(hyperparams)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\r\n    trainer.fit(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 860, in fit\r\n    self.barrier('fit_prepare_data')\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1261, in barrier\r\n    torch_distrib.barrier()\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 1484, in barrier\r\n    _check_default_pg()\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 187, in _check_default_pg\r\n    \"Default process group is not initialized\"\r\nAssertionError: Default process group is not initialized\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2254/comments",
    "author": "s-rog",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T02:47:50Z",
        "body": "can you post code to reproduce? just a minimal example that breaks\r\n\r\nBTW, the GPU template is fixed..."
      },
      {
        "user": "s-rog",
        "created_at": "2020-06-19T02:50:00Z",
        "body": "done, let me post my env as well"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T02:50:36Z",
        "body": "ok wait... i think i see it. one sec"
      },
      {
        "user": "s-rog",
        "created_at": "2020-06-19T04:50:07Z",
        "body": "I just tested the merged changes with both ddp and ddp_spawn again got this:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\r\n    main(hyperparams)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\r\n    trainer.fit(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 891, in fit\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    self.ddp_train(task, model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 479, in ddp_train\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\r\n    main(hyperparams)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\r\n    trainer.fit(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 907, in fit\r\n    self.setup()\r\nTypeError: setup() missing 1 required positional argument: 'stage'\r\n    self.spawn_ddp_children(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 441, in spawn_ddp_children\r\n    self.ddp_train(local_rank, model, is_master=True)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 479, in ddp_train\r\n    self.setup()\r\nTypeError: setup() missing 1 required positional argument: 'stage'\r\n```"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T05:14:30Z",
        "body": "try again. that was a typo"
      },
      {
        "user": "s-rog",
        "created_at": "2020-06-19T05:47:52Z",
        "body": "cheers, works now!"
      },
      {
        "user": "armancohan",
        "created_at": "2020-06-23T05:35:19Z",
        "body": "Still having the `Default process group is not initialized` issue when using trainer.test \r\n"
      },
      {
        "user": "wukailu",
        "created_at": "2020-06-23T06:30:56Z",
        "body": "> Still having the `Default process group is not initialized` issue when using trainer.test\r\n\r\nI still have this bug as well. One temporary solution is creating a new single GPU trainer to do the test.\r\n\r\nLike\r\n\r\n```\r\ntrainer = Trainer(gpus=1, deterministic=True, logger=logger)\r\ntrainer.model = model\r\ntrainer.test()\r\n```"
      },
      {
        "user": "armancohan",
        "created_at": "2020-06-23T19:57:28Z",
        "body": "Right, I know it works on single gpu. I have a large test set and ideally want faster inference using multiple gpus."
      },
      {
        "user": "zackcarson",
        "created_at": "2020-07-02T15:11:23Z",
        "body": "Can we re-open this issue? I am still having the `Default process group is not initialized` issue when I hit `trainer.test()` with ddp (with any number of gpus, even 1). I'm using the latest release from yesterday."
      },
      {
        "user": "armancohan",
        "created_at": "2020-07-02T15:33:13Z",
        "body": "+1, doesn't look like the issue is resolved yet."
      },
      {
        "user": "jxchen01",
        "created_at": "2020-07-04T05:32:04Z",
        "body": "having the same problem..... I also tried to downgrade pl to an older version, like 0.7.5, and try to using the older version to do the inference. But, the model trained and saved using the 0.8.x seems to not directly be compatible with older version.  "
      },
      {
        "user": "channingxiao",
        "created_at": "2020-07-09T12:11:00Z",
        "body": "version: 0.8.4  train with ddp,  Got \"Default process group is not initialized\" when run trainer.test()"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-09T12:18:32Z",
        "body": "could you try master? this is fixed there"
      },
      {
        "user": "zackcarson",
        "created_at": "2020-07-09T19:06:49Z",
        "body": "Just tried it, it works fine now! Thank you!\r\n"
      },
      {
        "user": "jxchen01",
        "created_at": "2020-08-17T19:13:27Z",
        "body": "@williamFalcon Trying 0.8.5\r\n\r\nTrained with ddp, and testing with ddp, but got the following error message:\r\n\r\n```\r\nAssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.\r\n```\r\n\r\nAny idea?\r\n\r\nThanks!"
      }
    ]
  },
  {
    "number": 2243,
    "title": "Model.load_from_checkpoint tries to open file path as URL and fail",
    "created_at": "2020-06-18T19:43:30Z",
    "closed_at": "2020-06-18T21:53:52Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2243",
    "body": "## 🐛 Bug\r\n load_from_checkpoint tries to classify if the input is a file path or an URL and detects the hard drive letter as a scheme and then classify wrongly the input because of this.\r\nurllib.error.URLError: \\<urlopen error unknown url type: d\\>\r\n\r\nMy input:\r\nD:\\Prog\\Projects\\AceriNet\\research_seed\\checkpoints\\acerinet\\bnacerinet0_target=OVA_OK_penalized=None_loss_fn=ce_normalized=True_balanced=FalseFalse_seed=42_val_loss=0.374_val_auroc=0.9041_v0.ckpt\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nUse any path with a hard drive letter at the start (windows formatting) for pl.LightningModule().load_from_checkpoint(path)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2243/comments",
    "author": "Molaire",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-18T19:44:15Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "yukw777",
        "created_at": "2020-06-18T19:47:46Z",
        "body": "Ah I didn’t think about Windows when I implemented this. I’ll take a look if there’s a better way."
      },
      {
        "user": "Molaire",
        "created_at": "2020-06-18T19:53:18Z",
        "body": "We could try to detect if the file exists with if os.path.isfile() or Path(path_of_chkpt).is_file() instead of checking for scheme"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-18T21:10:25Z",
        "body": "Also came across this when I was running tests on windows. I wonder why it was not showing up in CI."
      },
      {
        "user": "Molaire",
        "created_at": "2020-06-18T21:15:34Z",
        "body": "The only reason I see is that tmpdir is a relative path without prefix at the start, dodging the problem. It should be fixed by my PR, I hope."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-18T22:04:56Z",
        "body": "Thanks @Molaire. I can verify the tests now pass locally on windows."
      },
      {
        "user": "airium",
        "created_at": "2020-06-19T23:46:53Z",
        "body": "I am afraid the PR #2244 introduced a new bug, as now on Windows `Path(path_or_url).is_file()` will block every URL by raising `OSError`. I will submit a new PR to better fix it."
      }
    ]
  },
  {
    "number": 2240,
    "title": "Minibatch oversampling",
    "created_at": "2020-06-18T17:03:49Z",
    "closed_at": "2020-08-26T19:56:27Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2240",
    "body": "## 🚀 Feature\r\noption for over-sampling mini batches for imbalanced classes\r\n\r\n### Motivation\r\n\r\nmany datasets have large class imbalances and in lightning this needs to be implemented manually\r\n\r\n### Pitch\r\nMake an option in trainer function which would allow someone to change the class balances sampled for the mini batch.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2240/comments",
    "author": "seandatasci",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-18T17:04:27Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-17T19:00:01Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2238,
    "title": "when no checkpoints are saved, test fails",
    "created_at": "2020-06-18T15:02:14Z",
    "closed_at": "2020-07-10T01:20:47Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2238",
    "body": "Build a model that doesn't save a checkpoint and this crashes.\r\nIt should use the last model instead.\r\n```\r\nmodel = ...\r\ntrainer = Trainer(fast_dev_run)\r\ntrainer.fit(model)\r\ntrainer.test()\r\n```\r\n\r\n@yukw777 \r\n```\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <torch.serialization._open_file object at 0x7f443d4d88d0>, name = ''\r\nmode = 'rb'\r\n\r\n    def __init__(self, name, mode):\r\n>       super(_open_file, self).__init__(open(name, mode))\r\nE       FileNotFoundError: [Errno 2] No such file or directory: ''\r\n```\r\n\r\n\r\nThe test for this should be:\r\n\r\n```\r\ndef test_no_ckpt_test(tmpdir):\r\n    model = EvaluationModel()\r\n    trainer = Trainer(fast_dev_run)\r\n    trainer.fit(model)\r\n    trainer.test()\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2238/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "yukw777",
        "created_at": "2020-06-18T16:43:31Z",
        "body": "oops, i'll send out a fix soon!"
      },
      {
        "user": "nischal-sanil",
        "created_at": "2020-06-24T17:03:01Z",
        "body": "Hi,\r\n\r\nI am currently facing this issue, is there any work around that I could do to resolve this?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-24T18:24:33Z",
        "body": "```    trainer.test(model)```"
      },
      {
        "user": "yukw777",
        "created_at": "2020-06-24T18:40:40Z",
        "body": "@nischal-sanil could you help me understand the issue better by explaining how you don’t have any models saved?"
      },
      {
        "user": "nischal-sanil",
        "created_at": "2020-06-25T05:39:15Z",
        "body": "I was running the Trainer with `fast_dev_run` set to True, hence there were no checkpoints. Changing  `trainer.test()` to `trainer.test(model)` has resolved the issue.\r\nThanks,"
      },
      {
        "user": "nischal-sanil",
        "created_at": "2020-06-26T01:17:51Z",
        "body": "Hi again,\r\n\r\nConsider the following code snippet\r\n\r\n```\r\nclass Net(LightningModule):\r\n    ...\r\n\r\nresnet18 = models.resnet18(pretrained=True)\r\nmodel = Net(resnet18)\r\n\r\n# parameters before training\r\nold_params = model.parameters()\r\n\r\ntrainer = Trainer(max_steps=10) \r\ntrainer.fit(model)  \r\n\r\n# parameters after training \r\nnew_params = model.parameters()\r\n```\r\n\r\nWhere I store the parameters of the model before and after training in `old_params` and `new_params`, a check on their equality returns `True`, suggesting there're no changes to the parameters.\r\n\r\n```\r\ndef check_params(old,new):\r\n    return all([torch.equal(o,n) for o,n in zip(old,new)])\r\n\r\ncheck_params(old_params,new_params)\r\n## True\r\n```\r\n Therefore, while running `trainer.test(model)` the trained model is not used for testing. I had assumed that the `Trainer` would have updated the parameters to the `model`, but from the above snippet that does not seem to be the case.  And I am not able to figure out if this is an expected behavior of the `model` or there is a problem with my class definition. If this is the expected behavior of the `model`, then are there any work around for calling `trainer.test()` without using a checkpoint because in my case loading from a checkpoint results in an error similar to this issue #2359.\r\n"
      },
      {
        "user": "yukw777",
        "created_at": "2020-06-29T18:52:09Z",
        "body": "@williamFalcon is it true that if `fast_dev_run` is `True`, there is no checkpoints saved? I just tried to manually check this in `test_test_checkpoint_path`, but `checkpoint_callback` of the trainer still had a valid `best_model_path` even when `fast_dev_run` is `True`.\r\n\r\n@nischal-sanil your code comparing the old parameters to the new parameters is not quite correct. Since the tensors returned by `model.parameters()` are \"references\" to the underlying data, both `old_params` and `new_params` would point to the same \"trained\" tensors, hence `check_params()` would return `True`. You'd have to do something like `old_params = [p.clone() for p in model.parameters()]` to get around that.\r\n\r\n"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-29T18:59:52Z",
        "body": "fast_dev_run should not save checkpoints (nor write a logs file). it's like a \"compiler\""
      },
      {
        "user": "yukw777",
        "created_at": "2020-06-29T19:55:15Z",
        "body": "@williamFalcon hmm maybe there's a bug? I see this:\r\n\r\n```\r\n❯ python pl_examples/basic_examples/cpu_template.py --fast_dev_run True\r\n/Users/peteryu/.pyenv/versions/pl/lib/python3.7/site-packages/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\r\n  assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'\r\nRunning in fast_dev_run mode: will run a full train, val and test loop using a single batch\r\nGPU available: False, used: False\r\nTPU available: False, using: 0 TPU cores\r\n\r\n  | Name      | Type        | Params | In sizes   | Out sizes \r\n--------------------------------------------------------------------\r\n0 | c_d1      | Linear      | 39 M   | [2, 784]   | [2, 50000]\r\n1 | c_d1_bn   | BatchNorm1d | 100 K  | [2, 50000] | [2, 50000]\r\n2 | c_d1_drop | Dropout     | 0      | [2, 50000] | [2, 50000]\r\n3 | c_d2      | Linear      | 500 K  | [2, 50000] | [2, 10]   \r\nEpoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.18it/s, loss=2.523, v_num=0]\r\n❯ ls lightning_logs/version_0/checkpoints/epoch=0.ckpt \r\nlightning_logs/version_0/checkpoints/epoch=0.ckpt\r\n```"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-10T01:20:47Z",
        "body": "added a warning and return {}"
      }
    ]
  },
  {
    "number": 2235,
    "title": "CPU/GPU Template",
    "created_at": "2020-06-18T13:32:11Z",
    "closed_at": "2020-06-19T02:43:24Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2235",
    "body": "\r\n\r\n## 🐛 Bug\r\n\r\nThe GPU or CPU template do not run currently on master after changes including the setup hook. \r\n\r\n```\r\npython -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp\r\npython -m pl_examples.basic_examples.cpu_template\r\n```\r\n\r\nCPU Template Error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/anthony/Downloads/pytorch-lightning/pl_examples/basic_examples/cpu_template.py\", line 53, in <module>\r\n    main(args)\r\n  File \"/home/anthony/Downloads/pytorch-lightning/pl_examples/basic_examples/cpu_template.py\", line 34, in main\r\n    trainer.fit(model)\r\n  File \"/home/anthony/Downloads/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 952, in fit\r\n    self.run_pretrain_routine(model)\r\n  File \"/home/anthony/Downloads/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 1063, in run_pretrain_routine\r\n    self.reset_val_dataloader(ref_model)\r\n  File \"/home/anthony/Downloads/pytorch-lightning/pytorch_lightning/trainer/data_loading.py\", line 331, in reset_val_dataloader\r\n    self._reset_eval_dataloader(model, 'val')\r\n  File \"/home/anthony/Downloads/pytorch-lightning/pytorch_lightning/trainer/data_loading.py\", line 253, in _reset_eval_dataloader\r\n    dataloaders = self.request_dataloader(getattr(model, f'{mode}_dataloader'))\r\n  File \"/home/anthony/Downloads/pytorch-lightning/pytorch_lightning/trainer/data_loading.py\", line 352, in request_dataloader\r\n    dataloader = dataloader_fx()\r\n  File \"/home/anthony/Downloads/pytorch-lightning/pl_examples/models/lightning_template.py\", line 158, in val_dataloader\r\n    return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)\r\n  File \"/home/anthony/.cache/pypoetry/virtualenvs/robotics-zp-60jGk-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 594, in __getattr__\r\n    type(self).__name__, name))\r\nAttributeError: 'LightningTemplateModel' object has no attribute 'mnist_test'\r\n```\r\nGPU Template Error:\r\n```\r\n  File \"/home/anthony/Downloads/pytorch-lightning/pl_examples/models/lightning_template.py\", line 64, in __init__\r\n    self.c_d1_drop = nn.Dropout(self.drop_prob)\r\n  File \"/home/anthony/.cache/pypoetry/virtualenvs/robotics-zp-60jGk-py3.6/lib/python3.6/site-packages/torch/nn/modules/dropout.py\", line 10, in __init__\r\n    if p < 0 or p > 1:\r\nTypeError: '<' not supported between instances of 'Namespace' and 'int'\r\n```\r\n\r\n### Environment\r\n\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.18.4\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.5.0\r\n        - pytorch-lightning: 0.8.0\r\n        - tensorboard:       2.2.1\r\n        - tqdm:              4.46.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.6.8\r\n        - version:           #44~18.04.2-Ubuntu SMP Thu Apr 23 14:27:18 UTC 2020\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2235/comments",
    "author": "anthonytec2",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-18T15:00:45Z",
        "body": "try again?"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-18T23:22:25Z",
        "body": "> try again?\r\n\r\nit is in master now... :("
      }
    ]
  },
  {
    "number": 2208,
    "title": "Non-trainable non-differentiable baselines",
    "created_at": "2020-06-16T13:46:55Z",
    "closed_at": "2020-08-24T17:28:23Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2208",
    "body": "## 🚀 Feature\r\nAdd the possibility to implement non-trainable and non-differentiable models.\r\n\r\n### Motivation\r\nIt is often useful to compare models with simple baselines, those baselines often do not require parameters to optimize and are not differentiable at all. \r\n\r\nHowever, at the moment it is not possible to return `None` from `training_step` and `configure_optimizers`.\r\nWriting the baselines in the lightning style-guide is useful since it allows to define a parent model that defines how to compute the metrics for all the models.\r\n\r\n\r\n### Pitch\r\nOne common baseline is to _find the most similar sample in the train set and output the associated `y`_. In this way, you can understand if the model is better than simply memorizing the train set.\r\n\r\nWhen comparing models for the same task, it is useful to write the validation step and metrics computations in a parent model and then extend it. At the moment this type baseline is difficult to implement in lightning since it does not support non-trainable models. \r\n\r\n### Alternatives\r\nDefine fake parameters, an optimizer to optimize that parameters and a loss to trick the framework into running and computing the validation steps.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2208/comments",
    "author": "lucmos",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-16T13:47:35Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-15T13:52:06Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2199,
    "title": "Batch weight for accumulating gradients",
    "created_at": "2020-06-15T21:22:59Z",
    "closed_at": "2020-08-23T23:29:02Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2199",
    "body": "## 🚀 Feature\r\nSupport different weights for each batch during batch gradient accumulation. \r\n\r\n### Motivation\r\n\r\nIn many cases not every batch are equal. For example, when the inputs are sequences of tokens of variable length, the mean loss of several batches is not the average across the individual batch losses, since each batch would have varying number of samples. \r\n\r\n### Pitch\r\n\r\nIn training_loop.py, add a 'batch_total_weight' attribute initialized as zero. When fetching returns from training_step, if a certain 'batch_weight' key is in the returning dictionary, the batch's weight would be multiplied by 'batch_weight' during backward step and 'batch_weight' will be added to 'batch_total_weight'.  Before optimization step, the gradients are scaled by 'batch_total_weight' and the latter would be set to zero for next accumulating steps. \r\n\r\nA typical scenario is when 'batch_weight' is the number of samples within each batch, so that the average across multiple batches are computed correctly. \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2199/comments",
    "author": "rhythmswing",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-15T21:23:41Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-14T23:21:40Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2188,
    "title": "[hparams] save_hyperparameters doesn't save kwargs",
    "created_at": "2020-06-15T03:10:15Z",
    "closed_at": "2020-08-03T18:44:12Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2188",
    "body": "## ❓ Questions and Help\r\n\r\nwhen I use hyperparemeters like docs:\r\n```\r\nclass LitMNIST(LightningModule):\r\n\r\n    def __init__(self, layer_1_dim=128, learning_rate=1e-2, **kwargs):\r\n        super().__init__()\r\n        # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint\r\n        self.save_hyperparameters()\r\n\r\n```\r\nmodel checkpoint doesn't save args in kwargs. But kwargs is important. Args such as `num_frames, img_size, img_std ...` must be used in creating `dataloader`, but it will be tedious if writes them in `__init__` explicitly .  it can make code clean if hides them in `kwargs`. \r\n\r\nBefore I use `hparams`, it's ok. But now it's not recommended to use `hparams`,  is there any good idea to  deal with this problem?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2188/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "xiadingZ",
        "created_at": "2020-06-15T03:24:08Z",
        "body": "if don't use `hparams`, it will put all args of `model, dataset, dataloader...` in a `LightnModule`' s `__init__` method, and `save_hyperparameters ` doesn't save args in `kwargs`. It this really a good idea?"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-15T17:21:33Z",
        "body": "Could you please share your model example?"
      },
      {
        "user": "xiadingZ",
        "created_at": "2020-06-16T02:50:51Z",
        "body": "> Could you please share your model example?\r\n\r\n```\r\nclass LitMNIST(LightningModule):\r\n\r\n    def __init__(self, layer_1_dim=128, learning_rate=1e-2, **kwargs):\r\n        super().__init__()\r\n        # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint\r\n        self.save_hyperparameters()\r\n        self.kwargs = kwargs\r\n        ...\r\n    \r\n    def train_dataloader(self):\r\n        img_size = self.kwargs['img_size']\r\n        ...\r\n```\r\n\r\nI can train this model, but when I load from checkpoint, it says kwargs hasn't `img_size`"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-16T14:34:35Z",
        "body": "> I can train this model, but when I load from checkpoint, it says kwargs hasn't `img_size`\r\n\r\nI see, we need to ignore `kwargs` from the model hparams saving...\r\n@xiadingZ mind adding PR with a test for this case and I ll finish it with a patch?"
      },
      {
        "user": "s-rog",
        "created_at": "2020-06-19T00:48:13Z",
        "body": "```\r\nTraceback (most recent call last):\r\n  File \"./training.py\", line 101, in <module>\r\n    main(hparam_trial)\r\n  File \"./training.py\", line 86, in main\r\n    model = module(hparams, fold_train, fold_val, data_dir+img_dir)\r\n  File \"../main/module.py\", line 18, in __init__\r\n    self.hparams = hparams\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 638, in __setattr__\r\n    object.__setattr__(self, name, value)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\", line 1695, in hparams\r\n    self.save_hyperparameters(hp, frame=inspect.currentframe().f_back.f_back)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\", line 1662, in save_hyperparameters\r\n    cand_names = [k for k, v in init_args.items() if v == hp]\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\", line 1662, in <listcomp>\r\n    cand_names = [k for k, v in init_args.items() if v == hp]\r\n  File \"/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\", line 1479, in __nonzero__\r\n    f\"The truth value of a {type(self).__name__} is ambiguous. \"\r\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\r\n```\r\n\r\nI'm guessing this is why 0.8.0 causes this error, it's trying to save all args (including dataframes in my case) outside of hparams?\r\n\r\nEdit: #2250 "
      }
    ]
  },
  {
    "number": 2132,
    "title": "ddp: fp16 misaligned address",
    "created_at": "2020-06-09T15:04:49Z",
    "closed_at": "2020-10-29T06:10:43Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2132",
    "body": "Versions:\r\n```\r\ntorch==1.5\r\npytorch-lightning==0.8.0rc1\r\ncuda=10.1\r\n```\r\nIn both O1 and O2 mode and the new ddp `trainer.fit` fails with the following traceback. This does not happen with `ddp_spawn`.\r\n\r\nfp16 O2 mode:\r\n```bash\r\n  File \"finetune.py\", line 791, in <module>\r\n    main(args)\r\n  File \"finetune.py\", line 723, in main\r\n    trainer: pl.Trainer = generic_train(model, args, early_stopping_callback=True)\r\n  File \"/home/shleifer/transformers_fork/examples/lightning_base.py\", line 428, in generic_train\r\n    trainer.fit(model)\r\n  File \"/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 890, in fit\r\n    self.spawn_ddp_children(model)\r\n  File \"/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/distrib_data_parallel.py\", line 396, in spawn_ddp_children\r\n    self.ddp_train(local_rank, model, is_master=True)\r\n  File \"/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/distrib_data_parallel.py\", line 472, in ddp_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 1050, in run_pretrain_routine\r\n    self.train()\r\n  File \"/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 363, in train\r\n    self.run_training_epoch()\r\n  File \"/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 445, in run_training_epoch\r\n    _outputs = self.run_training_batch(batch, batch_idx)\r\n  File \"/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 621, in run_training_batch\r\n    loss, batch_output = optimizer_closure()\r\n  File \"/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 599, in optimizer_closure\r\n    model_ref.backward(self, closure_loss, optimizer, opt_idx)\r\n  File \"/home/shleifer/pytorch-lightning/pytorch_lightning/core/hooks.py\", line 155, in backward\r\n    scaled_loss.backward()\r\n  File \"/home/shleifer/.conda/envs/nb/lib/python3.7/contextlib.py\", line 119, in __exit__\r\n    next(self.gen)\r\n  File \"/home/shleifer/.conda/envs/nb/lib/python3.7/site-packages/apex/amp/handle.py\", line 123, in scale_loss\r\n    optimizer._post_amp_backward(loss_scaler)\r\n  File \"/home/shleifer/.conda/envs/nb/lib/python3.7/site-packages/apex/amp/_process_optimizer.py\", line 190, in post_backward_with_master_weights\r\n    models_are_masters=False)\r\n  File \"/home/shleifer/.conda/envs/nb/lib/python3.7/site-packages/apex/amp/scaler.py\", line 119, in unscale\r\n    self.unscale_python(model_grads, master_grads, scale)\r\n  File \"/home/shleifer/.conda/envs/nb/lib/python3.7/site-packages/apex/amp/scaler.py\", line 89, in unscale_python\r\n    self.dynamic)\r\n  File \"/home/shleifer/.conda/envs/nb/lib/python3.7/site-packages/apex/amp/scaler.py\", line 9, in scale_check_overflow_python\r\n    cpu_sum = float(model_grad.float().sum())\r\nRuntimeError: CUDA error: misaligned address\r\n```\r\n\r\nfeel free to cross post if this is a pytorch issue\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2132/comments",
    "author": "sshleifer",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-09T16:35:02Z",
        "body": "@mcarilli i think this error is on Apex though not native amp."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T02:24:18Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2129,
    "title": "Fractional epoch",
    "created_at": "2020-06-09T10:35:50Z",
    "closed_at": "2020-09-03T13:55:37Z",
    "labels": [
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2129",
    "body": "🐛 Bug\r\n\r\nSince upgrading to version 0.7.6, I have the following problem:\r\n\r\nIn my log, I get fractional epochs (like 0.5, 1.5, 2.5, ...) in addition to the expected integer epochs. Is this intentional? Does this mean that on_epoch_end is called twice per epoch?\r\n\r\nIn 0.7.2.dev0, this was not the case.\r\n\r\n### Expected behavior\r\n\r\nPreviously (in 0.7.2.dev0), epoch numbers in the log were integers and this is what I expect.\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce GTX TITAN X\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.18.5\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.5.0\r\n        - pytorch-lightning: 0.7.6\r\n        - tensorboard:       2.1.1\r\n        - tqdm:              4.46.1\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         x86_64\r\n        - python:            3.7.6\r\n        - version:           #132-Ubuntu SMP Tue Jan 9 19:52:39 UTC 2018\r\n\r\n```\r\n\r\n### Additional context\r\n\r\nI also upgraded Pytorch from 1.1.0 to 1.5.0, but this shouldn't affect the epoch counting in Lightning, right?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2129/comments",
    "author": "moi90",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-09T12:05:17Z",
        "body": "Try 0.8.0rc1?"
      },
      {
        "user": "moi90",
        "created_at": "2020-06-20T04:28:58Z",
        "body": "It happens with 0.8.1rc3 as well. After the validation pass, a half epoch is logged:\r\n\r\n```csv\r\ntrain_loss,epoch,step,datetime,val_loss,val_acc,val_f1\r\n0.0012006619945168495,1.0,0,2020-06-20 06:18:46.498589,,,\r\n0.0006965053617022932,1.0,50,2020-06-20 06:19:12.942534,,,\r\n0.00047231907956302166,1.0,100,2020-06-20 06:19:40.548979,,,\r\n0.00031291242339648306,1.0,150,2020-06-20 06:20:07.579803,,,\r\n0.0010144615080207586,1.0,200,2020-06-20 06:20:16.187760,,,\r\n,1.0,205,2020-06-20 06:20:16.349497,8.247037887573242,0.0,0.0\r\n0.000277636427199468,1.5,206,2020-06-20 06:20:49.028577,,,\r\n0.0005236843135207891,2.0,256,2020-06-20 06:21:15.270541,,,\r\n0.0007782027823850513,2.0,306,2020-06-20 06:21:41.330511,,,\r\n0.0003978404856752604,2.0,356,2020-06-20 06:22:07.464661,,,\r\n0.0005978736444376409,2.0,406,2020-06-20 06:22:16.083318,,,\r\n,2.0,411,2020-06-20 06:22:16.186468,8.181181907653809,0.0,0.0\r\n0.0005276629235595465,2.5,412,2020-06-20 06:22:48.220048,,,\r\n...\r\n```"
      },
      {
        "user": "moi90",
        "created_at": "2020-06-26T10:15:41Z",
        "body": "Any idea where this comes from?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-25T10:37:04Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2111,
    "title": "Increase test coverage to 0.98",
    "created_at": "2020-06-08T10:39:35Z",
    "closed_at": "2020-11-30T16:53:49Z",
    "labels": [
      "feature",
      "help wanted",
      "ci",
      "accelerator: tpu"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2111",
    "body": "Before V0.8.0 we want to make sure out test coverage is at least 0.98\r\nRequired steps:\r\n- merge TPU testing #1246 #2094 \r\n- prepare the infrastructure to run TPU tests",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2111/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-06-08T12:15:49Z",
        "body": "not sure to get this done for v0.8, rather v0.9"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-15T15:43:01Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-23T16:26:31Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2093,
    "title": "CI: Add type checking",
    "created_at": "2020-06-05T21:28:16Z",
    "closed_at": "2020-06-12T15:23:19Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "ci"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2093",
    "body": "Related to #855 add type check in CI",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2093/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "InCogNiTo124",
        "created_at": "2020-06-05T22:14:05Z",
        "body": "Hello, how much trouble do you think it would be?"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-10T22:53:40Z",
        "body": "@InCogNiTo124 mind check #2121 and eventually take it over?"
      }
    ]
  },
  {
    "number": 2081,
    "title": "RuntimeError: Address already in use on 'ddp' mode pl 0.8.0",
    "created_at": "2020-06-05T10:58:42Z",
    "closed_at": "2020-06-05T14:33:40Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2081",
    "body": " Trainer configuration:\r\n```\r\n    trainer = pl.Trainer(\r\n        logger= CometLogger( api_key=\"ID\"),\r\n        auto_select_gpus=True,\r\n        gpus=3,\r\n        distributed_backend=\"ddp\",\r\n   )\r\n```\r\nThe error:\r\n```\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2]\r\nCometLogger will be initialized in online mode\r\nCometLogger will be initialized in online mode\r\ninitializing ddp: LOCAL_RANK: 0/2 WORLD_SIZE:3\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 156, in <module>\r\n    main()\r\n  File \"train.py\", line 64, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  File \"train.py\", line 148, in main_train\r\n    trainer.fit(model)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 884, in fit\r\n    self.spawn_ddp_children(model)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 395, in spawn_ddp_children\r\n    self.ddp_train(local_rank, model, is_master=True)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 425, in ddp_train\r\n    model.init_ddp_connection(self.proc_rank, self.world_size, self.is_slurm_managing_tasks)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 962, in init_ddp_connection\r\n    torch_distrib.init_process_group(torch_backend, rank=proc_rank, world_size=world_size)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 393, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 172, in _env_rendezvous_handler\r\n    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\r\nRuntimeError: Address already in use\r\n```\r\nEnv\r\n```\r\n* CUDA:\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.18.4\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.5.0\r\n        - pytorch-lightning: 0.8.0-dev\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.46.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         x86_64\r\n        - python:            3.7.7\r\n        - version:           #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2081/comments",
    "author": "dvirginz",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-05T11:09:11Z",
        "body": "check ps -elf | grep python. maybe a previous run is occupying that port.  "
      },
      {
        "user": "dvirginz",
        "created_at": "2020-06-05T14:33:36Z",
        "body": "I'm not sure, because `pkill -f train.py` fixed it, but it makes sense.\r\nI will update if it will happen again, closing.\r\n\r\nthanks for the framework, and the responsiveness!"
      },
      {
        "user": "ZhaofengWu",
        "created_at": "2020-06-10T17:11:04Z",
        "body": "Does this mean I can't launch multiple DDP jobs on the same node with many GPUs because the port will conflict? (e.g. 2 independent DDP jobs requiring 4 GPU each in an 8 GPU machine)\r\n\r\nEDIT: it seems that I can set `MASTER_PORT` env var to avoid this issue, correct? If so it'd be nice if lightning can detect this and use a new port automatically :)"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-10T19:36:16Z",
        "body": "you can... just the the MASTER_PORT env var.\r\n\r\nLightning does in fact pick a random port... but you set the seed, so it's always the same haha. We need to disable the seed for the port choosing or continue trying ports if the ports are taken.\r\n\r\nThis would be a great PR!"
      },
      {
        "user": "ZhaofengWu",
        "created_at": "2020-06-10T20:35:49Z",
        "body": "@williamFalcon PR submitted at #2140"
      },
      {
        "user": "ShanakaRG",
        "created_at": "2023-12-19T01:55:53Z",
        "body": "` kill -9 $(ps aux | grep main.py | grep -v grep | awk '{print $2}')` \r\n\r\nThis solved my problem. However, if I did not use `kill -9` it does not work for me."
      },
      {
        "user": "chenfengshijie",
        "created_at": "2024-06-24T09:35:21Z",
        "body": "Use torchrun instead of python solve this problem?Since torchrun can choose a free port to launch."
      }
    ]
  },
  {
    "number": 2080,
    "title": "[test] Add memory parity tests",
    "created_at": "2020-06-05T09:51:35Z",
    "closed_at": "2020-12-23T19:38:58Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "priority: 0",
      "ci"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2080",
    "body": "Preferably add memory parity assertions to existing tests, to avoid extra run time.\r\n\r\nMemory parity tests are important to avoid memory leaks.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2080/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-07-12T22:53:49Z",
        "body": "Fron Will in core....\r\n`torch.cuda.memory_snapshot()`\r\nI think we could use this to generate a memory profiler\r\n```\r\n(Pdb) torch.cuda.memory_allocated(device=None)\r\n9580032\r\n(Pdb) torch.cuda.memory_allocated(device=‘cuda:0’)\r\n0\r\n(Pdb) torch.cuda.memory_allocated(device=‘cuda:1’)\r\n0\r\n```"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-03T17:04:17Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 2079,
    "title": "Handle KeyboardInterrupt during training",
    "created_at": "2020-06-05T08:37:19Z",
    "closed_at": "2020-06-15T10:35:27Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2079",
    "body": "## 🚀 Feature\r\n\r\nIt should be possible to examine the stack  if the training is interrupted manually.\r\n\r\n### Motivation\r\n\r\nIn my case, the program hangs at some point and I have to cancel it manually. Because Lightning catches KeyboardInterrupt, I don't get to know where the program was hanging.\r\n\r\n### Alternatives\r\n\r\n1. **Re-raise KeyboardInterrupt.** This would break the current behavior. This could be made configurable.\r\n2. **Save result of `sys.exc_info()` for later examination.** This could introduce memory issues like memory leaks or circular references.\r\n3. **Call a handler.** The users could then call `sys.exc_info()` in the handler. This would make memory issues less likely.\r\n\r\nI will prepare a pull request for the third option.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2079/comments",
    "author": "moi90",
    "comments": [
      {
        "user": "moi90",
        "created_at": "2020-06-05T09:11:23Z",
        "body": "The least invasive change would be to save the result of sys.exc_info() for later examination."
      },
      {
        "user": "Borda",
        "created_at": "2020-06-10T23:06:36Z",
        "body": "> The least invasive change would be to save the result of sys.exc_info() for later examination.\r\n\r\nwhere would you save it, to logs? mind sending a PR?"
      },
      {
        "user": "moi90",
        "created_at": "2020-06-11T20:35:02Z",
        "body": "I already did, see above ;) As I said, using a callback narrows the possibility of memory issues."
      },
      {
        "user": "SurajDonthi",
        "created_at": "2020-09-07T16:18:21Z",
        "body": "Is this feature up in the latest release - 0.9.0?\r\n\r\nI'm looking to automatically perform testing on Keyboard Interrupt but haven't been successful! "
      },
      {
        "user": "moi90",
        "created_at": "2020-09-10T09:02:41Z",
        "body": "```sh\r\ngit tag --contains fd1693e\r\n0.8.0\r\n0.8.1\r\n0.8.2\r\n0.8.3\r\n0.8.4\r\n0.8.5\r\n0.9.0\r\n0.9.1rc1\r\n```\r\n\r\nSo: Yes."
      }
    ]
  },
  {
    "number": 2067,
    "title": "0.8.0-dev can't load model with hparams, KeyError: 'module_arguments'",
    "created_at": "2020-06-04T02:58:09Z",
    "closed_at": "2020-06-14T15:36:47Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2067",
    "body": "## 🐛 Bug\r\nIn pt 0.8.0-dev, I  initialize my model with hparams as old version, but it can't be load using `load_from_checkpoint`\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 123, in <module>\r\n    main(hparams)\r\n  File \"main.py\", line 61, in main\r\n    checkpoint_path=checkpoint,\r\n  File \"/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1563, in load_from_checkpoint\r\n    checkpoint[CHECKPOINT_KEY_MODULE_ARGS].update(kwargs)\r\nKeyError: 'module_arguments'\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2067/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "versatran01",
        "created_at": "2020-06-06T13:48:28Z",
        "body": "You can just have your own function that load from the checkpoint, which avoid the mess with moudle_arguments.\r\nsomething like this hopefully works\r\n```python\r\n@classmethod\r\ndef load_checkpoint(cls, checkpoint_path):\r\n    checkpoint = th.load(checkpoint_path)\r\n    system = cls(checkpoint[\"hparams\"])\r\n    system.load_state_dict(checkpoint['state_dict'])\r\n    return system\r\n```"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-06T13:57:36Z",
        "body": "the PR fix is almost ready. I would just wait @Borda. \r\n\r\nthis is why generally it’s recommended to use releases instead of master since it’s always in active development and we can’t guarantee that it won’t be broken some times. \r\n\r\nbut we’re a month from stable 1.0.0.... so these issues shouldn’t happen anymore :)"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-12T16:39:57Z",
        "body": "@xiadingZ mind check #2160"
      }
    ]
  },
  {
    "number": 2046,
    "title": "Trainer should run the test loop with the best weights when ModelCheckpoint is used",
    "created_at": "2020-06-01T22:59:24Z",
    "closed_at": "2020-06-15T12:02:37Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2046",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nI noticed that even when `ModelCheckpoint` is used, `Trainer` by default runs the test loop with the last weights, not the best weights saved by `ModelCheckpoint`. I believe the sensible default here is to run the test loop with the best weights saved by `ModelCheckpoint`.\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nNow that `ModelCheckpoint` has a pointer to the best weights, `Trainer` can replace the last weights with the best weights before running the test loop automatically.\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nPossibly, this could be another option to `Trainer`. I don't like this as much b/c this is the behavior most users would expect.\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2046/comments",
    "author": "yukw777",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-06-02T06:08:37Z",
        "body": "Something like this?\r\n`trainer.test(model, load_best_checkpoint=True)`"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-02T13:11:55Z",
        "body": "I would make the `load_best_checkpoint=True` as default..."
      },
      {
        "user": "yukw777",
        "created_at": "2020-06-02T14:48:33Z",
        "body": "Yeah this should definitely be the default behavior.\r\n\r\nAnother question is, can we only do this when `ModelCheckpoint` is used since Trainer itself doesn’t keep track of the best weights? What if someone writes their own `ModelCheckpoint`? It seems like there needs to be a common interface that `Trainer` uses to retrieve the best weights for the test loop. The best weights could then be whatever the `checkpoint_callback` defines it to be. In this way, I don’t think we’d need to have yet another option on `Trainer`."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-02T15:31:07Z",
        "body": "why not make it:\r\n\r\n```\r\n# default\r\ntest(..., checkpoint=‘best’)\r\n\r\ntest(..., checkpoint=PATH/CKPT)\r\n```\r\nwith the option for a string ‘best’\r\n\r\nand make this the default"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-02T17:55:43Z",
        "body": "> why not make it:\r\n> \r\n> ```\r\n> # default\r\n> test(..., checkpoint=‘best’)\r\n> \r\n> test(..., checkpoint=PATH/CKPT)\r\n> ```\r\n> \r\n> with the option for a string ‘best’\r\n> \r\n> and make this the default\r\n\r\nvery good and `test(..., checkpoint=None)` uses the last..."
      },
      {
        "user": "yukw777",
        "created_at": "2020-06-02T21:30:51Z",
        "body": "Nice! I like that idea. To summarize:\r\n- add an option to `test()` called `checkpoint` whose default value is `best`.\r\n- if it's None, use the weights from the last epoch\r\n- if it's another string, treat it as a path."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-02T22:17:39Z",
        "body": "ummm. i prefer None to disable it. \r\nthere will 100% be cases where people need to disable that haha. \r\n"
      },
      {
        "user": "yukw777",
        "created_at": "2020-06-03T15:08:47Z",
        "body": "ah yeah since using the last epoch weights is the current behavior, setting it to `None` (and using the last epoch weights) would effectively disable it. Let me know if my understanding is incorrect."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-03T15:27:34Z",
        "body": "current behavior is equivalent to None\r\n\r\nnew default behavior should be “best”"
      }
    ]
  },
  {
    "number": 2043,
    "title": "production model design separation",
    "created_at": "2020-06-01T19:54:44Z",
    "closed_at": "2020-08-16T16:30:41Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2043",
    "body": "Consider more separation in deployments. Can we add a 'RunnableModule' which is a subset of a 'TrainableModule' which can be extracted for runtime? So it won't have the network definition, and the deployed model won't have things like dataloaders.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2043/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "edenlightning",
        "created_at": "2020-06-01T19:54:57Z",
        "body": "@williamFalcon "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-07T14:34:46Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 2041,
    "title": "speed-up too long tests",
    "created_at": "2020-06-01T19:42:20Z",
    "closed_at": "2020-11-30T16:56:34Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "priority: 0",
      "ci"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2041",
    "body": "We should make sure our tests run in a reasonable time.\r\nCheck the speed of all tests and set an upper bound for testing time. Also check if we can enforce that in GH (except parity tests).\r\nFind hanging tests that are taking the longest- and ping last author to fix. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2041/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-06-02T12:18:23Z",
        "body": "> Find hanging tests that are taking the longest- and ping last author to fix.\r\n\r\nHaha this is good :)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-03T17:04:13Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-11-22T22:30:44Z",
        "body": "After deploying the new docker image we lower GPU runtime from 14min to 9min #3658 \r\nsome more speed gain shall come from using a simpler model for testing #3827 "
      }
    ]
  },
  {
    "number": 2017,
    "title": "Data parallel (dp) distributes the loss computation across devices separately, unlike pytorch",
    "created_at": "2020-05-30T14:57:02Z",
    "closed_at": "2020-05-30T17:36:48Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2017",
    "body": "[please remove]",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2017/comments",
    "author": "ozanciga",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-30T14:57:41Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      }
    ]
  },
  {
    "number": 1995,
    "title": "Log a warning/ raise an error when lightning replaces an existing Sampler",
    "created_at": "2020-05-28T22:31:50Z",
    "closed_at": "2020-06-02T22:52:05Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1995",
    "body": "## 🚀 Feature\r\n\r\nFirst of all, thanks for this awesome project! Really enjoying using it!\r\n\r\nFeature Request: Log a warning or raise a MisconfigurationException when lightning replaces an existing sampler with DistributedSampler. \r\n\r\nEven though this behaviour is documented, it's not intuitive. Also, if someone has defined a sampler - it will lead to very different training results if you use Single GPU vs DDP. So best to warn a user to remove the sampler from their code if using `replace_sampler_ddp=True`.\r\n\r\n### Motivation\r\n\r\nI'm pretty new to lightning - and recently moved one of my models to lightning. Unaware of the details, I chose ddp training strategy (since it was recommended). For a long time, my model was not converging on the val set - and it took me a lot of time to realise what was happening - lightning had replaced my Dataloader's sampler with DistributedSampler! In my particular model, balanced sampling iskey to convergence (since data is highly imbalanced). Lightning's behaviour makes sense in hindsight, and is well documented - but  may have saved me a lot of debugging time had it given a simple warning or error when this happens.\r\n\r\n\r\n### Additional context\r\n\r\nThe code change is very trivial. We can take advantage of the fact that DataLoader's default sampler is `SequentialSampler` (if shuffle=False and sampler=None). \r\n\r\ndata_loading.py:115\r\n```\r\nif self.replace_sampler_ddp and need_dist_sampler:\r\n    if not isinstance(dataloader.dataset.sampler, SequentialSampler):\r\n        raise MisconfigurationException(...)\r\n```\r\n\r\nDoes this make sense? Happy to submit a PR if it does.\r\n\r\nOn a separate note, can anyone here explain why exactly is `DistributedSampler` needed when using DDP? I can see that it will help if I have shuffle=false. It will probably help maintain sanity of an epoch (since otherwise epoch length will become num_processes*epoch).\r\n\r\nBut let's say I am doing random shuffling - and am cognizant of the fact that my epoch sizes will be higher, there is no real reason that I need to use `DistributedSampler` for DDP right? Or am I missing something?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1995/comments",
    "author": "devashishshankar",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-28T22:32:30Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-05-29T21:04:53Z",
        "body": "@devashishshankar sounds good to me, mind send a PR?"
      },
      {
        "user": "devashishshankar",
        "created_at": "2020-05-30T09:58:58Z",
        "body": "@Borda Yes, will do!"
      }
    ]
  },
  {
    "number": 1976,
    "title": "Crash trying to construct module_arguments when module is created in a method",
    "created_at": "2020-05-28T01:29:52Z",
    "closed_at": "2020-06-12T16:39:04Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1976",
    "body": "Last line here crashes:\r\n\r\n```\r\nimport pytorch_lightning as pl\r\nclass Module(pl.LightningModule):\r\n    def forward(self):\r\n        return 0\r\n\r\ndef test_outside():\r\n    a = Module()\r\n    print(a.module_arguments)\r\n\r\nclass A:\r\n    def test(self):\r\n        a = Module()\r\n        print(a.module_arguments)\r\n\r\n    def test2(self):\r\n        test_outside()\r\n        \r\n        \r\ntest_outside() # prints {}\r\nA().test2() # prints {}\r\nA().test() # crashes\r\n```\r\n\r\nFor context, this happens when we want to instantiate LightningModules as part of a unit testing functions.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1976/comments",
    "author": "maximsch2",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-28T01:30:30Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-28T04:18:12Z",
        "body": "@Borda "
      },
      {
        "user": "Borda",
        "created_at": "2020-05-28T14:36:34Z",
        "body": "This is very strange behaviour, I can reproduce but not understand how it is possible that it misses the property..."
      },
      {
        "user": "tullie",
        "created_at": "2020-05-29T17:23:42Z",
        "body": "It's related to the hparams frame inspection magic, right?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-29T17:28:52Z",
        "body": "another idea here haha. maybe as @Borda suggested, we make a function:\r\n```\r\nauto_register_arguments()`\r\n```\r\nthat get saved to the checkpoint?\r\n\r\nor\r\n```\r\nregister_checkpoint_arg(arg)\r\n```\r\n\r\nand that way we don't need to do the magic? only optionally if the user wants it?"
      },
      {
        "user": "Borda",
        "created_at": "2020-05-29T18:36:22Z",
        "body": "well, the question is what is safer...\r\nagainst the auto-register:\r\n- it is not very safe in case your argument name match the existing method\r\n- while saving you need to filter all attributes to be pickable\r\n\r\nI would stay with parsing init arguments and add an optional fund to do auto register but it on user's responsibility if he uses it or not..."
      },
      {
        "user": "Borda",
        "created_at": "2020-05-29T18:37:54Z",
        "body": "> It's related to the hparams frame inspection magic, right?\r\n\r\nI am not sure as it properly creates the instance, I will check it again...\r\nany suggestion about how to debug it? :]"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-31T06:00:45Z",
        "body": "@maximsch2 @tullie \r\nFixed via #2025 \r\nThis is to unblock while we revisit this API.\r\n\r\nMind adding other tests we should consider to make sure we don't break anything?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-02T16:24:16Z",
        "body": "After spending some time with #2048 I think I know what's going on here. Since we are recursively operating on the stack frame and going up with `locals.f_back` we will eventually land in the e.g. test_outside() function which is not a constructor and so we have an issue, because we don't want to collect locals there. So we need to check when to stop recursion and always stay withtin the constructor. We also want to throw an error when the user calls self.auto_collect somewhere other than the init. \r\nPretty sure I can debug it. \r\nAdding this to TODO list #1937 for bookkeeping"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-12T16:39:04Z",
        "body": "fixed in #2047 "
      }
    ]
  },
  {
    "number": 1966,
    "title": "ModelCheckpointCallback.on_validation_end does not listen to val_check_interval?",
    "created_at": "2020-05-27T00:06:14Z",
    "closed_at": "2020-11-02T14:05:59Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1966",
    "body": "Evidence: I have a unit test where I train for 2 epochs with `val_check_interval=0.5` and I only get two model checkpoints. Set breakpoints and indeed it is only hit twice. Would expect 4 times.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1966/comments",
    "author": "sshleifer",
    "comments": [
      {
        "user": "HansBambel",
        "created_at": "2020-05-27T08:44:51Z",
        "body": "In the `ModelCheckpoint` class is a parameter `period` that tells the callback after how many epochs a model is saved. Since this value is supposed to be an int I think it is only possible to save the model after whole epochs."
      },
      {
        "user": "sshleifer",
        "created_at": "2020-06-05T16:45:44Z",
        "body": "Is there any plan to change that or a way to make a checkpointing callback to run on_validation_end?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-05T16:52:21Z",
        "body": "yes, checkpoint and that API is being reworked atm #1989. Much simpler and much more expressive. that should be rolled out by saturday"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-04T17:33:36Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T03:24:25Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 1964,
    "title": "Separate *_percent_check for each *_dataloader",
    "created_at": "2020-05-26T22:12:25Z",
    "closed_at": "2020-09-26T15:32:45Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "discussion"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1964",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nCan we have `*_percent_check` to be a list too where `len(*_percent_check) == len(*_dataloaders)`? In case if it is `int` then it will be same for all the dataloaders passed. Don't know how this can be useful in any case, just a thought.\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nFor each val_dataloader or test_dataloader we can have an option to pass `*_percent_check` as a list with a `percent_check` for each of the `dataloader`. For eg. `val_percent_check = [0.1, 0.4]` and `val_dataloaders = [val_dl1, val_dl2]`.\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nLater we can do the same for training as well if #1959 get's merged.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1964/comments",
    "author": "rohitgr7",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-27T21:48:24Z",
        "body": "Your proposal is definitely feasible, but I'm not sure what the use case for this would be. Currently the *_percent_check args are intented to be used for debugging purposes, e.g. trying to overfit to make sure the model has the capacity, in which case a separation into multiple sizes is not needed.\r\n\r\nCould be a follow up to your existing PR #1920 \r\n@PyTorchLightning/core-contributors "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-27T09:38:22Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-09-25T10:53:06Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-26T15:32:45Z",
        "body": "looks like not a useful feature."
      }
    ]
  },
  {
    "number": 1937,
    "title": "TODO list for \"replace Hparams by init args\" PR ",
    "created_at": "2020-05-24T23:28:26Z",
    "closed_at": "2020-08-08T03:26:06Z",
    "labels": [
      "bug",
      "feature",
      "help wanted",
      "docs",
      "priority: 0",
      "ci"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1937",
    "body": "## 🚀 TODO: Follow up work on module arguments rework in #1896\r\n\r\n- [ ] 1. (docs) Make clear the multiple ways args can and cannot be passed in.\r\n   Example:\r\n   ```python\r\n    class LitModel(LightningModule):\r\n       def __init__(self, arg1, arg2):\r\n        ...\r\n    Trainer.add_argparse_args(parser)\r\n    LitModel.add_model_specific_args(parser)\r\n    LitModel(parser.parse_args())  # this will fail\r\n    ```\r\n    This won't work since the list of arguments in constructor is a fixed size. \r\n    We can fix it in two ways:\r\n    1. Add `**kwargs` to the init signature to catch any unnecessary args (not good design but works)\r\n    2. Split the parsers to separate model args from Trainer args\r\n\r\n- [x] 2. (docs) make it clear which types we save to the checkpoints and which not (nn.Module for example). The name \"module_arguments\" maybe misleading to believe all args are saved.\r\n- [ ] 3. Some old code was left commented, including tests, as mentioned by @yukw777\r\n- [x] 4.  (tests) The model checkpointing has changed, we should thoroughly test that the correct args are loaded.\r\n- [ ] 5. (tests) Test case for positional args\r\n- [x] 6. (bugfix) Fix for when super() is not called or called after other local vars were added, e.g., \r\n   ```python\r\n    class LitModel(LightningModule):\r\n       def __init__(self, arg1, arg2):\r\n           my_local_var = 2\r\n           super().__init__()\r\n           # module_arguments now contains \"my_local_var\"\r\n    \r\n    LitModel.load_from_checkpoint(...)  # this fails\r\n    # TypeError: __init__ got an unexpected argument \"my_local_var\"\r\n    ```\r\n    We obviously don't want any local vars other than the arguments in the checkpoint.\r\n\r\n- [x] 7. (bugfix) In Python we are not forced to call the instance \"self\", this is currently hardcoded and leads to:\r\n   ```python\r\n    class LitModel(LightningModule):\r\n       def __init__(obj, arg1, arg2):\r\n           obj.arg1 = arg1\r\n           super().__init__()\r\n           # module_arguments will contain LitModel() itself\r\n    ```\r\n    same applies to the conventional naming of \"*args\" and \"**kwargs\"\r\n- [x] 8. (tests) make sure the LRfinder still works as expected by passing in the suggested learning rate as argument (fixed in #2821 )\r\n- [ ] 9. (enhancement) @festeh wants to add support for dataclasses\r\n- [x] 10. (bugfix) some of the examples are broken because of the problem mentioned in 1.\r\n- [ ] 11. (test) multiple inheritance\r\n- [x] 12. Should error or warn when `self.auto_collect_arguments()` is called somewhere other than in init. A specific use case that is currently not working is #1976\r\n\r\nFeel free to add additional bullet points I missed :)",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1937/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2020-05-25T05:46:22Z",
        "body": "We should also make sure, that the current hparams will always be supported. There are definitely usecases where hparams are not suitable."
      },
      {
        "user": "Borda",
        "created_at": "2020-05-25T06:50:47Z",
        "body": "> We should also make sure, that the current hparams will always be supported. There are definitely usecases where hparams are not suitable.\r\n\r\nthey are as `Namespace` and `dict` are in allowed primitives"
      },
      {
        "user": "justusschock",
        "created_at": "2020-05-25T06:55:28Z",
        "body": "@Borda yes, but to make sure, I'd prefer to have an explicit test for this :)\r\n\r\nSince we should really take care of backwards compatibility."
      },
      {
        "user": "Borda",
        "created_at": "2020-05-25T07:23:15Z",
        "body": "> @Borda yes, but to make sure, I'd prefer to have an explicit test for this :)\r\n> Since we should really take care of backwards compatibility.\r\n\r\nSure, agree, mind draw the test in PR and I will finish it / ensure the compatibility =) "
      },
      {
        "user": "HansBambel",
        "created_at": "2020-05-25T11:01:32Z",
        "body": ">   ```python\r\n>    class LitModel(LightningModule):\r\n>       def __init__(self, arg1, arg2):\r\n>        ...\r\n>    Trainer.add_argparse_args(parser)\r\n>    LitModel(parser.pase_args())  # this will fail\r\n>   ```\r\n\r\n@awaelchli Just for clarification: this will not fail because you have a typo in `parser.pase_args()`, but because the call is not supported, right?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-05-25T11:04:59Z",
        "body": "yes exactly, it will fail because the argparser has many more args than just arg1, arg2. \r\nI will fix the typo."
      },
      {
        "user": "Borda",
        "created_at": "2020-06-16T14:16:46Z",
        "body": "@awaelchli let's update the list with respect to what has been done... \r\n@edenlightning mind help?"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-08-03T18:45:20Z",
        "body": "@awaelchli whats left here?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-03T18:58:07Z",
        "body": "I think most of the points are outdated, much has changed. I think we can close it and track any remaining issues via reported bugs.  Although I think testing of the \"save_hyperparameters\" feature could be more thorough in general (bullet points 5., 8., 11)"
      }
    ]
  },
  {
    "number": 1921,
    "title": "More than one save per epoch?",
    "created_at": "2020-05-22T01:15:58Z",
    "closed_at": "2020-05-22T01:21:10Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1921",
    "body": "I'd like to checkpoint, and validation based on number of training steps rather than number of epochs. What's the simplest way of implementing this in pytorch-lightning?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1921/comments",
    "author": "sabetAI",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-22T01:16:40Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      }
    ]
  },
  {
    "number": 1919,
    "title": "Images not being logged after using auto_lr_find",
    "created_at": "2020-05-21T15:15:03Z",
    "closed_at": "2020-08-02T18:07:06Z",
    "labels": [
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1919",
    "body": "## 🐛 Bug\r\n\r\nWhen I use the auto LR finder, the images that I logged are no longer being displayed in Tensorboard. The logging works normally when setting it to `False` or when not specifying it.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Set  `auto_lr_find = True` for the trainer\r\n2. Log image during test/validation step as `self.logger.experiment.add_image(title, image, step)`\r\n3. Load the Tensorboard WebUI and see that the image is not being logged\r\n\r\n### Expected behavior\r\n\r\nThe image that I logged should be visible in Tensorboard, even when using auto LR finder.\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce GTX 1050 Ti with Max-Q Design\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.18.3\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.5.0\r\n        - pytorch-lightning: 0.7.6\r\n        - tensorboard:       2.2.1\r\n        - tqdm:              4.46.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.2\r\n        - version:           #41~1586790036~18.04~600aeb5-Ubuntu SMP Mon Apr 13 17:47:15 UTC\r\n```\r\n - How you installed PyTorch (`conda`, `pip`, source): `pip`\r\n\r\n\r\n### Additional context\r\n\r\nNot sure if relevant, but I couldn't figure out how to pass the params to the `Trainer` as I'm using `from_argparse_args(hparams)`, where `hparams` contains the keys and values obtained from command-line `ArgumentParser`. So I set `hparams.auto_lr_finder = True` before instantiating the `Trainer`. The LR finder work's, so I assume there's no issue with that.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1919/comments",
    "author": "SubodhDahal",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-21T15:15:40Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "danielhomola",
        "created_at": "2020-05-23T16:48:33Z",
        "body": "I can second this. Same goes for the graph of the net.. Spent almost a full day trying to figure out what am I doing wrong, but once auto_lr_find=False, everything works fine.. "
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-25T12:33:09Z",
        "body": "@SubodhDahal and @danielhomola could one of you try out the fix that is on master now, and report back if it worked. If so please close the issue :)"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-24T17:39:04Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1918,
    "title": "Allow passing dict object as batch",
    "created_at": "2020-05-21T09:59:17Z",
    "closed_at": "2020-05-21T14:48:50Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1918",
    "body": "## 🚀 Feature\r\nAllow the **batch** variable to be a (nested) _dict_ object (in _training_step()_, etc.)\r\n\r\n### Motivation\r\nI was porting my code from my own implementation of a Trainer to Pytorch Lightning, as I believe this is an asset for reproducibility and clarity.\r\nHowever I stumbled across a problem, it is not possible for the variable **batch** to be a (nested) _dict_ object and it can only be a _Tensor_, _tuple_ or _list_.\r\n\r\n### Pitch\r\nThe dataset structure that I am using is the following:\r\n```py\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\n\r\n\r\nclass MyDataset(Dataset):\r\n    def __init__(\r\n        self, n: int\r\n    ):\r\n        self.n = n\r\n\r\n    def __len__(self):\r\n        return self.n\r\n\r\n    def __getitem__(self, item):\r\n        frame = {\r\n            \"index\": item,\r\n            \"A\": {\r\n                \"image\": torch.randn(3, 128, 128),\r\n                \"R\": torch.randn(3, 3),\r\n                \"t\": torch.randn(3, 1),\r\n                \"K\": torch.randn(3, 3),\r\n            },\r\n            \"B\": {\r\n                \"image\": torch.randn(3, 128, 128),\r\n                \"R\": torch.randn(3, 3),\r\n                \"t\": torch.randn(3, 1),\r\n                \"K\": torch.randn(3, 3),\r\n            },\r\n        }\r\n        return frame\r\n\r\n```\r\nThe PyTorch Dataloader is able to make a **batch** of frames with the default _collate_fn_.\r\nTo move things onto the GPU, I am currently doing:\r\n```py\r\ndef move(self, d: dict, device) -> dict:\r\n    for k in d:\r\n        if isinstance(d[k], dict):\r\n            d[k] = self.move(d[k])\r\n        elif isinstance(d[k], (Tensor)):\r\n            d[k] = d[k].to(device=device, non_blocking=True)\r\n    return d\r\n```\r\nThere is probably a nicer way to do it, but it does the job.\r\n\r\nCould you please add this feature, as passing a _tuple_ is very impractical when the amount of variables is high?\r\n\r\nThanks in advance,\r\n\r\nGuillaume",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1918/comments",
    "author": "GuillaumeRochette",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-21T09:59:54Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-05-21T14:48:03Z",
        "body": "Hi \r\nthis  is already possible. dicts, tuples, lists etc are moved automatically to the correct device, and nesting is also supported. just try it :)\r\n\r\nWe also have a PR #1756 that allows one to define the transfer of custom batches in a hook. "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-05-21T14:48:39Z",
        "body": "In Lightning you don't have to move any data yourself. "
      },
      {
        "user": "cifkao",
        "created_at": "2021-03-30T13:30:34Z",
        "body": "@awaelchli The docs say that `batch` is \"A tensor, tuple or list.\" I had to google this issue to make sure I can use a dict as well. So the docs should probably be updated (you write about \"dicts, tuples, lists etc\", so I'm wondering what else is included)."
      },
      {
        "user": "Mollylulu",
        "created_at": "2021-10-19T09:24:20Z",
        "body": "I meet the same issue. I return a dict in my dataset, but after sending into trainer.fit(), it seems does not work. My batch does not work. would anyone help me figure out this prob? thanks!"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-10-24T16:45:33Z",
        "body": "Hi @Mollylulu \r\n\r\nHere is a concrete example how to work with data returned as a dict in the dataset. If you meet an error, please open an issue or discussion.\r\n\r\n\r\n```python\r\n\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return {\"data\": self.data[index], \"something\": \"else\"}\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        batch = batch[\"data\"]\r\n\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        return {\"loss\": loss}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        enable_model_summary=False,\r\n    )\r\n    trainer.fit(model, train_dataloaders=train_data)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n\r\n```"
      }
    ]
  },
  {
    "number": 1911,
    "title": "options for run only sanity check without training in 'run_pretrain_routine'",
    "created_at": "2020-05-21T01:44:15Z",
    "closed_at": "2020-07-29T07:41:11Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1911",
    "body": "## 🚀 Feature\r\n\r\n### Motivation\r\n\r\nfor pytest pytorch-lightning based project, the training phase is not essential in some cases (like testing only inference). So adding an argument for running sanity-check without the training phase could be reasonable.\r\n\r\n### Pitch\r\n\r\n`run_pretrain_routine(self, model)` => `run_pretrain_routine(self, model, run_train=True)`",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1911/comments",
    "author": "davinnovation",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-21T06:22:43Z",
        "body": "I don't understand this. Please elaborate more. \r\nIn some tests training a couple of steps or even an epoch is needed to test certain calls or events."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-20T07:21:08Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1903,
    "title": "`num_training_batches` can be 0 if `train_percent_check` is too small",
    "created_at": "2020-05-20T15:17:01Z",
    "closed_at": "2020-06-26T13:56:56Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1903",
    "body": "## 🐛 Bug\r\n\r\nif train_percent_check is set to a tiny value, then `num_training_batches` can be 0. Then no training happens, and no validation either, but no warning is raised and epochs are normally iterated over.\r\n\r\n### To Reproduce\r\n\r\nTrain on any dataset using limit sufficiently small (eg 0.01).\r\n\r\n#### Code sample\r\nI think there is a simple fix, in `data_loading.py`, adding a line after l. 171\r\n\r\n```\r\n            (l.171) self.num_training_batches = int(self.num_training_batches * self.train_percent_check)\r\n            self.num_training_batches = max(1, self.num_training_batches)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTraining at least on 1 batch to have a quick check.\r\n\r\n### Environment\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           None\r\n* Packages:\r\n        - numpy:             1.18.4\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.5.0\r\n        - pytorch-lightning: 0.7.6rc3\r\n        - tensorboard:       2.2.1\r\n        - tqdm:              4.46.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.2\r\n        - version:           #864-Microsoft Thu Nov 07 15:22:00 PST 2019\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1903/comments",
    "author": "annemariet",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-20T16:34:15Z",
        "body": "sounds reasonable. \r\nbtw there is also a Trainer arg \"fast_dev_run\" if you want to do a quick check with one batch per training/validation dataloader."
      },
      {
        "user": "annemariet",
        "created_at": "2020-05-20T16:35:38Z",
        "body": "Okay, thanks for the tip, indeed that would fit my needs here!"
      }
    ]
  },
  {
    "number": 1880,
    "title": "Support OmegaConf dicts in hparams",
    "created_at": "2020-05-19T00:55:52Z",
    "closed_at": "2020-07-27T05:38:23Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1880",
    "body": "## 🚀 Feature\r\nLightning hparams were recently changed to support only dicts and ArgParse namespaces. This diff (re)introduces compatibility with OmegaConf DictConfig, which is the type of config that Hydra uses.\r\n\r\n### Motivation\r\n\r\nUsing Hydra + Lightning\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1880/comments",
    "author": "Darktex",
    "comments": [
      {
        "user": "calclavia",
        "created_at": "2020-05-19T04:51:05Z",
        "body": "Running into the same issue when trying to use things like auto_lr_find \r\n`pytorch_lightning.utilities.exceptions.MisconfigurationException: When auto_lr_find is set to True, expects that hparams either has field `lr` or `learning_rate` that can overridden`"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-18T05:21:23Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1875,
    "title": "Training time estimation when max_epochs is given",
    "created_at": "2020-05-18T16:44:31Z",
    "closed_at": "2020-07-27T08:38:22Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1875",
    "body": "## 🚀 Feature\r\nTime estimation of training until `max_epochs` are reached.\r\n\r\n### Motivation\r\n\r\nRight now I don't see how long I am training for already and how long it is going to take to train e.g. 500 epochs.\r\nI usually have a tqdm progress bar for number of epochs to get an estimate how long my training will run maximally.\r\n\r\n### Pitch\r\n\r\nIf `max_epochs` is given, create a tqdm progress bar (probably with `leave=True`).\r\n\r\n### Alternatives\r\n\r\nKeep it like it is and make rough estimations per hand.\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1875/comments",
    "author": "HansBambel",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-18T16:45:14Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-18T16:56:46Z",
        "body": "? we already have a tqdm bar. in case you need a custom one you can also implement one using a callback @awaelchli "
      },
      {
        "user": "HansBambel",
        "created_at": "2020-05-18T17:01:34Z",
        "body": "I know, but it only shows the time for the current epoch, but not how long the overall training will take."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-18T17:07:28Z",
        "body": "don't you just multiply times the num of epochs?"
      },
      {
        "user": "HansBambel",
        "created_at": "2020-05-18T17:24:43Z",
        "body": "Then I would not have a time for how long I am training already and I would need to manually check how long one epoch takes and do the calculation manually.\r\n\r\nSuch a feature was mentioned in #765 and #1450 as well. There it was called global progress bar. That's what I meant. Sorry for the confusion."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-05-19T06:38:23Z",
        "body": "Maybe we could provide such a global progress bar as part of bolts. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-18T08:21:21Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "soot-bit",
        "created_at": "2024-05-24T17:57:04Z",
        "body": "its 2024, this issue hasnt been fixed? we need to see estimated time for training and how long training has been running on the progress bar\r\n"
      }
    ]
  },
  {
    "number": 1872,
    "title": "Lightning cant find visible gpu when using pytorch 1.5.0",
    "created_at": "2020-05-18T10:33:08Z",
    "closed_at": "2020-05-19T13:54:32Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1872",
    "body": "## 🐛 Bug\r\n\r\nWhen using the latest pytorch version 1.5.0 with multi-gpu's (backend ddp), lightning says there are no visible gpu's.\r\n\r\nThe same code runs without issues when using pytorch 1.4.0\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n```\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 464, in __init__\r\n    self.data_parallel_device_ids = parse_gpu_ids(self.gpus)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 730, in parse_gpu_ids\r\n    gpus = sanitize_gpu_ids(gpus)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 696, in sanitize_gpu_ids\r\n    \"\"\")\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: \r\n                You requested GPUs: [0, 1, 2, 3, 4, 5, 6, 7]\r\n                But your machine only has: []\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.5.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: Nvidia - V100 - 8 gpu's\r\n - Lightning - 0.7.6",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1872/comments",
    "author": "leslyarun",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-18T10:33:51Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-18T16:11:55Z",
        "body": "mind posting a script to recreate?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-18T16:12:07Z",
        "body": "also make sure you have the gpu version of pytorch"
      },
      {
        "user": "yukw777",
        "created_at": "2020-05-18T20:31:57Z",
        "body": "i'm kind of sensing that you stopped your aws instance and resumed it. I encountered this exact same \"bug\" when I did that. For some unknown reasons, when you stop a gpu instance on aws and resume it, the nvidia driver goes away.. Try `nvidia-smi` to see if you still see your GPUs."
      },
      {
        "user": "leslyarun",
        "created_at": "2020-05-19T13:54:32Z",
        "body": "I was using cuda 10.1 and upgraded pytorch (which by default expects cuda 10.2). Installed pytorch 1.5.0 in the right way and it fixed the issue"
      },
      {
        "user": "SaoYear",
        "created_at": "2021-05-26T09:12:37Z",
        "body": "This code saved my life.\r\n```\r\nconda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=10.2 -c pytorch\r\n```"
      }
    ]
  },
  {
    "number": 1870,
    "title": "load_from_checkpoint(): hparam_overrides only works with hparams_file",
    "created_at": "2020-05-18T01:38:55Z",
    "closed_at": "2020-06-26T13:57:32Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1870",
    "body": "## 🐛 Bug\r\n\r\nUsing the `hparam_overrides` argument for `load_from_checkpoint()` without the `hparams_file` argument gives an `UnboundLocalError`.\r\n\r\n### To Reproduce\r\n\r\n#### Code sample\r\nDo:\r\n```python3\r\nMyLightningModule.load_from_checkpoint(ckpt_path, hparam_overrides={'param': 0})\r\n```\r\nand the result will be `UnboundLocalError: local variable 'hparams' referenced before assignment`, but the following works:\r\n```python3\r\nMyLightningModule.load_from_checkpoint(ckpt_path, hparams_file=hp_path, hparam_overrides={'param': 0})\r\n```\r\n\r\n### Expected behavior\r\n\r\nOverriding should be possible even if the hparams are loaded from the checkpoint file rather than the separate hparams file.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.5.0\r\n - OS (e.g., Linux): Ubuntu 16.04 and 18.04\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.8.2\r\n - CUDA/cuDNN version: 10.1.243/7.6.5.32\r\n - GPU models and configuration: GTX1070\r\n\r\n### Additional context\r\n\r\nThis is a super simple fix---I thought about just submitting a PR directly but figured I should check that this behaviour is indeed erroneous first.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1870/comments",
    "author": "rightaditya",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-18T01:39:27Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-08T11:20:29Z",
        "body": "@rightaditya try from master? "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-08T11:20:54Z",
        "body": "```\r\nMyLightningModule.load_from_checkpoint(ckpt_path, arg_to_override=x)\r\n```"
      }
    ]
  },
  {
    "number": 1869,
    "title": "0.7.6 breaks model checkpoint",
    "created_at": "2020-05-18T01:00:26Z",
    "closed_at": "2020-05-18T06:07:31Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1869",
    "body": "## 🐛 Bug\r\ntraining crashes when model checkpoint is triggered\r\n\r\n### To Reproduce\r\nIdentical code works in 0.7.5, hparam types used are `int`, `float`, `str`, `bool` \r\nhparams is generated via test tube and saved in the module as `self.hparams`\r\n```\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 389, in ddp_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1015, in run_pretrain_routine\r\n    self.train()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 347, in train\r\n    self.run_training_epoch()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 452, in run_training_epoch\r\n    self.call_checkpoint_callback()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 790, in call_checkpoint_callback\r\n    self.checkpoint_callback.on_validation_end(self, self.get_model())\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py\", line 10, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 241, in on_validation_end\r\n    self._do_check_save(filepath, current, epoch)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 275, in _do_check_save\r\n    self._save_model(filepath)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 142, in _save_model\r\n    self.save_function(filepath)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py\", line 260, in save_checkpoint\r\n    checkpoint = self.dump_checkpoint()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py\", line 355, in dump_checkpoint\r\n    f' not {checkpoint[\"hparams_type\"]}'\r\nValueError: ('The acceptable hparams type is dict or argparse.Namespace,', ' not TTNamespace')\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1869/comments",
    "author": "s-rog",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-05-18T01:27:32Z",
        "body": "try master?"
      },
      {
        "user": "s-rog",
        "created_at": "2020-05-18T06:07:31Z",
        "body": "yep 0.7.7 dev fixed it, cheers."
      }
    ]
  },
  {
    "number": 1866,
    "title": "Replace val_loss with monitor_metric",
    "created_at": "2020-05-17T18:32:31Z",
    "closed_at": "2020-08-17T22:12:04Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1866",
    "body": "Validation epoch end has a special key called 'val_loss' which enables early stopping and checkpoint.\r\n\r\nHowever, a better name is likely monitor_metric\r\n\r\n```\r\n    def validation_epoch_end(self, outputs):\r\n        return {'monitor_metric': whatever_thing_i_want}\r\n```\r\n\r\nThis makes it clear to the user that they can pass in anything (epoch, loss, accuracy, bleu, etc).\r\n\r\nObviously this change needs to be backward compatible",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1866/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-08-08T20:57:01Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1831,
    "title": "DDP breaks LR finder",
    "created_at": "2020-05-14T02:26:31Z",
    "closed_at": "2020-06-01T15:00:34Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1831",
    "body": "## 🐛 Bug\r\n\r\nDDP breaks LR finder\r\n\r\n### To Reproduce\r\n\r\n```\r\nfinder = trainer.lr_find(model)\r\nprint(finder.suggestion())\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./training.py\", line 107, in <module>\r\n    main(hparam_trial)\r\n  File \"./training.py\", line 97, in main\r\n    finder = trainer.lr_find(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/lr_finder.py\", line 153, in lr_find\r\n    self.fit(model, train_dataloader=train_dataloader)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/opt/conda/lib/python3.6/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/opt/conda/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/opt/conda/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/opt/conda/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_LRFinder._get_new_optimizer.<locals>.configure_optimizers'\r\n```\r\n\r\nAt first I thought it's because `configure_optimizers` returns `[opt], [sched]` but returning `opt` still causes the error. Training works correctly with the same code.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1831/comments",
    "author": "s-rog",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-05-14T06:35:54Z",
        "body": "@SkafteNicki "
      },
      {
        "user": "Alikerin",
        "created_at": "2020-05-14T09:35:04Z",
        "body": "I also face a similar issue with Tensorboard logger whenever the logger flag is left as default both on GPU and TPU colab runtime. It throws the following exception on TPU runtime\r\n```\r\nException in device=TPU:0: dictionary update sequence element #0 has length 1; 2 is required\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 531, in tpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 980, in run_pretrain_routine\r\n    self.train()\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 347, in train\r\n    self.run_training_epoch()\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 465, in run_training_epoch\r\n    self.log_metrics(batch_step_metrics, grad_norm_dic)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/logging.py\", line 74, in log_metrics\r\n    self.logger.save()\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py\", line 10, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/loggers/tensorboard.py\", line 161, in save\r\n    save_hparams_to_yaml(hparams_file, self.hparams)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/saving.py\", line 151, in save_hparams_to_yaml\r\n    yaml.dump(hparams, fp)\r\n  File \"/usr/local/lib/python3.6/dist-packages/yaml/__init__.py\", line 200, in dump\r\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/yaml/__init__.py\", line 188, in dump_all\r\n    dumper.represent(data)\r\n  File \"/usr/local/lib/python3.6/dist-packages/yaml/representer.py\", line 26, in represent\r\n    node = self.represent_data(data)\r\n  File \"/usr/local/lib/python3.6/dist-packages/yaml/representer.py\", line 47, in represent_data\r\n    node = self.yaml_representers[data_types[0]](self, data)\r\n  File \"/usr/local/lib/python3.6/dist-packages/yaml/representer.py\", line 205, in represent_dict\r\n    return self.represent_mapping('tag:yaml.org,2002:map', data)\r\n  File \"/usr/local/lib/python3.6/dist-packages/yaml/representer.py\", line 116, in represent_mapping\r\n    node_value = self.represent_data(item_value)\r\n  File \"/usr/local/lib/python3.6/dist-packages/yaml/representer.py\", line 51, in represent_data\r\n    node = self.yaml_multi_representers[data_type](self, data)\r\nValueError: dictionary update sequence element #0 has length 1; 2 is required\r\n```\r\nSimilarly, on GPU runtime it throws an exception saying `can't pickle _thread.lock objects`.\r\nI resolve the issue by setting `logger=False` "
      }
    ]
  },
  {
    "number": 1830,
    "title": "Auto lr find is also missing experiments somehow?",
    "created_at": "2020-05-14T01:53:09Z",
    "closed_at": "2020-05-14T19:35:26Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1830",
    "body": "```\r\n  File \"vae.py\", line 83, in validation_epoch_end\r\n    self.logger.experiment.add_image('images', grid, 0)\r\nAttributeError: 'NoneType' object has no attribute 'experiment'\r\n(demo2) [waf251@loopy3 vae_demo]$ python vae.py --gpus 1 --auto_lr_find True\r\n```\r\n@SkafteNicki ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1830/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-14T10:27:41Z",
        "body": "Same problem as #1828 "
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-14T19:30:39Z",
        "body": "@williamFalcon this can be closed"
      }
    ]
  },
  {
    "number": 1828,
    "title": "Automatic batch-size scaling is missing properties",
    "created_at": "2020-05-14T01:50:05Z",
    "closed_at": "2020-05-14T14:34:12Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1828",
    "body": "```\r\n  File \"envs/demo2/lib/python3.7/site-packages/pytorch_lightning/trainer/training_tricks.py\", line 267, in _run_power_scaling\r\n    trainer.fit(model)\r\n  File \"envs/demo2/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 839, in fit\r\n    self.single_gpu_train(model)\r\n  File \"/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 499, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"pytorch_lightning/trainer/trainer.py\", line 981, in run_pretrain_routine\r\n    False)\r\n  File \"evaluation_loop.py\", line 326, in _evaluate\r\n    eval_results = model.validation_epoch_end(outputs)\r\n  File \"vae.py\", line 83, in validation_epoch_end\r\n    self.logger.experiment.add_image('images', grid, 0)\r\nAttributeError: 'NoneType' object has no attribute 'experiment'\r\n```\r\n@SkafteNicki \r\n\r\nLooks like loggers are gone?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1828/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-14T08:37:41Z",
        "body": "Loggers are disabled during the auto scaling, such that all the small runs that are done inside the auto scaling is not logged. This works fine if the user does not explicit call the logger inside their model (i.e. only called by lightning in the background). I am not sure the way around this, of cause the simplest solution would be to ask the user to do something like:\r\n```\r\nif self.logger:\r\n   self.logger.experiment.add_image('images', grid, 0)\r\n```\r\nBut that is not a good idea..."
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-14T09:19:30Z",
        "body": "Also this problem should also be present for the learning rate finder..."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-14T12:24:36Z",
        "body": "yeah, can’t ask the user to do that. maybe replace with a logger that has a no op somehow? "
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-14T13:32:35Z",
        "body": "Yes, I think you are right that we just initialize a dummy logger. Something like this should work\r\n```\r\nfrom pytorch_lightning.loggers import LightningLoggerBase\r\n\r\nclass DummyExperiment(object):\r\n    def nop(*args, **kw): pass\r\n    def __getattr__(self, _): return self.nop\r\n\r\n\r\nclass DummyLogger(LightningLoggerBase):\r\n    def __init__(self):\r\n        self._experiment = DummyExperiment()\r\n    \r\n    @property\r\n    def experiment(self):\r\n        return self._experiment    \r\n    \r\n    def log_metrics(self, metrics, step):\r\n        pass\r\n    \r\n    def log_hyperparams(self, params):\r\n        pass\r\n    \r\n    @property\r\n    def name(self):\r\n        pass\r\n    \r\n    @property\r\n    def version(self):\r\n        pass\r\n    \r\nlogger = DummyLogger()\r\nlogger.experiment.add_image('hest', [1,2,3], global_step=3) # has no effect, but still exist\r\n```"
      }
    ]
  },
  {
    "number": 1827,
    "title": "Automatic batch_size finder does not work with 16 bit and in ddp",
    "created_at": "2020-05-14T01:44:01Z",
    "closed_at": "2020-09-22T21:18:17Z",
    "labels": [
      "bug",
      "duplicate",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1827",
    "body": "The recursive .fit() call in this module means it can't be used with ddp interactive.\r\n\r\nIt also cannot be used with 16-bit since the scaler is not enabled and saving checkpoint fails.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1827/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-05-14T01:44:29Z",
        "body": "@SkafteNicki "
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-05-14T10:34:07Z",
        "body": "I have trouble reproducing your error. If I run `pl_examples/basic_examples/gpu_template.py` with `--use_16bit --distributed_backend ddp` and have set `auto_scale_batch_size='power` in trainer construction I do not get an error.\r\n "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-14T12:21:29Z",
        "body": "add —gpus 2"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-06-08T11:37:05Z",
        "body": "@williamFalcon is this still relevant?"
      },
      {
        "user": "edenlightning",
        "created_at": "2020-09-16T17:12:15Z",
        "body": "@SkafteNicki could you reproduce the error?"
      }
    ]
  },
  {
    "number": 1796,
    "title": "Need a grammar check in README.md file. ",
    "created_at": "2020-05-12T11:14:36Z",
    "closed_at": "2020-06-05T09:53:23Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1796",
    "body": "There are some grammar issues in README.md file. Can I work on this issue?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1796/comments",
    "author": "Deeksha2501",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-05-12T11:50:58Z",
        "body": "yes! please submit a PR :)"
      },
      {
        "user": "Deeksha2501",
        "created_at": "2020-05-12T13:00:36Z",
        "body": "Thank you, I will make PR today"
      },
      {
        "user": "Deeksha2501",
        "created_at": "2020-05-12T13:45:22Z",
        "body": "I have submitted PR, please review"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-05T09:53:23Z",
        "body": "finished with #1798, Thank you :]"
      }
    ]
  },
  {
    "number": 1795,
    "title": "Need a grammar check in README.md file.",
    "created_at": "2020-05-12T11:13:56Z",
    "closed_at": "2020-05-12T11:51:12Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1795",
    "body": "\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1795/comments",
    "author": "Deeksha2501",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-12T11:15:54Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-12T11:51:16Z",
        "body": "duplicate"
      }
    ]
  },
  {
    "number": 1736,
    "title": "Track model configuration in addition to hyperparamters",
    "created_at": "2020-05-05T14:58:10Z",
    "closed_at": "2020-07-13T17:35:18Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1736",
    "body": "## 🚀 Feature\r\nI would like to separate the current `hparams` (for example used to log models) into two groups:\r\n- Hyperparamters: are those hparams, which can be optimized without changing the models meaning and may change the performance. For example the amount of samples generated, the batch size or the learning rate.\r\n- Configuration: are those hparams, which actually change the 'meaning' of the model and can not be optimized in a useful, automatic manner. Possible examples are the noise variance for a denoising task or a random seed for reproducible noise generation.\r\n\r\n\r\n### Motivation\r\n\r\n\r\nI'd like to see both types of hparams in my logging, in order to be able to tell which model ran with what set of configuration. I think, separating actual hyperparamters from configuration also makes it easier to optimizer hyperparameters automatically.\r\n\r\n### Pitch\r\n\r\nI guess, the easiest way would be to duplicate (maybe slightly alter) the current behavior of the `LightningModule.hparams` field in an additional field (e.g. `LightningModule.config`).\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1736/comments",
    "author": "Uroc327",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-05T14:58:54Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-04T15:37:02Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1733,
    "title": "Allow modification of batch for `on_batch_start` hook",
    "created_at": "2020-05-05T01:09:10Z",
    "closed_at": "2020-07-14T18:27:36Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1733",
    "body": "## 🚀 Feature\r\nAllow modification of batch for `on_batch_start` hook in the `LightningModule`\r\n\r\n### Motivation\r\nFor the `LightningModule` there is an `on_batch_start` method that is called. Right now it either returns `-1` signifying that training should not continue, or `None`. There are scenarios whereby I want to change the batch that is being returned by the `DataLoader` before it is being transferred to the GPU.\r\n\r\n### Pitch\r\n\r\nLet the `on_batch_start` hook on the `LightningModule` return two variables:\r\n1. The batch\r\n2. Whether or not to continue training\r\n\r\n### Alternatives\r\n\r\nThe workaround that I currently have is:\r\n\r\n```\r\ndef on_batch_start(self, batch):\r\n    batch.to_pytorch_lightning_format()\r\n```\r\n\r\nWhere the `to_pytorch_lightning_format()` will alter the `Batch` object such that it mimics a dictionary so that the PTL will transfer the Tensors onto the GPU. Prior to doing this, since PTL only accepts recognizes lists, tuples, and dicts the Tensors in my `Batch` object will not be transferred onto the GPU.\r\n\r\nThough this isn't very desirable as it causes a side effect, ideally what I want to do is something like:\r\n\r\n```\r\ndef on_batch_start(self, batch):\r\n    batch = batch.to_pytorch_lightning_format()\r\n    return batch\r\n```\r\n\r\nThere are other use cases that I've encountered:\r\n1. More fine-grained control over which sample in the batch is to be used\r\n\r\nI've read through the relevant source code, and it seems like a simple fix (if I'm not mistaken). I'd be happy to make a PR if this is something that is aligned with the development of PTL.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1733/comments",
    "author": "rawmarshmellows",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-05T03:09:37Z",
        "body": "I'm not sure on_batch_start is the right place to provide this functionality. Looks like you want to do kind of the same as in #1245. There I proposed to add a transfer hook in the LightningModule. What do you think of that? Would it be appropriate for your use case?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-05T05:34:30Z",
        "body": "this sounds like you are looking for transforms... anything about formatting a batch should be done in the transforms going into the dataset "
      },
      {
        "user": "rawmarshmellows",
        "created_at": "2020-05-06T12:39:48Z",
        "body": "@williamFalcon  not sure what you mean by transforms, but in my scenario I use a `collate_fn` that I have defined for the Dataloader to return a Batch object. This object is used by the callbacks I have defined to do various things e.g. recording the training samples that have a high loss during each epoch etc. \r\n\r\n@awaelchli yes it would be appropriate though it still does seem like a hacky fix, as there are other scenarios whereby the batch may need to be modified before training. Would it not make more sense to give the user full control in the `on_batch_start` hook like `fastai` does?"
      },
      {
        "user": "dscarmo",
        "created_at": "2020-05-06T17:12:02Z",
        "body": "Best practice in PyTorch is to have your Dataset class do all the transforms needed over the data.\r\n\r\nAfter the data leaves your dataset, in my opnion, you shouldn't try to change it. The DataLoader should only handle collating the samples into a batch and parallelism (num_workers). \r\nIts very inefficient to modify data in the main loop. You can maybe try to edit the data in the collate_fn dataloader callback (where you build the batch), but i wouldn't recommend.\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-05T18:09:41Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1704,
    "title": "Error running on ddp (can't pickle local object 'SummaryTopic) with comet logger",
    "created_at": "2020-05-02T15:47:14Z",
    "closed_at": "2020-06-01T15:00:34Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1704",
    "body": "I have the following problem running on ddp mode with cometlogger.\r\nWhen I detach the logger from the trainer (i.e deleting`logger=comet_logger`) the code runs.\r\n```\r\nException has occurred: AttributeError\r\nCan't pickle local object 'SummaryTopic.__init__.<locals>.default'\r\n  File \"/path/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n  File \"/path/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"/path/multiprocessing/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"/path/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/path/multiprocessing/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/path/multiprocessing/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/path/site-packages/torch/multiprocessing/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"/path/site-packages/pytorch_lightning/trainer/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"/repo_path/train.py\", line 158, in main_train\r\n    trainer.fit(model)\r\n  File \"/repo_path/train.py\", line 72, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  File \"/repo_path/train.py\", line 167, in <module>\r\n    main()\r\n  File \"/path/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/path/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/path/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/path/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/path/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1704/comments",
    "author": "dvirginz",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-05-03T22:16:05Z",
        "body": "@ceyzaguirre4 pls ^^"
      }
    ]
  },
  {
    "number": 1700,
    "title": "minst multi-gpu You requested GPUs: [6, 7] But your machine only has: [0, 1]",
    "created_at": "2020-05-02T04:38:03Z",
    "closed_at": "2020-05-02T13:35:06Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1700",
    "body": "```\r\nimport os\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\n\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass CoolSystem(pl.LightningModule):\r\n\r\n    def __init__(self, hparams=None):\r\n        super().__init__()\r\n\r\n        # get hyperparams, etc...\r\n        self.hparams = hparams\r\n\r\n        # not the best model...\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n\r\n    def forward(self, x):\r\n        # called with self(x)\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # REQUIRED\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        tensorboard_logs = {'train_loss': loss}\r\n        return {'loss': loss, 'log': tensorboard_logs}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        # OPTIONAL\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        val_loss = F.cross_entropy(y_hat, y)\r\n        return {'val_loss': val_loss}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        # OPTIONAL\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        tensorboard_logs = {'val_loss': avg_loss}\r\n        return {'val_loss': avg_loss, 'log': tensorboard_logs}\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        # OPTIONAL\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        return {'test_loss': F.cross_entropy(y_hat, y)}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\r\n        tensorboard_logs = {'test_loss': avg_loss}\r\n        return {'test_loss': avg_loss, 'log': tensorboard_logs}\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.001)\r\n\r\n    def prepare_data(self):\r\n        self.mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\r\n        self.mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\r\n\r\n    def train_dataloader(self):\r\n        loader = DataLoader(self.mnist_train, batch_size=32, num_workers=4)\r\n        return loader\r\n\r\n    def val_dataloader(self):\r\n        loader = DataLoader(self.mnist_test, batch_size=32, num_workers=4)\r\n        return loader\r\n\r\n    def test_dataloader(self):\r\n        loader = DataLoader(self.mnist_test, batch_size=32, num_workers=4)\r\n        return loader\r\n\r\nfrom pytorch_lightning import Trainer\r\n\r\n\r\nmodel = CoolSystem()\r\n\r\n# most basic trainer, uses good defaults\r\ntrainer = Trainer(gpus=[6,7],num_nodes=2,distributed_backend='ddp', progress_bar_refresh_rate=10, max_epochs=10)\r\ntrainer.fit(model)\r\n```\r\n\r\n\r\nthe output is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home1/liuxinfang/anaconda3/envs/MomentRetrival/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\r\n    exitcode = _main(fd)\r\n  File \"/home1/liuxinfang/anaconda3/envs/MomentRetrival/lib/python3.7/multiprocessing/spawn.py\", line 114, in _main\r\n    prepare(preparation_data)\r\n  File \"/home1/liuxinfang/anaconda3/envs/MomentRetrival/lib/python3.7/multiprocessing/spawn.py\", line 225, in prepare\r\n    _fixup_main_from_path(data['init_main_from_path'])\r\n  File \"/home1/liuxinfang/anaconda3/envs/MomentRetrival/lib/python3.7/multiprocessing/spawn.py\", line 277, in _fixup_main_from_path\r\n    run_name=\"__mp_main__\")\r\n  File \"/home1/liuxinfang/anaconda3/envs/MomentRetrival/lib/python3.7/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/home1/liuxinfang/anaconda3/envs/MomentRetrival/lib/python3.7/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/home1/liuxinfang/anaconda3/envs/MomentRetrival/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home1/liuxinfang/projects/minist/model.py\", line 84, in <module>\r\n    trainer = Trainer(gpus=[6,7],num_nodes=2,distributed_backend='ddp', progress_bar_refresh_rate=10, max_epochs=10)\r\n  File \"/home1/liuxinfang/anaconda3/envs/MomentRetrival/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 438, in __init__\r\n    self.data_parallel_device_ids = parse_gpu_ids(self.gpus)\r\n  File \"/home1/liuxinfang/anaconda3/envs/MomentRetrival/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 712, in parse_gpu_ids\r\n    gpus = sanitize_gpu_ids(gpus)\r\n  File \"/home1/liuxinfang/anaconda3/envs/MomentRetrival/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 678, in sanitize_gpu_ids\r\n    \"\"\")\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: \r\n                You requested GPUs: [6, 7]\r\n                But your machine only has: [0, 1]\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1700/comments",
    "author": "xinfangliu",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-02T04:38:38Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-05-02T13:35:06Z",
        "body": "The error here:\r\n```\r\nYou requested GPUs: [6, 7]\r\nBut your machine only has: [0, 1]\r\n```\r\n\r\nSays your machine has 2 GPUs at index 0, 1... but you are requesting 2 GPUs at indexes [6,7].\r\n\r\nTo fix:\r\n```\r\ngpus=2\r\nor\r\ngpus=[0,1]\r\n```"
      },
      {
        "user": "xinfangliu",
        "created_at": "2020-05-02T16:09:29Z",
        "body": "Actually my machine has 8 gpus, since gpu 0,1 are used by other users, i need to use 6,7 with enough memory . The code performs normally with single gpu 6 or 7, but failed with more than one gpus."
      },
      {
        "user": "lalaha1",
        "created_at": "2022-03-31T06:39:46Z",
        "body": "> Actually my machine has 8 gpus, since gpu 0,1 are used by other users, i need to use 6,7 with enough memory . The code performs normally with single gpu 6 or 7, but failed with more than one gpus.\r\n\r\nDid you solve it? I met the same error."
      }
    ]
  },
  {
    "number": 1674,
    "title": "ModelCheckpoint.filepath cannot be None",
    "created_at": "2020-04-30T10:59:55Z",
    "closed_at": "2020-04-30T11:52:21Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1674",
    "body": "## 🐛 Bug\r\n\r\nThe  `ModelCheckpoint` callback's argument `filepath` cannot be None even though the docs says:\r\n>  Can also be set to `None`, then it will be set to default location during trainer construction.\r\n\r\nThe exception is raised:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/pi3ni0/Workspace/random/siamese_cnn_glove.py\", line 209, in <module>\r\n    checkpoint_callback = ModelCheckpoint(\r\n  File \"/home/pi3ni0/.venv/dev/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 89, in __init__\r\n    if save_top_k > 0 and os.path.isdir(filepath) and len(os.listdir(filepath)) > 0:\r\n  File \"/home/pi3ni0/.venv/dev/lib/python3.6/genericpath.py\", line 42, in isdir\r\n    st = os.stat(s)\r\nTypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType\r\n```\r\n\r\nIt is because `os.path.isdir` cannot be applied to None argument.\r\n\r\n#### Code sample\r\n```\r\n   checkpoint_callback = ModelCheckpoint(\r\n        filepath=None,\r\n    )\r\n\r\n   trainer = pl.Trainer(\r\n        checkpoint_callback=checkpoint_callback,\r\n    )\r\n\r\n   ...\r\n```\r\n\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX 1650 with Max-Q Design\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.18.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.0\r\n\t- pytorch-lightning: 0.7.5\r\n\t- tensorboard:       2.2.1\r\n\t- tqdm:              4.45.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1674/comments",
    "author": "elkotito",
    "comments": [
      {
        "user": "olineumann",
        "created_at": "2020-04-30T11:41:18Z",
        "body": "Duplicate issue #1660 #1535. Already fixed in PR #1654 "
      }
    ]
  },
  {
    "number": 1668,
    "title": "Ditch Trainer percent arguments, make overfit great again ",
    "created_at": "2020-04-29T20:47:58Z",
    "closed_at": "2020-06-17T12:03:28Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1668",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nReplace `*_percent_check` with `limit_*_batches` and redesign `overfit_pct` Trainer arguments. \r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nOver the past few days I was struggling to use these parameters in my tasks. For a example, I want to run a test on model overfitting for an image classification problem. I would take a few (say, 2) batches from my train dataset and train several epochs on them. Then I would assert 100% accuracy on these train batches.\r\n\r\nThe problems I stumbled on are the following:\r\n\r\n1. `overfit_pct` documentation is misleading. Recently a clarification was made that it sets `*_percent_check` parameters to a given value, but it still doesn't actually help to overfit a model since you can't simply run `trainer.test()` or `trainer.run_evaluation()` without manipulating model's dataloaders after running `trainer.fit(model)`. \r\n2. If the value of the `val_percent_check` is too small, which actually can happen if you use `overfit_pct` with small training dataset in mind, you just silently skip validation loop and run into an exception in `model.validation_epoch_end` trying to accumulate loss for batches. Yeah, handling the latter is reasonably on me since I override this method but it will be much nicer if such unexpected loop-skip is checked by Pytorch-Lightning. You guys are great and I want to love your project even more!\r\n3. `train_percent_check` doesn't guarantee training on the same small part of the training dataset for every epoch because *it is a best practice to shuffle your training data every epoch*. As a result, new batches are formed every epoch and thus no overfitting :( \r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n1. I find redesigning these dataset limiting parameters as **absolute number of batches** more straightforward and desired than now. After all, I dare to say that most of the researches and developers are thinking in terms of number of bathes rather than percent of dataset.\r\n2. `overfit_pct` is either removed or actually redesigned to help test overfitting, i.e. replacing validation or test loader with train loader. Ensure that training dataset isn't shuffled and the same batches are trained on every epoch.\r\n\r\n### Additional notes\r\n\r\nI realize that you cannot prohibit shuffling in case of using simply `*_percent_check` parameters. There can be experiments where you would like to see how your model performs training only on a portion of data. Therefore such prohibition is valid only for overfit mode.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1668/comments",
    "author": "iakremnev",
    "comments": [
      {
        "user": "tshrjn",
        "created_at": "2020-04-30T10:13:57Z",
        "body": "Couldn't agree more with the shuffling part, the `overfit_pct` doesn't control `shuffle` flag for the train_dataloader. I think that should really be set by lightning & not by the user to make this flag as as the do-it-all as it claims for overfitting. "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-16T23:59:11Z",
        "body": "Fixed!\r\nWill be available in 0.8.0"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-17T13:07:50Z",
        "body": "All of these were fixed in #2213 and #2220.\r\n\r\nThanks for the suggestions! keep them coming. "
      }
    ]
  },
  {
    "number": 1665,
    "title": "Trainer add args doesn't add default root dir",
    "created_at": "2020-04-29T15:59:49Z",
    "closed_at": "2020-05-12T12:53:27Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1665",
    "body": "## 🐛 Bug\r\n1. When using `parser = Trainer.add_argparse_args(parser)`, it's supposed to put all Trainer's arguments in the argparse with default values. Though currently it doesn't add `default_root_dir` and you get the error:\r\n\r\n```\r\n'Namespace' object has no attribute 'default_root_dir'\r\n```\r\nIt does add `default_save_path` which is deprecated.\r\n\r\n\r\n### To Reproduce\r\n#### Code Sample\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\n\r\nparser = argparse.ArgumentParser(description='demo')\r\nparser = Trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\n\r\nprint(args.default_root_dir)\r\n```\r\n\r\nA similar unit test could also be made, if not there already.\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.3\r\n        - tensorboard:       2.2.0\r\n        - tqdm:              4.45.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                -\r\n        - processor:         x86_64\r\n        - python:            3.6.7\r\n        - version:           #75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1665/comments",
    "author": "tshrjn",
    "comments": [
      {
        "user": "olineumann",
        "created_at": "2020-04-30T11:46:49Z",
        "body": "Did you tried to update to 0.7.5. Maybe it is already solved."
      },
      {
        "user": "tshrjn",
        "created_at": "2020-05-04T07:15:14Z",
        "body": "Hi @olineumann, yes updating did resolve this. However, the `profiler` arg is now broken. The same demo code above with `profiler` gives the same error `'Namespace' object has no attribute 'profiler'`."
      },
      {
        "user": "olineumann",
        "created_at": "2020-05-04T08:57:26Z",
        "body": "What do you mean 'with profiler'? Do you mean Trainer(..., profiler=True)? But you don't initialize a Trainer.\r\n\r\nRunning your code or this below didn't crash with any error on my machine.\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\n\r\nparser = argparse.ArgumentParser(description='demo')\r\ntrainer = Trainer(profiler=True)\r\nparser = trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\n\r\nprint(args.default_root_dir)\r\n```\r\n\r\nMaybe you could post the complete error message from the python interpreter. "
      },
      {
        "user": "tshrjn",
        "created_at": "2020-05-06T22:12:44Z",
        "body": "`add_argparse_args ` is supposed to add the args from trainer to parser. But it doesn't do that for a few args. In this case `profiler`, previously the issue was for `default_root_dir`.\r\n\r\nTry the following code by running:\r\n`python demo.py --profiler True` or  other possibly accepted way `python demo.py --profiler`  with the following code:\r\n\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\n\r\ntrainer = Trainer()\r\nparser = argparse.ArgumentParser(description='demo')\r\nparser = trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\n\r\nprint(args.profiler)\r\n\r\n```\r\n\r\n"
      },
      {
        "user": "tshrjn",
        "created_at": "2020-05-12T03:12:23Z",
        "body": "Any update?"
      },
      {
        "user": "olineumann",
        "created_at": "2020-05-12T10:43:03Z",
        "body": "I just created a PR. After looking at the code I found out that add_argparse_args is checking the argument types and is only adding attributes of type str, float, int or bool. The profiler attribute could be of type bool so it should be a bug.\r\n\r\nI saw that get_init_arguments_and_types() is returning profiler as argument but only of type BaseProfiler. After updating typing annotation of profiler argument it worked. Should be available in the next version.\r\n\r\nSee PR #1794 "
      },
      {
        "user": "tshrjn",
        "created_at": "2020-05-12T21:06:28Z",
        "body": "A similar issue is with the pickling of the profiler when it's a `Profile` object & the trainer tries to save the `hparams`.\r\n\r\n```python\r\nTypeError: can't pickle Profile objects\r\n```\r\n\r\n\r\nExample code:\r\n\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning import profiler\r\nfrom pl_bolts.models.gans import BasicGAN\r\n\r\ntrainer = Trainer()\r\nparser = argparse.ArgumentParser(description='demo')\r\nparser = trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\nmodel = BasicGAN()\r\n\r\ntrainer = Trainer.from_argparse_args(\r\n        args, profiler=profiler.AdvancedProfiler())\r\ntrainer.fit(model)\r\n\r\n```\r\n"
      },
      {
        "user": "olineumann",
        "created_at": "2020-05-13T08:02:45Z",
        "body": "Can't reproduce your issue with pl version 0.7.6rc1. On my machine your code runs and saves checkpoints without crashing. Also this wouldn't belong to the topic of this issue imo. This would be a bug in the saving routine. "
      }
    ]
  },
  {
    "number": 1649,
    "title": "DP - Getting tensor properties",
    "created_at": "2020-04-28T14:45:09Z",
    "closed_at": "2020-09-03T15:48:35Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1649",
    "body": "### 🐛 Bug\r\n\r\nIn validation step, I get a metric computed on parameters tensor that have an attribute that is set when initializing the network. E.g. some parameters have a binary flag \"ptype\" attached like so : `tensor.ptype = True`.\r\n\r\nIt works well with Pytorch native DataParallel or with single GPU. However, when using Lightning DataParallel the issue is that the validation step is done on a child process for which only the network replica exist: however those replica do not preserve tensors arbitrary properties.\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Attach some arbitrary properties to a neural network parameter, e.g. : \r\n```\r\n# in __init__ of the nn.Module\r\nself.register_parameter('some_param', nn.Parameter(torch.tensor([0.1]))\r\nsome_param.ptype = True\r\n```\r\n2. Run in dataparallel mode with >1 GPU\r\n3. In validation step, do a list comprehension like so : `[param.ptype for param in self.model.parameters() if hasattr(param, 'ptype')]`\r\n4. Notice the list is empty for the replica processes\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\nSee above.\r\n### Expected behavior\r\n\r\nThe returned list shouldn't be empty. On another hand, since the metric is a constant across replica, the function needn't be run in parallel.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\n```\r\nPyTorch version: 1.6.0.dev20200403\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: CentOS Linux release 7.8.2003 (Core)\r\nGCC version: (Homebrew GCC 5.5.0_7) 5.5.0\r\nCMake version: version 3.13.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: \r\nGPU 0: Tesla P100-SXM2-16GB\r\nGPU 1: Tesla P100-SXM2-16GB\r\nGPU 2: Tesla P100-SXM2-16GB\r\nGPU 3: Tesla P100-SXM2-16GB\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] kmeans-pytorch            0.2                      pypi_0    pypi\r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] pytorch                   1.6.0.dev20200403 py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch-nightly\r\n[conda] pytorch-lightning         0.7.5                    pypi_0    pypi\r\n[conda] pytorch-memlab            0.0.4                    pypi_0    pypi\r\n[conda] pytorch-pcen              0.0.1                    pypi_0    pypi\r\n[conda] torchvision               0.6.0.dev20200403      py37_cu101    pytorch-nightly\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1649/comments",
    "author": "sebastienwood",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-04-28T14:53:07Z",
        "body": "yeah, this has been SUPER annoying since day one haha... this is a limitation of DataParallel, but i know there's a way around it, i can't remember what though. I believe we have an issue for this, but this would be an amazing thing to tackle."
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-05-03T20:41:15Z",
        "body": "@sebastienwood Hey, as I understand it, in the list comprehension that you mentioned:\r\n\r\n[param.ptype for param in self.model.parameters() if hasattr(param, 'ptype')], \r\n\r\nif you replace 'param.ptype' with 'param.__class__' and remove  if hasattr(param, 'ptype', you get something along these lines:\r\n\r\ncuda:0\r\n[<class 'torch.nn.parameter.Parameter'>]\r\n\r\ncuda:1\r\n[<class 'torch.Tensor'>]\r\n\r\nSo it becomes clear that the other GPUs receive the Tensor contained in the Parameter at this point in the validation_step() function. You mentioned that the PyTorch native DataParallel works fine with something like this. Can you post a code snippet to replicate the desired behaviour in PyTorch's native DataParallel?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-25T15:31:28Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1637,
    "title": "Trainer DDP invoking load_spawn_weights() on each node",
    "created_at": "2020-04-27T13:41:30Z",
    "closed_at": "2020-04-28T00:08:07Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1637",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nOn a SLURM cluster, I am seeing the same problem as issue #1335 , despite that issue's fix being applied.  \r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n1. Allocate 4 nodes on a SLURM-managed cluster\r\n1. srun the script pl.py on each allocated node\r\n2. See the errors on 3 of 4 nodes:\r\n```\r\nINFO:lightning:GPU available: True, used: True\r\nINFO:lightning:VISIBLE GPUS: 0,1\r\nINFO:lightning:GPU available: True, used: True\r\nINFO:lightning:VISIBLE GPUS: 0,1\r\nINFO:lightning:GPU available: True, used: True\r\nINFO:lightning:VISIBLE GPUS: 0,1\r\nINFO:lightning:GPU available: True, used: True\r\nINFO:lightning:VISIBLE GPUS: 0,1\r\n/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/warnings.py:18: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\r\n  warnings.warn(*args, **kwargs)\r\n/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/warnings.py:18: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\r\n  warnings.warn(*args, **kwargs)\r\n/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/warnings.py:18: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\r\n  warnings.warn(*args, **kwargs)\r\n/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/warnings.py:18: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\r\n  warnings.warn(*args, **kwargs)\r\nd-12-3-1:47016:47016 [0] NCCL INFO Bootstrap : Using [0]ens2f0:172.31.130.132<0>\r\nd-12-3-1:47016:47016 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\nd-12-3-1:47016:47016 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE ; OOB ens2f0:172.31.130.132<0>\r\nNCCL version 2.4.8+cuda10.1\r\nd-12-3-1:47022:47022 [1] NCCL INFO Bootstrap : Using [0]ens2f0:172.31.130.132<0>\r\nd-12-3-1:47022:47022 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\nd-12-4-1:51815:51815 [0] NCCL INFO Bootstrap : Using [0]ens2f0:172.31.130.134<0>\r\nd-12-4-1:51820:51820 [1] NCCL INFO Bootstrap : Using [0]ens2f0:172.31.130.134<0>\r\nd-12-4-1:51820:51820 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\nd-12-4-1:51815:51815 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\nd-12-4-2:51993:51993 [0] NCCL INFO Bootstrap : Using [0]ens2f0:172.31.130.135<0>\r\nd-12-4-2:51997:51997 [1] NCCL INFO Bootstrap : Using [0]ens2f0:172.31.130.135<0>\r\nd-12-3-2:43991:43991 [1] NCCL INFO Bootstrap : Using [0]ens2f0:172.31.130.133<0>\r\nd-12-3-2:43985:43985 [0] NCCL INFO Bootstrap : Using [0]ens2f0:172.31.130.133<0>\r\nd-12-4-2:51993:51993 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\nd-12-4-2:51997:51997 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\nd-12-3-2:43991:43991 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\nd-12-3-2:43985:43985 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\nd-12-3-1:47022:47022 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE ; OOB ens2f0:172.31.130.132<0>\r\nd-12-3-1:47022:47036 [1] NCCL INFO Setting affinity for GPU 1 to ffff,f00000ff,fff00000\r\nd-12-4-2:51993:51993 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE ; OOB ens2f0:172.31.130.135<0>\r\nd-12-4-2:51997:51997 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE ; OOB ens2f0:172.31.130.135<0>\r\nd-12-4-1:51820:51820 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE ; OOB ens2f0:172.31.130.134<0>\r\nd-12-3-2:43991:43991 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE ; OOB ens2f0:172.31.130.133<0>\r\nd-12-3-2:43985:43985 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE ; OOB ens2f0:172.31.130.133<0>\r\nd-12-4-1:51815:51815 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE ; OOB ens2f0:172.31.130.134<0>\r\nd-12-4-2:51993:52008 [0] NCCL INFO Setting affinity for GPU 0 to ffff,f00000ff,fff00000\r\nd-12-4-2:51997:52010 [1] NCCL INFO Setting affinity for GPU 1 to ffff,f00000ff,fff00000\r\nd-12-4-1:51820:51830 [1] NCCL INFO Setting affinity for GPU 1 to ffff,f00000ff,fff00000\r\nd-12-3-2:43991:44003 [1] NCCL INFO Setting affinity for GPU 1 to ffff,f00000ff,fff00000\r\nd-12-3-2:43985:44002 [0] NCCL INFO Setting affinity for GPU 0 to ffff,f00000ff,fff00000\r\nd-12-4-1:51815:51832 [0] NCCL INFO Setting affinity for GPU 0 to ffff,f00000ff,fff00000\r\nd-12-3-1:47016:47034 [0] NCCL INFO Setting affinity for GPU 0 to ffff,f00000ff,fff00000\r\nd-12-4-2:51997:52010 [1] NCCL INFO CUDA Dev 1[1], IB NIC distance :  NODE\r\nd-12-4-2:51993:52008 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  NODE\r\nd-12-4-1:51815:51832 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  NODE\r\nd-12-4-1:51820:51830 [1] NCCL INFO CUDA Dev 1[1], IB NIC distance :  NODE\r\nd-12-3-2:43991:44003 [1] NCCL INFO CUDA Dev 1[1], IB NIC distance :  NODE\r\nd-12-3-2:43985:44002 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  NODE\r\nd-12-3-1:47022:47036 [1] NCCL INFO CUDA Dev 1[1], IB NIC distance :  NODE\r\nd-12-3-1:47016:47034 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  NODE\r\nd-12-3-1:47016:47034 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7\r\nd-12-3-1:47016:47034 [0] NCCL INFO Ring 00 : 7 -> 0 [receive] via NET/IB/0\r\nd-12-4-2:51993:52008 [0] NCCL INFO Ring 00 : 5 -> 6 [receive] via NET/IB/0\r\nd-12-3-2:43985:44002 [0] NCCL INFO Ring 00 : 1 -> 2 [receive] via NET/IB/0\r\nd-12-4-1:51815:51832 [0] NCCL INFO Ring 00 : 3 -> 4 [receive] via NET/IB/0\r\nd-12-3-2:43985:44002 [0] NCCL INFO Ring 00 : 2[0] -> 3[1] via direct shared memory\r\nd-12-4-2:51993:52008 [0] NCCL INFO Ring 00 : 6[0] -> 7[1] via direct shared memory\r\nd-12-4-1:51815:51832 [0] NCCL INFO Ring 00 : 4[0] -> 5[1] via direct shared memory\r\nd-12-3-1:47016:47034 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via direct shared memory\r\nd-12-4-2:51997:52010 [1] NCCL INFO Ring 00 : 7 -> 0 [send] via NET/IB/0\r\nd-12-3-2:43991:44003 [1] NCCL INFO Ring 00 : 3 -> 4 [send] via NET/IB/0\r\nd-12-4-1:51820:51830 [1] NCCL INFO Ring 00 : 5 -> 6 [send] via NET/IB/0\r\nd-12-3-1:47022:47036 [1] NCCL INFO Ring 00 : 1 -> 2 [send] via NET/IB/0\r\nd-12-3-2:43991:44003 [1] NCCL INFO Ring 00 : 3[1] -> 2[0] via direct shared memory\r\nd-12-4-1:51820:51830 [1] NCCL INFO Ring 00 : 5[1] -> 4[0] via direct shared memory\r\nd-12-4-2:51997:52010 [1] NCCL INFO Ring 00 : 7[1] -> 6[0] via direct shared memory\r\nd-12-3-1:47022:47036 [1] NCCL INFO Ring 00 : 1[1] -> 0[0] via direct shared memory\r\nd-12-4-1:51820:51830 [1] NCCL INFO Trees [0] 4->5->-1/-1/-1\r\nd-12-4-2:51997:52010 [1] NCCL INFO Trees [0] 6->7->-1/-1/-1\r\nd-12-3-2:43991:44003 [1] NCCL INFO Trees [0] 2->3->-1/-1/-1\r\nd-12-3-1:47022:47036 [1] NCCL INFO Trees [0] 0->1->-1/-1/-1\r\nd-12-4-1:51820:51830 [1] NCCL INFO comm 0x7fa5380022f0 rank 5 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\nd-12-3-2:43991:44003 [1] NCCL INFO comm 0x7ff3f40022f0 rank 3 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\nd-12-4-2:51997:52010 [1] NCCL INFO comm 0x7f84600022f0 rank 7 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\nd-12-3-1:47022:47036 [1] NCCL INFO comm 0x7f1f780022f0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\nd-12-4-1:51815:51832 [0] NCCL INFO Ring 00 : 2 -> 4 [receive] via NET/IB/0\r\nd-12-3-2:43985:44002 [0] NCCL INFO Ring 00 : 2 -> 4 [send] via NET/IB/0\r\nd-12-4-2:51993:52008 [0] NCCL INFO Ring 00 : 6 -> 4 [send] via NET/IB/0\r\nd-12-3-1:47016:47034 [0] NCCL INFO Ring 00 : 4 -> 0 [receive] via NET/IB/0\r\nd-12-4-1:51815:51832 [0] NCCL INFO Ring 00 : 6 -> 4 [receive] via NET/IB/0\r\nd-12-3-2:43985:44002 [0] NCCL INFO Ring 00 : 4 -> 2 [receive] via NET/IB/0\r\nd-12-4-1:51815:51832 [0] NCCL INFO Ring 00 : 4 -> 0 [send] via NET/IB/0\r\nd-12-4-2:51993:52008 [0] NCCL INFO Ring 00 : 4 -> 6 [receive] via NET/IB/0\r\nd-12-3-1:47016:47034 [0] NCCL INFO Ring 00 : 0 -> 4 [send] via NET/IB/0\r\nd-12-4-1:51815:51832 [0] NCCL INFO Ring 00 : 0 -> 4 [receive] via NET/IB/0\r\nd-12-3-1:47016:47034 [0] NCCL INFO Trees [0] -1->0->1/4/-1\r\nd-12-3-1:47016:47034 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees enabled up to size 79999\r\nd-12-4-1:51815:51832 [0] NCCL INFO Ring 00 : 4 -> 2 [send] via NET/IB/0\r\nd-12-3-1:47016:47034 [0] NCCL INFO comm 0x7fa36c0022f0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\nd-12-3-1:47016:47016 [0] NCCL INFO Launch mode Parallel\r\nd-12-4-1:51815:51832 [0] NCCL INFO Ring 00 : 4 -> 6 [send] via NET/IB/0\r\nd-12-3-2:43985:44002 [0] NCCL INFO Trees [0] 4->2->3/-1/-1\r\nd-12-3-2:43985:44002 [0] NCCL INFO comm 0x7febb00022f0 rank 2 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\nd-12-4-1:51815:51832 [0] NCCL INFO Trees [0] 0->4->5/2/6\r\nd-12-4-2:51993:52008 [0] NCCL INFO Trees [0] 4->6->7/-1/-1\r\nd-12-4-1:51815:51832 [0] NCCL INFO comm 0x7fab7c0022f0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\nd-12-4-2:51993:52008 [0] NCCL INFO comm 0x7f2bec0022f0 rank 6 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\nINFO:lightning:Set SLURM handle signals.\r\nINFO:lightning:Set SLURM handle signals.\r\nINFO:lightning:Set SLURM handle signals.\r\nINFO:lightning:\r\n  | Name    | Type   | Params\r\n-------------------------------\r\n0 | layer_1 | Linear | 100 K \r\n1 | layer_2 | Linear | 33 K  \r\n2 | layer_3 | Linear | 2 K   \r\nINFO:lightning:Set SLURM handle signals.\r\nINFO:lightning:Set SLURM handle signals.\r\nINFO:lightning:Set SLURM handle signals.\r\nINFO:lightning:Set SLURM handle signals.\r\nINFO:lightning:Set SLURM handle signals.\r\nspr= 5 snr= 2 sng= 2 gpu_idx= 1\r\nrank= 5  world_size= 8\r\nRoot node= d-12-3-1\r\nspr= 6 snr= 3 sng= 2 gpu_idx= 0\r\nrank= 6  world_size= 8\r\nRoot node= d-12-3-1\r\nspr= 0 snr= 0 sng= 2 gpu_idx= 0\r\nrank= 0  world_size= 8\r\nRoot node= d-12-3-1\r\nspr= 3 snr= 1 sng= 2 gpu_idx= 1\r\nrank= 3  world_size= 8\r\nRoot node= d-12-3-1\r\nspr= 4 snr= 2 sng= 2 gpu_idx= 0\r\nrank= 4  world_size= 8\r\nRoot node= d-12-3-1\r\nspr= 1 snr= 0 sng= 2 gpu_idx= 1\r\nrank= 1  world_size= 8\r\nRoot node= d-12-3-1\r\nspr= 2 snr= 1 sng= 2 gpu_idx= 0\r\nrank= 2  world_size= 8\r\nRoot node= d-12-3-1\r\nspr= 7 snr= 3 sng= 2 gpu_idx= 1\r\nrank= 7  world_size= 8\r\nRoot node= d-12-3-1\r\n/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/warnings.py:18: RuntimeWarning: Displayed epoch numbers in the progress bar start from \"1\" until v0.6.x, but will start from \"0\" in v0.8.0.\r\n  warnings.warn(*args, **kwargs)\r\nEpoch 2: 100%|##########| 118/118 [00:19<00:00,  5.98it/s, loss=0.197, v_num=436408]\r\nbefore lsw: self.proc_rank= 0                             \r\nload_spawn_weights called for self.proc_rank= 0\r\nbefore lsw: self.proc_rank= 0\r\nload_spawn_weights called for self.proc_rank= 0\r\nTraceback (most recent call last):\r\n  File \"pl.py\", line 119, in <module>\r\n    main()\r\n  File \"pl.py\", line 111, in main\r\n    trainer.fit(model)\r\n  File \"/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 697, in fit\r\n    self.load_spawn_weights(model)\r\n  File \"/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 376, in load_spawn_weights\r\n    loaded_model = original_model.__class__.load_from_checkpoint(path)\r\n  File \"/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\", line 1504, in load_from_checkpoint\r\n    checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\r\n  File \"/state/partition1/llgrid/pkg/anaconda/anaconda3-2020b/lib/python3.6/site-packages/torch/serialization.py\", line 525, in load\r\n    with _open_file_like(f, 'rb') as opened_file:\r\n  File \"/state/partition1/llgrid/pkg/anaconda/anaconda3-2020b/lib/python3.6/site-packages/torch/serialization.py\", line 212, in _open_file_like\r\n    return _open_file(name_or_buffer, mode)\r\n  File \"/state/partition1/llgrid/pkg/anaconda/anaconda3-2020b/lib/python3.6/site-packages/torch/serialization.py\", line 193, in __init__\r\n    super(_open_file, self).__init__(open(name, mode))\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/gridsan/groups/anaconda/dpc/lightning/__temp_weight_ddp_end.ckpt'\r\nbefore lsw: self.proc_rank= 0\r\nload_spawn_weights called for self.proc_rank= 0\r\nTraceback (most recent call last):\r\n  File \"pl.py\", line 119, in <module>\r\n    main()\r\n  File \"pl.py\", line 111, in main\r\n    trainer.fit(model)\r\n  File \"/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 697, in fit\r\n    self.load_spawn_weights(model)\r\n  File \"/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 376, in load_spawn_weights\r\n    loaded_model = original_model.__class__.load_from_checkpoint(path)\r\n  File \"/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\", line 1504, in load_from_checkpoint\r\n    checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\r\n  File \"/state/partition1/llgrid/pkg/anaconda/anaconda3-2020b/lib/python3.6/site-packages/torch/serialization.py\", line 525, in load\r\n    with _open_file_like(f, 'rb') as opened_file:\r\n  File \"/state/partition1/llgrid/pkg/anaconda/anaconda3-2020b/lib/python3.6/site-packages/torch/serialization.py\", line 212, in _open_file_like\r\n    return _open_file(name_or_buffer, mode)\r\n  File \"/state/partition1/llgrid/pkg/anaconda/anaconda3-2020b/lib/python3.6/site-packages/torch/serialization.py\", line 193, in __init__\r\n    super(_open_file, self).__init__(open(name, mode))\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/gridsan/groups/anaconda/dpc/lightning/__temp_weight_ddp_end.ckpt'\r\nbefore lsw: self.proc_rank= 0\r\nload_spawn_weights called for self.proc_rank= 0\r\nTraceback (most recent call last):\r\n  File \"pl.py\", line 119, in <module>\r\n    main()\r\n  File \"pl.py\", line 111, in main\r\n    trainer.fit(model)\r\n  File \"/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 697, in fit\r\n    self.load_spawn_weights(model)\r\n  File \"/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 376, in load_spawn_weights\r\n    loaded_model = original_model.__class__.load_from_checkpoint(path)\r\n  File \"/home/gridsan/dcampbell/.local/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\", line 1504, in load_from_checkpoint\r\n    checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\r\n  File \"/state/partition1/llgrid/pkg/anaconda/anaconda3-2020b/lib/python3.6/site-packages/torch/serialization.py\", line 525, in load\r\n    with _open_file_like(f, 'rb') as opened_file:\r\n  File \"/state/partition1/llgrid/pkg/anaconda/anaconda3-2020b/lib/python3.6/site-packages/torch/serialization.py\", line 212, in _open_file_like\r\n    return _open_file(name_or_buffer, mode)\r\n  File \"/state/partition1/llgrid/pkg/anaconda/anaconda3-2020b/lib/python3.6/site-packages/torch/serialization.py\", line 193, in __init__\r\n    super(_open_file, self).__init__(open(name, mode))\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/gridsan/groups/anaconda/dpc/lightning/__temp_weight_ddp_end.ckpt'\r\nsrun: error: d-12-4-1: task 2: Exited with exit code 1\r\nsrun: error: d-12-4-2: task 3: Exited with exit code 1\r\nsrun: error: d-12-3-1: task 0: Exited with exit code 1\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n\r\nsbatch script:\r\n```\r\n#!/bin/bash -l\r\n\r\n# SLURM SUBMIT SCRIPT\r\n#SBATCH --nodes=4\r\n#SBATCH --gres=gpu:volta:2\r\n#SBATCH --mem=0\r\n#SBATCH --time=0-02:00:00\r\n#SBATCH --partition=gaia\r\n\r\n# activate conda env\r\n#source activate $1\r\n\r\n# -------------------------\r\n# debugging flags (optional)\r\nexport NCCL_DEBUG=INFO\r\nexport PYTHONFAULTHANDLER=1\r\n\r\n# on your cluster you might need these:\r\n# set the network interface\r\n# export NCCL_SOCKET_IFNAME=^docker0,lo\r\n\r\n# might need the latest cuda\r\n# module load NCCL/2.4.7-1-cuda.10.0\r\n# -------------------------\r\n\r\nmodule load mpi/openmpi-4.0\r\nmodule load anaconda/2020b\r\n\r\nexport MASTER_PORT=`comm -23 <(seq 12000 18000 | sort) <(ss -Htan | awk '{print $4}' | cut -d':' -f2 | sort -u) | shuf | head -n 1`\r\n\r\n# run script from above\r\nsrun python pl.py\r\n```\r\n\r\npl.py (dervied from lightning tutorial's MNIST example)\r\n```\r\nimport torch\r\nfrom torch import nn\r\nimport pytorch_lightning as pl\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torch.nn import functional as F\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import datasets, transforms\r\nimport os\r\n\r\nimport torch.multiprocessing as mp\r\n\r\nfrom localtools import slurm_torch\r\nfrom hostlist import expand_hostlist\r\n\r\nclass LightningMNISTClassifier(pl.LightningModule):\r\n\r\n  def __init__(self):\r\n    super(LightningMNISTClassifier, self).__init__()\r\n\r\n    # mnist images are (1, 28, 28) (channels, width, height) \r\n    self.layer_1 = torch.nn.Linear(28 * 28, 128)\r\n    self.layer_2 = torch.nn.Linear(128, 256)\r\n    self.layer_3 = torch.nn.Linear(256, 10)\r\n\r\n  def forward(self, x):\r\n      batch_size, channels, width, height = x.size()\r\n\r\n      # (b, 1, 28, 28) -> (b, 1*28*28)\r\n      x = x.view(batch_size, -1)\r\n\r\n      # layer 1 (b, 1*28*28) -> (b, 128)\r\n      x = self.layer_1(x)\r\n      x = torch.relu(x)\r\n\r\n      # layer 2 (b, 128) -> (b, 256)\r\n      x = self.layer_2(x)\r\n      x = torch.relu(x)\r\n\r\n      # layer 3 (b, 256) -> (b, 10)\r\n      x = self.layer_3(x)\r\n\r\n      # probability distribution over labels\r\n      x = torch.log_softmax(x, dim=1)\r\n\r\n      return x\r\n\r\n  def cross_entropy_loss(self, logits, labels):\r\n    return F.nll_loss(logits, labels)\r\n\r\n  def training_step(self, train_batch, batch_idx):\r\n      x, y = train_batch\r\n      logits = self.forward(x)\r\n      loss = self.cross_entropy_loss(logits, y)\r\n\r\n      logs = {'train_loss': loss}\r\n      return {'loss': loss, 'log': logs}\r\n\r\n  def test_step(self, test_batch, batch_idx):\r\n      x, y = test_batch\r\n      logits = self.forward(x)\r\n      loss = self.cross_entropy_loss(logits, y)\r\n      return {'test_loss': loss}\r\n\r\n  def validation_step(self, val_batch, batch_idx):\r\n      x, y = val_batch\r\n      logits = self.forward(x)\r\n      loss = self.cross_entropy_loss(logits, y)\r\n      return {'val_loss': loss}\r\n\r\n  def validation_epoch_end(self, outputs):\r\n      # called at the end of the validation epoch\r\n      # outputs is an array with what you returned in validation_step for each batch\r\n      # outputs = [{'loss': batch_0_loss}, {'loss': batch_1_loss}, ..., {'loss': batch_n_loss}] \r\n      avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n      tensorboard_logs = {'val_loss': avg_loss}\r\n      return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\r\n\r\n  def prepare_data(self):\r\n    # transforms for images\r\n    transform=transforms.Compose([transforms.ToTensor(), \r\n                                  transforms.Normalize((0.1307,), (0.3081,))])\r\n      \r\n    # prepare transforms standard to MNIST\r\n    mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\r\n    mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\r\n    \r\n    self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\r\n\r\n  def train_dataloader(self):\r\n    return DataLoader(self.mnist_train, num_workers=16, batch_size=64)\r\n\r\n  def val_dataloader(self):\r\n    return DataLoader(self.mnist_val, num_workers=16, batch_size=64)\r\n\r\n  def test_dataloader(self):\r\n    return DataLoader(self,mnist_test, num_workers=16, batch_size=64)\r\n\r\n  def configure_optimizers(self):\r\n    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n    return optimizer\r\n\r\n# train\r\ndef main ():\r\n    rank, size = slurm_torch.torch_env()\r\n    nn = ' '.join(expand_hostlist( os.environ['SLURM_NODELIST']))\r\n    os.environ['SLURM_NODELIST']=nn\r\n    model = LightningMNISTClassifier()\r\n    trainer = pl.Trainer(gpus=2, num_nodes=size, distributed_backend='ddp', max_epochs=2)\r\n    trainer.fit(model)\r\n\r\nif __name__ == '__main__':\r\n    root_dir = os.path.dirname(os.path.realpath(__file__))\r\n\r\n    # TRAIN\r\n    main()\r\n```\r\n### Expected behavior\r\n\r\nWorkers on all nodes run to completion without errors\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - Tesla V100-PCIE-32GB\r\n                - Tesla V100-PCIE-32GB\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.3\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.45.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         x86_64\r\n        - python:            3.6.10\r\n        - version:           #1 SMP Fri Apr 3 11:13:11 EDT 2020\r\n```\r\n\r\n - How you installed PyTorch: pip\r\n\r\n\r\n### Additional context\r\n\r\nI'm not positive that this isn't a user-introduced problem as I had to do a little bit of tweaking to the supplied example application in order to run in my environment.  \r\n\r\nI have added some outputs to attempt to determine what's going on, and it seems as though self.proc_rank is 0 for the python process on each node at the point that it's attempting to load the spawn weights, so the check introduced to fix #1335 isn't preventing the attempts to load a non existent file.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1637/comments",
    "author": "danpcampbell",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-27T13:42:13Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-27T16:20:49Z",
        "body": "@danpcampbell try 0.7.5?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-27T16:22:31Z",
        "body": "but yeah this is a bug, it doesn't actually affect anything though... this is just for the end of training that has this annoying crash.\r\n\r\nthis is done so that .fit() on notebooks restores the new trained weights across processes. However, on a cluster this shouldn't even be called.\r\n\r\nThe fix is to ONLY call this function if the KAGGLE or COLAB flag are detected.\r\n\r\nMind submitting a PR?"
      },
      {
        "user": "danpcampbell",
        "created_at": "2020-04-27T16:52:22Z",
        "body": "> @danpcampbell try 0.7.5?\r\n\r\nPersists in 0.7.5 - I think based on your 2nd comment that this is expected. \r\n\r\nWill next attempt the fix you suggested in 2nd comment, verify, and work up a PR"
      }
    ]
  },
  {
    "number": 1633,
    "title": "add Trainer states",
    "created_at": "2020-04-27T09:12:39Z",
    "closed_at": "2020-08-09T10:24:10Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1633",
    "body": "## 🚀 Feature\r\n\r\nAdd trainer states so in each time user get info what is happening\r\n\r\n### Motivation\r\n\r\nsimplify the return values and clean up the interface\r\n\r\n### Pitch\r\n\r\nThe state will be implemented as enum and updated according to Trainer phase:\r\n- INITIALIZE\r\n- RUNNING\r\n- FINISHED\r\n- TEARING_DOWN\r\n- TERMINATED\r\n- ...",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1633/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-27T09:13:21Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "zerogerc",
        "created_at": "2020-07-02T09:32:57Z",
        "body": "As I understood it right, the change is to add enum with phases, add corresponding updates, and logging of the current phase. @Borda I can take it as my first issue."
      },
      {
        "user": "Borda",
        "created_at": "2020-07-02T09:37:37Z",
        "body": "@zerogerc cool, that it adds an enum (with possible texts) and change the states according to the training phase...\r\nif you need anything, just ping me on slack :rabbit: "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-09T02:30:42Z",
        "body": "Don't we want to control these states with decorators? I think having the code for these state changes mixed with the core training logic is a bit distracting and fragile, don't you think?"
      },
      {
        "user": "Borda",
        "created_at": "2020-07-09T11:52:38Z",
        "body": "> Don't we want to control these states with decorators? I think having the code for these state changes mixed with the core training logic is a bit distracting and fragile, don't you think?\r\n\r\nwell, decorators would be nicer :] @awaelchli mind comment/suggest on the open PR?"
      }
    ]
  },
  {
    "number": 1619,
    "title": "Add \"checkpoint()\" method to lightningmodule",
    "created_at": "2020-04-26T16:44:00Z",
    "closed_at": "2020-07-04T18:37:03Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1619",
    "body": "Sometimes for debugging we want to produce a quick checkpoint to test loading or pretraining or something.\r\n\r\nMaybe this would be useful as a way to generate dummy checkpoints or at arbitrary intervals.\r\n\r\n```python\r\nmodel = LitModel(hparams)\r\nmodel.checkpoint()\r\n```\r\n\r\nAlthough this would not have any of the trainer state",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1619/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-06-25T18:25:45Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1618,
    "title": "Early stopping + checkpoint key",
    "created_at": "2020-04-26T16:33:19Z",
    "closed_at": "2020-07-04T18:37:04Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1618",
    "body": "Consider updating how we condition early stopping or checkpoint\r\n\r\n```python\r\nreturn {'early_stop_on': mse_loss, 'checkpoint_on': other_metric}\r\n```\r\n\r\nInstead of:\r\n```python\r\n# only if val_loss is present\r\nreturn {'val_loss': val_loss}\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1618/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "jeremyjordan",
        "created_at": "2020-04-26T16:54:51Z",
        "body": "in my opinion all of the configuration for early stopping or model checkpointing should occur in the initialization of the callback object. if we condition based on a special key in the step return, users have to change their model to modify the behavior of a callback. "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-26T17:21:51Z",
        "body": "as a shortcut, can the trainer flags already support the key?\r\n\r\nearly_stop_callback=‘val_loss’ \r\n\r\nfor example"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-25T18:25:44Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1611,
    "title": "RuntimeError: CUDA error: an illegal memory access was encountered",
    "created_at": "2020-04-26T11:39:46Z",
    "closed_at": "2020-06-26T14:06:51Z",
    "labels": [
      "bug",
      "help wanted",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1611",
    "body": "## 🐛 Bug\r\n\r\nI have 10 nvidia cards(I am using thrid card, and using `precision = 16`), when I use apex, it run wrong with output:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"baseline8_simple_event_classification_argparse.py\", line 513, in <module>\r\n    if args.do_predict:\r\n  File \"/data/username/projs/project_name/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 602, in fit\r\n    self.single_gpu_train(model)\r\n  File \"/data/username/projs/project_name/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 470, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/data/username/projs/project_name/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 830, in run_pretrain_routine\r\n    self.train()\r\n  File \"/data/username/projs/project_name/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 343, in train\r\n    self.run_training_epoch()\r\n  File \"/data/username/projs/project_name/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 413, in run_training_epoch\r\n    output = self.run_training_batch(batch, batch_idx)\r\n  File \"/data/username/projs/project_name/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 562, in run_training_batch\r\n    loss = optimizer_closure()\r\n  File \"/data/username/projs/project_name/.venv/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 544, in optimizer_closure\r\n    model_ref.backward(self, closure_loss, optimizer, opt_idx)\r\n  File \"/data/username/projs/project_name/.venv/lib/python3.7/site-packages/pytorch_lightning/core/hooks.py\", line 148, in backward\r\n    scaled_loss.backward()\r\n  File \"/data/username/anaconda3/lib/python3.7/contextlib.py\", line 119, in __exit__\r\n    next(self.gen)\r\n  File \"/data/username/projs/project_name/apex/apex/amp/handle.py\", line 127, in scale_loss\r\n    should_skip = False if delay_overflow_check else loss_scaler.update_scale()\r\n  File \"/data/username/projs/project_name/apex/apex/amp/scaler.py\", line 200, in update_scale\r\n    self._has_overflow = self._overflow_buf.item()\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\n```\r\nactually, when `export CUDA_VISIBLE_DEVICES=3` and using `gpu=[0]`, it does not runing into error. It maybe a bug? or something\r\n\r\n\r\n### To Reproduce\r\n\r\nNone\r\n\r\n#### Code sample\r\n\r\nNone\r\n\r\n### Environment\r\n\r\n: python collect_env_details.py\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.18.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.1\r\n        - tensorboard:       2.1.1\r\n        - tqdm:              4.44.1\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                -\r\n        - processor:         x86_64\r\n        - python:            3.7.0\r\n        - version:           #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020\r\n\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1611/comments",
    "author": "menghuu",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-26T11:40:21Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-26T13:19:11Z",
        "body": "1. update to the latest pytorch (which uses native amp, not apex). (install nightly or the repo)\r\n2. update to the latest lightning.\r\n3. try again"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-13T14:01:39Z",
        "body": "this seems to be related to mixing apex and cuda somehow.\r\npytorch/pytorch#21819"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-26T14:06:51Z",
        "body": "will reopen if still an issue, but sounds like an apex+pytorch version discrepancy"
      }
    ]
  },
  {
    "number": 1607,
    "title": "Tensorboard loss graph differs from command-line output when using accumulated gradients",
    "created_at": "2020-04-26T06:03:49Z",
    "closed_at": "2020-07-04T07:37:03Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1607",
    "body": "## 🐛 Bug\r\n\r\nTensorboard loss graph differs from command-line output when using accumulated gradients\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run a model with accumulated gradients\r\n2. Compare printed loss to tensorboard loss\r\n3. See error\r\n\r\n### Expected behavior\r\n\r\nThe loss displayed via tensorboard should agree with the command-line output.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2070 with Max-Q Design\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.5.0\r\n        - pytorch-lightning: 0.7.3\r\n        - tensorboard:       2.2.1\r\n        - tqdm:              4.45.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.2\r\n        - version:           #41~1586789791~19.10~9593806-Ubuntu SMP Mon Apr 13 17:50:40 UTC\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1607/comments",
    "author": "GuardedAirplane",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-06-25T07:12:49Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1603,
    "title": "remove TrainerCallbackConfigMixin",
    "created_at": "2020-04-25T14:03:13Z",
    "closed_at": "2020-07-03T15:31:08Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1603",
    "body": "## 🚀 Feature\r\nCallbacks should be initialized and configured in their respective classes.\r\n\r\n### Motivation\r\nThe config mixin is opaque and reaches into callback objects which should be able to be self-contained.\r\n\r\n### Pitch\r\nMove the configuration setup to the actual callback classes.\r\n\r\n### Alternatives\r\nKeep the mixin to construct the objects and insert them into the callbacks list, but remove any of the other setup. The model checkpoint callback does a lot of configuration that should be happening within the class.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1603/comments",
    "author": "jeremyjordan",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-06-24T15:02:22Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1566,
    "title": "Batch being moved to gpu repeatedly with multiple optimizers and single gpu training",
    "created_at": "2020-04-22T21:32:23Z",
    "closed_at": "2020-04-23T18:28:21Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1566",
    "body": "If you have multiple optimizers, then transfer_batch_to_gpu winds up getting called once per opt_idx, and the batch is copied each time via copy.copy(batch) in training_forward. Why copy the batch when there is only a single gpu? By removing the copy.copy() my GAN model moves from 8.53it/s to 9.25it/s. Pretty significant speedup.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1566/comments",
    "author": "karlinjf",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-04-22T22:15:11Z",
        "body": "amazing find! mind submitting a PR?"
      },
      {
        "user": "karlinjf",
        "created_at": "2020-04-22T22:44:28Z",
        "body": "Sure, will do."
      }
    ]
  },
  {
    "number": 1554,
    "title": "fast_dev_run without val dataloader crashes",
    "created_at": "2020-04-22T02:50:09Z",
    "closed_at": "2020-05-16T05:14:06Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1554",
    "body": "",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1554/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-16T05:14:06Z",
        "body": "fixed by #1779, right?"
      }
    ]
  },
  {
    "number": 1551,
    "title": "Auto fix silly mistakes",
    "created_at": "2020-04-21T18:33:11Z",
    "closed_at": "2020-07-03T21:31:07Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1551",
    "body": "1. Detect when user has shuffle=True for validation and test data\r\n2. Detect when a layer does not use bias=False right before a batchnorm.\r\n\r\nAnyone interested in adding these checks?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1551/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "DavidAdamczyk",
        "created_at": "2020-04-22T08:53:45Z",
        "body": "Yes. I would like to add these checks."
      },
      {
        "user": "geekswaroop",
        "created_at": "2020-04-23T14:16:49Z",
        "body": "@williamFalcon  I would like to take up this issue. Could you point out where exactly the shuffling problem arises?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-25T20:49:35Z",
        "body": "I think I know a cool way to do 2)\r\nMind if I self assign myself?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-24T20:59:08Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1549,
    "title": "Unwanted accumulate_grad_batches behavior",
    "created_at": "2020-04-21T17:11:52Z",
    "closed_at": "2020-08-15T19:06:38Z",
    "labels": [
      "bug",
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1549",
    "body": "## 🐛 Bug\r\n\r\nWhen using the flag `accumulate_grad_batches` for the trainer, if an action is to be performed at the last mini-batch it isn't done.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. In `on_after_backward`, define some logic if we are at the last mini-batch\r\n``` \r\nif self.__nbbatch -1 <= self.__batchidx:\r\n some_param.grad += gradient_penalty\r\n```\r\n2. Run with and without the flag\r\n3. If running with the flag, the gradient penalty has not effect (the optimizer probably didn't take a step for the last mini-batch)\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\nSee above. \r\n### Expected behavior\r\n\r\nI manually compute a gradient penalty that is applied only at the last mini-batch of an epoch. Using the flag shouldn't break this behavior.\r\n\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\nPyTorch version: 1.6.0.dev20200403\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: CentOS Linux release 7.7.1908 (Core)\r\nGCC version: (Homebrew GCC 5.5.0_7) 5.5.0\r\nCMake version: version 3.13.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: \r\nGPU 0: Tesla P100-SXM2-16GB\r\nGPU 1: Tesla P100-SXM2-16GB\r\nGPU 2: Tesla P100-SXM2-16GB\r\nGPU 3: Tesla P100-SXM2-16GB\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] kmeans-pytorch            0.2                      pypi_0    pypi\r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] pytorch                   1.6.0.dev20200403 py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch-nightly\r\n[conda] pytorch-lightning         0.7.3                    pypi_0    pypi\r\n[conda] pytorch-memlab            0.0.4                    pypi_0    pypi\r\n[conda] pytorch-pcen              0.0.1                    pypi_0    pypi\r\n[conda] torchvision               0.6.0.dev20200403      py37_cu101    pytorch-nightly\r\n\r\n### Additional context\r\n\r\nIt isn't a bug per se, but it should at least be a documented behavior, ideally controllable with a flag.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1549/comments",
    "author": "sebastienwood",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-06-20T17:44:33Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-26T14:04:57Z",
        "body": "@sebastienwood thanks for bringing this up! we're looking at it for next sprint"
      }
    ]
  },
  {
    "number": 1531,
    "title": "add_argparse_args --gpus 1 error",
    "created_at": "2020-04-19T22:54:31Z",
    "closed_at": "2020-07-14T15:16:04Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1531",
    "body": "When using \r\n```python\r\nparser = Trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\n\r\n# \r\nTrainer.from_argparse_args(args)\r\n```\r\n```bash\r\npython main.py --gpus 1\r\n```\r\n\r\nThis fails because it thinks we requested GPU at index 1",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1531/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-06-05T10:03:57Z",
        "body": "is this still valid? :rabbit: "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-14T15:16:04Z",
        "body": "nope, this is fixed on master. \r\nI just tested it on my single gpu machine and it runs as expected on gpu 0. "
      }
    ]
  },
  {
    "number": 1522,
    "title": "Performance drop when activating gradient clipping",
    "created_at": "2020-04-17T23:15:19Z",
    "closed_at": "2020-04-19T03:07:16Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1522",
    "body": "Hello all,\r\n\r\nI experienced a substantial drop in computation time when activating gradient clipping (by passing a non-zero value to the keyword argument `gradient_clip_val` when initializing the Trainer).\r\n\r\nI noticed that in the current implementation of the `clipping_gradient` method in pytorch-lightning/trainer/training_tricks.py redundant computations are made by first computing the 2-norm and second squaring this result, which could be shortened by computing the sum of squares directly. This saves one square root and squaring operation per parameter set.\r\n\r\nBest,\r\nJonas\r\n\r\n### Environment\r\n\r\n```\r\ncuda:\r\n\tGPU:\r\n\tavailable:           False\r\n\tversion:             None\r\npackages:\r\n\tnumpy:               1.18.1\r\n\tpyTorch_debug:       False\r\n\tpyTorch_version:     1.4.0\r\n\tpytorch-lightning:   0.7.4-dev\r\n\ttensorboard:         2.2.1\r\n\ttqdm:                4.45.0\r\nsystem:\r\n\tOS:                  Darwin\r\n\tarchitecture:\r\n\t\t64bit\r\n\t\t\r\n\tprocessor:           i386\r\n\tpython:              3.8.2\r\n\tversion:             Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64\r\n```\r\n\r\n### Additional context\r\n\r\nI trained a relatively small (two-layered) MLP on MNIST; perhaps this performance drop does not become that apparent when training on larger network architectures.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1522/comments",
    "author": "Jonas-Jaeger",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-17T23:15:56Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Jonas-Jaeger",
        "created_at": "2020-04-17T23:20:13Z",
        "body": "Unfortunately, it seems that I do not have push access to this repository to push the proposed fix and create a pull request for this issue."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-17T23:42:13Z",
        "body": "fork the repo\r\nmake the fix\r\nsubmit a PR\r\n:)"
      }
    ]
  },
  {
    "number": 1501,
    "title": "Rank Zero Property Mixin",
    "created_at": "2020-04-15T15:23:12Z",
    "closed_at": "2020-05-04T00:58:51Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1501",
    "body": "## 🚀 Feature\r\n\r\n**This is an alternative to the change proposed in #1408 in case the global variable approach doesn't work.** \r\nI propose to add a mixin class for the rank property, e.g., named RankZeroPropertyMixin\r\n\r\n### Motivation\r\n\r\nThere are some parts of the code base that use a rank property in combination with the `@rank_zero_only` decorator. Refactoring this into a mixin would avoid code duplication and it would make it straightforward to add to a new callback for example. \r\nPR #1408 already solves the problem of code duplication.\r\n\r\n### Pitch\r\n\r\n```python\r\nclass RankZeroPropertyMixin:\r\n    def __init__(self):\r\n        self._rank = 0\r\n\r\n    @property\r\n    def rank(self) -> int:\r\n        return self._rank\r\n\r\n    @rank.setter\r\n    def rank(self, value: int) -> None:\r\n        self._rank = value\r\n\r\n```\r\nIn the Trainer init or distributed_parts, we will check each callback, logger for the rank property and set it to the appropriate value.\r\n\r\nThen, when we add a new callback:\r\n```python\r\nclass NewFancyCallback(RankZeroPropertyMixin, Callback):\r\n    def __init__(self):\r\n        ...\r\n\r\n    @rank_zero_only\r\n    def on_train_start():\r\n        print('only on rank 0')\r\n```\r\nThis does not just apply to callbacks of course, could be added to Logger, etc.\r\n\r\n### Alternatives\r\n\r\nLeave as is or go with #1408. In the future, Lightning will probably add more callbacks and features that are restricted to rank 0, so it would lead to code duplication.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1501/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-04-17T20:39:31Z",
        "body": "I think that we stay with global var for rank, right? @williamFalcon "
      }
    ]
  },
  {
    "number": 1487,
    "title": "ImportError: cannot import name 'WandbLogger' from 'pytorch_lightning.loggers'",
    "created_at": "2020-04-14T13:19:28Z",
    "closed_at": "2020-04-15T07:15:24Z",
    "labels": [
      "bug",
      "help wanted",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1487",
    "body": "## 🐛 Bug\r\n\r\nI tried importing wandb logger from pytorch_lightning.loggers and got import error.\r\n\r\n#### Code sample\r\n```python\r\nfrom pytorch_lightning.loggers import WandbLogger\r\n```\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Windows 10\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.7.7\r\n - CUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.105\r\n - GPU models and configuration: 1050ti",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1487/comments",
    "author": "braindotai",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-14T13:20:10Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "olineumann",
        "created_at": "2020-04-14T13:54:36Z",
        "body": "What is the exact error message? Which lightning version is installed?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-14T16:55:09Z",
        "body": "`from pytorch_lightning.loggers.wandb import WandbLogger` also doesn't work?"
      },
      {
        "user": "lezwon",
        "created_at": "2020-04-14T17:16:00Z",
        "body": "You need to install wandb package first. \r\n`pip install  wandb`"
      },
      {
        "user": "Borda",
        "created_at": "2020-04-14T17:23:04Z",
        "body": "You may also install all extra libs `pip install -r requirements-extra.txt`\r\n@braindotai feel free to re-open if needed :rabbit: "
      },
      {
        "user": "lezwon",
        "created_at": "2020-04-15T06:13:08Z",
        "body": "Guess it would be helpful if there was a error displayed suggesting to install the logger package while importing. I had this same issue while using comet logger and wasn't sure what was wrong."
      },
      {
        "user": "braindotai",
        "created_at": "2020-04-15T07:01:20Z",
        "body": ">  I had this same issue while using comet logger and wasn't sure what was wrong.\r\n\r\nActually I opened my code in vs code, and after that, I installed wandb and did the logging. It needed a restart to the editor to setup wandb properly with my editor command line, and after the restart, the error was gone indeed. I guess that's what happened with you as well.\r\n\r\nThe issue is solved. Thanks to everyone for the quick response. "
      }
    ]
  },
  {
    "number": 1479,
    "title": "validation_epoch_end behavior with DDP",
    "created_at": "2020-04-13T19:27:22Z",
    "closed_at": "2020-06-01T15:00:35Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1479",
    "body": "I might be misunderstanding how PL works, but when using DDP my validation_epoch_end argument still contains batches from single GPUs, and I thought they would be collated from all GPUs.  \r\nE.g. My validation dataset has 888 images, but when I validate on 8 GPUs (batch size of 1), I only get 111 batches in validation_epoch_end.  \r\nIf that's correct, how can I produce metrics that combine information from all GPUs?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1479/comments",
    "author": "VitorGuizilini",
    "comments": [
      {
        "user": "WSzP",
        "created_at": "2020-04-14T09:45:44Z",
        "body": "` validation_step` operates on a single batch of data from the validation set. \r\n`validation_epoch_end` is called at the end of the validation epoch with the outputs of all validation steps. \r\nSo I believe your problem lies at validation_step. Can you show us your validation step, ideally the whole model?\r\n"
      },
      {
        "user": "VitorGuizilini",
        "created_at": "2020-04-14T15:20:24Z",
        "body": "I asked around and apparently that is the intended behaviour right now, i.e. validation_epoch_end is per-process, and we cannot access global information for metrics or logging. I was able to solve this by doing the reduce_all myself, with something like this:\r\n\r\n```\r\nmetrics[key] = metrics[key].to('cuda:{}'.format(self.trainer.proc_rank))\r\ndist.all_reduce(metrics[key])\r\nmetrics[key] /= self.trainer.world_size\r\n```\r\nNot sure why I had to explicitly cast the tensors to their processes (their devices were all set to -1)."
      },
      {
        "user": "xiadingZ",
        "created_at": "2020-05-05T14:24:24Z",
        "body": "Also want native support of this ability. such as adding an argument to `validation_epoch_end `, when it's set to `True`, then metrics returned by `validation_epoch_end ` will automatically be `all_reduce` and logged, instead of only metrics on each process"
      },
      {
        "user": "hocop",
        "created_at": "2021-03-25T12:52:27Z",
        "body": "I am using the latest version `1.2.2` and I get the same behavior.\r\n@williamFalcon I see this was fixed in #2029. But I did not find how to make `validation_epoch_end` receive batches from all gpus.\r\nCould someone please give me a hint what can I do to fix this?"
      }
    ]
  },
  {
    "number": 1476,
    "title": "Learning rate scheduler should step after each optimizer step",
    "created_at": "2020-04-13T17:57:32Z",
    "closed_at": "2020-04-20T12:03:53Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1476",
    "body": "## 🐛 Bug\r\n\r\nI'm not sure that this is a bug or if it is a deliberate design decision, but right now the learning rate schedule gets updated at every \"step\" which actually corresponds to every forward pass. I think a more standard implementation would have the learning rate scheduler \"step\" interval correspond to being updated every backwards pass. This has caused me a lot of problems with instability as I did not realize that using standard learning rate warmups of say 16000 steps would actually only warm up for 1000 steps if I set `accumulate_grad_batches=16`.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1476/comments",
    "author": "rmrao",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-04-13T18:00:27Z",
        "body": "good point. it should be every backward pass as you mention."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-13T18:00:37Z",
        "body": "mind submitting  PR?"
      },
      {
        "user": "rmrao",
        "created_at": "2020-04-13T18:06:12Z",
        "body": "Sure, will do."
      }
    ]
  },
  {
    "number": 1472,
    "title": "Load model from checkpoint when model is not instantiated in __init__",
    "created_at": "2020-04-13T14:21:55Z",
    "closed_at": "2020-12-01T11:56:18Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix",
      "discussion"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1472",
    "body": "## 🚀 Feature\r\nBe able to load a model from a checkpoint path when the model is not instantiated in __init__\r\n\r\n### Motivation\r\n\r\nImagine I can only instantiate the model after looking at the train dataset. Ex: for the creation of emb layers you need the number of categories in categorical features.\r\n\r\n### Pitch\r\n\r\nI would like an option where when loading a model from a checkpoint I could tell it I need to run prepare data first so I can instantiate the model for instance.\r\n\r\n### Alternatives\r\n\r\nThere are probably better alternatives to this to the option presented in pitch.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1472/comments",
    "author": "jpvcaetano",
    "comments": [
      {
        "user": "arroqc",
        "created_at": "2020-04-13T21:58:28Z",
        "body": "Why not initiate based on some params that you derive from the data and then pass to the module ?"
      },
      {
        "user": "jpvcaetano",
        "created_at": "2020-04-13T22:10:11Z",
        "body": "Could do that. But would need to do the preprocessing of the datasets outside of lightning. Was wondering if it could be done inside."
      },
      {
        "user": "Borda",
        "created_at": "2020-06-11T13:59:38Z",
        "body": "@jpvcaetano @arroqc have you come to a solution? :rabbit: "
      },
      {
        "user": "raynardj",
        "created_at": "2020-08-23T07:55:23Z",
        "body": "If there is checkpoint, we can use the number of embeddings deduced from 'state_dicts'?\r\n\r\n## Or a universal way of letting user define \"a way of initiating\" of Lightning module.\r\n\r\n### A decorator allowing user defining \"how to initiate\"\r\n```python\r\n... Lightning Class\r\n    @classmethod\r\n    def initer(cls,f):\r\n        def creating_module(*args,**kwargs):\r\n            # some code creating LightningModule.__init__'s args,kwargs\r\n            lm_init_args,lm_init_kwargs = f(*args,**kwargs)\r\n            module = cls(*lm_init_args,**lm_init_kwargs)\r\n            return module\r\n        return creating_module\r\n```\r\n\r\nThis still defines many processing outside of the Lightning Module, it's just more comfortable to use.\r\n\r\n### Usage\r\n* We define lightning module class as usual, then, we allow user to define how we init later, on the fly.\r\n\r\n```python\r\n@ourLightningClass.initer\r\ndef create_from_dataset(train_ds):\r\n    num_of_users = train_ds.n_users\r\n    num_of_items = train_ds.n_items\r\n    # creating the args,kwargs for lightning module here\r\n    return list(),dict(n_users = n_users,n_items = n_items, hidden_size = 100)\r\n\r\n@ourLightningClass.initer\r\ndef create_from_checkpoint(ckpt_path,hidden_size):\r\n    ckpt=torch.load(ckpt_path,location_map = 'cpu')\r\n    weights = ckpt['state_dicts']\r\n    # creating the args,kwargs for lightning module here\r\n    return list(),dict(n_users=weights[\"user_embedding\"][0],\r\n            n_items=weights[\"item_embedding\"][0], \r\n            hidden_size=hidden_size)\r\n\r\n@ourLightningClass.initer\r\ndef create_for_inference()\r\n...\r\n```\r\n### Instantiate\r\n```\r\nnet  = create_from_checkpoint(\"recommander.ckpt\",100)\r\nnet2 = create_from_dataset(train_ds)\r\n```"
      },
      {
        "user": "raynardj",
        "created_at": "2020-08-25T08:33:53Z",
        "body": "@jpvcaetano any thoughts?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-24T23:48:07Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-24T10:11:46Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 1465,
    "title": "Weight Initialization blocks learning",
    "created_at": "2020-04-12T12:08:25Z",
    "closed_at": "2020-06-24T12:04:25Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1465",
    "body": "## 🐛 Bug\r\n\r\nWhen trying to perform a custom weight initialization, such as xavier_uniform_ or orthogonal_ from torch.nn.init, the weights are kept fixed and not updated during backpropagation.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Create a simple model, even with just FC layers and ReLU \r\n2. Initialize the weight with torch.nn.init.xavier_uniform_ or another method\r\n3. Start Training\r\n4. Weights are not getting updated\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n### Expected behavior\r\n\r\nI'd expect the initialization not to block the training process\r\n\r\n### Environment\r\n\r\n```\r\npackages:\r\nnumpy:               1.17.2\r\npyTorch_debug:       False\r\npyTorch_version:     1.3.0\r\npytorch-lightning:   0.7.3\r\ntensorboard:         2.0.0\r\ntqdm:                4.45.0\r\nsystem:\r\nOS:                  Linux\r\narchitecture:\r\n64bit\r\nprocessor:           x86_64\r\npython:              3.6.9\r\nversion:             #163-Ubuntu SMP Mon Sep 24 13:14:43 UTC 2018\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1465/comments",
    "author": "Giuseppe5",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-12T12:09:05Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "jeffling",
        "created_at": "2020-04-15T00:16:33Z",
        "body": "Do you have any code to share? I couldn't repro"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-24T08:43:07Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-24T12:04:25Z",
        "body": "Your model/algorithm might just be sensitive to the initialization. I just tried the template model from this repo (gpu_template.py) and tried different initializations in the `__init__()` of the template model, e.g.,  \r\n ```python \r\ntorch.nn.init.uniform_(self.c_d2.weight, -1, 1)\r\ntorch.nn.init.uniform_(self.c_d1.weight, -1, 1)\r\ntorch.nn.init.uniform_(self.c_d1.bias, -1, 1)\r\ntorch.nn.init.uniform_(self.c_d2.bias, -1, 1)\r\n```\r\nand it seems all of the methods I tried are worse than the default one used by PyTorch but if I wait long enough, the loss goes down to zero.\r\nFeel free to re-open if you find evidence that this is a problem with PL. "
      },
      {
        "user": "Demirrr",
        "created_at": "2022-03-31T13:36:59Z",
        "body": "Hello @awaelchli,\r\n\r\ncurrently, I am experiencing the same issue described by @Giuseppe5 .\r\n\r\nIn the context of neural arhictecture search, I would like to increase the embedding vector size by 4 after each epoch ends. \r\n\r\n```python\r\nclass KGE(pl.LightningModule):\r\n     ....\r\ndef training_epoch_end(self, training_step_outputs):\r\n       ....\r\n       self.embedding_dim += 4\r\n       self.emb_ent_real = nn.Embedding(self.num_entities,self.embedding_dim)\r\n       self.emb_rel_real = nn.Embedding(self.num_relations, self.embedding_dim)\r\n```\r\n\r\nDuring the training process, `self.emb_ent_real` and ` self.emb_rel_real` are not updated, i.e., gradient of the loss w.r.t. parameters (`self.emb_ent_real`, `self.emb_rel_real`) are not used in the paramter update.\r\n"
      }
    ]
  },
  {
    "number": 1461,
    "title": "Leaked semaphores with DDP training",
    "created_at": "2020-04-12T00:39:11Z",
    "closed_at": "2020-06-01T15:00:35Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1461",
    "body": "I constantly get this warning when training on an AWS instance (8 GPUs, using DDP). It does not crash, but the training hangs for a few seconds before continuing.\r\n\r\n```\r\n/usr/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 3 leaked semaphores to clean up at shutdown\r\n```\r\n\r\nI can share my docker container if necessary, as it might be an issue with library versions. \r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1461/comments",
    "author": "VitorGuizilini",
    "comments": [
      {
        "user": "VitorGuizilini",
        "created_at": "2020-04-13T20:18:44Z",
        "body": "FYI this behaviour only shows up in the latest master (8544b334e4af9caa060a280146e7d3bb10648332), if I install 0.7.3 it disappears."
      },
      {
        "user": "tullie",
        "created_at": "2020-04-19T10:43:37Z",
        "body": "@vguizilini  I haven't been able to reproduce this on latest master while running with 8 GPUS using DDP. Are you still getting the warning?"
      }
    ]
  },
  {
    "number": 1457,
    "title": "ddp causes an error when my model class has a lambda function",
    "created_at": "2020-04-11T15:25:19Z",
    "closed_at": "2020-04-11T16:03:23Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1457",
    "body": "## 🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Add ```self.fn_error = lambda x: x``` to the model (e.g., your_model).\r\n2. Run the trainer with ddp backend.\r\n4. It causes an error like 'AttributeError: Can't pickle local object 'your_model.__init__.<locals>.<lambda>'.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\nWhen I use dp backend, everything is ok.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n```\r\ncuda:\r\n\tGPU:\r\n\t\tTITAN RTX\r\n\t\tTITAN RTX\r\n\tavailable:           True\r\n\tversion:             10.2\r\npackages:\r\n\tnumpy:               1.17.2\r\n\tpyTorch_debug:       False\r\n\tpyTorch_version:     1.6.0a0+b55dee9\r\n\tpytorch-lightning:   0.7.4-dev\r\n\ttensorboard:         2.2.0\r\n\ttqdm:                4.45.0\r\nsystem:\r\n\tOS:                  Linux\r\n\tarchitecture:\r\n\t\t64bit\r\n\t\t\r\n\tprocessor:           x86_64\r\n\tpython:              3.7.4\r\n\tversion:             #86~16.04.1-Ubuntu SMP Mon Jan 20 11:02:50 UTC 2020\r\n```\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1457/comments",
    "author": "thnkim",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-11T15:26:00Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "thnkim",
        "created_at": "2020-04-11T15:29:41Z",
        "body": "It seems to be an issue on PyTorch (torch.multiprocessing)."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-11T16:03:39Z",
        "body": "this is a pytorch limitation "
      },
      {
        "user": "Borda",
        "created_at": "2020-04-14T13:29:23Z",
        "body": "can we crap the lambda with a `functools.partial`?"
      }
    ]
  },
  {
    "number": 1451,
    "title": "Disable log_save_interval",
    "created_at": "2020-04-11T00:48:45Z",
    "closed_at": "2020-04-13T03:39:18Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1451",
    "body": "What is the correct way (if there is one) to disable log_save_interval? I want to log information to WandB only at the end of each epoch, but cannot do so because logs are produced during validation steps. Even if I set log_save_interval to a very large number logs are still saved after the first validation step. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1451/comments",
    "author": "VitorGuizilini",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-04-11T14:33:15Z",
        "body": "logs only produce when you add the log key to the return dictionary "
      }
    ]
  },
  {
    "number": 1440,
    "title": "More granular callbacks",
    "created_at": "2020-04-10T09:00:56Z",
    "closed_at": "2020-07-05T13:09:42Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1440",
    "body": "## 🚀 Make callback system more granular\r\n\r\n### Motivation\r\n\r\nI am currently implementing #765 (make progress bar into a callback) and I need additional callback methods to do this.\r\n\r\n### Pitch\r\n\r\nintroduce these new callback methods:\r\n\r\n- `on_train_batch_start` (currently named `on_batch_start`)\r\n- `on_train_batch_end` (currently named `on_batch_end`)\r\n- `on_val_batch_start`\r\n- `on_val_batch_end`\r\n- `on_test_batch_start`\r\n- `on_test_batch_end`\r\n\r\nand make `on_batch_start` run on any of the above `*_start` (same for `on_batch_end`)\r\n\r\nFurther suggestions:\r\n- introduce `on_train_epoch_start`, `on_val_epoch_start`, `on_test_epoch_start` and corresponding `*_end` methods.\r\n\r\n\r\n### Alternatives\r\n\r\nKeep as is, but I don't know how to implement the  progress bar callback otherwise for validation/test updates.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1440/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-04-24T01:32:39Z",
        "body": "For the record, #1450 introduced callbacks for validation_start/end, test_start/end, and sanity_check_start/end. \r\n\r\nThe question remains whether we should rename `on_batch_*` to `on_training_batch_*` for consistency with the naming of the others."
      },
      {
        "user": "Borda",
        "created_at": "2020-04-24T07:37:24Z",
        "body": "> The question remains whether we should rename `on_batch_*` to `on_training_batch_*` for consistency with the naming of the others.\r\n\r\nthat it shall be for all batch train/valid..."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-26T12:21:20Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1421,
    "title": "run_training_batch breaks on None batch or -1 response from on_batch_start (in new 0.7.2 release)",
    "created_at": "2020-04-09T03:35:45Z",
    "closed_at": "2020-04-09T19:01:09Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1421",
    "body": "## 🐛 Bug\r\n\r\nrun_training_batch now is supposed to return a 4-tuple in 0.7.2 \r\n\r\nhowever, there are two places where it still returns a 3-tuple, which will cause the program to crash, saying \"ValueError: not enough values to unpack (expected 4, got 3)\"\r\n\r\n(training_loop.py:533)\r\n```\r\nif batch is None:\r\n    return 0, grad_norm_dic, {}\r\n``` \r\n\r\n(training_loop.py:543)\r\n```\r\nif response == -1:\r\n    return -1, grad_norm_dic, {}\r\n```\r\n\r\nvs. the standard return\r\n`return 0, grad_norm_dic, all_log_metrics, batch_output`\r\n\r\n### To reproduce\r\njust return -1 from on_batch_start\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1421/comments",
    "author": "david-alexander-white",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-04-09T12:44:13Z",
        "body": "good catch. mind submitting a PR?"
      },
      {
        "user": "david-alexander-white",
        "created_at": "2020-04-09T19:35:36Z",
        "body": "Nice, beat me to it"
      }
    ]
  },
  {
    "number": 1415,
    "title": "EarlyStopping restore_best_weights argument similar to Keras",
    "created_at": "2020-04-08T11:43:04Z",
    "closed_at": "2020-04-08T12:02:41Z",
    "labels": [
      "duplicate",
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1415",
    "body": "## 🚀 Feature\r\nEarlyStopping argument `restore_best_weights` restores model weights from the epoch with the best value of the monitored loss, similar to Keras. Would not need to run ModelCheckpoint every epoch.\r\n\r\n### Motivation\r\nGood to have to avoid even more pytorch boilerplate\r\n\r\n### Alternatives\r\nCalling `ModelCheckpoint` ever epoch\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1415/comments",
    "author": "vedal",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-08T11:43:44Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "philip30",
        "created_at": "2020-04-08T11:54:49Z",
        "body": "this is similar to my issue #1395 "
      },
      {
        "user": "Borda",
        "created_at": "2020-04-08T12:02:41Z",
        "body": "@vedal let's keep just one - #1395"
      }
    ]
  },
  {
    "number": 1412,
    "title": "Auto move input to proper device for inference",
    "created_at": "2020-04-08T05:44:46Z",
    "closed_at": "2020-06-15T21:04:34Z",
    "labels": [
      "feature",
      "help wanted",
      "discussion",
      "let's do it!"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1412",
    "body": "Does PyTorch Lightning provide abstractions for inference? In particular, does it provide ways of automatically handling the transfer to/from GPU when I call `model(x)`, or do I need to roll my own code for that?\r\n\r\n## Example Use Case\r\n\r\nI have a use case where I train a model on slices of a sliding window of an audio spectrogram (i.e., let's say 1 second chunks). When training is finished, I'd like to see the performance of the model on an entire file. Pseudocode:\r\n\r\n```python\r\n# generate training data\r\nX, Y = [], []\r\nfor audio_file in audio_files:\r\n    for x, y in sliding_window(audio_file):\r\n        X.append(x); Y.append(y)\r\nX, Y = shuffle(X, Y)  # shuffle the slices of all files\r\n\r\n# Train model on slices\r\nmodel = ExampleModel(X, Y)\r\ntrainer = Trainer(gpus=1)\r\ntrainer.fit(model)\r\n\r\n# Plot the performance on a whole test file:\r\ntest_Y = []\r\nfor x, _ in sliding_window(test_file)\r\n    test_Y.append(model(x))\r\nplt.plot(test_Y)\r\n```\r\n\r\nNotice that during training, the notion of a file is entirely gone, but when I plot my test file, I reintroduce that. Of course, in my real code, my training data `X, Y` is split into training, validation and test, as usual. The plotting step is an additional verification; sort of like putting the pieces together.\r\n\r\n## Problem\r\nWhen the model runs on the GPU, The last part of the code becomes:\r\n```python\r\n# Plot the performance on a whole test file:\r\nmodel.eval()\r\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\ntest_Y = []\r\nfor x, _ in sliding_window(test_file)\r\n    y = model(x.to(device)).cpu()\r\n    test_Y.append(y)\r\nplt.plot(test_Y)\r\n```\r\n\r\nThis isn't the end of the world, but it's not as nice as the other code that PyTorch Lightning helped me refactor. I also can't call `x.type_as(...)` since in that loop, I have no reference type that lives on the CPU/GPU that I could refer to (or maybe I can, but I haven't figured it out).\r\n\r\nA workaround to this is to save the model and load it again, on a CPU.\r\n\r\n```python\r\n# Train model on slices\r\n# ...\r\ntrainer.fit(model)\r\ntrainer.save_checkpoint(\"model.ckpt\")\r\nmodel = ExampleModel.load_from_checkpoint(\"model.ckpt\")\r\n\r\n# Plot the performance on a whole test file:\r\nmodel.eval()\r\ntest_Y = []\r\nfor x, _ in sliding_window(test_file)\r\n    test_Y.append(model(x))\r\nplt.plot(test_Y)\r\n```\r\n\r\nWhile this removes the noise of the `.to(device)` and `.cpu()` calls, it adds the overhead of having to save the model every time. I also still have to manually call `model.eval()`. The use case of running my model on an entire audio file is not for metrics but for visual inspection; as such I always only sample a few audio files. Running the model on a CPU instead of a GPU for inference thus isn't a problem.\r\n\r\n## Question\r\nIs there a more elegant way to achieve the above?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1412/comments",
    "author": "tcwalther",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-04-08T11:31:52Z",
        "body": "it may be a nice feature to have ddp/tpu also for inference time...\r\nany thoughts @PyTorchLightning/core-contributors @williamFalcon?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-09T17:10:06Z",
        "body": "We should definitely automate this! \r\nLet's do it?"
      },
      {
        "user": "Borda",
        "created_at": "2020-04-09T19:44:23Z",
        "body": "@tcwalther pls let me bring some light to it... what are the goal, always use the best you have responsible (GPU/TPU)?\r\n\r\nthinking about the inference, the case I see could be to have it as a parameter... because in my case (using notebook) I have GPU but most of the \"production\" models won't fit there so it will crash\r\nalso, you may want to keep the output on GPU so by the inference moving it back to CPU and back to GPU does not make sense\r\n"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-09T20:03:45Z",
        "body": "whenever a lightningModule is used \r\n```\r\nmodel(x)\r\n```\r\nwe put x on the proper device...\r\n\r\n@tcwalther this is what you mean no?"
      },
      {
        "user": "Borda",
        "created_at": "2020-04-09T21:08:26Z",
        "body": "> we put x on the proper device...\r\n\r\ndoes it also mean that we shall estimate if the model fits available device?\r\njust thinking about the case with a notebook with low-end GPU so the general request will return GPU is available but it does not reflect that the model does not fit in...\r\n\r\n@tcwalther ^^"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-11T13:58:37Z",
        "body": "@tcwalther @JonathanSchmidt1 it is close to #1467, right?"
      },
      {
        "user": "JonathanSchmidt1",
        "created_at": "2020-06-12T20:38:28Z",
        "body": "> @tcwalther @JonathanSchmidt1 it is close to #1467, right?\r\n\r\nYes I assume fixing this, would have also fixed my problem, although a recent update of pytorch lightning already fixed my problem and I could remove my code."
      }
    ]
  },
  {
    "number": 1395,
    "title": "Loading from best checkpoint",
    "created_at": "2020-04-06T17:25:34Z",
    "closed_at": "2020-10-25T10:06:54Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1395",
    "body": "## 🚀 Feature\r\nCurrently Pytorch lightning uses the latest version of the model for testing. In research, we want to first load the best checkpoint and do the testing from there. Also it would be good to restart from the best checkpoint after learning rate plateau as an option.\r\n\r\n### Motivation\r\n\r\nWe want the best model for training/testing. Also for NLP it is more natural to go to the best checkpoint and restart with decayed learning rate.\r\n\r\n\r\n### Alternatives\r\n\r\nManual loading and checking of the validation value (which is against lightning principal).\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1395/comments",
    "author": "philip30",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-06T17:26:16Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-08T12:07:50Z",
        "body": "this makes sense. how do you suggest to do it?  ideally you do this in lightning\r\n```\r\nmodel = Model.load_from_checkpoint(PATH)\r\ntrainer.test(model)\r\n```\r\nwhy doesn’t this fit your use case?"
      },
      {
        "user": "philip30",
        "created_at": "2020-04-08T12:18:01Z",
        "body": "There are 2 issues here:\r\n- Trainer has a training loop which involves testing (test_step, test_end) which does not behave as above [it uses the latest version of the model during training]. Meaning that I have to make a separate script for testing that load this checkpoint manually.\r\n- After loss increased in the development set, we would want to restart from best checkpoint, reduce the learning rate and restart the epoch totally from that checkpoint. Currently this can't be achieved without an external bash script that tracks the model evaluation performace and (1) kill the training if loss increased, (2) restart with decayed learning rate. Which is too much work.\r\n\r\nAt the beginning I think it is as easy as loading the checkpoint internally (during training loop). But I realized that in multi GPUs setting, these parameters need to be copied to each pytorch module inside every GPU. \r\n\r\nIn summary:\r\n- Let's implement module.restart_from_checkpoint_(.) for pytorch lightning module.\r\n\r\n\r\n\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-07T13:17:52Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "dvirginz",
        "created_at": "2020-06-13T13:17:19Z",
        "body": "This will need to have the specific checkpoint path right?\r\nIs there a way to give it the dir path, and it will load the best ckpt based on CheckpointModel obejct logic?\r\n\r\n> this makes sense. how do you suggest to do it? ideally you do this in lightning\r\n> \r\n> model = Model.load_from_checkpoint(PATH)\r\n> trainer.test(model)\r\n> \r\n> why doesn’t this fit your use case?\r\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-08-04T17:46:15Z",
        "body": "> This will need to have the specific checkpoint path right?\r\n> Is there a way to give it the dir path, and it will load the best ckpt based on CheckpointModel obejct logic?\r\n\r\nyes the path is in the Checkoint"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-24T23:48:09Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-25T10:06:54Z",
        "body": "already solved... 🐰 "
      },
      {
        "user": "GeorgeG92",
        "created_at": "2021-12-12T19:18:16Z",
        "body": "any update on this? Do we have the option to restore best model weights currently?"
      },
      {
        "user": "blacksnail789521",
        "created_at": "2023-03-04T17:20:41Z",
        "body": "Please consider adding the `restore_best_weights` functionality similar to Keras. Currently, this feature is not available."
      }
    ]
  },
  {
    "number": 1372,
    "title": "Dockerize test env",
    "created_at": "2020-04-04T12:25:17Z",
    "closed_at": "2020-08-04T17:37:35Z",
    "labels": [
      "feature",
      "help wanted",
      "ci"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1372",
    "body": "## 🚀 Feature\r\n\r\nthe simples way for speedup builds is have a docker image with all dependencies then preserving pip cache, that means we will create a docker image which will be pulled\r\n\r\n### Motivation\r\n\r\n\r\nfor that, the simples way is hawing it id docker hub as it is a native location for almost all CI\r\nthese \"devel-lightning\" docker images can be simply used by any contributor for testing lol\r\n\r\n### Pitch\r\n\r\nwe may also have a docker image with installed lightning and all requirements which would make the starting with lightning even easier and it would be also useful for people working on a production\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered if any. -->\r\n\r\n### Additional context\r\n\r\nlater I may configure a cron do this docker build every week\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1372/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "luiscape",
        "created_at": "2020-04-06T02:26:52Z",
        "body": "Given that Lightning doesn't have a lot of dependencies, I think this is a good idea. The one risk is that the image becomes stale when dependencies change and the image isn't updated. (One can solve that by updating the image every time there's an update in a dependency.)\r\n\r\nOn a related note, it seems to be that we should pin all dependency versions instead of having them use comparison terms like `>`. It's hard to know which exact version will be installed when the image is built and it also carries the risk of a dependency update breaking things. "
      },
      {
        "user": "Borda",
        "created_at": "2020-04-06T06:53:52Z",
        "body": "We will have a weekly builds and also we keep their `pip install requirements.txt` so if there is something new (missing in the Docker image) it will be installed standard way till new image is built... "
      },
      {
        "user": "Borda",
        "created_at": "2020-08-04T17:37:35Z",
        "body": "done in #1584 #2779 "
      }
    ]
  },
  {
    "number": 1366,
    "title": "ModelCheckpoint tries to remove already removed checkpoint in DDP mode",
    "created_at": "2020-04-03T16:01:47Z",
    "closed_at": "2020-04-24T21:21:01Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1366",
    "body": "##  🐛 Bug\r\n\r\nWhen training in DDP mode with ModelCheckpoint callback, the train process fails, when ModelCheckpoint callback tries to remove previous checkpoint. I assume that it was already deleted by another process.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun training with `\"ddp\"` backend and `ModelCheckpoint` callback with `save_top_k={some_number}`\r\n\r\n```Traceback (most recent call last):                                                                                   \r\n  File \"/home/myuser/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap                                                                                                 \r\n    fn(i, *args)                                                                                \r\n  File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 342, in ddp_train\r\n    self.run_pretrain_routine(model)                                                                                                        \r\n  File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 830, in run_pretrain_routine\r\n    self.train()                                                                                                                                                                                                   \r\n  File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 343, in train                                                      \r\n    self.run_training_epoch()                            \r\n  File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 452, in run_training_epoch                                                                       \r\n    self.call_checkpoint_callback()                                                                                                                                                   \r\n  File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 737, in call_checkpoint_callback\r\n    self.checkpoint_callback.on_validation_end(self, self.get_model())                                                                                                                                             \r\n  File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 204, in on_validation_end                                      \r\n    self._do_check_save(filepath, current, epoch)    \r\n  File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 221, in _do_check_save                                                                      \r\n    self._del_model(delpath)                                                                                                                                                           \r\n  File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 121, in _del_model\r\n    os.remove(filepath)                                                                                                                                                                                            \r\nFileNotFoundError: [Errno 2] No such file or directory: {PREVIOUS_CHECKPOINT_NAME}\r\n```\r\n\r\n### Expected behavior\r\n\r\nI expect that ModelCheckpoint callbacks from different DDP processes will not concurrent with each other in saving/deleteng files.\r\n\r\nI fixed by rewriting `_del_model` method of ModelCheckpoint callback:\r\n```\r\nclass DDPModelCheckpoint(ModelCheckpoint):\r\n    def _del_model(self, filepath):\r\n        try:\r\n            os.remove(filepath)\r\n        except Exception:\r\n            pass\r\n```\r\n\r\n### Environment\r\n\r\n - PyTorch Version: 1.4\r\n - OS: Ubuntu 18.04\r\n - How you installed PyTorch - `conda`\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: 2x2080Ti\r\n - pytorch-lightning version: 0.7.1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1366/comments",
    "author": "belskikh",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-03T16:02:40Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-04T08:16:38Z",
        "body": "Your suggestion to pass on an Exception is not the best, at least you should make it the specific error, i.e., FileNotFoundError. But in this case, I suggest to do simply\r\n```\r\nif os.path.isfile(filepath):\r\n    os.remove(filepath)\r\n```"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-04T08:18:29Z",
        "body": "Is there also an issue with saving? Does it save/overwrite the file in multiple processes?"
      },
      {
        "user": "iamkucuk",
        "created_at": "2020-04-06T07:34:57Z",
        "body": "I encountered that one too. From my perspective, model updates should be happening within main worker only (master worker). However, I guess each workers lightning created is trying to delete their own checkpoints. However, slave worker never created one (and it shouldn't be). I solved the problem in a similar way with @belskikh 's workaround but it did not feel right and downgraded to 0.6.0. "
      },
      {
        "user": "belskikh",
        "created_at": "2020-04-06T09:47:20Z",
        "body": "@awaelchli of course it is not the best (it may be the worse, actually) solution\r\nIt is just a fast workaround, waiting for solid fix\r\n\r\nI agree with logic, when only one ModelCheckpoint callback should save/delete weights, because all weights are the same on all nodes at the end of the training step. \r\nIt can be done somehow like this:\r\n\r\n```\r\nclass ModelCheckpoint(..., main_worker_rank: int = 0):\r\n....\r\n    def _del_model(self, filepath):\r\n        if self.main_worker_rank == dist.get_rank():\r\n             # do delete\r\n```\r\n\r\nAnd the same for the saving code.\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-06T10:00:02Z",
        "body": "@williamFalcon should Lightning do checkpoints only on rank 0? It could be a problem if writing to a shared filesystem between nodes. AFAIK the loggers already do that by only logging in process 0."
      },
      {
        "user": "belskikh",
        "created_at": "2020-04-06T11:20:09Z",
        "body": "I do not think, that it should do it only on specific rank, I think user should have ability to specify rank (node), where checkpoints will be saved"
      },
      {
        "user": "Borda",
        "created_at": "2020-04-07T19:23:25Z",
        "body": "> Your suggestion to pass on an Exception is not the best, at least you should make it the specific error, i.e., FileNotFoundError. But in this case, I suggest to do simply\r\n> \r\n> ```\r\n> if os.path.isfile(filepath):\r\n>     os.remove(filepath)\r\n> ```\r\n\r\nthis does not work in async, I have observed many times that the if pass for both but then when you try really delete it, it is missing for one of them..."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-07T19:28:48Z",
        "body": "should checkpointing be done in only one process then, like loggers?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-07T20:11:15Z",
        "body": "yup. checkpoint should only happen from world_rank = 0 gpu 0"
      },
      {
        "user": "Borda",
        "created_at": "2020-04-07T21:01:26Z",
        "body": "well, ModelCeckpoint doesn't have a rank..."
      }
    ]
  },
  {
    "number": 1322,
    "title": "Training loop temporarily hangs after every 4 steps",
    "created_at": "2020-03-31T17:35:57Z",
    "closed_at": "2020-04-05T15:07:16Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1322",
    "body": "I am porting some of my code to pytorch lightning, and everything seems to work fine. However, for some reason after every 4 training steps I see some temporary hanging (~1 second), which is severely slowing down my overall training time. Am I missing some obvious configuration?  This is my Trainer configuration:\r\n\r\n```\r\n    trainer = pl.Trainer(\r\n        gpus=8\r\n        num_nodes=1,\r\n        distributed_backend='ddp',\r\n        checkpoint_callback=False,\r\n        max_epochs=50,\r\n        max_steps=None,\r\n        progress_bar_refresh_rate=1,\r\n        check_val_every_n_epoch=1,\r\n        val_check_interval=1.0,\r\n        gradient_clip_val=0.0,\r\n        log_save_interval=0,\r\n        num_sanity_val_steps=0,\r\n        amp_level='O0',\r\n    )\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1322/comments",
    "author": "VitorGuizilini-TRI",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-31T17:36:45Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-04T12:34:00Z",
        "body": "@PyTorchLightning/core-contributors "
      },
      {
        "user": "ethanwharris",
        "created_at": "2020-04-04T12:39:03Z",
        "body": "Thanks for the issue! Would it be possible to post the code that reproduces this error? I've only seen this sort of behaviour before when the number of data loading workers is low - are you working with large data here (e.g. big images)?"
      },
      {
        "user": "VitorGuizilini",
        "created_at": "2020-04-04T16:24:16Z",
        "body": "I increased the number of workers and it works perfectly now, thank you very much! You can close this issue."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-04T16:27:14Z",
        "body": "should we throw a warning when users use few workers?"
      },
      {
        "user": "VitorGuizilini",
        "created_at": "2020-04-04T16:34:04Z",
        "body": "If possible, sure! Seems like an obvious solution now, but it could save a couple of hours for other people. :)"
      }
    ]
  },
  {
    "number": 1315,
    "title": "Automatic environment check",
    "created_at": "2020-03-31T03:53:59Z",
    "closed_at": "2020-06-13T08:51:44Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1315",
    "body": "## 🚀 Feature\r\n\r\nLightning could automatically detect a requirements.txt or environment.yml file and check if the packages in the current environment meet the specified versions. If these are not met, it could warn the user.\r\n\r\n### Motivation\r\n\r\nLightning facilitates and encourages reproducibility of research code. A feature like this could further improve this part and make a user's life easier. \r\n\r\n### Pitch\r\n\r\n- Check if there is a requirements.txt (pip, pipenv) or environment.yml (conda) file in the same path as the main script.\r\n- If there is, check the versions and warn the user if dependencies are not met.\r\n- Optional: Automatically upgrade/downgrade packages via pip / conda call (not sure if this is smart)\r\n\r\n### Alternatives\r\n\r\nKeep as is. The users have to take care of this themselves.\r\n\r\n### Additional context\r\n\r\nI have already implemented this for myself to keep track when working on multiple machines and code repositories. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1315/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-03-31T09:23:43Z",
        "body": "@awaelchli do you have something else than #1234?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-04T07:58:14Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1299,
    "title": "[Metrics] IOU",
    "created_at": "2020-03-30T13:09:22Z",
    "closed_at": "2020-06-18T13:06:32Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1299",
    "body": "## 🚀 Feature\r\nImplement (differentiable) IOU",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1299/comments",
    "author": "justusschock",
    "comments": [
      {
        "user": "j-dsouza",
        "created_at": "2020-05-27T15:35:43Z",
        "body": "I'll start taking a look at a Tensor IoU unless someone has already started"
      }
    ]
  },
  {
    "number": 1297,
    "title": "Metrics: Dice Coefficient",
    "created_at": "2020-03-30T13:07:23Z",
    "closed_at": "2020-06-16T22:32:52Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1297",
    "body": "## 🚀 Feature\r\nImplement General Dice Coefficient (multiclass)",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1297/comments",
    "author": "justusschock",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-06-16T22:32:51Z",
        "body": "done in #1488"
      }
    ]
  },
  {
    "number": 1296,
    "title": "Metrics: AUC",
    "created_at": "2020-03-30T13:05:56Z",
    "closed_at": "2020-06-13T12:47:27Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1296",
    "body": "## 🚀 Feature\r\nImplement general AUC (to be combined with other metrics like ROC)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1296/comments",
    "author": "justusschock",
    "comments": [
      {
        "user": "nickyi1990",
        "created_at": "2020-04-26T13:43:47Z",
        "body": "just use `from sklearn.metrics import roc_auc_score`, I don't think there's any need to write this function from scratch"
      },
      {
        "user": "justusschock",
        "created_at": "2020-04-27T07:06:51Z",
        "body": "Actually there is. Using the sklearn function involves the necessity to convert from `torch.Tensor` to `numpy.ndarray` before converting. While this is totally fine on cpu (no memory copy made here), this really causes a slow-down for gpu tensors, since it involves a GPU-sync. \r\nFor now #1327 already implements a sklearn version, but it shall be replaced with a native torch one."
      },
      {
        "user": "justusschock",
        "created_at": "2020-06-10T14:13:47Z",
        "body": "@Borda I reopened this, since this is meant for native implementations in #1488"
      }
    ]
  },
  {
    "number": 1295,
    "title": "Metrics: Accuracy, Precision, Recall, F1, ROC",
    "created_at": "2020-03-30T13:04:29Z",
    "closed_at": "2020-06-13T12:47:26Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1295",
    "body": "## 🚀 Feature\r\nImplement the following metrics (grouped since highly similar):\r\n- [ ] Accuracy\r\n- [ ] Precision\r\n- [ ] Recall\r\n- [ ] F1\r\n- [ ] ROC",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1295/comments",
    "author": "justusschock",
    "comments": [
      {
        "user": "xssChauhan",
        "created_at": "2020-05-20T15:24:32Z",
        "body": "@Borda @williamFalcon I can take this up if no one is working on it currently."
      },
      {
        "user": "justusschock",
        "created_at": "2020-05-22T09:20:55Z",
        "body": "there is also @cuent working on this and there is an early version in #1488 "
      },
      {
        "user": "justusschock",
        "created_at": "2020-06-10T14:12:39Z",
        "body": "@Borda I reopened this, since this is meant for native implementations in #1488 "
      }
    ]
  },
  {
    "number": 1289,
    "title": "ddp not working in conjunction with torch/nn/modules/conv.py",
    "created_at": "2020-03-30T10:13:43Z",
    "closed_at": "2020-03-31T07:36:39Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1289",
    "body": "### Common bugs:\r\nddp not working in conjunction with torch/nn/modules/conv.py\r\n\r\n## 🐛 Bug\r\n\r\nPL is cowardly refusing to serialize anything that inherits from class _ConvNd\r\nThis is probably due to the fact the pytorch is initializing tensors in __init__\r\n\r\nPlease see attached __init__ code below:\r\n\r\n\r\n### To Reproduce\r\nadd one of these models to your PL model, and try to run Trainer with DDP\r\n\r\n\r\n### Code\r\nclass _ConvNd(Module):\r\n\r\n    __constants__ = ['stride', 'padding', 'dilation', 'groups', 'bias',\r\n                     'padding_mode', 'output_padding', 'in_channels',\r\n                     'out_channels', 'kernel_size']\r\n\r\n    def __init__(self, in_channels, out_channels, kernel_size, stride,\r\n                 padding, dilation, transposed, output_padding,\r\n                 groups, bias, padding_mode):\r\n        super(_ConvNd, self).__init__()\r\n        if in_channels % groups != 0:\r\n            raise ValueError('in_channels must be divisible by groups')\r\n        if out_channels % groups != 0:\r\n            raise ValueError('out_channels must be divisible by groups')\r\n        self.in_channels = in_channels\r\n        self.out_channels = out_channels\r\n        self.kernel_size = kernel_size\r\n        self.stride = stride\r\n        self.padding = padding\r\n        self.dilation = dilation\r\n        self.transposed = transposed\r\n        self.output_padding = output_padding\r\n        self.groups = groups\r\n        self.padding_mode = padding_mode\r\n        if transposed:\r\n            self.weight = Parameter(torch.Tensor(\r\n                in_channels, out_channels // groups, *kernel_size))\r\n        else:\r\n            self.weight = Parameter(torch.Tensor(\r\n                out_channels, in_channels // groups, *kernel_size))\r\n        if bias:\r\n            self.bias = Parameter(torch.Tensor(out_channels))\r\n        else:\r\n            self.register_parameter('bias', None)\r\n        self.reset_parameters()\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1289/comments",
    "author": "nirkra",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-30T10:14:29Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-30T11:20:33Z",
        "body": "could you please share your env setting and minimal running example to reproduce? :robot: "
      },
      {
        "user": "nirkra",
        "created_at": "2020-03-30T14:05:56Z",
        "body": "Oops, apologies, the problem seems to be in the wavenet_vocoder module that was calling it.\r\nSo, it's a not a bug probably, but I would appreciate the help.\r\n\r\nUbuntu18.04, cuda==10.1, pytorch-lightning==0.7.1, torch==1.4.0, wavenet-vocoder==0.1.1\r\n\r\nThe problem seems to be in wavenet_vocoder/modules.py:\r\n````\r\ndef Conv1d(in_channels, out_channels, kernel_size, dropout=0, std_mul=4.0, **kwargs):\r\n    m = conv.Conv1d(in_channels, out_channels, kernel_size, **kwargs)\r\n    std = math.sqrt((std_mul * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\r\n    m.weight.data.normal_(mean=0, std=std)\r\n    m.bias.data.zero_()\r\n    return nn.utils.weight_norm(m)\r\n````\r\n\r\nIt's doing something similar to this:\r\n````\r\nfrom pytorch_lightning import Trainer\r\n\r\n\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torch.utils.data import DataLoader, Dataset\r\nimport math\r\n\r\nclass ToyDataset(Dataset):\r\n    def __len__(self):\r\n        return 512\r\n\r\n    def __getitem__(self, idx):\r\n        return torch.rand((128, 1)), torch.ones(1).long()\r\n\r\nclass LightConv(pl.LightningModule):\r\n    def __init__(self):\r\n        super(LightConv, self).__init__()\r\n        m = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=1, bias=True)\r\n        dropout=0\r\n        std_mul=4.0\r\n        in_channels=128\r\n        std = math.sqrt((std_mul * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\r\n        m.weight.data.normal_(mean=0, std=std)\r\n        m.bias.data.zero_()\r\n        self.cnn = nn.utils.weight_norm(m)\r\n\r\n    def prepare_data(self):\r\n            self.train_dataset = ToyDataset()\r\n            self.test_dataset = ToyDataset()\r\n            self.valid_dataset = ToyDataset()\r\n    def forward(self, x):\r\n        return self.cnn(x)\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.train_dataset, batch_size=32, num_workers=0,\r\n                          shuffle=True)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.valid_dataset, batch_size=32, num_workers=0)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.test_dataset, batch_size=32, num_workers=0)\r\n\r\n    def configure_optimizers(self):\r\n        g_optimizer = optim.Adam(self.cnn.parameters(), lr=0.0001)\r\n        return [g_optimizer]\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        out = self.forward(x)\r\n        loss = F.cross_entropy(out, y)\r\n        return {\"loss\": loss} \r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        out = self.forward(x)\r\n        loss = F.cross_entropy(out, y)\r\n        return {\"loss\": loss} \r\n\r\nif __name__ == '__main__':\r\n    net = LightConv()\r\n    trainer = Trainer(max_epochs=1,\r\n                    gpus=1, num_nodes=1, distributed_backend='ddp')\r\n    trainer.fit(net)\r\n    trainer.test(net)\r\n````"
      },
      {
        "user": "nirkra",
        "created_at": "2020-03-31T13:53:00Z",
        "body": "the problem is with nn.utils.weight_norm(m) - the pytorch implementation calculates the norm(m) durning intialization and before 'forward' is run. It therefore creates a dependency between the tensors.\r\nto work aroung it I create my own implementation of WeightNorm, based on some older version implementation:\r\n```\r\nclass WeightNorm(nn.Module):\r\n    append_g = '_g'\r\n    append_v = '_v'\r\n\r\n    def __init__(self, module, weights):\r\n        super(WeightNorm, self).__init__()\r\n        self.module = module\r\n        self.weights = weights\r\n        self._reset()\r\n\r\n    def _reset(self):\r\n        for name_w in self.weights:\r\n            w = getattr(self.module, name_w)\r\n\r\n            # construct g,v such that w = g/||v|| * v\r\n            with torch.no_grad():\r\n                g = torch.norm(w)\r\n                v = w/g.expand_as(w)\r\n            g = Parameter(g.data, requires_grad=True)\r\n            v = Parameter(v.data, requires_grad=True)\r\n            name_g = name_w + self.append_g\r\n            name_v = name_w + self.append_v\r\n\r\n            # remove w from parameter list\r\n            del self.module._parameters[name_w]\r\n\r\n            # add g and v as new parameters\r\n            self.module.register_parameter(name_g, g)\r\n            self.module.register_parameter(name_v, v)\r\n\r\n    def _setweights(self):\r\n        for name_w in self.weights:\r\n            name_g = name_w + self.append_g\r\n            name_v = name_w + self.append_v\r\n            g = getattr(self.module, name_g)\r\n            v = getattr(self.module, name_v)\r\n            w = v*(g/torch.norm(v)).expand_as(v)\r\n            setattr(self.module, name_w, w)\r\n\r\n    def forward(self, *args):\r\n        self._setweights()\r\n        return self.module.forward(*args)\r\n```\r\n\r\nand then change \r\n`\r\nnn.utils.weight_norm(m)\r\n`\r\nto:\r\n`\r\nWeightNorm(m, ['weight'])\r\n`"
      }
    ]
  },
  {
    "number": 1282,
    "title": "on_after_backward for multiple optimizer",
    "created_at": "2020-03-29T15:00:02Z",
    "closed_at": "2020-04-09T11:57:38Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1282",
    "body": "## 🚀 Feature\r\n`on_after_backward` is a perfect place to log grad, currently, `on_after_backward` make no distinguish for different optimizers, for GAN applications, in it would be nice to pass in `optimizer_id` as param for `on_after_backward`.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1282/comments",
    "author": "bobofzhang",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-29T15:00:45Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-04T23:00:21Z",
        "body": "Here are two alternative ways to do it:\r\nOption 1:\r\n```\r\ndef backward(self, trainer, loss, optimizer, optimizer_idx)\r\n    super().backward(trainer, loss, optimizer, optimizer_idx)\r\n    # do whatever you want after backward, and you can access optimizer_idx\r\n    # ...\r\n```\r\nOption 2:\r\n```\r\ndef on_after_backward(self):\r\n    # access the optimizer you want like so\r\n    optimizer_i = self.trainer.optimizers[i]  # or loop through all of them\r\n```"
      },
      {
        "user": "bobofzhang",
        "created_at": "2020-04-09T11:57:38Z",
        "body": "thanks !"
      }
    ]
  },
  {
    "number": 1252,
    "title": "Validation every epoch with non-finite dataloader",
    "created_at": "2020-03-26T21:58:04Z",
    "closed_at": "2020-03-29T19:27:45Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1252",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nProviding a way to do validation every epoch with non-finite (`__len__` not implemented) dataloaders.\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nDoing validation every epoch is a natural choice, and with finite dataloader you can do it easily by setting `val_check_interval=1.0`. However, with non-finite dataloader you cannot set `val_check_interval` to be float. There's no simple way to work around it yet.\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nThere can be several way to make it happen.\r\n\r\nOne solution on top of my head is to let `val_check_interval` to be `None`. if it is none, just do the validation according to the number of `check_val_every_n_epoch` at the end of every epoch.\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nAlternatively, you can let the user set `val_check_interval` to be 1.0 even with non-finite dataloaders. Anything below 1.0 would be invalid but only 1.0 can be valid. If it is 1.0 then do the validation according to the number of `check_val_every_n_epoch` at the end of every epoch.\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\r\nNot all dataloaders without `__len__` implemented are infinite dataloaders. Some just cannot decide length in advance. With these dataloaders the concept of 'epoch' is still valid. Pytorch-lightning needs to serve this kind of dataloaders better.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1252/comments",
    "author": "dreamgonfly",
    "comments": [
      {
        "user": "ethanwharris",
        "created_at": "2020-03-26T22:08:17Z",
        "body": "Thanks for the issue! Totally agree that we should support this. It might be a bit painful with how validation works at the moment. Currently we validate when the batch index reaches a certain value. But with Iterable stuff we'd need to do it once the itetation is over (and we can't know ahead of time when that will be). I guess that the way to do it is to add a seperate clause which deals specifically with the case when `val_check_interval=1.0`, although it's not the most elegant solution :/"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-27T11:47:20Z",
        "body": "cool, @ethanwharris @dreamgonfly mind drafting a PR?\r\ncc: @PyTorchLightning/core-contributors "
      }
    ]
  },
  {
    "number": 1239,
    "title": "Validation progress bar with metrics",
    "created_at": "2020-03-25T22:27:06Z",
    "closed_at": "2020-09-22T16:03:06Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "let's do it!"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1239",
    "body": "## 🚀 Feature\r\nLogging validation metrics throughout the duration of validation (e.g., current batch loss/acc or average batch loss/acc so far).\r\n\r\n### Motivation\r\n\r\nIf the validation set is large, it'd helpp to know right away if I've loaded the wrong checkpoint or loaded a checkpoint in the wrong way within a couple iterations, rather than waiting to run through the entire validation set.\r\n\r\n### Pitch\r\n\r\nI want to be able to see basically the same progress bar during training as I do during validation/testing.\r\n\r\n### Alternatives\r\n\r\nNot sure of any alternatives - this is quite helpful for debugging and is probably a not-too-large change since training already supports this feature.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1239/comments",
    "author": "ethanjperez",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-25T22:27:50Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-03-25T23:28:00Z",
        "body": "in summary, we need to support:\r\n\r\n```\r\nvalidation_step(...):\r\n  return {'log':{...}}\r\n```"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-26T13:38:02Z",
        "body": "@ethanjperez mind submit a PR?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-04T12:43:29Z",
        "body": "@PyTorchLightning/core-contributors anyone?"
      },
      {
        "user": "Borda",
        "created_at": "2020-06-11T13:57:34Z",
        "body": "will it it be also solved in #1989?"
      }
    ]
  },
  {
    "number": 1221,
    "title": "Colab weird behaviour and error when passing values from collate_fn to validation_step",
    "created_at": "2020-03-24T08:37:03Z",
    "closed_at": "2020-04-04T14:33:24Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1221",
    "body": "The same code that runs perfectly on my local machine (MacOS with Python3.7 and tested on an Ubuntu docker 18.04 with Python3.6.9) fails to run on colab.\r\n\r\nIt performs weirdly, calling the Dataset init() multiple times. However, it fails when the collate function that (correctly) returns (printed out the results) the following:\r\n```\r\nreturn x_tensor, x_lengths, y_tensor\r\n```\r\nget passed in the sanity check right at start in the ``validation_step``:\r\n```\r\ndef validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\r\n        x_tensor, x_lengths, y_tensor = batch <-- here it fails\r\n        .. forward(x_tensor,x _lengths) .. etc.\r\n```\r\nwith the error:\r\n```\r\nline 234, in validation_step\r\n    x_tensor, x_lengths, y_tensor = batch\r\nValueError: too many values to unpack (expected 3)\r\n```\r\nit actually just return the first value (x_tensor). I tried packing them in a dict from the collate function, but in the validation step ``batch`` comes to me just as the first key (as string!) from the dict created in collate_fn.\r\n\r\nThe environment is the same except the Python version (latest torch and lightning versions) across local runtime, docker ubuntu and colab. To ensure the same code is running on all machines, I'm doing just git clone, pip3 install -r requirements and python3 train.py. Am I missing something with Colab that's just not working? I can provide full code if needed.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1221/comments",
    "author": "dumitrescustefan",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-24T08:37:46Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-04T12:46:55Z",
        "body": "Try running master?"
      },
      {
        "user": "dumitrescustefan",
        "created_at": "2020-04-04T14:33:24Z",
        "body": "@williamFalcon \r\n\r\nConfirm that running on colab with version ``pytorch-lightning==0.7.2.dev0`` (from master directly) fixes the problem above. \r\n\r\nThanks a lot!"
      }
    ]
  },
  {
    "number": 1205,
    "title": "Logging the learning rate",
    "created_at": "2020-03-21T17:38:30Z",
    "closed_at": "2020-04-30T12:06:42Z",
    "labels": [
      "feature",
      "help wanted",
      "discussion"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1205",
    "body": "Hey, \r\n\r\nI think it would a cool feature to add a flag enabling the logging of the learning rate(s).\r\n\r\nThanks for your amazing work !",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1205/comments",
    "author": "maxime-louis",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-21T17:39:09Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-21T19:59:19Z",
        "body": "I think that's a great idea. Maybe it doesn't have to be a flag, it could be done by default like the other metrics that are already plotted automatically.\r\n \r\nSome things to consider:\r\n- How would it work for optimizers like Adam?\r\n- Optimizers may have different learning rates for different param groups"
      },
      {
        "user": "maxime-louis",
        "created_at": "2020-03-22T10:06:03Z",
        "body": "For Adam it's a pickle. Maybe rather log the scheduler information i.e. its scaling of the initial learning rate. It would solve the group problems as well I guess."
      },
      {
        "user": "Borda",
        "created_at": "2020-04-09T11:35:53Z",
        "body": "@PyTorchLightning/core-contributors do we want to add extra logging for LR or just stay with logging these extra parameters as a metric...?"
      },
      {
        "user": "Ir1d",
        "created_at": "2020-04-11T10:16:29Z",
        "body": "I proposed adding a `Trainer.lr` in #1003, but we decided to use the callbacks then."
      },
      {
        "user": "justusschock",
        "created_at": "2020-04-13T09:49:57Z",
        "body": "I'd also stick to callbacks. \r\nThe simplest approach to train a network doesn't even include lr changes and it does not make any sense to log something that doesn't change by design.\r\n\r\nHowever, for convenience we could provide an implementation of such a callback"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-04-13T10:23:00Z",
        "body": "I have earlier implemented a callback that logs the learning rate for some experiments I did. I can bring it up to date and create a PR if wanted. "
      }
    ]
  },
  {
    "number": 1190,
    "title": "Call load_from_checkpoint when trainer load state",
    "created_at": "2020-03-18T21:25:50Z",
    "closed_at": "2020-04-30T11:57:25Z",
    "labels": [
      "feature",
      "help wanted",
      "discussion"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1190",
    "body": "## 🚀 Feature\r\nWhen use `Trainer` class to load the model state instead of using `load_from_checkpoint`, the hook `on_load_checkpoint` won't be called. It's better to let trainer call `load_from_checkpoint` instead of directly call `model.load_state_dict`, thus `on_load_checkpoint` and other hooks can be called.\r\n\r\n### Motivation\r\n\r\nI need to do some sanity check of saved `hparams` and I found the suitable location for this is `on_load_checkpoint`. But this method is not called when use Trainer to load states.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1190/comments",
    "author": "cmpute",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-03-18T21:57:51Z",
        "body": "to me it makes sense call `on_load_checkpoint` inside `load_from_checkpoint` :]\r\n@jeffling @hadim ano thoughts?"
      },
      {
        "user": "jeffling",
        "created_at": "2020-03-20T23:19:15Z",
        "body": "This is a good change, but we may want to think about the versioning here since it may break some codebases. "
      },
      {
        "user": "Borda",
        "created_at": "2020-04-09T11:27:15Z",
        "body": "@PyTorchLightning/core-contributors any other thoughts?"
      },
      {
        "user": "yukw777",
        "created_at": "2020-04-14T05:42:02Z",
        "body": "I just hit this issue, and would love it in the next release!"
      }
    ]
  },
  {
    "number": 1189,
    "title": "Confusing setting of validation frequency",
    "created_at": "2020-03-18T20:23:25Z",
    "closed_at": "2020-12-02T05:48:09Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix",
      "discussion"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1189",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nCurrently the validation frequency is set by `check_val_every_n_epoch` and `val_check_interval` of Trainer. The former one and latter one have an overlap in terms of frequency control actually. I propose use `check_val_every_n_epoch` and `check_val_every_n_batches` as the parameter:\r\n\r\n`check_val_every_n_epoch=10` is equivalent to `check_val_every_n_epoch=10` previously\r\n`check_val_every_n_epoch=0.1` is equivalent to `val_check_interval=0.1` previously\r\n`check_val_every_n_batches=10` is equivalent to `val_check_interval=10`.\r\n\r\nAnd furthermore, these two options should be mutually exclusive. If one sets `check_val_every_n_batches` then shouldn't set `check_val_every_n_batches`.\r\n\r\n### Motivation\r\n\r\nI propose this because I get confused when I try to tweak the validation frequency and find two options for setting it...\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1189/comments",
    "author": "cmpute",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-18T20:24:06Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-20T22:46:13Z",
        "body": "@cmpute do you have a specific suggestion? :]"
      },
      {
        "user": "cmpute",
        "created_at": "2020-03-21T03:15:50Z",
        "body": "Yeah just as I said, change the option `val_check_interval` to `check_val_every_n_batches` and change their meanings to correspondant one"
      },
      {
        "user": "Borda",
        "created_at": "2020-04-09T11:36:57Z",
        "body": "@PyTorchLightning/core-contributors do you see this API change as needed?"
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-04-10T03:05:48Z",
        "body": "i agree it would be more clear"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-09T03:52:11Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "cmpute",
        "created_at": "2020-06-10T01:04:44Z",
        "body": "bump"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-10T01:37:32Z",
        "body": "agree that this is a good rename. can you think of something shorter?"
      },
      {
        "user": "cmpute",
        "created_at": "2020-06-10T01:51:10Z",
        "body": "Maybe just remove the \"check_\" prefix?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-10T02:19:19Z",
        "body": "how about\r\n`validate_frequency`\r\n`validation_rate`\r\n`val_rate`\r\n`val_freq`\r\n\r\nbecause it can also be a fraction within an epoch 0.5 for instance not just steps."
      },
      {
        "user": "cmpute",
        "created_at": "2020-06-10T14:26:00Z",
        "body": "Then the name can be `val_epoch_freq` and `val_batch_freq`?"
      },
      {
        "user": "qiuhuaqi",
        "created_at": "2020-07-29T18:47:54Z",
        "body": "Why not merge the two flags together? \r\ne.g. keep only `val_check_interval`, use `int` to specify the number of batches, use `float` to specify in terms of epochs but allows `>1.0`\r\n\r\n`val_check_interval=3.0` => validation every 3 epochs, \r\n`val_check_interval=10` => validation every 10 batches within an epoch\r\nin the unlikely usecase of `val_check_interval=3.5` => validation every 3.5 epochs, (triggering `validation_epoch_end` at the end of epoch 4?)\r\n\r\nThis would also solve the mutual exclusiveness issue. Or would this complicate things too much? "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-29T19:33:56Z",
        "body": "yeah... this seems pretty complicated. if you need to read the docs and not derive it from the syntax, it's too complicated"
      },
      {
        "user": "qiuhuaqi",
        "created_at": "2020-07-29T19:58:08Z",
        "body": "Fair enough. Renaming would be great though. Took me a while to find both flags. \r\nHow about:\r\n`check_val_every_n_epoch` -> `val_every_n_epoch`,\r\n`val_check_interval` -> `val_freq_in_epoch`\r\nA bit more literal?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-22T08:51:56Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "cmpute",
        "created_at": "2020-10-25T22:38:43Z",
        "body": "bump"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-10-26T00:12:44Z",
        "body": "cc @edenafek "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-11-25T02:22:06Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "turian",
        "created_at": "2021-08-26T22:33:07Z",
        "body": "I am also interested in this"
      }
    ]
  },
  {
    "number": 1178,
    "title": "DeprecatedWarnings when loaded",
    "created_at": "2020-03-18T01:10:19Z",
    "closed_at": "2020-04-07T09:42:17Z",
    "labels": [
      "bug",
      "help wanted",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1178",
    "body": "Ubuntu 19.10, Python 3.7.5, pytorch 1.4.0, venv, installed with pip3\r\n\r\nBuilds based on clones of master after ~3/14 may issue DeprecationWarnings when loaded. This occurs if built within venv environment with `include-system-site-packages = true` or built outside venv. It does not occur if `include-system-site-packages = false`, or if using earlier repo clone. \r\n\r\nThis is a fresh ubuntu install, and the only python packages installed are in support of ml work - pytorch, pytorch-lightning, numpy, sci-py, tensorflow, etc.\r\n\r\nThe errors:\r\n```\r\n/home/seth/dev/pytorch_work/.venv.ptl.master/lib/python3.7/site-packages/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\r\n  assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'\r\n/home/seth/dev/pytorch_work/.venv.ptl.master/lib/python3.7/site-packages/graphql/type/typemap.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\r\n  from collections import OrderedDict, Sequence, defaultdict\r\n/home/seth/dev/pytorch_work/.venv.ptl.master/lib/python3.7/site-packages/trains/backend_interface/metrics/events.py:27: DeprecationWarning: The usage of `cmp` is deprecated and will be removed on or after 2021-06-01.  Please use `eq` and `order` instead.\r\n  @attr.attrs(cmp=False, slots=True)\r\n```\r\nSeems like this might be due to new code that depend on newer versions of installed packages, so here is pip3 freeze outside of venv:\r\n```\r\nnumpy==1.18.1\r\nPillow==6.1.0\r\ntorch==1.4.0\r\ntorchvision==0.5.0\r\ntqdm==4.43.0\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1178/comments",
    "author": "sneiman",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-03-18T21:33:22Z",
        "body": "could you please give full error trace?\r\nso far I see only `trains` error which is third-party lib...\r\nmaybe @bmartinn could help..."
      },
      {
        "user": "sneiman",
        "created_at": "2020-03-18T21:51:13Z",
        "body": "That is the full  trace. Three warnings come up - 2 for graphql, 1 for trace. Both 3rd party libs. The reason I raised the question is that they did not start appearing until repo clones after ~march 14. I haven't changed the environment ...\r\n\r\nCan you see all 3 - I can see them here."
      },
      {
        "user": "Borda",
        "created_at": "2020-03-18T22:09:37Z",
        "body": "that is about the time we have added Trains..."
      },
      {
        "user": "bmartinn",
        "created_at": "2020-03-18T22:29:27Z",
        "body": "@Borda , I can probably take care of the warning inside the Trains package, but as you mentioned those are 3rd party libraries... \r\n**EDIT:**\r\nMoreover `attrs` had no \"eq\" argument in previous versions, and I would hate to bump the minimum `attrs` version in the Trains requirements ..."
      },
      {
        "user": "sneiman",
        "created_at": "2020-03-18T22:45:47Z",
        "body": "If its not due to new ptl code making an error or using the wrong version of something, it seems like this is at most a release note - easy for users to mask with a filter. Shall we close?\r\n\r\nFYI - just noticed that for some reason the whole freeze list does not show. Don't know why you would need it at this point, but ask if you do ..."
      },
      {
        "user": "Borda",
        "created_at": "2020-03-18T23:04:48Z",
        "body": "@bmartinn kind of extreme case would be using some warning ignore?"
      },
      {
        "user": "bmartinn",
        "created_at": "2020-03-18T23:08:54Z",
        "body": "@Borda Doable, but this is a Trains/GraphQL repository issue, don't you think?\r\n"
      },
      {
        "user": "sneiman",
        "created_at": "2020-03-19T00:15:14Z",
        "body": "fwiw - i think ptl should let the warnings stand if they are not due to some 'mis-behavior' in ptl.  they are easily screened by a warning filter in user code and if trains doesn't fix them before they become fatal at least user will know."
      },
      {
        "user": "Borda",
        "created_at": "2020-04-07T09:42:17Z",
        "body": "as it is Trains issue, I ll close it here... feel free reopen if needed :rabbit: "
      }
    ]
  },
  {
    "number": 1161,
    "title": "multi-gpu ddp calls validation and testing loops too many times",
    "created_at": "2020-03-16T18:09:55Z",
    "closed_at": "2020-03-30T16:13:34Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1161",
    "body": "When using ddp with multiple gpus, each validation and test loop is called with the entire validation dataset for each gpu.\r\n\r\nExpected behavior is that the dataset is divided appropriately across the gpus.\r\n\r\nI am using current master (cloned Mar 14), Ubuntu 19.10, Cuda 10.1, python 3.7.5, pytorch 1.4, venv environment.\r\n\r\nThe problem appears to be in `auto_add_sampler()` in data_loading.py. It does not create a `DistributedSampler` for validation or test datasets.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1161/comments",
    "author": "sneiman",
    "comments": [
      {
        "user": "sneiman",
        "created_at": "2020-03-16T21:24:51Z",
        "body": "Latest pull - 1 hour ago, no longer this behavior. Closing."
      },
      {
        "user": "sneiman",
        "created_at": "2020-03-17T00:22:10Z",
        "body": "Sorry - this issue still exists in some configurations. My proposed fix is not the total picture. Still investigating - will provide reproducible example."
      },
      {
        "user": "sneiman",
        "created_at": "2020-03-17T03:29:44Z",
        "body": "Testing underway. Will make PR tomorrow."
      },
      {
        "user": "sneiman",
        "created_at": "2020-03-17T23:18:43Z",
        "body": "Dont want to clutter up PR world if no one is interested in this. Let me know ..."
      },
      {
        "user": "Borda",
        "created_at": "2020-03-18T21:54:08Z",
        "body": "that sounds a good contribution to me... mind send a PR?\r\nAny suggestion @PyTorchLightning/core-contributors?\r\nin a technical note when you refer some master state pls use coit hash as there can be multiple commits each day..."
      },
      {
        "user": "sneiman",
        "created_at": "2020-03-18T22:05:28Z",
        "body": "will do on both pr, and hash ref"
      }
    ]
  },
  {
    "number": 1157,
    "title": "Learning Rate Schedulers' default dictionary parameters should be set via the Trainer",
    "created_at": "2020-03-15T20:43:52Z",
    "closed_at": "2020-03-19T13:22:30Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1157",
    "body": "## 🚀 Feature\r\nThe default Learning Rate Schedulers (LRS) dictionary parameters should be settable from the Trainer constructor. \r\n\r\n### Motivation\r\nThe documentation doesn't seem to be clear that the LRS have the following additional parameters available to be set when you configure the optimizers: \r\n```\r\n    'interval': 'epoch',  # default every epoch\r\n    'frequency': 1,  # default every epoch/batch\r\n    'reduce_on_plateau': False,  # most often not ReduceLROnPlateau scheduler\r\n    'monitor': 'val_loss'\r\n```\r\n### Pitch\r\nSet those defaults in the constructor of the Trainer and the user can set them themselves. \r\n\r\n### Alternatives\r\nForce the return type of the function LightningModule.configure_optimizers() to be List[Optimizer], List[Dict]\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1157/comments",
    "author": "danieltudosiu",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-03-16T15:24:54Z",
        "body": "I disagree mainly due to:\r\n* The trainer constructor already have many arguments, and this would potentially add 3 extra (the `'reduce_on_plateau'` is automatically determined from the LRS) arguments\r\n* The user can already set these in `configure_optimizers` method by passing a LRS as a dict with these keywords.\r\n* If I have two LRS and I want one to step after every epoch and one after every batch i.e. setting `interval=\"epoch\"` for the first and `interval=\"step\"` for the second, I would still need to change at least one of them in `configure_optimizers()` away from the default\r\n\r\nI do agree that it is not very clear in the documentation that these are an option to set (this feature was introduced in 0.7.0), this probably needs to be changed \r\n"
      }
    ]
  },
  {
    "number": 1155,
    "title": "No validation checks when overfit_pct is set",
    "created_at": "2020-03-15T13:43:17Z",
    "closed_at": "2020-05-03T23:15:57Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1155",
    "body": "## 🐛 Bug\r\n\r\nWhen setting the `overfit_pct` to any value between 0 and 1 (exclusive) in trainer, the validation checks are disabled.\r\n\r\n### To Reproduce\r\n\r\nI have worked on a minimal example to reproduce the bug:\r\n\r\n```python3\r\nimport pytorch_lightning as pl\r\nimport torch\r\n\r\nclass Dataset(torch.utils.data.Dataset):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Dataset, self).__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n\r\n    def __getitem__(self, idx):\r\n        X = torch.rand(1, self.input_dim)\r\n        y = torch.randint(0, self.output_dim, (1,))\r\n        return X, y\r\n\r\n    def __len__(self):\r\n        return 1000\r\n\r\nclass Model(pl.LightningModule):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Model, self).__init__()\r\n        self.layer = torch.nn.Linear(input_dim, output_dim)\r\n        self.dataset = Dataset(input_dim, output_dim)\r\n\r\n    def forward(self, x, y):\r\n        yhat = torch.softmax(self.layer(x), -1)\r\n        return F.nll_loss(logits, y)\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=64)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        return {'loss': loss, 'log': {'loss': loss}}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        return {'val_loss': loss, 'log': {'val_loss': loss}}\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = Model(100, 10)\r\n    trainer = pl.Trainer(overfit_pct=.01)\r\n    trainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nValidation checks occur normally\r\n\r\n### Environment\r\n```bash\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Manjaro Linux\r\nGCC version: (GCC) 8.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] pytorch-lightning==0.7.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] mkl                       2020.0                      166  \r\n[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.7.1                    pypi_0    pypi\r\n[conda] torchvision               0.5.0                py37_cu101    pytorch\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1155/comments",
    "author": "qmeeus",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-15T13:43:56Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-18T21:49:24Z",
        "body": "@jeffling @hadim @awaelchli mind check?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-21T04:01:07Z",
        "body": "~~Yes there is a bug here~~, but I had to fix @qmeeus's code sample to make it visible. \r\nThe sanity validation checks run, but the validation at the end of the epoch doesn't.\r\nWhen setting `overfit_pct=1`, validation checks work as expected.\r\nHere is the fixed minimal code sample:\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass Dataset(torch.utils.data.Dataset):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Dataset, self).__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n\r\n    def __getitem__(self, idx):\r\n        X = torch.rand(self.input_dim)\r\n        y = torch.randint(0, self.output_dim, (1,))\r\n        return X, y\r\n\r\n    def __len__(self):\r\n        return 1000\r\n\r\n\r\nclass Model(pl.LightningModule):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Model, self).__init__()\r\n        self.layer = torch.nn.Linear(input_dim, output_dim)\r\n        self.dataset = Dataset(input_dim, output_dim)\r\n\r\n    def forward(self, x, y):\r\n        logits = torch.softmax(self.layer(x), -1)\r\n        return F.nll_loss(logits, y.flatten(0))\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=64)\r\n\r\n    def val_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=64)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        return {'loss': loss, 'log': {'loss': loss}}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        print('see that validation runs only in sanity check')\r\n        return {'val_loss': loss, 'log': {'val_loss': loss}}\r\n\r\n    def validation_end(self, outputs):\r\n        loss = torch.stack([output['val_loss'] for output in outputs]).mean()\r\n        return {'val_loss': loss, 'log': {'val_loss': loss}}\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = Model(100, 10)\r\n    trainer = pl.Trainer(overfit_pct=0.1, max_epochs=10)\r\n    trainer.fit(model)\r\n```\r\nFor the record, @qmeeus your code had these issues:\r\n- No val_dataloader defined\r\n- Wrong shapes returned in dataloader\r\n- Wrong shape for nll_loss labels"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-21T04:14:27Z",
        "body": "Actually `overfit_pct `argument is not documented in the Trainer class. We should fix that and say that setting `overfit_pct `is the same as setting `train_percent_check`, `val_percent_check `and `test_percent_check`."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-21T06:26:11Z",
        "body": "**False alarm!** Turns out it is simply because you chose a too small value for `overfit_pct`. \r\nYour dataset has size 1000, and dataloader has batch_size 64. \r\n1000 / 64 ~= 15 batches\r\nWhen you choose overfit_pct = .01, then that gives 15 * 0.01 < 1 batch. \r\n\r\n@qmeeus Please let me know if it isn't clear. I think the behaviour of `overfit_pct `is correct."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-21T06:30:40Z",
        "body": "@williamFalcon Should we make it so that `overfit_pct `does not round to 0 batches?\r\n"
      },
      {
        "user": "qmeeus",
        "created_at": "2020-03-21T10:42:11Z",
        "body": "> **False alarm!** Turns out it is simply because you chose a too small value for `overfit_pct`.\r\n> Your dataset has size 1000, and dataloader has batch_size 64.\r\n> 1000 / 64 ~= 15 batches\r\n> When you choose overfit_pct = .01, then that gives 15 * 0.01 < 1 batch.\r\n> \r\n> @qmeeus Please let me know if it isn't clear. I think the behaviour of `overfit_pct `is correct.\r\n\r\nAwesome, thanks ! "
      }
    ]
  },
  {
    "number": 1150,
    "title": "Propose to save new model before deleting previous ones in ModelCheckpointing",
    "created_at": "2020-03-14T23:47:36Z",
    "closed_at": "2020-04-16T16:40:51Z",
    "labels": [
      "feature",
      "help wanted",
      "let's do it!"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1150",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nIn an edge case, the trainer deleted previous model and then was killed because of system error before successfully saving new model. Thus all the models were lost.\r\nI understand specifying save_top_k > 1 helps, and saving before deleting leads to larger disk consumption. But it might be good to provide an option for this?\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nin the worst case, you have two but never none...\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1150/comments",
    "author": "Ir1d",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-03-16T23:00:41Z",
        "body": "That is good point, I like it... @PyTorchLightning/core-contributors ^^"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-07T12:53:17Z",
        "body": "@Ir1d sounds great. submit a PR?"
      },
      {
        "user": "Ir1d",
        "created_at": "2020-04-07T12:55:54Z",
        "body": "I can do it this friday"
      }
    ]
  },
  {
    "number": 1148,
    "title": "[Bug] Progress bar displays wrong total iterations for train",
    "created_at": "2020-03-14T20:36:32Z",
    "closed_at": "2020-03-14T20:48:52Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1148",
    "body": "#1018  🐛 Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nfor example, my training dataset has 448 samples, and I use a batch size of 4, then the loader should have length 448 / 4 = 112, but the progress bar is displaying N/171, which I have no idea how it computes that. The training then proceeds normally until it hits 112/171 and enters validation.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1148/comments",
    "author": "versatran01",
    "comments": [
      {
        "user": "failable",
        "created_at": "2020-03-16T03:34:16Z",
        "body": "Have you figure out how the total iteration is computed?"
      },
      {
        "user": "versatran01",
        "created_at": "2020-03-16T15:04:40Z",
        "body": "I think it is the sum of train and val iteration, which is confusing."
      },
      {
        "user": "ThierryDeruyttere",
        "created_at": "2020-06-18T13:32:28Z",
        "body": "Yes it is very confusing. I think this should be fixed though. "
      },
      {
        "user": "dtoniolo",
        "created_at": "2021-12-17T17:57:55Z",
        "body": "Agree, I find it counter-intuitive"
      }
    ]
  },
  {
    "number": 1119,
    "title": " Checkpoint fails in single node multi-GPU mode using  DDP",
    "created_at": "2020-03-11T13:02:06Z",
    "closed_at": "2020-03-12T14:50:01Z",
    "labels": [
      "bug",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1119",
    "body": "## 🐛 Bug\r\n\r\nCheckpoint fails in single node multi-GPU mode using  DDP.\r\n\r\n### To Reproduce\r\n\r\n```bash\r\npython pl_examples/basic_examples/gpu_template.py --distributed_backend ddp --gpus 2\r\n```\r\n```bash\r\nEpoch 2: : 700it [00:28, 42.69it/s, l/home/xz/anaconda3/envs/x/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown                                                                                                                                                                                                                 \r\n  len(cache))\r\nTraceback (most recent call last):\r\n  File \"gpu_template.py\", line 79, in <module>\r\n    main(hyperparams)\r\n  File \"gpu_template.py\", line 40, in main\r\n    trainer.fit(model)\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 590, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\r\n    raise Exception(msg)\r\nException: \r\n\r\n-- Process 1 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 342, in ddp_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 830, in run_pretrain_routine\r\n    self.train()\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 343, in train\r\n    self.run_training_epoch()\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 452, in run_training_epoch\r\n    self.call_checkpoint_callback()\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 737, in call_checkpoint_callback\r\n    self.checkpoint_callback.on_validation_end(self, self.get_model())\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 204, in on_validation_end\r\n    self._do_check_save(filepath, current, epoch)\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 221, in _do_check_save\r\n    self._del_model(delpath)\r\n  File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 121, in _del_model\r\n    os.remove(filepath)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/xz/pytorch-lightning/pl_examples/basic_examples/lightning_logs/version_1/checkpoints/epoch=0.ckpt'\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1119/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-03-11T13:43:26Z",
        "body": "yeah we shall run all examples in CI too"
      },
      {
        "user": "sneiman",
        "created_at": "2020-03-11T20:39:10Z",
        "body": "I believe this happens with multiple gpus as well. And it only seems to happen if `ModelCheckpoint(save_top_k)` is set greater than 1. Still converting models to 0.7.1 but wanted to share this ..."
      },
      {
        "user": "ghost",
        "created_at": "2020-03-12T02:19:25Z",
        "body": "@Borda fixed. part of the code that caused the bug was removed a few commits back."
      },
      {
        "user": "sneiman",
        "created_at": "2020-03-12T03:28:12Z",
        "body": "fix is in master?"
      },
      {
        "user": "ghost",
        "created_at": "2020-03-12T03:32:04Z",
        "body": "fix for DDP checkpoint is in #1125, still waiting for it to be reviewed and merged. \r\n\r\n> I believe this happens with multiple gpus as well. And it only seems to happen if `ModelChckepoint(save_top_k)` is set greater than 1. Still converting models to 0.7.1 but wanted to share this ...\r\n\r\nas for this issue, on my side it seems to work fine. can you double check?"
      }
    ]
  },
  {
    "number": 1111,
    "title": "TensorBoardLogger should be able to add metric names in hparams",
    "created_at": "2020-03-10T08:59:33Z",
    "closed_at": "2020-07-12T15:22:09Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1111",
    "body": "## 🚀 Feature\r\nTensorBoard allows investigating the effect of hyperparameters in the hparams tab. Unfortunately, the `log_hyperparams` function in `TensorBoardLogger` cannot add any information about which of the logged metrics is actually a \"metric\" which can be used for such a comparison.\r\n\r\n### Motivation\r\n\r\nI would like to use the built-in hparams module of TensorBoard to evaluate my trainings.\r\n\r\n### Pitch\r\n\r\nPyTorch-Lightning should give me the possibility to define the metrics of my model in some way such that any logger is able to derive which metric may be used for hyperparameter validation, as well as other possible characteristics which may be defined for those.\r\n\r\n### Additional context\r\n\r\nThe `hparams` method of a summary takes the following parameters:\r\n```python\r\ndef hparams(hparam_dict=None, metric_dict=None):\r\n```\r\n`metric_dict` is basically a dictionary mapping metric names to values, whereas the values are omitted in the function itself.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1111/comments",
    "author": "tstumm",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-10T09:00:17Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-10T19:48:35Z",
        "body": "Since this is specific to tensorboard and other loggers handle hparams and metrics differently, it is better to use the SummaryWriter object directly. You can always do that with\r\n`self.logger.experiment.add_hparams(hparam_dict, metric_dict)` within your LightningModule. "
      },
      {
        "user": "tstumm",
        "created_at": "2020-03-11T10:25:42Z",
        "body": "I think if Lightning offers such a logger mechanism, it should offer an abstraction to enable this functionality. I'd be fine with having a `register_metric` function in `TensorBoardLogger`, but I don't want to rely on implementation details of the underlying logging mechanism."
      },
      {
        "user": "Borda",
        "created_at": "2020-03-14T01:11:12Z",
        "body": "@tstumm that sounds good to me, would you mind to send a PR?\r\ncc: @PyTorchLightning/core-contributors "
      },
      {
        "user": "Borda",
        "created_at": "2020-04-09T11:25:59Z",
        "body": "@tstumm with logger you can access directly to the base TensorBoard so whatever is allowed there you could be able to do also here... May point some example of this Tensofoard functionality/use-case?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-05-03T23:24:49Z",
        "body": "It was introduced here recently: #1630\r\nFeel free to reopen if issues remain. "
      },
      {
        "user": "elkotito",
        "created_at": "2020-05-04T11:58:55Z",
        "body": "@awaelchli  Is there a plan to automatically log all metrics for hparams tab in TensorBoard? I mean all metrics returned in `log` key inside methods like `validation_step`  using newly merged `TensorBoardLogger().log_hyperparams()`?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-05-04T13:01:06Z",
        "body": "I'm not up to date with the logger features atm. Will reopen to keep track of your suggestion and also because I just saw that there is still a bugfix in the works here: #1647"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-07-03T14:31:07Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 1099,
    "title": "Avoid running `on_training_end` after Keyboard Interrupt",
    "created_at": "2020-03-09T04:13:38Z",
    "closed_at": "2020-04-05T15:12:42Z",
    "labels": [
      "feature",
      "help wanted",
      "let's do it!"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1099",
    "body": "## 🚀 Feature\r\nRight now, due to #795 , `on_training_end` runs after a `KeyboardInterrupt`. This ends up running code that's meant to be run only after successful training completion. This feature should either be reverted, or an alternative should be provided, so as to run some code only after successful training.\r\n\r\n### Motivation\r\n\r\nI am training a model on Sagemaker and have added a notebook shutdown code within the `on_training_end` method. There were times where I had to manually cancel my model training because some parameters were incorrect. However, If I do that, the notebook shuts down immediately. This is because the `on_training_end` method runs even after a `KeyboardInterrupt`.  I don't want my notebook shutting down after a keyboard interrupt, only after successful training completion.\r\n\r\n### Pitch\r\nMaybe add an `on_training_completed` method for code that's meant to be run after successful training.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1099/comments",
    "author": "lezwon",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-09T04:14:18Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-11T23:12:54Z",
        "body": "Good point, it makes sense to skip eval of the unfinished train also when a user is it he doesn't want to wait for another hour lol Mind send a PR? :robot: "
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-12T01:26:38Z",
        "body": "Thanks for bringing this use-case to our attention @lezwon , definitely something we'd want to consider.\r\n\r\nWhat if the `Trainer` object had a `status` property? Similar to how compute jobs will have a status `{PENDING, RUNNING, COMPLETE, FAILED}` we could apply something similar here for the training job. In this case we could differentiate the two cases with a status of INTERRUPTED vs COMPLETED and your callback logic can check for the proper status before closing (eg. shutdown notebook on FAILED/COMPLETED but don't shutdown for INTERRUPTED). \r\n\r\nWhat do you think?"
      },
      {
        "user": "lezwon",
        "created_at": "2020-03-12T07:33:33Z",
        "body": "@jeremyjordan that sounds great. We could definitely do that."
      },
      {
        "user": "Borda",
        "created_at": "2020-03-12T08:58:23Z",
        "body": "Not sure if we really need to add complexity to the existing callbacks...\r\nAlso `on_training_completed` is standard behavior so rather some cleanup for interrupted?\r\n@jeremyjordan status sounds cool, by my opinion it is much better signal the returned 0/1 from e.g. `.fit` =) "
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-13T03:14:32Z",
        "body": "@Borda are you suggesting that we add a new callback (`on_training_completed` that @lezwon mentioned) which runs conditionally according to the value returned from `fit()`? My only worry is that it might be confusing to know the difference between `on_training_end` (existing) and `on_training_complete` (proposed)."
      },
      {
        "user": "Borda",
        "created_at": "2020-03-13T13:41:49Z",
        "body": "in this moment I was just thinking about adding Trainer status... \r\nnot sure if we want to add complexity with new callback..."
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-14T15:39:10Z",
        "body": "Ok yes I agree, I think the Trainer status would be a simple solution that may also be useful in other situations as well :)"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-14T18:13:09Z",
        "body": "Cool, does anyone send a PR with Trainer status? I may suggest to implement it as enum/numbering similar like `logging.INFO/DEBUG/...`"
      },
      {
        "user": "lezwon",
        "created_at": "2020-03-15T05:40:06Z",
        "body": "Will this status also account for validation/test completed/interrupted?"
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-15T14:05:21Z",
        "body": "yes we could do something like:\r\n\r\n```\r\n>>> from enum import Enum\r\n>>> class TrainerStatus(Enum):\r\n...     PENDING = \"Initializing Trainer object\"\r\n...     TRAINING = \"Optimizing model via train loop\"\r\n...     VALIDATING = \"Running model on validation set\"\r\n...     TESTING = \"Running model on test set\"\r\n...     FAILED = \"Trainer failed to complete a successful run\"\r\n...     INTERRUPTED = \"Training was interrupted by the user\"\r\n...     COMPLETED = \"Training completed successfully\"\r\n```\r\n\r\nI can work on getting this into a PR"
      },
      {
        "user": "lezwon",
        "created_at": "2020-03-15T18:00:12Z",
        "body": "Hey @jeremyjordan, what if we would like to execute different actions if the trainer failed during a validation task? Can we find out which task it failed at from the status? Just wondering if this would be flexible in such a scenario."
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-15T18:41:54Z",
        "body": "@lezwon do you have an example in mind of where you'd need that? \r\n\r\ni'd prefer to keep the implementation simple and generic enough such that we don't keep adding new status types."
      },
      {
        "user": "lezwon",
        "created_at": "2020-03-16T01:43:48Z",
        "body": "Don't really have an example yet. Was just considering a scenario like that though. \n\nI guess we could go ahead with the current proposal you mentioned. That should solve my issue for sure 😊"
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-21T15:41:23Z",
        "body": "probably want to consider building upon the Trainer state defined in #770 \r\n\r\nfyi @xingzhaolee - what do you think? (we don't need to include it in #770 just thinking about building off of that)"
      },
      {
        "user": "ghost",
        "created_at": "2020-03-22T03:34:32Z",
        "body": "@jeremyjordan sounds like a good idea! also I was thinking that since we have TrainerStatus, can it be the main class while TrainerMode servers as a nested enum instead of putting them all under a single enum? something like:\r\n```python\r\nclass TrainerMode(enum.Enum):\r\n    TRAINING = enum.auto()\r\n    VALIDATING = enum.auto()\r\n    TESTING = enum.auto()\r\n\r\nclass TrainerStatus(enum.Enum):\r\n    PENDING = enum.auto()\r\n    FAILED = enum.auto()\r\n    INTERRUPTED = enum.auto()\r\n    ...\r\n    MODE = TrainerMode\r\n```\r\n\r\nthen user can access the current status and mode through:\r\n```python\r\n# Interrupted when validation is running\r\nif ... is TrainerStatus.INTERRUPTED and ... is TrainerStatus.MODE.VALIDATING:\r\n```"
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-22T21:06:08Z",
        "body": "sure! that would be flexible enough to address @lezwon 's earlier comment. i had started a branch to work on TrainerStatus, once #770 is merged i can pick up that work with your new suggestion here :)"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-22T21:28:30Z",
        "body": "if I understand it correctly the `.auto` means that it can be every run different `enum` value, right? Even it seems to be starting from 1 and incrementally increase...\r\nif so I would recommend using some exact numbers in case of exporting/importing :]"
      },
      {
        "user": "ghost",
        "created_at": "2020-03-23T01:13:52Z",
        "body": "not too sure about that, but probably the order matters too when new status or mode is added. so I guess we should use exact numbers like you suggested in case any import or export is involved! 🙂"
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-04-04T02:57:09Z",
        "body": "i went back and forth on whether the trainer status would be a valuable addition. ultimately, i decided to opt for a simple attribute denoted when a `KeyboardInterrupt` has been caught. "
      }
    ]
  },
  {
    "number": 1083,
    "title": "Lower default progress_bar_refresh_rate",
    "created_at": "2020-03-07T11:29:04Z",
    "closed_at": "2020-03-12T16:40:31Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1083",
    "body": "## 🚀 Feature\r\nFrom a conversation in Slack.\r\nIn v0.7.1, the default value of  `progress_bar_refresh_rate` is 50, which I think too high.\r\nIt is a measure to prevent notebook freezing, however, I believe most users run PL on CLI.\r\nIn addition, the high default value confuses the users in `fast_dev_run` because it has only 2 iterations.\r\n\r\nSo, I suggest the following:\r\n- Set the default value as `1` or `2`. (I think `1` is better.)\r\n- Add doc for notebook users like \"you should raise `progress_bar_refresh_rate` to prevent freezing!\"\r\n\r\nfor participants of the conversation: @williamFalcon @Borda @jeremyjordan ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1083/comments",
    "author": "S-aiueo32",
    "comments": [
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-08T17:59:49Z",
        "body": "Thanks @S-aiueo32! Do you mind submitting a PR with the change?"
      },
      {
        "user": "S-aiueo32",
        "created_at": "2020-03-08T22:52:55Z",
        "body": "@jeremyjordan yeah, just a moment 👍 "
      }
    ]
  },
  {
    "number": 1046,
    "title": "Logger-specific checkpoint callbacks",
    "created_at": "2020-03-05T12:28:15Z",
    "closed_at": "2020-05-13T13:43:57Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1046",
    "body": "## 🚀 Feature\r\nAdd checkpoint callbacks corresponding to each logger.\r\n\r\n### Motivation\r\nRecently, PL supports a lot of loggers like comet.ml, eptune.ai or etc. Many of them have the functionality to store the weight files on their cloud storage, however, PL has logging and checkpointing features independently. The loggers also have automatic detection of the checkpoints and upload them, however, it is difficult to interpret PL's high-level wrapping completely. In this situation, we cannot fully benefit from the loggers.\r\n\r\n### Pitch\r\n- Adding checkpoint callbacks corresponding to each logger. e.g. `CometLogger - CometCheckpoint`.\r\n\r\n### Alternatives\r\nI think it is possible with small changes of `save_function` because `TrainerCallbackConfigMixin` has already had the logger object as a member. The specific usage should be investigated in each community.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1046/comments",
    "author": "S-aiueo32",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-05-04T13:06:39Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 994,
    "title": "Simplification: Merge load_from_metrics and load_from_checkpoint ",
    "created_at": "2020-03-01T22:07:13Z",
    "closed_at": "2020-03-03T10:04:57Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/994",
    "body": "## 🚀 Feature\r\n\r\nThe two ways of loading a LightningModule from checkpoint only differ in one  argument, the tags_csv. \r\n\r\n### Motivation\r\n\r\nThe code is almost identical for both and the purpose is the same. If we merge these two into one function, it would simplify the API.\r\n\r\n### Pitch\r\n\r\nCombine\r\n`load_from_metrics(cls, weights_path, tags_csv, map_location=None)` and \r\n`load_from_checkpoint(cls, checkpoint_path, map_location=None)` into a single signature:\r\n\r\n```\r\nload_from_checkpoint(cls, checkpoint_path, tags_csv=None, map_location=None)\r\n```\r\nand make load_from_metrics deprecated.\r\n\r\n### Alternatives\r\n\r\nkeep as is, not a big deal :)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/994/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-03-01T22:18:11Z",
        "body": "let’s do it!\r\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-01T23:00:00Z",
        "body": "@awaelchli I agree, just which method shall be the remaining one?\r\n `load_from_metrics` or `load_from_checkpoint` :]"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-01T23:39:05Z",
        "body": "Probably load_from_checkpoint as it used more often :) I already made PR :)"
      }
    ]
  },
  {
    "number": 975,
    "title": "How to show more log infos during training instead of using tqdm to show only once?",
    "created_at": "2020-02-28T11:16:11Z",
    "closed_at": "2020-06-07T21:17:52Z",
    "labels": [
      "feature",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/975",
    "body": "## 🚀 Feature\r\n\r\n\r\n### Motivation\r\n\r\nSometimes we want to check whether our model can converge. The most intuitive is to print the training information at regular intervals. However, tqdm only shows once, from which we cannot clearly see the trend of convergence.\r\n\r\n\r\n### Alternatives\r\n\r\nIn this context, I wonder is it possible to use the normal method (e.g. simply print or logging) to log the training and validation information? Or does tqdm support print the information on both screen and file?\r\n\r\n\r\n\r\n### Additional context\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/975/comments",
    "author": "marsggbo",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-02-28T11:33:11Z",
        "body": "you can always call print whenever you want. but ideally we look at tensorboard or another logger to check for convergence. \r\n\r\nbut you’re free to do print(loss) in your code haha"
      },
      {
        "user": "marsggbo",
        "created_at": "2020-02-29T07:26:47Z",
        "body": "I got it. I can first set `show_progress_bar=False`, then print the info in `training_step`, but I wonder how can I print the average value of accuracy and loss?"
      },
      {
        "user": "djbyrne",
        "created_at": "2020-03-23T15:35:50Z",
        "body": "Is there anyway to use something like logging.info() instead of print? "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-03-30T16:09:12Z",
        "body": "you can use self.print() inside the lightningModule"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-29T20:52:37Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 958,
    "title": "Process runs on more GPUs than specified",
    "created_at": "2020-02-26T23:19:05Z",
    "closed_at": "2020-06-01T15:00:35Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/958",
    "body": "I have a single 8-GPU machine with a faulty GPU0. \r\nI'm running imagenet_example.py on 7 GPUs on this machine by specifying `gpus=[1,2,3,4,5,6,7]` in the Trainer i.e. I do not want to use GPU0\r\n\r\nHowever, when i run `nvidia-smi`, I see the Trainer's pid shows on all 8 GPUs, just with lower memory on GPU0 (see output below). I also find it to be slower than non-PL code by about 4x. I don't see this behavior if I manually set `CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7` followed by `gpus=7` in Trainer. Similarly, it works fine when using a single GPU with, say, `gpus=[1]`.\r\nI'm not sure if it's relevant but I also see `gpu=0` in the tqdm progress bar\r\n\r\n#### nvidia-smi with Trainer(gpus=[1,2,3,4,5,6,7]) and CUDA_VISIBLE_DEVICES unset\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     40155      C   python                                       719MiB |\r\n|    1     40155      C   python                                      6003MiB |\r\n|    2     40155      C   python                                      6019MiB |\r\n|    3     40155      C   python                                      6019MiB |\r\n|    4     40155      C   python                                      6019MiB |\r\n|    5     40155      C   python                                      6019MiB |\r\n|    6     40155      C   python                                      6019MiB |\r\n|    7     40155      C   python                                      6019MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n#### nvidia-smi with Trainer(gpus=7) and CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    1     34452      C   python                                      6003MiB |\r\n|    2     34452      C   python                                      6019MiB |\r\n|    3     34452      C   python                                      6019MiB |\r\n|    4     34452      C   python                                      6019MiB |\r\n|    5     34452      C   python                                      6019MiB |\r\n|    6     34452      C   python                                      6019MiB |\r\n|    7     34452      C   python                                      6019MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe process should run on the specified GPUs without manually setting CUDA_VISIBLE_DEVICES\r\n\r\n### Environment\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\nGPU 2: GeForce RTX 2080 Ti\r\nGPU 3: GeForce RTX 2080 Ti\r\nGPU 4: GeForce RTX 2080 Ti\r\nGPU 5: GeForce RTX 2080 Ti\r\nGPU 6: GeForce RTX 2080 Ti\r\nGPU 7: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 418.87.00\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] pytorch-lightning==0.6.0\r\n[pip] torch==1.4.0\r\n[pip] torch-lr-finder==0.1.2\r\n[pip] torchvision==0.5.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2020.0                      166\r\n[conda] mkl-service               2.3.0            py38he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py38ha843d7b_0\r\n[conda] mkl_random                1.1.0            py38h962f231_0\r\n[conda] pytorch                   1.4.0           py3.8_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.6.0                    pypi_0    pypi\r\n[conda] torch-lr-finder           0.1.2                    pypi_0    pypi\r\n[conda] torchvision               0.5.0                py38_cu101    pytorch\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/958/comments",
    "author": "sahnimanas",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-02-26T23:19:44Z",
        "body": "Hey, thanks for your contribution! Great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-02-26T23:28:53Z",
        "body": "Thx for comment, I do not think that the training is fully running on the GPU0, just some memory allocation... Could you also share the GPU utilization during the training process?"
      },
      {
        "user": "sahnimanas",
        "created_at": "2020-02-26T23:47:13Z",
        "body": "I also think that the training is likely not running on GPU0 but not sure why the pid shows up on it\r\nHere's the full output of nvidia-smi during training\r\n\r\n```\r\nWed Feb 26 18:44:17 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  On   | 00000000:3D:00.0 Off |                  N/A |\r\n| 31%   41C    P8    33W / 250W |    730MiB / 10989MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 208...  On   | 00000000:3E:00.0 Off |                  N/A |\r\n| 31%   48C    P2    99W / 250W |   6014MiB / 10989MiB |     14%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce RTX 208...  On   | 00000000:60:00.0 Off |                  N/A |\r\n| 31%   50C    P2    98W / 250W |   6030MiB / 10989MiB |     13%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce RTX 208...  On   | 00000000:61:00.0 Off |                  N/A |\r\n| 30%   45C    P2    73W / 250W |   6030MiB / 10989MiB |     14%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  GeForce RTX 208...  On   | 00000000:B1:00.0 Off |                  N/A |\r\n| 32%   45C    P2    73W / 250W |   6030MiB / 10989MiB |     14%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  GeForce RTX 208...  On   | 00000000:B2:00.0 Off |                  N/A |\r\n| 32%   45C    P2    68W / 250W |   6030MiB / 10989MiB |     14%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  GeForce RTX 208...  On   | 00000000:DA:00.0 Off |                  N/A |\r\n| 31%   51C    P2    92W / 250W |   6030MiB / 10989MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  GeForce RTX 208...  On   | 00000000:DB:00.0 Off |                  N/A |\r\n| 31%   44C    P2    99W / 250W |   6030MiB / 10989MiB |     13%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     35809      C   python                                       719MiB |\r\n|    1     35809      C   python                                      6003MiB |\r\n|    2     35809      C   python                                      6019MiB |\r\n|    3     35809      C   python                                      6019MiB |\r\n|    4     35809      C   python                                      6019MiB |\r\n|    5     35809      C   python                                      6019MiB |\r\n|    6     35809      C   python                                      6019MiB |\r\n|    7     35809      C   python                                      6019MiB |\r\n+-----------------------------------------------------------------------------+\r\n```"
      },
      {
        "user": "sahnimanas",
        "created_at": "2020-02-27T04:14:39Z",
        "body": "BTW I switched the distributed backend from dp (default) to ddp and this went away. No PID is shown on GPU0 and its memory usage is at 11MiB (same as any other inactive GPU)"
      },
      {
        "user": "Borda",
        "created_at": "2020-02-27T07:31:29Z",
        "body": "Ok, in such case I would assume it as resolved, but feel free to reopne it if you need to :robot: "
      },
      {
        "user": "yakobyd",
        "created_at": "2020-04-14T15:21:29Z",
        "body": "I have cloned the repository yesterday (pytorch-lightning==0.7.4.dev0) and there are some edge cases that are still not fixed by #1349. Below is minimal code for reproduction:\r\n\r\n```python\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\n\r\nimport pytorch_lightning as pl\r\n\r\n\r\nclass Model(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(1000, 10)\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        return {'loss': F.cross_entropy(y_hat, y)}\r\n\r\n    def training_epoch_end(self, outputs):\r\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\r\n        return {'avg_loss': avg_loss}\r\n\r\n    def train_dataloader(self):\r\n        data = torch.rand(4096, 1000)\r\n        labels = torch.randint(high=10, size=(4096,))\r\n        return DataLoader(list(zip(data, labels)), batch_size=64, pin_memory=True)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.001)\r\n\r\n\r\ntrainer = pl.Trainer(gpus=[3])\r\nmodel = Model()\r\n\r\ntrainer.fit(model)\r\n```\r\n\r\nAfter running the above code, `nvidia-smi` outputs the following:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     26657      C   python                                       479MiB |\r\n|    3     26657      C   python                                       482MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nI have tested a few scenarios, and found that this is caused by two factors:\r\n\r\n- In the DataLoader, `pin_memory==True`.\r\n- Presence of `training_epoch_end`.\r\n\r\nThese factors must happen together for the problem to arise. For example, if `pin_memory==True`, but `training_epoch_end` is not implemented, then the GPU memory does not leak.\r\n\r\nSimilarly, the problem happens if the validation phase is defined together with `validation_epoch_end` and the corersponding validation DataLoader has `pin_memory==True`. Moreover, even if the training DataLoader defines `pin_memory==True` and `validation_epoch_end` is also defined, the GPU memory leaks.\r\n\r\nI am afraid I will not have time to dig deeper here. But hopefully the maintainers will find this usefull.\r\n"
      },
      {
        "user": "jiahuei",
        "created_at": "2020-05-10T06:32:51Z",
        "body": "This issue occurred to me during validation sanity check even if `pin_memory==False`.\r\nMy `validation_epoch_end ` is defined.\r\n\r\nI had to use this as a temporary fix\r\n```\r\nif __name__ == \"__main__\":\r\n    args = parse_arguments()\r\n    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, args.gpus))\r\n    args.gpus = list(range(len(args.gpus)))\r\n    ...\r\n    trainer = pl.Trainer(gpus=args.gpus)\r\n```\r\n\r\nHowever, if I call `torch.cuda.device_count()` beforehand like so, the issue still occurs. There might be something that occurred when `device_count` is called.\r\n```\r\n    if len(args.gpus) != torch.cuda.device_count():\r\n        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, args.gpus))\r\n        args.gpus = list(range(len(args.gpus)))\r\n```"
      }
    ]
  },
  {
    "number": 943,
    "title": "Support both Namespace and dict for hyperparameter saving/loading. ",
    "created_at": "2020-02-25T18:26:29Z",
    "closed_at": "2020-03-05T12:05:10Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/943",
    "body": "## 🚀 Feature\r\n\r\nLet the user pass a dict to `LightningModule` so that after model saving it can be restored using `load_from_checkpoint` or `load_from_metrics`.\r\n\r\n### Motivation\r\n\r\nCurrently, there is nothing that prevents the user from passing in hyperparameters to `LightningModule` via dictionary (or even somthing else). However, the model loading/saving assumes it is always a `argparse.Namespace`. This could potentially be an issue when `load_from_checkpoint` restores the module with a Namespace passed in instead of dict.\r\n\r\n### Pitch\r\n\r\nThe model saving currently converts Namespace to dict and the model loading converts it back to Namespace. \r\n\r\n**Pitch 1:** Also save the type inside the checkpoint, e.g., \r\n\r\n```\r\ncheckpoint['hparams'] = dict(hparams)\r\ncheckpoint['hparams_type'] = type(hparams)\r\n```\r\nand when restoring we instantiate with the appropriate type. \r\n\r\n**Pitch 2:** Dump the whole hparams object (Namespace, dict, ...) into checkpoint without converting it to dict first and let pickle take care of the rest. Most flexible option but could give problems when loading from checkpoint.\r\n\r\n\r\n### Alternatives\r\n\r\nSomehow restrict the user to only use argparse.\r\n\r\n### Additional context\r\n\r\nThe idea was suggested by @williamFalcon in PR #919.\r\n ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/943/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-02-25T18:27:48Z",
        "body": "support both argparse and dict..."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-02-26T04:11:04Z",
        "body": "partially related to #525 \r\n\r\nAlso, there was an attempt to pickle the hparams directly but it never got merged: #615 \r\n@neggert do you have any thoughts about this? would pickling the hparams object directly work?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-05T12:05:10Z",
        "body": "implemented in #1029 "
      }
    ]
  },
  {
    "number": 927,
    "title": "Automate choosing sampler ",
    "created_at": "2020-02-24T18:49:00Z",
    "closed_at": "2020-02-25T03:23:26Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/927",
    "body": "## 🚀 Feature\r\nLet's automate choosing the sampler.\r\n\r\n### \r\nCase 1 (DDP, training):\r\nDefault to distributedSampler\r\n```\r\nsampler = DistributedSampler(dataset)\r\n```\r\n\r\nCase 2 (training):\r\n```\r\nsampler = RandomSampler(dataset)\r\n```\r\n\r\nCase 3 (val, test):\r\n```\r\nsampler = SequentialSampler(dataset)\r\n```\r\n\r\nCase 4 (tpu, train, val, test):\r\n```\r\nxm.DistributedSampler(dataset)\r\n```\r\n### Motivation\r\n\r\nSamplers are hard and should be automated.\r\n\r\n@srush \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/927/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "srush",
        "created_at": "2020-02-24T19:10:39Z",
        "body": "I don't know if it is possible, but it would also be great is TPU Test at least could somehow be sequential. In NLP it is common just to print out the test results to a file and use an external script for final eval. Having them in order is nice (alternatively I guess id's could be threaded through and sorted.)"
      }
    ]
  },
  {
    "number": 925,
    "title": "Add \"epoch\" options to basic templates",
    "created_at": "2020-02-24T13:10:33Z",
    "closed_at": "2020-02-25T14:46:02Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/925",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nAdd \"epochs\" option to parser of 'basic_examples/lightning_module_template.py'\r\n\r\n### Motivation\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nThanks to 'basic_examples/lightning_module_template.py', I could build my deep learning model. Some beginners like me might build their model from this basic template. However, there are no options to manipulate epochs. I just thought that what people use often should be included in the basic template, so I uploaded my issue.\r\n\r\n### Pitch\r\n<!-- A clear and concise description of what you want to happen. -->\r\nI suggest that the basic template includes \"epoch\" option in the basic template.\r\n\r\n### Alternatives\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nAdd \"epoch\" options to parser of 'basic_examples/lightning_module_template.py'\r\n```python\r\nparser.add_argument('--epochs', default=10, type=int, metavar='N',\r\n                            help='number of total epochs to run')\r\n\r\ntrainer = pl.Trainer(max_epochs=hparams.epochs)\r\n```\r\n\r\n### Additional context\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nI am really enjoying PytorchLightning framework. Thanks 😄 \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/925/comments",
    "author": "baeseongsu",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-02-24T13:11:13Z",
        "body": "Hey, thanks for your contribution! Great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-02-24T18:41:10Z",
        "body": "Possibly linked to #916 and would cover the alternative the op suggests."
      },
      {
        "user": "Borda",
        "created_at": "2020-02-24T23:12:47Z",
        "body": "yeah, it can be solved a part of that one, could you check on the follow-up PR? :]"
      }
    ]
  },
  {
    "number": 908,
    "title": "Neptune Logger improvement",
    "created_at": "2020-02-21T08:41:02Z",
    "closed_at": "2020-03-30T18:12:08Z",
    "labels": [
      "feature",
      "help wanted",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/908",
    "body": "## 🚀 Feature\r\nI think we could improve user experience for people logging experiment\r\ndata with neptune if we let people use the logger after the training is finished.\r\n\r\nAn example would be:\r\n\r\n```python\r\nneptune_logger = NeptuneLogger(..., close_after_train=False)\r\ntrainer = Trainer(logger=neptune_logger)\r\ntrainer.fit(CoolSystem())\r\n```\r\n\r\n- log some external validation metric\r\n\r\n```python\r\nauc = get_external_validation_auc()\r\nneptune_logger.experiment.log_metric('roc_auc_score', auc)\r\n```\r\n\r\n- log performance chart like roc auc\r\n\r\n```python\r\nfig, ax = plt.subplots(figsize=(16, 12))\r\nplot_roc(y, y_pred, ax=ax)\r\nneptune_logger.experiment.log_image('roc_curve', fig)\r\n```\r\n\r\n- log final model \r\n\r\n```python\r\ntorch.save(model.state_dict(), 'final_model.pth')\r\nneptune_logger.experiment.log_artifact('final_model.pth')\r\n```\r\n\r\nI think the only thing that needs changing is this:\r\n\r\n```python\r\n    @rank_zero_only\r\n    def finalize(self, status):\r\n        if self.close_after_train:\r\n             self.experiment.stop()\r\n``` \r\n\r\n### Additional context\r\n\r\nI would be happy to create a PR",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/908/comments",
    "author": "jakubczakon",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-02-21T11:04:12Z",
        "body": "@jakubczakon great suggestions, PR is really welcome!\r\nBtw, would you like extend this kind of feature also for other loggers? "
      },
      {
        "user": "jakubczakon",
        "created_at": "2020-02-21T11:09:48Z",
        "body": "Sure thing!"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-30T16:56:57Z",
        "body": "@jakubczakon how is it going? ^^"
      },
      {
        "user": "jakubczakon",
        "created_at": "2020-03-30T17:23:45Z",
        "body": "Oh, I must have miscommunicated.\r\nI wrote in the PR:\r\n\r\n> @Borda\r\n> I haven't added this option to other loggers as I saw people did some work there already (comet).\r\n> I can add this close_after_fit option to other loggers but I just don't want to step into other folks territory (where they may not appreciate me being) :).\r\n> It's your call.\r\n\r\nAnd since you haven't answered there I assumed we would just stick with `NeptuneLogger`.\r\nWould you like me to play with those other ones?\r\nI'd definitely feel ok improving/updating `TestTubeLogger` and the open-source ones but I don't feel good about touching the \"commercial\" ones.\r\n\r\nWhat do you think @Borda ? "
      },
      {
        "user": "Borda",
        "created_at": "2020-03-30T18:12:08Z",
        "body": "that sounds fair, for `TestTubeLogger` it is different story/repo..\r\nso closing this issue though :robot: "
      }
    ]
  },
  {
    "number": 904,
    "title": "Update torchvision to 0.5.0",
    "created_at": "2020-02-20T08:54:10Z",
    "closed_at": "2020-02-20T09:23:44Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/904",
    "body": "## 🚀 Feature\r\nUpdate torchvision to 0.5.0 (pip install downgrades to 0.4.2)\r\n\r\n```bash\r\nERROR: torchvision 0.4.2 has requirement torch==1.3.1, but you'll have torch 1.4.0 which is incompatible.\r\n\r\nInstalling collected packages: tqdm, torchvision, oauthlib, requests-oauthlib, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, markdown, grpcio, absl-py, werkzeug, protobuf, tensorboard, pytorch-lightning\r\n  Attempting uninstall: torchvision\r\n    Found existing installation: torchvision 0.5.0\r\n    Uninstalling torchvision-0.5.0:\r\n      Successfully uninstalled torchvision-0.5.0\r\n\r\nSuccessfully installed absl-py-0.9.0 cachetools-4.0.0 google-auth-1.11.2 google-auth-oauthlib-0.4.1 grpcio-1.27.2 markdown-3.2.1 oauthlib-3.1.0 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytorch-lightning-0.6.0 requests-oauthlib-1.3.0 rsa-4.0 tensorboard-2.1.0\r\ntorchvision-0.4.2 tqdm-4.43.0 werkzeug-1.0.0\r\n```\r\nUpgrade dependency so that the above doesn't happen.\r\n\r\n### Motivation\r\n\r\nI recently conda installed pytorch and it came with `torchvision-0.5.0`. However, after pip installing lightning, it downgraded to `torchvision-0.4.2`.\r\n\r\n### Pitch\r\n\r\nSupport the latest torchvision.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/904/comments",
    "author": "NumesSanguis",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-02-20T08:54:51Z",
        "body": "Hey, thanks for your contribution! Great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-02-20T09:23:44Z",
        "body": "Hi, thanks for your suggestion. We have already dropped `torchvision` dependency #797\r\nPls, it will be released in 0.6.1 or you can use actual master :]"
      }
    ]
  },
  {
    "number": 901,
    "title": "[dp/ddp mode]Enable checking which process I'm in",
    "created_at": "2020-02-20T00:57:24Z",
    "closed_at": "2020-03-07T00:05:38Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/901",
    "body": "## 🚀 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\n### Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nThe motivation is that in dp/ddp mode, one print statement in `training_step` ends up with multiple printed lines (4 lines because I'm using 4 GPUs)\r\n### Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nI hope that there's a `self.rank` to let the user check which process they're in.\r\n\r\nSo they may choose to print for only rank 0, or print the rank to screen with the msg to avoid confusion.\r\n\r\n### Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n@williamFalcon suggests make it automatically handled with a self.print\r\n### Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\nI'd prefer a self.rank because horovod has this ranking. I'm not sure if there are similar things in pytorch.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/901/comments",
    "author": "Ir1d",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-03-07T00:05:38Z",
        "body": "```self.print()``` is available in 0.7.1\r\n\r\nyou can also check:\r\n```self.trainer.proc_rank```"
      }
    ]
  },
  {
    "number": 888,
    "title": "transfer_batch_to_gpu not moving List[torch.tensor] and like datastructures to GPU",
    "created_at": "2020-02-18T00:04:36Z",
    "closed_at": "2020-03-07T00:06:07Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/888",
    "body": "## 🐛 Bug\r\n\r\n```trainer.transfer_batch_to_gpu(torch.randn(3, 3), 0)``` gives:\r\n```\r\ntensor([[-1.0600,  0.3306,  1.1276],\r\n        [ 1.0012, -0.2687, -0.5493],\r\n        [-0.5619, -1.7161, -0.8625]], device='cuda:0')\r\n```\r\n\r\nBut\r\n```trainer.transfer_batch_to_gpu([ torch.randn(3, 3) ], 0)``` gives\r\n```\r\n[tensor([[ 1.3631,  0.3408, -1.1074],\r\n         [-1.1176, -0.8056,  0.2937],\r\n         [-0.4235,  0.7321, -0.8811]])]\r\n```\r\n\r\nwhich is not the expected behaviour as we are supposed to move all tensors in the data structure onto GPU. \r\n\r\nSame issue occurs for tuple and dictionary datastructures.\r\n\r\nAlso - would it be possible to get gpu transfer for argparse.Namespace datastructures, or class datastructures? \r\n\r\nIE if I make a class like so: \r\n```\r\nclass Batch(object):\r\n    def __init__(self, \r\n                 src_input_ids,\r\n                 tgt_input_ids,\r\n                 src_attn_mask,\r\n                 tgt_attn_mask,\r\n                 src_seg_ids,\r\n                 tgt_labels\r\n                ):\r\n        self.__dict__.update(locals())\r\n```\r\nit would be cool if I could directly pass this in with gpu transfer.\r\n\r\nThanks a lot !\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\nJust copy one of the examples to get the `Trainer` class.\r\n\r\nThen do `trainer=Trainer(...)` and run the above code\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\n\r\nPython 3.6\r\nPytorch 1.3.1\r\nCuda 10.1\r\nPytorchLightning master (as of today)\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/888/comments",
    "author": "Laksh1997",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-02-22T17:08:19Z",
        "body": "I cannot reproduce this on master. For me\r\n`trainer.transfer_batch_to_gpu([ torch.randn(3, 3) ], 0)`\r\n\r\nreturns\r\n\r\n```\r\n[tensor([[ 1.6940, -0.8183,  0.6075],\r\n        [ 0.5152,  0.2925, -0.3388],\r\n        [ 0.9064,  0.2509,  0.7852]], device='cuda:0')]\r\n```\r\nas expected. Same with tuples and dicts. Was it perhaps fixed in the meantime? Could you try again @Laksh1997? "
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-03-07T00:06:07Z",
        "body": "we can reopen if it's still an issue"
      }
    ]
  },
  {
    "number": 885,
    "title": "Fix .test() on ddp",
    "created_at": "2020-02-17T18:14:39Z",
    "closed_at": "2020-03-03T02:50:39Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/885",
    "body": "This might be broken on notebooks only.\r\n#875 solves a few problems with .test()\r\n\r\nHowever, ddp + .test might be broken on notebooks because of the \"spawn\" option. (likely #747).\r\n ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/885/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-02-17T18:15:47Z",
        "body": "Anyone interested in looking at this?\r\n@Borda "
      },
      {
        "user": "sneiman",
        "created_at": "2020-02-17T18:51:45Z",
        "body": "I am investigating this. Thats how I ran into the spawn issue with shared Q's. I am not using a notebook. This is (I believe) because ddp reduces tensors and parameters back to dev_ids[0] and there is no obvious way to get them back to the spawning CPU. I am slowly making progress, but don't have a lot of consistent time to put in."
      },
      {
        "user": "s-rog",
        "created_at": "2020-02-24T02:29:16Z",
        "body": "Isn't ddp broken in notebooks in the first place?"
      }
    ]
  },
  {
    "number": 878,
    "title": "Example testing",
    "created_at": "2020-02-17T08:42:16Z",
    "closed_at": "2020-08-17T22:12:00Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix",
      "ci"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/878",
    "body": "## 🚀 Feature\r\nFind a way how to test examples in CI\r\n\r\n### Motivation\r\n\r\nDue to some API modification, our examples may become invalid and we would not notice it until a bug report...\r\n\r\n### Pitch\r\n\r\nThe best way is running examples as a part of CI, on the other hand, it may be quite a time consuming",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/878/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-04-09T11:17:43Z",
        "body": "write a wrapper around the main parsing CLI and add mock with some small arguments\r\n```\r\ndef run_cli():\r\n    arg_params = _get_args()\r\n    run_analyse_videos(**arg_params)\r\n\r\nif __name__ == \"__main__\":\r\n    run_cli()\r\n```\r\nand later \r\n```\r\nfrom unittest.mock import patch\r\n\r\ndef test_cpu_example(tmpdir):\r\n    from pl_examples.cpu_example import run_cli\r\n\r\n    with patch(\r\n        \"argparse._sys.argv\", [\"script.py\", \"--epochs\", 1, \"--output\", str(tmpdir)]\r\n    ):\r\n        run_cli()\r\n```\r\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-08T20:57:08Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 862,
    "title": "Add test guidance to contributer guide",
    "created_at": "2020-02-16T05:18:58Z",
    "closed_at": "2020-05-26T16:00:30Z",
    "labels": [
      "help wanted",
      "good first issue",
      "won't fix",
      "docs"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/862",
    "body": "## 📚 Documentation\r\n\r\nFrom discussion on PR #837 , we should include some guidelines for how code is tested, especially for new contributions. This would probably be included in `CONTRIBUTING.md` in the docs.\r\n\r\nThings to discuss would be:\r\n- usage of fixtures in tests\r\n- parameterized tests\r\n- expected level of documentation in tests",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/862/comments",
    "author": "jeremyjordan",
    "comments": [
      {
        "user": "jeremyjordan",
        "created_at": "2020-02-16T05:22:54Z",
        "body": "cc @Borda from our chats on the PR, made this issue"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-01T21:50:25Z",
        "body": "maybe this could also include guidelines about how code is formatted and documented, e.g. format for doc strings, typing, how exceptions are handled etc., so that the codebase has a uniform style. "
      },
      {
        "user": "Borda",
        "created_at": "2020-03-27T14:49:09Z",
        "body": "partially done in #993 "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-26T15:46:41Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 860,
    "title": "coverage check/cleaning",
    "created_at": "2020-02-16T00:08:11Z",
    "closed_at": "2020-04-25T02:59:34Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/860",
    "body": "## 🚀 Feature\r\n\r\nThis is aiming at technical package debt. \r\nIn the coverage config, there is quite a long list of skipped patterns and files which does not look nice...\r\nThis shall be check, and list pruned... also we do not want to lower coverage, which means we shall come with test-cases exposing these patterns...\r\n\r\n\r\n### Motivation\r\n\r\nProper and full test coverage wich will lover chance of crashing...\r\n\r\n### Pitch\r\n\r\nNot found bug is still a bug :D",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/860/comments",
    "author": "Borda",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-04-16T01:25:25Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 855,
    "title": "[Pyright] Cannot access member 'X' for type 'None'",
    "created_at": "2020-02-15T14:38:03Z",
    "closed_at": "2021-03-21T19:57:04Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/855",
    "body": "Pyright raises 89 errors on `master` about `Cannot access member 'X' for type 'None'`. Here is an example in `evaluation_loop.py`:\r\n\r\n```python\r\n                # track outputs for collation\r\n                dl_outputs.append(output)\r\n\r\n                # batch done\r\n                if test:\r\n                    self.test_progress_bar.update(1)  # PYRIGHT ERROR\r\n                else:\r\n                    self.val_progress_bar.update(1)  # PYRIGHT ERROR\r\n                    self.main_progress_bar.update(1)  # PYRIGHT ERROR\r\n            outputs.append(dl_outputs)\r\n\r\n        eval_results = {}\r\n```\r\n\r\nOne way to fix this is to indicate the type of those variables:\r\n\r\n```python\r\nself.test_progress_bar: Any = None\r\n```\r\n\r\nUnless you see a more elegant way?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/855/comments",
    "author": "hadim",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-02-18T15:28:35Z",
        "body": "This shall be fixed, may you consider sed a PR?"
      },
      {
        "user": "hadim",
        "created_at": "2020-02-18T15:30:08Z",
        "body": "Not sure I would have time on the short term but I'll keep it on my TODO list."
      },
      {
        "user": "Dev-Akash",
        "created_at": "2020-03-02T02:14:41Z",
        "body": "Hi there, I am interested in solving this bug, can i work on this ?"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-02T07:09:31Z",
        "body": "Hi @Dev-Akash sure, that would be great! "
      },
      {
        "user": "Borda",
        "created_at": "2020-06-05T10:06:03Z",
        "body": "Hi @Dev-Akash how is it going? any progress here... :rabbit: "
      },
      {
        "user": "Borda",
        "created_at": "2020-06-05T10:07:12Z",
        "body": "@hadim maybe we can also add this check to CI, does it run on Linux?"
      },
      {
        "user": "hadim",
        "created_at": "2020-06-05T10:45:11Z",
        "body": "You can install it with npm (see website). Sorry I don't have time to do that at the moment."
      },
      {
        "user": "uditarora",
        "created_at": "2020-06-08T13:17:39Z",
        "body": "I'd be happy to take a look at this if no one else is working on it. Let me know.\r\n\r\nCC: @Borda "
      },
      {
        "user": "Borda",
        "created_at": "2020-06-11T20:37:03Z",
        "body": "@Dev-Akash still interested in finishing the type check after #2121?"
      },
      {
        "user": "Dev-Akash",
        "created_at": "2020-08-17T06:24:17Z",
        "body": "> @Dev-Akash still interested in finishing the type check after #2121?\r\n\r\nCurrently not able to contribute in this issue, you can unassign me from this issue.\r\nThank you !"
      },
      {
        "user": "Borda",
        "created_at": "2020-08-17T21:12:30Z",
        "body": "@uditarora still interested in taking this over? :]"
      },
      {
        "user": "uditarora",
        "created_at": "2020-08-19T15:13:10Z",
        "body": "> @uditarora still interested in taking this over? :]\r\n\r\nSure, I have some free time over the next week. Let me try and fix more Pyright errors :)\r\n\r\nLet me know if you have something specific in mind that I should look at. Else I can pick up some module randomly."
      },
      {
        "user": "Borda",
        "created_at": "2020-08-21T17:04:22Z",
        "body": "I think that we may start with ones which are most used that e continue with others... for the most used lest doing just module by  module so the PR is not pending for long... and avoid collisions"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-10-24T23:48:10Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-10-25T10:07:21Z",
        "body": "@uditarora how is it going here?"
      },
      {
        "user": "uditarora",
        "created_at": "2020-10-28T21:22:52Z",
        "body": "> @uditarora how is it going here?\r\n\r\nI made a little progress earlier but not enough to send a PR, before sadly getting caught up with school-related deadlines. I'm not sure how much I'll be able to contribute for the next month, so please feel free to assign it to someone else in the meanwhile if they're interested :)"
      },
      {
        "user": "gianscarpe",
        "created_at": "2020-11-23T14:16:10Z",
        "body": "As discussed with @Borda, I would suggest improving the typing of all the code base to make static type checking more robust! :) "
      },
      {
        "user": "akihironitta",
        "created_at": "2021-02-21T08:37:56Z",
        "body": "I think we can close this issue as we have a tracking issue for typing: #5023."
      }
    ]
  },
  {
    "number": 828,
    "title": "Managing Checkpoints",
    "created_at": "2020-02-12T17:35:53Z",
    "closed_at": "2020-06-17T12:45:21Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/828",
    "body": "## 🚀 Feature\r\n\r\nDuring training there is support for checkpointing multiple variants of the model. However to utilize these checkpoints you need to parse the file names to get out information. It would be nice if lightning either wrote out statistics to make it easy to find the right model, or even better allowed you to load based on validation statistics. \r\n\r\n```\r\nload_from_checkpoint(ckpt, metric=‘f-score’, mode=‘max’)\r\n``` ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/828/comments",
    "author": "srush",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-02-20T13:20:35Z",
        "body": "sounds interesting, @srush would you be interested in submitting such PR?"
      },
      {
        "user": "Borda",
        "created_at": "2020-04-09T11:04:04Z",
        "body": "Well, I would maybe rather add a method that would return a path to such checkpoint and then you can load it on your own...\r\n```\r\nfind_checkpoint(folder, metric=‘f-score’, mode=‘max’)\r\n```\r\ncc: @PyTorchLightning/core-contributors thoughts?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-08T11:58:48Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 827,
    "title": "Support/Features for step-based models",
    "created_at": "2020-02-12T17:33:30Z",
    "closed_at": "2020-03-30T16:05:24Z",
    "labels": [
      "bug",
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/827",
    "body": "## 🐛 Bug+Feature\r\n\r\nFor models like transformer, we utilize step-based learning rates and evaluation. \r\n\r\nIt would be nice to have several features along this line. \r\n\r\n1) Support for step-based schedulers. Right now we use cannot give the scheduler to lightning, because it calls `scheduler.step(epochs=epoch)` internally which resets the scheduler. \r\n\r\n2) Fix docs for step based evaluation and checkpointing. It seems like it exists but it is hard to tell which is epochs / steps. \r\n\r\n3) Add a `max_steps` option for stopping training. \r\n\r\n4) Helper functions for converting between steps and epochs. For instance, from the number of epochs and parallelism, get the steps for `configure_optimizers`\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/827/comments",
    "author": "srush",
    "comments": [
      {
        "user": "peteriz",
        "created_at": "2020-02-13T09:44:08Z",
        "body": "Hi @srush \r\nFor 1. I have opened issue #806 to discuss about it and I will contribute this soon.\r\nFor 3. I added PR #728 that adds max/min steps for training such models.\r\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-02-20T13:11:15Z",
        "body": "@srush good suggestions, would you mind to send a PR for 2 and 4?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-02-20T13:14:24Z",
        "body": "we just merged a pr #728 about this"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-30T16:05:24Z",
        "body": "feel free to reopen if needed :robot: "
      }
    ]
  },
  {
    "number": 815,
    "title": "Revert \"Fix backwards compatibility for deprecated logging module\"",
    "created_at": "2020-02-11T06:01:29Z",
    "closed_at": "2020-02-19T20:21:16Z",
    "labels": [
      "bug",
      "help wanted",
      "good first issue"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/pull/815",
    "body": "Reverts PyTorchLightning/pytorch-lightning#799",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/815/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-02-11T08:24:32Z",
        "body": "Well, let's prepare the proper fix of #798 in this PR, @tullie?"
      },
      {
        "user": "Borda",
        "created_at": "2020-02-14T09:21:15Z",
        "body": "@tullie could you have look at it..."
      },
      {
        "user": "Borda",
        "created_at": "2020-02-19T20:21:16Z",
        "body": "Closing in favor of #900"
      }
    ]
  },
  {
    "number": 806,
    "title": "Enable stepwise processing flag for schedulers",
    "created_at": "2020-02-09T10:27:03Z",
    "closed_at": "2020-03-05T11:48:54Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/806",
    "body": "## 🚀 Feature\r\nAsking if it makes sense adding a flag in the Trainer class for calling `scheduler.step()` after every update (per #640). \r\n\r\n### Motivation\r\n\r\nThis makes sense for training NLP models such as BERT/XLNet or any other that update the lr based on the current step (and training defined in terms of steps instead of epochs) instead of the current state that it is called after an epoch ends.\r\nI'm aware that users can override `optimizer_step` of the model, however it's a quite common training pattern for training such NLP models.\r\nI think this feature is worthwhile and I will contribute my changes. Let me know if not.\r\n\r\n### Pitch\r\n\r\nAdd `scheduler.step()` call after every step (modify the optimizer step accordingly)\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/806/comments",
    "author": "peteriz",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-02-11T13:03:36Z",
        "body": "instead of a flag, why not add the config to the scheduler? mostly because you might want to do different things to each  scheduler.\r\n\r\nmaybe allow a dictionary of options?\r\n\r\nHere's an option:\r\n```\r\ndef configure_optimizers(...):\r\n    return [adam], [{'scheduler': Scheduler(), \"step_frequency\": \"epoch|batch\"}]\r\n```\r\n\r\n@neggert @jeffling "
      }
    ]
  },
  {
    "number": 754,
    "title": "Allow a flag into trainer to save checkpoints at partial epochs",
    "created_at": "2020-01-26T13:20:57Z",
    "closed_at": "2020-01-26T14:44:53Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/754",
    "body": "## 🚀 Feature\r\nAllow a flag into trainer to save checkpoints at partial epochs\r\n\r\n### Motivation\r\n\r\nWhen you have a large dataset that takes tens of hours per epoch, it's important to have checkpoints along the way. Right now we only get a checkpoint on_epoch_end.\r\n\r\n### Workaround\r\nAlso interested to see if there is a good workaround. I guess I can set a reference to trainer inside my model and manually call on_epoch_end, but that feels like a hack and won't work without changing lightning code because of \r\n\r\nif self.epochs_since_last_check >= self.period:\r\n\r\ninside on_epoch_end.\r\n\r\n### Other ideas?\r\nAlso interested if there are better ways to solve this. Also thought about taking samples out of the dataset to make 'mini epochs,' but this breaks epochs naming convention. (eg an epoch implies all data has been run through once)\r\n\r\nThank you!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/754/comments",
    "author": "thisisjeffchen",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-01-26T14:44:53Z",
        "body": "@thisisjeffchen val_check_interval does this. \r\n\r\n```\r\nTrainer(val_check_interval=0.1)\r\n```\r\n\r\nThis will check every 10% of the epoch and save checkpoints\r\n"
      },
      {
        "user": "thisisjeffchen",
        "created_at": "2020-01-26T15:34:09Z",
        "body": "Ah, thank you. Didn't realize it saves over the last checkpoint, so only one file per epoch despite many saves :)"
      }
    ]
  },
  {
    "number": 716,
    "title": "Be able to pass non-scalar tensors through from validation_step to validation_end",
    "created_at": "2020-01-20T14:38:50Z",
    "closed_at": "2020-01-21T12:14:56Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/716",
    "body": "## 🚀 Feature\r\nCurrently, upon collation of validation steps, there is a line which does `tensor = tensor.item()` **(trainer/logging.py line 90-93)**. This is not good news if the tensor is not a scalar / rank-0 tensor. The solution could be to simply change to `tensor.detach()` if the goal is to just detach the computational graph.\r\n\r\n```\r\n        for k, v in callback_metrics.items():\r\n            if isinstance(v, torch.Tensor):\r\n                callback_metrics[k] = v.item()\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/716/comments",
    "author": "Laksh1997",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-01-21T12:14:56Z",
        "body": "@Laksh1997 the  purpose is to  log those keys. whatever you pass back through validation_step should go into validation_end but it has to be a tensor which is a pytorch limitation.\r\n\r\ncan you give  me an  example where this breaks for  you? will reopen if this needs a code change"
      },
      {
        "user": "MarioIshac",
        "created_at": "2020-09-20T19:38:30Z",
        "body": "I think I just ran into the same limitation. I am trying to log a confusion matrix (`ConfusionMatrix` from `pytorch_lightning.metrics.classification`) as my metric. I'm interested in logging confusion matrices through train / val / test. This is why I'm implementing it at the metric level. The code fails on the exact line that the OP pointed out, particularly because the tensor is 2D. \r\n\r\n@williamFalcon Does the PyTorch limitation force metrics to be tensors, or scalar tensors? I wonder how big of a change it would be to support non-scalar tensors. Looking through `LightningLoggerBase` the type hints suggest that `log_metrics` only takes `Dict[str, float]`, so looks like the code assumes scalar metrics in atleast couple places.\r\n\r\nI could log the confusion matrix through a custom logger I make with a custom method `log_matrix_metric` (such that I could call `self.logger.log_matrix_metric` in `LightningModule`), but I want to be able to use the existing functionality. For example, my scalar metrics are reduced through `reduce_fx`, and for my confusion matrices I would use `torch.sum` instead of the default `torch.mean`. "
      }
    ]
  },
  {
    "number": 698,
    "title": "Error with Iterable Dataset",
    "created_at": "2020-01-17T05:04:44Z",
    "closed_at": "2020-02-26T21:55:19Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/698",
    "body": " \r\n## 🐛 Bug\r\nCurrently, the `num_training_batches` is set to `inf` when the dataset is in iterable-style, which may lead to this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"scripts/msmacro.py\", line 119, in <module>\r\n    main()\r\n  File \"scripts/msmacro.py\", line 115, in main\r\n    trainer.fit(model)\r\n  File \"/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 417, in fit\r\n    self.run_pretrain_routine(model)\r\n  File \"/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 481, in run_pretrain_routine\r\n    self.get_dataloaders(ref_model)\r\n  File \"/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py\", line 199, in get_dataloaders\r\n    self.init_train_dataloader(model)\r\n  File \"/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py\", line 78, in init_train_dataloader\r\n    self.val_check_batch = int(self.num_training_batches * self.val_check_interval)\r\nOverflowError: cannot convert float infinity to integer\r\n```\r\n \r\nworkaround: set `val_check_interval` to an integer.\r\n\r\n\r\nHowever, if the validation dataset is also in iterable style, then the following error will be raised, as there is no dataset type check in loading validation dataset\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"scripts/msmacro.py\", line 119, in <module>\r\n    main()\r\n  File \"scripts/msmacro.py\", line 115, in main\r\n    trainer.fit(model)\r\n  File \"/home/zhaohao/Documents/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 417, in fit\r\n    self.run_pretrain_routine(model)\r\n  File \"/home/zhaohao/Documents/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 481, in run_pretrain_routine\r\n    self.get_dataloaders(ref_model)\r\n  File \"/home/zhaohao/Documents/pytorch-lightning/pytorch_lightning/trainer/data_loading.py\", line 201, in get_dataloaders\r\n    self.init_val_dataloader(model)\r\n  File \"/home/zhaohao/Documents/pytorch-lightning/pytorch_lightning/trainer/data_loading.py\", line 117, in init_val_dataloader\r\n    self.num_val_batches = sum(len(dataloader) for dataloader in self.get_val_dataloaders())\r\n  File \"/home/zhaohao/Documents/pytorch-lightning/pytorch_lightning/trainer/data_loading.py\", line 117, in <genexpr>\r\n    self.num_val_batches = sum(len(dataloader) for dataloader in self.get_val_dataloaders())\r\n  File \"/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 297, in __len__\r\n    return len(self._index_sampler)  # with iterable-style dataset, this will error\r\n  File \"/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/sampler.py\", line 212, in __len__\r\n    return (len(self.sampler) + self.batch_size - 1) // self.batch_size\r\n  File \"/home/zhaohao/.anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 57, in __len__\r\n    raise TypeError('Cannot determine the DataLoader length of a IterableDataset')\r\nTypeError: Cannot determine the DataLoader length of a IterableDataset\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/698/comments",
    "author": "matthew-z",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-01-21T12:24:49Z",
        "body": "@matthew-z want to submit a PR? "
      }
    ]
  },
  {
    "number": 662,
    "title": "Extract dataset definition out of the LightningModule",
    "created_at": "2020-01-04T16:57:15Z",
    "closed_at": "2020-02-26T04:29:06Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/662",
    "body": "## 🚀 Feature\r\nExtract dataset definition out of the `LightningModule`\r\n\r\n### Motivation\r\nSeparation of data from the model. \r\n\r\n### Pitch\r\nThe datasets loaders could easily be passed to the `fit` method directly instead of having to define them inside the `LightningModule`, this avoids having a single class that possibly contains: data, data pipeline params, model, model hyperparams.\r\n\r\nThe basic example cloud look like this:\r\n\r\n```python\r\nfrom pytorch_lightning import Trainer\r\n\r\ntrain_dataloader = DataLoader(...)\r\nval_dataloader = DataLoader(...)\r\n\r\nmodel = CoolSystem()\r\n\r\n# most basic trainer, uses good defaults\r\ntrainer = Trainer()    \r\ntrainer.fit(model, train_dataloader=train_dataloader, val_dataloader=val_dataloader)\r\n```\r\n\r\nIts much more natural to how you usually structure your code in `scikit-learn` or `keras`.\r\n\r\n### Alternatives\r\nThey could also be based to the `Trainer`s constructor.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/662/comments",
    "author": "cgarciae",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-01-05T19:39:35Z",
        "body": "Why not allow for both? add 3 args to trainer or do it in .fit().  \r\n\r\nif any of these datasets are passed in the corresponding lightningModule class isn’t called. \r\n\r\nthoughts?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-01-21T12:29:03Z",
        "body": "@cgarciae want to submit this PR? i agree that we should allow both ways of doing it"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-02-26T04:24:26Z",
        "body": "@cgarciae can the issue be closed or is there something left to do?"
      },
      {
        "user": "cgarciae",
        "created_at": "2020-02-26T04:29:06Z",
        "body": "Looks awesome!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-02-26T11:02:36Z",
        "body": "@lorenzoFabbri @tullie @myleott @ashwinb @shootingsoul @vreis @darktex\r\n\r\nFYI"
      }
    ]
  },
  {
    "number": 652,
    "title": "How to save checkpoint when turning off the validation?",
    "created_at": "2019-12-27T08:33:08Z",
    "closed_at": "2020-03-05T04:05:22Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/652",
    "body": "##  Help\r\n<!-- A clear and concise description of the feature proposal -->\r\nIn some cases like fintuning bert, We don't need the validation step, but have to save the model checkpoint. But I can't make it. If anyone know, please tell me. Thank you!\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/652/comments",
    "author": "Shawn1993",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-03-05T04:05:22Z",
        "body": "closed #1043 "
      }
    ]
  },
  {
    "number": 645,
    "title": "add \"no logging\" option",
    "created_at": "2019-12-21T12:16:07Z",
    "closed_at": "2020-03-07T00:26:59Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/645",
    "body": "I may be wrong, but I see no way to entirely avoid logging during training, which sometimes may be convenient for quick exploratory experiments. \r\n\r\nI suggest to have \r\n```python\r\ntrainer = Trainer(logger=None) \r\n```\r\nconstruct a trainer that does no logging at all",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/645/comments",
    "author": "CarloLucibello",
    "comments": [
      {
        "user": "CarloLucibello",
        "created_at": "2019-12-21T12:32:14Z",
        "body": "wait, actually this avoids logging\r\n```\r\ntrainer = Trainer(logger=False, checkpoint_callback=False)\r\n```\r\nIf `checkpoint_callback` is not set, a `checkpoints` folder is created in the root directory. \r\n\r\nTwo suggestions:\r\n- both arguments should accept the `None` value in addition (or as an alternative) to `False`. It seems more natural to me and the first thing I attempted. \r\n- Setting `logger=False` should also trigger `checkpoint_callback=False`.\r\n\r\nIf there is no support for these suggestions the issue can ble closed. \r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-05T12:30:41Z",
        "body": "> Setting logger=False should also trigger checkpoint_callback=False.\r\n\r\nMaybe not. The user may want to save checkpoints but no logging turned on. The logger is for experiment tracking (loss curves and other visualizations) and independent of checkpointing."
      }
    ]
  },
  {
    "number": 641,
    "title": "Any plans for going beyond SLURM?",
    "created_at": "2019-12-20T07:37:34Z",
    "closed_at": "2020-01-14T04:00:53Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/641",
    "body": "## 🚀 Feature\r\nIt would be nice to have support for various cloud clusters (e.g. we're using mostly Azure VMs and VM scale sets for training). \r\n\r\n### Motivation\r\nSpinning up a VM cluster and starting SLURM each time sounds unoptimal.\r\n\r\nWe could maybe assume that a cluster has been set up and `MASTER_ADDR` and `MASTER_PORT` env variables have been set up for `torch.dist.init_process_group`, since this would probably always require very cloud-specific scripting etc.\r\n\r\nI could maybe try to take a stab at this if pointed to the right direction, although I'm definitely not a backend/ ML infra engineer or anything...\r\n\r\n... or maybe it _is_ easiest to just set up SLURM... opinions, anyone?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/641/comments",
    "author": "harpone",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-01-14T04:00:53Z",
        "body": "yes! currently working on that :)\r\nETA should be a few weeks. we'll first support AWS then Google Cloud!"
      }
    ]
  },
  {
    "number": 633,
    "title": "Trainers' .fit() mimics .test() after first call to .test() + .test() doesn't print metrics",
    "created_at": "2019-12-17T19:39:16Z",
    "closed_at": "2020-03-03T02:50:39Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/633",
    "body": "## 🐛 Bug\r\n\r\n1) After first call to `Trainer.test()` all subsequent calls to `Trainer.fit()` exhibit output behavior of `Trainer.test()`\r\n2) `Trainer.test()` doesn't print metrics (and returns `None`) returned by `LightningModule.test_end()`\r\n\r\n\r\n### To Reproduce\r\nRun following code in a `Python 3.6.8` env with `torch=1.3.1` and `pytorch_lightning=0.5.3.2` installed:\r\n#### Code sample\r\n\r\n<details>\r\n  <summary>Click to view the code sample.</summary>\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\n\r\nimport pytorch_lightning as pl\r\n\r\nprint(torch.__version__, pl.__version__)\r\n\r\n\r\nclass TestModule(pl.LightningModule):\r\n\r\n    def __init__(self, bs):\r\n        super(TestModule, self).__init__()\r\n        self.fc = nn.Linear(2, 2)\r\n        self.bs = bs\r\n        self.criterion = nn.MSELoss()\r\n\r\n    def forward(self, x):\r\n        x = self.fc(x)\r\n        return x\r\n    \r\n    def training_step(self, batch, batch_nb):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return {'loss': self.criterion(y_hat, y)}\r\n\r\n    def test_step(self, batch, batch_nb):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return {'test_loss': self.criterion(y_hat, y)}\r\n    \r\n    def test_end(self, outputs):\r\n        test_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\r\n        test_metrics = {'test_loss': test_loss}\r\n        return {'progress_bar': test_metrics, 'log': test_metrics}\r\n\r\n    def configure_optimizers(self):\r\n        self.optimizer = optim.Adam(self.parameters())\r\n        return self.optimizer\r\n\r\n    @pl.data_loader\r\n    def train_dataloader(self):\r\n        x = torch.rand(1000, 2) - 0.5\r\n        y = torch.sign(x)\r\n        ds = TensorDataset(x, y)\r\n        dl = DataLoader(ds, batch_size=self.bs, shuffle=True)\r\n        return dl\r\n\r\n    @pl.data_loader\r\n    def test_dataloader(self):\r\n        x = torch.rand(100, 2) - 0.5\r\n        y = torch.sign(x)\r\n        ds = TensorDataset(x, y)\r\n        dl = DataLoader(ds, batch_size=self.bs * 2)\r\n        return dl\r\n\r\n\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n\r\nnet = TestModule(bs=32).to(device)\r\n\r\nepochs = 10\r\ntrainer = pl.Trainer(gpus=-1, max_nb_epochs=epochs, min_nb_epochs=epochs)\r\n\r\ntrainer.fit(net)\r\ntrainer.test()\r\ntrainer.fit(net)\r\ntrainer.fit(net)\r\n```\r\n\r\n</details>\r\n\r\n#### Code output\r\nOutput of the sequence of calls `fit` -> `test` -> `fit` -> `fit`\r\n```\r\n1.3.1 0.5.3.2\r\nEpoch 10: 100%|██████████| 32/32 [00:00<00:00, 272.26batch/s,\r\nbatch_nb=31, gpu=0, loss=1.009, v_nb=65]\r\nTesting: 100%|██████████| 2/2 [00:00<00:00, 222.23batch/s]\r\nTesting: 100%|██████████| 2/2 [00:00<00:00, 357.40batch/s]\r\nTesting: 100%|██████████| 2/2 [00:00<00:00, 380.52batch/s]\r\n```\r\n### Expected behavior\r\n\r\n1) `Trainer.fit()` should always run model training, even after it was already tested once via `Trainer.test()`.\r\n2) `Trainer.test()` should return metrics produced by `LightningModule.test_end()`. What is processing the whole test dataset through the model good for if not collecting performance metrics?\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Click to view the environment.</summary>\r\n\r\n```\r\nPyTorch version: 1.3.1 [0/47800]\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: TITAN V\r\nNvidia driver version: 418.87.00\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip3] botorch==0.1.4\r\n[pip3] gpytorch==0.3.6\r\n[pip3] numpy==1.17.4\r\n[pip3] pytorch-lightning==0.5.3.2\r\n[pip3] torch==1.3.1\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchvision==0.4.2\r\n[conda] Could not collect\r\n```\r\n\r\n</details>\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/633/comments",
    "author": "lmartak",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-01-21T12:36:50Z",
        "body": "@lmartak good catch. looks like we need to reset a few flags after test?\r\nsubmit a PR?"
      }
    ]
  },
  {
    "number": 629,
    "title": "slim down progress bar default printing",
    "created_at": "2019-12-15T09:44:08Z",
    "closed_at": "2020-02-05T11:24:43Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/629",
    "body": "## 🚀 Feature\r\nThe progress bar is cluttered, too many prints by default. Let's remove some of them.\r\n\r\n### Pitch\r\nHi guys,\r\nI feel the progress bar contains some uninformative prints and leaves little space for user customization. \r\n\r\nThis is the progress bar print before validation, with empty tqdm_dict (so no user addition to the progress bar metrics):\r\n```\r\nEpoch 1:  39%|███▉      | 343/875 [00:07<00:10, 48.46batch/s, batch_nb=342, gpu=0, loss=0.408, v_nb=18]  \r\n```\r\n\r\nThis is after a validation step where I add to the tqdm_dict validation accuracy and loss:\r\n```\r\nEpoch 2:  20%|█████████████████▏                                                                      | 171/875 [00:03<00:11, 58.99batch/s, batch_nb=170, gpu=0, loss=0.267, v_nb=21, val_acc=91.8, val_loss=0.278]\r\n```                                                \r\nTo avoid more clutter and make some room for user-defined metrics, I propose to **remove** the following outputs from the progress bar:\r\n- **batch_nb**: this is redundant, essentially the same number is printed on the left\r\n- **loss**: this is something always computed in the training step. If the user wants to show it, he just adds it to the tqdm_dict\r\n- **gpu**: this info is useless in single-gpu setting, and likely also in multi-gpu ones. \r\n- **v_nb**:  This is the experiment version number, which is very convenient to know. Since it stays the same during the whole experiment, we could just print it by default before the training starts. \r\n\r\nAlso, I propose to rename \"batch/s\" to tqdm default \"it/s\"  to save a few extra characters. \r\n                                                                                       \r\nPinging @tullie @Borda since this discussion almost started in #531 .\r\n\r\nBest,\r\nC",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/629/comments",
    "author": "CarloLucibello",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-12-16T16:21:38Z",
        "body": "great suggestion. let's make all the changes you suggested except:\r\n- keep loss\r\n- keep v_nb (but rename to v_num)\r\n\r\nSubmit a PR when ready!"
      },
      {
        "user": "tullie",
        "created_at": "2019-12-16T19:58:48Z",
        "body": "Yeah great suggestion. This is something i've been thinking about as my tqdm dictionaries tend to be pretty big.\r\n\r\n@williamFalcon, what's your reasoning for keeping loss? One issue I have with the default loss is that it only displays with 3 precision numbers so I have to put it in the tqdm anyway. Of course, that could be fixed in other ways but this seems like a decent solution. I'd suggest keeping loss as a default only when the tqdm dictionary isn't supplied.\r\n\r\nI'd like to hear your justification on keeping v_nb too. It does seem wasteful considering it's constant throughout training."
      },
      {
        "user": "CarloLucibello",
        "created_at": "2019-12-16T21:22:43Z",
        "body": "I would be as lean as possible with the defaults because it is quite easy for the user to add stuff, while  it is impossible to remove the defaults.\r\n\r\n> I'd suggest keeping loss as a default only when the tqdm dictionary isn't supplied.\r\n\r\nI like this proposal: with a \"default\" tqdm_dict that we can override we achieve both convenience and full flexibility"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-01-21T13:32:01Z",
        "body": "@CarloLucibello @tullie want to submit a PR?"
      }
    ]
  },
  {
    "number": 600,
    "title": "map_location hasn't been implemented in load_from_checkpoint",
    "created_at": "2019-12-06T22:29:59Z",
    "closed_at": "2019-12-15T04:24:46Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/600",
    "body": "**Describe the bug**\r\nCurrent `load_from_checkpoint` function does not support to load a gpu checkpoint on cpu devices, i.e. map the location. \r\n\r\n**To Reproduce**\r\n```\r\n# on a cpu only device:\r\nModel.load_from_checkpoint(\"GPU_CHECKPOINT_FILE\")\r\n```\r\n\r\n**Expected behavior**\r\n`AssertionError: Torch not compiled with CUDA enabled`\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: macOS Catalina 10.15.1\r\n - Browser: chrome\r\n - Version: 0.5.3.1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/600/comments",
    "author": "haossr",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-12-07T17:18:40Z",
        "body": "weird, i thought we did. mind submitting a PR?"
      },
      {
        "user": "haossr",
        "created_at": "2019-12-07T21:57:48Z",
        "body": "Sure will do. "
      },
      {
        "user": "gaoalexander",
        "created_at": "2025-01-31T00:01:50Z",
        "body": "This appears to still be unresolved, currently using Lightning 2.3.0."
      }
    ]
  },
  {
    "number": 565,
    "title": "State maintenance in DP",
    "created_at": "2019-12-02T09:57:38Z",
    "closed_at": "2020-12-12T23:56:14Z",
    "labels": [
      "feature",
      "help wanted",
      "strategy: dp (removed in pl)"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/565",
    "body": "In many image generation tasks with GANs, generator and discriminator is trained through the same generated image single iteration.\r\nIn PyTorch Lightning, the procedure is written like below:\r\n```python\r\ndef training_step(self, batch, batch_nb, optimizer_i):\r\n    foo = batch['foo']\r\n    bar = batch['bar']\r\n\r\n    if optimizer_i == 0:  # train discriminator\r\n        self.foo_out = self.netG(foo)  # register as a instance variable\r\n\r\n        # calc d_loss\r\n        d_loss = ...\r\n\r\n        return {'loss': d_loss}\r\n\r\n    elif optimizer_i == 1:  # train generator\r\n        # common reconstruction error\r\n        g_loss = F.l1_loss(self.foo_out, bar)\r\n        # other losses\r\n        ...\r\n\r\n        return {'loss': g_loss}\r\n```\r\nIt works well on single GPU, however, `self.foo_out` has been flushed in `optimizer_i == 1` branch when DP is set.\r\n\r\nI think it is a undesired behavior, any help or fix?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/565/comments",
    "author": "S-aiueo32",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-12-04T12:11:17Z",
        "body": "@S-aiueo32 yeah, this is a limitation of PyTorch. I've been looking at how to maintain state when using DP but there seems to be no clear way... \r\n\r\n@pietern I think we talked about this a few months ago. Any suggestions on how to maintain state when using DP?"
      },
      {
        "user": "pietern",
        "created_at": "2019-12-04T13:37:11Z",
        "body": "DP replicates the source module for every call to forward. If you want to maintain state, you can't do this and rather should replicate once and then broadcast parameters and buffers from `module[0]` to the others. See `torch/nn/parallel/{data_parallel,replicate}.py` for more details. You'll see a section that broadcasts and sets the parameters/buffers. That's what still needs to be done for every iteration. The part that runs `_replicate_for_data_parallel` is what you'd want to skip."
      },
      {
        "user": "S-aiueo32",
        "created_at": "2019-12-07T15:30:08Z",
        "body": "@williamFalcon @pietern \r\nThank you for the polite explanation.\r\nI understood the limitations and that it is not avoidable as long as `LightningModule` inherits `nn.Module`."
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-12-07T15:33:22Z",
        "body": "actually, it should be avoidable given the explanation above. we just need to make the appropriate changes to the dp subclass"
      },
      {
        "user": "pietern",
        "created_at": "2019-12-19T10:18:00Z",
        "body": "This should be a companion class to `nn.DataParallel`. I don't want to change the behavior of the existing wrapper because I'm sure folks depend on replicating the model on every call to forward. It shouldn't be too hard though, and can use `nn.DataParallel` as a starting point."
      },
      {
        "user": "BradSegal",
        "created_at": "2020-03-10T20:34:35Z",
        "body": "Just wanted to check if there was any update/advice on this type of issue? I've got a similar situation with a GAN producing images in the first optimizer iteration then using them to update the discriminator in the second. It works well on a single GPU, but when distributing I run into the same issue. I initially thought adding the property as a buffer would maintain it, but it seems to be flushed when using DP in the same way. Is the only solution to run the generator in the discriminator's optimizer iteration? "
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-05-09T21:02:09Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-12-05T23:26:45Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      }
    ]
  },
  {
    "number": 508,
    "title": "When no val dataloader is present and user implements validation_step need to throw useful error ",
    "created_at": "2019-11-14T12:51:45Z",
    "closed_at": "2020-04-03T06:10:11Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/508",
    "body": "I have been using pytorch before, but for performance I decided to use lightning. I rewrote my pytorch into a pl.LightningModule class. While training for the first epoch, everything seems fine, but when it starts validation there is a TypeError: 'NoneType' object is not iterable.\r\nEpoch 1: 100%|████████████████████████| 6514/6514 [01:27<00:00, 71.17batch/s, batch_nb=6513, gpu=0, loss=1.099, v_nb=7]\r\nValidating: 0batch [00:00, ?batch/s]\r\n\r\n\r\n```\r\ntrain_dataset = t.utils.data.TensorDataset(X,y)\r\ntrainloader = t.utils.data.DataLoader(train_dataset, batch_size=64)\r\n\r\nclass FastNN(pl.LightningModule):\r\n\r\n    def __init__(self, dataload):\r\n        super(FastNN, self).__init__()\r\n        self.fc1 = nn.Linear(110, 1024)\r\n        self.fc2 = nn.Linear(1024, 512)\r\n        self.fc3 = nn.Linear(512, 256)\r\n        self.fc4 = nn.Linear(256, 128)\r\n        self.fc5 = nn.Linear(128, 64)\r\n        self.fc6 = nn.Linear(64, 32)\r\n        self.fc7 = nn.Linear(32, 3)\r\n        self.dataloader = dataload\r\n\r\n    def forward(self, x):\r\n        x = f.relu(self.fc1(x))\r\n        x = f.relu(self.fc2(x))\r\n        x = f.relu(self.fc3(x))\r\n        x = f.relu(self.fc4(x))\r\n        x = f.relu(self.fc5(x))\r\n        x = f.relu(self.fc6(x))\r\n        x = f.relu(self.fc7(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_nb):\r\n        # REQUIRED\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        loss = t.nn.functional.cross_entropy(y_hat, y)\r\n        tensorboard_logs = {'train_loss': loss}\r\n        return {'loss': loss, 'log': tensorboard_logs}\r\n\r\n    def validation_step(self, batch, batch_nb):\r\n        # OPTIONAL\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return {'val_loss': t.nn.functional.cross_entropy(y_hat, y)}\r\n\r\n    def validation_end(self, outputs):\r\n        # OPTIONAL\r\n        avg_loss = t.stack([x['val_loss'] for x in outputs]).mean()\r\n        tensorboard_logs = {'val_loss': avg_loss}\r\n        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\r\n\r\n    def configure_optimizers(self):\r\n        # REQUIRED\r\n        # can return multiple optimizers and learning_rate schedulers\r\n        # (LBFGS it is automatically supported, no need for closure function)\r\n        return t.optim.Adam(self.parameters(), lr=0.02)\r\n\r\n    @pl.data_loader\r\n    def train_dataloader(self):\r\n        # REQUIRED\r\n        return self.dataloader\r\n\r\nnet = FastNN(trainloader)\r\ntrainer = pl.Trainer(gpus=1)    \r\ntrainer.fit(net)\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/508/comments",
    "author": "Ne0Ment",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-11-14T17:07:46Z",
        "body": "You have to add the val_dataloader method:\r\n```python\r\n    @pl.data_loader\r\n    def val_dataloader(self):\r\n        # REQUIRED\r\n        return self.other_dataloader\r\n```"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-11-14T17:08:48Z",
        "body": "@ILYEEV thanks for pointing out though. We should have a nicer user experience for this error."
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-22T02:06:28Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-02-22T23:03:05Z",
        "body": "@ILYEEV are you interested in sending PR for a better warning? maybe even do this kind of check before starting the training... something like self-diagnostic?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-03T05:11:11Z",
        "body": "This is fixed on master. \r\nIf the user implements `validation_step` but no validation dataloader, it throws an exception:\r\n```\r\npytorch_lightning.utilities.exceptions.MisconfigurationException: \r\nYou have defined `validation_step()`, but have not passed in a val_dataloader().\r\n```\r\nThis issue can be closed."
      }
    ]
  },
  {
    "number": 446,
    "title": "Add a way to operate on all outputs from training_step",
    "created_at": "2019-10-31T18:45:21Z",
    "closed_at": "2019-11-05T15:01:53Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/446",
    "body": "I realized the ddp2 implementation i put together doesn't allow the user to operate on the ouputs of all DP processes.\r\n\r\nFor instance, you calculate some logits on each process and current ddp2 forces the loss to be calculated in each process individually. However, if you wanted to say normalize across all examples in the batch you'd need to somehow share the output of each process.\r\n\r\nCurrently:\r\n```python\r\ntotal_loss = []\r\nfor process:\r\n     # training step\r\n     out = model(x)\r\n     loss = loss(out)\r\n     total_loss.append(loss)\r\n\r\nloss = total_loss.mean()\r\n```\r\n\r\nProposed:\r\n```python\r\nouts = []\r\nfor process:\r\n     # training step\r\n     out = model(x)\r\n     outs.append(out)\r\n\r\n# allow training_end to (softmax for instance) using ALL outputs\r\nloss = model.training_end(outs)\r\nloss.backward()\r\n```\r\n\r\nThe implication is adding an optional:\r\n```python\r\ndef training_end(...):\r\n\r\n```\r\nTo model which when defines gives you all  the outputs of training_step.\r\nIf you don't need anything advanced like this, you return a loss from training_step and don't implement training_end. If you need more advanced control, you implement ```training_end```.\r\n\r\n@tullie @neggert Any thoughts on this?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/446/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "neggert",
        "created_at": "2019-11-01T15:10:51Z",
        "body": "I think it's a little confusing for the user. If I'm understanding correctly, this would only work for dp and ddp2, but not for ddp. Would this break the idea that the same code should work across different backends?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-11-01T15:36:27Z",
        "body": "well, for ddp you don’t need to do this because you’re not sharing across processes. \r\n\r\nthe PR i submitted does this:\r\n1. everything stays the same. \r\n2. if you want to aggregate across all split items then you can implement the training_end. however only used right now in dp or ddp2. "
      },
      {
        "user": "neggert",
        "created_at": "2019-11-01T15:47:31Z",
        "body": "Okay, I think that makes sense. So if you implement `training_end`, everything still works in single-gpu and ddp modes, it's just that you'll only get one item in `outs`?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-11-01T15:51:24Z",
        "body": "in the other modes you just wouldn’t implement training_end (sllingle gpu, etc). if you did though, you would only have 1 out. "
      },
      {
        "user": "neggert",
        "created_at": "2019-11-01T15:52:27Z",
        "body": "Right, I'm just thinking about cases where we want the same code to be able to run with different backends."
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-11-01T15:58:41Z",
        "body": "yeah, that’s fair. then you’d implement it regardless i guess. \r\n\r\nmaybe i have to think about that more. it is annoying to have to do both, but don’t know of another way to keep the abstraction. \r\n\r\nmaybe somehow do it in training_step where everything in forward goes to dp and training_step aggregates but that doesn’t seem straightforward right now"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-11-03T10:49:01Z",
        "body": "@neggert can you take a look at #448?"
      }
    ]
  },
  {
    "number": 433,
    "title": "Fix checkpoint pickle during ddp (on master only)",
    "created_at": "2019-10-25T12:58:04Z",
    "closed_at": "2020-03-30T16:05:46Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/433",
    "body": "@neggert  looks like the new hparams saving fails with ddp/ddp2\r\n```\r\nFile \"/private/home/falc/.conda/envs/ddt2/lib/python3.7/site-packages/pytorch_lightning/callbacks/pt_callbacks.py\", line 245, in on_epoch_end\r\nself.save_model(filepath, overwrite=True)\r\nFile \"/private/home/falc/.conda/envs/ddt2/lib/python3.7/site-packages/pytorch_lightning/callbacks/pt_callbacks.py\", line 224, in save_model\r\nself.save_function(filepath)\r\nFile \"/private/home/falc/.conda/envs/ddt2/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer_io.py\", line 127, in save_checkpoint\r\ntorch.save(checkpoint, filepath)\r\nFile \"/private/home/falc/.local/lib/python3.7/site-packages/torch/serialization.py\", line 224, in save\r\nreturn _with_file_like(f, \"wb\", lambda f: _save(obj, f, pickle_module, pickle_protocol))\r\nFile \"/private/home/falc/.local/lib/python3.7/site-packages/torch/serialization.py\", line 149, in _with_file_like\r\nreturn body(f)\r\nFile \"/private/home/falc/.local/lib/python3.7/site-packages/torch/serialization.py\", line 224, in\r\nreturn _with_file_like(f, \"wb\", lambda f: _save(obj, f, pickle_module, pickle_protocol))\r\nFile \"/private/home/falc/.local/lib/python3.7/site-packages/torch/serialization.py\", line 296, in _save\r\npickler.dump(obj)\r\nAttributeError: Can't pickle local object 'ArgumentParser.init..identity'\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/433/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "neggert",
        "created_at": "2019-10-28T14:47:46Z",
        "body": "Will take a look."
      },
      {
        "user": "neggert",
        "created_at": "2019-10-30T21:03:45Z",
        "body": "I'm having trouble reproducing this. I'm running\r\n\r\n    python gpu_template.py  --gpus 2 --distributed_backend ddp\r\n\r\non a single node with 2 GPUs (I've locally reverted your patch that catches the error). That runs fine, and I'm able to see that `hparams` are saved to the checkpoint by loading it manually.\r\n\r\nI even tried replacing the `ArgumentParser` with a `HyperOptArgumentParser`, but everything still works. Can you share any more details about what you were doing to get this error?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-02-22T02:06:31Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-02-22T23:12:54Z",
        "body": "@neggert does it mean that the problem is gone? :robot: "
      }
    ]
  },
  {
    "number": 406,
    "title": "Save/load hyperparameters with checkpoint",
    "created_at": "2019-10-21T21:08:23Z",
    "closed_at": "2019-10-23T08:48:25Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/406",
    "body": "Right now, test tube users can have `model.load_from_metrics`, but there's no equivalent functionality for users of other logging frameworks. I believe we discussed a while ago that this functionality should be decoupled from logggers.\r\n\r\nProposal:\r\n* When hyperparameters are stored as an `argparse.Namespace` in the `hparams` property of the model (as already required to get automatic hyperparam logging), save these parameters as part of checkpoints.\r\n* Add a `load_from_checkpoint` classmethod to LightningModule that basically duplicates the functionality of `load_from_metrics`, but gets the hyperparameters from the checkpoint directly.\r\n\r\nI'm happy to send a PR if this proposal looks okay.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/406/comments",
    "author": "neggert",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-10-22T02:15:14Z",
        "body": "yeah that makes sense.\r\n\r\nSo in the checkpoint do:\r\n\r\n```python\r\ncheckpoint['hparams'] = ...\r\n```\r\n?\r\n\r\nThat should be the standard going forward. Logging is nice for Testtube UI purposes to see what went into an experiment. But that's separate from loading weights imho."
      }
    ]
  },
  {
    "number": 356,
    "title": "Support for retain_graph=True",
    "created_at": "2019-10-11T00:44:07Z",
    "closed_at": "2019-10-24T11:56:58Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/356",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nSome models require retain_graph=True, but it's not possible to set it in the .backward() call inside of Trainer.__run_training_batch(...)\r\n\r\n**Describe the solution you'd like**\r\nAdd train_graph member function the LightningModule have the trainer read this option and then pass it into the .backward() call.\r\n\r\n**Describe alternatives you've considered**\r\nDriving a version of Trainer to support retain_graph=True is tough because __run_training_batch and other functions are name-mangled.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/356/comments",
    "author": "momeara",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-10-12T09:47:32Z",
        "body": "why don’t we put a hook for the backward pass and you can override if you need retain_graph?  that keeps it flexible for anything else someone wants to do later"
      },
      {
        "user": "momeara",
        "created_at": "2019-10-12T11:14:28Z",
        "body": "perfect! that keeps with the overall philosophy, which has been very useful btw\n\n> On Oct 12, 2019, at 5:47 AM, William Falcon <notifications@github.com> wrote:\n> \n> ﻿\n> why don’t we put a hook for the backward pass and you can override if you need retain_graph? that keeps it flexible for anything else someone wants to do later\n> \n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-10-24T10:08:42Z",
        "body": "@yl1991 @momeara did you guys want to submit a PR for this?"
      }
    ]
  },
  {
    "number": 344,
    "title": "Stablize api, provide backward compatibility and deprecation warning",
    "created_at": "2019-10-09T12:54:16Z",
    "closed_at": "2019-10-24T02:16:57Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/344",
    "body": "Hi,\r\n\r\nPytorch Lightning is great and I enjoy the Keras-like standard training routine with the highly customizable pytorch experience. \r\n\r\nI do plan to move most of my pytorch models training to PyTorch-lightning but the rapid release this weeks broke my code several time, and maintaining multiple version on different environments is such a headache.\r\n\r\nI do appreciate the move to a better api interface ie tng_dataloader vs training_dataloader.\r\nHowever, I do think it is necessary to provide backward compatibility between minor version update. \r\n\r\nI would like to create a pull-request to provide the polyfilling code and corresponding deprecation warning.  The list of changes I observed in these few days are\r\n\r\n1.  - [x] tng_dataloader -> training_dataloader\r\n\r\n2. - [x] experiment-> logger\r\n\r\nIs there any more things that i can add to the list?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/344/comments",
    "author": "tamyiuchau",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-10-09T14:00:12Z",
        "body": "Great idea! Version 5.0 should have been the last major API change. The minor releases where to fix critical bugs around logging, and the new ddp2.\r\n\r\nYou can find all those changes in #119 "
      },
      {
        "user": "tamyiuchau",
        "created_at": "2019-10-24T02:16:57Z",
        "body": "Implemented."
      }
    ]
  },
  {
    "number": 317,
    "title": "Using multiple loggers",
    "created_at": "2019-10-06T03:43:34Z",
    "closed_at": "2019-10-09T12:25:25Z",
    "labels": [
      "feature",
      "help wanted",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/317",
    "body": "**Describe the solution you'd like**\r\nTrainer could receive a list of loggers instead of only one, all loggers would be called at the appropriate times.\r\n\r\n**Describe alternatives you've considered**\r\nAlternatively could create a customLogger that logged the information to all relevant destinations (eg. tensorboard AND mlflow)",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/317/comments",
    "author": "ceyzaguirre4",
    "comments": [
      {
        "user": "neggert",
        "created_at": "2019-10-08T20:08:55Z",
        "body": "What about using the existing base logger to create a composite logger that takes a list of loggers and calls them all?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-10-09T12:25:56Z",
        "body": "Would it be helpful to add an example of this to the docs?"
      }
    ]
  },
  {
    "number": 305,
    "title": "Add DP DDP benchmarks",
    "created_at": "2019-10-05T00:13:20Z",
    "closed_at": "2020-08-30T09:45:14Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix",
      "example"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/305",
    "body": "Benchmark DP, and all the DDP implementations in Lightning on 1 epoch through cifar-10 across 4 GPUs",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/305/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-02-22T02:06:36Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "Borda",
        "created_at": "2020-02-22T23:08:13Z",
        "body": "What kind of GPUs do you recommend?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-22T23:26:04Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-22T08:35:44Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-08-21T09:16:45Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 300,
    "title": "Add TPU support",
    "created_at": "2019-10-04T19:53:33Z",
    "closed_at": "2020-02-17T21:01:59Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/300",
    "body": "Although still experimental, we should add the according support for it. Likely as a distributed backend flag?\r\n\r\nI'm not familiar with the TPU APIs but I assume there is a DP and DDP version? So, something like\r\n```python\r\nTrainer(distributed_backend='tpu_dp')\r\nTrainer(distributed_backend='tpu_ddp')\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/300/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-02-15T22:41:48Z",
        "body": "shall we catch this one in 0.6.1 or next 0.6.2 as you mentioned we already have nice features..."
      }
    ]
  },
  {
    "number": 297,
    "title": "Pass experiment tags to MLFlowLogger",
    "created_at": "2019-10-04T13:36:04Z",
    "closed_at": "2019-10-09T21:47:18Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/297",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nWhen using MLFlowLogger, I'm unable to easily set experiment tags, like username or run name.\r\n\r\n**Describe the solution you'd like**\r\nAdd parameter `tags=None` which is passed to `MLFlowLogger`. Tags will be passed to `create_run` method\r\n\r\n**Describe alternatives you've considered**\r\nManually hack logger, get experiment from it and set tag there\r\n\r\nIf you don't see any drawbacks, I can make a PR\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/297/comments",
    "author": "festeh",
    "comments": [
      {
        "user": "neggert",
        "created_at": "2019-10-08T20:11:27Z",
        "body": "Sounds like a great idea. Want to take a stab at a PR?"
      }
    ]
  },
  {
    "number": 289,
    "title": "GPU memory show max/min only",
    "created_at": "2019-10-03T17:47:20Z",
    "closed_at": "2019-10-05T14:23:30Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/289",
    "body": "Instead of logging GPU mem for all GPUs, probably better to just show the min/max usage? Maybe the flag can be modified?\r\n\r\n```python\r\n# now\r\nTrainer(log_gpu_memory=False)  \r\n\r\n# proposed\r\nTrainer(log_gpu_memory='min_max')  \r\n\r\n# and for all GPUs\r\nTrainer(log_gpu_memory='all')  \r\n\r\n```\r\n\r\n@adefazio",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/289/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "vhsw",
        "created_at": "2019-10-03T18:09:09Z",
        "body": "Should it be by default `min_max` or empty string or `False`? "
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-10-03T18:23:01Z",
        "body": "default = None"
      }
    ]
  },
  {
    "number": 288,
    "title": "Decouple training_step return from logging",
    "created_at": "2019-10-03T17:45:19Z",
    "closed_at": "2019-10-05T14:23:18Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/288",
    "body": "We should adopt a standard for all validation and training functions.\r\nMaybe something like:\r\n\r\n```python\r\nreturn {loss: loss, \r\nlog: {},\r\nprog: {}\r\n}\r\n```\r\nWhere log goes to self.experiment.\r\nprog goes to the progress bar?\r\n\r\n@neggert @adefazio",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/288/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "adefazio",
        "created_at": "2019-10-03T17:47:41Z",
        "body": "Sounds reasonable"
      }
    ]
  },
  {
    "number": 287,
    "title": "Integrate slurm submission into Lightning",
    "created_at": "2019-10-03T17:40:39Z",
    "closed_at": "2020-05-02T00:09:32Z",
    "labels": [
      "feature",
      "help wanted",
      "good first issue",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/287",
    "body": "Test tube currently abstracts the slurm cluster submission. However, now that Lightning knows about number of nodes, gpus, etc... it should configure the script directly.\r\n\r\nFor instance, if you init a trainer with:\r\n```python\r\nTrainer(distributed_backend='ddp', gpus=8, nb_nodes=20)\r\n```\r\n\r\nLightning should generate the correct SLURM config automatically without the user having to do this.  \r\n\r\nLet's figure out what a good API to do this is.\r\n\r\nSome requirements:\r\n- use best practices given the trainer config.   \r\n- allow users to modify those best practices if they need to.\r\n- allow multiple job submission for things like grid/random search.   \r\n- allow other hyperparam tuning frameworks to interface with this.   \r\n- snapshot the relevant code into a separate directory so that when the job runs, it runs on that code.\r\n\r\n\r\nA proposed solution is to have an option such as:\r\n```\r\ntrainer.fit_on_cluster(email='a@b.com', ..., )\r\n```\r\n\r\nFYI @adefazio\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/287/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-02-22T02:06:37Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-04-22T23:26:03Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ]
  },
  {
    "number": 284,
    "title": "Modify param printing to show top level modules only",
    "created_at": "2019-10-02T16:48:16Z",
    "closed_at": "2019-10-08T19:30:07Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/284",
    "body": "Add a model summary option that just shows the number of parameters for the top level modules. \r\n\r\n- Set pd.options.display.float_format = ’{:,}’.for- mat so it uses comma separators.\r\n- Also summarize the number of parameters like 1,000,234 to 1M.\r\n- use 1k, 1M, 1B, 1T, etc...\r\n\r\nSuggested by @adefazio\r\n\r\nSpecifically, change:\r\n```python\r\nTrainer(print_weights_summary=True)\r\n```   \r\n\r\nTo:\r\n```python\r\n# default\r\nTrainer(weights_summary='full')\r\n\r\n# second option\r\nTrainer(weights_summary='top')\r\n```  ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/284/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2019-10-08T09:43:47Z",
        "body": "@williamFalcon I would like to do this.\r\nRegarding the parameter printing: \r\nDo you prefer `39M` or `39 M` (with space)?\r\nHere are two example outputs.\r\nOption 1 (spacing):\r\n```\r\n        Name         Type Params    In_sizes   Out_sizes\r\n0       c_d1       Linear   39 M    [5, 784]  [5, 50000]\r\n1    c_d1_bn  BatchNorm1d  100 K  [5, 50000]  [5, 50000]\r\n2  c_d1_drop      Dropout    0    [5, 50000]  [5, 50000]\r\n3       c_d2       Linear  500 K  [5, 50000]     [5, 10]\r\n\r\n```\r\nOption 2 (no spacing)\r\n```\r\n        Name         Type Params    In_sizes   Out_sizes\r\n0       c_d1       Linear    39M    [5, 784]  [5, 50000]\r\n1    c_d1_bn  BatchNorm1d   100K  [5, 50000]  [5, 50000]\r\n2  c_d1_drop      Dropout      0  [5, 50000]  [5, 50000]\r\n3       c_d2       Linear   500K  [5, 50000]     [5, 10]\r\n```\r\n\r\nAlso, what do we print after trillion? Perhaps we can stop abbreviating and continue with comma separation:  `100T`, `10,000T`, `1,000,000T` etc. \r\nI know it's unrealistic to have so many params but let's anyway handle these edge cases."
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-10-08T12:15:22Z",
        "body": "awesome! \r\n1. spaces\r\n2. good idea for trillion"
      }
    ]
  },
  {
    "number": 283,
    "title": "Validation_end outputs should be auto logged to logger",
    "created_at": "2019-10-02T16:43:18Z",
    "closed_at": "2019-10-05T14:23:07Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/283",
    "body": "Currently, output of validation_end end up nowhere. We should log them to tf automatically.\r\n\r\nSuggestion by @adefazio",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/283/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "neggert",
        "created_at": "2019-10-03T15:59:32Z",
        "body": "I think right now they get added to `tqdm_dict`, then they end up getting logged during the next training epoch. This means that one epoch's validation metrics end up getting logged many times, which is kind of annoying. I think maybe we should decouple external logging from tqdm.\r\n\r\nThings to log:\r\n* During training epoch, every `row_log_interval`, log loss and anything in 'prog' dictionary returned by `training_step`.\r\n* After validation, log values returned by `validation_end`\r\n\r\nAnything else?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-10-05T14:23:07Z",
        "body": "Fixed in #307 "
      }
    ]
  },
  {
    "number": 265,
    "title": "Add tensorboard logger",
    "created_at": "2019-09-27T15:20:54Z",
    "closed_at": "2019-12-08T04:25:38Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/265",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/265/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-10-02T16:40:09Z",
        "body": "@neggert i think you mentioned adding this at some point. tagged you to track this issue"
      },
      {
        "user": "yassersouri",
        "created_at": "2019-10-10T22:28:20Z",
        "body": "What kind of functionality do you expect for the tensoarboard logger?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-10-10T23:11:56Z",
        "body": "everything listed in the summarywriter. but we that for free. this PR of for a wrapper on that logger (see mlflow or testtube logger)"
      },
      {
        "user": "tshrjn",
        "created_at": "2019-10-12T00:24:26Z",
        "body": "Also, a way to do custom logging, perhaps by getting access to SummaryWriter at end of each forward pass?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-10-12T00:33:12Z",
        "body": "you can use self.logger from anywhere to do whatever you want. check out the gan demo"
      },
      {
        "user": "Borda",
        "created_at": "2019-11-09T12:29:40Z",
        "body": "@williamFalcon could you please update description of this ticket or at least remove the template text... Thx"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-12-04T12:20:54Z",
        "body": "@Borda @yassersouri anyone interested in implementing this?\r\n\r\nThis would be a new logger using strictly summarywriter under the hood from PyTorch."
      }
    ]
  },
  {
    "number": 243,
    "title": "Aggregate output of validation_end across all ddp processes",
    "created_at": "2019-09-23T08:53:27Z",
    "closed_at": "2020-02-11T17:46:37Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/243",
    "body": "In `validation_end` I want to gather all outputs from `validation_step`, then use another file to calculate scores. but when use multi-gpu ddp mode, I find it launches multiple process of `validation_end` to calculate scores. how to make it only call once?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/243/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-09-23T12:09:28Z",
        "body": "hi, the way ddp works, each process is walled from the other. I’m not sure you can transfer arbitrary tensors around but we can look into it (likely using the dist library). \r\n\r\nthe validation_end will be called for every process to calculate all the scores.\r\n\r\nI guess it would be helpful to know what you are trying to do to see how we modify the code. "
      },
      {
        "user": "xiadingZ",
        "created_at": "2019-09-23T12:50:27Z",
        "body": "It seems that I want to do test in val function. I find a workaround to do this only when `self.trainer.proc_rank ==0`, in validation_end. \r\nbut there is another problem\r\n```\r\n-- Process 2 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/home/dingxia/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/home/dingxia/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 815, in ddp_train\r\n    self.__run_pretrain_routine(model)\r\n  File \"/home/dingxia/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 925, in __run_pretrain_routine\r\n    self.__train()\r\n  File \"/home/dingxia/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 953, in __train\r\n    self.run_tng_epoch()\r\n  File \"/home/dingxia/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1004, in run_tng_epoch\r\n    self.__run_evaluation(test=self.testing)\r\n  File \"/home/dingxia/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1292, in __run_evaluation\r\n    self.__add_tqdm_metrics(eval_out_metrics)\r\n  File \"/home/dingxia/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 457, in __add_tqdm_metrics\r\n    for k, v in metrics.items():\r\nAttributeError: 'NoneType' object has no attribute 'items'\r\n```\r\nIf I only want to do validation_end only once and records its score, instead of do it on each process, how can I do?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-09-23T13:12:34Z",
        "body": "so, let’s break this down a bit. \r\n\r\nCase 1 (current):\r\nEach process calls validation_end. If you log anything or monitor early stopping it only uses the metrics from proc 0.  This is equivalent to using 1/nb_process in this calculation. \r\n\r\nCase 2 (i think this is the one we need to support):\r\nSame as above, but use dist.all_reduce for each metric returned from validation_end to get the full value instead of the estimated 1/nb_process value. \r\n\r\nexample:\r\nCase 1:\r\nyou calculate accuracy. then the accuracy you log (only from proc 0) is an estimate of the full accuracy. \r\n\r\nCase 2:\r\nyou calculate accuracy, then the dist.all_reduce is not an estimate but the full accuracy. \r\n\r\nIf your data are uniformly shuffled and your batches are big enough, case 1 and 2 are almost identical. if your batch is too small, the estimate will be a little bit off. \r\n\r\n"
      },
      {
        "user": "neggert",
        "created_at": "2019-09-23T16:28:53Z",
        "body": "Agree that case 2 is important to support, and should maybe even be the default. That seems like what a new user would expect."
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-09-23T16:34:51Z",
        "body": "Yeah, agreed. I'll turn this into a ticket.\r\n\r\nThe first approach I can think of is to make this change:\r\n```python\r\nout = validation_end\r\nfor k, v in out.items():\r\n    out[k] = dist.all_reduce(v)\r\n```"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-09-23T16:38:13Z",
        "body": "@neggert does it also make sense to do the same after training_step? I'm a bit concerned about the speed impact of adding these calls... so maybe only validation_end needs it as it's called once per validation cycle?"
      },
      {
        "user": "neggert",
        "created_at": "2019-09-24T22:09:58Z",
        "body": "I can't think of a good reason to do it after a training step. Any metrics measured batch-by-batch are going to be noisy anyway, so only taking numbers from one process shouldn't make much difference. It's possible there's some use case I haven't thought of, though."
      },
      {
        "user": "neggert",
        "created_at": "2019-09-24T22:18:04Z",
        "body": "I guess I'll also note that averaging across nodes isn't always going to be the right thing to do. Metric learning problems, for instance, actually need a full set of feature vectors to compute recall and NMI.\r\n\r\nThe cleanest thing IMO would be to collect `validation_step` outputs from all nodes using something along the lines of `all_gather`, then pass the whole thing to `validation_end` on each node. I'm not sure how to actually do this, though."
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-10-02T16:41:58Z",
        "body": "FYI @adefazio.\r\n\r\n@neggert Let's go for this behavior:\r\n- reduce each metric on validation_step only.   \r\n"
      },
      {
        "user": "magic282",
        "created_at": "2019-12-07T14:27:35Z",
        "body": "Also having this issue. Temporarily disabling the distributedsampler so I can have a full validation set for each process."
      },
      {
        "user": "brucemuller",
        "created_at": "2020-01-19T16:14:13Z",
        "body": "Hello\r\n\r\n@neggert @williamFalcon did an update happen for this? I'm also trying to aggregate validation_end between ddp GPU processes on a single node. I'm trying to use dist.all_gather or dist.all_gather_multigpu but really unsure how it is done with pytorch lightning. I also think new users would expect info on how to aggregate from multiple processes on GPUs"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-02-11T17:46:37Z",
        "body": "moving the discussion to #702 "
      }
    ]
  },
  {
    "number": 239,
    "title": "Training on a single specific GPU",
    "created_at": "2019-09-20T04:12:14Z",
    "closed_at": "2019-09-20T05:26:38Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/239",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nI can't run my training on 1 single GPU, where I choose the GPU. Running the trainer with :\r\n\r\n`Trainer(gpus='3')` gives :\r\n\r\n> RuntimeError: CUDA error: invalid device ordinal\r\n\r\n**Describe the solution you'd like**\r\nWhen a single GPU ID is given, training run on that specific GPU\r\n\r\n**Describe alternatives you've considered**\r\nWe have to set CUDA_VISIBLE_DEVICES=3 and run with `Trainer(gpus=1)`\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/239/comments",
    "author": "astariul",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-09-20T04:39:52Z",
        "body": "Trainer(gpus=[3])\r\n\r\nNo need to modify cuda flags (in fact, not recommended that you do)"
      },
      {
        "user": "astariul",
        "created_at": "2019-09-20T04:52:03Z",
        "body": "`Trainer(gpus=[3])`\r\n\r\ngives me :\r\n\r\n> gpu available: False, used: False\r\nVISIBLE GPUS: 3\r\n[...]\r\nRuntimeError: CUDA error: invalid device ordinal\r\n\r\n(I have 4 GPU on the server)"
      },
      {
        "user": "astariul",
        "created_at": "2019-09-20T05:26:38Z",
        "body": "It worked when I do :\r\n\r\n`export CUDA_VISIBLE_DEVICES=0,1,2,3`\r\n\r\nbefore running the trainer.\r\n\r\nI have no idea why though..."
      }
    ]
  },
  {
    "number": 214,
    "title": "Add Lightning style guide",
    "created_at": "2019-09-07T13:23:13Z",
    "closed_at": "2020-03-03T22:03:25Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/214",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nIn order to encourage reproducibility, I propose a style guide of best practices for structuring code. This guide will go hand-in-hand with the research-seed for conferences.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/214/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "rish-16",
        "created_at": "2019-10-02T15:13:12Z",
        "body": "Hello there! I'd love to work on this. Any way I can contribute?"
      }
    ]
  },
  {
    "number": 185,
    "title": "Log metric row after validation passes",
    "created_at": "2019-08-31T21:14:40Z",
    "closed_at": "2019-09-01T11:31:21Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/185",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nmetrics.csv file, used with test-tube Experiment, is pretty nice way to report some simple results. Unfortunately, logging gets invoked only after training steps. As a result, I can't get the last validation pass results in this file. \r\n\r\n**Describe the solution you'd like**\r\nOption to write metrics from validation_end in a file, similar to metrics.csv, as a Trainer parameter.\r\n\r\n**Describe alternatives you've considered**\r\nLogging directly in \"validation_end\" method, but I haven't found a way to get the context (Experiment, Trainer state) to enrich the information.\r\nPerhaps some access to active Trainer from LightningModule functions, so we can control logging without global variables or duplicating this functionality.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/185/comments",
    "author": "IvanLazarevsky",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-09-01T11:31:21Z",
        "body": "1. \r\nin any part of the LightningModule you can access self.experiment and self.trainer.  This includes validation_end. \r\n\r\n2. \r\nmetrics.csv logs whatever you add to self.experiment.log(...). In addition, tensorboard metrics can be downloaded from any graph by clicking download as csv. \r\n\r\nis that what you mean?"
      },
      {
        "user": "IvanLazarevsky",
        "created_at": "2019-09-01T11:47:40Z",
        "body": "1. Exactly. Feeling embarrassed not to check that."
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-09-01T13:05:10Z",
        "body": "no need to be embarrassed haha. maybe the docs can be made more clear regarding this?"
      },
      {
        "user": "expectopatronum",
        "created_at": "2019-09-10T11:11:28Z",
        "body": "> metrics.csv logs whatever you add to self.trainer.log(...). In addition, tensorboard metrics can be downloaded from any graph by clicking download as csv.\r\n\r\nI'm confused - I can't find a log function in the trainer class - am I missing something?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-09-10T11:22:56Z",
        "body": "self.experiment.log (typo before)"
      }
    ]
  },
  {
    "number": 183,
    "title": "Add .load function for more flexible loading",
    "created_at": "2019-08-31T11:25:11Z",
    "closed_at": "2019-12-04T12:14:59Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/183",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nI'm using Lightning for a few hours, so maybe I am missing something. pl.LightningModule's tng_dataloader and similar methods couple DataLoaders to the module. Ok, I can pass them as the constructor arguments, but then methods like MyModule.load_from_metrics will not work.\r\n\r\n**Describe the solution you'd like**\r\nOption to pass DataLoaders to Trainer or Trainer.fit instead of specifying them in the LigthningModule.\r\n\r\n**Describe alternatives you've considered**\r\nMake it possible to pass additional arguments to load_from_metrics and other methods, which create a specified LightningModule instance.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/183/comments",
    "author": "IvanLazarevsky",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-08-31T13:21:27Z",
        "body": "you wouldn’t pass them as constructor arguments to the module. why would you want to do that?\r\n\r\ncan u post your code?\r\n\r\nif u want to go between datasets just do:\r\n\r\n```\r\ndef __init__(self, my_arg):\r\n     self.ds = my_arg\r\n\r\ndef tng_dataloader(...):\r\n    if self.ds:\r\n       return cifar10\r\n     else:\r\n        imagenet \r\n```\r\n\r\nalso remember that you can use the standard pytorch methods to load a lightningModule, so you don’t need to use load_from_metrics, it’s there for convenience. \r\n\r\nie:\r\n```\r\ncheckpoint = torch.load(...)\r\nmodel = MyLightningModule(arg1, ...)\r\nmodel.load_state_dict(checkpoint[`state_dict‘])\r\n```\r\n\r\nWe could however modify the load_from_metrics signature to allow for any set of args or create a new method called load\r\n\r\n```\r\nmodel = MyModule.load(any_arg1, anyarg2, ckpt_path, map_location)\r\n```\r\n\r\nFinally, the datasets are lazy eval, so they will never be init unless you call .fit on trainer or call them yourself"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-08-31T21:25:15Z",
        "body": "@ivanlazarevsky this make sense? closing the ticket. i’m renaming this ticket to implement the load function discussed "
      },
      {
        "user": "IvanLazarevsky",
        "created_at": "2019-09-01T06:48:29Z",
        "body": "> @IvanLazarevsky this make sense? closing the ticket. i’m renaming this ticket to implement the load function discussed\r\n\r\nYes, thanks."
      }
    ]
  },
  {
    "number": 179,
    "title": "Pass output from all val dataloaders to `validation_end`",
    "created_at": "2019-08-29T20:19:54Z",
    "closed_at": "2019-09-06T11:37:26Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/179",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nI'm working on a retrieval problem. To compute validation scores, I'd like to compute feature vectors for a query set and a catalog set, then measure information retrieval metrics (precision, recall, etc) of the query set against the catalog set.\r\n\r\nTo do this, I'd like to return the two datasets from `val_dataloader` (already possible), then use `validation_step` to accumulate all the feature vectors for each dataset (already possible). Once I have all the feature vectors, I need some way to bring the vectors from the two datasets together so that I can compute retrieval metrics. Right now, `validation_end` processes the outputs of multiple validation dataloaders separately.\r\n\r\n**Describe the solution you'd like**\r\nChange the behavior of `validation_end` in cases where there are multiple validation dataloaders. I propose that it only be called once, but that it be passed a list of outputs dicts, one from each dataloader. Using it would look something like this:\r\n\r\n```python\r\ndef validation_end(self, outputs):\r\n    query_output, catalog_output = outputs\r\n    query_vectors = torch.cat([o['vectors'] for o in query_output])\r\n    query_labels = torch.cat([o['labels'] for o in query_output])\r\n    catalog_vectors = torch.cat([o['vectors'] for o in catalog_output])\r\n    catalog_labels = torch.cat([o['labels'] for o in catalog_output])\r\n    return retrieval_metrics(\r\n        query_vectors,\r\n        query_labels,\r\n        catalog_vectors,\r\n        catalog_labels\r\n    )\r\n```\r\n\r\n**Describe alternatives you've considered**\r\n* Breaking outside of lightning entirely to do validation\r\n* Concatenating query and catalog datasets, and somehow adding a flag to indicate which dataset a given example came from\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/179/comments",
    "author": "neggert",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-08-29T20:23:14Z",
        "body": "good point. do you want to take a stab at implementing? otherwise one of us can look at this end of week"
      },
      {
        "user": "neggert",
        "created_at": "2019-08-29T20:58:42Z",
        "body": "Sure, I can take a look at it over the weekend."
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-08-30T16:51:20Z",
        "body": "also needs to happen for test_end"
      }
    ]
  },
  {
    "number": 172,
    "title": "propose to enable netlify for previewing the docs in PR",
    "created_at": "2019-08-27T14:06:27Z",
    "closed_at": "2019-10-05T16:05:58Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/172",
    "body": "Deploy-preview feature enables building docs in PR, which make it easier for reviewing",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/172/comments",
    "author": "Ir1d",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-09-02T11:40:49Z",
        "body": "idk what this is. can you explain? why is this needed?"
      },
      {
        "user": "Ir1d",
        "created_at": "2019-09-02T11:43:32Z",
        "body": "it allows previewing the edited docs(for each pr), making reviewing easier"
      }
    ]
  },
  {
    "number": 167,
    "title": "Adding Support for Torchtext iterators",
    "created_at": "2019-08-26T01:35:55Z",
    "closed_at": "2019-08-26T23:04:12Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/167",
    "body": "I recently came across pytorch lightning and i am absolutely loving it  until now. Not having to worry about my training cycle and making it super efficient and fast. It has increased the amount of experiments i can pull off and good results have come out from it. \r\n\r\nRight now, i have been using torchtext with its dataset classes and its custom iterators. But when i tried to use the iterators option from torchtext such as Iterator or BucketIterator instead of Dataloader i get the following error:\r\n\r\n``` TypeError: embedding(): argument 'indices' (position 2) must be Tensor, not NoneType```\r\n\r\nThe problem is that instead of getting a Tensor im getting a NoneType. And i dont know why that is.\r\n\r\nNow, i tried to load the Dataset classes from torchtext with the DataLoader itself and i find the next error:\r\n\r\n```TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torchtext.data.example.Example'> ```\r\n\r\nSo, ideally i would really like to have the torchtext iterators supported with pytorch-lighting. But i dont know if there is a way around this issue that i havent found, still using the torchtext Dataset classes. Could anybody help me out with this?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/167/comments",
    "author": "dehoyosb",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-08-26T01:39:01Z",
        "body": "thanks for bringing this up. \r\nCan you try with the latest version? i think we fixed this. \r\n\r\notherwise, can you post a code snippet that generates this error so we can add a patch?\r\n"
      },
      {
        "user": "dehoyosb",
        "created_at": "2019-08-26T22:57:59Z",
        "body": "Yeah! Thank you, i checked and i had the previous version of the package. With the latest one i can use torchtext iterators with no problem. Thank you very much."
      }
    ]
  },
  {
    "number": 111,
    "title": "Choice of Logging Backend",
    "created_at": "2019-08-13T22:08:04Z",
    "closed_at": "2019-08-13T22:09:40Z",
    "labels": [
      "feature",
      "help wanted",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/111",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently only logging to TensorBoard is supported, but there are many other great logging tools out there (Visdom, W&B, MLflow).\r\n\r\n**Describe the solution you'd like**\r\nThere should be a way for the researcher to choose a logging backend and/or a way to implement/override a custom logger given the interface Lightning requires. \r\n\r\n**Describe alternatives you've considered**\r\nForce everyone to use TensorBoard :(\r\n\r\n**Additional context**\r\nSince the Experiment is a subclass of SummaryWriter, it cannot be changed easily. It would require a lot of refactoring.  \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/111/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-08-13T22:09:39Z",
        "body": "duplicate #47"
      },
      {
        "user": "awaelchli",
        "created_at": "2019-08-13T22:23:06Z",
        "body": "Stupid of me, how did I not see the other issue... Thanks for taking a look at this."
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-08-14T02:15:48Z",
        "body": "all good!!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-08-14T10:48:16Z",
        "body": "@awaelchli want to try implementing this support? (using the other issue)"
      },
      {
        "user": "awaelchli",
        "created_at": "2019-08-15T11:00:01Z",
        "body": "@williamFalcon I would like to try it this weekend. I only discovered lightning recently and I am not yet familiar with the details of this library."
      }
    ]
  },
  {
    "number": 82,
    "title": "Is it possible to make `validation_step` and `val_dataloader` no-ops?",
    "created_at": "2019-08-08T22:12:47Z",
    "closed_at": "2019-08-11T14:03:28Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/82",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nSometimes I don't have a separate validation split, only a train/test split.  I'm trying out pytorch-lightning to prototype / experiment, and trying to see what the best of way of doing this is.\r\n\r\nI could make the train dataset and then do `torch.utils.data.random_split` or use `torch.utils.data.SubsetRandomSampler` to build a validation set as well, but if I don't have enough data (or just don't want to do a separate validation step) this isn't ideal.\r\n\r\n**Describe the solution you'd like**\r\nI'd like to be able to implement only the training_step, train_dataloader, and test_dataloader methods and then have the validation step and validation metrics be omitted (maybe explicit no-ops).  Right now, I'm experimenting with having an empty DataLoader for the validation data.\r\n\r\n**Describe alternatives you've considered**\r\n- Implement `val_dataloader` with an empty (dummy) DataLoader\r\n  - Not sure if this will work yet (if lightning will still call validation_step and validation_end).\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/82/comments",
    "author": "tc-wolf",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-08-09T00:51:54Z",
        "body": "great idea. val call can just be made optional! very easy to do. \r\n\r\ndo you want to give it a shot?\r\n\r\n1. remove notimplemented warning. add pass instead. (from validation_step)\r\n2. allow get val_dataloader to be none. \r\n3. in val loop check if val_loader is none (at very beginning). return if it is none. "
      },
      {
        "user": "tc-wolf",
        "created_at": "2019-08-09T15:59:51Z",
        "body": "Thanks, I'll take a stab at that this weekend!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-08-11T14:03:28Z",
        "body": "live on master now"
      },
      {
        "user": "singularperturbation",
        "created_at": "2019-08-11T17:57:14Z",
        "body": "Dang, too fast for me :)  Thanks for working on this!"
      },
      {
        "user": "pamparana34",
        "created_at": "2020-06-03T20:15:46Z",
        "body": "Sorry for writing to this thread but  how can I use it in my model? Is  it enough to do something like:\r\n\r\n```\r\n@pl.data_loader\r\ndef val_dataloader(self):\r\n     if has_valset:        \r\n        return self.__dataloader(train=False)\r\n    \r\n    # No validation set\r\n    return None\r\n```\r\n\r\nAfter that do I need to do something like:\r\n\r\n```\r\ndef validation_step(self, batch, batch_idx):\r\n    # Is this check necessary?\r\n    if not has_vaset:\r\n        return\r\n   \r\n   # process the validation batch\r\n```"
      }
    ]
  },
  {
    "number": 81,
    "title": "Relax requirement for DistributedSampler with ddp",
    "created_at": "2019-08-08T21:30:28Z",
    "closed_at": "2019-08-10T19:58:13Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/81",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nI have an application where I'm using a custom `BatchSampler` to construct batches for the N-Pairs metric learning loss. I need all of the data to be available on all processes when using `DistributedDataParallel`, so I wouldn't want to use `DistributedSampler`, even if it was compatible with a custom `BatchSampler`. Right now, I've hit a wall because lightning throws this exception:\r\n\r\n```\r\npytorch_lightning.utilities.debugging.MisconfigurationException: \r\nwhen using multiple gpus and multiple nodes you must pass\r\n a DistributedSampler to DataLoader(sampler).\r\n\r\nie: this:\r\ndataset = myDataset()\r\ndataloader = Dataloader(dataset)\r\n\r\nbecomes:\r\ndataset = myDataset()\r\ndist_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\r\ndataloader = Dataloader(dataset, sampler=dist_sampler)\r\n```\r\n\r\n**Describe the solution you'd like**\r\nCould this exception be turned into a warning? I'm all for letting the user know when they're violating best practices, but throwing an exception removes flexibility for advanced users.\r\n\r\n**Describe alternatives you've considered**\r\nI looked at using the dp backend, but that's not going to work because the n-pairs loss needs the entire batch to compute the loss. Splitting it into chunks breaks things.\r\n\r\nIf I'm understanding correctly, this is actually another limitation introduced by Lightning. In a usual `DataParallel` setting, the batch would be merged back together before computing the loss and everything would be fine.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/81/comments",
    "author": "neggert",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-08-09T03:08:07Z",
        "body": "good point. mind changing to a warning and submitting a PR?"
      }
    ]
  },
  {
    "number": 79,
    "title": "Update Lightning compatibility with PyTorch 1.2.0",
    "created_at": "2019-08-08T17:11:17Z",
    "closed_at": "2019-08-08T18:24:42Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/79",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nPyTorch 1.2.0 has breaking changes for the experiment object. \r\nLikely underlying changes to SummaryWriter.   \r\n\r\nFor now, Lightning requires pytorch 1.1.0 but need to update compatibility.   ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/79/comments",
    "author": "williamFalcon",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-08-08T17:18:23Z",
        "body": "@Borda apparently 1.2.0 came out a few hours ago which is breaking the summaryWriter... fixed the pytorch version for now to 1.1.0 unitl i sort that out (thus the 0.4.1 update...)"
      },
      {
        "user": "cwerner",
        "created_at": "2019-08-08T17:20:20Z",
        "body": "Hm, I just downgraded torch to 1.1 but I still have problems with Experiment. I'm not getting any output for TensorBoard. Are there some more examples I could look at?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-08-08T17:22:24Z",
        "body": "you’re probably on a jupyter notebook. tensorbord and jupyter have issues"
      },
      {
        "user": "cwerner",
        "created_at": "2019-08-08T17:23:11Z",
        "body": "I am indeed. Bummer... So no jupyterlab + TensorBoard for now? "
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-08-08T18:25:17Z",
        "body": "@cwerner yeah, maybe in this new version PT just released. update to 1.2.0 and lighting 0.4.2 to see what happens. they changed summaryWriter quite a bit"
      },
      {
        "user": "cwerner",
        "created_at": "2019-08-08T18:26:54Z",
        "body": "Cool. Will do. Thanks 🙏 "
      },
      {
        "user": "Borda",
        "created_at": "2019-08-08T19:21:30Z",
        "body": "Sorry, I don't have connection now.. "
      },
      {
        "user": "cwerner",
        "created_at": "2019-08-09T11:13:07Z",
        "body": "@williamFalcon just a heads-up. Still no output in Lightning, but it get tb output if I use summarywriter directly in my PyTorch Code from jupyterlab (on Port 6006, not in the extension). Could be that I do miss something in the lightning code?!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-08-09T13:51:34Z",
        "body": "@cwerner ok, i need to look into  this bc Experiment IS a SummaryWriter as well.\r\n\r\nCan you post code with both?\r\n\r\nBut at a minimum i can make it so SummaryWriter can just be used directly. That way we can start the process to support other loggers."
      },
      {
        "user": "cwerner",
        "created_at": "2019-08-09T22:08:11Z",
        "body": "Will be of my machine for a few days but will update and try. If I still have issues I’ll post some code here..."
      }
    ]
  }
]