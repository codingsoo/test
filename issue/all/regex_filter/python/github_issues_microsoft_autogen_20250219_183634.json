[
  {
    "number": 3074,
    "title": "[Issue]: In an async group chat with function calling, does having multiple user proxy agents make things go faster?",
    "created_at": "2024-07-03T17:27:55Z",
    "closed_at": "2025-02-03T16:16:37Z",
    "labels": [
      "question",
      "0.2",
      "needs-triage"
    ],
    "url": "https://github.com/microsoft/autogen/issues/3074",
    "body": "### Describe the issue\n\nFrom what I understand, only a user_proxy agent can execute a function.  If I have 3 agents in an async chat who can call 1 different function each, should I have 3 different user proxy agents to execute those functions?  Or is it ok to just have one user_proxy agent?  Will one setup perform faster than the other?  Thank you for your help.\n\n### Steps to reproduce\n\n_No response_\n\n### Screenshots and logs\n\n_No response_\n\n### Additional Information\n\n_No response_",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/3074/comments",
    "author": "tyler-suard-parker",
    "comments": [
      {
        "user": "Mahnoor-Rana",
        "created_at": "2024-07-04T12:26:02Z",
        "body": "yess, it is okay to have one user proxy agent, it will work fine in that case if each agent has their own function"
      },
      {
        "user": "jackgerrits",
        "created_at": "2025-02-03T16:16:37Z",
        "body": "I think the answer is no, unless you have constructed things so that they can happen concurrently instead of simply async. This is easy to do in the core API now"
      }
    ]
  },
  {
    "number": 1806,
    "title": "[Issue]: assistant not giving complete response/stop streaming with incomplete sentence and user_proxy keeps replying empty",
    "created_at": "2024-02-28T08:39:45Z",
    "closed_at": "2024-03-13T03:17:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/microsoft/autogen/issues/1806",
    "body": "### Describe the issue\n\n```\r\n# create an AssistantAgent named \"assistant\"\r\nassistant = autogen.AssistantAgent(\r\n    name=\"assistant\",\r\n    llm_config={\r\n        \"cache_seed\": 41,  # seed for caching and reproducibility\r\n        \"config_list\": config_list,  # a list of OpenAI API configurations\r\n        \"temperature\": 0,  # temperature for sampling\r\n    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\r\n)\r\n# create a UserProxyAgent instance named \"user_proxy\"\r\nuser_proxy = autogen.UserProxyAgent(\r\n    name=\"user_proxy\",\r\n    human_input_mode=\"NEVER\",\r\n    max_consecutive_auto_reply=10,\r\n    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\r\n    code_execution_config={\r\n        \"work_dir\": \"coding\",\r\n        \"use_docker\": False,  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\r\n    },\r\n)\r\n# the assistant receives a message from the user_proxy, which contains the task description\r\nchat_res = user_proxy.initiate_chat(\r\n    assistant,\r\n    message=\"\"\"How to define a junction using scenariogeneration, a Python library. After that, save the Python code in a file.\"\"\",\r\n    summary_method=\"reflection_with_llm\",\r\n)\r\n```\r\n\r\nuser_proxy (to assistant):\r\n\r\nWrite code to define a junction using scenariogeneration, a Python library. After that, save the Python code in a file.\r\n\r\n--------------------------------------------------------------------------------\r\nassistant (to user_proxy):\r\n\r\nTo define a junction using the `scenariogeneration` Python library,\r\n\r\n--------------------------------------------------------------------------------\r\nuser_proxy (to assistant):\r\n\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nassistant (to user_proxy):\r\n\r\nFirst, we need to install the `scenariogeneration` library if\r\n\r\n--------------------------------------------------------------------------------\r\nuser_proxy (to assistant):\r\n\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nassistant (to user_proxy):\r\n\r\nIt seems there was a misunderstanding. Let me provide you with the Python code to\r\n\r\n--------------------------------------------------------------------------------\r\nuser_proxy (to assistant):\r\n\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nassistant (to user_proxy):\r\n\r\nTo define a junction using the `scenariogeneration` Python library,\r\n\r\n--------------------------------------------------------------------------------\r\nuser_proxy (to assistant):\r\n\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nassistant (to user_proxy):\r\n\r\nTo define a junction using the `scenariogeneration` Python library,\r\n\r\n--------------------------------------------------------------------------------\r\nuser_proxy (to assistant):\r\n\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nassistant (to user_proxy):\r\n\r\nTo define a junction using the `scenariogeneration` Python library,\r\n\r\n--------------------------------------------------------------------------------\r\nuser_proxy (to assistant):\r\n\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nassistant (to user_proxy):\r\n\r\nTo define a junction using the `scenariogeneration` Python library,\r\n\r\n--------------------------------------------------------------------------------\r\nuser_proxy (to assistant):\r\n\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nassistant (to user_proxy):\r\n\r\nTo define a junction using the `scenariogeneration` Python library,\r\n\r\n--------------------------------------------------------------------------------\r\nuser_proxy (to assistant):\r\n\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nassistant (to user_proxy):\r\n\r\nTo define a junction using the `scenariogeneration` Python library,\r\n\r\n--------------------------------------------------------------------------------\r\nuser_proxy (to assistant):\r\n\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nassistant (to user_proxy):\r\n\r\nTo define a junction using the `scenariogeneration` Python library,\r\n\r\n--------------------------------------------------------------------------------\r\nuser_proxy (to assistant):\r\n\r\n\r\n\r\n--------------------------------------------------------------------------------\r\nassistant (to user_proxy):\r\n\r\nTo define a junction using the `scenariogeneration` Python library,\r\n\r\n--------------------------------------------------------------------------------\n\n### Steps to reproduce\n\nI am using the example agentchat_auto_feedback_from_code_execution with my own azure GPT4-V endpoint. However, none of the example notebooks seem to reproduce any valid answer as suggested.\n\n### Screenshots and logs\n\n_No response_\n\n### Additional Information\n\n_No response_",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/1806/comments",
    "author": "xuanyuxin",
    "comments": [
      {
        "user": "gagb",
        "created_at": "2024-02-28T21:31:56Z",
        "body": "user_proxy returns empty because there are no code blocks to execute.\r\nPer your current config of is_termination_msg it will only terminate if the assistant message ends with the string \"TERMINATE\" or if you reach max consecutive auto reply.\r\n\r\n@afourney you have used the azure gpt4-v endpoints. Do you see this issue?"
      },
      {
        "user": "xuanyuxin",
        "created_at": "2024-03-13T03:17:33Z",
        "body": "Solved by setting max_tokens to a larger non-default value. Thx"
      }
    ]
  },
  {
    "number": 1705,
    "title": "[Bug]:  Notebook, chess example.",
    "created_at": "2024-02-16T23:22:12Z",
    "closed_at": "2024-02-16T23:49:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/microsoft/autogen/issues/1705",
    "body": "### Describe the bug\n\n```python\r\n    def _generate_reply_for_player(\r\n        self,\r\n        messages: Optional[List[Dict]] = None,\r\n        sender: Optional[autogen.Agent] = None,\r\n        config: Optional[BoardAgent] = None,\r\n    ) -> Union[str, Dict, None]:\r\n        board_agent = config\r\n        # add a system message about the current state of the board.\r\n        board_state_msg = [{\"role\": \"system\", \"content\": f\"Current board:\\n{board_agent.board}\"}]\r\n        # propose a reply which will be sent to the board agent for verification.\r\n        message = self.generate_reply(\r\n            messages + board_state_msg, sender, exclude=[ChessPlayerAgent._generate_reply_for_player]\r\n        )\r\n        if message is None:\r\n            return True, None\r\n        # converse with the board until a legal move is made or max allowed retries.\r\n        # change silent to False to see that conversation.\r\n        self.initiate_chat(board_agent, clear_history=False, message=message, silent=self.human_input_mode == \"NEVER\")\r\n        # last message sent by the board agent\r\n        last_message = self._oai_messages[board_agent][-1]\r\n        if last_message[\"role\"] == \"assistant\":\r\n            # didn't make a legal move after a limit times of retries.\r\n            print(f\"{self.name}: I yield.\")\r\n            return True, None\r\n        return True, self._oai_messages[board_agent][-2]\r\n```\r\n\r\nThe code above is a function from the ChessPlayerAgent.\r\nAlthough it should chat with another agent, it is initiation the chat with board_agent.\r\n\r\nself.initiate_chat(board_agent, clear_history=False, message=message, silent=self.human_input_mode == \"NEVER\")\r\n\r\nshould be\r\n\r\nself.initiate_chat(sender, clear_history=False, message=message, silent=self.human_input_mode == \"NEVER\")\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### Screenshots and logs\n\n_No response_\n\n### Additional Information\n\n_No response_",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/1705/comments",
    "author": "crimson206",
    "comments": [
      {
        "user": "sonichi",
        "created_at": "2024-02-16T23:43:42Z",
        "body": "It's by design. Before the player replies to another player, it needs to talk to the board agent to verify the move.\r\ncc @qingyun-wu @ekzhu "
      },
      {
        "user": "crimson206",
        "created_at": "2024-02-16T23:49:00Z",
        "body": "@sonichi \r\n\r\nYou are write. I just rechecked."
      }
    ]
  },
  {
    "number": 1598,
    "title": "[Issue]: Autogen code not communicating to machine through VPN",
    "created_at": "2024-02-08T15:39:20Z",
    "closed_at": "2024-02-12T04:44:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/microsoft/autogen/issues/1598",
    "body": "### Describe the issue\r\n\r\nMy autogen is running on a Mac - 10.10.10.1\r\nVPN to Another Machine IP is 10.10.10.5\r\n\r\nI m trying to reach the machine 10.10.10.5 on a different network connected via VPN for Autogen to communicate with. It doesnt connect to it, but when I have my mac ping that VPN machine it pings just fine. \r\n\r\nIs there something I can change to ensure that agents can communicate with another machine?\r\n\r\n### Steps to reproduce\r\n\r\n_No response_\r\n\r\n### Screenshots and logs\r\n\r\n_No response_\r\n\r\n### Additional Information\r\n\r\nLatest Autogen Version\r\nMacOS\r\nPython3\r\n",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/1598/comments",
    "author": "Shubham-Khichi",
    "comments": [
      {
        "user": "sonichi",
        "created_at": "2024-02-10T06:19:49Z",
        "body": "Do you mean making an inference call to a remote endpoint? Does it work without using autogen?"
      },
      {
        "user": "Shubham-Khichi",
        "created_at": "2024-02-10T14:20:45Z",
        "body": "> Do you mean making an inference call to a remote endpoint? Does it work without using autogen?\n\nYes it does work without autogen but autogen agents don't communicate with those remote endpoints "
      },
      {
        "user": "sonichi",
        "created_at": "2024-02-10T14:46:02Z",
        "body": "Is the same setting passed in `llm_config` of the agents that use the endpoints?"
      },
      {
        "user": "Shubham-Khichi",
        "created_at": "2024-02-12T03:08:35Z",
        "body": "> Is the same setting passed in `llm_config` of the agents that use the endpoints?\n\nFixed it. Created a docker image from the base image of autogen and now the agent communicates over VPN "
      }
    ]
  },
  {
    "number": 1447,
    "title": "ModuleNotFoundError: No module named 'openai.openai_object'  ；Tried too many times and couldn't solve",
    "created_at": "2024-01-29T13:34:47Z",
    "closed_at": "2024-08-27T06:26:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/microsoft/autogen/issues/1447",
    "body": "### Is your feature request related to a problem? Please describe.\n\nuser_proxy (to assistant):\r\n\r\nPlot a chart of NVDA and TESLA stock price change YTD.\r\n\r\n--------------------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/wingzheng/Desktop/github/autogen/test/twoagent.py\", line 46, in <module>\r\n    user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\r\n  File \"/Users/wingzheng/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py\", line 698, in initiate_chat\r\n    self.send(self.generate_init_message(**context), recipient, silent=silent)\r\n  File \"/Users/wingzheng/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py\", line 441, in send\r\n    recipient.receive(message, self, request_reply, silent)\r\n  File \"/Users/wingzheng/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py\", line 599, in receive\r\n    reply = self.generate_reply(messages=self.chat_messages[sender], sender=sender)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/wingzheng/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py\", line 1284, in generate_reply\r\n    final, reply = reply_func(self, messages=messages, sender=sender, config=reply_func_tuple[\"config\"])\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/wingzheng/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py\", line 799, in generate_oai_reply\r\n    response = client.create(\r\n               ^^^^^^^^^^^^^^\r\n  File \"/Users/wingzheng/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/oai/client.py\", line 264, in create\r\n    response: ChatCompletion = cache.get(key, None)\r\n                               ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/wingzheng/anaconda3/envs/autogen/lib/python3.11/site-packages/autogen/cache/disk_cache.py\", line 47, in get\r\n    return self.cache.get(key, default)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/wingzheng/anaconda3/envs/autogen/lib/python3.11/site-packages/diskcache/core.py\", line 1173, in get\r\n    value = self._disk.fetch(mode, filename, db_value, read)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/wingzheng/anaconda3/envs/autogen/lib/python3.11/site-packages/diskcache/core.py\", line 284, in fetch\r\n    return pickle.load(io.BytesIO(value))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nModuleNotFoundError: No module named 'openai.openai_object'\n\n### Describe the solution you'd like\n\ndelete the.cache folder is invalid. Can you tell me what version of openai matches python3.10 and python3.11? After many unsuccessful attempts? I don't know what version of everyone can work properly?\n\n### Additional context\n\npython3.11/3.10  openai==1.3.7 /1.10.0 I've tried them all. They don't work",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/1447/comments",
    "author": "20130216",
    "comments": [
      {
        "user": "gagb",
        "created_at": "2024-01-29T18:39:57Z",
        "body": "How did you install? Which machine are you using?"
      },
      {
        "user": "20130216",
        "created_at": "2024-01-30T01:02:46Z",
        "body": "mac ；python3.11/3.10  openai==1.3.7 /1.10.0 I've tried them all. They don't work    always：“ModuleNotFoundError: No module named 'openai.openai_object' ”"
      },
      {
        "user": "20130216",
        "created_at": "2024-01-30T01:03:16Z",
        "body": "mac +VS Code"
      }
    ]
  },
  {
    "number": 1308,
    "title": "[Issue]: speaker selection when specified ",
    "created_at": "2024-01-17T17:44:20Z",
    "closed_at": "2024-08-27T06:08:41Z",
    "labels": [
      "question",
      "group chat/teams"
    ],
    "url": "https://github.com/microsoft/autogen/issues/1308",
    "body": "### Describe the issue\n\nI have a list of agents for example sde1, sd2, TeamManager and I have set max round as 3,  when I ask a query \"hey Sd2, Can you help me with somethin\", the first speaker selection is right but in the second round another speaker is getting chosen and is replying.. my expected output would be as soon as the sd2 speaks the chat should get terminated..\r\n\r\nI am not sure where I am going wrong and need your inputs on this.\n\n### Steps to reproduce\n\nI have a list of agents for example sde1, sd2, TeamManager and I have set max round as 3,  when I ask a query \"hey Sd2, Can you help me with somethin\", the first speaker selection is right but in the second round another speaker is getting chosen and is replying.. my expected output would be as soon as the sd2 speaks the chat should get terminated..\n\n### Screenshots and logs\n\n_No response_\n\n### Additional Information\n\n_No response_",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/1308/comments",
    "author": "Srivishnu27feb",
    "comments": [
      {
        "user": "gagb",
        "created_at": "2024-01-17T17:45:32Z",
        "body": "Can you please share your code?"
      }
    ]
  },
  {
    "number": 1197,
    "title": "[Bug]: In Groupchat, existing agents based on OpenAI assistant API throw error",
    "created_at": "2024-01-10T16:12:13Z",
    "closed_at": "2024-05-10T22:01:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/microsoft/autogen/issues/1197",
    "body": "### Describe the bug\r\n\r\nWhen assistant already exists and is instantiated using code such as:\r\n``` python\r\n# Configure Agents\r\n\r\n#Technical Analysis Critic\r\nassistant_id = os.environ.get(\"ASSISTANT_ID\", \"asst_rECmBeeNUz43ZQ3H2JUycUdk\")\r\nllm_config = {\r\n    \"config_list\": config_list,\r\n    \"assistant_id\": assistant_id,\r\n#    \"tools\": [{\"type\": \"retrieval\"},{\"type\": \"code_interpreter\"}],\r\n#    \"file_ids\": [\"file-LHgfBxjo3D8IQPTseZOrQKwA\"]\r\n    # add id of an existing file in your openai account\r\n    # in this case I added the implementation of conversable_agent.py\r\n}\r\n\r\ngpt_technical_analysis_critic = GPTAssistantAgent(\r\n    name=\"Technical Analysis Critic\", description=\"Technical Analysis Critic is a specialist at stock trading methodologies and best practice.\", llm_config=llm_config\r\n)\r\n```\r\nThen in GroupChat the following error is shown:\r\n```\r\nTypeError: Completions.create() got an unexpected keyword argument 'assistant_id'\r\n```\r\nThis only happens when I want to re-use agents that already exists and is called via the following example code:\r\n\r\n``` python\r\n# Group Solve\r\nimport autogen\r\n\r\n# define group chat\r\ngroupchat = autogen.GroupChat(agents=[user_proxy, gpt_system_project_planner, gpt_technical_analysis_critic, gpt_solution_architect], messages=[], max_round=10)\r\nmanager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\r\n\r\nuser_proxy.initiate_chat(\r\n    manager,\r\n    message=\"Get the number of issues and pull requests for the repository 'microsoft/autogen' over the past three weeks and offer analysis to the data. You should print the data in csv format grouped by weeks.\",\r\n)\r\n# type exit to terminate the chat\r\n```\r\nIt seems to be specific to when the existing agent is referenced.\r\n\r\n### Steps to reproduce\r\n\r\n``` python\r\nimport logging\r\nimport os\r\n\r\n\r\nimport autogen\r\nfrom autogen.agentchat import AssistantAgent\r\nfrom autogen import UserProxyAgent, config_list_from_json\r\nfrom autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent\r\n\r\nlogger = logging.getLogger(__name__)\r\nlogger.setLevel(logging.WARNING)\r\n\r\nconfig_list = config_list_from_json(\"OAI_CONFIG_LIST\")\r\n\r\n# Configure Agents\r\n\r\n#Technical Analysis Critic\r\nassistant_id = os.environ.get(\"ASSISTANT_ID\", \"asst_rECmBeeNUz43ZQ3H2JUycUdk\")\r\nllm_config = {\r\n    \"config_list\": config_list,\r\n    \"assistant_id\": assistant_id,\r\n#    \"tools\": [{\"type\": \"retrieval\"},{\"type\": \"code_interpreter\"}],\r\n#    \"file_ids\": [\"file-LHgfBxjo3D8IQPTseZOrQKwA\"]\r\n    # add id of an existing file in your openai account\r\n    # in this case I added the implementation of conversable_agent.py\r\n}\r\n\r\ngpt_technical_analysis_critic = GPTAssistantAgent(\r\n    name=\"Technical Analysis Critic\", description=\"Technical Analysis Critic is a specialist at stock trading methodologies and best practice.\", llm_config=llm_config\r\n)\r\n# Configure Agents\r\n\r\n#System Project Planner\r\nassistant_id = os.environ.get(\"ASSISTANT_ID\", \"asst_rE85ECt63wNZOGK7i11cTcLW\")\r\nllm_config = {\r\n    \"config_list\": config_list,\r\n    \"assistant_id\": assistant_id,\r\n#    \"tools\": [{\"type\": \"retrieval\"},{\"type\": \"code_interpreter\"}],\r\n#    \"file_ids\": [\"file-LHgfBxjo3D8IQPTseZOrQKwA\"]\r\n    # add id of an existing file in your openai account\r\n    # in this case I added the implementation of conversable_agent.py\r\n}\r\n\r\ndescription=\"System Project Planner creates a project delivery plan and is the first resource to engage in thinking through the delivery process step by step.  System Project Planner provides step-by-step from project Definition, through to Implementation.\"\r\n\r\ngpt_system_project_planner = GPTAssistantAgent(\r\n    name=\"System Project Planner\", description=description, llm_config=llm_config\r\n)\r\n# Configure Agents\r\n\r\n#Solution Architect\r\nassistant_id = os.environ.get(\"ASSISTANT_ID\", \"asst_4FSUH7tvkNcHIFzV4J4pGbgV\")\r\nllm_config = {\r\n    \"config_list\": config_list,\r\n    \"assistant_id\": assistant_id,\r\n#    \"tools\": [{\"type\": \"retrieval\"},{\"type\": \"code_interpreter\"}],\r\n#    \"file_ids\": [\"file-LHgfBxjo3D8IQPTseZOrQKwA\"]\r\n    # add id of an existing file in your openai account\r\n    # in this case I added the implementation of conversable_agent.py\r\n}\r\n\r\ngpt_solution_architect = GPTAssistantAgent(\r\n    name=\"Solution Architect\", description=\"Solution Architect provides solution designs using plantuml syntax.  The Solution Architect do not provide code.  The Solution Architect do review code to ensure it is aligned with sound architecture principles.\", llm_config=llm_config\r\n)\r\n\r\nuser_proxy = UserProxyAgent(\r\n    name=\"user_proxy\",\r\n    is_termination_msg=lambda msg: \"TERMINATE\" in msg[\"content\"],\r\n    code_execution_config={\r\n        \"work_dir\": \"coding\",\r\n        \"use_docker\": False,  # set to True or image name like \"python:3\" to use docker\r\n    },\r\n    human_input_mode=\"NEVER\",\r\n)\r\n# define group chat\r\ngroupchat = autogen.GroupChat(agents=[user_proxy, gpt_system_project_planner, gpt_technical_analysis_critic, gpt_solution_architect], messages=[], max_round=10)\r\nmanager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\r\n\r\nuser_proxy.initiate_chat(\r\n    manager,\r\n    message=\"Get the number of issues and pull requests for the repository 'microsoft/autogen' over the past three weeks and offer analysis to the data. You should print the data in csv format grouped by weeks.\",\r\n)\r\n# type exit to terminate the chat\r\n``` \r\n\r\n### Expected Behavior\r\n\r\nChat should have started as is currently working with group chat or agents which is created as part of the process.\r\n\r\n### Screenshots and logs\r\n\r\n_No response_\r\n\r\n### Additional Information\r\n\r\nWin 10 Pro\r\nCurrent version\r\nPython 9.10\r\n",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/1197/comments",
    "author": "DementedWeasel1971",
    "comments": [
      {
        "user": "rickyloynd-microsoft",
        "created_at": "2024-01-11T00:02:29Z",
        "body": "@afourney "
      },
      {
        "user": "ekzhu",
        "created_at": "2024-01-11T03:33:26Z",
        "body": "Looks like an issue related to support for assistant API. @gagb @sidhujag "
      },
      {
        "user": "sonichi",
        "created_at": "2024-01-11T16:43:40Z",
        "body": "@IANTHEREAL fyi"
      },
      {
        "user": "IANTHEREAL",
        "created_at": "2024-01-12T12:26:11Z",
        "body": "The problem is caused by the llm_config in `manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)`, you can try to remove assistant_id from chat manager's configuration"
      },
      {
        "user": "DementedWeasel1971",
        "created_at": "2024-01-12T18:16:46Z",
        "body": "If I follow your advice and remove the assistant_id, how do I activate an already existing agent in my openai account.  The assistant_id is used as reference to an already configured agent that I have in openai.  It works if I have a single chat between agents.  I have full use of code interpreter as well as any knowledge or functions that I have on the openai platform.  So I can configure and tweak an agent once and feed it with specific knowledge once and then call it.\r\n\r\nThis works like a charm, except for the group chat.  If it worked as well in the group chat as it is working on the single chat, it opens up huge potential, as not only do I have custom agents, I have remote compute (code interpreters for each agent), without needing to config this at \"run time\".  The issue is specific to group chat, works fantastic on everything else, but I do not want to use networkx to orchastrate engagement between agents, group chat on autogen works brilliant on agents created locally.  You might think of a code change in the group chat or llm_config to add the agent names, as I suspect that is what you are using.\r\n\r\nIf this one works or I find a workarround, for me this would be a huge thing.  Group Chats between existing super agents.\r\n\r\nIf you add to this a later feature which is to manage the thread allocation, autogen would be on its way to enable building a \"wisdom enabler\".  (But I do not want to digress), I would want to have existing agents enabled in a group chat while they have access to all their knowledge, functions and preset instructions.  The way it works in a 1:1 chat with an existing agent.  But in the group chat.\r\n\r\nIf I remove the assistant_id I remove the ability to reference an existing agent, or is there another way to reference, if so, I will gladly test."
      },
      {
        "user": "IANTHEREAL",
        "created_at": "2024-01-13T01:00:46Z",
        "body": "I'm a bit confused, have you encountered a new error? The previous issue was in the chat manager's function, not the GPT assistant agent. You just need to remove the assistant id from the chat manager's llm config. The rest of the GPT assistant agent's configuration, including the assistant id, can remain unchanged. chat manager doesn't need assistant id, right?  @DementedWeasel1971 "
      },
      {
        "user": "DementedWeasel1971",
        "created_at": "2024-01-13T05:24:53Z",
        "body": "Confusion might be on my side. @IANTHEREAL when you said remove assistant id from the chat manager's llm config, I took it to imply that it should not be there to start off with.  I will test and revert."
      },
      {
        "user": "sidhujag",
        "created_at": "2024-01-15T09:42:54Z",
        "body": "I hit this and found if i pop the assistant_id in the gpt assistant it fixes it."
      },
      {
        "user": "DementedWeasel1971",
        "created_at": "2024-01-15T14:16:13Z",
        "body": "> I hit this and found if i pop the assistant_id in the gpt assistant it fixes it.\r\n\r\nPlease share code.  Would love to find the work around.  If I can get the existing agents to work in a group chat, Wow, then each's specialisation can be used to in theory  create a better quality output or even output in the context of a company's existing source code, which I could have uploaded as knowledge."
      },
      {
        "user": "gagb",
        "created_at": "2024-01-29T21:44:54Z",
        "body": "@DementedWeasel1971  you need to use different llm_config for group chat manager and gpt assistant. The former does not need assistant id and will complain. And the latter can use one. Hope this helps!"
      }
    ]
  },
  {
    "number": 1012,
    "title": "[Bug]: when i give agent function,there is the errorline 898, in _request     raise self._make_status_error_from_response(err.response) from None openai.NotFoundError: Error code: 404 - {'error': {'message': 'Unrecognized request argument supplied: functions (request id: 20231218185632700815098gdho4iSo)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
    "created_at": "2023-12-18T10:58:45Z",
    "closed_at": "2024-10-18T17:36:58Z",
    "labels": [
      "question",
      "0.2"
    ],
    "url": "https://github.com/microsoft/autogen/issues/1012",
    "body": "### Describe the bug\n\n_No response_\n\n### Steps to reproduce\n\n`\r\nfunction_llm_config = {\r\n    \"cache_seed\": 42,  # change the cache_seed for different trials\r\n    \"temperature\": 0,\r\n    \"config_list\": config_list,\r\n    \"timeout\": 120,\r\n    \"functions\": [\r\n        {\r\n            \"name\": \"get_account_id\",\r\n            \"description\": \"retrieves the account id for a user given their name\",\r\n            \"parameters\": {\r\n                \"type\": \"object\",\r\n                \"properties\": {\r\n                    \"name\": {\r\n                        \"type\": \"string\",\r\n                        \"description\": \"The name of the customer that will be used to lookup the account id\"\r\n                    }\r\n                },\r\n                \"required\": [\"name\"]\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"get_last_bill_amount\",\r\n            \"description\": \"Retrieves the last bill amount for a user for a given account id.\",\r\n            \"parameters\": {\r\n                \"type\": \"object\",\r\n                \"properties\": {\r\n                    \"account_id\": {\r\n                        \"type\": \"string\",\r\n                        \"description\": \"The account id fetched from get_account_id that will be used to lookup the last bill for the customer\"\r\n                    }\r\n                },\r\n                \"required\": [\"account_id\"]\r\n            }\r\n        }\r\n    ]\r\n}\r\n\r\nnofunction_llm_config = {\r\n    \"cache_seed\": 42,  # change the cache_seed for different trials\r\n    \"temperature\": 0,\r\n    \"config_list\": config_list,\r\n    \"timeout\": 120,\r\n\r\n}\r\n\r\nname_to_account_id = {\r\n    \"Alice\": \"A123\",\r\n    \"Bob\": \"B456\",\r\n    \"Charlie\": \"C789\"\r\n}\r\n\r\naccount_id_to_bill = {\r\n    \"A123\": 120.50,\r\n    \"B456\": 200.75,\r\n    \"C789\": 99.99\r\n}\r\n\r\ndef get_account_id(name):\r\n    return name_to_account_id.get(name, \"Name not found\")\r\n\r\ndef get_last_bill_amount(account_id):\r\n    return account_id_to_bill.get(account_id, \"Account ID not found\")\r\n\r\n\r\nimport autogen\r\nfrom autogen import config_list_from_json\r\nfrom autogen.code_utils import execute_code\r\nimport json\r\n\r\nconfig_list = autogen.config_list_from_json(\r\n    env_or_file='config.json',\r\n    filter_dict={\r\n        \"model\": [\"gpt-4\"]\r\n    },\r\n)\r\n\r\nfunction_llm_config = {\r\n    \"cache_seed\": 42,  # change the cache_seed for different trials\r\n    \"temperature\": 0,\r\n    \"config_list\": config_list,\r\n    \"timeout\": 120,\r\n    \"functions\": [\r\n        {\r\n            \"name\": \"get_account_id\",\r\n            \"description\": \"retrieves the account id for a user given their name\",\r\n            \"parameters\": {\r\n                \"type\": \"object\",\r\n                \"properties\": {\r\n                    \"name\": {\r\n                        \"type\": \"string\",\r\n                        \"description\": \"The name of the customer that will be used to lookup the account id\"\r\n                    }\r\n                },\r\n                \"required\": [\"name\"]\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"get_last_bill_amount\",\r\n            \"description\": \"Retrieves the last bill amount for a user for a given account id.\",\r\n            \"parameters\": {\r\n                \"type\": \"object\",\r\n                \"properties\": {\r\n                    \"account_id\": {\r\n                        \"type\": \"string\",\r\n                        \"description\": \"The account id fetched from get_account_id that will be used to lookup the last bill for the customer\"\r\n                    }\r\n                },\r\n                \"required\": [\"account_id\"]\r\n            }\r\n        }\r\n    ]\r\n}\r\n\r\nnofunction_llm_config = {\r\n    \"cache_seed\": 42,  # change the cache_seed for different trials\r\n    \"temperature\": 0,\r\n    \"config_list\": config_list,\r\n    \"timeout\": 120,\r\n\r\n}\r\n\r\nname_to_account_id = {\r\n    \"Alice\": \"A123\",\r\n    \"Bob\": \"B456\",\r\n    \"Charlie\": \"C789\"\r\n}\r\n\r\naccount_id_to_bill = {\r\n    \"A123\": 120.50,\r\n    \"B456\": 200.75,\r\n    \"C789\": 99.99\r\n}\r\n\r\n\r\ndef get_account_id(name):\r\n    print(\"--------------------\")\r\n    print(name_to_account_id.get(name, \"Name not found\")),\r\n    return name_to_account_id.get(name, \"Name not found\")\r\n\r\n\r\ndef get_last_bill_amount(account_id):\r\n    return account_id_to_bill.get(account_id, \"Account ID not found\")\r\n\r\n# 为我们的代理创建提示\r\nbilling_assistant_agent_prompt = '''\r\n该代理是一个有用的助手，可以检索客户的账户 ID 和上次账单金额。\r\n任何其他客户服务请求都不属于该代理的工作范围。\r\n完成对用户的帮助后，输出 TERMINATE'''\r\n# 创建代理，并为其配置已定义的函数定义\r\nbilling_assistant_agent = autogen.AssistantAgent(\r\n    name=\"billing_assistant_agent\",\r\n    system_message=billing_assistant_agent_prompt,\r\n    llm_config=function_llm_config,\r\n)\r\n\r\n# 首先定义我们的用户代理，记住将 human_input_mode 设置为 NEVER，这样它才能\r\n# 正确执行功能\r\nuser_proxy = autogen.UserProxyAgent(\r\n    name=\"user_proxy\",\r\n    human_input_mode=\"NEVER\",\r\n    max_consecutive_auto_reply=10,\r\n    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\r\n)\r\n# 接下来我们注册我们的函数，这非常简单\r\n# 我们只需提供一个字典，将我们希望 llm 调用的名称映射为实际函数的引用\r\n# 好的做法是让 llm 试图调用的名称与实际的预定义函数相同\r\n# 但我们也可以随心所欲地调用它们、\r\nuser_proxy.register_function(\r\n    function_map={\r\n        \"get_account_id\": get_account_id,\r\n        \"get_last_bill_amount\": get_last_bill_amount,\r\n    }\r\n)\r\n\r\nuser_proxy.initiate_chat(billing_assistant_agent, message=\"My name is Bob, can you tell me what my last bill was?\")`\n\n### Expected Behavior\n\n_No response_\n\n### Screenshots and logs\n\n_No response_\n\n### Additional Information\n\n_No response_",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/1012/comments",
    "author": "safa1018",
    "comments": [
      {
        "user": "thinkall",
        "created_at": "2024-06-18T09:26:24Z",
        "body": "Hi @safa1018 ,  is your issue resolved with latest version of AutoGen? If not, could you update the issue since it's difficult to read in the current format. Thanks."
      },
      {
        "user": "rysweet",
        "created_at": "2024-10-18T17:36:58Z",
        "body": "appears resolved. "
      }
    ]
  },
  {
    "number": 618,
    "title": "Will there be a distributed agent support in the future",
    "created_at": "2023-11-10T08:34:34Z",
    "closed_at": "2023-11-30T18:54:44Z",
    "labels": [
      "question",
      "roadmap"
    ],
    "url": "https://github.com/microsoft/autogen/issues/618",
    "body": "Do you plan to add something like this in the future?\r\n\r\nCheers",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/618/comments",
    "author": "antoan",
    "comments": [
      {
        "user": "afourney",
        "created_at": "2023-11-10T16:59:48Z",
        "body": "Interesting question. It's not on the roadmap yet, but might be a good candidate."
      },
      {
        "user": "Donovoi",
        "created_at": "2023-11-23T23:04:40Z",
        "body": "Just to clarify is that distributed as in each agent holds their own context that is then indexed and access by an overarching agent? Similar to database functionality?"
      }
    ]
  },
  {
    "number": 502,
    "title": "How do I make agents know about configurations and secrets in the environment?",
    "created_at": "2023-10-31T12:23:11Z",
    "closed_at": "2024-06-18T04:12:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/microsoft/autogen/issues/502",
    "body": "```[tasklist]\n### Tasks\n```\n",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/502/comments",
    "author": "abhishek-ravikumar-kulkarni",
    "comments": [
      {
        "user": "afourney",
        "created_at": "2023-10-31T17:28:00Z",
        "body": "There are a few things you can do. You can wrap them in functions, and use function calling. Or you can try telling the agents where there secrets are. For example, here is one query I recently issued:\r\n\r\n```\r\nFind Microsoft's logo from 1983, and save it to disk. If searching the web,\r\nuse Bing with the API key stored in `os.environ['BING_API_KEY']`\r\n```\r\n"
      },
      {
        "user": "abhishek-ravikumar-kulkarni",
        "created_at": "2023-11-01T10:37:15Z",
        "body": "@afourney and we dont have any examples in notebook for a use case where we can limit the agent's search in a custom knowledge base."
      },
      {
        "user": "thinkall",
        "created_at": "2024-06-18T04:12:15Z",
        "body": "We are closing this issue due to inactivity; please reopen if the problem persists."
      }
    ]
  },
  {
    "number": 267,
    "title": "Trouble reading API keys from OAI Config List ",
    "created_at": "2023-10-17T05:05:47Z",
    "closed_at": "2024-06-18T03:52:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/microsoft/autogen/issues/267",
    "body": "I'm trying out the code in the notebooks that work great whenever I hard code my API key into the notebook as:\r\n\r\n`[\r\n    {\r\n        \"model\": \"gpt-4\",\r\n        \"api_key\": \"sk-***\"\r\n    }\r\n]`\r\n\r\nBut not when I use the same format in the OAI_COnfig_LIST file and call it using:\r\n\r\n`import autogen\r\n\r\nconfig_list_gpt4 = autogen.config_list_from_json(\r\n    \"OAI_CONFIG_LIST\",\r\n    filter_dict={\r\n        \"model\": [\"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\r\n    },\r\n)`\r\n\r\nIs there something wrong with the format of the OAI_CONFIG_LIST file?",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/267/comments",
    "author": "thesauce25",
    "comments": [
      {
        "user": "sonichi",
        "created_at": "2023-10-17T05:20:48Z",
        "body": "the model list doesn't contain \"gpt-4\" :)\r\nif you just have one endpoint in the config file you can remove the `filter_dict`"
      },
      {
        "user": "thinkall",
        "created_at": "2024-06-18T03:52:12Z",
        "body": "We are closing this issue due to inactivity; please reopen if the problem persists."
      }
    ]
  },
  {
    "number": 209,
    "title": "API failing ",
    "created_at": "2023-10-12T02:17:46Z",
    "closed_at": "2024-08-27T03:47:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/microsoft/autogen/issues/209",
    "body": "Hello \r\n\r\nI use colab and all of your examples work fine - I  enter my gpt 4 API keys all works fine - and the notebook completes successfully \r\nI download the same notebook, then upload it into MS Fabric Synapse Spark (i believe they use version of spark 3.2) - it is unable to connect to GPT API keys either 3.5 turbo or 4.0 or any other gpt \r\n\r\nI get this error \r\n\r\nuser_proxy (to assistant):\r\n\r\n\r\nFind $a + b + c$, given that $x+y \\neq -1$ and\r\n\\begin{align}\r\n\tax + by + c & = x + 7,\\\r\n\ta + bx + cy & = 2x + 6y,\\\r\n\tay + b + cx & = 4x + y.\r\n\\end{align}.\r\n\r\n\r\n--------------------------------------------------------------------------------\r\n---------------------------------------------------------------------------\r\nInvalidRequestError                       Traceback (most recent call last)\r\nCell In[16], line 11\r\n      1 math_problem_to_solve = \"\"\"\r\n      2 Find $a + b + c$, given that $x+y \\\\neq -1$ and\r\n      3 \\\\begin{align}\r\n   (...)\r\n      7 \\\\end{align}.\r\n      8 \"\"\"\r\n     10 # the assistant receives a message from the user, which contains the task description\r\n---> 11 user_proxy.initiate_chat(assistant, message=math_problem_to_solve)\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:531, in ConversableAgent.initiate_chat(self, recipient, clear_history, silent, **context)\r\n    517 \"\"\"Initiate a chat with the recipient agent.\r\n    518 \r\n    519 Reset the consecutive auto reply counter.\r\n   (...)\r\n    528         \"message\" needs to be provided if the `generate_init_message` method is not overridden.\r\n    529 \"\"\"\r\n    530 self._prepare_chat(recipient, clear_history)\r\n--> 531 self.send(self.generate_init_message(**context), recipient, silent=silent)\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334, in ConversableAgent.send(self, message, recipient, request_reply, silent)\r\n    332 valid = self._append_oai_message(message, \"assistant\", recipient)\r\n    333 if valid:\r\n--> 334     recipient.receive(message, self, request_reply, silent)\r\n    335 else:\r\n    336     raise ValueError(\r\n    337         \"Message can't be converted into a valid ChatCompletion message. Either content or function_call must be provided.\"\r\n    338     )\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:462, in ConversableAgent.receive(self, message, sender, request_reply, silent)\r\n    460 if request_reply is False or request_reply is None and self.reply_at_receive[sender] is False:\r\n    461     return\r\n--> 462 reply = self.generate_reply(messages=self.chat_messages[sender], sender=sender)\r\n    463 if reply is not None:\r\n    464     self.send(reply, sender, silent=silent)\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:779, in ConversableAgent.generate_reply(self, messages, sender, exclude)\r\n    777     continue\r\n    778 if self._match_trigger(reply_func_tuple[\"trigger\"], sender):\r\n--> 779     final, reply = reply_func(self, messages=messages, sender=sender, config=reply_func_tuple[\"config\"])\r\n    780     if final:\r\n    781         return reply\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:606, in ConversableAgent.generate_oai_reply(self, messages, sender, config)\r\n    603     messages = self._oai_messages[sender]\r\n    605 # TODO: #1143 handle token limit exceeded error\r\n--> 606 response = oai.ChatCompletion.create(\r\n    607     context=messages[-1].pop(\"context\", None), messages=self._oai_system_message + messages, **llm_config\r\n    608 )\r\n    609 return True, oai.ChatCompletion.extract_text_or_function_call(response)[0]\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/autogen/oai/completion.py:789, in Completion.create(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\r\n    787     base_config[\"max_retry_period\"] = 0\r\n    788 try:\r\n--> 789     response = cls.create(\r\n    790         context,\r\n    791         use_cache,\r\n    792         raise_on_ratelimit_or_timeout=i < last or raise_on_ratelimit_or_timeout,\r\n    793         **base_config,\r\n    794     )\r\n    795     if response == -1:\r\n    796         return response\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/autogen/oai/completion.py:820, in Completion.create(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\r\n    818 with diskcache.Cache(cls.cache_path) as cls._cache:\r\n    819     cls.set_cache(seed)\r\n--> 820     return cls._get_response(params, raise_on_ratelimit_or_timeout=raise_on_ratelimit_or_timeout)\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/autogen/oai/completion.py:210, in Completion._get_response(cls, config, raise_on_ratelimit_or_timeout, use_cache)\r\n    208         response = openai_completion.create(**config)\r\n    209     else:\r\n--> 210         response = openai_completion.create(request_timeout=request_timeout, **config)\r\n    211 except (\r\n    212     ServiceUnavailableError,\r\n    213     APIConnectionError,\r\n    214 ):\r\n    215     # transient error\r\n    216     logger.info(f\"retrying in {retry_wait_time} seconds...\", exc_info=1)\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25, in ChatCompletion.create(cls, *args, **kwargs)\r\n     23 while True:\r\n     24     try:\r\n---> 25         return super().create(*args, **kwargs)\r\n     26     except TryAgain as e:\r\n     27         if timeout is not None and time.time() > start + timeout:\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:155, in EngineAPIResource.create(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\r\n    129 @classmethod\r\n    130 def create(\r\n    131     cls,\r\n   (...)\r\n    138     **params,\r\n    139 ):\r\n    140     (\r\n    141         deployment_id,\r\n    142         engine,\r\n   (...)\r\n    152         api_key, api_base, api_type, api_version, organization, **params\r\n    153     )\r\n--> 155     response, _, api_key = requestor.request(\r\n    156         \"post\",\r\n    157         url,\r\n    158         params=params,\r\n    159         headers=headers,\r\n    160         stream=stream,\r\n    161         request_id=request_id,\r\n    162         request_timeout=request_timeout,\r\n    163     )\r\n    165     if stream:\r\n    166         # must be an iterator\r\n    167         assert not isinstance(response, OpenAIResponse)\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/openai/api_requestor.py:299, in APIRequestor.request(self, method, url, params, headers, files, stream, request_id, request_timeout)\r\n    278 def request(\r\n    279     self,\r\n    280     method,\r\n   (...)\r\n    287     request_timeout: Optional[Union[float, Tuple[float, float]]] = None,\r\n    288 ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:\r\n    289     result = self.request_raw(\r\n    290         method.lower(),\r\n    291         url,\r\n   (...)\r\n    297         request_timeout=request_timeout,\r\n    298     )\r\n--> 299     resp, got_stream = self._interpret_response(result, stream)\r\n    300     return resp, got_stream, self.api_key\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/openai/api_requestor.py:710, in APIRequestor._interpret_response(self, result, stream)\r\n    702     return (\r\n    703         self._interpret_response_line(\r\n    704             line, result.status_code, result.headers, stream=True\r\n    705         )\r\n    706         for line in parse_stream(result.iter_lines())\r\n    707     ), True\r\n    708 else:\r\n    709     return (\r\n--> 710         self._interpret_response_line(\r\n    711             result.content.decode(\"utf-8\"),\r\n    712             result.status_code,\r\n    713             result.headers,\r\n    714             stream=False,\r\n    715         ),\r\n    716         False,\r\n    717     )\r\n\r\nFile /nfs4/pyenv-f4056475-393f-4abe-8234-de9d7e4fbf43/lib/python3.10/site-packages/openai/api_requestor.py:775, in APIRequestor._interpret_response_line(self, rbody, rcode, rheaders, stream)\r\n    773 stream_error = stream and \"error\" in resp.data\r\n    774 if stream_error or not 200 <= rcode < 300:\r\n--> 775     raise self.handle_error_response(\r\n    776         rbody, rcode, resp.data, rheaders, stream_error=stream_error\r\n    777     )\r\n    778 return resp\r\n\r\nInvalidRequestError: The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.\r\n\r\n\r\n\r\nis it not supoosed to work in MS fabric? or do we need to change the authentication\r\n\r\n",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/209/comments",
    "author": "mkhazen",
    "comments": [
      {
        "user": "sonichi",
        "created_at": "2023-10-12T03:14:17Z",
        "body": "@thinkall might have the answer :)"
      },
      {
        "user": "mkhazen",
        "created_at": "2023-10-14T17:16:24Z",
        "body": "@thinkall is it because MS fabric is running on 3.10? version of python - is there anything i can do such as a custom library to ensure it works with 3.10 because i cannot change the python version in MS fabric"
      },
      {
        "user": "thinkall",
        "created_at": "2023-10-16T01:57:20Z",
        "body": "> @thinkall is it because MS fabric is running on 3.10? version of python - is there anything i can do such as a custom library to ensure it works with 3.10 because i cannot change the python version in MS fabric\r\n\r\nHi @mkhazen , the python version is good. Could you share more details about how you set up the API keys for GPT models?\r\nThe error message shows that maybe you were using openai models but the package called  Azure OpenAI endpoints."
      },
      {
        "user": "sonichi",
        "created_at": "2023-10-22T15:50:55Z",
        "body": "Is it because the default `api_type` in the MS Fabric env is \"azure\"?"
      }
    ]
  },
  {
    "number": 2139,
    "title": "Simply config_list creation in notebooks and fix examples where llm configs are shared",
    "created_at": "2024-03-25T06:26:07Z",
    "closed_at": "2024-08-27T01:37:58Z",
    "labels": [
      "documentation",
      "good first issue",
      "help wanted"
    ],
    "url": "https://github.com/microsoft/autogen/issues/2139",
    "body": "This is a notebook janitorial work. But easy to do. \n\n1. Overwhelming feedback is that using the config list creation utils creates steep learning curve for users. We can keep that as advanced utils like caching, but we can use raw config list -- a simple list of dictionaries in our notebooks. See our tutorial for example.\n\n2. Sharing llm_config is bad! Because registering functions modifies agent's llm_config. Go through the notebooks to fix examples where llm_config is shared. Replace them with separate llm_config for each agent. ",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/2139/comments",
    "author": "ekzhu",
    "comments": [
      {
        "user": "prithvi2226",
        "created_at": "2024-05-30T23:20:54Z",
        "body": "HI @ekzhu ! Is this issue still open? Thanks!!\r\n"
      }
    ]
  },
  {
    "number": 2134,
    "title": "[Feature Request]: different termination message function for different sender agent",
    "created_at": "2024-03-24T07:34:46Z",
    "closed_at": "2024-08-27T01:37:36Z",
    "labels": [
      "help wanted",
      "nested-chat"
    ],
    "url": "https://github.com/microsoft/autogen/issues/2134",
    "body": "### Is your feature request related to a problem? Please describe.\n\nA single agent needs to have different termination check for different sender agent. For example, when you register nested chats, the parent agent needs to check termination for the inner chats, and for different inner agents' different termination message might be needed. E.g., if you want an inner chat to keep working until no \"Error\" in the reply, but only do it for a specific inner recipient. \r\n\r\nRight now, the termination message check is performed on the message only, need to expand that to include sender agent as well. \n\n### Describe the solution you'd like\n\nPossible solutions:\n- Change `is_termination_msg` parameter to allow `Agent` as a parameter. \n- Supply a dictionary that maps Agent --> termination message check function. \n- Add `is_termination_msg` parameter to `register_reply` and expose this through `register_nested_chats`.\n\n\n### Additional context\n\nWe also need sender specific system message so we can have different instructions for replying to different senders.\n\n_No response_",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/2134/comments",
    "author": "ekzhu",
    "comments": [
      {
        "user": "afourney",
        "created_at": "2024-03-24T17:42:26Z",
        "body": "At least in the group chat setting, I believe you can get the sender via the message name field — but I haven’t tried it in this specific application context "
      }
    ]
  },
  {
    "number": 1344,
    "title": "[P2] Stream messages from agents to UI as they are generated",
    "created_at": "2024-01-19T23:09:15Z",
    "closed_at": "2024-10-18T19:19:17Z",
    "labels": [
      "help wanted",
      "proj-studio",
      "0.2"
    ],
    "url": "https://github.com/microsoft/autogen/issues/1344",
    "body": "\nCurrently, AutoGen studio shows the final response of agents.\nThis issue is meant to discuss approaches to stream intermediate LLM or agent results to the UI, to improve the user experience \n\nPossibly related to #1290",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/1344/comments",
    "author": "victordibia",
    "comments": [
      {
        "user": "tyler-suard-parker",
        "created_at": "2024-01-22T17:50:23Z",
        "body": "Hello Dr. Dibia.  I found a solution for this.  I am using a singleton class in a file in the autogen/oai directory.  I import that singleton class into my top script, and I store my websocket as a variable in that class.  I also modify autogen/oai/client.py to import that same script and that same websocket, which I can then use to stream data from the _completions_create function in that file."
      },
      {
        "user": "rysweet",
        "created_at": "2024-10-18T19:19:17Z",
        "body": "appears resolved."
      }
    ]
  },
  {
    "number": 1079,
    "title": "[Feature Request]: Allow a new agent join existed chat",
    "created_at": "2023-12-28T03:14:37Z",
    "closed_at": "2024-10-18T17:59:01Z",
    "labels": [
      "help wanted",
      "0.2"
    ],
    "url": "https://github.com/microsoft/autogen/issues/1079",
    "body": "### Is your feature request related to a problem? Please describe.\n\nCan Autogen allow a new agent join the conversation, interacting with other agents or human? \n\n### Describe the solution you'd like\n\nDevelop the ability to allow a new agent to join the conversation. \n\n### Additional context\n\n_No response_",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/1079/comments",
    "author": "deerleo",
    "comments": [
      {
        "user": "rickyloynd-microsoft",
        "created_at": "2023-12-28T04:28:14Z",
        "body": "@afourney "
      },
      {
        "user": "rajib76",
        "created_at": "2024-01-02T20:36:17Z",
        "body": "Adding my inputs based on the discord discussion.\r\n\r\nRajib — Yesterday at 10:06 PM\r\nI have one question on Autogen and wanted to check if anyone can please clarify. Currently in the framework, we bring the required agents together through the GROUPCHAT abstraction. This GROUPCHAT abstraction currently is static , what I mean by that is I need to predefine the participants within the groupchat. If for example I need to add or remove another assistant in the groupchat, it is not possible without changing the code. Is my understanding correct? If my understanding is correct will making the GROUPCHAT event driven(leveraging for example Kafka) will make it more dynamic?\r\n\r\nsonichi — Today at 8:39 AM\r\nCould you give an example of the use case?\r\n\r\nRajib — Today at 11:36 AM\r\nSo, it is similar to an event driven architecture. For example lets say I have three agents MATH, PHYSICS and CHEMISTRY. Each of them is capable of consuming an event from kafka and producing an event in kafka. The user proxy asks a question which based on the question classification(lets say it is a MATHS question) goes into the MATH topic. The MATH agent listening to that topic picks up the question and answers it. The answer is then produced in a topic from where the user proxy can consume and send back to the user. "
      },
      {
        "user": "rysweet",
        "created_at": "2024-10-18T17:59:01Z",
        "body": "@deerleo @rajib76 - check out the new event-driven autogen 0.4"
      }
    ]
  },
  {
    "number": 1001,
    "title": "Seeking general advise on unit testing autogen applications",
    "created_at": "2023-12-16T15:23:05Z",
    "closed_at": "2024-10-18T17:15:06Z",
    "labels": [
      "help wanted",
      "0.2"
    ],
    "url": "https://github.com/microsoft/autogen/issues/1001",
    "body": "### Describe the issue\n\nI'd be very grateful for any examples on writing tests for autogen apps, including testing for expected values, ways of locating the message containing the result.\r\n\r\nUnless this is something that can be addressed with the TestBed or AgentEval, but I gather these are not intended for unit test level evaluation.\r\n\r\nI have also come across DeepEval and wondered if this is something you'd recommend or use internally?\r\n\r\nCheers,\n\n### Steps to reproduce\n\n_No response_\n\n### Screenshots and logs\n\n_No response_\n\n### Additional Information\n\n_No response_",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/1001/comments",
    "author": "antoan",
    "comments": [
      {
        "user": "anshumankmr",
        "created_at": "2024-06-05T12:12:35Z",
        "body": "Did you find anything ?"
      },
      {
        "user": "gagb",
        "created_at": "2024-08-28T07:47:28Z",
        "body": "This is a good question. We have used AutoGenBench to establish performance of a backend. @afourney "
      },
      {
        "user": "rysweet",
        "created_at": "2024-10-18T17:15:06Z",
        "body": "quite a few examples in latest code"
      }
    ]
  },
  {
    "number": 964,
    "title": "Exception: UnicodeEncodeError: 'charmap' codec can't encode character ",
    "created_at": "2023-12-13T00:09:40Z",
    "closed_at": "2024-06-18T17:02:49Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/microsoft/autogen/issues/964",
    "body": "### Describe the bug\n\nSometimes when the Agent searches for something and tries to print the results, I get this error.  It might be an idea to deal with this right before the agent's print statement in \r\n\\autogen\\agentchat\\conversable_agent.py\", line 417, in _print_received_message\r\n    print(message[\"content\"], flush=True)\n\n### Steps to reproduce\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### Screenshots and logs\n\n_No response_\n\n### Additional Information\n\n_No response_",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/964/comments",
    "author": "tyler-suard-parker",
    "comments": [
      {
        "user": "julianakiseleva",
        "created_at": "2023-12-15T02:13:25Z",
        "body": "@afourney can you look into this please?"
      },
      {
        "user": "afourney",
        "created_at": "2023-12-15T06:52:10Z",
        "body": "@tyler-suard-parker Can you provide more information about your setup? Does \"_when the Agent searches for something_\" mean you are using retrievechat or RAG?"
      },
      {
        "user": "tyler-suard-parker",
        "created_at": "2023-12-15T19:08:10Z",
        "body": "@afourney I am using an assitant agent to download a webpage, and the agent automatically prints the results to the command line in Windows.  Windows uses \"Windows-1252\" encoding for the strings it renders in the command line, so anything represented in that character set will cause an error.  Because we want to make AutoGen as user-friendly as possible, do you think we could include something in the code that checks to see if the script is running on Windows, and if so then convert the console output to UTF-8?"
      },
      {
        "user": "thinkall",
        "created_at": "2024-06-18T08:19:38Z",
        "body": "@tyler-suard-parker has the issue been resolved?"
      },
      {
        "user": "tyler-suard-parker",
        "created_at": "2024-06-18T17:02:49Z",
        "body": "I'm sorry but we moved on from downloading web pages months ago, so I don't remember.  I think I was able to fix the issue by just setting encoding to utf-8 or just filtering out the unwanted characters."
      }
    ]
  },
  {
    "number": 930,
    "title": "[Feature Request]: Callback for openai api",
    "created_at": "2023-12-10T04:42:13Z",
    "closed_at": "2024-10-18T17:07:09Z",
    "labels": [
      "help wanted",
      "0.2"
    ],
    "url": "https://github.com/microsoft/autogen/issues/930",
    "body": "### Is your feature request related to a problem? Please describe.\r\n\r\nwhen trying to run locally, I'm finding I'm constantly tripping over everyone going to open ai /api, when I'm trying to run local llm\r\nMy request is to be able to assign a callback to the api,\r\nThat is, before calling the api, the call back is called. If the callback is handled locally it returns true, and the parameters are updated from the callback instead of going out to the api. \r\n\r\n### Describe the solution you'd like\r\n\r\nThe callback function would do the equivalent of this c++ code  in python. (Sorry, don't know python well enough)\r\n```\r\nbool Callback(const char *ToApi, char *Result)\r\n{\r\n// if possible, do stuff according to ToApi, and put the resulting value in Result, return true\r\n// if not possible return false, and normal api is called.\r\n}\r\n```\r\n\r\nIf this feature is already implemented in some way, please let me know.\r\n\r\n### Additional context\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/930/comments",
    "author": "iplayfast",
    "comments": [
      {
        "user": "rickyloynd-microsoft",
        "created_at": "2023-12-10T04:56:45Z",
        "body": "@sonichi fyi"
      },
      {
        "user": "rysweet",
        "created_at": "2024-10-18T17:07:09Z",
        "body": "closing as stale, please reopen if you disagree. "
      }
    ]
  },
  {
    "number": 864,
    "title": "Cloud Vector DB for TeachableAgent",
    "created_at": "2023-12-04T15:01:22Z",
    "closed_at": "2024-10-12T03:33:55Z",
    "labels": [
      "help wanted",
      "rag",
      "0.2"
    ],
    "url": "https://github.com/microsoft/autogen/issues/864",
    "body": "It would be really useful if the TeachableAgent use a cloud vector db for storing and retrieving memos instead of having the vector db stored locally on my machine. \r\n\r\nSomething similar to the QdrantRetrieveUserProxyAgent (e.g. a QdrantTeachableAgent) would be really good since Qdrant is a cloud-native vector db. Even better would be a teachable agent where any vector db client (including cloud-native ones) can be passed in the teach_config\r\n\r\n@Anush008 @AyushExel ",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/864/comments",
    "author": "Abhishek-Balram",
    "comments": [
      {
        "user": "rickyloynd-microsoft",
        "created_at": "2023-12-04T20:22:01Z",
        "body": "This would be a reasonable extension. Users can currently plug in other DBs by writing a class that inherits from TeachableAgent then modifying that class. Meanwhile, it would be great if somebody volunteers to enhance TeachableAgent so that users can plug in any vector DB without writing code. I don't have time myself, since I'm concentrating on making teachable agents fundamentally more capable, independent of any particular DB. "
      },
      {
        "user": "AyushExel",
        "created_at": "2023-12-04T20:42:46Z",
        "body": "@rickyloynd-microsoft I can work on it next week"
      },
      {
        "user": "benjamin-mogensen",
        "created_at": "2023-12-08T08:00:52Z",
        "body": "Is there any example code of how to initialise and load the Chrome DB separate from the RAG agent? I would like to do this as part of a start up of the app, of when someone actively wants / needs to refresh the Chrome DB. Apologies if it is trivial, I am new to autogen :)"
      },
      {
        "user": "rickyloynd-microsoft",
        "created_at": "2023-12-08T17:07:57Z",
        "body": "> Is there any example code of how to initialise and load the Chrome DB separate from the RAG agent? I would like to do this as part of a start up of the app, of when someone actively wants / needs to refresh the Chrome DB. Apologies if it is trivial, I am new to autogen :)\r\n\r\nYes, please see autogen/agentchat/contrib/teachable_agent.py"
      },
      {
        "user": "DevOtts",
        "created_at": "2024-05-21T18:39:50Z",
        "body": "Hi @AyushExel, do you have any news about this feature? It would be awesome!"
      },
      {
        "user": "rickyloynd-microsoft",
        "created_at": "2024-10-12T03:33:55Z",
        "body": "Stale. No longer planned."
      }
    ]
  },
  {
    "number": 63,
    "title": "Add support to different models to num_tokens_from_text",
    "created_at": "2023-10-01T13:44:27Z",
    "closed_at": "2023-10-09T03:21:46Z",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "url": "https://github.com/microsoft/autogen/issues/63",
    "body": "Currently only gpt-x models are supported, would be nice to add support to more models.",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/63/comments",
    "author": "thinkall",
    "comments": [
      {
        "user": "vidhula17",
        "created_at": "2023-10-02T10:19:50Z",
        "body": "I would like to work on this. Can you please assign this to me? "
      },
      {
        "user": "thinkall",
        "created_at": "2023-10-02T11:25:54Z",
        "body": "> I would like to work on this. Can you please assign this to me?\r\n\r\nHi @vidhula17 , thanks for your interest, I’ve assigned the task to you. Let me know if you need any assistance."
      },
      {
        "user": "vidhula17",
        "created_at": "2023-10-03T12:25:46Z",
        "body": "I have created a PR for this issue. Kindly look into it."
      }
    ]
  },
  {
    "number": 16,
    "title": "Add parallel tunning for openAI integration",
    "created_at": "2023-03-17T14:19:04Z",
    "closed_at": "2024-10-12T01:30:39Z",
    "labels": [
      "help wanted",
      "0.2"
    ],
    "url": "https://github.com/microsoft/autogen/issues/16",
    "body": "It would be useful to support parallel tunning in openAI integration to accelerate the tunning process. Plus, it would be better to support multi-apikeys to mitigate the API call ratio limitation of openAI.",
    "comments_url": "https://api.github.com/repos/microsoft/autogen/issues/16/comments",
    "author": "thinkall",
    "comments": [
      {
        "user": "rysweet",
        "created_at": "2024-10-12T01:30:39Z",
        "body": "@thinkall - I think this is addressed now - please reopen if you disagree, and add more details."
      }
    ]
  }
]