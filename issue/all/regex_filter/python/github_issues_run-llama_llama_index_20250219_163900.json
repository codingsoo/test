[
  {
    "number": 17191,
    "title": "[Question]: When llama-index-core will be updated to use latest pydantic version 2.10.3",
    "created_at": "2024-12-07T20:54:45Z",
    "closed_at": "2024-12-08T04:30:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/17191",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIt creates dependency issues",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/17191/comments",
    "author": "Danana1",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-12-07T21:10:05Z",
        "body": "There was tons of issues. Will need to try and see if it works again"
      }
    ]
  },
  {
    "number": 17142,
    "title": "[Question]: How to avoid loading my model everytime?",
    "created_at": "2024-12-04T09:10:02Z",
    "closed_at": "2024-12-05T15:26:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/17142",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHello everyone,\r\n\r\nI apologize if my question seems simple or naive. ğŸ˜…\r\n\r\nI'm currently developing a script that loads models (downloaded from HuggingFace) each time it runs. However, loading the necessary models takes approximately 30 seconds, which is unacceptable for my rapid development process.\r\n\r\nI've tried using Ollama, which allows models to reside in GPU memory. Unfortunately, Ollama doesn't support the models I'm using.\r\n\r\nA potential solution I'm considering is starting another process that holds the model in GPU memory and performs inference. My script could then communicate with this process via HTTP APIs. However, I'm new to LlamaIndex and could use some help. Can anyone give me some advice?\r\n\r\nThank you!ğŸ¥°",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/17142/comments",
    "author": "HairlessVillager",
    "comments": [
      {
        "user": "hristogg",
        "created_at": "2024-12-04T09:24:46Z",
        "body": "I am not sure I fully understand the details, however I will try to give an idea anyway hope it's not completely far off :)\r\n\r\nOne approach would be as you have mentioned to have a running service (e.g. Flask) on same machine/different machine which is always live and you can do an HTTP request to that service so that you do not deal with this inside your llamaindex pipeline.\r\n\r\nI am guessing you could also look at workflow deployment through llama-deploy, I guess if you make your code into a workflow it will be loaded once and you can run the workflow through an HTTP call as well, doing whatever you are doing.\r\n\r\n"
      },
      {
        "user": "HairlessVillager",
        "created_at": "2024-12-04T09:32:03Z",
        "body": "@hristogg Thanks!ğŸ¥°I'll take a look"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-12-05T15:26:24Z",
        "body": "@HairlessVillager another option is an optimized server dedicated for models, like TGI and TEI from huggingface, or vllm. I think this is a fairly common practice "
      },
      {
        "user": "HairlessVillager",
        "created_at": "2024-12-05T15:35:57Z",
        "body": "@logan-markewich Yes! That's what I expect.ğŸ˜Very thanks!"
      }
    ]
  },
  {
    "number": 17019,
    "title": "[Question]: Query engine returns 'empty response' from pinecone",
    "created_at": "2024-11-21T03:21:58Z",
    "closed_at": "2024-11-23T06:06:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/17019",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nI have been searching and scratching my head more than a week.\r\n\r\nScenario:\r\n\r\nHaving markdown and text files stored to pinecone, and would like to read those vectors from pinecone in the future.\r\n\r\nIssues:\r\n\r\nThe markdown files and text files are vectorized and stored to pinecone successfully, but when I retrieve them back as query. It shows empty response.\r\n\r\nCode:\r\n\r\nSetting up connection.\r\n\r\n```python\r\npc = Pinecone(api_key=PINECONE_API_KEY)\r\n\r\npc.create_index(\r\n        name = 'index-data',\r\n        dimension = 1536,\r\n        metric = 'cosine',\r\n        spec = ServerlessSpec(\r\n            cloud = 'aws',\r\n            region = AWS_REGION_NAME,\r\n        )\r\n    )\r\n\r\npc_index = pc.Index('index-data')\r\n``` \r\n\r\nThere are multiple files stored as individual namespaces.\r\n\r\n```python\r\n                namespace = file_name.rsplit('.', 1)[0]\r\n                reader = SimpleDirectoryReader(input_files=[file_path])\r\n                documents = reader.load_data()\r\n                parser = SimpleNodeParser()\r\n                nodes = parser.get_nodes_from_documents(documents)\r\n                vector_store = PineconeVectorStore(pinecone_index=pc_index, namespace=namespace)\r\n                storage_context = StorageContext.from_defaults(vector_store=vector_store)\r\n                index = VectorStoreIndex(nodes, service_context=Settings, storage_context=storage_context)\r\n``` \r\n\r\nThis is code loading vectors from pinecone.\r\n\r\n```python\r\nvector_store = PineconeVectorStore(pinecone_index=pc_index)\r\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\r\nindex = VectorStoreIndex.from_vector_store(vector_store, service_context = Settings, storage_context=storage_context, embedding_model=Settings.embed_model)\r\n\r\n``` \r\n\r\nThis shows all files are vectorized and stored to pinecone.\r\n\r\n```python\r\n{'dimension': 1536,\r\n 'index_fullness': 0.0,\r\n 'namespaces': {'A': {'vector_count': 193},\r\n                'B': {'vector_count': 42},\r\n                'C': {'vector_count': 82},},\r\n 'total_vector_count': 361}\r\n``` \r\n\r\nThis is query retriever, which shows empty response.\r\n\r\n```python\r\nuser_prompt = \"what is acoustic\"\r\nquery_engine = index.as_query_engine()\r\ninit_response = query_engine.query(user_prompt)\r\nresult_str = str(init_response)\r\nprint (result_str)\r\n``` \r\n --> Empty Response\r\n\r\nQuestion:\r\n\r\nHow can I vectorize file and store to the pinecone? \r\nHow can I retrieve vectors from pinecone?\r\nWhat is the best way to write prompt query?\r\n\r\n\r\nThanks\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/17019/comments",
    "author": "kxu-prosera",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-11-21T03:23:01Z",
        "body": "This looks correct to me. But note that pinecone serverless takes like a minute or more  between indexing and querying before you can use it "
      },
      {
        "user": "kxu-prosera",
        "created_at": "2024-11-21T03:30:08Z",
        "body": "Thank you for commenting. I just wonder if there are anything I did wrongly?\r\n\r\nI am able to see the vectors in the pinecone, but the issue is that I can not retrieve anything from query prompt.\r\n\r\nTo echo your point, I did wait for long time.\r\n\r\nAny other potential issues we can think about?"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-11-21T04:21:05Z",
        "body": "Hmm, probably unrelated maybe, but `service_context=Settings` is not correct. Either set the models on the settings directly, or pass in the models directly\r\n```\r\nVectorStoreIndex(...  embed_model=embed_model)\r\n...\r\nindex.as_query_engine(.., llm=llm)\r\n```\r\n\r\nAlso, you probably shouldn't pass the storage context into `from_vector_store`"
      },
      {
        "user": "kxu-prosera",
        "created_at": "2024-11-21T04:31:06Z",
        "body": "Let me modify and give a shot of those two places.\r\n\r\nJust for your reference. Here are settings.\r\n\r\n```python\r\nSettings.llm = llm_model\r\nSettings.embed_model = embed_model\r\nSettings.chunk_size = 512\r\n``` "
      },
      {
        "user": "kxu-prosera",
        "created_at": "2024-11-21T04:50:05Z",
        "body": "Hummm no lucky.\r\n\r\nHowever, just some reference here. I am able to retrieve response back, if I use llamaindex in-memory vectoring storage -- load_index_from_storage, rather than from pinecone.\r\n\r\n\r\nHere is the code.\r\n\r\n```python\r\nif not os.path.exists(PERSIS_DIR):\r\n    #create the new index\r\n    reader = SimpleDirectoryReader(input_dir=input_directory, recursive=True)\r\n    documents = reader.load_data()\r\n    parser = SimpleNodeParser()\r\n    nodes = parser.get_nodes_from_documents(documents)\r\n    Settings.llm = llm_model\r\n    Settings.embed_model = embed_model\r\n    Settings.chunk_size = 512\r\n    storage_context = StorageContext.from_defaults()\r\n    index = VectorStoreIndex(nodes, service_context = Settings, storage_context=storage_context)\r\n    index.storage_context.persist(persist_dir=PERSIS_DIR)\r\nelse:\r\n    Settings.llm = llm_model\r\n    Settings.embed_model = embed_model\r\n    Settings.chunk_size = 512\r\n    storage_context = StorageContext.from_defaults(persist_dir=PERSIS_DIR)\r\n    index = load_index_from_storage(service_context = Settings, storage_context=storage_context)\r\n\r\n``` "
      },
      {
        "user": "kxu-prosera",
        "created_at": "2024-11-21T05:57:31Z",
        "body": "Just sharing the resolution.\r\n\r\nThe legacy package of llama_index is the culprit. Use the latest one to fix the issue."
      }
    ]
  },
  {
    "number": 16922,
    "title": "[Question]: Alternative Methods for Report Generation Using LLMs Without LlamaParse or LlamaCloud?",
    "created_at": "2024-11-12T08:58:58Z",
    "closed_at": "2024-11-12T16:14:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/16922",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\n\r\nIâ€™m currently working on generating reports using large language models (LLMs) and would like to explore alternatives that donâ€™t rely on LlamaParse or LlamaCloud. Are there any other recommended libraries, tools, or techniques that are open-source or free to use for effective report generation using LLMs?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/16922/comments",
    "author": "LikhithRishi",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-11-12T16:14:10Z",
        "body": "@LikhithRishi You can replace the components in any report-gen tutorial with local equivalents (unstructured, docling, or marker for parsing, literally any vector db for retrieval). It will probably work a little less better though, since llama-cloud/llama-parse are heavily optimized for their specific use cases"
      }
    ]
  },
  {
    "number": 16194,
    "title": "[Question]: Does `LLMTextCompletionProgram` have a retry capability?",
    "created_at": "2024-09-24T17:00:57Z",
    "closed_at": "2024-09-24T18:53:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/16194",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nDoes `LLMTextCompletionProgram` have a retry capability (if the generated result isn't a valid model)?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/16194/comments",
    "author": "brycecf",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-09-24T17:02:38Z",
        "body": "It does not. Been meaning to add something like this but haven't had a chance yet"
      }
    ]
  },
  {
    "number": 15721,
    "title": "[Question]: Can i  pass context to a FunctionTool , i dont see any examples with FunctionTool having context , can we send specific inputs to a FunctionTool? Thank you",
    "created_at": "2024-08-29T19:28:59Z",
    "closed_at": "2024-08-30T10:57:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/15721",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nCan i  pass context to a FunctionTool , i dont see any examples with FunctionTool having context , can we send specific inputs to a FunctionTool? Thank you",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/15721/comments",
    "author": "Rohith-Scalers",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-08-29T19:30:01Z",
        "body": "Your going to have to expand on that slightly.\r\n\r\nThe llm writes the inputs yo any tool. If there is additional info needed beyond that, that should be included within the function you write "
      },
      {
        "user": "Rohith-Scalers",
        "created_at": "2024-08-29T19:38:07Z",
        "body": "i am trying to use this as a FunctionalTool , inside React agent work flow i have defined all these variables but when i am running the workflow the index input to the functionalTool is string rather than the index . is this approach wrong ?\r\n```\r\ndef vector_query_docker_logs(docker_logs_index,llm,query,time_data):\r\n    \"\"\"\r\n    Executes a query on the docker_logs index with time-based filters.\r\n\r\n    This function constructs metadata filters based on the provided time data \r\n    (previous and current timestamps). It applies the filters to the query engine \r\n    associated with the docker_logs index and executes the query using the specified \r\n    language model (LLM). The function returns the response from the query engine.\r\n\r\n    Args:\r\n        docker_logs_index (Index): The index to be queried you can get this from self.docker_logs_index.\r\n        llm (LLM): The language model to be used for the query engine.\r\n        query (str): The query string to be executed on the docker_logs index.\r\n        time_data (dict): A dictionary containing 'previous_time' and 'current_time' \r\n                          as Unix timestamps, used to create time-based filters.\r\n\r\n    Returns:\r\n        Response: The response object from the query engine after executing the query.\r\n    \"\"\"\r\n    ts_metadata_dicts = [{\"key\": \"time\", \"value\": ts} for ts in range(int(time_data[\"previous_time\"]), int(time_data[\"current_time\"]))]\r\n    filters = MetadataFilters.from_dicts(ts_metadata_dicts, condition=FilterCondition.OR)\r\n    query_engine = docker_logs_index.as_query_engine(llm=llm, filters=filters)\r\n    query_engine = docker_logs_index.as_query_engine(llm=llm)\r\n    response = query_engine.query(query)\r\n    return response``"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-08-29T20:30:27Z",
        "body": "Yea, the idea with `FunctionTool` is that the LLM writes the inputs. Here, with complex objects like an index or llm, the LLM itself cannot insert these. \r\n\r\nThese variables should either be included in the function body itself, or referenced as globals.\r\n\r\nFor example, I might rewrite this as (type annotations are also extremely important!)\r\n\r\n```python\r\ndef vector_query_docker_logs(query: str, time_data: Dict[str, str]) -> str:\r\n    \"\"\"\r\n    Executes a query on the docker_logs index with time-based filters.\r\n\r\n    This function constructs metadata filters based on the provided time data \r\n    (previous and current timestamps). It applies the filters to the query engine \r\n    associated with the docker_logs index and executes the query using the specified \r\n    language model (LLM). The function returns the response from the query engine.\r\n\r\n    Args:\r\n        query (str): The query string to be executed on the docker_logs index.\r\n        time_data (dict): A dictionary containing 'previous_time' and 'current_time' \r\n                          as Unix timestamps, used to create time-based filters.\r\n\r\n    Returns:\r\n        Response: The response object from the query engine after executing the query.\r\n    \"\"\"\r\n    llm = OpenAI(...)\r\n    docker_logs_index = VectorStoreIndex.from_vector_store(vector_store)\r\n    ...\r\n```"
      },
      {
        "user": "Rohith-Scalers",
        "created_at": "2024-08-30T04:47:29Z",
        "body": "Thank you , it worked but is there way to make llm send complex objects than just strings ? any references would be highly helpful Thank you for quick response :)"
      }
    ]
  },
  {
    "number": 15412,
    "title": "[Question]: ",
    "created_at": "2024-08-15T21:35:16Z",
    "closed_at": "2024-08-15T21:41:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/15412",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nThe documentation for persisting and storing index's isn't clear. \r\n\r\nFor example I get the error `Cannot initialize from a vector store that does not store text.` when all the documents are that is loaded is `.md` files, or in otherwords text. I can't seem to find much help on the topic, and the documentation shows the usage just how I use it save for the service context -- and isn't clear what can and cannot be stored. \r\n\r\nI store like:\r\n\r\n```python\r\n        temp = folder_paths.get_temp_directory()\r\n        vector_path = os.path.join(temp, str(uuid.uuid4()))\r\n        \r\n        llm_index.storage_context.persist(persist_dir=vector_path)\r\n```\r\n\r\nAnd load like:\r\n\r\n```python\r\n        if not os.path.exists(vector_store_path) or not os.path.isdir(vector_store_path):\r\n            raise Exception(f\"Invalid vector store path: {vector_store_path}\")\r\n        \r\n        storage_context = StorageContext.from_defaults(persist_dir=vector_store_path)\r\n        llm_index = VectorStoreIndex.from_vector_store(\r\n            vector_store=storage_context.vector_store,\r\n            storage_context=storage_context,\r\n            service_context=llm_service_context\r\n        )\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/15412/comments",
    "author": "WAS-PlaiLabs",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-08-15T21:37:00Z",
        "body": "You should be loading with \r\n\r\n`index = load_index_from_storage(storage_context, service_context=service_context)`"
      },
      {
        "user": "WAS-PlaiLabs",
        "created_at": "2024-08-15T21:41:19Z",
        "body": "Ohh! Well that would do it. Thanks for the prompt reply!"
      }
    ]
  },
  {
    "number": 15351,
    "title": "[Question]: Future of Query Engines?",
    "created_at": "2024-08-13T19:32:27Z",
    "closed_at": "2024-08-14T17:44:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/15351",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nWith the feature freeze of query pipelines, what is the roadmap for query engines with regards to workflows (given their dispersion throughout the codebase)?\r\n\r\nShould workflows be wrapped up as query engines to support broader compatibility with LlamaIndex, as you could with query engines, or are they intended to be a replacement?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/15351/comments",
    "author": "brycecf",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-08-13T19:34:01Z",
        "body": "We have plans to convert query engines to workflows yes ğŸ‘ This should be undisruptive for users though, just under the hood changes "
      },
      {
        "user": "brycecf",
        "created_at": "2024-08-14T12:45:16Z",
        "body": "@logan-markewich In that case, the existing `QueryEngine` classes would remain but their underlying would become workflows?"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-08-14T16:57:24Z",
        "body": "@brycecf exactly ğŸ‘ğŸ» The reason for doing this is, its non-breaking for users, but also opens up interesting optimizations for the future (i.e. any feature from workflows would be available in any query engine)"
      }
    ]
  },
  {
    "number": 15178,
    "title": "[Question]: Getting a list of Document content from SimpleDirectoryReader",
    "created_at": "2024-08-06T20:17:46Z",
    "closed_at": "2024-08-06T20:27:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/15178",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nllama-index: 0.10.36\r\npython: 3.11.9\r\nUbunutu 22.04\r\n\r\nSuppose I am using a `SimpleDirectoryReader` in the following manner:\r\n\r\n```python\r\ndocs = SimpleDirectoryReader(\"/path/to/my/data\").load_data()\r\n```\r\n\r\nI can see that `docs` is a list of `Document` objects. What is the most efficient way to create a list that contains the content of each one of those `Document` objects?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/15178/comments",
    "author": "aclifton314",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-08-06T20:21:46Z",
        "body": "`texts = [doc.text for doc in docs]`"
      },
      {
        "user": "aclifton314",
        "created_at": "2024-08-06T20:27:46Z",
        "body": "@logan-markewich thanks!"
      }
    ]
  },
  {
    "number": 15008,
    "title": "åœ¨åˆ›å»ºçŸ¥è¯†å›¾è°±çš„ä»£ç ä¸­ï¼Œæ‰§è¡Œåˆ°retrieveæ—¶ï¼Œå‘ç”Ÿäº†ä¸‹é¢çš„é”™è¯¯ï¼Œè¡¨ç¤ºValueError: too many values to unpack (expected 2) é€šå¸¸è¡¨ç¤ºåœ¨è§£åŒ…æ“ä½œä¸­ï¼Œå³ä¾§æä¾›çš„å€¼æ•°é‡å¤šäºå·¦ä¾§é¢„æœŸæ¥æ”¶çš„å˜é‡æ•°é‡",
    "created_at": "2024-07-29T02:51:31Z",
    "closed_at": "2024-11-04T16:04:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/15008",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\næˆ‘çš„ä»£ç å¦‚ä¸‹ï¼š\r\nfrom llama_index.core import KnowledgeGraphIndex,SimpleDirectoryReader,Settings\r\n\r\nfrom llama_index.graph_stores.neo4j import Neo4jGraphStore\r\nfrom llama_index.core.node_parser import SentenceSplitter\r\nfrom llama_index.core import PromptTemplate\r\nfrom llama_index.llms.huggingface import HuggingFaceLLM\r\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\r\nfrom llama_index.core import StorageContext, load_index_from_storage\r\nimport os\r\nimport torch\r\nimport json\r\nimport re\r\n#\r\n# llm = HuggingFaceLLM(\r\n#     tokenizer_name='/home/cdhd/Desktop/glm4/glm-4-9b-chat',\r\n#     model_name='/home/cdhd/Desktop/glm4/glm-4-9b-chat',\r\n# )\r\n\r\nSYSTEM_PROMPT = \"\"\"ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±æŸ¥è¯¢ä¸“å®¶ï¼Œè¯·å°†æŸ¥è¯¢åˆ°çš„å†…å®¹æ•´ç†æˆå®Œæ•´çš„ä¸€å¥è¯è¿”å›ï¼Œä¸æ·»åŠ å…¶ä»–çš„å†…å®¹å’Œè‡ªå·±çš„ç†è§£.\r\n\"\"\"\r\n\r\nquery_wrapper_prompt = PromptTemplate(\r\n    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\r\n)\r\n\r\nllm = HuggingFaceLLM(\r\n    context_window=4096,\r\n    max_new_tokens=1024,\r\n    generate_kwargs={\"temperature\": 0.1, \"do_sample\": True},\r\n    query_wrapper_prompt=query_wrapper_prompt,\r\n    # tokenizer_name='/home/cdhd/PythonCode/pythonProject/model/gpt2',\r\n    # model_name='/home/cdhd/PythonCode/pythonProject/model/gpt2',\r\n    tokenizer_name='/home/huidao/SDHD_AI_project/glm4chat/glm-4-9b-chat',\r\n    model_name='/home/huidao/SDHD_AI_project/glm4chat/glm-4-9b-chat',\r\n    device_map=\"auto\",\r\n    # change these settings below depending on your GPU\r\n    model_kwargs={\"torch_dtype\": torch.float16},\r\n\r\n\r\n)\r\n\r\nembed= HuggingFaceEmbedding(\r\n    model_name='/home/huidao/SDHD_AI_project/flask_project/KnowledgeBase/models/bge-large-zh-v1.5/models--BAAI--bge-large-zh-v1.5/snapshots/79e7739b6ab944e86d6171e44d24c997fc1e0116',\r\n\r\n)\r\n\r\ndef graph_ground_process(documents,question,flag):\r\n\r\n    # æŒ‡å®šè¦æ£€æŸ¥çš„è·¯å¾„\r\n    path = './ground'\r\n    # åˆå§‹åŒ–Neo4jå›¾å­˜å‚¨\r\n    graph_store = Neo4jGraphStore(\r\n        username=\"neo4j\",\r\n        password=\"12345678\",\r\n        url=\"bolt://localhost:7687\",\r\n        database=\"ground\"\r\n    )\r\n\r\n    # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡\r\n    storage_context = StorageContext.from_defaults(graph_store=graph_store)\r\n\r\n    Settings.llm = llm\r\n\r\n    Settings.embed_model = embed\r\n\r\n    # node_parser = SentenceSplitter()\r\n    # nodes = node_parser.get_nodes_from_documents(documents)\r\n\r\n    if os.path.exists(path) and os.path.isdir(path) and (flag == True):\r\n        print(\"exit!!!!!!\")\r\n        index = load_index_from_storage(\r\n            StorageContext.from_defaults(persist_dir=path)\r\n        )\r\n        # embed_model=embed\r\n        retriever = index.as_retriever(retriever_mode=\"hybrid\",embed_model=embed)\r\n        ans = retriever.retrieve(question)\r\n        print(\"1\",ans)\r\n        for node in ans:\r\n            text = node.metadata['kg_rel_texts']\r\n            print(text)\r\n        query_engine = index.as_query_engine(\r\n            include_text=True, response_mode=\"tree_summarize\",embedding_mode=\"hybrid\",\r\n\r\n        )\r\n        response = query_engine.query(question)\r\n        response_str = response.__str__()\r\n\r\n        print(\"hh\", response_str)\r\n\r\n        result = re.split(r\"[/INST]|<</INST>>\", response_str)\r\n        result = result[0]\r\n        print(\"result:\", result)\r\n\r\n    else:\r\n\r\n        print(\"no exit!!!!!!!\")\r\n        # æ¸…ç†æ•°æ®é›†\r\n        graph_store.query(\r\n            \"\"\"\r\n        MATCH (n) DETACH DELETE n\r\n        \"\"\"\r\n        )\r\n\r\n        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ç´¢å¼•\r\n        index = KnowledgeGraphIndex.from_documents(\r\n            documents,\r\n            # [],\r\n            storage_context=storage_context,\r\n            max_triplets_per_chunk=None,\r\n            include_embeddings=True,\r\n        )\r\n\r\n        json_filename = '/home/huidao/KnowledgeProject/pythonProject/GroundSystem/Data_Json/data.json'\r\n        with open(json_filename, 'r' ,encoding='utf-8') as file:\r\n            data = json.load(file)\r\n            file.close()\r\n\r\n        tups = [(item['first_entity'],item['relation'],item['two_entity']) for item in data]\r\n\r\n        for tup in tups:\r\n            # index.upsert_triplet_and_node(tup, nodes[0])\r\n            index.upsert_triplet(tup,include_embeddings=True)\r\n\r\n        # embed_model=embed_model\r\n        retriever = index.as_retriever(retriever_mode=\"hybrid\",embed_model=embed,similarity_top_k=10)\r\n        print(\"1111\")\r\n        ans = retriever.retrieve(question)\r\n        print(\"1\",ans)\r\n        for node in ans:\r\n            text = node.metadata['kg_rel_texts']\r\n            print(text)\r\n        query_engine = index.as_query_engine(\r\n            include_text=True, response_mode=\"tree_summarize\",embedding_mode=\"hybrid\",\r\n            similarity_top_k = 10,\r\n        )\r\n        response = query_engine.query(question)\r\n\r\n        response_str = response.__str__()\r\n\r\n        print(\"hh\",response_str )\r\n\r\n        result = re.split(r\"[/INST]|<</INST>>\", response_str)\r\n        result = result[0]\r\n        print(\"result:\",result)\r\n        # display(Markdown(f\"<b>{response}</b>\"))\r\n        # save and load\r\n        index.storage_context.persist(persist_dir=path)\r\n\r\n    return result\r\n\r\n\r\n\r\n\r\n# æ£€æŸ¥æ˜¯å¦ç›´æ¥è¿è¡Œäº†æ­¤è„šæœ¬\r\nif __name__ == \"__main__\":\r\n    documents = SimpleDirectoryReader(\r\n        '/home/huidao/KnowledgeProject/pythonProject/åœ°é¢æµç¨‹'\r\n    ).load_data()\r\n    question = \"èŠ‚æµé˜€çš„åˆ†ç±»æœ‰å“ªå‡ ç§ï¼Œåˆ†åˆ«å…·æœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ\"\r\n    graph_ground_process(documents,question,False)\r\n    \r\n    æŠ¥é”™ä¿¡æ¯ï¼š\r\n    Traceback (most recent call last):\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/GroundSystem/ground_demo.py\", line 160, in <module>\r\n    graph_ground_process(documents,question,False)\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/GroundSystem/ground_demo.py\", line 105, in graph_ground_process\r\n    index = KnowledgeGraphIndex.from_documents(\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py\", line 145, in from_documents\r\n    return cls(\r\n           ^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/core/indices/knowledge_graph/base.py\", line 109, in __init__\r\n    super().__init__(\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py\", line 94, in __init__\r\n    index_struct = self.build_index_from_nodes(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py\", line 216, in build_index_from_nodes\r\n    return self._build_index_from_nodes(nodes, **build_kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/core/indices/knowledge_graph/base.py\", line 215, in _build_index_from_nodes\r\n    triplets = self._extract_triplets(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/core/indices/knowledge_graph/base.py\", line 159, in _extract_triplets\r\n    return self._llm_extract_triplets(text)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/core/indices/knowledge_graph/base.py\", line 163, in _llm_extract_triplets\r\n    response = self._llm.predict(\r\n               ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 230, in wrapper\r\n    result = func(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/core/llms/llm.py\", line 438, in predict\r\n    response = self.complete(formatted_prompt, formatted=True)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 230, in wrapper\r\n    result = func(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py\", line 429, in wrapped_llm_predict\r\n    f_return_val = f(_self, *args, **kwargs)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/llama_index/llms/huggingface/base.py\", line 360, in complete\r\n    tokens = self._model.generate(\r\n             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 1914, in generate\r\n    result = self._sample(\r\n             ^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2651, in _sample\r\n    outputs = self(\r\n              ^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/.cache/huggingface/modules/transformers_modules/glm-4-9b-chat/modeling_chatglm.py\", line 878, in forward\r\n    transformer_outputs = self.transformer(\r\n                          ^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/.cache/huggingface/modules/transformers_modules/glm-4-9b-chat/modeling_chatglm.py\", line 774, in forward\r\n    hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(\r\n                                                                      ^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/.cache/huggingface/modules/transformers_modules/glm-4-9b-chat/modeling_chatglm.py\", line 607, in forward\r\n    layer_ret = layer(\r\n                ^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/.cache/huggingface/modules/transformers_modules/glm-4-9b-chat/modeling_chatglm.py\", line 510, in forward\r\n    attention_output, kv_cache = self.self_attention(\r\n                                 ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/KnowledgeProject/pythonProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/huidao/.cache/huggingface/modules/transformers_modules/glm-4-9b-chat/modeling_chatglm.py\", line 376, in forward\r\n    cache_k, cache_v = kv_cache\r\n    ^^^^^^^^^^^^^^^^\r\nValueError: too many values to unpack (expected 2)\r\n\r\nè¿™ä¸ªå…·ä½“åº”è¯¥å¦‚ä½•ä¿®æ”¹ä»£ç è§£å†³æˆ‘çš„é—®é¢˜ã€‚",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/15008/comments",
    "author": "li-hhhh",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-07-29T02:54:10Z",
        "body": "Seems like an issue with huggingface/transformers, not llamaindex "
      },
      {
        "user": "li-hhhh",
        "created_at": "2024-07-29T06:27:38Z",
        "body": "> Seems like an issue with huggingface/transformers, not llamaindex\r\n\r\nå¥½çš„ï¼Œè°¢è°¢ã€‚çœŸçš„å¾ˆå¥‡æ€ªï¼Œæˆ‘ä¹‹å‰ç›¸åŒçš„ä»£ç éƒ½èƒ½æ­£å¸¸è¿è¡Œï¼Œä»Šå¤©çªç„¶å‘ç”Ÿè¿™æ ·çš„é—®é¢˜ã€‚"
      }
    ]
  },
  {
    "number": 15006,
    "title": "[Question]: {'message': 'Failed to send 1 objects in a batch of 1. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}",
    "created_at": "2024-07-29T02:23:31Z",
    "closed_at": "2024-07-29T02:29:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/15006",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nimport time\r\nimport weaviate\r\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\r\nfrom llama_index.vector_stores.weaviate import WeaviateVectorStore\r\nfrom llama_index.core.node_parser import SentenceSplitter\r\nfrom llama_index.core import StorageContext, Settings\r\nfrom llama_index.readers.file import PyMuPDFReader\r\nfrom llama_index.embeddings.openai import OpenAIEmbedding\r\nfrom llama_index.llms.openai import OpenAI\r\nfrom dotenv import load_dotenv, find_dotenv\r\n_ = load_dotenv(find_dotenv())\r\nimport nest_asyncio\r\nnest_asyncio.apply()  # Only needed in Jupyter notebooks\r\nweaviate_client = weaviate.connect_to_local()\r\nweaviate_client.connect()\r\nSettings.llm = OpenAI(temperature=0, model=\"gpt-4o\")\r\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=512)\r\nsplitter = SentenceSplitter(chunk_size=512, chunk_overlap=100)\r\ndocuments = SimpleDirectoryReader(\"./data1\").load_data()\r\nnodes = splitter.get_nodes_from_documents(documents)\r\nprint(nodes)\r\nif weaviate_client.collections.exists(\"TextNode\"):\r\n    weaviate_client.collections.delete(\"TextNode\")\r\nschema = {\r\n           \"class\": \"TextNode\",\r\n           \"properties\": [\r\n               {\"name\": \"id_\", \"dataType\": [\"string\"], },\r\n               {\"name\": \"embedding\", \"dataType\": [\"number[]\"], },\r\n               {\"name\": \"file_path\", \"dataType\": [\"string\"], },\r\n               {\"name\": \"file_name\", \"dataType\": [\"string\"], },\r\n               {\"name\": \"file_type\", \"dataType\": [\"string\"], },\r\n               {\"name\": \"file_size\", \"dataType\": [\"int\"], },\r\n               {\"name\": \"creation_date\", \"dataType\": [\"string\"], },\r\n               {\"name\": \"last_modified_date\", \"dataType\": [\"string\"], },\r\n               # {\"name\": \"source\", \"dataType\": [\"string\"], },\r\n               {\"name\": \"text\", \"dataType\": [\"text\"], },\r\n               {\"name\": \"start_char_idx\", \"dataType\": [\"int\"], },\r\n               {\"name\": \"end_char_idx\", \"dataType\": [\"int\"], }\r\n               # {\"name\": \"metadata_str\", \"dataType\": [\"string\"], },\r\n               # {\"name\": \"content\", \"dataType\": [\"text\"], },\r\n           ]\r\n       }\r\nweaviate_client.collections.create_from_dict(schema)\r\ntry:\r\n    collection = weaviate_client.collections.get(\"TextNode\")\r\n    data_lines = []\r\n    for node in nodes:\r\n        embedding = Settings.embed_model.get_text_embedding(node.text)  # ç”ŸæˆåµŒå…¥\r\n        node.embedding = embedding \r\n        properties = {\r\n            \"id\": node.id_,\r\n            \"embedding\": node.embedding,\r\n            \"file_path\": node.metadata.get(\"file_path\"),\r\n            \"file_name\": node.metadata.get(\"file_name\"),\r\n            \"file_type\": node.metadata.get(\"file_type\"),\r\n            \"file_size\": node.metadata.get(\"file_size\"),\r\n            \"creation_date\": node.metadata.get(\"creation_date\"),\r\n            \"last_modified_date\": node.metadata.get(\"last_modified_date\"),\r\n            # \"source\": node.metadata.get(\"source\"),\r\n            \"text\": node.text,\r\n            \"start_char_idx\": node.start_char_idx,\r\n            \"end_char_idx\": node.end_char_idx,\r\n            # \"metadata_str\": node.metadata_template,\r\n            # \"content\": node.text,\r\n        }\r\n        data_lines.append(properties)\r\n    print(data_lines)\r\n    with collection.batch.dynamic() as batch:\r\n        for data_line in data_lines:\r\n            batch.add_object(properties=data_line)\r\n    print(\"node insert completationï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼\")\r\n    vector_store = WeaviateVectorStore(weaviate_client=weaviate_client, index_name=\"TextNode\")\r\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\r\n    index = VectorStoreIndex.from_vector_store(vector_store)\r\n    print(index.index_struct)\r\n    print(index.storage_context)\r\n\r\n    query_engine = index.as_query_engine()\r\n\r\n    while True:\r\n        question = input(\"User: \")\r\n        if question.strip() == \"\":\r\n            break\r\n        start_time = time.time()\r\n        response = query_engine.query(question)\r\n        end_time = time.time()\r\n        print(f\"Time taken: {end_time - start_time} seconds\")\r\n        print(f\"AI: {response}\")\r\nfinally:\r\n    weaviate_client.close()\r\n\r\nError message is:\r\n{'message': 'Failed to send 1 objects in a batch of 1. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\r\n\r\nHow should I solve it?Thank you",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/15006/comments",
    "author": "haozhuoyuan",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-07-29T02:29:18Z",
        "body": "You opened this twice, closing as duplicate "
      }
    ]
  },
  {
    "number": 14923,
    "title": "[Question]: Different similarity_top_k values return data with significant differences and varying scores",
    "created_at": "2024-07-24T03:24:37Z",
    "closed_at": "2024-08-01T00:18:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14923",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nmy method code like:\r\n```\r\ndef retrieve_data(self, collection_name: str, similarity_top_k: int, query: str, **kwargs) -> list:\r\n        result = []\r\n        try:\r\n            # è®¾ç½®å‘é‡æ•°æ®é›†\r\n            chroma_collection = self.chroma_client.get_collection(name=collection_name)\r\n\r\n            # è®¾ç½®å‘é‡å­˜å‚¨å¼•æ“\r\n            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\r\n            index = VectorStoreIndex.from_vector_store(vector_store)\r\n\r\n            # æ£€ç´¢\r\n            retriever = index.as_retriever(similarity_top_k=similarity_top_k)\r\n            response = retriever.retrieve(query)\r\n\r\n            for node in response:\r\n                result.append(node.get_content())\r\n                result.append(node.get_score())\r\n\r\n        except Exception as e:\r\n            log.error(f\"Failed to retrieve {collection_name} data from chroma store. {e}\", exc_info=True)\r\n            raise e\r\n\r\n        return result\r\n\r\n```\r\n\r\ntake method as: vector.retrieve_data(collection_name=\"hive\", similarity_top_k=15, query=\"è§†é¢‘è¢«è¸©çš„ç´¯è®¡å¿«ç…§äº‹å®è¡¨\")\r\n\r\nwhile set similarity_top_k to 15ï¼Œit returns top3 data with score as:\r\n```\r\ntop1: {'è¡¨ä¿¡æ¯': {'db': 'algo', 'table': 'video_statistical_table', 'å­—æ®µ': [{'name': 'video_id'}, {'name': 'vv'}, {'name': 'product_click'}, {'name': 'effect_click'}, {'name': 'music_click'}, {'name': 'duet_click'}, {'name': 'topic_click'}, {'name': 'follow_cnt'}, {'name': 'like_cnt'}, {'name': 'click_cnt'}, {'name': 'shared_cnt'}, {'name': 'comments_cnt'}, {'name': 'exposed'}, {'name': 'country_region'}], 'åˆ†åŒº': [{'name': 'day'}]}}\r\nscore: 0.32294393044439756\r\n\r\ntop2: {'è¡¨ä¿¡æ¯': {'db': 'algo', 'table': 'video_valid_play_table', 'å­—æ®µ': [{'name': 'country_region'}, {'name': 'video_id'}, {'name': 'valid_play_count'}, {'name': 'dispatch_cnt'}, {'name': 'impression_cnt'}, {'name': 'play_cnt'}, {'name': 'like_cnt'}, {'name': 'follow_cnt'}, {'name': 'share_cnt'}, {'name': 'comment_cnt'}, {'name': 'complete_cnt'}, {'name': 'pos_cnt'}, {'name': 'fix_valid_play_count'}, {'name': 'dur'}, {'name': 'play_second'}, {'name': 'fix_complete_cnt'}], 'åˆ†åŒº': [{'name': 'day'}]}}\r\nscore: 0.3165133714145665\r\n\r\ntop3: {'è¡¨ä¿¡æ¯': {'db': 'mysql_tb', 'table': 'welog_tbl_video_counter', 'å­—æ®µ': [{'name': 'rdeleted'}, {'name': 'rversion'}, {'name': 'post_id'}, {'name': 'comment_count'}, {'name': 'like_count'}, {'name': 'play_count'}, {'name': 'share_count'}, {'name': 'update_time'}, {'name': 'robot_comment_count'}, {'name': 'robot_like_count'}, {'name': 'robot_play_count'}], 'åˆ†åŒº': []}}\r\nscore: 0.3164510076639161\r\n```\r\n\r\nbut while set similarity_top_k to 16ï¼Œit returns top3 data with score as:\r\n```\r\ntop1: {'ä¸­æ–‡å': 'è§†é¢‘è¢«è¸©çš„ç´¯è®¡å¿«ç…§äº‹å®è¡¨(likee-æ±‡æ€»-ç¤¾äº¤äº’åŠ¨)', 'è¡¨ä¿¡æ¯': {'db': 'like_dw_sid', 'table': 'dws_like_sid_his_acc_video_dislike_producer', 'desc': 'è§†é¢‘è¢«è¸©çš„ç´¯è®¡å¿«ç…§äº‹å®è¡¨', 'å­—æ®µ': [{'name': 'video_id', 'desc': 'è§†é¢‘id', 'type': 'bigint'}, {'name': 'video_author_uid', 'desc': 'è§†é¢‘ä½œè€…id', 'type': 'bigint'}, {'name': 'video_create_time', 'desc': 'è§†é¢‘ç”Ÿäº§æ—¶é—´', 'type': 'bigint'}, {'name': 'his_acc_video_dislike_count_02', 'desc': 'å†å²ç´¯è®¡è§†é¢‘è¢«è¸©æ¬¡æ•°', 'type': 'bigint'}, {'name': 'first_video_dislike_dt_02', 'desc': 'é¦–æ¬¡è§†é¢‘è¢«è¸©æ—¥æœŸ', 'type': 'string'}, {'name': 'latest_video_dislike_dt_02', 'desc': 'æœ€è¿‘1æ¬¡è§†é¢‘è¢«è¸©æ—¥æœŸ', 'type': 'string'}], 'åˆ†åŒº': [{'name': 'day', 'desc': 'æ•°æ®ä¸ŠæŠ¥æ—¥æœŸ', 'type': 'string'}]}}\r\nscore: 0.442986748854532\r\n\r\ntop2: {'ä¸­æ–‡å': 'è§†é¢‘è¢«è¸©çš„æ±‡æ€»äº‹å®è¡¨(likee-æ±‡æ€»-ç¤¾äº¤äº’åŠ¨)', 'è¡¨ä¿¡æ¯': {'db': 'like_dw_sid', 'table': 'dws_like_sid_video_dislike_producer_1d_01', 'desc': 'è§†é¢‘è¢«è¸©çš„æ±‡æ€»äº‹å®è¡¨', 'å­—æ®µ': [{'name': 'video_id', 'desc': 'è§†é¢‘id', 'type': 'bigint'}, {'name': 'video_author_uid', 'desc': 'è§†é¢‘ä½œè€…id', 'type': 'bigint'}, {'name': 'video_author_hdid', 'desc': 'è§†é¢‘ä½œè€…hdid', 'type': 'string'}, {'name': 'video_create_time', 'desc': 'è§†é¢‘åˆ›å»ºæ—¶é—´', 'type': 'bigint'}, {'name': 'country', 'desc': 'å›½å®¶', 'type': 'string'}, {'name': 'os', 'desc': 'æ‰‹æœºæ“ä½œç³»ç»Ÿ', 'type': 'string'}, {'name': 'refer_list', 'desc': 'è§†é¢‘åˆ—è¡¨', 'type': 'string'}, {'name': 'video_dislike_count_1d_02', 'desc': 'æœ€è¿‘1å¤©è§†é¢‘è¢«è¸©æ¬¡æ•°', 'type': 'bigint'}], 'åˆ†åŒº': [{'name': 'day', 'desc': 'æ•°æ®ä¸ŠæŠ¥æ—¥æœŸ', 'type': 'string'}]}}\r\nscore: 0.40496785241401567\r\n\r\ntop3: {'ä¸­æ–‡å': 'å†å²ç´¯è®¡å¿«ç…§äº‹å®è¡¨(likee-æ±‡æ€»-ç¤¾äº¤äº’åŠ¨)', 'è¡¨ä¿¡æ¯': {'db': 'like_dw_sid', 'table': 'dws_like_sid_his_acc_video_share_viewer', 'desc': 'å†å²ç´¯è®¡å¿«ç…§äº‹å®è¡¨', 'å­—æ®µ': [{'name': 'uid', 'desc': 'ç”¨æˆ·id', 'type': 'bigint'}, {'name': 'hdid', 'desc': 'æµ·åº¦id', 'type': 'string'}, {'name': 'his_acc_video_share_send_count_01', 'desc': 'å†å²ç´¯è®¡è§†é¢‘åˆ†äº«æ¬¡æ•°', 'type': 'bigint'}, {'name': 'first_video_share_send_dt_01', 'desc': 'é¦–æ¬¡è§†é¢‘åˆ†äº«æ—¥æœŸ', 'type': 'string'}, {'name': 'latest_video_share_send_dt_01', 'desc': 'æœ€è¿‘ä¸€æ¬¡è§†é¢‘åˆ†äº«æ—¥æœŸ', 'type': 'string'}, {'name': 'his_acc_video_download_count_01', 'desc': 'å†å²ç´¯è®¡è§†é¢‘ä¸‹è½½æ¬¡æ•°', 'type': 'bigint'}, {'name': 'first_video_download_dt_01', 'desc': 'é¦–æ¬¡è§†é¢‘ä¸‹è½½æ—¥æœŸ', 'type': 'string'}, {'name': 'latest_video_download_dt_01', 'desc': 'æœ€è¿‘ä¸€æ¬¡è§†é¢‘ä¸‹è½½æ—¥æœŸ', 'type': 'string'}], 'åˆ†åŒº': [{'name': 'day', 'desc': 'æ•°æ®ä¸ŠæŠ¥æ—¥æœŸ', 'type': 'string'}]}}\r\nscore: 0.3822781247007505\r\n```\r\n\r\nboth data and score is significant differencesï¼Œwhy? Only set similarity_top_k bigger than 16, it return the right value.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14923/comments",
    "author": "sekingme",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-07-24T03:25:47Z",
        "body": "Chroma uses HMSW (an approximately method) to search. Likely some symptom of that?"
      },
      {
        "user": "sekingme",
        "created_at": "2024-07-24T03:33:30Z",
        "body": "> Chroma uses HMSW (an approximately method) to search. Likely some symptom of that?\r\n\r\n@logan-markewich \r\nwhile use milvus dbï¼Œit shows the same symptom."
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-07-24T03:43:48Z",
        "body": "ğŸ¤·ğŸ»  probably the same issue with hsnw? I don't think this issue is related to llama-index. If you used milvus or chroma directly, I would expect a similar behavior "
      },
      {
        "user": "sekingme",
        "created_at": "2024-07-24T03:52:16Z",
        "body": "> ğŸ¤·ğŸ» probably the same issue with hsnw? I don't think this issue is related to llama-index. If you used milvus or chroma directly, I would expect a similar behavior\r\n\r\nIs there any fix way?"
      },
      {
        "user": "sekingme",
        "created_at": "2024-07-25T10:17:30Z",
        "body": "already fix myself,  thanks all."
      }
    ]
  },
  {
    "number": 14713,
    "title": "Can RecursiveRetriever be used with QueryFusionRetriever?",
    "created_at": "2024-07-12T02:37:53Z",
    "closed_at": "2024-07-20T08:57:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14713",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nDATA: Split the document data into multiple \"parent\" chunks, and then further subdivide each block into \"child\" chunks.\r\n\r\nI originally wanted to first build a hybrid recursive search using the RecursiveRetriever retriever and the QueryFusionRetriever retriever together. Enter RecursiveRetriever in QueryFusionRetriever.\r\n\r\nCODE:\r\n`\r\nfrom llama_index.core.retrievers import QueryFusionRetriever\r\n\r\nretriever = QueryFusionRetriever(\r\n    [recursive_retriever1, recursive_retriever2, ...],\r\n    similarity_top_k=2,\r\n    num_queries=4,  # set this to 1 to disable query generation\r\n    use_async=True,\r\n    verbose=True,\r\n    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\r\n)\r\n`\r\n\r\nERROR: ValueError: Query id 23ca7db4-1f10-4236-88c9-b0102081b8b1 not found in either `retriever_dict` or `query_engine_dict`.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14713/comments",
    "author": "hingkan",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-07-12T02:39:23Z",
        "body": "It can, but it seems like something is wrong with one of your recursive retrievers\r\n\r\nI bet you'd get a similar error with `recursive_retriever1.retrieve(\"query\")`"
      },
      {
        "user": "hingkan",
        "created_at": "2024-07-12T03:14:46Z",
        "body": "> It can, but it seems like something is wrong with one of your recursive retrievers\r\n> \r\n> I bet you'd get a similar error with `recursive_retriever1.retrieve(\"query\")`\r\n\r\nSure, and indeed it has already gone wrong in RecursiveRetriever.\r\nI found the problem, RecursiveRetriever must pass the \"node_dict\" argument. \r\nThank you."
      },
      {
        "user": "hingkan",
        "created_at": "2024-07-19T01:39:12Z",
        "body": "RecursiveRetriever in return only the retriever, how to use together with PrevNextNodePostprocessor, for how can achieve the same function as PrevNextNodePostprocessor.\r\n\r\nPrerequisite: RecursiveRetriever is used together with QueryFusionRetriever."
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-07-19T01:44:34Z",
        "body": "Postprocessors are generally used after retrieval. So you run your query fusion retriever. Then run your postprocessor on the results\r\n\r\nIf you want to postprocess individual retrievers inside the query fusion retriever, i would write a custom retriever "
      },
      {
        "user": "hingkan",
        "created_at": "2024-07-19T01:50:46Z",
        "body": "> Postprocessors are generally used after retrieval. So you run your query fusion retriever. Then run your postprocessor on the results\r\n> \r\n> If you want to postprocess individual retrievers inside the query fusion retriever, i would write a custom retriever\r\n\r\nHow does CustomRetriever implement a single retriever postprocess in QueryFusionRetriever."
      }
    ]
  },
  {
    "number": 14684,
    "title": "[Question]: Content not loading after package updates.",
    "created_at": "2024-07-10T18:04:27Z",
    "closed_at": "2024-07-11T19:01:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14684",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI need some help with an issue I'm facing. I was using a set of packages on my local machine, and everything was working fine. But after moving to AWS Cloud9, I had to update those packages due to compatibility issues.\r\n\r\nNow, my problem is that my vectorized content is taking an extremely long time to load. On my local machine, it used to take about 12 minutes, but on Cloud9, I waited for over an hour and a half and it still wasn't done.\r\n\r\nCould this be related to the package updates, or is there something else that could be causing this? Any insights would be appreciated.\r\n\r\nPrevious Packages:\r\nllama-index==0.10.4\r\nllama-index-agent-openai==0.1.1\r\nllama-index-core==0.10.36\r\nllama-index-embeddings-adapter==0.1.0\r\nllama-index-embeddings-openai==0.1.9\r\nllama-index-finetuning==0.1.0\r\nllama-index-legacy==0.9.48\r\nllama-index-llms-gradient==0.1.0\r\nllama-index-llms-openai==0.1.19\r\nllama-index-multi-modal-llms-openai==0.1.1\r\nllama-index-postprocessor-cohere-rerank==0.1.0\r\nllama-index-program-openai==0.1.1\r\nllama-index-question-gen-openai==0.1.1\r\nllama-index-readers-file==0.1.3\r\nllamaindex-py-client==0.1.19\r\n\r\nUpdated Packages:\r\nllama-index==0.10.54\r\nllama-index-agent-openai==0.2.8\r\nllama-index-cli==0.1.12\r\nllama-index-core==0.10.53.post1\r\nllama-index-embeddings-adapter==0.1.3\r\nllama-index-embeddings-openai==0.1.10\r\nllama-index-finetuning==0.1.10\r\nllama-index-indices-managed-llama-cloud==0.2.4\r\nllama-index-legacy==0.9.48\r\nllama-index-llms-gradient==0.1.2\r\nllama-index-llms-mistralai==0.1.17\r\nllama-index-llms-openai==0.1.25\r\nllama-index-multi-modal-llms-openai==0.1.7\r\nllama-index-postprocessor-cohere-rerank==0.1.7\r\nllama-index-program-openai==0.1.6\r\nllama-index-question-gen-openai==0.1.3\r\nllama-index-readers-file==0.1.29\r\nllama-index-readers-llama-parse==0.1.6\r\nllama-parse==0.4.6\r\nllamaindex-py-client==0.1.19\r\n\r\nThe code that is taking forever to load is:\r\n```\r\nstorage_context = StorageContext.from_defaults(persist_dir=\"/home/ubuntu/environment/revised-Project/Sources/Indexed Docs3/azure\")\r\nazure_index = load_index_from_storage(storage_context, show_progress=True)\r\n\r\nstorage_context = StorageContext.from_defaults(persist_dir=\"/home/ubuntu/environment/revised-Project/Sources/Indexed Docs3/assessment\")\r\nassessment_index = load_index_from_storage(storage_context, show_progress=True)\r\n\r\nstorage_context = StorageContext.from_defaults(persist_dir=\"/home/ubuntu/environment/revised-Project/Sources/Indexed Docs3/control\")\r\ncontrol_index = load_index_from_storage(storage_context, show_progress=True)\r\n\r\nstorage_context = StorageContext.from_defaults(persist_dir=\"/home/ubuntu/environment/revised-Project/Sources/Indexed Docs3/questionaire\")\r\nquestionaire_index = load_index_from_storage(storage_context, show_progress=True)\r\n        \r\nstorage_context = StorageContext.from_defaults(persist_dir=\"/home/ubuntu/environment/revised-Project/Sources/Indexed Docs3/AWS Docs\")\r\naws_index = load_index_from_storage(storage_context, show_progress=True)\r\n```\r\nMy local machine is MacOS and the system in Cloud9 is Ubuntu 22.04.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14684/comments",
    "author": "JoseGHdz",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-07-10T18:08:14Z",
        "body": "Could be a difference in CPU/hardware?\r\n\r\nI would suggest not using saving to disk if you have a lot of data. Should be using some actual vector db after a certain point (qdrant, weaviate, pinecone, chroma, etc.)"
      },
      {
        "user": "JoseGHdz",
        "created_at": "2024-07-10T18:13:52Z",
        "body": "> not using saving to disk if you have a lot of\r\n\r\nFor the hardware it could be. My Cloud9 instance is m5.2xlarge (32 GiB RAM + 8 vCPU) meanwhile my Mac is a 10 Core M2 Pro with 16GB of RAM. \r\n\r\nI was setting up a Chroma DB but since I didn't have lots of data I stopped setting it up. But then I started getting lots of data so it might be good for me to finish setting that up. "
      }
    ]
  },
  {
    "number": 14616,
    "title": "[Question]: Set the frequency_penalty when using openailike",
    "created_at": "2024-07-07T20:03:45Z",
    "closed_at": "2024-07-07T20:16:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14616",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHow to set the frequency_penalty and other model parameters when using openailike?\r\n\r\nI am currently setting as below:\r\n`Settings.llm = OpenAILike(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", api_base=openai_api_base, api_key=\"\",\r\n max_tokens=2000, \r\n frequency_penalty=0.8,\r\n presence_penalty=0.5 ,\r\n top_p=0.9,\r\n stop=stop_phrases,\r\n model_kwargs={\r\n    \"frequency_penalty\": 1.0,\r\n  })`\r\n\r\nBut when checking on my vllm server (On a different instance hence using OpenAILike) it shows 0 as frequency_penalty.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14616/comments",
    "author": "mashuk999",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-07-07T20:05:27Z",
        "body": "Set it under additional kwargs \r\n\r\nadditional_kwargs={...}"
      },
      {
        "user": "mashuk999",
        "created_at": "2024-07-07T20:16:05Z",
        "body": "@logan-markewich Many thanks, It works perfectly"
      }
    ]
  },
  {
    "number": 14610,
    "title": "[Question]: Imports no longer working",
    "created_at": "2024-07-07T02:06:46Z",
    "closed_at": "2024-10-15T16:05:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14610",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nI am on llamaindex version 0.10.30. I have a lot of import statements:\r\n\r\n`from llama_index.llms.openai import OpenAI\r\n\r\nfrom llama_index.core.indices.vector_store.base import VectorStoreIndex\r\n\r\nfrom llama_index.core.storage import StorageContext\r\n\r\nfrom llama_index.core.indices import load_index_from_storage\r\n\r\nfrom llama_index.core.service_context import ServiceContext\r\n\r\nfrom llama_index.core.readers import SimpleDirectoryReader, download_loader\r\n\r\nfrom llama_index.core import ChromaVectorStore\r\n\r\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\r\n\r\nfrom llama_index.core.retrievers import VectorIndexRetriever\r\n\r\nfrom llama_index.core.query_engine import RetrieverQueryEngine\r\n\r\nfrom llama_index.core.chat_engine import CondensePlusContextChatEngine\r\n\r\nfrom llama_index.core.node_parser import SentenceSplitter\r\n\r\nfrom llama_index.core.types import ChatMessage\r\n\r\nfrom llama_index.core.vector_stores.types import MetadataFilters\r\n\r\nfrom llama_index.core.vector_stores.types import MetadataFilter\r\n\r\nfrom llama_index.postprocessor.colbert_rerank import ColbertRerank\r\n\r\nfrom llama_index.core import base_query_engine`\r\n\r\n\r\n\r\nThe imports are different than those in examples because for some reason when I do follow the examples, for instance importing the VectorStoreIndex by doing `from llama_index.core import VectorStoreIndex` I get the error: ImportError: cannot import name 'VectorStoreIndex' from 'llama_index.core' (unknown location)\r\n\r\nUsing these import statements were the only way I could make the text not white, and not cause the flagging by the IDE. When I run the code now it says:\r\n from llama_index.core.image_retriever import BaseImageRetriever\r\nModuleNotFoundError: No module named 'llama_index.core.image_retriever'\r\n\r\nI never use the BaseImageRetriever though.\r\n\r\nHow do I go about fixing this?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14610/comments",
    "author": "RonRastorguev",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-07-07T02:11:14Z",
        "body": "Seems like a botched update from v0.9.x\r\n\r\nJust install with a fresh venv, should be fine "
      },
      {
        "user": "KanrongYu",
        "created_at": "2024-07-09T18:51:00Z",
        "body": "had the same issue with you. Though was able to fix some of these, e.g. changing the import to `from llama_index.legacy import VectorStoreIndex`, but this seems non-sustainable. In the end, do what @logan-markewich suggests and no issue at all when running sample code in the new venv"
      }
    ]
  },
  {
    "number": 14574,
    "title": "[Question]: index.docstore is empty after persisting nodes in chromadb",
    "created_at": "2024-07-04T18:43:17Z",
    "closed_at": "2024-07-04T22:32:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14574",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHello,\r\n\r\nI have persisted the nodes in ChromaDB along with the storage context. However, when retrieving the vector index, the index.docstore is empty, how can I get the nodes later to use for BM25Retriever? Here is the code used for persisting and retrieving:\r\n\r\n```python\r\n# node transformation\r\nnode_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\r\n\r\n# collect llama index documents\r\ndocuments = process_documents(df)\r\n\r\n# initialize chroma client, setting path to save data\r\ndb = chromadb.PersistentClient(path=chroma_db_path)\r\n\r\n# create collection\r\nchroma_collection = db.get_or_create_collection(collection_name)\r\n\r\n# assign chroma as the vector_store to the context\r\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\r\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\r\n\r\n# Embedding Model\r\nembed_model = HuggingFaceEmbedding(model_name=hf_model_name, device=hf_device)\r\n\r\n# create your index\r\nindex = VectorStoreIndex.from_documents(\r\n        documents,\r\n        storage_context=storage_context,\r\n        show_progress=True,\r\n        transformations=[node_parser],\r\n        embed_model=embed_model,\r\n)\r\n\r\n# Here we save the index to the path we want\r\nindex.storage_context.persist(persist_dir=os.path.join(chroma_db_path, \"llamai\"))\r\n```\r\n\r\n```python\r\n# initialize chroma client, setting path to save data\r\ndb = chromadb.PersistentClient(path=chroma_db_path)\r\n\r\n# create collection\r\nchroma_collection = db.get_or_create_collection(collection_name)\r\n\r\n# assign chroma as the vector_store to the context\r\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\r\nstorage_context = StorageContext.from_defaults(\r\n      vector_store=vector_store, persist_dir=os.path.join(chroma_db_path, \"llamai\")\r\n)\r\n\r\n# Embedding Model\r\nembed_model = HuggingFaceEmbedding(model_name=hf_model_name, device=hf_device)\r\n\r\n# get the index\r\nindex = VectorStoreIndex.from_vector_store(\r\n      vector_store=vector_store,\r\n      storage_context=storage_context,\r\n      embed_model=embed_model,\r\n)\r\n\r\n# return the index\r\nreturn index\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14574/comments",
    "author": "BalasubramanyamEvani",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-07-04T18:45:40Z",
        "body": "This is correct. The docstore is disabled with most 3rd party vector stores to simplify storage, since the nodes are stored in chroma itself\r\n\r\nYou can override this if you want: `VectorStoreIndex.from_documents(...., store_nodes_override=True)`"
      },
      {
        "user": "BalasubramanyamEvani",
        "created_at": "2024-07-04T19:05:49Z",
        "body": "I understand. Could you please clarify the correct way to use BM25Retriever? Instead of providing the nodes during initialization, I supplied a reference to the docstore, but it resulted in an error.\r\n\r\n```python\r\n  File \"/usr/local/anaconda3/envs/rag-search/lib/python3.9/site-packages/llama_index/retrievers/bm25/base.py\", line 73, in from_defaults\r\n    return cls(\r\n  File \"/usr/local/anaconda3/envs/rag-search/lib/python3.9/site-packages/llama_index/retrievers/bm25/base.py\", line 40, in __init__\r\n    self.bm25 = BM25Okapi(self._corpus)\r\n  File \"/usr/local/anaconda3/envs/rag-search/lib/python3.9/site-packages/rank_bm25.py\", line 83, in __init__\r\n    super().__init__(corpus, tokenizer)\r\n  File \"/usr/local/anaconda3/envs/rag-search/lib/python3.9/site-packages/rank_bm25.py\", line 27, in __init__\r\n    nd = self._initialize(corpus)\r\n  File \"/usr/local/anaconda3/envs/rag-search/lib/python3.9/site-packages/rank_bm25.py\", line 52, in _initialize\r\n    self.avgdl = num_doc / self.corpus_size\r\nZeroDivisionError: division by zero\r\n```"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-07-04T21:39:59Z",
        "body": "You'll need to either manually populate the docstore or use the flag above. And then persist the dcostore somewhere.\r\n\r\nOr, you can directly save the nodes somewhere "
      },
      {
        "user": "BalasubramanyamEvani",
        "created_at": "2024-07-04T22:31:58Z",
        "body": "Got it! Thanks for your help @logan-markewich "
      }
    ]
  },
  {
    "number": 14559,
    "title": "[Question]: Can we split query engine's query ",
    "created_at": "2024-07-04T03:32:11Z",
    "closed_at": "2024-07-04T05:00:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14559",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\n I have a inference pipeline deployed on serverless google cloud function where i am getting stored index from a cloud bucket and then it has alibaba nlp embedding model and llama3 llm .\r\nMy problem is that while doing inference I need to set embedding model in settings.embed_model . My embedding model is hosted on vertex ai , now to fulfil this requirement , i need to again load the model and this adds up in each inference. Is there a way I can just get the embeddings of query and get the response ?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14559/comments",
    "author": "pulkitmehtaworkmetacube",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-07-04T03:34:12Z",
        "body": "I'm not sure what you mean?\r\n\r\nInitializing a vertex ai embedding model should be zero cost. It's just creating a client?\r\n\r\nAnd in any case, you need the embedding model to get the embeddings of the query no?"
      },
      {
        "user": "pulkitmehtaworkmetacube",
        "created_at": "2024-07-04T03:44:27Z",
        "body": "so , embedding model( Alibaba NLP gte-large-en) is deployed on vertex ai . Using vertex ai sdk and initializing client  , we wont get the model object itself which needs to be set up in Settings . So , extra time will be spent in loading the model again . Please let me know if it is clear now .   "
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-07-04T04:41:25Z",
        "body": "So what do you want instead? You still need to get a query embedding somehow ğŸ˜…\r\n\r\n(Ideally you only have to initialize it once, and again, its not setting up anything except an api connection)"
      }
    ]
  },
  {
    "number": 14519,
    "title": "[Question]: Streaming response with metadata",
    "created_at": "2024-07-02T18:27:51Z",
    "closed_at": "2024-07-02T18:39:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14519",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\n\r\ndef get_completion(query: str, namespace: HomeNamespace, home_id: int):\r\n    \"\"\"\r\n    Queries document from <namespace> for a specific property and returns the response and citations.\r\n    Args:\r\n        query (str): The query string.\r\n        namespace (str): The namespace for the Pinecone index.\r\n        home_id (str): The home ID to filter the documents.\r\n    Returns:\r\n        tuple: A tuple containing the response string and a list of citations.\r\n    \"\"\"\r\n    # Initialize Pinecone index\r\n    vector_store = PineconeVectorStore(pinecone_index=get_index(PineconeIndexEnum.HOME), namespace=namespace)\r\n    index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\r\n\r\n    # Configure the re-ranking optimizer\r\n    rerank = SentenceEmbeddingOptimizer(embed_model=Settings.embed_model, percentile_cutoff=0.5, threshold_cutoff=0.85)\r\n\r\n    # Set metadata filters for the query\r\n    filters = MetadataFilters(\r\n        filters=[\r\n            MetadataFilter(key=\"home_id\", operator=FilterOperator.EQ, value=home_id),\r\n        ]\r\n    )\r\n\r\n    # Initialize the citation query engine\r\n    citation_query_engine = CitationQueryEngine.from_args(\r\n        index,\r\n        similarity_top_k=5,\r\n        verbose=True,\r\n        postprocessor=[rerank],\r\n        filters=filters,\r\n        citation_chunk_size=512,\r\n        citation_qa_template=citation_qa_template,\r\n        llm=OpenAI(model=\"gpt-4o-2024-05-13\", api_key=get_secret_value(\"OPENAI_API_KEY\")),\r\n        streaming=True,\r\n    )\r\n\r\n    # Perform the query\r\n    response = citation_query_engine.query(query)\r\n\r\n    # Extract citations and modify the response string\r\n    # citation_indices, response_str = extract_citations_and_modify_string(str(response))\r\n    # citations = [response.source_nodes[i - 1].text for i in citation_indices]\r\n\r\n    for text in response.response_gen:\r\n        yield text\r\n\r\n\r\nwhen I use this function I am only able to get the text of the response but I also want to  access the metadata attributes so that I can also cite my page_number and other metadata",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14519/comments",
    "author": "narenSb1837",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-07-02T18:31:35Z",
        "body": "I think I shared this on discord, by either yield the metadata at the start or end, or attach it to every text that you yield. Its still on the response object\r\n\r\nSo either\r\n\r\n```\r\nyield response.source_nodes # or whatever other metadata\r\nfor text in response.response_gen:\r\n    yield text\r\n```\r\n\r\nor\r\n\r\n```\r\nfor text in response.response_gen:\r\n    yield text\r\nyield response.source_nodes # or whatever other metadata\r\n```\r\n\r\nor\r\n\r\n```\r\nfor text in response.response_gen:\r\n    yield {\"text\": text, \"metadata\": ....}\r\n```"
      },
      {
        "user": "narenSb1837",
        "created_at": "2024-07-02T18:39:27Z",
        "body": "Okk thank you so much got itğŸ‘"
      }
    ]
  },
  {
    "number": 14449,
    "title": "[Question]: kg index embeddings insertion",
    "created_at": "2024-06-28T15:44:03Z",
    "closed_at": "2024-06-28T15:51:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14449",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHello, I am wondering how can I can use embeddings for querying with this kg index in my script, I have used the `include_embeddings=True` and `embedding_mode=\"hybrid\"` and then storing mt created index in a persistent storage. However, when I try to check if the embeddings are created and are present in my index, I run into problems, additionally, within my docstore in my persistent storage I can see the embeddings field being null, making me even more confused about if the embeddings are even being generated. Can anyone help me here?\r\n\r\n```\r\nfrom nebula3.gclient.net import ConnectionPool\r\nfrom nebula3.Config import Config\r\nfrom llama_index.core import (\r\n    VectorStoreIndex,\r\n    SimpleDirectoryReader,\r\n    KnowledgeGraphIndex,\r\n    Settings,\r\n    StorageContext,\r\n    PromptTemplate,\r\n    load_index_from_storage\r\n)\r\nfrom llama_index.core import Document\r\nfrom llama_index.embeddings.openai import OpenAIEmbedding\r\nfrom llama_index.llms.openai import OpenAI\r\nfrom llama_index.graph_stores.nebula import NebulaGraphStore\r\nfrom llama_index.core.query_engine import KnowledgeGraphQueryEngine\r\nfrom llama_index.core.retrievers import KnowledgeGraphRAGRetriever\r\nfrom typing import List\r\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\r\nimport os\r\nimport json\r\nimport base64\r\nimport subprocess\r\n\r\n# Configure OpenAI settings\r\nSettings.llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\r\nembed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\r\nSettings.embed_model = embed_model\r\nSettings.chunk_size = 512\r\n\r\n# Environment variables for NebulaGraph connection\r\nos.environ[\"NEBULA_USER\"] = \"root\"\r\nos.environ[\"NEBULA_PASSWORD\"] = \"nebula\"\r\nos.environ[\"NEBULA_ADDRESS\"] = \"127.0.0.1:9669\"\r\n\r\n# NebulaGraph store configuration\r\nspace_name = \"embtest\"\r\nedge_types, rel_prop_names = [\"relationship\"], [\"relationship\"]\r\ntags = [\"entity\"]\r\n\r\ngraph_store = NebulaGraphStore(\r\n    space_name=space_name,\r\n    edge_types=edge_types,\r\n    rel_prop_names=rel_prop_names,\r\n    tags=tags\r\n)\r\n\r\nstorage_context = StorageContext.from_defaults(graph_store=graph_store)\r\n\r\n# Load documents\r\ndocuments = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\r\n\r\n# Convert document text to lowercase\r\nfor doc in documents:\r\n    doc.text = doc.text.lower()\r\n\r\n# Generate embeddings and create KnowledgeGraphIndex\r\nprint(\"Generating embeddings and creating KnowledgeGraphIndex...\")\r\nkg_index = KnowledgeGraphIndex.from_documents(\r\n    documents,\r\n    storage_context=storage_context,\r\n    max_triplets_per_chunk=10,\r\n    space_name=space_name,\r\n    edge_types=edge_types,\r\n    rel_prop_names=rel_prop_names,\r\n    tags=tags,\r\n    max_knowledge_sequence=15,\r\n    include_embeddings=True,\r\n)\r\n\r\n# Debug: Print out embeddings during the indexing process\r\nfor doc in documents:\r\n    embedding = embed_model.embed(doc.text)\r\n    print(f\"Document ID: {doc.id}\")\r\n    print(f\"Embedding: {embedding[:20]}\")  # Print first 20 elements of the embedding\r\n\r\n# Persist the KnowledgeGraphIndex\r\nkg_index.storage_context.persist(persist_dir='./storage_graph2')\r\nprint(\"KnowledgeGraphIndex created and persisted.\")\r\n\r\n# Load the persisted KnowledgeGraphIndex\r\nprint(\"Loading KnowledgeGraphIndex from persistent storage...\")\r\nkg_index = load_index_from_storage(storage_context=storage_context, persist_dir='./storage_graph2')\r\n\r\n# Print out embeddings from the loaded index\r\nprint(\"Printing embeddings from the loaded index:\")\r\nnodes = kg_index.graph_store.get_nodes()\r\nfor node in nodes:\r\n    if hasattr(node, 'embedding'):\r\n        embedding = node.embedding\r\n        print(f\"Node ID: {node.id}\")\r\n        print(f\"Embedding: {embedding[:20]}\")  # Print first 20 elements of the embedding\r\n\r\nprint(\"Loaded KnowledgeGraphIndex and printed embeddings.\")\r\n\r\n# Set up query engine using the as_query_engine method\r\nquery_engine = kg_index.as_query_engine(\r\n    include_text=True,\r\n    response_mode=\"tree_summarize\",\r\n    embedding_mode=\"hybrid\",\r\n    similarity_top_k=5,\r\n)\r\n\r\n# Execute a sample query\r\nresponse = query_engine.query(\"What is Hacker news\")\r\nprint(response)\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14449/comments",
    "author": "jjoaqu7",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-28T15:45:07Z",
        "body": "The embeddings are in the index store"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-06-28T15:45:52Z",
        "body": "They won't be directly attached to the documents."
      },
      {
        "user": "jjoaqu7",
        "created_at": "2024-06-28T15:51:35Z",
        "body": "Oh I see now! Thanks for the help!"
      }
    ]
  },
  {
    "number": 14324,
    "title": "[Question]: SchemaLLMPathExtractor lacks structure for relationship directions",
    "created_at": "2024-06-23T14:40:07Z",
    "closed_at": "2024-06-26T18:50:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14324",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nHello,\r\n\r\nI'm opening this issue in relation to #14259, and because it's a question on larger generalization of `PropertyGraphIndex` to allow multiple graph stores and the imposition of more structure in the allowed relationships, I figured this would be a better place for it than discord.\r\n\r\nI'm currently trying to integrate KÃ¹zu, a structured property graph store with the new `PropertyGraphIndex`, and the PR #14259 that integrates TiDB seems to have some degree of similarity with my tool in terms of having structured tables as prerequisites.\r\n\r\nThe default arguments for `SchemaLLMPathExtractor` are not very useful and any realistic scenario would require a custom user-defined schema, where the user specifies `possible_entities` and `possible_relations`. Basically, with the following arguments.\r\n\r\n```py\r\nfrom llama_index.core.indices.property_graph import SchemaLLMPathExtractor\r\n\r\nschema_path_extractor = SchemaLLMPathExtractor(\r\n    llm=extraction_llm,\r\n    possible_entities=entities,\r\n    possible_relations=relations,\r\n    kg_validation_schema=validation_schema,\r\n    strict=True,  # if false, will allow triples outside of the schema\r\n)\r\n```\r\n\r\nFrom my perspective, I'm stuck with the lack of structure in the current template schema, as the user only specifies *which* relationships are possible, but what a structured graph store actually requires is that the *direction* of the relationships be known beforehand.\r\n\r\nRather than going too deep into designing my own Pydantic schema to deal with this, I thought I'd chime in here and check in with the team here on whether we can come up with a more general solution for more structured graph stores. The current approach seems too tailored for Neo4j's schema definition style, which is understandably flexible, but a lot of other graph stores (like TiDB and KÃ¹zu, maybe even Nebula) are more structured and it would be far more generalizable if we could provide something like the below during schema definition (so that I can specify the `FROM` and `TO` relationship directions in KÃ¹zu beforehand).\r\n\r\n```py\r\nfrom typing import Literal\r\n\r\nentities = Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\r\nrelations = Literal[\r\n    \"IS_CEO_OF\",\r\n    \"IN_LOCATION\",\r\n    \"MANAGES\",\r\n]\r\n\r\n# Define relationship validation schema as a list of triples (provides direction information)\r\nvalidation_schema = [\r\n    (\"PERSON\", \"IS_CEO_OF\", \"ORGANIZATION\"),\r\n    (\"PERSON\", \"MANAGES\", \"PERSON\"),\r\n    (\"ORGANIZATION\", \"IN_LOCATION\", \"LOCATION\"),\r\n]\r\n```\r\n\r\nLooking forward to learning more from the maintainers, or from others in the community on this. Thanks in advance!\r\n\r\nEDIT: To make it a list of triples per Logan's comment.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14324/comments",
    "author": "prrao87",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-23T14:43:03Z",
        "body": "Yea i agree, but also seems like a pretty easy change to make.\r\n\r\nI think your example is almost there. It probably needs to be a list of dict "
      },
      {
        "user": "prrao87",
        "created_at": "2024-06-23T14:44:47Z",
        "body": "Thanks! I updated the example. But I'm open to other ideas in terms of how to constrain the relationships in the validation schema. Should the key be the relationship name, or should it be the node label (as it is currently) - but with the added ability to specify the `from` and `to` directions?"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-06-23T17:41:24Z",
        "body": "Personally I think having the key as the relationship type make sense ğŸ¤” It could also just be a straight list of triples too tbh, which would be easier to define: `[(LABEL1, REL1, LABEL2), ...]` "
      },
      {
        "user": "prrao87",
        "created_at": "2024-06-23T17:54:58Z",
        "body": "I think I agree too - the relationships are easily interpretable as a list of triples. Even if there are multiple node label pairs with the same kind of relationship, it's still better to explicitly state the triples and avoid nested JSON for readability reasons."
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-06-24T01:54:41Z",
        "body": "I'm on board with this change. Happy to merge if you open a PR. Just need to update any docs + this specific extractor.\r\n\r\nI think just for backwards compatibility sake, we might need to detect the old format. But, not too bad. "
      },
      {
        "user": "prrao87",
        "created_at": "2024-06-24T12:33:30Z",
        "body": "Will get on it and send two PRs - one for the extractor itself and another one for the KÃ¹zu integreation. Will keep you posted. Thanks!"
      },
      {
        "user": "mphipps2",
        "created_at": "2024-06-24T17:54:04Z",
        "body": "But the validation schema itself is only to prune invalid triplets after generation, right? So I think the better more general solution @prrao87 was getting at is to update this schema in the function call itself. \r\n\r\nIf I'm understanding correctly, just updating the validation schema will lead to pretty aggressive pruning and risks missing important relationships (depending how large you set max_triplets_per_chunk)"
      },
      {
        "user": "prrao87",
        "created_at": "2024-06-24T17:59:32Z",
        "body": "> just updating the validation schema will lead to pretty aggressive pruning and risks missing important relationships (depending how large you set max_triplets_per_chunk)\r\n\r\nCould you give an example of how important relationships can be missed? I'm testing this on a variety of cases and it seems to be capturing them all with `max_triplets_per_chunk=10`."
      },
      {
        "user": "mphipps2",
        "created_at": "2024-06-24T18:57:23Z",
        "body": "Maybe some of it is me not having tuned the entities and relationships much, but at the moment I have about 15 different entities with 10 different relations. Without defining strict from/to conditions I end up with things like (\"Person\", \"attended\", \"client\"). Rather than (\"Person\", \"attended\", \"meeting\"). So this is something that would be pruned when what I really want is to have it included but as (\"Person\", \"attended\", \"meeting\")"
      },
      {
        "user": "prrao87",
        "created_at": "2024-06-24T18:59:58Z",
        "body": "@mphipps2 could you take a look at my current progress in #14357? I'm seeing decent results in cases like yours where an invalid or missing relationship is correctly skipped. If there are any edge cases there, please feel free to chime in."
      },
      {
        "user": "prrao87",
        "created_at": "2024-06-24T19:02:03Z",
        "body": "> So this is something that would be pruned when what I really want is to have it included but as (\"Person\", \"attended\", \"meeting\")\r\n\r\nI think this is a case of the LLM extraction and not to do with the schema constraints? In the example `(\"Person\", \"attended\", \"client\")`, that's an invalid triple extracted by the LLM and schema validation can't help you much there."
      },
      {
        "user": "mphipps2",
        "created_at": "2024-06-24T19:06:45Z",
        "body": "That was my point though. Testing with your updates now but ideally we'd be passing the \"from\"/\"to\" constraints directly to the LLM"
      },
      {
        "user": "prrao87",
        "created_at": "2024-06-24T19:08:09Z",
        "body": "I think that's a larger issue outside the scope of what I raised here. But it's worth revisiting for @logan-markewich and any others who may have ideas on this in the future."
      }
    ]
  },
  {
    "number": 14281,
    "title": "[Question]: How do I pass a static parameter when I defined FnComponent ",
    "created_at": "2024-06-20T17:51:39Z",
    "closed_at": "2024-09-26T16:10:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14281",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI would like to pass in a connection to a Function Component like the following \r\n\r\n```\r\ndef parse_sql_guard_component(response: ChatResponse, test_engine: Engine) -> str:\r\n        \"\"\"Parse response to SQL.\"\"\"\r\n        # simple clean SQL\r\n        sSQL = response.text.split('\\n')[0].strip().strip(\"```\").strip().replace('\"', \"'\")\r\n        # connect to a Read Only test DB that cannot be harmed\r\n        with test_engine.connect() as connection:\r\n            try:\r\n                connection.execute(text(sSQL))\r\n                connection.commit()  # commits \"some statement\"\r\n            except Exception as e:\r\n                raise SQLGuardError(\"Unsafe SQL attempt\", [e.message])\r\n        return sSQL\r\n    logging.info(\"Initiating sql_guard_component\")\r\n\r\n# i was thinking something like this \r\nsql_guard_component = FnComponent(fn=parse_sql_guard_component, test_engine=my_test_engine)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14281/comments",
    "author": "yukiman76",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-20T17:53:18Z",
        "body": "@yukiman76 hmm, I think you could use a partial, but I also think a partial would break the schema parsing for the function\r\n\r\nProbably, the fn_component needs a dedicated partials dict ğŸ¤” "
      },
      {
        "user": "yukiman76",
        "created_at": "2024-06-20T18:09:44Z",
        "body": "I was thinking the same, and FnComponent derives from QueryComponent which does have an internal partial support I can do the following temp \r\n\r\n\r\n```\r\ndef parse_sql_guard_component(response: ChatResponse, test_engine: Engine) -> str:\r\n        \"\"\"Parse response to SQL.\"\"\"\r\n        # simple clean SQL\r\n        sSQL = response.text.split('\\n')[0].strip().strip(\"```\").strip().replace('\"', \"'\")\r\n        # connect to a Read Only test DB that cannot be harmed\r\n        with test_engine.connect() as connection:\r\n            try:\r\n                connection.execute(text(sSQL))\r\n                connection.commit()  # commits \"some statement\"\r\n            except Exception as e:\r\n                raise SQLGuardError(\"Unsafe SQL attempt\", [e.message])\r\n        return sSQL\r\n    logging.info(\"Initiating sql_guard_component\")\r\n\r\nsql_guard_component = FnComponent(fn=parse_sql_guard_component,\r\n                                  req_params={\"response\", \"test_engine\"})\r\nsql_guard_component.partial(test_engine=test_engine)\r\n```\r\n\r\ntesting now "
      },
      {
        "user": "yukiman76",
        "created_at": "2024-06-20T19:16:03Z",
        "body": "Looks like that's working, we should think of a better solution tho,  add it to the FnComponent class and make it a first class "
      },
      {
        "user": "yukiman76",
        "created_at": "2024-09-26T16:19:56Z",
        "body": "This did work "
      }
    ]
  },
  {
    "number": 14183,
    "title": "[Question]: is there a way to emit all the chunks and similarity score from llamaindex ?",
    "created_at": "2024-06-16T19:56:01Z",
    "closed_at": "2024-09-29T16:07:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14183",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI wanted to get all the chunks with similarity score using VectorStoreIndex. Is it possible?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14183/comments",
    "author": "vkaul11",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-16T19:58:34Z",
        "body": "Set the top k to 10000 and retrieve?\r\n\r\n```\r\nretriever = index.as_retriever(similarity_top_k=10000)\r\nnodes = retriever.retrieve(\"query\")\r\n```"
      },
      {
        "user": "vkaul11",
        "created_at": "2024-06-21T21:55:03Z",
        "body": "Doesn't work "
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-06-23T22:15:57Z",
        "body": "What do you mean doesn't work lol? You'll have to give more info than that "
      }
    ]
  },
  {
    "number": 14171,
    "title": "[Question]: Big problem on saving and retrieve KnowledgeGraphIndex (Neo4j)",
    "created_at": "2024-06-15T13:49:02Z",
    "closed_at": "2024-06-16T14:16:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14171",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have a big problem, i can't save or retrieve the graph in memory do perform queries on it, so everytime the graph gets recalculated. This is the code that after 2 days Im arrived at, also asking help to chatgpt and reading  docs, but it doesn't work.\r\n\r\nIf anyone know how to do it please help me.\r\n\r\nThank you!\r\n\r\n```\r\nimport os\r\nimport openai\r\nfrom llama_index.llms.azure_openai import AzureOpenAI\r\nfrom llama_index.core import SimpleDirectoryReader, KnowledgeGraphIndex\r\nfrom dotenv import load_dotenv\r\nfrom llama_index.core import Settings\r\nfrom llama_index.graph_stores.neo4j import Neo4jGraphStore\r\nfrom llama_index.embeddings.ollama import OllamaEmbedding\r\nfrom llama_index.core import StorageContext\r\nfrom llama_index.core.indices.loading import load_graph_from_storage\r\nfrom llama_index.core.indices.composability.graph import ComposableGraph\r\n\r\nload_dotenv()\r\nprint(os.getenv('AZURE_OPENAI_LLM_DEPLOYMENT_NAME'))\r\nprint(os.getenv('AZURE_OPENAI_API_ENDPOINT'))\r\nprint(os.getenv('AZURE_OPENAI_API_KEY'))\r\nprint(os.getenv('AZURE_OPENAI_API_VERSION'))\r\n\r\nllm = AzureOpenAI(\r\n    engine=os.getenv('AZURE_OPENAI_LLM_DEPLOYMENT_NAME'),\r\n    model=\"gpt-4o\",\r\n    temperature=0.0,\r\n    azure_endpoint=os.getenv('AZURE_OPENAI_API_ENDPOINT'),\r\n    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\r\n    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\r\n)\r\n\r\nembed_model = OllamaEmbedding(model_name=\"mxbai-embed-large:335m\", embed_batch_size=512)\r\n\r\nSettings.llm = llm\r\nSettings.embed_model = embed_model\r\nSettings.chunk_size = 512\r\n\r\nusername = \"neo4j\"\r\ndatabase = \"neo4j\"\r\npassword = \"xxx\"\r\nurl = \"bolt://localhost:7687\"\r\nprint(username, password, url, database)\r\n\r\ngraph_store = Neo4jGraphStore(\r\n    username=username,\r\n    password=password,\r\n    url=url,\r\n    database=database,\r\n)\r\n\r\n# Directory for the serialized graph\r\nstorage_dir = './storage'\r\nos.makedirs(storage_dir, exist_ok=True)\r\n\r\n# Define a consistent root ID\r\nroot_id = 'knowledge_graph_index'\r\n\r\n# Check if storage context files exist\r\ndocstore_path = os.path.join(storage_dir, 'docstore.json')\r\nindex_store_path = os.path.join(storage_dir, 'index_store.json')\r\ngraph_store_path = os.path.join(storage_dir, 'graph_store.json')\r\n\r\n# Attempt to load the graph\r\ntry:\r\n    if os.path.exists(docstore_path) and os.path.exists(index_store_path) and os.path.exists(graph_store_path):\r\n        storage_context = StorageContext.from_defaults(graph_store=graph_store, persist_dir=storage_dir)\r\n        knowledge_graph_index = load_graph_from_storage(storage_context, root_id=root_id)\r\n        print(\"Loaded graph from storage.\")\r\n        print(f\"Root ID: {root_id}\")\r\n    else:\r\n        raise FileNotFoundError(\"Required storage files not found, creating new graph.\")\r\nexcept Exception as e:\r\n    print(f\"Failed to load graph from storage: {e}\")\r\n    # Graph doesn't exist, so create it from documents\r\n    documents = SimpleDirectoryReader(\"./content/Documents\").load_data()\r\n    storage_context = StorageContext.from_defaults(graph_store=graph_store)\r\n\r\n    # NOTE: can take a while!\r\n    knowledge_graph_index = KnowledgeGraphIndex.from_documents(\r\n        documents,\r\n        storage_context=storage_context,\r\n        max_triplets_per_chunk=3,\r\n        show_progress=True,\r\n        include_embeddings=True,\r\n    )\r\n    # Set the root ID and save the newly created graph\r\n    knowledge_graph_index.set_index_id(root_id)\r\n    \r\n    storage_context.persist(persist_dir=storage_dir)\r\n    print(f\"Persisted graph in directory: {storage_dir}\")\r\n\r\n# Verify that the graph is correctly loaded\r\ntry:\r\n    if knowledge_graph_index is None:\r\n        raise ValueError(\"Failed to create or load KnowledgeGraphIndex.\")\r\n    print(\"Successfully created or loaded KnowledgeGraphIndex.\")\r\n    \r\n    # Check the contents of all_indices\r\n    print(f\"Contents of all_indices: {knowledge_graph_index.all_indices}\")\r\n    # Check the root_id\r\n    print(f\"Root ID set in graph: {knowledge_graph_index._root_id}\")\r\n\r\n    # Verify the root_id is in all_indices\r\n    if knowledge_graph_index.root_id not in knowledge_graph_index.all_indices:\r\n        raise KeyError(f\"The specified root_id '{knowledge_graph_index.index_id()}' was not found in the graph indices.\")\r\n    print(\"The root ID was found in the graph indices.\")\r\nexcept Exception as e:\r\n    print(f\"Error verifying the KnowledgeGraphIndex: {e}\")\r\n\r\n# Now, whether loaded or created, you can use `knowledge_graph_index` as before\r\ntry:\r\n    query_engine = knowledge_graph_index.as_query_engine(\r\n        include_text=True,\r\n        response_mode=\"tree_summarize\",\r\n        embedding_mode=\"hybrid\",\r\n        similarity_top_k=5,\r\n    )\r\n    response = query_engine.query(\"Quali sono le chiese disegnate da Raffaello?\")\r\n    print(response)\r\nexcept KeyError as e:\r\n    print(f\"KeyError: {e} - The specified root_id '{root_id}' was not found in the graph indices.\")\r\nexcept Exception as e:\r\n    print(f\"An error occurred while creating the query engine: {e}\")\r\n    \r\n    \r\n    \r\n\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14171/comments",
    "author": "robertobalestri",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-15T13:53:34Z",
        "body": "Is there any error? Does this line every print?\r\n\r\n`print(f\"Failed to load graph from storage: {e}\")` ?"
      },
      {
        "user": "robertobalestri",
        "created_at": "2024-06-15T13:58:02Z",
        "body": "This is my output... but it desn't print your string. \r\n\r\nneo4j xxx bolt://localhost:7687 neo4j\r\nFailed to load graph from storage: Required storage files not found, creating new graph.\r\nParsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 124.90it/s]\r\nGenerating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.36s/it] \r\nGenerating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.22s/it] \r\nGenerating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.20s/it] \r\nGenerating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.21s/it] \r\nGenerating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.22s/it] \r\nGenerating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.23s/it] \r\nProcessing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:02<00:00, 10.37s/it] \r\nPersisted graph in directory: ./storage\r\nSuccessfully created or loaded KnowledgeGraphIndex.\r\nError verifying the KnowledgeGraphIndex: 'KnowledgeGraphIndex' object has no attribute 'all_indices'\r\nUna delle chiese disegnate da Raffaello Ã¨ S. Eligio degli Orefici.\r\n\r\n\r\n\r\n"
      },
      {
        "user": "robertobalestri",
        "created_at": "2024-06-15T14:19:11Z",
        "body": "Ok, after days tryng i fount out that the load_graph doesn't work, but load index does.\r\n\r\n\r\n```\r\nimport os\r\nfrom llama_index.llms.azure_openai import AzureOpenAI\r\nfrom llama_index.core import SimpleDirectoryReader, KnowledgeGraphIndex, Settings, StorageContext\r\nfrom llama_index.graph_stores.neo4j import Neo4jGraphStore\r\nfrom llama_index.embeddings.ollama import OllamaEmbedding\r\nfrom llama_index.core.indices.loading import load_index_from_storage\r\nfrom dotenv import load_dotenv\r\n\r\n# Load environment variables from .env file\r\nload_dotenv()\r\n\r\n# Configure Azure OpenAI\r\nllm = AzureOpenAI(\r\n    engine=os.getenv('AZURE_OPENAI_LLM_DEPLOYMENT_NAME'),\r\n    model=\"gpt-4o\",\r\n    temperature=0.0,\r\n    azure_endpoint=os.getenv('AZURE_OPENAI_API_ENDPOINT'),\r\n    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\r\n    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\r\n)\r\n\r\n# Configure the embedding model\r\nembed_model = OllamaEmbedding(model_name=\"mxbai-embed-large:335m\", embed_batch_size=512)\r\n\r\n# Set configuration parameters\r\nSettings.llm = llm\r\nSettings.embed_model = embed_model\r\nSettings.chunk_size = 512\r\n\r\n# Configure the Neo4j database connection\r\nusername = \"neo4j\"\r\ndatabase = \"neo4j\"\r\npassword = \"password\"\r\nurl = \"bolt://localhost:7687\"\r\n\r\ngraph_store = Neo4jGraphStore(\r\n    username=username,\r\n    password=password,\r\n    url=url,\r\n    database=database,\r\n)\r\n\r\n# Directory for storage\r\nstorage_dir = './storage'\r\nos.makedirs(storage_dir, exist_ok=True)\r\n\r\n# Consistent root ID\r\nroot_id = 'knowledge_graph_index'\r\n\r\n# Load or create the knowledge graph\r\ntry:\r\n    storage_context = StorageContext.from_defaults(graph_store=graph_store, persist_dir=storage_dir)\r\n    knowledge_graph_index = load_index_from_storage(storage_context, index_id=root_id)\r\n    print(\"Graph loaded from storage.\")\r\nexcept Exception as e:\r\n    print(f\"Failed to load graph from storage: {e}\")\r\n    # Create the graph from documents if it doesn't exist\r\n    documents = SimpleDirectoryReader(\"./content/Documents\").load_data()\r\n    storage_context = StorageContext.from_defaults(graph_store=graph_store)\r\n    knowledge_graph_index = KnowledgeGraphIndex.from_documents(\r\n        documents,\r\n        storage_context=storage_context,\r\n        max_triplets_per_chunk=3,\r\n        show_progress=True,\r\n        include_embeddings=True,\r\n    )\r\n    knowledge_graph_index.set_index_id(root_id)\r\n    storage_context.persist(persist_dir=storage_dir)\r\n    print(f\"Graph created and stored in: {storage_dir}\")\r\n\r\n# Verify the graph is loaded correctly\r\ntry:\r\n    if knowledge_graph_index is None:\r\n        raise ValueError(\"Failed to create or load KnowledgeGraphIndex.\")\r\n    print(\"KnowledgeGraphIndex created or loaded successfully.\")\r\n    root_id_set = knowledge_graph_index.index_id\r\n    if root_id_set != root_id:\r\n        raise KeyError(f\"The specified root ID '{root_id}' does not match the loaded root ID '{root_id_set}'.\")\r\n    print(\"The root ID matches and is correct.\")\r\nexcept Exception as e:\r\n    print(f\"Error verifying the KnowledgeGraphIndex: {e}\")\r\n\r\n# Use `knowledge_graph_index` for queries\r\ntry:\r\n    query_engine = knowledge_graph_index.as_query_engine(\r\n        include_text=True,\r\n        response_mode=\"tree_summarize\",\r\n        embedding_mode=\"hybrid\",\r\n        similarity_top_k=5,\r\n    )\r\n    response = query_engine.query(\"Quali chiese ha disegnato Raffaello?\")\r\n    print(response)\r\nexcept KeyError as e:\r\n    print(f\"KeyError: {e} - The specified root_id '{root_id}' was not found in the graph indices.\")\r\nexcept Exception as e:\r\n    print(f\"An error occurred while creating the query engine: {e}\")\r\n\r\n```"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-06-16T14:16:31Z",
        "body": "@robertobalestri ah good catch `load_graph` is an old method for something completely unrelated actually.\r\n\r\nLoad index is the one to use, I didn't notice that in your code "
      }
    ]
  },
  {
    "number": 14124,
    "title": "[Question]: How big of a VectorStore is too big ? ",
    "created_at": "2024-06-13T14:53:53Z",
    "closed_at": "2024-06-21T12:57:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14124",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have generated a vector store for a series of documents using the `llama_index.core` VectorIndexStore class. The total size of the final vector store directory is about **540 mb**, with a `default__vector_store.json` file of about **511 mb**. \r\n\r\nMy question is, is my **vector store too big** ? I am under the impression that the vector store takes an eternity to **load** (about 240 seconds) and each node **deletion** or **update** takes about 200 seconds. \r\n\r\nThe use case in which I want to use this VectorStore needs me to update it on a regular basis but it just takes too long to do. Is this a normal time for processing a Vector Store of this size or am I proceeding wrong when deleting or adding documents to it ? \r\n\r\nThanks for your responses, if you need any more details to answer my question feel free to ask ! \r\n\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14124/comments",
    "author": "Reda-LemonLearning",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-13T15:47:41Z",
        "body": "The default vector store is mostly for quick POC or getting started. Use a real vector store for scale (qdrant, chroma, weaviate, pinecone, etc.)"
      }
    ]
  },
  {
    "number": 14096,
    "title": "[Question]: Retriever search depth in a custom node relationship environment ",
    "created_at": "2024-06-12T05:26:14Z",
    "closed_at": "2024-09-20T16:08:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14096",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHello, I use llama-index to configure custom nodes and create parent-child correlations between them.\r\n\r\nHowever, it was confirmed that only the depth immediately above and below the node retrieved when as_query_engine was referenced.\r\nI want to increase the reference range because I have nodes that have very short top-down relationships.\r\n\r\nIs there a way to adjust the options related to this?\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14096/comments",
    "author": "whoo9112",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-12T05:34:42Z",
        "body": "What index are you using? What retriever?"
      },
      {
        "user": "whoo9112",
        "created_at": "2024-06-12T05:36:21Z",
        "body": "> What index are you using? What retriever?\r\n\r\ni'm using index = VectorStoreIndex , and  index.as_query_engine()"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-06-12T20:12:06Z",
        "body": "The default vector store index and query engine does not follow or use the node relationships ğŸ‘€ you'd have to introduce a node postprocessor to use them "
      },
      {
        "user": "whoo9112",
        "created_at": "2024-06-14T04:36:55Z",
        "body": "1. \r\n\r\n> The default vector store index and query engine does not follow or use the node relationships ğŸ‘€ you'd have to introduce a node postprocessor to use them\r\n\r\nSo what method should I use to refer to relocationship when retrever? Is there an example? I'm thinking about faiss, chroma, etc. as vector db right now.\r\n"
      }
    ]
  },
  {
    "number": 14082,
    "title": "[Question]: nested agents ",
    "created_at": "2024-06-11T18:44:32Z",
    "closed_at": "2024-09-17T16:09:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14082",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nDoes llamaindex has nested agents like autogen",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14082/comments",
    "author": "gamercoder153",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-11T18:46:59Z",
        "body": "You can wrap any agent as a tool for another agent, using FunctionTool "
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-06-11T18:47:26Z",
        "body": "(Which conveniently wraps any function, like a function that calls an agent)"
      }
    ]
  },
  {
    "number": 14071,
    "title": "Agent enters infinite loop , even max_iteration=3 wont work",
    "created_at": "2024-06-11T07:14:30Z",
    "closed_at": "2024-06-11T15:37:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14071",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nfrom langchain_experimental.agents.agent_toolkits import create_csv_agent\r\nimport streamlit as st\r\nimport os\r\nimport tempfile\r\n\r\nfrom langchain_community.llms import CTranslate2\r\nmodel_id = 'NHL2-13b-chat-Llama2-ct2'\r\ntokenizer_id = 'Nous-Hermes-Llama2-13b'\r\nllm=CTranslate2(model_path=model_id,tokenizer_name=tokenizer_id,device=\"auto\",compute_type=\"int8\r\n\r\ndef main():\r\n    # Configure Streamlit page\r\n    st.set_page_config(page_title=\"Ask your CSV\")\r\n    st.header(\"Ask your CSV\")\r\n\r\n    # Allow the user to upload a CSV file\r\n    file = st.file_uploader(\"upload file\", type=\"csv\")\r\n\r\n    if file is not None:\r\n        # Create a temporary file to store the uploaded CSV data\r\n        with tempfile.NamedTemporaryFile(mode='w+', suffix=\".csv\", delete=False) as f:\r\n            # Convert bytes to a string before writing to the file\r\n            data_str = file.getvalue().decode('utf-8')\r\n            f.write(data_str)\r\n            f.flush()\r\n            llm =llm\r\n\r\n            # Ask the user to input a question\r\n            user_input = st.text_input(\"Question here:\")\r\n\r\n            # Create a CSV agent using the OpenAI language model and the temporary file\r\n            agent = create_csv_agent(llm, f.name, verbose=True,agent_executor_kwargs={\"handle_parsing_error\":True} )\r\n\r\n            if user_input:\r\n                # Run the agent on the user's question and get the response\r\n                response = agent.run(user_input)\r\n                st.write(response)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\nWhen I execute this code , I can upload the file and ask a question , it outputs the result in terminal and then continues to loop back , \r\nI have tried max_iteration=2 -- this runs 2 times giving output 2 times in terminal and ends with error like Agent terminated because it reached max iterations\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14071/comments",
    "author": "nikhitaKanoj",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-11T15:37:06Z",
        "body": "@nikhitaKanoj this is a question about langchain, but this is the llamaindex repo. Better to ask in a langchain location :) "
      }
    ]
  },
  {
    "number": 14028,
    "title": "[Question]:  Is it expected that `VectorStoreIndex.persist` and `load_index_from_storage` are not symmetric?",
    "created_at": "2024-06-08T22:18:12Z",
    "closed_at": "2024-06-08T22:40:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/14028",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIs it expected that persisting (serializing) a `VectorStoreIndex` and then loading (deserializing) it is not symmetric?\r\n\r\nIn the code snippet below, `loaded_vector_store_index` is a `BaseIndex[Unknown]` while `vector_store_index` is a `VectorStoreIndex`. These classes have different behaviors.\r\n\r\nFor example, creating a query engine or retriever from each will have very different results. The ones coming from `VectorStoreIndex` having much better results.\r\n\r\n```python\r\ndocuments = [...]\r\nnodes = markdown_parser.get_nodes_from_documents(documents)\r\nvector_store_index = VectorStoreIndex(nodes=nodes)\r\nvector_store_index.storage_context.persist(persist_dir=\"/tmp/vector_store_index\")\r\n\r\nembed_model = OpenAIEmbedding(api_key=os.environ[\"OPENAI_API_KEY\"], model=\"text-embedding-3-small\")\r\nstorage_context = StorageContext.from_defaults(persist_dir=\"/tmp/vector_store_index\")\r\nloaded_vector_store_index = load_index_from_storage(\r\n    storage_context=storage_context,\r\n    embed_model=embed_model,\r\n)\r\n```\r\n\r\nI spent a lot of time today figuring this one out. I was seeing good results from the `vector_store_index` object in my ingester process, while my API process which was loading the result of ingestion into `loaded_vector_store_index` was showing really poor results.\r\n\r\nTo make it work, I'm manually creating a `VectorStoreIndex` from the `BaseIndex[Unknown]` in the API process:\r\n\r\n```python3\r\nnodes = loaded_vector_store_index.docstore.docs.values()\r\nactual_loaded_vector_store_index = VectorStoreIndex(nodes=list(nodes))\r\n```\r\n\r\nQuestions:\r\n1. Is there a better way of doing this?\r\n2. Am I missing something obvious?\r\n3. Should `persist`/`load_index_from_storage` be symmetric?\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/14028/comments",
    "author": "mpereira",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-08T22:26:27Z",
        "body": "@mpereira The one thing I noticed, is that when creating the initial index, you do not set an embedding model, but then when loading it, you do. I suspect if you updated your code like this, it would be fine\r\n\r\n```python\r\ndocuments = [...]\r\nnodes = markdown_parser.get_nodes_from_documents(documents)\r\n\r\n# use the same embed model for both\r\nembed_model =  OpenAIEmbedding(api_key=os.environ[\"OPENAI_API_KEY\"], model=\"text-embedding-3-small\")\r\n\r\nvector_store_index = VectorStoreIndex(nodes=nodes, embed_model=embed_model)\r\nvector_store_index.storage_context.persist(persist_dir=\"/tmp/vector_store_index\")\r\n\r\nstorage_context = StorageContext.from_defaults(persist_dir=\"/tmp/vector_store_index\")\r\nloaded_vector_store_index = load_index_from_storage(\r\n    storage_context=storage_context,\r\n    embed_model=embed_model,\r\n)\r\n```\r\n\r\nFor a longer explanation on typing:\r\n`load_index_from_storage` can return any index (a vector store index, property graph index, tree index, etc.) -- it works for all of them.\r\n\r\nIt knows what index to load because the index structure contains what type of index it is.\r\n\r\nBecause of how python typing works, `load_index_from_storage` has to have the return type of `BaseIndex` -- that's just how it is. And it is symmetrical, but you need to provide the proper embedding model that matches how the index was built.\r\n\r\n"
      },
      {
        "user": "mpereira",
        "created_at": "2024-06-08T22:37:36Z",
        "body": "Hey @logan-markewich, thanks for the super quick reply.\r\n\r\nI just tested it here. Your suggestion works. Thank you!\r\n\r\nFeel free to close this."
      }
    ]
  },
  {
    "number": 13986,
    "title": "[Question]: Generate Only SQL Query",
    "created_at": "2024-06-06T16:45:20Z",
    "closed_at": "2024-06-07T11:30:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/13986",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI am using the NLSQLTableQueryEngine to generate SQL queries from text as described in the official documentation. However I don't want NLSQLTableQueryEngine to execute the query directly on my DB. I want it to only generate the SQL statements so that I can screen it and run it my self. \r\n\r\nI tried the `sql_only` parameter provided in the docs but it didn't seem to have effect. How can i acheive this\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/13986/comments",
    "author": "Omotade-MY",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-06T16:50:31Z",
        "body": "I think it should be `sql_only=True, synthesize_response=False` in the constructor args"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-06-06T16:51:00Z",
        "body": "I didn't look closer why there's two params, but the above should do it"
      },
      {
        "user": "Omotade-MY",
        "created_at": "2024-06-07T11:30:47Z",
        "body": "It worked. Thank you.\r\n\r\nThe issues were from the tables I passed. "
      }
    ]
  },
  {
    "number": 13982,
    "title": "[Question]: sent-len of sentence-spliter",
    "created_at": "2024-06-06T14:28:40Z",
    "closed_at": "2024-06-06T15:12:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/13982",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nthere is one arg of SentenceSplitter: chunk_size, i assume this could control the length of each split-sentence, but i found that sentence-length can be larger than the chunk_size, i want to know why this happens\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/13982/comments",
    "author": "guangyuli-uoe",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-06T15:12:51Z",
        "body": "@guangyuli-uoe the sentence splitter splits into chunks, while trying to respect sentence boundaries. It does not split and return single sentences. "
      },
      {
        "user": "guangyuli-uoe",
        "created_at": "2024-06-06T15:26:36Z",
        "body": "@logan-markewich thanks !"
      }
    ]
  },
  {
    "number": 13965,
    "title": "[Question]: Excessive content returned by the Chat engine, How can I get it to not match so much?",
    "created_at": "2024-06-06T02:44:33Z",
    "closed_at": "2024-09-12T16:08:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/13965",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nchat_engine = index.as_chat_engine(\r\n        chat_mode='condense_plus_context', use_async=True, system_prompt=SYSTEM_PROMPT,\r\n    )\r\nThe returned content is too long, how can I make it return only the top1 or 2 matches?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/13965/comments",
    "author": "yuyu990116",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-06T03:08:11Z",
        "body": "It's already the top 2 by default? ğŸ‘€"
      },
      {
        "user": "yuyu990116",
        "created_at": "2024-06-06T03:17:22Z",
        "body": "> It's already the top 2 by default? ğŸ‘€\r\n\r\nbut through verbose=True, I saw 8 matched results (followed file_path:  / / )"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-06-06T03:30:56Z",
        "body": "Ah. I guess I assumed you used a vector index, but what kind of index did you create?"
      },
      {
        "user": "yuyu990116",
        "created_at": "2024-06-06T03:36:01Z",
        "body": "> Ah. I guess I assumed you used a vector index, but what kind of index did you create?\r\n\r\n\r\nDocumentSummaryIndex\r\n\r\nand this is how I used it:\r\nstorage_context = StorageContext.from_defaults(persist_dir=persist_dir)\r\nindex = load_index_from_storage(storage_context)"
      },
      {
        "user": "yuyu990116",
        "created_at": "2024-06-06T03:38:34Z",
        "body": "> > Ah. I guess I assumed you used a vector index, but what kind of index did you create?\r\n> \r\n> DocumentSummaryIndex\r\n> \r\n> and this is how I used it: storage_context = StorageContext.from_defaults(persist_dir=persist_dir) index = load_index_from_storage(storage_context)\r\n\r\nThis is how I created the index:\r\n    splitter = SentenceSplitter(chunk_size=1024)\r\n    response_synthesizer = get_response_synthesizer(\r\n        response_mode=\"tree_summarize\", use_async=True\r\n    )\r\n    doc_summary_index = DocumentSummaryIndex.from_documents(\r\n        docs,\r\n        llm=OurLLM(), #customed local model\r\n        transformations=[splitter],\r\n        response_synthesizer=response_synthesizer,\r\n        show_progress=True,\r\n    )\r\n    doc_summary_index.storage_context.persist(persist_dir)"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-06-06T16:30:18Z",
        "body": "@yuyu990116 the document summary index works by generating summaries of documents, and using those to decide which documents to send to the LLM\r\n\r\nThis will send ALL nodes associated with the chosen documents to the LLM. \r\n\r\nThere isn't really a way to limit the number of nodes sent with this index, since its pulling all nodes from the selected document"
      }
    ]
  },
  {
    "number": 13924,
    "title": "[Question]: Potential Mode Error in from_persist_path Method in SimpleKVStore",
    "created_at": "2024-06-04T01:44:07Z",
    "closed_at": "2024-10-11T16:06:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/13924",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIn the from_persist_path method of SimpleKVStore, the file is opened in binary mode (\"rb\") but json.load() is used to load the data. Typically, json.load() expects a file opened in text mode (\"r\").\r\n\r\nWhy is the file opened in binary mode (\"rb\") instead of text mode (\"r\") for json.load()?\r\n\r\n```python\r\n    def persist(\r\n        self, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\r\n    ) -> None:\r\n        \"\"\"Persist the store.\"\"\"\r\n        fs = fs or fsspec.filesystem(\"file\")\r\n        dirpath = os.path.dirname(persist_path)\r\n        if not fs.exists(dirpath):\r\n            fs.makedirs(dirpath)\r\n\r\n        with fs.open(persist_path, \"w\") as f:\r\n            f.write(json.dumps(self._data))\r\n\r\n    @classmethod\r\n    def from_persist_path(\r\n        cls, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\r\n    ) -> \"SimpleKVStore\":\r\n        \"\"\"Load a SimpleKVStore from a persist path and filesystem.\"\"\"\r\n        fs = fs or fsspec.filesystem(\"file\")\r\n        logger.debug(f\"Loading {__name__} from {persist_path}.\")\r\n        with fs.open(persist_path, \"rb\") as f:\r\n            data = json.load(f)\r\n        return cls(data)\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/13924/comments",
    "author": "yewentao256",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-04T02:06:10Z",
        "body": "Not sure, but it works, since all our simple stores save and load to disk just fine ğŸ˜…"
      },
      {
        "user": "yewentao256",
        "created_at": "2024-06-05T01:32:19Z",
        "body": "The file is opened in binary mode (\"rb\") because JSON support is designed to handle str, bytes, and bytearray types. However, this introduces additional decoding steps. How about switching to text mode (\"r\")? I suppose it can simplify the process by directly handling strings, potentially improving performance and readability without compromising functionality.\r\n\r\n```python\r\n# lib/python3.11/json/__init__.py\r\ndef loads(s, *, cls=None, object_hook=None, parse_float=None,\r\n        parse_int=None, parse_constant=None, object_pairs_hook=None, **kw):\r\n    if isinstance(s, str):\r\n        if s.startswith('\\ufeff'):\r\n            raise JSONDecodeError(\"Unexpected UTF-8 BOM (decode using utf-8-sig)\",\r\n                                  s, 0)\r\n    else:\r\n        if not isinstance(s, (bytes, bytearray)):\r\n            raise TypeError(f'the JSON object must be str, bytes or bytearray, '\r\n                            f'not {s.__class__.__name__}')\r\n        s = s.decode(detect_encoding(s), 'surrogatepass')\r\n```"
      },
      {
        "user": "yewentao256",
        "created_at": "2024-07-05T01:23:22Z",
        "body": "Please CC @logan-markewich "
      }
    ]
  },
  {
    "number": 13922,
    "title": "[Question]: Is there a threshold when we use knowledge graph index to extract entities",
    "created_at": "2024-06-03T21:36:53Z",
    "closed_at": "2024-09-09T16:04:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/13922",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIs there a threshold when we use knowledge graph index to extract entities?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/13922/comments",
    "author": "DataNoob0723",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-03T21:37:52Z",
        "body": "You might have to give me info here\r\n\r\nWhat code/modules are you using? Do you mean a prediction threshold? "
      },
      {
        "user": "DataNoob0723",
        "created_at": "2024-06-03T21:41:58Z",
        "body": "> You might have to give me info here\r\n> \r\n> What code/modules are you using? Do you mean a prediction threshold?\r\n\r\nI am using KnowledgeGraphIndex, I am wondering is there any threhold this functions uses when extracting entities for the knowledge graph. "
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-06-03T22:08:35Z",
        "body": "its using the LLM to extract, so there is no threshold. Just an LLM outputting text that is then parsed"
      }
    ]
  },
  {
    "number": 13907,
    "title": "[Question]: Could we add OpenSearch to the DocStore and IndexStore catalog?",
    "created_at": "2024-06-03T09:22:00Z",
    "closed_at": "2024-09-09T16:04:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/13907",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nAnalogous to MongoDB, I see OpenSearch as a potential DB for storing and querying nodes. Llama-index already supports OpenSearch as VectorStore and thus save node information (text, metadata, etc.). So, why not use it as docstore or indexstore?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/13907/comments",
    "author": "kobiche",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-06-03T15:17:02Z",
        "body": "Sure, someone just needs to make a contribution for it :)"
      }
    ]
  },
  {
    "number": 13563,
    "title": "[Question]: FunctionCallingAgentWorker with functionary Local LLM",
    "created_at": "2024-05-17T17:33:31Z",
    "closed_at": "2024-05-17T17:37:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/13563",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nWhat is the best way to get FunctionCallingAgentWorker to work with a local LLM (with native function calling) like functionary?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/13563/comments",
    "author": "pseudorandomrants",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-05-17T17:37:07Z",
        "body": "An LLM needs to implement the `FunctionCallingLLM` class -- currently only openai, anthropic, TGI, and mistral implement this\r\n\r\nThere would need to be an LLM integration with functionary for example that implements this class"
      }
    ]
  },
  {
    "number": 13502,
    "title": "When will gpt-4o be supported?",
    "created_at": "2024-05-15T03:01:45Z",
    "closed_at": "2024-05-15T03:42:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/13502",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nWhen will gpt-4o be supported?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/13502/comments",
    "author": "cxycxm",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-05-15T03:02:04Z",
        "body": "<!-- Greeting -->\nHey @cxycxm, nice to see you back in the matrix of issues! Diving into the GPT-4o query, hang tight.\n\n\n"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-05-15T03:18:54Z",
        "body": "It already is. Since day 0\r\n\r\n`pip install -U llama-index-llms-openai`\r\n\r\n`llm = OpenAI(model=\"gpt-4o\")`"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-05-15T03:21:24Z",
        "body": "Install the latest openai package as above. If you are running in a notebook, you'll have to restart it as well "
      },
      {
        "user": "cxycxm",
        "created_at": "2024-05-15T03:26:35Z",
        "body": "thanks"
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2024-05-15T03:29:01Z",
        "body": "<!-- Answer -->\nYou're welcome! If you have any more questions or need further assistance, feel free to ask.\n\n\n<!-- Dosu Comment Footer -->\n<details>\n<summary>\n\n#### Details\n</summary>\n\n\n\n**About Dosu**\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n</details>\n\n*To continue the conversation, mention @dosu.*\n"
      }
    ]
  },
  {
    "number": 13221,
    "title": "[Question]: is it possible use function + Context Retriever both combine (ContextRetrieverOpenAIAgent)",
    "created_at": "2024-05-02T13:52:33Z",
    "closed_at": "2024-05-02T14:01:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/13221",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nOur requirement is to use openAI function Calling API along with ContextChatEngine for make more better responses of users queries.\r\n\r\nSo We use JS for Write Function is there any ways for pass direct Schema in FunctionTool \r\n\r\nI want to execute Function if query is relavent to it otherwise normal Retriever flow of Question/Answer will execuet",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/13221/comments",
    "author": "avipaghadar-maruti",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-05-02T13:53:00Z",
        "body": "<!-- Greeting -->\nHey there, @avipaghadar-maruti! Great to see you back. ğŸ˜Š I remember our last interaction. Let me dive into your question and I'll get back to you with an answer shortly.\n\n\n"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-05-02T14:01:36Z",
        "body": "You've opened this issue twice."
      }
    ]
  },
  {
    "number": 13167,
    "title": "[Question]: Agents-function calling with followup question",
    "created_at": "2024-04-30T04:40:09Z",
    "closed_at": "2024-08-11T16:07:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/13167",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI am trying to build a framework where Agents can call specific functions whenever necessary, but how do I add a functionality for it to ask follow up questions when it doesn't have all the parameters needed to call a function.\r\n\r\nExample: Calling an email sending function using Agent Framework but the to_send address is missing in the query, in such situation, I want my agent to ask a followup question or call any additional function, instead it just ends up sending to some example email address.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/13167/comments",
    "author": "risos8200",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-04-30T16:49:16Z",
        "body": "@risos8200 You could add a tool that specifically requires input from a user when it runs\r\n\r\n```\r\ndef ask_user(message: str) -> str:\r\n  \"\"\"Useful for asking a user for any required information that you are missing.\"\"\"\r\n  response = input(message).strip()\r\n  return response\r\n  \r\ntool = FunctionTool.from_defaults(ask_user)\r\n```"
      },
      {
        "user": "risos8200",
        "created_at": "2024-05-03T00:56:30Z",
        "body": "Thanks, that worked!"
      },
      {
        "user": "risos8200",
        "created_at": "2024-05-05T08:54:03Z",
        "body": "How do I do it for web based applications? Like I want to return a response to the agent to ask for further details in my flask app. But it goes into a infinite loop of asking itself the same question and calling the ask_user function.\r\n\r\n@logan-markewich can you please help out?"
      }
    ]
  },
  {
    "number": 12917,
    "title": "[Question]:How do I customize the search workflow",
    "created_at": "2024-04-18T08:57:02Z",
    "closed_at": "2024-07-25T16:09:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/12917",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have used llama_index to build vector database and neo4j knowledge graph based on local knowledge base, and the query effect is very good. \r\nBut when it comes to querying, the response is slow. In the whole process, openai parses the original problem to obtain the data (A), and then the A data is sent to the vector library and neo4j for query. The query context is summarized and processed to (B) and fed into openai. \r\nNow I want to analyze and **debug A and B**. And **how to directly feed it into llama_index if you already have an A or B**. \r\nIs there an expert who can help me? Thank you",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/12917/comments",
    "author": "dtsgx126",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-04-18T08:57:19Z",
        "body": "<!-- Greeting -->\nHey there, @dtsgx126! ğŸ‘‹ I'm here to lend a hand while you're navigating through the cosmos of LlamaIndex. Think of me as your friendly neighborhood bot, ready to tackle bugs, answer your burning questions, and help you contribute more effectively. While we wait for a human maintainer to join us, I'll be diving into your issue to provide you with a detailed response soon. Stay tuned!\n\n\n"
      },
      {
        "user": "dtsgx126",
        "created_at": "2024-04-18T09:03:44Z",
        "body": "**my parameters and how I use llama_index are as follows:**\r\ngraph_store = Neo4jGraphStore(\r\n    username=username,\r\n    password=password,\r\n    url=url,\r\n    database=database\r\n)\r\n\r\nstorage_context = StorageContext.from_defaults(graph_store=graph_store, persist_dir=\"./neo4j_storage\")\r\n\r\nindex = load_index_from_storage(\r\n    storage_context=storage_context,\r\n    max_triplets_per_chunk=3,\r\n    include_embeddings=True,\r\n    verbose=True,\r\n)\r\n\r\nquery_engine = index.as_query_engine(\r\n    include_text=True,\r\n    response_mode=\"tree_summarize\",\r\n    embedding_mode=\"hybrid\",\r\n    similarity_top_k=10,\r\n    explore_global_knowledge=True\r\n)\r\nquery_str = \"some quesitions\"\r\nresponse = query_engine.query(query_str)\r\n"
      }
    ]
  },
  {
    "number": 12689,
    "title": "[Question]: How to save a text node and then load it up again?",
    "created_at": "2024-04-10T05:00:14Z",
    "closed_at": "2024-04-10T05:19:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/12689",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have text nodes in the format - \r\nTextNode(id_='node_0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='97a68807-c87d-4332-b23e-833aa75d204c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='f54321c2afdcd2bd45d2b9c8324fcc6d4d6d75c78b07be6c35679b66efd0aa38'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6d3af5ce-1039-4542-bc34-3d9f697ac160', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='47b028b14677a7f280b425bae9a305f91526c8652123e2d26ea38c48c70be0bf')}, text=\"blah blah blah.\\n\\n\", start_char_idx=0, end_char_idx=5867, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')\r\nHow can i save this and then use this in another file?\r\n\r\nPlease help, thanks",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/12689/comments",
    "author": "JINO-ROHIT",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-04-10T05:00:42Z",
        "body": "<!-- Greeting -->\nHey @JINO-ROHIT, great to see you back! ğŸ˜Š It's like dÃ©jÃ  vu, but in the best possible way. I'm diving into your text node conundrum as we speak. Hang tight!\n\n\n"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-04-10T05:01:05Z",
        "body": "@JINO-ROHIT \r\n\r\n```\r\njson_str = node.json()\r\n\r\nnode = TextNode.parse_raw(json_str)\r\n```"
      },
      {
        "user": "JINO-ROHIT",
        "created_at": "2024-04-10T05:05:00Z",
        "body": "@logan-markewich thanks, and for a list of text nodes? can i save it as some file name and then read it into another py script"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-04-10T05:06:38Z",
        "body": "Just make a list of json node strings, and read/write to a file ğŸ‘ğŸ» \r\n\r\n```\r\nfor json_str in json_strs\r\n  f.write(json_str + \"\\n\")\r\n```"
      },
      {
        "user": "JINO-ROHIT",
        "created_at": "2024-04-10T05:07:12Z",
        "body": "thanks so much!"
      }
    ]
  },
  {
    "number": 12396,
    "title": "[Question]: deploy llamaindex as a server and has some error.",
    "created_at": "2024-03-29T02:13:50Z",
    "closed_at": "2024-07-05T16:04:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/12396",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nI deploy llamaindex as a webserver and it will handle requests from user. the code is below\r\n\r\n```\r\nfrom gevent import monkey\r\nmonkey.patch_all()\r\n\r\nfrom llama_index.core import  SimpleDirectoryReader\r\nimport sys\r\n\r\n\"\"\"### LLM\r\n\r\nThis should run on a T4 instance on the free tier\r\n\"\"\"\r\n\r\nimport torch\r\n# from transformers import BitsAndBytesConfig\r\n\r\nfrom llama_index.llms.huggingface import HuggingFaceLLM\r\nfrom llama_index.core.prompts import PromptTemplate\r\n\r\nfrom llama_index.core import Settings\r\n\r\nfrom llama_index.core import VectorStoreIndex\r\nfrom llama_index.core import  SimpleDirectoryReader\r\nfrom llama_index.core.response.notebook_utils import display_response\r\n\r\nimport llama_index.core \r\nimport time \r\nimport os\r\nimport gevent\r\nimport json\r\nfrom typing import Dict\r\nimport sys\r\nfrom flask import Flask, request\r\napp = Flask(__name__)\r\nimport torch.cuda.nvtx as nvtx\r\n\r\nllm = HuggingFaceLLM(\r\n    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\r\n    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\r\n    query_wrapper_prompt=PromptTemplate(\"<s>[INST] {query_str} [/INST] </s>\\n\"),\r\n    context_window=3900,\r\n    max_new_tokens=256,\r\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\r\n    # tokenizer_kwargs={},\r\n    generate_kwargs={\"temperature\": 0.2, \"top_k\": 5, \"top_p\": 0.95},\r\n    device_map=\"auto\",\r\n)\r\n\r\nSettings.llm = llm\r\n\r\n# recordMemory(\"after embedding \")\r\n\r\nllama2_paper_path = \"./data/llama2_paper/llama2_paper.json\"\r\n\r\n\r\n\r\nimport json\r\norigin_queries = []\r\nwith open(llama2_paper_path) as f:\r\n    data = json.load(f)\r\n    i  = 0\r\n    for key in data['examples']:\r\n        if i < 20:\r\n            origin_queries.append(key['query'])\r\n            i = i + 1\r\n\r\nquery_gen_str = \"\"\"\\\r\nYou are a helpful assistant that generates multiple search queries based on a \\\r\nsingle input query. Generate {num_queries} search queries, one on each line, \\\r\nrelated to the following input query:\r\nQuery: {query}\r\nQueries:\r\n\"\"\"\r\nquery_gen_prompt = PromptTemplate(query_gen_str)\r\n\r\nembeddings = \"local:BAAI/bge-small-en-v1.5\"\r\n\r\nSettings.embed_model = embeddings\r\n\r\ndef generate_queries(query: str, llm, num_queries: int = 4):\r\n    start = time.time()\r\n    response = llm.predict(\r\n        query_gen_prompt, num_queries=num_queries, query=query\r\n    )\r\n    end = time.time()\r\n    print(f\"query write duration:{end - start}\")\r\n    query_rewrite_duration = end - start    \r\n    # assume LLM proper put each query on a newline\r\n    queries = response.split(\"\\n\")\r\n    queries_str = \"\\n\".join(queries)\r\n    print(f\"Generated queries:\\n{queries_str}\")\r\n    return queries, query_rewrite_duration\r\n    \r\n\r\ndocuments = SimpleDirectoryReader(\"./data/llama2_paper/\").load_data() #data/llmama2_paper.json ./data/survery/llm_survery_paper.json\r\nindex = VectorStoreIndex.from_documents(documents=documents)\r\nquery_engine = index.as_query_engine()\r\n\r\n#warm up \r\ntmp_query = origin_queries[0]\r\nfor k in range(2):\r\n        Settings.llm.complete(tmp_query)\r\n\r\ndef run_query(j):\r\n    all_time = [] \r\n    query = origin_queries[j]\r\n    start = time.time()\r\n    queries,quert_rewrite_time = generate_queries(query,Settings.llm)\r\n    end = time.time()\r\n    query_rewrite_duration = end - start\r\n    all_time.append(query_rewrite_duration)\r\n    for query1 in queries:\r\n        start = time.time()\r\n        # print(f\"type(query1):{type(query1)} and type(query_engine):{type(query_engine)}\")\r\n        response = query_engine.query(query1)\r\n        end = time.time()\r\n        query_time = end - start\r\n        all_time.append(query_time)\r\n        all_time.append(len(query1))\r\n        print(f\"query+generate  duration:{end - start}, query1:{query1}\")\r\n        display_response(response)\r\n    return sum(all_time)\r\n\r\n\r\n\r\n@app.route('/run',  methods = ['POST'])\r\ndef run():\r\n    data = request.get_json(force=True, silent=True)\r\n    index = data['index']\r\n    latency = run_query(index)\r\n    print(f\"index:{index}, latency:{latency}\")\r\n    return json.dumps({'status': 'ok', 'latency': latency})\r\n\r\n\r\n\r\nGET_NODE_INFO_INTERVAL = 0.1\r\n\r\n\r\nfrom gevent.pywsgi import WSGIServer\r\nimport logging\r\nif __name__ == '__main__':\r\n    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%H:%M:%S', level='INFO')\r\n    ip = \"127.0.0.1\"\r\n    port = \"8080\"\r\n    server = WSGIServer((ip, port), app)\r\n    server.serve_forever()\r\n    gevent.spawn_later(GET_NODE_INFO_INTERVAL)\r\n```\r\n\r\nmy error is \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/uw_llama_index/query_rewrite/query_throughput.py\", line 4, in <module>\r\n    from llama_index.core import  SimpleDirectoryReader\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index/core/__init__.py\", line 19, in <module>\r\n    from llama_index.core.indices import (\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index/core/indices/__init__.py\", line 4, in <module>\r\n    from llama_index.core.indices.composability.graph import ComposableGraph\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index/core/indices/composability/__init__.py\", line 4, in <module>\r\n    from llama_index.core.indices.composability.graph import ComposableGraph\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index/core/indices/composability/graph.py\", line 7, in <module>\r\n    from llama_index.core.indices.base import BaseIndex\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index/core/indices/base.py\", line 12, in <module>\r\n    from llama_index.core.ingestion import run_transformations\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index/core/ingestion/__init__.py\", line 2, in <module>\r\n    from llama_index.core.ingestion.pipeline import (\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index/core/ingestion/pipeline.py\", line 15, in <module>\r\n    from llama_index_client import (\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index_client/__init__.py\", line 3, in <module>\r\n    from .types import (\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index_client/types/__init__.py\", line 3, in <module>\r\n    from .api_key import ApiKey\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index_client/types/api_key.py\", line 6, in <module>\r\n    from ..core.datetime_utils import serialize_datetime\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index_client/core/__init__.py\", line 4, in <module>\r\n    from .client_wrapper import AsyncClientWrapper, BaseClientWrapper, SyncClientWrapper\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index_client/core/client_wrapper.py\", line 5, in <module>\r\n    import httpx\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/httpx/__init__.py\", line 2, in <module>\r\n    from ._api import delete, get, head, options, patch, post, put, request, stream\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/httpx/_api.py\", line 6, in <module>\r\n    from ._client import Client\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/httpx/_client.py\", line 32, in <module>\r\n    from ._transports.default import AsyncHTTPTransport, HTTPTransport\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/httpx/_transports/default.py\", line 32, in <module>\r\n    import httpcore\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/httpcore/__init__.py\", line 1, in <module>\r\n    from ._api import request, stream\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/httpcore/_api.py\", line 5, in <module>\r\n    from ._sync.connection_pool import ConnectionPool\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/httpcore/_sync/__init__.py\", line 1, in <module>\r\n    from .connection import HTTPConnection\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/httpcore/_sync/connection.py\", line 12, in <module>\r\n    from .._synchronization import Lock\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/httpcore/_synchronization.py\", line 11, in <module>\r\n    import trio\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/trio/__init__.py\", line 23, in <module>\r\n    from ._core import TASK_STATUS_IGNORED as TASK_STATUS_IGNORED  # isort: split\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/trio/_core/__init__.py\", line 21, in <module>\r\n    from ._local import RunVar, RunVarToken\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/trio/_core/_local.py\", line 9, in <module>\r\n    from . import _run\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/trio/_core/_run.py\", line 2809, in <module>\r\n    from ._io_epoll import (\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/trio/_core/_io_epoll.py\", line 202, in <module>\r\n    class EpollIOManager:\r\n  File \"/home/ubuntu/anaconda3/envs/llama_index/lib/python3.10/site-packages/trio/_core/_io_epoll.py\", line 203, in EpollIOManager\r\n    _epoll: select.epoll = attrs.Factory(select.epoll)\r\nAttributeError: module 'select' has no attribute 'epoll'. Did you mean: 'poll'?\r\n\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/12396/comments",
    "author": "lambda7xx",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-03-29T02:14:06Z",
        "body": "<!-- Answer -->\nHey @lambda7xx, after reviewing the error and the code you've shared, it seems like the issue is related to the `select.epoll` call in the `trio` library, which is not available on your system. This typically happens when you're running your code on an operating system that does not support `epoll`, such as Windows. `epoll` is specific to Linux-based systems.\n\nHere are a few steps you can take to resolve this issue:\n\n1. **Check Your Operating System**: Ensure that you are running your code on a Linux-based system since `epoll` is not available on Windows. If you are on Windows, consider using WSL (Windows Subsystem for Linux) to run your code.\n\n2. **Environment Compatibility**: Make sure that your Python environment is correctly set up for Linux-based development if you're running on Linux or WSL. This includes having the right versions of `trio`, `httpx`, and other dependencies that are compatible with Linux.\n\n3. **Alternative Libraries**: If you are bound to use Windows without WSL, you might need to look into alternative asynchronous libraries that are compatible with Windows. For instance, `asyncio` might be a more suitable choice for cross-platform asynchronous I/O operations.\n\n4. **Update Your Dependencies**: Sometimes, such issues can be resolved by updating your dependencies to the latest versions. Libraries like `trio` and `httpx` receive updates that might include fixes for compatibility issues.\n\n5. **Consult Documentation**: Both `trio` and `httpx` have extensive documentation. Reviewing them might provide specific instructions or configurations needed for your operating system.\n\nIf you're already on a Linux system or using WSL and still encountering this issue, it might be helpful to provide more details about your setup, such as the Linux distribution and version, Python version, and versions of the `trio` and `httpx` libraries. This information can help in diagnosing the problem more accurately.\n\nHope this helps! If you have any more questions or need further assistance, feel free to ask.\n\n\n<!-- Dosu Comment Footer -->\n<details>\n<summary>\n\n#### About Dosu\n</summary>\n\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n\nIf you want to continue the conversation, start your reply with @dosu-bot.\n</details>\n"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-03-29T15:50:32Z",
        "body": "@lambda7xx I think this is an error in your httpx version? What version are you using?"
      }
    ]
  },
  {
    "number": 12250,
    "title": "[Question]: can I use IngestionPipeline with TextNode objects?  ",
    "created_at": "2024-03-25T17:42:52Z",
    "closed_at": "2024-03-25T17:43:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/12250",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nCan I use IngestionPipeline with TextNode objects instead of documents? Thanks!\r\n\r\nimport asyncio\r\n\r\nloop = asyncio.get_event_loop()\r\n%timeit loop.run_until_complete(pipeline.arun(documents=documents, num_workers=4))",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/12250/comments",
    "author": "haolxx",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-03-25T17:43:05Z",
        "body": "<!-- Greeting -->\nHey there, @haolxx! Great to see you back with another intriguing question. ğŸ˜Š I'm diving into your query about integrating IngestionPipeline with TextNode objects right now and will circle back with a detailed response shortly!\n\n\n"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-03-25T17:43:30Z",
        "body": "`pipeline.run(nodes=nodes, ...)`"
      }
    ]
  },
  {
    "number": 12082,
    "title": "[Question]: LLM Caching",
    "created_at": "2024-03-19T13:51:06Z",
    "closed_at": "2024-03-19T13:59:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/12082",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nDoes llama-index offer LLM Caching via exact match or semantic matching?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/12082/comments",
    "author": "gich2009",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-03-19T13:51:20Z",
        "body": "<!-- Greeting -->\nHey there, @gich2009! Nice to see you around again. I'm diving into your question about LLM Caching in LlamaIndex and will get back to you with a detailed answer shortly. Hang tight!\n\n\n"
      },
      {
        "user": "gich2009",
        "created_at": "2024-03-19T13:51:40Z",
        "body": "I think this issue is related to #10645"
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2024-03-19T13:58:03Z",
        "body": "<!-- Answer -->\nThank you for your question regarding LLM Caching in LlamaIndex. I'm sorry, but I was unable to find an answer within the repository. To better assist you, could you please provide more details about your use case? Specifically, any error messages you're encountering, the version of LlamaIndex you're using, and the specific LLM applications you're working with would be helpful. Also, if you could elaborate on how you believe this issue might be related to issue #10645, that would be beneficial.\n\n\n<!-- Dosu Comment Footer -->\n<details open>\n<summary>\n\n#### About Dosu\n</summary>\n\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n\nIf you want to continue the conversation, start your reply with @dosu-bot.\n</details>"
      }
    ]
  },
  {
    "number": 11950,
    "title": "[Question]: VectorStoreIndex from pre-defined embeddings uses API calls?",
    "created_at": "2024-03-14T16:54:13Z",
    "closed_at": "2024-06-21T16:37:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/11950",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI am confused why initializing VectorStoreIndex from pre-defined embeddings costs API calls? That is: I have created my text embeddings manually and I feed the embeddings as nodes to the VectorStoreIndex. Why am I being charged here?: \r\n```\r\nnodes = [TextNode(text=self.texts[i], id_=str(i), embeddings = self.embeddings[i]) for i in indices]\r\nindex = VectorStoreIndex(nodes)\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/11950/comments",
    "author": "MaartenCVonk",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-03-14T16:55:51Z",
        "body": "I think I pointed this out in another issue, but you have a typo, should be `embedding` without the `s`\r\n\r\n```\r\nnodes = [TextNode(text=self.texts[i], id_=str(i), embedding= self.embeddings[i]) for i in indices]\r\nprint(nodes[0].embedding)\r\nindex = VectorStoreIndex(nodes=nodes)\r\n```"
      }
    ]
  },
  {
    "number": 11608,
    "title": "[Question]: ReActOutputParser not  support chinese ",
    "created_at": "2024-03-04T11:32:35Z",
    "closed_at": "2024-06-10T16:04:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/11608",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nat output_parser.py  18 line number \r\n r\"\\s*Thought: (.*?)\\nAction: ([a-zA-Z0-9_]+).*?\\nAction Input: .*?(\\{.*\\})\"\r\nchange code\r\n r\"\\s*Thought: (.*?)\\nAction: ([a-zA-Z0-9_\\u4e00-\\u9fa5]+).*?\\nAction Input: .*?(\\{.*\\})\"",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/11608/comments",
    "author": "492580195",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-03-04T11:32:49Z",
        "body": "<!-- Answer -->\nIt's great to see your interest in improving the ReActOutputParser to support Chinese characters. Your proposed code change looks like a step in the right direction. I encourage you to go ahead and create a pull request with your proposed fix. Your contribution will be valuable to the project!\n\n\n<!-- Dosu Comment Footer -->\n<details open>\n<summary>\n\n#### About Dosu\n</summary>\n\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n\nIf you want to continue the conversation, start your reply with @dosu-bot.\n</details>"
      }
    ]
  },
  {
    "number": 11521,
    "title": "E5-Large Llama Index embeddings don't match Langchain",
    "created_at": "2024-02-29T23:04:08Z",
    "closed_at": "2024-03-01T18:43:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/11521",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nCan you help me understand why this doesn't tie out? I see that the embeddings are normalized by default in LlamaIndex's implementation and have passed the argument when creating the Langchain object. \r\n\r\n```\r\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\r\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\n\r\nembedding_func_li = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\"#, max_length=512\r\n)\r\n\r\nembedding_func_lc = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\", encode_kwargs={\"normalize_embeddings\": True})\r\n\r\n\r\ntext_to_embed = \"The Nasdaq notched its first record close since 2021. The tech-heavy index rose 0.9% to 16091.92, as enthusiasm about artificial intelligence has helped lift technology shares.\"\r\n\r\nprint(embedding_func_li.get_text_embedding(text_to_embed))\r\nprint(embedding_func_lc.embed_query(text_to_embed))\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/11521/comments",
    "author": "airwindk",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-02-29T23:08:01Z",
        "body": "I thiiiiiiink E5 requires some special pooling that got added recently\r\n\r\n`HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\", pooling=\"last\")`"
      },
      {
        "user": "airwindk",
        "created_at": "2024-02-29T23:17:02Z",
        "body": "Mean pooling seemed to do the trick. Thank you! These seem to tie out. Appreciate the quick response here!\r\n\r\nAnd not at all on your take. I needed a model that performed decently on medium / longer contexts, which the smaller sentence models seem to struggle with. If you have any other suggestions on open source embedding models let me know.\r\n\r\n```from llama_index.embeddings.huggingface import HuggingFaceEmbedding\r\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\n\r\nembedding_func_li = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\", pooling=\"mean\")\r\nembedding_func_lc = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\", encode_kwargs={\"normalize_embeddings\": True})\r\n```"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-03-01T18:43:13Z",
        "body": "I know nomic has some recent embedding models that work with long context.\r\n\r\nAnyways, glad it works!\r\n"
      }
    ]
  },
  {
    "number": 11419,
    "title": "[Question]: How to visulize tree (index)",
    "created_at": "2024-02-27T05:41:23Z",
    "closed_at": "2024-02-27T15:12:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/11419",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHi, after building the TreeIndex, is there any way to visualize it please? Any help is appreciated!",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/11419/comments",
    "author": "DataNoob0723",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-02-27T05:41:38Z",
        "body": "_ğŸ¤–_\n<!-- Answer -->\nTo assist you better with visualizing the TreeIndex, could you please provide more details? Specifically, it would be helpful to know the programming language you're using, the version of LlamaIndex you're working with, and any specific requirements or constraints you have for the visualization. This information will help me guide you more accurately.\n\n\n<!-- Dosu Comment Footer -->\n<details open>\n<summary>\n\n#### About Dosu\n</summary>\n\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n\nIf you want to continue the conversation, start your reply with @dosu-bot.\n</details>"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-02-27T15:12:17Z",
        "body": "Sorry, nothing available to visualize it right now. I would encourage a PR if you are interested. The tree index has seen very few contributions to it."
      },
      {
        "user": "DataNoob0723",
        "created_at": "2024-02-27T15:46:35Z",
        "body": "> Sorry, nothing available to visualize it right now. I would encourage a PR if you are interested. The tree index has seen very few contributions to it.\r\n\r\nThanks a lot for the reply! BTW, is there a way to tune the parameters how the trees are built please?"
      }
    ]
  },
  {
    "number": 11380,
    "title": "[Question]: include_text parameter in index.as_query_engine method",
    "created_at": "2024-02-26T03:35:04Z",
    "closed_at": "2024-02-26T03:37:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/11380",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHi, I don't quite understand the include_text parameter in index.as_query_engine method. What's the difference between when it is set to be True or False please? Thanks a lot!",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/11380/comments",
    "author": "DataNoob0723",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-02-26T03:35:20Z",
        "body": "_ğŸ¤–_\n<!-- Greeting -->\nHey there, @DataNoob0723! Great to see you diving into more features of LlamaIndex. I remember our last interaction - always a pleasure to assist you. I'm taking a look at your question about the `include_text` parameter right now and will get back to you shortly with a detailed explanation. Hang tight!\n\n\n"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-02-26T03:37:49Z",
        "body": "This is specific to knowledge graph indexes.\r\n\r\nIf it's true, the chunk where a matching triplet is found will also be sent to the LLM\r\n\r\nIf false, then only the matching triplets are sent to the LLM"
      },
      {
        "user": "DataNoob0723",
        "created_at": "2024-02-26T03:39:57Z",
        "body": "> This is specific to knowledge graph indexes.\r\n> \r\n> If it's true, the chunk where a matching triplet is found will also be sent to the LLM\r\n> \r\n> If false, then only the matching triplets are sent to the LLM\r\n\r\nThanks a lot!"
      }
    ]
  },
  {
    "number": 10923,
    "title": "[Question]: Do you foresee VERSION under llama_index.core ",
    "created_at": "2024-02-17T17:28:18Z",
    "closed_at": "2024-02-18T01:33:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/10923",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nAt  llama_index.legacy  you have \r\nwith open(Path(__file__).absolute().parents[0] / \"VERSION\") as _f:\r\n    __version__ = _f.read().strip()\r\n\r\nUnder llama_index.core there is no VERSION do you have any option to ad similar \r\nllama_index.legacy.__version__  with llama_index.core.__version__",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/10923/comments",
    "author": "Khan-Tzar",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-02-17T17:38:58Z",
        "body": "Probably not the version file, but we can just add it to the core `__init__`"
      },
      {
        "user": "Khan-Tzar",
        "created_at": "2024-02-17T21:53:42Z",
        "body": "OK add it in core __init__"
      }
    ]
  },
  {
    "number": 10919,
    "title": "[Question]: How to get vector from Node without checking the databases?",
    "created_at": "2024-02-17T16:32:41Z",
    "closed_at": "2024-02-17T16:55:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/10919",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHi2,\r\n\r\nI'm writing a simple unit test to use our custom embedding capability, how to get the vector embedding of a node in the database?\r\n\r\nI try to use the ` index.docstore.get_node(node_id)`, but the node doesn't seem to have any embedding, even if I can clearly see them on the databse\r\n\r\n```python\r\ndocuments = [\r\n    Document(\r\n        id=\"1\",\r\n        text=\"Foo Bar\",\r\n    ),\r\n    Document(\r\n        id=\"2\",\r\n        text=\"AI World\",\r\n    ),\r\n]\r\n\r\nfor document in documents:\r\n    index.insert(document)\r\n\r\nall_docs = index.docstore.get_all_ref_doc_info()\r\nindex.storage_context.persist(persist_dir=\"data\")\r\n\r\nfor doc_id in all_docs:\r\n    doc = all_docs[doc_id]\r\n\r\n    node = index.docstore.get_node(doc.node_ids[0])\r\n    print(node.id_)\r\n    print(node.text)\r\n    print(node.embedding)\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/10919/comments",
    "author": "rendyfebry",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-02-17T16:35:09Z",
        "body": "The embedding is stored in the vector store ğŸ‘€ \r\n\r\nIf you are using the base simple vector store, you can do\r\n\r\n`embedding = index.vector_store.get(node_id)`"
      },
      {
        "user": "rendyfebry",
        "created_at": "2024-02-17T16:45:32Z",
        "body": "Ahh, thanks @logan-markewich \r\n\r\nThe base VectorStore class itself doesn't have `get()` function, so it didn't shown in my editor.\r\n\r\nThank you"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-02-17T16:55:08Z",
        "body": "Yea, it's the only one with that method ğŸ˜… been meaning to add to the base class, but it's some work to then go and add to the children classes.\r\n\r\nGlad it worked!"
      }
    ]
  },
  {
    "number": 10752,
    "title": "[Question]: error occurred",
    "created_at": "2024-02-15T14:51:28Z",
    "closed_at": "2024-05-31T23:40:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/10752",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nFailed to load file data\\image.png with error: name 'partition_image' is not defined. Skipping...",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/10752/comments",
    "author": "sarthak20cse",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-02-15T14:57:48Z",
        "body": "@sarthak20cse you are going to have to give a few more details. What were you running? Can you give some sample code?"
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2024-05-16T16:02:41Z",
        "body": "<!-- Answer -->\nHi, @sarthak20cse,\n\nI'm helping the LlamaIndex team manage their backlog and am marking this issue as stale. It looks like you reported a \"name 'partition_image' is not defined\" error while attempting to load a file. There hasn't been a resolution yet, and @logan-markewich has requested more details, including the code being run.\n\nCould you please confirm if this issue is still relevant to the latest version of the LlamaIndex repository? If it is, please let the LlamaIndex team know by commenting on the issue. Otherwise, feel free to close the issue yourself, or it will be automatically closed in 7 days. Thank you!\n\n\n"
      }
    ]
  },
  {
    "number": 10176,
    "title": "[Question]: memory in ContextChatEngine isn't working",
    "created_at": "2024-01-21T19:31:58Z",
    "closed_at": "2024-01-22T15:32:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/10176",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have defined ContextChatEngine with memory as follows\r\n```\r\nchat_store = SimpleChatStore()\r\nmemory = ChatMemoryBuffer.from_defaults(token_limit=3000, chat_store=chat_store,)\r\nchat_engine = ContextChatEngine.from_defaults(hybrid_retriever, service_context=service_context, memory=memory, node_postprocessors=[reranker])\r\n```\r\n```\r\nq = \"Tell me about recommendation engine\" \r\nresp = self.chat_engine.stream_chat(q) # this query works\r\n```\r\nHowever, when I refer to `recommendation engine` as it, the `chat_engine` can't figure that out\r\n```\r\nq = \"Tell me about it\" \r\nresp = self.chat_engine.stream_chat(q) # it can't refer to it as recommendation engine\r\n```\r\n\r\nHow to make this work?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/10176/comments",
    "author": "cocoza4",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-01-21T19:32:30Z",
        "body": "_ğŸ¤–_\n<!-- Greeting -->\nHey @cocoza4! Good to see you again. I see you've run into some trouble with the memory in ContextChatEngine. Let me dive into this issue for you. I'll be back with a detailed solution shortly.\n\n\n"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-01-21T23:32:13Z",
        "body": "If the retrieved nodes are bigger than the token limit, there won't be room for the chat history. Not sure how many nodes you are returning or their size "
      },
      {
        "user": "cocoza4",
        "created_at": "2024-01-22T15:32:02Z",
        "body": "@logan-markewich thanks for your response, it turns out to be a bug in my code."
      }
    ]
  },
  {
    "number": 10074,
    "title": "[Question]: Why Service_context is not used in metadata_extractors",
    "created_at": "2024-01-16T16:43:52Z",
    "closed_at": "2024-01-16T16:47:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/10074",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI was working with some of the metadata_extractors using LangChainLLM in my project. When calling the `KeywordExtractor` api without the **llm** argument throws OpenAI api-key error, even though i `set_global_service_context` with my LangChainLLM, the KeywordExtractor class doesn't look on the `service_context` when the **llm** argument is not passed and it uses `resolve_llm('default')` function to get the OpenAI model as the llm. \r\n\r\nMy Question is while some Classes in llama_index uses `service_context` as the class argument and fetches the llm from it, why does the metadata_extractors and other classes are not using it, can it be still modified to use `ServiceContext.from_defaults().llm ` instead of `resolve_llm(\"default\")`.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/10074/comments",
    "author": "Kirushikesh",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2024-01-16T16:47:13Z",
        "body": "service context is confusing, I know. \r\n\r\nFor now, just pass in the LLM to the extractor as a kwarg. Working on a proper global settings object and removing service context soon"
      }
    ]
  },
  {
    "number": 9934,
    "title": "[Question]: Sentence transfromer semeantic_search and VectorIndexRetriever retrieve",
    "created_at": "2024-01-09T17:11:59Z",
    "closed_at": "2024-01-15T13:56:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/9934",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have text data that I embed using a sentence transformer \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\r\n\r\nFirst try I used the sentence_transformers semantic_search method that uses cos_sim for the score function and I got the results that I want\r\n\r\nThen I used VectorIndexRetriever with CustomEmbeddings using the same sentence_transformers model but when I used the retrieve method I got different results with a different score  \r\n\r\nI thought I would get the same results for both methods, what did I get the same results? I couldn't find anything about this ",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9934/comments",
    "author": "mim201820",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2024-01-09T17:12:11Z",
        "body": "_ğŸ¤–_\n<!-- Greeting -->\nHello again, @mim201820! Good to see you back. I appreciate your detailed question and I'm on it! I'll dig into the specifics of your issue and will respond with a comprehensive answer shortly. Hang tight!\n\n\n"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-01-09T17:15:28Z",
        "body": "These methods are only comparable if you use the same embeddings in both.\r\n\r\nCan you share some replication code?"
      },
      {
        "user": "mim201820",
        "created_at": "2024-01-09T17:24:24Z",
        "body": "So lets say we have a text file that we want to embed its lines.\r\n\r\nFor semantic_search: \r\n```\r\n    with open(file_path, \"r\") as f:\r\n        inputs = f.readlines()\r\n    embedding_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\r\n    embeddings = embedding_model.encode(inputs)\r\n    output = model.encode(query)\r\n    query_embeddings = torch.FloatTensor(output)\r\n    hits = semantic_search(query_embeddings, dataset_embeddings, top_k=2)\r\n```\r\nFor retrieve:\r\nFirst I define the custom embedding:\r\n```\r\nclass CustomEmbeddings(BaseEmbedding):\r\n    _model: SentenceTransformer = PrivateAttr()\r\n    _instruction: str = PrivateAttr()\r\n\r\n    def __init__(\r\n        self,\r\n        instructor_model_name: str = \"sentence-transformers/distiluse-base-multilingual-cased-v1\",\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        self._model = SentenceTransformer(instructor_model_name)\r\n        super().__init__(**kwargs)\r\n\r\n    @classmethod\r\n    def class_name(cls) -> str:\r\n        return \"instructor\"\r\n\r\n    async def _aget_query_embedding(self, query: str) -> List[float]:\r\n        return self._get_query_embedding(query)\r\n\r\n    async def _aget_text_embedding(self, text: str) -> List[float]:\r\n        return self._get_text_embedding(text)\r\n\r\n    def _get_query_embedding(self, query: str) -> List[float]:\r\n        embeddings = self._model.encode(query)\r\n        return embeddings.tolist()\r\n\r\n    def _get_text_embedding(self, text: str) -> List[float]:\r\n        embeddings = self._model.encode(text)\r\n        return embeddings.tolist()\r\n\r\n    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\r\n        embeddings = self._model.encode(texts)\r\n        return embeddings.tolist()\r\n```\r\nthen I create my index retriever\r\n``` \r\n       embed_model = CustomEmbeddings()\r\n        nodes = []\r\n        with open(f_path, \"r\") as f:\r\n            entries = f.readlines()\r\n        file_name = os.path.basename(f_path)\r\n        print(f'Loading {file_name}')\r\n        for index, entry in enumerate(entries):\r\n                   node = TextNode(text=entry,)\r\n                  node_embedding = embed_model.get_text_embedding(entry)\r\n                  nodes.append(node)\r\n          db = chromadb.PersistentClient(path=\"data/chroma_db\")\r\n          chroma_collection = db.get_or_create_collection(\"chroma_db\")\r\n          vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\r\n          vector_store.add(nodes)\r\n          storage_context = StorageContext.from_defaults(vector_store=vector_store)\r\n          service_context = ServiceContext.from_defaults(embed_model=embed_model)\r\n          index = VectorStoreIndex.from_vector_store(\r\n              vector_store=vector_store, service_context=service_context\r\n          )\r\n          index_retriever = index.as_retriever()\r\n          results = index_retriever.retrieve(query)\r\n          for r in results: print(f\"{r.text}\\n score {r.score}\")\r\n```"
      },
      {
        "user": "logan-markewich",
        "created_at": "2024-01-11T22:30:37Z",
        "body": "@mim201820 chroma uses an approximate vector search (better for large amounts of data), while I suspect sentence transformers is exact. If you try another vector db (our default vector db, qdrant, weaviate, etc.) you will probably have the results you expect"
      },
      {
        "user": "mim201820",
        "created_at": "2024-01-15T13:56:45Z",
        "body": "correct chroma does use cosin  similarity by default, thanks for the support "
      }
    ]
  },
  {
    "number": 9334,
    "title": "[Question]: Add TextNode metadata to help Retriever ",
    "created_at": "2023-12-05T23:10:28Z",
    "closed_at": "2023-12-06T03:27:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/9334",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHi ,\r\n\r\nI wonder if it's possible to append some metadata that would appended to TextNode's text during search.\r\n\r\nI suppose including this metadata will help retriever greatly.\r\n\r\nI can't rely on Document metadata provided by PDF parser . Actually , I want similar functionality for Nodes\r\n\r\nThanks,\r\nNissim",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9334/comments",
    "author": "snassimr",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-12-05T23:10:40Z",
        "body": "_ğŸ¤–_\n<!-- Greeting -->\nHello @snassimr,\n\nGreat to see you again and thank you for your question!\n\nI'm currently looking into your issue regarding the appending of metadata to TextNode's text. Please hang tight while I gather all the necessary information to provide you with a comprehensive answer.\n\nI will get back to you shortly.\n\n\n"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-12-05T23:12:00Z",
        "body": "Nodes and documents are nearly the same object -- they both support the same metadata methods and whatnot. Anything a document has, a node has too\r\n\r\nWas there something specific you wanted to do?"
      },
      {
        "user": "snassimr",
        "created_at": "2023-12-05T23:24:43Z",
        "body": "I want to tag TextNode if it contains specific information :. Here the example . I assume node metadata is a dictionary ğŸ‘ \r\n\r\n{\r\n   'code' 'Doesn't contain code'\r\n   'example' : \"Contain Example 13'\r\n}\r\n\r\nI want the node to be retrieved if chunk doesn't contain code and does contain Example 13.\r\n\r\nI am not sure if LLM would be able to exploit the metadata . It just a test\r\n"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-12-05T23:50:30Z",
        "body": "You can do this with \r\n\r\n`node.metadata = metadata`\r\n\r\nOr\r\n\r\n`node = TextNode(text=text, metadata=metadata)`\r\n\r\nIf your input documents already have this metadata, it would be inherited to the nodes automatically.\r\n\r\nThen for retrieval, you can use metadata filters \r\n\r\n```\r\nfrom llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\r\n\r\nfilters = MetadataFilters(\r\n    filters=[ExactMatchFilter(key=\"key\", value=\"val\")]\r\n)\r\n\r\nquery_engine = index.as_query_engine(similarity_top_k=3, filters=filters)\r\n```"
      },
      {
        "user": "snassimr",
        "created_at": "2023-12-06T00:01:47Z",
        "body": "Great . Let me check . The idea of MetadataFitlers is also very powerful . Adding some types of filters extend possibilities even more . Thanks"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-12-06T03:27:53Z",
        "body": "Yup, we are working on greatly expanding metadata filter types. \r\n\r\nRecently, chroma, qdrant, weaviate, and pinecone support some new filters we are slowly rolling out"
      }
    ]
  },
  {
    "number": 9028,
    "title": "[Question]: Are we able to set system prompt for GPT-4? ",
    "created_at": "2023-11-20T18:17:25Z",
    "closed_at": "2023-11-20T18:18:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/9028",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI cannot find any doc about setting system prompt in this version. Are we still supporting setting system prompt? ",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9028/comments",
    "author": "staypo",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-11-20T18:17:35Z",
        "body": "_ğŸ¤–_\n<!-- Greeting -->\nHello @staypo,\n\nI'm Dosu, a friendly bot here to assist you while we wait for a human maintainer. I'm programmed to help solve bugs, answer questions, and guide you in becoming a contributor. I'm currently reviewing your issue about setting system prompt in the LlamaIndex. I'll have a detailed response for you shortly. Hang tight!\n\n\n"
      }
    ]
  },
  {
    "number": 8743,
    "title": "[Question]: How to use PyTorch_MPS_HIGH_WATERMARK_RATIO?",
    "created_at": "2023-11-07T14:21:08Z",
    "closed_at": "2023-11-07T14:59:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/8743",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHow to use PyTorch_MPS_HIGH_WATERMARK_RATIO?\r\n\r\n\r\n```\r\nindex = VectorStoreIndex.from_documents(\r\n    documents, storage_context=storage_context, service_context=service_context, PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\r\n) \r\n```\r\nMakes  it crash with following error\r\n`RuntimeError: MPS backend out of memory (MPS allocated: 2.44 GB, other allocations: 4.34 GB, max allowed: 6.77 GB). Tried to allocate 10.58 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).`",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8743/comments",
    "author": "taynotfound",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-11-07T14:21:25Z",
        "body": "_ğŸ¤–_\n<!-- Greeting -->\nHello @taynotfound,\n\nI'm Dosu, a bot designed to assist you with LlamaIndex. I can help you debug issues, answer any questions you might have, and guide you on how to contribute to the repository. While we're waiting for a human maintainer to join us, I'm here to assist you. \n\nI'm currently reviewing your issue and will get back to you with a response soon.\n<!-- Will answer -->\n\n\n"
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2023-11-07T14:34:10Z",
        "body": "_ğŸ¤–_\n<!-- Answer -->\nIt seems like you're encountering an out of memory error when trying to use the `PYTORCH_MPS_HIGH_WATERMARK_RATIO` parameter. To better assist you, could you provide more information about your setup? Specifically, it would be helpful to know:\n\n1. The version of PyTorch you're using.\n2. The specifications of your hardware, particularly your GPU.\n3. The size and nature of the data you're working with.\n\nThis additional information will help in understanding the root cause of the issue and provide a more accurate solution.\n\n\n---\n\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n\nIf you want to continue the conversation, start your reply with @dosu-bot.\n"
      },
      {
        "user": "taynotfound",
        "created_at": "2023-11-07T14:50:41Z",
        "body": "@dosu-bot PyTorch Version: 2.1.0\r\nHardware: MacBook Pro 2,3 GHz 8-Core Intel Core i9 8th gen. Radeon Pro Vega 20 4 GB\r\nIntel UHD Graphics 630 1536 MB. 32 GB 2400 MHz DDR4\r\n\r\nI am working with multiple PDF's 134.657.334 bytes (134,7 MB on disk) for 48 items"
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2023-11-07T14:57:13Z",
        "body": "_ğŸ¤–_\n<!-- Answer -->\nThank you for providing the detailed information. The error you're encountering is related to the memory allocation for PyTorch on your system. The `PYTORCH_MPS_HIGH_WATERMARK_RATIO` parameter is used to set the maximum amount of memory that PyTorch can use. When you set it to 0.0, you're essentially telling PyTorch that it can't use any memory, which is likely why your application is crashing.\n\nHowever, to provide a more accurate solution, I need a bit more information. Could you please provide the following:\n\n1. The version of LlamaIndex you're using.\n2. The specific error message you're seeing when your application crashes.\n3. Any other relevant code snippets, especially those related to the initialization of `storage_context` and `service_context`.\n4. The size of the largest individual PDF in your dataset.\n\nThis additional information will help me understand the issue better and provide a more accurate solution.\n\n\n---\n\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n\nIf you want to continue the conversation, start your reply with @dosu-bot.\n"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-11-07T14:59:23Z",
        "body": "@taynotfound this is an environment variable actually (and also not really related to llama-index)\r\n\r\nIn a terminal, I would do something like `export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0` before running my program to apply that setting"
      }
    ]
  },
  {
    "number": 8586,
    "title": "[Question]: how to relsove asyncio.run() cannot be called from a running event loop when app evaluation?",
    "created_at": "2023-10-30T14:19:58Z",
    "closed_at": "2024-02-07T16:05:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/8586",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nI apply evaluation to my code, but when i call to function, i see error : asyncio.run() cannot be called from a running event loop\r\nI try find solution to solve issue, but couldn't. can you help me? \r\nMy code: \r\n\r\ndef evaluate_result(query: str, reference: str, answer: str) -> EvaluationResult:\r\n\r\n    llm = VertexAI(\r\n            model_name=\"text-bison\",\r\n            max_output_tokens=800,\r\n            temperature=0.2,\r\n            top_p=0.2,\r\n            top_k=1.0,\r\n            verbose=True\r\n        )\r\n    service_context = ServiceContext.from_defaults(llm=llm)\r\n    evaluator = CorrectnessEvaluator(service_context=service_context)\r\n\r\n    return evaluator.evaluate(\r\n        query=query,\r\n        response=answer,\r\n        reference=reference,\r\n    )\r\n\r\n\r\nAnd i call: \r\nasync def answer_chat_question():\r\n          data = evaluate_result(query=result[\"query\"], reference=result[\"source_documents\"], answer=result[\"result\"])\r\n           print(data)\r\n\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8586/comments",
    "author": "sieudx",
    "comments": [
      {
        "user": "ravi03071991",
        "created_at": "2023-10-30T14:23:16Z",
        "body": "@sieu-anika did you have following snippet in your code? This should solve your issue.\r\n\r\nimport nest_asyncio\r\nnest_asyncio.apply()`"
      },
      {
        "user": "TutubanaS",
        "created_at": "2023-11-01T10:51:30Z",
        "body": "I've been dealing with this problem now on my JupyterHub where we share GPUs, @ravi03071991's solution seems to make the cell give an output but the cell never finishes the execution and I have no idea why this even occurs. "
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2024-01-31T16:15:52Z",
        "body": "Hi, @sieu-anika,\n\nI'm helping the LlamaIndex team manage their backlog and am marking this issue as stale. The issue involves encountering the error \"asyncio.run() cannot be called from a running event loop\" when trying to call a function. The code includes an async function `answer_chat_question()` that calls `evaluate_result()`, which may be causing the issue. In the comments, ravi03071991 suggested using `nest_asyncio.apply()` to solve the issue, which received positive feedback from you and wkoziej. Additionally, TutubanaS mentioned experiencing a similar problem on JupyterHub and shared their experience with the suggested solution.\n\nCould you please confirm if this issue is still relevant to the latest version of the LlamaIndex repository? If it is, please let the LlamaIndex team know by commenting on the issue. Otherwise, feel free to close the issue yourself, or the issue will be automatically closed in 7 days.\n\nThank you for your understanding and cooperation. If you have any further questions or need assistance, feel free to reach out."
      }
    ]
  },
  {
    "number": 8218,
    "title": "[Question]: Nvidia Triton/TensorRT support ",
    "created_at": "2023-10-20T05:48:50Z",
    "closed_at": "2023-10-24T06:32:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/8218",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIs there any support to wrap Triton server/TensorRT hosted models in llamaindex ? Not able to find enough documentation or support for this.Thankyou.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8218/comments",
    "author": "pranavbhat12",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-10-20T05:55:27Z",
        "body": "_ğŸ¤–_\n<!-- Answer -->\nHi there,\n\nCurrently, I'm unable to find any specific information regarding Nvidia Triton/TensorRT support in the LlamaIndex repository. To better assist you, could you please provide more details about your use case? Specifically, it would be helpful to know:\n\n1. What version of LlamaIndex are you currently using?\n2. What specific functionality or feature are you trying to implement with Triton server/TensorRT?\n3. Have you encountered any errors or issues while trying to integrate Triton server/TensorRT with LlamaIndex? If so, could you please share the error messages or describe the issues?\n\nThe more information you can provide, the better I can assist you. Thanks for your patience and cooperation.\n\n---\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n\nIf you want to continue the conversation, start your reply with @dosu-bot."
      },
      {
        "user": "pranavbhat12",
        "created_at": "2023-10-21T14:27:31Z",
        "body": "To increase the inference speed I want to use Triton server.So even if Model is hosted on Triton how do I integrate it as llm in llama index?"
      }
    ]
  },
  {
    "number": 8143,
    "title": "[Question]: LangchainEmbedding with huggingfaceEmbeddings vs native llamaindex huggingfaceembedding",
    "created_at": "2023-10-16T01:47:40Z",
    "closed_at": "2023-10-16T16:46:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/8143",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHi. I'm newb on LLM tasks.\r\nI tried to build local LLM system via llamaindex.\r\nbut i got difference result between langchain huggingfaceembedding and native huggingfaceembedding.\r\n\r\nHere are sample codes.\r\nLanghchainEmbedding with HuggingFaceEmbedding\r\n```\r\n# LlamaIndex with Langchain HuggingFaceEmbedding\r\n#!pip install sentence-transformers\r\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\nfrom llama_index import LangchainEmbedding, ServiceContext\r\n\r\nembed_model = LangchainEmbedding(\r\n  HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\r\n)\r\nembeddings = embed_model.get_text_embedding(\"Hello World!\")\r\nprint(len(embeddings))\r\nprint(embeddings[:5])\r\n\r\n[-0.0032757034059613943, -0.011690760031342506, 0.04155919700860977, -0.03814806044101715, 0.024183105677366257]\r\n```\r\nNative ver.\r\n```\r\nfrom llama_index.embeddings import HuggingFaceEmbedding\r\n\r\nembed_model = HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')\r\nembeddings = embed_model.get_text_embedding(\"Hello World!\")\r\nprint(len(embeddings))\r\nprint(embeddings[:5])\r\n\r\n[-0.030880559235811234, -0.1102105900645256, 0.3917849361896515, -0.3596276342868805, 0.22797785699367523]\r\n```\r\n\r\nAs I know, floating error can occur returning slightly different value. but i found that return of native HugEmbedding is average 10 times of return of LangchainEmbedding(wHug).",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8143/comments",
    "author": "jungwooooo",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-10-16T01:49:39Z",
        "body": "@jungwooooo I could have sworn I confirmed these were equivalent. Will double double check the issue"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-10-16T02:13:47Z",
        "body": "@jungwooooo I stepped through the source code of each\r\n\r\nThe exact same token IDs get passed to each model. The output of the model itself is different between both libraries, but the post-processing is the same\r\n\r\nI'm not sure if sentence-transformers is doing something different to the models when loading, but I can confirm the inputs are the exact same, and the post-processing is the exact same."
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-10-16T02:19:04Z",
        "body": "@jungwooooo spoke too soon, I see sentence transformers has an extra normalize step ğŸ¤¦ğŸ» \r\n\r\nThe real question is how to fix this without breaking peoples existing embeddings..."
      },
      {
        "user": "jungwooooo",
        "created_at": "2023-10-16T02:21:07Z",
        "body": "@logan-markewich Thank you for your kind reply. You said that sentence-transformer has an extra normalize step, so what is the more precise result? native Hug or langchainEmb(w Hug)?"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-10-16T02:26:34Z",
        "body": "@jungwooooo I think in practice both are fairly similar. Normalization helps avoid outliers though.\r\n\r\nI have it fixed locally, so now both give the same results. Just deciding on how best to patch this"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-10-16T02:27:23Z",
        "body": "If you are curious, the code looks like this now (with the added normalization)\r\n\r\n```\r\n        model_output = self._model(**encoded_input)\r\n\r\n        if self.pooling == \"cls\":\r\n            embeddings = self._cls_pooling(model_output)\r\n        else:\r\n            embeddings = self._mean_pooling(\r\n                model_output, encoded_input[\"attention_mask\"]\r\n            )\r\n        \r\n        if self.normalize:\r\n            import torch\r\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1).tolist()\r\n        \r\n        return embeddings\r\n```"
      },
      {
        "user": "jungwooooo",
        "created_at": "2023-10-16T02:33:06Z",
        "body": "@logan-markewich So, is it right that if i use native HugEmb => add normalization step, elif i use langchainemb(w Hug) => use output?"
      },
      {
        "user": "jungwooooo",
        "created_at": "2023-10-16T02:35:48Z",
        "body": "@logan-markewich \r\ni fix this like below code.\r\n```\r\nfrom llama_index.embeddings import HuggingFaceEmbedding\r\n\r\nembed_model = HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')\r\nembeddings = embed_model.get_text_embedding(\"Hello World!\")\r\nprint(len(embeddings))\r\nprint(embeddings[:5])\r\nimport torch\r\nembeddings = torch.nn.functional.normalize(torch.tensor(embeddings).reshape(1,-1), p=2, dim=1).tolist()\r\nprint(embeddings[0][:5])\r\n```\r\nthen it return same output value as langchainEmbedding(w Hug)"
      },
      {
        "user": "jungwooooo",
        "created_at": "2023-10-16T02:37:07Z",
        "body": "@logan-markewich As i understood, native HuggingFaceEmbedding will need to be updated this normalize issue."
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-10-16T02:37:18Z",
        "body": "@jungwooooo in some initial testing, they still return the same nodes\r\n\r\nAnd yes, I have the code ready to make a PR. But it needs some thought I think"
      },
      {
        "user": "jungwooooo",
        "created_at": "2023-10-16T02:41:18Z",
        "body": "@logan-markewich Thank you for your kind reply. I hope llama_index be the best LLM application package.\r\nAnd also i will raise another issues for your development!"
      }
    ]
  },
  {
    "number": 7939,
    "title": "[Question]:Chat Engine with context information with customized prompt - Low-level API",
    "created_at": "2023-10-02T18:36:14Z",
    "closed_at": "2023-10-24T06:31:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/7939",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI am using the currently high level below APIs for Context Chat Engine.\r\n``chat_engine = index.as_chat_engine(\r\n            chat_mode='context', \r\n            llm = my_llm,  memory=chat_memory, similarity_top_k=top_k,\r\n            system_prompt=system_prompt, verbose=True) `\r\n            \r\nI would like to give my own prompt template of system prompt, CHAT_TEXT_QA_PROMPT, CHAT_REFINE_PROMPT, as well as a context template.  Request help on that.\r\n\r\nWhen, I attempted with `ContextChatEngine`, Unable to provideCHAT_TEXT_QA_PROMPT. Moreover, I need to explicitly give chat_history, as the memory.chat_history is empty always.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/7939/comments",
    "author": "ramakrse",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-10-02T18:47:27Z",
        "body": "There is no text qa prompt for a context engine. It just calls the retriever for your index, and inserts the retrieved text into the system prompt (along with the system prompt you provided)\r\n\r\nYou cant pass in both memory and chat history. Add that chat history to the memory, and then pass that in "
      }
    ]
  },
  {
    "number": 7779,
    "title": "[Question]: about milvus version",
    "created_at": "2023-09-22T09:25:02Z",
    "closed_at": "2023-09-22T15:35:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/7779",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nThanks for your great work!\r\nI am work with llama_index and milvus and meet \"ImportError: cannot import name 'MilvusClient' from 'pymilvus'\".\r\nI wandor to know which milvus version should we use?  \r\n The current version is pymilvus-2.2.0 and llama_index=0.8.29.post1",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/7779/comments",
    "author": "JDZW2014",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-09-22T09:33:01Z",
        "body": "_ğŸ¤–_\n<!-- Answer -->\nHi there,\n\nThanks for reaching out and for your kind words about our work!\n\nTo help you better with the ImportError you're encountering, could you please provide a bit more information? Specifically, it would be helpful to know:\n\n1. The exact error message you're seeing, including any traceback.\n2. The operating system you're using.\n3. The Python version you're using.\n4. Any other dependencies you've installed that might be relevant.\n\nThis information will help me understand the context of the issue you're facing and provide a more accurate solution. Thanks in advance!\n\nBest,\nDosu\n\n---\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n\nIf you want to continue the conversation, start your reply with @dosu-bot."
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-09-22T15:35:57Z",
        "body": "@JDZW2014 seems to work with the latest -- ` pymilvus==2.3.1`"
      },
      {
        "user": "JDZW2014",
        "created_at": "2023-09-25T01:59:03Z",
        "body": "use pymilvus=2.2.0 pymilvus get error ï¼š ImportError: cannot import name 'MilvusClient' from 'pymilvus' \r\nuse pymilvus=2.3.0 or 2.3.1 get errorï¼špymilvus.exceptions.MilvusException: <MilvusException: (code=1, message=this version of sdk is incompatible with server, please downgrade your sdk or upgrade your server)>\r\n\r\nSo must I  upgrade my milvus server? It's there any other solutions?\r\n"
      }
    ]
  },
  {
    "number": 7757,
    "title": "[Question]: Seeking Feedback on Using LLMs for Hierarchical Multi-Label Text Classification on Large Archive Files (warc)",
    "created_at": "2023-09-21T12:33:34Z",
    "closed_at": "2023-10-24T06:31:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/7757",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI'm new to LLMs and would like feedback on using them for hierarchical multi-label text classification on extensive archive files (.warc). I have training abstracts with over 1000 target categories, and each abstract can have up to 10 labels. \r\n\r\n**Plan**\r\n1) Create embeddings for the warc files or create an abstract.\r\n2) Create embeddings for all 1000 hierarchical target classes.\r\n3) Use Retrieval Augmented Generation to prompt the LLM to select up to 10 labels based on embeddings\r\n4) Fine-tune the model with my training data.\r\n\r\n\r\n**Question**\r\n1. Is my proposed workflow realistic?\r\n2. Can I fine-tune either the model or the embeddings?\r\n3. Which is better for my project: LLama or ChatGPT?\r\n4. Can I train the LLM by supplying both the abstract and target category classes within the prompt?\r\n5. Are agents more effective? \r\n6. Would using an agent for recursive category selection from root to leaf nodes be more realistic? Is it worth exploring agents?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/7757/comments",
    "author": "reversingentropy",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-09-21T12:40:28Z",
        "body": "_ğŸ¤–_\n<!-- Answer -->\nHi there,\n\nThanks for reaching out with your questions. Your plan seems well thought out and it's great to see you're considering using LLMs for such a complex task. Let's go through your questions:\n\n1. Your proposed workflow seems realistic, but the success will largely depend on the quality of your data and the specifics of your implementation. \n\n2. Yes, you can fine-tune both the model and the embeddings. Fine-tuning the model involves adjusting the weights of the model based on your specific task and data. Fine-tuning the embeddings involves adjusting the representations of the words/phrases in your data to better suit your task.\n\n3. The choice between LLama and ChatGPT would depend on the specifics of your task. Both models have their strengths and weaknesses. LLama is designed for tasks that involve structured data, while ChatGPT is designed for conversational tasks. \n\n4. Yes, you can train the LLM by supplying both the abstract and target category classes within the prompt. This is a common approach in multi-label classification tasks.\n\n5. The effectiveness of agents would depend on the specifics of your task. Agents can be very effective for tasks that involve complex decision-making or tasks that require the model to maintain a state.\n\n6. Using an agent for recursive category selection from root to leaf nodes could be a good approach, especially if your categories are hierarchical. However, this would likely require a more complex implementation and more computational resources.\n\nTo provide more specific advice, it would be helpful to know more about your data and your task. For example, what is the nature of your hierarchical target classes? How are your .warc files structured? What is the size of your training data? \n\nLooking forward to hearing more about your project.\n\n---\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n\nIf you want to continue the conversation, start your reply with @dosu-bot."
      }
    ]
  },
  {
    "number": 7583,
    "title": "[Question]: nl2graphquery.",
    "created_at": "2023-09-07T16:57:01Z",
    "closed_at": "2023-10-24T06:31:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/7583",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\n@wey-gu \r\nWhile using the nl2graphquery i am getting this error. For the context i havent index anything but this nl2graphquery suppose to give the answer from the knowledge graph itself or do I need to index some document for reference. Please help to find the solution\r\n\r\nParam: {}Error message: Error found in optimization stage: IndexNotFound: No valid index found\r\nThanks. \r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/7583/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-09-07T17:20:48Z",
        "body": "@yashdeepyds you need to have some existing knowledge graph to connect to. Can you share more details? "
      },
      {
        "user": "wey-gu",
        "created_at": "2023-09-07T23:15:08Z",
        "body": "Could you plz share your code? In case itâ€™s kgquery engine no index should be required indeed."
      },
      {
        "user": "ghost",
        "created_at": "2023-09-08T04:36:37Z",
        "body": "Before doing this i have created a knowledge graph.\r\n\r\nfrom llama_index.query_engine import RetrieverQueryEngine\r\nfrom llama_index.retrievers import KnowledgeGraphRAGRetriever\r\nfrom llama_index.storage.storage_context import StorageContext\r\nfrom llama_index.graph_stores import NebulaGraphStore\r\nfrom llama_index.llms import OpenAI\r\nfrom IPython.display import Markdown, display\r\n\r\nspace_name = \"abc\"\r\nedge_types = [\"has_column\"]\r\ntags = [\"table\", \"column\"]\r\n\r\ngraph_store = NebulaGraphStore(\r\n    space_name=space_name,\r\n    edge_types=edge_types,\r\n    tags=tags,\r\n)\r\nstorage_context = StorageContext.from_defaults(graph_store=graph_store)\r\n\r\ngraph_rag_retriever_with_nl2graphquery = KnowledgeGraphRAGRetriever(\r\n    storage_context=storage_context,\r\n    service_context=service_context,\r\n    llm=llm,\r\n    verbose=True,\r\n    with_nl2graphquery=True,\r\n)\r\n\r\nquery_engine_with_nl2graphquery = RetrieverQueryEngine.from_args(\r\n    graph_rag_retriever_with_nl2graphquery, service_context=service_context\r\n)\r\n\r\nresponse = query_engine_with_nl2graphquery.query(\r\n    \"what is this graph about\",\r\n)\r\ndisplay(Markdown(f\"<b>{response}</b>\"))"
      },
      {
        "user": "ghost",
        "created_at": "2023-09-11T05:30:57Z",
        "body": "Hi @wey-gu \r\nAny suggestions on this .\r\nThanks."
      }
    ]
  },
  {
    "number": 7194,
    "title": "[Question]:  NameError: name 'DBAPIConnection' is not defined",
    "created_at": "2023-08-08T11:24:49Z",
    "closed_at": "2023-10-24T06:30:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/7194",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have a application running on kubernetes, I have added llamindex and other packages required for llamaindex in requirements.txt file all of them are installed successfully but when I try to access the script with llamaindex import I get this error \" NameError: name 'DBAPIConnection' is not defined\". There might be an issue with my environment. I just want to know what is the reason for this bug that might be helpful to fix it.\r\nThanks ",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/7194/comments",
    "author": "usama-hameed",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-08-08T15:02:58Z",
        "body": "@usama-hameed a search across both llama-hub and llama-index code doesn't show this import anywhere\r\n\r\nI'm guessing this is unrelated to llama-index? "
      }
    ]
  },
  {
    "number": 7142,
    "title": "[Question]: Better implementation/embedding takes too long.",
    "created_at": "2023-08-03T20:04:05Z",
    "closed_at": "2023-09-12T02:09:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/7142",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nI'm trying to create a standard Q&A app over my documents. I am at the step of embedding these docs and am struggling with how slow the process is. At the time of this writing, I'm currently attempting to embed 29 files as a test, for a total of 150MB. It has been running for 24 minutes and is still not complete - Is this to be expected? I realize embedding is a resource intense process, but how do you realistically build an application with this bottleneck? This function currently works with 1-5 files. My target knowledge corpus size is 10GB. Is this feasible? I have yet to see a demo that shows an implementation of RAG using more than a few files. If there is a better way, could someone tell me what I should do differently? Thank you\r\n\r\n```\r\n`def generate_or_return_embeddings():\r\n    #Load files\r\n    documents = SimpleDirectoryReader('Investments', recursive=True).load_data()\r\n    \r\n    #Define embedding model\r\n    embed_model = LangchainEmbedding(\r\n        HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\r\n    )\r\n    #Define service context - bundle of commonly used resources using during indexing/querying stage in pipeline\r\n    service_context = ServiceContext.from_defaults(embed_model=embed_model) \r\n\r\n    #Connect to Chroma\r\n    chroma_db = chromadb.HttpClient(host=\"\", port=8000)\r\n    #Name the collection\r\n    collection_name = \"alpha-mini-1\"\r\n    collections = [col.name for col in chroma_db.list_collections()]\r\n    \r\n    if collection_name in collections:\r\n        print(\"Collection exists. Building index..\")\r\n        chroma_collection = chroma_db.get_collection(collection_name)\r\n        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\r\n        index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)\r\n        return index    \r\n    else:\r\n        print(\"New collection. Generating embeddings then building index...\")\r\n        chroma_collection = chroma_db.create_collection(collection_name)\r\n        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\r\n        #Abstraction for storing Nodes, indices, and vectors\r\n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\r\n        index = VectorStoreIndex.from_documents(\r\n            documents, storage_context=storage_context, service_context=service_context, embed_model=embed_model\r\n        )\r\n        return index`\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/7142/comments",
    "author": "kylemassimilian",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-08-03T21:01:16Z",
        "body": "@kylemassimilian you are using local embeddings I see. Do you have CUDA installed? Otherwise, it will be running on CPU and be pretty slow (I think it prints at the start whether it's running on CPU or CUDA)\r\n\r\nAssuming you have enough memory, you can also try increasing/decreasing the batch size, the default is 10\r\n\r\n```\r\nembed_model = LangchainEmbedding(\r\n      HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"),\r\n      embed_batch_size=10\r\n  )\r\n```"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-08-03T21:02:00Z",
        "body": "Larger batch sizes also take longer per batch, so there's some efficiency curve there with an optimal number somewhere"
      },
      {
        "user": "kylemassimilian",
        "created_at": "2023-08-04T20:13:37Z",
        "body": "@logan-markewich Thank you for helping yet again. My plan is to use a Ray cluster on AWS to get GPUs involved. But this will have to wait since the quality of my responses in my sample data is pretty poor."
      }
    ]
  },
  {
    "number": 7058,
    "title": "[Question]: what makes it different for custom query engine vs vector index query engine",
    "created_at": "2023-07-27T10:32:57Z",
    "closed_at": "2023-10-24T06:30:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/7058",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nBackground:\r\n\r\n- I created a vector index and created a query engine with the default configurations, say vector_query_engine\r\n- I created a custom query engine with a custom retriever first called vector_query_engine._retriever and then call another retriever and union the result following the docs, and I then created the query engine with RetrieverQueryEngine from the custom retriver and the response_synthesizer from vector_query_engine._response_synthesizer, say custom_query_engine\r\n\r\nThe strange thing here is, in case a question is about the data got nothing related:\r\n- vector_query_engine got a wrong answer\r\n- custom_query_engine said don't know\r\n\r\nI checked both response's node are the same(from vector search), it seems something is right in the custom query engine but not in vector index query engine, I looked into the code but couldn't find any(default kwargs) that's related, could you please help point where I could be missing?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/7058/comments",
    "author": "wey-gu",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-07-27T14:23:15Z",
        "body": "Are you customizing the LLM or service context at all? Once you start customizing retrievers and response synthesizers, there's a lot of places that need the service context. It's usually best to set a global service context to simplify things ğŸ¤”\r\n\r\nAlso, if the temperature is higher than zero, then getting different answers also seems possible "
      },
      {
        "user": "wey-gu",
        "created_at": "2023-07-27T15:45:42Z",
        "body": "Thanks @logan-markewich !\n\nDue to previously I was using azure oai thus I always explicitly specify one same service context for all classes(is this approach the global service context? I'll search it from docs tomorrow!), I'll double check tomorrow on LLM args(I recalled I have only one configuration calls my local model through)."
      },
      {
        "user": "wey-gu",
        "created_at": "2023-07-28T01:26:21Z",
        "body": "> Are you customizing the LLM or service context at all? Once you start customizing retrievers and response synthesizers, there's a lot of places that need the service context. It's usually best to set a global service context to simplify things ğŸ¤”\r\n> \r\n> Also, if the temperature is higher than zero, then getting different answers also seems possible\r\n\r\nThanks @logan-markewich , now with `set_global_service_context` being set, the custom query engine got the same results from the synthesizer, should be that phase the service context was not properly passed?"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-07-28T03:41:37Z",
        "body": "@wey-gu yea I'm guessing before there was a spot that the service context wasn't passed. Maybe into the response synthesizer?\r\n\r\nAt least with the global it's less worrisome ğŸ‘"
      }
    ]
  },
  {
    "number": 7036,
    "title": "[Question]: How to deal with utf-8 language?",
    "created_at": "2023-07-25T11:05:11Z",
    "closed_at": "2023-10-24T06:29:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/7036",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nHello, \r\n\r\nI'm trying to test accuracy in phase \"find the right document\" over than 50 sequence model with llama index in Vietnamese with xsquad_dataset but the accuracy highest is 30%.\r\nCan you give me some advise to enhance the accuracy with utf-8 ? :(\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/7036/comments",
    "author": "284nnuS",
    "comments": [
      {
        "user": "Disiok",
        "created_at": "2023-07-26T06:07:45Z",
        "body": "Could you share some sample documents and queries? Would help us reproduce any issue."
      },
      {
        "user": "284nnuS",
        "created_at": "2023-07-28T02:38:59Z",
        "body": "Sure, I will give it for you"
      }
    ]
  },
  {
    "number": 7012,
    "title": "Adding documents to a Composible graph after creating the graph",
    "created_at": "2023-07-22T19:42:23Z",
    "closed_at": "2023-07-23T04:54:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/7012",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have been able to create a composible graph over my documents. But I want to make it dynamic such that new documents can be added if needed, how best can I do this?\r\n\r\nIf I could get access to the index that is created over my documents it would be easy to add a node, but I don't know how to directly get access to the index. I also want to be able to change the default retrieval option for the index. I tried using a custom retriever as the documentation shows, but that only worked for the sub-indices, not the index itself. I tried passing the composible graph itself to the custom retriever, but that gave an error.\r\n\r\nPlease assist.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/7012/comments",
    "author": "Samshive",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-07-22T19:44:17Z",
        "body": "There isn't a single index in a graph, just a collection of sub-indexes.\r\n\r\nYou'll have to add the document to the sub-index it belongs to, and possibly re-build the graph I think ğŸ¤” "
      },
      {
        "user": "Samshive",
        "created_at": "2023-07-22T20:05:20Z",
        "body": "Ya, I figured that I could do that, but it seems like a wasteful way to do it. I know there is an underlying index that retrieves the sub-indices to query, so it makes sense that if I can access it I can treat it as a normal index and just add a node."
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-07-22T20:22:00Z",
        "body": "But how would an insert function know which sub index to put the new document in? It wpuld almost have to use an LLM to decide?\r\n\r\nTbh if you just update the appropriate sub index, I don't think you have to rebuild unless you change the index summaries "
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-07-22T20:22:54Z",
        "body": "I would almost recommend using a data agent or sub question engine here instead of a composable index. The composable index hasn't been maintained as well, hence the janky UX"
      },
      {
        "user": "Samshive",
        "created_at": "2023-07-22T20:55:02Z",
        "body": "> But how would an insert function know which sub index to put the new document in? It wpuld almost have to use an LLM to decide?\r\n> \r\n> Tbh if you just update the appropriate sub index, I don't think you have to rebuild unless you change the index summaries\r\n\r\nAm I understanding the composible graph incorrectly? I thought it was just a wrapper that takes in indexes of individual documents, treats them as nodes (hence why they need a summary), and creates a new index on top of them (defined by the index type that you specify).\r\n\r\nSo when you pass a query, it first identifies which document is the most appropriate to query and passes that query onto just that index. \r\n\r\nSo my thinking is that there is an index in the back that you can add another node to as a sub-index. \r\n\r\nBut ya, thanks for the feedback, I'll try some other options like sub-querying."
      }
    ]
  },
  {
    "number": 7001,
    "title": "[Question]: ",
    "created_at": "2023-07-21T17:32:39Z",
    "closed_at": "2023-10-24T06:29:56Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/7001",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nWe are trying to connect to PGVectorStore using RDS PostgreSQL proxy (Amazon) using SSL , however we had no luck connecting using from_params or just by passing connection string \r\n\r\nFATAL:  This RDS Proxy requires TLS connections\r\n\r\nwe tried both \r\n\r\nconnstr = build_connection_string(proxy_endpoint,5432,\"DBNAME\",\"USER\",iam_auth_token)\r\n\r\nconn_str = f\"postgresql://{USER}:{iam_auth_token}@{proxy_endpoint}:{port}/{databaseName}?sslmode=verify-ca&sslrootcert=/path/to/rds-combined-ca-bundle.pem\"\r\n\r\nNeither worked \r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/7001/comments",
    "author": "ddzmitry",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-10-20T16:02:49Z",
        "body": "Hi, @ddzmitry! I'm Dosu, and I'm here to help the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nFrom what I understand, you opened this issue regarding connecting to PGVectorStore using RDS PostgreSQL proxy with SSL. You mentioned that you have tried different connection methods, but none of them have worked. Currently, there hasn't been any activity or comments on the issue. \n\nBefore we close this issue, we wanted to check with you if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on the issue. Otherwise, feel free to close the issue yourself, or it will be automatically closed in 7 days. \n\nThank you for your understanding and contribution to the LlamaIndex project!"
      }
    ]
  },
  {
    "number": 6986,
    "title": "[Question]: Truncation Issues ",
    "created_at": "2023-07-20T16:42:31Z",
    "closed_at": "2023-07-21T15:12:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6986",
    "body": "### Question Validation\r\n\r\n- [x] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nI am currently using my own custom Embeddings to have weighted embeddings to feed into the database. However, when I try to to embed the data it seems as if the text is being truncated leading to many errors. I believe it is because of the ServiceContext truncating the text however there is nothing on what the default chunk size is. I am wondering how to make it so it does not truncate any text chunk",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6986/comments",
    "author": "azaiman1",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-07-20T17:40:26Z",
        "body": "Can you explain what kinds of errors you are getting?\r\n\r\nThe default chunk size is 1024"
      },
      {
        "user": "azaiman1",
        "created_at": "2023-07-21T15:12:22Z",
        "body": "Thank you I believe it is fixed now but the service context seemed to truncate my text so I was unable to parse it to generate embeddings "
      }
    ]
  },
  {
    "number": 6956,
    "title": "[Question]: Sources (documents in metadata) are missing on QueryPlanTool and OpenAIAgent when dependencies are included in the QueryPlan",
    "created_at": "2023-07-18T14:56:45Z",
    "closed_at": "2023-10-24T06:29:49Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6956",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nHello everyone,\r\n\r\nI've seen using OpenAIAgent and QueryPlanTool for my use-case and works really good. However, when the QueryPlan is composed of multiple nodes that includes dependencies, the sources (i.e. documents consulted and such) is not included as for instance if only one node is executed.\r\n\r\nExample:\r\n\r\nCalling function: query_tool with args: {\r\n  \"nodes\": [\r\n    {\r\n      \"id\": 1,\r\n      \"query_str\": \"Question A\",\r\n      \"tool_name\": \"test\",\r\n      \"dependencies\": []\r\n    },\r\n    {\r\n      \"id\": 2,\r\n      \"query_str\": \"Question B\",\r\n      \"tool_name\": \"test\",\r\n      \"dependencies\": []\r\n    },\r\n    {\r\n      \"id\": 3,\r\n      \"query_str\": \"Qyestion C\",\r\n      \"tool_name\": \"test\",\r\n      \"dependencies\": [1, 2]\r\n    }\r\n  ]\r\n}\r\n\r\nThe response.sources (documents consulted/metadata) don't include any information after this query is executed. Anyone knows how I can solve this problem?\r\n\r\nNote: When only one node is executed (no dependencies), I can extract the documents consulted.\r\n\r\nThank you in advance.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6956/comments",
    "author": "carpric",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-10-17T16:01:53Z",
        "body": "Hi, @carpric. I'm Dosu, and I'm helping the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale.\n\nBased on my understanding, the issue you reported is related to the missing inclusion of sources (documents consulted) in the response when using OpenAIAgent and QueryPlanTool with multiple nodes that have dependencies. Currently, there hasn't been any activity or updates on this issue.\n\nIf this issue is still relevant to the latest version of the LlamaIndex repository, please let the LlamaIndex team know by commenting on this issue. Otherwise, feel free to close the issue yourself, or it will be automatically closed in 7 days.\n\nThank you for your contribution, and please don't hesitate to reach out if you have any further questions or concerns."
      }
    ]
  },
  {
    "number": 6952,
    "title": "[Question]: How do I load an already existing index created using NebulaGraphStore for querying instead of running KnowledgeGraphIndex.from_documents every time?",
    "created_at": "2023-07-18T12:13:35Z",
    "closed_at": "2023-10-08T23:02:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6952",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHow do I load an already existing index created using NebulaGraphStore for querying instead of running KnowledgeGraphIndex.from_documents every time?\r\nIn documentation there is clarification about this and also there is no loader/ reader for nebula graph db\r\nThere is no clear way to use an already loaded index.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6952/comments",
    "author": "sridharac93",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-07-18T14:23:37Z",
        "body": "You'll need to persist and load the index. Tbh the graph store is pretty new, but I thiiiink this is how it works.\r\n\r\n```python\r\nindex.storage_context.persist(persist_dir=\"./storage\")\r\n\r\nfrom llama_index import StorageContext, load_index_from_storage\r\ngraph_store = [setup NebulaGraphStore]\r\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\", graph_store=graph_store)\r\nindex = load_index_from_storage(storage_context)\r\n```"
      },
      {
        "user": "aetherwu",
        "created_at": "2023-07-20T09:35:47Z",
        "body": "> ```python\r\n> load_index_from_storage\r\n> ```\r\n\r\nMay I ask for how to do it in RedisVectorStore?\r\nI can also help to submit it to document after I validate the way."
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-07-20T16:25:10Z",
        "body": "@aetherwu If you are using RedisVectorStore, the entire index is stored in the vector store.\r\n\r\nYou can setup the vector store object to connect to the existing vector store, and then do this\r\n\r\n```\r\nindex = VectorStoreIndex.from_vector_store(vector_store)\r\n```"
      }
    ]
  },
  {
    "number": 6946,
    "title": "[Question]: Errors with persisting graph as KeywordTableIndex/SimpleKeywordTableIndex ",
    "created_at": "2023-07-17T22:54:45Z",
    "closed_at": "2023-10-23T16:02:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6946",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI am persisting a SimpleKeywordTableIndex or KeywordTableIndex graph to storage (pushing to an Azure fileshare), and when reloading and querying, it goes into a continuous loop trying to answer the query until I hit an openai RateLimit error. This is not happening with other types of indices like TreeIndex as far as I have tested.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6946/comments",
    "author": "yuvansharma",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-10-16T16:02:09Z",
        "body": "Hi, @yuvansharma! I'm Dosu, and I'm here to help the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nBased on my understanding of the issue, you are encountering errors when persisting a graph as a KeywordTableIndex or SimpleKeywordTableIndex. Reloading and querying the graph leads to a continuous loop and eventually triggers an OpenAI RateLimit error. It seems that other types of indices like TreeIndex do not exhibit this issue. \n\nBefore we close this issue, we wanted to check with you if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on the issue. Otherwise, feel free to close the issue yourself, or the issue will be automatically closed in 7 days.\n\nThank you for your understanding and cooperation!"
      }
    ]
  },
  {
    "number": 6928,
    "title": "Different Results between direct Azure Open API call vs through llamaindex Interface",
    "created_at": "2023-07-16T11:14:59Z",
    "closed_at": "2023-10-22T16:03:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6928",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nInput CSV\r\n\r\nMyUtilisation, Q1FY22,Q2FY22,Q3FY22,Q4FY22,Q1FY23,Q2FY23,Q3FY23,Q4FY23,Q1FY24\r\nproject1,17.20%,16.10%,15.60%,15.00%,14.70%,14.70%,14.90%,14,90%\r\nproject2,82.80%,83.90%,84.40%,85.00%,85.30%,85.30%,85.10%,85.10%\r\n\r\n**Direct API Call:**\r\n\r\nresponse = openai.ChatCompletion.create(\r\n  engine=\"gpt-35-turbo\",\r\n  messages = [{\"role\": \"user\", \"content\": \"Context information is below.\\\\n---------------------\\\\nMyUtilisation,  Q1FY22, Q2FY22, Q3FY22, Q4FY22, Q1FY23, Q2FY23, Q3FY23, Q4FY23, Q1FY24\\\\nproject1, 17.20%, 16.10%, 15.60%, 15.00%, 14.70%, 14.70%, 14.90%, 14, 90%\\\\nproject2, 82.80%, 83.90%, 84.40%, 85.00%, 85.30%, 85.30%, 85.10%, 85.10%\\\\n---------------------\\\\nGiven the context information and not prior knowledge, answer the question: What is MyUtilisation of Q3FY22?\\\\n\"}],\r\n  temperature=0.0,\r\n  max_tokens=800,\r\n  top_p=0.95,\r\n  frequency_penalty=0,\r\n  presence_penalty=0,\r\n  stop=None)\r\n\r\n\r\nprint(response)\r\n\r\nResponse: \r\n{\r\n  \"choices\": [\r\n    {\r\n      \"finish_reason\": \"stop\",\r\n      \"index\": 0,\r\n      \"message\": {\r\n        \"content\": **\"The MyUtilisation of Q3FY22 is 15.60%.\",**\r\n        \"role\": \"assistant\"\r\n      }\r\n    }\r\n  ]\r\n\r\n**Through Llama interface:** \r\n\r\nUsing Azure openAi embeddings, retrieved the context and connection made to Azureopenai Chat/Completion api \r\n\r\napi_version=2023-05-15 data='{\"messages\": [{\"role\": \"user\", \"content\": \"Context information is below.\\\\n---------------------\\\\nMyUtilisation,  Q1FY22, Q2FY22, Q3FY22, Q4FY22, Q1FY23, Q2FY23, Q3FY23, Q4FY23, Q1FY24\\\\nproject1, 17.20%, 16.10%, 15.60%, 15.00%, 14.70%, 14.70%, 14.90%, 14, 90%\\\\nproject2, 82.80%, 83.90%, 84.40%, 85.00%, 85.30%, 85.30%, 85.10%, 85.10%\\\\n---------------------\\\\nGiven the context information and not prior knowledge, answer the question: What is MyUtilisation of Q4FY22?\\\\n\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\r\n\r\n\r\nResponse: DEBUG:llama_index.llm_predictor.base:The MyUtilisation of Q4FY22 is not provided in the given context information.\r\nThe MyUtilisation of Q4FY22 is not provided in the given context information.\r\nThe MyUtilisation of Q4FY22 is not provided in the given context information.\r\n\r\n\r\nIs this expected behavior? ",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6928/comments",
    "author": "BharahthyKannan",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-10-15T16:01:50Z",
        "body": "Hi, @BharahthyKannan! I'm Dosu, and I'm here to help the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nBased on my understanding, you reported an issue regarding different results obtained when making a direct Azure Open API call versus using the llamaindex interface. The direct API call returns the correct answer, while the llamaindex interface does not provide the expected result. \n\nSince there hasn't been any activity or comments on the issue, we wanted to check if this issue is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on the issue. Otherwise, feel free to close the issue yourself, or the issue will be automatically closed in 7 days. \n\nThank you for your understanding and contribution to the LlamaIndex project!"
      }
    ]
  },
  {
    "number": 6858,
    "title": "Can not load it into Chroma, and then query it ï¼index.as_query_engine(chroma_collection=chroma_collection)",
    "created_at": "2023-07-12T02:08:52Z",
    "closed_at": "2023-07-12T17:05:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6858",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\n# save to disk\r\nfrom dotenv import load_dotenv\r\n\r\nload_dotenv()\r\nfrom chromadb import Settings\r\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\r\nfrom llama_index.vector_stores import ChromaVectorStore\r\nfrom llama_index.storage.storage_context import StorageContext\r\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\nfrom llama_index.embeddings import LangchainEmbedding\r\nfrom IPython.display import Markdown, display\r\nimport chromadb\r\n# set up OpenAI\r\nimport os\r\nimport getpass\r\n\r\n# create client and a new collection\r\nchroma_client = chromadb.Client()\r\nchroma_collection = chroma_client.create_collection(\"quickstart\")\r\n\r\nprint(chroma_collection.count())\r\n\r\n# define embedding function\r\nembed_model = LangchainEmbedding(\r\n    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\r\n)\r\n\r\n# load documents\r\ndocuments = SimpleDirectoryReader(\r\n    \"news\"\r\n).load_data()\r\n\r\n# print(documents)\r\n\r\n# set up ChromaVectorStore and load in data\r\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\r\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\r\nindex = VectorStoreIndex.from_documents(\r\n    documents, storage_context=storage_context, embed_model=embed_model\r\n)\r\n\r\nprint(index)\r\nprint(chroma_collection.count())\r\n# print(chroma_collection.get()['documents'])\r\n# print(chroma_collection.get()['metadatas'])\r\n\r\n# Query Data\r\nquery_engine = index.as_query_engine(chroma_collection=chroma_collection)\r\nresponse = query_engine.query(\"ä¸­å›½æœ€è¿‘å‘ç”Ÿäº†ä»€ä¹ˆ,è¯´å‡ºå‘ç”Ÿçš„æ—¶é—´?\")\r\nprint(response)\r\ndisplay(Markdown(f\"<b>{response}</b>\"))\r\n\r\n\r\nOUTPUTS:\r\nF:\\Anaconda\\python.exe D:\\EmbeddingsSearch\\Chroma_Search\\test.py \r\n0\r\n<llama_index.indices.vector_store.base.VectorStoreIndex object at 0x00000210F8D88E50>\r\n77\r\n\r\nä¸­å›½æœ€è¿‘å‘ç”Ÿäº†2022å¹´11æœˆæœ«ï¼Œä¸­å›½åŒ—äº¬ã€ä¸Šæµ·ã€å—äº¬ã€å¹¿å·ã€æˆéƒ½ã€é‡åº†ç­‰å¤šä¸ªåŸå¸‚çˆ†å‘äº†å¤§æ‰¹å¹´è½»äººæŠ—è®®æ‰€è°“â€œåŠ¨æ€æ¸…é›¶â€æç«¯é˜²ç–«æ”¿ç­–çš„ç™½çº¸è¿åŠ¨ï¼Œä»¥åŠæ­¦æ±‰ã€å¤§è¿ã€éå±±ç­‰åœ°å‡ºç°äº†å¤§æ‰¹é€€ä¼‘äººå‘˜ä¸Šè¡—æŠ—è®®åŒ»ä¿ç¦åˆ©å‰Šå‡çš„â€œç™½å‘è¿åŠ¨â€ï¼Œä»¥åŠä¸­å…±é¢†å¯¼äººåå¤å¼ºè°ƒå®‰å…¨ï¼Œä»¥åŠè¾¾èµ–å–‡å˜›ç§°åŒ—äº¬æ–¹é¢æœ‰æ„ä¸ä»–æ¥è§¦ï¼Œä»–å¯¹æ¢å¤å¯¹è¯æ²¡æœ‰ä»»ä½•é—®é¢˜ï¼Œå‘ç”Ÿçš„æ—¶é—´æ˜¯2022å¹´11æœˆæœ«è‡³2023å¹´7æœˆ10æ—¥ã€‚\r\n<IPython.core.display.Markdown object>\r\n\r\nProcess finished with exit code 0\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6858/comments",
    "author": "ZhuJD-China",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-07-12T16:29:55Z",
        "body": "This line\r\n\r\n`query_engine = index.as_query_engine(chroma_collection=chroma_collection)`\r\n\r\nIs not needed. The index is already loaded and connected to chroma.\r\n\r\nAlso, you are not creating the embed_model properly. Should be like this\r\n\r\n```python\r\nfrom llama_index import ServiceContext\r\n\r\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\r\nindex = VectorStoreIndex.from_documents(\r\n    documents, \r\n    storage_context=storage_context, \r\n    service_context= service_context\r\n)\r\n```\r\n\r\n\r\nTo reload the index from chroma after you created it with llama-index, you can create the vector store and use it like this\r\n\r\n```python\r\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\r\nindex = VectorStoreIndex.from_vector_store(vector_store, service_context=service_context)\r\n```"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-07-12T16:58:50Z",
        "body": "Looks like the docs for chroma are wrong. Updating now..\r\n"
      }
    ]
  },
  {
    "number": 6844,
    "title": "pydantic.error_wrappers.ValidationError: 1 validation error for PromptTemplate",
    "created_at": "2023-07-11T09:26:54Z",
    "closed_at": "2023-07-12T17:07:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6844",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\n```\r\ncontext str = \"abc\"\r\nquery_str = \"def\"\r\n    template = (\"We have provided context information below. \\n\"\r\n                \"---------------------\\n\"\r\n                f\"{context_str}\\n\"\r\n                \"\\n---------------------\\n\"\r\n                f\"Given this information, please answer the question: {query_str}\\n\")\r\n    text_qa_template = Prompt(template=template)\r\n    vector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"LlamaIndex\")\r\n    index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\r\n\r\n    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\r\n    response = str(query_engine.query(query_str))\r\n```\r\n\r\nI am getting the error below in the above snippet of code:\r\npydantic.error_wrappers.ValidationError: 1 validation error for PromptTemplate\r\n__root__\r\nInvalid prompt schema; check for mismatched or missing input parameters. &#39;&#34;url&#34;&#39;\r\n(type=value_error)\r\n\r\nCan anyone please help me with the same? Thanks.\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6844/comments",
    "author": "vruti1121",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-07-12T17:06:45Z",
        "body": "@vruti1121 the `context_str` and `query_str` variables should not be formatted, they will be filled in automatically by llama-index. Leave them as string variables :) "
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-07-12T17:07:04Z",
        "body": "template = (\"We have provided context information below. \\n\"\r\n                \"---------------------\\n\"\r\n                \"{context_str}\\n\"\r\n                \"\\n---------------------\\n\"\r\n                \"Given this information, please answer the question: {query_str}\\n\")"
      }
    ]
  },
  {
    "number": 6817,
    "title": "[Question]:How can I speed up the pipeline?",
    "created_at": "2023-07-10T06:41:15Z",
    "closed_at": "2023-10-16T16:03:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6817",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nEvery time I  use the pipline to load the local model ,it will take about 20 mins.How can i accelerate it .\r\nIs it possible to speed up this process by calling multiple GPUs? How do I do that?\r\nAt the same time, it also takes a lot of time when generating answers to questions.\r\nThere are my codes below.\r\n\r\n`model_name = \"vicuna-chinese\"\r\npipeline = pipeline(\"text-generation\", model=model_name, device=\"cuda:1\", model_kwargs={\"torch_dtype\": torch.bfloat16})\r\n\r\nclass CustomLLM(LLM):\r\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\r\n        prompt_length = len(prompt)\r\n        response = pipeline(prompt, max_new_tokens=num_output,temperature=0)[0][\"generated_text\"]\r\n        # only return newly generated tokens\r\n        return response[prompt_length:]\r\n    @property\r\n    def _identifying_params(self) -> Mapping[str, Any]:\r\n        return {\"name_of_model\": model_name}\r\n    @property\r\n    def _llm_type(self) -> str:\r\n        return \"custom\"\r\n\r\nllm_predictor = LLMPredictor(llm=CustomLLM())\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\n#embed_model= SentenceModel('./text2vec-base-chinese')\r\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name='./text2vec-base-chinese'))\r\n#embed_model = SentenceTransformer(\"./text2vec-base-chinese\")\r\nprompt_helper = PromptHelper(\r\n    context_window=4000,\r\n    num_output=512,\r\n    chunk_size_limit=512,\r\n    chunk_overlap_ratio=0.1,\r\n)\r\nservice_context = ServiceContext.from_defaults(\r\n    llm_predictor=llm_predictor,\r\n    embed_model=embed_model,\r\n    prompt_helper=prompt_helper,\r\n    chunk_size_limit=512\r\n)\r\n\r\n#new_index = VectorStoreIndex.from_documents(documents, service_context=service_context,)\r\n\r\n\r\nfrom llama_index import SimpleDirectoryReader\r\ndocuments = SimpleDirectoryReader('./data').load_data()\r\n\r\nfrom llama_index.node_parser import SimpleNodeParser\r\nparser = SimpleNodeParser()\r\nnodes = parser.get_nodes_from_documents(documents)\r\n\r\nfrom llama_index import GPTVectorStoreIndex,VectorStoreIndex\r\nindex = GPTVectorStoreIndex(nodes, service_context=service_context)\r\nquery_engine = index.as_query_engine()`",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6817/comments",
    "author": "wyzhhhh",
    "comments": [
      {
        "user": "jon-chuang",
        "created_at": "2023-07-10T07:09:59Z",
        "body": "To my understanding, local model is very slow. There is nothing we can do about it. Recommended to use OpenAI APIs for now.\r\n\r\nWhat is your GPU model?"
      },
      {
        "user": "wyzhhhh",
        "created_at": "2023-07-10T07:18:39Z",
        "body": "> To my understanding, local model is very slow. There is nothing we can do about it. Recommended to use OpenAI APIs for now.\r\n> \r\n> What is your GPU model?\r\n\r\nI want to see how well the local model's answer works, and whether I can increase the pipeline loading speed or the model generation answer speed by calling multiple GPUs in parallel\r\n\r\nMy GPU model is A800"
      },
      {
        "user": "jon-chuang",
        "created_at": "2023-07-10T07:24:37Z",
        "body": "It is not possible to call multiple GPUs unless you can shard the model using pytorch FSDP or Alpa or similar. You can check huggingface support for this. \r\n\r\nI also don't believe loading should take 20 minutes. Are you sure you were not just downloading the model? Perhaps try again and loading should be near instant."
      },
      {
        "user": "wyzhhhh",
        "created_at": "2023-07-10T07:32:02Z",
        "body": "> \r\n\r\nIs there a problem with my code?\r\nThe model has been downloaded on the local server, It should not be caused by the redownload,\r\nIt takes a total of 20mins from \"pipeline=..\" to the end of code \"query_engine=...\" , maybe generate the nodes take some times? "
      },
      {
        "user": "jon-chuang",
        "created_at": "2023-07-10T07:41:21Z",
        "body": "Perhaps try logging? I would guess that it is the generation.  \r\n\r\n```python\r\nimport logging\r\n\r\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\r\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\r\n```\r\n\r\nWhat is the number of parameters? 13B? "
      },
      {
        "user": "wyzhhhh",
        "created_at": "2023-07-10T07:55:02Z",
        "body": "> Perhaps try logging? I would guess that it is the generation.\r\n> \r\n> ```python\r\n> import logging\r\n> \r\n> logging.basicConfig(stream=sys.stdout, level=logging.INFO)\r\n> logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\r\n> ```\r\n> \r\n> What is the number of parameters? 13B?\r\n\r\nyes it's 13B.\r\nIn addition to the long time, I also encountered another problem, the answer of the local model is '---', there is no text answer, is it related to setting the hyperparameters of the model,?how do I set the temperature of the model?"
      },
      {
        "user": "jon-chuang",
        "created_at": "2023-07-10T07:56:24Z",
        "body": "This could be due to the tokenizer, we encountered similar issues when using Chinese tokenizer before. Could you try running the huggingface pipeline on a small prompt?"
      },
      {
        "user": "jon-chuang",
        "created_at": "2023-07-10T07:57:36Z",
        "body": "If you can get the hugginface stats like tokens/s that would be nice as well"
      },
      {
        "user": "wyzhhhh",
        "created_at": "2023-07-10T08:03:46Z",
        "body": "> This could be due to the tokenizer, we encountered similar issues when using Chinese tokenizer before. Could you try running the huggingface pipeline on a small prompt?\r\n\r\nHow do you solve this problem?  can it generate the text answer now? Is it due to the size of the prompt?"
      },
      {
        "user": "jon-chuang",
        "created_at": "2023-07-10T08:07:15Z",
        "body": "It was when passing Latin alphabet into Chinese tokenizer, somehow it resulted in weird artifacts like _. Did not investigate further."
      },
      {
        "user": "jon-chuang",
        "created_at": "2023-07-10T08:08:47Z",
        "body": "Recommend you test individual components of your pipeline first  for instance swap out the llm_predictor for OpenAIPredictor."
      },
      {
        "user": "wyzhhhh",
        "created_at": "2023-07-10T08:11:58Z",
        "body": "> Recommend you test individual components of your pipeline first for instance swap out the llm_predictor for OpenAIPredictor.\r\n\r\nok Iâ€˜ll try it! Thank you "
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2023-10-09T16:02:05Z",
        "body": "Hi, @wyzhhhh! I'm Dosu, and I'm helping the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nBased on the conversation in the comments, it seems like you were asking for help on how to speed up the pipeline for loading a local model. There were suggestions to try using multiple GPUs or using OpenAI APIs instead. Additionally, you mentioned encountering another problem where the answer of the local model is '---' and asked for help in setting the hyperparameters of the model. The conversation continued with suggestions to try logging and testing individual components of the pipeline.\n\nBefore we close this issue, we wanted to check if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on the issue. Otherwise, feel free to close the issue yourself or it will be automatically closed in 7 days.\n\nThank you for your understanding and contribution to the LlamaIndex repository! Let us know if you have any further questions or concerns."
      }
    ]
  },
  {
    "number": 6815,
    "title": "[Question]: Is it possible to extract similarity values before sending prompt to gpt?",
    "created_at": "2023-07-10T03:26:24Z",
    "closed_at": "2023-07-10T10:10:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6815",
    "body": "### Question Validation\r\n\r\n- [x] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nHi! I'm fairly new to llamaindex, this is my first time working with it. I am trying to create a chatbot which uses base gpt-3.5-turbo's knowledge if it is unable to answer the question using the context I have provided. I have managed to achieve this using a custom prompt template and few shot learning. \r\n\r\nHowever, I find that I am using extra tokens in context which ends up getting wasted when gpt is not using it to construct the answer. The idea I have in mind is to create a custom parser that takes in my query text and can extract the embeddings similarity prior to sending the context to gpt-3.5-turbo(after receiving query embeddings from ada). If the similarity is below a threshold, say 0.85, I will reset the context to be 'Context is vague' or something similar. This will help me save a lot of tokens as I do not have to send the entire custom prompt each time. \r\n\r\nI am leaning towards extracting the similarity from the node post-processors but I am unsure where should this function be called i.e. as a argument to query_engine.query() or somewhere else. Hope my question is clear, I would be happy to provide more info/code if needed. Thank you!",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6815/comments",
    "author": "rmj1405",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-07-10T04:37:40Z",
        "body": "yea, node post-processors get called after retrieval, but before response synthesis\r\n\r\nYou could make a custom node postprocessor to filter out nodes, as well as add different nodes to return\r\n\r\n```\r\nquery_engine = index.as_query_engine(node_postprocessors=[MyCustomProcessor()])\r\n```\r\n\r\nIf you want to prevent calling gpt-3.5 altogether, you'll have to run the retrieval and response synthesis steps outside of the query engine\r\n\r\n```\r\nfrom llama_index import get_response_synthesizer\r\nretriever = index.as_retriever()\r\n\r\nresponse_synthesizer = get_response_synthesizer(response_mode=\"compact\", service_context=service_context)\r\n\r\nnodes = retriever.retrieve(query)\r\n\r\n<insert filter logic>\r\n\r\nresponse = response_synthesizer.synthesize(query, nodes=nodes)\r\n```\r\n\r\n"
      },
      {
        "user": "rmj1405",
        "created_at": "2023-07-10T10:10:04Z",
        "body": "Thank you so much, I managed to do it following the way you've provided!"
      }
    ]
  },
  {
    "number": 6773,
    "title": "[Question]:  Running old code always fail. ",
    "created_at": "2023-07-07T09:41:29Z",
    "closed_at": "2023-07-07T16:26:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6773",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIt's a big frustration by using llama_index . When you try to run 1 month old code , always fail . When you try to search old version documentation , no result .  Man,  you 're keeping adding lots of fancy features , personally , I guess you're losing focus and trying to be a master key for all problems when the whole industry is moving really fast .  Focus on couple scenarios you really have deep understanding . \r\n\r\nWish you guys good luck .  ",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6773/comments",
    "author": "deter3",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-07-07T16:26:05Z",
        "body": "The original codebase had a decent amount of tech debt, making contributing and development difficult.\r\n\r\nWe are working towards a more solid codebase, while also providing new features. Usually, most breaking changes are very minor to work around.\r\n\r\nHappy to help migrate any code you have, and we also do our best to keep our official docs updated."
      }
    ]
  },
  {
    "number": 6771,
    "title": "DECOMPOSE_QUERY_TRANSFORM with LLMs other than OpenAI",
    "created_at": "2023-07-07T08:31:51Z",
    "closed_at": "2023-10-14T20:08:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6771",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nHi - Has anyone tried query transformation/decomposition with any models other than OpenAI ones? Which all LLMs support the decomposition prompt? I am unable to get the response right from other LLMs. Tried with Huggingface LLMs, but did not get the response correctly from decomposition. \r\n\r\nTried the same query as what was given in the documentation: \"Compare and contrast the airports in Seattle, Houston, and Toronto\"",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6771/comments",
    "author": "mattkallo",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-10-06T16:01:45Z",
        "body": "Hi, @mattkallo! I'm Dosu, and I'm here to help the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nFrom what I understand, you were looking for information on query transformation/decomposition using language models other than OpenAI. You mentioned that you tried using Huggingface models but didn't get the desired results. Unfortunately, there hasn't been any activity or response on this issue yet.\n\nBefore we close this issue, we wanted to check with you if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on the issue. Otherwise, feel free to close the issue yourself, or it will be automatically closed in 7 days.\n\nThank you for your understanding, and we appreciate your contribution to the LlamaIndex project! Let us know if you have any further questions or concerns."
      }
    ]
  },
  {
    "number": 6746,
    "title": "[Question]: how to set the temperature for local LLM",
    "created_at": "2023-07-06T00:50:51Z",
    "closed_at": "2023-10-14T20:08:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6746",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nFor custom LLM using LLMPredictor to define index, is it possible to define the temperature in predicting? In definition of LLMPredictor, I only found following where it is possible in OpenAI interface? Thanks.\r\n\r\n```\r\n        self._llm = llm or OpenAI(\r\n            temperature=0, model_name=\"text-davinci-003\", max_tokens=-1\r\n        )\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6746/comments",
    "author": "stl2015",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-07-06T17:23:13Z",
        "body": "If you are using a custom LLM, how to set the temperature will be up to how you create the LLM right? What are you trying to use?"
      },
      {
        "user": "stl2015",
        "created_at": "2023-07-07T01:08:09Z",
        "body": "It makes sense. I'm trying to make sure to have deterministic response for any prompt. For example, HuggingFaceLLMPredictor in llama-index has no option for temperature - it will give deterministic response, right? In my case I was trying to use a Vicuna model from HF."
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-07-07T01:10:30Z",
        "body": "You have to set that in generate kwargs, since huggingface models have endless parameters to change they all go into generate kwargs \r\n\r\n`HuggingFaceLLMPredictor(..., generate_kwargs={\"temperature\": 0})`"
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2023-10-06T16:02:27Z",
        "body": "Hi, @stl2015! I'm Dosu, and I'm helping the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nFrom what I understand, you had a question about how to set the temperature for a local LLM using LLMPredictor. You mentioned that you searched the documentation and Discord for an answer, but couldn't find any information on how to define the temperature in predicting. \n\nFortunately, user Logan-markewich provided a solution by suggesting to set the temperature in generate kwargs for HuggingFace models. They even gave an example: `HuggingFaceLLMPredictor(..., generate_kwargs={\"temperature\": 0})`. Both devstein and you reacted with a thumbs up to this solution, so it seems like the issue has been resolved.\n\nBefore we close this issue, we just wanted to confirm if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on the issue. Otherwise, feel free to close the issue yourself or it will be automatically closed in 7 days.\n\nThank you for your contribution and understanding! Let us know if you have any further questions or concerns."
      }
    ]
  },
  {
    "number": 6700,
    "title": "Difference between vector store and index. ",
    "created_at": "2023-07-03T09:34:16Z",
    "closed_at": "2023-10-09T16:03:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6700",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nWhat is the difference between **vector store** and index in **llamaindex**. \r\nWhat is the basic conecpt behind this? Also, share the situations to use each over other.\r\n\r\nvectore stores found at\r\n```\r\nllama_index.vector_stores\r\n```\r\nIndices found at\r\n```\r\nfrom llama_index.indices.vector_store\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6700/comments",
    "author": "pradeepdev-1995",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-07-03T14:16:00Z",
        "body": "An index is a way to structure your data (list, graph, vector store, etc.)\r\n\r\nA vector store itself is just the place to store vector data. LlamaIndex supports a basic in-memory vector store by default, but there are also tons of integrations for vector dbs like weaviate and chroma etc. Basically, you'd usually use one of these when you are planning to index a LOT of data"
      },
      {
        "user": "pradeepdev-1995",
        "created_at": "2023-07-03T15:33:45Z",
        "body": "@logan-markewich  So what is the exact difference between **vector store, vector db** and **index?**\r\nsuppose I need to store the unstructured data with embeddings(say pdf) then which method i should choose a vector store or a vector db or an index? and why?\r\n\r\nmy understanding is , if I have a small amount of data then, go with index, otherwise go with vector store. Is this correct?\r\nAnd hope vector store and vector DBS are the same. "
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-07-03T17:09:59Z",
        "body": "A vector db and vector store are the same thing (just two different names to them)\r\n\r\nAn index uses a vector db. You don't really usea vector db on its own.\r\n\r\nYou only need to think about a vector store if you have a lot data to index and need to host/persist the index elsewhere. Otherwise, the default vector store works fine "
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2023-10-02T16:01:00Z",
        "body": "Hi, @pradeepdev-1995! I'm Dosu, and I'm helping the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nFrom what I understand, you were seeking clarification on the difference between a vector store and an index in LlamaIndex, as well as when to use each one. Logan-markewich provided a clear explanation, stating that a vector store is simply a place to store vector data, while an index is a way to structure the data. They also mentioned that a vector db and vector store are the same thing, and that an index utilizes a vector db. Additionally, Logan-markewich suggested using a vector store when there is a large amount of data to index and it needs to be hosted or persisted elsewhere.\n\nBefore we close this issue, we wanted to check if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on this issue. Otherwise, feel free to close the issue yourself, or it will be automatically closed in 7 days.\n\nThank you for your contribution to LlamaIndex!"
      }
    ]
  },
  {
    "number": 6676,
    "title": "[Question]: Design Pattern: Retrieval-Based Feature Bootstrapping",
    "created_at": "2023-07-01T07:26:07Z",
    "closed_at": "2023-10-12T16:04:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6676",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nThe idea for this patttern is that to extract global features, it makes sense to first index the entire document without the relevant features, enrich the document with new features via retrieval, and then reembed the enriched document into a new index. This can proceed for K iterations to yield successively more enriched documents, usually with K=1.\r\n\r\nThis is related to the pattern of refinement (i.e. chains).\r\n\r\nFor global features, methods such as HyDE may be beneficial.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6676/comments",
    "author": "jon-chuang",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-10-05T16:03:33Z",
        "body": "Hi, @jon-chuang. I'm Dosu, and I'm helping the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nFrom what I understand, you were seeking clarification on the \"Retrieval-Based Feature Bootstrapping\" design pattern and its relation to the refinement pattern. You mentioned the potential usefulness of methods like HyDE for global features. However, there hasn't been any further activity or comments on this issue.\n\nCould you please let us know if this issue is still relevant to the latest version of the LlamaIndex repository? If it is, please comment on the issue to let the LlamaIndex team know. Otherwise, feel free to close the issue yourself, or the issue will be automatically closed in 7 days.\n\nThank you for your understanding and contribution to the LlamaIndex project."
      }
    ]
  },
  {
    "number": 6657,
    "title": "[Question]: Average query time is 25 seconds for me",
    "created_at": "2023-06-30T10:00:33Z",
    "closed_at": "2023-07-22T01:45:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6657",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nHi all, \r\nI am having fun with LLAMA index and now trying to make it more efficient.\r\n\r\nI am using a bit older version, but even if I upgrade the time improves just a bit.\r\nI am happy of the answers but the average query time is 25 seconds using OpenAI. \r\nAny suggestion? Do you advise any specific model? My goal is to retrieve fast from documents\r\n\r\nI save the index by project and then load it:\r\n`index = GPTSimpleVectorIndex.load_from_disk(projectId + '.json')`\r\n`results = index.query(prompt)`\r\n`message = results.response`",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6657/comments",
    "author": "sorrentinopasquale",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-06-30T15:26:31Z",
        "body": "text-davinci-003 (the default) is definitely slower than using something like gpt-3.5\r\n\r\nAlso in the older versions of llama-index, the default chunk size was quite a lot bigger (3900 vs. 1024 now), so newer versions of llama-index are a bit faster (which you noticed)\r\n\r\nThe main bottlenecks are \r\na) how many LLM calls the query makes (default settings with newer versions of llama-index should be 1 llm call)\r\nb) how much text you send\r\nc) how much text the model generates and \r\nd) which LLM you use\r\n\r\nUsually I have response times around 3-10 seconds ğŸ¤” It varies depending on OpenAI server load"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-07-22T01:45:10Z",
        "body": "Going to close for now! Feel free to reach out on the discord with any more questions"
      }
    ]
  },
  {
    "number": 6641,
    "title": "[Question]: Which vectordb is using inside VectorStoreIndex function",
    "created_at": "2023-06-29T09:03:58Z",
    "closed_at": "2023-07-22T01:46:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6641",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nWhich vector db like chroma/pinecone/fiass ..etc is using internally in **VectorStoreIndex** function",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6641/comments",
    "author": "pradeepdev-1995",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-06-29T16:05:42Z",
        "body": "By default it's just a simple in-memory list. Very basic and simple to use.\r\n\r\nYou can change the vector store backend to use any other integration though"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-07-22T01:46:11Z",
        "body": "Going to close for now, feel free to reach out on discord if you have more questions!"
      }
    ]
  },
  {
    "number": 6624,
    "title": "[Question]:  AuthenticationError: No API key provided",
    "created_at": "2023-06-28T17:07:41Z",
    "closed_at": "2023-07-22T01:52:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6624",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\n**Description:**\r\nI encountered an issue while running the llama_index_server.py script. It seems that the OpenAI API key is not being properly utilized, resulting in an \"AuthenticationError: No API key provided\" exception.\r\n\r\n**Expected Behavior:**\r\nThe script should initialize the LLMPredictor with the provided OpenAI API key and continue running without any authentication errors.\r\n\r\n**Actual Behavior:**\r\nThe script raises an AuthenticationError with the message \"No API key provided.\"\r\n\r\n**Code:**\r\n\r\n```\r\n#  /server/llama_index_server.py\r\n\r\nfrom flask import Flask, request, Response\r\nfrom flask_cors import CORS\r\nfrom dotenv import load_dotenv\r\nimport os\r\nimport pandas as pd\r\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\r\nfrom llama_index import Document\r\nfrom langchain.chat_models import ChatOpenAI\r\nfrom llama_index import LLMPredictor\r\nfrom llama_index import ServiceContext\r\n\r\napp = Flask(__name__)\r\nCORS(app)\r\n\r\nload_dotenv()\r\n\r\n# Get the API key from the environment variable\r\napi_key = os.getenv('OPENAI_API_KEY')\r\n\r\n# Print the API key for verification\r\n# print(f\"API key: {api_key}\")\r\n\r\n# Loading documents from an Excel file\r\ndf = pd.read_excel('data/SupplierSample.xlsx')\r\n\r\n# Convert DataFrame rows into documents\r\n# Concatenate all column values into a single string for each row\r\ndocuments = [Document(text=' '.join(map(str, row.values))) for _, row in df.iterrows()]\r\n\r\n# Construct a simple vector index\r\nindex = GPTVectorStoreIndex(documents)\r\n\r\n# LLM that supports streaming\r\nllm_predictor = LLMPredictor(llm=ChatOpenAI(api_key=api_key, temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True))\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\r\n\r\n# Configure query engine to use streaming\r\nquery_engine = index.as_query_engine(streaming=True, similarity_top_k=1)\r\n\r\n@app.route('/api/query', methods=['POST'])\r\ndef query():\r\n    # Get the payload from the request\r\n    payload = request.json\r\n    \r\n    # Get the question from the messages in the payload\r\n    question = [m['content'] for m in payload['messages'] if m['role'] == 'user'][-1]\r\n    \r\n    # Now, query returns a StreamingResponse object\r\n    streaming_response = query_engine.query(question)\r\n\r\n    def response_stream():\r\n        for text in streaming_response.response_gen:\r\n            yield text + \"\\n\"\r\n\r\n    return Response(response_stream(), mimetype=\"text/event-stream\")\r\n\r\nif __name__ == '__main__':\r\n    # Start the server, to run this script use \"python llama_index_server.py\" in terminal\r\n    app.run(port=5000)\r\n```\r\n\r\n**Error Message:**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"llama_index_server.py\", line 33, in <module>\r\n    index = GPTVectorStoreIndex(documents)\r\n  ...\r\n  File \"llama_index\\embeddings\\openai.py\", line 150, in get_embeddings\r\n    ...\r\nopenai.error.AuthenticationError: No API key provided.\r\n```\r\n\r\n**Additional Information:**\r\n\r\n    The OPENAI_API_KEY environment variable is properly set and printed to the console before the error occurs.\r\n    The OpenAI API key is retrieved using os.getenv('OPENAI_API_KEY') and assigned to the api_key variable.\r\n    The LLMPredictor is initialized with the ChatOpenAI model, but the API key is not passed to the model.\r\n\r\n**Possible Solution:**\r\nTo resolve this issue, the retrieved API key (api_key) should be passed as an argument to the ChatOpenAI model during initialization. This can be done as follows:\r\n\r\n```\r\nllm_predictor = LLMPredictor(llm=ChatOpenAI(api_key=api_key, temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True))\r\n```\r\n\r\nPlease let me know if you need any further information or assistance with this issue.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6624/comments",
    "author": "SeloSlav",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-06-28T18:26:14Z",
        "body": "The correct kwarg is `openai_api_key=api_key`\r\n\r\nBut you are only passing it to the LLM, and not the embed model. Furthermore, you are not actually using the service context anywhere either ğŸ‘€ \r\n\r\nHere's a modified example that I think should work\r\n\r\n```python\r\n#  /server/llama_index_server.py\r\n\r\nfrom flask import Flask, request, Response\r\nfrom flask_cors import CORS\r\nfrom dotenv import load_dotenv\r\nimport os\r\nimport pandas as pd\r\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\r\nfrom llama_index import Document\r\nfrom langchain.chat_models import ChatOpenAI\r\nfrom llama_index import LLMPredictor\r\nfrom llama_index import ServiceContext\r\n\r\napp = Flask(__name__)\r\nCORS(app)\r\n\r\nload_dotenv()\r\n\r\n# Get the API key from the environment variable\r\nimport openai\r\napi_key = os.getenv('OPENAI_API_KEY')\r\nopenai.api_key = api_key\r\n\r\n# Print the API key for verification\r\n# print(f\"API key: {api_key}\")\r\n\r\n# Loading documents from an Excel file\r\ndf = pd.read_excel('data/SupplierSample.xlsx')\r\n\r\n# Convert DataFrame rows into documents\r\n# Concatenate all column values into a single string for each row\r\ndocuments = [Document(text=' '.join(map(str, row.values))) for _, row in df.iterrows()]\r\n\r\n# LLM that supports streaming\r\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True))\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\r\n\r\n# Construct a simple vector index\r\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\r\n\r\n# Configure query engine to use streaming\r\nquery_engine = index.as_query_engine(streaming=True, similarity_top_k=1)\r\n\r\n@app.route('/api/query', methods=['POST'])\r\ndef query():\r\n    # Get the payload from the request\r\n    payload = request.json\r\n    \r\n    # Get the question from the messages in the payload\r\n    question = [m['content'] for m in payload['messages'] if m['role'] == 'user'][-1]\r\n    \r\n    # Now, query returns a StreamingResponse object\r\n    streaming_response = query_engine.query(question)\r\n\r\n    def response_stream():\r\n        for text in streaming_response.response_gen:\r\n            yield text + \"\\n\"\r\n\r\n    return Response(response_stream(), mimetype=\"text/event-stream\")\r\n\r\nif __name__ == '__main__':\r\n    # Start the server, to run this script use \"python llama_index_server.py\" in terminal\r\n    app.run(port=5000)\r\n```"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-07-22T01:52:32Z",
        "body": "Going to close this for now. Feel free to reopen or reach out on discord!"
      }
    ]
  },
  {
    "number": 6593,
    "title": "httpx.ReadTimeout",
    "created_at": "2023-06-26T12:57:48Z",
    "closed_at": "2023-07-18T08:48:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6593",
    "body": null,
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6593/comments",
    "author": "vruti1121",
    "comments": [
      {
        "user": "dcp4ever",
        "created_at": "2024-01-03T22:00:03Z",
        "body": "I have a similar error. How did it get fixed?"
      }
    ]
  },
  {
    "number": 6447,
    "title": "[Question]: I want to ask about MongoDb Reader",
    "created_at": "2023-06-13T07:55:18Z",
    "closed_at": "2023-09-19T16:01:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6447",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nCurrently, I see that using SimpleMongoReader, we can only access a collection with specific field names. But my database has a lot of collections with a lot of field names. More, the collections has relationship with each other. How do I use SimpleMongoReader with my database? Or, is there any other method?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6447/comments",
    "author": "dinhan92",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-09-12T16:19:37Z",
        "body": "Hi, @dinhan92! I'm Dosu, and I'm here to help the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nFrom what I understand, you are asking about using the SimpleMongoReader with a database that has multiple collections with different field names and relationships. You are seeking guidance on how to handle this scenario or if there is an alternative method. \n\nSince there hasn't been any activity on this issue, we wanted to check if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on the issue. Otherwise, feel free to close the issue yourself or it will be automatically closed in 7 days. \n\nThank you for your understanding and we look forward to hearing from you soon!"
      }
    ]
  },
  {
    "number": 6445,
    "title": "[Question]: Can you create an index with one LLM and query using another",
    "created_at": "2023-06-13T07:32:40Z",
    "closed_at": "2023-07-22T02:11:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6445",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHi \r\n\r\nGreat tool btw, I was wondering if it is possible to create an index with one LLM and query using another. I'm specifically trying to reduce the cost for index creation by using a cheaper model for index creation, but I want the power of the more capable LLMs when responding to queries. \r\n\r\nYour assistance is much appreciated. \r\n\r\nKind regards",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6445/comments",
    "author": "Samshive",
    "comments": [
      {
        "user": "matthiaskern",
        "created_at": "2023-06-15T08:34:02Z",
        "body": "This is supported! You can specify different `service_context` instances for the different stages:\r\n\r\ne.g.:\r\n\r\n```\r\nindex = VectorStoreIndex.from_documents(docs, service_context=service_context_cheap)\r\nquery_engine = index.as_query_engine(service_context=service_context_expensive)\r\n```"
      },
      {
        "user": "Samshive",
        "created_at": "2023-06-15T16:36:14Z",
        "body": "Thanks, much appreciated."
      }
    ]
  },
  {
    "number": 6431,
    "title": "[Question]: How to config BeautifulSoupWebReader to get in other page inside a domain?",
    "created_at": "2023-06-12T08:22:13Z",
    "closed_at": "2023-09-18T16:01:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6431",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIt seems that BeautifulSoupWebReader or SimpleWebpageReader can only get data from first page, I have tried both but can not get data from the pages indside, for example pagination or the detail post inside a list of url in a page",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6431/comments",
    "author": "dinhan92",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-09-11T17:03:05Z",
        "body": "Hi, @dinhan92! I'm Dosu, and I'm here to help the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nFrom what I understand, you opened this issue to ask about configuring BeautifulSoupWebReader to retrieve data from pages within a domain. You mentioned that you have already searched the documentation and Discord for an answer. Currently, there hasn't been any activity or response on this issue. \n\nBefore we close this issue, we wanted to check with you if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on this issue. Otherwise, feel free to close the issue yourself, or it will be automatically closed in 7 days. \n\nThank you for your understanding and contribution to the LlamaIndex project!"
      }
    ]
  },
  {
    "number": 6415,
    "title": "[Question]: Why are Japanese characters being saved as escaped Unicode?",
    "created_at": "2023-06-11T06:00:11Z",
    "closed_at": "2023-10-24T06:28:59Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6415",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHello,\r\nI create saved a docstore in the way below.\r\nIt has texts written in unicode with escape, not in Japanese character(or UTF-8) which I want.\r\n\r\nHow can I solve the problem?\r\nThank you for your help.\r\n\r\n```\r\nCode:\r\nfrom langchain import text_splitter\r\nfrom llama_index.indices.loading import load_index_from_storage\r\nfrom llama_index import StorageContext\r\nfrom llama_index.storage.docstore import SimpleDocumentStore\r\nfrom llama_index.storage.index_store import SimpleIndexStore\r\nfrom llama_index.vector_stores import SimpleVectorStore\r\n\r\nfrom llama_index import ServiceContext\r\nfrom llama_index.node_parser import SimpleNodeParser\r\nfrom llama_index.embeddings.openai import OpenAIEmbedding\r\nfrom llama_index import LLMPredictor\r\nfrom llama_index.indices.prompt_helper import PromptHelper\r\nfrom llama_index.logger.base import LlamaLogger\r\nfrom llama_index.callbacks.base import CallbackManager\r\n\r\nfrom llama_index import VectorStoreIndex\r\nfrom llama_index import ResponseSynthesizer\r\nfrom llama_index.optimization.optimizer import SentenceEmbeddingOptimizer\r\nfrom llama_index import SimpleDirectoryReader\r\nfrom llama_index.indices.vector_store.retrievers import VectorIndexRetriever\r\nfrom llama_index.vector_stores.types import VectorStoreQueryMode\r\nfrom llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\r\nfrom llama_index import LLMPredictor\r\n\r\nfrom llama_index import GPTListIndex\r\nfrom llama_index.llm_predictor.chatgpt import ChatOpenAI\r\n\r\ndocuments = SimpleDirectoryReader('data').load_data()\r\n\r\nllm_predictor = ChatGPTLLMPredictor(\r\n    llm=ChatOpenAI(\r\n        temperature=0,\r\n        model_name='gpt-3.5-turbo',\r\n        streaming=True\r\n        )\r\n)\r\n\r\n'''\r\nstorage_context = StorageContext.from_defaults(\r\n    docstore=SimpleDocumentStore(),\r\n    vector_store=SimpleVectorStore(),\r\n    index_store=SimpleIndexStore()\r\n)\r\n\r\nfrom llama_index.langchain_helpers.text_splitter import TokenTextSplitter\r\nfrom llama_index.constants import DEFAULT_CHUNK_OVERLAP, DEFAULT_CHUNK_SIZE\r\nimport tiktoken\r\n\r\ntext_splitter = TokenTextSplitter(separator=\" \", chunk_size=DEFAULT_CHUNK_SIZE\r\n    , chunk_overlap=DEFAULT_CHUNK_OVERLAP\r\n    , tokenizer=tiktoken.get_encoding(\"cl100k_base\").encode)\r\n\r\nservice_context = ServiceContext.from_defaults(\r\n    node_parser=SimpleNodeParser(text_splitter=text_splitter),\r\n    embed_model=OpenAIEmbedding(),\r\n    llm_predictor=llm_predictor,\r\n    prompt_helper=PromptHelper.from_llm_metadata(llm_predictor.get_llm_metadata()),\r\n    llama_logger=LlamaLogger(),\r\n    callback_manager=CallbackManager([])\r\n)\r\n\r\nindex.set_index_id(\"test\")\r\nindex.storage_context.persist(\"./storage\")\r\n```\r\n\r\ndocstore\r\n{\"docstore/metadata\":...},` \"docstore/data\": {\"...\": {\"__data__\": {\"text\": \"\\u6c11\\u4e8b\\u88c1\\u5224\\u306b\\u304a\\u3044\\u3066...\", \"doc_id\": ...\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6415/comments",
    "author": "tmforai60",
    "comments": [
      {
        "user": "Disiok",
        "created_at": "2023-06-11T07:56:09Z",
        "body": "Why do you need to inspect the docstore? We don't do any special encoding when saving to disk."
      },
      {
        "user": "quarhodron",
        "created_at": "2023-07-17T16:06:49Z",
        "body": "> Why do you need to inspect the docstore? We don't do any special encoding when saving to disk.\r\n\r\nI have the same question because in my docstore Norwegian diacritic characters are also Unicode escaped. I didn't specify any parameter that could cause this. Docstore is UTF-8 encoded so I don't know why they are escaped?\r\nIt's mainly the question if it has any negative impact on the data retrieval process - so if unicode escapes are converted back to the right characters before analysis?\r\nBut I think it would be nice if we could have docstore in UTF-8 characters."
      },
      {
        "user": "bl-taira-m",
        "created_at": "2023-07-18T00:57:13Z",
        "body": "There is a way to avoid Unicode escaping, by giving the argument ensure_ascii=False to json.dump(). However,  I believe that there would be no negative impacts as it is."
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2023-10-17T16:01:51Z",
        "body": "Hi, @tmforai60! I'm Dosu, and I'm helping the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nFrom what I understand, the issue you raised is about Japanese characters being saved as escaped Unicode instead of UTF-8. There have been some discussions in the comments about inspecting the docstore and the impact of Unicode escapes on data retrieval. One user suggested using the argument `ensure_ascii=False` to avoid Unicode escaping. \n\nBefore we close this issue, we wanted to check with you if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let the LlamaIndex team know by commenting on this issue. Otherwise, feel free to close the issue yourself, or it will be automatically closed in 7 days. \n\nThank you for your contribution to LlamaIndex!"
      }
    ]
  },
  {
    "number": 6384,
    "title": "[Question]: Want to ask how to support the streaming of multi-document question",
    "created_at": "2023-06-10T02:09:46Z",
    "closed_at": "2023-06-10T02:30:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6384",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI want to implement multi-document streaming answers, I use SubQuestionQueryEngine, I found a built-in parameter of this method response_synthesizer, set ResponseSynthesizer.from_args(streaming=Trueï¼‰It seems to work, but it still reports ValueError: LLM must support streaming and set streaming=True. I tried to use service_context to solve this problem, but it didn't work",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6384/comments",
    "author": "MonsterZwj",
    "comments": [
      {
        "user": "MonsterZwj",
        "created_at": "2023-06-10T02:29:38Z",
        "body": "I was happy to solve the problem myself\r\n "
      },
      {
        "user": "dinhan92",
        "created_at": "2023-06-13T08:50:00Z",
        "body": "@MonsterZwj how do you solve it, I get that error too ?"
      }
    ]
  },
  {
    "number": 6278,
    "title": "[Question]: why fetch the nodes is return None by ResponseMode.NO_TEXT",
    "created_at": "2023-06-09T10:13:59Z",
    "closed_at": "2023-06-12T04:21:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6278",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nWhy does llama return None when response_mode is no_text?\r\n`       service_context = create_service_context()\r\n        index = self.embedding.load_index_simple(game_id, service_context)\r\n        retriever = index.as_retriever(similarity_top_k=15)\r\n        engine = RetrieverQueryEngine.from_args(retriever, service_context,\r\n                                                response_mode=ResponseMode.NO_TEXT)\r\n`\r\ni saw the nodes was print, but it not return",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6278/comments",
    "author": "youbai1995",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-06-09T15:04:12Z",
        "body": "the nodes are in the response object\r\n\r\n```python\r\nresponse = index.as_query_engine(response_mode=\"no_text\").query(\"query\")\r\nprint(response.source_nodes)\r\n```"
      },
      {
        "user": "youbai1995",
        "created_at": "2023-06-12T03:14:31Z",
        "body": "> thank you very much! the document need update ^.^\r\n\r\n"
      }
    ]
  },
  {
    "number": 6222,
    "title": "[Question]: How can the extra_info metadata be used for query responses?",
    "created_at": "2023-06-08T14:28:21Z",
    "closed_at": "2023-06-13T14:39:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6222",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHey, do we need to explicitly add a parameter or something during retrieval or querying to use the extra_info metadata?\r\n\r\nFor example, if I have the metadata `filename`, and the query is:\r\n\r\n`What is the topic of the <filename> document ?`\r\n\r\nis the retrieval going to use metadata to return nodes / reply to the question?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6222/comments",
    "author": "YasmineMh",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-06-08T15:23:06Z",
        "body": "Actually, the metadata is inserted into the node text, so the embeddings and query should be interacting with it"
      },
      {
        "user": "YasmineMh",
        "created_at": "2023-06-09T09:09:04Z",
        "body": "Thanks for your response!\r\n\r\nThe embeddings and query should be interacting with it directly without we explicitly activate that?\r\n\r\nOtherwise, when I retrieve a chunk, I get `extra_info={}` even though I have metadata in my database.\r\n\r\nI created my nodes like that:\r\n\r\n`node = Node(chunk, extra_info={\"filename\": \"filename\"}, node_info={'start': 0, 'end': 102})`\r\n\r\nFrom database:\r\n\r\n```\r\n    {\r\n      \"class\": \"....\",\r\n      \"creationTimeUnix\": .....,\r\n      \"id\": \"....\",\r\n      \"lastUpdateTimeUnix\": ....,\r\n      \"properties\": {\r\n        \"doc_id\": \"....\",\r\n        \"document_id\": \"None\",\r\n        \"filename\": \"filename\",\r\n        \"node_info\": \"{\\\"start\\\": 0, \\\"end\\\": 102}\",\r\n        \"ref_doc_id\": \"None\",\r\n        \"relationships\": \"{\\\"2\\\": \\\"....\\\", \\\"3\\\": \\\"....\\\"}\",\r\n        \"text\": \".....\"\r\n      },\r\n      \"vectorWeights\": null\r\n    },\r\n```\r\n\r\n\r\nI use this code to retrieve the chunk:\r\n```\r\n# import index\r\nindex = GPTVectorStoreIndex([], storage_context=storage_context, service_context=service_context)\r\n# configure retriever\r\nretriever = VectorIndexRetriever(\r\n    index=index, \r\n    similarity_top_k=1\r\n)\r\nretrieved_nodes = retriever.retrieve(question)\r\nprint(retrieved_nodes)\r\n\r\n```\r\n\r\nOutput:\r\n`[NodeWithScore(node=Node(text=\".....\", doc_id='....', embedding=[....], doc_hash='....', extra_info={}, node_info={'start': 0, 'end': 102}, relationships={<DocumentRelationship.PREVIOUS: '2'>: '....', <DocumentRelationship.NEXT: '3'>: '....'}), score=None)]`\r\n\r\nI tested with \"0.6.15\" then \"0.6.21.post1\" versions, and got the same thing.\r\n\r\nThanks!"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-06-09T21:06:22Z",
        "body": "Are you using a specific vector db? Or just the base one? Can I see how you created the index/nodes?"
      },
      {
        "user": "YasmineMh",
        "created_at": "2023-06-12T11:36:25Z",
        "body": "I'm using weaviate as a vector db.\r\n\r\nIndex:\r\n\r\n```\r\nfrom llama_index import (\r\n    GPTVectorStoreIndex,\r\n    OpenAIEmbedding,\r\n    ServiceContext,\r\n    StorageContext,\r\n)\r\nfrom llama_index.data_structs.node import DocumentRelationship, Node\r\nfrom llama_index.vector_stores import WeaviateVectorStore\r\nimport weaviate\r\nclient = weaviate.Client(url=WEAVIATE_URN)\r\nclass_prefix = \"Test\"\r\nembed_model = OpenAIEmbedding()\r\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\r\nvector_store = WeaviateVectorStore(weaviate_client=client, class_prefix=class_prefix)\r\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\r\nnodes = []\r\nnode = Node(\"some text\", extra_info={\"filename\":\"filename\"}, node_info={'start': 0, 'end': 102})\r\nnodes.append(node)\r\nindex = GPTVectorStoreIndex(nodes, storage_context=storage_context, service_context=service_context)\r\n```\r\n\r\nQuery:\r\n\r\n```\r\nfrom llama_index import (\r\n    GPTVectorStoreIndex,\r\n    OpenAIEmbedding,\r\n    ServiceContext,\r\n    StorageContext,\r\n)\r\nfrom llama_index.vector_stores import WeaviateVectorStore\r\nfrom llama_index.retrievers import VectorIndexRetriever\r\nimport weaviate\r\nclient = weaviate.Client(url=WEAVIATE_URN)\r\nclass_prefix = \"Test\"\r\nembed_model = OpenAIEmbedding()\r\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\r\nvector_store = WeaviateVectorStore(weaviate_client=client, class_prefix=class_prefix)\r\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\r\nindex = GPTVectorStoreIndex([], storage_context=storage_context, service_context=service_context)\r\nretriever = VectorIndexRetriever(\r\n    index=index, \r\n    similarity_top_k=1\r\n)\r\nretrieved_nodes = retriever.retrieve(\"text\")\r\nprint(retrieved_nodes)\r\n```\r\n"
      },
      {
        "user": "YasmineMh",
        "created_at": "2023-06-13T14:39:00Z",
        "body": "I retested with one of the latest versions `0.6.24` and it's working now"
      }
    ]
  },
  {
    "number": 6185,
    "title": "[Question]: Injecting context to SqlDatabase, context is not stored locally",
    "created_at": "2023-06-07T16:45:00Z",
    "closed_at": "2023-09-14T16:12:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6185",
    "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nI am trying to inject additional context related to sql database tables and trying to store the index created locally. The index files stored do not have additional context getting stored.\r\n\r\n`def create_index(selected_tables, table_contexts, model_option, model_temperature, index_name):\r\n    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\r\n    # Set up Azure OpenAI chat model\r\n    llm = AzureChatOpenAI(\r\n        model=model_option,\r\n        deployment_name=model_option,\r\n        temperature=model_temperature,\r\n        max_tokens=512,\r\n        openai_api_base=openai.api_base,\r\n        openai_api_key=AZURE_OPENAI_KEY,\r\n        openai_api_version=AZURE_OPENAI_API_VERSION\r\n    )\r\n    llm_predictor = LLMPredictor(llm=llm)\r\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=st.session_state[\"embedding_llm\"])\r\n    \r\n    sql_database = SQLDatabase(st.session_state[\"engine\"], include_tables=selected_tables, view_support=True)\r\n    st.session_state.index_sql_db_map[index_name] = sql_database\r\n    if table_contexts is not None and len(table_contexts)>0:\r\n        # Retrieve additional table context text boxes input\r\n        additional_contexts = {}\r\n        for table in selected_tables:\r\n            if table in table_contexts and table_contexts[table]:\r\n                additional_contexts[table] = [Document(table_contexts[table])]\r\n\r\n        context_builder = SQLContextContainerBuilder.from_documents(\r\n            additional_contexts, \r\n            sql_database,\r\n            service_context=service_context\r\n        )\r\n        context_container = context_builder.build_context_container()\r\n        index = GPTSQLStructStoreIndex.from_documents(\r\n            [],\r\n            sql_database=sql_database,\r\n            sql_context_container=context_container,\r\n            service_context=service_context,\r\n            storage_context=storage_context\r\n        )\r\n    else:\r\n        index = GPTSQLStructStoreIndex.from_documents(\r\n            [],\r\n            sql_database=sql_database,\r\n            service_context=service_context,\r\n            storage_context=storage_context\r\n        )\r\n    storage_context.persist(persist_dir=f'./storage/streamlit_indexes/llama/'+index_name)\r\n    st.session_state.index_id_map[index_name] = index.index_id\r\n    st.session_state.index_chain_map[index_name] = initialize_chain(model_temperature, index)\r\n    return index`",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6185/comments",
    "author": "sumanthp",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-06-07T17:29:11Z",
        "body": "actually, `persist()` doesn't really work for SQL indexes, since everything is stored in the DB for the most part\r\n\r\nI would hardcode the table descriptions personally"
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2023-09-06T16:41:09Z",
        "body": "Hi, @sumanthp! I'm Dosu, and I'm helping the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nFrom what I understand, you were facing a problem with injecting additional context to a SQL database and storing the index locally. It seems that the issue has been resolved, as logan-markewich suggested hardcoding the table descriptions instead of using `persist()` since SQL indexes store everything in the database. \n\nBefore we close this issue, we wanted to check with you if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on the issue. Otherwise, feel free to close the issue yourself or it will be automatically closed in 7 days. \n\nThank you for your contribution to the LlamaIndex repository!"
      }
    ]
  },
  {
    "number": 6120,
    "title": "GPTVectorStoreIndex gives me error - ValueError: One of documents or index_struct must be provided.",
    "created_at": "2023-06-06T19:58:19Z",
    "closed_at": "2023-06-06T21:12:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6120",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\n\r\nI want to train LLM on custom dataset. This is my code:\r\n\r\n```\r\nfrom llama_index import SimpleDirectoryReader, GPTListIndex, GPTVectorStoreIndex, LLMPredictor, PromptHelper\r\nfrom langchain import OpenAI\r\nimport os\r\n\r\napikey = \"sk-fzDu2MtAdhPiR\"\r\nos.environ[\"OPENAI_API_KEY\"] = apikey\r\n\r\n\r\ndef createVectorIndex(path):\r\n    \r\n    max_input = 4000\r\n    tokens = 256\r\n    chunk_size = 600\r\n    max_chunk_overlap = 0.20\r\n    \r\n    prompt_helper = PromptHelper(max_input, tokens, max_chunk_overlap, chunk_size_limit = chunk_size)\r\n    \r\n    #define LLM\r\n    llm_predictor = LLMPredictor(llm=OpenAI(temperature = 0, model_name = \"text-ada-001\", max_tokens = tokens))\r\n    \r\n    #load data\r\n    data = SimpleDirectoryReader(path).load_data()\r\n\r\n    vector_index = GPTVectorStoreIndex(documents = data, \r\n                                       llm_predictor = llm_predictor, \r\n                                       prompt_helper = prompt_helper, \r\n                                       index_name=\"my_index\")\r\n    \r\n    vector_index.save_to_disc(\"vector_index.json\")\r\n    \r\n    return vector_index=\r\n\r\nvector_index = createVectorIndex(\"gpt_data\")\r\n```\r\n\r\n\r\nI am getting this error:\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-39-de1d095ec55f> in <module>\r\n----> 1 vector_index = createVectorIndex(\"gpt_data\")\r\n\r\n<ipython-input-38-f5b73669d881> in createVectorIndex(path)\r\n     17         print(i)\r\n     18 \r\n---> 19     vector_index = GPTVectorStoreIndex(documents = data, \r\n     20                                        llm_predictor = llm_predictor,\r\n     21                                        prompt_helper = prompt_helper,\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py in __init__(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, **kwargs)\r\n     43         self._use_async = use_async\r\n     44         self._store_nodes_override = store_nodes_override\r\n---> 45         super().__init__(\r\n     46             nodes=nodes,\r\n     47             index_struct=index_struct,\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/llama_index/indices/base.py in __init__(self, nodes, index_struct, storage_context, service_context, **kwargs)\r\n     43         \"\"\"Initialize with parameters.\"\"\"\r\n     44         if index_struct is None and nodes is None:\r\n---> 45             raise ValueError(\"One of documents or index_struct must be provided.\")\r\n     46         if index_struct is not None and nodes is not None:\r\n     47             raise ValueError(\"Only one of documents or index_struct can be provided.\")\r\n\r\nValueError: One of documents or index_struct must be provided.\r\n```\r\n\r\n\r\nI have tried with and without index_name=\"my_index\", error is the same.\r\n\r\n\"gpt_data\" is folder with 1 txt file.\r\n\r\nThis line:\r\n\r\ndata = SimpleDirectoryReader(path).load_data()\r\nGives me this:\r\n\r\n[Document(text=\"Kimberlites are ... m thermochemica, doc_id='5bbe7a75-7817-4c00-bce1-db9256fde270', embedding=None, doc_hash='87fb32e4834c847365178498a6ce166a68df5d5547c8631df46d577616dd37e8', extra_info=None)]\r\n\r\nSo I manage to read the document, i added it to data variable, but GPTVectorStoreIndex treat it like an empty.\r\n\r\nI am using:\r\n\r\nVersion: 0.6.20\r\n\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6120/comments",
    "author": "aleksandar-devedzic",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-06-06T21:12:54Z",
        "body": "I think we were discussing this on discord\r\n\r\nThe correct usage is as follows:\r\n\r\n```python\r\nfrom llama_index import GPTVectorStoreIndex, ServiceContext\r\n...\r\nservice_context = ServiceContext.from_defualts(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\r\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context) \r\n```\r\n\r\nGoing to close this for now!"
      }
    ]
  },
  {
    "number": 6111,
    "title": "[Question]: Is it possible to insert documents one by one into a ComposableGraph?",
    "created_at": "2023-06-06T13:17:55Z",
    "closed_at": "2023-06-07T00:25:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6111",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIs it possible to insert documents one by one into a ComposableGraph?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6111/comments",
    "author": "YasmineMh",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-06-06T14:50:31Z",
        "body": "Not quite. Since a graph is made up of several sub-indexes, each of those sub-indexes usually contains a specific subset of documents.\r\n\r\nYou'd have to pick which sub-index to insert into ahead of time.\r\n\r\nAlthough auto-inserting is an interesting idea to explore ğŸ¤”"
      },
      {
        "user": "YasmineMh",
        "created_at": "2023-06-06T15:02:02Z",
        "body": "Thanks for your response!\r\n\r\nAnd can I insert a new sub-index? in my case instead of loading all documents at once and index them, then create the graph based on those indexes and their summaries, I'm loading document by document, then I want to index it and add it to the graph, not sure if I can do that"
      },
      {
        "user": "logan-markewich",
        "created_at": "2023-06-07T00:25:00Z",
        "body": "You'd have to re-create the graph in order to add a new sub-index ğŸ¤” If this is the approach you want to take, I would save each index individually and then build the graph at runtime.\r\n\r\nDefinitely, the operations around the graph could be better supported ğŸ™‚\r\n\r\nGoing to close this issue since I think we were also chatting on discord. Feel free to reach out anytime over there!"
      },
      {
        "user": "YasmineMh",
        "created_at": "2023-06-07T07:52:19Z",
        "body": "yeah sure, thank you!\r\n"
      }
    ]
  },
  {
    "number": 6089,
    "title": "[Question]: how to get index from ChromaReader?",
    "created_at": "2023-06-06T09:37:34Z",
    "closed_at": "2023-07-22T18:38:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/6089",
    "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nhow to get index from ChromaReader?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6089/comments",
    "author": "vegetax",
    "comments": [
      {
        "user": "logan-markewich",
        "created_at": "2023-06-07T00:25:55Z",
        "body": "What's the exact use-case here? Did you create an index on chroma with llama index and want to re-connect to it?"
      }
    ]
  },
  {
    "number": 4908,
    "title": "API Documentation in openapi 3.0.0 format",
    "created_at": "2023-06-03T16:53:49Z",
    "closed_at": "2023-09-14T16:12:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/4908",
    "body": "I'm trying to understand how well the model understands an API specification document written in OpenAPI 3.0.0 (JSON) format and how complex the questions I can ask can be. Given my input, the model replies with a sequence of calls that should lead to the final result. The difficulty of dealing with documents in the OpenAPI 3.0.0 format, in my opinion, is that the \"retriever\" mechanism, made available, for example, on LangChain is not very efficient because the definition of the endpoints is found at the beginning of the document, and the definition of the components (return types, parameters schema, classes and other resuable schemas), however, is at the end of the document, so one or two (truncated) chunks cannot simply be enough, but the query should be repeated on all the chunks, as for the \"refine\" strategy offered by LlamaIndex. However I notice that the model fails to answer simple questions like \"what is the list of all available endpoints?\", or it tends to hallucinate even if the temperature is zero, as if it doesn't have access to all parts of the document.\r\nI understand that the concept of \"retriever\" works perfectly with a knowledge base of documents, as the question can be answered simply by looking at some \"chunks\" of the documents, but in the case of API documentations, especially those written in the OpenAPI format 3.0.0, it may always be necessary to submit the entire document to the LLM before response. \r\nIn these cases, what type of index and what strategy do you recommend me to adopt, to improve the understanding of the documentation as much as possible and push the reasoning of the model to the maximum? Do you also think that OpenAPI is not a suitable format here, and maybe I should create of a more compressed format, with a custom JSON schema, or maybe an explaination of the API in plain english? \r\n\r\nThis is the main part of the code, it works good, but I feel it's not giving the model all the informations.\r\n\r\n```\r\nJSONReader = download_loader(\"JSONReader\")\r\nloader = JSONReader()\r\ndocuments = loader.load_data('./json-data/openapi-docs.json')\r\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0.0, model_name=\"gpt-3.5-turbo\", max_tokens=512))\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size=512)\r\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\r\n\r\n#retriever = index.as_retriever()\r\n#query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='refine')\r\nquery_engine = index.as_query_engine()\r\nuser_question = input('> ')\r\nwhile user_question != \"quit\":\r\n    response = query_engine.query(\"\"\"You are an API call expert virtual assistant for the XYZ project.\r\n         Your job is to compose sequences of API calls to answer the user's question.\r\n         You must answer only with the sequence of calls necessary to obtain the result.\r\n         You can only use the endpoints in the documentation, if you don't find a suitable endpoint explain which endpoint you would like to use and why.\r\n         You can only use one value with the \"explode\" parameter.\r\n\r\n        Let's begin!\r\n\r\n        Question: {user_question}\r\n        Call sequence:\"\"\".format(user_question=user_question))\r\n    print(response)\r\n    user_question = input('> ')\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/4908/comments",
    "author": "labbitroma",
    "comments": [
      {
        "user": "dosubot[bot]",
        "created_at": "2023-09-06T16:40:59Z",
        "body": "Hi, @labbitroma! I'm Dosu, and I'm here to help the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nFrom what I understand, you opened this issue to improve a model's comprehension of API documentation written in OpenAPI 3.0.0 format. You were seeking recommendations on the type of index and strategy to adopt to enhance the model's understanding and reasoning. Additionally, you questioned whether OpenAPI is the best format or if a more compressed format or plain English explanation would be more suitable. \n\nSince there hasn't been any activity or comments on this issue, we wanted to check with you if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on the issue. Otherwise, feel free to close the issue yourself, or it will be automatically closed in 7 days. \n\nThank you for your understanding, and please don't hesitate to reach out if you have any further questions or concerns."
      }
    ]
  },
  {
    "number": 801,
    "title": "How to clarify the meaning of a schema column in structured data",
    "created_at": "2023-03-20T09:43:15Z",
    "closed_at": "2023-03-26T01:37:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/801",
    "body": "I am using mysql as a structured data source and sometimes the query does not understand exactly what the column means, resulting in an undesirable response, here is my code.\r\n\r\n```\r\nfrom pathlib import Path\r\nfrom llama_index import GPTSQLStructStoreIndex, SQLDatabase\r\nfrom sqlalchemy import create_engine\r\n\r\nengine = create_engine(\"mysql+mysqlconnector://root:123456@localhost:3306/xxx\")\r\nsql_database = SQLDatabase(engine, include_tables=[\"t_goods\"])\r\nindex = GPTSQLStructStoreIndex(\r\n    [], \r\n    sql_database=sql_database, \r\n    table_name=\"t_goods\",\r\n)\r\n\r\nresponse = index.query(\"What is the weight and physical dimensions of the WDM50-2805SL? Please answer in Chinese\")\r\nprint(response)\r\n```\r\n\r\nThe table t_goods has the columns model, weight, size. Mass and weight are the same field, and also commented on the weight column: weight or mass, but he cannot identify or determine that weight is a mass value, only that it is a weight.\r\n",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/801/comments",
    "author": "shangdev",
    "comments": [
      {
        "user": "Disiok",
        "created_at": "2023-03-25T02:32:50Z",
        "body": "Hey @shangdev, did you try improving the query string with the information you describe? Or improving the prompt template to better give this information to the LLM? "
      },
      {
        "user": "shangdev",
        "created_at": "2023-03-25T02:50:31Z",
        "body": "@Disiok when i add sql_context_container, he return correct result.\r\n\r\n```\r\n# manually set text\r\ntable_context_dict={\"t_goods\": \"å­—æ®µweightè¡¨ç¤ºé‡é‡æˆ–è´¨é‡ï¼Œå­—æ®µspecè¡¨ç¤ºäº§å“å‹å·ï¼Œå¦‚ï¼šxxx\"}\r\ncontext_builder = SQLContextContainerBuilder(sql_database, context_dict=table_context_dict)\r\ncontext_container = context_builder.build_context_container()\r\n\r\nindex = GPTSQLStructStoreIndex(\r\n    [], \r\n    sql_database=sql_database, \r\n    table_name=\"t_goods\",\r\n    sql_context_container=context_container\r\n)\r\n```\r\n\r\nresult is\r\n\r\n```\r\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 693 tokens\r\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\r\n[('95gÂ±2g', '76.2Â±0.2,38Â±0.2,10.5Â±0.2,6.35Â±0.2,1.02Â±0.05')]\r\nSELECT weight, size FROM t_goods WHERE spec = 'xxx';\r\n```\r\n\r\nWhat I want to know is why he returns not a semantic text like \"weight is 95gÂ±2g, size is 76.2Â±0.2,38Â±0.2,10.5Â±0.2,6.35Â±0.2,1.02Â±0.05\", but also a result like [('x','y')]"
      },
      {
        "user": "jerryjliu",
        "created_at": "2023-03-25T16:22:20Z",
        "body": "> @Disiok when i add sql_context_container, he return correct result.\r\n> \r\n> ```\r\n> # manually set text\r\n> table_context_dict={\"t_goods\": \"å­—æ®µweightè¡¨ç¤ºé‡é‡æˆ–è´¨é‡ï¼Œå­—æ®µspecè¡¨ç¤ºäº§å“å‹å·ï¼Œå¦‚ï¼šxxx\"}\r\n> context_builder = SQLContextContainerBuilder(sql_database, context_dict=table_context_dict)\r\n> context_container = context_builder.build_context_container()\r\n> \r\n> index = GPTSQLStructStoreIndex(\r\n>     [], \r\n>     sql_database=sql_database, \r\n>     table_name=\"t_goods\",\r\n>     sql_context_container=context_container\r\n> )\r\n> ```\r\n> \r\n> result is\r\n> \r\n> ```\r\n> INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 693 tokens\r\n> INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\r\n> [('95gÂ±2g', '76.2Â±0.2,38Â±0.2,10.5Â±0.2,6.35Â±0.2,1.02Â±0.05')]\r\n> SELECT weight, size FROM t_goods WHERE spec = 'xxx';\r\n> ```\r\n> \r\n> What I want to know is why he returns not a semantic text like \"weight is 95gÂ±2g, size is 76.2Â±0.2,38Â±0.2,10.5Â±0.2,6.35Â±0.2,1.02Â±0.05\", but also a result like [('x','y')]\r\n\r\nadding a semantic response and not just the raw SQL response is something that's in the works :) "
      }
    ]
  },
  {
    "number": 793,
    "title": "Reading PDF file from cloud storage (GCS)",
    "created_at": "2023-03-19T23:52:20Z",
    "closed_at": "2023-03-21T17:21:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/793",
    "body": "Hi there,\r\n\r\nCame across `SimpleDirectoryReader` which lets you read in a file, given the file path. \r\n\r\nI have files that I store on cloud (Google Cloud Storage) that I fetch from. Is there a way to use `SimpleDirectoryReader` without the path that's required, but instead provide the file itself?\r\n\r\nCurrently I workaround it with the following by parsing myself and providing the documents to `GPTSimpleVectorIndex`.",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/793/comments",
    "author": "ctle-vn",
    "comments": [
      {
        "user": "jerryjliu",
        "created_at": "2023-03-21T17:21:06Z",
        "body": "@ctle-vn do you mean providing the file name instead of folder name? yeah you can do that by specifying input_files kwarg e.g. \r\n\r\n```\r\nreader = SimpleDirectoryReader(input_files=[\"path/to/downloaded/file.ext\"]).load_data()\r\n```"
      },
      {
        "user": "jerryjliu",
        "created_at": "2023-03-21T17:21:23Z",
        "body": "going to close issue for now but feel free to reopen if further bugs/questions "
      },
      {
        "user": "ctle-vn",
        "created_at": "2023-03-21T18:02:52Z",
        "body": "Hey @jerryjliu thanks for the reply.\r\n\r\nMore like having the actual file in memory, this is the function header:\r\n\r\n`def generate_embeddings_with_pdf_filer(file: UploadFile) -> None:`\r\n\r\nam i able to pass the file directly in?"
      },
      {
        "user": "jerryjliu",
        "created_at": "2023-03-21T18:04:43Z",
        "body": "> Hey @jerryjliu thanks for the reply.\r\n> \r\n> More like having the actual file in memory, this is the function header:\r\n> \r\n> `def generate_embeddings_with_pdf_filer(file: UploadFile) -> None:`\r\n> \r\n> am i able to pass the file directly in?\r\n\r\nAh got it. At the moment no :/  we do have an s3 reader on llamahub, but that also just downloads files to a temp directory and uses SimpleDirectoryReader too "
      },
      {
        "user": "ctle-vn",
        "created_at": "2023-03-22T03:45:22Z",
        "body": "Got it got it, thanks for the reply!"
      }
    ]
  },
  {
    "number": 413,
    "title": "How to compare two documents?",
    "created_at": "2023-02-09T07:01:22Z",
    "closed_at": "2023-09-12T16:21:16Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/413",
    "body": "Hi there, I am trying to compare two documents and hope to leverage gpt_index, any ideas on how to implement such functionality? thanks a lot! ",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/413/comments",
    "author": "SoulEvill",
    "comments": [
      {
        "user": "jerryjliu",
        "created_at": "2023-02-15T23:51:56Z",
        "body": "by compare documents do you mean a line-by-line comparison? or comparing summaries? depending on your use case some of these functionalities may already be available in gpt index "
      },
      {
        "user": "Emasoft",
        "created_at": "2023-02-23T14:41:47Z",
        "body": "Same here. The comparison I need are various.  \r\n\r\nFor example: \r\n\"Which document has more words?\"\r\n\"Which document has more sentences?\"\r\n\"Which document uses more arcaic english words?\"\r\n\"Which documents uses more times the sentence 'language model'?\"\r\n\"Which document has more references to external documents?\"\r\n\"Which document has more stanzas in its poems?\"\r\n\"Which document has more poems with rhyme scheme AABB?\"\r\n\"Which document has more Limericks?\"\r\n\"Which document has more Haikus?\"\r\n\"Which document makes more use of onomatopeias?\"\r\n\"Compare meter, rhyme scheme and stanzas forms of the two documents.\"\r\n\"Compare the vocabulary of the two documents and use it to estimate what is the oldest and in what century it was composed.\"\r\n\"Compare the documents and determine the one that has the greater number of occurrences of foreign words.\"\r\n\"Compare the two novels and determine which has more different Point of Views in the narrative.\"\r\n\"Compare the two novels and determine which has more explicit sex scenes.\"\r\n\"Compare the two novels and determine which has more explicit scenes of violence.\"\r\n\"Compare the two novels and determine which has more frequent use of foul language and of swear, obscene or racist expressions.\"\r\n\"Compare the two novels and determine which has more Shakespear's quotes.\"\r\n\"Compare the two novels and determine which has more soliloquies.\"\r\n\r\nAlso, it should be possible to compare more than 2 documents.  \r\n\r\nFor example:\r\n\"Compare all the short novels provided and determine if there are more short novels with a male protagonist or with a female protagonist.\"\r\n\"Rank all the short novels according to the number of different occurrences of foul language and racist expressions.\"\r\n\"Rank all the short novels according to the number of different female characters with at least one sentence of dialog.\"\r\n"
      },
      {
        "user": "iraadit",
        "created_at": "2023-03-12T22:38:46Z",
        "body": "Hi, I'm also interested by the comparison of several documents.\r\n\r\nDocument would be about the same subject but for different locations / products,...\r\n\r\nBy example, for documents about different countries of the world:\r\n\"Between France and UK, which has the highest GDP?\"\r\n\"Which country has the highest proportion of male population?\"\r\n\"Which countries are presidencies, and which are kingdoms?\"\r\n\"What are the political differences between the USA and Russia?\"\r\n\r\nI want to also be able to get answers from a specific document, without mixing with information from other documents.\r\nBy example,\r\n\"What is the average height of the female population in Netherlands?\"\r\n\"What is the population pyramid of South Africa?\"\r\n\r\nThanks\r\n\r\n"
      },
      {
        "user": "robertsilen",
        "created_at": "2023-04-12T10:57:39Z",
        "body": "I would be interested in this too. Does llama_index retrieve an answer from only one document at a time?"
      },
      {
        "user": "DrShrinker",
        "created_at": "2023-04-12T20:39:21Z",
        "body": "I'd like to do something similar - I have a large book that I parsed using SimpleNodeParser, and I would like Chat GPT to review a term paper to check if it accurately discusses the book."
      },
      {
        "user": "dosubot[bot]",
        "created_at": "2023-09-05T16:41:33Z",
        "body": "Hi, @SoulEvill! I'm Dosu, and I'm here to help the LlamaIndex team manage their backlog. I wanted to let you know that we are marking this issue as stale. \n\nBased on my understanding, you are seeking guidance on how to compare two documents using gpt_index. Several other users, including jerryjliu, Emasoft, iraadit, robertsilen, and DrShrinker, have also expressed interest in similar functionality and have provided examples of the types of comparisons they would like to make. However, no solution has been provided yet.\n\nBefore we close this issue, we wanted to check with you if it is still relevant to the latest version of the LlamaIndex repository. If it is, please let us know by commenting on the issue. Otherwise, feel free to close the issue yourself, or it will be automatically closed in 7 days.\n\nThank you for your understanding and contribution to the LlamaIndex project!"
      }
    ]
  },
  {
    "number": 400,
    "title": "How to insert 2 GptListIndex of GptSimpleVectorIndex nodes into another GptListIndex ",
    "created_at": "2023-02-07T07:21:48Z",
    "closed_at": "2023-03-25T02:19:48Z",
    "labels": [
      "question",
      "discord"
    ],
    "url": "https://github.com/run-llama/llama_index/issues/400",
    "body": "Hey,\r\nI want to store a GptListIndex for a document with multiple nodes as GptSimpleVectorIndex in db. for easy selection of knowlede-base based on the requirement.\r\n\r\nbut the problem lies how to insert 2 GptListIndex of GptSimpleVectorIndex nodes into another GptListIndex ?",
    "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/400/comments",
    "author": "siddhant01",
    "comments": [
      {
        "user": "jerryjliu",
        "created_at": "2023-02-10T14:34:31Z",
        "body": "Hi @siddhant01 , not sure what you mean by \"store a gpt list index as a gptsimplevectorindex\" - those are two separate indices, and i'm not quite sure why you'd want to convert a list index to a simple vector index "
      },
      {
        "user": "Disiok",
        "created_at": "2023-03-25T02:19:48Z",
        "body": "Closing since thread is stale. "
      }
    ]
  }
]