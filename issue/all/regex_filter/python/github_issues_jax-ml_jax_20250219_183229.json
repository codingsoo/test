[
  {
    "number": 25822,
    "title": "Unable to Run JAX on GPU, can't find GPU device",
    "created_at": "2025-01-10T10:24:07Z",
    "closed_at": "2025-01-12T11:12:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/25822",
    "body": "### Description\r\n\r\nI met the problem as follows:\r\n\r\n```\r\n2025-01-10 10:16:07.380897: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2025-01-10 10:16:07.381132: E external/xla/xla/stream_executor/cuda/cuda_diagnostics.cc:262] kernel version 535.129.3 does not match DSO version 545.23.8 -- cannot find working devices in this configuration\r\nTraceback (most recent call last):\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 885, in backends\r\n    backend = _init_backend(platform)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 971, in _init_backend\r\n    backend = registration.factory()\r\n              ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 671, in factory\r\n    return xla_client.make_c_api_client(plugin_name, updated_options, None)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jaxlib/xla_client.py\", line 200, in make_c_api_client\r\n    return _xla.get_c_api_client(plugin_name, options, distributed_client)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\njaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: No visible GPU devices.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jovyan/alphafold/workspaces/jyj/test/test-jax.py\", line 3, in <module>\r\n    print(jax.devices())\r\n          ^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 1083, in devices\r\n    return get_backend(backend).devices()\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 1017, in get_backend\r\n    return _get_backend_uncached(platform)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 996, in _get_backend_uncached\r\n    bs = backends()\r\n         ^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 901, in backends\r\n    raise RuntimeError(err_msg)\r\nRuntimeError: Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\r\n```\r\n\r\nThe story is, after adding the following code snippets to my Python script for running AlphaFold, I'm no longer able to execute the GPU - version of AlphaFold, and the system seems to be locked into using the CPU despite having a capable GPU and the necessary GPU - related dependencies installed.\r\n\r\n```python\r\njax.config.update('jax_platform_name', 'cpu')\r\nos.environ['JAX_PLATFORMS'] = 'cpu'\r\n```\r\n\r\nFor further analysis:\r\n\r\n`nvidia-smi`:\r\n```\r\n❯ nvidia-smi\r\nFri Jan 10 10:14:51 2025       \r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.3     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:4F:00.0 Off |                    0 |\r\n| N/A   28C    P0              33W / 250W |     18MiB / 40960MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:50:00.0 Off |                    0 |\r\n| N/A   31C    P0              35W / 250W |     18MiB / 40960MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   2  NVIDIA A100-PCIE-40GB          Off | 00000000:53:00.0 Off |                    0 |\r\n| N/A   32C    P0              33W / 250W |     18MiB / 40960MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   3  NVIDIA A100-PCIE-40GB          Off | 00000000:57:00.0 Off |                    0 |\r\n| N/A   30C    P0              32W / 250W |     18MiB / 40960MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   4  NVIDIA A100-PCIE-40GB          Off | 00000000:9C:00.0 Off |                    0 |\r\n| N/A   29C    P0              32W / 250W |     18MiB / 40960MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   5  NVIDIA A100-PCIE-40GB          Off | 00000000:9D:00.0 Off |                    0 |\r\n| N/A   33C    P0              36W / 250W |     18MiB / 40960MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   6  NVIDIA A100-PCIE-40GB          Off | 00000000:A0:00.0 Off |                    0 |\r\n| N/A   31C    P0              35W / 250W |     18MiB / 40960MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   7  NVIDIA A100-PCIE-40GB          Off | 00000000:A4:00.0 Off |                    0 |\r\n| N/A   29C    P0              34W / 250W |     18MiB / 40960MiB |      0%      Default |\r\n|                                         |                      |             Disabled |\r\n+-----------------------------------------+----------------------+----------------------+\r\n                                                                                         \r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n+---------------------------------------------------------------------------------------+\r\n```\r\n\r\n`nvcc`:\r\n```\r\n❯ nvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2023 NVIDIA Corporation\r\nBuilt on Fri_Nov__3_17:16:49_PDT_2023\r\nCuda compilation tools, release 12.3, V12.3.103\r\nBuild cuda_12.3.r12.3/compiler.33492891_0\r\n```\r\n\r\nI also specified local cuda path like `CUDA_HOME`, `LD_LIBRARY_PATH`, `PATH`, in .zshrc: \r\n\r\n```\r\nexport CUDA_HOME=/usr/local/cuda-12.3\r\nexport PATH=/usr/local/cuda-12.3/bin:$PATH\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.3/lib64:$LD_LIBRARY_PATH\r\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\n```\r\n\r\nand successfully sourced.\r\n\r\n### System info (python version, jaxlib version, accelerator, etc.)\r\n\r\n`python`: 3.11.11\r\n`pip list`:\r\n```\r\nPackage                  Version  Editable project location\r\n------------------------ -------- --------------------------------------\r\nabsl-py                  2.1.0\r\nalphafold3               3.0.0    /home/jovyan/alphafold/alphafold3-main\r\nchex                     0.1.88\r\ndm-haiku                 0.0.13\r\ndm-tree                  0.1.8\r\nfilelock                 3.16.1\r\njax                      0.4.34\r\njax-cuda12-pjrt          0.4.34\r\njax-cuda12-plugin        0.4.34\r\njax-triton               0.2.0\r\njaxlib                   0.4.34\r\njaxtyping                0.2.34\r\njmp                      0.0.4\r\nml_dtypes                0.5.0\r\nnumpy                    2.2.1\r\nnvidia-cublas-cu12       12.6.4.1\r\nnvidia-cuda-cupti-cu12   12.6.80\r\nnvidia-cuda-nvcc-cu12    12.6.85\r\nnvidia-cuda-runtime-cu12 12.6.77\r\nnvidia-cudnn-cu12        9.6.0.74\r\nnvidia-cufft-cu12        11.3.0.4\r\nnvidia-cusolver-cu12     11.7.1.2\r\nnvidia-cusparse-cu12     12.5.4.2\r\nnvidia-nccl-cu12         2.23.4\r\nnvidia-nvjitlink-cu12    12.6.85\r\nopt_einsum               3.4.0\r\npackaging                24.2\r\npathspec                 0.12.1\r\npillow                   11.1.0\r\npip                      24.3.1\r\nrdkit                    2024.3.5\r\nscikit_build_core        0.10.7\r\nscipy                    1.15.0\r\nsetuptools               75.6.0\r\ntabulate                 0.9.0\r\ntoolz                    1.0.0\r\ntqdm                     4.67.1\r\ntriton                   3.1.0\r\ntypeguard                2.13.3\r\ntyping_extensions        4.12.2\r\nwheel                    0.45.1\r\nzstandard                0.23.0\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/25822/comments",
    "author": "drewjin",
    "comments": [
      {
        "user": "pearu",
        "created_at": "2025-01-10T10:42:18Z",
        "body": "Notice:\r\n```\r\nkernel version 535.129.3 does not match DSO version 545.23.8\r\n```\r\nthat is, the nvidia driver version (12.2.2) is older that cuda version (12.3.2).\r\n\r\nTry updating the nvidia driver to 12.3.2 or newer."
      },
      {
        "user": "drewjin",
        "created_at": "2025-01-11T03:36:20Z",
        "body": "> Notice:\r\n> \r\n> ```\r\n> kernel version 535.129.3 does not match DSO version 545.23.8\r\n> ```\r\n> \r\n> that is, the nvidia driver version (12.2.2) is older that cuda version (12.3.2).\r\n> \r\n> Try updating the nvidia driver to 12.3.2 or newer.\r\n\r\nThanks for your early reply.\r\n\r\nUnfortunately, I do not have the sudo right of the machine, I do not know how could I update my Nvidia Driver without it.\r\n\r\nMaybe I can try to downgrade my JAX into older versions to match the required CUDA version?\r\n\r\nOr I should use conda to install older version of CUDA?\r\n\r\nI was wondering where I could find out the complete version dependency doc, in Nvidia Official Site, but where (pls).\r\n\r\nYou know, noticing nvidia-smi output, `| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.3     |`, it seems the driver supports up to CUDA version 12.3, however, the required JAX version is `12.6`.\r\n\r\nActually, that makes me confused which I could run it one day ago, but it collapsed all of a sudden after I tried run CPU version."
      },
      {
        "user": "drewjin",
        "created_at": "2025-01-11T09:16:21Z",
        "body": "> Notice:\r\n> \r\n> ```\r\n> kernel version 535.129.3 does not match DSO version 545.23.8\r\n> ```\r\n> \r\n> that is, the nvidia driver version (12.2.2) is older that cuda version (12.3.2).\r\n> \r\n> Try updating the nvidia driver to 12.3.2 or newer.\r\n\r\nSorry to bother, I set the env variables by the following commands:\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=\"0\" \\\r\nLD_PRELOAD=/usr/lib/x86_64-linux-gnu/libcuda.so.535.129.03 \\\r\npython test-jax.py\r\n```\r\n\r\nAnd the error about Driver vanished, however it still couldn't find CUDA devices:\r\n\r\n```\r\n2025-01-11 09:13:57.300793: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\nTraceback (most recent call last):\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 885, in backends\r\n    backend = _init_backend(platform)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 971, in _init_backend\r\n    backend = registration.factory()\r\n              ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 671, in factory\r\n    return xla_client.make_c_api_client(plugin_name, updated_options, None)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jaxlib/xla_client.py\", line 200, in make_c_api_client\r\n    return _xla.get_c_api_client(plugin_name, options, distributed_client)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\njaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: No visible GPU devices.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jovyan/alphafold/workspaces/jyj/test/test-jax.py\", line 30, in <module>\r\n    print(jax.devices())\r\n          ^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 1083, in devices\r\n    return get_backend(backend).devices()\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 1017, in get_backend\r\n    return _get_backend_uncached(platform)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 996, in _get_backend_uncached\r\n    bs = backends()\r\n         ^^^^^^^^^^\r\n  File \"/home/jovyan/miniforge3/envs/AF-jyj/lib/python3.11/site-packages/jax/_src/xla_bridge.py\", line 901, in backends\r\n    raise RuntimeError(err_msg)\r\nRuntimeError: Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\r\n```"
      },
      {
        "user": "drewjin",
        "created_at": "2025-01-12T11:12:48Z",
        "body": "> Notice:\r\n> \r\n> ```\r\n> kernel version 535.129.3 does not match DSO version 545.23.8\r\n> ```\r\n> \r\n> that is, the nvidia driver version (12.2.2) is older that cuda version (12.3.2).\r\n> \r\n> Try updating the nvidia driver to 12.3.2 or newer.\r\n\r\nThank you for your understanding. It's no trouble at all. We've identified that the issue stems from the machine itself after testing with CUDA Kernels and torch, making further reproduction attempts unnecessary.\r\n\r\nWe regret any inconvenience caused by not discovering this sooner and appreciate your patience.\r\n\r\n**We are truly grateful for your prompt assistance earlier!**"
      }
    ]
  },
  {
    "number": 23126,
    "title": "Precision of jnp.linalg.solve",
    "created_at": "2024-08-19T19:57:45Z",
    "closed_at": "2024-08-20T16:19:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/23126",
    "body": "### Description\n\nI noticed a difference in numerical precision between jnp.linalg.solve and np.linalg.solve.\r\n\r\n**Numpy:**\r\n```\r\nimport numpy as np\r\nmatrix = np.array([[ 1,                  0,  0],\r\n                   [-1.+2.0958450e-06j,  1, -2.1878743-2.0878208e+06j],\r\n                   [ 0,                  0,  1]], dtype=np.complex64)\r\nvector = np.array([0, 0, 1], dtype=np.complex64)\r\nnp.linalg.solve(matrix, vector)\r\n```\r\n```\r\nOutput: array([0.             -0.j , 2.1878743+2087820.8j, 1.             +0.j ], dtype=complex64)\r\n```\r\n\r\n**JAX:**\r\n```\r\nfrom jax import numpy as jnp\r\nmatrix = jnp.array([[ 1,                  0,  0],\r\n                    [-1.+2.0958450e-06j,  1, -2.1878743-2.0878208e+06j],\r\n                    [ 0,                  0,  1]], dtype=jnp.complex64)\r\nvector = jnp.array([0, 0, 1], dtype=jnp.complex64)\r\njnp.linalg.solve(matrix, vector)\r\n```\r\n```\r\nOutput: Array([-2.3841858e-07-4.9968840e-13j,  2.1878741e+00+2.0878208e+06j, 1.0000000e+00+0.0000000e+00j], dtype=complex64)\r\n```\r\n\r\nSwitching to jnp.complex128 helps, but only reduces the error:\r\n```\r\nfrom jax import numpy as jnp\r\nimport jax\r\njax.config.update(\"jax_enable_x64\", True)\r\nmatrix = jnp.array([[ 1,                  0,  0],\r\n                    [-1.+2.0958450e-06j,  1, -2.1878743-2.0878208e+06j],\r\n                    [ 0,                  0,  1]], dtype=jnp.complex128)\r\nvector = jnp.array([0, 0, 1], dtype=jnp.complex128)\r\njnp.linalg.solve(matrix, vector)\r\n```\r\n```\r\nOutput: Array([8.8817842e-16+1.8614843e-21j, 2.1878743e+00+2.0878208e+06j, 1.0000000e+00+0.0000000e+00j], dtype=complex128)\r\n```\r\n\r\nI would have expected highest precision when switching both arrays to jnp.complex128, but if I only change the vector to jnp.complex128 and leave the matrix on jnp.complex64, the numerical error is gone:\r\n```\r\nfrom jax import numpy as jnp\r\nimport jax\r\njax.config.update(\"jax_enable_x64\", True)\r\nmatrix = jnp.array([[ 1,                  0,  0],\r\n                    [-1.+2.0958450e-06j,  1, -2.1878743-2.0878208e+06j],\r\n                    [ 0,                  0,  1]], dtype=jnp.complex64)\r\nvector = jnp.array([0, 0, 1], dtype=jnp.complex128)\r\njnp.linalg.solve(matrix, vector)\r\n```\r\n```\r\nOutput: Array([0.              -0.j  , 2.18787432+2087820.75j, 1.              +0.j  ], dtype=complex128)\r\n```\r\nWhen the matrix is jnp.complex128 and the vector jnp.complex64, there is still some error left.\r\n\r\nIs there anything to learn here? Like a general rule of when to choose which combination of jnp.complex128 and jnp.complex64?\n\n### System info (python version, jaxlib version, accelerator, etc.)\n\n```\r\njax:    0.4.26\r\njaxlib: 0.4.26\r\nnumpy:  1.26.4\r\npython: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 11.4.0]\r\njax.devices (1 total, 1 local): [cuda(id=0)]\r\nprocess_count: 1\r\nplatform: uname_result(system='Linux', release='6.8.0-40-generic', version='#40~22.04.3-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 30 17:30:19 UTC 2', machine='x86_64')\r\n\r\n\r\n$ nvidia-smi\r\nMon Aug 19 21:34:58 2024       \r\n+-----------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\r\n|-----------------------------------------+------------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                        |               MIG M. |\r\n|=========================================+========================+======================|\r\n|   0  NVIDIA GeForce GTX 1050 Ti     Off |   00000000:01:00.0 Off |                N[/]A |\r\n| N/A   56C    P8           N[/]A / ERR!  |    3174MiB /   4096MiB |      0%      Default |\r\n|                                         |                        |                  N/A |\r\n+-----------------------------------------+------------------------+----------------------+\r\n                                                                                         \r\n+-----------------------------------------------------------------------------------------+\r\n| Processes:                                                                              |\r\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n|        ID   ID                                                               Usage      |\r\n|=========================================================================================|\r\n|    0   N/A  N/A      2630      G   [/usr/lib/xorg/Xorg]                            4MiB |\r\n|    0   N/A  N/A     13661      C   venv/bin/python3.11                          3168MiB |\r\n+-----------------------------------------------------------------------------------------+\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/23126/comments",
    "author": "PhylomatX",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2024-08-19T20:56:08Z",
        "body": "JAX generally matches its internal computation to the precision of the inputs, so if you call a function with float32 inputs, it will perform the computation in float32. This makes sense because JAX is often used on accelerators like GPU and TPU, where hardware is not necessarily optimized for 64-bit computation.\r\n\r\nBy contrast, NumPy generally performs its computations in float64, regardless of the dtype of the inputs. This makes sense because NumPy only supports CPU backends, and modern CPU hardware mostly supports efficient 64-bit computations.\r\n\r\nThe net result is, if you want JAX to behave like NumPy, you need to (1) set `jax_enable_x64` to True, and (2) make sure to cast your inputs to 64-bit (casting just one is often fine, becuase the function will promote inputs to a common type).\r\n\r\nAlso note that the solver used by JAX will in general be different from the solver used by NumPy, so even at identical precision you should not expect the outputs to be bitwise-identical (as is the case in general with different implementations of floating-point math).\r\n\r\nI think this explains everything you're seeing in your examples – please let me know if you still have questions!"
      },
      {
        "user": "PhylomatX",
        "created_at": "2024-08-20T06:10:47Z",
        "body": "Thank you for your answer! One doubt left are the casting mechanics behind my third and fourth example. With both inputs casted to 64-bit, the result seems to be less precise than the one from casting only the vector to 64-bit while leaving the matrix on 32. \r\n\r\nSo casting only one input and not both seems to improve the result. Is there a general rule or is it just a coincidence for this specific case?"
      },
      {
        "user": "jakevdp",
        "created_at": "2024-08-20T14:09:26Z",
        "body": "The mixed-precision function call will promote both inputs to a common type, in this case complex128. The reason this leads to different results is that when you define your matrix as `complex64` and then cast to `complex128`, you are truncating the vaues to float32 precision resulting in a different matrix, and so different numerics for the output is not unexpected:\r\n```python\r\nM1 = jnp.array([[ 1,                  0,  0],\r\n                [-1.+2.0958450e-06j,  1, -2.1878743-2.0878208e+06j],\r\n                [ 0,                  0,  1]], dtype=jnp.complex64).astype(jnp.complex128)\r\nM2 = jnp.array([[ 1,                  0,  0],\r\n                [-1.+2.0958450e-06j,  1, -2.1878743-2.0878208e+06j],\r\n                [ 0,                  0,  1]], dtype=jnp.complex128)\r\n\r\nprint(M1 == M2)\r\n# [[ True  True  True]\r\n#  [False  True False]\r\n#  [ True  True  True]]\r\n```"
      }
    ]
  },
  {
    "number": 22881,
    "title": "Numerical precision differences in jitted vs. non-jitted code! ",
    "created_at": "2024-08-05T22:04:09Z",
    "closed_at": "2024-08-06T23:21:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/22881",
    "body": "### Description\r\n\r\nHi all! I am seeing some strange numerical behavior in jitted vs. non-jitted code. I start by defining the two functions below. They should ideally give the same result since the second term in `f2` gets multiplied by zero! \r\n\r\n```py\r\nfrom jax import jit, vmap, config\r\nimport jax.numpy as jnp\r\nconfig.update(\"jax_enable_x64\", True)\r\n\r\ndef f1(x):\r\n    return jnp.cos(x/2)\r\n\r\ndef f2(x):\r\n    return jnp.cos(x/2) + 0 * jnp.sin(x/2)\r\n```\r\n\r\nIf I then calculate this over a given range `xs = jnp.linspace(-2, 2, 101) `, I get different results depending on whether jit is used or not:\r\n\r\n```py\r\nxs = jnp.linspace(-2, 2, 101) \r\nprint(vmap(f1)(xs) - vmap(f2)(xs))\r\n\r\nOutput: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n 0. 0. 0. 0. 0.]\r\n\r\nprint(jit(vmap(f1))(xs) - jit(vmap(f2))(xs))\r\n\r\nOutput: [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00 -1.11022302e-16  0.00000000e+00  0.00000000e+00\r\n -1.11022302e-16 -1.11022302e-16 -1.11022302e-16  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  1.11022302e-16  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00 -1.11022302e-16  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00 -1.11022302e-16  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n -1.11022302e-16 -1.11022302e-16  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00 -1.11022302e-16 -1.11022302e-16\r\n -1.11022302e-16  0.00000000e+00 -1.11022302e-16  0.00000000e+00\r\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\r\n  0.00000000e+00]\r\n```\r\n\r\nI'll also note that in this case, I could do away with the `vmap` and just pass in `xs` to the two functions. However, I am seeing differences between the jitted and non-jitted versions there as well. \r\n\r\nIs this kind of behavior expected? This feels like a numerical precision problem, but I have enabled 64 bit mode in the config - so I'm not sure why that would be the case. \r\n\r\nAny assistance would be greatly appreciated!\r\n\r\n### System info (python version, jaxlib version, accelerator, etc.)\r\n\r\n```py\r\njax:    0.4.30\r\njaxlib: 0.4.30\r\nnumpy:  1.24.3\r\npython: 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:38:11)  [Clang 14.0.6 ]\r\njax.devices (1 total, 1 local): [CpuDevice(id=0)]\r\nprocess_count: 1\r\nplatform: uname_result(system='Darwin', node='dhcp-10-29-164-99.dyn.MIT.EDU', release='21.1.0', version='Darwin Kernel Version 21.1.0: Wed Oct 13 17:33:01 PDT 2021; root:xnu-8019.41.5~1/RELEASE_ARM64_T6000', machine='arm64')\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/22881/comments",
    "author": "shoumikdc",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2024-08-05T22:25:49Z",
        "body": "Hi - thanks for the question! This kind of difference is expected: in general, floating point operations are only accurate to within a particular precision that depends on the width of the float representation. When you compute the \"same\" result in two ways, the results will not in general be bitwise-equivalent. JIT-compilation replaces your original sequence of operations with a more efficient compiled kernel, and so in general you should not expect bitwise-equivalent outputs.\r\n\r\nYou can see the approximate expected precision using `finfo`:\r\n```python\r\n\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: np.finfo(np.float64).eps\r\nOut[2]: np.float64(2.220446049250313e-16)\r\n```\r\nThe differences you're seeing are consistent with those expectations."
      },
      {
        "user": "jakevdp",
        "created_at": "2024-08-05T22:55:45Z",
        "body": "Also, a side note: you might wonder why the compiler doesn't just simplify `x + 0 * y` to `x`: the reason for this is that for floating point math, these two expressions may return different results! For example, if you plug-in `y = np.inf` or `y = np.nan`, the first and second expression are not equivalent."
      },
      {
        "user": "mattjj",
        "created_at": "2024-08-06T23:21:57Z",
        "body": "I think we should probably close this as working-as-intended, and Jake's answer covers the reasoning well."
      }
    ]
  },
  {
    "number": 22094,
    "title": "Different roundings on GPU vs. CPU",
    "created_at": "2024-06-25T17:44:44Z",
    "closed_at": "2024-06-25T18:33:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/22094",
    "body": "### Description\n\nHello development team,\r\n\r\nI am experiencing different results depending on which platform I use for the execution.\r\n\r\n``` python\r\n# Execution with CUDA\r\nimport jax, jax.numpy as jnp\r\njax.config.update('jax_platforms', \"cuda\")\r\ninitializer = jax.nn.initializers.xavier_uniform()\r\nvals = initializer(jnp.array([3473907285,  989146414], dtype=jnp.uint32), (1164, 256), jnp.float32)\r\nprint(vals[0][-2])\r\n```\r\nresults in `0.042758033`.\r\n\r\nBut the following example:\r\n\r\n``` python\r\n# Execution on CPU\r\nimport jax, jax.numpy as jnp\r\njax.config.update('jax_platforms', \"cpu\")\r\ninitializer = jax.nn.initializers.xavier_uniform()\r\nvals = initializer(jnp.array([3473907285,  989146414], dtype=jnp.uint32), (1164, 256), jnp.float32)\r\nprint(vals[0][-2])\r\n```\r\nresults in `0.042758036`.\r\n\r\nIs this expected behavior? \r\n\r\nThis is not ideal in my situation because I am coding on my notebook with the speed benefits of the GPU. But for longer calculations, I am using a server cluster with only CPUs. Is there a way to get the same results on GPU and CPU?\n\n### System info (python version, jaxlib version, accelerator, etc.)\n\n```\r\n>>> import jax\r\n>>> jax.print_environment_info()\r\njax:    0.4.29\r\njaxlib: 0.4.29\r\nnumpy:  1.26.4\r\npython: 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\r\njax.devices (1 total, 1 local): [cuda(id=0)]\r\nprocess_count: 1\r\nplatform: uname_result(system='Linux', node='debianProArt', release='6.7.12+bpo-amd64', version='#1 SMP PREEMPT_DYNAMIC Debian 6.7.12-1~bpo12+1 (2024-05-06)', machine='x86_64')\r\n\r\n\r\n$ nvidia-smi\r\nTue Jun 25 19:42:47 2024       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   45C    P4     4W /  35W |    179MiB /  8188MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A     75543      C   python                            128MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/22094/comments",
    "author": "ysz0507",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2024-06-25T17:54:13Z",
        "body": "This is working as expected. For floating point operations, different ways of calculating the \"same\" value will have different rounding errors. The difference between the values in your example is smaller than the expected `eps` for float32:\r\n```python\r\n>>> val1 = 0.042758036\r\n>>> val2 = 0.042758033\r\n>>> print((val1 - val2) / val1)\r\n7.0162249697833e-08\r\n\r\n>>> import numpy as np\r\n>>> print(np.finfo('float32').eps)\r\n1.1920929e-07\r\n```\r\nWhen working with floating point arithmetic in any framework, you need to make sure your analysis is robust to inaccuracies at this level."
      },
      {
        "user": "ysz0507",
        "created_at": "2024-06-25T18:33:56Z",
        "body": "Thank you for the clarification! "
      }
    ]
  },
  {
    "number": 20466,
    "title": "Error raised in comparing static argument for a vmapped function after running the function without vmap before",
    "created_at": "2024-03-27T21:59:30Z",
    "closed_at": "2024-08-06T18:48:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/20466",
    "body": "### Description\r\n\r\nWhile running a single jitted instance of our jax function, we  found that a subsequent run of the same function, but vmapped, leads to a jax.errors.TracerBoolConversionError.  Specifically  this occurs when the our static_arg is checked for similarity as the class has been traced during the prior single run leading to the boolean conversion error.\r\nThe MWE code is enclosed below:\r\n\r\n``` import dataclasses\r\nimport functools\r\n\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\n@jax.tree_util.register_pytree_node_class\r\n@dataclasses.dataclass\r\nclass SomeStaticType:\r\n  params: jnp.ndarray = dataclasses.field(\r\n      default_factory=lambda: jnp.zeros((10,))\r\n  )\r\n\r\n  def tree_flatten(self):\r\n    children = (self.params,)\r\n    return (children, ('params',))\r\n\r\n  @classmethod\r\n  def tree_unflatten(cls, _, children):\r\n    return cls(*children)\r\n\r\n  def __hash__(self):\r\n    return 1  # Make sure the `__eq__` is called.\r\n\r\n  def __eq__(self, other):\r\n    return (self.params == other.params).all()  # !!! Raises error because `(self.params == other.params)` becomes a tracer.\r\n\r\n\r\n@functools.partial(jax.jit, static_argnames=(\"static_arg\",))\r\ndef inner_func(x, static_arg: SomeStaticType):\r\n    return jnp.sum(x*x + 3*x)\r\n\r\n\r\n@functools.partial(jax.jit, static_argnames=(\"static_arg\",))  # `jit`.\r\ndef outer_func(x, static_arg: SomeStaticType):\r\n    return jax.vmap(inner_func, in_axes=(1,None))(x, static_arg)  # `vmap`.\r\n\r\n\r\n\r\n## Main\r\nstatic_arg = SomeStaticType()\r\nprint(static_arg)\r\nx = jnp.zeros((3,))\r\n\r\n### First call `inner_func` once to get the cache filled.\r\ninner_func(x, static_arg)\r\n\r\n### Then call `outer_func` whch leads to errors.\r\nx_extended = jnp.zeros((3,2))\r\nouter_func(x_extended, static_arg)``` \r\n\r\n### System info (python version, jaxlib version, accelerator, etc.)\r\n\r\njax:    0.4.25\r\njaxlib: 0.4.25\r\nnumpy:  1.26.4\r\npython: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]\r\njax.devices (1 total, 1 local): [CpuDevice(id=0)]",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/20466/comments",
    "author": "JasonMH17",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2024-04-01T04:29:05Z",
        "body": "Hi, thanks for the report! This is working as intended: when you pass arrays through a JAX transformation like `jit` or `vmap`, they become traced, and you can no longer perform logic that requires concrete values.\r\n\r\nYour `SomeStaticType` class is not actually static because it contains JAX arrays. It correctly flattens the parameter array in `children`, which specifies the dynamic attributes of a pytree. Because `SomeStatictype` is not static, it's not correct to mark corresponding arguments as static, and you can fix the problem by avoiding doing so:\r\n```python\r\n@jax.jit\r\ndef inner_func(x, static_arg: SomeStaticType):\r\n    return jnp.sum(x*x + 3*x)\r\n\r\n@jax.jit\r\ndef outer_func(x, static_arg: SomeStaticType):\r\n    return jax.vmap(inner_func, in_axes=(1,None))(x, static_arg)\r\n```\r\nWith this change your code runs without any error.\r\n\r\n(Side-note: I'd also remove the `__hash__` and `__eq__` definitions from your type, because your structure is not actually hashable given that it contains non-hashable contents!)"
      },
      {
        "user": "yuhonglin",
        "created_at": "2024-04-01T09:51:22Z",
        "body": "> Your `SomeStaticType` class is not actually static because it contains JAX arrays. It correctly flattens the parameter array in `children`, which specifies the dynamic attributes of a pytree. Because `SomeStatictype` is not static, it's not correct to mark corresponding arguments as static\r\n\r\nThanks for the reply. I don't understand the logic behind `JAX array can't be static`?\r\n1. Does it mean that even in `jax.jit`, labeling an JAX array as static is considered inappropriate? (Then why doesn't `jax.jit` throw an error in such cases?)\r\n2. What if we really need some static arrays as static parameters? Should we always use `tuples` and converge them into arrays every time we use them?\r\n3. Is it a side effect of omnistaging?"
      },
      {
        "user": "jakevdp",
        "created_at": "2024-04-01T20:09:57Z",
        "body": "> 1. Does it mean that even in jax.jit, labeling an JAX array as static is considered inappropriate? (Then why doesn't jax.jit throw an error in such cases?)\r\n\r\nYes. Static arguments must be hashable, and JAX arrays are not hashable, so they cannot be used as static arguments. The reasoning behind this is that cached JIT compilations are indexed by a hash table with keys built from the hashes of static arguments along with the static attributes of any array arguments. If we have non-hashable static arguments, then we have no way of knowing whether a particular function call should be re-compiled.\r\n\r\n`jax.jit` does throw an error if you try to pass a JAX array directly as a static argument, but as you found you can work around this by wrapping your arrays in some class that defines a `__hash__` method. Typically such a workaround will lead to a different error further down in the stack.\r\n\r\n> 2. What if we really need some static arrays as static parameters? Should we always use tuples and converge them into arrays every time we use them?\r\n\r\nI suspect that in any situation in which you think a static array is the right solution, a dynamic array is probably the better solution.\r\n\r\n> 3. Is it a side effect of omnistaging?\r\n\r\nSort of. Previous to omnistaging it was possible to mix staged and non-staged JAX computations; now all jax computations are staged. But the requirement that static data be hashable has been true even pre-omnistaging. In very early JAX versions it was possible to mark jax arrays as static while implicitly using their object ID as their hash, but this frequently led to surprising recompilations, so we made it an error."
      },
      {
        "user": "yuhonglin",
        "created_at": "2024-04-01T22:32:05Z",
        "body": "Thanks for the further reply! For OP's original question: is it true that a \"static argument of pytree class\" can't be used with `vmap` because there is no way in `vmap` to designate which parameter is static? Although this may fall into the category as you said \"lead to a different error further down in the stack\", IIUC, this really feels like a limitation of `vmap` and at least a better error message might be needed...\r\n\r\nPlease see my other responses inline below.\r\n> Typically such a workaround will lead to a different error further down in the stack.\r\n\r\nThanks for telling us this. We currently heavily depend on this \"workaround\", i.e., wrapping `jnp.ndarray` into hashable classes. Hope it won't cause lots of issues (I suppose as long as this usage it is allowed, it should be OK...)\r\n\r\n> I suspect that in any situation in which you think a static array is the right solution, a dynamic array is probably the better solution.\r\n\r\nSome times, we need to store some parameters like the parameters of a linear model or filter. These parameters are vectors of floats and won't be changed in training (they are hyper-parameters). So it is more convenient to store in a static `jnp.ndarray`."
      },
      {
        "user": "jakevdp",
        "created_at": "2024-04-01T23:03:30Z",
        "body": "> These parameters are vectors of floats and won't be changed in training\r\n\r\nNote that \"static\" and \"constant\" are not identical concepts in JAX. It's fine to have a constant array that is dynamic, and you shouldn't see any performance penalties in this case. If you have constant arrays, you should not go out of your way to mark them as static, especially if it requires workarounds like wrapping them in a hashable class."
      },
      {
        "user": "yuhonglin",
        "created_at": "2024-04-01T23:46:33Z",
        "body": "> > These parameters are vectors of floats and won't be changed in training\r\n> \r\n> Note that \"static\" and \"constant\" are not identical concepts in JAX. It's fine to have a constant array that is dynamic, and you shouldn't see any performance penalties in this case. If you have constant arrays, you should not go out of your way to mark them as static, especially if it requires workarounds like wrapping them in a hashable class.\r\n\r\nThanks and understood. We feel having such parameters as \"static\" is better because,\r\n- Compared with plain dynamic input argument, it is more conceptually clear. And it is easier when we want to do conditionals etc. based on it.\r\n- Compared with some global constants, it is safer because JAX can automatically recompile the function every time the value changes."
      },
      {
        "user": "jakevdp",
        "created_at": "2024-04-02T03:27:55Z",
        "body": "Well, I disagree on \"conceptually clear\" (as I mentioned, \"static\" is a different concept than \"constant\"). And doing conditionals on such values is somewhat fraught, because element access is a traced operation under JIT.\r\n\r\nBut if you want to treat arrays as static values, I'd recommend one of two approaches:\r\n\r\n1. split the values into tuples of ints or floats, and then mark these as static. The advantage here is that such tuples are actually hashable, and individual elements are statically hashable (so e.g. it's trivial to use them for trace-time control flow). The disadvantage is that converting to and from an array will be somewhat expensive, particularly as the size of the parameter array grows.\r\n2. A second option is to use a wrapper class that looks something like this:\r\n\r\n```python\r\nimport jax\r\nfrom functools import partial\r\nimport dataclasses\r\n\r\n@dataclasses.dataclass(frozen=True)\r\nclass HashableArrayWrapper:\r\n  val: jax.Array\r\n  def __hash__(self):\r\n    return id(self.val)\r\n  def __eq__(self, other):\r\n    return isinstance(other, HashableArrayWrapper) and id(self.val) == id(other.val)\r\n\r\n@partial(jax.jit, static_argnums=0)\r\ndef f(x):\r\n  val = x.val\r\n  return val ** 2\r\n\r\nx = jax.numpy.arange(5)\r\nf(HashableArrayWrapper(x))\r\n# Array([ 0,  1,  4,  9, 16], dtype=int32)\r\n```\r\nThe disadvantage here is that any time you change `val`, it will needlessly trigger a re-compilation. You also will not be able to use elements of `val` statically for control flow, because array indexing is a traced operation. I honestly cannot think of any advantages to this approach, and I would not recommend this as a solution over just using your parameter array directly as a dynamic variable. But if it's important to you that your parameters are stored in an array, and that the array be treated as static by JIT, then this is probably the best way to do it."
      }
    ]
  },
  {
    "number": 18766,
    "title": "randint can not create random values of full uint32 range",
    "created_at": "2023-12-01T13:13:01Z",
    "closed_at": "2023-12-19T19:56:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/18766",
    "body": "### Description\r\n\r\n```\r\n        seed = jax.random.randint(rngs_i1, (1,), minval=0, maxval=2 ** 32 - 1, dtype=jnp.uint32)\r\n```\r\n\r\nthis fails in random.py:495\r\n\r\n```\r\n@partial(jit, static_argnums=(1, 4))\r\ndef _randint(key, shape, minval, maxval, dtype) -> Array:\r\n  _check_shape(\"randint\", shape, np.shape(minval), np.shape(maxval))\r\n  if not jnp.issubdtype(dtype, np.integer):\r\n    raise TypeError(f\"randint only accepts integer dtypes, got {dtype}\")\r\n\r\n  check_arraylike(\"randint\", minval, maxval)\r\n  minval = jnp.asarray(minval)\r\n  maxval = jnp.asarray(maxval)  #<---------------------- here\r\n  if not jnp.issubdtype(minval.dtype, np.integer):\r\n    minval = minval.astype(int)\r\n  if not jnp.issubdtype(maxval.dtype, np.integer):\r\n    maxval = maxval.astype(int)\r\n```\r\n\r\nI think these lines should be\r\n```\r\n  minval = jnp.asarray(minval, dtype=dtype)\r\n  maxval = jnp.asarray(maxval, dtype=dtype)\r\n```\r\n\r\n\r\n### What jax/jaxlib version are you using?\r\n\r\njax v0.4.20\r\n\r\n### Which accelerator(s) are you using?\r\n\r\nCPU\r\n\r\n### Additional system info?\r\n\r\nPython3.11 on Windows11\r\n\r\n### NVIDIA GPU info\r\n\r\nN/A",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/18766/comments",
    "author": "RogerJL",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2023-12-01T13:35:04Z",
        "body": "Hi - thanks for the report! I'm confused by the error you're seeing – it should have errored at the function boundary. Perhaps you're running your code in a `disable_jit` context?\r\n\r\nIn any case, the issue here is that JAX always chooses the default integer type for python integers, rather than doing value-based dtype semantics. That is a deliberate choice we made, because value-based semantics can lead to bigger problems.\r\n\r\nThe fix you suggest will work as long as the code is run under `disable_jit()`, but otherwise will have no effect.\r\n\r\nThe best fix here would be to cast the out-of-bound integer to `uint32` at the start – then your code will work whether or not `diable_jit` is activated:\r\n```python\r\nseed = jax.random.randint(rngs_i1, (1,), minval=0, maxval=jnp.uint32(2 ** 32 - 1), dtype=jnp.uint32)\r\n```\r\nNote, however, that this will not generate the full range of `uint32` values, which includes `2 ** 32 - 1` (the semantics of `randint` are that the maximum value is exclusive). If you want the full range of unsigned integers, you can do this:\r\n```python\r\njax.random.bits(rngs_i1, (1,), dtype=jnp.uint32)\r\n```"
      },
      {
        "user": "RogerJL",
        "created_at": "2023-12-01T16:36:09Z",
        "body": "Yes, it is very likely that I was running under disable_jit()  - the example was taken from Gymnasium.\r\nThanks for the random.bits idea\r\nBut how do you handle int8, shouldn't there be opportunity for optimizations (or do you only care about floating point types)?"
      },
      {
        "user": "jakevdp",
        "created_at": "2023-12-01T16:41:35Z",
        "body": "> But how do you handle int8, shouldn't there be opportunity for optimizations (or do you only care about floating point types)?\r\n\r\nI don't understand the question. Can you elaborate?"
      },
      {
        "user": "jakevdp",
        "created_at": "2023-12-19T19:56:09Z",
        "body": "I'm going to close this because I believe the original question was asked. Feel free to open another issue if you still have questions!"
      }
    ]
  },
  {
    "number": 18165,
    "title": "The result of Array slice calculation does not match that of direct calculation with date type bfloat16",
    "created_at": "2023-10-18T09:07:00Z",
    "closed_at": "2023-10-23T01:08:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/18165",
    "body": "### Description\n\n```np.random.seed(13)\r\nw = np.random.randn(2048, 2048).astype(np.float32)\r\nx = np.random.randn(2048, 2048).astype(np.float32)\r\n\r\nres1 = jnp.asarray(x, dtype=jnp.bfloat16) @ jnp.asarray(w, dtype=jnp.bfloat16)\r\nres2 = jnp.asarray(x[0:1,:], dtype=jnp.bfloat16) @ jnp.asarray(w, dtype=jnp.bfloat16)\r\nerror = jnp.max(jnp.abs(res1[0:1,:] - res2))\r\nprint(f\"Max error: {error} \") \r\n\r\n#The error is 0.25 and will increase with the increase of matrix size.\r\n```\r\nMaybe it's due to ```bfloat32``` precision? But they have the same date type and the same operation ```@``` , I wonder why the results are inconsistent. Looking forward to your answer, thanks.\n\n### What jax/jaxlib version are you using?\n\njax 0.4.13, jaxlib 0.4.13+cuda12.cudnn89\n\n### Which accelerator(s) are you using?\n\n_No response_\n\n### Additional system info\n\n_No response_\n\n### NVIDIA GPU info\n\n```Wed Oct 18 17:04:39 2023       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA A100 80G...  Off  | 00000000:9D:00.0 Off |                   On |\r\n| N/A   38C    P0    76W / 300W |                  N/A |     N/A      Default |\r\n|                               |                      |              Enabled |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| MIG devices:                                                                |\r\n+------------------+----------------------+-----------+-----------------------+\r\n| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\r\n|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\r\n|                  |                      |        ECC|                       |\r\n|==================+======================+===========+=======================|\r\n|  0    3   0   0  |     13MiB / 19968MiB | 28      0 |  2   0    1    0    0 |\r\n|                  |      0MiB / 32767MiB |           |                       |\r\n+------------------+----------------------+-----------+-----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/18165/comments",
    "author": "Sun-Xiaohui",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2023-10-18T16:48:40Z",
        "body": "Hi - this is working as expected.\r\n\r\nAny floating point computation will accumulate rounding error, and in the case of `bfloat16` there are only eight mantissa bits, meaning that rounding error will generally be about 1 part in $2^8$, or roughly `0.004` relative error.\r\n\r\nYou might expect in this particular case that the results would be identical, because the first operation is a subset of the second operation. This would be the case if the backend were executing the floating point operations in the same order in both cases, but you generally can't depend on this being the case when performing operations between matrices of different shape."
      },
      {
        "user": "Sun-Xiaohui",
        "created_at": "2023-10-19T02:18:40Z",
        "body": "Thanks for your reply. However, in the example above,  I modify the code to output the result as shown below:\r\n```\r\nnp.random.seed(13)\r\nw = np.random.randn(2048, 2048).astype(np.float32)\r\nx = np.random.randn(2048, 2048).astype(np.float32)\r\n\r\nres1 = jnp.asarray(x, dtype=jnp.bfloat16) @ jnp.asarray(w, dtype=jnp.bfloat16)\r\nres2 = jnp.asarray(x[0:1,:], dtype=jnp.bfloat16) @ jnp.asarray(w, dtype=jnp.bfloat16)\r\nerrors = jnp.max(jnp.abs(res1[0:1,:] - res2))\r\nindex = jnp.argmax(jnp.abs(res1[0:1,:] - res2))\r\nres1_fp32 = jnp.asarray(x, dtype=jnp.float32) @ jnp.asarray(w, dtype=jnp.float32)\r\nres2_fp32 = jnp.asarray(x[0:1,:], dtype=jnp.float32) @ jnp.asarray(w, dtype=jnp.float32)\r\n\r\nprint(f\"res1: {res1[0][index]}, res1_fp32: {res1_fp32[0][index]}, relative error: {(res1_fp32[0][index] - res1[0][index])/res1_fp32[0][index]},\\n\\\r\n        res2: {res2[0][index]}, res2_fp32: {res2_fp32[0][index]}, relative error: {(res2_fp32[0][index] - res2[0][index])/res2_fp32[0][index]}\")\r\nprint(f\"Max error: {errors} \") \r\n\r\n#output is:\r\nres1: -34.5, res1_fp32: -34.54495620727539, relative error: 0.0013013827847316861,\r\nres2: -34.75, res2_fp32: -34.55058670043945, relative error: -0.005771632771939039\r\nMax error: 0.25\r\n```\r\nI think the absolute error and relative error are too large，it's greater than 0.004. Is it still as expected? Looking forward to your answer, thanks."
      },
      {
        "user": "jakevdp",
        "created_at": "2023-10-19T02:46:39Z",
        "body": "That looks right: the relative error is on order 0.004 (0.001 and 0.005 are not inconsistent with the expected approximate relative error of 0.004) and given the size of the entries, this translates to an absolute error of about 0.2 to 0.3."
      },
      {
        "user": "Sun-Xiaohui",
        "created_at": "2023-10-19T09:57:57Z",
        "body": "Thanks very much. By the way, why  JAX got different result from Torch's on GPU?\r\n```\r\nres1 = jnp.asarray(x, dtype=jnp.float32) @ jnp.asarray(w, dtype=jnp.float32)\r\nres2 = torch.tensor(x).to(torch.float32).cuda() @ torch.tensor(w).to(torch.float32).cuda()\r\nres2 = res2.cpu().float().numpy()\r\nerr = np.max(np.abs(np.asarray(res1,dtype=np.float32) - res2))\r\n\r\n# err: 0.125\r\n```"
      },
      {
        "user": "jakevdp",
        "created_at": "2023-10-19T17:00:54Z",
        "body": "I suspect this indicates that JAX dot products and pytorch dot projects are lowering to different kernels. It looks like the error here is consistent for what we'd expect from floating point roundoff error in bfloat16 precision."
      },
      {
        "user": "Sun-Xiaohui",
        "created_at": "2023-10-20T00:54:52Z",
        "body": "OK, thanks. So is it an inherent feature of JAX that causes the deviation compared with PyTorch? Or what can we do to eliminate the deviation?"
      },
      {
        "user": "jakevdp",
        "created_at": "2023-10-20T01:56:43Z",
        "body": "Neither JAX nor PyTorch is incorrect here. Both are as close as can be expected to the true, real-valued answer given the inherent inaccuracies of bfloat16 arithmetic. The only way to eliminate the deviation between two bfloat16 computations is to ensure that the operations are computed identically at a low level - and generally when using high level libraries like PyTorch or JAX, you don’t have a lot of control over those low-level details."
      },
      {
        "user": "Sun-Xiaohui",
        "created_at": "2023-10-23T01:08:39Z",
        "body": "Thanks a lot."
      }
    ]
  },
  {
    "number": 18049,
    "title": "Is there a way to register a particular Python object as a PyTree leaf?",
    "created_at": "2023-10-10T21:47:16Z",
    "closed_at": "2023-10-10T23:01:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/18049",
    "body": "Please:\r\n\r\n- [x] Check for duplicate requests.\r\n- [x] Describe your goal, and if possible provide a code snippet with a motivating example.\r\n\r\n---\r\n\r\nI need to make `jax.tree_map` treat certain Python lists as leaves. Is this possible without using a dummy class `LeafList(list)`? I can't use a dummy class for reasons.\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/18049/comments",
    "author": "NightMachinery",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2023-10-10T22:43:21Z",
        "body": "You can pass an `is_leaf` function to `tree_map`; for example:\r\n```python\r\ndef is_leaf(x):\r\n  return isinstance(x, list) and len(x) == 3  # or whatever logic you need\r\ntree_map(func, values, is_leaf=is_leaf)\r\n```"
      }
    ]
  },
  {
    "number": 17629,
    "title": "Unexpected exception from jax.lax.fori_loop",
    "created_at": "2023-09-15T20:16:25Z",
    "closed_at": "2023-09-15T20:29:48Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/17629",
    "body": "### Description\r\n\r\nThere appears to be an issue with `jax.lax.fori_loop`. When I try to use this function, I get the following exception:\r\n\r\n\"the input carry component loop_carry[1][3].positions has type float32[0] but the corresponding output carry component has type float32[10,3], so the shapes do not match\"\r\n\r\nThe code producing this error is the following:\r\n\r\n```python\r\n@partial(jax.jit, static_argnames=('targetForce', 'timesteps')\r\ndef loss(model: controller, ball: BouncingBall, targetForce: float = 1.0, timesteps: int = 10):\r\n\r\n    positions = jp.array([[0]*3]*timesteps, dtype=jp.float32)\r\n    velocities = jp.array([[0]*6]*timesteps, dtype=jp.float32)\r\n    constraints = jp.array([[0]*6]*timesteps, dtype=jp.float32)\r\n    carry_i = (positions, velocities, constraints, ball, model)\r\n\r\n    def step(i: int, carry: tuple):\r\n\r\n        positions_s, velocities_s, constraints_s, ball_s, model_s = carry\r\n\r\n        positions_s = positions_s.at[i,:].add(ball_s.state.x.pos[0])\r\n        velocities_s = velocities_s.at[i,:].add(ball_s.state.qd)\r\n        constraints_s = constraints_s.at[i,:].add(ball_s.state.qf_constraint)\r\n\r\n        x = jp.array([ball_s.state.x.pos[0][2], ball_s.state.qd[2]])\r\n        force = model_s(x.transpose())\r\n\r\n        newstate = pipeline.step(ball_s.system, ball_s.state, force)\r\n        ball_s = ball_s.create(ball_s.system, newstate, positions_s, velocities_s, ball_s.contacts, constraints_s, model_s)\r\n        \r\n        newStuff = (positions_s, velocities_s, constraints_s, ball_s, model_s)\r\n\r\n        return newStuff\r\n\r\n    positions, velocities, constraints, ball, model = jax.lax.fori_loop(0, timesteps, step, carry_i)\r\n\r\n    states = (positions, velocities, constraints)\r\n\r\n    loss_value = jp.linalg.norm(constraints[:,2] - jp.array([targetForce]*timesteps))\r\n\r\n    return loss_value, states\r\n```\r\n\r\nA similar exception is being thrown for velocities and constraints.\r\n\r\nIn this function, `controller` extends `equinox.Module`, and `BouncingBall` is a `flax.struct.dataclass` that wraps a Brax `System` with some other arrays for state information at different timesteps.\r\n\r\nWhen I disable jit compiling using \r\n```python\r\nfrom jax.config import config\r\nconfig.update('jax_disable_jit', True)\r\n```\r\n\r\nthe function runs without issues, but when it is JIT compiled it throws these exceptions.\r\n\r\n### What jax/jaxlib version are you using?\r\n\r\njax v0.4.14, jaxlib 0.4.14\r\n\r\n### Which accelerator(s) are you using?\r\n\r\nCPU\r\n\r\n### Additional system info\r\n\r\nPython 3.10.12, Ubuntu 22.04, Intel Xeon E3-1230 V2\r\n\r\n### NVIDIA GPU info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/17629/comments",
    "author": "cdagher",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2023-09-15T20:21:51Z",
        "body": "When running `fori_loop` under `jit`, the shapes of input arrays must match the shapes of output arrays. From the error message:\r\n```\r\nthe input carry component loop_carry[1][3].positions has type float32[0] but the corresponding output carry component has type float32[10,3], so the shapes do not match\r\n```\r\nIt looks like `loop_carry[1][3]` is the variable you call `ball`, and on input `ball.positions` has shape `(0,)` and on output `ball.positions` has shape `(10, 3)`.\r\n\r\nThe way to fix this is to ensure that the input arrays have the same shape as the output arrays. I would look for where you're initializing `ball` in your code, and make sure it's initialized with the same shape arrays as you expect on output."
      },
      {
        "user": "cdagher",
        "created_at": "2023-09-15T20:29:48Z",
        "body": "Thanks @jakevdp! I hadn't thought to look at ball.positions. I changed the array in `BouncingBall` to have a pre-allocated size and now it works."
      }
    ]
  },
  {
    "number": 17496,
    "title": "Custom VJP with jax.custom_vjp returns incorrect gradient for a chained function",
    "created_at": "2023-09-07T15:31:39Z",
    "closed_at": "2023-09-07T16:16:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/17496",
    "body": "### Description:\r\n\r\nWhen implementing a custom VJP for a particular operation (using matrix-vector multiplication as an illustrative example), I have encountered a peculiar gradient behavior in chained operations. Specifically, while the gradient is correctly computed for one input when the custom operation is applied in isolation, it fails to compute the gradient accurately in the context of chained operations.\r\n\r\nUsing the matrix-vector multiplication as an example: \r\n- The custom VJP was designed to compute the gradient only concerning matrices, leaving out vectors. \r\n- For a standalone application of this custom operation, gradients are evaluated correctly. \r\n- However, when this operation is part of a sequence (for instance, two matrix multiplications separated by non-linearities), the gradient for the first matrix is observed to be zero, while the gradient for the second matrix is computed correctly. \r\n- It's noteworthy that this occurs even though there was no need for a gradient with respect to the input vector in the chained sequence.\r\n\r\nThis behavior suggests potential issues with how gradients are propagated through chained operations when a custom VJP is involved.\r\n\r\nBelow is a minimal example to reproduce the issue:\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\n@jax.custom_vjp\r\ndef fn(matrix, vector):\r\n    return jnp.dot(matrix, vector)\r\n    \r\ndef fn_fwd(matrix, vector):\r\n    return fn(matrix, vector), (matrix, vector)\r\n\r\ndef fn_bwd(res, g):\r\n    matrix, vector = res\r\n    matrix_dot = jnp.outer(g, vector)\r\n    # The following gradient should not be needed for the example\r\n    vector_dot = None # jnp.dot(matrix.T, g) \r\n    return matrix_dot, vector_dot\r\n\r\nfn.defvjp(fn_fwd, fn_bwd)\r\n\r\ncustom_dot = fn\r\n\r\nx = jnp.ones((2,))\r\nA = B = jnp.ones((2, 2))\r\nparams = (A, B)\r\n\r\n@jax.grad\r\ndef func(params, x):\r\n    x = jax.nn.relu(jnp.dot(params[0], x))\r\n    x = jax.nn.relu(jnp.dot(params[1], x))\r\n    return x.mean()\r\n\r\nprint('Expected Behavior')\r\ngrads = func(params, x)\r\nprint(grads)  # This provides the expected output\r\n\r\n@jax.grad\r\ndef func_with_custom_dot(params, x):\r\n    x = jax.nn.relu(custom_dot(params[0], x))\r\n    x = jax.nn.relu(custom_dot(params[1], x))\r\n    return x.mean()\r\n\r\ngrads = func_with_custom_dot(params, x)\r\nprint('Actual Behavior')\r\nprint(grads)  # This does not provide the expected output\r\n```\r\n\r\n### Expected Behavior:\r\n\r\nBoth matrices A and B should have their gradients computed correctly.\r\n```\r\n(Array([[1., 1.],\r\n       [1., 1.]], dtype=float64), Array([[1., 1.],\r\n       [1., 1.]], dtype=float64))\r\n```\r\n### Actual Behavior:\r\n\r\nThe gradient for matrix A is zero while the gradient for matrix B is correct.\r\n```\r\nActual Behavior\r\n(Array([[0., 0.],\r\n       [0., 0.]], dtype=float64), Array([[1., 1.],\r\n       [1., 1.]], dtype=float64))\r\n```\r\n\r\n### What jax/jaxlib version are you using?\r\n\r\njax v0.4.14\r\n\r\n### Which accelerator(s) are you using?\r\n\r\nCPU\r\n\r\n### Additional system info\r\n\r\nPython 3.10, MacOS with M2 chip\r\n\r\n### NVIDIA GPU info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/17496/comments",
    "author": "qdevpsi3",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2023-09-07T16:16:48Z",
        "body": "Thanks for the question!\r\n\r\nThis isn't a JAX bug (indeed the propagation rules for custom_vjps are the same as for everything else, and autodiff is perhaps the best-tested part of JAX), but just a bug in the `custom_vjp` rule. The issue is that this isn't accurate:\r\n```python\r\n    # The following gradient should not be needed for the example\r\n    vector_dot = None # jnp.dot(matrix.T, g) \r\n```\r\n\r\nIndeed if you just put that commented-out `jnp.dot(matrix.T, g)` back in, you get the correct behavior, as you may have noticed.\r\n\r\nIf you don't propagate gradients back to the \"data\" (rather than \"params\") side, you've effectively disconnected the first `custom_dot` from the output. Indeed with that `vector_dot = None` you've effectively written this:\r\n\r\n```python\r\n@jax.grad\r\ndef func_with_custom_dot(params, x):\r\n    x = jax.nn.relu(custom_dot(params[0], x))\r\n    x = jax.lax.stop_gradient(x)  # NOTE disconnecting!\r\n    x = jax.nn.relu(custom_dot(params[1], x))\r\n    return x.mean()\r\n```\r\n\r\nSo I think perhaps this is just a bug in understanding why your rule needs to propagate gradients on both sides."
      },
      {
        "user": "mattjj",
        "created_at": "2023-09-07T16:18:14Z",
        "body": "Indeed if you change the reference function to read:\r\n\r\n```python\r\n@jax.grad\r\ndef func(params, x):\r\n    x = jax.nn.relu(jnp.dot(params[0], x))\r\n    x = jax.lax.stop_gradient(x)  # NOTE added this\r\n    x = jax.nn.relu(jnp.dot(params[1], x))\r\n    return x.mean()\r\n```\r\n\r\nyou'll get the same result as the `custom_dot` version using `vector_dot = None`."
      }
    ]
  },
  {
    "number": 17214,
    "title": "bf16 * int8 matmul results in incorrect value",
    "created_at": "2023-08-22T03:27:26Z",
    "closed_at": "2023-08-23T02:10:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/17214",
    "body": "### Description\r\n```\r\n# Let us define a bf16 array and an int8 array:\r\n\r\nX=jnp.array([[-1.6171875,0.5703125]],dtype=jax.numpy.bfloat16)\r\nW=jnp.array([[127],[-4]],dtype=jax.numpy.int8)\r\n\r\n# perform matrix multiplication:\r\njax.numpy.matmul(X,W,precision=jax.lax.Precision.HIGHEST)\r\nDeviceArray([[-208]], dtype=bfloat16)\r\n\r\n\r\n# However, if we manually do the multiplication:\r\nX[0,0]*W[0,0]\r\nDeviceArray(-205, dtype=bfloat16)\r\nX[0,1]*W[1,0]\r\nDeviceArray(-2.28125, dtype=bfloat16)\r\nX[0,0]*W[0,0]+X[0,1]*W[1,0]\r\nDeviceArray(-207, dtype=bfloat16)\r\n\r\n# That is -207 which is different to -208 from the matmul function. \r\n```\r\nI have been trying to find a DL framework that does bf16 and int8 matrix multiplication, so far only Jax supports it, but it seems to have this rounding issue at the moment.\r\n\r\n### What jax/jaxlib version are you using?\r\n\r\n0.3.20+cuda11.cudnn82\r\n\r\n### Which accelerator(s) are you using?\r\n\r\n_No response_\r\n\r\n### Additional system info\r\n\r\n_No response_\r\n\r\n### NVIDIA GPU info\r\n\r\nA100",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/17214/comments",
    "author": "YingHH1",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2023-08-22T04:04:22Z",
        "body": "Thanks for the question! I believe this is working as expected: you're doing math at `bfloat16` precision, and `bfloat16` only has 7 bits of mantissa, meaning that you should generally expect numerical results to be good to within roughly one part in $2^7$.\r\n\r\nDoing this computation in `float32` reveals the \"true\" result:\r\n```python\r\nX.astype('float32') @ W.astype('float32')\r\n# Array([[-207.66406]], dtype=float32)\r\n```\r\nIn `bfloat16`, you got `-208`, which is actually the closest bfloat16-representable value to the true answer. You can see this by using the `jnp.nextafter` function to see what the next representable value is:\r\n```python\r\nprint(jnp.nextafter(jnp.bfloat16(-208), jnp.bfloat16(0)))\r\n# -207\r\n```\r\nThe next bfloat16-representable value greater than `-208` is `-207`, so it's clear that `-208` is the best possible bfloat16 representation of the answer to your computation. The reason your manual matmul returns this incorrect value is because by splitting the ops you incur bfloat16 rounding errors twice instead of once.\r\n\r\nHope that helps!"
      },
      {
        "user": "YingHH1",
        "created_at": "2023-08-22T04:35:23Z",
        "body": "Great, thank you for the help!"
      },
      {
        "user": "YingHH1",
        "created_at": "2023-08-22T04:51:56Z",
        "body": "I guess this implies that matmul internally converts the bf16/int8 arrays to fp32 for both multiplication and accumulation?\r\n```\r\n# i.e. y=x1.float32()*W1.float32()+x2.float32()*W2.float32()+...\r\n\r\nprint(X[0,0].astype(jnp.float32)*W[0,0].astype(jnp.float32)+X[0,1].astype(jnp.float32)*W[1,0].astype(jnp.float32))\r\n-207.66406\r\n# in this case the closest bf16 number is -208\r\n```\r\n\r\nbut this means we cast everything to fp32 such that the acceleration from low-bit computation is lost. Thus, what I would have expected is:\r\n```\r\n# i.e. y=(x1*W1).float32()+(x2*W2).float32()+...\r\n\r\nprint((X[0,0]*W[0,0]).astype(jnp.float32)+(X[0,1]*W[1,0]).astype(jnp.float32))\r\n-207.28125\r\n# in this case the closest bf16 number is -207\r\n```\r\n\r\nI am unfamiliar with A100's internal instruction, but I would have thought the bf16/int8 matrix multiplication is performed in low-bit for mul and high-bit for add, in order to reduce accumulation error whilst maintaining a performance edge."
      },
      {
        "user": "jakevdp",
        "created_at": "2023-08-22T11:56:39Z",
        "body": "The implementation of bfloat16 matmul is hardware-specific, and I’m not sure of the details on A100."
      }
    ]
  },
  {
    "number": 16704,
    "title": "No implicit conversions from *int4 to int8",
    "created_at": "2023-07-13T04:32:54Z",
    "closed_at": "2023-11-03T20:54:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/16704",
    "body": "### Description\n\nWhen JAX's `int4` support is enabled, there are no implicit conversions from `int4` and `uint4` to larger integer types. There are conversions defined for `i4` and `u4` and those are explicitly removed for `int4` and `uint4`.\n\n### What jax/jaxlib version are you using?\n\n_No response_\n\n### Which accelerator(s) are you using?\n\n_No response_\n\n### Additional system info\n\n_No response_\n\n### NVIDIA GPU info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/16704/comments",
    "author": "jewillco",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2023-07-13T13:34:50Z",
        "body": "Hi - thanks for the report. This is the expected behavior: the idea is that if you are working with `int4`, you are doing it quite intentionally, and implicit casting would be surprising. If you want the values to be cast for an operation, you can do it explicitly with e.g. `arr.astype('int8')`."
      },
      {
        "user": "jakevdp",
        "created_at": "2023-11-03T20:54:35Z",
        "body": "I'm going to close this, because I don't think there are plans to add implicit conversion for 4-bit integers."
      }
    ]
  },
  {
    "number": 16643,
    "title": "Jaxpr of a function without input argument is wrong",
    "created_at": "2023-07-06T18:15:50Z",
    "closed_at": "2023-07-06T18:29:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/16643",
    "body": "### Description\n\nI am writing a function without any input argument and want to translate it into Jaxpr. Here is the example,\r\n\r\n```py\r\ndef func():\r\n  frag_coord = jnp.zeros(([4]))\r\n  real = (frag_coord[0] / 1080.0 - 0.5) * 5.0\r\n  imag = (frag_coord[1] / 1080.0 - 0.5) * 5.0\r\n  r_a = real\r\n  r_b = imag\r\n  max_iteration = 500\r\n\r\n  def body_func(carry):\r\n    i, a, b = carry\r\n    t_a = a\r\n    a = a * a - b * b + r_a\r\n    b = 2 * t_a * b + r_b\r\n    return i + 1, a, b\r\n\r\n  def cond_func(carry):\r\n    i, a, b = carry\r\n    return ((a * a + b * b) <= 4) & (i < max_iteration)\r\n\r\n  i = lax.while_loop(cond_func, body_func, (0, real, imag))[0]\r\n  res = jnp.where(\r\n      i == max_iteration,\r\n      jnp.array([0, 0, 0, 1], jnp.float32),\r\n      jnp.array([0, i / max_iteration, 0, 1], jnp.float32),\r\n  )\r\n  return res\r\n\r\njaxpr = jax.make_jaxpr(func)().jaxpr\r\nprint(jaxpr)\r\n```\r\n\r\nThe output Jaxpr:\r\n\r\n```py\r\n{ lambda a:f32[4]; . let\r\n    b:f32[4] = broadcast_in_dim[broadcast_dimensions=() shape=(4,)] 0.0\r\n    c:f32[1] = dynamic_slice[slice_sizes=(1,)] b 0\r\n    d:f32[] = squeeze[dimensions=(0,)] c\r\n    e:f32[] = div d 1080.0\r\n    f:f32[] = sub e 0.5\r\n    g:f32[] = mul f 5.0\r\n    h:f32[1] = dynamic_slice[slice_sizes=(1,)] b 1\r\n    i:f32[] = squeeze[dimensions=(0,)] h\r\n    j:f32[] = div i 1080.0\r\n    k:f32[] = sub j 0.5\r\n    l:f32[] = mul k 5.0\r\n    m:i32[] _:f32[] _:f32[] = while[\r\n      body_jaxpr={ lambda ; n:f32[] o:f32[] p:i32[] q:f32[] r:f32[]. let\r\n          s:f32[] = mul q q\r\n          t:f32[] = mul r r\r\n          u:f32[] = sub s t\r\n          v:f32[] = add u n\r\n          w:f32[] = mul 2.0 q\r\n          x:f32[] = mul w r\r\n          y:f32[] = add x o\r\n          z:i32[] = add p 1\r\n        in (z, v, y) }\r\n      body_nconsts=2\r\n      cond_jaxpr={ lambda ; ba:i32[] bb:f32[] bc:f32[]. let\r\n          bd:f32[] = mul bb bb\r\n          be:f32[] = mul bc bc\r\n          bf:f32[] = add bd be\r\n          bg:bool[] = le bf 4.0\r\n          bh:bool[] = lt ba 500\r\n          bi:bool[] = convert_element_type[new_dtype=bool weak_type=False] bh\r\n          bj:bool[] = and bg bi\r\n        in (bj,) }\r\n      cond_nconsts=0\r\n    ] g l 0 g l\r\n    bk:bool[] = eq m 500\r\n    bl:f32[] = convert_element_type[new_dtype=float32 weak_type=True] m\r\n    bm:f32[] = div bl 500.0\r\n    bn:f32[] = convert_element_type[new_dtype=float32 weak_type=False] bm\r\n    bo:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0.0\r\n    bp:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] bn\r\n    bq:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0.0\r\n    br:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 1.0\r\n    bs:f32[4] = concatenate[dimension=0] bo bp bq br\r\n    bt:f32[4] = pjit[\r\n      jaxpr={ lambda ; bu:bool[] bv:f32[4] bw:f32[4]. let\r\n          bx:bool[4] = broadcast_in_dim[broadcast_dimensions=() shape=(4,)] bu\r\n          by:f32[4] = select_n bx bw bv\r\n        in (by,) }\r\n      name=_where\r\n    ] bk a bs\r\n  in (bt,) }\r\n```\r\n\r\nThe Jaxpr treats the `jnp.array([0, 0, 0, 1])` as an input argument, which is a wrong behavior. But I found that the `invars` for the Jaxpr is empty.\r\n\r\nIs this a bug or feature? If it is a feature, how can I get the Jaxpr for a function without argument correctly?\n\n### What jax/jaxlib version are you using?\n\nInternal version\n\n### Which accelerator(s) are you using?\n\nCPU\n\n### Additional system info\n\n_No response_\n\n### NVIDIA GPU info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/16643/comments",
    "author": "YangChenyuan",
    "comments": [
      {
        "user": "YangChenyuan",
        "created_at": "2023-07-06T18:19:35Z",
        "body": "It seems that it is not related to whether there is any argument or not. After I add one argument to the function, it stills treat the `jnp.array([0, 0, 0, 1], jnp.float32)` in the `jnp.where` as one *addtional* input argument."
      },
      {
        "user": "jakevdp",
        "created_at": "2023-07-06T18:23:04Z",
        "body": "Hi - thanks for the report! This is expected behavior. Essentially the only way to get array data into jaxprs is to either (1) create the array with a primitive like `iota` (i.e. `arange`) or `full`, or (2) pass the data as an argument to the jaxpr.\r\n\r\nIn this case, you created an array within your function, but there's no XLA primitive for `jnp.asarray` with arbitrary Python arguments. So in the process of tracing this, JAX constructs that array and adds it as an implicit argument to the jaxpr.\r\n\r\nDoes that make sense?"
      },
      {
        "user": "YangChenyuan",
        "created_at": "2023-07-06T18:29:26Z",
        "body": "Thanks for your explanation! I will create the array in another way."
      }
    ]
  },
  {
    "number": 15997,
    "title": "sparse-sparse matrix multiply creates unnecessary zero entries",
    "created_at": "2023-05-13T21:02:05Z",
    "closed_at": "2023-05-16T12:28:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/15997",
    "body": "### Description\r\n\r\nWhen multiplying two sparse BCOO matrices it seems the result always stores explicit zero-entries even when the corresponding row/column of `a` and `b` are all zero:\r\n```python\r\nimport jax\r\nimport numpy as np\r\na = jax.experimental.sparse.BCOO.fromdense(np.diag([1., 2.]))\r\nb = jax.experimental.sparse.BCOO.fromdense(np.diag([3., 4.]))\r\n(a @ b).data, (a @ b).indices\r\n>>> (Array([3., 0., 0., 8.], dtype=float64),\r\n     Array([[0, 0],\r\n            [0, 1],\r\n            [1, 0],\r\n            [1, 1]], dtype=int32))\r\n```\r\nExpected output:\r\n```python\r\n>>> (Array([3., 8.], dtype=float64),\r\n     Array([[0, 0],\r\n            [1, 1]], dtype=int32))\r\n```\r\n\r\n\r\n### What jax/jaxlib version are you using?\r\n\r\n0.4.8\r\n\r\n### Which accelerator(s) are you using?\r\n\r\nGPU\r\n\r\n### Additional system info\r\n\r\n_No response_\r\n\r\n### NVIDIA GPU info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/15997/comments",
    "author": "Linusnie",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2023-05-13T21:44:51Z",
        "body": "Hi - thanks for the report! This is working as intended. You're correct that sparse-sparse matmul often results in more stored elements than are strictly required, but those extra stored arguments are necessary due to the constraints of JAX's compilation model, which requires array shapes (and in this case the size of the sparse matrix buffers) to be known at compile time.\r\n\r\nThe issue is that the sparse matrix indices are only known at runtime, so the output buffers must be able to handle the worst case. When multiplying two matrices with number of specified elements `a.nse` and `b.nse`, the worst case is an output with `out.nse = a.nse * b.nse` (an easy way to imagine this is if the first matrix has all entries in a single column, and the second matrices has all elements in a single row). In anything but this worst case, the result will be padded with zeros.\r\n\r\nTo handle this, you have two options:\r\n\r\n1) Call `out.sum_duplicates()` on the result of the matmul, outside JIT, in order to sum and remove duplicated entries. It might look like this:\r\n```python\r\nout = (a @ b).sum_duplicates()\r\nprint(out.data)\r\n# [3. 8.]\r\nprint(out.indices)\r\n# [[0 0]\r\n#  [1 1]]\r\n```\r\n\r\n2) If appropriate, you can use a structured sparse representation (e.g. with `n_batch=1` on the leftmost input) such that the output *nse* will be more constrained.\r\n\r\nHope that helps!"
      },
      {
        "user": "Linusnie",
        "created_at": "2023-05-14T10:51:21Z",
        "body": "ah I see, that makes sense! Would it somehow be possible to manually set the number of specified elements for the output? eg in this case I'm computing `Bi = S.T @ Ai @ S` for a bunch of very sparse matrices that are too large to store densely on the gpu but I know `Bi.nse == Ai.nse`."
      },
      {
        "user": "jakevdp",
        "created_at": "2023-05-14T13:43:07Z",
        "body": "How do you *know* that the output has the same nse as the input? Could you encode that knowledge by using structured sparsity for the `S` matrix (i.e. option 2 in my answer above)?"
      },
      {
        "user": "Linusnie",
        "created_at": "2023-05-16T12:28:18Z",
        "body": "The `Ai`s are non-zero only on sub-blocks (different for every i) and `S = [[D, b], [0, 1]]` where `D` is diagonal\r\n\r\nI ended up getting around the issue by simply rescaling the elements of `Ai` before constructing the sparse matrix, so no need for matrix-matrix multiplies :smile: \r\n\r\nIn case it's useful here's a basic example to illustrate, goes OOM on my 12GB GPU:\r\n```python\r\nimport numpy as np\r\nimport jax.numpy as jnp\r\nfrom jax.experimental import sparse\r\n\r\ndef get_inds(n, block_size):\r\n    block_inds = np.random.choice(n - 1, block_size - 1, replace=False)\r\n    block_inds = np.hstack([np.sort(block_inds), n - 1])\r\n    return block_inds[np.array(list(np.ndindex(block_size, block_size)))]\r\n\r\nn = 48\r\nn_batch = 3000\r\nblock_size = 5\r\nA = sparse.bcoo_concatenate([\r\n    sparse.BCOO(\r\n        (\r\n            np.random.randn(block_size * block_size),\r\n            get_inds(n, block_size)\r\n        ),\r\n        shape=(n, n),\r\n    )[None]\r\n    for _ in range(n_batch)\r\n], dimension=0)\r\n\r\nS = sparse.BCOO.fromdense(np.block([\r\n    [np.diag(np.random.randn(n - 1)), np.random.randn(n - 1)[:, None]],\r\n    [np.zeros((1, n - 1)), 1.]\r\n]))\r\n\r\nA_scaled = (A @ S).transpose((0, 2, 1)) @ S\r\n```"
      }
    ]
  },
  {
    "number": 14589,
    "title": "_pjit_lower_cached not caching same code",
    "created_at": "2023-02-20T19:31:17Z",
    "closed_at": "2023-02-21T16:00:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/14589",
    "body": "### Description\n\nHi JAXers, \r\n\r\nit seems that pjit jaxpr -> XLA cache (_pjit_lower_cached) does not cache simple cases like this:\r\n\r\n```python\r\n@pjit\r\ndef f(p):\r\n  return sum(p.values())\r\n\r\nbefore = pjit_lib._pjit_lower_cached.cache_info()\r\nf({\"a\": 1.0})\r\nf({\"b\": 1.0})\r\n\r\nafter = pjit_lib._pjit_lower_cached.cache_info()\r\nassert after.misses == before.misses + 1\r\n```\r\n\r\nWhat I observed is that ClosedJaxpr have different hash, even though they have identical operations.\r\n\n\n### What jax/jaxlib version are you using?\n\nhead\n\n### Which accelerator(s) are you using?\n\n_No response_\n\n### Additional system info\n\n_No response_\n\n### NVIDIA GPU info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/14589/comments",
    "author": "prazek",
    "comments": [
      {
        "user": "yashk2810",
        "created_at": "2023-02-21T00:03:56Z",
        "body": "Your inputs are different leading to a different `in_tree` that you get via `arg_flat, in_tree = tree_flatten(arg)`"
      },
      {
        "user": "mattjj",
        "created_at": "2023-02-21T00:11:29Z",
        "body": "In terms of the user API, we can't get a cache hit between `{'a': 1.0}` and `{'b': 1.0}`, i.e. on dicts with different keys, because the function might depend on the key values, e.g. the function could be `lambda p: f1(p) if 'a' in p else f2(p)`."
      },
      {
        "user": "yashk2810",
        "created_at": "2023-02-21T00:15:01Z",
        "body": "That's a much better explanation! Thanks Matt!"
      },
      {
        "user": "mattjj",
        "created_at": "2023-02-21T16:00:38Z",
        "body": "To put a fine point on it: we'd get a cache hit here:\r\n\r\n```python\r\n@pjit\r\ndef f(p):\r\n  return sum(p.values())\r\n\r\nf({\"a\": 1.0})\r\nf({\"a\": 1.0})  # use the same key\r\n```\r\n\r\nand here:\r\n\r\n```python\r\nf({\"a\": 1.0})\r\nf({\"a\": 2.0})  # use the same key, even with different value\r\n```\r\n\r\nBut JAX tracing and lowering (re)specializes on dict keys and hence doesn't get cache hits for dicts with different keys.\r\n\r\nI think the TPU and GPU disk compilation caches _do_ get hits in these cases, since the HLO fingerprints compare equal (because the Python function, even though we retraced it, yielded the same jaxpr text and hence the same HLO)."
      }
    ]
  },
  {
    "number": 14228,
    "title": "GSoC'23 in Jax",
    "created_at": "2023-01-31T18:41:14Z",
    "closed_at": "2023-02-05T01:10:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/14228",
    "body": "What are potential projects for Jax re: it's participation in GSoC'23 ?\r\n\r\nI'm interested ! I would love to do some prior contributions as well regarding the same",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/14228/comments",
    "author": "shivance",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2023-02-05T01:10:28Z",
        "body": "Thanks for asking about this, and for your eagerness to help!\r\n\r\nI don't think the JAX core team is doing anything for GSoC though.\r\n\r\nSince this issue tracker is mainly for the JAX core development team, and since we're not involved in GSoC, I think it makes sense to close this issue.\r\n\r\nHowever, separate from GSoC, we try to mark issues as \"good first issue\" or \"contributions welcome\", so if you want to make contributions to the JAX core codebase then those may be the best place to start. Also, there may be other places to contribute in higher-level libraries.\r\n\r\nThanks again for asking!"
      }
    ]
  },
  {
    "number": 14149,
    "title": "Inconsistency between `jacfwd` and `jacrev` for functions with multiple outputs",
    "created_at": "2023-01-25T04:04:12Z",
    "closed_at": "2024-07-23T04:54:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/14149",
    "body": "### Description\n\nWhen a function returns multiple outputs, one of which is `nan`, `jacfwd` seems to return a `nan` corresponding to that output whereas `jacrev` returns a `nan` corresponding to all the outputs.\r\n\r\nConsider the following example\r\n```\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\nx = jnp.array([-1., -2., -3.])\r\nfunc = lambda x: (jnp.mean(x), jnp.mean(x**2), jnp.mean(jnp.sqrt(x))) \r\nprint(func(x))\r\nfunc1 = jax.jacrev(func)\r\nfunc2 = jax.jacfwd(func)\r\nprint(func1(x))\r\nprint(func2(x))\r\n```\r\n\r\nFollowing is the output\r\n```\r\n(DeviceArray(-2., dtype=float32), DeviceArray(4.666667, dtype=float32), DeviceArray(nan, dtype=float32))\r\n(DeviceArray([nan, nan, nan], dtype=float32), DeviceArray([nan, nan, nan], dtype=float32), DeviceArray([nan, nan, nan], dtype=float32))\r\n(DeviceArray([0.33333334, 0.33333334, 0.33333334], dtype=float32), DeviceArray([-0.6666667, -1.3333334, -2.       ], dtype=float32), DeviceArray([nan, nan, nan], dtype=float32))\r\n```\r\n\r\nGiven that the docs specify that both functions should compute the same values, is this the expected behaviour? Thanks 🙂\n\n### What jax/jaxlib version are you using?\n\njax v0.3.14, jaxlib v0.3.14\n\n### Which accelerator(s) are you using?\n\nCPU\n\n### Additional system info\n\nPython 3.8.10, Ubuntu 20.04.5 LTS (Focal Fossa)\n\n### NVIDIA GPU info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/14149/comments",
    "author": "vedpatwardhan",
    "comments": [
      {
        "user": "vedpatwardhan",
        "created_at": "2023-07-31T04:08:04Z",
        "body": "Hey @mattjj, any update on this? Thanks!"
      },
      {
        "user": "mattjj",
        "created_at": "2024-07-23T04:54:43Z",
        "body": "Thanks for the question! Sorry I missed it for... a year and a half! Oops.\r\n\r\nIt's extra tragic because it has a simple answer: this is actually expected behavior, because of how un-mathematical nans are!\r\n\r\nThe issue is that `jacfwd` computes the Jacobian `J` as something like `J = J @ I`, while `jacrev` computes it like `J = I @ J`. (Those definitions are circular if we take the notation too literally, but what I really mean is `jacfwd` applies Jacobian-vector products to an input-space identity matrix, while `jacrev` applies vector-Jacobian products to the rows of an output-space identity matrix.)\r\n\r\nSo how come only `jacfwd` gives the correct answer? Notice:\r\n\r\n```\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: J = np.array([[0.33333334, 0.33333334, 0.33333334], [-0.6666667, -1.3333334, -2.       ], [np.nan, np.nan, np.nan]])\r\n\r\nIn [3]: J @ np.eye(3)\r\nOut[3]:\r\narray([[ 0.33333334,  0.33333334,  0.33333334],\r\n       [-0.6666667 , -1.3333334 , -2.        ],\r\n       [        nan,         nan,         nan]])\r\n\r\nIn [4]: np.eye(3) @ J\r\nOut[4]:\r\narray([[nan, nan, nan],\r\n       [nan, nan, nan],\r\n       [nan, nan, nan]])\r\n```\r\n\r\nIt's strange that we have a matrix `A` for which `I @ A` and `A @ I` don't agree, but that's just because nans don't follow the usual laws of real numbers.\r\n\r\nApologies if that's unsatisfying, but it's just an unfortunate behavior of nans, as true for autodiff as it is for direct matrix multiplication in numpy.\r\n"
      }
    ]
  },
  {
    "number": 12392,
    "title": "single function which subsumes different vmapped functions",
    "created_at": "2022-09-16T18:33:18Z",
    "closed_at": "2022-09-16T18:52:14Z",
    "labels": [
      "duplicate",
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/12392",
    "body": "assume we have written a function which work when a, b are single instances\r\n\r\n```\r\ndef f(a,b):\r\n   #some computation  \r\n   return result\r\n```\r\n\r\nusing **vmap** we can repeat same computation for different instances of a and single instance of b as follows\r\n\r\n```\r\nv = partial(vmap, in_axes=(0, None))\r\naf = v(f)\r\n```\r\n\r\nnow similarly, we can repeat same computation for single instances of a and multiple instances of b as follows\r\n\r\n```\r\nv = partial(vmap, in_axes=(None, 0))\r\nbf = v(f)\r\n```\r\n\r\nwe have different functions `af`, `bf`. Is there way to combine this into single function and handle automatically ? If input is batched over **a** use `af` and if it is batched over **b** use `bf`.\r\n\r\nThanks in advance.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/12392/comments",
    "author": "SaitejaUtpala",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2022-09-16T18:47:07Z",
        "body": "Thanks for the question!\r\n\r\nI may not understand it though. It seems like the issue is independent of JAX and vmap, and just has to do with the caller. Shouldn't this look just like e.g.\r\n\r\n```python\r\nif a.ndim > 1:\r\n  return af(a, b)\r\nelif b.ndim > 1:\r\n  return bf(a, b)\r\n```\r\n\r\nwhere I've just assumed you check for \"if the input is batched\" by checking the rank of `a` and `b` somehow."
      },
      {
        "user": "mattjj",
        "created_at": "2022-09-16T18:52:14Z",
        "body": "Is this a duplicate of #12391? Let's consolidate there."
      }
    ]
  },
  {
    "number": 10815,
    "title": "Incorrect cholesky jacobians?",
    "created_at": "2022-05-24T21:39:44Z",
    "closed_at": "2022-05-24T23:54:04Z",
    "labels": [
      "question",
      "useful read"
    ],
    "url": "https://github.com/jax-ml/jax/issues/10815",
    "body": "I'm computing jacobians of the following equation with respect to B,\r\na = B<sup>-1</sup>c,\r\nwhere a, c &in; R<sup> n</sup> and B &in; R<sup> n x n</sup> is SPD.\r\n\r\nThe jacobian should be,\r\nda/dvec(B) = -(a^{T} &otimes; B <sup>-1</sup>),\r\nwhere &otimes; indicates the Kronecker product. \r\n\r\nIf I compute a = jnp.dot(inv(B), c) and then compute the jacobian with respect to B, I get what I would expect. If I compute a = cho_solve(cho_factor(B),c) and then compute the jacobian I get something different.\r\n\r\nI've included a short snippet below highlighting the potential issue. \r\n\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import random, jacfwd\r\nfrom jax.scipy.linalg import cho_solve, cho_factor, inv\r\nfrom functools import partial\r\n\r\njax.config.update(\"jax_enable_x64\", True)\r\njax.config.update(\"jax_platform_name\", \"cpu\")\r\n\r\n\r\nrng = random.PRNGKey(2022)\r\nd = 2\r\n\r\n\r\ndef init_spd(d, rng):\r\n    tril_ind = jnp.tril_indices(d)\r\n    Q = jnp.zeros((d, d))\r\n    Q = Q.at[tril_ind[0], tril_ind[1]].set(random.normal(rng, (d * (d + 1) // 2,)))\r\n    Q = jnp.dot(Q, Q.T) + jnp.eye(d) * 1e-6\r\n    return Q\r\n\r\n\r\nrng, subkey = random.split(rng)\r\nB = init_spd(d, subkey)\r\nrng, subkey = random.split(rng)\r\nc = random.normal(subkey, (d,))\r\n\r\n\r\ndef a(mode, B):\r\n    if mode == \"chol\":\r\n        a = cho_solve(cho_factor(B), c)\r\n    elif mode == \"inv\":\r\n        a = jnp.dot(inv(B), c)\r\n    else:\r\n        raise ValueError(\"No recognized mode\")\r\n    return a\r\n\r\n\r\n# computing a with chol & inv gives the same result\r\nprint(\"a using chol\")\r\nprint(a(\"chol\", B))\r\nprint(\"a using inv\")\r\nprint(a(\"inv\", B))\r\n\r\n# computing jacobians with chol & inv gives different results\r\nprint(\"da/dvec(B) with chol\")\r\nprint(jacfwd(partial(a, \"chol\"))(B).transpose(0, 2, 1).reshape(d, d ** 2))\r\nprint(\"da/dvec(B) with inv\")\r\nprint(jacfwd(partial(a, \"inv\"))(B).transpose(0, 2, 1).reshape(d, d ** 2))\r\nprint(\"da/dvec(B) manual\")\r\nprint(-jnp.kron(a(\"chol\", B).reshape(1, -1), inv(B)))\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/10815/comments",
    "author": "coursekevin",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2022-05-24T22:30:57Z",
        "body": "Thanks for raising this!\r\n\r\nI wouldn't quite call this a bug, but rather a subtle issue in writing a Python function which corresponds to the mathematical function we want. Indeed there are multiple reasonable mathematical functions we might want here!\r\n\r\nThe mathematical question has to do with whether we want to consider asymmetric perturbations to the input matrix. Is the input tangent space the space of all nxn matrices, or just all _symmetric_ nxn matrices? That is, is the domain of the mathematical function we have in mind all invertible matrices, or just symmetric (and positive definite) ones?\r\n\r\nTo make the `chol` and `inv` paths agree, we can add a call to `symmetrize = lambda X: (X + X.T) / 2.` like this:\r\n\r\n```python\r\ndef a(mode, B):\r\n    if mode == \"chol\":\r\n      a = cho_solve(cho_factor(symmetrize(B)), c)  # note symmetrize(B)\r\n    elif mode == \"inv\":\r\n        a = jnp.dot(inv(symmetrize(B)), c)  # note symmetrize(B)\r\n    else:\r\n        raise ValueError(\"No recognized mode\")\r\n    return a\r\n```\r\n\r\n```\r\nda/dvec(B) with chol\r\n[[-449.75533508  -45.56447342  -45.56447342   -3.94970749]\r\n [ -62.87687425  -16.29812641  -16.29812641   -1.79947677]]\r\nda/dvec(B) with inv\r\n[[-449.75533508  -45.56447342  -45.56447342   -3.94970749]\r\n [ -62.87687425  -16.29812641  -16.29812641   -1.79947677]]\r\n```\r\n\r\nBy adding these calls to `symmetrize` we're effectively projecting the input perturbations onto the vector subspace of symmetric matrices. These calls don't affect the primal part of the function (since it's being evaluated at a symmetric matrix input anyway).\r\n\r\nWithout the call to `symmetrize`, the `inv` version of the function represents a mathematical function on all invertible matrices (not just symmetric ones) and so naturally the tangent space is all nxn matrices.\r\n\r\nThe `chol` version without the call to `symmetrize`, on the other hand, actually represents a mathematical function on the lower triangle of its input, and the space of perturbations is projected to the same. (That's because the `cho_factor` function only reads the lower triangle of its input, and the strict upper triangle is ignored.)\r\n\r\nBy having calls to `symmetrize` on both paths, we are (by composition) making them both functions on the symmetric part only of their input.\r\n\r\nWhat do you think?"
      },
      {
        "user": "mattjj",
        "created_at": "2022-05-24T23:06:33Z",
        "body": "By the way, to get the symmetric \"manual\" version, just write this:\r\n\r\n```python\r\nprint((-jnp.kron(a(\"chol\", B).reshape(1, -1), inv(B))\r\n       - jnp.kron(inv(B), a(\"chol\", B).reshape(1, -1))) / 2.)\r\n```"
      },
      {
        "user": "coursekevin",
        "created_at": "2022-05-24T23:54:04Z",
        "body": "Thanks for your very thoughtful response, this was really helpful! Your explanation makes total sense.  Definitely not a bug, this was my mistake. "
      }
    ]
  },
  {
    "number": 10360,
    "title": "unjitted jnp.where evaluates both branches",
    "created_at": "2022-04-19T15:25:51Z",
    "closed_at": "2022-04-19T15:53:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/10360",
    "body": "The following function unexpectedly fails due to division by zero, while according to the documentation it should return 0:\r\n\r\n```\r\ndef f(x):\r\n    print(jnp.abs(x) < 1.)\r\n    return jnp.where(jnp.abs(x) < 1., 1. / (x - 1.), 0.)\r\nf(1.) # prints false, then fails with division by zero\r\n```\r\n\r\nInterestingly enough, the jitted version does run correctly:\r\n```\r\n@jax.jit\r\ndef f_jit(x):\r\n    return jnp.where(jnp.abs(x) < 1., 1. / (x - 1.), 0.)\r\nf(1.) # returns 0\r\n```\r\n\r\nIt seems that the unjitted jnp.where evaluates both branches? Is this expected behaviour - if so should a warning be added to the docs? \r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/10360/comments",
    "author": "GJBoth",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2022-04-19T15:52:12Z",
        "body": "Thanks for the question! Yes, it's expected behavior that `jnp.where` evaluates both branches. Actually the jitted function does too. The issue here is that without `jax.jit` the division is just Python builtin floating point division, not involving JAX at all, and so that's causing the error.\r\n\r\nTo involve JAX in the version without `jit`, try this:\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\ndef f(x):\r\n    print(jnp.abs(x) < 1.)\r\n    return jnp.where(jnp.abs(x) < 1., 1. / (x - 1.), 0.)\r\nf(jnp.array(1.))  # notice `jnp.array(1.)` instead of `1.`\r\n```\r\n\r\n`jax.numpy.where` is just like `numpy.where`, and it's just an ordinary function in Python, where arguments are evaluated before function application."
      }
    ]
  },
  {
    "number": 9347,
    "title": "JIT trace+compile without running function",
    "created_at": "2022-01-27T01:10:05Z",
    "closed_at": "2022-01-27T01:16:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/9347",
    "body": "It'd be nice to be able to trace and compile a function -- including all the optimisation done by the XLA compiler, i.e. more than just than is provided by `jax.xla_computation` -- without actually running the function afterwards. In particular this would be useful when benchmarking and optimising compile times.\r\n\r\nI think this is already actually doable via `jax.lib.xla_bridge.get_backend().compile`, so this is really just a request to expose this functionality publicly.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/9347/comments",
    "author": "patrick-kidger",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2022-01-27T01:13:14Z",
        "body": "You are in luck my friend! Try this out:\r\n\r\n```python\r\nimport jax\r\n\r\ncompiled = jax.jit(lambda x: x + 2).lower(3).compile()\r\n```\r\n\r\nThe `3` above is used as an example argument. Instead of a concrete example value, you can also pass any arguments with `shape` and `dtype` attributes.\r\n\r\nYou can also run e.g. `compiled(4)`, but you'll get a dtype error if you try `compiled(4.)`.\r\n\r\nWhat do you think?"
      },
      {
        "user": "mattjj",
        "created_at": "2022-01-27T01:16:19Z",
        "body": "See #7733. I'm not sure if there are docs yet, but all staging decorators (`jit`, `pjit`, `pmap`, `xmap`) should have roughly the same API here, or will soon."
      },
      {
        "user": "patrick-kidger",
        "created_at": "2022-01-27T01:16:33Z",
        "body": "Hurrah - that's great, thanks! :D"
      }
    ]
  },
  {
    "number": 9087,
    "title": "Cannot do \"nan_to_num\" in customized JVP functions",
    "created_at": "2022-01-04T12:13:43Z",
    "closed_at": "2022-02-08T10:18:53Z",
    "labels": [
      "question",
      "better_errors"
    ],
    "url": "https://github.com/jax-ml/jax/issues/9087",
    "body": "\r\nWe were trying to remove NAN in a customized JVP function but hit some issues. Please see below for a (overly) simplified example. Not sure if it's a feature or bug. If the behavior is as expected, please help provide some guidance on how to remove or mask out NAN (as well as INF) values in a customized JVP function. Thanks!\r\n\r\nJax version: 0.2.26\r\nJaxlib version: 0.1.75\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\n@jax.custom_jvp\r\ndef func(x):\r\n    return jnp.sum(x)\r\n\r\n@func.defjvp\r\ndef func_jvp(primals, tangents):\r\n    tangent, = tangents\r\n    tangent = jnp.nan_to_num(tangent)\r\n    return func(*primals), func(tangent)\r\n\r\nval_and_grad = jax.value_and_grad(func)\r\n\r\nval_and_grad(jnp.ones(3))\r\n```\r\n\r\nStack trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"bin/reproduce_simple_case.py\", line 16, in <module>\r\n    val_and_grad(jnp.ones(3))\r\n  File \"bin/reproduce_simple_case.py\", line 11, in func_jvp\r\n    tangent = jnp.nan_to_num(tangent)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\", line 2455, in nan_to_num\r\n    x = where(isneginf(x), array(neginf, dtype=x.dtype), x)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\", line 2170, in where\r\n    return _where(condition, x, y)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\", line 2149, in _where\r\n    return lax.select(condition, x, y) if not core.is_empty_shape(np.shape(x)) else x\r\njax._src.source_info_util.JaxStackTraceBeforeTransformation: AssertionError\r\n\r\nThe preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\r\n\r\n--------------------\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"bin/reproduce_simple_case.py\", line 16, in <module>\r\n    val_and_grad(jnp.ones(3))\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/traceback_util.py\", line 162, in reraise_with_filtered_traceback\r\n    return fun(*args, **kwargs)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/api.py\", line 1064, in value_and_grad_f\r\n    g = vjp_py(jax.lax._one(ans))\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/tree_util.py\", line 326, in <lambda>\r\n    func = lambda *args, **kw: original_func(*args, **kw)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/api.py\", line 2373, in _vjp_pullback_wrapper\r\n    ans = fun(*args)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/tree_util.py\", line 326, in <lambda>\r\n    func = lambda *args, **kw: original_func(*args, **kw)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/interpreters/ad.py\", line 123, in unbound_vjp\r\n    arg_cts = backward_pass(jaxpr, reduce_axes, consts, dummy_args, cts)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/interpreters/ad.py\", line 223, in backward_pass\r\n    params, call_jaxpr, invals, cts_in, cts_in_avals, reduce_axes)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/interpreters/ad.py\", line 558, in call_transpose\r\n    out_flat = primitive.bind(fun, *all_args, **new_params)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/core.py\", line 1661, in bind\r\n    return call_bind(self, fun, *args, **params)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/core.py\", line 1652, in call_bind\r\n    outs = primitive.process(top_trace, fun, tracers, params)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/core.py\", line 1664, in process\r\n    return trace.process_call(self, fun, tracers, params)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/core.py\", line 633, in process_call\r\n    return primitive.impl(f, *tracers, **params)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/dispatch.py\", line 129, in _xla_call_impl\r\n    *unsafe_map(arg_spec, args))\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/linear_util.py\", line 263, in memoized_fun\r\n    ans = call(fun, *args)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/dispatch.py\", line 156, in _xla_callable_uncached\r\n    *arg_specs).compile().unsafe_call\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/profiler.py\", line 206, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/dispatch.py\", line 170, in lower_xla_callable\r\n    fun, abstract_args, pe.debug_info_final(fun, \"jit\"))\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/profiler.py\", line 206, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/interpreters/partial_eval.py\", line 1566, in trace_to_jaxpr_final\r\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(fun, main, in_avals)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/interpreters/partial_eval.py\", line 1543, in trace_to_subjaxpr_dynamic\r\n    ans = fun.call_wrapped(*in_tracers)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/linear_util.py\", line 166, in call_wrapped\r\n    ans = self.f(*args, **dict(self.params, **kwargs))\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/interpreters/ad.py\", line 223, in backward_pass\r\n    params, call_jaxpr, invals, cts_in, cts_in_avals, reduce_axes)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/interpreters/ad.py\", line 558, in call_transpose\r\n    out_flat = primitive.bind(fun, *all_args, **new_params)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/core.py\", line 1661, in bind\r\n    return call_bind(self, fun, *args, **params)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/core.py\", line 1652, in call_bind\r\n    outs = primitive.process(top_trace, fun, tracers, params)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/core.py\", line 1664, in process\r\n    return trace.process_call(self, fun, tracers, params)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/interpreters/partial_eval.py\", line 1352, in process_call\r\n    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(f, self.main, in_avals)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/interpreters/partial_eval.py\", line 1543, in trace_to_subjaxpr_dynamic\r\n    ans = fun.call_wrapped(*in_tracers)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/linear_util.py\", line 166, in call_wrapped\r\n    ans = self.f(*args, **dict(self.params, **kwargs))\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/interpreters/ad.py\", line 229, in backward_pass\r\n    **eqn.params)\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/lax/lax.py\", line 3104, in _select_transpose_rule\r\n    assert not ad.is_undefined_primal(pred)\r\njax._src.traceback_util.UnfilteredStackTrace: AssertionError\r\n\r\nThe stack trace below excludes JAX-internal frames.\r\nThe preceding is the original exception that occurred, unmodified.\r\n\r\n--------------------\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"bin/reproduce_simple_case.py\", line 16, in <module>\r\n    val_and_grad(jnp.ones(3))\r\n  File \"/home/tiger/.local/lib/python3.7/site-packages/jax/_src/lax/lax.py\", line 3104, in _select_transpose_rule\r\n    assert not ad.is_undefined_primal(pred)\r\nAssertionError\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/9087/comments",
    "author": "connection-on-fiber-bundles",
    "comments": [
      {
        "user": "connection-on-fiber-bundles",
        "created_at": "2022-01-04T12:22:11Z",
        "body": "Sorry, some further investigation shows that it's ok to use `nan_to_num` in customized JVP functions. It's that we cannot use the `tangents` (the second argument of the customized JVP function) in `nan_to_num`. Even more general, `tangents` cannot show up in the condition part in the `jnp.where` functions."
      },
      {
        "user": "connection-on-fiber-bundles",
        "created_at": "2022-01-04T13:22:55Z",
        "body": "Just realized doing `nan_to_num` on `tangents` may break the linearity required for doing transpose automatically (hinted by the function raising the exception, namely `_select_transpose_rule`). Not sure if it's the source of the issue though. \r\n\r\nLet's say we are writing the customized JVP function for the loss function of our model, which would only be used in back-propagation. Does that mean we could write a customized VJP function, instead of JVP, to be used in BP, and we don't need to worry about the linearity and can do `nan_to_num` in customized VJP function in that case?"
      },
      {
        "user": "mattjj",
        "created_at": "2022-01-07T03:49:12Z",
        "body": "Thanks for the questions! You pretty much nailed it.\r\n\r\nIndeed it seems JAX considers `nan_to_num` to be nonlinear (because of the `where` as you say), and so using it on tangents makes the result non-transposable. (This is a pretty confusing error message though...)\r\n\r\nAnd yes, if you write a custom VJP then you're telling JAX how to perform the transposition, so automatic transposition is no longer necessary and this issue won't come up.\r\n\r\nDoes using a custom VJP make sense for your use case?"
      },
      {
        "user": "connection-on-fiber-bundles",
        "created_at": "2022-02-08T10:18:53Z",
        "body": "@mattjj Thanks for your response and sorry for getting back so late. Yes, I implemented a custom VJP and it worked. I will close the issue here"
      }
    ]
  },
  {
    "number": 8605,
    "title": "\"TypeError: iteration over a 0-d array\" when putting tuple of carriers to jax.lax.scan",
    "created_at": "2021-11-18T22:55:27Z",
    "closed_at": "2021-11-19T00:01:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/8605",
    "body": "I have got a function: \r\n```python\r\ndef holtExponentialSmoothingAdditiveError(params, x): # \r\n    s0, alpha, beta = params\r\n    def step(s, x):\r\n        previousLevel, previousTrend = s\r\n        a = jax.nn.sigmoid(alpha)\r\n        b = jax.nn.sigmoid(beta)\r\n        trainingError = x - previousLevel - previousTrend\r\n        levelEquasion = previousLevel + previousTrend + a*trainingError\r\n        trendEquasion = previousTrend + b*trainingError\r\n\r\n        return (levelEquasion, trendEquasion), previousLevel + previousTrend + trainingError\r\n    return jax.lax.scan(step, s0, x)\r\n```\r\n\r\ntimeSeries : [452500. 765000. 549000. 560000. 580000. 570000. 510000. 499000. 510000.\r\n 503625. 516500. 583000. 575000. 590000. 558750. 583250. 601000. 600000.\r\n 606000. 560000. 569000. 550000. 573750. 605000. 570000. 595000. 579000.\r\n 603500. 610500. 612500. 600000. 615000. 640000. 630000. 633000. 675000.\r\n 665000. 673750. 675000. 690000. 725000. 730000. 745000. 767500. 770000.\r\n 768250. 747000. 760000. 757500. 715000. 662500.]\r\n\r\nWhen I execute:\r\n\r\n```python\r\nalpha = 0.16\r\nbeta = 0.1\r\nprint(timeSeries)\r\nholtTimeSeries = holtExponentialSmoothingAdditive((timeSeries[0], alpha, beta), timeSeries)\r\n```\r\n\r\nI receive an error:\r\n\r\n```---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/tmp/ipykernel_31725/277494675.py in <module>\r\n      2 beta = 0.1\r\n      3 print(timeSeries)\r\n----> 4 holtTimeSeries = holtExponentialSmoothingAdditiveError((timeSeries[0], alpha, beta), timeSeries)\r\n\r\n/tmp/ipykernel_31725/2370628103.py in holtExponentialSmoothingAdditiveError(params, x)\r\n     10 \r\n     11         return (levelEquasion, trendEquasion), previousLevel + previousTrend + trainingError\r\n---> 12     return jax.lax.scan(step, s0, x)\r\n\r\n    [... skipping hidden 12 frame]\r\n\r\n/tmp/ipykernel_31725/2370628103.py in step(s, x)\r\n      2     s0, alpha, beta = params\r\n      3     def step(s, x):\r\n----> 4         previousLevel, previousTrend = s\r\n      5         a = jax.nn.sigmoid(alpha)\r\n      6         b = jax.nn.sigmoid(beta) \r\n\r\n    [... skipping hidden 1 frame]\r\n\r\n~/.local/lib/python3.9/site-packages/jax/_src/lax/lax.py in _iter(tracer)\r\n   2215 def _iter(tracer):\r\n   2216   if tracer.ndim == 0:\r\n-> 2217     raise TypeError(\"iteration over a 0-d array\")  # same as numpy error\r\n   2218   else:\r\n   2219     n = int(tracer.shape[0])\r\n\r\nTypeError: iteration over a 0-d array\r\n```\r\n\r\nIt looks like ```jax.lax.scan``` doesn't like when I pass carriers as a tuple, although I don't understand, why doesn't it work. May somebody explain to me, whether it is a bug or my mistake? \r\nNote, that I have simpleExponentialSmoothing coded very similar to holt's exponential smoothing and it works just fine, the only difference is that I pass single value in carry instead of tuple.\r\nTimeSeries is <class 'numpy.ndarray'> array, the same I pass to simpleExponentialSmoothing function.\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/8605/comments",
    "author": "EmperorTransisthor",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2021-11-18T23:09:38Z",
        "body": "It looks like you're passing a single value to `s` via `s0`, and then attempting to iterate over it using\r\n```\r\npreviousLevel, previousTrend = s\r\n```\r\nPerhaps you meant for `s0` to be a tuple of two values?"
      },
      {
        "user": "jakevdp",
        "created_at": "2021-11-18T23:11:26Z",
        "body": "For example, this executes without an error:\r\n```python\r\nholtExponentialSmoothingAdditiveError(((0.0, timeSeries[0]), alpha, beta), timeSeries)\r\n```"
      },
      {
        "user": "EmperorTransisthor",
        "created_at": "2021-11-19T00:01:34Z",
        "body": "Oh my, a shame I didn't spot this. Thanks a lot :D\r\n\r\nBtw it should not be labeled as bug, if someone can moderate."
      },
      {
        "user": "jakevdp",
        "created_at": "2021-11-19T00:44:27Z",
        "body": "Great, thanks!"
      }
    ]
  },
  {
    "number": 5914,
    "title": "Summing NamedTuple as if they were arrays with named axes",
    "created_at": "2021-03-03T14:50:12Z",
    "closed_at": "2021-03-08T19:27:20Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/5914",
    "body": "I heavily use `NamedTuple`s (maybe too heavily) as I find it quite convenient to treat them as arrays with named axes.\r\n\r\nThe only problem is that some basic primitives do not work for them.\r\nAddition actually works with the default operator `+`, but it has a different meaning - concatenation.\r\n\r\n\r\nWould it be possible to allow numpy operations on NamedTuples?\r\n\r\n```python\r\nfrom typing import NamedTuple\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\nclass NamedArray(NamedTuple):\r\n    a: jnp.ndarray\r\n    b: jnp.ndarray\r\n\r\nx = jnp.ones((2,), float)\r\na = NamedArray(x, x)\r\n\r\ndef add_named_array(l, r):\r\n    return jnp.add(l, r)\r\n\r\n\r\nprint(add_named_array(a, a))\r\n```\r\n\r\n\r\n<summary>\r\n<details>\r\nTrace:\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n\r\n<ipython-input-8-999792ade930> in <module>()\r\n     14 \r\n     15 \r\n---> 16 print(add_named_array(a, a))\r\n\r\n<ipython-input-8-999792ade930> in add_named_array(l, r)\r\n     11 \r\n     12 def add_named_array(l, r):\r\n---> 13     return jnp.add(l, r)\r\n     14 \r\n     15 \r\n\r\n/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py in fn(x1, x2)\r\n    383 def _maybe_bool_binop(numpy_fn, lax_fn, bool_lax_fn, lax_doc=False):\r\n    384   def fn(x1, x2):\r\n--> 385     x1, x2 = _promote_args(numpy_fn.__name__, x1, x2)\r\n    386     return lax_fn(x1, x2) if x1.dtype != bool_ else bool_lax_fn(x1, x2)\r\n    387   return _wraps(numpy_fn)(fn)\r\n\r\n/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py in _promote_args(fun_name, *args)\r\n    320 def _promote_args(fun_name, *args):\r\n    321   \"\"\"Convenience function to apply Numpy argument shape and dtype promotion.\"\"\"\r\n--> 322   _check_arraylike(fun_name, *args)\r\n    323   _check_no_float0s(fun_name, *args)\r\n    324   return _promote_shapes(fun_name, *_promote_dtypes(*args))\r\n\r\n/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py in _check_arraylike(fun_name, *args)\r\n    304                     if not _arraylike(arg))\r\n    305     msg = \"{} requires ndarray or scalar arguments, got {} at position {}.\"\r\n--> 306     raise TypeError(msg.format(fun_name, type(arg), pos))\r\n    307 \r\n    308 def _check_no_float0s(fun_name, *args):\r\n\r\nTypeError: add requires ndarray or scalar arguments, got <class '__main__.NamedArray'> at position 0.\r\n```\r\n\r\n</summary>",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/5914/comments",
    "author": "epignatelli",
    "comments": [
      {
        "user": "cgarciae",
        "created_at": "2021-03-03T17:07:42Z",
        "body": "You can easily implement it using `jax.tree_multimap`:\r\n\r\n```python\r\ndef add_named_array(l, r):\r\n    return jax.tree_multimap(jnp.add, l, r)\r\n```"
      },
      {
        "user": "jakevdp",
        "created_at": "2021-03-03T17:16:27Z",
        "body": "Hi @epignatelli - thanks for the question! I don't think it's likely that JAX will add this kind of polymorphism at the numpy layer, but I think you could probably create a decorator that does what you want following @cgarciae's solution, and use it where appropriate. Here's a simple version:\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom functools import wraps\r\nfrom typing import NamedTuple\r\n\r\ndef mapped(func):\r\n  @wraps(func)\r\n  def new_func(*args, **kwargs):\r\n    return jax.tree_multimap(func, *args, **kwargs)\r\n  return new_func\r\n\r\nclass NamedArray(NamedTuple):\r\n    a: jnp.ndarray\r\n    b: jnp.ndarray\r\n\r\nx = NamedArray(jnp.ones(2), jnp.arange(3))\r\ny = NamedArray(0, 1)\r\n\r\nmapped(jnp.add)(x, y)\r\n# NamedArray(a=DeviceArray([1., 1.], dtype=float32), b=DeviceArray([1, 2, 3], dtype=int32))\r\n```\r\nYou'd have to do some additional work to make it support mixtures of tuple and non-tuple arguments. Would that work for your use case?"
      },
      {
        "user": "epignatelli",
        "created_at": "2021-03-04T10:41:26Z",
        "body": "Thanks guys! I am using that exact pattern right now.\r\n\r\nJust out of curiosity, what's the reason is not on the roadmap? Is it out-of-jax-phylosophy or likely to create more maintainance pain than benefits? Or am I simply the only one using it this way? 😆 "
      },
      {
        "user": "jakevdp",
        "created_at": "2021-03-08T19:26:07Z",
        "body": "I'd say it's not in the `jax.numpy` roadmap because such operations are not supported by NumPy."
      },
      {
        "user": "jakevdp",
        "created_at": "2021-03-08T19:27:20Z",
        "body": "I'm going to close for now. Let us know if other questions come up!"
      }
    ]
  },
  {
    "number": 5802,
    "title": "[GSoC 2021 Proposal] Few shot algorithms in JAX",
    "created_at": "2021-02-21T23:27:47Z",
    "closed_at": "2021-03-03T16:46:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/5802",
    "body": "Hi!\r\n\r\nI want to implement an end-to-end framework for few/one shot learning algorithms using jax.\r\nThis is will be a long term project but currently, I am planning to implement the following as a part of GSoC 2021\r\n\r\n- Siamese network with contrastive loss and triplet loss\r\n- Prototype Network\r\n- Matching Network\r\n\r\nAny references to how to proceed will be immensely helpful.\r\n\r\nCheers!\r\nRakesh",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/5802/comments",
    "author": "INF800",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2021-03-03T16:46:44Z",
        "body": "Hey @rakesh4real, that sounds really fun!\r\n\r\nI suggest checking out the Flax or Haiku libraries, as you'll likely want to use more than just core JAX to build and train these kinds of models.\r\n\r\nHope your project goes well!"
      }
    ]
  },
  {
    "number": 5780,
    "title": "Inconsistent return type from jax.numpy.linalg.norm",
    "created_at": "2021-02-18T23:27:13Z",
    "closed_at": "2021-02-20T05:46:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/5780",
    "body": "With `jax=0.2.9, jaxlib=0.1.61` and `jaxlib=0.1.60`, the return type of `jax.numpy.linalg.norm` is inconsistent.  \r\n\r\nThe evaluation of `norm` returns a `_DeviceArray`.  But any subsequent evaluations return a `Buffer`:\r\n\r\n```python\r\nimport jax\r\nfrom jax.numpy.linalg import norm\r\n\r\nkey = jax.random.PRNGKey(0)\r\nx = jax.random.normal(key, (32, 32))\r\n\r\nprint(type(norm(x)))  #  <class 'jax.interpreters.xla._DeviceArray'>\r\nprint(type(norm(x)))  #  <class 'jax.interpreters.xla.Buffer'>\r\nprint(type(norm(x)))  #  <class 'jax.interpreters.xla.Buffer'>\r\n```\r\nNote that this doesn't happen with `sum`:\r\n```python \r\n\r\nprint(type(jax.numpy.sum(x)))  #  <class 'jax.interpreters.xla._DeviceArray'>\r\nprint(type(jax.numpy.sum(x)))  #  <class 'jax.interpreters.xla._DeviceArray'>\r\nprint(type(jax.numpy.sum(x)))  #  <class 'jax.interpreters.xla._DeviceArray'>\r\n```\r\n\r\nThis behavior is not present with `jax=0.26, jaxlib=0.1.57`.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/5780/comments",
    "author": "lukepfister",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2021-02-19T01:23:01Z",
        "body": "The difference between `Buffer` and `_DeviceArray` is intended to be an internal difference (one is generated by Python code, the other by C++, and we are gradually moving some logic into C++). Can you say more about why it affects you? It shouldn't change anything other than the type. Note both are instances of `DeviceArray`, which is a superclass."
      },
      {
        "user": "lukepfister",
        "created_at": "2021-02-19T20:18:26Z",
        "body": "I noticed it while debugging a different issue.  This wasn't my real problem, just thought the variable nature of the return type was odd.  Feel free to close if this behavior is expected."
      }
    ]
  },
  {
    "number": 5773,
    "title": "RuntimeError: Internal: Non-root tuple types are not handled.",
    "created_at": "2021-02-18T09:59:34Z",
    "closed_at": "2021-05-12T02:14:14Z",
    "labels": [
      "question",
      "XLA"
    ],
    "url": "https://github.com/jax-ml/jax/issues/5773",
    "body": "I'm currently having an issue with jax (jaxlib 0.1.61), when using GPU with cuda 10.1 (I don't get this error when using only CPU).\r\nI can't produce an MWA at the moment. I can't find any info on this error.. Can anyone give me some pointers on which type of behaviour might generate this error?\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/project/clebschnet/Examples/hparams.py\", line 549, in <module>\r\n    run_opt(args)\r\n  File \"/project/clebschnet/Examples/hparams.py\", line 348, in run_opt\r\n    ma = JaxClebschTreeTrax(hi, D, 1/2, depth,\r\n  File \"../clebschnet/machine.py\", line 234, in JaxClebschTreeTrax\r\n    return Trax(\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 628, in __init__\r\n    self.jax_init_parameters(rescale=self.rescale)\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 657, in jax_init_parameters\r\n    self.rescale_weights()\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 682, in rescale_weights\r\n    self.eval_in_batches(tree_unflatten(flat_ptree, flat_params), self._state, states)\r\n  File \"/opt/conda/lib/python3.8/site-packages/jax/_src/profiler.py\", line 115, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 839, in eval_in_batches\r\n    ch_out, new_state = self._forward_fn_with_state(params, state, ch, **kwargs)\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 528, in <lambda>\r\n    self._forward_fn_with_state = jax.jit(lambda pars, state, x: self._forward_fn_t(pars, state, x))\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 517, in <lambda>\r\n    self._forward_fn_t = lambda pars, state, x: net.pure_fn(x, pars, state, None) # returns (Tensor, state)\r\n  File \"/opt/conda/lib/python3.8/site-packages/trax/layers/base.py\", line 548, in pure_fn\r\n    raise LayerError(name, 'pure_fn',\r\njax._src.traceback_util.FilteredStackTrace: trax.layers.base.LayerError: Exception passing through layer Serial (in pure_fn):\r\n  layer created in file [...]/../clebschnet/layers_trax_tree.py, line 193\r\n  layer input shapes: ShapeDtype{shape:(64, 64), dtype:float64}\r\n\r\n  File [...]/trax/layers/combinators.py, line 88, in forward\r\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\r\n\r\nLayerError: Exception passing through layer InputTransform (in pure_fn):\r\n  layer created in file [...]/../clebschnet/layers_trax_tree.py, line 406\r\n  layer input shapes: ShapeDtype{shape:(64, 64), dtype:float64}\r\n\r\n  File [...]/../clebschnet/layers_trax_tree.py, line 433, in forward\r\n    x = self.vsmallest_lexicographically(tempx, n_sites).reshape((-1,1) + self.og_lattice)\r\n\r\n  File [...]/jax/_src/traceback_util.py, line 139, in reraise_with_filtered_traceback\r\n    return fun(*args, **kwargs)\r\n\r\n  File [...]/site-packages/jax/api.py, line 1222, in batched_fun\r\n    out_flat = batching.batch(\r\n\r\n  File [...]/site-packages/jax/linear_util.py, line 166, in call_wrapped\r\n    ans = self.f(*args, **dict(self.params, **kwargs))\r\n\r\n  File [...]/../clebschnet/layers_trax_tree.py, line 461, in smallest_lexicographically\r\n    return x[jnp.lexsort(sorted_x)[0]]\r\n\r\n  File [...]/_src/numpy/lax_numpy.py, line 3932, in lexsort\r\n    return lax.sort((*keys[::-1], iota), dimension=axis, num_keys=len(keys))[-1]\r\n\r\n  File [...]/_src/lax/lax.py, line 1414, in sort\r\n    return tuple(sort_p.bind(*operand, dimension=dimension,\r\n\r\n  File [...]/site-packages/jax/core.py, line 282, in bind\r\n    out = top_trace.process_primitive(self, tracers, params)\r\n\r\n  File [...]/jax/interpreters/batching.py, line 151, in process_primitive\r\n    val_out, dim_out = batched_primitive(vals_in, dims_in, **params)\r\n\r\n  File [...]/_src/lax/lax.py, line 5782, in _sort_batch_rule\r\n    return (sort_p.bind(*new_args, dimension=new_dimension, is_stable=is_stable, num_keys=num_keys),\r\n\r\n  File [...]/site-packages/jax/core.py, line 282, in bind\r\n    out = top_trace.process_primitive(self, tracers, params)\r\n\r\n  File [...]/site-packages/jax/core.py, line 628, in process_primitive\r\n    return primitive.impl(*tracers, **params)\r\n\r\n  File [...]/jax/interpreters/xla.py, line 238, in apply_primitive\r\n    compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args), **params)\r\n\r\n  File [...]/jax/_src/util.py, line 198, in wrapper\r\n    return cached(bool(FLAGS.jax_enable_x64), *args, **kwargs)\r\n\r\n  File [...]/jax/_src/util.py, line 191, in cached\r\n    return f(*args, **kwargs)\r\n\r\n  File [...]/jax/interpreters/xla.py, line 288, in xla_primitive_callable\r\n    compiled = backend_compile(backend, built_c, options)\r\n\r\n  File [...]/jax/interpreters/xla.py, line 352, in backend_compile\r\n    return backend.compile(built_c, compile_options=options)\r\n\r\nRuntimeError: Internal: Non-root tuple types are not handled.\r\n\r\nThe stack trace above excludes JAX-internal frames.\r\nThe following is the original exception that occurred, unmodified.\r\n\r\n--------------------\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/project/clebschnet/Examples/hparams.py\", line 549, in <module>\r\n    run_opt(args)\r\n  File \"/project/clebschnet/Examples/hparams.py\", line 348, in run_opt\r\n    ma = JaxClebschTreeTrax(hi, D, 1/2, depth,\r\n  File \"../clebschnet/machine.py\", line 234, in JaxClebschTreeTrax\r\n    return Trax(\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 628, in __init__\r\n    self.jax_init_parameters(rescale=self.rescale)\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 657, in jax_init_parameters\r\n    self.rescale_weights()\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 682, in rescale_weights\r\n    self.eval_in_batches(tree_unflatten(flat_ptree, flat_params), self._state, states)\r\n  File \"/opt/conda/lib/python3.8/site-packages/jax/_src/profiler.py\", line 115, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 839, in eval_in_batches\r\n    ch_out, new_state = self._forward_fn_with_state(params, state, ch, **kwargs)\r\n  File \"/opt/conda/lib/python3.8/site-packages/jax/_src/traceback_util.py\", line 139, in reraise_with_filtered_traceback\r\n    return fun(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.8/site-packages/jax/api.py\", line 396, in f_jitted\r\n    return cpp_jitted_f(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 528, in <lambda>\r\n    self._forward_fn_with_state = jax.jit(lambda pars, state, x: self._forward_fn_t(pars, state, x))\r\n  File \"/opt/conda/lib/python3.8/site-packages/netket/machine/jax.py\", line 517, in <lambda>\r\n    self._forward_fn_t = lambda pars, state, x: net.pure_fn(x, pars, state, None) # returns (Tensor, state)\r\n  File \"/opt/conda/lib/python3.8/site-packages/trax/layers/base.py\", line 548, in pure_fn\r\n    raise LayerError(name, 'pure_fn',\r\ntrax.layers.base.LayerError: Exception passing through layer Serial (in pure_fn):\r\n  layer created in file [...]/../clebschnet/layers_trax_tree.py, line 193\r\n  layer input shapes: ShapeDtype{shape:(64, 64), dtype:float64}\r\n\r\n  File [...]/trax/layers/combinators.py, line 88, in forward\r\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\r\n\r\nLayerError: Exception passing through layer InputTransform (in pure_fn):\r\n  layer created in file [...]/../clebschnet/layers_trax_tree.py, line 406\r\n  layer input shapes: ShapeDtype{shape:(64, 64), dtype:float64}\r\n\r\n  File [...]/../clebschnet/layers_trax_tree.py, line 433, in forward\r\n    x = self.vsmallest_lexicographically(tempx, n_sites).reshape((-1,1) + self.og_lattice)\r\n\r\n  File [...]/jax/_src/traceback_util.py, line 139, in reraise_with_filtered_traceback\r\n    return fun(*args, **kwargs)\r\n\r\n  File [...]/site-packages/jax/api.py, line 1222, in batched_fun\r\n    out_flat = batching.batch(\r\n\r\n  File [...]/site-packages/jax/linear_util.py, line 166, in call_wrapped\r\n    ans = self.f(*args, **dict(self.params, **kwargs))\r\n\r\n  File [...]/../clebschnet/layers_trax_tree.py, line 461, in smallest_lexicographically\r\n    return x[jnp.lexsort(sorted_x)[0]]\r\n\r\n  File [...]/_src/numpy/lax_numpy.py, line 3932, in lexsort\r\n    return lax.sort((*keys[::-1], iota), dimension=axis, num_keys=len(keys))[-1]\r\n\r\n  File [...]/_src/lax/lax.py, line 1414, in sort\r\n    return tuple(sort_p.bind(*operand, dimension=dimension,\r\n\r\n  File [...]/site-packages/jax/core.py, line 282, in bind\r\n    out = top_trace.process_primitive(self, tracers, params)\r\n\r\n  File [...]/jax/interpreters/batching.py, line 151, in process_primitive\r\n    val_out, dim_out = batched_primitive(vals_in, dims_in, **params)\r\n\r\n  File [...]/_src/lax/lax.py, line 5782, in _sort_batch_rule\r\n    return (sort_p.bind(*new_args, dimension=new_dimension, is_stable=is_stable, num_keys=num_keys),\r\n\r\n  File [...]/site-packages/jax/core.py, line 282, in bind\r\n    out = top_trace.process_primitive(self, tracers, params)\r\n\r\n  File [...]/site-packages/jax/core.py, line 628, in process_primitive\r\n    return primitive.impl(*tracers, **params)\r\n\r\n  File [...]/jax/interpreters/xla.py, line 238, in apply_primitive\r\n    compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args), **params)\r\n\r\n  File [...]/jax/_src/util.py, line 198, in wrapper\r\n    return cached(bool(FLAGS.jax_enable_x64), *args, **kwargs)\r\n\r\n  File [...]/jax/_src/util.py, line 191, in cached\r\n    return f(*args, **kwargs)\r\n\r\n  File [...]/jax/interpreters/xla.py, line 288, in xla_primitive_callable\r\n    compiled = backend_compile(backend, built_c, options)\r\n\r\n  File [...]/jax/interpreters/xla.py, line 352, in backend_compile\r\n    return backend.compile(built_c, compile_options=options)\r\n\r\nRuntimeError: Internal: Non-root tuple types are not handled.\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/5773/comments",
    "author": "jwnys",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2021-02-18T13:38:58Z",
        "body": "This error originates inside XLA, but I'm actually unsure how to produce it. Can you share either a reproduction or an HLO dump?\r\n\r\nYou can get an HLO dump by running JAX with the environment variable `XLA_FLAGS=--xla_dump_to=/tmp/somewhere` and zipping up the files you get out. (Note this does essentially share your model's code on Github, if that matters.)\r\n"
      },
      {
        "user": "hawkinsp",
        "created_at": "2021-02-23T03:14:00Z",
        "body": "Any updates? Is there a way for me to reproduce this?"
      },
      {
        "user": "hawkinsp",
        "created_at": "2021-05-12T02:14:14Z",
        "body": "Closing. I suspect the bug has already been fixed in newer jaxlibs, and without a way to reproduce it there's nothing we can do. Please reopen if there's a way for us to reproduce the issue at head!"
      }
    ]
  },
  {
    "number": 5647,
    "title": "Redundant computation in expm?",
    "created_at": "2021-02-05T14:13:51Z",
    "closed_at": "2022-01-21T23:40:13Z",
    "labels": [
      "question",
      "performance",
      "open"
    ],
    "url": "https://github.com/jax-ml/jax/issues/5647",
    "body": "Hi all:\r\n\r\nIt looks to me there are redundant calculations in the `expm` function, but wanted to ask to see if I'm missing something (perhaps something clever is happening in compilation). \r\n\r\nE.g., for `float64` and `complex128`, the numerator/denominator for the Pade approximant is generated as follows:\r\n\r\n```\r\nif A.dtype == 'float64' or A.dtype == 'complex128':\r\n   U3, V3 = _pade3(A)\r\n   U5, V5 = _pade5(A)\r\n   U7, V7 = _pade7(A)\r\n   U9, V9 = _pade9(A)\r\n   maxnorm = 5.371920351148152\r\n   n_squarings = jnp.maximum(0, jnp.floor(jnp.log2(A_L1 / maxnorm)))\r\n   A = A / 2**n_squarings\r\n   U13, V13 = _pade13(A)\r\n   conds=jnp.array([1.495585217958292e-002, 2.539398330063230e-001,\r\n                    9.504178996162932e-001, 2.097847961257068e+000])\r\n   U = jnp.select((A_L1<conds), (U3, U5, U7, U9), U13)\r\n   V = jnp.select((A_L1<conds), (V3, V5, V7, V9), V13)\r\n```\r\n\r\nThe algorithm being used (I believe by Al-Mohy and Higham, same as in `scipy`) prescribes choosing between 1 of 5 different Pade approximations (of various orders) based on the norm of `A`. In the above, it appears all potential Pade approximations are being computed, and only afterwards is one chosen based on the norm of `A`. Hence, the computations for the others are discarded and ultimately unnecessary.\r\n\r\nIt seems this function could benefit from changing the above logic to use nested `lax.cond` statements (or perhaps `switch`?) to avoid computing all possible versions. To motivate this: I quickly tried modifying the `expm` function to simply always do the `U13, V13` version of of the computation (the most expensive) and not compute any of the lower order versions, and observed roughly a factor of 2 speed increase in compiled execution. My speed tests weren't rigorous - I didn't test it for a variety of norms - but I believe this should speed things up in all cases (again, unless I'm missing something with lax behaviour). Edit: This is on CPU.\r\n\r\nThoughts?\r\n\r\nP.s. In some of my use cases of jax, the most expensive piece is a ton of matrix exponentials, so I'm very interested in potential speed gains for this function :D.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/5647/comments",
    "author": "DanPuzzuoli",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2021-02-05T16:44:57Z",
        "body": "This sounds right to me! Indeed using `jnp.select` is wasting compute, and `cond`/`switch` could save it. I don't remember any context around why `expm` was written this way (@zhangqiaorjc do you remember?) but it may just have been for simplicity. In that case, what you describe would be a welcome upgrade!"
      },
      {
        "user": "shoyer",
        "created_at": "2021-02-05T17:47:40Z",
        "body": "I guess that on GPU it _might_ be better to compute everything rather than pass control flow back to the host. But that's really an optimization problem for XLA GPU, not something we should worry about in JAX.\r\n\r\nIn general, matrix-matrix multiplication (L3 BLAS) seems like the level of computation where we should be using `cond` rather than `where`."
      },
      {
        "user": "mattjj",
        "created_at": "2021-02-05T20:22:24Z",
        "body": "Does `cond` always pass control flow back to the host on GPU? That sounds plausible, though also it seems like at least in some cases XLA:GPU could avoid that. (At the very least, it could decide to optimize a `cond` into a `select` if needed!) On TPU (where control flow can stay on the device) and CPU (where there's no host/device separation) it seems profitable.\r\n\r\nGood rule of thumb!"
      },
      {
        "user": "DanPuzzuoli",
        "created_at": "2021-02-08T14:37:10Z",
        "body": "If there are no objections, given how small this is, and my personal interest in seeing it go through asap, I'm willing to implement this change. My plan would be to just change it to use nested `cond` statements. In a PR I can show some local CPU benchmarks."
      },
      {
        "user": "zhangqiaorjc",
        "created_at": "2021-02-24T03:44:56Z",
        "body": "@DanPuzzuoli Feel free to send in a PR. I don't think anyone from JAX team is currently working on this. Thanks!"
      }
    ]
  },
  {
    "number": 5554,
    "title": "[Feature request] option to make jax.random.split(1, n) return tuple of 1 and n keys",
    "created_at": "2021-01-29T12:49:44Z",
    "closed_at": "2021-01-29T16:56:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/5554",
    "body": "\r\nMotivation:\r\nI am trying to create a batch of keys, and I also need to update the original key. \r\nHere is what I am using:\r\n```\r\ndef get_batch_keys(key, batch_size):\r\n    keys = jax.random.split(key, batch_size+1)\r\n    return keys[0], keys[1:]\r\n```\r\nDoes this make sense? It could be a flag or a new function.\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/5554/comments",
    "author": "cccntu",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2021-01-29T16:56:45Z",
        "body": "Thanks for the question! That looks like a good way to write it.\r\n\r\nWe probably wouldn't add this to JAX as a wrapper function or flag, but it seems like a nice short function to have in your own code if it does what you want."
      }
    ]
  },
  {
    "number": 5530,
    "title": "'jaxlib.cusolver' has no attribute 'potrf'",
    "created_at": "2021-01-27T11:41:15Z",
    "closed_at": "2021-01-27T22:04:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/5530",
    "body": "With the latest jax (0.2.9) and jaxlib (0.1.59) from conda-forge I cannot import jax:\r\n\r\n```\r\nimport jax\r\n~/anaconda3/lib/python3.7/site-packages/jax/__init__.py in <module>\r\n     91 # These submodules are separate because they are in an import cycle with\r\n     92 # jax and rely on the names imported above.\r\n---> 93 from . import image\r\n     94 from . import lax\r\n     95 from . import nn\r\n\r\n~/anaconda3/lib/python3.7/site-packages/jax/image/__init__.py in <module>\r\n     16 \r\n     17 # flake8: noqa: F401\r\n---> 18 from jax._src.image.scale import (\r\n     19   resize,\r\n     20   ResizeMethod,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/jax/_src/image/scale.py in <module>\r\n     18 \r\n     19 from jax import jit\r\n---> 20 from jax import lax\r\n     21 from jax import numpy as jnp\r\n     22 import numpy as np\r\n\r\n~/anaconda3/lib/python3.7/site-packages/jax/lax/__init__.py in <module>\r\n    349   conv_general_dilated_patches\r\n    350 )\r\n--> 351 from . import linalg\r\n\r\n~/anaconda3/lib/python3.7/site-packages/jax/lax/linalg.py in <module>\r\n     14 \r\n     15 # flake8: noqa: F401\r\n---> 16 from jax._src.lax.linalg import (\r\n     17   cholesky,\r\n     18   cholesky_p,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/jax/_src/lax/linalg.py in <module>\r\n    342 if cusolver is not None:\r\n    343   xla.backend_specific_translations['gpu'][cholesky_p] = partial(\r\n--> 344     _cholesky_cpu_gpu_translation_rule, cusolver.potrf)\r\n    345 \r\n    346 if rocsolver is not None:\r\n\r\nAttributeError: module 'jaxlib.cusolver' has no attribute 'potrf'\r\n```\r\n\r\nIt worked before the upgrade.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/5530/comments",
    "author": "gurgeh",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2021-01-27T17:53:05Z",
        "body": "@hawkinsp any ideas?"
      },
      {
        "user": "hawkinsp",
        "created_at": "2021-01-27T19:07:54Z",
        "body": "We don't provide the `conda-forge` builds, the community does, but let's try to figure this out...\r\n\r\nIs this with a CPU jaxlib or a GPU jaxlib?\r\n\r\nIf it's a CPU jaxlib (I'm pretty sure the `conda-forge` builds are CPU-only), I'm wondering if something stale is left over in your `jaxlib` installation. Can you try deleting `jaxlib`, verifying that its installed path is gone, and reinstalling it? `cusolver.py` is no longer included in `jaxlib` on CPU. So I'm wondering whether a stale version was left from a previous installation somehow.\r\n"
      },
      {
        "user": "gurgeh",
        "created_at": "2021-01-27T22:04:40Z",
        "body": "You are correct! For some reason the jaxlib-directory contained two 1 year old files, cusolver.py and cuda_prng.py. I removed them and now it works.\r\nThank you both for a quick response and a great project!"
      }
    ]
  },
  {
    "number": 5091,
    "title": "RuntimeError: Invalid argument: Unknown NumPy type f size 16",
    "created_at": "2020-12-03T18:53:40Z",
    "closed_at": "2020-12-03T19:20:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/5091",
    "body": "Can anyone explain this error?\r\n`\r\n  File \"/python3.8/site-packages/jax/_src/numpy/lax_numpy.py\", line 2540, in array\r\n    out = _device_put_raw(object)\r\n  File \"/python3.8/site-packages/jax/_src/lax/lax.py\", line 1439, in _device_put_raw\r\n    return xla.array_result_handler(None, aval)(*xla.device_put(x))\r\n  File \"/python3.8/site-packages/jax/interpreters/xla.py\", line 127, in device_put\r\n    return device_put_handlers[type(x)](x, device)\r\n  File \"/python3.8/site-packages/jax/interpreters/xla.py\", line 135, in _device_put_array\r\n    return (backend.buffer_from_pyval(x, device),)\r\nRuntimeError: Invalid argument: Unknown NumPy type f size 16\r\n`\r\nI am using scipy.optimize.fsolve with NDArray of jax.numpy.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/5091/comments",
    "author": "s-mostafa-a",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2020-12-03T18:56:12Z",
        "body": "Can you please provide a minimal reproduction of the problem?"
      },
      {
        "user": "s-mostafa-a",
        "created_at": "2020-12-03T19:16:13Z",
        "body": "I am converting a onp.float128 NDArray to jnp NDArray.\r\n(It was not related to fsolve at all)"
      },
      {
        "user": "hawkinsp",
        "created_at": "2020-12-03T19:20:42Z",
        "body": "That's simple enough then: we don't support `float128`. (I don't think we plan to, either, since NumPy's `float128` appears to be something of a mess.)"
      }
    ]
  },
  {
    "number": 5013,
    "title": "Masking + Jit to deal with dynamic/variable shape arrays?",
    "created_at": "2020-11-25T10:41:05Z",
    "closed_at": "2021-01-10T00:00:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/5013",
    "body": "Hi!\r\n\r\nI have been struggling with the issue that my array cannot change the shapes of my array with every iteration if I use the JIT compiler. I tried to follow #2521 , but everything seems in the air.\r\n\r\nThe goal is to compute the following, where jnp.sum(block_lengths) result/value **changes** with every iteration( m and l are static/concrete !!)\r\n\r\n`u = random.randint(rng_key, (jnp.sum(block_lengths), ), 0, m)`\r\n\r\nI get the following error:\r\n\r\n```\r\n  File \"/home/.../numpyro/numpyro/contrib/hmcecs.py\", line 128, in _sample_u_poisson\r\n    u = random.randint(sub_key, (jnp.sum(block_lengths), m), 0, m)\r\n  File \"/home/.../anaconda3/lib/python3.7/site-packages/jax/random.py\", line 442, in randint\r\n    shape = abstract_arrays.canonicalize_shape(shape)\r\n  File \"/home/.../anaconda3/lib/python3.7/site-packages/jax/core.py\", line 1130, in canonicalize_shape\r\n    raise TypeError(msg.format(shape))\r\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[]):JaxprTrace(level=-1/1)>, 22).\r\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\r\n```\r\n\r\nI have been wondering around several issues and possible solutions, including **static arguments** ,  **masking**, but none have worked and I end up back in the same error.\r\n\r\n_Static arguments approach:_\r\n\r\n```\r\nb = jnp.sum(block_lengths)\r\nu_random = jit(random.randint, static_argnums=(0,1, 2,3))\r\nu = u_random((sub_key, (b,m), 0, m))\r\n\r\n```\r\n\r\n**Error** Static args:\r\n\r\n```\r\n  File \"/home/.../numpyro/numpyro/contrib/hmcecs.py\", line 138, in _sample_u_poisson\r\n    u = u_random(sub_key, (b, m), 0, m)\r\n  File \"/home/../anaconda3/lib/python3.7/site-packages/jax/_src/traceback_util.py\", line 133, in reraise_with_filtered_traceback\r\n    return fun(*args, **kwargs)\r\n  File \"/home/.../anaconda3/lib/python3.7/site-packages/jax/api.py\", line 207, in f_jitted\r\n    f, dyn_args = argnums_partial_except(f, static_argnums, args)\r\n  File \"/home/.../anaconda3/lib/python3.7/site-packages/jax/api_util.py\", line 90, in argnums_partial_except\r\n    \"Non-hashable static arguments are not supported, as this can lead \"\r\nValueError: Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 0) of type <class 'numpy.ndarray'> for function randint is non-hashable.\r\n\r\n```\r\n_Masking approach A:_\r\n\r\n```\r\n   @partial(mask, in_shapes=['(_,)'], out_shape='(_, _)')\r\n   def u_rand(block_lenghts):\r\n        b = jnp.sum(block_lengths)\r\n        return random.randint(sub_key, (b,m), 0, m)\r\n   u = u_rand([block_lengths],{} )\r\n\r\n```\r\n**Error** masking A:: Back to same original error\r\n\r\n```\r\n File \"/home/.../numpyro/numpyro/contrib/hmcecs.py\", line 134, in _sample_u_poisson\r\n    u = u_rand([block_lengths],{})#dict(b=jnp.sum(block_lengths).astype(int),m=m,l=l))\r\n  File \"/home/../anaconda3/lib/python3.7/site-packages/jax/api.py\", line 1592, in wrapped_fun\r\n    flat_fun, logical_env, padded_env, args_flat, in_shapes)\r\n  File \"/home/../anaconda3/lib/python3.7/site-packages/jax/interpreters/masking.py\", line 85, in mask_fun\r\n    out_vals = fun.call_wrapped(*(logical_env_vals + in_vals))\r\n  File \"/home/lys/anaconda3/lib/python3.7/site-packages/jax/linear_util.py\", line 160, in call_wrapped\r\n    ans = self.f(*args, **dict(self.params, **kwargs))\r\n  File \"/home/.../numpyro/numpyro/contrib/hmcecs.py\", line 133, in u_rand\r\n    return random.randint(sub_key, (b,m), 0, m)\r\n  File \"/home/lys/anaconda3/lib/python3.7/site-packages/jax/random.py\", line 442, in randint\r\n    shape = abstract_arrays.canonicalize_shape(shape)\r\n  File \"/home/.../anaconda3/lib/python3.7/site-packages/jax/core.py\", line 1130, in canonicalize_shape\r\n    raise TypeError(msg.format(shape))\r\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[]):JaxprTrace(level=-1/1)>, 22).\r\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\r\n```\r\n\r\n\r\n_Masking approach B:_\r\n```\r\n  @partial(mask, in_shapes=['(l,)'], out_shape='(b, m)')\r\n   def u_rand(block_lenghts):\r\n        b = jnp.sum(block_lengths)\r\n        return random.randint(sub_key, (b,m), 0, m)\r\n   u = u_rand([block_lengths],dict(b=jnp.sum(block_lengths).astype(int),m=m,l=l))\r\n```\r\n\r\n\r\n**Error** masking B: The block lenghts is a ShapeArray, so I cannot reach it's concrete value\r\n\r\n```\r\n  File \"/home/.../numpyro/numpyro/contrib/hmcecs.py\", line 764, in _init_state\r\n    self._u = _sample_u_poisson(rng_key, self.m, self._l)\r\n  File \"/home/.../numpyro/numpyro/contrib/hmcecs.py\", line 134, in _sample_u_poisson\r\n    u = u_rand([block_lengths],dict(b=jnp.sum(block_lengths).astype(int),m=m,l=l))\r\n  File \"/home/.../anaconda3/lib/python3.7/site-packages/jax/api.py\", line 1602, in wrapped_fun\r\n    masking.check_shapes(out_specs, out_spec_tree, list(out_shapes), out_tree)\r\n  File \"/home/.../anaconda3/lib/python3.7/site-packages/jax/interpreters/masking.py\", line 489, in check_shapes\r\n    raise ShapeError(f\"{message_prefix} shapes should be {specs} but are {shapes}.\")\r\njax.interpreters.masking.ShapeError: Output shapes should be ShapeSpec(b, m) but are (58, 22).\r\n```\r\n\r\nI am not sure whether what I am trying to accomplish is doable at the moment. \r\n\r\nThanks so much for your attention and help!\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/5013/comments",
    "author": "LysSanzMoreta",
    "comments": [
      {
        "user": "LysSanzMoreta",
        "created_at": "2020-11-30T13:50:41Z",
        "body": "Hi! Does anyone have any insights on this issue? Thanks!!"
      },
      {
        "user": "mattjj",
        "created_at": "2021-01-09T18:56:54Z",
        "body": "Hey, sorry for the long delay.\r\n\r\nIf you're still interested in this, can you provide a complete runnable example of what you'd like to accomplish? I'm thinking of something that runs without `jit`, but that you'd ideally like to stage out with `jit` and have it keep working.\r\n\r\n(If you're not still interested in pursuing this, let's close the issue!)"
      },
      {
        "user": "OlaRonning",
        "created_at": "2021-01-09T19:14:24Z",
        "body": "Hi @mattjj,\n\nAwesome you'll take a look at it, there is a simple runnable example here #5100. "
      },
      {
        "user": "LysSanzMoreta",
        "created_at": "2021-01-09T21:04:48Z",
        "body": "Thanks @mattjj ! Yes, now @OlaRonning is mainly taking care of the code but we are both interested :D"
      },
      {
        "user": "mattjj",
        "created_at": "2021-01-10T00:00:32Z",
        "body": "That's a great example!\r\n\r\nHowever, it doesn't fit into what `mask` can support: the `mask` transformation (which is really just a prototype) wasn't designed to handle arrays with shapes that depend on values computed at runtime, like random samples. Instead, it only handles shapes of intermediates that have a polynomial dependence on arguments to the function being `mask`-transformed (in particular on the shapes of other arguments).\r\n\r\nThis is the sort of thing we'd like to be able to support in the future, but `mask` isn't enough. For now the only option is not to `jit` this code. You can still apply `jit` to other functions and subroutines, but not the `jax.random.randint(sub_key, (rand_shape.sum(), 3), 0, n)` part.\r\n\r\nWhile we're actively working on improvements that will help here, I think it's best to close this issue for now (since otherwise we might have a proliferation of such issues waiting for dynamic shape support in `jit`). Perhaps check back in a couple months for progress!"
      }
    ]
  },
  {
    "number": 4853,
    "title": "Jax saves forward-pass intermediate values under lax.stop_gradient",
    "created_at": "2020-11-10T08:48:04Z",
    "closed_at": "2020-11-11T05:51:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4853",
    "body": "The following code illustrates the problem:\r\n\r\n```\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import jit, grad\r\n\r\nnum_iters = 10_000\r\ndim = 1_000\r\n\r\ndef long_scan(X):\r\n  def scan_inner(carry, _):\r\n    return carry @ X, None\r\n  \r\n  carry, _ = jax.lax.scan(scan_inner, X, None, length=num_iters)\r\n  return carry\r\n\r\n@jit\r\ndef outer(x):\r\n  scan_out = long_scan(x)\r\n  scan_out = jax.lax.stop_gradient(scan_out)\r\n  return jnp.sum(x @ scan_out)\r\n\r\ninput_matrix = jax.random.normal(jax.random.PRNGKey(0), shape=(dim, dim))\r\nouter(input_matrix).block_until_ready()\r\nprint('Does forward pass OK')\r\ngrad(outer)(input_matrix).block_until_ready()\r\n```\r\n\r\nWhen run on the colab GPU we get `RuntimeError: Resource exhausted: Out of memory while trying to allocate 40004000128 bytes.` More generally, the memory usage scales with the length of the scan. As far as I understand, normally that makes sense--the intermediate values have to be saved for the reverse pass of the grad. But here, those intermediate values are never used because of the `stop gradient`. \r\n\r\nI think we can avoid the memory growth by using `remat(scan_inner)` instead of `scan_inner` inside the scan (like in #3186), but it would be great if jax could automatically do this, since we should never need the intermediate values. \r\n\r\nThe actual use-case is adversarial training, where the `long_scan` computes adversarial inputs for a model but we don't take the gradient wrt the model parameters through the process of computing those inputs. ",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4853/comments",
    "author": "C-J-Cundy",
    "comments": [
      {
        "user": "shoyer",
        "created_at": "2020-11-10T17:00:39Z",
        "body": "Have you tried `long_scan(stop_gradient(x))` instead?\r\n\r\n`stop_gradient()` actually get applied during the JVP calculation from the forward pass"
      },
      {
        "user": "C-J-Cundy",
        "created_at": "2020-11-10T17:46:16Z",
        "body": "~`long_scan(stop_gradient(x))` also runs out of memory.~ (not true, see below)\r\nI can get it to not save intermediate values by using a version of `long_scan` with `scan_inner` stopping the gradient in each iteration:\r\n\r\n```\r\ndef long_scan_stopped(X):\r\n  def scan_inner(carry, _):\r\n    return jax.lax.stop_gradient(carry @ X), jax.lax.stop_gradient(None)\r\n  \r\n  carry, _ = jax.lax.scan(scan_inner, X, None, length=num_iters)\r\n  return carry\r\n```\r\n\r\nIt would be nice if jax could do this automatically though, since it seems like a bug if it's storing intermediate values that we know are never used. "
      },
      {
        "user": "mattjj",
        "created_at": "2020-11-10T20:43:22Z",
        "body": "Are you willing to put a `jit` on the outside, as in `jit(grad(outer))(input_matrix)`? That way XLA will do the memory pruning for you."
      },
      {
        "user": "mattjj",
        "created_at": "2020-11-10T21:59:17Z",
        "body": "It's really surprising to me that @shoyer's suggestion didn't work!\r\n\r\nHere's a look at the forward and backward passes of the original code as jaxprs (I tweaked the jaxpr pretty-printing to show us shapes of jaxpr invars and outvars):\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import jit, grad\r\n\r\nnum_iters = 10_000\r\ndim = 1_000\r\n\r\ndef long_scan(X):\r\n  def scan_inner(carry, _):\r\n    return carry @ X, None\r\n  \r\n  carry, _ = jax.lax.scan(scan_inner, X, None, length=num_iters)\r\n  return carry\r\n\r\n@jit\r\ndef outer(x):\r\n  scan_out = long_scan(x)\r\n  scan_out = jax.lax.stop_gradient(scan_out)\r\n  return jnp.sum(x @ scan_out)\r\n\r\ninput_matrix = jax.random.normal(jax.random.PRNGKey(0), shape=(dim, dim))\r\nouter(input_matrix).block_until_ready()\r\nprint('Does forward pass OK')\r\ngrad(outer)(input_matrix).block_until_ready()\r\n```\r\n\r\n```\r\n=== forward pass ===\r\n{ lambda  ; a:float32[1000,1000].\r\n  let b _ c = xla_call[ backend=None\r\n                        call_jaxpr={ lambda  ; a:float32[1000,1000] b:*.\r\n                                     let c _ _ _ =\r\n                                           scan[ jaxpr={ lambda  ; e:float32[1000,1000] a:* b:* c:float32[1000,1000] d:*.\r\n                                                         let f = dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\r\n                                                                              precision=None ] c e\r\n                                                         in (f:float32[1000,1000] *:* *:* c:float32[1000,1000]) }\r\n                                                 length=10000\r\n                                                 linear=(False, True, True, False, True)\r\n                                                 num_carry=2\r\n                                                 num_consts=3\r\n                                                 reverse=False\r\n                                                 unroll=1 ] a * * a *\r\n                                         d = stop_gradient c\r\n                                         e = dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\r\n                                                          precision=None ] a d\r\n                                         f = reduce_sum[ axes=(0, 1) ] e\r\n                                     in (f:float32[] *:* d:float32[1000,1000]) }\r\n                        device=None\r\n                        donated_invars=(False, False)\r\n                        name=jvp(outer) ] a *\r\n  in (b:float32[] c:float32[1000,1000]) }\r\n\r\n=== backward pass ===\r\n{ lambda a ; b:float32[].\r\n  let c = xla_call[ backend=None\r\n                    call_jaxpr={ lambda  ; a:float32[1000,1000] b:float32[].\r\n                                 let c = broadcast_in_dim[ broadcast_dimensions=(  )\r\n                                                           shape=(1000, 1000) ] b\r\n                                     d = dot_general[ dimension_numbers=(((1,), (1,)), ((), ()))\r\n                                                      precision=None ] c a\r\n                                 in (d:float32[1000,1000]) }\r\n                    device=None\r\n                    donated_invars=(False, False)\r\n                    name=transpose(jvp(outer)) ] a b\r\n  in (c:float32[1000,1000]) }\r\n```\r\n\r\nIt's a bit subtle to read, but the fourth `scan` output is going to be of shape `(10000, 1000, 1000)` here. It's unused in the outer jaxpr (which is why it is assigned to an underscore) but it'll still be computed in the forward pass.\r\n\r\nApplying @shoyer's suggestion:\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import jit, grad\r\n\r\nnum_iters = 10_000\r\ndim = 1_000\r\n\r\ndef long_scan(X):\r\n  def scan_inner(carry, _):\r\n    return carry @ X, None\r\n\r\n  carry, _ = jax.lax.scan(scan_inner, X, None, length=num_iters)\r\n  return carry\r\n\r\n@jit\r\ndef outer(x):\r\n  scan_out = long_scan(jax.lax.stop_gradient(x))\r\n  return jnp.sum(x @ scan_out)\r\n\r\ninput_matrix = jax.random.normal(jax.random.PRNGKey(0), shape=(dim, dim))\r\n\r\nfwd_jaxpr = jax.make_jaxpr(lambda x: jax.vjp(outer, x))(input_matrix)\r\nprint('=== forward pass ===')\r\nprint(fwd_jaxpr)\r\n\r\noutput, outer_vjp = jax.vjp(outer, input_matrix)\r\nbwd_jaxpr = jax.make_jaxpr(outer_vjp)(output)\r\nprint('=== backward pass ===')\r\nprint(bwd_jaxpr)\r\n```\r\n\r\n```\r\n=== forward pass ===\r\n{ lambda  ; a:float32[1000,1000].\r\n  let b _ c = xla_call[ backend=None\r\n                        call_jaxpr={ lambda  ; a:float32[1000,1000] b:*.\r\n                                     let c = stop_gradient a\r\n                                         d = scan[ jaxpr={ lambda  ; a:float32[1000,1000] b:float32[1000,1000].\r\n                                                           let c = dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\r\n                                                                                precision=None ] b a\r\n                                                           in (c:float32[1000,1000]) }\r\n                                                   length=10000\r\n                                                   linear=(False, False)\r\n                                                   num_carry=1\r\n                                                   num_consts=1\r\n                                                   reverse=False\r\n                                                   unroll=1 ] c c\r\n                                         e = dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\r\n                                                          precision=None ] a d\r\n                                         f = reduce_sum[ axes=(0, 1) ] e\r\n                                     in (f:float32[] *:* d:float32[1000,1000]) }\r\n                        device=None\r\n                        donated_invars=(False, False)\r\n                        name=jvp(outer) ] a *\r\n  in (b:float32[] c:float32[1000,1000]) }\r\n\r\n=== backward pass ===\r\n{ lambda a ; b:float32[].\r\n  let c = xla_call[ backend=None\r\n                    call_jaxpr={ lambda  ; a:float32[1000,1000] b:float32[].\r\n                                 let c = broadcast_in_dim[ broadcast_dimensions=(  )\r\n                                                           shape=(1000, 1000) ] b\r\n                                     d = dot_general[ dimension_numbers=(((1,), (1,)), ((), ()))\r\n                                                      precision=None ] c a\r\n                                 in (d:float32[1000,1000]) }\r\n                    device=None\r\n                    donated_invars=(False, False)\r\n                    name=transpose(jvp(outer)) ] a b\r\n  in (c:float32[1000,1000]) }\r\n```\r\n\r\nIt sure looks to me like the issue is gone: the scan has no scanned-over outputs whatsoever now, and only outputs the final value of the carry.\r\n\r\n@C-J-Cundy maybe the OOM issue with `long_scan(stop_gradient(x))` has some other cause, rather than this scan? Is it worth double-checking?"
      },
      {
        "user": "C-J-Cundy",
        "created_at": "2020-11-10T22:39:27Z",
        "body": "@mattjj, you're completely right, @shoyer's suggestion did work. \r\nI misread the suggestion as ` scan_out = jax.lax.stop_gradient(long_scan(x))` (which didn't work) instead of \r\n`long_scan(jax.lax.stop_gradient(x))`. My mistake! 🤦‍♀️\r\n\r\nInterestingly, it seems like the memory pruning doesn't get done at the XLA level with jit-of-grad.\r\nIf I take the initial example and change the last line to \r\n`jit(grad(outer))(input_matrix).block_until_ready()` (and remove the @jit on outer) then I still get an OOM error. \r\n\r\n"
      },
      {
        "user": "mattjj",
        "created_at": "2020-11-11T05:51:28Z",
        "body": "Hrm interesting, I wonder if somehow XLA is missing the optimization.\r\n\r\nGlad to hear that putting stop_gradient earlier fixes things! I think that's the best solution; to notice this optimization automatically is tricky in the grad-of-jit situation, basically because grad thinks it's operating eagerly (i.e. it lives in a \"dynamic graph\" world and doesn't do any compiler-y optimizations). When doing jit-of-grad (or jit-of-grad-of-jit) I'd expect XLA to take care of this optimization for us, but it sounds like it's missing it, at least on the backend you're using.\r\n\r\nIn general it seems it's a good idea to put stop_gradient as early as possible.\r\n\r\nIf it's alright with you, I'll close this issue, but let me know if we should reopen it, and don't hesitate to open new issues!"
      }
    ]
  },
  {
    "number": 4743,
    "title": "Use system-wide flatbuffers",
    "created_at": "2020-10-30T16:28:49Z",
    "closed_at": "2021-05-11T01:12:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4743",
    "body": "The old libstdc++ problem, with flatbuffers\r\n\r\n```\r\nINFO: Found 1 target...\r\n[0 / 21] [Prepa] BazelWorkspaceStatusAction stable-status.txt\r\n[379 / 3,869] Executing genrule @local_config_cuda//cuda:cuda-lib; 8s local ... (80 actions, 79 running)\r\n[569 / 3,869] Executing genrule @local_config_cuda//cuda:cuda-lib; 16s local ... (80 actions, 79 running)\r\n[686 / 3,869] Executing genrule @local_config_python//:numpy_include; 26s local ... (80 actions, 79 running)\r\n[828 / 3,869] Executing genrule @local_config_python//:python_include; 37s local ... (80 actions, 79 running)\r\n[939 / 3,869] Executing genrule @local_config_python//:python_include; 50s local ... (80 actions, 79 running)\r\n[1,125 / 3,869] Executing genrule @local_config_python//:python_include; 65s local ... (80 actions, 79 running)\r\n[1,385 / 3,869] Executing genrule @local_config_python//:python_include; 82s local ... (80 actions, 79 running)\r\nERROR: /dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5/jax-jax-v0.2.5/jaxlib/BUILD:187:22: Generating flatbuffer files for <source file jaxlib/pocketfft.fbs>: failed (Exit 1): flatc failed: error executing command \r\n  (cd /dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5/.home/.cache/bazel/_bazel_strube1/e32c15cefdda779df66db3467646629f/execroot/__main__ && \\\r\n  exec env - \\\r\n  bazel-out/k8-opt/bin/external/flatbuffers/flatc --python -o bazel-out/k8-opt/bin/jaxlib/pocketfft_flatbuffers_py_srcs_no_include_all -I ./ -I bazel-out/k8-opt/bin -I bazel-out/k8-opt/bin --no-includes --no-union-value-namespacing --gen-object-api jaxlib/pocketfft.fbs)\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/k8-opt/bin/external/flatbuffers/flatc: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by bazel-out/k8-opt/bin/external/flatbuffers/flatc)\r\nTarget //build:install_xla_in_source_tree failed to build\r\nINFO: Elapsed time: 141.896s, Critical Path: 93.97s\r\nINFO: 2679 processes: 1761 internal, 918 local.\r\nFAILED: Build did NOT complete successfully\r\nERROR: Build failed. Not running target\r\nFAILED: Build did NOT complete successfully\r\n\r\n     _   _  __  __\r\n    | | / \\ \\ \\/ /\r\n _  | |/ _ \\ \\  /\r\n| |_| / ___ \\/  \\\r\n \\___/_/   \\/_/\\_\\\r\n\r\n\r\nBazel binary path: /p/software/juwels/stages/Devel-2020/software/Bazel/3.6.0-GCCcore-9.3.0/bin/bazel\r\nPython binary path: /p/software/juwels/stages/Devel-2020/software/Python/3.8.5-GCCcore-9.3.0/bin/python\r\nPython version: 3.8\r\nMKL-DNN enabled: no\r\n-march=native: no\r\nCUDA enabled: yes\r\nCUDA toolkit path: /p/software/juwels/stages/Devel-2020/software/CUDA/11.0\r\nCUDNN library path: /p/software/juwels/stages/Devel-2020/software/cuDNN/8.0.2.39-CUDA-11.0\r\nCUDA compute capabilities: 3.5,5.2,6.0,6.1,7.0\r\n\r\nBuilding XLA and installing it in the jaxlib source tree...\r\n/p/software/juwels/stages/Devel-2020/software/Bazel/3.6.0-GCCcore-9.3.0/bin/bazel run --verbose_failures=true --action_env=PYTHONPATH --config=short_logs --config=cuda --define=xla_python_enable_gpu=true :install_xla_in_source_tree /dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5/jax-jax-v0.2.5/build\r\nTraceback (most recent call last):\r\n  File \"build/build.py\", line 380, in <module>\r\n    main()\r\n  File \"build/build.py\", line 375, in main\r\n    shell(command)\r\n  File \"build/build.py\", line 47, in shell\r\n    output = subprocess.check_output(cmd)\r\n  File \"/p/software/juwels/stages/Devel-2020/software/Python/3.8.5-GCCcore-9.3.0/lib/python3.8/subprocess.py\", line 411, in check_output\r\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\r\n  File \"/p/software/juwels/stages/Devel-2020/software/Python/3.8.5-GCCcore-9.3.0/lib/python3.8/subprocess.py\", line 512, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['/p/software/juwels/stages/Devel-2020/software/Bazel/3.6.0-GCCcore-9.3.0/bin/bazel', 'run', '--verbose_failures=true', '--action_env=PYTHONPATH', '--config=short_logs', '--config=cuda', '--define=xla_python_enable_gpu=true', ':install_xla_in_source_tree', '/dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5/jax-jax-v0.2.5/build']' returned non-zero exit status 1.\r\n (at easybuild/tools/run.py:533 in parse_cmd_output)\r\n== 2020-10-30 17:15:58,223 filetools.py:1610 INFO Removing lock /p/software/juwels/stages/Devel-2020/software/.locks/_p_software_juwels_stages_Devel-2020_software_JAX_0.2.5-gpsmkl-2020-Python-3.8.5.lock...\r\n== 2020-10-30 17:15:58,224 filetools.py:330 INFO Path /p/software/juwels/stages/Devel-2020/software/.locks/_p_software_juwels_stages_Devel-2020_software_JAX_0.2.5-gpsmkl-2020-Python-3.8.5.lock successfully removed.\r\n== 2020-10-30 17:15:58,224 filetools.py:1614 INFO Lock removed: /p/software/juwels/stages/Devel-2020/software/.locks/_p_software_juwels_stages_Devel-2020_software_JAX_0.2.5-gpsmkl-2020-Python-3.8.5.lock\r\n== 2020-10-30 17:15:58,224 easyblock.py:3311 WARNING build failed (first 300 chars): cmd \" export TF_CUDA_PATHS=${EBROOTCUDA} && export HOME=/dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5/.home/ && python build/build.py --enable_cuda --cuda_path ${EBROOTCUDA} --cudnn_path ${EBROOTCUDNN} --bazel_path ${EBROOTBAZEL}/bin/bazel --bazel_options=--action_env=PYTHONPATH --noenable_mkl\r\n== 2020-10-30 17:15:58,224 easyblock.py:295 INFO Closing log for application name JAX version 0.2.5\r\n== 2020-10-30 17:15:58,224 build_log.py:265 INFO FAILED: Installation ended unsuccessfully (build directory: /dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5): build failed (first 300 chars): cmd \" export TF_CUDA_PATHS=${EBROOTCUDA} && export HOME=/dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5/.home/ && python build/build.py --enable_cuda --cuda_path ${EBROOTCUDA} --cudnn_path ${EBROOTCUDNN} --bazel_path ${EBROOTBAZEL}/bin/bazel --bazel_options=--action_env=PYTHONPATH --noenable_mkl (took 2 min 29 sec)\r\n== FAILED: Installation ended unsuccessfully (build directory: /dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5): build failed (first 300 chars): cmd \" export TF_CUDA_PATHS=${EBROOTCUDA} && export HOME=/dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5/.home/ && python build/build.py --enable_cuda --cuda_path ${EBROOTCUDA} --cudnn_path ${EBROOTCUDNN} --bazel_path ${EBROOTBAZEL}/bin/bazel --bazel_options=--action_env=PYTHONPATH --noenable_mkl (took 2 min 29 sec)\r\n== 2020-10-30 17:15:58,225 build_log.py:265 INFO Results of the build can be found in the log file(s) /tmp/eb-b4d0za4p/easybuild-JAX-0.2.5-20201030.171329.wPoIS.log\r\n== Results of the build can be found in the log file(s) /tmp/eb-b4d0za4p/easybuild-JAX-0.2.5-20201030.171329.wPoIS.log\r\n== 2020-10-30 17:15:58,225 build_log.py:169 ERROR EasyBuild crashed with an error (at easybuild/base/exceptions.py:124 in __init__): build failed (first 300 chars): cmd \" export TF_CUDA_PATHS=${EBROOTCUDA} && export HOME=/dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5/.home/ && python build/build.py --enable_cuda --cuda_path ${EBROOTCUDA} --cudnn_path ${EBROOTCUDNN} --bazel_path ${EBROOTBAZEL}/bin/bazel --bazel_options=--action_env=PYTHONPATH --noenable_mkl (at easybuild/main.py:118 in build_and_install_software)\r\n== 2020-10-30 17:15:58,227 build_log.py:169 ERROR EasyBuild crashed with an error (at easybuild/base/exceptions.py:124 in __init__): Build of /p/project/cjsc/a.strube/zlixo/2020-2/easybuild-repository/Golden_Repo/j/JAX/JAX-0.2.5-gpsmkl-2020-Python-3.8.5.eb failed (err: 'build failed (first 300 chars): cmd \" export TF_CUDA_PATHS=${EBROOTCUDA} && export HOME=/dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5/.home/ && python build/build.py --enable_cuda --cuda_path ${EBROOTCUDA} --cudnn_path ${EBROOTCUDNN} --bazel_path ${EBROOTBAZEL}/bin/bazel --bazel_options=--action_env=PYTHONPATH --noenable_mkl') (at easybuild/main.py:150 in build_and_install_software)\r\nERROR: Build of /p/project/cjsc/a.strube/zlixo/2020-2/easybuild-repository/Golden_Repo/j/JAX/JAX-0.2.5-gpsmkl-2020-Python-3.8.5.eb failed (err: 'build failed (first 300 chars): cmd \" export TF_CUDA_PATHS=${EBROOTCUDA} && export HOME=/dev/shm/strube1/JAX/0.2.5/gpsmkl-2020-Python-3.8.5/.home/ && python build/build.py --enable_cuda --cuda_path ${EBROOTCUDA} --cudnn_path ${EBROOTCUDNN} --bazel_path ${EBROOTBAZEL}/bin/bazel --bazel_options=--action_env=PYTHONPATH --noenable_mkl')\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4743/comments",
    "author": "surak",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2020-11-05T14:37:58Z",
        "body": "I don't think your issue is related to flatbuffers per se. The issue is that the `flatc` compiler built by bazel is not able to run successfully on your machine, and that would be an issue for other binaries (e.g., JAX itself!) even if we used a system-provided `flatc`. I'm not sure what's going on, but without more details it's impossible for me to debug this. Can you provide details of your build environment? Can I reproduce this problem, in, for example, a cloud VM set up a particular way?"
      },
      {
        "user": "surak",
        "created_at": "2020-11-05T14:45:17Z",
        "body": "This is on 3 of our supercomputers, JUWELS/JUWELSBOOSTER, JUSUF and a smaller HDFML cluster.\r\n\r\nEverything is built from source on them, by using EasyBuild (as ou can see by the comment). That goes all the way down from the softwares necessary to bootstrap GCC.\r\n\r\nThis is a regression from 0.1.77, which is what I am using now, instead of 0.2.5, as it works for me. \r\n\r\nIt happens when part of the compilation uses the environment-set stdlibc++, but later, bazel decides to use the hard-coded version in /usr/lib64 or something similar, which is older and will fail. \r\n\r\n"
      },
      {
        "user": "hawkinsp",
        "created_at": "2020-11-05T14:54:52Z",
        "body": "It's going to be very difficult for me to debug this problem if I can't reproduce it somehow.\r\n\r\nI also wonder whether it might be preferable to use a prebuilt jaxlib rather than building one from scratch. Is that possible? Could we make it possible?\r\n\r\nOne thing I might try is setting the `LD_LIBRARY_PATH` environment to point to the directory with the correct `libstdc++`. Does that help?"
      },
      {
        "user": "surak",
        "created_at": "2020-11-05T16:09:04Z",
        "body": "> It's going to be very difficult for me to debug this problem if I can't reproduce it somehow.\r\n\r\nIs it ok to have it on a container image?\r\n\r\n> I also wonder whether it might be preferable to use a prebuilt jaxlib rather than building one from scratch. Is that possible? Could we make it possible?\r\n\r\nUnfortunately not. The machines have the whole software stack upgraded once a year, while keeping the old versions still working - having a prebuilt library would be a showstopper for fixing bugs on different versions of everything.\r\n \r\n> One thing I might try is setting the `LD_LIBRARY_PATH` environment to point to the directory with the correct `libstdc++`. Does that help?\r\n\r\nIt does not, as bazel tends to do its own stuff regarding that. Most supercomputers use LMOD to keep track of LD_LIBRARY_PATH (and other environment variables), and so do we. \r\n"
      },
      {
        "user": "hawkinsp",
        "created_at": "2021-05-11T01:12:26Z",
        "body": "Closing. Without the ability to reproduce the problem, we can't help debug it, I'm afraid. But we accept PRs, if there's something we can do to improve things!"
      }
    ]
  },
  {
    "number": 4729,
    "title": "Performance difference between @jit and jit()",
    "created_at": "2020-10-28T11:49:56Z",
    "closed_at": "2020-11-10T14:39:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4729",
    "body": "I've been playing around with JAX and noticed the following behavior: A function jitted by using the corresponding decorator seems to be much faster in compilation time than using the `jit()` function. Is this intended and does it mean to always prefer the \"decorator way\"?\r\n\r\n```\r\nimport jax.numpy as jnp\r\nfrom jax import grad, jit\r\n\r\ndef relu_default(x):\r\n  return jnp.maximum(0, x)\r\n\r\n@jit\r\ndef relu_decorator(x):\r\n  return jnp.maximum(0, x)\r\n\r\n\r\n# jit the function without any decorator and trigger its first compilation.\r\nrelu_jit = jit(relu_default)\r\n%time relu_jit(2.0).block_until_ready()       # around 11 ms\r\n\r\n# do the same for the function with the @jit decorator.\r\n%time relu_decorator(2.0).block_until_ready() # around 6 ms\r\n\r\n# why is the decorator version faster?\r\n\r\n# after the initial complilation, the speed discrepancy seems to vanish.\r\n%timeit relu_jit(2.0).block_until_ready()         # 320 µs per loop\r\n%timeit relu_decorator(2.0).block_until_ready()   # 319 µs per loop\r\n```\r\n\r\nHope I didn't miss any of the beginner pitfalls here. In any case, I did check the documentation.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4729/comments",
    "author": "fabiannagel",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2020-10-28T12:48:39Z",
        "body": "There is no difference in calling jit via a decorator or via a function. So why the different timings?\r\n\r\nIf you try this again, but first run\r\n```python\r\njit(jnp.maximum)(0, 2.0)\r\n```\r\nyou'll find that the compilation times are much more similar.\r\n\r\nWhy? The first time `jnp.maximum` is encountered in a jit context, it is traced and compiled, and this takes some time. In your version, the first statement does the work to jit-compile `jnp.maximum` and the second statement re-uses this cached result."
      },
      {
        "user": "mattjj",
        "created_at": "2020-10-29T14:32:23Z",
        "body": "I think perhaps the surprise here is that these two functions share the same cache:\r\n\r\n```python\r\nrelu_jit1 = jit(relu_default)\r\nrelu_jit2 = jit(relu_default)\r\n```\r\n\r\nThe `jit` compilation cache is a module-level dict keyed on the callable you give it (i.e. keyed on `relu_default` in this case). (It holds a weak reference to the callable so that if all other references are dropped then the corresponding cache entries are cleared.) That lets you write things like `jit(f)(x, y, z)` at a call-site and you can still get compilation caching."
      },
      {
        "user": "fabiannagel",
        "created_at": "2020-11-10T14:39:19Z",
        "body": "Right, that makes sense. Thanks for the clarification!"
      }
    ]
  },
  {
    "number": 4712,
    "title": "aggressive JIT recompilation with equal-but-not-identical args",
    "created_at": "2020-10-26T23:13:21Z",
    "closed_at": "2020-10-26T23:50:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4712",
    "body": "I'm having an issue with a JIT-compiled function being recompiled at every run. This seems to happen whenever an argument set as static is not _identical_ to its previous value (in the sense of `is` or `id(arg)`), rather than when it is not _equal_ to its previous value (in the sense of `==`).\r\n\r\nIs this an expected behavior / a limitation of the compilation model?\r\n\r\n---\r\n\r\nAs a simple example, consider:\r\n\r\n```python\r\n@jax.partial(jax.jit, static_argnums=(0,))\r\ndef dummy_add_fn(dummy, x):\r\n    return x + 1\r\n```\r\n\r\nIf we run + profile this, we find that whenever the identity of `dummy` changes, the function recompiles.\r\n\r\n```python\r\ndummy_arg = [0]\r\nreal_arg = jnp.zeros((3,))\r\nwith jax.profiler.TraceContext(\"Run 1\"):\r\n    dummy_add_fn(dummy_arg, real_arg)  # <- JIT compilation\r\nwith jax.profiler.TraceContext(\"Run 2\"):\r\n    dummy_add_fn(dummy_arg, real_arg)  # <- no compilation\r\n\r\ndummy_arg = [0]\r\nwith jax.profiler.TraceContext(\"Run 3\"):\r\n    dummy_add_fn(dummy_arg, real_arg)  # <- compiles again\r\n```\r\n\r\nThis happens even though `[0] == [0]`.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4712/comments",
    "author": "willwhitney",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2020-10-26T23:46:45Z",
        "body": "This is expected behavior, but we intend to revise it. It's not a limitation, just a choice we made early on that was a bad one.\r\n\r\nThe idea is that unhashable objects (like lists) are silently treated via object identity semantics. See #2813 and the recent discussion on #4572.\r\n\r\nWe think this is a major foot-gun and so we're working to revise it into an error instead, but it required updating a bunch of Google-internal users who were relying on the previous behavior. #3712 made JAX's own internals not rely on the old/current work-by-object-identity-on-unhashable-arguments behavior. I think within a month JAX won't support this silently-work-by-object-id behavior anymore at all. Hopefully sooner.\r\n\r\nFor now, the solution is just not to use a list here: use a tuple instead. In general, any class with `__eq__` _and_ `__hash__` defined (i.e. any hashable class) will work the way you expect based on the equality semantics those two methods define, whereas any unhashable class will silently work by object identity semantics (until we revise it to raise an error).\r\n\r\nHope that makes sense!"
      },
      {
        "user": "willwhitney",
        "created_at": "2020-10-26T23:50:39Z",
        "body": "I see, that makes sense! Appreciate the quick reply!\r\n\r\nIn my actual code I was running into this with a Flax `struct.dataclass`, not a list — it sounds like even with the current behavior this could have been worked around on their end. <ninja edit -- it's probably because my dataclass contains ndarrays; they're equal but have been serialized and de-serialized between uses>"
      }
    ]
  },
  {
    "number": 4638,
    "title": "question regarding jax.lax.all_to_all behaviour ",
    "created_at": "2020-10-19T10:04:54Z",
    "closed_at": "2023-11-07T16:04:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4638",
    "body": "Hi! I have a question regarding the expected behavior of `all_to_all`. I'm running the code below on my CPU (setting XLA device number to 8). I just wanted to ask if my expectations for the shapes of the resulting arrays `res1` and `res2` below are correct,  or if I am missing something. Also, the last `ravel()` statement throws a `RuntimeError` (see below) that I can't quite wrap my head around.\r\n\r\nThanks much in advance!!\r\n\r\n\r\n```python \r\nimport jax\r\nimport numpy as np\r\n\r\nfrom jax import pmap\r\ntensor = pmap(lambda a: a, in_axes=(0,))(jax.numpy.array(np.random.rand(8,4,8)))\r\n\r\nall_to_all= pmap(lambda x, y, z: jax.lax.all_to_all(x, axis_name='i',split_axis=y,\r\n                                                    concat_axis=z), in_axes=(0,),\r\n                 axis_name='i', static_broadcasted_argnums=(1, 2))\r\n\r\naxis_size=8\r\n\r\nsplit_axis,concat_axis = 1, 0\r\nres1 = all_to_all(tensor,split_axis, concat_axis)\r\nprint(f\"res1.shape: {res1.shape}, expected shape: {[axis_size] + list(np.insert(np.delete(tensor.shape[1:], split_axis), concat_axis, axis_size))}\")\r\n\r\nsplit_axis,concat_axis = 1, 1\r\nres2 = all_to_all(tensor,split_axis, concat_axis)\r\nprint(f\"res2.shape: {res2.shape}, expected shape: {[axis_size] + list(np.insert(np.delete(tensor.shape[1:], split_axis), concat_axis, axis_size))}\")\r\n\r\nres2.ravel()\r\nres1.ravel()\r\n\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-9-7b1dcc96ba70> in <module>\r\n     20 \r\n     21 res2.ravel()\r\n---> 22 res1.ravel()\r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/numpy/lax_numpy.py in ravel(a, order)\r\n   1197   if order == \"K\":\r\n   1198     raise NotImplementedError(\"Ravel not implemented for order='K'.\")\r\n-> 1199   return reshape(a, (size(a),), order)\r\n   1200 \r\n   1201 \r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/numpy/lax_numpy.py in reshape(a, newshape, order)\r\n   1144 def reshape(a, newshape, order=\"C\"):\r\n   1145   try:\r\n-> 1146     return a.reshape(newshape, order=order)  # forward to method for ndarrays\r\n   1147   except AttributeError:\r\n   1148     return _reshape(a, newshape, order=order)\r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/numpy/lax_numpy.py in _reshape_method(a, *newshape, **kwargs)\r\n   1190           type(newshape[0]) is not Poly):\r\n   1191     newshape = newshape[0]\r\n-> 1192   return _reshape(a, newshape, order=order)\r\n   1193 \r\n   1194 \r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/numpy/lax_numpy.py in _reshape(a, newshape, order)\r\n   1168   computed_newshape = _compute_newshape(a, newshape)\r\n   1169   if order == \"C\":\r\n-> 1170     return lax.reshape(a, computed_newshape, None)\r\n   1171   elif order == \"F\":\r\n   1172     dims = np.arange(ndim(a))[::-1]\r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/lax/lax.py in reshape(operand, new_sizes, dimensions)\r\n    688     return reshape_p.bind(\r\n    689       operand, new_sizes=new_sizes,\r\n--> 690       dimensions=None if dimensions is None or same_dims else tuple(dimensions))\r\n    691 \r\n    692 def pad(operand: Array, padding_value: Array,\r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/core.py in bind(self, *args, **params)\r\n    264     top_trace = find_top_trace(args)\r\n    265     tracers = map(top_trace.full_raise, args)\r\n--> 266     out = top_trace.process_primitive(self, tracers, params)\r\n    267     return map(full_lower, out) if self.multiple_results else full_lower(out)\r\n    268 \r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/core.py in process_primitive(self, primitive, tracers, params)\r\n    572 \r\n    573   def process_primitive(self, primitive, tracers, params):\r\n--> 574     return primitive.impl(*tracers, **params)\r\n    575 \r\n    576   def process_call(self, primitive, f, tracers, params):\r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/lax/lax.py in _reshape_impl(operand, new_sizes, dimensions)\r\n   3337       return xla.DeviceArray(aval, operand._device, lazy_expr, operand.device_buffer)\r\n   3338   return xla.apply_primitive(reshape_p, operand, new_sizes=new_sizes,\r\n-> 3339                              dimensions=dimensions)\r\n   3340 \r\n   3341 def _is_singleton_reshape(old, new):\r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/interpreters/xla.py in apply_primitive(prim, *args, **params)\r\n    223   \"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\r\n    224   compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args), **params)\r\n--> 225   return compiled_fun(*args)\r\n    226 \r\n    227 @cache()\r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/interpreters/xla.py in _execute_compiled_primitive(prim, compiled, result_handler, *args)\r\n    327 def _execute_compiled_primitive(prim, compiled, result_handler, *args):\r\n    328   device, = compiled.local_devices()\r\n--> 329   input_bufs = [device_put(x, device) for x in args if x is not token]\r\n    330   out_bufs = compiled.execute(input_bufs)\r\n    331   if FLAGS.jax_debug_nans:\r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/interpreters/xla.py in <listcomp>(.0)\r\n    327 def _execute_compiled_primitive(prim, compiled, result_handler, *args):\r\n    328   device, = compiled.local_devices()\r\n--> 329   input_bufs = [device_put(x, device) for x in args if x is not token]\r\n    330   out_bufs = compiled.execute(input_bufs)\r\n    331   if FLAGS.jax_debug_nans:\r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/interpreters/xla.py in device_put(x, device)\r\n    118   x = canonicalize_dtype(x)\r\n    119   try:\r\n--> 120     return device_put_handlers[type(x)](x, device)\r\n    121   except KeyError as err:\r\n    122     raise TypeError(f\"No device_put handler for type: {type(x)}\") from err\r\n\r\n~/PY3_TF_2.1/lib/python3.6/site-packages/jax/interpreters/xla.py in _device_put_array(x, device)\r\n    124 def _device_put_array(x, device: Optional[Device]):\r\n    125   backend = xb.get_device_backend(device)\r\n--> 126   return backend.buffer_from_pyval(x, device)\r\n    127 \r\n    128 def _device_put_scalar(x, device):\r\n\r\nRuntimeError: Invalid argument: from_python argument must be an array.\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4638/comments",
    "author": "mganahl",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2023-11-07T16:04:43Z",
        "body": "Sorry we didn't look at this issue for years :-(\r\n\r\nThe `RuntimeError` no longer reproduces at head: the code works fine.\r\n\r\nAnd I'd say your expectations are right on the money (i.e., the actual output shapes print what your code says it expects).\r\n\r\n```\r\nres1.shape: (8, 8, 4), expected shape: [8, 8, 4]\r\nres2.shape: (8, 4, 8), expected shape: [8, 4, 8]\r\n```\r\n\r\nI hope that answers the question, if belatedly!"
      }
    ]
  },
  {
    "number": 4585,
    "title": "Is it possible to make nonzero jittable",
    "created_at": "2020-10-14T21:47:11Z",
    "closed_at": "2020-10-14T22:35:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4585",
    "body": "Currently, it's not possible to do things like\r\n```python\r\nfrom jax import jit\r\nimport jax.numpy as jnp\r\n\r\njit(jnp.where(a>1))(jnp.arange(3))\r\n```\r\nbecause `nonzero` is not jittable. Is there some trick that would accomplish the same thing as `jnp.where(condition)`?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4585/comments",
    "author": "Joshuaalbert",
    "comments": [
      {
        "user": "Joshuaalbert",
        "created_at": "2020-10-14T21:50:25Z",
        "body": "Suppose also that you know how large the output should be, e.g. `jnp.where(a=jnp.max(a))` where `a` has a unique maximum."
      },
      {
        "user": "Joshuaalbert",
        "created_at": "2020-10-14T21:58:39Z",
        "body": "I guess something like,\r\n```python\r\n#jnp.where(a=jnp.max(a))\r\njnp.unravel_index(jnp.argmax(a.flatten()), a.shape)\r\n```\r\nSo one way to use this for a general `jnp.where(condition)` is to use `unravel_index` on all elements and then to use a mask in any downstrea operations."
      },
      {
        "user": "mattjj",
        "created_at": "2020-10-14T22:04:32Z",
        "body": "Good question! There isn't a good trick to get the same thing as \"unary-where\" `jnp.where(condition)`, though I expect we'll add support for it to `jax.mask` soon.\r\n\r\nUsually there is a good way to re-express the overall computation involving the unary-where to avoid its use, but it's a case-by-case thing.\r\n\r\nFor that reason I'm not sure if we'll be able to provide a more general answer to this question (though we're happy to help look at specific examples!). WDYT?"
      },
      {
        "user": "Joshuaalbert",
        "created_at": "2020-10-14T22:31:18Z",
        "body": "This seem reasonable. My need was to select largest location in an array with one unique maximum, which I was able to do without unary-where as able. I don't know about `jax.mask`. What is its intended purpose?"
      },
      {
        "user": "mattjj",
        "created_at": "2020-10-14T22:35:47Z",
        "body": "Indeed, we haven't documented `jax.mask`, because it's just a prototype! It's a tool for supporting certain kinds of shape polymorphism. We'll post a design doc or tutorial at some point in the next few months, so keep an eye out!\r\n\r\nIn the meantime I'll close this issue, but please don't hesitate to open others!"
      }
    ]
  },
  {
    "number": 4537,
    "title": "Jax is slower than Numpy on CPU",
    "created_at": "2020-10-11T04:55:53Z",
    "closed_at": "2020-10-11T05:14:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4537",
    "body": "I ran the sample Colab notebook from Jax Quickstart\" on CPU and found that Jax is slower compared to NumPy. \r\n\r\nNumpy took **523 ms** while Jax took **847 ms**.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4537/comments",
    "author": "CleanPegasus",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2020-10-11T05:00:27Z",
        "body": "Which cell are you referring to?\r\n\r\nIn general, JAX can be slower than NumPy on CPU; it all depends on how `jax.jit` is used. In fact, without `jax.jit`, we should expect JAX to be slower than NumPy; NumPy is quite hard to beat! With `jax.jit`, it depends on the workload, and whether there are enough optimizations available to do better than NumPy can."
      }
    ]
  },
  {
    "number": 4474,
    "title": "Cross multiplication on JAX is faster in CPU compared to GPU",
    "created_at": "2020-10-07T13:59:03Z",
    "closed_at": "2020-10-10T03:23:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4474",
    "body": "I tried taking the cross product of two (10,000 * 10,000) matrics on NumPy, TensorFlow and Jax to compare the time it takes to complete the operation.\r\nOn CPU:\r\nNumpy took **58.32** seconds\r\nTensorFlow took **64.802** seconds\r\nJax took **0.034** seconds\r\n\r\nOn GPU:\r\nNumpy took **59.04** seconds (understandable because NumPy doesn't use GPU or TPU acceleration)\r\nTensorFlow took **0.197** seconds\r\nJax took **2.02** seconds\r\n\r\nWhy is Jax slower on GPU(2.02 seconds) as compared to CPU(0.034 seconds)?",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4474/comments",
    "author": "CleanPegasus",
    "comments": [
      {
        "user": "clemisch",
        "created_at": "2020-10-07T14:36:32Z",
        "body": "From the large difference on CPU I suspect you did not use `.block_until_ready()` on the result array when measuring the time? JAX normally computes asynchronously, which means that the function call returns immediately even though the actual numerical computation is not finished. \r\n\r\nSo, instead of \r\n\r\n```python\r\n%timeit fun(arr)\r\n```\r\n\r\nfor `fun` returning an array, rather use \r\n\r\n```python\r\n%timeit fun(arr).block_until_ready()\r\n```"
      },
      {
        "user": "CleanPegasus",
        "created_at": "2020-10-10T03:23:03Z",
        "body": "It works. Thank you"
      }
    ]
  },
  {
    "number": 4418,
    "title": "advanced boolean indexing",
    "created_at": "2020-09-29T13:30:06Z",
    "closed_at": "2020-10-05T04:55:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4418",
    "body": "Hi!\r\n\r\nI think issue #166 does not resolve my problem, and I require advanced indexing. Please correct me if I am wrong on the implementation or there is an alternative solution. I am using boolean indexing to create a mask from a multidimensional array as follows:\r\n\r\n```\r\nDataset = [[1,2,0],\r\n              [1,4,0],\r\n              [0,0,0]]\r\n\r\nax1, ax2 = np_jax.where(~Dataset[:, 0].any(axis=2)) # Returns axes where Dataset is 0 for dimension 2 for column 0\r\nmask = np_jax.ones(Dataset.shape)  \r\nmask = jax.ops.index_update(mask, jax.ops.index[ax1,ax2], 0) #equivalent to mask[ax1, ax2] = 0  # zeroes\r\n\r\n\r\n```\r\n\r\nI get the following error:\r\n\r\n> IndexError: Array boolean indices must be concrete.\r\n\r\n\r\nOpen to alternatives, otherwise I would like to please request advanced indexing,\r\n\r\nThanks!\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4418/comments",
    "author": "LysSanzMoreta",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2020-09-29T19:59:49Z",
        "body": "Hi,\r\nThe issue is that the single argument version of `jnp.where` is not compatible with JIT, because the size of the returned arrays is dependent on the content of the input array.\r\n\r\nI think you could instead use the three-argument version of `np.where`; something along the lines of this:\r\n```\r\nmask = np_jax.where(~Dataset[:, 0].any(axis=2), 0, 1)\r\n```"
      },
      {
        "user": "LysSanzMoreta",
        "created_at": "2020-09-30T10:03:05Z",
        "body": "Ohh! It worked, thanks for rethinking it. Last question, because I have the same error problem but with np_jax.isin. I try to use as:\r\n\r\n```\r\nc_indexes = [4,5]\r\nsequences = [[3, 1, 4],\r\n                      [5,6,1],\r\n                      [2,5,1],\r\n                      [4,7,8]] \r\nix = np_jax.isin(sequences[:,0], c_indexes) \r\nc = sequences[np_jax.where(ix),1:] \r\n```\r\n\r\nThanks for your help, I struggle thinking in this unmutable version of numpy, getting used to it hehhe\r\n\r\nThanks again! and have  anice day\r\n"
      },
      {
        "user": "jakevdp",
        "created_at": "2020-09-30T13:32:30Z",
        "body": "The only way to JIT-compile this code is for `sequences` and `c_indices` to be a static values, because the size of `c` depends on their content, and array sizes must be static within JIT-compiled code."
      },
      {
        "user": "LysSanzMoreta",
        "created_at": "2020-10-01T09:49:54Z",
        "body": "Thanks! I am looking into it, my c_indexes size also changes so might be a problem, but I will try think about something...Thanks again!"
      }
    ]
  },
  {
    "number": 4311,
    "title": "statically determine VJP",
    "created_at": "2020-09-16T19:53:17Z",
    "closed_at": "2020-10-22T02:34:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4311",
    "body": "I have a use case where I'd like a function transformation that looks roughly like:\r\n```\r\nf_fwd, f_bwd = jax.shaped_vjp(f, *example_primals)\r\nf_fwd :: primals_in -> (primals_out, activations)\r\nf_bwd :: (activations, cotangents_in) -> cotangents_out\r\n```\r\nWhere I'm happy raising to ShapedVal for the primals. I'd like to do this statically so I don't end up recompiling `f_fwd` and `f_bwd`.\r\nIt seems like the autodiff machinery could reasonably expose this - after all, this is what grad-of-jit sort of does already - but I'm not sure how to reach in and expose this.\r\n\r\nNotes from follow-up offline:\r\nI want `f_fwd` and `f_bwd` to be parts of different XLA computations, i.e. in different `jax.pmap` scopes, and to be able to manipulate the activations output of `f_fwd` (e.g. by pulling it onto host or moving it between devices).\r\nThere's no need to have a sensible internal structure; I'm happy to just treat it as an opaque pytree of DeviceArrays.\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4311/comments",
    "author": "trevorcai",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2020-10-20T21:47:39Z",
        "body": "This might take some iteration to get exactly right, so bear with me.\r\n\r\nTo some extent this already works just using `jax.vjp` (thanks to @NeilGirdhar and #3667), in that the callable returned by `jax.vjp` is a pytree (i.e. a container) of its activations/residuals:\r\n\r\n```python\r\nimport jax\r\nfrom jax.tree_util import tree_flatten\r\nimport jax.numpy as jnp\r\n\r\ndef f(x):\r\n  y = jnp.sin(x)\r\n  z = jnp.sin(y)\r\n  return z\r\n\r\nx = jnp.array([1., 2., 3.])\r\ny, f_vjp = jax.vjp(f, x)\r\n\r\nleaves, _ = tree_flatten(f_vjp)\r\nprint(leaves)\r\n# [DeviceArray([ 0.5403023 , -0.41614684, -0.9899925 ], dtype=float32), DeviceArray([0.66636676, 0.6143003 , 0.9900591 ], dtype=float32)]\r\n```\r\n\r\n(Note that with a scalar argument, no leaves come out because of how jaxprs inline scalars as literals. We could iterate on that if it's undesirable but I'm going to assume scalars don't matter for the moment.)\r\n\r\nMoreover, we don't have to worry about recompilation if we just put a `jax.jit` on `f`:\r\n\r\n```python\r\nimport jax\r\nfrom jax.tree_util import tree_flatten\r\nimport jax.numpy as jnp\r\n\r\n@jax.jit\r\ndef f(x):\r\n  print('re-tracing / re-compiling f')\r\n  y = jnp.sin(x)\r\n  z = jnp.sin(y)\r\n  return z\r\n\r\nx = jnp.array([1., 2., 3.])\r\ny, f_vjp = jax.vjp(f, x)  # prints\r\n\r\ny, f_vjp = jax.vjp(f, x)  # no print\r\n```\r\n\r\nWe could restructure that to put even more under the `jit`, again leveraging the fact that `f_vjp` is a pytree:\r\n\r\n```python\r\nimport jax\r\nfrom jax.tree_util import tree_flatten\r\nimport jax.numpy as jnp\r\n\r\ndef f(x):\r\n  y = jnp.sin(x)\r\n  z = jnp.sin(y)\r\n  return z\r\n\r\n@jax.jit\r\ndef f_fwd(x):\r\n  return jax.vjp(f, x)\r\n\r\nx = jnp.array([1., 2., 3.])\r\ny, f_vjp = f_fwd(x)\r\n```\r\n\r\nThis is close to your example, but without needing `jax.shaped_vjp` or `example_primals` at all. To bring it even closer:\r\n\r\n```python\r\nimport jax\r\nfrom jax.tree_util import tree_flatten, tree_unflatten, Partial\r\nimport jax.numpy as jnp\r\n\r\ndef f(x):\r\n  y = jnp.sin(x)\r\n  z = jnp.sin(y)\r\n  return z\r\n\r\n@jax.jit\r\ndef f_fwd(x):\r\n  y, f_vjp = jax.vjp(f, x)\r\n  res, f_vjp_tree = tree_flatten(f_vjp)\r\n  def f_bwd(res, cotangents):\r\n    f_vjp = tree_unflatten(f_vjp_tree, res)\r\n    return f_vjp(cotangents)\r\n  return y, res, Partial(f_bwd)\r\n\r\nx = jnp.array([1., 2., 3.])\r\ny, res, f_bwd = f_fwd(x)\r\nprint(res)\r\n# [DeviceArray([ 0.5403023 , -0.41614684, -0.9899925 ], dtype=float32), DeviceArray([0.66636676, 0.6143003 , 0.       9900591 ], dtype=float32)]\r\n\r\ny_bar = y  # reuse y as cotangents\r\nx_bar = f_bwd(res, y_bar)\r\nprint(x_bar)\r\nprint(jax.vjp(f, x)[1](y))\r\n# (DeviceArray([ 0.26845413, -0.20171776, -0.13786028], dtype=float32),)\r\n# (DeviceArray([ 0.26845413, -0.20171776, -0.13786028], dtype=float32),)\r\n```\r\n\r\nIf you really want the `jax.shaped_vjp` step with `example_primals`, we could make that work but it won't save anything (i.e. it won't save recompiles), and I think it'd require some more boilerplate using internal APIs. The above version uses only public APIs.\r\n\r\nWDYT?"
      },
      {
        "user": "trevorcai",
        "created_at": "2020-10-21T15:20:19Z",
        "body": "Nice, this makes a lot of sense! In my case the `jax.shaped_vjp` step makes life a lot easier for me, but it seems quite straightforward now that you've shown the tree_flatten/tree_unflatten trick with `f_vjp`:\r\n\r\n```\r\n# Top-level JIT to avoid useless FLOPs when finding vjp tree structure.\r\n@functools.partial(jax.jit, static_argnums=0)\r\ndef shaped_vjp(f, x):\r\n  f_vjp_tree = jax.tree_structure(jax.vjp(f, x)[1])\r\n\r\n  def f_fwd(x):\r\n    print('tracing fwd')\r\n    y, f_vjp = jax.vjp(f, x)\r\n    return y, jax.tree_leaves(f_vjp)\r\n\r\n  def f_bwd(res, cotangents):\r\n    print('tracing bwd')\r\n    f_vjp = jax.tree_unflatten(f_vjp_tree, res)\r\n    return f_vjp(cotangents)\r\n\r\n  return jax.tree_util.Partial(f_fwd), jax.tree_util.Partial(f_bwd)\r\n```"
      },
      {
        "user": "trevorcai",
        "created_at": "2020-10-21T15:22:42Z",
        "body": "Some quick tests seem to say that this is doing something reasonable, so I'm happy to move forward with this as a library function in my codebase (no upstream required). Feel free to close the issue!"
      },
      {
        "user": "mattjj",
        "created_at": "2020-10-22T02:34:29Z",
        "body": "Nice! Glad this worked out."
      }
    ]
  },
  {
    "number": 4191,
    "title": "Obtain a list of mapped axes",
    "created_at": "2020-09-02T13:43:29Z",
    "closed_at": "2020-10-27T16:29:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4191",
    "body": "Is there a way to obtain a list of mapped axes from within a function? \r\n\r\nMy use case is the following. I have a function where I would like to `jax.lax.pmean` over **all** mapped axes, so that I can run it with either `vmap` or `pmap` + `vmap`, or another depth of mapping. Being able to see the mapped axes and apply `pmean` in a custom order would be a great feature. It could also allow for better runtime error checking and debugging. ",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4191/comments",
    "author": "hamzamerzic",
    "comments": [
      {
        "user": "apaszke",
        "created_at": "2020-09-07T21:22:28Z",
        "body": "We don't support a function like that at the moment, although this information is tracked internally. I think that the recommended way to do this would be to pass in the axes you want to reduce over as an argument to your function. Note that all reduction collectives accept tuples of axis names as well."
      },
      {
        "user": "hamzamerzic",
        "created_at": "2020-09-08T10:29:08Z",
        "body": "Thanks Adam. Do you foresee these internals being exposed to the user?\r\n\r\nWe are building a modular codebase, with functions arbitrarily nested, so following the current recommended way would require passing a list of mapped axes to every function. \r\n\r\nAlso, since we work with Haiku, we have separate init and apply methods with shared initialization. Having a way to programatically obtain the list of mapped axes would simplify our code since during init we wouldn't need to map over all axes. Currently, with omnistaging, we have nested `pmap(vmap(fn, 'j'), 'i')`, and downstream modules reduce over `i` and `j`. Thus, on init we need to create a \"fake\" outer pmap to get the params on a single device - doing this multiple times makes the code overly verbose and impacts readability."
      },
      {
        "user": "mattjj",
        "created_at": "2020-10-27T16:29:38Z",
        "body": "Thanks for the question! (Hamza and I also talked about this out-of-band back when he first asked, but I'm only now getting around to replying here.)\r\n\r\nWe can't provide an API to do this because that would mean a user wouldn't be able to map a function and predict its behavior, since the function would be able to switch its behavior based on the mapping context. That is, it'd break mapping semantics. (One way to view this is it'd be a transfer of power, or complexity, from the user/caller to the library-writer/callee.)\r\n\r\nPlumbing axis names yourself in Python is a way to resolve the potential ambiguity about what a function might do with mapped axes (and to make the caller and callee cooperate/power-share explicitly via an API). That is, the callee can specify its behavior explicitly in terms of what axis names are passed in, and the caller can control what axes it wants the callee to know about. Explicit is good!\r\n\r\nHope that provides a somewhat-satisfying rationale. I think as we improve the mapping constructs in JAX, pain points like this might fade away."
      }
    ]
  },
  {
    "number": 4164,
    "title": "How to create a device array for flax.jax_utils.prefetch_to_device?",
    "created_at": "2020-08-28T03:33:40Z",
    "closed_at": "2020-08-28T03:56:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4164",
    "body": "I was trying to call the function in a line like this:\r\n```\r\ntarget_iter = jax_utils.prefetch_to_device(iter(target_data), 2, devices=[1])\r\n```\r\nBut the \"devices\" parameter wants a jaxlib.xla_extension.Device array. I wonder how to make one. Specifically, I want to place the iterator on my GPU:1. ",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4164/comments",
    "author": "BoyuanJackChen",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2020-08-28T03:53:19Z",
        "body": "In general Flax questions are better on the Flax issue tracker, but this one is easy enough to answer here! You can use `jax.devices()` or `jax.local_devices()` to get lists of available devices."
      },
      {
        "user": "BoyuanJackChen",
        "created_at": "2020-08-28T03:56:53Z",
        "body": "@mattjj Thank you! It worked! "
      }
    ]
  },
  {
    "number": 4122,
    "title": "Getting gradients with respect to each output from Jacobian",
    "created_at": "2020-08-22T00:31:21Z",
    "closed_at": "2020-08-22T06:07:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/4122",
    "body": "I have a neural network with a loss function that outputs four different losses. I'd like to calculate the gradient of the neural network with respect to each of the four separate losses to do four different gradient-based parameter updates. One way I think works to do that efficiently is calculating the Jacobian of the loss, and use it to get each gradient separately:\r\n```\r\njac = jax.jacobian(loss)(params, x)\r\nfor index in range(4):\r\n  grad = jax.tree_map(lambda leaf: leaf[index], jac)\r\n  ... use grad ...\r\n```\r\nIs there a better way of getting each gradient separately? Is `jax.tree_map` the way to go?",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4122/comments",
    "author": "frechette-alex",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2020-08-22T00:56:15Z",
        "body": "Thanks for the question!\r\n\r\nIndeed I think `jax.jacrev` is what you want (`jax.jacobian` is an alias for it). If you think of your function `loss` as an R^n ->R^4 function, then what we want is exactly its Jacobian (the gradients of each separate loss function are its rows). Because n is likely greater than 4 it's best to use reverse-mode to compute it.\r\n\r\nUsing `tree_map` seems reasonable, but it depends a bit on what you want to do with the gradients downstream; maybe you can keep them stacked together in an array and process them in a batched way, rather than slicing them out and consuming them in a Python loop?"
      },
      {
        "user": "frechette-alex",
        "created_at": "2020-08-22T04:43:48Z",
        "body": "Thank you for the information!\r\n\r\nRegarding batching an operation over the Jacobian, would it be possible to `vmap` over it? The Jacobian is a `FlatMapping` with many keys (parameters for each component of the neural network). What kind of `in_axes` would provide to `vmap`? Would I need to fully describe the `FlatMapping`'s structure in the `in_axes`? Sorry that's a bit ambiguous."
      },
      {
        "user": "mattjj",
        "created_at": "2020-08-22T06:07:11Z",
        "body": "Hard to say without more code to go on, but that sounds plausible: in general you should make `in_axes` have the same container (i.e. pytree) structure as the input arguments. (`jacrev` is computed with `vmap` intenally, by the way.)\r\n\r\nIt kind of depends on how different the computations you need to do with the different gradients are; if you need to apply a totally distinct function to each one, to keep things batched you'd basically need to apply each function to the full stack of gradients, then use a `jnp.where` to select off the results. You could also write it with `lax.switch`, but it ends up doing the same thing once you `vmap` it.\r\n\r\nIf it's alright, let's close this issue to keep it focused, but open new ones if you need to!"
      }
    ]
  },
  {
    "number": 3857,
    "title": "jnp.dtype is not idempotent",
    "created_at": "2020-07-24T21:54:03Z",
    "closed_at": "2020-07-24T23:34:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/3857",
    "body": "```python\r\nimport jax.numpy as np\r\n\r\nprint(np.dtype(np.int32))\r\n# int32\r\nprint(np.int32)\r\n# <class 'jax.numpy.lax_numpy.int32'>\r\n```\r\n\r\nThis is a minor annoyance when writing tests and verifying that the expected dtype via the `is` operator.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/3857/comments",
    "author": "SiegeLordEx",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2020-07-24T21:57:57Z",
        "body": "JAX follows Numpy on this:\r\n```python\r\nimport numpy as np\r\nprint(np.int32)\r\n# <class 'numpy.int32'>\r\nprint(np.dtype(np.int32))\r\n# int32\r\n```\r\nI've found it a bit annoying as well, but I think there's not much chance of Numpy changing its API at this point."
      },
      {
        "user": "mattjj",
        "created_at": "2020-07-24T22:12:12Z",
        "body": "@jakevdp @SiegeLordEx given we're following NumPy behavior here, should we close this issue?"
      },
      {
        "user": "SiegeLordEx",
        "created_at": "2020-07-24T22:21:50Z",
        "body": "JAX certainly could choose to improve upon NumPy here, but given the relatively minor impact of this and the consistency with NumPy, closing seems reasonable."
      },
      {
        "user": "mattjj",
        "created_at": "2020-07-24T22:50:23Z",
        "body": "You're right; I wonder if this is something we should consider deviating on. I defer to @jakevdp and @shoyer for wisdom on that."
      },
      {
        "user": "jakevdp",
        "created_at": "2020-07-24T23:14:21Z",
        "body": "The issue is that dtype objects aren't Python types, so you can't instantiate values with them. So if you do this, you get an error:\r\n```python\r\n>>> import numpy as np\r\n>>> dt = np.dtype('int32')\r\n>>> type(dt)\r\nnumpy.dtype\r\n>>> dt(1)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-91945648777b> in <module>\r\n----> 1 dt(1)\r\n\r\nTypeError: 'numpy.dtype' object is not callable\r\n```\r\nNumpy exposes python types that are associated with the dtypes:\r\n```python\r\n>>> np.int32(1)\r\n1\r\n>>> np.int32 is np.dtype('int32')                                                                                                                        \r\nFalse\r\n>>> np.int32 is np.dtype('int32').type                                                                                                                   \r\nTrue\r\n```\r\nAn **array** has a **dtype**. A **value** in an array has a **type** that is associated with a dtype, but is not that dtype (because a dtype is a Python instance, not a Python type).\r\n\r\nI don't think we can deviate from this in JAX without substantially redefining what a dtype is, and I think that would probably lead to more issues that it would be worth."
      },
      {
        "user": "mattjj",
        "created_at": "2020-07-24T23:34:13Z",
        "body": "Good call, thanks for explaining!"
      }
    ]
  },
  {
    "number": 3809,
    "title": "Can't `eval_shape` of `lax.reduce_window`",
    "created_at": "2020-07-21T00:12:06Z",
    "closed_at": "2020-07-21T08:15:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/3809",
    "body": "Below I can evaluate a `lax.reduce_window` call:\r\n```\r\nfrom jax import eval_shape, lax, numpy as np\r\nimport operator\r\n\r\nlax.reduce_window(np.ones((1,)), 1., lax.add, (1,), (1,), 'VALID')\r\n```\r\n\r\n```\r\nDeviceArray([2.], dtype=float32)\r\n```\r\nBut not `eval_shape`:\r\n```\r\neval_shape(lax.reduce_window, np.ones((1,)), 1., lax.add, (1,), (1,), 'VALID')\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-30-5607e6dcc34d> in <module>()\r\n----> 1 eval_shape(lax.reduce_window, np.ones((1,)), 1., lax.add, (1,), (1,), 'VALID')\r\n\r\n4 frames\r\ngoogle3/third_party/py/jax/api.py in eval_shape(fun, *args, **kwargs)\r\n   1799   wrapped_fun, out_tree = flatten_fun(lu.wrap_init(fun), in_tree)\r\n   1800   out = pe.abstract_eval_fun(wrapped_fun.call_wrapped,\r\n-> 1801                              *map(abstractify, args_flat))\r\n   1802   out = [ShapeDtypeStruct(x.shape, x.dtype) for x in out]\r\n   1803   return tree_unflatten(out_tree(), out)\r\n\r\ngoogle3/third_party/py/jax/util.py in safe_map(f, *args)\r\n     32   for arg in args[1:]:\r\n     33     assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args)))\r\n---> 34   return list(map(f, *args))\r\n     35 \r\n     36 def unzip2(xys):\r\n\r\ngoogle3/third_party/py/jax/api.py in abstractify(x)\r\n   1795   \"\"\"\r\n   1796   def abstractify(x):\r\n-> 1797     return ShapedArray(np.shape(x), dtypes.result_type(x))\r\n   1798   args_flat, in_tree = tree_flatten((args, kwargs))\r\n   1799   wrapped_fun, out_tree = flatten_fun(lu.wrap_init(fun), in_tree)\r\n\r\ngoogle3/third_party/py/jax/dtypes.py in result_type(*args)\r\n    255   # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.\r\n    256   if len(args) < 2:\r\n--> 257     return dtype(args[0])\r\n    258   scalars = []\r\n    259   dtypes = []\r\n\r\ngoogle3/third_party/py/jax/dtypes.py in dtype(x)\r\n    249   if type(x) in python_scalar_dtypes:\r\n    250     return python_scalar_dtypes[type(x)]\r\n--> 251   return np.result_type(x)\r\n    252 \r\n    253 def result_type(*args):\r\n\r\nTypeError: data type not understood\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/3809/comments",
    "author": "romanngg",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2020-07-21T00:51:31Z",
        "body": "I think that's just the usual contract on JAX APIs: you need to pass non-JAX values like strings or functions another way (e.g., `functools.partial` or a lambda). `eval_shape` is much like `jit` in that respect.\r\n\r\nTry:\r\n```\r\nIn [5]: jax.eval_shape(lambda x: lax.reduce_window(x, 1., lax.add, (1,), (1,), 'VALID'), np.ones((1,)))\r\n   ...:\r\nOut[5]: ShapeDtypeStruct(shape=(1,), dtype=float32)\r\n```\r\n\r\nDoes that resolve the issue?"
      },
      {
        "user": "romanngg",
        "created_at": "2020-07-21T08:15:56Z",
        "body": "Thanks, it does!"
      }
    ]
  },
  {
    "number": 3702,
    "title": "Unclear evaluation semantics of gradients of random functions",
    "created_at": "2020-07-09T12:04:50Z",
    "closed_at": "2022-06-28T21:05:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/3702",
    "body": "This isn't a bug, more a request to open a discussion around clarifying how random functions interact with `jax.grad`.\r\n\r\nConsider the following example:\r\n\r\n```python\r\ndef r(x, key):\r\n  return x * jax.random.uniform(key)\r\n\r\nrg = jax.value_and_grad(r)\r\nkey = jax.random.PRNGKey(42)\r\nrg(10.0, key)\r\n\r\n> (DeviceArray(4.2672753, dtype=float32), DeviceArray(0.42672753, dtype=float32))\r\n```\r\n\r\nThe question is how a user might understand the gradient values that are being returned. Chatting with some of my colleagues about their mental model for this yielded three answers:\r\n\r\n1. some found this behaviour surprising, expecting perhaps that the gradient should not be well defined.\r\n2. some thought that this was expected as `uniform` is a deterministic function of `key`, independent of `x` therefore it's reasonable that if `uniform(key) = c` and `r = c * x` then `dr/dx = c`.\r\n3. some thought that this was expected because first the random number was instantiated, and then the partial derivative taken with the random number held fixed.\r\n\r\nI think these are all reasonable guesses to what the evaluation semantics would be. And the actual evaluation semantics (the second on this list) is also reasonable, and fits well with JAX's \"just transforming the code\" ethos.\r\n\r\nConsider, though, this example:\r\n\r\n```python\r\ndef r2(x, key):\r\n  return jax.random.uniform(key, maxval=x)\r\n\r\nrg2 = jax.value_and_grad(r2)\r\nkey = jax.random.PRNGKey(42)\r\nrg2(10.0, key)\r\n\r\n> (DeviceArray(4.2672753, dtype=float32), DeviceArray(0.42672753, dtype=float32))\r\n```\r\n\r\nQuite a few people I talked with found this unexpected ... more than found the first example unexpected. People with the option 3 mental model would find this surprising because, once the random number has been generated it is no longer dependent on `x`, therefore they would expect the derivative of `rg2` to be zero w.r.t. `x`. Another way that this implicit option 3 view manifested was one person who noted that it looks like \"leaking an implementation detail\" as it suggests that the random number is calculated as `x * U(0,1)`.\r\n\r\nIf I had to guess from the (very anecdotal) results of chatting about these two examples with people, I'd say a small minority of people think the evaluation semantics would be option 1, with a roughly even split between those that (perhaps implicitly) are thinking it's option 2 and option 3.\r\n\r\nI'm not really trying to take a position on which evaluation semantics are best here. More wanting to note that they're not obvious, and can lead to results that many users might find unexpected. It's also worth noting that the current semantics don't necessarily yield the gradients that would be useful for training an ML model, so user misunderstandings on this matter could potentially lead to research errors.\r\n\r\nI think the right thing to do would be to make sure that there's first a clear consensus on what the semantics should be. Then I think it would be wise to document this quite prominently.\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/3702/comments",
    "author": "JonyEpsilon",
    "comments": [
      {
        "user": "shoyer",
        "created_at": "2020-07-09T16:08:08Z",
        "body": "For understanding how JAX defines gradients for random number generation (RNG) functions, it's helpful to realize that pseudo-random number generators are actually _deterministic_ functions in JAX, based on `key`. (Interpretation (2) in your list.)\r\n\r\nSo for the first example, this is exactly the same as if `jax.random.uniform` is replaced by any other function of a single integer value, e.g.,\r\n```python\r\ndef r(x, key):\r\n  return x * any_function(key)\r\n```\r\n\r\nI'll let someone else comment on differentiation with respect to float parameters of random number generation functions. I agree that the `r2` example is a little confusing, but I think there is a case that this is the only meaningful way to define the gradient -- the alternative would be raising an error."
      },
      {
        "user": "mattjj",
        "created_at": "2020-07-09T17:29:40Z",
        "body": "One short way to describe JAX's behavior is that the semantics match formal probability theory (which is functional!), and the functional PRNG makes that possible.\r\n\r\nA \"random variable\" is really a deterministic function that takes a random element (and potentially some parameters) and produces a real value (for simplicity), so that we can think of it has having type \\Omega \\to R. A parameterized random variable might have type R \\times \\Omega \\to R, so that its value for a particular parameter value \\theta and a particular random element \\omega might be denoted as X(\\theta, \\omega). If the partial derivative of X with respect to its first input exists for each \\omega, then we can think of ∂_0 X : R \\times \\Omega -> R as a new well-defined random variable (where ∂_0 means partial derivative with respect to the first argument, i.e. the argument at index 0).\r\n\r\nIn Python, we can model random elements as PRNG keys, and random variables as Python functions that take keys as arguments. In particular, we can model X as a Python function `r = lambda theta, key: ...`. Then `grad(r)` models the random variable ∂_0 X.\r\n\r\nSo partial derivatives are well-defined (closest to your interpretation 2) _so long as the random variables in question are well-defined as functions_. But just specifying the distribution of a random variable is not enough to unambiguously pin down its definition. I think that's the ambiguity that arises in the second example `r2`.\r\n\r\nThe function \\theta \\mapsto ∂_0 X(\\theta, \\omega) represents the answer to \"for this particular fixed seed, how does the value of X change as I change the parameter?\" This derivative need not be defined, and whether it is indeed depends on more details about X than just its distribution. Let's say our base probability space has a uniform measure over \\Omega. As you've observed, we could take X(\\theta, \\omega) = \\theta * U(\\omega) where U has a uniform law on [0, 1). But if we instead define X(\\theta, \\omega) = \\theta * U(f(\\theta, \\omega)), where f has type \\R times \\Omega -> \\Omega and is an indexed hash-like bijection that scrambles all the elements of the sample space by hashing its two inputs together, then this new X still has the right distribution, but won't be smooth enough for ∂_0 X(\\theta, \\omega) to be defined.\r\n\r\nSo I'd say yes, `grad(r2)` is indeed \"leaking\" more information about `r2`, and indeed about `jax.random.uniform`'s implementation, than just their distributions alone would tell you. Derivatives do tend to do that: they give us more information about functions!\r\n\r\nWDYT?"
      },
      {
        "user": "JonyEpsilon",
        "created_at": "2020-07-22T19:14:42Z",
        "body": "Sorry for the slow reply @mattjj , and thanks for the clear explanation.\r\n\r\nOn expressions involving random samples from distributions not parameterised by variables that will be differentiated w.r.t : I agree that this seems like a reasonable thing to do. Since it's not entirely expected by everyone, though, I wonder whether a useful action item would be to add something to the docs on this point? Maybe a short addition to the \"sharp bits\" would be appropriate, showing perhaps the `r1` example and explaining the result?\r\n\r\nThe second case discussed, where a derivative is taken w.r.t. a distribution parameter is interesting. Your explanation of why it feels like it's \"leaking\" makes sense. I guess another way of putting it is that the gradients are revealing something about the generative process behind the random numbers. The thing I can't get straight in my head (frustratingly, as I've tried to prove the relevant point a few times, and failed - I suspect due to lack of imagination) is what impact this has on users of the random numbers. The point that's confusing me is this:\r\n\r\nImagine we have two parameterised generative processes, and the outputs of these processes agree in distribution. What can we say about gradients of these processes w.r.t. their parameters? My gut tells me that the *expectation* of the gradient should be the same for all reasonable processes that have outputs that are equal in distribution, otherwise I don't see how expectations of the variable itself could be made to do the right thing. My gut also tells me, though, that the other moments of the distribution of gradients could differ. Think of a location parameter for the distribution. I'm imagining one generative process where changing the location parameter just moves all of the sampled points along equally by the required amount. And I'm imagining a second generative process which moves the points around much more vigorously, but in a way that is contrived such that the distribution is the same as before, just moved by the right amount. The distribution of gradients from the latter process would be much broader than from the former. And clearly, this could be problematic for an ML experiment where the variance in the gradients matters because of finite sampling to approximate expectations.\r\n\r\nWhat's bugging me is that I can't decide whether the second generative process can exist or not, while still being differentiable w.r.t. its parameters. In your answer above you give an example of a non-differentiable process that is equal in distribution. The question is whether there's space \"in between\" for something differentiable, different, but still equal in distribution. I tried a few times to construct illustrative maps explicitly on simple distributions, but failed!\r\n\r\nI guess the answer to this question informs what the right thing to do is. If there's only really one distribution of gradients that can come out in the case that the process is differentiable, then jax.random is probably doing the right thing already. But if there are many reasonable parameterisations that would give different distributions of gradients, then I'd say it's definitely doing the wrong thing and it should probably force the user to express the generative process directly by erroring when taking gradients w.r.t. distribution parameters.\r\n\r\nSorry it's a bit rambling. Hopefully the gist of it makes sense!"
      },
      {
        "user": "mattjj",
        "created_at": "2020-07-22T20:26:30Z",
        "body": "Thanks for the discussion. Rambling is welcome; I do plenty myself!\r\n\r\n> My gut tells me that the expectation of the gradient should be the same for all reasonable processes that have outputs that are equal in distribution, otherwise I don't see how expectations of the variable itself could be made to do the right thing.\r\n\r\nI don't think that's true though (unless you offer some definition of \"reasonable\"!). As a more concrete example, consider the random variable Y(ω, µ) = X(ω) + µ where X(ω) ~ N(0, 1). Then consider Z(ω, µ) = f(Y(ω, µ)) where f(x) = -x if x is rational else x. Unless I'm mistaken, Z is nowhere differentiable with respect to its second argument, since f is nowhere differentiable (or even continuous), yet Y and Z have the same distribution. So we don't have that the expectation of the gradient is the same for Y and Z, since the latter has no gradient at all.\r\n\r\nThe point is just that saying two RVs have the same distribution is not pinning them down nearly enough to say whether their derivatives agree (or exist at all).\r\n\r\n> If there's only really one distribution of gradients that can come out in the case that the process is differentiable, then jax.random is probably doing the right thing already.\r\n\r\nThat condition might be sufficient but I don't think it's necessary. If there are many possible conventions, we can just pick one that seems reasonable. What would be the alternative; just to return nans?\r\n\r\nI think jax.random is following a good convention already. If a problem with its behavior arises then we should consider revising it, but unless/until that happens I don't think there's anything to revise.\r\n\r\nWDYT?"
      },
      {
        "user": "jakevdp",
        "created_at": "2022-06-28T21:05:53Z",
        "body": "It seems like this question has been resolved."
      }
    ]
  },
  {
    "number": 3534,
    "title": "parallelize a loop like Numba's prange",
    "created_at": "2020-06-24T11:07:42Z",
    "closed_at": "2020-06-25T19:52:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/3534",
    "body": "Hi, \r\n\r\nI have the follow code, which I'd like to parallelize over the for loop.  This can be done quite easily using Numba's prange.  Is there something of an equivalent in JAX?\r\n```python\r\ngrad_jit = jit(grad(func), static_argnums=[1])\r\ngrad_out = np.zeros((n,m,k),dtype=np.float32)\r\nfor i in range(n):\r\n    din = de_noise(raw_input[i])\r\n    grad_out[i]=grad_jit(din,params)\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/3534/comments",
    "author": "docyyz",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2020-06-24T21:59:52Z",
        "body": "Not exactly... what jax provides are mapping transforms (`pmap` and `vmap`) which will transform your function into something that does the loop implicitly, exeuting in parallel where available on whatever backend you're using. For your code snippet above, this is how you would compute `grad_out` via vmapped functions:\r\n```python\r\nfgrad = vmap(grad(func), (0, None))\r\ndenoise = vmap(denoise)\r\ngrad_out = fgrad(denoise(raw_input), params)\r\n```\r\nTHe details of `pmap` are slightly different, but it is a similar idea."
      },
      {
        "user": "mattjj",
        "created_at": "2020-06-24T22:08:41Z",
        "body": "Using `vmap` amounts to pushing the parallelism down into every primitive operation, while using `pmap` keeps it at the top level; that is, `pmap` is the closest thing to `prange`. However, `pmap` parallelizes over XLA devices, most commonly used for multiple GPUs or multiple TPU cores. On CPU, you'd need to set the environment variable `XLA_FLAGS=--xla_force_host_platform_device_count=8` or similar to present your multi-core CPU as separate XLA devices."
      },
      {
        "user": "mattjj",
        "created_at": "2020-06-25T19:52:07Z",
        "body": "Concretely, here's how you could write and execute something like this on CPU:\r\n\r\n```python\r\nimport jax.numpy as jnp\r\nfrom jax import pmap, grad\r\n\r\n@pmap\r\ndef process(row_in):\r\n  din = de_noise(row_in)\r\n  grad_out = grad(func)(din)\r\n  return grad_out\r\n\r\n# dummy functions and data\r\nde_noise = lambda x: x / 3.\r\nfunc = lambda x: jnp.sum(x ** 2)\r\nraw = jnp.arange(40).reshape(4, 10)  # 4 sequences of length 10 each\r\n\r\nresult = process(raw)\r\n```\r\n\r\n```\r\nXLA_FLAGS=--xla_force_host_platform_device_count=8 python issue3534.py\r\n```\r\n\r\nNotice that we're not pre-allocating an output and then doing in-place updating assignments into it. JAX's programming model is functional!\r\n\r\nMultithreading like this isn't really the main intended use case of `pmap` though, as you might guess from the need to set the environment variable; its main purpose is to parallelize over hardware accelerators, and to leave any parallel evaluation on each accelerator up to XLA.\r\n\r\nI think this covers the question so I'm going to close this issue, but please don't hesitate to open new ones with new questions!"
      }
    ]
  },
  {
    "number": 3359,
    "title": "Compute gradient during the forward pass",
    "created_at": "2020-06-08T14:54:40Z",
    "closed_at": "2020-09-09T20:32:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/3359",
    "body": "Hi,\r\n\r\nI have a specific use case where I can compute the gradient of my function efficiently during the forward pass. What is the preferred way of implementing the custom gradient in this case?\r\n\r\nThanks\r\n\r\nWhat I have tried (reproducing a similar example without the real maths):\r\n\r\n```python\r\nimport jax\r\nfrom jax.lax import while_loop\r\nimport jax.numpy as jnp\r\nfrom functools import partial\r\n\r\n\r\ndef example_loop_fwd(x, n, seed):\r\n    return _example_loop_fwd(x, n, seed)\r\n\r\n\r\ndef _example_loop_fwd(x, n, seed):\r\n    key = jax.random.PRNGKey(seed)\r\n    grads = jnp.ones_like(x)\r\n    def cond_fun(vars):\r\n        _, _, _, i = vars\r\n        return i < n\r\n\r\n    def body_fun(vars):\r\n        k, y, g, i = vars\r\n        k, subkey = jax.random.split(k)\r\n        uniforms = jax.random.uniform(subkey, shape=x.shape, minval=0.5, maxval=1.5)\r\n        y = y * uniforms\r\n        return key, y, g * uniforms, i + 1\r\n\r\n    _, res, g, _ = while_loop(cond_fun, body_fun, (key, x, grads, 0))\r\n    return jnp.sum(res), g\r\n\r\n\r\n@partial(jax.custom_vjp, nondiff_argnums=(1, 2))\r\ndef example_loop(x, n, seed=0):\r\n    res, g = _example_loop_fwd(x, n, seed)\r\n    return res\r\n\r\ndef example_loop_bwd(n, seed, g, dres):\r\n    return (g * dres,)\r\n\r\n\r\nexample_loop.defvjp(example_loop_fwd, example_loop_bwd)\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/3359/comments",
    "author": "AdrienCorenflos",
    "comments": [
      {
        "user": "gnecula",
        "created_at": "2020-06-09T11:11:35Z",
        "body": "I would have done the same as you did. Does it work? "
      },
      {
        "user": "mattjj",
        "created_at": "2020-06-16T05:06:28Z",
        "body": "Very cool! I think this is a case of \"gradient preaccumulation\" (Sec 10.2 of \"Evaluating Derivatives\"), where we can efficiently calculate the coefficients of a diagonal Jacobian during the forward pass and then just use them in a multiply in the backward pass. We used to have a helper function for this called `jarrett`, see e.g. #525 (the `_elementwise_std_basis` helper is directly analogous to the `jnp.ones_like(x)` call here), though it bitrotted so I deleted it at one point.\r\n\r\nI probably would've written this as a custom_jvp, since then you'd get both forward- and reverse-mode support automatically:\r\n\r\n```python\r\nimport jax\r\nfrom jax.lax import fori_loop\r\nimport jax.numpy as jnp\r\nfrom functools import partial\r\n\r\n\r\n@partial(jax.custom_jvp, nondiff_argnums=(1, 2))\r\ndef example_loop(x, n, seed=0):\r\n  keys = jax.random.split(jax.random.PRNGKey(0), n)\r\n\r\n  def body_fun(i, y):\r\n    uniforms = jax.random.uniform(keys[i], shape=y.shape)\r\n    return y * uniforms\r\n\r\n  y = fori_loop(0, n, body_fun, x)\r\n  return jnp.sum(y)\r\n\r\n@example_loop.defjvp\r\ndef example_loop_jvp(n, seed, primals, tangents):\r\n  (x,), (x_dot,) = primals, tangents\r\n  keys = jax.random.split(jax.random.PRNGKey(0), n)\r\n\r\n  def body_fun(i, carry):\r\n    y, jac = carry\r\n    uniforms = jax.random.uniform(keys[i], shape=y.shape)\r\n    return y * uniforms, jac * uniforms\r\n\r\n  y, jac = fori_loop(0, n, body_fun, (x, jnp.ones_like(x)))\r\n  return jnp.sum(y), jnp.sum(jac * x_dot)\r\n\r\n\r\nprint(example_loop(1., 5))  # 0.0006550594\r\nprint(jax.grad(partial(example_loop, n=5))(1.))  # 0.0006550594\r\n```\r\n\r\nI also wanted to show:\r\n1. using `fori_loop` (or alternatively `scan`) for loops with a fixed trip count;\r\n2. splitting the keys all at once outside the loop, which can be more performant.\r\n\r\nIn your real use case, it might be easier just to call `jax.jvp` on the primal `body_fun`, rather than writing a separate one for the primal and the jvp functions. It's just a little annoying with the loop counter (and potentially the PRNG key) in the way.\r\n\r\nWDYT?"
      },
      {
        "user": "mattjj",
        "created_at": "2020-06-16T05:35:48Z",
        "body": "Maybe we should revive the `jax.jarrett` function... "
      },
      {
        "user": "AdrienCorenflos",
        "created_at": "2020-06-24T14:22:59Z",
        "body": "Thanks for all that Matt. Was a bit busy lately, I'll give it a try asap"
      }
    ]
  },
  {
    "number": 3201,
    "title": "Using vmap inside defjvp",
    "created_at": "2020-05-25T04:13:35Z",
    "closed_at": "2021-04-16T20:40:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/3201",
    "body": "I have my own defined Jacobian-vector product function. It requires vertorizing function inside its computation. It seems that this will result in shape dismatch if I then use `jax.grad` or `jax.jacrev`. Although I can replace `vmap` result by my manually designed vectorized function, is it possible to allow `vmap` inside defvjp for convenience?\r\nMy code is long, so I put a brief description. (I can try to make a simple example if this is unclear):\r\n```python\r\nimport jax\r\n\r\n@jax.custom_jvp\r\ndef f(args):\r\n    # do something\r\n    pass\r\n\r\n@f.defjvp\r\ndef f_jvp(primals, tangents):\r\n    # do something\r\n    pass\r\n\r\n    # an inside function\r\n    def g(args):\r\n        # do something\r\n        pass\r\n\r\n    # vectorize it\r\n    # vjp_py WILL RAISE SHAPE EXCEPTION AFTER f_jvp\r\n    bat_g = jax.vmap(g)\r\n\r\n    # do something with bat_g\r\n    pass\r\n\r\n# they will raise shape error at vjp_py\r\njax.grad(f)(something)\r\njax.jacrev(f)(something)\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/3201/comments",
    "author": "gao462",
    "comments": [
      {
        "user": "gao462",
        "created_at": "2020-05-25T04:55:30Z",
        "body": "An example of the same problem:\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport jax.scipy as jsc\r\n\r\nseed = 47\r\nkey = jax.random.PRNGKey(seed)\r\nmat = jax.random.normal(key, (2, 2))\r\n\r\n@jax.custom_jvp\r\ndef f(mat, aux):\r\n    num_rows, num_cols = mat.shape\r\n    return jnp.ones((num_rows, 1)) / num_cols\r\n\r\n@f.defjvp\r\ndef f_jvp(primals, tangents):\r\n    mat, aux = primals\r\n    vec, _ = tangents\r\n    output = f(*primals)\r\n    num_rows, num_cols = mat.shape\r\n    size = num_rows * num_cols\r\n    bd_mat = mat.reshape(num_rows, 1, num_cols)\r\n    bd_mat = jnp.tile(bd_mat, reps=(1, num_cols, 1))\r\n    bd_mat = jsc.linalg.block_diag(*bd_mat)\r\n    bd_sum = jnp.sum(mat, axis=1).reshape(num_rows, 1)\r\n    bd_sum = jnp.tile(bd_sum, reps=(1, num_cols)).reshape(size)\r\n    bd_sum = jsc.linalg.block_diag(*bd_sum)\r\n    bd_buf = (bd_mat + bd_sum).reshape(num_rows, num_cols, size)\r\n    bd_buf = jnp.moveaxis(bd_buf, (0, 1, 2), (2, 1, 0))\r\n    # -----\r\n    # Vertorizing or Broadcasting will raise shape error\r\n    bd_buf = jnp.matmul(bd_buf, bd_mat.reshape(1, num_rows, num_cols))\r\n    # -----\r\n    # Manually Eumerate along axis can avoid shape error\r\n    # // bd_buf_list = []\r\n    # // for itr in bd_buf:\r\n    # //     bd_buf_list.append(jnp.matmul(itr, mat))\r\n    # // bd_buf = jnp.stack(bd_buf_list)\r\n    # -----\r\n    bd_buf = bd_buf / aux\r\n    jvp = jnp.sum(bd_buf, axis=0)\r\n    jvp = jnp.mean(jvp, axis=1, keepdims=True)\r\n    return (output, jvp)\r\n\r\njax.jacrev(f)(mat, 0.5)\r\n```"
      },
      {
        "user": "hawkinsp",
        "created_at": "2020-05-26T20:23:08Z",
        "body": "Can you make the example self-contained and runnable? e.g., your second example doesn't use `vmap` and the first example isn't runnable."
      },
      {
        "user": "gao462",
        "created_at": "2020-05-26T21:28:47Z",
        "body": "Update the example. The problem is that JVP function ends successfully, but still get a shape error.\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nimport jax.scipy as jsc\r\n\r\nseed = 47\r\nkey = jax.random.PRNGKey(seed)\r\nmat = jax.random.normal(key, (2, 3))\r\n\r\n@jax.custom_jvp\r\ndef f(mat, aux):\r\n    num_rows, num_cols = mat.shape\r\n    return jnp.ones((num_rows, 1)) / num_cols\r\n\r\n@f.defjvp\r\ndef f_jvp(primals, tangents):\r\n    mat, aux = primals\r\n    vec, _ = tangents\r\n    output = f(*primals)\r\n    num_rows, num_cols = mat.shape\r\n    size = num_rows * num_cols\r\n    # -----\r\n    bd_mat = mat.reshape(1, 1, num_rows, num_cols)\r\n    bd_mat = jnp.tile(bd_mat, reps=(num_rows, num_cols))\r\n    bd_mat = bd_mat.reshape(size, num_rows, num_cols)\r\n    # -----\r\n    rowsum = jnp.sum(mat, axis=1, keepdims=True)\r\n    colsum = jnp.sum(mat, axis=0, keepdims=True)\r\n    bd_rowsum = jnp.tile(rowsum, reps=(1, num_rows))\r\n    bd_colsum = jnp.tile(colsum, reps=(num_cols, 1))\r\n    # -----\r\n    bd_vec = vec.reshape(size, 1)\r\n    # -----\r\n    def operate(mx, val):\r\n        buf = 0\r\n        for i in range(2):\r\n            buf = buf + jnp.matmul(mx, bd_colsum) / jnp.power(aux, i)\r\n        buf = jnp.matmul(bd_rowsum, buf)\r\n        return buf * val\r\n    # -----\r\n    # Vertorizing will raise shape error\r\n    bd_buf = jax.vmap(operate, in_axes=(0, 0), out_axes=0)(bd_mat, bd_vec)\r\n    # -----\r\n    bd_buf = bd_buf / aux\r\n    jvp = jnp.sum(bd_buf, axis=0)\r\n    jvp = jnp.mean(jvp, axis=1, keepdims=True)\r\n    # -----\r\n    # JVP ends successfully, but still raise an error\r\n    print(output.shape, jvp.shape)\r\n    return (output, jvp)\r\n\r\njax.grad(lambda mat, aux: jnp.sum(f(mat, aux)))(mat, 0.5)\r\n```\r\nMy error message is\r\n```\r\n(2, 1) (2, 1)\r\nTraceback (most recent call last):\r\n  File \"try.py\", line 51, in <module>\r\n    jax.grad(lambda mat, aux: jnp.sum(f(mat, aux)))(mat, 0.5)\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/api.py\", line 383, in grad_f\r\n    _, g = value_and_grad_f(*args, **kwargs)\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/api.py\", line 450, in value_and_grad_f\r\n    g = vjp_py(onp.ones((), dtype=dtype))\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/api.py\", line 1402, in _vjp_pullback_wrapper\r\n    ans = fun(*args)\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/interpreters/ad.py\", line 113, in vjp_\r\n    arg_cts = backward_pass(jaxpr, consts, dummy_args, dummy_primals_and_cts)\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/interpreters/ad.py\", line 225, in backward_pass\r\n    cts_out = get_primitive_transpose(eqn.primitive)(cts_in, *invals, **eqn.params)\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/interpreters/ad.py\", line 466, in linear_transpose2\r\n    return zero if cotangent is zero else transpose_rule(cotangent, *args, **kwargs)\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/lax/lax.py\", line 2986, in _reshape_transpose_rule\r\n    return [reshape(t, operand.aval.shape)]\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/lax/lax.py\", line 685, in reshape\r\n    dimensions=None if dimensions is None or same_dims else tuple(dimensions))\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/core.py\", line 211, in bind\r\n    return self.impl(*args, **kwargs)\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/lax/lax.py\", line 2886, in _reshape_impl\r\n    dimensions=dimensions)\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/interpreters/xla.py\", line 217, in apply_primitive\r\n    compiled_fun = xla_primitive_callable(prim, *map(arg_spec, args), **params)\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/interpreters/xla.py\", line 231, in xla_primitive_callable\r\n    aval_out = prim.abstract_eval(*avals, **params)\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/lax/lax.py\", line 1712, in standard_abstract_eval\r\n    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))\r\n  File \"/homes/gao462/Miniconda3/envs/JAX/lib/python3.7/site-packages/jax/lax/lax.py\", line 2966, in _reshape_shape_rule\r\n    raise TypeError(msg.format(new_sizes, onp.shape(operand)))\r\nTypeError: reshape total size must be unchanged, got new_sizes (6, 1) for shape (6, 2, 3).\r\n```"
      },
      {
        "user": "mattjj",
        "created_at": "2020-11-17T05:16:46Z",
        "body": "Hey @gao462, I noticed that if I replace this line:\r\n\r\n```python\r\n    bd_buf = jax.vmap(operate, in_axes=(0, 0), out_axes=0)(bd_mat, bd_vec)\r\n```\r\n\r\nwith this semantically equivalent line which avoids vmap:\r\n\r\n```python\r\n    bd_buf = jnp.stack([operate(x, y) for x, y in zip(bd_mat, bd_vec)])\r\n```\r\n\r\nThen I still get a shape error. That suggests to me the issue isn't specific to vmap. (Still, it's unclear to me exactly where or why this shape error is only cropping up in JAX internals...)\r\n\r\nOne other clue is that `jax.jvp(lambda mat: jnp.sum(f(mat, 0.5)), (mat,), (mat,))` runs fine."
      },
      {
        "user": "mattjj",
        "created_at": "2020-11-17T05:41:35Z",
        "body": "Ah, I see the issue: it's because of an implicit broadcast. Unfortunately I can't provide a great summary of it now, but hopefully this note is enough for my future self to go on..."
      },
      {
        "user": "mattjj",
        "created_at": "2021-04-16T16:58:36Z",
        "body": "#6470 will fix this!"
      }
    ]
  },
  {
    "number": 3125,
    "title": "Question about block_until_ready() on tuple",
    "created_at": "2020-05-17T20:41:47Z",
    "closed_at": "2020-05-17T21:12:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/3125",
    "body": "I want to time the following:\r\n`opt_state = update(itr, grad(loss)(get_params(opt_state)), opt_state)`.\r\n\r\n`opt_state` is a Python tuple so I can't call `block_until_ready()` directly.\r\n\r\nWhat is the best way to ensure that `opt_state` is consumed from the host so I get accurate time?\r\n\r\n- nothing; does containment in a native Python contain imply the values have already been consumed?\r\n- `tree_map` and call `block_until_ready()` over all the leaves of `opt_state`\r\n- make `opt_state` a JAX type and call `block_until_ready()` once (If so, how to convert it to JAX type?)\r\n- directly consume from the host in some other way?",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/3125/comments",
    "author": "jacobjinkelly",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2020-05-17T20:52:47Z",
        "body": "I think tree-mapping `block_until_ready` is a decent idea. I don't think it should add noticeable overheads (based on my guess about how much time the computation itself will take).\r\n\r\n> nothing; does containment in a native Python contain imply the values have already been consumed?\r\n\r\nNo, loops won't do anything special. The only thing that blocks the Python thread (e.g. so that timers are accurate) is executing a non-jax operation on it (like printing a value, which will entail blocking until that value is ready and then also transferring it to the CPU) or `block_until_ready`.\r\n\r\n> make opt_state a JAX type and call block_until_ready() once (If so, how to convert it to JAX type?)\r\n\r\nWe used to have JaxTuples! But they make the system much more complex, both in terms of \"front-end\" transformation stuff and \"back-end\" low-level runtime stuff.\r\n\r\n> directly consume from the host in some other way?\r\n\r\nThat works, e.g. printing the values, but then you'd also be timing the transfer-to-host time as well as whatever operation (e.g. printing) is being performed.\r\n\r\n\r\nSo yeah I'm thinking `tree_map(lambda x: x.block_until_ready, opt_state)`! But also if `update` is `jit`ted then you can just do `tree_flatten(opt_state)[0][0].block_until_ready()`, since all results of a `jit`ted function become available at the same time."
      },
      {
        "user": "jacobjinkelly",
        "created_at": "2020-05-17T21:12:00Z",
        "body": "Thanks for the very detailed reply as always @mattjj :)\r\n\r\n> No, loops won't do anything special. The only thing that blocks the Python thread (e.g. so that timers are accurate) is executing a non-jax operation on it\r\n\r\nInteresting, good to know!\r\n\r\n> We used to have JaxTuples! But they make the system much more complex, both in terms of \"front-end\" transformation stuff and \"back-end\" low-level runtime stuff.\r\n\r\nHaha so I'm not crazy, I remember noticing these before I think! The way JAX handles nested containers is super nice. I suppose it's one of the simpler features but honestly one of my favourite things about JAX btw.\r\n\r\n> That works, e.g. printing the values, but then you'd also be timing the transfer-to-host time as well as whatever operation (e.g. printing) is being performed.\r\n\r\nGood point, I guess that's why `block_until_ready()` is useful in the first place.\r\n\r\n> So yeah I'm thinking tree_map(lambda x: x.block_until_ready, opt_state)! But also if update is jitted then you can just do tree_flatten(opt_state)[0][0].block_until_ready(), since all results of a jitted function become available at the same time.\r\n\r\nAh, yes `update` is `jit`ted so I think this is what I'll go with, thanks for pointing out this additional simplification.\r\n"
      }
    ]
  },
  {
    "number": 2920,
    "title": "stax.serial.apply_fun is not a valid JAX type inside odeint ",
    "created_at": "2020-05-01T17:13:18Z",
    "closed_at": "2020-05-02T17:25:53Z",
    "labels": [
      "question",
      "documentation",
      "better_errors"
    ],
    "url": "https://github.com/jax-ml/jax/issues/2920",
    "body": "Hi, \r\nFWIW, I'm using a self-built jax and jaxlib following instructions from #2083. \r\n```\r\n#\r\n# Name                    Version                   Build  Channel\r\njax                       0.1.64                    <pip>\r\njaxlib                    0.1.45                    <pip>\r\n``` \r\n\r\nI'm trying to do get gradients through an ODE solver. First, I ran into `AssertionError` issue  #2718 and I think I solved it by passing all the arguments directly into `odeint`.  Then I followed instructions to solve another `AssertionError` issue #2531 by doing `vmap` of `grads` instead of `grads` of `vmap` . Now I'm getting the following error. \r\n<details>\r\n<summary>Full trace back.</summary>\r\n<p>\r\n\r\n```\r\n----> 1 batch_grad(batch_y0, batch_t, batch_y,[1.3,1.8], [U1,U2], [U1_params,U2_params])\r\n\r\n~/Code/jax/jax/api.py in batched_fun(*args)\r\n    805     _check_axis_sizes(in_tree, args_flat, in_axes_flat)\r\n    806     out_flat = batching.batch(flat_fun, args_flat, in_axes_flat,\r\n--> 807                               lambda: _flatten_axes(out_tree(), out_axes))\r\n    808     return tree_unflatten(out_tree(), out_flat)\r\n    809 \r\n\r\n~/Code/jax/jax/interpreters/batching.py in batch(fun, in_vals, in_dims, out_dim_dests)\r\n     32   # executes a batched version of `fun` following out_dim_dests\r\n     33   batched_fun = batch_fun(fun, in_dims, out_dim_dests)\r\n---> 34   return batched_fun.call_wrapped(*in_vals)\r\n     35 \r\n     36 @lu.transformation_with_aux\r\n\r\n~/Code/jax/jax/linear_util.py in call_wrapped(self, *args, **kwargs)\r\n    148     gen = None\r\n    149 \r\n--> 150     ans = self.f(*args, **dict(self.params, **kwargs))\r\n    151     del args\r\n    152     while stack:\r\n\r\n~/Code/jax/jax/api.py in value_and_grad_f(*args, **kwargs)\r\n    436     f_partial, dyn_args = argnums_partial(f, argnums, args)\r\n    437     if not has_aux:\r\n--> 438       ans, vjp_py = _vjp(f_partial, *dyn_args)\r\n    439     else:\r\n    440       ans, vjp_py, aux = _vjp(f_partial, *dyn_args, has_aux=True)\r\n\r\n~/Code/jax/jax/api.py in _vjp(fun, *primals, **kwargs)\r\n   1437   if not has_aux:\r\n   1438     flat_fun, out_tree = flatten_fun_nokwargs(fun, in_tree)\r\n-> 1439     out_primal, out_vjp = ad.vjp(flat_fun, primals_flat)\r\n   1440     out_tree = out_tree()\r\n   1441   else:\r\n\r\n~/Code/jax/jax/interpreters/ad.py in vjp(traceable, primals, has_aux)\r\n    104 def vjp(traceable, primals, has_aux=False):\r\n    105   if not has_aux:\r\n--> 106     out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\r\n    107   else:\r\n    108     out_primals, pvals, jaxpr, consts, aux = linearize(traceable, *primals, has_aux=True)\r\n\r\n~/Code/jax/jax/interpreters/ad.py in linearize(traceable, *primals, **kwargs)\r\n     93   _, in_tree = tree_flatten(((primals, primals), {}))\r\n     94   jvpfun_flat, out_tree = flatten_fun(jvpfun, in_tree)\r\n---> 95   jaxpr, out_pvals, consts = pe.trace_to_jaxpr(jvpfun_flat, in_pvals)\r\n     96   out_primals_pvals, out_tangents_pvals = tree_unflatten(out_tree(), out_pvals)\r\n     97   assert all(out_primal_pval.is_known() for out_primal_pval in out_primals_pvals)\r\n\r\n~/Code/jax/jax/interpreters/partial_eval.py in trace_to_jaxpr(fun, pvals, instantiate, stage_out, bottom, trace_type)\r\n    435   with new_master(trace_type, bottom=bottom) as master:\r\n    436     fun = trace_to_subjaxpr(fun, master, instantiate)\r\n--> 437     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)\r\n    438     assert not env\r\n    439     del master\r\n\r\n~/Code/jax/jax/linear_util.py in call_wrapped(self, *args, **kwargs)\r\n    148     gen = None\r\n    149 \r\n--> 150     ans = self.f(*args, **dict(self.params, **kwargs))\r\n    151     del args\r\n    152     while stack:\r\n\r\n~/Code/jax/jax/api.py in f_jitted(*args, **kwargs)\r\n    152     flat_fun, out_tree = flatten_fun(f, in_tree)\r\n    153     out = xla.xla_call(flat_fun, *args_flat, device=device, backend=backend,\r\n--> 154                        name=flat_fun.__name__)\r\n    155     return tree_unflatten(out_tree(), out)\r\n    156 \r\n\r\n~/Code/jax/jax/core.py in _call_bind(processor, post_processor, primitive, f, *args, **params)\r\n   1003     tracers = map(top_trace.full_raise, args)\r\n   1004     process = getattr(top_trace, processor)\r\n-> 1005     outs = map(full_lower, process(primitive, f, tracers, params))\r\n   1006   return apply_todos(env_trace_todo(), outs)\r\n   1007 \r\n\r\n~/Code/jax/jax/interpreters/ad.py in process_call(self, call_primitive, f, tracers, params)\r\n    342     name = params.get('name', f.__name__)\r\n    343     params = dict(params, name=wrap_name(name, 'jvp'))\r\n--> 344     result = call_primitive.bind(f_jvp, *primals, *nonzero_tangents, **params)\r\n    345     primal_out, tangent_out = tree_unflatten(out_tree_def(), result)\r\n    346     return [JVPTracer(self, p, t) for p, t in zip(primal_out, tangent_out)]\r\n\r\n~/Code/jax/jax/core.py in _call_bind(processor, post_processor, primitive, f, *args, **params)\r\n   1003     tracers = map(top_trace.full_raise, args)\r\n   1004     process = getattr(top_trace, processor)\r\n-> 1005     outs = map(full_lower, process(primitive, f, tracers, params))\r\n   1006   return apply_todos(env_trace_todo(), outs)\r\n   1007 \r\n\r\n~/Code/jax/jax/interpreters/partial_eval.py in process_call(self, call_primitive, f, tracers, params)\r\n    175     in_pvs, in_consts = unzip2([t.pval for t in tracers])\r\n    176     fun, aux = partial_eval(f, self, in_pvs)\r\n--> 177     out_flat = call_primitive.bind(fun, *in_consts, **params)\r\n    178     out_pvs, jaxpr, env = aux()\r\n    179     env_tracers = map(self.full_raise, env)\r\n\r\n~/Code/jax/jax/core.py in _call_bind(processor, post_processor, primitive, f, *args, **params)\r\n   1003     tracers = map(top_trace.full_raise, args)\r\n   1004     process = getattr(top_trace, processor)\r\n-> 1005     outs = map(full_lower, process(primitive, f, tracers, params))\r\n   1006   return apply_todos(env_trace_todo(), outs)\r\n   1007 \r\n\r\n~/Code/jax/jax/interpreters/batching.py in process_call(self, call_primitive, f, tracers, params)\r\n    146     else:\r\n    147       f, dims_out = batch_subtrace(f, self.master, dims)\r\n--> 148       vals_out = call_primitive.bind(f, *vals, **params)\r\n    149       return [BatchTracer(self, v, d) for v, d in zip(vals_out, dims_out())]\r\n    150 \r\n\r\n~/Code/jax/jax/core.py in _call_bind(processor, post_processor, primitive, f, *args, **params)\r\n    999   if top_trace is None:\r\n   1000     with new_sublevel():\r\n-> 1001       outs = primitive.impl(f, *args, **params)\r\n   1002   else:\r\n   1003     tracers = map(top_trace.full_raise, args)\r\n\r\n~/Code/jax/jax/interpreters/xla.py in _xla_call_impl(fun, device, backend, name, *args)\r\n    460 \r\n    461 def _xla_call_impl(fun: lu.WrappedFun, *args, device, backend, name):\r\n--> 462   compiled_fun = _xla_callable(fun, device, backend, name, *map(arg_spec, args))\r\n    463   try:\r\n    464     return compiled_fun(*args)\r\n\r\n~/Code/jax/jax/linear_util.py in memoized_fun(fun, *args)\r\n    219       fun.populate_stores(stores)\r\n    220     else:\r\n--> 221       ans = call(fun, *args)\r\n    222       cache[key] = (ans, fun.stores)\r\n    223     return ans\r\n\r\n~/Code/jax/jax/interpreters/xla.py in _xla_callable(fun, device, backend, name, *arg_specs)\r\n    477   pvals: Sequence[pe.PartialVal] = [pe.PartialVal.unknown(aval) for aval in abstract_args]\r\n    478   jaxpr, pvals, consts = pe.trace_to_jaxpr(\r\n--> 479       fun, pvals, instantiate=False, stage_out=True, bottom=True)\r\n    480 \r\n    481   _map(prefetch, it.chain(consts, jaxpr_literals(jaxpr)))\r\n\r\n~/Code/jax/jax/interpreters/partial_eval.py in trace_to_jaxpr(fun, pvals, instantiate, stage_out, bottom, trace_type)\r\n    435   with new_master(trace_type, bottom=bottom) as master:\r\n    436     fun = trace_to_subjaxpr(fun, master, instantiate)\r\n--> 437     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)\r\n    438     assert not env\r\n    439     del master\r\n\r\n~/Code/jax/jax/linear_util.py in call_wrapped(self, *args, **kwargs)\r\n    148     gen = None\r\n    149 \r\n--> 150     ans = self.f(*args, **dict(self.params, **kwargs))\r\n    151     del args\r\n    152     while stack:\r\n\r\n<ipython-input-17-de50dc731d85> in loss(batch_y0, batch_t, batch_y, params, ufuncs, uparams)\r\n      1 @partial(jit, static_argnums=(4,))\r\n      2 def loss(batch_y0, batch_t, batch_y, params, ufuncs,uparams):\r\n----> 3     pred_y = odeint(batch_y0,batch_t,params,ufuncs,uparams)\r\n      4     loss = np.mean(np.abs(pred_y-batch_y))\r\n      5     return loss\r\n\r\n~/Code/jax/jax/experimental/ode.py in odeint(func, y0, t, rtol, atol, mxstep, *args)\r\n    152     shape/structure as `y0` except with a new leading axis of length `len(t)`.\r\n    153   \"\"\"\r\n--> 154   return _odeint_wrapper(func, rtol, atol, mxstep, y0, t, *args)\r\n    155 \r\n    156 @partial(jax.jit, static_argnums=(0, 1, 2, 3))\r\n\r\n~/Code/jax/jax/api.py in f_jitted(*args, **kwargs)\r\n    149       dyn_args = args\r\n    150     args_flat, in_tree = tree_flatten((dyn_args, kwargs))\r\n--> 151     _check_args(args_flat)\r\n    152     flat_fun, out_tree = flatten_fun(f, in_tree)\r\n    153     out = xla.xla_call(flat_fun, *args_flat, device=device, backend=backend,\r\n\r\n~/Code/jax/jax/api.py in _check_args(args)\r\n   1558     if not (isinstance(arg, core.Tracer) or _valid_jaxtype(arg)):\r\n   1559       raise TypeError(\"Argument '{}' of type {} is not a valid JAX type\"\r\n-> 1560                       .format(arg, type(arg)))\r\n   1561 \r\n   1562 def _valid_jaxtype(arg):\r\n\r\nTypeError: Argument '<function serial.<locals>.apply_fun at 0x2b06c3d6f7a0>' of type <class 'function'> is not a valid JAX type\r\n```\r\n</details>\r\n\r\nI'm passing two `stax.Serial` modules with three `Dense` layers each as an input to `odeint` to integrate the Lotka-Volterra ODEs. `ufuncs` and `uparams` contains apply functions and params of `stax.Serial` module. \r\n\r\n```\r\ndef lv_UDE(y,t,params,ufuncs,uparams):\r\n    R, F = y\r\n    alpha, theta = params\r\n    U1, U2 = ufuncs\r\n    U1_params, U2_params = uparams\r\n    dRdt = alpha*R - U1(U1_params, y)\r\n    dFdt = -theta*F + U2(U2_params, y)\r\n    return np.array([dRdt,dFdt])\r\n```\r\nI'm trying to get gradients through an `odeint` w.r.t `uparams`. Is there a workaround to pass `stax.Serial` modules as an argument? Thanks in advance. ",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/2920/comments",
    "author": "skrsna",
    "comments": [
      {
        "user": "shoyer",
        "created_at": "2020-05-02T05:56:18Z",
        "body": "Could you please share a full example of how you get this error? Ideally something that I could copy into a terminal and run."
      },
      {
        "user": "skrsna",
        "created_at": "2020-05-02T15:33:21Z",
        "body": "Hi, \r\nI just noticed that even the non vmapped version of a function with `stax.serial` as an input errors out with the same error message.  Here's the full example. Thanks \r\n```\r\nimport jax \r\nimport jax.numpy as np\r\nimport numpy as onp\r\nfrom jax import random\r\nfrom jax import grad, jit, vmap, value_and_grad\r\nfrom jax.experimental.ode import odeint\r\nfrom jax.experimental import stax\r\nfrom functools import partial\r\n\r\n\r\ndef lv(y,t,params):\r\n    \"\"\"\r\n    original lotka-volterra equations\r\n    \"\"\"\r\n    R,F = y\r\n    alpha, beta, gamma, theta = params\r\n    dRdt = alpha*R - beta*R*F\r\n    dFdt = gamma*R*F - theta*F\r\n    return np.hstack([dRdt,dFdt])\r\n\r\nt = np.linspace(0.,4.,num=1000)\r\ny0 = np.array([0.44249296,4.6280594])\r\n\r\ntrue_y = odeint(partial(lv,params=[1.3,0.9,0.5,1.8]),y0=y0,t=t) #training data generation\r\n\r\n\r\ndef lv_UDE(y,t,params,ufuncs,uparams):\r\n    \"\"\"\r\n    additional parameters include stax.Serial \r\n    modules and uparams associated with them\r\n    \"\"\"\r\n    R, F = y\r\n    alpha, theta = params\r\n    U1, U2 = ufuncs\r\n    U1_params, U2_params = uparams\r\n    dRdt = alpha*R - U1(U1_params, y)\r\n    dFdt = -theta*F + U2(U2_params, y)\r\n    return np.hstack([dRdt,dFdt])\r\n\r\n#two modules of stax Serial\r\nU1_init, U1 = stax.serial(stax.Dense(32),stax.Tanh, \r\n                            stax.Dense(32), stax.Tanh, \r\n                            stax.Dense(32),stax.Tanh,\r\n                           stax.Dense(1))\r\nU2_init, U2 = stax.serial(stax.Dense(32),stax.Tanh, \r\n                            stax.Dense(32), stax.Tanh, \r\n                            stax.Dense(32),stax.Tanh,\r\n                           stax.Dense(1))\r\n\r\nkey, subkey = random.split(random.PRNGKey(0))\r\n\r\n_,U1_params = U1_init(key,(2,)) #inputs of size 2\r\n_,U2_params = U2_init(subkey,(2,))\r\nkey,subkey = random.split(subkey)\r\n\r\n\r\ndef get_batch():\r\n    \"\"\"\r\n    Get batches of inital conditions and \r\n    times along with true time history\r\n    \"\"\"\r\n    s = onp.random.choice(onp.arange(1000 - 20, \r\n                        dtype=onp.int64), 20, replace=False)\r\n    batch_y0 = true_y[s]  # (M, D)\r\n    batch_t = t[:20]  # (T)\r\n    batch_y = np.stack([true_y[s + i] for i in range(20)])  # (T, M, D)\r\n    return batch_y0, batch_t, batch_y\r\n\r\n\r\ndef loss(batch_y0, batch_t, batch_y, params, ufuncs,uparams):\r\n    \"\"\"\r\n    Mean absolute loss \r\n    \"\"\"\r\n    pred_y = odeint(batch_y0,batch_t,params,ufuncs,uparams) # integrate using odeint\r\n    loss = np.mean(np.abs(pred_y-batch_y)) #calculate loss\r\n    return loss\r\n\r\n\r\ngrads = value_and_grad(loss,(5,)) #grads w.r.t uparams \r\nbatch_grad = vmap(grads,(0, None, None, None, None, None)) #vectorize over initial conditions (batch_y0)\r\n\r\n \r\ngrads(y0,t,true_y,[1.3,1.8], [U1,U2], \r\n      [U1_params,U2_params]) #non vmappped  doesn't work\r\nbatch_grad(batch_y0, batch_t, batch_y,[1.3,1.8], \r\n           [U1,U2], [U1_params,U2_params]) #vmap version same error\r\n```"
      },
      {
        "user": "mattjj",
        "created_at": "2020-05-02T16:00:47Z",
        "body": "Hey @skrsna , thanks for the question! \r\n\r\nIn your example, it seems the `lv_UDE` is never called. Is that intentional?\r\n\r\nThe underlying issue here is that `odeint` can't take function-valued arguments in `*args`; those must be arrays (or potentially-nested containers of arrays, like potentially-nested lists/tuples/dicts of arrays). Instead of passing `ufuncs` via the `*args` of `odeint`, maybe you can instead just write something like:\r\n\r\n```python\r\ndef lv_UDE(ufuncs,y,t,params,uparams):  # moved ufuncs to front\r\n    ...\r\n\r\nodeint(partial(lv_UDE, ufuncs), ...)\r\n```\r\n\r\nWDYT?"
      },
      {
        "user": "mattjj",
        "created_at": "2020-05-02T16:01:27Z",
        "body": "It's possible we could support passing function-valued arguments in `*args`, but I'm not sure it'd be worth the extra complexity. We could at least raise a better error..."
      },
      {
        "user": "skrsna",
        "created_at": "2020-05-02T16:05:38Z",
        "body": "Hi @mattjj , thanks for the super fast response. My bad I forgot to add `lv_UDE` while refactoring the code to make it look nice. I'll try your solution and update the issue with the workaround. Thanks again. "
      },
      {
        "user": "mattjj",
        "created_at": "2020-05-02T16:18:15Z",
        "body": "Awesome, glad to hear that might help!\r\n\r\nI just pushed #2931 to improve the error message. Now running your test program we get:\r\n\r\n```\r\nTypeError: The contents of odeint *args must be arrays or scalars, but got\r\n<function serial.<locals>.apply_fun at 0x7f17fc69ca70>.\r\n```\r\n\r\nI also improved the docstring from this:\r\n\r\n```\r\n     *args: tuple of additional arguments for `func`.\r\n```\r\n\r\nTo this:\r\n\r\n```\r\n    *args: tuple of additional arguments for `func`, which must be arrays\r\n      scalars, or (nested) standard Python containers (tuples, lists, dicts,\r\n      namedtuples, i.e. pytrees) of those types.\r\n```\r\n\r\nTo make `odeint` handle those types in `*args` automatically, we could try to hoist non-arrays out of `*args` inside `odeint`. But maybe we can open a separate issue for that enhancement if it's a high priority for anyone. (@shoyer interested to hear if you have a strong opinion!)"
      },
      {
        "user": "mattjj",
        "created_at": "2020-05-02T16:18:59Z",
        "body": "I'm going to let #2931 close this issue, just so as to keep our issues under control. Let me know if that's a bad idea :)"
      },
      {
        "user": "skrsna",
        "created_at": "2020-05-02T16:20:24Z",
        "body": "Sure, please close the issue. I'm currently trying to try out your suggestions and I'll update the issue with working code just in case if anyone else runs into the same error. "
      },
      {
        "user": "skrsna",
        "created_at": "2020-05-02T16:34:34Z",
        "body": "Hi @mattjj , I tried your solution and it works seamlessly with `vmap`. Thanks again. "
      }
    ]
  },
  {
    "number": 2825,
    "title": "Accessing DeviceArray pointers in Cython",
    "created_at": "2020-04-24T15:59:06Z",
    "closed_at": "2020-07-14T19:20:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/2825",
    "body": "Hi,\r\n\r\nI'm looking into using `jax.numpy` as an alternative for NumPy for C/C++ codebase with Cython interface. Most of the C++ functions take pointers to original NumPy ndarrays as an argument. This is primarily done by doing `cimport numpy` and defining numpy ndarrays as `cdef numpy.ndarray[dtype, ndim] original_numpy_array` in the `.pyx` files and passing the pointers to original numpy array as an argument to C++ functions e.g `CXX_object.setValue(&original_numpy_array[0])`. Is there an easy way to do this in jax. \r\n\r\nThanks ",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/2825/comments",
    "author": "skrsna",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2020-04-24T17:43:52Z",
        "body": "Can you clarify what exactly you want to do? Is it: on CPU, share access to the buffer that backs a `DeviceArray` with NumPy?\r\n\r\nYou can do that already using either the Python buffer protocol (e.g., CPU `DeviceArray`s can be passed to `np.asarray` in a zero-copy way right now), or by using DLPack. Do they satisfy your needs?"
      },
      {
        "user": "skrsna",
        "created_at": "2020-04-24T18:33:23Z",
        "body": "Sorry for being vague, I’m fairly new to JAX and I'm using JAX on GPUs. I’m working on some code where the core functionality is written in C++ but has an interface to Python using Cython.  The issue I’m facing can be best explained by the following code \r\n```\r\ndef set_values(self, Y):\r\n    cdef np.ndarray[np.double_t, ndim=1] data\r\n    data = np.ascontiguousarray(Y, dtype=np.double)            \r\n    self.CXXObject.setValues(&data[0])\r\n```\r\nThis bit of code uses the user supplied `Y` and converts into `numpy.ndarra` and updates the information in C++ object by using the `setValues` C++ method. I’d like to use `jax.numpy` module to do this so that I can use other JAX functionalities like `jit` , `grad` etc. I'm using the following versions of jax and jaxlib \r\n```\r\njax                       0.1.64                    <pip>\r\njaxlib                    0.1.45                    <pip>\r\n```"
      },
      {
        "user": "hawkinsp",
        "created_at": "2020-07-14T19:20:41Z",
        "body": "As the comment above mentions, you can already access the buffers of JAX arrays in several ways:\r\n* the Python buffer protocol (on CPU)\r\n* `__cuda_array_interface__` (on GPU)\r\n* DLPack (on CPU or GPU).\r\n\r\nSo I don't think there's any work remaining for JAX to do here; it sounds like the remaining work is in your code base."
      }
    ]
  },
  {
    "number": 2820,
    "title": "Tracing error when using cumsum inside xla.lower_fun",
    "created_at": "2020-04-24T13:29:44Z",
    "closed_at": "2020-04-26T18:50:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/2820",
    "body": "Repro:\r\n\r\n```import jax.numpy as jnp\r\nfrom jax import api\r\nfrom jax import core\r\nfrom jax import abstract_arrays\r\nimport jax.ops as jops\r\nfrom jax import xla\r\n\r\nv = jnp.reshape(jnp.array([1, 0, -1], dtype=jnp.float32), [3, 1])\r\n\r\na = jnp.array([0, 2, 2, 0, 1, 2], dtype=jnp.int32)\r\nb = jnp.array([0, 2, 3, 6], dtype=jnp.int32)\r\n\r\nrepro_p = core.Primitive(\"repro\")\r\n\r\ndef repro_prim(v, a, b):\r\n  return repro_p.bind(v, a=a, b=b)\r\n  \r\ndef repro_abstract_eval(v, a, b):\r\n  out_m = a.shape[0]\r\n  return abstract_arrays.ShapedArray( (out_m, ), dtype=v.dtype)\r\n\r\nrepro_p.def_abstract_eval(repro_abstract_eval)\r\n\r\ndef _get_foo(a, b):\r\n  foo = jops.index_update(jnp.zeros_like(a), b, jnp.ones(b.shape[0]))\r\n  return jnp.cumsum(foo)\r\n\r\ndef repro_translation(c, v, a, b):\r\n  foo = xla.lower_fun(_get_foo)(c, a=a, b=b)\r\n  return foo\r\n\r\nxla.translations[repro_p] = repro_translation\r\n\r\napi.jit(repro_prim)(v, a, b)\r\n```\r\n\r\nError is:\r\n\r\n```UnexpectedTracerError: Encountered an unexpected tracer. Perhaps this tracer escaped through global state from a previously traced function.\r\nThe functions being transformed should not save traced values to global state.\r\nDetails: Tracer from a higher level: Traced<AbstractUnit():JaxprTrace(level=-1/2)> in trace JaxprTrace(level=-1/2).\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/2820/comments",
    "author": "ekelsen",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2020-04-26T18:49:49Z",
        "body": "The issue is setting up `a` and `b` as primitive parameters here (by passing them as kwargs, which is our convention for separating value-level inputs from metadata-like parameters):\r\n\r\n```python\r\n  return repro_p.bind(v, a=a, b=b)\r\n```\r\n\r\nThat's inconsistent with treating `a` and `b` as values, e.g. when you write `api.jit(repro_prim)(v, a, b)`.\r\n\r\nThis runs:\r\n\r\n```python\r\nimport jax.numpy as jnp\r\nfrom jax import api\r\nfrom jax import core\r\nfrom jax import abstract_arrays\r\nimport jax.ops as jops\r\nfrom jax import xla\r\n\r\nv = jnp.reshape(jnp.array([1, 0, -1], dtype=jnp.float32), [3, 1])\r\n\r\na = jnp.array([0, 2, 2, 0, 1, 2], dtype=jnp.int32)\r\nb = jnp.array([0, 2, 3, 6], dtype=jnp.int32)\r\n\r\nrepro_p = core.Primitive(\"repro\")\r\n\r\ndef repro_prim(v, a, b):\r\n  return repro_p.bind(v, a, b)  # changed!\r\n\r\ndef repro_abstract_eval(v, a, b):\r\n  out_m = a.shape[0]\r\n  return abstract_arrays.ShapedArray( (out_m, ), dtype=v.dtype)\r\n\r\nrepro_p.def_abstract_eval(repro_abstract_eval)\r\n\r\ndef _get_foo(a, b):\r\n  foo = jops.index_update(jnp.zeros_like(a), b, jnp.ones(b.shape[0]))\r\n  return jnp.cumsum(foo)\r\n\r\ndef repro_translation(c, v, a, b):\r\n  foo = xla.lower_fun(_get_foo)(c, a, b)  # changed!\r\n  return foo\r\n\r\nxla.translations[repro_p] = repro_translation\r\n\r\napi.jit(repro_prim)(v, a, b)\r\n```"
      },
      {
        "user": "mattjj",
        "created_at": "2020-04-26T18:50:51Z",
        "body": "I think we've got this sorted, so in the spirit of minimizing our active issues, I'm going to close this."
      },
      {
        "user": "mattjj",
        "created_at": "2020-04-26T18:52:49Z",
        "body": "You should think of primitive params, which are identified by being passed via keyword arguments to Primitive.bind, as things like strings (e.g. \"VALID\" or \"SAME\") or shapes (e.g. (2, 3)) which can be construed of as part of the primitive's name or in some cases type-level arguments. Things that are array-like tend to be value-level arguments and must be passed to `Primitive.bind` as positional arguments (by the same convention)."
      }
    ]
  },
  {
    "number": 2774,
    "title": "Jaxify numpy function",
    "created_at": "2020-04-20T16:24:29Z",
    "closed_at": "2022-06-24T20:24:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/2774",
    "body": "I have not used JAX enough.\r\n\r\nIn my understanding to be able to use JAX, for instance in order to get the gradient of a function, one needs to define a function using `jax.numpy` functions instead of `numpy`.\r\n\r\nHow do I avoid double declarations?\r\n\r\nSome would suggest to use only `JAX`, but, aside performance and correctness concerns, I have concerns about behavior: do `jax.numpy` function behave  identically to `numpy` functions? What happens when `numpy` functions are updated to change some behaviors?\r\n\r\nWouldn't it be possible to create an instance of the function for JAX purposes with a function which would inspect the code and replace the `numpy` or `scipy` methods with JAX analogs?",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/2774/comments",
    "author": "sursu",
    "comments": [
      {
        "user": "jakevdp",
        "created_at": "2020-04-20T19:05:48Z",
        "body": "That's a really interesting idea, and it could be quite useful, although it would be a challenge to implement it well.\r\n\r\nI was thinking about whether this could be done with no change to the jax source; this is a terrible hack that is quite brittle, but something like this will work in simple cases:\r\n```python\r\nimport inspect\r\nimport numpy as np\r\nfrom functools import wraps\r\n\r\ndef jaxify(func):\r\n  import jax.numpy\r\n  namespace = func.__globals__.copy()\r\n  namespace['np'] = namespace['numpy'] = jax.numpy\r\n  namespace['jaxify'] = lambda func: func\r\n  source = inspect.getsource(func)\r\n  exec(source, namespace)\r\n  return wraps(func)(namespace[func.__name__])\r\n\r\n@jaxify\r\ndef my_func(N):\r\n  return np.arange(N).sum()\r\n\r\nmy_func(10)\r\n# DeviceArray(45, dtype=int32)\r\n```"
      },
      {
        "user": "jekbradbury",
        "created_at": "2020-04-21T00:32:01Z",
        "body": "An alternative approach to overloading NumPy code in-place was explored in #1565 and prototyped in #611. If you're interested in that functionality, please chime in on #1565, since a major reason it wasn't merged was a relative lack of interested users (compared to the added complexity)."
      },
      {
        "user": "shoyer",
        "created_at": "2020-04-21T06:37:59Z",
        "body": "Numba does tricks like this inside `numba.jit` and it seems to work pretty well for their users.\r\n\r\nThat said, I think this would be very hard to do in JAX because we occasionally see people using original NumPy inside JAX functions. It's also not very explicit or composable.\r\n\r\nI do like the idea of trying to support overrides of NumPy's API via NumPy's own protocols (#1565), which would at least solve most of the composability issues."
      },
      {
        "user": "jakevdp",
        "created_at": "2022-06-24T20:24:38Z",
        "body": "I'm going to close this as out of scope for the project."
      }
    ]
  },
  {
    "number": 2522,
    "title": "Index all but one element in an array",
    "created_at": "2020-03-26T23:36:20Z",
    "closed_at": "2020-03-27T01:02:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/2522",
    "body": "Hello!\r\n\r\nI have a function:\r\n```\r\n@jit \r\nremove_random_element(rng, arr):\r\n    n = arr.shape[0]\r\n     i = random.randint(rng, shape=(1,), minval=0, maxval=n)[0]\r\n    indices = np.hstack((np.arange(i), np.arange(i + 1, n)))\r\n    return arr[indices]\r\n```\r\nwhich does not work because arange tries to convert `i` into an `int` when it is an abstract value (using `astype` did not solve this.\r\n\r\nI have tried other functional approaches such as:\r\n```indices = np.where(np.arange(n) - i)```\r\nbut I receive a boolean indices error.\r\n\r\nIs it possible to do this? Thanks!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/2522/comments",
    "author": "john-heyer",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2020-03-27T00:03:04Z",
        "body": "Great question! This is a fun puzzle. The \"static shape\" requirement can be a bit tricky in these cases.\r\n\r\nI think your idea to use indexing is a good one. How about this?\r\n\r\n```python\r\nfrom jax import jit\r\nfrom jax import random\r\nimport jax.numpy as np\r\n\r\n@jit\r\ndef remove_random_element(rng, arr):\r\n  n = arr.shape[0]\r\n  i = random.randint(rng, shape=(), minval=0, maxval=n)\r\n  indices = np.arange(n - 1) + (np.arange(n - 1) >= i)\r\n  return arr[indices]\r\n\r\n\r\nkey = random.PRNGKey(0)\r\narr = np.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5])\r\n\r\narr2 = remove_random_element(key, arr)\r\nprint(arr2)\r\n```\r\n\r\nAnother way to do it would be to use a `lax.while_loop` or two (e.g. one that copies over all the elements up to but excluding the i'th, then another that copies over the rest). I've found that almost anything can be done with a `lax.while_loop`, but that's a bit of a last resort since generating a gather or scatter op (as indexing does) would be more efficient, and `while_loop`s are awkward to write.\r\n\r\nWDYT?"
      },
      {
        "user": "john-heyer",
        "created_at": "2020-03-27T00:41:39Z",
        "body": "Awesome! I really appreciate the quick response! \r\n\r\nI had also tried `arr[np.arange(n) != i] ` which gave the boolean indices error as well, but this solution is nice.  Thanks again :)"
      },
      {
        "user": "mattjj",
        "created_at": "2020-03-27T01:02:21Z",
        "body": "Glad it helped! Don't hesitate to ask similar questions in the future. Maybe we can make a \"`jit` golf\" compendium of challenge problems."
      }
    ]
  },
  {
    "number": 2300,
    "title": "index-dependent scan function `lax.scani`",
    "created_at": "2020-02-24T17:34:11Z",
    "closed_at": "2020-03-10T15:05:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/2300",
    "body": "I am interested in training recurrent networks for which the transition dynamics have some sort of time-dependence. For example, the network might evolve linear from time `t1=0` to time `t2` and is clamped at some constant parameter array `u` from then on. In normal python code I might write some thing like this\r\n\r\n```python\r\nfor step in range(n_steps):\r\n  x = a.dot(x) if step < t2 else u\r\n```\r\nI would like to differentiate through these dynamics using reverse-mode, so I've been trying to use `lax.scan`. \r\nHowever, I'm not sure how to introduce time-dependence into the scanning function `f`. Right now, I've defined two transition functions `f1` and `f2` one for each of the two cases:\r\n\r\n```python\r\ncarry, _ = lax.scan(f1, x0, length=t2)\r\ncarry, _ = lax.scan(f2, carry, length=n_steps - t2)\r\n```\r\nThis would get quite annoying when my transition dynamics is much more complicated.\r\n\r\nTherefore, I was wondering if it would be possible to have a function `lax.scani` which takes a scanning function `f` with type signature `f : int -> c -> a -> (c, b)`  where the first argument of `f` is the index of the element it is scanning; and importantly, we can use this integer index to do control flow. In the example above, we would have \r\n\r\n```python\r\ndef f(t, carry, x):\r\n   return a.dot(carry) if t < t2 else u\r\n\r\ncarry, _ = lax.scani(f, x0, length=n_steps)\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/2300/comments",
    "author": "tachukao",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2020-02-25T03:20:40Z",
        "body": "Thanks for the question!\r\n\r\nOne way to write it is like this:\r\n\r\n```python\r\ndef f(carry, i_x):\r\n  i, x = i_x\r\n  ...\r\n\r\ncarry, ys = lax.scan(f, init_carry, (np.arange(n_steps), xs))\r\n```\r\n\r\nbut then you couldn't use Python control flow on `i` in the body of `f`, and you'd need to use `lax.cond` instead. \r\n\r\nWould the dependence on `i` be arbitrary, or is there some regularity to it?"
      },
      {
        "user": "tachukao",
        "created_at": "2020-02-26T09:35:26Z",
        "body": "Thanks for the fast response. I've considered doing what you suggested, but the inability to do control flow on `i` was the main reason that I didn't.\r\n\r\nI wasn't aware of the function `lax.cond`. Would I be able to do control flow on `i` using `lax.cond` then? A use case I have in mind is \r\n\r\n```python\r\nx = a.dot(x) if i > 0 else x\r\n```\r\nI'm not sure if this is considered arbitrary.\r\n\r\nThanks again for your help!\r\n"
      },
      {
        "user": "NeilGirdhar",
        "created_at": "2020-03-09T18:11:55Z",
        "body": "Can't you put the time into your carry, and increment it in `f`?"
      },
      {
        "user": "tachukao",
        "created_at": "2020-03-09T18:36:59Z",
        "body": "Hi Neil, thanks for the suggestion - I certainly can. I guess the problem I have now is just that I need to figure out how to use `lax.cond` to do control flow on the time index `i` in a way that is differentiable, as @mattjj suggested above. This I haven't really explored."
      },
      {
        "user": "mattjj",
        "created_at": "2020-03-10T15:05:36Z",
        "body": "@tachukao yes, using `lax.cond` the control flow you write can always be staged out (i.e. by jit, or use in a scan body) and also differentiated. It's awkward, but it's the only robust way we've found to embed structured control flow in Python.\r\n\r\nYou can always avoid all this structured control flow stuff (`lax.scan`, `lax.cond`, etc) and write things with regular Python for-loops and ifs. JAX can differentiate native Python! But if you use `jit` on a Python loop, compile times may get long (because the loop is essentially unrolled into the XLA computation). (The purpose of `lax.scan` is to stage out a loop construct to XLA (without unrolling) and thus give good compile times.)\r\n\r\nHere's *sketch* code for how you might write it so that the loop and other control flow stays in Python, but you can still use `jit` on some parts:\r\n\r\n```python\r\nfrom functools import partial\r\nfrom jax import jit\r\n\r\n@jit\r\ndef f(params, hidden, x):\r\n  ...\r\n\r\n@jit \r\ndef g(params, hidden, x):\r\n  ...\r\n\r\n...\r\n\r\n\r\ndef rnn(params, hidden, inputs):\r\n  for i, x in enumerate(inputs):\r\n    if i % 10 == 0:\r\n      hidden, y = f(params, hidden, x)\r\n    elif i % 10 == 1:\r\n      hidden, y = g(params, hidden, x)\r\n    elif ...\r\n    outputs.append(y)\r\n  return hidden, outputs\r\n```\r\n\r\nYou only need to write things in terms of `lax.scan`/`lax.cond` if you need more performance because you want to `jit` the whole `rnn` function.\r\n\r\nIf we introduced a `lax.scani` kind of function, it'd just be a wrapper around `lax.scan` and `lax.cond`, but our policy is to avoid wrappers unless they're very commonly needed.\r\n\r\nI think we covered the original question, so I'm going to close this issue (otherwise we'll drown in issues!), but please open a new one if you have new questions!"
      },
      {
        "user": "tachukao",
        "created_at": "2020-03-10T22:11:13Z",
        "body": "Thanks @mattjj! That makes a lot of sense  👍 "
      }
    ]
  },
  {
    "number": 2115,
    "title": "Checking if an object is an array",
    "created_at": "2020-01-29T16:12:23Z",
    "closed_at": "2020-02-14T15:56:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/2115",
    "body": "Is there a best practice to verify that an object is an array, e.g. during input checks?\r\nComparing types seems unfeasable, since the function will be be called with abstract array types during tracing.\r\n\r\nEssentially i want to do something like\r\n```python\r\nimport jax.numpy as np\r\n\r\ndef is_array(obj):\r\n   return type(obj) == np.ndarray\r\n```\r\nBut also consider all jax types that might occur during tracing.\r\n\r\nIf it doesn't already exist, such an `is_array` function would be nice to have, especially if the abstract types used during racing should change in the future\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/2115/comments",
    "author": "Jakob-Unfried",
    "comments": [
      {
        "user": "jekbradbury",
        "created_at": "2020-01-31T05:09:19Z",
        "body": "I think `isinstance(x, jax.numpy.ndarray)` does what you want (it'll return True for JAX arrays, including abstract ones, as well as NumPy ndarrays and subtypes of ndarrays)"
      },
      {
        "user": "mattjj",
        "created_at": "2020-02-14T15:56:35Z",
        "body": "@jekbradbury 's answer is the right one; though we might revise it (see #2014) right now you should check `isinstance(x, jax.numpy.ndarray)`.\r\n\r\nHope that answers your question!"
      }
    ]
  },
  {
    "number": 2097,
    "title": "Optimizer does not change weights",
    "created_at": "2020-01-28T15:36:28Z",
    "closed_at": "2020-01-29T11:23:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/2097",
    "body": "I want to train a simple binary classifier in JAX STAX:\r\n```python\r\nimport jax.numpy as np\r\nfrom jax import grad, jit, random\r\nfrom jax.experimental import optimizers, stax\r\nfrom jax.experimental.stax import Dense, Relu, Sigmoid\r\nfrom sklearn.datasets import make_circles\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\n\r\ndef prepare_circles(n_samples):\r\n    X, y = make_circles(n_samples, noise=0.2, factor=0.5, random_state=1)\r\n    X = StandardScaler().fit_transform(X)\r\n    X_train, X_test, y_train, y_test = train_test_split(\r\n        X, y, test_size=0.4, random_state=42\r\n    )\r\n    return X_train, X_test, y_train, y_test\r\n\r\n\r\nlearning_rate = 0.01\r\nn_epochs = 100\r\nn_features = 2\r\nn_hidden_layers = 1\r\nn_nodes = 4\r\nn_samples = 1000\r\n\r\nX_train, X_test, y_train, y_test = prepare_circles(n_samples)\r\n\r\ninit_fun, apply_fun = stax.serial(\r\n    Dense(n_nodes), Relu, Dense(n_nodes), Relu, Dense(1), Sigmoid\r\n)\r\nout_shape, params = init_fun(random.PRNGKey(2), (n_samples, n_features))\r\nprint(params)\r\n\r\nopt_init, opt_update, get_params = optimizers.adam(step_size=learning_rate)\r\nopt_state = opt_init(params)\r\n\r\n\r\ndef loss(params, x, y):\r\n    p = apply_fun(params, x)\r\n    ce_loss = -np.sum(y * np.log(y) + (1 - y) * np.log(1 - y))\r\n    return ce_loss\r\n\r\n\r\n# Define a compiled update step\r\n@jit\r\ndef step(i, opt_state, x, y):\r\n    params = get_params(opt_state)\r\n    return opt_update(i, grad(loss)(params, x, y), opt_state)\r\n\r\n\r\nfor i in range(n_epochs):\r\n    opt_state = step(i, opt_state, X_train, y_train)\r\n\r\nparams = get_params(opt_state)\r\nprint(params)\r\n```\r\n\r\nThe problem is that the weights seem to be not updated at all.\r\nIs it a bug or am I missing something?",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/2097/comments",
    "author": "homocomputeris",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2020-01-29T02:12:56Z",
        "body": "Thanks for the issue report!\r\n\r\n`grad(loss)(params, x, y)` takes the derivative of `loss` with respect to `params`, but your loss function doesn't actually depend on the parameters (only on `y`).\r\n\r\n```\r\ndef loss(params, x, y):\r\n    p = apply_fun(params, x)\r\n    ce_loss = -np.sum(y * np.log(y) + (1 - y) * np.log(1 - y))\r\n    return ce_loss\r\n```\r\n\r\nDid you mean to use `p` in `loss`?\r\n\r\nDoes that answer your question?\r\n"
      },
      {
        "user": "homocomputeris",
        "created_at": "2020-01-29T11:23:08Z",
        "body": "Yep, obviously my bad. Thanks!"
      }
    ]
  },
  {
    "number": 2048,
    "title": "'Can't lift Traced value' errors when nesting traces",
    "created_at": "2020-01-23T11:57:41Z",
    "closed_at": "2020-01-24T12:41:34Z",
    "labels": [
      "question",
      "documentation"
    ],
    "url": "https://github.com/jax-ml/jax/issues/2048",
    "body": "Reduced example:\r\n\r\n```python\r\ndef D(f, x):\r\n    return jax.jvp(f, (x,), (1.0,))[1]\r\n\r\ndef f(x):\r\n    def inner(y):\r\n        nonlocal x\r\n        x = y\r\n        return x\r\n    return D(inner, x)*x\r\n\r\nD(f, 1.0) # Exception: Can't lift Traced<ConcreteArray(1.0)>with<JVPTrace(level=4/0)> to JVPTrace(level=3/0)\r\n```\r\n\r\nPresumably related to JAX's mechanism for distinguishing between different traces when nesting. Seems like this could come up in a few different ways; I couldn't find any mention in the gotchas.\r\n\r\nRelated example:\r\n\r\n```python\r\ndef test():\r\n    x = 1\r\n    def inner(y):\r\n        nonlocal x\r\n        x = x*y\r\n        return x\r\n    a = D(inner, 1.0)\r\n    b = D(inner, 1.0)\r\n    return b\r\n\r\ntest() # Exception: Different traces at same level: Traced<ConcreteArray(1.0, weak_type=True)>with<JVPTrace(level=4/0)>, JVPTrace(level=4/0)\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/2048/comments",
    "author": "MikeInnes",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2020-01-23T18:18:55Z",
        "body": "Thanks for the question, Mike!\r\n\r\nThe trouble here is there's a side-effect, namely where you write `x = y` for the nonlocal `x`. Side-effects void your JAX warranty (i.e. JAX transformations only work on pure functions), and this is exactly the error you see when your code has side effects.\r\n\r\nSo this is working as intended, insofar as JAX disallows side effects (and there are no plans to support general Python side effects, which we consider impossible without owning the Python language implementation).\r\n\r\nWDYT?"
      },
      {
        "user": "mattjj",
        "created_at": "2020-01-23T18:21:20Z",
        "body": "I think we can be clearer in the readme's gotcha section that JAX only works with pure functions (I wonder if it used to be clearer and the readme revision in December removed some key lines), and even point out that this is the kind of error you'd see if you have side effects in code you're trying to transform with JAX."
      },
      {
        "user": "mattjj",
        "created_at": "2020-01-23T18:26:02Z",
        "body": "I attempted to improve the language a bit in a61bcff. WDYT?"
      },
      {
        "user": "MikeInnes",
        "created_at": "2020-01-24T12:41:34Z",
        "body": "Thanks a lot for the explanation! Yeah, that makes total sense to me, and I think the text you added to the gotchas is very helpful.\r\n\r\nI think there's a slight subtlety here in that most (internal) side effects are actually OK as long as the function being traced is referentially transparent overall. If \"function\" is read as \"the function object passed to JAX\" then the text you added is completely clear on that, but if it's read as \"each function definition involved\" it might be taken in an overly-strict way. Just a thought; I'm personally quite happy to encourage people to use pure functions everywhere :)\r\n\r\nIf you wanted to be really precise I think you'd have to say something along the lines of \"the set of functions that JAX traces must behave like a set of referentially transparent functions.\" I say \"behaves like\" because things like unnecessary `nonlocal`s will work, even if they violate referential transparency. (I just mention this as a curiosity, it's obviously not necessary to document at this level even if it's a reasonable statement.)\r\n\r\n<details>\r\n\r\n```python\r\ndef f1(x):\r\n    def f2(y):\r\n        nonlocal x\r\n        x = 2*x\r\n        return x*y\r\n    return D(f2, x)\r\n\r\nD(f1, 1.0) # => 2.0\r\n```\r\n</details>\r\n\r\nAnyway, I think this issue is resolved; thanks a lot for addressing it."
      }
    ]
  },
  {
    "number": 2041,
    "title": "vmap nested within pmap",
    "created_at": "2020-01-22T19:00:16Z",
    "closed_at": "2020-01-23T04:21:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/2041",
    "body": "Am I right to assume that using vmap nested within pmap is all fine and safe to do? Say I have a function to do message-passing on a (1, T) long data array. I could then use vmap to do this simultaneously for N independent data-sequences, i.e. it allows us to operate on (N, T).  And finally if we were to reshape that to (device_count, -1, T) then in principle I should be able to have pmap on top of vmap. Just checking if there are any gotchas with this / more \"correct\" way of doing this.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/2041/comments",
    "author": "HHalva",
    "comments": [
      {
        "user": "HHalva",
        "created_at": "2020-01-22T19:03:43Z",
        "body": "p.s. im asking this just in principle -- i realize in real application there will complications such as probably having to replicate parameters etc. "
      },
      {
        "user": "shoyer",
        "created_at": "2020-01-22T22:04:23Z",
        "body": "Yes, that works.\r\n\r\nIf you just want this for simulating a larger batch size, you might also find the currently undocumented/experimental `jax.soft_pmap` transform useful for this. It basically exists to do exactly this, splitting a `vmap` across multiple devices."
      },
      {
        "user": "HHalva",
        "created_at": "2020-01-22T23:07:09Z",
        "body": "thanks - that's very cool!"
      },
      {
        "user": "mattjj",
        "created_at": "2020-01-23T04:21:02Z",
        "body": "We hope to clean up and document `soft_pmap` in the next month or two. It works now, but I think we can make it cover more cases and simplify the implementation.\r\n\r\nFor now, though, I'll close this question. Please reopen if I'm mistaken, or open new ones for new questions!"
      },
      {
        "user": "gerdm",
        "created_at": "2023-02-02T13:50:38Z",
        "body": "Hi @mattjj, @shoyer \r\nI have an embarrassingly parallel training loop that I'd like to pmap. Since the total number of trials I want to to run is greater than the number of devices, I was considering using `soft_pmap` as suggested in this issue, but I can't seem find any mention of it in the documentation.\r\n\r\nIs `soft_pmap` still part of jax? "
      }
    ]
  },
  {
    "number": 1883,
    "title": "Casting from list of strings to floats",
    "created_at": "2019-12-17T22:15:23Z",
    "closed_at": "2019-12-18T09:47:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/1883",
    "body": "Hi,\r\n\r\nI ran into the following issue and wondered what the best way to proceed is.  I loaded some data from a text file and tried to convert it to an array. This seemed to work fine in ordinary numpy but raises an error in jax.\r\n\r\nIs this a feature that Jax might benefit from? Do you have a recommended way around this?\r\n\r\nthanks!\r\n\r\nheres a minimal reproduction:\r\n```\r\n>>> import numpy as np\r\n>>> import jax.numpy as jnp\r\n>>> x = np.array('3.4').astype(np.float32)\r\n>>> y = jnp.array('3.4').astype(jnp.float32)\r\nTraceback (most recent call last):\r\n  File \"/Users/Raza/miniconda3/envs/active-learning/lib/python3.6/site-packages/jax/interpreters/xla.py\", line 126, in abstractify\r\n    return pytype_aval_mappings[type(x)](x)\r\nKeyError: <class 'str'>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/Raza/miniconda3/envs/active-learning/lib/python3.6/site-packages/jax/numpy/lax_numpy.py\", line 1653, in array\r\n    out = lax.reshape(object, ())\r\n  File \"/Users/Raza/miniconda3/envs/active-learning/lib/python3.6/site-packages/jax/lax/lax.py\", line 635, in reshape\r\n    old_sizes=onp.shape(operand))\r\n  File \"/Users/Raza/miniconda3/envs/active-learning/lib/python3.6/site-packages/jax/core.py\", line 150, in bind\r\n    return self.impl(*args, **kwargs)\r\n  File \"/Users/Raza/miniconda3/envs/active-learning/lib/python3.6/site-packages/jax/lax/lax.py\", line 2475, in _reshape_impl\r\n    dimensions=dimensions, old_sizes=old_sizes)\r\n  File \"/Users/Raza/miniconda3/envs/active-learning/lib/python3.6/site-packages/jax/interpreters/xla.py\", line 142, in apply_primitive\r\n    compiled_fun = xla_primitive_callable(prim, *abstract_args, **params)\r\n  File \"/Users/Raza/miniconda3/envs/active-learning/lib/python3.6/site-packages/jax/interpreters/xla.py\", line 128, in abstractify\r\n    raise TypeError(\"No abstraction handler for type: {}\".format(type(x)))\r\nTypeError: No abstraction handler for type: <class 'str'>\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/1883/comments",
    "author": "Razcle",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2019-12-18T01:34:18Z",
        "body": "In general JAX doesn't support string types. However, in this case, there's an easy workaround: you can first cast your array to a classic Numpy array and then convert the result to a JAX array, e.g., `jnp.array(np.array('3.4').astype(np.float32))`\r\n\r\nDoes that work for you?"
      },
      {
        "user": "Razcle",
        "created_at": "2019-12-18T09:47:22Z",
        "body": "Thanks. Thats exactly what I ended up doing! :) "
      }
    ]
  },
  {
    "number": 1852,
    "title": "Feature request: ndarray.norm(), ndarray.H",
    "created_at": "2019-12-12T18:20:05Z",
    "closed_at": "2019-12-17T15:44:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/1852",
    "body": "I have recently come to Jax from pyTorch because i need the holomorphic autodiff.\r\nI want to request two features because they would improve the readability of my code:\r\n\r\n1. calling `norm()` on tensors:\r\nI want to use something like\r\n```\r\nimport jax.numpy as np\r\na = np.array([[1, 2], [3, 4]])\r\na_norm = a.norm()\r\n```\r\nwhich should just return `np.linalg.norm(a)`\r\n\r\n2. a shorthand for the hermitian conjugate\r\nJust as there is a shorthand `a.T` for the transpose, i would like to have `a.H == a.conj().T`\r\n\r\nDisclaimer: As i am new to Jax, it is very possible that i am unaware of a good reason these do not exist.\r\nUnfortunately this also means that I do not feel up to the task of proposing implementations myself.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/1852/comments",
    "author": "Jakob-Unfried",
    "comments": [
      {
        "user": "shoyer",
        "created_at": "2019-12-13T17:31:14Z",
        "body": "`jax.numpy` strives to reproduce NumPy's API. Given that goal, I'm not sure adding these methods/attributes makes sense for us, given that it would be a deviation from NumPy.\r\n\r\nMy own opinion is that we should reserve extending NumPy's API in `jax.numpy` for cases where it *isn't possible* do something in NumPy (e.g., we added the `bfloat16` dtype), not merely for cases where we would prefer different syntactic sugar."
      },
      {
        "user": "hawkinsp",
        "created_at": "2019-12-16T16:41:22Z",
        "body": "I agree with Stephan: on the whole we'd prefer not to deviate from NumPy without a good reason.\r\n\r\nI know PyTorch has a convention that uses methods on Tensor (e.g., `.norm()`); I think on the whole JAX prefers to use free functions (`jax.numpy.linalg.norm()`). One advantage of free functions is that they are an open set: anyone can define one. Methods on `DeviceArray` are a bit harder for users to add freely; I'd characterize them as a closed set.\r\n\r\nSyntactically there doesn't seem to be much difference to me between `x.norm()` vs `norm(x)`, and in fact the latter is one fewer character to type, although it's more characters if you qualify the name.\r\n\r\nFor the hermitian conjugate, in some of our libraries we define:\r\n```\r\ndef T(x): return np.swapaxes(x, -1, -2)\r\ndef H(x): return np.conj(T(x))\r\n```\r\nand then you can write:\r\n```\r\nH(x)\r\n```\r\n\r\nI wouldn't want to export that name, but you can easily do that in your own code.\r\n\r\nWhat do you think?\r\n"
      },
      {
        "user": "Jakob-Unfried",
        "created_at": "2019-12-17T15:44:05Z",
        "body": "@hawkinsp \r\n\r\nI agree, that what i requested is nowhere near useful enough to justify a deviation from the original numpy.\r\n\r\nThank you for the suggestions of defining `H(x)`\r\nWhile i had thought about that, it somehow didn't occur to me that i don't have to use `.T` but can define a `T(x)` myself.\r\n\r\nThanks for taking the time =)"
      }
    ]
  },
  {
    "number": 1804,
    "title": "Build dependency on TensorFlow",
    "created_at": "2019-12-03T14:24:51Z",
    "closed_at": "2020-03-09T19:58:06Z",
    "labels": [
      "question",
      "build"
    ],
    "url": "https://github.com/jax-ml/jax/issues/1804",
    "body": "I tried to build on Mac OS X, but I get this error when running `python3 setup.py`:\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_cj/de96de179f580494f32631716f8b8a5c/external/org_tensorflow/tensorflow/core/BUILD:2513:1: @org_tensorflow//tensorflow/core:version_info_gen depends on @local_config_git//:gen/spec.json in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/private/var/tmp/_bazel_cj/de96de179f580494f32631716f8b8a5c/external/org_tensorflow/third_party/git/git_configure.bzl\", line 64\r\n\t\t_fail(result.stderr)\r\n\tFile \"/private/var/tmp/_bazel_cj/de96de179f580494f32631716f8b8a5c/external/org_tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_cj/de96de179f580494f32631716f8b8a5c/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 32, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n```\r\n\r\nAnd the question is: Do I really need to install TensorFlow in order to build Jax?",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/1804/comments",
    "author": "CharlesJQuarra",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2019-12-03T14:27:43Z",
        "body": "You should try the prebuilt Jaxlib packages (from `pip`) first; they are much easier to install. If they don't work for you we should look into that.\r\n\r\nJAX uses XLA as its JIT compiler, which is distributed as part of the TensorFlow tree. Hence `jaxlib` builds that part of the TensorFlow tree as a submodule. It does not use most of TensorFlow (e.g., the TF kernels, graph logic, or any of the Python code).\r\n\r\nWhat version of Python do you have and how did you install it?"
      },
      {
        "user": "hawkinsp",
        "created_at": "2019-12-03T14:31:14Z",
        "body": "Also to be completely clear: you should not have to install TensorFlow yourself. JAX downloads and builds the necessary part as part of its build process."
      },
      {
        "user": "lauradriscoll",
        "created_at": "2019-12-21T01:04:41Z",
        "body": "@hawkinsp We're having the same issue. Can't use pip install because we're using the stanford cluster which runs centos 7 (GLIBC issue #854). We specified cuda path and cudnn path with this command:\r\n\r\n$CUDA_PATH=/share/software/user/open/cuda/10.0.130/\r\n$CUDNN_PATH=/share/software/user/open/cudnn/7.6.4\r\n$python build/build.py --cuda_path $CUDA_PATH --cudnn_path $CUDNN_PATH --enable_cuda\r\n\r\n```python\r\n     _   _  __  __\r\n    | | / \\ \\ \\/ /\r\n _  | |/ _ \\ \\  /\r\n| |_| / ___ \\/  \\\r\n \\___/_/   \\/_/\\_\\\r\n\r\n\r\nWARNING: Output base '/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nStarting local Bazel server and connecting to it...\r\nBazel binary path: ./bazel-0.29.1-linux-x86_64\r\nPython binary path: /share/software/user/open/python/2.7.13/bin/python\r\nMKL-DNN enabled: yes\r\n-march=native: no\r\nCUDA enabled: yes\r\nCUDA toolkit path: /share/software/user/open/cuda/10.0.130/\r\nCUDNN library path: /share/software/user/open/cudnn/7.4.1.5\r\n\r\nBuilding XLA and installing it in the jaxlib source tree...\r\n./bazel-0.29.1-linux-x86_64 run --verbose_failures=true --config=mkl_open_source_only --config=cuda --define=xla_python_enable_gpu=true :install_xla_in_source_tree /home/users/lndrisco/code/jax/build\r\nWARNING: Output base '/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'run' from /home/users/lndrisco/code/jax/.bazelrc:\r\n  Inherited 'build' options: --repo_env PYTHON_BIN_PATH=/share/software/user/open/python/2.7.13/bin/python --python_path=/share/software/user/open/python/2.7.13/bin/python --repo_env TF_NEED_CUDA=1 --distinct_host_configuration=false --copt=-Wno-sign-compare -c opt --apple_platform_type=macos --macos_minimum_os=10.9 --announce_rc --define=no_aws_support=true --define=no_gcp_support=true --define=no_hdfs_support=true --define=no_kafka_support=true --define=no_ignite_support=true --define=grpc_no_ares=true --spawn_strategy=standalone --strategy=Genrule=standalone --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --action_env CUDA_TOOLKIT_PATH=/share/software/user/open/cuda/10.0.130/ --action_env CUDNN_INSTALL_PATH=/share/software/user/open/cudnn/7.4.1.5\r\nINFO: Found applicable config definition build:mkl_open_source_only in file /home/users/lndrisco/code/jax/.bazelrc: --define=tensorflow_mkldnn_contraction_kernel=1\r\nINFO: Found applicable config definition build:cuda in file /home/users/lndrisco/code/jax/.bazelrc: --crosstool_top=@local_config_cuda//crosstool:toolchain --define=using_cuda=true --define=using_cuda_nvcc=true\r\nLoading:\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nDEBUG: /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:5:\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nAnalyzing: target //build:install_xla_in_source_tree (1 packages loaded, 0 targets configured)\r\nAnalyzing: target //build:install_xla_in_source_tree (36 packages loaded, 130 targets configured)\r\nWARNING: /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/core/platform/BUILD:52:1: in srcs attribute of cc_library rule @org_tensorflow//tensorflow/core/platform:env_time_impl: please do not import '@org_tensorflow//tensorflow/core/platform/default:env_time.cc' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'tf_instantiate_platform_libraries', the error might have been caused by the macro implementation in /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/core/platform/BUILD:52:1\r\nWARNING: /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/core/platform/BUILD:52:1: in srcs attribute of cc_library rule @org_tensorflow//tensorflow/core/platform:logging_impl: please do not import '@org_tensorflow//tensorflow/core/platform/default:logging.cc' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'tf_instantiate_platform_libraries', the error might have been caused by the macro implementation in /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/core/platform/BUILD:52:1\r\nTraceback (most recent call last):\r\n  File \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 32, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\nINFO: Call stack for the definition of repository 'local_config_git' which is a git_configure (rule definition at /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/git/git_configure.bzl:66:17):\r\n - /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/workspace.bzl:74:5\r\n - /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/workspace.bzl:61:5\r\n - /home/users/lndrisco/code/jax/WORKSPACE:41:1\r\nERROR: An error occurred during the fetch of repository 'local_config_git':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/git/git_configure.bzl\", line 64\r\n\t\t_fail(result.stderr)\r\n\tFile \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 32, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nINFO: Call stack for the definition of repository 'llvm' which is a tf_http_archive (rule definition at /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/repo.bzl:121:19):\r\n - /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/workspace.bzl:549:5\r\n - /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/workspace.bzl:61:5\r\n - /home/users/lndrisco/code/jax/WORKSPACE:41:1\r\nERROR: /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/core/util/BUILD:265:1: @org_tensorflow//tensorflow/core/util:version_info_gen depends on @local_config_git//:gen/spec.json in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/git/git_configure.bzl\", line 64\r\n\t\t_fail(result.stderr)\r\n\tFile \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 32, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nERROR: /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/core/util/BUILD:265:1: @org_tensorflow//tensorflow/core/util:version_info_gen depends on @local_config_git//:gen/head in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/git/git_configure.bzl\", line 64\r\n\t\t_fail(result.stderr)\r\n\tFile \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 32, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nERROR: /home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/core/util/BUILD:265:1: @org_tensorflow//tensorflow/core/util:version_info_gen depends on @local_config_git//:gen/branch_ref in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/git/git_configure.bzl\", line 64\r\n\t\t_fail(result.stderr)\r\n\tFile \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 32, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nERROR: Analysis of target '//build:install_xla_in_source_tree' failed; build aborted: no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/git/git_configure.bzl\", line 64\r\n\t\t_fail(result.stderr)\r\n\tFile \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/home/users/lndrisco/.cache/bazel/_bazel_lndrisco/211ff64f4219b1450e0e3c09844491f0/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 32, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nINFO: Elapsed time: 271.579s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (95 packages loaded, 1974 targets configured)\r\nERROR: Build failed. Not running target\r\nFAILED: Build did NOT complete successfully (95 packages loaded, 1974 targets configured)\r\nTraceback (most recent call last):\r\n  File \"build/build.py\", line 351, in <module>\r\n    main()\r\n  File \"build/build.py\", line 346, in main\r\n    shell(command)\r\n  File \"build/build.py\", line 50, in shell\r\n    output = subprocess.check_output(cmd)\r\n  File \"/share/software/user/open/python/2.7.13/lib/python2.7/subprocess.py\", line 219, in check_output\r\n    raise CalledProcessError(retcode, cmd, output=output)\r\nsubprocess.CalledProcessError: Command '['./bazel-0.29.1-linux-x86_64', 'run', '--verbose_failures=true', '--config=mkl_open_source_only', '--config=cuda', '--define=xla_python_enable_gpu=true', ':install_xla_in_source_tree', '/home/users/lndrisco/code/jax/build']' returned non-zero exit status 1\r\n```"
      },
      {
        "user": "skye",
        "created_at": "2019-12-21T02:22:58Z",
        "body": "Can you try `pip install future`? That should include the builtins module."
      },
      {
        "user": "hawkinsp",
        "created_at": "2020-03-09T19:58:06Z",
        "body": "This build error pertained to Python 2; we don't support Python 2 any more so it's likely the issue is moot. Closing."
      }
    ]
  },
  {
    "number": 1629,
    "title": "How to use jit with jax.numpy.reshape?",
    "created_at": "2019-11-05T11:05:01Z",
    "closed_at": "2020-07-14T18:23:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/1629",
    "body": "Hi,\r\n\r\ni started to use JAX today to compute some gradients and it works perfectly. \r\nNow i want to speed up things with `jit`, but i encountered a problem with `jax.numpy.reshape`.\r\n\r\nIf someone could tell me what is going wrong or how to debug the function, that would be great.\r\n\r\n```\r\n def functional(X):\r\n   r = X.shape[0]\r\n   X_imshape = np.reshape(X, (r, m, n), order = 'F')\r\n   ....\r\n```\r\n\r\nThe output i get is\r\n\r\n```\r\nUserWarning:\r\n\r\nNo GPU/TPU found, falling back to CPU.\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 61, in _wrapfunc\r\n    return bound(*args, **kwds)\r\nTypeError: 'JaxprTracer' object cannot be interpreted as an integer\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"spatial_unmixing.py\", line 111, in <module>\r\n    main()\r\n  File \"spatial_unmixing.py\", line 87, in main\r\n    x_res = ms.optimII(x0, lambda x: jax_gradient_spatial_functional(x, Y, K_GT, m, n, lambda_, tau, h), ms.projectVectorsCanonicalSimplex, alpha0, mu, L, restart_nesterov, maxIter, threshold)\r\n  File \"/Users/xxx/Desktop/xxx/Projects/unmixing/unmixing_example/utilities/msiplib.py\", line 675, in optimII\r\n    xNew = projC(yOld - 1 / L * fGrad(yOld))\r\n  File \"spatial_unmixing.py\", line 87, in <lambda>\r\n    x_res = ms.optimII(x0, lambda x: jax_gradient_spatial_functional(x, Y, K_GT, m, n, lambda_, tau, h), ms.projectVectorsCanonicalSimplex, alpha0, mu, L, restart_nesterov, maxIter, threshold)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/api.py\", line 148, in f_jitted\r\n    out = xla.xla_call(flat_fun, *args_flat, device=device, backend=backend)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/core.py\", line 591, in call_bind\r\n    outs = primitive.impl(f, *args, **params)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/interpreters/xla.py\", line 418, in _xla_call_impl\r\n    compiled_fun = _xla_callable(fun, device, backend, *map(abstractify, args))\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/linear_util.py\", line 217, in cached_fun\r\n    ans, f_prev = cached_fun_body(f, args)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/linear_util.py\", line 214, in cached_fun_body\r\n    return call(f, *args), f\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/interpreters/xla.py\", line 434, in _xla_callable\r\n    jaxpr, (pvals, consts, env) = pe.trace_to_subjaxpr(fun, master, False).call_wrapped(pvals)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/linear_util.py\", line 165, in call_wrapped\r\n    ans = self.f(*args, **dict(self.params, **kwargs))\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/api.py\", line 343, in grad_f\r\n    _, g = value_and_grad_f(*args, **kwargs)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/api.py\", line 391, in value_and_grad_f\r\n    ans, vjp_py = vjp(f_partial, *dyn_args)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/api.py\", line 1149, in vjp\r\n    out_primal, out_vjp = ad.vjp(flat_fun, primals_flat)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/interpreters/ad.py\", line 107, in vjp\r\n    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/interpreters/ad.py\", line 96, in linearize\r\n    jaxpr, out_pvals, consts = pe.trace_to_jaxpr(jvpfun_flat, in_pvals)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/interpreters/partial_eval.py\", line 343, in trace_to_jaxpr\r\n    jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/linear_util.py\", line 165, in call_wrapped\r\n    ans = self.f(*args, **dict(self.params, **kwargs))\r\n  File \"/Users/xxx/Desktop/xxx/Projects/unmixing/unmixing_example/jax_unmixing.py\", line 24, in spatial_functional_jax\r\n    X_imshape = np.reshape(X, (r, m, n), order = 'F')\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/numpy/lax_numpy.py\", line 778, in reshape\r\n    return a.reshape(newshape, order=order)  # forward to method for ndarrays\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/numpy/lax_numpy.py\", line 808, in _reshape_method\r\n    return _reshape(a, newshape, order=order)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/jax/numpy/lax_numpy.py\", line 784, in _reshape\r\n    computed_newshape = onp.reshape(dummy_val, newshape).shape\r\n  File \"<__array_function__ internals>\", line 6, in reshape\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 301, in reshape\r\n    return _wrapfunc(a, 'reshape', newshape, order=order)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 70, in _wrapfunc\r\n    return _wrapit(obj, method, *args, **kwds)\r\n  File \"/Users/xxx/anaconda/envs/xxx/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 47, in _wrapit\r\n    result = getattr(asarray(obj), method)(*args, **kwds)\r\nTypeError: 'JaxprTracer' object cannot be interpreted as an integer\r\n\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/1629/comments",
    "author": "JLenssen",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2019-11-05T15:05:21Z",
        "body": "Thanks for the question! Can you share a minimal but runnable code snippet that reproduces the error?"
      },
      {
        "user": "hawkinsp",
        "created_at": "2020-07-14T18:23:13Z",
        "body": "This issue has been open with no response for a while; closing. Feel free to reopen with details. (It might be more appropriate as a discussion thread, rather than an issue.)"
      }
    ]
  },
  {
    "number": 1615,
    "title": "Orthogonal initialization fails for (at least) 2d matrices",
    "created_at": "2019-11-01T01:22:34Z",
    "closed_at": "2019-11-01T01:37:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/1615",
    "body": "The following code should generate an orthogonal 10x10 matrix.  \r\n\r\n```\r\nfrom jax.nn.initializers import orthogonal, uniform\r\nfrom jax import random\r\n\r\nkey = random.PRNGKey(0)\r\n\r\no_init = orthogonal()\r\northogonal_matrix = o_init(key, (10,10))\r\n```\r\n\r\nHowever, the actual output is the following:\r\n\r\n```\r\n--------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-af5241da1f40> in <module>\r\n----> 1 o_init(key, (10,10))\r\n\r\n~/VirtualEnvs/Jax/lib/python3.7/site-packages/jax/nn/initializers.py in init(key, shape, dtype)\r\n     93     Q *= np.sign(np.diag(R)) # needed for a uniform distribution\r\n     94     if n_rows < n_cols: Q = Q.T\r\n---> 95     Q = np.reshape(Q, onp.delete(shape, column_axis) + (shape[column_axis],))\r\n     96     Q = np.moveaxis(Q, -1, column_axis)\r\n     97     return scale * Q\r\n\r\n~/VirtualEnvs/Jax/lib/python3.7/site-packages/jax/numpy/lax_numpy.py in reshape(a, newshape, order)\r\n    730 def reshape(a, newshape, order=\"C\"):\r\n    731   try:\r\n--> 732     return a.reshape(newshape, order=order)  # forward to method for ndarrays\r\n    733   except AttributeError:\r\n    734     return _reshape(a, newshape, order=order)\r\n\r\n~/VirtualEnvs/Jax/lib/python3.7/site-packages/jax/numpy/lax_numpy.py in _reshape_method(a, *newshape, **kwargs)\r\n    760   if len(newshape) == 1 and not isinstance(newshape[0], int):\r\n    761     newshape = newshape[0]\r\n--> 762   return _reshape(a, newshape, order=order)\r\n    763 \r\n    764 \r\n\r\n~/VirtualEnvs/Jax/lib/python3.7/site-packages/jax/numpy/lax_numpy.py in _reshape(a, newshape, order)\r\n    736 def _reshape(a, newshape, order=\"C\"):\r\n    737   dummy_val = onp.broadcast_to(0, shape(a))  # zero strides\r\n--> 738   computed_newshape = onp.reshape(dummy_val, newshape).shape\r\n    739 \r\n    740   if order == \"C\":\r\n\r\n<__array_function__ internals> in reshape(*args, **kwargs)\r\n\r\n~/VirtualEnvs/Jax/lib/python3.7/site-packages/numpy/core/fromnumeric.py in reshape(a, newshape, order)\r\n    299            [5, 6]])\r\n    300     \"\"\"\r\n--> 301     return _wrapfunc(a, 'reshape', newshape, order=order)\r\n    302 \r\n    303 \r\n\r\n~/VirtualEnvs/Jax/lib/python3.7/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds)\r\n     59 \r\n     60     try:\r\n---> 61         return bound(*args, **kwds)\r\n     62     except TypeError:\r\n     63         # A TypeError occurs if the object does have such a method in its\r\n\r\nValueError: cannot reshape array of size 100 into shape (20,)\r\n```\r\n\r\nAs a sanity check, running almost the identical code for a uniform initialization works fine:\r\n```\r\nfrom jax.nn.initializers import orthogonal, uniform\r\nfrom jax import random\r\n\r\nkey = random.PRNGKey(0)\r\n\r\nu_init = uniform()\r\nuniform_matrix = u_init(key, (10,10))\r\n```\r\n\r\nFrom looking at the code for the orthogonal initializer, it seems like the problem occurs after the QR decomposition is completed and the Q matrix is being reshaped.  Here is the source:\r\n```\r\ndef orthogonal(scale=1.0, column_axis=-1):\r\n   \"\"\"\r\n   Construct an initializer for uniformly distributed orthogonal matrices.\r\n  \r\n   If the shape is not square, the matrices will have orthonormal rows or columns\r\n   depending on which side is smaller.\r\n   \"\"\"\r\n   def init(key, shape, dtype=np.float32):\r\n     if len(shape) < 2:\r\n        raise ValueError(\"orthogonal initializer requires at least a 2D shape\")\r\n     n_rows, n_cols = onp.prod(shape) // shape[column_axis], shape[column_axis]\r\n     matrix_shape = (n_cols, n_rows) if n_rows < n_cols else (n_rows, n_cols)\r\n     A = random.normal(key, matrix_shape, dtype)\r\n     Q, R = np.linalg.qr(A)\r\n     Q *= np.sign(np.diag(R)) # needed for a uniform distribution\r\n     if n_rows < n_cols: Q = Q.T\r\n     Q = np.reshape(Q, onp.delete(shape, column_axis) + (shape[column_axis],))\r\n     Q = np.moveaxis(Q, -1, column_axis)\r\n     return scale * Q\r\n    return init    \r\n```\r\n\r\nIt looks as if the line ```Q = np.reshape(Q, onp.delete(shape, column_axis) + (shape[column_axis],))``` is trying to reshape the array into some shape, but that shape is not properly getting specified.  Specifically, the line ```onp.delete(shape, column_axis) + (shape[column_axis],)``` does not seem to be doing what it was intended to do.  ",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/1615/comments",
    "author": "ramasesh",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2019-11-01T01:24:56Z",
        "body": "What version of the `jax` package do you have? I think this may be already fixed in the latest release (0.1.49)."
      },
      {
        "user": "ramasesh",
        "created_at": "2019-11-01T01:37:51Z",
        "body": "Awesome, you are right.  I had (0.1.48).  I upgraded to (0.1.49) and the issue is fixed.  Thanks!"
      }
    ]
  },
  {
    "number": 1583,
    "title": "List comprehensions and for-loops on xla.DeviceArrays return numpy.ndarrays",
    "created_at": "2019-10-28T21:18:57Z",
    "closed_at": "2021-11-01T14:41:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/1583",
    "body": "Looping through a `DeviceArray` like in a list comprehension, for loop, or enumerate returns `numpy.ndarray`. Is this expected behaviour?\r\n\r\n```python\r\nimport jax.numpy as np\r\n\r\nX = np.array(range(3)).reshape(3,1)\r\nprint(type(X))\r\n#<class 'jax.interpreters.xla.DeviceArray'>\r\n\r\nx0,x1,x2 = [xi for xi in X]\r\nprint(type(x0))\r\n#<type 'numpy.ndarray'>\r\n\r\nfor xi in X:\r\n  print(type(xi))\r\n#<type 'numpy.ndarray'>\r\n#<type 'numpy.ndarray'>\r\n#<type 'numpy.ndarray'>\r\n```\r\n\r\nThis is unexpected to me. Why are these not also `DeviceArray`s?",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/1583/comments",
    "author": "jessebett",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2019-10-28T22:13:17Z",
        "body": "Thanks for the question! It is intended behavior: I used git blame to pull up commit 71605f4, which implemented this behavior by changing the implementation of `DeviceArray.__iter__` in xla.py. The commit message has more information:\r\n\r\n> Change `DeviceArray.__iter__` and `DeviceArray.__reversed__` to forward to the _value.\r\n>\r\n> This has the effect of transferring the entire array to the host and iterating over it in host memory, rather than slicing out individual elements in device memory one by one.\r\n> \r\n> This is much faster for examples like `list(np.arange(10000))`; previously this took several seconds the first time due to compilation and 100ms+ subsequent times. With this change it takes < 1ms.\r\n\r\nIn short, we do it for performance reasons.\r\n\r\nIn general, JAX doesn't guarantee when it will return an ndarray versus a DeviceArray. That lets us adjust the behavior in cases like this one.\r\n\r\nWDYT?"
      },
      {
        "user": "jessebett",
        "created_at": "2019-10-29T15:38:11Z",
        "body": "Okay, thanks @mattjj. I was pretty sure that I don't want to be looping through these `DeviceArrays` anyway, but now am confident that I'll need to do a vectorized approach. Correct me if I'm wrong but the drop down to ndarray also drops gradient tracking?"
      },
      {
        "user": "jessebett",
        "created_at": "2019-10-29T16:05:53Z",
        "body": "Actually, @mattjj this shows up in more surprising ways that I think are a problem for me. \r\n\r\nFor the `jet` stuff, we want to have jet coefficents as a list of `DeviceArrays`. However, `prop` rules for many of the primitives are more efficient if you treat them together as a multidimensional `DeviceArray`. So not changing the list-of-coefficients API, internally I concatenate them into a multidimensional array, perform the propogation rules, which produces a new multidimensional array of the output coefficients. Now, I'd expect that I could just call `list` on this to return the list of `DeviceArrays`, but this is not supported:\r\n\r\n```python\r\nIn [7]: import jax.numpy as np                                                                              \r\n\r\nIn [8]: np.array(np.arange(10)).reshape(5,2)                                                                \r\nOut[8]: \r\nDeviceArray([[0, 1],\r\n             [2, 3],\r\n             [4, 5],\r\n             [6, 7],\r\n             [8, 9]], dtype=int32)\r\n\r\nIn [9]: list(np.array(np.arange(10)).reshape(5,2))                                                          \r\nOut[9]: \r\n[array([0, 1], dtype=int32),\r\n array([2, 3], dtype=int32),\r\n array([4, 5], dtype=int32),\r\n array([6, 7], dtype=int32),\r\n array([8, 9], dtype=int32)]\r\n```\r\n\r\nPossibly there's a workaround or a different approach that is more appropriate in general."
      },
      {
        "user": "jekbradbury",
        "created_at": "2019-10-30T17:48:40Z",
        "body": "Why can't you use multidimensional arrays throughout as the representation of jet coefficients?"
      },
      {
        "user": "jessebett",
        "created_at": "2019-10-30T19:52:33Z",
        "body": "The performance gains wasn't clear (to me) until recently. So another solution might look like rewriting our assumptions on the types of the coefficents. This would actually be non-trivial, though. I believe I had them as a multidimensional array at one point. @mattjj do you remember if we had a good reason to write the coefficients as lists of primal-dimensional arrays instead of together as a order-dimensional array?"
      }
    ]
  },
  {
    "number": 1503,
    "title": "`jit` breaks `is`",
    "created_at": "2019-10-14T18:09:16Z",
    "closed_at": "2019-10-18T23:08:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/1503",
    "body": "Example:\r\n```\r\nimport jax.numpy as np\r\nfrom jax import jit\r\n\r\ndef _is(x, y):\r\n  return x is y\r\n\r\n\r\na = np.ones((1, 1))\r\nb = a\r\n\r\n_is(a, b)       # True\r\njit(_is)(a, b)  # False\r\n```\r\n\r\nIs it possible to make `jit` preserve the default `is` behavior? Thanks!",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/1503/comments",
    "author": "romanngg",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2019-10-14T18:14:08Z",
        "body": "JAX tracing doesn't preserve object identity. Nor should it, I think. In general, we want to trace a function once and run it many times, irrespective of the object aliasing relationships amongst the parameters. JAX thinks of the parameters as values, not references.\r\n\r\nDo you have a more concrete example of how this comes up in practice?\r\n"
      },
      {
        "user": "romanngg",
        "created_at": "2019-10-14T18:25:46Z",
        "body": "Thanks Peter!\r\n\r\nI was thinking about optimizations of this kind (simplified example):\r\n```\r\ndef diff(x, y):\r\n  is x is y:\r\n    return np.zeros_like(x)\r\n  return x - y\r\n```\r\nOur current workaround is to pass `y=None`, but I thought supporting `is` would be a bit cleaner and allow the user to call `diff(x, x)` without having to know the convention of passing `y=None` if the arguments are the same."
      },
      {
        "user": "jekbradbury",
        "created_at": "2019-10-14T19:49:38Z",
        "body": "I agree with Peter; the embedded language in JAX (on which all the transformations operate) only knows about values and functions of values, and doesn't have a notion of pointers, references, or object identity. In general we rely on end-to-end compilation with XLA to perform optimizations that might otherwise rely on things like mutation and object identity (if XLA sees a large chunk of your program, it may be able to prove that `x` and `y` always contain the same values and avoid the subtraction)."
      },
      {
        "user": "mattjj",
        "created_at": "2019-10-18T23:08:07Z",
        "body": "Thanks for raising this!\r\n\r\n+1 to what Peter and James said. To summarize: JAX programs under `jit` must be purely functional, and so as in Haskell and other purely functional languages object identity isn't preserved. We'd expect XLA to perform optimizations like these if it sees enough of the computation.\r\n\r\nI'm going to close this issue, but please reopen if we should keep discussing."
      }
    ]
  },
  {
    "number": 1166,
    "title": "Build fails",
    "created_at": "2019-08-11T09:16:38Z",
    "closed_at": "2019-08-12T18:56:55Z",
    "labels": [
      "question",
      "build"
    ],
    "url": "https://github.com/jax-ml/jax/issues/1166",
    "body": "With Python 2.7 and Ubuntu 18.04, after running this line:\r\npython build/build.py --enable_cuda\r\n\r\nI get the following errors:\r\n\r\nBuilding XLA and installing it in the jaxlib source tree...\r\nDEBUG: /home/chen/.cache/bazel/_bazel_chen/e15eb051390cace74a40a2626236fc36/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:5:\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nINFO: Analyzed target //build:install_xla_in_source_tree (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/chen/.cache/bazel/_bazel_chen/e15eb051390cace74a40a2626236fc36/external/com_google_absl/absl/hash/BUILD.bazel:28:1: C++ compilation of rule '@com_google_absl//absl/hash:hash' failed (Exit 1)\r\ngcc: error trying to exec 'cc1plus': execvp: No such file or directory\r\nTarget //build:install_xla_in_source_tree failed to build\r\nERROR: /home/chen/.cache/bazel/_bazel_chen/e15eb051390cace74a40a2626236fc36/external/org_tensorflow/tensorflow/compiler/xla/python/BUILD:242:1 C++ compilation of rule '@com_google_protobuf//:protoc_lib' failed (Exit 1)\r\nINFO: Elapsed time: 0.494s, Critical Path: 0.10s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\nTraceback (most recent call last):\r\n  File \"build/build.py\", line 334, in <module>\r\n    main()\r\n  File \"build/build.py\", line 329, in main\r\n    [\":install_xla_in_source_tree\", os.getcwd()])\r\n  File \"build/build.py\", line 50, in shell\r\n    output = subprocess.check_output(cmd)\r\n  File \"/home/chen/anaconda3/envs/LFADS/lib/python2.7/subprocess.py\", line 223, in check_output\r\n    raise CalledProcessError(retcode, cmd, output=output)\r\nsubprocess.CalledProcessError: Command '['/home/chen/bin/bazel', 'run', '--verbose_failures=true', '--config=mkl_open_source_only', '--config=cuda', ':install_xla_in_source_tree', '/home/chen/jax/build']' returned non-zero exit status 1\r\n\r\nAny help would be much appreciated.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/1166/comments",
    "author": "Chen-Beer",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2019-08-12T14:15:58Z",
        "body": "It sounds to me like you don't have a C++ compiler installed or it isn't working for some reason. Can you double check that?\r\n\r\nTry something like\r\n```\r\nsudo apt-get install build-essential python3-scipy cython3\r\npython3 build/build.py\r\n```\r\n\r\nI'll confirm exactly which packages are needed."
      }
    ]
  },
  {
    "number": 1160,
    "title": "Target //build:install_xla_in_source_tree failed to build",
    "created_at": "2019-08-10T10:22:13Z",
    "closed_at": "2020-04-16T18:34:20Z",
    "labels": [
      "question",
      "build"
    ],
    "url": "https://github.com/jax-ml/jax/issues/1160",
    "body": "While compiling I got the following error\r\n```\r\nINFO: Analyzed target //build:install_xla_in_source_tree (109 packages loaded, 10023 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/rory/jax/jaxlib/BUILD:24:1: Executing genrule //jaxlib:lapack.pyx_cython_translation failed (Exit 1)\r\nmodule.js:540\r\n    throw err;\r\n    ^\r\n\r\nError: Cannot find module '/home/rory/.cache/bazel/_bazel_rory/7428d4622c43829d3557b4741ba55bca/execroot/__main__/python'\r\n    at Function.Module._resolveFilename (module.js:538:15)\r\n    at Function.Module._load (module.js:468:25)\r\n    at Function.Module.runMain (module.js:684:10)\r\n    at startup (bootstrap_node.js:187:16)\r\n    at bootstrap_node.js:608:3\r\nTarget //build:install_xla_in_source_tree failed to build\r\nINFO: Elapsed time: 496.624s, Critical Path: 1.84s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/1160/comments",
    "author": "qwertpi",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2019-08-10T23:17:10Z",
        "body": "Can you say more about what platform you are building on and what version of `jax` you have? Could you perhaps use one of the prebuilt wheels instead?\r\n\r\nDo you have `cython` and `scipy` installed?"
      },
      {
        "user": "qwertpi",
        "created_at": "2019-08-11T07:29:06Z",
        "body": "> Can you say more about what platform you are building on and what version of `jax` you have? Could you perhaps use one of the prebuilt wheels instead?\r\n> \r\n> Do you have `cython` and `scipy` installed?\r\n\r\nI'm building on x86 Ubuntu Xenial, I'm using an Intel Celeron 3215U, when I use the prebuilt wheels I get an `Illegal instruction (core dumped)` error when running jax.grad (I've had these before with tensorflow due to the use of AVX instructions which my CPU doesn't support). I have scipy installed but didn't have cython installed a compile-time although by complete coincidence do have it installed now"
      },
      {
        "user": "hawkinsp",
        "created_at": "2020-04-16T18:34:20Z",
        "body": "Is this still a problem with an up to date `jaxlib`? (Closing, but reopen if you can still reproduce.)"
      }
    ]
  },
  {
    "number": 1130,
    "title": "slow compiling compared to a few weeks ago",
    "created_at": "2019-08-07T00:19:30Z",
    "closed_at": "2019-08-09T15:04:10Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/1130",
    "body": "I don't have a repo for this, but I have noticed a very significant (roughly 30x) slowdown in compilation when I run some jax code now compared to a few weeks ago (exact same code, no modifications at all). I'll share the code if needed, but it includes a number of vmap and scan calls. \r\n\r\nHave there been any updates recently that could possibly lead to such a slowdown?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/1130/comments",
    "author": "cpgoodri",
    "comments": [
      {
        "user": "hawkinsp",
        "created_at": "2019-08-07T00:48:28Z",
        "body": "That's unfortunate!\r\n\r\nThere are frequent changes to JAX, any one of which might have caused your use case to regress. Without a reproduction we can run or bisecting the problem to a particular git revision it's going to be very hard to say what happened. Can you provide a self-contained, ideally small reproduction?\r\n\r\nThanks!"
      },
      {
        "user": "cpgoodri",
        "created_at": "2019-08-07T00:51:58Z",
        "body": "I figured that was the case. I'll work on a *small* reproduction if the tests I'm working on don't lead anywhere."
      },
      {
        "user": "mattjj",
        "created_at": "2019-08-08T16:33:49Z",
        "body": "I think we spotted the issue in #1131 and fixed it in #1143. If you're able to pull the master branch, can you check? I'll also update pypi soon so you can check with that."
      },
      {
        "user": "mattjj",
        "created_at": "2019-08-08T16:35:10Z",
        "body": "Updated `jax` on pypi to version 0.1.41!"
      },
      {
        "user": "cpgoodri",
        "created_at": "2019-08-08T17:47:59Z",
        "body": "Yes, I've been following #1131 religiously, thank you all for following up so fast! And yes, it completely solved the issue, my compile time for a particular calculation just went from 12 minutes to 20 seconds. \r\n\r\nThanks again!"
      },
      {
        "user": "hawkinsp",
        "created_at": "2019-08-09T15:04:10Z",
        "body": "Great! Sounds like everything is fixed!"
      }
    ]
  },
  {
    "number": 1076,
    "title": "noob vmap question with if statement",
    "created_at": "2019-07-30T03:17:41Z",
    "closed_at": "2019-10-28T20:15:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/1076",
    "body": "Hi\r\n\r\nI'd like to vectorize the following '1d truncated Gaussian kernel' function\r\n\r\n```\r\ndef kernel_scalar(x, lam=0.18, beta=1.0):\r\n    if np.abs(x) > lam:\r\n        return 0.0\r\n    else:\r\n        return np.exp(-beta*x**2)\r\n```\r\n\r\nI tried this\r\n```\r\ndef kernel(xs):\r\n    kernels = vmap(kernel_scalar)(xs)\r\n    return kernels\r\n```\r\n\r\nbut  when I try this\r\n```\r\nn = 10\r\nxs = onp.random.randn(n)\r\nk = kernel(xs)\r\n```\r\nI get this error\r\n```\r\n...\r\n  File \"/Users/kpmurphy/github/pyprobml/scripts/mean_shift_opt.py\", line 8, in kernel_scalar\r\n    if np.abs(x) > lam:\r\n\r\n  File \"/anaconda3/lib/python3.6/site-packages/jax/core.py\", line 342, in __bool__\r\n    def __bool__(self): return self.aval._bool(self)\r\n\r\n  File \"/anaconda3/lib/python3.6/site-packages/jax/abstract_arrays.py\", line 38, in error\r\n    raise TypeError(concretization_err_msg(fun))\r\n\r\nTypeError: Abstract value passed to `bool`, which requires a concrete value. The function to be transformed can't be traced at the required level of abstraction. If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions instead\r\n```\r\n\r\nI realize that dynamic if is not allowed inside jit, but I am not (explicitly) calling jit.\r\nAlso the return of both branches has the same type and shape, so why does\r\nthe value matter?\r\n\r\n(I also realize it's easy to vectorize this  function with vanilla numpy, but I was hoping vmap would let me do everything with just scalar arithmetic :)\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/1076/comments",
    "author": "murphyk",
    "comments": [
      {
        "user": "jekbradbury",
        "created_at": "2019-07-30T23:08:40Z",
        "body": "`vmap` is essentially adding a batch dimension, and each example in the \"batch\" currently needs to execute the same instructions in the same order, putting the same restrictions on user code as `jit`. You can get around this by using `np.where` instead of `if` (or perhaps `lax.cond`, but that's unnecessary complexity for this example)."
      },
      {
        "user": "mattjj",
        "created_at": "2019-10-28T20:15:38Z",
        "body": "As @jekbradbury said, just like `jit` must trace your code on an abstract value representing the set of all possible arrays `x` with a given shape/dtype (rather than on one concrete value for `x`), `vmap` must similarly trace your code on an abstract value that could represent any value in your batch. (For simplicity, we don't trace on an abstract value representing just the finite set of possible values in the batch, though in principle we could; we instead trace on an abstract value representing the set of all possible arrays with given shape/dtype, i.e. we trace at the same level for `vmap` and for `jit`.) That puts constraints on the Python code we can successfully trace with `vmap`, and in particular we can't trace value-dependent Python control flow and instead raise the error you saw.\r\n\r\nHere are two ways to write this in a way that works with vectorization:\r\n\r\n```python\r\ndef kernel_scalar(x, lam=0.18, beta=1.0):\r\n  return np.where(np.abs(x) > lam, 0., np.exp(-beta * x ** 2)\r\n\r\ndef kernel_scalar(x, lam=0.18, beta=1.0):\r\n  return lax.cond(np.abs(x) > lam, x, lambda _: np.zeros_like(x), x, lambda x: np.exp(-beta * x**2))\r\n```\r\n\r\nI think the former is more ergonomic, and when you `vmap` the latter function it basically turns into the former function anyway. But it does mean you're vectorizing it just like in vanilla NumPy. The latter _does_ let you assume the scalar case, which is cool, but the fact that you have to write `lax.cond` instead of a native Python `if` takes the fun out of it.\r\n\r\nPlease reopen if we failed to explain this constraint! "
      }
    ]
  },
  {
    "number": 931,
    "title": "Question about stax batch dimension",
    "created_at": "2019-06-26T19:35:50Z",
    "closed_at": "2021-05-11T01:18:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/931",
    "body": "Should it be possible to define a network in stax without setting a non-batch dimension?\r\n\r\n```\r\nimport jax.numpy as np\r\nfrom jax import random\r\nfrom jax.experimental import stax\r\nfrom jax.experimental.stax import Dense, Relu, Flatten, LogSoftmax\r\n\r\n# Use stax to set up network initialization and evaluation functions\r\nnet_init, net_apply = stax.serial(Dense(128), Relu, Dense(10), LogSoftmax)\r\n\r\nrng = random.PRNGKey(0)\r\n# Initialize parameters, not committing to a batch shape\r\n\r\n# works\r\nin_shape = (-1, 1)\r\nout_shape, net_params = net_init(rng, in_shape)\r\n\r\n# doesn't work (fails with ValueError: all elements of broadcast shape must be non-negative)\r\nin_shape = (-1,)\r\nout_shape, net_params = net_init(rng, in_shape)\r\n```\r\n\r\nAs a workaround, I can of course add a fake 1 dimension on the end, and then remove it later, but I was a little surprised that this doesn't automatically work.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/931/comments",
    "author": "christopher-hesse",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2019-06-26T21:10:58Z",
        "body": "We'd like something like that to be possible, but it isn't right now. (Actually it's because stax predates the existence of vmap, and we haven't really updated it in a while.) I think this might be a duplicate of #381 though.\r\n\r\nYou'd still need to provide a nonemtpy input shape tuple with positive elements to `init_fun` to specify the example size (i.e. `in_shape` won't be just `(-1,)` because that doesn't provide any shape information, but maybe it'd be e.g. `(28 * 28,)`), but we'd like it not to be necessary to specify a batch size."
      },
      {
        "user": "mattjj",
        "created_at": "2019-06-26T21:11:13Z",
        "body": "Can you check #381 and see if you think this is a duplicate?"
      },
      {
        "user": "christopher-hesse",
        "created_at": "2019-06-27T00:50:20Z",
        "body": "I saw that issue when I filed this one, this could be a duplicate, but I also think I worded this confusingly.\r\n\r\nIf my inputs is a list of scalars, I could consider that of shape `(batchsize,)` or `(batchsize, 1)`.  The second one seems to work with stax but not the first.  I could of course, wrap the stax model with `jp.expand_dims(input, axis=-1)` but wanted to make sure I wasn't missing something."
      },
      {
        "user": "hawkinsp",
        "created_at": "2021-05-11T01:18:18Z",
        "body": "We've decided to keep stax as an example rather than evolving it any further. These days, we would point users to one of the great neural network libraries built on top of JAX, such as Haiku or Flax."
      }
    ]
  },
  {
    "number": 876,
    "title": "Jax issue with numpy",
    "created_at": "2019-06-19T03:15:03Z",
    "closed_at": "2019-06-19T20:55:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/876",
    "body": "When I import other packages when contains `import numpy`, it contradicts with the jax numpy. How do people solve this when they want to use jax but also need to import other packages?",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/876/comments",
    "author": "JiahaoYao",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2019-06-19T03:22:16Z",
        "body": "We use NumPy a lot in our implementation; we follow the convention of `import numpy as onp` and `import jax.numpy as np`, but you could imagine other conventions, like `import jax.numpy as jnp` if the issue is name conflicts.\r\n\r\nIf the issue is instead wanting to use an existing NumPy library with jax.numpy, I don't think we have a great solution. Maybe you could monkey-patch the module in-memory, as in `some_module.np = jax.numpy`.\r\n\r\n@shoyer and #611 may have a better long-term solution, where regular NumPy can learn how to work with JAX.\r\n\r\nWhat do you think? "
      },
      {
        "user": "JiahaoYao",
        "created_at": "2019-06-19T20:55:33Z",
        "body": "That is good, thanks @mattjj !"
      }
    ]
  },
  {
    "number": 725,
    "title": "Cyclic references could lead to memory leaks",
    "created_at": "2019-05-17T03:29:36Z",
    "closed_at": "2019-05-30T14:52:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/725",
    "body": "Cyclic references (for example, in graph data structures) don't get garbage collected by python immediately. This can cause tensors to not be deallocated when all references are removed, causing memory leaks on device and possible OOM crashes.",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/725/comments",
    "author": "chaserileyroberts",
    "comments": [
      {
        "user": "jekbradbury",
        "created_at": "2019-05-17T06:15:28Z",
        "body": "Can you give a MWE of what you mean by in-graph data structures? You might be able to fix this with Python weakrefs if the cycles are created in data structures your code manages."
      },
      {
        "user": "chaserileyroberts",
        "created_at": "2019-05-17T07:04:25Z",
        "body": "Yeah that's what we ended up doing. But weakref is a fairly low level API to be using."
      },
      {
        "user": "hawkinsp",
        "created_at": "2019-05-17T13:21:04Z",
        "body": "Can you clarify what you mean? Is your concern that (a) references don't get collected by Python *immediately* or (b) that references don't get collected at all?\r\n\r\nI expect you mean the former, in which case there isn't a whole lot we can do about it at our level — it's something you'll have to fix in your code. The only thing I can possibly suggest is that we could add a Delete() method to our DeviceArrays to give you more explicit control over destruction.\r\n\r\nI'd be surprised if the latter is happening, but if you have a repro I'd be happy to look into it."
      },
      {
        "user": "mattjj",
        "created_at": "2019-05-17T15:14:07Z",
        "body": "Put another way: is this an issue with JAX specifically, or would e.g. regular NumPy have this problem?"
      },
      {
        "user": "chaserileyroberts",
        "created_at": "2019-05-17T15:41:38Z",
        "body": "Yes adding a `Delete` method would be great. \r\n\r\nThe issue is amplified with JAX when compared to numpy since python calls garbage collection when it starts to run out of runtime memory, but not when JAX runs out of device memory.\r\n\r\nNote this cyclic issue is only true outside of a `jit`."
      }
    ]
  },
  {
    "number": 564,
    "title": "Equivalent to autograd's elementwise_grad?",
    "created_at": "2019-04-03T08:01:42Z",
    "closed_at": "2019-04-03T20:18:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/564",
    "body": "Hi there,\r\n\r\nIn autograd, I use the function \"elementwise_grad\" a fair bit. Is there an equivalent in jax? In particular, I would like to compute the elements of a diagonal Hessian, which I do in autograd by calling elementwise_grad twice:\r\n\r\n    from autograd import elementwise_grad as egrad\r\n    h = egrad(egrad(fun))(x)\r\n\r\nInitially I thought\r\n\r\n    vmap(grad(grad(fun)))(x)\r\n\r\nwould do the trick, but although it worked on a toy example, it gives a different result in general.\r\n\r\nHope that's enough information. Happy to put together a proper example if not, please let me know!",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/564/comments",
    "author": "martiningram",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2019-04-03T14:50:26Z",
        "body": "Ah, unfortunately calling `elementwise_grad` twice won't give you the diagonal of the Hessian:\r\n\r\n```python\r\nfrom autograd import grad, elementwise_grad, hessian\r\nimport autograd.numpy as np\r\nimport numpy.random as npr\r\n\r\nrng = npr.RandomState(0)\r\nA = rng.randn(4, 4)\r\nx = rng.randn(4)\r\n\r\n\r\ndef f(x):\r\n  return np.sum(np.tanh(np.dot(A, x)))\r\n\r\nprint np.diag(hessian(f)(x))\r\n# array([-2.93841869, -0.97483706, -0.07164367, -0.20771311])\r\n\r\nprint elementwise_grad(elementwise_grad(f))(x)\r\n# array([-1.26875883,  0.40277148, -0.31810185,  0.05497358])\r\n```\r\n\r\nI think @dougalm and I saw some issues on the Autograd issue tracker about this, but didn't have time to respond, and maybe those threads came to the incorrect conclusion that `elementwise_grad` would work here. It only works when the underlying function has a diagonal Jacobian, i.e. basically only for elementwise functions. It can't give you the diagonal of a general Hessian efficiently. (What it does is compute the VJP with an all-ones vector; when the Jacobian is diagonal, that reveals all the nonzero coefficients of the Jacobian, and similarly when the Hessian is diagonal then calling this twice would reveal all the nonzero coefficients of the Hessian. But if the Jacobian isn't diagonal then `elementwise_grad` is just giving you the sum of its rows. This confusion is a reason not to include it in JAX, and to prefer `vmap(grad(f))` for elementwise differentiation, since the `vmap` semantics are clearer.)\r\n\r\nIn general, computing the diagonal of the Hessian is as hard as computing the full Hessian itself. That is, you'd basically have to call `jax.hessian` and take its diagonal.\r\n\r\nDoes that make sense?"
      },
      {
        "user": "martiningram",
        "created_at": "2019-04-03T20:18:00Z",
        "body": "It does! Thank you so much for the detailed response!"
      }
    ]
  },
  {
    "number": 557,
    "title": "Better tracing of intermediate grads (tangents)",
    "created_at": "2019-04-01T22:56:36Z",
    "closed_at": "2019-04-07T21:39:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/557",
    "body": "Is it possible to have better support for inspecting intermediate tangents of composed functions? Eg. if we have a function f(g(x)) - the chain rule for df/dx necessitates df/dg.dg/dx - it'd be super useful to able to map back to the tangents of python variables as opposed to having to try to dig through jaxpr to see what the values of the left hand side and right hand side of the chain rule is.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/557/comments",
    "author": "proteneer",
    "comments": [
      {
        "user": "proteneer",
        "created_at": "2019-04-01T22:57:02Z",
        "body": "@sschoenholz Filed issue per discussion."
      },
      {
        "user": "mattjj",
        "created_at": "2019-04-01T23:03:49Z",
        "body": "Thanks for bringing this up. Can you elaborate on your example? I'd like to better understand what you mean.\r\n\r\nOne thing you can do in Python is this:\r\n\r\n```python\r\nfrom jax import vjp\r\n\r\nintermediate, g_vjp = vjp(g, x)\r\ny, f_vjp = vjp(f, intermediate)  # y = f(g(x))\r\n\r\nintermediate_cotangent = f_vjp(1.)\r\ngradval = g_vjp(intermediate_cotangent)  # gradval = grad(lambda x: f(g(x))(x)\r\n```\r\n\r\nThat is, if you've manually split your code into functions to be composed, you can use `vjp` (and `jvp`) to get intermediate derivative information. (You could even imagine a helper function to do this.)\r\n\r\nBut maybe you're asking something else. Maybe you want to be able to associate lines and/or variables in a jaxpr with original source lines of your program.\r\n\r\nAny of this on the right track?"
      },
      {
        "user": "mattjj",
        "created_at": "2019-04-02T02:51:52Z",
        "body": "cross-ref #522 "
      },
      {
        "user": "proteneer",
        "created_at": "2019-04-02T03:12:45Z",
        "body": "Your code sample is basically the gist of the problem. I have very long composed function and it's not always trivial to split and recompose (especially when you're inside loops).\r\n\r\n``` python\r\nimport numpy as onp\r\nimport jax\r\nimport jax.numpy as np\r\n\r\ndef fog(x):\r\n    # h(x) = (fog)(x) = (2x)^2\r\n    g = 2*x # accumulated tangent: dg/dx = 2 \r\n    fg = g**2 # accumulated tangent: df/dg*dg/dx = 2*2x*2 = 8x\r\n    # in principle with fwd mode autodiff I should be able to inspect g.tangent fg.tangent\r\n    # to actually inspect the both parts of the dual.\r\n\r\n    # is there a way to directly inspect their values?\r\n    print(\"??\", g.tangent)\r\n    print(\"??\", fg.tangent)\r\n\r\n    return fg\r\n\r\ndfog_dx = jax.jacfwd(fog, argnums=(0,))\r\nprint(dfog_dx(np.array([3.0])))\r\n```\r\n\r\nThe jaxpr is\r\n\r\n```\r\n-- { lambda b d f g h ;  ; a.\r\n  let c = mul a b\r\n      e = pow c d\r\n      i = pow c h\r\n      j = mul g i\r\n      k = safe_mul f j\r\n      l = pack e k\r\n      (m n) = id l\r\n      o = pack m n\r\n      (p q) = id o\r\n      r = reshape[ new_sizes=()\r\n                   dimensions=None\r\n                   old_sizes=(1,) ] q\r\n      s = pack r\r\n  in s }\r\n```\r\nI have no ability to actually read jaxpr but I suspect that the two muls (c and k) correspond to the two derivatives via fwdmode AD.\r\n\r\nI was hoping there'd be a way to directly inspect the values inside the tangents as the code is running. "
      },
      {
        "user": "mattjj",
        "created_at": "2019-04-02T03:31:45Z",
        "body": "Ah, thanks for explaining!\r\n\r\nIt may be that this example isn't representative of everything you're interested in, in which case the advice I'm about to provide won't always apply, but as long as you're not using `jit` or `vmap` then printing values is actually pretty easy:\r\n\r\n```python\r\nfrom jax import custom_transforms\r\nfrom jax.interpreters.ad import defjvp\r\n\r\ndef print_tangent_jvp(t, x):\r\n  print(t)\r\n  return t\r\nprint_tangent = custom_transforms(lambda x: x)\r\ndefjvp(print_tangent.primitive, print_tangent_jvp)\r\n\r\ndef fog(x):\r\n  g = 2*x\r\n  fg = g**2\r\n\r\n  print_tangent(g)\r\n  print_tangent(fg)\r\n\r\n  return fg\r\n\r\nout, out_tangent = jax.jvp(fog, (3.,), (1.,))\r\n```\r\n\r\nNotice I didn't use `jacfwd` like you did in your example. That's because it uses `vmap` internally, meaning the tangents get abstracted to the Shaped level.\r\n\r\nCould this kind of thing be useful? Or is it missing some important piece?"
      },
      {
        "user": "proteneer",
        "created_at": "2019-04-02T07:48:42Z",
        "body": "Thanks this temporarily suffices. Though I'm using both jit/vmap it seems in my production code, I can live with turning them off in debug mode. "
      },
      {
        "user": "proteneer",
        "created_at": "2019-04-07T21:39:14Z",
        "body": "Thanks guys - this works for me for now. Hoping there'd be a way to do this in the future without the existing limitations (lack of JIT and proper vmapp'd jacfwd) but for now I can't compain."
      },
      {
        "user": "mattjj",
        "created_at": "2019-04-08T16:30:44Z",
        "body": "Glad that the temporary fix works! For a longer-term solution, I think we should view this as #364."
      }
    ]
  },
  {
    "number": 396,
    "title": "clarification to README",
    "created_at": "2019-02-17T13:33:12Z",
    "closed_at": "2019-03-02T20:27:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/396",
    "body": "First, thanks a lot for this amazing package. \r\n\r\nI noted that in the README.md there is an example that says: \r\n\r\n```\r\nIn [5]: print(np.dot(x, x.T) / 2)  # fast!\r\n[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...\r\n\r\nIn [6]: print(np.dot(x, x.T) / 2)  # even faster!\r\n[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...\r\n```\r\nGiven that both examples are exactly the same, maybe there is a mistake here in saying the other is \"even faster\". \r\n\r\nThanks again for JAX, I feel that this will be a gift that keeps giving. Bonus points for a funky and very well crafted logo!",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/396/comments",
    "author": "mikkokotila",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2019-02-17T13:47:38Z",
        "body": "Thanks for the kind words!\r\n\r\nActually the idea behind that readme line is that on the second call some JIT-compiled operations are cached, so the second call to them saves on compile time (and things like autotuning).\r\n\r\nDoes that make sense? Maybe we could add something to the readme to clarify what that line means. "
      }
    ]
  },
  {
    "number": 395,
    "title": "Clarify: Larger-than-memory arrays",
    "created_at": "2019-02-16T16:28:41Z",
    "closed_at": "2019-02-17T00:25:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/395",
    "body": "Hi, \r\n\r\nOne thing I'm interested in is larger-than-memory DRL, to store weight matrices and things on disk for larger scale projects in genomics and structural bioinformatics. Can JAX XLA compile memmap arrays like dask.array.dot to compute dot products of absurdly huge arrays? Numba does something similar but it would be fun to do this sort of thing with Jax. Apologies if already supported.\r\n\r\nExample, in meta-learning, we might want a function which produces a new weight matrix, and with a naive example of 1-layer dense feedback/feedforward meta-control this can be on the order of \r\n((d_state, d_action, d_reward), ((d_state, d_action, d_reward), d_action)),   which can be doable on smaller RL projects but not necessarily the case for larger MIMO RL envs with images included in state and action vectors. GPU memory is expensive so it's nice to be able to memmap these things. \r\n\r\n(Yes I know about sampling, convolution, and Huffman coding -- just wanted to play with huge dense meta stuffs on XLA)\r\n\r\nThanks,\r\n\r\nBion @ bitpharma.com",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/395/comments",
    "author": "bionicles",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2019-02-17T00:25:30Z",
        "body": "Cool ideas!\r\n\r\nWe don't have anything like that in JAX or XLA, but would love to see it built. JAX solves a specific set of problems (e.g. JIT compilation of Python+NumPy code with end-to-end array-level optimization, accelerator execution, autodiff, autobatching), and hopefully it can be used as a tool by other libraries that might want to pair the things that JAX does with other capabilities (like supporting absurdly large arrays). But JAX isn't a monolith that solves everything, and disk-backed arrays are out of scope.\r\n\r\nNumPy itself is in an analogous position: it has a specific scope, yet pretty much every numerical computing project (Dask, TensorFlow, etc) can use it to build things outside of that scope.\r\n\r\nIf you have ideas for how to interface JAX with Dask, or how to build a library that supports super large arrays while using JAX for autodiff and/or JIT compilation to accelerators, we'll do what we can on the JAX side to support that use case!\r\n\r\n(Please re-open the issue if I failed to answer your question :) )"
      },
      {
        "user": "bionicles",
        "created_at": "2019-02-17T22:46:30Z",
        "body": "Thank you"
      }
    ]
  },
  {
    "number": 182,
    "title": "add Softmax layer to stax",
    "created_at": "2019-01-01T10:07:28Z",
    "closed_at": "2019-01-05T18:35:42Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/182",
    "body": "test in examples/mnist_classifier.py,\r\n```python\r\ndef accuracy(params, batch):    \r\n  inputs, targets = batch    \r\n  target_class = np.argmax(targets, axis=1)\r\n  x = predict(params, inputs)\r\n  print(x[0])\r\n```\r\nthe final layer is LogSoftmax, but the output seems not correct, \r\n```python  \r\nStarting training...\r\n[-2.3113673 -2.6440005 -2.4797316 -1.79847   -1.6207608 -2.931935\r\n -3.303906  -2.7275395 -2.2099946 -2.1403143]\r\n```",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/182/comments",
    "author": "cookfish",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2019-01-03T17:46:24Z",
        "body": "Can you say a bit more about what might be incorrect here? The sum of the elementwise-exp of those numbers is 1, which is intended."
      },
      {
        "user": "cookfish",
        "created_at": "2019-01-03T18:19:16Z",
        "body": "I mean the sum of those numbers should be 1, not their elementwise-exp's."
      },
      {
        "user": "mattjj",
        "created_at": "2019-01-03T18:25:17Z",
        "body": "That would be true of a softmax layer, but `LogSoftmax` intended to be a log-softmax layer.\r\n\r\nWould it be useful to you for us to add a softmax layer (including the exp)?"
      },
      {
        "user": "cookfish",
        "created_at": "2019-01-03T18:51:40Z",
        "body": "sorry, i misunderstood log-softmax, maybe it will be more convenient if providing loss functions."
      },
      {
        "user": "mattjj",
        "created_at": "2019-01-03T21:31:49Z",
        "body": "Got it! I'll add that to my todo list."
      }
    ]
  },
  {
    "number": 170,
    "title": "Random key error in stax.Dropout layer",
    "created_at": "2018-12-23T20:14:30Z",
    "closed_at": "2018-12-24T18:33:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/jax-ml/jax/issues/170",
    "body": "Dropout layer not working due to it's apply_fun `keep = random.bernoulli(rng, rate, inputs.shape)` .\r\nWhen I add `rng = PRNGKey(seed)` before this line, the apply_fun works well",
    "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/170/comments",
    "author": "cookfish",
    "comments": [
      {
        "user": "mattjj",
        "created_at": "2018-12-24T18:30:18Z",
        "body": "Hey, thanks for raising this. However, that change isn't what you want: it will make the dropout layer always to use the same fixed pattern of dropped out units (rather than sampling random ones).\r\n\r\nThe actual issue is that you need to pass a PRNG key into your top-level apply_fun. Here's an example of using the Dropout layer constructor in stax:\r\n\r\n```python\r\nfrom __future__ import print_function\r\nimport jax.numpy as np\r\nfrom jax import jit, grad\r\nfrom jax import random\r\nfrom jax.experimental import minmax\r\nfrom jax.experimental import stax\r\nfrom jax.experimental.stax import Dense, Relu, Dropout, LogSoftmax\r\n\r\ninit_fun, apply_fun = stax.serial(\r\n    Dense(512), Relu, Dropout(0.4, mode='train'),\r\n    Dense(512), Relu, Dropout(0.4, mode='train'),\r\n    Dense(3), LogSoftmax\r\n)\r\n\r\n## Initialize parameters, not committing to a batch shape\r\nin_shape = (-1, 28 * 28)\r\nout_shape, net_params = init_fun(in_shape)\r\n\r\n## Apply network to dummy inputs.\r\n\r\n# Every time we want a new random dropout pattern, we split the prng key and\r\n# pass a fresh subkey into the call\r\nkey = random.PRNGKey(0)\r\ninputs = np.ones((10, 28 * 28))\r\n\r\nkey, subkey = random.split(key)\r\nprint(apply_fun(net_params, inputs, subkey))\r\n\r\nkey, subkey = random.split(key)\r\nprint(apply_fun(net_params, inputs, subkey))\r\n\r\n\r\n# If we don't pass a prng key, we should get a clear error (this is new)\r\n# print(apply_fun(net_params, inputs))  # NOTE: now an error!\r\n# ValueError: Dropout layer requires apply_fun to be called with an rng argument.\r\n\r\n\r\n## Run a training loop on dummy data\r\n\r\n# When writing a training loop, we need to be sure to split the PRNG for each\r\n# call that we want to have a different dropout pattern (i.e. each train step).\r\n\r\nopt_init, opt_update = minmax.momentum(step_size=1e-3, mass=0.9)\r\n\r\ndef loss(params, batch, key):\r\n  inputs, targets = batch\r\n  predictions = apply_fun(params, inputs, key)\r\n  return np.sum((predictions - targets) ** 2)\r\n\r\n@jit\r\ndef step(i, opt_state, batch, key):\r\n  params = minmax.get_params(opt_state)\r\n  g = grad(loss)(params, batch, key)\r\n  return opt_update(i, g, opt_state)\r\n\r\n# Dummy input data stream\r\ndata_generator = ((np.zeros((10, 28 * 28)), np.zeros((10, 3)))\r\n                  for _ in range(10))\r\n\r\n# Optimize parameters in a loop\r\nopt_state = opt_init(net_params)\r\nfor i in range(10):\r\n  key, subkey = random.split(key)\r\n  opt_state = step(i, opt_state, next(data_generator), subkey)\r\nnet_params = minmax.get_params(opt_state)\r\n```\r\n\r\nLook for the line with the comment \"NOTE: now an error!\".\r\n\r\nIt's too easy to forget to pass in a PRNG key, and then the error that happens isn't very informative. I'll improve the error message, but the real solution will be for us to include some better examples and make the PRNG system less surprising."
      },
      {
        "user": "cookfish",
        "created_at": "2018-12-24T20:37:53Z",
        "body": "got it, seems the PRNG system need more work on it"
      }
    ]
  }
]