[
  {
    "number": 1782,
    "title": "java  DynamicCustomDictionary load 词典时候不生效",
    "created_at": "2022-09-06T07:30:17Z",
    "closed_at": "2022-09-06T10:59:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/1782",
    "body": "\r\n**Describe the bug**\r\nDynamicCustomDictionary\r\n\r\n**Code to reproduce the issue**\r\n  dictionary.load(\"/home/duanfa/trash/845.txt\");\r\n  DynamicCustomDictionary load 词典时候不生效 \r\n845.txt 文件内容\r\n\r\n爱慕官方花园速写内衣女无钢圈中厚杯小胸聚拢蕾丝文胸AM171791 nz 1\r\n爱慕官方花 nz 1\r\n\r\n\r\n 用  insert 生效\r\n  dictionary.insert(\"爱慕官方花园速写内衣女无钢圈中厚杯小胸聚拢蕾丝文胸AM171791\", \"N 1\");\r\n\r\n或者在hanlp.properties里面配置\r\nCustomDictionaryPath=data/dictionary/custom/duanfa/845.txt \r\n生效后，把 data/dictionary/custom/duanfa/845.txt.bin  拷贝到 /home/duanfa/trash/  目录下，dictionary.load(\"/home/duanfa/trash/845.txt\");就可以生效了\r\n\r\n版本\r\n <dependency>\r\n            <groupId>com.hankcs</groupId>\r\n            <artifactId>hanlp</artifactId>\r\n            <version>portable-1.8.3</version>\r\n        </dependency>\r\n\r\n```import com.hankcs.hanlp.HanLP;\r\nimport com.hankcs.hanlp.dictionary.DynamicCustomDictionary;\r\nimport com.hankcs.hanlp.seg.Segment;\r\nimport com.hankcs.hanlp.seg.common.Term;\r\n\r\npublic class LoadCustomFile {\r\n\tpublic DynamicCustomDictionary dictionary = new DynamicCustomDictionary();\r\n\tpublic Segment hanlpSegmenter;\r\n\r\n\tpublic LoadCustomFile() {\r\n\t\ttry {\r\n\t\t\thanlpSegmenter = HanLP.newSegment();\r\n\t\t\thanlpSegmenter.enableCustomDictionary(dictionary).enableCustomDictionaryForcing(true);\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\t}\r\n\r\n\tpublic static void main(String[] args) {\r\n\t\tString text = \"爱慕官方花园速写内衣女无钢圈中厚杯小胸聚拢蕾丝文胸AM171791\";\r\n\t\tLoadCustomFile lcf = new LoadCustomFile();\r\n\t\tlcf.dictionary.load(\"/home/duanfa/trash/845.txt\");\r\n\t\tfor (Term term : lcf.hanlpSegmenter.seg(text)) {\r\n\t\t\tSystem.out.println(term);\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n代码打印结果：\r\n爱慕/vn\r\n官方/n\r\n花园/n\r\n速写/n\r\n内衣/n\r\n女/b\r\n无/v\r\n钢圈/n\r\n中/f\r\n厚/a\r\n杯/q\r\n小/a\r\n胸/ng\r\n聚拢/v\r\n蕾/ng\r\n丝/q\r\n文/ng\r\n胸/ng\r\nAM/nx\r\n171791/m\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nA clear and concise description of what happened.\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Python version:\r\n- HanLP version:\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n* [x] I've completed this form and searched the web for solutions.\r\n<!-- 发表前先搜索，此处一定要勾选！ -->\r\n<!-- 发表前先搜索，此处一定要勾选！ -->\r\n<!-- 发表前先搜索，此处一定要勾选！ -->",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/1782/comments",
    "author": "duanfa",
    "comments": [
      {
        "user": "duanfa",
        "created_at": "2022-09-06T11:00:45Z",
        "body": "先把动态insert的新词保存成bin文件，然后下次load就可以了"
      },
      {
        "user": "Huyueeer",
        "created_at": "2022-09-07T08:39:08Z",
        "body": "@duanfa 请问一下您，保存成bin是指比方说`xxx.txt`变成`xxx.txt.bin`吗？\r\n"
      },
      {
        "user": "duanfa",
        "created_at": "2022-12-01T13:40:38Z",
        "body": "> @duanfa 请问一下您，保存成bin是指比方说`xxx.txt`变成`xxx.txt.bin`吗？\r\n\r\nsave的时候就会自己保存成.bin后缀的文件"
      }
    ]
  },
  {
    "number": 1734,
    "title": "pypi上未更新 CTB9_TOK_ELECTRA_BASE_CRF、CTB9_TOK_ELECTRA_BASE、MSR_TOK_ELECTRA_BASE_CRF",
    "created_at": "2022-05-18T10:37:49Z",
    "closed_at": "2022-05-19T02:35:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/1734",
    "body": "<!--\r\n提问请上论坛，不要发这里！\r\n提问请上论坛，不要发这里！\r\n提问请上论坛，不要发这里！\r\n\r\n以下必填，否则恕不受理。\r\n-->\r\n\r\n**Describe the bug**\r\n加载 CTB9_TOK_ELECTRA_BASE_CRF、CTB9_TOK_ELECTRA_BASE、MSR_TOK_ELECTRA_BASE_CRF\r\n模型报错\r\n\r\n**Code to reproduce the issue**\r\n执行代码\r\n\r\n```python\r\ntok = hanlp.load(hanlp.pretrained.tok.CTB9_TOK_ELECTRA_BASE_CRF)\r\n```\r\n\r\n**Describe the current behavior**\r\n报错信息:\r\n`AttributeError: module 'hanlp.pretrained.tok' has no attribute 'CTB9_TOK_ELECTRA_BASE_CRF'`\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Python version: python3.9\r\n- HanLP version: 2.1.0b27\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n* [x] I've completed this form and searched the web for solutions.\r\n<!-- 发表前先搜索，此处一定要勾选！ -->\r\n<!-- 发表前先搜索，此处一定要勾选！ -->\r\n<!-- 发表前先搜索，此处一定要勾选！ -->",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/1734/comments",
    "author": "SoaringTiger",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2022-05-18T23:46:06Z",
        "body": "Hi，该模型在dev上，还未merge到master，请耐心等待，或者直接load URL。"
      }
    ]
  },
  {
    "number": 1604,
    "title": "自定义词典对KBeamArcEagerDependencyParser无效",
    "created_at": "2021-01-20T08:48:19Z",
    "closed_at": "2021-01-20T15:42:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/1604",
    "body": "<!--\r\n提问请上论坛，不要发这里！\r\n提问请上论坛，不要发这里！\r\n提问请上论坛，不要发这里！\r\n\r\n以下必填，否则恕不受理。\r\n-->\r\n\r\n**Describe the bug**\r\n添加新词进入自定义词典后，KBeamArcEagerDependencyParser仍然无法对该词进行正确分词。\r\n\r\n**Code to reproduce the issue**\r\n添加\"申请笔 n 100\"进入自定义词典\r\n```scala\r\nval parser = new KBeamArcEagerDependencyParser()\r\nparser.parse(\"又有新的申请笔可以拿了\")\r\n```\r\n```\r\n1      又      又      A       AD      _       2       advmod  _       _\r\n2       有      有      V       VE      _       0       ROOT    _       _\r\n3       新      新      V       VA      _       6       rcmod   _       _\r\n4       的      的      D       DEC     _       3       cpm     _       _\r\n5       申请    申请    N       NN      _       6       nn      _       _\r\n6       笔      笔      N       NN      _       8       nsubj   _       _\r\n7       可以    可以    V       VV      _       8       mmod    _       _\r\n8       拿      拿      V       VV      _       2       dep     _       _\r\n9       了      了      S       SP      _       2       dep     _       _\r\n```\r\n\r\n**Describe the current behavior**\r\n即使添加了新词，仍然无法对新词进行正确分词，而另一个接口HanLP.parseDependency可以根据新加入的词进行正确分词\r\n\r\n**Expected behavior**\r\n期待能通过加入自定义词典，KBeamArcEagerDependencyParser能够先正确分词，并在此基础上返回正确结果\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux\r\n- Scala version:2.12.2\r\n- HanLP version:1.7.8\r\n\r\n* [x] I've completed this form and searched the web for solutions.\r\n<!-- 发表前先搜索，此处一定要勾选！ -->\r\n<!-- 发表前先搜索，此处一定要勾选！ -->\r\n<!-- 发表前先搜索，此处一定要勾选！ -->",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/1604/comments",
    "author": "jimmy-walker",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2021-01-20T15:42:36Z",
        "body": "Hi，KBeamArcEagerDependencyParser的自定义词典是默认关闭的，为了在CTB上面跑分。你可以将其开启：\r\n\r\n```java\r\n1\t又\t又\tA\tAD\t_\t2\tadvmod\t_\t_\r\n2\t有\t有\tV\tVE\t_\t0\tROOT\t_\t_\r\n3\t新\t新\tV\tVA\t_\t5\trcmod\t_\t_\r\n4\t的\t的\tD\tDEC\t_\t3\tcpm\t_\t_\r\n5\t申请笔\t申请笔\tn\tnz\t_\t7\tnsubj\t_\t_\r\n6\t可以\t可以\tV\tVV\t_\t7\tmmod\t_\t_\r\n7\t拿\t拿\tV\tVV\t_\t2\tdep\t_\t_\r\n8\t了\t了\tA\tAS\t_\t7\tasp\t_\t_\r\n9\t。\t。\tP\tPU\t_\t2\tpunct\t_\t_\r\n```"
      },
      {
        "user": "jimmy-walker",
        "created_at": "2021-01-21T02:49:53Z",
        "body": "> Hi，KBeamArcEagerDependencyParser的自定义词典是默认关闭的，为了在CTB上面跑分。你可以将其开启：\r\n> \r\n> ```java\r\n> 1\t又\t又\tA\tAD\t_\t2\tadvmod\t_\t_\r\n> 2\t有\t有\tV\tVE\t_\t0\tROOT\t_\t_\r\n> 3\t新\t新\tV\tVA\t_\t5\trcmod\t_\t_\r\n> 4\t的\t的\tD\tDEC\t_\t3\tcpm\t_\t_\r\n> 5\t申请笔\t申请笔\tn\tnz\t_\t7\tnsubj\t_\t_\r\n> 6\t可以\t可以\tV\tVV\t_\t7\tmmod\t_\t_\r\n> 7\t拿\t拿\tV\tVV\t_\t2\tdep\t_\t_\r\n> 8\t了\t了\tA\tAS\t_\t7\tasp\t_\t_\r\n> 9\t。\t。\tP\tPU\t_\t2\tpunct\t_\t_\r\n> ```\r\n\r\n@hankcs 感谢您的回复。我按照方法开启了自定义词典，但发现有一个词总是被切错。请问这有什么办法吗？\r\n我设置自己的自定义词典，代替了默认的自定义词典：\r\n```\r\n代表作 NN 1000\r\n· PU 1000\r\n```\r\n但是在KBeamArcEagerDependencyParser下，始终被切分为\"作· \"，无论我在自定义词典中设置多大的词频，都无法解决这个问题。请问这个应该如何改正呢？\r\n\r\n```java\r\n        IDependencyParser parser = new KBeamArcEagerDependencyParser();\r\n        System.out.printf(parser.parse(\"这是我的代表作\").toString());\r\n        System.out.printf(parser.parse(\"这是我的代表作·\").toString());\r\n```\r\n\r\n```\r\n1   这   这   P   PN  _   2   top _   _\r\n2   是   是   V   VC  _   0   ROOT    _   _\r\n3   我   我   P   PN  _   5   assmod  _   _\r\n4   的   的   D   DEG _   3   assm    _   _\r\n5   代表作 代表作 N   NN  _   2   attr    _   _\r\n\r\n1   这   这   P   PN  _   2   top _   _\r\n2   是   是   V   VC  _   0   ROOT    _   _\r\n3   我   我   P   PN  _   5   assmod  _   _\r\n4   的   的   D   DEG _   3   assm    _   _\r\n5   代表  代表  N   NN  _   6   nsubj   _   _\r\n6   作·  作·  V   VV  _   2   ccomp   _   _\r\n```\r\n"
      },
      {
        "user": "hankcs",
        "created_at": "2021-01-21T02:55:55Z",
        "body": "你可以把·加到词典里然后enableCustomDictionaryForcing，也可以CharTable['·']='。'"
      },
      {
        "user": "jimmy-walker",
        "created_at": "2021-01-21T02:57:28Z",
        "body": "> 你可以把·加到词典里然后enableCustomDictionaryForcing，也可以CharTable['·']='。'\r\n\r\n谢谢你。我在看源码时看到了这个方法，感谢。"
      },
      {
        "user": "seabiscuityj",
        "created_at": "2021-02-19T14:43:24Z",
        "body": "> > Hi，KBeamArcEagerDependencyParser的自定义词典是默认关闭的，为了在CTB上面跑分。你可以将其开启：\r\n> > ```java\r\n> > 1\t又\t又\tA\tAD\t_\t2\tadvmod\t_\t_\r\n> > 2\t有\t有\tV\tVE\t_\t0\tROOT\t_\t_\r\n> > 3\t新\t新\tV\tVA\t_\t5\trcmod\t_\t_\r\n> > 4\t的\t的\tD\tDEC\t_\t3\tcpm\t_\t_\r\n> > 5\t申请笔\t申请笔\tn\tnz\t_\t7\tnsubj\t_\t_\r\n> > 6\t可以\t可以\tV\tVV\t_\t7\tmmod\t_\t_\r\n> > 7\t拿\t拿\tV\tVV\t_\t2\tdep\t_\t_\r\n> > 8\t了\t了\tA\tAS\t_\t7\tasp\t_\t_\r\n> > 9\t。\t。\tP\tPU\t_\t2\tpunct\t_\t_\r\n> > ```\r\n> \r\n> @hankcs 感谢您的回复。我按照方法开启了自定义词典，但发现有一个词总是被切错。请问这有什么办法吗？\r\n> 我设置自己的自定义词典，代替了默认的自定义词典：\r\n> \r\n> ```\r\n> 代表作 NN 1000\r\n> · PU 1000\r\n> ```\r\n> \r\n> 但是在KBeamArcEagerDependencyParser下，始终被切分为\"作· \"，无论我在自定义词典中设置多大的词频，都无法解决这个问题。请问这个应该如何改正呢？\r\n> \r\n> ```java\r\n>         IDependencyParser parser = new KBeamArcEagerDependencyParser();\r\n>         System.out.printf(parser.parse(\"这是我的代表作\").toString());\r\n>         System.out.printf(parser.parse(\"这是我的代表作·\").toString());\r\n> ```\r\n> \r\n> ```\r\n> 1   这   这   P   PN  _   2   top _   _\r\n> 2   是   是   V   VC  _   0   ROOT    _   _\r\n> 3   我   我   P   PN  _   5   assmod  _   _\r\n> 4   的   的   D   DEG _   3   assm    _   _\r\n> 5   代表作 代表作 N   NN  _   2   attr    _   _\r\n> \r\n> 1   这   这   P   PN  _   2   top _   _\r\n> 2   是   是   V   VC  _   0   ROOT    _   _\r\n> 3   我   我   P   PN  _   5   assmod  _   _\r\n> 4   的   的   D   DEG _   3   assm    _   _\r\n> 5   代表  代表  N   NN  _   6   nsubj   _   _\r\n> 6   作·  作·  V   VV  _   2   ccomp   _   _\r\n> ```\r\n\r\n请问该怎么开启KBeamArcEagerDependencyParser的自定义词典？"
      },
      {
        "user": "jimmy-walker",
        "created_at": "2021-02-20T07:58:05Z",
        "body": "@seabiscuityj  可以修改KBeamArcEagerDependencyParser.java中的源码\r\n```java\r\nenableCustomDictionaryForcing(true)\r\n```"
      },
      {
        "user": "seabiscuityj",
        "created_at": "2021-02-21T05:54:30Z",
        "body": "> @seabiscuityj 可以修改KBeamArcEagerDependencyParser.java中的源码\r\n> \r\n> ```java\r\n> enableCustomDictionaryForcing(true)\r\n> ```\r\n\r\n谢谢回复，按照建议重新训练了模型。\r\n另外，还有一个问题想请教您，我在用NeuralNetworkDependencyParser进行依存句法分析的时候，总是提示空指针异常。\r\n\r\n我的做法是：运行ch13下的DemoNeuralParser，就报错空指针异常。不知道是哪儿出问题？期待您的回答。\r\n\r\n我的版本是HanLP1.8.0，按照提示下载了数据包data，NNParserModel.txt.bin的存放路径就是源码中的NNParserModelPath指向的\"data/model/dependency/NNParserModel.txt\"。"
      },
      {
        "user": "jimmy-walker",
        "created_at": "2021-02-22T07:44:18Z",
        "body": "> > @seabiscuityj 可以修改KBeamArcEagerDependencyParser.java中的源码\r\n> > ```java\r\n> > enableCustomDictionaryForcing(true)\r\n> > ```\r\n> \r\n> 谢谢回复，按照建议重新训练了模型。\r\n> 另外，还有一个问题想请教您，我在用NeuralNetworkDependencyParser进行依存句法分析的时候，总是提示空指针异常。\r\n> \r\n> 我的做法是：运行ch13下的DemoNeuralParser，就报错空指针异常。不知道是哪儿出问题？期待您的回答。\r\n> \r\n> 我的版本是HanLP1.8.0，按照提示下载了数据包data，NNParserModel.txt.bin的存放路径就是源码中的NNParserModelPath指向的\"data/model/dependency/NNParserModel.txt\"。\r\n\r\nsorry，这个情况我没遇到过，或许你可以新开个贴@下作者。"
      },
      {
        "user": "seabiscuityj",
        "created_at": "2021-02-23T01:59:09Z",
        "body": "> > > @seabiscuityj 可以修改KBeamArcEagerDependencyParser.java中的源码\r\n> > > ```java\r\n> > > enableCustomDictionaryForcing(true)\r\n> > > ```\r\n> > \r\n> > \r\n> > 谢谢回复，按照建议重新训练了模型。\r\n> > 另外，还有一个问题想请教您，我在用NeuralNetworkDependencyParser进行依存句法分析的时候，总是提示空指针异常。\r\n> > 我的做法是：运行ch13下的DemoNeuralParser，就报错空指针异常。不知道是哪儿出问题？期待您的回答。\r\n> > 我的版本是HanLP1.8.0，按照提示下载了数据包data，NNParserModel.txt.bin的存放路径就是源码中的NNParserModelPath指向的\"data/model/dependency/NNParserModel.txt\"。\r\n> \r\n> sorry，这个情况我没遇到过，或许你可以新开个贴@下作者。\r\n\r\n非常感谢您的回复！作者似乎是建议新的issue放到论坛上提，我在蝴蝶效应论坛上发过帖子但是还没有回复。请问能帮我看一下吗？ @hankcs"
      },
      {
        "user": "seabiscuityj",
        "created_at": "2021-02-23T02:07:12Z",
        "body": "> > > @seabiscuityj 可以修改KBeamArcEagerDependencyParser.java中的源码\r\n> > > ```java\r\n> > > enableCustomDictionaryForcing(true)\r\n> > > ```\r\n> > \r\n> > \r\n> > 谢谢回复，按照建议重新训练了模型。\r\n> > 另外，还有一个问题想请教您，我在用NeuralNetworkDependencyParser进行依存句法分析的时候，总是提示空指针异常。\r\n> > 我的做法是：运行ch13下的DemoNeuralParser，就报错空指针异常。不知道是哪儿出问题？期待您的回答。\r\n> > 我的版本是HanLP1.8.0，按照提示下载了数据包data，NNParserModel.txt.bin的存放路径就是源码中的NNParserModelPath指向的\"data/model/dependency/NNParserModel.txt\"。\r\n> \r\n> sorry，这个情况我没遇到过，或许你可以新开个贴@下作者。\r\n\r\n还有几点问题想请教您：\r\n1. 使用的是什么版本，是portable吗？\r\n2. 有手动下载数据包data吗？\r\n3. hanlp.properties这份文件放在什么路径下？（我是clone到本地，将hanlp.properties放在src目录下，在文件里面配置了root路径为data数据包的父目录）"
      },
      {
        "user": "jimmy-walker",
        "created_at": "2021-02-23T03:03:59Z",
        "body": "> > > > @seabiscuityj 可以修改KBeamArcEagerDependencyParser.java中的源码\r\n> > > > ```java\r\n> > > > enableCustomDictionaryForcing(true)\r\n> > > > ```\r\n> > > \r\n> > > \r\n> > > 谢谢回复，按照建议重新训练了模型。\r\n> > > 另外，还有一个问题想请教您，我在用NeuralNetworkDependencyParser进行依存句法分析的时候，总是提示空指针异常。\r\n> > > 我的做法是：运行ch13下的DemoNeuralParser，就报错空指针异常。不知道是哪儿出问题？期待您的回答。\r\n> > > 我的版本是HanLP1.8.0，按照提示下载了数据包data，NNParserModel.txt.bin的存放路径就是源码中的NNParserModelPath指向的\"data/model/dependency/NNParserModel.txt\"。\r\n> > \r\n> > \r\n> > sorry，这个情况我没遇到过，或许你可以新开个贴@下作者。\r\n> \r\n> 还有几点问题想请教您：\r\n> \r\n> 1. 使用的是什么版本，是portable吗？\r\n> 2. 有手动下载数据包data吗？\r\n> 3. hanlp.properties这份文件放在什么路径下？（我是clone到本地，将hanlp.properties放在src目录下，在文件里面配置了root路径为data数据包的父目录）\r\n\r\n1.我是下载源代码，自己编译的，没用portable。版本是1.7.8。\r\n2.手动下载了数据包。\r\n3.hanlp.properties放在了target/classes目录下。"
      }
    ]
  },
  {
    "number": 1578,
    "title": "spark中使用",
    "created_at": "2020-10-27T03:30:39Z",
    "closed_at": "2020-10-27T03:53:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/1578",
    "body": "<!--\r\n提问请上论坛，不要发这里！\r\n提问请上论坛，不要发这里！\r\n提问请上论坛，不要发这里！\r\n\r\n以下必填，否则恕不受理。\r\n-->\r\n\r\n**Describe the bug**\r\nspark中只能本地使用hanlp的依存句法功能，在udf或分区中无法使用。\r\n\r\n**Code to reproduce the issue**\r\n报错信息：`Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.hankcs.hanlp.tokenizer.NLPTokenizer`\r\n\r\n**Describe the current behavior**\r\n我试着在spark中加载portable包’hanlp-portable-1.7.8.jar’，并且将data放到hdfs上，配置了hanlp.properties和ioadapter。\r\n本地进行依存句法是可以执行的。但是运用udf或分布式执行就会报错：`Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.hankcs.hanlp.tokenizer.NLPTokenizer`\r\n\r\n**Expected behavior**\r\n希望不要报错，udf顺利执行\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Spark version: 2.4.5\r\n- HanLP version:1.7.8\r\n\r\n* [x] I've completed this form and searched the web for solutions.\r\n<!-- 发表前先搜索，此处一定要勾选！ -->\r\n<!-- 发表前先搜索，此处一定要勾选！ -->\r\n<!-- 发表前先搜索，此处一定要勾选！ -->",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/1578/comments",
    "author": "jimmy-walker",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2020-10-27T03:42:26Z",
        "body": "`Predefine.HANLP_PROPERTIES_PATH = \"/data1/XXXX/file/upload/hanlp.properties\"`是一种硬编码的用法，它强制每个节点从该节点的`/data1/XXXX/file/upload/hanlp.properties`读取配置文件，那么你有在每个节点上存放该配置文件吗？非要硬编码的话不如在你的open方法里为每个path头部拼接一下root。\r\n\r\n配置文件应当放在spark启动时的classpath目录，或者打包到具体业务的jar中。"
      }
    ]
  },
  {
    "number": 1454,
    "title": "pyhanlp 和 Hanlp 分词结果不一致, 词性不一致",
    "created_at": "2020-04-17T06:24:44Z",
    "closed_at": "2020-04-17T14:51:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/1454",
    "body": "<!--\r\nThank you for reporting a possible bug in HanLP.\r\nPlease fill in the template below to bypass our spam filter.  \r\n以下必填，否则直接关闭。\r\n-->\r\n\r\n**Describe the bug**\r\npyhanlp 和 java 版的Hanlp分词结果不一致\r\n词性也不一致\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\npyhanlp,  0.1.63\r\n```python\r\nfrom pyhanlp import *\r\nprint(HanLP.segment('你说的这句话没有根据,根据以往的经验,我是对的.'))\r\n[你/rr, 说/v, 的/ude1, 这/rzv, 句话/q, 没有/v, 根据/p, ,/w, 根据/p, 以往/t, 的/ude1, 经验/n, ,/w, 我/rr, 是/vshi, 对/p, 的/ude1, ./w]\r\n```\r\ncom.hankcs:hanlp:portable-1.7.5\r\n```java\r\nString text = \"你说的这句话没有根据,根据以往的经验,我是对的.\";\r\nSystem.out.println(HanLP.segment(text));\r\n[你/r, 说/v, 的/uj, 这/r, 句话/q, 没/d, 有/v, 根据/p, ,/w, 根据/p, 以往/t, 的/uj, 经验/n, ,/w, 我/r, 是/v, 对/p, 的/uj, ./w]\r\n```\r\n\r\n**Describe the current behavior**\r\npyhanlp 的分词 没有/v,  词性: 你/rr\r\nHanlp 分词: 没/d, 有/v,  词性: 你/r\r\n\r\n**Expected behavior**\r\n表现一致\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Python version: 3.62\r\n- HanLP version: portable-1.7.5\r\n\r\n**Other info / logs**\r\nNone\r\n\r\n* [x] I've completed this form and searched the web for solutions.\r\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/1454/comments",
    "author": "yan-rui",
    "comments": [
      {
        "user": "yan-rui",
        "created_at": "2020-04-17T08:43:42Z",
        "body": "把java版换成pyhanlp依赖一致的1.7.7后结果如下\r\n```java\r\n[你/r, 说/v, 的/uj, 这/r, 句话/q, 没/d, 有/v, 根据/p, ,/w, 根据/p, 以往/t, 的/uj, 经验/n, ,/w, 我/r, 是/v, 对/p, 的/uj, ./w]\r\n```"
      },
      {
        "user": "hankcs",
        "created_at": "2020-04-17T14:51:27Z",
        "body": "老生常谈了，检查一下配置文件是否一致。"
      }
    ]
  },
  {
    "number": 1372,
    "title": "pyhanlp和hanlp",
    "created_at": "2020-01-02T08:03:35Z",
    "closed_at": "2020-01-02T08:10:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/1372",
    "body": "你好，\r\n请问pyhanlp和hanlp是什么关系呢？然后pyhanlp的依存句法分析，支持输入分好词的句子嘛？谢谢回复 ~\r\n新年快乐",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/1372/comments",
    "author": "OKtang",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2020-01-02T08:10:51Z",
        "body": "pyhanlp=1.x，是Java版hanlp1.x的wrapper，传统机器学习路线。\r\nhanlp现在发布了2.0，基于深度学习，与pyhanlp再无关系。\r\n支持。"
      }
    ]
  },
  {
    "number": 1150,
    "title": "敏感信息过滤",
    "created_at": "2019-04-15T03:19:28Z",
    "closed_at": "2019-04-19T15:27:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/1150",
    "body": "你好什么时候上线敏感信息识别功能？",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/1150/comments",
    "author": "SANDUO421",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2019-04-19T15:27:01Z",
        "body": "你可以用StopWordDictionary之类的工具过滤任何词语。但从良知的角度来讲，本开源项目不会发布阻碍言论自由的功能。"
      }
    ]
  },
  {
    "number": 711,
    "title": "请教作者，依存句法分析，对复合句子解析不够合理，有什么方法改进？",
    "created_at": "2017-12-07T11:05:24Z",
    "closed_at": "2017-12-08T19:06:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/711",
    "body": "比如对这个句子： 如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。\r\n整体解析感觉有问题，如下：\r\n\r\n1\t如果\t如果\tc\tc\t_\t4\tADV\t_\t_\r\n2\t一个\t一个\tm\tmq\t_\t3\tATT\t_\t_\r\n3\t算法\t算法\tn\tn\t_\t4\tSBV\t_\t_\r\n4\t有\t有\tv\tvyou\t_\t0\tHED\t_\t_\r\n5\t缺陷\t缺陷\tn\tn\t_\t4\tVOB\t_\t_\r\n6\t,\t,\twp\tw\t_\t4\tWP\t_\t_\r\n7\t或\t或\tc\tc\t_\t9\tLAD\t_\t_\r\n8\t不\t不\td\td\t_\t9\tADV\t_\t_\r\n9\t适合于\t适合于\tv\tv\t_\t4\tCOO\t_\t_\r\n10\t某个\t某个\tr\trz\t_\t11\tATT\t_\t_\r\n11\t问题\t问题\tn\tn\t_\t9\tVOB\t_\t_\r\n12\t,\t,\twp\tw\t_\t4\tWP\t_\t_\r\n13\t执行\t执行\tv\tv\t_\t4\tCOO\t_\t_\r\n14\t这个\t这个\tr\trz\t_\t15\tATT\t_\t_\r\n15\t算法\t算法\tn\tn\t_\t13\tVOB\t_\t_\r\n16\t将\t将\td\td\t_\t18\tADV\t_\t_\r\n17\t不会\t不会\tv\tv\t_\t18\tADV\t_\t_\r\n18\t解决\t解决\tv\tv\t_\t13\tCOO\t_\t_\r\n19\t这个\t这个\tr\trz\t_\t20\tATT\t_\t_\r\n20\t问题\t问题\tn\tn\t_\t18\tVOB\t_\t_\r\n21\t.\t.\twp\tw\t_\t18\tWP\t_\t_\r\n\r\n\r\n我期望的中心词是 “解决” 。 神经网络模型返回的是“有“，条件随机场模型返回的是“适合于”\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/711/comments",
    "author": "junphine",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-12-08T19:06:46Z",
        "body": "目前没有办法改进，因为\r\n\r\n1. 我记得CTB语料中大部分是单句，复合句顶多只有2个子句。像这个句子，模型只注意到第一个子句。\r\n2. 大部分来自新闻领域，不太适应IT领域。\r\n\r\n下次发issue请严格按模板填写，勾选注意事项，谢谢合作。"
      }
    ]
  },
  {
    "number": 653,
    "title": "如何获取字符串中的所有可能分词",
    "created_at": "2017-10-20T06:28:56Z",
    "closed_at": "2017-10-20T15:59:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/653",
    "body": "我目前采用的是1.3.1版本，使用用户自定义字典，字典中包含：牛仔马甲、 牛仔、 马甲、牛仔马甲外套、马甲。\r\n针对源字符串：“帅气无袖连帽牛仔马甲外套女短款2017秋装新款韩版复古牛仔马夹” 进行分词时，采用的是索引分词器，得到的结果是：“牛仔马甲外套，牛仔，马甲”，无法分词得出“牛仔马甲”。hanlp中是否有办法获取到所有可能存在的分词？",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/653/comments",
    "author": "baiselanjingyi",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-10-20T15:59:36Z",
        "body": "用最新版"
      }
    ]
  },
  {
    "number": 649,
    "title": "请求进行一次常规性更新",
    "created_at": "2017-10-12T05:42:23Z",
    "closed_at": "2017-10-14T08:35:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/649",
    "body": "距离上一次release发布已经有好几个月了，可否将近几个月的小幅修正整理发布一个新版本的release，同时更新一下Maven版本。",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/649/comments",
    "author": "AnyListen",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-10-14T04:25:47Z",
        "body": "暂时还在赶论文，等我稍微闲一点吧。"
      },
      {
        "user": "AnyListen",
        "created_at": "2017-10-14T08:35:29Z",
        "body": "OK"
      }
    ]
  },
  {
    "number": 586,
    "title": "\"大学城里\"分词错误的问题",
    "created_at": "2017-07-19T04:29:42Z",
    "closed_at": "2017-07-20T14:50:09Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/586",
    "body": "## 版本号\r\n\r\n当前最新版本号是：v1.3.4\r\n我使用的版本是：v1.3.4\r\n\r\n\r\n## 我的问题\r\n\r\n对以下句子进行分词\"深圳大学城里各大国内外知名高校合办学校校牌在阳光下熠熠发光。\"\r\n发现“深圳大学城里”分词错误，可以在CoreNatureDictionary.ngram.txt中添加\"大学城@里 10\"解决该问题。\r\n\r\n## 复现问题\r\n\r\n### 触发代码\r\n\r\n```\r\npublic class DemoSegment\r\n{\r\n    public static void main(String[] args)\r\n    {\r\n        String[] testCase = new String[]{\r\n                \"深圳大学城里各大国内外知名高校合办学校校牌在阳光下熠熠发光。毗邻深圳大学城的是以创新产业为发展核心\"\r\n        };\r\n        for (String sentence : testCase)\r\n        {\r\n            List<Term> termList = HanLP.segment(sentence);\r\n            System.out.println(termList);\r\n        }\r\n    }\r\n}\r\n\r\n```\r\n### 期望输出\r\n\r\n```\r\n深圳大学/ntu, 城里/s, 各/rz, 大/a, 国内外/s, 知名/a, 高校/n\r\n```\r\n\r\n### 实际输出\r\n\r\n```\r\n深圳/ns, 大学城/nz, 里/f, 各/rz, 大/a, 国内外/s, 知名/a, 高校/n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/586/comments",
    "author": "fanyongbin",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-07-20T14:43:59Z",
        "body": "1. 个人觉得实际输出更符合习惯\r\n1. 加入`大学城@里`后，得到[深圳大学城/nz, 里/f, 各/rz, 大/a, 国内外/s, 知名/a, 高校/n, 合办/v, 学校/nis, 校牌/n, 在/p, 阳光/n, 下/f, 熠熠/z, 发光/vi]\r\n1. 如果你一定要得到`城里`的话，可以试试加入 `深圳大学@城里` 或`未##团@城里`"
      },
      {
        "user": "hankcs",
        "created_at": "2017-07-20T14:50:09Z",
        "body": "问题应该解决了，如果还有问题，欢迎重开issue。"
      },
      {
        "user": "fanyongbin",
        "created_at": "2017-07-20T14:52:29Z",
        "body": "谢谢，因实际需求，需将“深圳大学城里”分为“深圳/大学城/里”，避免分为“深圳/大学/城里”，而关联到机构深圳大学。"
      }
    ]
  },
  {
    "number": 538,
    "title": "个别句子句法依存的核心词有误",
    "created_at": "2017-05-18T03:40:08Z",
    "closed_at": "2017-05-19T07:05:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/538",
    "body": "版本1.3.2\r\n对于句法依存的结果，有种无从修改的感觉，是不是只能重新训练一个模型呢？\r\n\r\n**\"元旦下午8点03分提醒我去后海和老朋友聚会\",**\r\n核心词为“聚会”，应该为“提醒”，聚会本来是动词的，我修改成了名词。\r\n\r\n1\t元旦\tnt\tt\t_\t2\t定中关系\t_\t_\r\n2\t下午\tnt\tt\t_\t3\t定中关系\t_\t_\r\n3\t8点\tnt\tt\t_\t4\t定中关系\t_\t_\r\n4\t03分\tnt\tt\t_\t5\t状中结构\t_\t_\r\n5\t提醒\tv\tv\t_\t8\t定中关系\t_\t_\r\n6\t我\tr\trr\t_\t5\t兼语\t_\t_\r\n7\t去\tv\tvf\t_\t5\t动宾关系\t_\t_\r\n8\t后\tnd\tf\t_\t11\t定中关系\t_\t_\r\n9\t海和老\tnh\tnr\t_\t10\t定中关系\t_\t_\r\n10\t朋友\tn\tn\t_\t11\t定中关系\t_\t_\r\n11\t聚会\tn\tn\t_\t0\t核心关系\t_\t_\r\n\r\n元旦 --(定中关系)--> 下午\r\n下午 --(定中关系)--> 8点\r\n8点 --(定中关系)--> 03分\r\n03分 --(状中结构)--> 提醒\r\n提醒 --(定中关系)--> 后\r\n我 --(兼语)--> 提醒\r\n去 --(动宾关系)--> 提醒\r\n后 --(定中关系)--> 聚会\r\n海和老 --(定中关系)--> 朋友\r\n朋友 --(定中关系)--> 聚会\r\n聚会 --(核心关系)--> ##核心##\r\n\r\n**“每星期日中午一点半提醒我出去玩的时间到了\"**\r\n核心词为“到”，应该为“提醒”\r\n\r\n1\t每星期日\tnt\tt\t_\t2\t定中关系\t_\t_\r\n2\t中午\tnt\tt\t_\t4\t定中关系\t_\t_\r\n3\t一点\tnt\tt\t_\t4\t定中关系\t_\t_\r\n4\t半\tm\tmq\t_\t5\t状中结构\t_\t_\r\n5\t提醒\tv\tv\t_\t10\t定中关系\t_\t_\r\n6\t我\tr\trr\t_\t5\t兼语\t_\t_\r\n7\t出去\tv\tvf\t_\t5\t动宾关系\t_\t_\r\n8\t玩\tv\tv\t_\t7\t并列关系\t_\t_\r\n9\t的\tu\tude1\t_\t7\t右附加关系\t_\t_\r\n10\t时间\tn\tn\t_\t11\t主谓关系\t_\t_\r\n11\t到\tv\tv\t_\t0\t核心关系\t_\t_\r\n12\t了\tu\tule\t_\t11\t右附加关系\t_\t_\r\n\r\n每星期日 --(定中关系)--> 中午\r\n中午 --(定中关系)--> 半\r\n一点 --(定中关系)--> 半\r\n半 --(状中结构)--> 提醒\r\n提醒 --(定中关系)--> 时间\r\n我 --(兼语)--> 提醒\r\n出去 --(动宾关系)--> 提醒\r\n玩 --(并列关系)--> 出去\r\n的 --(右附加关系)--> 出去\r\n时间 --(主谓关系)--> 到\r\n到 --(核心关系)--> ##核心##\r\n了 --(右附加关系)--> 到",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/538/comments",
    "author": "hx78",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-05-18T09:21:46Z",
        "body": "第一个句子人名识别误命中了，也许你在用98年的模型。这个模型里面应该没有后海这个词。对14年来讲没有问题\r\n\r\n1\t元旦\t元旦\tnt\tt\t_\t2\t定中关系\t_\t_\r\n2\t下午\t下午\tnt\tt\t_\t6\t定中关系\t_\t_\r\n3\t8\t8\tm\tm\t_\t4\t定中关系\t_\t_\r\n4\t点\t点\tq\tqt\t_\t6\t定中关系\t_\t_\r\n5\t03\t03\tm\tm\t_\t6\t定中关系\t_\t_\r\n6\t分\t分\tq\tqt\t_\t7\t状中结构\t_\t_\r\n7\t提醒\t提醒\tv\tv\t_\t0\t核心关系\t_\t_\r\n8\t我\t我\tr\trr\t_\t7\t兼语\t_\t_\r\n9\t去\t去\tv\tvf\t_\t7\t动宾关系\t_\t_\r\n10\t后海\t后海\tnd\tf\t_\t9\t动宾关系\t_\t_\r\n11\t和\t和\tc\tcc\t_\t12\t左附加关系\t_\t_\r\n12\t老朋友\t老朋友\tn\tn\t_\t10\t并列关系\t_\t_\r\n13\t聚会\t聚会\tv\tvi\t_\t9\t并列关系\t_\t_\r\n\r\n第二个句子“一点半”最好分为一个词，且为时间词性，不然不好修饰提醒。\r\n"
      },
      {
        "user": "hx78",
        "created_at": "2017-05-19T07:05:04Z",
        "body": "tks"
      }
    ]
  },
  {
    "number": 510,
    "title": "NS分词直接就取第一个分词结果",
    "created_at": "2017-04-27T11:41:12Z",
    "closed_at": "2017-05-02T12:56:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/510",
    "body": "版本1.3.2\r\n如 List<Vertex> vertexList = coarseResult.get(0);\r\n\r\n**eg:创建一个下午3点举办下午茶的提醒**\r\n\r\n”下午茶“被分成”下午 茶“\r\n我尝试增大”下午茶“频率  和  删除”未##时@茶“也没有作用\r\n只得暂时增加”举办@下午茶 1“来解决\r\n\r\n打印词图：========按终点打印========\r\nto:  1, from:  0, weight:04.60, word:始##始@创\r\nto:  2, from:  0, weight:04.60, word:始##始@创建\r\nto:  3, from:  1, weight:11.32, word:创@建\r\nto:  4, from:  2, weight:03.01, word:创建@未##数\r\nto:  4, from:  3, weight:02.21, word:建@未##数\r\nto:  6, from:  4, weight:05.10, word:未##数@下\r\nto:  6, from:  5, weight:08.38, word:个@下\r\nto:  7, from:  4, weight:04.59, word:未##数@未##时\r\nto:  7, from:  5, weight:05.18, word:个@未##时\r\nto:  8, from:  6, weight:09.41, word:下@午\r\nto:  9, from:  7, weight:02.52, word:未##时@未##时\r\nto:  9, from:  8, weight:11.60, word:午@未##时\r\nto: 11, from:  9, weight:05.80, word:未##时@举\r\nto: 11, from: 10, weight:09.78, word:点@举\r\nto: 12, from:  9, weight:05.69, word:未##时@举办\r\nto: 12, from: 10, weight:09.78, word:点@举办\r\nto: 13, from: 11, weight:11.40, word:举@办\r\nto: 14, from: 12, weight:10.87, word:举办@下\r\nto: 14, from: 13, weight:06.25, word:办@下\r\nto: 15, from: 12, weight:03.78, word:举办@未##时\r\nto: 15, from: 13, weight:05.21, word:办@未##时\r\nto: 16, from: 12, weight:10.87, word:举办@下午茶\r\nto: 16, from: 13, weight:10.63, word:办@下午茶\r\nto: 17, from: 14, weight:09.41, word:下@午\r\nto: 18, from: 15, weight:05.80, word:未##时@茶\r\nto: 18, from: 17, weight:11.60, word:午@茶\r\nto: 19, from: 16, weight:02.38, word:下午茶@的\r\nto: 19, from: 18, weight:02.61, word:茶@的\r\nto: 20, from: 19, weight:05.67, word:的@提\r\nto: 21, from: 19, weight:05.64, word:的@提醒\r\nto: 22, from: 20, weight:11.05, word:提@醒\r\nto: 23, from: 21, weight:03.00, word:提醒@末##末\r\nto: 23, from: 22, weight:04.25, word:醒@末##末\r\n\r\n粗分结果[创建/v, 一个/mq, 下午/t, 3点/t, 举办/v, 下午/t, 茶/n, 的/ude1, 提醒/v]\r\n人名角色观察：[  A 22202445 ][创建 L 16 ][一个 K 90 L 84 ][下午 L 15 K 12 ][3点 A 22202445 ][举办 K 11 L 9 ][下午 L 15 K 12 ][茶 D 19 C 2 E 2 L 2 ][的 L 15411 K 11354 M 96 C 1 ][提醒 L 123 K 14 M 2 ][  A 22202445 ]\r\n人名角色标注：[ /A ,创建/L ,一个/K ,下午/L ,3点/A ,举办/K ,下午/L ,茶/D ,的/L ,提醒/K , /A]\r\n粗分结果[创建/v, 一个/mq, 下午/t, 3点/t, 举办/v, 下午茶/nf, 的/ude1, 提醒/v]\r\n人名角色观察：[  A 22202445 ][创建 L 16 ][一个 K 90 L 84 ][下午 L 15 K 12 ][3点 A 22202445 ][举办 K 11 L 9 ][下午茶 A 22202445 ][的 L 15411 K 11354 M 96 C 1 ][提醒 L 123 K 14 M 2 ][  A 22202445 ]\r\n人名角色标注：[ /A ,创建/L ,一个/K ,下午/L ,3点/A ,举办/K ,下午茶/A ,的/K ,提醒/L , /A]\r\n[创建/v, 一个/mq, 下午/t, 3点/t, 举办/v, 下午/t, 茶/n, 的/ude1, 提醒/v]\r\n打印词图：========按终点打印========\r\nto:  1, from:  0, weight:04.60, word:始##始@创\r\nto:  2, from:  0, weight:04.60, word:始##始@创建\r\nto:  3, from:  1, weight:11.32, word:创@建\r\nto:  4, from:  2, weight:03.01, word:创建@未##数\r\nto:  4, from:  3, weight:02.21, word:建@未##数\r\nto:  6, from:  4, weight:05.10, word:未##数@下\r\nto:  6, from:  5, weight:08.38, word:个@下\r\nto:  7, from:  4, weight:04.59, word:未##数@未##时\r\nto:  7, from:  5, weight:05.18, word:个@未##时\r\nto:  8, from:  6, weight:09.41, word:下@午\r\nto:  9, from:  7, weight:02.52, word:未##时@未##时\r\nto:  9, from:  8, weight:11.60, word:午@未##时\r\nto: 11, from:  9, weight:05.80, word:未##时@举\r\nto: 11, from: 10, weight:09.78, word:点@举\r\nto: 12, from:  9, weight:05.69, word:未##时@举办\r\nto: 12, from: 10, weight:09.78, word:点@举办\r\nto: 13, from: 11, weight:11.40, word:举@办\r\nto: 14, from: 12, weight:10.87, word:举办@中\r\nto: 14, from: 13, weight:08.25, word:办@中\r\nto: 15, from: 12, weight:03.78, word:举办@未##时\r\nto: 15, from: 13, weight:05.21, word:办@未##时\r\nto: 16, from: 14, weight:08.17, word:中@午\r\nto: 17, from: 15, weight:05.80, word:未##时@茶\r\nto: 17, from: 16, weight:11.60, word:午@茶\r\nto: 18, from: 17, weight:02.61, word:茶@的\r\nto: 19, from: 18, weight:05.67, word:的@提\r\nto: 20, from: 18, weight:05.64, word:的@提醒\r\nto: 21, from: 19, weight:11.05, word:提@醒\r\nto: 22, from: 20, weight:03.00, word:提醒@末##末\r\nto: 22, from: 21, weight:04.25, word:醒@末##末\r\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/510/comments",
    "author": "hx78",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-04-27T14:12:56Z",
        "body": "NShort的目的不是最终返回N个结果，而是在N条路径上跑命名实体识别，增加召回率。"
      }
    ]
  },
  {
    "number": 506,
    "title": "分词问题",
    "created_at": "2017-04-26T08:35:53Z",
    "closed_at": "2017-07-07T03:11:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/506",
    "body": "**问题**\r\n对句子\"我想提点钱出来\"进行分词，\r\n得到结果: [我/rr, 想/v, 提/v, 点钱/nz, 出来/vf]，分词中“点钱”切分错误？\r\n\r\n**版本**\r\n- 程序版本号：hanlp-portable-1.2.7\r\n- 数据版本号：data-for-1.3.2.zip\r\n\r\n**触发代码**\r\n```java\r\nHanLP.segment(\"我想提点钱出来\");\r\n```\r\n**代码输出**\r\n``` \r\n[我/rr, 想/v, 提/v, 点钱/nz, 出来/vf]\r\n```\r\n\r\n**尝试**\r\n分词结果有问题，把`点钱`作为一个词切分出来，而不是切分为`提点/v 钱/n`。\r\n我在词库CoreNatureDictionary.txt中发现 “点钱 nz 110”，而没有“提点”词，所以我用CustomDictionary将“提点”加入，但是还是没有效果，麻烦帮忙看一下，非常感谢。",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/506/comments",
    "author": "iwaller",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-04-26T21:10:47Z",
        "body": "感谢指正，人民日报语料库的标准认为“点钱”是一个词，里面存在大量例句：\r\n\r\n```\r\n消费者/n 愿意/v 多/a 花/n 点钱/nz\r\n都/d 出/vf 点钱/nz \r\n```\r\n\r\n但HanLP灵活的模型机制为用户提供了完全的控制权，你可以在\r\ndata/dictionary/CoreNatureDictionary.ngram.txt 中加一句\r\n\r\n```\r\n提点@钱 10000\r\n```\r\n\r\n"
      },
      {
        "user": "iwaller",
        "created_at": "2017-04-27T12:31:58Z",
        "body": "多谢，我尝试了， 可以实现。\r\n但由于“提点”在词典中没有出现，所有需要在data\\dictionary\\CoreNatureDictionary.txt中添加词“提点”才可，还有就是加入时，需要将频率调大点才行，我测试的是“提点 v 10000”才可以切分出来。"
      }
    ]
  },
  {
    "number": 500,
    "title": "关于model（模型）在下载的时候百度云实效还是给墙了？",
    "created_at": "2017-04-24T08:13:40Z",
    "closed_at": "2017-04-27T07:55:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/500",
    "body": "你好：\r\n        我在下载data-1.28-full（大致）的时候，百度云连接失效，请问还有什么路径或者链接吗？我尝试了翻墙去链接还是不可以的。谢谢",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/500/comments",
    "author": "wankaiss",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-04-26T21:34:41Z",
        "body": "噢，1.2.8的确莫名其妙地审核不过。你可以用最新版的，旧版不再维护了。"
      },
      {
        "user": "wankaiss",
        "created_at": "2017-04-27T07:55:18Z",
        "body": "好了，我已经下载到最新的了。若是这种审核不通过的能否加密压缩？"
      },
      {
        "user": "hankcs",
        "created_at": "2017-04-27T13:43:01Z",
        "body": "是个好主意。1.2.8才搞笑，是先通过了很长一段时间，然后突然不通过的。"
      }
    ]
  },
  {
    "number": 499,
    "title": "Crf分词结合ngram词典不能取得理想结果",
    "created_at": "2017-04-23T06:33:47Z",
    "closed_at": "2017-05-02T10:24:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/499",
    "body": "@hanks, 你好。\r\n关于下列输入\r\nCRFSegment segment = new CRFSegment().;\r\nsegment.enableCustomDictionary(true);\r\n\t\tList<Term> seg = segment.seg(\"哪些药产自上海\");\r\n\r\n一直出现不理想的分词结果\r\n\r\n[哪些/r, 药产/nz, 自/p, 上海/ns]\r\n\r\n理想的结果是 [哪些/r, 药/n,产自/v, 上海/ns]\r\n\r\n已经按照您在其他issue里面说的修改了核心词典CoreNatureDictionary.mini.txt\r\n其中包含（产自\tv\t999）新增，（药\tn\t41\tv\t1）药这个词是原来就有的\r\n也修改了CoreNatureDictionary.ngram.mini.txt\r\n（药@产自 10000）新增，\r\n并且在自定义词典CustomDictionary.txt中新增（产自 v 999）\r\n删除所有缓存，并且运行，还是出现[哪些/r, 药产/nz, 自/p, 上海/ns]结果\r\n\r\n不知道我哪里做的不对，或者缺少了哪些步骤，还请务必告知\r\n非常感谢！\r\n@TylunasLi",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/499/comments",
    "author": "kelciej",
    "comments": [
      {
        "user": "kelciej",
        "created_at": "2017-04-23T06:43:48Z",
        "body": "版本号是1.3.2，差点忘了"
      },
      {
        "user": "TylunasLi",
        "created_at": "2017-04-23T13:23:33Z",
        "body": "BiGram模型只作用于基于词图的分词方法，在HanLP里面是 Viterbi最短路分词、Dijkstra最短路分词、N-最短路分词三种。\r\nCRF分词采用“由字构词”的策略，本身并没有涉及到Bigram模型，也不能如BiGram类算法一般简单地调整参数，只能重新训练。\r\n请了解相关算法大致原理再来看如何调整。"
      },
      {
        "user": "kelciej",
        "created_at": "2017-05-02T10:24:31Z",
        "body": "谢谢你的解答"
      }
    ]
  },
  {
    "number": 495,
    "title": "有关动态添加单词的问题",
    "created_at": "2017-04-20T15:30:24Z",
    "closed_at": "2017-04-23T03:50:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/495",
    "body": "hanks，你好，\r\n在经过动态添加单词，或者单词+词性\r\nCustomDictionary.insert(word);\r\nCustomDictionary.add(word);\r\n之后，应该通过什么方法才能获得该词的原分词结果和词性\r\n比方说，强制添加了一个新词CustomDictionary.insert(“睡不着觉 xx 1”);\r\n之后想获得该词的原来的分词结果\r\n[睡/v, 不/d, 着/uz, 觉/v\r\n可有方法？",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/495/comments",
    "author": "kelciej",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-04-20T15:55:39Z",
        "body": "得自行编写业务逻辑了。"
      },
      {
        "user": "kelciej",
        "created_at": "2017-04-20T15:57:45Z",
        "body": "嗯嗯，我知道啦，谢谢"
      },
      {
        "user": "kelciej",
        "created_at": "2017-04-23T03:50:33Z",
        "body": "我后来解决的办法就是，把原来的分词结果，写入txt"
      }
    ]
  },
  {
    "number": 489,
    "title": "维特比算法有一处BUG",
    "created_at": "2017-04-17T12:35:32Z",
    "closed_at": "2017-04-21T01:47:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/489",
    "body": "#### ViterbiSegment.java#148 行\r\n`Vertex from = nodes[nodes.length - 1].getFirst();`\r\n**测试代码：**\r\n\r\n```java\r\nSystem.out.println(new ViterbiSegment()\r\n               .enableNameRecognize(true)\r\n               .enableIndexMode(true)\r\n               .enablePlaceRecognize(true)\r\n               .enableOrganizationRecognize(true)\r\n               .enableCustomDictionary(true).seg(\"癌症死亡率与发病率逐年增高,已成为世界范围内的一大健康问题。多西他赛(docetaxel,DTX)是紫杉烷类抗癌药,其作用机制是抑制微管蛋白解聚,从而阻断肿瘤细胞增殖。多西他赛抗癌谱较广,可以用来治疗肺癌、乳腺癌、前列腺癌、卵巢癌,治疗效果是紫杉醇的2-4倍。但是其水溶性较差,临床注射液应用吐温80增溶,易引起毒副反应；其分布无特异性,易引起全身毒性。此外,由于紫杉烷类药物的大量使用,导致癌症细胞对紫杉烷类药物产生了多药耐药(multi-drug resistance,MDR)性,这成为DTX使用中的另一大问题。癌症细胞产生耐药性的主要原因是细胞膜转运蛋白p-glycoprotein(p-gp)的过量表达。由于高分子科学的快速发展,应用聚合物高分子作为药物载体包载疏水性药物以增加疏水药物溶解性和治疗效果的研究成为热点。两亲性嵌段聚合物可以在水中聚集为胶束；胶束具有亲-疏水核壳结构,内核疏水而外壳亲水,其疏水内核可以包载疏水性药物。聚合物胶束可以通过物理性包埋和化学结合两种方式包载药物。化学结合载药是将药物和聚合物嵌段通过化学键化学结合形成聚合物-药物结合物(polymer-drug conjugate),药物成为聚合物的一部分,参与胶束的组装。此方法可以增加胶束的载药量和稳定性；还可利用环境敏感的化学键共价连接药物和聚合物,起到在肿瘤部位集中释放药物,提高药物疗效的作用,这也是此类药物传递系统的一大创新点。肿瘤组织内具有较低的pH值,特殊酶和较强的还原性环境,基于此特点,可以采用pH敏感、酶敏感和氧化还原敏感的化学键连接药物,使聚合物具有环境敏感性质。本课题成功合成了具有氧化还原敏感的聚合物-药物结合物,使其在水中自组装,并进一步用于包载其他药物。选择两亲性聚合物甲氧基聚乙二醇-聚丙交酯乙交酯(mPEG-PLGA,PP)作为大分子骨架,通过氧化还原敏感的二硫键连接DTX,合成敏感的nPEG-PLGA-SS-DTX (PP-SS-DTX)结合物。利用此结合物进一步包载多西他赛或维拉帕米(verapamil, VRP),实现载药体系功能多样化。本课题研究包括以下几个方面：1、PP-SS-DTX结合物的合成和表征：选择含有二硫键的二硫代二丙酸(即DTDP)作为连接分子,通过三步化学反应成功合成了PP-SS-DTX结合物,并通过熔点测定、核磁共振氢谱(1H-NMR);和傅立叶变换红外色谱(FT-IR)验证其结构。此外,通过芘探针法测定其临界聚集浓度(critical aggregation concentration, CAC),发现PP-SS-DTX结合物的CAC值很小,为10.20 μmol/L。2、氧化还原敏感型双模式载多西他赛PP-SS-DTX/DTX胶束系统的评价：通过透析法制备双模式载多西他赛PP-SS-DTX/DTX胶束,双模式是指同一体系中通过化学结合和物理包埋两种模式包载DTX。通过TEM和DLS测定胶束的形态和粒径,所制备的PP-SS-DTX/DTX胶束呈球形,粒径为112.3 nm;PP-SS-DTX/DTX胶束具有较高的载药量,为(14.65±0.71)%。通过溶血试验初步判断PP-SS-DTX/DTX胶束具有一定生物相容性。为验证PP-SS-DTX/DTX胶束释药是否具有氧化还原敏感性,在释放介质中加入还原型物质DTT。释放实验结果表明,与DTX原料药溶液相比,PP-SS-DTX/DTX胶束具有药物缓释、氧化还原敏感性释药和程序性释药的特点。选择乳腺癌MCF-7细胞系和黑色素瘤B16F10细胞系进行体外细胞毒性实验与细胞摄取实验。在两种细胞系中,PP-SS-DTX/DTX胶束的毒性作用均明显强于DTX溶液。由于DTX不能发荧光,利用香豆素-6(coumarin-6, C-6)模拟疏水药物DTX,制备了PP-SS-DTX/C-6胶束,通过荧光倒置显微镜技术和流式细胞术定性、定量的研究细胞对PP-SS-DTX/C-6胶束的摄取。结果表明,MCF-7和B16F10两种细胞对PP-SS-DTX/C-6胶束的摄取效率高于C-6溶液。细胞对胶束制剂的高摄取效率,保证了药物浓集于病变细胞,是提高抗肿瘤效果的重要原因。3、PP-SS-DTX/VRP双模式双载药胶束系统的抗肿瘤多药耐药的评价：采用探头超声法制备PP-SS-DTX/VRP胶束,双模式是指分别通过化学结合和物理包埋方式载药,双载药是指一个胶束体系同时包载两种药物——多西他赛和维拉帕米。维拉帕米是有效的p-gp的拮抗剂,可与p-gp结合并使其失活,逆转癌症细胞的多药耐药。在制备胶束过程中,考察探头超声的超声时间对维拉帕米的包封率与载药量的影响,发现探头超声的时间越短,维拉帕米的载药量和包封率越高。通过TEM和DLS测定胶束的形态和粒径,PP-SS-DTX胶束和PP-SS-DTX/VRP胶束均呈球形,粒径分别为(79.3±1.2)nm和(78.3±4.6)nm。PP-SS-DTX/VRP胶束中DTX载药量为(8.94±0.72)%,维拉帕米的载药量为(5.66±1.60)%,包封率为(53.49±10.96)%。为验证胶束的体外释放是否具有敏感性,在释放介质中加入DTT,实验发现DTX和VRP的释放均具有氧化还原敏感性。为研究胶束的抗肿瘤多药耐药的作用,选择乳腺癌MCF-7细胞和耐药的乳腺癌MCF-7/ADR细胞,进行DTX溶液、PP-SS-DTX胶束和PP-SS-DTX/VRP胶束的细胞毒性实验、细胞摄取实验和细胞凋亡实验。细胞毒性实验表明,PP-SS-DTX胶束和PP-SS-DTX/VRP胶束对两种细胞系的细胞毒性作用均好于DTX溶液；此外,PP-SS-DTX/VRP胶束对MCF-7/ADR细胞的细胞毒性作用强于PP-SS-DTX胶束。由于DTX和VRP不能发荧光,因此选择p-gp底物罗丹明123(rhodamine 123,RH 123)作为荧光分子对胶束进行标记研究细胞摄取情况和p-gp抑制效果。结果表明,MCF-7对PP-SS-DTX胶束和PP-SS-DTX/VRP胶束的细胞摄取效率相似,且均大于RH 123溶液；MCF-7/ADR细胞对RH 123几乎无摄取,对PP-SS-DTX/VRP胶束的摄取大于对PP-SS-DTX胶束的摄取。细胞凋亡实验表明,PP-SS-DTX胶束和PP-SS-DTX/VRP胶束培养的细胞凋亡数目多于DTX溶液组。以上实验结果表明,PP-SS-DTX包载VRP后,可以抑制p-gp的作用,增强耐药细胞对胶束的摄取,加快细胞凋亡的进程,从而提高多西他赛的抗肿瘤效应。4、PP-SS-DTX胶束体内药动学研究：实验动物选择Wistar大鼠,通过尾静脉注射给药,利用HPLC法检测大鼠血浆中DTX的含量。以DTX原料药溶液作对照,观察PP-SS-DTX胶束的体内药动学过程,得到血药浓度-时间曲线,并通过DSA 2.0软件对血药浓度-时间曲线拟合,得PP-SS-DTX胶束房室模型和药动学参数。将DTX制备成PP-SS-DTX胶束后,血药浓度-时间曲线变平缓；通过对曲线拟合,发现DTX溶液和PP-SS-DTX胶束均符合二室模型。将DTX制备成PP-SS-DTX胶束后,体内清除率降低,消除半衰期和体内滞留时间延长,有利于药物缓释并保证药物疗效。综上所述,本研究首次合成了具有氧化还原敏感的聚合物-药物结合物PP-SS-DTX,此结合物可以自组装为胶束,包载DTX或VRP,提高DTX的溶解度和抗肿瘤效果,并能够逆转DTX的MDR。本课题研究具有重要的意义,为提高疏水性药物溶解度、提高药物疗效、解决肿瘤细胞多药耐药的研究提供一定的理论基础。\"));\r\n```\r\n\r\n**报错信息如下：**\r\n```\r\nException in thread \"main\" java.util.NoSuchElementException\r\n\tat java.util.LinkedList.getFirst(LinkedList.java:242)\r\n\tat com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.viterbi(ViterbiSegment.java:148)\r\n\tat com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:103)\r\n\tat com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)\r\n\tat TestMain.main(TestMain.java:11)\r\n```\r\n\r\n**其他说明：**\r\n>不使用HanLp自带的自定义词典不会报错\r\n\r\n**改进后的代码：**\r\n```java\r\nLinkedList<Vertex> node = null;\r\nint index = 1;\r\nwhile (node == null || node.size() <= 0){\r\n     node = nodes[nodes.length - index];\r\n     index++;\r\n}\r\nVertex from = node.getFirst();\r\n```",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/489/comments",
    "author": "AnyListen",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-04-20T15:36:22Z",
        "body": "奇怪，我这边没有错误，测试了mini和非mini两版模型，都enableCustomDictionary了：\r\n\r\n```\r\nSystem.out.println(new ViterbiSegment()\r\n               .enableNameRecognize(true)\r\n               .enableIndexMode(true)\r\n               .enablePlaceRecognize(true)\r\n               .enableOrganizationRecognize(true)\r\n               .enableCustomDictionary(true).seg(\"癌症死亡率与发病率逐年增高,已成为世界范围内的一大健康问题。多西他赛(docetaxel,DTX)是紫杉烷类抗癌药,其作用机制是抑制微管蛋白解聚,从而阻断肿瘤细胞增殖。多西他赛抗癌谱较广,可以用来治疗肺癌、乳腺癌、前列腺癌、卵巢癌,治疗效果是紫杉醇的2-4倍。但是其水溶性较差,临床注射液应用吐温80增溶,易引起毒副反应；其分布无特异性,易引起全身毒性。此外,由于紫杉烷类药物的大量使用,导致癌症细胞对紫杉烷类药物产生了多药耐药(multi-drug resistance,MDR)性,这成为DTX使用中的另一大问题。癌症细胞产生耐药性的主要原因是细胞膜转运蛋白p-glycoprotein(p-gp)的过量表达。由于高分子科学的快速发展,应用聚合物高分子作为药物载体包载疏水性药物以增加疏水药物溶解性和治疗效果的研究成为热点。两亲性嵌段聚合物可以在水中聚集为胶束；胶束具有亲-疏水核壳结构,内核疏水而外壳亲水,其疏水内核可以包载疏水性药物。聚合物胶束可以通过物理性包埋和化学结合两种方式包载药物。化学结合载药是将药物和聚合物嵌段通过化学键化学结合形成聚合物-药物结合物(polymer-drug conjugate),药物成为聚合物的一部分,参与胶束的组装。此方法可以增加胶束的载药量和稳定性；还可利用环境敏感的化学键共价连接药物和聚合物,起到在肿瘤部位集中释放药物,提高药物疗效的作用,这也是此类药物传递系统的一大创新点。肿瘤组织内具有较低的pH值,特殊酶和较强的还原性环境,基于此特点,可以采用pH敏感、酶敏感和氧化还原敏感的化学键连接药物,使聚合物具有环境敏感性质。本课题成功合成了具有氧化还原敏感的聚合物-药物结合物,使其在水中自组装,并进一步用于包载其他药物。选择两亲性聚合物甲氧基聚乙二醇-聚丙交酯乙交酯(mPEG-PLGA,PP)作为大分子骨架,通过氧化还原敏感的二硫键连接DTX,合成敏感的nPEG-PLGA-SS-DTX (PP-SS-DTX)结合物。利用此结合物进一步包载多西他赛或维拉帕米(verapamil, VRP),实现载药体系功能多样化。本课题研究包括以下几个方面：1、PP-SS-DTX结合物的合成和表征：选择含有二硫键的二硫代二丙酸(即DTDP)作为连接分子,通过三步化学反应成功合成了PP-SS-DTX结合物,并通过熔点测定、核磁共振氢谱(1H-NMR);和傅立叶变换红外色谱(FT-IR)验证其结构。此外,通过芘探针法测定其临界聚集浓度(critical aggregation concentration, CAC),发现PP-SS-DTX结合物的CAC值很小,为10.20 μmol/L。2、氧化还原敏感型双模式载多西他赛PP-SS-DTX/DTX胶束系统的评价：通过透析法制备双模式载多西他赛PP-SS-DTX/DTX胶束,双模式是指同一体系中通过化学结合和物理包埋两种模式包载DTX。通过TEM和DLS测定胶束的形态和粒径,所制备的PP-SS-DTX/DTX胶束呈球形,粒径为112.3 nm;PP-SS-DTX/DTX胶束具有较高的载药量,为(14.65±0.71)%。通过溶血试验初步判断PP-SS-DTX/DTX胶束具有一定生物相容性。为验证PP-SS-DTX/DTX胶束释药是否具有氧化还原敏感性,在释放介质中加入还原型物质DTT。释放实验结果表明,与DTX原料药溶液相比,PP-SS-DTX/DTX胶束具有药物缓释、氧化还原敏感性释药和程序性释药的特点。选择乳腺癌MCF-7细胞系和黑色素瘤B16F10细胞系进行体外细胞毒性实验与细胞摄取实验。在两种细胞系中,PP-SS-DTX/DTX胶束的毒性作用均明显强于DTX溶液。由于DTX不能发荧光,利用香豆素-6(coumarin-6, C-6)模拟疏水药物DTX,制备了PP-SS-DTX/C-6胶束,通过荧光倒置显微镜技术和流式细胞术定性、定量的研究细胞对PP-SS-DTX/C-6胶束的摄取。结果表明,MCF-7和B16F10两种细胞对PP-SS-DTX/C-6胶束的摄取效率高于C-6溶液。细胞对胶束制剂的高摄取效率,保证了药物浓集于病变细胞,是提高抗肿瘤效果的重要原因。3、PP-SS-DTX/VRP双模式双载药胶束系统的抗肿瘤多药耐药的评价：采用探头超声法制备PP-SS-DTX/VRP胶束,双模式是指分别通过化学结合和物理包埋方式载药,双载药是指一个胶束体系同时包载两种药物——多西他赛和维拉帕米。维拉帕米是有效的p-gp的拮抗剂,可与p-gp结合并使其失活,逆转癌症细胞的多药耐药。在制备胶束过程中,考察探头超声的超声时间对维拉帕米的包封率与载药量的影响,发现探头超声的时间越短,维拉帕米的载药量和包封率越高。通过TEM和DLS测定胶束的形态和粒径,PP-SS-DTX胶束和PP-SS-DTX/VRP胶束均呈球形,粒径分别为(79.3±1.2)nm和(78.3±4.6)nm。PP-SS-DTX/VRP胶束中DTX载药量为(8.94±0.72)%,维拉帕米的载药量为(5.66±1.60)%,包封率为(53.49±10.96)%。为验证胶束的体外释放是否具有敏感性,在释放介质中加入DTT,实验发现DTX和VRP的释放均具有氧化还原敏感性。为研究胶束的抗肿瘤多药耐药的作用,选择乳腺癌MCF-7细胞和耐药的乳腺癌MCF-7/ADR细胞,进行DTX溶液、PP-SS-DTX胶束和PP-SS-DTX/VRP胶束的细胞毒性实验、细胞摄取实验和细胞凋亡实验。细胞毒性实验表明,PP-SS-DTX胶束和PP-SS-DTX/VRP胶束对两种细胞系的细胞毒性作用均好于DTX溶液；此外,PP-SS-DTX/VRP胶束对MCF-7/ADR细胞的细胞毒性作用强于PP-SS-DTX胶束。由于DTX和VRP不能发荧光,因此选择p-gp底物罗丹明123(rhodamine 123,RH 123)作为荧光分子对胶束进行标记研究细胞摄取情况和p-gp抑制效果。结果表明,MCF-7对PP-SS-DTX胶束和PP-SS-DTX/VRP胶束的细胞摄取效率相似,且均大于RH 123溶液；MCF-7/ADR细胞对RH 123几乎无摄取,对PP-SS-DTX/VRP胶束的摄取大于对PP-SS-DTX胶束的摄取。细胞凋亡实验表明,PP-SS-DTX胶束和PP-SS-DTX/VRP胶束培养的细胞凋亡数目多于DTX溶液组。以上实验结果表明,PP-SS-DTX包载VRP后,可以抑制p-gp的作用,增强耐药细胞对胶束的摄取,加快细胞凋亡的进程,从而提高多西他赛的抗肿瘤效应。4、PP-SS-DTX胶束体内药动学研究：实验动物选择Wistar大鼠,通过尾静脉注射给药,利用HPLC法检测大鼠血浆中DTX的含量。以DTX原料药溶液作对照,观察PP-SS-DTX胶束的体内药动学过程,得到血药浓度-时间曲线,并通过DSA 2.0软件对血药浓度-时间曲线拟合,得PP-SS-DTX胶束房室模型和药动学参数。将DTX制备成PP-SS-DTX胶束后,血药浓度-时间曲线变平缓；通过对曲线拟合,发现DTX溶液和PP-SS-DTX胶束均符合二室模型。将DTX制备成PP-SS-DTX胶束后,体内清除率降低,消除半衰期和体内滞留时间延长,有利于药物缓释并保证药物疗效。综上所述,本研究首次合成了具有氧化还原敏感的聚合物-药物结合物PP-SS-DTX,此结合物可以自组装为胶束,包载DTX或VRP,提高DTX的溶解度和抗肿瘤效果,并能够逆转DTX的MDR。本课题研究具有重要的意义,为提高疏水性药物溶解度、提高药物疗效、解决肿瘤细胞多药耐药的研究提供一定的理论基础。\"));\r\n```\r\n\r\n```\r\n[癌症/n, 死亡率/n, 死亡/vi, 与/cc, 发病率/n, 发病/vn, 逐年/d, 增高/vi, ,/w, 已/d, 成为/v, 世界/n, 范围内/nz, 范围/n, 的/ude1, 一大/n, 健康/a, 问题/n, 。/w, 多西/nrf, 他/rr, 赛/vn, (/w, docetaxel/nx, 多西/nrf, ,/w, DTX/nx, )/w, 是/vshi, 紫杉/n, 烷/n, 类/q, 抗癌药/nz, 抗癌/vn, ,/w, 其/rz, 作用/n, 机制/n, 是/vshi, 抑制/v, 微管/gb, 蛋白/n, 解聚/gb, ,/w, 从而/c, 阻断/v, 肿瘤/n, 细胞增殖/gb, 细胞/n, 增殖/vn, 增溶/gb, 。/w, 多西/nrf, 他/rr, 赛/vn, 抗癌/vn, 谱/ng, 较广/d, ,/w, 可以/v, 用来/v, 治疗/vn, 肺癌/nhd, 、/w, 乳腺癌/nhd, 乳腺/n, 腺癌/gb, 、/w, 前列腺癌/nhd, 前列/n, 前列腺/n, 腺癌/gb, 、/w, 卵巢癌/nhd, 卵巢/n, ,/w, 治疗/vn, 效果/n, 是/vshi, 紫杉醇/nmc, 紫杉/n, 的/ude1, 2/m, -/nx, 4/m, 倍/q, 。/w, 但是/c, 其/rz, 水溶性/n, 溶性/nz, 较差/d, ,/w, 临床/vn, 注射液/n, 注射/v, 应用/vn, 吐温/nmc, 80/m, 增溶/gb, ,/w, 易/ad, 引起/v, 毒/n, 副反应/gb, 反应/vn, ；/w, 其/rz, 分布/vi, 无/v, 特异性/gb, 特异/z, 异性/n, ,/w, 易/ad, 引起/v, 全身/n, 毒性/n, 。/w, 此外/c, ,/w, 由于/p, 紫杉/n, 烷/n, 类/q, 药物/n, 的/ude1, 大量/m, 使用/v, ,/w, 导致/v, 癌症/n, 细胞/n, 对/p, 紫杉/n, 烷/n, 类/q, 药物/n, 产生/v, 了/ule, 多/a, 药/n, 耐药/nz, (/w, multi-drug/nx, 结合物/n,  /w, resistance/nx, ,/w, MDR/nx, )/w, 性/ng, ,/w, 这/rzv, 成为/v, DTX/nx, 使用/v, 中的/v, 另/rz, 一大/n, 问题/n, 。/w, 癌症/n, 细胞/n, 产生/v, 耐药性/n, 耐药/nz, 药性/n, 的/ude1, 主要/b, 原因/n, 是/vshi, 细胞膜/n, 细胞/n, 转运/v, 蛋白/n, p-glycoprotein/nx, (/w, p-gp/nx, )/w, 的/ude1, 过量/vi, 表达/v, 。/w, 由于/p, 高分子/n, 分子/n, 科学/n, 的/ude1, 快速/d, 发展/vn, ,/w, 应用/vn, 聚合物/n, 聚合/vi, 高分子/n, 分子/n, 作为/p, 药物/n, 载体/n, 包载/nr, 疏水/nz, 性药物/n, 药物/n, 以/p, 增加/v, 疏水/nz, 药物/n, 溶解性/gc, 溶解/v, 和/cc, 治疗/vn, 效果/n, 的/ude1, 研究/vn, 成为/v, 热点/n, 。/w, 两亲性/gb, 嵌/v, 段/q, 聚合物/n, 聚合/vi, 可以/v, 在/p, 水中/s, 聚集/v, 为/p, 胶束/gc, ；/w, 胶束/gc, 具有/v, 亲/ng, -/nx, 疏水/nz, 核/n, 壳/ng, 结构/n, ,/w, 内核/n, 疏水/nz, 而/cc, 外壳/n, 亲水/n, ,/w, 其/rz, 疏水/nz, 内核/n, 可以/v, 包载/nr, 疏水/nz, 性药物/n, 药物/n, 。/w, 聚合物/n, 聚合/vi, 结合物/n, 胶束/gc, 可以/v, 通过/p, 物理/n, 性/ng, 包埋/gb, 和/cc, 化学/n, 结合/v, 两/m, 种/q, 方式包/nr, 方式/n, 载/v, 药物/n, 。/w, 化学/n, 结合/v, 载/v, 药/n, 是/vshi, 将/d, 药物/n, 和/cc, 聚合物/n, 聚合/vi, 嵌/v, 段/q, 通过/p, 化学键/n, 化学/n, 化学/n, 结合/v, 形成/v, 聚合物/n, 聚合/vi, -/nx, 药物/n, 结合物/n, 结合/v, 法测定/n, (/w, polymer-drug/nx,  /w, conjugate/nx, ),/w, 药物/n, 成为/v, 聚合物/n, 聚合/vi, 的/ude1, 一部分/mq, 部/q, 部分/n, ,/w, 参与/v, 胶束/gc, 的/ude1, 组装/vn, 。/w, 此/rzs, 方法/n, 可以/v, 增加/v, 胶束/gc, 的/ude1, 载/v, 药量/nz, 和/cc, 稳定性/n, 稳定/an, 定性/vn, ；/w, 还/d, 可/v, 利用/v, 环境/n, 敏感/a, 的/ude1, 化学键/n, 化学/n, 双模式/nz, 共价/gb, 连接/v, 药物/n, 和/cc, 聚合物/n, 聚合/vi, ,/w, 起到/v, 在/p, 肿瘤/n, 部位/n, 集中/v, 释放/v, 药物/n, ,/w, 提高/v, 药物/n, 疗效/n, 的/ude1, 作用/n, ,/w, 这/rzv, 也/d, 是/vshi, 此类/r, 药物/n, 传递/v, 系统/n, 的/ude1, 一大/n, 创新/vi, 点/qt, 。/w, 肿瘤/n, 组织内/nt, 组织/n, 具有/v, 较低/d, 的/ude1, pH/nx, 值/n, ,/w, 特殊/a, 酶/nz, 和/cc, 较强/d, 的/ude1, 还原性/nz, 还原/vi, 环境/n, ,/w, 基于/p, 此/rzs, 特点/n, ,/w, 可以/v, 采用/v, pH/nx, 敏感/a, 、/w, 酶/nz, 敏感/a, 和/cc, 氧化/vn, 还原/vi, 敏感/a, 的/ude1, 化学/n, 键连接/nz, 连接/v, 药物/n, ,/w, 使/v, 聚合物/n, 聚合/vi, 具有/v, 环境/n, 敏感/a, 性质/n, 。/w, 本/rz, 课题/n, 成功/a, 合成/vn, 了/ule, 具有/v, 氧化/vn, 还原/vi, 敏感/a, 的/ude1, 聚合物/n, 聚合/vi, -/nx, 药物/n, 结合物/n, 结合/v, ,/w, 使/v, 其/rz, 在/p, 水中/s, 自/p, 组装/vn, ,/w, 并/cc, 进一步/d, 用于/v, 包载/nr, 其他/rzv, 药物/n, 。/w, 选择/v, 两亲性/gb, 聚合物/n, 聚合/vi, 甲氧基/gc, 聚乙二醇/nmc, -/nx, 聚丙/nr, 交/v, 酯/ng, 乙交/nr, 酯/ng, (/w, mPEG-PLGA/nx, ,/w, PP/nx, )/w, 作为/p, 大分子/n, 分子/n, 骨架/n, ,/w, 通过/p, 氧化/vn, 还原/vi, 敏感/a, 的/ude1, 二/m, 硫/n, 键连接/nz, 连接/v, DTX/nx, ,/w, 合成/vn, 敏感/a, 的/ude1, nPEG-PLGA-SS-DTX/nx, 胶束/gc, 强于/v,  /w, (/w, PP-SS-DTX/nx, 香豆素/nmc, )/w, 结合物/n, 结合/v, 。/w, 利用/v, 此/rzs, 结合物/n, 结合/v, 进一步/d, 包载/nr, 多西/nrf, 他/rr, 赛或/nr, 维拉帕米/nrf, 维拉/nrf, (/w, verapamil/nx, ,/w,  /w, VRP/nx, ),/w, 实现/v, 载/v, 药/n, 体系/n, 功能/n, 多样化/vi, 多样/a, 。/w, 本/rz, 课题/n, 研究/vn, 包括/v, 以下/f, 几/d, 个/q, 方面/n, ：/w, 1/m, 、/w, PP-SS-DTX/nx, 结合物/n, 结合/v, 的/ude1, 合成/vn, 和/cc, 表征/n, ：/w, 选择/v, 含有/v, 二硫键/gb, 胶束/gc, 的/ude1, 二/m, 硫/n, 代/q, 二/m, 丙酸/nmc, (/w, 即/v, DTDP/nx, )/w, 作为/p, 连接/v, 分子/n, ,/w, 通过/p, 三/m, 步/qv, 化学反应/n, 化学/n, 胶束/gc, 反应/vn, 成功/a, 合成/vn, 了/ule, PP-SS-DTX/nx, 结合物/n, 结合/v, ,/w, 并/cc, 通过/p, 熔点/n, 测定/v, 、/w, 核磁共振/n, 核磁/nz, 磁共振/n, 共振/vi, 胶束/gc, 氢/n, 谱/ng, (/w, 1/m, H-NMR/nx, 多西/nrf, );/w, 和/cc, 傅立叶/nrf, 变换/v, 红外/n, 色谱/n, (/w, FT-IR/nx, )/w, 验证/v, 其/rz, 结构/n, 。/w, 此外/c, ,/w, 通过/p, 芘/x, 探针/n, 法测定/n, 测定/v, 其/rz, 临界/b, 聚集/v, 浓度/n, (/w, critical/nx, 胶束/gc,  /w, aggregation/nx, 胶束/gc,  /w, concentration/nx, 胶束/gc, ,/w,  /w, CAC/nx, ),/w, 发现/v, PP-SS-DTX/nx, 结合物/n, 结合/v, 的/ude1, CAC/nx, 值/n, 很小/a, ,/w, 为/p, 10.20/m,  μ/w, mol/nx, //w, L/nx, 。/w, 2/m, 、/w, 氧化/vn, 还原/vi, 敏感/a, 型/k, 双模式/nz, 双模/gm, 模式/n, 载/v, 多西/nrf, 他/rr, 赛/vn, PP-SS-DTX/nx, //w, DTX/nx, 胶束/gc, 系统/n, 的/ude1, 评价/vn, ：/w, 通过/p, 透析法/nz, 透析/v, 析法/nz, 法制/n, 制备/v, 双模式/nz, 双模/gm, 模式/n, 载/v, 多西/nrf, 他/rr, 赛/vn, PP-SS-DTX/nx, //w, DTX/nx, 胶束/gc, ,/w, 双模式/nz, 双模/gm, 模式/n, 胶束/gc, 是/vshi, 指/v, 同一/b, 体系/n, 中/f, 通过/p, 化学/n, 结合/v, 和/cc, 物理/n, 包埋/gb, 两/m, 种/q, 模式/n, 包载/nr, DTX/nx, 。/w, 通过/p, TEM/nx, 和/cc, DLS/nx, 测定/v, 胶束/gc, 的/ude1, 形态/n, 和/cc, 粒径/nz, ,/w, 所/usuo, 制备/v, 的/ude1, PP-SS-DTX/nx, 胶束/gc, //w, DTX/nx, 胶束/gc, 呈/v, 球形/n, ,/w, 粒径/nz, 为/p, 112.3/m, 胶束/gc,  /w, nm/nx, ;/w, PP-SS-DTX/nx, //w, DTX/nx, 胶束/gc, 具有/v, 较高/ad, 的/ude1, 载/v, 药量/nz, ,/w, 为/p, (/w, 14.65/m, ±/w, 0.71/m, )/w, %/nx, 。/w, 通过/p, 溶血/nhd, 试验/vn, 初步判断/nz, 初步/d, 判断/v, PP-SS-DTX/nx, 强于/v, //w, DTX/nx, 胶束/gc, 胶束/gc, 具有/v, 一定/b, 生物/n, 相容性/gm, 相容/vi, 。/w, 为/p, 验证/v, PP-SS-DTX/nx, 罗丹明/nr, //w, DTX/nx, 胶束/gc, 释/vg, 药/n, 是否/v, 具有/v, 氧化/vn, 还原/vi, 敏感性/gi, 敏感/a, 感性/n, ,/w, 在/p, 释放/v, 介质/n, 中/f, 加入/v, 还原型/n, 还原/vi, 原型/n, 物质/n, DTT/nx, 。/w, 释放/v, 实验/vn, 结果表明/n, 结果/n, 表明/v, ,/w, 与/cc, DTX/nx, 原料药/n, 原料/n, 溶液/n, 相比/vi, ,/w, PP-SS-DTX/nx, 胶束/gc, //w, DTX/nx, 胶束/gc, 具有/v, 药物/n, 缓释/v, 、/w, 氧化/vn, 还原/vi, 敏感性/gi, 敏感/a, 感性/n, 释/vg, 药/n, 和/cc, 程序性/n, 程序/n, 释/vg, 药/n, 的/ude1, 特点/n, 。/w, 选择/v, 乳腺癌/nhd, 乳腺/n, 腺癌/gb, MCF-/nx, 7/m, 细胞系/n, 细胞/n, 和/cc, 黑色素瘤/nhd, 黑色/n, 黑色素/n, 色素/n, B/nx, 16/m, F/nx, 10/m, 细胞系/n, 细胞/n, 进行/vn, 体外/nz, 细胞/n, 毒性/n, 实验/vn, 与/cc, 细胞/n, 摄取/v, 实验/vn, 。/w, 在/p, 两/m, 种/q, 细胞系/n, 细胞/n, 中/f, ,/w, PP-SS-DTX/nx, 结果表明/n, //w, DTX/nx, 胶束/gc, 的/ude1, 毒性/n, 作用/n, 均/d, 明显/a, 强于/v, DTX/nx, 胶束/gc, 溶液/n, 。/w, 由于/p, DTX/nx, 多西/nrf, 不能/v, 发/v, 荧光/n, ,/w, 利用/v, 香豆素/nmc, -/nx, 6/m, (/w, coumarin-/nx, 胶束/gc, 6/m, ,/w,  /w, C-/nx, 6/m, )/w, 模拟/vn, 疏水/nz, 药物/n, DTX/nx, ,/w, 制备/v, 了/ule, PP-SS-DTX/nx, //w, C-/nx, 6/m, 胶束/gc, ,/w, 通过/p, 荧光/n, 倒置/vi, 显微镜/n, 显微/gb, 技术/n, 和/cc, 流式细胞/l, 细胞/n, 术/ng, 定性/vn, 、/w, 定量/n, 的/ude1, 研究/vn, 细胞/n, 对/p, PP-SS-DTX/nx, 胶束/gc, //w, C-/nx, 6/m, 胶束/gc, 的/ude1, 摄取/v, 。/w, 结果表明/n, 结果/n, 表明/v, ,/w, MCF-/nx, 胶束/gc, 7/m, 和/cc, B/nx, 16/m, F/nx, 10两/m, 种/q, 细胞/n, 对/p, PP-SS-DTX/nx, //w, C-/nx, 6/m, 胶束/gc, 的/ude1, 摄取/v, 效率/n, 高于/v, C-/nx, 6/m, 溶液/n, 。/w, 细胞/n, 对/p, 胶束/gc, 制剂/n, 的/ude1, 高/a, 摄取/v, 效率/n, ,/w, 保证/v, 了/ule, 药物/n, 浓集/nz, 于/p, 病变/vn, 细胞/n, ,/w, 是/vshi, 提高/v, 抗肿瘤/nz, 肿瘤/n, 效果/n, 的/ude1, 重要/a, 原因/n, 。/w, 3/m, 、/w, PP-SS-DTX/nx, //w, VRP/nx, 双模式/nz, 双模/gm, 模式/n, 双/q, 载/v, 药/n, 胶束/gc, 系统/n, 的/ude1, 抗肿瘤/nz, 肿瘤/n, 性药物/n, 多/a, 药/n, 耐药/nz, 的/ude1, 评价/vn, ：/w, 采用/v, 探头/vi, 超声/n, 法/n, 制备/v, PP-SS-DTX/nx, //w, VRP/nx, 胶束/gc, ,/w, 双模式/nz, 双模/gm, 模式/n, 是/vshi, 指/v, 分别/d, 通过/p, 化学/n, 结合/v, 和/cc, 物理/n, 包埋/gb, 方式载/nr, 方式/n, 药/n, ,/w, 双载/nr, 药/n, 是/vshi, 指/v, 一个/mq, 胶束/gc, 体系/n, 同时/c, 包载/nr, 两/m, 种/q, 药物/n, ——/w, 多西/nrf, 他/rr, 赛/vn, 和/cc, 维拉帕米/nrf, 维拉/nrf, 。/w, 维拉帕米/nrf, 维拉/nrf, 是/vshi, 有效/a, 的/ude1, p-gp/nx, 的/ude1, 拮抗剂/gb, 拮抗/nz, ,/w, 可/v, 与/cc, p-gp/nx, 结合/v, 并/cc, 使/v, 其/rz, 失/v, 活/v, ,/w, 逆转/vi, 癌症/n, 细胞/n, 的/ude1, 多/a, 药/n, 耐药/nz, 。/w, 在/p, 制备/v, 胶束/gc, 过程/n, 中/f, ,/w, 考察/v, 探头/vi, 超声/n, 的/ude1, 超声/n, 时间/n, 对/p, 维拉帕米/nrf, 维拉/nrf, 的/ude1, 包封/gi, 率/v, 与/cc, 载/v, 药量/nz, 的/ude1, 影响/vn, ,/w, 发现/v, 探头/vi, 超声/n, 的/ude1, 时间/n, 越短/v, ,/w, 维拉帕米/nrf, 维拉/nrf, 的/ude1, 载/v, 药量/nz, 和/cc, 包封/gi, 率/v, 越高/d, 。/w, 通过/p, TEM/nx, 和/cc, DLS/nx, 测定/v, 胶束/gc, 的/ude1, 形态/n, 和/cc, 粒径/nz, ,/w, PP-SS-DTX/nx, 胶束/gc, 和/cc, PP-SS-DTX/nx, //w, VRP/nx, 胶束/gc, 均/d, 呈/v, 球形/n, ,/w, 粒径/nz, 分别为/v, 分别/d, (/w, 79.3/m, ±/w, 1.2/m, )/w, nm/nx, 和/cc, (/w, 78.3/m, ±/w, 4.6/m, )/w, nm/nx, 。/w, PP-SS-DTX/nx, //w, VRP/nx, 胶束/gc, 中/f, DTX/nx, 载/v, 药量/nz, 为/p, (/w, 8.94/m, ±/w, 0.72/m, )/w, %/nx, ,/w, 维拉帕米/nrf, 维拉/nrf, 的/ude1, 载/v, 药量/nz, 为/p, (/w, 5.66/m, ±/w, 1.60/m, )/w, %/nx, ,/w, 包封/gi, 率/v, 为/p, (/w, 53.49/m, ±/w, 10.96/m, )/w, %/nx, 。/w, 为/p, 验证/v, 胶束/gc, 的/ude1, 体外/nz, 释放/v, 是否/v, 具有/v, 敏感性/gi, 敏感/a, 感性/n, ,/w, 在/p, 释放/v, 介质/n, 中/f, 加入/v, DTT/nx, ,/w, 实验/vn, 发现/v, DTX/nx, 和/cc, VRP/nx, 的/ude1, 释放/v, 均/d, 具有/v, 氧化/vn, 还原/vi, 敏感性/gi, 敏感/a, 感性/n, 。/w, 为/p, 研究/vn, 胶束/gc, 的/ude1, 抗肿瘤/nz, 肿瘤/n, 多/a, 药/n, 耐药/nz, 的/ude1, 作用/n, ,/w, 选择/v, 乳腺癌/nhd, 乳腺/n, 腺癌/gb, MCF-/nx, 7/m, 细胞/n, 和/cc, 耐药/nz, 的/ude1, 乳腺癌/nhd, 乳腺/n, 腺癌/gb, MCF-/nx, 7/m, //w, ADR/nx, 细胞/n, ,/w, 进行/vn, DTX/nx, 溶液/n, 、/w, PP-SS-DTX/nx, 胶束/gc, 和/cc, PP-SS-DTX/nx, //w, VRP/nx, 胶束/gc, 的/ude1, 细胞/n, 毒性/n, 实验/vn, 、/w, 细胞/n, 摄取/v, 实验/vn, 和/cc, 细胞/n, 凋亡/nz, 实验/vn, 。/w, 细胞/n, 毒性/n, 实验/vn, 表明/v, ,/w, PP-SS-DTX/nx, 胶束/gc, 和/cc, PP-SS-DTX/nx, //w, VRP/nx, 胶束/gc, 对/p, 两/m, 种/q, 细胞系/n, 细胞/n, 的/ude1, 细胞/n, 毒性/n, 作用/n, 均/d, 好/a, 于/p, DTX/nx, 溶液/n, ；/w, 此外/c, ,/w, PP-SS-DTX/nx, //w, VRP/nx, 胶束/gc, 对/p, MCF-/nx, 7/m, //w, ADR/nx, 细胞/n, 的/ude1, 细胞/n, 毒性/n, 作用/n, 强于/v, PP-SS-DTX/nx, 胶束/gc, 。/w, 由于/p, DTX/nx, 和/cc, VRP/nx, 不能/v, 发/v, 荧光/n, ,/w, 因此/c, 选择/v, p-gp/nx, 底物/gc, 罗丹明/nr, 罗丹/nrf, 123/m, (/w, rhodamine/nx,  /w, 123/m, ,/w, RH/nx,  /w, 123/m, )/w, 作为/p, 荧光/n, 分子/n, 对/p, 胶束/gc, 进行/vn, 标记/n, 研究/vn, 细胞/n, 摄取/v, 情况/n, 和/cc, p-gp/nx, 抑制/v, 效果/n, 。/w, 结果表明/n, 结果/n, 表明/v, ,/w, MCF-/nx, 7/m, 对/p, PP-SS-DTX/nx, 胶束/gc, 和/cc, PP-SS-DTX/nx, //w, VRP/nx, 胶束/gc, 的/ude1, 细胞/n, 摄取/v, 效率/n, 相似/a, ,/w, 且/c, 均/d, 大于/v, RH/nx,  /w, 123/m, 溶液/n, ；/w, MCF-/nx, 7/m, //w, ADR/nx, 细胞/n, 对/p, RH/nx,  /w, 123/m, 几乎/d, 无/v, 摄取/v, ,/w, 对/p, PP-SS-DTX/nx, //w, VRP/nx, 胶束/gc, 的/ude1, 摄取/v, 大于/v, 对/p, PP-SS-DTX/nx, 胶束/gc, 的/ude1, 摄取/v, 。/w, 细胞/n, 凋亡/nz, 实验/vn, 表明/v, ,/w, PP-SS-DTX/nx, 胶束/gc, 和/cc, PP-SS-DTX/nx, //w, VRP/nx, 胶束/gc, 培养/v, 的/ude1, 细胞/n, 凋亡/nz, 数目/n, 多于/v, DTX/nx, 溶液/n, 组/n, 。/w, 以上/f, 实验/vn, 结果表明/n, 结果/n, 表明/v, ,/w, PP-SS-DTX/nx, 包载/nr, VRP/nx, 后/f, ,/w, 可以/v, 抑制/v, p-gp/nx, 的/ude1, 作用/n, ,/w, 增强/v, 耐药/nz, 细胞/n, 对/p, 胶束/gc, 的/ude1, 摄取/v, ,/w, 加快/v, 细胞/n, 凋亡/nz, 的/ude1, 进程/n, ,/w, 从而/c, 提高/v, 多西/nrf, 他/rr, 赛/vn, 的/ude1, 抗肿瘤/nz, 肿瘤/n, 效应/n, 。/w, 4/m, 、/w, PP-SS-DTX/nx, 胶束/gc, 体内/s, 药/n, 动/v, 学/v, 研究/vn, ：/w, 实验/vn, 动物/n, 选择/v, Wistar/nx, 大鼠/gb, ,/w, 通过/p, 尾/ng, 静脉注射/nz, 静脉/n, 注射/v, 给/p, 药/n, ,/w, 利用/v, HPLC/nx, 法/n, 检测/vn, 大鼠/gb, 血浆/n, 中/f, DTX/nx, 的/ude1, 含量/n, 。/w, 以/p, DTX/nx, 原料药/n, 原料/n, 溶液/n, 作/v, 对照/v, ,/w, 观察/v, PP-SS-DTX/nx, 胶束/gc, 的/ude1, 体内/s, 药/n, 动/v, 学/v, 过程/n, ,/w, 得到/v, 血药浓度/nz, 浓度/n, -/nx, 时间/n, 曲线/n, ,/w, 并/cc, 通过/p, DSA/nx,  /w, 2.0软件/nt, 2.0/m, 软件/n, 对/p, 血药浓度/nz, 浓度/n, -/nx, 时间/n, 曲线拟合/gc, 曲线/n, 拟合/gi, ,/w, 得/ude3, PP-SS-DTX/nx, 胶束/gc, 房室/n, 模型/n, 和/cc, 药/n, 动/v, 学/v, 参数/n, 。/w, 将/d, DTX/nx, 制备/v, 成/v, PP-SS-DTX/nx, 胶束/gc, 后/f, ,/w, 血药浓度/nz, 浓度/n, -/nx, 时间/n, 曲线/n, 变/v, 平缓/a, ；/w, 通过/p, 对/p, 曲线拟合/gc, 曲线/n, 拟合/gi, ,/w, 发现/v, DTX/nx, 溶液/n, 和/cc, PP-SS-DTX/nx, 胶束/gc, 均/d, 符合/v, 二室/nt, 模型/n, 。/w, 将/d, DTX/nx, 制备/v, 成/v, PP-SS-DTX/nx, 胶束/gc, 后/f, ,/w, 体内/s, 清除率/gb, 清除/v, 降低/v, ,/w, 消除/v, 半衰期/n, 和/cc, 体内/s, 滞留/v, 时间/n, 延长/v, ,/w, 有利于/v, 有利/a, 利于/v, 药物/n, 缓释/v, 并/cc, 保证/v, 药物/n, 疗效/n, 。/w, 综上所述/c, 综上/nz, 所述/nz, ,/w, 本/rz, 研究/vn, 首次/mq, 合成/vn, 了/ule, 具有/v, 氧化/vn, 还原/vi, 敏感/a, 的/ude1, 聚合物/n, 聚合/vi, -/nx, 药物/n, 结合物/n, 结合/v, PP-SS-DTX/nx, ,/w, 此/rzs, 结合物/n, 结合/v, 可以/v, 自/p, 组装/vn, 为/p, 胶束/gc, ,/w, 包载/nr, DTX/nx, 或/c, VRP/nx, ,/w, 提高/v, DTX/nx, 的/ude1, 溶解度/n, 溶解/v, 和/cc, 抗肿瘤/nz, 肿瘤/n, 效果/n, ,/w, 并/cc, 能够/v, 逆转/vi, DTX/nx, 的/ude1, MDR/nx, 。/w, 本/rz, 课题/n, 研究/vn, 具有/v, 重要/a, 的/ude1, 意义/n, ,/w, 为/p, 提高/v, 疏水/nz, 性药物/n, 药物/n, 溶解度/n, 溶解/v, 、/w, 提高/v, 药物/n, 疗效/n, 、/w, 解决/v, 肿瘤细胞/gb, 肿瘤/n, 细胞/n, 多/a, 药/n, 耐药/nz, 的/ude1, 研究/vn, 提供/v, 一定/b, 的/ude1, 理论/n, 基础/n, 。/w]\r\n```"
      },
      {
        "user": "hankcs",
        "created_at": "2017-04-20T15:38:24Z",
        "body": "我们唯一的不同可能是自定义词典的内容，或者代码版本，请问是否版本库代码报错，另我们的配置和文件内容是一样的吗？\r\n\r\n```\r\nCustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf;\r\n```"
      },
      {
        "user": "hankcs",
        "created_at": "2017-04-20T15:43:36Z",
        "body": "感谢提出宝贵意见。\r\n另外，如果算法正确，那么nodes[nodes.length - 1]一定是一个含有且仅含有一个元素的链表，即末##末。所以即使有错，错的也是最短路算法，不好在这里打补丁。"
      },
      {
        "user": "AnyListen",
        "created_at": "2017-04-21T01:47:39Z",
        "body": "我使用最原始的词典并没发现问题，该问题后续我会继续跟踪一下，多谢"
      }
    ]
  },
  {
    "number": 472,
    "title": "关于用CRF进行词性标注的几个问题",
    "created_at": "2017-04-10T03:10:30Z",
    "closed_at": "2017-05-02T13:03:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/472",
    "body": "在使用CRF++时，发现example的例子是有3列的，中间列是词性\r\n我做了一个简单的语料\r\n                \"放周杰伦\",\r\n                \"放周杰伦的歌\",\r\n                \"放电台\",\r\n                \"播电台\",\r\n                \"播电台北京怀旧金曲\",\r\n                \"听电台\",\r\n                \"要听电台\",\r\n                \"我想听电台\",\r\n                \"我要听电台\",\r\n                \"放有声书\",\r\n                \"我想听相声\",\r\n                \"播鼓曲\",\r\n                \"我想听鼓曲\",\r\n                \"放越剧\",\r\n                \"播越剧\",\r\n                \"收听心动FM\",\r\n                \"收听深圳动听102\",\r\n\r\n放      v       S\r\n周      nt      B\r\n杰      nt      M\r\n伦      nt      E\r\n放      v       S\r\n周      ns      B\r\n杰      ns      M\r\n伦      ns      E\r\n的      ude1    S\r\n歌      n       S\r\n台      n       E\r\n播      v       S\r\n电      a       B\r\n台      n       E\r\n听      v       S\r\n电      a       B\r\n台      n       E\r\n我      rr      S\r\n想      v       S\r\n听      v       S\r\n电      a       B\r\n...\r\n\r\n**Q1: “周杰伦”是一首歌曲还是一个人名**\r\n其中 “放周杰伦”词性标注为 “放/v, 周/nt,  杰/nt, 伦/nt”\r\n其中 “放周杰伦的歌”词性标注为 “放/v, 周/ns,  杰/ns, 伦/ns, 的/ude1, 歌/n”\r\n这里的周杰伦分别为nt,ns属性，nt和ns只是用来测试而已，其目的是能根据上下文件确定“周杰伦”是一首歌曲还是一个人名。\r\n\r\n**QA:“周杰伦”被拆分成“周 杰 伦”三个字后，每个字应该标注什么词性？**\r\n周杰伦本身是一个人名，拆分成“周 杰 伦”三个字后，应该如何对每个字进行词性标注呢？\r\n\r\n**Q3:在hanlp在，通过CRF进行词性标注会不会生效？**\r\n我发现hanlp的词性标注是通过自定义字典实现的，CRF本身只是分词，并没有进行词性标注。\r\n1）如果我对CRF的模型本身已经加入词性一列进行训练，那通过hanlp的CRF能否标注出词性，试了一下，发现是没有的，用的是上面的例子，所有字词词性都是null\r\n2）我理解CRF本身是不需要词典的（或者说词典本身就在训练的语料中）,而hanlp中的CRF似乎使用了core字典。\r\n3）hanlp的CRF分词器没有使用实体识别，这是为什么呢？",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/472/comments",
    "author": "hx78",
    "comments": [
      {
        "user": "530154436",
        "created_at": "2017-04-13T11:42:26Z",
        "body": "多看看人家源码，尤其是那个viterbi分词器"
      },
      {
        "user": "hankcs",
        "created_at": "2017-04-13T20:47:26Z",
        "body": "1. 你使用的example混淆了词性标注和分词两个过程。如果设计者认为这是个“joint model”的话，那也错了，因为CRF只预测一列。通常做法是将词性附加在BMES后面：B_nr\r\n2. 字没有词性，或者说应该继承词的词性\r\n3. 对的，本来就应该是两个过程。"
      },
      {
        "user": "hx78",
        "created_at": "2017-04-14T03:21:13Z",
        "body": "@hankcs \r\n感谢。\r\nQ1: CRF比HMM有更好的消歧效果，请问你怎么没有采用CRF进行词性标注，而用HMM呢？是考虑到性能的问题吗？\r\nQ2: 我理解如果要用CRF进行词性标注有两种做法\r\n1）词性附加在BMES后面：B_nr，这个改动比较大\r\n2）训练两个模型，一个用于分词，一个用来做词性标注\r\n我疑惑的是，进行词性标注之前，必须要先分词，我这两个模型的结果如何统一起来呢？\r\n即如何对分好的词进行词性标注"
      },
      {
        "user": "hx78",
        "created_at": "2017-04-27T06:15:49Z",
        "body": "**mark**\r\n用43万个句子的语料（约25M，约100个句式）训练了一个音乐的CRF分词模型，在本地用虚拟机，开9个G的内存，跑了90分钟才训练好一个模型，但分词的效果并没有达到想要的效果；本想增大语料（对语料进行复制）以加强每种句式的概率，但发现仅仅是复制一次，即约50M，86万个句子，开了10个G也跑不起来，只能作罢。\r\n总的来说，CRF能考虑每个字词的上下文，然后进行分词，这样的分词是很合理的，我认为这里的模型分词的结果不理想和音乐的歌名、歌手等专有名词本身没有规律所造成的，也就是说CRF学习到的规律本身就是不正确的。对于音乐这些语料，我觉得已经不适用CRF来分词，反而用字典分词达到的效果要好得多，也易于控制。"
      },
      {
        "user": "hankcs",
        "created_at": "2017-04-27T13:55:21Z",
        "body": "Q1\r\n1. 目前单线程CRF的性能是20+万字每秒，HMM是200+，两者相差一个数量级\r\n1. CRF一个模型几百兆，HMM几兆，两个数量级\r\n所以可以说目前CRF\\GLM\\PA是超过了一般人的心理价位的，过个几年可能会有所改观。\r\n\r\nQ2\r\n如果是B_nr的话，则是完美的统一。\r\n\r\n歌名的话，跟普通短语没什么区别的。比如“今天你要嫁给我”是一首歌，“明天你要嫁给我”则不是，的确没有任何统计规律在里面。\r\n"
      }
    ]
  },
  {
    "number": 461,
    "title": "“听电台北京怀旧金曲”VS“听电台北京音乐频道”",
    "created_at": "2017-03-31T09:10:26Z",
    "closed_at": "2017-04-14T03:39:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/461",
    "body": "调用Segment segment = new NShortSegment().enableCustomDictionary(true);\r\n\r\n1. **听电台北京怀旧金曲-->[听/v, 电/n, 台北/ns, 京/b, 怀旧/vn, 金曲/nz]**\r\n打印词图：========按终点打印========\r\nto:  1, from:  0, weight:04.60, word:始##始@听\r\nto:  2, from:  1, weight:05.49, word:听@电\r\nto:  3, from:  1, weight:10.85, word:听@电台\r\nto:  4, from:  2, weight:10.35, word:电@台\r\nto:  5, from:  2, weight:02.90, word:电@未##地\r\nto:  6, from:  3, weight:11.44, word:电台@北\r\nto:  6, from:  4, weight:10.57, word:台@北\r\nto:  7, from:  3, weight:04.54, word:电台@未##地\r\nto:  7, from:  4, weight:05.39, word:台@未##地\r\nto:  8, from:  5, weight:04.28, word:未##地@京\r\nto:  8, from:  6, weight:10.84, word:北@京\r\nto:  9, from:  7, weight:07.56, word:未##地@怀\r\nto:  9, from:  8, weight:11.18, word:京@怀\r\nto: 10, from:  7, weight:09.53, word:未##地@怀旧\r\nto: 10, from:  8, weight:11.18, word:京@怀旧\r\nto: 11, from:  9, weight:11.50, word:怀@旧\r\nto: 12, from: 10, weight:11.56, word:怀旧@金\r\nto: 12, from: 11, weight:11.27, word:旧@金\r\nto: 13, from: 10, weight:04.24, word:怀旧@金曲\r\nto: 13, from: 11, weight:11.27, word:旧@金曲\r\nto: 14, from: 12, weight:10.94, word:金@曲\r\nto: 15, from: 13, weight:11.58, word:金曲@末##末\r\nto: 15, from: 14, weight:03.15, word:曲@末##末\r\n\r\n粗分结果[听/v, 电/n, 台北/ns, 京/b, 怀旧/vn, 金曲/nz]\r\n人名角色观察：[  A 22202445 ][听 L 75 K 22 C 5 M 2 D 1 ][电 E 20 K 19 C 14 L 6 D 3 ][台北 K 1 ][京 C 305 E 274 D 97 B 24 ][怀旧 L 4 ][金曲 A 22202445 ][  A 22202445 ]\r\n人名角色标注：[ /A ,听/K ,电/E ,台北/K ,京/B ,怀旧/L ,金曲/A , /A]\r\n粗分结果[听/v, 电台/nis, 北京/ns, 怀旧/vn, 金曲/nz]\r\n人名角色观察：[  A 22202445 ][听 L 75 K 22 C 5 M 2 D 1 ][电台 K 3 ][北京 K 37 L 28 ][怀旧 L 4 ][金曲 A 22202445 ][  A 22202445 ]\r\n人名角色标注：[ /A ,听/K ,电台/K ,北京/K ,怀旧/L ,金曲/A , /A]\r\n[听/v, 电/n, 台北/ns, 京/b, 怀旧/vn, 金曲/nz]\r\n\r\n其中有一些边是找不到的，这个数值哪来的呢\r\n“ ”@听 181\r\n听@电 12 \r\n听@电台 0\r\n电@台 0\r\n电@台北 354\r\n电台@北 0\r\n电台@北京 5 **查不到这个路径**\r\n台@北 0\r\n台@北京 21 **查不到这个路径**\r\n台北@京 8 **查不到这个路径**\r\n北@京 0\r\n北京@怀 8  **查不到这个路径**\r\n北京@怀旧 0\r\n京@怀旧 0\r\n怀@旧 0\r\n怀旧@金 0\r\n怀旧@金曲 2\r\n旧@金曲 0\r\n金@曲 0\r\n金曲@“ ” 0\r\n曲@“ ” 9\r\n\r\n\r\n2. **听电台北京音乐频道-->[听/v, 电台/nis, 北京/ns, 音乐/n, 频道/n]**\r\n打印词图：========按终点打印========\r\nto:  1, from:  0, weight:04.60, word:始##始@听\r\nto:  2, from:  1, weight:05.49, word:听@电\r\nto:  3, from:  1, weight:10.85, word:听@电台\r\nto:  4, from:  2, weight:10.35, word:电@台\r\nto:  5, from:  2, weight:02.90, word:电@未##地\r\nto:  6, from:  3, weight:11.44, word:电台@北\r\nto:  6, from:  4, weight:10.57, word:台@北\r\nto:  7, from:  3, weight:04.54, word:电台@未##地\r\nto:  7, from:  4, weight:05.39, word:台@未##地\r\nto:  8, from:  5, weight:04.28, word:未##地@京\r\nto:  8, from:  6, weight:10.84, word:北@京\r\nto:  9, from:  7, weight:05.53, word:未##地@音\r\nto:  9, from:  8, weight:11.18, word:京@音\r\nto: 10, from:  7, weight:05.12, word:未##地@音乐\r\nto: 10, from:  8, weight:11.18, word:京@音乐\r\nto: 11, from:  9, weight:11.52, word:音@乐\r\nto: 12, from: 10, weight:11.02, word:音乐@频\r\nto: 12, from: 11, weight:11.33, word:乐@频\r\nto: 13, from: 10, weight:04.64, word:音乐@频道\r\nto: 13, from: 11, weight:11.33, word:乐@频道\r\nto: 14, from: 12, weight:11.46, word:频@道\r\nto: 15, from: 13, weight:05.94, word:频道@末##末\r\nto: 15, from: 14, weight:04.92, word:道@末##末\r\n\r\n粗分结果[听/v, 电台/nis, 北京/ns, 音乐/n, 频道/n]\r\n人名角色观察：[  A 22202445 ][听 L 75 K 22 C 5 M 2 D 1 ][电台 K 3 ][北京 K 37 L 28 ][音乐 L 9 ][频道 K 19 ][  A 22202445 ]\r\n人名角色标注：[ /A ,听/K ,电台/K ,北京/K ,音乐/L ,频道/K , /A]\r\n粗分结果[听/v, 电/n, 台北/ns, 京/b, 音乐/n, 频道/n]\r\n人名角色观察：[  A 22202445 ][听 L 75 K 22 C 5 M 2 D 1 ][电 E 20 K 19 C 14 L 6 D 3 ][台北 K 1 ][京 C 305 E 274 D 97 B 24 ][音乐 L 9 ][频道 K 19 ][  A 22202445 ]\r\n人名角色标注：[ /A ,听/K ,电/E ,台北/K ,京/B ,音乐/L ,频道/K , /A]\r\n[听/v, 电台/nis, 北京/ns, 音乐/n, 频道/n]",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/461/comments",
    "author": "hx78",
    "comments": [
      {
        "user": "TylunasLi",
        "created_at": "2017-04-02T13:32:32Z",
        "body": "为了缓解Bigram数据稀疏问题 “北京” “台北” 被编译成了 未##地，共享了地名词语的上下文。\r\n要解决这个问题，得调整路径得分（就是修改Bigram词典）。以上下文来说，“未##地@京”费用太低，“听@电台”费用太高。"
      },
      {
        "user": "hx78",
        "created_at": "2017-04-06T02:09:10Z",
        "body": "1、问题一，下面这些路径在bigram字典是找不到的，但调试过程数值得分却不是0，而是下面的值\r\n电台@北京 5 **查不到这个路径**\r\n台@北京 21 **查不到这个路径**\r\n台北@京 8 **查不到这个路径**\r\n北京@怀 8 **查不到这个路径**\r\n\r\n2、就上下文来说两个句子“听电台北京”是完全一样的，后面的词组是怎么影响到分词结果的？\r\n\r\n**“听电台北京”**完全一样\r\n打印词图：========按终点打印========\r\nto: 1, from: 0, weight:04.60, word:始##始@听\r\nto: 2, from: 1, weight:05.49, word:听@电\r\nto: 3, from: 1, weight:10.85, word:听@电台\r\nto: 4, from: 2, weight:10.35, word:电@台\r\nto: 5, from: 2, weight:02.90, word:电@未##地\r\nto: 6, from: 3, weight:11.44, word:电台@北\r\nto: 6, from: 4, weight:10.57, word:台@北\r\nto: 7, from: 3, weight:04.54, word:电台@未##地\r\nto: 7, from: 4, weight:05.39, word:台@未##地\r\nto: 8, from: 5, weight:04.28, word:未##地@京\r\nto: 8, from: 6, weight:10.84, word:北@京\r\n"
      },
      {
        "user": "hankcs",
        "created_at": "2017-04-07T16:39:42Z",
        "body": "1. 你的问题其实 @TylunasLi 已经回答了，额外参考：\r\n\r\n```\r\nSystem.out.println(CoreBiGramTableDictionary.getBiFrequency(\"电台\", \"北京\"));\r\nSystem.out.println(CoreBiGramTableDictionary.getBiFrequency(\"电台\", Predefine.TAG_PLACE));\r\n```\r\n\r\n2. 原理还请搜索BiGram分词，可能需要很长的文章才能讲清楚。"
      }
    ]
  },
  {
    "number": 460,
    "title": "你好，DoubleArrayTrie并没有看到有添加或删除某个字词的接口",
    "created_at": "2017-03-31T08:27:35Z",
    "closed_at": "2017-04-11T01:49:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/460",
    "body": "比如“我要听雪漫频道”就被分成“我 要 听雪 漫 频道”，而我希望的是分成“我 要 听 雪漫 频道”。如果能动态地从DoubleArrayTrie结构动态删除某个字词，那我就不需要重启服务了",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/460/comments",
    "author": "hx78",
    "comments": [
      {
        "user": "TylunasLi",
        "created_at": "2017-04-02T13:00:33Z",
        "body": "DoubleArrayTrie是一种将固定词典嵌入到两个数组中的数据结构，因此难以在运行过程中动态增删条目。\r\n你可以重新调用build()方法，把修改后的DoubleArrayTrie构建出来"
      },
      {
        "user": "hankcs",
        "created_at": "2017-04-07T16:19:05Z",
        "body": "的确如此。"
      },
      {
        "user": "linyixi",
        "created_at": "2017-04-07T17:35:33Z",
        "body": "DoubleArrayTrie 提供删除操作的实现网上有很多啊"
      },
      {
        "user": "hankcs",
        "created_at": "2017-04-07T17:43:45Z",
        "body": "重点在于“代价大”上面。你可以推荐一些实现，或者做一些性能benchmark，试试动态增删的成本，或者说带来的性能下降。"
      },
      {
        "user": "hx78",
        "created_at": "2017-04-10T02:48:16Z",
        "body": "ths.目前看来对DoubleArrayTrie进行动态的增删，其实就是对DoubleArrayTrie的重建。"
      }
    ]
  },
  {
    "number": 452,
    "title": "调用crf分词的时候出现ArrayIndexOutOfBoundsException错误",
    "created_at": "2017-03-28T11:48:09Z",
    "closed_at": "2017-04-23T04:41:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/452",
    "body": "\r\n错误如下：\r\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 148\r\n\tat com.hankcs.hanlp.seg.common.Vertex.compileRealWord(Vertex.java:115)\r\n\tat com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:99)\r\n\tat com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:91)\r\n\tat com.hankcs.hanlp.seg.common.Vertex.<init>(Vertex.java:196)\r\n\tat com.hankcs.hanlp.seg.Segment.combineWords(Segment.java:380)\r\n\tat com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:221)\r\n\tat com.hankcs.hanlp.seg.CRF.CRFSegment.segSentence(CRFSegment.java:142)\r\n\tat com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)\r\n\tat test.TegTest.main(TegTest.java:33)\r\n\r\n找到是下面这句出错\r\nString str = \"什么是谷精草\"; \r\nList<Term> seg = segment.seg(str);\r\n\r\n谷精草这个东西我通过自定义词典定义为其他的词语，此处不知道出现了什么错误，希望能够得到解答，谢谢",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/452/comments",
    "author": "kelciej",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-03-30T23:45:11Z",
        "body": "我无法复现你的问题，有可能是反射词性引发的，警告已经说了现在的策略了。额外附赠一点建议，试试升级JDK1.8或其他高一点版本的JDK，现在的反射代码在1.8上面运行良好。\r\n\r\n    public void testIssue452() throws Exception\r\n    {\r\n        CustomDictionary.insert(\"谷精草\", \"其他 123\");\r\n        System.out.println(new CRFSegment().enableCustomDictionary(true).seg(\"什么是谷精草\"));\r\n    }\r\n\r\nMar 30, 2017 6:42:37 PM com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>\r\n警告: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!\r\n如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类\r\n[什么/ry, 是/vshi, 谷精草/其他]"
      },
      {
        "user": "kelciej",
        "created_at": "2017-03-31T01:30:44Z",
        "body": "先谢谢你的回答。我的jdk已经是1.8的。然后目前已经解决了这个问题，我在源代码的某些地方加上了我新增的词性。出现这个错误大概是由于 某些词 本身的词性为 null。进入自定义词性遍历的步骤时，没办法匹配词性，故而导致上述问题。\r\n不管怎么样，还是很感谢你的回答。"
      }
    ]
  },
  {
    "number": 451,
    "title": "关于如何分析句子且能够自定义的分类",
    "created_at": "2017-03-27T09:56:59Z",
    "closed_at": "2017-04-10T10:23:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/451",
    "body": "时间是很宝贵的，借用大家宝贵的时间帮我解一下疑惑，感激不尽。\r\n-----------进入正题--------------\r\n将要分析内容：B2B企业CMO的职责包括两块：一个是品牌，一个是销售线索。营销团队里边一定是有一群人是做品牌的，另一拨人做销售线索。简单说是一个部门是花钱的，另一个部门是去挣钱的，做品牌虽然不直接拿单，但能够给销售创造非常坚实的基础。\r\nB2B企业有一个很大的优势，就是有着丰富并且明确的行业洞察，比如我们要做一个真正让世界变得更美好的机器人、自动化，得包含多少我们对这个行业的见解？但像卖碳酸饮料、卖巧克力的这些2C企业，就很难去表现这些。所以你看那些大的2B企业，都在不遗余力的去树立自己意见领袖的地位。例如西门子、IBM以及包括GE等在内。\r\n\r\n希望通过上面是一篇文章的一部分，分析出以下结果：\r\n1）相关企业：IBM,GE,西门子\r\n2）相关行业：制造业\r\n3）受众类别：2B\r\n4）受众群体：管理人员、销售人员\r\n\r\n个人思路：\r\n我自己的思路是：里面出现了”IBM,GE,西门子“等词然后”西门子“又可以关联上制造业，文章中出现B2B字样，可以分析为2B，然后有CMO、营销团队可以对应上管理人员和销售人员。但是我不太明白如何将这些关联到不同的标签下。我的想法可能比较肤浅，还望指点。如果能够提供思路代码更好。\r\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/451/comments",
    "author": "potenlife",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-03-30T23:34:07Z",
        "body": "感觉是个很个性化的词性标注或命名实体识别"
      },
      {
        "user": "potenlife",
        "created_at": "2017-03-31T02:14:56Z",
        "body": "@hankcs  灰常感谢您的答复，词性标注我知道怎么弄，然后关于“命名试题识别”能否帮忙解释一下或者有没有参考思路代码。"
      },
      {
        "user": "TylunasLi",
        "created_at": "2017-04-02T13:04:26Z",
        "body": "除了第一个“相关企业识别”，更像是一种文本分类问题。\r\n你需要的这种关联关系要么自己去构造一个词典，要们用文本分类的方法，建立一个数据集，人工给文档打上标签。"
      },
      {
        "user": "potenlife",
        "created_at": "2017-04-10T10:23:03Z",
        "body": "Thanks，对我的帮助很大。"
      },
      {
        "user": "potenlife",
        "created_at": "2017-04-18T04:25:42Z",
        "body": "有没有适合入门的学习书籍。然后可以基于某个框架例如：“TensorFlow。另外我知道很多公司都是厚积薄发的例如：“百度NLP、Google”的一些产品，作为一个实施益者和使用者可能没有那些知识和投入，但是想站在巨人的肩膀上学习。"
      },
      {
        "user": "hankcs",
        "created_at": "2017-04-20T15:47:08Z",
        "body": "估计没有现成的吧……"
      },
      {
        "user": "potenlife",
        "created_at": "2017-04-21T02:31:34Z",
        "body": "多谢告知。"
      }
    ]
  },
  {
    "number": 447,
    "title": "CRF分词模型 数字（m）和 英文字符（w）",
    "created_at": "2017-03-23T12:27:21Z",
    "closed_at": "2017-03-27T04:29:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/447",
    "body": "看完您的CRF源码，有个疑惑，就是训练的时候，请问您是不是处理了一下，就是把连续的数字也换成了m进行训练，连续的英文字符换成w进行训练。",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/447/comments",
    "author": "tiandiweizun",
    "comments": [
      {
        "user": "rockyzhengwu",
        "created_at": "2017-03-24T13:58:10Z",
        "body": "我也看了下，应该是这样"
      },
      {
        "user": "hankcs",
        "created_at": "2017-03-24T14:44:45Z",
        "body": "是的"
      },
      {
        "user": "TylunasLi",
        "created_at": "2017-04-02T13:36:46Z",
        "body": "可以参看testCRF.java\r\n也就是说，在解码的时候可以直接拿到字母和数字的词性。根据这点我修改了CRFSegment以输出词性。"
      }
    ]
  },
  {
    "number": 445,
    "title": "关于是否可以组建QQ社群的小建议",
    "created_at": "2017-03-21T11:42:26Z",
    "closed_at": "2017-03-27T09:18:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/445",
    "body": "hankcs 你好，首先感谢你和你的团队以及好老板研发了HanLP并大公无私的开源。我是一个NLP的初学者（唯一的优势是java开发 经验有6年了），我通过百度找到由咱们这么一个工具，但是我有很多的疑惑，感觉没地方可以沟通，能否组织一个qq群，大家有问题可以问一些已经使用的比较熟练的高手们，然后大家都可以相互进步。也可以更好的发挥群众的力量。",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/445/comments",
    "author": "potenlife",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-03-24T15:16:54Z",
        "body": "1. 感谢建议\r\n1. 我觉得还是将GitHub作为唯一的交流平台最好。因为QQ、邮件等都是私密的，其他人看不到，搜索引擎也搜索不到。而大部分问题都是重复的，积累下来的旧问题可以在issue区或搜索引擎找到答案。\r\n1. 我个人比较忙，恐怕无法参与即时交流，大概每周末会过来看看。其他贡献者想必也是如此。另外我也不用QQ。\r\n1. 开源不是一个人的事，是所有用户的事。如果GitHub上没有人回答问题，我估计QQ群一样没人回答问题。希望大家在提问的同时，尽量翻翻旧帖子，要么能找到已有答案，要么顺手帮助一下其他未解决的用户。不要只伸手，不给予。我们不是买家和卖家的关系，是互相帮助的关系。我也会尽量先回复多回复那些乐于助人的用户，维护良好的社区氛围，形成良性循环。\r\n1. 过低的交流门槛其实并不好。有个别人士认为用了开源软件，那出现所有问题都是该开源软件造成的，社区就有义务帮他调试业务逻辑；特别是自然语言处理这种比较学术化的技术，有些术语或思想没有得到理解，有些人就会说，这个词就应该按我的标准我拍板这么分，你分的不一样还说是语料标准不同你就是在回避问题推卸责任等等。说实话我讨厌这种人，我希望这种交流少一点的好，交流门槛再提高一点比较好。"
      },
      {
        "user": "potenlife",
        "created_at": "2017-03-27T02:53:44Z",
        "body": "hankcs，我表示理解和感谢。关于“出现问题是开源软件造成且社区有义务帮助”和“标准不一，就说明有问题”这两点我赞同你的观点，我想只要是经验丰富一点的使用开源软件或工具出了问题首先想到的是如何解决问题，并不应该如何去找社区的麻烦，我是做java的用的基本上都是开源的产品，这点都明白；然后关于后者我觉得首先NLP目前也没有达到众所周知的程度，很多都在摸索阶段，而且用在每种不同的场景肯定有一些不同的定义，例如用在智能聊天和文章解析，对于同样的词语可能要的结果不一样。然后其他的问题可能每个人想法不一样吧。\r\n       关于这里面我也确实看了好多issue，有些人提问题或者描述问题，用的都是一种质问的口语和态度。我也很无法忍受，例如“xx是错的，我用xxx词语得出的结果不对，你们bug很多”，如果是我我就不会去回复这个人的问题，第一这个人没有基本的素养，第二人家没有帮助你的义务。但是你还是帮助回复了，有能力没脾气，但是我觉得这种人还需要指点和教育一下。\r\n      最后我感觉HanLP在文档、Demo结构上做的非常不错，挺清晰的。我个人感觉如果有一个稍微完整一点的成功案例demo，对于类似我这种初学者可能帮助更大，毕竟初学者跟熟练者的经验和理解差距还是挺大的。\r\n    thinks！"
      }
    ]
  },
  {
    "number": 436,
    "title": "CRF识别新词似乎不起效果",
    "created_at": "2017-03-15T09:44:19Z",
    "closed_at": "2017-03-21T04:46:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/436",
    "body": "我使用\"你看过穆赫兰道吗\"这句话做实验，没有得到结果。\r\n\r\n然后我使用同样的一篇新闻做实验，在NLPIR上面可以得到新词“国际机场” ， “马移民局”等新词，但是用hanlp无法获得。\r\n\r\n文章如下：\r\n\r\n环球网报道 记者 王敏日前，网名为乔妹的台湾女游客通过脸谱发文表示，自己在入境马来西亚时，因护照破损，受到马来西亚海关非人道对待，她不但被没收了手机、护照等私人物品，还被关押长达35个小时。\r\n“03/09晚上08：30我下飞机，我依照规定排队等候盖印出关，轮到我时，盖印人员他说我护照损坏，不能进去马来西亚，他要我等一下”\r\n乔妹表示，他回来后询问她护照为何会损坏，她解释说，在日本购物免税单被日本海关撕下时造成的破损。海关人员随即将其带入办公室表示，马来西亚不接受并将护照没收。\r\n乔妹随后被要求进入更里边的办公室，当她进入后，手机又被没收了。这时，乔妹再次被要求进入更后面的小房间里放置行李，在那里出现了一个很凶的男子。\r\n“我把外套放在手提行李上，他就把我的外套很大力的丢向我，行李箱往上丢，还叫我把钱拿出来，我不想给他钱，假装听不懂，然后才有一个女生来，就叫我把钱放口袋”\r\n乔妹随后手被铐起来带进了牢房，对方仅告诉她第二天早上可回台湾。“里面黑嘛嘛，有时才有开灯，厕所好脏也没有门，只能坐地上，累了睡地上，好多人”乔妹称她还遇到了另一个台湾人。\r\n一个细节，乔妹表示，有马来西亚工作人员向其表示，只要1000马币便能先出去，但她却分明看到墙上密密麻麻的“不要给钱，都是骗钱”的字。\r\n贴文发布后，在马来西亚，事件迅速热起，其后，台媒也相继跟进，做出了报道。马来西亚网站“辣手网”14日下午报道，马首相东亚特使兼民都鲁区国会议员张庆信认为，这件事关乎国家形象和官员素质，可能打击马来西亚旅游业。\r\n张庆信表示，遭扣留的台湾旅客不是罪犯，不该用这么不人道的方式对待台湾旅客。因此向内政部反映。而据台“联合新闻网”报道称，马国副首相兼内政部长阿末扎希得知后大怒，已下令彻查。\r\n台湾“外交部”得知消息后表示，已于第一时间主动掌握讯息，并经“驻处”洽系吉隆坡国际机场移民局执法组，转述贴文所述情节，了解当日相关情形。\r\n而马来西亚则回复，该名女性于3月9日搭乘亚洲航空班机于晚间8时30分抵达吉隆坡第2国际机场，拟入境时，因所持护照毁损，被裁定拒绝入境并予以遣返，于是在3月11日上午9时20分搭乘亚洲航空第D7372号班机返回台湾桃园国际机场。\r\n那么，马来西亚遣返的标准作业是什么呢，台“外交部”表示，根据了解，旅客经机场移民官裁定拒绝入境并予以遣返后，倘当日无法登机返回，旅客将被安置在机场之“照护室”，并尽速进行遣返。\r\n旅客在留置期间相关情形，马移民局例不知会各相关驻马“使领馆”单位，若马移民局认为有必要将旅客移送至机场附近的“临时安置所”，予以较长时间的留置或调查时，则将同步知会。\r\n事件同样很快便在社交网络上发酵，截止此文时，转发数达10280次。不少网友对乔妹的遭遇表示同情，但也有些网友质疑事件的真实性，甚至表示，乔妹并不是台湾人，所做只是为了炒作。\r\n比如有网友即认为，有人因为要红而败坏，损人不利己的行为不一定高明，更何况是损害一个国家的名誉（马来西亚）。也有网民表示自己就是马来西亚人，要求出示护照。\r\n就此，乔妹在脸谱表示，“大家一直说我想红，我想问这个很光荣吗？你们可以体会被关的心情吗？里面环境有多糟吗？”随后更是贴上护照证明自身。\r\n乔妹的遭遇另一方面则招来了不少岛内外的共感，名叫“Kuan wei pan”的网友表示自己也有过相同的经验，并称“你永远不要让这里的人知道你的口袋有多深”。\r\n也有不少马来西亚的网友为乔妹的遭遇感到抱歉，这位网友表示，自己作为一个马来西他人，都为这里的键盘侠感到心寒，这样的事情发生在谁的身上，都会崩溃的。\r\n当然也有网友看的更多些，比如这位地址显示为台北的网友这样说，如果持有的是中华人民共和国的护照，被处理的过程就会不一样。",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/436/comments",
    "author": "endymecy",
    "comments": [
      {
        "user": "yesseecity",
        "created_at": "2017-03-16T02:16:51Z",
        "body": "那就加入自訂詞典吧"
      },
      {
        "user": "hankcs",
        "created_at": "2017-03-20T01:29:59Z",
        "body": "[我, 使用, \", 你, 看过, 穆赫兰道, 吗, \", 这, 句话, 做, 实验, ，, 没有, 得到, 结果, 。, \r\n\r\n, 然后, 我, 使用, 同样, 的, 一篇, 新闻, 做, 实验, ，, 在, NLPIR, 上面, 可以, 得到, 新词, “, 国际, 机场, ”,  , ，,  , “马, 移民局, ”, 等, 新词, ，, 但是, 用, hanlp, 无法, 获得, 。]\r\n分词标准不同，没有谁规定什么叫词、“你好”是一个词还是两个词。特别是训练语料中某某机场就是两个词的时候，结果当然是两个词了。"
      },
      {
        "user": "endymecy",
        "created_at": "2017-03-21T04:46:53Z",
        "body": "你这是避重就轻了，对于你的分词器而言，“马移民局”是不是一个新词，如果是，那就是没有识别出来。哪来那么多的理由。"
      }
    ]
  },
  {
    "number": 435,
    "title": "python 词共现统计 Occurrence 如何停用 CustomDictionary",
    "created_at": "2017-03-15T07:02:08Z",
    "closed_at": "2017-03-27T08:32:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/435",
    "body": "和 DemoOccurrence 結果不同\r\n同樣都是用\r\n`在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法`\r\n\r\nDemo 的 uniGram 結果\r\n[信息=信息=1, 先进=先进=1, 图形图像=图形图像=1, 处理=处理=2, 技术=技术=1, 方面=方面=1, 比较=比较=1, 目前=目前=1, 算法=算法=2, 视频=视频=1, 计算机=计算机=1, 音视频=音视频=1]\r\n\r\n我的結果\r\n[先进=先进=1, 图形图像=图形图像=1, 处理=处理=2, 技术=技术=1, 比较=比较=1]\r\n\r\n因為我的自訂詞典中\r\n\r\n[在/p, 计算机/product, 音视频/product, 和/cc, 图形图像/nz, 技术/keyword, 等/udeng, 二/m, 维/b, 信息/product, 算法/product, 处理/keyword, 方面/product, 目前/product, 比较/keyword, 先进/a, 的/ude1, 视频/product, 处理/keyword, 算法/product]\r\n\r\nCRF有停用自訂詞典的功能\r\n`CRFSegment().enableCustomDictionary(false);`\r\n\r\n我要如何在 使用 occurrence 時停用 CustomDictionary ?\r\n\r\n\r\n-------------\r\n另外，自訂字詞我是用 LexiconUtility.setAttribute 去加詞，因為我加的詞含有 `space`\r\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/435/comments",
    "author": "yesseecity",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-03-20T01:23:49Z",
        "body": "可以通过com.hankcs.hanlp.corpus.occurrence.Occurrence#addAll(java.util.List<com.hankcs.hanlp.seg.common.Term>)加入提前过滤了停用词的单词列表。"
      },
      {
        "user": "yesseecity",
        "created_at": "2017-03-20T03:07:51Z",
        "body": "這樣子？\r\n`JClass('com.hankcs.hanlp.corpus.occurrence.Occurrence#addAll')`\r\n\r\n系統報錯\r\n`java.lang.RuntimeExceptionPyRaisable: java.lang.RuntimeException: Class com.hankcs.hanlp.corpus.occurrence.Occurrence#addAll not found`\r\n"
      },
      {
        "user": "hankcs",
        "created_at": "2017-03-24T14:52:59Z",
        "body": "不是的，这是个非静态方法，直接在对象上调用即可。我用这种方式告诉你具体路径而已。"
      }
    ]
  },
  {
    "number": 421,
    "title": "关于Project打包的问题",
    "created_at": "2017-03-09T08:06:40Z",
    "closed_at": "2017-03-24T16:23:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/421",
    "body": "我在项目里用了你的jia包（maven方式和直接下载jar的方式）我在IDE里运行都没问题，我是用Swing框架做了个图形化工具，然后打包完之后一旦运行就出问题，问题信息是：\r\n\r\n> Error: A JNI error has occurred, please check your installation and try again\r\n> Exception in thread \"main\" java.lang.SecurityException: Invalid signature file d\r\n> igest for Manifest main attributes\r\n>         at sun.security.util.SignatureFileVerifier.processImpl(Unknown Source)\r\n>         at sun.security.util.SignatureFileVerifier.process(Unknown Source)\r\n>         at java.util.jar.JarVerifier.processEntry(Unknown Source)\r\n>         at java.util.jar.JarVerifier.update(Unknown Source)\r\n>         at java.util.jar.JarFile.initializeVerifier(Unknown Source)\r\n>         at java.util.jar.JarFile.getInputStream(Unknown Source)\r\n>         at sun.misc.URLClassPath$JarLoader$2.getInputStream(Unknown Source)\r\n>         at sun.misc.Resource.cachedInputStream(Unknown Source)\r\n>         at sun.misc.Resource.getByteBuffer(Unknown Source)\r\n>         at java.net.URLClassLoader.defineClass(Unknown Source)\r\n>         at java.net.URLClassLoader.access$100(Unknown Source)\r\n>         at java.net.URLClassLoader$1.run(Unknown Source)\r\n>         at java.net.URLClassLoader$1.run(Unknown Source)\r\n>         at java.security.AccessController.doPrivileged(Native Method)\r\n>         at java.net.URLClassLoader.findClass(Unknown Source)\r\n>         at java.lang.ClassLoader.loadClass(Unknown Source)\r\n>         at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\r\n>         at java.lang.ClassLoader.loadClass(Unknown Source)\r\n>         at sun.launcher.LauncherHelper.checkAndLoadMain(Unknown Source)\r\n\r\n我在网上查了很多种方法 都没办法，就是很多人说了在maven文件里加这些：\r\n\r\n```\r\n\r\n> <artifact>*:*</artifact>\r\n> <excludes>\r\n>         <exclude>META-INF/*.SF</exclude>\r\n>          <exclude>META-INF/*.DSA</exclude>\r\n>           <exclude>META-INF/*.RSA</exclude>\r\n> </excludes>\r\n```\r\n\r\n可是无论怎么样都不行。请问这是什么问题，等待回复。。。",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/421/comments",
    "author": "Bahramudin",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-03-09T18:59:33Z",
        "body": "从错误信息来看跟jar包没有关系\r\nError: A JNI error has occurred, please check your installation and try again\r\n看样子是Java Native Interface，也就是本地C代码有问题。应该有个什么库用了JNI，然后出错了。"
      },
      {
        "user": "Bahramudin",
        "created_at": "2017-03-10T02:26:22Z",
        "body": "我知道JNI，可是问题是当我用你的这个库的时候就出问题了，如果用别的库的话 就没事儿，肯定是和你的库有关系，当我把你的库删除了再打包运行就欧克完全能运行。我不是新手，我非常了解Java，可是这个我怎么研究了都没法解决了，所以问你来了 看看是怎么办？"
      },
      {
        "user": "Bahramudin",
        "created_at": "2017-03-10T08:37:22Z",
        "body": "请把这问题给我看一下是怎么解决，就是跟你的库有关系，等我不用的你的库的时候就好好的\r\n"
      }
    ]
  },
  {
    "number": 406,
    "title": "关于demo",
    "created_at": "2017-02-28T08:06:34Z",
    "closed_at": "2017-03-07T15:13:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/406",
    "body": "可不可以提供更多关于train和evaluate的demo？这样用户可以根据自己的语料进行训练。wiki中给出的demo不够系统。",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/406/comments",
    "author": "endymecy",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-03-02T18:20:36Z",
        "body": "其实训练就只有这么一点代码，没什么复杂的。另外Github上已有第三方的cws_evaluation项目可供参考。"
      }
    ]
  },
  {
    "number": 378,
    "title": "hankcs，多模式字符串匹配由于模式不是特别多，我觉得不需要使用自动机，坏字符跳转预处理后，使用一个hashmap就可以了",
    "created_at": "2017-01-05T04:50:23Z",
    "closed_at": "2019-09-17T09:37:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/378",
    "body": "",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/378/comments",
    "author": "LukeGoldberg",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2017-01-05T05:13:51Z",
        "body": "欢迎自由修改和pull request"
      }
    ]
  },
  {
    "number": 359,
    "title": "Viterbi 算法中求解最大概率都变成加法，非乘法",
    "created_at": "2016-12-02T01:25:11Z",
    "closed_at": "2016-12-05T03:25:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/359",
    "body": "com.hankcs.hanlp.algoritm 下的Viterbi算法求解最大概率都变成加法，这是为何？`V[0][y] = start_p[y] + emit_p[y][obs[0]];`",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/359/comments",
    "author": "shanghai-Jerry",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-12-02T16:01:40Z",
        "body": "因为都提前取了对数。"
      },
      {
        "user": "shanghai-Jerry",
        "created_at": "2016-12-05T03:25:46Z",
        "body": "哦， 明白了。 可好像取的概率为最小概率， 难道是取完对数后添加了负号（- log）"
      },
      {
        "user": "zeekvfu",
        "created_at": "2018-03-08T07:39:46Z",
        "body": "@hankcs \r\nhello，请教下：\r\nViterbi 解码时，把乘法运算转化为加法运算，目的是为了减小计算开销么？\r\n因为加法运算比乘法运算开销小。虽然需要取对数，但是只要提前计算一次即可。\r\n是这样么？"
      },
      {
        "user": "hankcs",
        "created_at": "2018-03-08T13:50:56Z",
        "body": "不是，而是为了防止下溢出。"
      },
      {
        "user": "zeekvfu",
        "created_at": "2018-03-08T15:55:43Z",
        "body": "@hankcs Thanks :-)"
      }
    ]
  },
  {
    "number": 338,
    "title": "ExtractSummary的问题",
    "created_at": "2016-11-09T01:27:01Z",
    "closed_at": "2016-11-09T15:23:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/338",
    "body": "您好，非常感谢您提供这么好的开源项目，但是在看ExtractSummary时发现，对博文中这个例子做summary，得到的结果是：[无限算法的产生是由于未能确定的定义终止条件, 这类算法在有限的时间内终止, 有限的非确定算法]，和文中所说不一致，请问问题出在哪？谢谢",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/338/comments",
    "author": "bingyupiaoyao",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-11-09T02:14:17Z",
        "body": "不客气。这是正常的，最初版本的分词和停用词与现在相比都发生了变化。\n"
      },
      {
        "user": "bingyupiaoyao",
        "created_at": "2016-11-09T15:23:54Z",
        "body": "感谢答复！\n"
      }
    ]
  },
  {
    "number": 321,
    "title": "标准分词，最短路分词，N最短路分词的区别",
    "created_at": "2016-10-09T07:54:58Z",
    "closed_at": "2016-10-10T03:13:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/321",
    "body": "我原来的理解是：\n标准分词就是用动态规划去算词图的最短路径\n最短路分词就是用Dijkstra算法算词图的最短路径\nN最短路就是用那个什么NShortPath算法算词图的最短路径\n这些分词方式仅仅是算法不同，最终的分词效果是一样的。\n\n但文档中说：\n\n```\nN最短路分词器NShortSegment比最短路分词器慢，但是效果稍微好一些，对命名实体识别能力更强。\n```\n\n才知道这些方式可能不仅仅是求最短路径的算法有区别，最终的分词效果都是会不一样的。\n\n我想请问一下：\n1. 这三个分词算法有什么区别？\n2. 为什么能出不同的分词效果？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/321/comments",
    "author": "konghuarukhr",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-10-09T15:25:50Z",
        "body": "标准分词与最短路效果是一样的，NShortPath在N条最短路上跑NER，所以命名实体召回率稍高一些。\n"
      },
      {
        "user": "konghuarukhr",
        "created_at": "2016-10-10T03:13:29Z",
        "body": "了解了。谢谢！\n"
      }
    ]
  },
  {
    "number": 312,
    "title": "用户词库的问题",
    "created_at": "2016-09-12T07:32:45Z",
    "closed_at": "2016-09-13T09:35:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/312",
    "body": "目前自带的CustomDictionary.txt.bin是17M多，我在CustomDictionary.txt中添加自定义词也可以起作用，但是新的自动生成的CustomDictionary.txt.bin只有685K，而且分词效果下降了，请问17M的CustomDictionary.txt.bin是如何生成的？还是需要运行什么函数？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/312/comments",
    "author": "Icomming",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-09-13T03:07:21Z",
        "body": "#自定义词典路径，用;隔开多个自定义词典，空格开头表示在同一个目录，使用“文件名 词性”形式则表示这个词典的词性默认是该词性。优先级递减。\n#另外data/dictionary/custom/CustomDictionary.txt是个高质量的词库，请不要删除\nCustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf\n"
      },
      {
        "user": "Icomming",
        "created_at": "2016-09-13T09:35:10Z",
        "body": "非常感谢，是我路径没设对，问题解决。\n"
      }
    ]
  },
  {
    "number": 306,
    "title": "关于模型问题",
    "created_at": "2016-08-18T02:56:30Z",
    "closed_at": "2016-08-18T05:50:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/306",
    "body": "@hankcs 您好。 请问输入模型是否可以缩减尺寸？在Android设备上跑离线版本，发现使用依存语法接口的时候，载入模型有800M大小，导致设备不能正常运行。请问模型加载过程是用到相应接口的时候再加载还是初始化的时候统一加载？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/306/comments",
    "author": "Sharon-Shao",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-08-18T02:59:48Z",
        "body": "1. 是用到相应接口的时候再加载\n2. 但依存句法模型光文件都有几百兆，在Android设备上还是够呛的，不知道Android单app的内存限制是多少\n"
      },
      {
        "user": "Sharon-Shao",
        "created_at": "2016-08-18T05:50:58Z",
        "body": "1、对于手机来说，android手机单app内存限制是512M，最大可以达到1G，不过一般不推荐使用超过512M。加载依存句法模型之后，光首次加载就超过800M了，程序显示out of memory。\n"
      },
      {
        "user": "Sharon-Shao",
        "created_at": "2016-08-18T05:56:57Z",
        "body": "请问能否通过自己训练模型，达到减小模型占用的目的？\n"
      },
      {
        "user": "Sharon-Shao",
        "created_at": "2016-08-18T09:13:21Z",
        "body": "@hankcs \n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-08-18T10:53:06Z",
        "body": "1. 你说首次加载就超过800M了，是否意味着加载成功？\n2. 如果加载成功的话你应该限制一下句子长度\n3. 重新训练不现实，你没有语料\n4. 有几个小模型，maxent之类的，但效果不好\n"
      },
      {
        "user": "Sharon-Shao",
        "created_at": "2016-08-19T02:22:17Z",
        "body": "1.首次加载不包括依存语法调用，只是分词功能。只是在后面的依存语法调用中，Android系统显示out of memory。\n2.句子长度应该限制在多少长度以下？复杂度有要求否？\n\nnew question：\n1.发现在使用HMM segment的时候，new Segment对象花了8.298秒的时间，这个时间延迟是不是太大？\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-08-19T02:52:23Z",
        "body": "1. 分词用不了那么多内存的，你是否在用CRF分词。或者你看到的应该包含了其他Android相关组件占用的内存\n2. 可以信手给个，100字。\n3. HMM2模型同样很大\n"
      },
      {
        "user": "Sharon-Shao",
        "created_at": "2016-08-19T08:06:19Z",
        "body": "如果我有语料，如何进行训练得到模型？您的介绍中并未提及。非常感谢！\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-08-20T00:50:51Z",
        "body": "NNParser训练部分并没有移植。\n"
      }
    ]
  },
  {
    "number": 305,
    "title": "Hanlp Segment类seg分词方法报内存溢出错误",
    "created_at": "2016-08-17T09:37:26Z",
    "closed_at": "2016-08-17T20:35:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/305",
    "body": "在使用Hanlp对获取的文本进行预处理的时候，当处理的是重复的相同内容文本时正常，当处理大量不同文本时报内存溢出的错误。\n`for (File txt : files) {\n                    content = readtxt(txt.getAbsolutePath());\n                    // 分词\n                    /*System.out.println(content);*/\n                    word_arr = StopWordsHandler.mPhraseDel(segmentNoTag.seg(content));\n                /*  word_arr = new ArrayList<String>();\n                    for (int i = 0; i < 10000; i++){\n                        word_arr.add(\"大冒险\");\n                    }*/\n                    //System.out.println(\"content:\"+content+\"；size：\"+word_arr.size());\n                    fileContent.add(StringUtils.strip(word_arr.toString().replaceAll(\",\", \"\\t\"),\"[]\")+\"\\t\");\n                    count++;\n                }`\n如果把content换成固定字符串就不会报错。请问是什么原因呢？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/305/comments",
    "author": "JoyChung",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-08-17T20:35:12Z",
        "body": "你把这么多文件拼凑成这么大一个字符串，是不对的。\n按文件或按行处理。\n"
      },
      {
        "user": "JoyChung",
        "created_at": "2016-08-18T01:08:50Z",
        "body": "我是按每个文件处理分词的，只是在分词之后才存在一起，每个文件的大小不过三四行。\n"
      },
      {
        "user": "JoyChung",
        "created_at": "2016-08-18T01:10:56Z",
        "body": "我需要对每个文件进行按行处理么？但是我把content换成报错处的那个文件的内容，运行2000次循环也不会报错，就是用content就不行\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-08-18T01:13:15Z",
        "body": "每个文件进行按行处理\n"
      },
      {
        "user": "JoyChung",
        "created_at": "2016-08-18T01:39:20Z",
        "body": "可是我在执行读取文件后，把content换成两到三个文件内容拼成的字符串也不会报错呀\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-08-18T02:45:20Z",
        "body": "事实上，你给定一个JVM内存，比如说256MB吧，总有一个足够长的字符串能让任何程序OOM，更别说分词器了。\n你的这些文本文件中肯定有一个达到了这个阈值。\n"
      },
      {
        "user": "JoyChung",
        "created_at": "2016-08-18T02:59:32Z",
        "body": "嗯嗯，找到了，那我设个阈值吧，谢谢你的耐心回答^-^~~\n"
      }
    ]
  },
  {
    "number": 251,
    "title": "mac下 heap Size",
    "created_at": "2016-06-28T00:57:47Z",
    "closed_at": "2016-06-28T03:11:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/251",
    "body": "很有可能我是在问蠢问题：\n我在windows运行无任何问题，1000次跑下来benchmark在122mb左右（自己的程序）\n但帮助其他人部署在Mac下出现了heap size 问题\n使用Intellij和Eclipse均无法跑起Demo1\n-Xmx6g 传入VMoption还是无果，\n-Xmx4g -Xmx2g -Xmn2g也已经试过\n\n`\nSystem.out.println(HanLP.segment(\"你好，欢迎使用HanLP汉语处理包！\"));\n`\n\n`usr/lib/jvm/java-8-oracle/bin/java -d64 -Xmx6g -Didea.launcher.port=7534 -Didea.launcher.bin.path=/home/master/Prog/idea-IU-145.1617.8/bin -Dfile.encoding=UTF-8 -classpath /usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/deploy.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jfxrt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-8-oracle/jre/lib/javaws.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfxswt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/management-agent.jar:/usr/lib/jvm/java-8-oracle/jre/lib/plugin.jar:/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/home/master/Documents/<Project Name>/target/classes:/home/master/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/master/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/master/Documents/<Project Name>/lib/hanlp-1.2.9.jar:/home/master/Prog/idea-IU-145.1617.8/lib/idea_rt.jar com.intellij.rt.execution.application.AppMain Main\nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n    at com.hankcs.hanlp.dictionary.CoreDictionary.loadDat(CoreDictionary.java:141)\n    at com.hankcs.hanlp.dictionary.CoreDictionary.load(CoreDictionary.java:63)\n    at com.hankcs.hanlp.dictionary.CoreDictionary.<clinit>(CoreDictionary.java:40)\n    at com.hankcs.hanlp.seg.common.Vertex.<clinit>(Vertex.java:56)\n    at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:71)\n    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)\n    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:441)\n    at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)\n    at com.hankcs.hanlp.HanLP.segment(HanLP.java:386)\n    at Main.main(Main.java:30)\n`\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/251/comments",
    "author": "jackalsin",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-06-28T02:03:56Z",
        "body": "我也在使用mac，一切正常：\n\n```\n/Library/Java/JavaVirtualMachines/jdk1.8.0_20.jdk/Contents/Home/bin/java -Xms120m -Xmx120m -Xmn64m -Didea.launcher.port=7533 \"-Didea.launcher.bin.path=/Applications/IntelliJ IDEA.app/Contents/bin\"\n```\n\n 我怀疑你拷贝过去的data破损了。你可以从代码库clone一份，重新生成bin。\n"
      },
      {
        "user": "jackalsin",
        "created_at": "2016-06-28T02:08:15Z",
        "body": "感谢回复，正在测试\n\n其实我问题很诡异，RedHat也没问题，Ubuntu却有问题\n"
      },
      {
        "user": "jackalsin",
        "created_at": "2016-06-28T02:51:46Z",
        "body": "多谢，已经解决，问题如下：\n每一次新的data的字典会自动创建.txt.bin文件，此部分文件需要在每一台新的机子上重新编译，不可以使用git lfs track\n多谢回复。\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-06-28T03:11:38Z",
        "body": "好的，二进制bin文件不应该提交到版本库。git有可能在其中插入了解决冲突的标记，这破坏了文件。\n"
      }
    ]
  },
  {
    "number": 250,
    "title": " 关于机构分词学习新词问题",
    "created_at": "2016-06-26T06:50:40Z",
    "closed_at": "2016-10-14T07:55:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/250",
    "body": "读了您写的文档，在使用如何使用功能写的非常详细。自定义的公司名语料怎么添加到机构分词模型中。\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/250/comments",
    "author": "chenyishun",
    "comments": [
      {
        "user": "skedin",
        "created_at": "2016-06-27T00:32:37Z",
        "body": "In `hanlp.properties`, add a dict in path `CustomDictionaryPath`, you can refer to `./hanlp-1.2.10/data/dictionary/custom/机构名词典.txt`.\n"
      },
      {
        "user": "chenyishun",
        "created_at": "2016-06-27T08:54:57Z",
        "body": "我用了您说的这种方式，新增了一个“公司名.txt” 在目data/dictionary/custom/目录下。然后在hanlp.properties CustomDictionaryPath追加了“;公司名.txt nt”,新加的那些词都不生效。以前的老词可以用。看上去像要编译一样。我看了一下这里面还有一些是.bin文件。是不是txt编译出来就是.bin文件?\n"
      },
      {
        "user": "ghost",
        "created_at": "2016-06-27T08:56:13Z",
        "body": "把bin删掉。在重启下。\n"
      },
      {
        "user": "chenyishun",
        "created_at": "2016-06-27T09:01:13Z",
        "body": "删掉以后报错了\n六月 27, 2016 4:57:47 下午 com.hankcs.hanlp.corpus.io.IOUtil readBytes\n警告: 读取/Users/Yishun.Chen/Downloads/data/dictionary/custom/CustomDictionary.txt.bin时发生异常java.io.FileNotFoundException: \n不是是删了后还要用个什么重新生成一下\n"
      },
      {
        "user": "ghost",
        "created_at": "2016-06-27T09:03:07Z",
        "body": "不要紧，你看看重启完整后，是不是有个新的bin了\n"
      },
      {
        "user": "chenyishun",
        "created_at": "2016-06-27T09:08:50Z",
        "body": "不知道你是重启是指什么。我是main函数调用的。没有新的bin\n"
      },
      {
        "user": "chenyishun",
        "created_at": "2016-06-27T09:20:09Z",
        "body": "是我错了。报错后。程序还有运行中。新的bin在创建中，重新生成后就OK了。万分感谢\n"
      }
    ]
  },
  {
    "number": 246,
    "title": "hankcs, 为啥在CoreNatureDictionary.tr.txt里没有Yg标签啊？",
    "created_at": "2016-06-22T06:11:40Z",
    "closed_at": "2019-09-17T09:37:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/246",
    "body": "是因为它出现的次数老少了吗，在那个北大标注集里有这个标签的\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/246/comments",
    "author": "LukeGoldberg",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-06-22T06:45:29Z",
        "body": "因为它在2014人民日报里面根本没出现过。这个标签是为了兼容其他语料库刻意放到代码里的。\n"
      }
    ]
  },
  {
    "number": 243,
    "title": "用户词典词性问题",
    "created_at": "2016-06-15T05:14:32Z",
    "closed_at": "2016-06-16T07:36:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/243",
    "body": "用户词典采用默认词性，也就是nz，能否在分词的时候，自动识别新添加词的词性，而不是全部都是名词？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/243/comments",
    "author": "breakjiang",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-06-16T07:36:52Z",
        "body": "不能\n"
      }
    ]
  },
  {
    "number": 187,
    "title": "如何设置替换匹配",
    "created_at": "2016-04-13T08:08:39Z",
    "closed_at": "2016-04-16T03:32:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/187",
    "body": "我现在在自定义字段中设置了“中国好声音第一季”为音乐专辑类型， 我想后面无论是“中国好声音第二季”还是“中国好声音第三季”等等都可以识别为音乐专辑类型，这个该如何去做？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/187/comments",
    "author": "kingdeewang",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-04-16T03:32:14Z",
        "body": "正则\n"
      },
      {
        "user": "kingdeewang",
        "created_at": "2016-04-19T07:14:18Z",
        "body": "你好，能否详细说明一下？\n"
      },
      {
        "user": "intohole",
        "created_at": "2016-04-20T02:19:12Z",
        "body": "就是你自己写规则去匹配， 现在没有这么智能的算法，去实现你说的功能\n"
      }
    ]
  },
  {
    "number": 177,
    "title": "提取关键词",
    "created_at": "2016-04-05T03:57:40Z",
    "closed_at": "2016-04-08T01:25:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/177",
    "body": "我有大量如下格式的文本\n\n```\n姓名：张三性别：男年龄18身-高180cm发 型-飞机头母亲姓名：张妈妈......\n```\n\n这里的“姓名 性别 年龄 身高 发型”仅仅为了说明，实际上有大量的类似关键词 ，且每个文本中关键词出现的次数是不定的(有的包含其中10个 有的包含其中一个) 但每个关键词最多只会出现一次(姓名和母亲姓名认为是两个关键词)\n希望能够实现提取到所有关键词(移除关键词内的特殊符号如空格、-等)列表，同时能够在关键词处添加分隔符\n如下\n\n```\n姓名：张三 性别：男 年龄18 身高180cm 发型-飞机头 母亲姓名：张妈妈......\n```\n\n是否可以使用您提供的库实现，可否给予一些专业意见\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/177/comments",
    "author": "wanghaisheng",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-04-07T21:01:15Z",
        "body": "你对关键词的理解恐怕与一般意义不同，另外分隔符之类的属于用户自己的业务逻辑。\n"
      },
      {
        "user": "wanghaisheng",
        "created_at": "2016-04-08T01:25:02Z",
        "body": "不好意思 困扰之处 这种应该如何表达才专业呢\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-04-08T02:08:47Z",
        "body": "你认为关键字是由你决定的，而不是算法决定的，那么你应该自己写规则规定哪些是，哪些不是。\n"
      },
      {
        "user": "wanghaisheng",
        "created_at": "2016-04-08T03:09:08Z",
        "body": "也不是 我认为关键词是算法去发现和筛选 我来决定\n"
      }
    ]
  },
  {
    "number": 169,
    "title": "三元组提取的问题",
    "created_at": "2016-03-30T08:51:19Z",
    "closed_at": "2019-01-08T06:01:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/169",
    "body": " @hankcs 感谢你开发的HanLP工具，实在太棒了！\n\n我刚学NLP，有个问题想向你请教。\n我有这么个需求，输入是一个句子，输出是能表达句子语义的三元组（主谓宾）。例如：\n\n原句：格言是简练而含义深刻并具有教育意义的警句。\n结果：（格言，定义，简练而含义深刻并具有教育意义的警句）\n\n原句：儿化具有区别词义、区分词性和表示感情色彩的作用。\n结果：（儿化，作用，区别词义、区分词性和表示感情色彩）\n\n原句：点号又分句末点号和句内点号。\n结果：（点号，包含，句末点号和句内点号）\n\n我看过你写的提取句子主谓宾的项目，所以我打算用类似的方法来做，但是自己刚入门NLP，不知道行不行。你有什么方法或者建议么？\n\n谢谢！\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/169/comments",
    "author": "javayhu",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-04-02T23:40:41Z",
        "body": "大致就是句法分析+规则吧\nROOT一般是谓语，左右两边找主语和宾语，当然还涉及到很多规则修正\n"
      },
      {
        "user": "javayhu",
        "created_at": "2016-04-03T02:49:53Z",
        "body": "@hankcs 谢谢你。我还想问下，类似word embeding或者神经网络等技术对这个需求会有作用么？或者说作用大么？\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-04-03T04:00:55Z",
        "body": "据据Chen and Manning试验，对句法分析的效果提升很大。\n"
      },
      {
        "user": "ljdawn",
        "created_at": "2016-07-26T01:59:03Z",
        "body": "hankcs你好。请问下三元组融合有啥好的办法么？只能靠自己写规则？  谢谢。\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-07-27T12:57:55Z",
        "body": "这实际上属于知识提取，据我所知，能发挥实际作用的方案都是基于规则的。\n"
      },
      {
        "user": "ljdawn",
        "created_at": "2016-08-02T10:10:51Z",
        "body": "@hankcs  谢谢。。。 写规则实在是太😖。。天天看数据\n"
      },
      {
        "user": "crapthings",
        "created_at": "2019-01-02T07:49:11Z",
        "body": "@hujiaweibujidao @ljdawn 你们这个问题研究的怎么样了哦"
      },
      {
        "user": "javayhu",
        "created_at": "2019-01-08T02:31:13Z",
        "body": "@crapthings 已经毕业了，不搞了。当时还是采用了人工标注的方式处理的，标注系统是基于开源的Pundit系统二次开发的，可以了解下。"
      },
      {
        "user": "crapthings",
        "created_at": "2019-01-08T03:52:26Z",
        "body": "@hujiaweibujidao \r\nPundit 这是啥"
      }
    ]
  },
  {
    "number": 168,
    "title": "人名识别不准确",
    "created_at": "2016-03-30T06:49:20Z",
    "closed_at": "2016-03-30T14:50:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/168",
    "body": "林军和大壮从侧门，被两个警察带进了市场旁边的一个大院内，随即直接进了办公楼里。\n\n　　“这派出所挺大啊？”林军看着狭长的走廊，表情有些惊愕的说了一句。\n\n　　“大哥，这是市局七处！”男警察斜眼回了一句。\n\n　　市公安局第七刑侦大队，俗称七处，部门职责是主抓特大重点案件。\n\n　　“咋给我们带这儿来了？”林军听后一愣。\n\n　　“来，左边一个，右边一个，靠着暖气站好！”女警冲着林军和大壮，俏脸面无表情的说道。\n\n　　二人听到这话，也没争辩，随后各自靠着暖气站了下来。而女警走进办公室取了两幅手铐，随即将二人分别铐在暖气管子上说道：“等着吧，一会派出所过来取你们俩。”\n\n　　林军右手被铐上时,正好与女警脸对脸，随即他双眼本能的打量了女警一下。\n\n　　她的长相有些特别，长发披肩，五官精致，但鼻梁很高，眼窝较深，一双灵动的大眼睛非常夺目。看着有点不像汉人五官，而是有点像史密斯夫妇中的安吉丽娜.茱莉！\n\n　　女警身段挺直，个子起码一米七左右，上半身套着一件紧身的半袖警服衬衫，领口扣子系的一丝不苟。她下半身穿着黑蓝色宽松的长裤，脚上蹬着一双平底的黑色瓢鞋，整个人的气质给人一种充满活力，英姿飒爽的感觉。\n\n　　“哎，郑警官，我俩这就是喝多了瞎闹腾，犯不上在你这儿占地方。你给我俩松开，我俩一块去派出所和解了得了。”大壮此刻已经有点被揍的醒酒了，他左手捂着还在淌血的嘴唇，随即含糊不清的喊道。\n\n　　“闭上你的嘴，呆着！”女警厌恶的扫了他一眼，随后冲收发室喊道：“李叔，帮忙看一下，一会把他们交给派出所就行。”\n\n　　“好叻！”　收发室的大爷回了一句。\n\n　　随后女警踩着平底鞋就上了楼，而跟他一起的那个男警察转身再次去了市场，继续去给加班的同事买盒饭。\n\n　　走廊内，工作人员来回穿梭，而林军和大壮相互对视了一眼。\n\n　　“操你玛，你等出去的！我让你知道，你打我的那一酒瓶子有多无知！”大壮看着林军小声骂道。\n\n　　林军将头扭过去，根本没回话。\n\n　　......\n\n　　四十分钟以后，派出所一个民警，带着一个二十六七的青年，并肩走进了走廊。民警进来以后，就直接走进了办公室，而青年腋下夹着包，脖子上挂着佛牌，手里搓着珠子冲大壮骂道：“一天净他妈给我惹事儿！”\n\n　　“涛，你看他给我干的，嘴唇子都整豁豁了。”大壮指着自己的嘴唇子说道。\n\n　　“你闭嘴吧！”青年回了一句，随后朝着民警走进的办公室走去。\n\n　　二十分钟以后，民警和青年走了出来。\n\n　　“王涛，谁是你朋友啊？”民警虎着脸，背手问道。\n\n　　“就他！”叫王涛的青年指了指大壮。\n\n　　民警扫了一眼林军和大壮，随即皱眉问道：“就这点破事儿，还用我调解啊？用验伤吗？”\n\n　　“我不用！”大壮思考了一下，干脆的回道。\n\n　　“我也不用！”林军扫了一眼三人，也面无表情的回道。\n\n　　“真不用啊？”民警冲着林军再次问道。\n\n　　“不用。”林军毫不犹豫的摇了摇头。\n\n　　“打开，走吧！”民警随手拿着钥匙交给了王涛。\n\n　　王涛接过钥匙，将大壮的铐子打开，然后又将钥匙扔给了林军。\n\n　　“谭哥，麻烦了，明儿请你吃饭啊。”王涛笑着冲民警说道。\n\n　　“轻点嘚瑟比啥都强，走吧，走吧。”民警淡然的摆了摆手。\n\n　　“那我走了，谭哥！”\n\n　　王涛冲民警打了个招呼，随后带着大壮扬长而去。林军摘下手铐以后，竖起大拇指冲民警说道：“这案子办的真利索！”\n\n　　“你还有事儿啊？”民警回头，面无表情的问道。\n\n　　“呵呵，没事儿。”林军放下手铐，随即头也不回的走出了七处。\n\n　　.......\n\n　　一个半小时以后，时间接近晚上七点多。\n\n　　林军刚刚收拾完自己的小摊，并将烧烤用具放在了张小乐的三轮子上。\n\n　　“今天不出了？”\n\n　　张小乐站在一旁，张嘴问道。\n\n　　“还出啥出，货都让他踩了。”林军有点心烦的回道。\n\n　　“行，那我跟你把东西送回去。”张小乐穿着工作服，挺仗义的回了一句。\n\n　　“不用了，你卖货吧，车借我用用就行。”林军骑上三轮子，咧嘴一笑说道。\n\n　　“要不今天你别送了，东西直接扔我这儿得了。”张小乐明显有点担心的说道。\n\n　　“呵呵。”林军一笑，也没多说，骑车就走了。\n\n　　．．．．．．．\n\n　　市场后方的小路上，林军健硕有力的双腿蹬着人力三轮车，顺着灯光昏暗的街道一路前行。\n\n　　“咚咚咚！”\n\n　　距离存放烧烤用具的车棚，还有一半路程时，小路对面突然泛起一阵农用三轮子的声响。\n\n　　“吱嘎！”\n\n　　林军踩了一脚刹车，右脚点地，眯着眼睛向前方望去。\n\n　　“就那个傻Ｂ，一会给我往死怼他！”骑在三轮子上的大壮，满嘴漏风的大吼了一句。\n\n　　大壮喊完，对方三轮车距离林军不超过二十米远后停滞，几乎同时，一副极为震撼的画面出现在林军眼中！\n\n　　农用三轮子是摩托式的，马力很小，具体大小也就跟路边拉黑活的那种“摩的”差不多，而这车的车斗载重量，估计也就能拉几袋百斤重的大米。\n\n　　但今天这个农用三轮子却突破了极限，就不足一米半长的车斗，竟然宛若春运火车车厢一般，拥挤得往下跳人！\n\n　　一个，两个，三个.......\n\n　　数秒过后，车斗之上竟然跳下来七个成年人！七个啊！天知道他们是怎么挤上去的，此场景即使跟印度三哥PK一下，那他妈也不差啥了！\n\n　　“我操！”林军数着对方跳下来的人，脸色被雷的有点惊愕。\n\n　　“呼啦啦！”\n\n　　大壮跳下摩托车，右手从车斗中抽出一把片刀，随即带着七个人，手里拿着铁棍子，镐把子，镰刀，还有街头斗殴中百年难得一见的炉钩子等异样凶器，蜂拥着冲向林军。\n\n　　“咣当当！”\n\n　　林军下车，在自知无法躲避这场斗殴之时，立马回手从三轮车上抽出一根半米长的空心钢管，随后眉头都没皱一下，迈步就冲向人群。\n\n　　双方碰见，基本没有废话，直接就开怼。\n\n　　对方一个老农，抡着镰刀直接刨向林军，而林军侧身一闪，右臂摆动幅度很小，但右手攥着的钢管却闪电般的抽在了老农的手腕上。\n\n　　当的一声，老农本能一缩手，林军手持钢管对着他脑袋，眨眼间就抽了三下，直接将其放倒。\n\n　　其余众人冲上，林军左手抓过一人的脖领子，宛若拎着鸡崽子一般，直接将其摆在身前，随即他身体晃了一下，右手攥着钢管，对着旁边的大壮，反手就抽了过去！\n\n　　“嘭！”\n\n　　钢管抽在大壮嘴上，他疼的一蹦半米高。\n\n　　“噼里啪啦！”\n\n　　对方砸下来的武器，根本无处躲避的干在林军和对方那人的身上。\n\n　　“往他手上砍！就照一万块钱干他了！”大壮捂着嘴，跳脚吼道。\n\n　　林军额头，胳膊开始冒血，他左胳膊一甩，右腿一扫，直接将抓着的汉子绊倒。\n\n　　“操你玛，我拿枪说话，拿刀吃饭的时候，你们还蹲地沟垄里唱东方红呢！”林军根本没管其他人，双手攥着钢管，胳膊卯足劲的往抡了数下。\n\n　　“嘭！”\n\n　　“嘭！”\n\n　　“嘭！”\n\n　　三声脆响，在林军身下这人的脑袋上，脖子上，后背上接连响起！\n\n　　“呼啦啦！”\n\n　　林军心黑手狠的干完这三下，人群顿时散开，众人看着他稍微有那么点犯怵！\n\n　　“唰唰！”\n\n　　与此同时，街口处有四台出租车匆忙赶来，这些车支着远光灯，停在路边。\n\n　　“咣当！”\n\n　　车门推开，张小乐扯脖子喊道：“军，谁他妈要干你啊？”\n\n　　大壮团伙一看街口停了四台出租车，同时双眼又被大灯晃的看不清楚张小斌带来多少人，所以，他们第一时间掉头就跑，连能拉七个人的神奇农用三轮子都扔下了。\n\n　　“咣当！”\n\n　　林军脸不红气不喘的将钢管扔进自己的三轮车，随后伸手熟练的摸了一下后背。手指碰触皮肤，他感觉出后背没有刀伤，但回头再看右臂的时候，一个不足半指长的刀口，流着血，而皮肉已经翻开了。\n\n　　“没事儿吧？”张小乐呼哧带喘的跑过来问道。\n\n　　“没事儿，胳膊上划了一下。”林军拿起车上的餐巾纸，一下抽出了半盒的厚度堵在了伤口上，随即扭头冲着张小乐问道：“你都带谁过来的？”\n\n　　“带个屁，四台出租车全是空的，现在的人，能借给你钱，就算好哥们了，哪有还能帮忙干仗的？”张小乐随口回了一句。\n\n　　“谢了，乐乐！”林军愣了一下，随即认真的说道。\n\n　　“谢的事儿回头再说，走吧，上医院看看！”张小乐拉着林军，继续说道：“他们这帮人，全是周边农村的，相互都认识，一会说不定叫来多少人！”\n\n　　“他们跟谁玩的？”林军思考一下，直接问道。\n\n　　“你要干啥啊？”张小乐一愣。\n\n　　“这点破事儿不整明白了，我看是没完没了了。”林军低头回道。\n\n　　“军，犯得上吗？”张小乐一听这话，顿时沉默几秒后皱眉问道。\n\n　　“干都干了，你说咋整？今天要是没个结果，那明天我还能不能干活了？”林军简洁明了的回了一句，随即再次问道：“他们是跟谁玩的？”\n\n　　“王涛。”张小乐思考了一下，随后还是如实相告。\n\n　　“二十多岁，脖子上挂着佛牌儿，没事儿手里还愿意搓着珠子，是他吗？”林军脑中瞬间想起在七处走廊碰见的那个青年。\n\n　　“对！”\n\n　　“他不行，段位太低，他上面还有人吗？”林军摇头再问。\n\n　　“大哥，你太狂点了吧？”张小乐愣了一下，随即惊愕的问道。\n\n　　“这事儿跟你说不明白，一个段位，一个谈法！”林军干脆的回道。\n\n　　“……王涛是跟满北伐玩的！”\n\n　　“他在哪儿？”\n\n　　“满北伐是整建筑的，手里有车队，人好像在江北望江别苑的三期工地里呢！”张小乐回了一句。\n\n　　“谢了，你帮我把东西送回去，回来请你吃饭！”林军听完以后拍了拍张小乐的肩膀，随即转身就走。\n\n　　两分钟以后，林军单人单骑，打了一辆出租车，直奔江北望江别苑。\n\n这样一段文本，识别人名结果是：[林军, 史密斯, 安吉丽娜, 茱莉, 郑警官, 李叔, 操你玛, 都整豁, 王涛, 瑟比, 张小乐, 米高, 钱干他, 黑手, 张小斌, 连能拉, 都带谁]\n请问识别出的这些不准确的，应该怎样才能尽量排除？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/168/comments",
    "author": "xiyangzp",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-03-30T14:50:54Z",
        "body": "这些大量使用方言和网络用语的网络小说建议使用CRFSegment。\n\n[嘴唇子, 都, 整, 豁豁, 了]\n"
      },
      {
        "user": "xiyangzp",
        "created_at": "2016-04-01T01:55:10Z",
        "body": "CRFSegment为什么比较适合呢？试过了，比较准确，但是不是很全。\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-04-03T04:03:35Z",
        "body": "这涉及到底层模型原理了，请查一下论文。\n"
      }
    ]
  },
  {
    "number": 157,
    "title": "经常 Java heap space",
    "created_at": "2016-03-17T03:34:33Z",
    "closed_at": "2016-03-18T01:52:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/157",
    "body": "你好，程序跑一会之后，就经常报这个\n\n```\njava.lang.OutOfMemoryError: Java heap space\n        at com.hankcs.hanlp.summary.TextRankSentence.<init>(TextRankSentence.java:73)\n        at com.hankcs.hanlp.summary.TextRankSentence.getSummary(TextRankSentence.java:245)\n        at com.hankcs.hanlp.HanLP.getSummary(HanLP.java:472)\n```\n\n是不是还要配置什么？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/157/comments",
    "author": "haorendashu",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-03-17T05:05:52Z",
        "body": "文本大小？\n该类不储存中间结果，不可能有内存泄露。问题只可能是输入文本过大，请加一些异常处理逻辑，定位是多大的文本引发这个异常。\n"
      },
      {
        "user": "haorendashu",
        "created_at": "2016-03-18T01:52:25Z",
        "body": "果然限制了文本大小之后，异常没了。\n\n谢谢。\n"
      }
    ]
  },
  {
    "number": 147,
    "title": "求教连接java的class丢失了，该如何找回？",
    "created_at": "2016-03-04T15:33:43Z",
    "closed_at": "2016-05-04T03:04:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/147",
    "body": "运行时突然出现jpype._jexception.ExceptionPyRaisable: java.lang.Exception: Class com.hankcs.hanlp.HanLP not found的错误，之前不小心卸载了python2换成3，现在又换成2就变成这样了，跪求高人的解决办法\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/147/comments",
    "author": "OriginalXY",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-03-04T16:24:42Z",
        "body": "应该是jar的路径错了。\n"
      },
      {
        "user": "OriginalXY",
        "created_at": "2016-03-05T07:39:37Z",
        "body": "路径看了好像没问题\n`# -_\\- coding:utf-8 -_-\nfrom jpype import *\n\nstartJVM(getDefaultJVMPath(), \"-Djava.class.path=D:\\PycharmProjects\\untitled\\hanlp-protable-1.2.8.jar;D:\\PycharmProjects\\untitled\")\nHanLP = JClass('com.hankcs.hanlp.HanLP')\n\n# 中文分词\n\nprint(HanLP.segment('你好，欢迎在Python中调用HanLP的API'))`\njdk、python、jpype都重装过，位数也都相同，但还是会报错\n`Traceback (most recent call last):\n  File \"D:/PycharmProjects/untitled/main.py\", line 8, in <module>\n    HanLP = JClass('com.hankcs.hanlp.HanLP')\n  File \"C:\\Python27\\lib\\site-packages\\jpype_jclass.py\", line 54, in JClass\n    raise _RUNTIMEEXCEPTION.PYEXC(\"Class %s not found\" % name)\njpype._jexception.ExceptionPyRaisable: java.lang.Exception: Class com.hankcs.hanlp.HanLP not found`\n"
      },
      {
        "user": "OriginalXY",
        "created_at": "2016-03-05T08:37:11Z",
        "body": "而且谷歌了下基本都是说JDK版本问题，是这样吗？我都换成32位的都还是不行\n\n1）thanks for the answer but I still have same error: Traceback (most recent call last): File \"aa.py\", line 14, in <module> A = JClass('stm.Stm') File \"/usr/lib/python2.7/dist-packages/jpype/_jclass.py\", line 54, in JClass raise _RUNTIMEEXCEPTION.PYEXC(\"Class %s not found\" % name) jpype._jexception.ExceptionPyRaisable: java.lang.Exception: Class stm.Stm not found – Jeren May 12 '14 at 13:17 \n\nMake sure the class is built (/home/jeren/Desktop/Project/TweetParse/Parse_Tweets/stm/build/classes/stm/Stm.‌​class exists). Check Java version using by your Python library and by your Java compiler. Also when you resolve that you will need to add default (no-arg) constructor to instantiate a = A(). – Radim May 12 '14 at 13:50\n\nStm.class exists! how can I use constructor with arguments??? is it possible anyway??????? – Jeren May 12 '14 at 14:08\n\nCheck Java version using by your Python library and by your Java compiler ===> both use java 1.7 – Jeren May 12 '14 at 15:46 \n\nI solved it by changing the JDK I used from 1.6 to 1.7 .. thanks to Radim for comments :) – Jeren May 12 '14 at 17:46\n\n2) Problem was that my Python was 32 bits and my java sdk (including the javac bytecode compiler) was 64 bits. I uninstalled the java sdk and re-installed a 32bits version. Done! Solved!\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-03-05T15:57:41Z",
        "body": "1. 安装JDK非常简单，分清楚32位和64位即可，必须与OS和Python的位数一致\n2. 针对不同位数，不同Python版本也有不同的JPype版本\n3. 我手上没有出问题的机器，无法进一步试错\n4. 你是“卸载了python2换成3，现在又换成2”导致这个问题，情况更复杂\n"
      },
      {
        "user": "OriginalXY",
        "created_at": "2016-03-06T10:08:46Z",
        "body": "先感谢博主解答。本人将换一台电脑尝试，不过新电脑OS是64位的，JDK和python有64位的，但JPype官网好像只有win32的，JPype-0.5.4.2.win32-py2.7。不知这样有没有影响？\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-03-06T14:57:46Z",
        "body": "应该可以通过编译的方式得到64位，另外pychar的pip install其实也是编译安装。\n"
      },
      {
        "user": "OriginalXY",
        "created_at": "2016-03-07T12:55:36Z",
        "body": "感谢博主，换到linux下一样的问题，暂未找到解决办法，以后再做尝试\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-03-07T20:17:40Z",
        "body": "我怀疑jar包是否破损了？拓展名改为.zip试试看能不能解压。\n"
      },
      {
        "user": "OriginalXY",
        "created_at": "2016-03-08T04:27:21Z",
        "body": "尝试了都不行，portable版的和正式版的都试过，对Java的运行不是很了解，估计是调用JVM的过程中出了什么茬子\n"
      }
    ]
  },
  {
    "number": 146,
    "title": "jpype提示依存句法分析出错",
    "created_at": "2016-03-04T07:10:30Z",
    "closed_at": "2016-05-04T03:06:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/146",
    "body": "请问jpype._jexception.VirtualMachineErrorPyRaisable: java.lang.OutOfMemoryError: Java heap space\n为何会超时呢？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/146/comments",
    "author": "OriginalXY",
    "comments": [
      {
        "user": "miaomushan",
        "created_at": "2016-03-04T07:40:25Z",
        "body": "请教下这个怎么解决？我刚才也遇到了。\n"
      },
      {
        "user": "OriginalXY",
        "created_at": "2016-03-04T10:29:04Z",
        "body": "pycharm的内存配置也改了，初始128，最大1024，但依然是那个问题，求博主解答\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-03-04T16:25:57Z",
        "body": "不关pycharm的事，试试这个参数：\n\nstartJVM(getDefaultJVMPath(), \"-Djava.class.path=C:\\hanlp\\hanlp-1.2.8.jar;C:\\hanlp\", \"-Xms1g\", \"-Xmx1g\")\n"
      }
    ]
  },
  {
    "number": 136,
    "title": "HanLP词频",
    "created_at": "2016-02-02T02:53:09Z",
    "closed_at": "2016-05-04T03:05:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/136",
    "body": "HanLP词频怎么统计，rank没有加入词频权重。如何实现 ？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/136/comments",
    "author": "guiyinzhang",
    "comments": [
      {
        "user": "intohole",
        "created_at": "2016-02-02T03:29:11Z",
        "body": "第一，textrank 是支持权重的 ；但是怎么按照你这个词频权重是件怎么算权重？ 一般停用词词频很高\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-02-02T05:18:06Z",
        "body": "为什么算法一定要用词频？\n"
      },
      {
        "user": "guiyinzhang",
        "created_at": "2016-02-02T11:44:01Z",
        "body": "1.停用词及停用词的词频忽略。\n2.算法可以不用词频。\n\n问题:\n1.分词后如何快速得到词频个数，不使用迭代，性能是非线性的。\n2.如何在rank的关键字统计的时候，附带加入这个关键字的词频功能。\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-02-06T20:36:00Z",
        "body": "1. 这是你自己的业务逻辑，你需要自己hack segSentence方法\n2. 同上\n"
      }
    ]
  },
  {
    "number": 135,
    "title": "进行分词测试，怎么屏蔽自带的词典的影响",
    "created_at": "2016-02-02T02:38:04Z",
    "closed_at": "2016-02-02T09:41:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/135",
    "body": "我想要用SIGHAN Bakeoff 2005测试不同的分词算法的效果。这里要屏蔽Hanlp自带的词典的影响，我看data里有好多词典，要怎么做才行。\n\n目前我的想法是用自定义的词典替换掉CoreNatureDictionary.txt，但是不知道其他词典对分词有什么影响？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/135/comments",
    "author": "huchhong",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-02-02T05:19:17Z",
        "body": "enableCustomDictionary(false)\n"
      }
    ]
  },
  {
    "number": 133,
    "title": "用相同的词库，比对了1.2.6和2.8，发现6效果好",
    "created_at": "2016-01-29T09:50:13Z",
    "closed_at": "2016-05-04T03:05:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/133",
    "body": "比如2.8分出4个何文，2.6分出何文全，j何文桂，何文玲，何文莲\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/133/comments",
    "author": "sheenshine",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-01-29T12:59:40Z",
        "body": "1. 因为127版本增加了用户词典的优先级\n2. 不使用人名词典.txt就行了：CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词库.txt; 全国地名大全.txt ns; 机构名词典.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf\n"
      },
      {
        "user": "sheenshine",
        "created_at": "2016-02-04T07:20:35Z",
        "body": "了解，谢谢！\n"
      }
    ]
  },
  {
    "number": 131,
    "title": "命名实体识别问题",
    "created_at": "2016-01-26T06:16:11Z",
    "closed_at": "2016-05-04T03:05:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/131",
    "body": " 文本 = \"交易账号：96600009934569490中国银行股份有限公司济南\";\n\n分词结果 [交易/vn, 账号/n, ：/w, 96600009934569490中国银行股份有限公司济南/nt]\n\nBasicTokenizer中SEGMENT = HanLP.newSegment().enableAllNamedEntityRecognize(true).enableCustomDictionary(true);\n\n貌似只开启命名实体识别或是自定义词典中的一个就能将数字和机构名分开，若两个同时开启则当做一个整体，这个问题怎么解决呢\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/131/comments",
    "author": "congjiao1988",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-01-27T04:10:40Z",
        "body": "1. 暂时需要你去写过滤逻辑\n2. 在缺乏人工或规则过滤的情况下，不建议开启机构名识别\n"
      }
    ]
  },
  {
    "number": 130,
    "title": "建议增加字符标准化方法",
    "created_at": "2016-01-22T07:47:00Z",
    "closed_at": "2016-05-04T03:05:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/130",
    "body": "把英文、数字的全角字符都转换为半角格式。\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/130/comments",
    "author": "jdb110",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-01-22T19:43:03Z",
        "body": "com.hankcs.hanlp.dictionary.other.CharTable#convert(java.lang.String)\n"
      }
    ]
  },
  {
    "number": 129,
    "title": "jvm内存问题",
    "created_at": "2016-01-19T09:08:56Z",
    "closed_at": "2016-01-22T03:52:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/129",
    "body": "有个问题很困惑，是不是jvm比较占内存，我在用python时，还是比较平稳的，但是在用jvm调用hanlp时，就会飙升上去\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/129/comments",
    "author": "lxj0276",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-01-19T09:11:21Z",
        "body": "分词内存120MB以上（-Xms120m -Xmx120m -Xmn64m）\n\n句法分析的时候，模型本身就有500-600MB。\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-01-22T03:52:03Z",
        "body": "这个问题应该解决了，如果还有问题，欢迎再开issue。\n"
      }
    ]
  },
  {
    "number": 115,
    "title": "核心词典生成的示例代码出现错误",
    "created_at": "2016-01-11T17:40:11Z",
    "closed_at": "2016-05-04T03:07:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/115",
    "body": "HanLP版本v1.2.8，示例代码是项目wiki页面用于训练HMM-NGram模型的代码：\n`\n        final NatureDictionaryMaker dictionaryMaker = new NatureDictionaryMaker();\n        CorpusLoader.walk(\"path/to/your/corpus\", new CorpusLoader.Handler()\n        {\n            @Override\n            public void handle(Document document)\n            {\n                dictionaryMaker.compute(document.getComplexSentenceList());\n            }\n        });\n        dictionaryMaker.saveTxtTo(\"data/test/CoreNatureDictionary\");\n`\n错误原因是dictionaryMaker.compute()输入的链表存在CompoundWord，而Precompile.java中95行：\n`\n    public static Word compile(IWord word)\n    {\n        return compile((Word)word);\n    }`\n强行将word转为Word所致。\n\n所以我想问，HanLP的API dictionaryMaker.compute()以后打算接受CompoundWord吗？现在wiki页面的调用会出问题。\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/115/comments",
    "author": "starkingpku",
    "comments": [
      {
        "user": "starkingpku",
        "created_at": "2016-01-11T18:10:01Z",
        "body": "另外，我看到在NatureDictionaryMaker.java中有如下的代码：\n`\n        final NatureDictionaryMaker dictionaryMaker = new NatureDictionaryMaker();\n        CorpusLoader.walk(\"D:\\\\JavaProjects\\\\CorpusToolBox\\\\data\\\\2014\", new CorpusLoader.Handler()\n        {\n            @Override\n            public void handle(Document document)\n            {\n                dictionaryMaker.compute(CorpusUtil.convert2CompatibleList(document.getSimpleSentenceList(false))); // 再打一遍不拆分的\n                dictionaryMaker.compute(CorpusUtil.convert2CompatibleList(document.getSimpleSentenceList(true)));  // 先打一遍拆分的\n            }\n        });\n        dictionaryMaker.saveTxtTo(\"data/test/CoreNatureDictionary\");`\n@hankcs 实际在制作核心词典时真是这么处理两遍吗（一遍不拆分，一遍拆分）？\n"
      },
      {
        "user": "hankcs",
        "created_at": "2016-01-13T09:55:56Z",
        "body": "1. 感谢反馈，wiki的确写得不对。\n2. public static Word compile(IWord word)的确有点暴力，最初没想到要将训练接口开放出来。\n3. List<IWord>是Java泛型的老毛病，设计得也不优雅。\n4. dictionaryMaker.compute()以后会接受CompoundWord的，暂时的解决方案请参考你粘贴出来的NatureDictionaryMaker.java中的代码。\n5. 的确处理了两遍，希望分得细的人可以选择单个词，希望分得粗的可以选择复合词，我是中立的。\n"
      }
    ]
  },
  {
    "number": 112,
    "title": "关于自定义词典修改后，编译的文件变大了",
    "created_at": "2016-01-08T02:40:51Z",
    "closed_at": "2016-01-08T02:58:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/112",
    "body": "为什么我自己修改自定义词典后，重新编译，文件比以前大很多呢，难道编译时有特殊方法\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/112/comments",
    "author": "laugha",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-01-08T02:42:53Z",
        "body": "这是正常现象，跟数据结构有关。\n"
      },
      {
        "user": "laugha",
        "created_at": "2016-01-08T02:58:41Z",
        "body": "多谢\n"
      }
    ]
  },
  {
    "number": 109,
    "title": "怎么关掉extractKeyword中的输出呀？",
    "created_at": "2016-01-02T04:55:56Z",
    "closed_at": "2016-05-04T02:59:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/109",
    "body": "想减少系统的开销。。。\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/109/comments",
    "author": "pkwv",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2016-01-03T04:17:13Z",
        "body": "输出？调试输出？关闭debug即可。\n"
      }
    ]
  },
  {
    "number": 92,
    "title": "无法在代码中配置数据目录，能否添加setProperty的方式在代码中自由使用",
    "created_at": "2015-12-03T11:20:03Z",
    "closed_at": "2015-12-03T14:15:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/92",
    "body": "作为Lib，直接调用jar，进行分词时，无法像你说的那样用配置文件配置data的目录，导致类似如下问题发生：\n\nERROR HanLP: HMM分词模型[ data/model/segment/HMMSegmentModel.bin ]不存在\n\n看到HanLP里有一段\npublic static final class Config 写死了词典和模型数据的地址，尽管有一个root通过配置文件实现了设置，但是还不方便，能否加一个类似setProperty的方式，在代码中自由使用？\n\n谢谢。\n\nPS: 我用的scala\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/92/comments",
    "author": "ericxsun",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-12-03T11:34:26Z",
        "body": "1. 不可能存在“无法像你说的那样用配置文件配置data的目录”，如果你坚持这么认为，请给出详细的环境说明。\n2. 没有写死，不仅root，所有配置都是活的。\n3. 并不需要setProperty，properties仅仅用做将配置读入内存。如果你喜欢硬编码，你可以这么写：\n\nHanLP.Config.HMMSegmentModelPath = \"绝对路径\";\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-12-03T11:39:23Z",
        "body": "如果问题依然没有得到解决，欢迎继续讨论。\n"
      },
      {
        "user": "YangliAtGitHub",
        "created_at": "2015-12-03T12:20:37Z",
        "body": "java.lang.System.getProperties().setProperty()的方法可以解决C# 访问HanLP的情况。目前没有发现任何问题。\n"
      },
      {
        "user": "ericxsun",
        "created_at": "2015-12-03T14:15:22Z",
        "body": "@hankcs 很抱歉，因为我代码里使用方式写错了，调了很久没设置成功，以为无法修改。现在改过来。谢谢。\n"
      }
    ]
  },
  {
    "number": 91,
    "title": "语义距离模块计算方法",
    "created_at": "2015-12-02T09:23:18Z",
    "closed_at": "2016-05-04T03:02:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/91",
    "body": "请问一下语义距离计算方法，具体原理是什么呢？有具体参考文献么？O(∩_∩)O谢谢\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/91/comments",
    "author": "houzw",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-12-02T11:26:58Z",
        "body": "这块没有论文，是自己设计的一个朴素的方案，根据同义词词林的类目计算词语相似度。\n以后可能会试验word2vec，比较得出更好的方案。\n"
      },
      {
        "user": "iou2much",
        "created_at": "2019-07-02T02:40:42Z",
        "body": "> 这块没有论文，是自己设计的一个朴素的方案，根据同义词词林的类目计算词语相似度。\r\n> 以后可能会试验word2vec，比较得出更好的方案。\r\n\r\n请问后续有用新的方法吗？"
      }
    ]
  },
  {
    "number": 90,
    "title": "修改词性",
    "created_at": "2015-12-01T05:04:01Z",
    "closed_at": "2016-05-04T03:02:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/90",
    "body": "在一般的地址里 “号” 这个词是很特殊 比如 150号港汇广场 就成了 150号港/ns 了，怎么把\"号\"改成个名词，希望的结果是 150/m,号/n，港/n这样的就好。\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/90/comments",
    "author": "lordheart1",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-12-01T07:39:44Z",
        "body": "这是地名识别模块带来的问题，你可以从data/dictionary/place/ns.txt中删除\n号 B 41 A 10 D 3\n"
      },
      {
        "user": "lordheart1",
        "created_at": "2015-12-09T02:51:30Z",
        "body": "非常感谢。那是不说我把地名识别开关关掉，也可以做到同样效果？\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-12-09T03:02:46Z",
        "body": "对\n"
      },
      {
        "user": "Tony-Wang",
        "created_at": "2015-12-09T03:03:45Z",
        "body": "奇怪，你们的对话怎么会发到我的邮箱\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-12-10T06:25:29Z",
        "body": "大概是因为你是issue的participants吧\n"
      }
    ]
  },
  {
    "number": 82,
    "title": "使用HMM分词后，结果的词性标注都为null是为什么",
    "created_at": "2015-11-08T13:08:36Z",
    "closed_at": "2016-05-04T03:02:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/82",
    "body": "使用的是HanLP-1.2.6，在运行DemoHMMSegment.java时，分词的词性结果都为null\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/82/comments",
    "author": "xyl576807077",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-11-08T13:24:29Z",
        "body": "分词和词性标注是两个任务，只不过在基于词的统计分词模型中可以顺手把最频繁的词性给出来，然而HMM2字标注分词器是基于字的。\n"
      },
      {
        "user": "xyl576807077",
        "created_at": "2015-11-08T13:46:45Z",
        "body": "那请问HanLP里面有什么能直接对词进行词性标注呢，还有另外一个问题，就是用CRF对下面这个文本分词后\n\n```\n南华大学跳楼 男子疑与女友吵架翻6层阳台身亡\n\n本报衡阳讯 5月15日早上6时30分左右，衡阳市南华大学城建学院一名学生从教学楼6楼坠下。下午5时许，南华大学一位相关负责人证实，坠楼者经医院全力抢救无效死亡。\n　　事发前，5点36分，有网名为“桃园果断不结义”的人在百度“南华大学贴吧”上发出一个帖子：“今日即将在1教赴死”。早上6点30分左右，一位在南华大学内施工的工人看到，在南华大学一教学楼的6楼北侧，一名男子与一名女子争执，随后男生往教学楼南侧奔去，随即从南侧的弧形阳台纵身一跃。\n　　事发后，在南华大学的学生QQ群流传着一则学生“初步调查”的消息：跳楼者为城建学院道桥专业，可能曾留级。不过，这些信息尚未得到校方确认。\n　　笔者拨打负责南华大学片区的红湘派出所负责人电话，截至15日下午5点仍没人接听。南华大学一位相关负责人透露，跳楼学生已经死亡，目前跳楼原因正在调查当中。\n\n\n```\n\n有些字符串为空的被标注为nz是怎么回事呢\n\n```\n南华  n\n大学  nis\n跳楼  v\n    nz\n男子  n\n疑 vg\n与 cc\n女友  n\n吵架  vi\n翻 v\n6   nz\n层 qv\n阳台  n\n身亡  vi\n    nz\n\n    nz\n\n    nz\n本报  rz\n衡阳  ns\n讯 ng\n    nz\n5   nz\n月 n\n15  nz\n日 b\n早上  t\n6   nz\n时 qt\n30  nz\n分 qt\n左右  f\n， w\n衡阳市   ns\n南华  n\n大学  nis\n城建学院    ns\n一名  nz\n学生  nnt\n从 p\n教学楼   n\n6   nz\n楼 n\n坠下  nz\n。 w\n下午  t\n5   nz\n时 qt\n许 v\n， w\n南华  n\n大学  nis\n一位  nz\n相关  vn\n负责人   nnt\n证实  v\n， w\n坠楼  nz\n者 k\n经 p\n医院  nis\n全力  d\n抢救  v\n无效  vi\n死亡  vi\n。 w\n\n    nz\n　　  nz\n事发  vi\n前 f\n， w\n5   nz\n点 qt\n36  nz\n分 qt\n， w\n有 vyou\n网名  n\n为 p\n“ w\n桃园  ns\n果断  a\n不 d\n结义  nz\n” w\n的 ude1\n人 n\n在 p\n百度  nt\n“ w\n南华  n\n大学  nis\n贴吧  n\n” w\n上 f\n发出  v\n一个  mq\n帖子  n\n： w\n“ w\n今日  t\n即将  d\n在 p\n1   nz\n教 v\n赴死  v\n” w\n。 w\n早上  t\n6   nz\n点 qt\n30  nz\n分 qt\n左右  f\n， w\n一位  nz\n在 p\n南华  n\n大学  nis\n内 f\n施工  vn\n的 ude1\n工人  nnt\n看到  v\n， w\n在 p\n南华  n\n大学  nis\n一 nz\n教学楼   n\n的 ude1\n6   nz\n楼 n\n北侧  f\n， w\n一名  nz\n男子  n\n与 cc\n一 nz\n名 q\n女子  n\n争执  vn\n， w\n随后  d\n男生  n\n往 p\n教学楼   n\n南侧  f\n奔去  nz\n， w\n随即  d\n从 p\n南侧  f\n的 ude1\n弧形  n\n阳台  n\n纵身  vi\n一跃  d\n。 w\n\n    nz\n　　  nz\n事发  vi\n后 f\n， w\n在 p\n南华  n\n大学  nis\n的 ude1\n学生  nnt\nQQ  nz\n群 ng\n流传  v\n着 uzhe\n一则  c\n学生  nnt\n“ w\n初步  d\n调查  vn\n” w\n的 ude1\n消息  n\n： w\n跳楼  v\n者 k\n为 p\n城建学院    ns\n道桥  nz\n专业  n\n， w\n可能  v\n曾 d\n留级  vi\n。 w\n不过  c\n， w\n这些  rz\n信息  n\n尚未  d\n得到  v\n校方  n\n确认  v\n。 w\n\n    nz\n　　  nz\n笔者  n\n拨打  v\n负责  v\n南华  n\n大学  nis\n片 q\n区 n\n的 ude1\n红湘  nz\n派出所   nis\n负责人   nnt\n电话  n\n， w\n截至  v\n15  nz\n日 b\n下午  t\n5   nz\n点 qt\n仍 d\n没 d\n人 n\n接 v\n听 v\n。 w\n南华  n\n大学  nis\n一位  nz\n相关  vn\n负责人   nnt\n透露  v\n， w\n跳楼  v\n学生  nnt\n已经  d\n死亡  vi\n， w\n目前  t\n跳楼  v\n原因  n\n正在  d\n调查  vn\n当中  f\n。 w\n\n    nz\n\n    nz\n\n```\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-11-08T13:53:10Z",
        "body": "1. StandardTokenizer、NLPTokenizer都可以。\n2. CRF分词依然是字标注，只不过很多用户老是在问为什么它不出词性，所以我让它默认在分词后去词典里查词性。词典里没有的都标称nz，很显然空格、换行符都不在词典中。\n3. 接下来把空格标注成x\n"
      },
      {
        "user": "xyl576807077",
        "created_at": "2015-11-08T14:10:20Z",
        "body": "非常您的解答\n"
      }
    ]
  },
  {
    "number": 73,
    "title": "如何才能分出带空格的英文/数字词",
    "created_at": "2015-10-15T05:57:23Z",
    "closed_at": "2016-05-04T03:02:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/73",
    "body": "比如 iPad Pro\n比如 PM 2.5\n谢谢！\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/73/comments",
    "author": "laoyang945",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-10-15T10:44:40Z",
        "body": "目前词典是用空格分隔的，所以无法通过词典文件引入。\n但是com.hankcs.hanlp.dictionary.CustomDictionary#insert(java.lang.String)一系列的方法可以接受这种词语。\n未来会逐渐用\\t作为词典的默认分隔符。\n"
      }
    ]
  },
  {
    "number": 69,
    "title": "比较简单的问题：.txt.bin文件如何提取？",
    "created_at": "2015-10-09T13:29:18Z",
    "closed_at": "2016-05-04T03:03:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/69",
    "body": "因为研究需要，想学习HanLP中的句法分析部分，按照介绍下载了data文件夹，发现里面的文件全部为txt.bin，而demo中提示\n“严重: CRF分词模型加载 .../data/model/segment/CRFSegmentModel.txt 失败，耗时 12 ms”\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/69/comments",
    "author": "andyhxy",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-10-09T14:03:16Z",
        "body": "请贴一下你的配置文件。\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-10-09T14:12:13Z",
        "body": "而且日志已经提示得很清楚了， .../data/model/segment/CRFSegmentModel.txt 这不可能是合法路径。\n"
      }
    ]
  },
  {
    "number": 60,
    "title": "Lucene5.3调用HanLP创建索引报错",
    "created_at": "2015-09-20T12:21:11Z",
    "closed_at": "2015-10-09T02:19:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/60",
    "body": "在Lucene5.3调用HanLP创建索引的时候，报错如下：\nException in thread \"main\" java.lang.AbstractMethodError: org.apache.lucene.analysis.Analyzer.createComponents(Ljava/lang/String;)Lorg/apache/lucene/analysis/Analyzer$TokenStreamComponents;\n    at org.apache.lucene.analysis.Analyzer.tokenStream(Analyzer.java:179)\n    at org.apache.lucene.document.Field.tokenStream(Field.java:562)\n    at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:607)\n    at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:344)\n    at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:300)\n    at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:234)\n    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:450)\n    at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1475)\n    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1254)\n    at com.ohoyee.test.search.SearchTest.createIndex(SearchTest.java:51)\n    at com.ohoyee.test.search.SearchTest.main(SearchTest.java:104)\n\n索引创建代码如下： \nstatic void createIndex() throws Exception {\n        Directory dir = FSDirectory.open(Paths.get(indexPath, new String[0]));\n        Analyzer analyzer = new HanLPAnalyzer();\n        IndexWriterConfig iwc = new IndexWriterConfig(analyzer);\n        IndexWriter writer = new IndexWriter(dir, iwc);\n        File[] files = new File(targetPath).listFiles();\n        for (File file : files) {\n            Document doc = new Document();\n            String content = getContent(file);\n            String name = file.getName();\n            String path = file.getAbsolutePath();\n            doc.add(new TextField(\"content\", content, Store.YES));\n            doc.add(new TextField(\"name\", name, Store.YES));\n            doc.add(new TextField(\"path\", path, Store.YES));\n            System.out.println(name + \"===\" + content + \"===\" + path);\n            writer.addDocument(doc);\n            writer.commit();\n        }\n\n```\n}\n```\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/60/comments",
    "author": "liucong315",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-09-20T12:34:25Z",
        "body": "请贴一下插件jar版本。\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-09-20T12:37:28Z",
        "body": "看起来像是用4.x的Lucene插件放到5.x的Lucene里用。\n"
      },
      {
        "user": "lsq88334753",
        "created_at": "2015-10-06T05:58:20Z",
        "body": "请问 ：lucene5.2.1 (hanlp-portable-1.2.4, hanlp-solr-plugin-1.0)索引创建成功，搜索却无命中记录(搜索没报错，并且肯定有这个词)。\n建立索引：\n\n```\n Analyzer analyzer = new HanLPAnalyzer();////////////////////////////////////////////////////\n IndexWriterConfig config = new IndexWriterConfig(analyzer);\n config.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);\n Directory directory = FSDirectory.open(Paths.get(INDEX_DIR));\n IndexWriter indexWriter = new IndexWriter(directory, config);\n\n Document document = new Document();\n document.add(new TextField(\"time\", res, Store.YES));\n document.add(new TextField(\"content\", content, Store.YES));\n indexWriter.addDocument(document);\n\n indexWriter.commit();\n closeWriter(indexWriter);\n```\n\n搜索\n\n```\n Directory directory = FSDirectory.open(Paths.get(INDEX_DIR));\n Analyzer analyzer = new HanLPAnalyzer();//////////////////////////////////////////////\n IndexReader ireader = DirectoryReader.open(directory);\n IndexSearcher isearcher = new IndexSearcher(ireader);\n QueryParser parser = new QueryParser(\"content\", analyzer);\n Query query = parser.parse(text);\n ScoreDoc[] hits = isearcher.search(query, 300000).scoreDocs;\n```\n\n当搜索 “time:[20140101 TO 20150101]” 有命中结果显示，而搜索 “被告人” 这个词命中结果为0个，这个词是一定有的。请问您知道是什么原因么？\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-10-06T08:49:42Z",
        "body": "我测试正常\n\n```\n        Analyzer analyzer = new HanLPAnalyzer();////////////////////////////////////////////////////\n        IndexWriterConfig config = new IndexWriterConfig(analyzer);\n        config.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);\n        String INDEX_DIR = System.getProperty(\"java.io.tmpdir\") + File.separator + \"index\";\n        Directory directory = FSDirectory.open(Paths.get(INDEX_DIR));\n        IndexWriter indexWriter = new IndexWriter(directory, config);\n\n        Document document = new Document();\n        document.add(new TextField(\"content\", \"被公诉机关指控涉嫌犯罪的当事人称作被告人。\", Field.Store.YES));\n        indexWriter.addDocument(document);\n\n        document = new Document();\n        document.add(new TextField(\"content\", \"商品和服务\", Field.Store.YES));\n        indexWriter.addDocument(document);\n\n        document = new Document();\n        document.add(new TextField(\"content\", \"和服的价格是每镑15便士\", Field.Store.YES));\n        indexWriter.addDocument(document);\n\n        indexWriter.commit();\n        indexWriter.close();\n\n        IndexReader ireader = DirectoryReader.open(directory);\n        IndexSearcher isearcher = new IndexSearcher(ireader);\n        QueryParser parser = new QueryParser(\"content\", analyzer);\n        Query query = parser.parse(\"被告人\");\n        ScoreDoc[] hits = isearcher.search(query, 300000).scoreDocs;\n        for (ScoreDoc scoreDoc : hits)\n        {\n            Document targetDoc = isearcher.doc(scoreDoc.doc);\n            System.out.println(targetDoc.getField(\"content\").stringValue());\n        }\n```\n\n如果你那边也能通过这个测试，那么问题可能并不在这里。\n"
      },
      {
        "user": "lsq88334753",
        "created_at": "2015-10-06T10:11:03Z",
        "body": "折腾一天找到原因了，就是在添加的字符串(content)中存在两个或以上的换行符时后面的文本就不被识别了。例如：\"\\n\\n\" + \"被公诉机关指控涉嫌犯罪的当事人称作被告人。\" 再搜索被告人 结果就为0\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-10-06T10:57:06Z",
        "body": "感谢排查，问题已经确认，马上修复这个bug。\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-10-09T02:19:23Z",
        "body": "这个问题应该解决了，如果还有问题，欢迎再开issue。\n"
      },
      {
        "user": "lsq88334753",
        "created_at": "2015-10-09T02:23:20Z",
        "body": "非常感谢！\n"
      }
    ]
  },
  {
    "number": 57,
    "title": "有没有办法关闭分词后的后缀词性标注",
    "created_at": "2015-09-12T08:27:56Z",
    "closed_at": "2016-05-04T02:58:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/57",
    "body": "我想得到的结果是分词保留原来的形式，只是分词之后有空格分开，不会出现斜杠和后面的词性标注。\n请问怎样办？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/57/comments",
    "author": "sunningboy",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-09-13T12:06:29Z",
        "body": "分词结果是否展示词性的配置项ShowTermNature=true\n"
      },
      {
        "user": "sunningboy",
        "created_at": "2015-09-14T01:55:16Z",
        "body": "谢谢！不好意思，我没找到。请问是在最新的版本中吗？\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-09-14T04:35:03Z",
        "body": "不客气，v1.1.3以上\n"
      },
      {
        "user": "sunningboy",
        "created_at": "2015-09-14T08:41:59Z",
        "body": "难怪我没找到。呵呵。我用的版本太低了，现在只能把源代码重新下载下来弄。\n另，我想肯定可以用eclipse直接连接github更新，你的更新中提到了“Portable同步升级到v1.2.4”，请问具体怎么弄？还请指点哈，谢谢！\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-09-14T08:57:10Z",
        "body": "请谷歌git用法。Portable版指的是Maven中央仓库里的版本，该版本将data打包进了jar，所以是便携的。\n"
      },
      {
        "user": "frank-zx",
        "created_at": "2015-11-15T04:51:27Z",
        "body": " Segment segment = HanLP.newSegment().enablePartOfSpeechTagging(false);\n这样不可以关闭斜杠和后面的词性标注吗？\nPortable版\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-11-15T11:01:38Z",
        "body": "分词结果是否展示词性的配置项ShowTermNature=true，或在代码中HanLP.Config.ShowTermNature = false;    // 关闭词性显示\n"
      },
      {
        "user": "guiyinzhang",
        "created_at": "2016-02-01T09:30:51Z",
        "body": "如何在TextRank上面加入词性、词频个数、权重。一块return ?\n"
      }
    ]
  },
  {
    "number": 51,
    "title": "提取关键词、短语",
    "created_at": "2015-08-26T01:19:56Z",
    "closed_at": "2015-09-01T13:18:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/51",
    "body": "你好，HanLP提取新闻的关键词或短语的时候效果总不尽人意，通常提取出来的词不是特别能表达出新闻关键关注点，有啥好的解决方案吗？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/51/comments",
    "author": "lizhitao0923",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-08-26T01:33:49Z",
        "body": "1. TextRank算法的确只有这个程度的效果，任何算法肯定都无法做到跟人一样。\n2. 单文档关键字算法无法利用背景知识，可能需要探索补充外部知识库，比如某个词在互联网的丰度，新鲜程度等等。\n3. 如果你发现了更好用的算法\\开源项目，欢迎向我推荐。\n"
      },
      {
        "user": "zhengzhuangjie",
        "created_at": "2017-12-15T02:35:34Z",
        "body": "请问一下，怎么样再提取文章概要时，尽量把带数字的语句提取出来？"
      }
    ]
  },
  {
    "number": 49,
    "title": "CRFSegment.cs中tag方法有一行代码没看懂",
    "created_at": "2015-08-24T00:36:44Z",
    "closed_at": "2015-08-24T08:35:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/49",
    "body": "博主，在tag方法中，利用维特比方法计算字符串的标注路径时，有一段代码是if (matrix[pre][now] <= 0) continue； 为什么tag的转移概率<0，就略过了呢？\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/49/comments",
    "author": "starkingpku",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-08-24T01:19:54Z",
        "body": "这仅仅是为了运行效率，既然BMES是连续的标签，那么转移矩阵的任何一行都至少有一个值大于零，所以小于零的都不用考虑了。\n"
      },
      {
        "user": "starkingpku",
        "created_at": "2015-08-24T07:07:37Z",
        "body": "第一个问题，HMM模型的发射概率p(observ|state)和状态转移概率p(state_j|state_i)是转移矩阵, CRF中的f(state_i,state_j)不是转移概率吧，只是表示状态之间相关强度吧？不知我理解的对不对。\n第二个问题，对于当前状态now，假设有两个前驱状态pre1、pre2的状态转移矩阵满足matrix[pre1][now]<0 并且 matrix[pre2][now]>0，但是会不会可能net[i - 1][pre1]>net[i - 1][pre2]，造成score1>score2呢？如果这样，那么score1的计算就不应该略过了吧？\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-08-24T07:53:14Z",
        "body": "的确不是转移概率，你说得有道理，感谢指正。\n"
      },
      {
        "user": "starkingpku",
        "created_at": "2015-08-24T08:20:45Z",
        "body": "我采用CRF++0.58，编译时无论是mac还是linux,configure时都会遇到这个问题：\nconfigure: error: Your compiler is not powerful enough to compile CRF++. If it should be, see config.log for more information of why it failed.\n解决起来复杂吗？\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-08-24T08:23:11Z",
        "body": "我没遇到过这种问题，直接编译成功了。你可以使用binary。\n"
      },
      {
        "user": "starkingpku",
        "created_at": "2015-08-24T08:34:58Z",
        "body": "谢谢！\n"
      }
    ]
  },
  {
    "number": 37,
    "title": "繁体中文识别人名",
    "created_at": "2015-07-17T03:14:10Z",
    "closed_at": "2015-07-17T10:21:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/37",
    "body": "\"「國際足球總會」（FIFA）主席布拉特（Sepp Blatter）5月29日連任成功.\"\n\n将“任成功”识别为人名，感觉粗暴的将“任成功”加到nr.txt里似乎不妥，也许某些语境里真有人叫“任成功”，不知道有没有更好的处理办法。\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/37/comments",
    "author": "Tony-Wang",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-07-17T03:28:32Z",
        "body": "繁体中文请优先使用繁体分词器：\n\nSystem.out.println(TraditionalChineseTokenizer.segment(\n                \"5月29日連任成功.\"\n        ));\n\n输出：\n\n[「/w, 國際足球/nz, 總會/nis, 」/w, （/w, FIFA/nx, ）/w, 主蓆/nnt, 布拉特/nrf, （/w, Sepp/nx,  /w, Blatter/nx, ）/w, 5/m, 月/n, 29/m, 日/b, 連任/v, 成功/a, ./w]\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-07-17T03:43:51Z",
        "body": "简繁混合的模式下，最好开启自动正规化，详细请通过IDE阅读这些配置项的文档\n\n```\n    HanLP.Config.Normalization = true;\n    System.out.println(StandardTokenizer.segment(\n            \"5月29日連任成功.简体中文连任成功\"\n    ));\n```\n\n[5/m, 月/n, 29/m, 日/b, 连任/v, 成功/a, ./w, 简体中文/gi, 连任/v, 成功/a]\n"
      },
      {
        "user": "Tony-Wang",
        "created_at": "2015-07-20T00:20:15Z",
        "body": "谢谢，问题得到了解决。\n"
      }
    ]
  },
  {
    "number": 31,
    "title": "crf problem",
    "created_at": "2015-07-01T08:43:25Z",
    "closed_at": "2015-07-01T09:09:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/31",
    "body": "当采用data-for1.2.2中的crf训练数据进行分词时,出现错误,具体是\n代码为:\nCRFModel crfModel = CRFModel.loadTxt(\"E:\\scalaworkspace\\Eyas\\data\\model\\segment\\CRFSegmentModel.txt.bin\");\n        System.out.println(\"locad finish\");\n错误为\nException in thread \"main\" java.lang.NumberFormatException: For input string: \"U 0 0 : (   \u0004??z?_K????]S?况??{???\u000ek??n??\"\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/31/comments",
    "author": "double-headed-eagle",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-07-01T08:50:43Z",
        "body": "1. loadTxt是加载文本txt格式的crf模型的，你拿去加载binary，当然会错\n2. 该模型是自动加载的，你不需要去调用\n"
      },
      {
        "user": "double-headed-eagle",
        "created_at": "2015-07-01T08:54:30Z",
        "body": "是我自己搞错了,我再研究下.非常感谢^_^\n"
      }
    ]
  },
  {
    "number": 30,
    "title": "自定义词典似乎没用",
    "created_at": "2015-06-27T16:21:31Z",
    "closed_at": "2015-06-28T10:33:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/30",
    "body": "System.out.println(CustomDictionary.add(\"高大上\",\"a 1024\"));\ntermList = HanLP.segment(\"外观绝对高大上，不信的是没见过.\");\nSystem.out.println(termList);\n\n输出结果\n\n[外观/n, 绝对/d, 高大/a, 上/f, ，/w, 不信/v, 的/ude1, 是/vshi, 没/d, 见过/v, ./w]\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/30/comments",
    "author": "jdb110",
    "comments": [
      {
        "user": "yuchaozhou",
        "created_at": "2015-06-28T04:57:06Z",
        "body": "1.使用CustomDictionary添加新词，在粗分阶段会切分出来，没有问题。\n\n2.问题出在ngram词频处理上。\n删掉ngram缓存，你需要将\"绝对@高大上\"加入CoreNatureDictionary.ngram.txt中。\n不过，还存在一个问题，ngram加载时，CoreBiGramTableDictionary类对产后接续词进行了是否在核心词典的检测，如果在则加载，若不在则跳过。所以，此处直接用CustomDictionary存在问题是，新添加的词并没有在核心词典，ngram接续词会跳过。\n\n3.存在的问题或许还是需要后续开发完善的。\n\n4.目前直接将“高大上”加入核心词典、绝对@高大上\"加入CoreNatureDictionary.ngram.txt后，问题解决。\n\n[外观/n, 绝对/d, 高大上/a, ，/w, 不信/v, 的/ude1, 是/vshi, 没/d, 见过/v, ./w]\n"
      },
      {
        "user": "jdb110",
        "created_at": "2015-06-28T06:34:25Z",
        "body": " 增加个CoreNatureDictionary.add的方法就好了\n"
      },
      {
        "user": "jdb110",
        "created_at": "2015-06-28T06:39:30Z",
        "body": "建议：增加个CoreNatureDictionary.add的方法，CoreDictionaryPath可以向CustomDictionaryPath一样追加多个词典。\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-06-28T10:33:08Z",
        "body": "问题的确如@yuchaozhou所言，在于缺少NGram接续。所以需要\n1. 把词加到CoreNatureDictionary.txt里\n2. 把接续加到CoreNatureDictionary.ngram.txt里\n\n带有Core字样的是训练出来的模型，不太希望用户增删，而且双数组tire动态增删代价非常大。所以不打算实现CoreNatureDictionary.add\n"
      }
    ]
  },
  {
    "number": 29,
    "title": "按照你手册上提供方法添加人名误判不成功",
    "created_at": "2015-06-12T08:43:36Z",
    "closed_at": "2015-06-22T08:33:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/29",
    "body": "按照你提供的方法，修改了data /dictionary / person / nr.txt，加入一个新词条，但是系统依然误判。\n比如：“万余元” 本不该是人名，添加该词条，但是最终结果依然误判！求解决方法。\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/29/comments",
    "author": "michaelliu03",
    "comments": [
      {
        "user": "yuchaozhou",
        "created_at": "2015-06-12T13:08:42Z",
        "body": "确定添加新词条后也一同清理了*.bin词典缓存？\n"
      },
      {
        "user": "michaelliu03",
        "created_at": "2015-06-15T01:47:01Z",
        "body": "多谢指点！^_^\n"
      },
      {
        "user": "michaelliu03",
        "created_at": "2015-06-15T02:28:24Z",
        "body": "尝试清除*.bin 词典缓存，添加了多条词条到data /dictionary / person / nr.txt 中，\n目前在结果中只有一词条进行误判处理，其余依旧误判。求解决方法。\n"
      },
      {
        "user": "yuchaozhou",
        "created_at": "2015-06-15T09:26:02Z",
        "body": "把你测试的结果贴一下，然后再详细描述一下问题。\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-06-15T12:51:48Z",
        "body": "万余元 A 1\n\n应该就可以了，请说明详情\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-06-15T13:02:55Z",
        "body": "其实这种词语还有一个办法，余元可以作为一个量词 mq 嘛，搜狗拼音也能打出来说明这个词语也比较常用了\n"
      },
      {
        "user": "michaelliu03",
        "created_at": "2015-06-16T01:07:19Z",
        "body": "博主您好！我查了您相关的帮助和代码，试图修改nr.txt.比如我在此字典中加入了\n万余元 A 1  钱是 A 1 应对其 A 1   方就其 A 1   微智 A 1  途歌 A 1  曾两度 A 1 鹿晗暖 A 1 帅比萌 A 1\n么? A 1 .........等词语来试图提高对人名识别的精度，但是我跑出来的结果是，加入的词语有一部分进行了处理，有一部分没有进行处理。比如我之前给的例子中“万余元 ” 就被处理了 。未被处理的词语比如我在这里提到的“钱是 ”，“ 应对其” 等应该怎么去除掉。感谢您的回复！\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-06-22T08:33:03Z",
        "body": "1. 抱歉现在才回复\n2. 鹿晗暖是一个收录在data/dictionary/custom/人名词典.txt中的人名，如果你认为不是人名，可以删掉，或者不加载这个用户词典\n3. 从代码的角度上讲，不可能存在一部分有效一部分失效的情况\n4. 请保证每个词语都换行，同时删除了data/dictionary/person/nr.txt.trie.dat和data/dictionary/person/nr.txt.value.dat这两个缓存文件\n5. 你提交的这部分bad case我已经整合到nr词典中了，你可以check out最新词典然后重建缓存，跑如下test代码，肯定没问题的。\n\n```\n        String[] textArray = \"万余元 A 1 钱是 A 1 应对其 A 1 方就其 A 1 微智 A 1 途歌 A 1 曾两度 A 1 帅比萌 A 1\".split(\"A 1\");\n        System.out.println(Arrays.toString(textArray));\n        for (String text : textArray)\n        {\n            List<Term> termList = StandardTokenizer.segment(text);\n            System.out.println(termList);\n            for (Term term : termList)\n            {\n                assertEquals(false, term.nature == Nature.nr);\n            }\n        }\n```\n"
      }
    ]
  },
  {
    "number": 28,
    "title": "如何添加新的停用词（还是不行）",
    "created_at": "2015-06-12T03:25:25Z",
    "closed_at": "2015-06-15T12:52:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/28",
    "body": "我在dictionary目录下的默认停用词文件中添加新的停用词，重新运行之后发现新的停用词没有去掉\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/28/comments",
    "author": "newthis",
    "comments": [
      {
        "user": "liuia",
        "created_at": "2015-06-14T06:15:13Z",
        "body": "所有字典都会建立缓存文件，所以修改字典后需要删除缓存文件，一般是同名文件的bin文件\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-06-15T12:52:30Z",
        "body": "对，感谢楼上，应该没问题了吧\n"
      },
      {
        "user": "newthis",
        "created_at": "2015-07-02T06:54:46Z",
        "body": "哦， 还是有问题，按照楼主的方法去做，还是不行，删掉stopwords缓存之后，再添加新的停用词\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-07-02T07:10:39Z",
        "body": "调一下com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary#contains单步追踪\n"
      }
    ]
  },
  {
    "number": 24,
    "title": "能否提供主词典动态删除方法？",
    "created_at": "2015-06-05T06:24:14Z",
    "closed_at": "2015-06-08T10:29:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/24",
    "body": "看了下原码，发现BinTrie有提供remove方法，所以自定义词典可以动态删除。\n不过DoubleArrayTrie好像并没有提供删除方法，请问是为什么？是否是因为数据结构的问题？\n\n建议能提供主词典动态删除方法，因为若是把hanLP拿来做生产环境solr的分词，服务不能停，\n如果要修改词库的话，建议能有动态删除所有词库的方法。\n\n谢谢！\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/24/comments",
    "author": "timlincool",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-06-08T10:29:01Z",
        "body": "你好，抱歉没有及时回复。\n\nDAT比较特殊，动态增删非常复杂。如果你很需要删除操作的话，可以给每个key新增一个状态变量，删除则置为false，粗分的时候忽略状态为false的词语。\n"
      }
    ]
  },
  {
    "number": 8,
    "title": "关于分词的问题?",
    "created_at": "2015-05-06T08:57:09Z",
    "closed_at": "2015-05-06T12:19:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/hankcs/HanLP/issues/8",
    "body": "博主! 你好!\n     分词\"手机套\",结果：[手机/n, 套/q]\n但是在词库CoreNatureDictionary.txt中找到有手机套的相关词，如下：\n    Line 11870: 使手机 n 2\n    Line 61022: 手机 n 6656\n    Line 61023: 手机党 nz 2\n    Line 61024: 手机卡 nz 17\n    Line 61025: 手机套 nz 2\n    Line 61026: 手机报 nz 37\n    Line 137480: 部手机 n 53\n为什么分词不是分成　[手机套/nz] 呢　\n",
    "comments_url": "https://api.github.com/repos/hankcs/HanLP/issues/8/comments",
    "author": "a198720",
    "comments": [
      {
        "user": "hankcs",
        "created_at": "2015-05-06T09:42:39Z",
        "body": "这涉及到NGram的原理，简单地说，语料库中没有以手机套结尾的句子。\n其实，对于“手机套的颜色”“手机套价格”都能切分出“手机套”。\n如果非要将单个的手机套切分出来，可以在data/dictionary/CoreNatureDictionary.ngram.txt中加入一条：\n\n```\n手机套@末##末 1000\n```\n"
      },
      {
        "user": "a198720",
        "created_at": "2015-05-06T09:57:30Z",
        "body": "了解! 谢谢! \n"
      },
      {
        "user": "a198720",
        "created_at": "2015-05-06T10:44:10Z",
        "body": "追问一下哈,如果有个词 如\"天翼\" 是在句子中,如\"呼伦贝尔天翼大众\",这样的话,我想要切分成[呼伦贝尔/ns, 天翼/nz, 大众/ntc] 该如何处理呢? \n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-05-06T10:56:57Z",
        "body": "可以打开调试模式观察这个结果是怎么出来的：\n\n```\n        HanLP.Config.enableDebug(true);\n        HanLP.Config.ShowTermNature = false;\n        Segment segment = new DijkstraSegment().enableAllNamedEntityRecognize(false);\n        System.out.println(segment.seg(\n                \"呼伦贝尔天翼大众\"\n        ));\n```\n\n输出：\n\n```\n粗分词图：========按终点打印========\nto:  1, from:  0, weight:04.60, word:始##始@呼\nto:  2, from:  0, weight:03.41, word:始##始@未##地\nto:  3, from:  1, weight:13.28, word:呼@伦\nto:  4, from:  1, weight:13.28, word:呼@未##人\nto:  5, from:  3, weight:03.82, word:伦@贝\nto:  6, from:  3, weight:15.43, word:伦@未##人\nto:  7, from:  5, weight:14.99, word:贝@尔\nto:  8, from:  2, weight:02.45, word:未##地@天\nto:  8, from:  4, weight:12.31, word:未##人@天\nto:  8, from:  6, weight:15.58, word:未##人@天\nto:  8, from:  7, weight:13.93, word:尔@天\nto:  9, from:  2, weight:16.25, word:未##地@天翼\nto:  9, from:  4, weight:12.31, word:未##人@天翼\nto:  9, from:  6, weight:15.58, word:未##人@天翼\nto:  9, from:  7, weight:13.93, word:尔@天翼\nto: 10, from:  8, weight:10.28, word:天@翼\nto: 11, from:  9, weight:16.21, word:天翼@大\nto: 11, from: 10, weight:14.44, word:翼@大\nto: 12, from:  9, weight:16.21, word:天翼@未##团\nto: 12, from: 10, weight:14.44, word:翼@未##团\nto: 13, from: 11, weight:09.13, word:大@众\nto: 14, from: 12, weight:00.33, word:未##团@末##末\nto: 14, from: 13, weight:05.22, word:众@末##末\n\n粗分结果[呼伦贝尔, 天, 翼, 大众]\n```\n\n呼伦贝尔是一个地名，对应未##地\n由于CoreNatureDictionary.ngram.txt中缺少未##地@天翼，导致这种切分结果weight过高，所以只要在CoreNatureDictionary.ngram.txt补充一个\n\n```\n未##地@天翼 1000\n```\n\n同理还有\n\n```\n天翼@未##团 1000\n```\n\n应该就可以了。\n"
      },
      {
        "user": "hankcs",
        "created_at": "2015-05-06T10:59:58Z",
        "body": "事实上，`CRFSegment`对这种现象有很好的效果：\n\n```\n        HanLP.Config.ShowTermNature = false;    // 关闭词性显示\n        Segment segment = new CRFSegment();\n        String[] sentenceArray = new String[]\n                {\n                        \"HanLP是由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用。\",\n                        \"鐵桿部隊憤怒情緒集結 馬英九腹背受敵\",           // 繁体无压力\n                        \"馬英九回應連勝文“丐幫說”：稱黨內同志談話應謹慎\",\n                        \"高锰酸钾，强氧化剂，紫红色晶体，可溶于水，遇乙醇即被还原。常用作消毒剂、水净化剂、氧化剂、漂白剂、毒气吸收剂、二氧化碳精制剂等。\", // 专业名词有一定辨识能力\n                        \"《夜晚的骰子》通过描述浅草的舞女在暗夜中扔骰子的情景,寄托了作者对庶民生活区的情感\",    // 非新闻语料\n                        \"这个像是真的[委屈]前面那个打扮太江户了，一点不上品...@hankcs\",                       // 微博\n                        \"鼎泰丰的小笼一点味道也没有...每样都淡淡的...淡淡的，哪有食堂2A的好次\",\n                        \"克里斯蒂娜·克罗尔说：不，我不是虎妈。我全家都热爱音乐，我也鼓励他们这么做。\",\n                        \"今日APPS：Sago Mini Toolbox培养孩子动手能力\",\n                        \"财政部副部长王保安调任国家统计局党组书记\",\n                        \"2.34米男子娶1.53米女粉丝 称夫妻生活没问题\",\n                        \"你看过穆赫兰道吗\",\n                        \"呼伦贝尔天翼大众\",\n                        \"乐视超级手机能否承载贾布斯的生态梦\"\n                };\n        for (String sentence : sentenceArray)\n        {\n            List<Term> termList = segment.seg(sentence);\n            System.out.println(termList);\n        }\n```\n\n输出：\n\n```\n[HanLP, 是, 由, 一系列, 模型, 与, 算法, 组成, 的, Java, 工具包, ，目标, 是, 普及, 自然, 语言, 处理, 在, 生产, 环境, 中, 的, 应用, 。]\n[鐵桿, 部隊, 憤怒, 情緒, 集結,  , 馬英九, 腹背受敵]\n[馬英九, 回應, 連勝文, “, 丐幫, 說, ”, ：, 稱, 黨內, 同志, 談話, 應, 謹慎]\n[高锰酸钾, ，强氧化剂, ，紫红色, 晶体, ，可, 溶于, 水, ，遇, 乙醇, 即, 被, 还原, 。常, 用作, 消毒剂, 、, 水, 净化剂, 、, 氧化剂, 、, 漂白剂, 、, 毒气, 吸收剂, 、, 二氧化碳, 精制剂, 等, 。]\n[《, 夜晚, 的, 骰子, 》, 通过, 描述, 浅草, 的, 舞女, 在, 暗夜, 中, 扔, 骰子, 的, 情景, ,寄托, 了, 作者, 对, 庶民, 生活区, 的, 情感]\n[这个, 像, 是, 真的, [, 委屈, ], 前面, 那个, 打扮, 太, 江户, 了, ，一点, 不, 上品, ., ., ., @, hankcs]\n[鼎泰丰, 的, 小笼, 一点, 味道, 也, 没有, ., ., ., 每样, 都, 淡淡的, ., ., ., 淡淡的, ，哪, 有, 食堂, 2, A, 的, 好次]\n[克里斯蒂娜·克罗尔, 说, ：, 不, ，我, 不是, 虎妈, 。我, 全家, 都, 热爱, 音乐, ，我, 也, 鼓励, 他们, 这么, 做, 。]\n[今日, APPS, ：, Sago,  , Mini,  , Toolbox, 培养, 孩子, 动手, 能力]\n[财政部, 副部长, 王保安, 调任, 国家, 统计局, 党组, 书记]\n[2.34, 米, 男子, 娶, 1.53, 米, 女, 粉丝,  , 称, 夫妻, 生活, 没问题]\n[你, 看过, 穆赫兰道, 吗]\n[呼伦贝尔, 天翼, 大众]\n[乐, 视, 超级, 手机, 能否, 承载, 贾布斯, 的, 生态, 梦]\n```\n\n不过CRF模型比较耗内存，速度也较慢。\n"
      }
    ]
  }
]